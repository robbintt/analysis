---
ver: rpa2
title: 'DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training
  of Large Language Models'
arxiv_id: '2505.09655'
source_url: https://arxiv.org/abs/2505.09655
tags:
- terms
- block
- reward
- reasoning
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the diversity-quality inconsistency problem
  in Group Relative Policy Optimization (GRPO), where scalar correctness rewards fail
  to capture semantic diversity among reasoning paths. The proposed Diversity-aware
  Reward Adjustment (DRA) method uses Submodular Mutual Information (SMI) to downweight
  redundant completions and amplify rewards for diverse ones, effectively implementing
  inverse propensity scoring.
---

# DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models

## Quick Facts
- arXiv ID: 2505.09655
- Source URL: https://arxiv.org/abs/2505.09655
- Reference count: 40
- Primary result: Achieves 58.2% average accuracy on mathematical reasoning benchmarks using 7,000 training samples and $55 training cost

## Executive Summary
DRA-GRPO addresses the diversity-quality inconsistency problem in Group Relative Policy Optimization (GRPO) by using Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This diversity-aware reward adjustment effectively implements inverse propensity scoring, encouraging better exploration while maintaining exploitation of high-quality samples. The method achieves state-of-the-art performance on mathematical reasoning benchmarks with minimal runtime overhead (6%) and low training cost.

## Method Summary
DRA-GRPO integrates Submodular Mutual Information (SMI) with GRPO by adjusting rewards based on semantic redundancy. For each completion group, SMI measures similarity between completions using cosine similarity over embeddings. The adjusted reward is computed as R̃(q,oi) = R(q,oi) / (1 + Σj∈C\{oi} s(oi,j)), where s is cosine similarity. This downweights redundant completions and amplifies diverse ones. The method maintains GRPO's core advantage computation while implementing inverse propensity scoring through the SMI adjustment, effectively reweighting gradients to account for the model's prior sampling distribution.

## Key Results
- Achieves 58.2% average accuracy across five mathematical reasoning benchmarks
- Outperforms strong baselines including DeepScaleR-1.5B-Preview
- Maintains only 6% runtime overhead while improving performance
- Demonstrates robust performance across different model scales and embedding spaces

## Why This Works (Mechanism)

### Mechanism 1
Semantic redundancy adjustment via Submodular Mutual Information (SMI) converts scalar rewards into structure-aware learning signals. For each completion, SMI measures semantic similarity to other group members using cosine similarity over embeddings, then downweights rewards for redundant completions. This assumes sentence embeddings capture semantic similarity relevant to reasoning path diversity.

### Mechanism 2
SMI-based adjustment implements Inverse Propensity Scoring (IPS), de-biasing gradient estimation against the model's prior sampling distribution. The SMI term approximates a Kernel Density Estimator of the proposal distribution, and scaling rewards by 1/p(o) reweights gradients so all high-reward regions contribute proportionally to their uniqueness rather than sampling frequency.

### Mechanism 3
Diversity-aware rewards create a "repulsive force" against redundancy, expanding policy coverage of the high-reward landscape. By penalizing semantically similar completions, optimization shifts probability mass from over-exploited dominant modes toward sparser novel modes that would otherwise receive insufficient gradient signal.

## Foundational Learning

- **Submodular Functions and Mutual Information**: SMI provides the mathematical foundation for measuring diminishing marginal redundancy. Quick check: Can you explain why submodularity (diminishing returns) is appropriate for modeling semantic redundancy in a group of completions?

- **Importance Sampling and Inverse Propensity Sampling**: The paper's theoretical justification frames SMI adjustment as IPS. Quick check: If samples are drawn from proposal q(x) but we want expectations under p(x), how does importance weighting achieve this?

- **Group Relative Policy Optimization (GRPO)**: DRA is a plug-in enhancement to GRPO. Quick check: Why does GRPO compute advantages by normalizing rewards within a group rather than using an absolute reward scale?

## Architecture Onboarding

- **Component map**: Prompt → 6 completions → rewards + embeddings → similarity matrix → SMI weights → adjusted rewards → advantage computation → policy update
- **Critical path**: vLLM-based sampling → reward computation → embedding encoding → similarity matrix → SMI weight computation → reward adjustment → GRPO training
- **Design tradeoffs**: Embedding model size vs. speed (small-en used for efficiency); group size G vs. IPS quality (larger groups improve KDE but increase O(G²) computation); Graph-Cut vs. Logdet SMI (Graph-Cut is 35× faster with similar accuracy)
- **Failure signatures**: Low Spearman correlation between reward and diversity suggests embedding misalignment; training divergence indicates reward collapse from high similarity; no accuracy gain suggests insufficient group diversity
- **First 3 experiments**: 1) Run vanilla GRPO vs. DRA-GRPO on 1K subset to verify ~2% accuracy gain and 6% runtime overhead; 2) Swap embedding models to confirm consistent gains across variants; 3) Compare Graph-Cut vs Logdet SMI to verify speed-accuracy tradeoff

## Open Questions the Paper Calls Out
- **Logical correctness validation**: How can the framework be extended to validate the logical correctness of intermediate reasoning steps in diverse paths?
- **Domain generalization**: Does DRA-GRPO generalize to non-mathematical domains with subjective or complex reward landscapes?
- **Scalability to larger models**: Does the performance improvement from diversity adjustment scale to models with significantly larger parameter counts?

## Limitations
- Embedding alignment uncertainty: Sentence embeddings may not capture semantic diversity relevant to reasoning paths
- IPS mechanism validation gap: Theoretical IPS connection lacks direct empirical validation in GRPO context
- Scalability constraints: O(G²) complexity limits practical group sizes for very large models

## Confidence
- **High confidence**: Empirical claim of 58.2% accuracy improvement with demonstrated 6% runtime overhead and $55 training cost
- **Medium confidence**: Theoretical justification via IPS and interpretation as "repulsive force" encouraging exploration
- **Low confidence**: Universal benefit claim across all reasoning tasks, given focus on mathematical reasoning with multiple solution paths

## Next Checks
1. Measure Spearman correlation between SMI diversity scores and human-judged reasoning path uniqueness to validate embedding alignment
2. Systematically vary group size G from 2 to 16 to characterize scalability boundary and identify optimal tradeoff
3. Implement SMI weight permutation variant to test whether gains stem from IPS correction versus other diversity effects