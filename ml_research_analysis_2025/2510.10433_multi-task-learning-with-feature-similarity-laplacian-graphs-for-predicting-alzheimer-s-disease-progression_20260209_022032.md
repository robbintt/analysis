---
ver: rpa2
title: Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting
  Alzheimer's Disease Progression
arxiv_id: '2510.10433'
source_url: https://arxiv.org/abs/2510.10433
tags:
- feature
- disease
- learning
- time
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of predicting Alzheimer\u2019\
  s Disease (AD) progression using longitudinal data. While Multi-Task Learning (MTL)\
  \ has shown promise for modeling temporal cognitive changes, existing methods often\
  \ overlook the time-varying nature of feature correlations and rely on overly simplistic\
  \ feature-sharing assumptions."
---

# Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression

## Quick Facts
- **arXiv ID:** 2510.10433
- **Source URL:** https://arxiv.org/abs/2510.10433
- **Reference count:** 29
- **Key outcome:** Proposed MTL-FSL framework achieves state-of-the-art performance predicting ADAS and MMSE scores on ADNI dataset by modeling time-varying feature correlations with Feature Similarity Laplacian penalty.

## Executive Summary
This study addresses the challenge of predicting Alzheimer's Disease (AD) progression using longitudinal data. While Multi-Task Learning (MTL) has shown promise for modeling temporal cognitive changes, existing methods often overlook the time-varying nature of feature correlations and rely on overly simplistic feature-sharing assumptions. To address this, we propose a novel Feature Similarity Laplacian graph Multi-Task Learning (MTL-FSL) framework. Our approach explicitly models the dynamic relationships among features using a Feature Similarity Laplacian (FSL) penalty, capturing time-varying feature correlations while ensuring temporal smoothness. Experiments on the ADNI dataset demonstrate that MTL-FSL achieves state-of-the-art performance, outperforming various baseline methods in predicting ADAS and MMSE scores.

## Method Summary
The method predicts ADAS-Cog or MMSE scores at six time points (baseline, M06, M12, M24, M36, M48) using 314 MRI features from ADNI. It constructs a fused feature correlation matrix S by computing Pearson correlations at each time point, weighting by patient counts, thresholding at τ, and applying Laplacian regularization. The optimization combines squared loss with L1 sparsity, FSL penalty, and fused lasso regularization, solved via ADMM with soft-thresholding updates. The framework was evaluated using rMSE, nMSE, and weighted correlation (wR) metrics across tasks.

## Key Results
- MTL-FSL achieves state-of-the-art performance on ADNI dataset for predicting ADAS and MMSE scores
- Outperforms baseline methods including standard MTL, L21MTL, cFSGL, and TGL
- Identifies stable MRI biomarkers associated with disease progression through longitudinal stability selection

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Feature Correlation Modeling
- **Claim:** Explicitly modeling time-varying feature correlations improves longitudinal prediction accuracy over static feature-sharing assumptions.
- **Mechanism:** The FSL penalty constructs a fused feature correlation matrix S by computing Pearson correlations at each time point, weighting by patient counts, thresholding at τ, and applying Laplacian regularization (‖SW‖₁). This forces strongly correlated features to have similar weights while allowing uncorrelated features to diverge, capturing the dynamic importance of biomarkers across disease stages.
- **Core assumption:** Strongly correlated MRI features should exhibit minimal differences in model weights.
- **Evidence anchors:** [abstract]: "Our framework introduces a novel Feature Similarity Laplacian (FSL) penalty that explicitly models the time-varying relationships between features."

### Mechanism 2: Joint Temporal Smoothness Regularization
- **Claim:** Joint temporal smoothness regularization stabilizes predictions at sparse later time points.
- **Mechanism:** The fused lasso penalty (λ₃‖WH‖₁) enforces small differences between consecutive time point predictions. Combined with multi-task parameter sharing, this allows later time points (e.g., M36, M48 with fewer patients) to borrow statistical strength from earlier, data-rich time points.
- **Core assumption:** Cognitive decline is gradual; adjacent time points have similar scores.
- **Evidence anchors:** [abstract]: "By simultaneously considering temporal smoothness among tasks and the dynamic correlations among features..."

### Mechanism 3: Efficient ADMM Optimization
- **Claim:** ADMM decomposition enables efficient optimization of the non-smooth composite objective.
- **Mechanism:** The objective combines squared loss, L1 sparsity (λ₁‖W‖₁), FSL penalty (λ₂‖SW‖₁), and fused lasso (λ₃‖WH‖₁). ADMM introduces slack variables Q, P, V and solves via alternating updates with closed-form soft-thresholding solutions.
- **Core assumption:** The problem is convex with a unique global minimum.
- **Evidence anchors:** [abstract]: "To solve the non-smooth optimization problem arising from our proposed penalty terms, we adopt the Alternating Direction Method of Multipliers (ADMM) algorithm."

## Foundational Learning

- **Concept:** Multi-Task Learning (MTL) with shared representations
  - **Why needed here:** Understanding why prediction at each time point is treated as a separate but related task.
  - **Quick check question:** Can you explain why predicting MMSE at M12 and M24 should share information rather than be modeled independently?

- **Concept:** Graph Laplacian regularization
  - **Why needed here:** The FSL penalty applies Laplacian-style smoothing over feature correlations.
  - **Quick check question:** Given a feature correlation matrix S, what does the penalty ‖SW‖₁ encourage when sₘ,ₗ is large and positive?

- **Concept:** ADMM with soft-thresholding (proximal operators)
  - **Why needed here:** Implementing the Q, P, V updates requires understanding the soft-thresholding operator.
  - **Quick check question:** For θᵢⱼ = 0.5 and λ₁/ρ = 0.3, what is qᵢⱼ after the soft-thresholding update?

## Architecture Onboarding

- **Component map:** Data preprocessing (standardize features) -> Correlation computation (Pearson per time point) -> Fusion module (weighted average → threshold → matrix S) -> Optimization core (ADMM loop) -> Output (learned weight matrix W)
- **Critical path:** Correlation fusion (τ selection) → ADMM convergence → longitudinal stability selection for biomarker interpretation. The threshold τ is the key hyperparameter controlling graph sparsity.
- **Design tradeoffs:**
  - Lower τ: Denser graph, more feature relationships captured, risk of spurious correlations
  - Higher τ: Sparser graph, only strong correlations retained, may miss weak but meaningful relationships
  - λ₂ vs λ₃: FSL penalty (feature correlation) vs fused lasso (temporal smoothness) balance
- **Failure signatures:**
  - Convergence stalls → check ρ scaling or positive definiteness of Lᵢ
  - All weights sparse to zero → λ₁ too large
  - No differentiation across time points → λ₃ too large (over-smoothing)
  - Correlation matrix S is near-identity → τ too high or features already decorrelated
- **First 3 experiments:**
  1. Reproduce convergence curve (Figure 2): Run ADMM on ADAS/MMSE with default λ values; verify convergence at ~1500 iterations.
  2. Ablation on FSL penalty (λ₂): Compare performance with λ₂ = 0 (no FSL) vs. optimal λ₂; quantify contribution of feature correlation modeling.
  3. Threshold τ sensitivity: Sweep τ ∈ {0.1, 0.3, 0.5, 0.7} and observe impact on nMSE/wR and number of stable biomarkers identified.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MTL-FSL framework generalize effectively to other neurodegenerative diseases such as Parkinson's disease, or is the FSL penalty design specific to AD pathology patterns?
- **Basis in paper:** [explicit] The conclusion states: "For future work, we plan to extend the application of MTL-FSL to other neurodegenerative diseases, such as Parkinson's disease, to validate its generalizability."
- **Why unresolved:** The paper only evaluates on ADNI data for AD progression. Different neurodegenerative diseases may exhibit different temporal correlation patterns among biomarkers.
- **What evidence would resolve it:** Application of MTL-FSL to Parkinson's disease progression datasets (e.g., PPMI) with comparison of learned feature correlation structures against those from AD.

### Open Question 2
- **Question:** How does the FSL penalty interact with multi-modal biomarkers (e.g., genetic, PET imaging) that may have fundamentally different correlation dynamics compared to MRI features alone?
- **Basis in paper:** [explicit] The conclusion states: "Furthermore, we will explore the integration of multi-modal data, such as genetic and PET imaging data, to further enhance the model's predictive power and interpretability."
- **Why unresolved:** The current study uses only MRI features (314 features). Cross-modal feature correlations may exhibit different time-varying behaviors that the current FSL formulation may not capture optimally.
- **What evidence would resolve it:** Experiments combining MRI, PET, and genetic data from ADNI, analyzing whether the unified FSL penalty adequately captures cross-modal relationships or requires modification.

### Open Question 3
- **Question:** Is the linear relationship assumption between features and cognitive scores sufficient, or would incorporating non-linear relationships better capture the complex disease progression dynamics?
- **Basis in paper:** [inferred] The method section states: "We assume a linear relationship between the features and the targets and employ the squared loss function."
- **Why unresolved:** AD progression is inherently non-linear, and the linear assumption may limit capturing complex biomarker-score relationships, though it maintains interpretability.
- **What evidence would resolve it:** Comparative experiments with kernel-based or neural network extensions of the FSL framework, evaluating whether non-linear modeling improves prediction accuracy while preserving feature correlation interpretability.

## Limitations

- The method relies on strong assumptions about feature correlation stability across disease progression
- The specific Laplacian formulation lacks direct comparative validation against alternative graph constructions
- The exclusive focus on MRI features omits potentially valuable clinical and genetic data

## Confidence

- **High Confidence:** The MTL framework structure, ADMM optimization procedure, and convergence behavior are well-established and reproducible
- **Medium Confidence:** The FSL penalty's effectiveness in capturing dynamic feature relationships is supported by performance improvements over baselines
- **Low Confidence:** The biomarker stability interpretations require additional validation, as they depend on both the model's feature selection and the quality of the fused correlation matrix S

## Next Checks

1. **Temporal correlation stability:** Compute test-retest reliability of feature correlations across time points to quantify how stable the fused matrix S is, and whether it introduces noise.
2. **Ablation study with alternative graph constructions:** Replace the Laplacian FSL penalty with alternative graph-based regularizers (e.g., graph attention, dynamic Bayesian networks) to assess whether the specific Laplacian formulation is optimal.
3. **External validation on independent cohort:** Apply MTL-FSL to a separate ADNI phase or an independent AD cohort to verify that biomarker stability findings generalize beyond the training population.