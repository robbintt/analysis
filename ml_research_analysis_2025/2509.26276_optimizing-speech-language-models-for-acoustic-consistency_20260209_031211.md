---
ver: rpa2
title: Optimizing Speech Language Models for Acoustic Consistency
arxiv_id: '2509.26276'
source_url: https://arxiv.org/abs/2509.26276
tags:
- speech
- text
- acoustic
- speech-only
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve consistency in speech language
  models through LM-side design rather than architectural changes. The authors propose
  CAST, which initializes speech embeddings using self-supervised features, applies
  a light alignment loss, and trains with thinning and auxiliary planning objectives
  to enhance acoustic stability.
---

# Optimizing Speech Language Models for Acoustic Consistency

## Quick Facts
- arXiv ID: 2509.26276
- Source URL: https://arxiv.org/abs/2509.26276
- Reference count: 0
- Primary result: 0.7B speech-only model achieves 90.8% speaker and 90.0% gender consistency, outperforming 7B baselines

## Executive Summary
This paper investigates improving consistency in speech language models through LM-side design rather than architectural changes. The authors propose CAST, which initializes speech embeddings using self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary planning objectives to enhance acoustic stability. They train three models: 0.7B and 1.0B speech-only models, and a 1.0B interleaved text-speech model. Acoustic studies on SALMON show the 0.7B speech-only model achieves 90.8% speaker consistency and 90.0% gender consistency, outperforming larger baselines including 7B models.

## Method Summary
The approach uses WavTokenizer for codec-based audio encoding, extended with speech token embeddings initialized from HuBERT SSL centroids via a learned projection. A stop-gradient alignment loss encourages hidden states to stay near SSL features without backpropagating through the encoder. Multi-rate thinning and span erasure during training enforce invariance to timing jitter and missing context. Coarse and next-code auxiliary losses encourage hierarchical planning. Three variants are trained: 0.7B and 1.0B speech-only models (with reduced text vocabulary for 0.7B), and a 1.0B interleaved text-speech model where text covers 35-55% of utterance duration.

## Key Results
- Speech-only 0.7B achieves 90.8% speaker and 90.0% gender consistency, surpassing 7B baselines
- Interleaving improves sWUGGY from 65.6 to 73.7 and sBLiMP from 55.9 to 58.3 but reduces acoustic consistency by 5-9 points
- Linear probes show initialization biases toward content structure (+16% on content tasks) while slightly reducing prosody detail (-7%)

## Why This Works (Mechanism)

### Mechanism 1: SSL Initialization Bias
Self-supervised initialization of speech token embeddings biases representations toward phonetic structure, improving content modeling while trading off prosodic detail. Codec indices are mapped to embeddings initialized from HuBERT SSL centroids via a learned projection P, with a stop-gradient alignment loss encouraging hidden states to stay near SSL features. This gives the LM a phonetic head start rather than forcing it to rediscover speech structure from random initialization.

### Mechanism 2: Multi-Rate Thinning and Span Erasure
Multi-rate thinning and span erasure during training enforce invariance to timing jitter and missing context, improving acoustic consistency at inference. Speech sequences are subsampled at rates r ∈ {1,2,3,4} and random spans are erased with probability p_erase, forcing the model to predict accurately under degraded temporal context.

### Mechanism 3: Coarse-to-Fine Auxiliary Supervision
Delayed coarse and next-code auxiliary losses encourage hierarchical planning (content first, acoustics second), improving speaker and gender consistency. Codec centroids are clustered into K buckets, each token receives a coarse label b_t, and an auxiliary loss L_coarse predicts b_t before fine acoustic prediction, biasing the model toward global acoustic planning.

## Foundational Learning

- **Concept: Residual Vector Quantization (RVQ) codecs**
  - Why needed here: WavTokenizer produces discrete 4096-code tokens at 24kHz. Understanding that these tokens are optimized for reconstruction (not phonetics) explains why CAST introduces semantic initialization and alignment losses.
  - Quick check question: If you replace WavTokenizer with a different codec, which components of CAST would need re-initialization?

- **Concept: Decoder-only language modeling over mixed vocabularies**
  - Why needed here: The LM treats speech tokens and BPE text tokens as a unified vocabulary with next-token prediction. Interleaving text and speech changes the training distribution, affecting the stability-grounding tradeoff.
  - Quick check question: What happens to the loss when the model predicts a speech token vs. a text token?

- **Concept: Linear probing for representation analysis**
  - Why needed here: ARCH probes use linear classifiers on frozen LM features to assess content vs. prosody bias. This reveals tradeoffs (+16% content, -7% prosody) that raw benchmarks may not show.
  - Quick check question: If a probe achieves high accuracy on VIVAE (prosody-leaning) but low on ESC-50 (content-leaning), what does that imply about the representation?

## Architecture Onboarding

- **Component map:** Frozen WavTokenizer (codec) -> LM tokenizer (BPE text + [Sp1]–[Sp4096] + delimiters) -> Embedding layer (HuBERT-initialized speech embeddings) -> Decoder-only transformer -> Losses (next-token + L_ssl + L_coarse + L_next)

- **Critical path:** 1) Pre-compute HuBERT centroids for each codec index 2) Initialize speech embeddings via projection P(μ_k) + noise 3) Apply thinning/erasure to audio sequences during training 4) Forward pass with auxiliary coarse/next-code heads 5) Inference: mask logits to {[Sp*], </s>}, decode via frozen codec

- **Design tradeoffs:** Speech-only vs. interleaved: speech-only maximizes acoustic consistency; interleaving improves sWUGGY/sBLiMP but drops consistency 5-9 points. 0.7B vs. 1.0B: smaller vocab (56k vs. 262k text tokens) reduces parameters 24% without changing attention; 0.7B achieves best consistency. Semantic initialization vs. random: improves content tasks (+16%) but reduces prosody (-7%).

- **Failure signatures:** Speaker/gender consistency drops >5 points → check if auxiliary losses are disabled. Prosody degradation (flat pitch, timing) → initialization may over-bias toward content; consider reducing L_ssl weight. Interleaved model collapses to text-only prediction → verify speech span masking and modality delimiter handling.

- **First 3 experiments:**
  1. Train linear probes on frozen embeddings from random vs. SSL-initialized models on ESC-50 and VIVAE. Confirm +16% content / -7% prosody tradeoff.
  2. Disable L_coarse and L_next, measure SALMON speaker/gender scores. Expect 5-7 point drop.
  3. Vary r ∈ {1,2,4,8} and p_erase ∈ {0.0, 0.1, 0.3}. Plot consistency vs. naturalness (MOS) to find operating point.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the stability-grounding trade-off be mitigated through multi-stage training or curriculum strategies that first establish acoustic consistency before introducing interleaved text? The conclusion states this trade-off "is critical" for dialogue and creative tasks, but interleaving reduces acoustic consistency by 5-9 points while improving semantics.

- **Open Question 2:** How should the balance between content structure bias and prosodic detail be controlled within the semantic initialization framework for applications requiring expressive speech? The authors report that their "initialization biases the model toward content structure while trading off prosody detail," but offer no mechanism to tune this trade-off per application.

- **Open Question 3:** How does the interface extend to real-time spoken dialogue where latency constraints and turn-taking require continuous generation rather than fixed-length continuation? The conclusion proposes "extending this interface to interactive tasks such as spoken dialogue" as future work, but all current evaluations use pre-recorded continuation and scoring paradigms.

## Limitations

- **Mechanism validation gaps:** The paper presents correlations between design choices and performance but lacks direct causal evidence for key mechanisms, particularly whether HuBERT initialization actually improves stability compared to random initialization.
- **Dataset composition opacity:** The People's Speech subset used for training is described only as "clean" without specifying duration, domain distribution, or quality metrics.
- **Evaluation granularity limits:** The paper doesn't report speaker consistency disaggregated by demographic factors (age, accent, speaking style), which would reveal whether improvements are uniform or concentrated in certain speaker types.

## Confidence

**High confidence:** The correlation between interleaving text and improved semantic probes (sWUGGY: 73.7 vs 65.6; sBLiMP: 58.3 vs 55.9) while degrading acoustic consistency is well-supported by the presented data. The linear probe results showing initialization bias (+16% content, -7% prosody) are also robust and directly measurable.

**Medium confidence:** The claim that auxiliary losses (coarse and next-code) improve hierarchical planning and thus consistency is plausible but not definitively proven. The 5-7 point drop when removing these losses is observed, but the mechanism (planning vs. regularization) is inferred rather than demonstrated.

**Low confidence:** The assertion that HuBERT initialization specifically transfers "linguistically-relevant structure" to codec prediction is the weakest claim. The paper shows initialization correlates with content bias, but doesn't establish that this transfer is linguistically meaningful rather than an artifact of centroid clustering.

## Next Checks

1. **Initialization ablation study:** Train two 0.7B speech-only models from scratch - one with HuBERT initialization and one with random initialization (same projection architecture). Compare SALMON speaker/gender consistency and ARCH linear probe scores to isolate the initialization effect from other CAST components.

2. **Thinning/erasure disentanglement:** Run three variants of the 0.7B model: (a) thinning only (p_erase=0), (b) erasure only (r=1), and (c) both. Measure consistency and naturalness to determine whether timing invariance or context robustness drives the improvements.

3. **Speaker consistency demographics:** Re-run SALMON speaker consistency on a dataset with annotated speaker demographics (age, gender, accent). Analyze whether the 90.8% overall consistency holds across different speaker groups or shows systematic variance that could indicate bias in the LM-side approach.