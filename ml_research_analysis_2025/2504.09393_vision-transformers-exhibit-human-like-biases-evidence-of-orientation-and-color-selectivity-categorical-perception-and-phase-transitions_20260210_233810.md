---
ver: rpa2
title: 'Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and
  Color Selectivity, Categorical Perception, and Phase Transitions'
arxiv_id: '2504.09393'
source_url: https://arxiv.org/abs/2504.09393
tags:
- color
- angle
- line
- noise
- orientation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether Vision Transformers (ViTs) exhibit\
  \ human-like perceptual biases in orientation and color processing. Using synthetic\
  \ datasets with controlled variations in noise, angles, lengths, widths, and colors,\
  \ the research finds that ViTs display an oblique effect\u2014showing lowest angle\
  \ prediction errors at 180\xB0 (horizontal) across all conditions."
---

# Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions

## Quick Facts
- **arXiv ID:** 2504.09393
- **Source URL:** https://arxiv.org/abs/2504.09393
- **Reference count:** 5
- **Primary result:** Vision Transformers show human-like perceptual biases in orientation and color processing, with lowest angle errors at horizontal (180°) and color categories aligning with human perception.

## Executive Summary
This study investigates whether Vision Transformers (ViTs) exhibit human-like perceptual biases using synthetic datasets with controlled variations in noise, angles, lengths, widths, and colors. The research finds that ViTs display an oblique effect—showing lowest angle prediction errors at 180° (horizontal) across all conditions—and that angle prediction errors vary systematically by color, with bluish hues producing highest errors and yellowish hues the lowest. Clustering analysis revealed that ViTs grouped colors in alignment with human perceptual categories. The study also observed phase transitions during training, with delayed transitions when color was added as a data attribute, suggesting these biases arise from pre-training and architectural constraints rather than solely from downstream data statistics.

## Method Summary
The study used a frozen ViT-Base backbone with LoRA(r=8) adapters on attention layers, trained on synthetic 224×224 images across four progressively complex datasets: (I) angle-only with fixed parameters, (II) variable lengths, (III) variable widths, and (IV) variable colors. Models predicted multiple properties (angle, coordinates, length, width, color RGB) using weighted Huber loss. Training employed AdamW with learning rate scheduling and early stopping. Evaluation focused on angle prediction error distributions across orientations and colors, correlation coefficients, and phase transition patterns in loss curves.

## Key Results
- ViTs exhibited an oblique effect with lowest angle prediction errors at 180° (horizontal) across all tested conditions, independent of line length, width, and color.
- Angle prediction errors varied by color, with bluish hues producing highest errors and yellowish hues producing lowest errors.
- Clustering analysis showed colors grouped into categories (Blue/Purple, Green/Teal, Magenta/Red, Cyan/Yellow/White/Pink/Orange) aligning with human perceptual categories.
- Two distinct phase transitions occurred in training loss curves, with delayed transitions when color was incorporated as an additional data attribute.

## Why This Works (Mechanism)

### Mechanism 1: Orientation Selectivity Emerges from Pre-training and Architecture
- **Claim:** ViTs exhibit preferential processing for horizontal orientations (180°), producing lowest angle prediction errors across all tested conditions.
- **Mechanism:** The pre-trained ViT backbone encodes anisotropic feature representations shaped by natural scene statistics where horizontal/vertical orientations dominate. These representations persist through LoRA fine-tuning, which adapts attention layers but preserves foundational feature extractors.
- **Core assumption:** Biases originate from both pre-training on natural images and inherent architectural constraints.
- **Evidence anchors:** Lowest angle errors at 180° across all datasets; pre-training encodes orientation statistics from natural scenes.
- **Break condition:** If a randomly initialized ViT fine-tuned on uniform orientation distributions shows no horizontal bias, pre-training is the dominant cause.

### Mechanism 2: Color-Orientation Error Coupling Reflects Categorical Sensitivity
- **Claim:** Angle prediction errors vary systematically by line color, with blue/purple hues producing highest errors and yellowish/orange hues producing lowest errors.
- **Mechanism:** The ViT's feature representations encode color with non-uniform sensitivity reflecting statistical regularities from pre-training. When color is added as a task attribute, it introduces noise sensitivity that degrades angle prediction, particularly for hues with weaker feature fidelity.
- **Core assumption:** Color categories emerge from representations shaped during pre-training, and color interferes with geometric task performance when jointly predicted.
- **Evidence anchors:** Blue/purple hues show highest angle errors; yellowish/orange hues show lowest; colors cluster into groups aligning with human perceptual categories.
- **Break condition:** If color error patterns reverse when pre-training uses images with balanced chromatic distributions, dataset statistics are the cause.

### Mechanism 3: Phase Transitions Mark Hierarchical Circuit Formation
- **Claim:** Training loss curves exhibit two distinct phase transitions across all conditions, with timing delayed when dataset complexity increases.
- **Mechanism:** Phase transitions correspond to formation of specialized circuits. Earlier transitions with simpler datasets indicate rapid circuit formation; delayed transitions with complex datasets indicate higher abstraction thresholds requiring more evidence accumulation.
- **Core assumption:** Phase transitions are not optimization artifacts but reflect critical periods of circuit formation modulated by data characteristics.
- **Evidence anchors:** Two distinct phase transitions in loss curves; delayed transitions with complex datasets; synchronized emergence in both loss curves and feature correlation dynamics.
- **Break condition:** If phase transitions vanish under different optimizers or with architectural modifications, the phenomenon may be optimizer- or architecture-specific.

## Foundational Learning

- **Concept: Vision Transformer Patch Embedding**
  - **Why needed here:** Explains how ViTs process 224×224 images as sequences of patches, creating tokenized representations where orientation and color biases manifest at the token level.
  - **Quick check question:** Given a 224×224 input image with 16×16 patches, how many tokens does the ViT process (excluding CLS token)?

- **Concept: LoRA Fine-Tuning (Low-Rank Adaptation)**
  - **Why needed here:** The study freezes the ViT backbone and adapts only attention layers via LoRA (rank=8), constraining what can be learned while preserving pre-trained biases.
  - **Quick check question:** If LoRA adds low-rank matrices A and B to a weight matrix W (where W has dimension d×d and LoRA rank r), what is the parameter count of the adaptation compared to full fine-tuning?

- **Concept: Attention Head Specialization**
  - **Why needed here:** Heads 4, 5, and 12 in Layer II exhibit task-agnostic edge-detection behavior across different fine-tuning tasks, suggesting certain heads stabilize into interpretable functions.
  - **Quick check question:** What evidence would distinguish between "inherent architectural specialization" versus "task-driven specialization" for a given attention head?

## Architecture Onboarding

- **Component map:** Image → Patch embedding → Positional encoding → Transformer encoder with LoRA-adapted attention → CLS token extraction → Shared feature representation → Parallel regression heads → Multi-task outputs
- **Critical path:** Image → Patch embedding → Positional encoding → Transformer encoder with LoRA-adapted attention → CLS token → Multi-task regression heads
- **Design tradeoffs:**
  - Frozen vs. full fine-tuning: Freezing preserves pre-trained biases but limits task adaptation; LoRA offers a middle ground
  - Single vs. multi-task heads: Multi-task learning introduces interference—color degrades angle accuracy under noise
  - Synthetic vs. natural data: Synthetic enables controlled attribution; generalization to real-world data is unverified
- **Failure signatures:**
  - High angle errors for blue/purple lines with thin widths under noise → likely color-width-noise interaction
  - Delayed or absent phase transitions → may indicate insufficient training steps, learning rate issues, or data too uniform
  - Inconsistent attention head specialization across runs → suggests stochastic initialization effects
- **First 3 experiments:**
  1. **Ablate pre-training:** Train a randomly initialized ViT on Dataset I; compare orientation bias and phase transition patterns to pre-trained baseline.
  2. **Isolate color-orientation interference:** Train separate models for angle-only and angle+color tasks on identical data; quantify error increase attributable to color coupling.
  3. **Probe attention head consistency:** Fine-tune the same pre-trained ViT on object detection; test whether heads 4, 5, 12 in Layer II retain edge-detection behavior or re-specialize.

## Open Questions the Paper Calls Out

- **Do the observed orientation and color biases persist when testing Vision Transformers on naturalistic datasets?**
  - The authors note that "simple line stimuli may not fully capture perceptual biases in real-world scenarios" and suggest future work test on naturalistic data.

- **Do these perceptual biases generalize to tasks other than regression, such as classification or object detection?**
  - The paper states the ViT was trained on a regression task, "leaving open whether biases generalize to classification or generative tasks."

- **What are the specific mechanistic circuits within the ViT responsible for the observed orientation selectivity?**
  - The authors state that "the specific ViT circuits underlying orientation and color biases remain unclear" despite the observation of phase transitions.

## Limitations

- Synthetic datasets may not fully capture the complexity of natural image distributions where human perceptual biases originate.
- Fixed ViT-Base architecture and specific pre-training constrain generalizability to different architectures or pre-training datasets.
- Multi-task learning setup introduces interference effects that complicate attribution of color degradation to categorical perception.

## Confidence

- **High confidence:** Orientation selectivity (oblique effect) showing minimum angle errors at 180° across all conditions.
- **Medium confidence:** Color-orientation error coupling and categorical clustering, though causal mechanism requires further validation.
- **Low confidence:** Phase transition interpretations as circuit formation milestones, which could represent various optimization phenomena.

## Next Checks

1. **Pre-training ablation:** Train an identical ViT architecture from scratch on uniform orientation distributions and compare orientation bias patterns to the pre-trained model.
2. **Color interference isolation:** Train separate models for angle-only versus angle+color tasks on identical datasets with balanced color distributions; quantify specific degradation in angle accuracy.
3. **Attention head consistency across tasks:** Fine-tune the same pre-trained ViT on object detection or segmentation and test whether heads 4, 5, and 12 in Layer II retain edge-detection behavior or re-specialize.