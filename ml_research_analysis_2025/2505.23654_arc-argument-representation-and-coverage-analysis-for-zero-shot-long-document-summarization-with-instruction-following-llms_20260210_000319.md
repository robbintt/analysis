---
ver: rpa2
title: 'ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document
  Summarization with Instruction Following LLMs'
arxiv_id: '2505.23654'
source_url: https://arxiv.org/abs/2505.23654
tags:
- argument
- roles
- summary
- coverage
- canlii
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARC, a framework for evaluating how well
  long-context LLM summaries preserve salient arguments. ARC decomposes argument roles
  into atomic facts and measures coverage hierarchically, distinguishing between missing
  and factually incorrect content.
---

# ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs

## Quick Facts
- arXiv ID: 2505.23654
- Source URL: https://arxiv.org/abs/2505.23654
- Authors: Mohamed Elaraby; Diane Litman
- Reference count: 40
- Key outcome: ARC framework shows LLM summaries frequently omit critical arguments, with larger models performing better but still missing substantial content, especially in middle document positions.

## Executive Summary
This paper introduces ARC, a framework for evaluating how well long-context LLM summaries preserve salient arguments. ARC decomposes argument roles into atomic facts and measures coverage hierarchically, distinguishing between missing and factually incorrect content. It is applied to two domains: legal opinions (CANLII) and scientific articles (DRI). Results show that while LLMs capture some argument roles, they frequently omit critical information, especially when arguments are sparsely distributed. ARC also reveals systematic biases: models exhibit positional bias, favoring content at the start or end of documents, and role-specific bias, with conclusions often overrepresented compared to issues and reasons. Among evaluated models, larger models generally perform better, but coverage remains incomplete even for the best-performing models. ARC correlates strongly with human judgments and provides interpretable, multi-level diagnostics for improving summarization in high-stakes domains.

## Method Summary
ARC evaluates LLM-generated summaries by first decomposing salient argument roles (Issue, Reason, Conclusion in legal texts; Own Claim, Background Claim, Data in scientific articles) into atomic facts using GPT-4o with entailment filtering. Each atomic fact is then evaluated against the summary via an LLM judge, which classifies it as supported, missing, or contradicted. Coverage is computed bottom-up: individual facts → argument roles (ARCrole) → overall summary (ARCscore). The framework is applied to 8 open-weight LLMs on CANLII (1049 legal cases) and DRI (40 scientific articles) datasets, using generic prompts and greedy decoding with 2048 token limits.

## Key Results
- Coverage scores range from 0.29-0.50 across models, with larger models generally performing better
- Missing facts dominate error types across both datasets, indicating omission rather than hallucination is the primary issue
- Models show U-shaped positional bias, favoring content at document start and end
- Conclusions receive significantly higher coverage than issues and reasons in legal documents
- ARC correlates strongly with human judgments (Kendall's τ=0.65-0.77)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition of arguments into atomic facts enables fine-grained coverage evaluation that correlates better with human judgment than lexical overlap metrics.
- Mechanism: The ARC framework first uses a strong LLM (e.g., GPT-4o) to decompose salient argument roles into atomic facts. It then filters these facts via entailment checks against the original argument. Coverage is computed bottom-up: an argument role is "covered" only if its constituent atomic facts are supported by the summary.
- Core assumption: Complex argumentative structures can be reliably reduced to independent atomic facts without losing semantic nuance.
- Evidence anchors:
  - [abstract] "ARC decomposes argument roles into atomic facts and measures coverage hierarchically..."
  - [Section 4.1] "We employ a strong LLM (GPT4-o) to decompose $r_i$ into a set of atomic facts... filtered using an entailment check."
  - [corpus] Corpus papers (e.g., ArgCMV) focus on key point extraction, but do not validate the specific atomic decomposition mechanism used here; support is weak for this specific evaluation method.
- Break condition: If the decomposition LLM hallucinates facts that pass the entailment filter but are irrelevant, the downstream coverage scores may inflate or misrepresent actual summary quality.

### Mechanism 2
- Claim: Separating error types (missing vs. non-factual) reveals that coverage gaps in long-context summarization are primarily driven by omission rather than hallucination.
- Mechanism: The framework's $\delta$ function evaluates each atomic fact against the summary, returning a categorical label: *supported*, *missing*, or *contradicted*. By aggregating these, ARC distinguishes between a model failing to retrieve information (missing) versus distorting it (non-factual).
- Core assumption: The evaluation LLM judge can consistently distinguish between a fact being absent versus contradicted in a long summary.
- Evidence anchors:
  - [Section 4.2] "The $\delta$ function distinguishes between two error types: missing facts... and incorrectly covered facts..."
  - [Section 5.2] "Missing facts are the dominant source of error across both datasets... indicating that salient information is more often omitted than misrepresented."
  - [corpus] No direct corpus evidence for this specific error separation mechanism.
- Break condition: If summaries are dense and implicitly contradict facts (e.g., via negation or subtle shifts in meaning), the LLM judge may misclassify "non-factual" as "missing," obscuring the root cause of failure.

### Mechanism 3
- Claim: Role-level aggregation exposes systematic positional and semantic biases in LLMs that summary-level metrics mask.
- Mechanism: By computing $ARC_{role}$ (coverage per argument type) and correlating it with document position, the framework quantifies biases. It uses a specific bias score $\beta$ to show models favor conclusions over reasons or issue statements, particularly in legal texts.
- Core assumption: The distribution of argument roles in the source text is not inherently correlated with the model's attention mechanism in a way that confounds the bias score.
- Evidence anchors:
  - [Section 5.3] "Table 7 shows a consistent negative correlation [for position] in CANLII... confirming that LLM context windows systematically bias coverage."
  - [Section 5.4] "Figure 3 shows that... $\beta$ is consistently lowest for conclusions, indicating stronger coverage of this role."
  - [corpus] Related work like *StrucSum* (arXiv:2505.22950) supports the general finding that LLMs struggle with long-document structure, though it proposes graph-based solutions rather than this specific bias metric.
- Break condition: If arguments of a specific type (e.g., "Reasons") are statistically longer or more complex, the lower coverage might be due to compression difficulty rather than a semantic "role bias."

## Foundational Learning

- Concept: **Atomic Fact Decomposition**
  - Why needed here: ARC relies on breaking down high-level arguments into minimal, verifiable units (facts). Without understanding this granular approach, the hierarchical scoring system cannot be interpreted.
  - Quick check question: Given the sentence "The court dismissed the appeal because the evidence was insufficient," what are two atomic facts?

- Concept: **U-shaped Positional Bias**
  - Why needed here: The paper explicitly diagnoses this bias (favoring start/end of context windows). Understanding this is critical for interpreting why models fail on "middle" arguments.
  - Quick check question: If a legal document has a critical reason in the middle paragraph, will an LLM likely include it in the summary based on the paper's findings?

- Concept: **Argument Mining (IRC Scheme)**
  - Why needed here: The framework evaluates coverage based on specific argument roles: Issue, Reason, and Conclusion (IRC). You must distinguish these to understand the role-specific bias results.
  - Quick check question: In the IRC scheme, does a "Conclusion" represent a legal question or a final ruling?

## Architecture Onboarding

- Component map: Source document + Gold Argument Roles → Decomposer (LLM) → Filter (Entailment) → Judge (LLM) → Aggregator → ARCscore
- Critical path: The **Decomposition and Filtering stage**. If the LLM generates facts that are not strictly entailed by the original argument (false positives), the subsequent evaluation will measure coverage against a "gold standard" that is itself flawed.
- Design tradeoffs:
  - **Recall vs. Precision**: ARC is recall-oriented (checking if source facts appear in summary). It does not penalize the summary for including non-salient facts (precision).
  - **Judge Selection**: The paper shows DeepSeek-R1-Distill-Qwen-14B correlates slightly better with humans than GPT-4o, offering a cheaper open-weight alternative, but requires validation on new domains.
- Failure signatures:
  - **Inflated Scores**: If the Judge LLM is too lenient, it marks "missing" facts as "supported."
  - **Role Collapse**: If the Decomposer fails to distinguish "Reasons" from "Conclusions," the role-specific bias analysis becomes invalid.
- First 3 experiments:
  1. **Judge Consistency Check**: Run ARC on 10 summaries using both GPT-4o and DeepSeek-R1. Measure Kendall's $\tau$ between the two judges to ensure reliability before scaling.
  2. **Positional Stress Test**: Take a legal document and swap the "Issue" (usually at start) and "Conclusion" (usually at end). Run summarization and measure if ARC detects the drop in "start-of-document" coverage.
  3. **Error Profile Analysis**: Generate summaries for 5 documents. Use ARC to categorize errors. If >90% are "Missing" (as the paper suggests), focus engineering efforts on retrieval/context extension rather than hallucination reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating explicit argument structure into training or prompting significantly improve argument coverage in LLM-generated summaries?
- Basis in paper: [explicit] The conclusion states "Future work can extend this framework by incorporating explicit argument structures into training or prompting." Additionally, the argument-aware prompting experiment (Appendix G) showed mixed results, with only Qwen-2.5-3B/7B achieving modest improvements.
- Why unresolved: Simple instruction-based prompts to focus on arguments were insufficient. The paper did not explore fine-tuning, retrieval-augmented generation, or structured argument injection methods.
- What evidence would resolve it: Experiments comparing argument-aware fine-tuning, structured prompting with extracted argument graphs, or retrieval-augmented summarization against baseline zero-shot models on ARC metrics.

### Open Question 2
- Question: How can evaluation frameworks jointly assess both recall (coverage of salient arguments) and precision (avoidance of non-salient content) in high-stakes summarization?
- Basis in paper: [explicit] The Limitations section states "ARC currently emphasizes recall—whether salient argument-role facts are preserved... this ignores precision, i.e., over-inclusion of non-salient content. Future work should jointly assess both dimensions."
- Why unresolved: The precision-recall analysis in Appendix J showed moderate reordering of model rankings, suggesting different optimization targets. No unified metric balancing both was proposed or validated.
- What evidence would resolve it: Development of a combined metric (e.g., F1-style harmonic mean of argument recall and precision) and correlation analysis with human preferences across diverse summarization tasks.

### Open Question 3
- Question: What mechanisms cause LLMs to systematically favor certain argument roles (e.g., conclusions over issues and reasons), and can this bias be mitigated?
- Basis in paper: [explicit] Section 5.4 and Figure 3 show systematic role-specific bias where conclusions receive higher coverage than issues and reasons in legal documents. The bias score β quantifies this but does not explain its cause.
- Why unresolved: The paper documents the bias phenomenon but does not investigate whether it stems from training data distributions, attention patterns, or domain-specific conventions in legal writing.
- What evidence would resolve it: Attention analysis on intermediate layers, experiments with role-balanced training data, or controlled studies varying argument role positions to isolate positional versus role-specific effects.

## Limitations
- ARC relies on LLM-based decomposition and entailment filtering, which may introduce false positives in the "gold standard" atomic facts
- The framework emphasizes recall over precision, not penalizing summaries for including non-salient content
- Analysis is limited to two specific domains (legal and scientific) and two argument annotation schemes

## Confidence
**High Confidence**
- ARC successfully identifies that coverage gaps in LLM summaries are primarily due to omission rather than hallucination
- Models exhibit systematic positional bias favoring content at document start/end
- Larger models generally achieve higher coverage scores
- ARC correlates strongly with human judgments

**Medium Confidence**
- Role-specific bias shows conclusions are consistently better covered than issues or reasons
- The hierarchical decomposition mechanism reliably captures argument structure
- DeepSeek-R1-Distill-Qwen-14B serves as a viable open-weight alternative to GPT-4o for evaluation

**Low Confidence**
- The atomic fact decomposition does not lose semantic nuance from original arguments
- The bias scores would remain consistent if argument types were distributed differently in source texts
- ARC would generalize to domains with different argumentative structures

## Next Checks
1. **Judge Consistency Validation**: Run ARC on 50 randomly selected summaries using both GPT-4o and DeepSeek-R1-Distill-Qwen-14B. Compute Kendall's τ between judges and ensure it exceeds 0.8. If correlation drops below 0.7, investigate cases where judges disagree to identify systematic evaluation differences.

2. **Cross-Domain Generalizability Test**: Apply ARC to a new domain (e.g., news articles or policy documents) with different argumentative structures. Compare coverage patterns to CANLII and DRI to determine if positional and role-specific biases persist across domains.

3. **Error Type Validation**: For 20 summaries with high missing fact counts, manually verify a sample of 5 missing facts per summary to confirm they are truly absent versus the judge failing to recognize implicit mentions. Calculate the false negative rate for the missing fact detection mechanism.