---
ver: rpa2
title: 'YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in
  Online Educational Videos from Learners'' Perspectives'
arxiv_id: '2501.11712'
source_url: https://arxiv.org/abs/2501.11712
tags:
- questions
- question
- cognitive
- education
- youtube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents YouLeQD, a dataset of 57,242 learner-posed
  questions extracted from YouTube educational video comments, and two RoBERTa-based
  classification models for detecting questions and analyzing their cognitive complexity
  using Bloom's Taxonomy. The authors fine-tuned RoBERTa models using Knowledge Distillation
  and data augmentation to classify questions into six cognitive levels (Knowledge,
  Comprehension, Application, Analysis, Evaluation, Synthesis) plus an "Irrelevant"
  category for out-of-distribution examples.
---

# YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in Online Educational Videos from Learners' Perspectives

## Quick Facts
- arXiv ID: 2501.11712
- Source URL: https://arxiv.org/abs/2501.11712
- Authors: Nong Ming; Sachin Sharma; Jiho Noh
- Reference count: 40
- Key outcome: 57,242 learner-posed questions from YouTube educational videos with 84.5% weighted F1 for BT classification and 99.42% F1 for question detection

## Executive Summary
This paper introduces YouLeQD, a dataset of 57,242 learner-posed questions extracted from YouTube educational video comments, and two RoBERTa-based classification models for detecting questions and analyzing their cognitive complexity using Bloom's Taxonomy. The authors fine-tuned RoBERTa models using Knowledge Distillation and data augmentation to classify questions into six cognitive levels (Knowledge, Comprehension, Application, Analysis, Evaluation, Synthesis) plus an "Irrelevant" category for out-of-distribution examples. Analysis revealed that most questions fall under the Knowledge level (44-48% across subjects), with an inverse relationship between question popularity and cognitive complexity, but a positive correlation between higher cognitive levels and interaction rates.

## Method Summary
The authors fine-tuned RoBERTa models for two classification tasks: (1) binary question detection using Knowledge Distillation from GPT-4o as teacher, achieving 99.42% F1, and (2) 7-class Bloom's Taxonomy classification with two-stage training that includes identifying and filtering out irrelevant questions. The BT classifier was first trained on DASQBT dataset, then expanded with GPT-4o augmented questions and 500 low-confidence samples labeled as "Irrelevant". Training used Adam optimizer with lr=1e-05, batch size=16, max sequence length=128, and two dense layers with 0.2 dropout.

## Key Results
- Question detection model achieved 99.42% F1-score using Knowledge Distillation from GPT-4o
- BT classification model reached 84.5% weighted average F1-score on human-labeled samples
- Most questions fall under Knowledge level (44-48% across subjects)
- Inverse relationship between question popularity and cognitive complexity, but positive correlation between higher cognitive levels and interaction rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge Distillation enhances interrogative sentence detection by transferring soft probability distributions from GPT-4o teacher to student RoBERTa model
- **Mechanism**: Student RoBERTa trained on both hard binary labels and soft probability distributions (KL divergence minimization) to learn uncertainty patterns for edge cases
- **Core assumption**: GPT-4o captures nuanced interrogative structures that hard labels miss, transferable via softmax probabilities
- **Evidence anchors**: Section III.B describes KD framework and loss function L = LCE + α·LKL with τ=2, α=2.5
- **Break condition**: Teacher model miscalibration or hallucinations will propagate errors to student model

### Mechanism 2
- **Claim**: Low softmax confidence scores effectively identify Out-of-Distribution "Irrelevant" questions
- **Mechanism**: Thresholding maximum softmax probability filters OOD inputs (rhetorical questions, unrelated topics) into separate "Irrelevant" class
- **Core assumption**: In-distribution educational questions elicit higher confidence than informal YouTube comments
- **Evidence anchors**: Section III.C.2 identifies top 500 low-confidence examples as "Irrelevant" class
- **Break condition**: Overconfident model on OOD data fails to filter noise effectively

### Mechanism 3
- **Claim**: Cognitive complexity creates trade-off between popularity (likes) and interaction (replies)
- **Mechanism**: Higher cognitive questions trigger detailed replies from knowledgeable viewers but lack broad accessibility for likes
- **Core assumption**: Viewers "like" easily understood comments but "reply" to complex discussion points
- **Evidence anchors**: Abstract states inverse relationship between popularity and cognitive complexity, positive correlation with interaction rates
- **Break condition**: Higher-level questions may be confusing rather than stimulating, reducing overall engagement

## Foundational Learning

- **Concept**: **Bloom's Taxonomy (BT)**
  - **Why needed here**: Target label schema for entire classification system; required to interpret model outputs
  - **Quick check question**: Can you distinguish between an "Application" question (using a formula) and an "Analysis" question (comparing methods)?

- **Concept**: **Knowledge Distillation (KD)**
  - **Why needed here**: Primary method boosting Question Detection model performance to 99.42% F1
  - **Quick check question**: Why would a model train on "soft" probability targets instead of just correct "hard" label?

- **Concept**: **Out-of-Distribution (OOD) Detection**
  - **Why needed here**: Addresses gap between curated training data and noisy real-world YouTube comments
  - **Quick check question**: If model sees completely unrelated topic (sports comment on math video), should softmax probability be high or low?

## Architecture Onboarding

- **Component map**: Raw YouTube comments -> Preprocessing (remove emojis, URLs, filter length) -> Stage 1 (Detector: RoBERTa with KD from GPT-4o) -> Binary Class (Question/Not-Question) -> Stage 2 (Classifier: RoBERTa with DASQBT + GPT-4o Augmented + "Irrelevant") -> 7 Classes (6 BT + Irrelevant)

- **Critical path**: Two-stage training of BT Classifier is most critical: (1) train on DASQBT, (2) run inference to find low-confidence samples, (3) label these "Irrelevant", (4) retrain with expanded dataset

- **Design tradeoffs**:
  - GPT-4o Data Augmentation: Lowered DASQBT performance (F1 0.836→0.825) but improved YouTube sample performance; don't optimize solely for academic test set
  - Verb-based Features: YouTube questions lack formal BT verbs (e.g., "differentiate"); model must rely on semantic context, not keyword matching

- **Failure signatures**:
  - Code Snippets: Misclassifies code keywords (e.g., `len`, `int`) as verbs, tagging coding questions as "Application"
  - Informal Language: "How to" questions default to "Application", rhetorical questions may slip through "Irrelevant" filter if confident

- **First 3 experiments**:
  1. Baseline Replication: Fine-tune standard RoBERTa on DASQBT without "Irrelevant" class to establish baseline error rate on noisy YouTube data
  2. OOD Threshold Tuning: Run baseline on YouTube comments, plot softmax probability distribution to find optimal "Irrelevant" cutoff (paper used top 500 lowest)
  3. Data Augmentation Ablation: Train two final models (with/without GPT-4o augmented data), compare confusion matrices specifically on "Analysis" and "Evaluation" classes showing distinct verb mismatches

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does LLM-based data augmentation improve classification performance on noisy, in-the-wild data while degrading performance on curated educational benchmarks?
  - Basis: Paper notes GPT-4o augmentation had negative impact on DASQBT test set but significant improvement on human-labeled YouTube samples
  - Unresolved: Doesn't explain why distribution shift specifically benefits noisy domain
  - Evidence needed: Ablation study comparing linguistic features of GPT-generated vs human questions in curated vs informal contexts

- **Open Question 2**: Do correlations between cognitive complexity and engagement generalize to non-STEM educational contexts?
  - Basis: Methodology explicitly restricts to five STEM subjects, leaving humanities/arts unexplored
  - Unresolved: Cognitive load and engagement patterns may differ significantly in qualitative disciplines
  - Evidence needed: Replication on humanities/social sciences lectures with statistical comparison of correlation coefficients

- **Open Question 3**: Can YouLeQD dataset effectively train AI models to generate questions that stimulate higher cognitive complexity?
  - Basis: Authors state dataset aids development of AI models for education and mention interest in automatic question generation
  - Unresolved: Paper demonstrates classification but not utility for generating questions targeting specific BT levels
  - Evidence needed: Fine-tuning experiment using YouLeQD to train generative model, evaluating generated questions' ability to meet target BT levels in live educational setting

## Limitations
- Findings based on observational YouTube data may not generalize to other educational platforms or formal learning environments
- GPT-4o dependency for Knowledge Distillation and data augmentation creates reproducibility concerns and questions about open-source alternatives
- "Irrelevant" class identification using fixed top-500 selection may not scale well to different dataset sizes or distributions

## Confidence

**High Confidence**: Question detection model's 99.42% F1-score and BT classification model's 84.5% weighted F1-score on human-labeled samples are well-supported by reported methodology and evaluation metrics

**Medium Confidence**: Knowledge Distillation significantly improves question detection performance is supported by metrics, though baseline comparison is not explicitly provided; inverse relationship between popularity and cognitive complexity is based on correlation analysis

**Low Confidence**: Generalizability of engagement patterns across STEM subjects is limited by small human-labeled sample (60 questions per subject) and observational nature of YouTube data; effectiveness of "Irrelevant" class filtering for OOD detection is not thoroughly validated

## Next Checks

1. **Cross-platform validation**: Apply trained models to questions from Coursera, edX, Stack Exchange to test whether observed engagement patterns and classification performance hold across different learning environments

2. **Ablation study on GPT-4o dependency**: Systematically remove GPT-4o components (Knowledge Distillation and data augmentation), retrain models using only original DASQBT dataset, compare performance on academic test set vs real-world YouTube samples

3. **OOD detection threshold optimization**: Implement dynamic threshold based on softmax probability distributions instead of fixed top-500 selection, evaluate multiple thresholds on held-out YouTube data to find optimal balance between capturing irrelevant questions and preserving valid educational content, reporting precision/recall/F1 for "Irrelevant" class specifically