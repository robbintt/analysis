---
ver: rpa2
title: 'Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in Large
  Language Models'
arxiv_id: '2503.11336'
source_url: https://arxiv.org/abs/2503.11336
tags:
- performer
- answer
- rules
- teacher
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Rule-Guided Feedback (RGF), a framework that\
  \ improves LLM performance by enforcing strict rule adherence and encouraging strategic\
  \ information seeking. RGF employs a teacher-student paradigm where a Teacher model\
  \ evaluates the Performer\u2019s outputs against task-specific rules and provides\
  \ constructive feedback."
---

# Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in Large Language Models

## Quick Facts
- arXiv ID: 2503.11336
- Source URL: https://arxiv.org/abs/2503.11336
- Authors: Aissatou Diallo; Antonis Bikakis; Luke Dickens; Anthony Hunter; Rob Miller
- Reference count: 40
- Primary result: RGF achieves 26.5% average accuracy improvement over direct prompting across five reasoning tasks

## Executive Summary
Rule-Guided Feedback (RGF) is a teacher-student dialogue framework that improves LLM reasoning by enforcing strict rule adherence and encouraging strategic information seeking. The framework employs a Teacher model to evaluate a Performer's outputs against task-specific rules and provide corrective feedback referencing specific violated rules. This iterative process maintains solutions within defined constraints while prompting proactive information seeking to resolve uncertainties. Evaluated on Checkmate-in-One puzzles, Sonnet Writing, Penguins-In-a-Table classification, GSM8k, and StrategyQA, RGF consistently outperforms baseline methods.

## Method Summary
RGF implements a multi-agent dialogue system where a Teacher LLM evaluates a Performer LLM's outputs against explicit task rules. The Teacher provides feedback referencing specific rule violations rather than direct answers, forcing the Performer to revise reasoning within constrained boundaries. The framework supports clarifying questions within turns 1-3 to resolve ambiguity before full solution commitment. Expert verification components validate outputs when LLMs lack domain knowledge (e.g., rhyme verification, syllable counting). The system runs for maximum 5 iterations with early termination on valid solutions.

## Key Results
- RGF achieves 26.5% average accuracy improvement over direct prompting across all tasks
- Clarified information seeking yields 15% accuracy gains in Checkmate-in-One when enabled
- Removing Teacher's access to explicit rules degrades evaluation accuracy by 8.5%
- Expert verification becomes necessary when LLMs cannot reliably evaluate domain-specific criteria

## Why This Works (Mechanism)

### Mechanism 1
- Iterative rule-violation feedback reduces cascading errors by catching mistakes before they compound
- Core assumption: Teacher can accurately identify which rule was violated and articulate it clearly
- Evidence: 8.5% evaluation accuracy drop when rules are removed, with 12% increase in undetected violations

### Mechanism 2
- Early clarifying questions resolve ambiguity before committing to incorrect solution paths
- Core assumption: Model can self-detect uncertainty and formulate productive clarifying questions
- Evidence: 15% accuracy decrease across tasks when Performer cannot ask clarifying questions

### Mechanism 3
- External expert verification compensates for LLM domain-knowledge gaps in evaluation
- Core assumption: Reliable programmatic verification exists for the task
- Evidence: Sonnet Writing requires external libraries (pronounce, syllables, pyphen) for rhyme and syllable evaluation

## Foundational Learning

- **Teacher-Student Multi-Agent Architecture**: Why needed? RGF fundamentally relies on role separation—one agent generates, another evaluates. Quick check: Can you explain why the Teacher must not reveal the correct answer directly, only rule violations?

- **Constraint-Satisfaction Problem Formulation**: Why needed? RGF treats reasoning as adhering to explicit natural-language rules. Quick check: If a chess move is legal but doesn't produce checkmate, which rule(s) from Appendix E.1 would be violated?

- **Iterative Refinement with Early Exit**: Why needed? The framework uses max 5 turns with early success termination. Quick check: What is the tradeoff between setting max iterations to 3 vs. 5, based on the ablation results?

## Architecture Onboarding

- Component map: Task description → Rule Generator → Task rules → Performer → Proposed solution → Teacher → Valid/Invalid + feedback → (if Invalid: feedback → Performer) → (if Valid OR max turns: terminate)

- Critical path: Task rules generated once per task, then Performer-Teacher dialogue loop with history tracking, max 5 turns, early stopping on valid solution

- Design tradeoffs: Temperature=0 ensures reproducibility but reduces exploration; question window balances early ambiguity resolution against gaming; expert verification adds latency but critical for objective evaluation; max turns=5 optimal per ablation

- Failure signatures: Feedback ignored (Performer repeats wrong answers), Teacher hallucinates violations (feedback references non-existent rules), premature termination (valid solution rejected), infinite clarification loop (Performer keeps asking questions)

- First 3 experiments: 1) Baseline replication on Penguins-in-a-Table to validate environment, 2) Ablation on question window thresholds on Checkmate-in-One to verify 15% accuracy drop, 3) Iteration limit sweep on GSM8k to reproduce 13% improvement curve

## Open Questions the Paper Calls Out

1. Can reward-based alignment improve Performer's integration of Teacher feedback compared to current prompt-based approach? The paper suggests future work could use reward systems to better align Performer with Teacher.

2. How does RGF perform with different model families or smaller models as Teacher/Performer? All experiments use GPT-4; generalizability to other architectures untested.

3. What is the accuracy-vs-cost tradeoff given RGF's higher token usage? No cost-benefit analysis provided despite token count differences (386.1 vs 556.4 tokens for Checkmate).

4. How robust is RGF when expert verification is unavailable for tasks with subjective criteria? Framework's reliance on Teacher's domain knowledge creates uncertainty for non-verifiable domains.

## Limitations
- Applicability constrained to domains with verifiable rules or programmatic expert verification
- Framework relies heavily on explicit rule completeness; ambiguous or incomplete rules could cause systematic evaluation errors
- All experiments use GPT-4; performance with smaller or different model families unknown

## Confidence
- **High confidence**: Iterative feedback mechanism's effectiveness in reducing cascading errors
- **Medium confidence**: 15% accuracy improvement from clarifying questions (single-task validation)
- **Medium confidence**: Framework's superiority over baselines (26.5% average improvement)
- **Low confidence**: Scalability to domains without programmatic verification

## Next Checks
1. External validation: Implement RGF on a sixth task domain (e.g., legal contract analysis) to test framework generalizability
2. Rule ambiguity stress test: Systematically degrade rule quality and measure Teacher evaluation accuracy drop
3. Alternative teacher evaluation: Replace GPT-4 Teacher with a smaller, fine-tuned model to assess evaluation accuracy correlation with model size