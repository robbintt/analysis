---
ver: rpa2
title: 'LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases'
arxiv_id: '2512.12643'
source_url: https://arxiv.org/abs/2512.12643
tags:
- legal
- relation
- relations
- object
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LexRel, the first comprehensive benchmark
  for legal relation extraction in Chinese civil law. It proposes a structured schema
  covering 265 relation types across 9 legal domains, with precise definitions of
  subjects, objects, and content for each relation.
---

# LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases

## Quick Facts
- arXiv ID: 2512.12643
- Source URL: https://arxiv.org/abs/2512.12643
- Reference count: 40
- Primary result: First comprehensive benchmark for legal relation extraction in Chinese civil law, covering 265 relation types across 9 domains

## Executive Summary
This paper introduces LexRel, the first comprehensive benchmark for legal relation extraction in Chinese civil law. The authors propose a structured schema covering 265 relation types across 9 legal domains, with precise definitions of subjects, objects, and content for each relation. Based on this schema, they construct LexRel using expert-annotated data from real-world court cases. Evaluation of state-of-the-art LLMs reveals significant performance gaps, with the best models achieving micro-F1 scores around 0.76 for type extraction and 0.38 for argument extraction in zero-shot settings. The study also demonstrates that incorporating structured legal relation knowledge consistently improves performance on downstream legal AI tasks including document proofreading, statute prediction, and legal consultation.

## Method Summary
The authors constructed LexRel by first developing a comprehensive schema through expert collaboration, defining 265 legal relation types across 9 domains with precise subject-object-content specifications. They then annotated 1,140 real-world Chinese civil cases with this schema. For evaluation, they decomposed legal relation extraction into two tasks: type extraction (classifying relation types from text) and argument extraction (extracting structured subject-object-content JSON). They evaluated zero-shot performance across multiple LLMs and conducted supervised fine-tuning experiments using relation-enhanced data generated by larger models. The benchmark also includes experiments demonstrating downstream task improvements when incorporating extracted legal relations.

## Key Results
- Zero-shot micro-F1 scores: ~0.76 for type extraction, ~0.38 for argument extraction (best models)
- Open-source models performed poorly on argument extraction (micro-F1 < 0.2)
- Incorporating legal relations improved downstream task performance across document proofreading, statute prediction, and legal consultation
- Long-tail distribution evident: 25 cause types account for 80% of judgments, leading to macro-F1 < micro-F1 disparities

## Why This Works (Mechanism)

### Mechanism 1: Structured Schema as a Domain-Specific Ontological Prior
A manually constructed, expert-validated schema provides a necessary top-down prior for models to map raw legal text to a formal, legally-grounded representation. Legal relations are normative, not just semantic. The schema defines the taxonomy of 265 relation types and provides precise definitions for their arguments (subject, object, content). This acts as an external knowledge base that constrains the model's interpretation of ambiguous text, mapping it to a pre-defined set of valid legal concepts instead of relying on potentially noisy or incorrect statistical correlations from the pre-training corpus.

### Mechanism 2: Capability Distillation from Teacher to Student Model
The complex reasoning ability required for legal relation extraction, present in large models (e.g., o3-mini), can be effectively transferred to smaller models via supervised fine-tuning (SFT) on generated data. The study uses more capable models to generate training data (relation types and arguments) from full judgment texts. Fine-tuning smaller models on this data forces them to internalize the patterns of extracting structured relations from unstructured text, thereby distilling the capability. The SFT data acts as a high-quality demonstration set.

### Mechanism 3: Relation Extraction as an Intermediate Task for Downstream Augmentation
Extracting structured legal relations serves as a powerful intermediate task that, when used to augment prompts for downstream tasks, improves performance. Tasks like statute prediction require understanding the core legal dynamics of a case. By first extracting the legal relations (subject, object, content) and providing them as context in the prompt, the model is given a pre-structured summary of the essential legal facts. This reduces the cognitive load on the model, allowing it to focus its reasoning capacity on applying the relevant statutes to the pre-digested facts.

## Foundational Learning

- **Concept**: Civil Legal Relation (Subject-Object-Content)
  - **Why needed here**: This tripartite structure is the core analytical unit. The entire benchmark is built on extracting these three components, and understanding their definitions is prerequisite to any work with the schema.
  - **Quick check question**: In the legal relation "Party A (Subject) leases Property P (Object) to Party B, creating a duty to pay rent (Content)," can you correctly identify each component based on the paper's definitions?

- **Concept**: Type Extraction vs. Argument Extraction
  - **Why needed here**: The task is decomposed into these two sequential subtasks. Understanding this pipeline is critical for interpreting the results, as model performance differs drastically between them (F1 ~0.76 for type vs. ~0.38 for argument).
  - **Quick check question**: Which subtask does the paper identify as significantly more challenging for current LLMs in a zero-shot setting, and what does this suggest about their capabilities?

- **Concept**: Long-Tail Distribution
  - **Why needed here**: The paper explicitly notes the long-tail distribution of legal relations and its correlation with real-world case frequencies (Pareto principle: 25 cause types account for 80% of cases). This is essential for understanding performance disparities (macro-F1 < micro-F1) and data challenges.
  - **Quick check question**: If a model achieves high micro-F1 but low macro-F1 on LexRel, what does that indicate about its performance across the long tail of relation types?

## Architecture Onboarding

- **Component map**: Schema (Taxonomy of 265 relations + definitions) -> LexRel (1,140 annotated cases) -> Pipeline (Type Extraction -> Argument Extraction)
- **Critical path**: The **Schema** is the most critical component. Any inaccuracy or ambiguity in its definitions propagates directly into the benchmark annotation process and limits the maximum achievable accuracy of any model trained or evaluated on it.
- **Design tradeoffs**:
  - Static vs. Adaptive Schema: The schema is fixed and expert-defined, ensuring high legal validity but potentially lacking flexibility for novel or unforeseen case types. This is a tradeoff between precision and recall.
  - LLM-Augmented Annotation: Using an LLM for initial drafts before expert review reduces cost and time but introduces the risk of subtle LLM-induced biases in the initial candidate generation.
- **Failure signatures**:
  - Argument Hallucination: A model correctly identifies the relation type (e.g., "Contractual Legal Relation") but fabricates the subject, object, or content details that are not supported by the text. This is the primary failure mode indicated by low argument extraction scores.
  - Long-Tail Collapse: The model performs well on high-frequency relations (e.g., loan disputes) but fails on low-frequency ones (e.g., specific torts), resulting in a significant gap between micro-F1 and macro-F1.
- **First 3 experiments**:
  1. Baseline & Error Analysis: Run zero-shot inference on LexRel with a target model. Separate results into type and argument extraction. Manually inspect 20-30 failure cases in argument extraction to identify common confusion patterns.
  2. SFT Distillation Experiment: Using the prompt templates, generate training data from a larger teacher model on a separate set of judgments. Fine-tune a smaller student model and evaluate the performance lift on the LexRel validation set.
  3. Downstream Ablation: Select a downstream task like Statute Prediction. Run two conditions: (a) vanilla zero-shot, and (b) with the input augmented by gold-standard legal relations from the benchmark. Quantify the improvement to validate the practical utility of the extraction task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models be improved to handle long-tail legal relation types that occur sparsely in real-world litigation data?
- Basis in paper: The paper shows a clear long-tail distribution with 25 cause types accounting for 80% of judgments, and notes macro-F1 scores are lower than micro-F1 scores across all settings.
- Why unresolved: The benchmark reveals the problem but offers no targeted solutions for improving performance on infrequent relation types.
- What evidence would resolve it: Experiments comparing standard training versus long-tail-aware methods showing improved macro-F1 on rare relations.

### Open Question 2
- Question: What techniques can close the substantial performance gap between legal relation type extraction (micro-F1 ~0.76) and argument extraction (micro-F1 ~0.38)?
- Basis in paper: Argument extraction proves significantly more challenging with highest zero-shot micro-F1 of 0.382 and most open-source models performing poorly (micro-F1 < 0.2).
- Why unresolved: The paper documents the gap but does not investigate specific failure modes or propose targeted interventions for argument extraction.
- What evidence would resolve it: Ablation studies identifying whether errors stem from span detection, argument classification, or schema comprehension, followed by targeted architectural or prompting improvements.

### Open Question 3
- Question: Can the LexRel schema and extraction methods transfer effectively to other civil law jurisdictions with distinct legal doctrines and terminology?
- Basis in paper: The authors acknowledge their schema focuses specifically on Chinese civil law and may limit direct transferability to other jurisdictions.
- Why unresolved: No cross-jurisdictional experiments were conducted; the authors acknowledge this as a limitation.
- What evidence would resolve it: Experiments applying LexRel-trained models to legal texts from other civil law jurisdictions, measuring transfer performance and identifying necessary schema modifications.

## Limitations

- Reliance on LLM-as-a-judge for argument extraction introduces potential subjectivity, with limited human validation (60 items) that may not capture systematic biases
- Performance disparities due to long-tail distribution (25 types accounting for 80% of cases) make macro-level metrics less representative of practical utility
- Lack of detailed SFT hyperparameter specifications makes exact reproduction challenging
- Benchmark may be more suitable for evaluating high-frequency legal scenarios rather than rare or novel case types

## Confidence

- **High Confidence**: The schema construction methodology and its expert validation are well-documented and reproducible. The decomposition of legal relation extraction into type and argument extraction tasks is clearly defined.
- **Medium Confidence**: The effectiveness of relation extraction for downstream tasks is demonstrated, but the evaluation is limited to three specific tasks and may not generalize to all legal AI applications.
- **Low Confidence**: The specific impact of individual schema design choices on model performance cannot be isolated from the overall framework effectiveness.

## Next Checks

1. **Human Evaluation Expansion**: Conduct systematic human evaluation on a stratified sample covering both high-frequency and long-tail relation types to validate LLM-as-a-judge reliability across the full distribution.

2. **Cross-Domain Transferability**: Test whether models trained on LexRel data can generalize to legal relations in different Chinese civil law subdomains not well-represented in the original corpus (e.g., maritime law, intellectual property).

3. **Schema Evolution Impact**: Create a modified version of the schema with reduced granularity and evaluate whether this impacts downstream task performance, helping identify which schema details are most critical for practical applications.