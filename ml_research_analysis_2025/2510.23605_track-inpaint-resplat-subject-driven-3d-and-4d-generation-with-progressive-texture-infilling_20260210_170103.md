---
ver: rpa2
title: 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive
  Texture Infilling'
arxiv_id: '2510.23605'
source_url: https://arxiv.org/abs/2510.23605
tags:
- generation
- image
- diffusion
- wang
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIRE, a three-stage pipeline for subject-driven
  3D/4D generation that improves identity preservation by progressively inpainting
  occluded regions. The method takes an initial 3D asset from existing models and
  uses video tracking to identify regions needing modification, then progressively
  inpaints these regions with a subject-driven 2D model while maintaining identity,
  and finally resplats the modified observations back to 3D with cross-view consistency.
---

# Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling

## Quick Facts
- **arXiv ID:** 2510.23605
- **Source URL:** https://arxiv.org/abs/2510.23605
- **Reference count:** 40
- **Primary result:** Three-stage pipeline improves identity preservation in subject-driven 3D/4D generation by progressively inpainting occluded regions

## Executive Summary
This paper introduces TIRE, a three-stage pipeline that addresses the challenge of identity preservation in subject-driven 3D/4D generation. The method takes an initial 3D asset and progressively modifies it using video tracking to identify regions needing improvement, then applies subject-driven inpainting with a 2D model, and finally resplats the modified observations back to 3D with cross-view consistency. The approach demonstrates significant improvements over state-of-the-art methods in VLM-based evaluations, achieving superior subject consistency while also improving geometry quality and reducing ghosting artifacts.

## Method Summary
TIRE operates through three sequential stages: First, it tracks visibility across multi-view frames using backward video tracking to identify occluded regions requiring inpainting. Second, it progressively inpaints these regions using a subject-driven 2D model trained in stages (starting from source view ±20°, then ±90°, then full 180°), with a 30% denoising schedule and masked latent updates to preserve source identity. Third, it resplats the modified 2D observations back to 3D using multi-view diffusion models with mask-aware refinement, ensuring cross-view consistency while maintaining the subject's identity.

## Key Results
- VLM-based evaluations show TIRE achieves 1.854 average subject consistency score versus 1.703 for Hunyuan3D-v2.5
- The method significantly reduces ghosting artifacts and improves geometry quality compared to baseline approaches
- Progressive inpainting stages demonstrate better identity preservation than single-stage inpainting methods
- TIRE is complementary to efficiency-focused approaches and applicable to various 3D/4D generation methods

## Why This Works (Mechanism)

### Mechanism 1: Backward Tracking Superiority
Backward tracking from target views to source view produces more coherent inpainting masks than forward tracking. By tracing correspondences FROM unseen regions TO the identity-rich source view, the system establishes maximal overlap with known appearance, reducing inpainting ambiguity. Forward tracking fragments into grainy masks because visible pixels in source often lack clear trajectories into occluded target regions. The source view contains the most reliable identity information; correspondences are recoverable via video tracking models trained on diverse motion patterns.

### Mechanism 2: Progressive Texture Infilling
Progressive texture infilling with anchor viewpoints enables subject-driven inpainting of distant views without direct reference. The pipeline expands outward from source (0°) → anchor at ±20° → anchor at ±90° → full ±180°. Each stage uses previously inpainted frames as pseudo-references, reducing the gap between available context and target viewpoint. Incremental viewpoint shifts allow the subject-driven diffusion model to generalize identity features without catastrophic drift; the "sweet spot" ±20° balance between exploration and reliability holds across subjects.

### Mechanism 3: Mask-Aware Latent Updates
Mask-aware latent updates during multi-view refinement preserve source identity while enforcing cross-view consistency. Equation 8 performs masked latent mixing—only updating regions marked as "unseen" (mask=1) while preserving source-view latents. This selective diffusion, combined with limited denoising schedule (first 30%), acts as a consistency regularizer that prevents over-editing. Multi-view diffusion models encode geometric priors that can resolve inconsistencies; 30% denoising is sufficient for refinement without structural disruption.

## Foundational Learning

- **Latent Diffusion Models & DDIM Sampling**: TIRE builds on Stable Diffusion inpainting with LoRA fine-tuning; understanding Equations 1–5 (noise schedules, denoising) is prerequisite for grasping the 30% denoising schedule and latent mixing. Quick check: Can you explain why limiting denoising to 30% of steps preserves structure while allowing texture refinement?

- **3D Gaussian Splatting (3DGS) and Feed-Forward Reconstruction**: LGM/L4GM generate 3D Gaussians directly from multi-view images; "resplat" refers to pixel-to-Gaussian projection. Understanding splatting is critical for Stage III. Quick check: What are the 14 parameters per Gaussian, and how does feed-forward prediction differ from per-scene optimization?

- **Video Tracking and Correspondence (CoTracker)**: Stage I relies on dense point tracking to propagate visibility across frames; backward tracking leverages temporal coherence. Quick check: Why would tracking from target→source produce different mask quality than source→target in a rotating-camera sequence?

## Architecture Onboarding

- **Component map:** Input 3D/4D asset + reference video → Render multi-view frames → CoTracker backward tracking → Binary inpainting masks → LoRA-fine-tuned SD inpainting (progressive stages) → Inpainted frames → Mask-aware multi-view diffusion → LGM/L4GM resplat → Final 3D/4D Gaussians

- **Critical path:** Backward tracking mask quality → Progressive inpainting stage ordering → Mask-aware latent update masking

- **Design tradeoffs:** Denoising schedule (15% vs 30% vs 50%) → Progressiveness degree (10° vs 20° vs 30°) → Tracking direction (forward intuitive but grainy vs backward requires inverse logic but superior)

- **Failure signatures:** Grainy inpainting → using forward tracking or insufficient mask dilation; Identity drift in back views → skipped anchor viewpoints or excessive denoising; Ghosting artifacts → multi-view diffusion refinement not applied or mask M incorrectly set to all 1s; Color mismatch → Wonder3D color guidance not used

- **First 3 experiments:**
  1. Validate backward vs forward tracking on 5 subjects: Render multi-view frames, run both tracking directions, compare mask IoU against manually annotated ground truth; expect backward to show 15–25% higher mask coherence.
  2. Ablate progressive inpainting stages: Run pipeline with single-stage inpainting (0°→180° directly) vs 3-stage progressive; measure VLM-based identity scores; expect progressive to outperform by 0.1–0.2 average score.
  3. Test denoising schedule sensitivity: Run inpainting with 15%, 30%, 50% schedules on held-out subjects; visualize texture preservation vs distortion; identify optimal range per subject category.

## Open Questions the Paper Calls Out

### Open Question 1
What quantitative evaluation metrics can accurately capture both geometric correctness and identity preservation in subject-driven 3D/4D generation? Current vision-based metrics like DINO similarity fail because methods with incorrect geometry score higher than those with correct back-view geometry. A metric that jointly evaluates structural/geometry quality and appearance fidelity, validated against human judgments showing monotonic improvement with visual quality, would resolve this.

### Open Question 2
How can subject-driven 4D generation preserve temporal appearance patterns (e.g., dynamic textures) that change over time in reference videos? The current TIRE pipeline treats 4D as a sequence of 3D frames without explicit temporal reasoning. Extension incorporating temporal consistency constraints and evaluation on reference videos with known dynamic appearance changes would resolve this.

### Open Question 3
Can feed-forward personalization approaches replace per-scene optimization in subject-driven 3D/4D generation without sacrificing identity fidelity? Current efficient methods sacrifice identity preservation, while TIRE's per-scene LoRA finetuning is computationally expensive. Development of a zero-shot or few-shot subject-driven 3D generation model achieving comparable VLM scores (≥1.85) to TIRE without per-scene training would resolve this.

### Open Question 4
What determines the optimal degree of progressiveness (viewpoint step size) and denoising schedule for diverse subjects with varying occlusion patterns? The optimal hyperparameters likely depend on subject geometry, texture complexity, and initial asset quality, but no adaptive mechanism or theoretical guidance is provided. Systematic evaluation across subject categories or an adaptive algorithm that predicts optimal hyperparameters from initial asset properties would resolve this.

## Limitations

- The method's performance on highly dynamic subjects or those with significant non-rigid deformations remains untested
- Computational overhead of the three-stage pipeline may be prohibitive for real-time applications
- Reliance on specific training datasets (DreamBooth-Dynamic) and pre-trained models limits generalizability

## Confidence

- **High Confidence:** Backward tracking producing superior masks (supported by direct qualitative comparison in Figure 3)
- **Medium Confidence:** Progressive inpainting stage ordering benefits (based on ablation studies, but limited subject diversity)
- **Medium Confidence:** Mask-aware latent updates improving consistency (theory-grounded but not extensively validated across diverse subjects)

## Next Checks

1. **Cross-dataset Generalization:** Test TIRE on subjects outside the DreamBooth-Dynamic dataset, including real-world videos with varied backgrounds and lighting conditions.

2. **Extreme Viewpoint Validation:** Systematically evaluate identity preservation at extreme angles (>150° from source) and under non-Lambertian surface conditions to identify failure modes.

3. **Computational Efficiency Analysis:** Measure end-to-end runtime and resource usage across different hardware configurations, and compare against baseline methods to quantify the practical tradeoff between quality and efficiency.