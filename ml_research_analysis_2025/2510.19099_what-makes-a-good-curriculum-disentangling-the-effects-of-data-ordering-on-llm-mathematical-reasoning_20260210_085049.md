---
ver: rpa2
title: What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on
  LLM Mathematical Reasoning
arxiv_id: '2510.19099'
source_url: https://arxiv.org/abs/2510.19099
tags:
- curriculum
- arxiv
- reasoning
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates curriculum learning (CL) in mathematical reasoning
  for large language models, addressing the open question of which difficulty metrics
  and data ordering strategies are most effective. It proposes a unified offline evaluation
  framework with five difficulty metrics: problem difficulty, model surprisal, confidence
  margin, predictive uncertainty, and decision variability.'
---

# What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning

## Quick Facts
- arXiv ID: 2510.19099
- Source URL: https://arxiv.org/abs/2510.19099
- Reference count: 31
- One-line primary result: No single curriculum learning strategy universally dominates; effectiveness depends on model capability, task complexity, and the difficulty metric used.

## Executive Summary
This work evaluates curriculum learning (CL) in mathematical reasoning for large language models, addressing the open question of which difficulty metrics and data ordering strategies are most effective. It proposes a unified offline evaluation framework with five difficulty metrics: problem difficulty, model surprisal, confidence margin, predictive uncertainty, and decision variability. Through controlled post-training experiments on MetaMathQA-40K with Llama3.1-8B, Mistral-7B, and Gemma3-4B, the study finds that no single CL strategy universally dominates—effectiveness depends jointly on model capability, task complexity, and the metric used. Forward and reverse curricula show task- and model-dependent performance differences, and difficulty-level-specific gains vary with task difficulty. Curricula not only improve reasoning accuracy but also modulate internal model states such as confidence and uncertainty, with internal-state metrics influencing convergence dynamics and task-aligned metrics shaping generalization.

## Method Summary
The study employs a unified offline evaluation framework where training data is reordered according to five difficulty metrics before supervised fine-tuning (SFT). Five metrics are computed: problem difficulty (rating score, step count, concept diversity, accuracy@K), model surprisal (surprisal loss, token-level perplexity), confidence margin (logit gap), predictive uncertainty (sample-level and token-level entropy), and decision variability (accuracy variability). Training uses LoRA with fixed hyperparameters (lr=5e-5, 3 epochs, cosine schedule with 10% warmup). The framework compares forward (easy-to-hard) and reverse (hard-to-easy) curricula against shuffled baselines across three 7-8B models on mathematical reasoning benchmarks including MetaMathQA, ASDiv, GSM8K, MATH, and MathBench.

## Key Results
- No single curriculum learning strategy universally dominates; effectiveness depends on model capability, task complexity, and the difficulty metric used.
- Forward and reverse curricula show task- and model-dependent performance differences, with FCL producing more cautious models and RCL preserving higher confidence.
- Difficulty-level-specific gains vary with task difficulty: easier tasks benefit from medium-to-high entropy tiers, while harder tasks improve with low-entropy tiers.
- Internal-state metrics (perplexity, entropy, logit gap) primarily regulate convergence dynamics, while task-aligned metrics (accuracy, variability) shape final generalization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-aligned curriculum metrics influence final convergence and generalization rather than training speed.
- Mechanism: Metrics derived from task-level outcomes (Acc@K, Decision Variability) primarily affect where the model converges, not how fast. Training on easier or higher-variability samples leads to lower steady-state loss, indicating improved data fitting.
- Core assumption: Task-aligned metrics capture objective-relevant difficulty that shapes representational quality.
- Evidence anchors: [abstract] "task-aligned curricula focus on shaping the model's final representations and generalization"; [section 3.5] "task-aligned metrics (ACC, V_ACC)... having limited effect on convergence speed but substantial impact on the final loss and generalization".

### Mechanism 2
- Claim: Internal-state curriculum metrics regulate convergence dynamics by modulating confidence and uncertainty distributions.
- Mechanism: Model-side metrics (perplexity, entropy, logit gap) capture the model's predictive behavior. Starting with low-perplexity, more-certain samples produces higher initial loss but faster convergence; beginning with high-perplexity samples yields slower convergence but similar final loss.
- Core assumption: Internal-state metrics reflect learnable difficulty that the model can progressively master.
- Evidence anchors: [abstract] "inner-state curricula modulate internal states such as confidence and uncertainty"; [section 3.5] "internal-state metrics (TLP, TLE, LG) capture difficulty from the model's own predictive behavior and primarily regulate the convergence dynamics".

### Mechanism 3
- Claim: Curriculum direction (forward vs. reverse) systematically shifts model calibration properties independently of task accuracy.
- Mechanism: Forward CL (easy-to-hard) yields more cautious, uncertainty-aware models with lower internal entropy. Reverse CL preserves decisiveness with higher confidence and logit gaps. This occurs because early training samples anchor the model's confidence distribution.
- Core assumption: Early training examples disproportionately influence calibration priors that persist through fine-tuning.
- Evidence anchors: [abstract] "prioritizing decision-uncertain samples can further enhance learning outcomes"; [section 3.4] "for TLP, SLP, and LG, we observe a consistent trend of FCL < RCL, suggesting that training on low-perplexity or low-confidence samples first yields more conservative and stable internal representations".

## Foundational Learning

- Concept: **Curriculum Learning (CL)**
  - Why needed here: The paper's entire framework builds on CL principles—ordering training data by difficulty to improve learning. Without understanding CL basics, the comparison of forward vs. reverse strategies and metric-driven ordering will be opaque.
  - Quick check question: Can you explain why presenting easy examples before hard ones might help or hurt learning, and under what conditions each direction could be beneficial?

- Concept: **Problem-side vs. Model-side Difficulty Metrics**
  - Why needed here: The paper decomposes difficulty into five dimensions across these two categories. Understanding this distinction is essential for interpreting why different metrics produce different curriculum effects.
  - Quick check question: Given a math problem, would "number of reasoning steps" be a problem-side or model-side metric? What about "model perplexity on the solution"?

- Concept: **Model Calibration (Confidence vs. Accuracy)**
  - Why needed here: Section 3.4 shows that curriculum direction affects internal metrics (confidence, uncertainty) independently of accuracy. Understanding calibration helps interpret why FCL produces more conservative models.
  - Quick check question: If a model achieves 80% accuracy but assigns 99% confidence to all predictions, is it well-calibrated? Why might curriculum direction affect this?

## Architecture Onboarding

- Component map:
  Metric Estimation Module -> Curriculum Builder -> Training Loop -> Evaluation Suite

- Critical path:
  1. Select difficulty metric(s) based on task type (easier tasks → low-entropy/confidence tiers; harder tasks → medium-to-high tiers).
  2. Compute metric values for all training samples (20K samples, temperature 0.7, 20 generations for stochastic metrics).
  3. Construct curriculum schedule (GFC/GRC recommended for stability over fine-grained FCL/RCL).
  4. Train with fixed hyperparameters (lr=5e-5, 3 epochs, cosine schedule, 10% warmup).
  5. Evaluate on target benchmarks and internal metrics to verify both accuracy and calibration effects.

- Design tradeoffs:
  - **FCL vs. RCL**: FCL yields more cautious, well-calibrated models; RCL produces higher confidence. Choose based on whether calibration or decisiveness matters more for deployment.
  - **Fine-grained vs. Grouped curricula**: Fine-grained (FCL/RCL) may achieve higher peak performance but less stable; grouped (GFC/GRC) provides more consistent results across seeds and models.
  - **Single metric vs. Multi-metric**: No single metric dominates; consider task complexity when selecting (e.g., entropy-based metrics work better for easier tasks, logit gap more stable across difficulties).

- Failure signatures:
  - **Curriculum shows no improvement over baseline**: Check if task difficulty ceiling has been reached (complex tasks like MATH show <2% gains); model may already be saturated.
  - **FCL and RCL produce identical results**: Metric values may lack variance (verify distribution spread); or model scale too large to be sensitive to ordering.
  - **Internal metrics shift without accuracy change**: Expected behavior for internal-state metrics (TLP, TLE, LG); verify using task-aligned metrics if accuracy gains are the goal.
  - **Performance degrades on OOD benchmarks**: Overfitting to curriculum-induced distribution; try grouped curricula (GFC/GRC) to reduce ranking noise sensitivity.

- First 3 experiments:
  1. **Metric sensitivity test**: Train Llama3-8B with logit gap (LG) curriculum in both directions on ASDiv and MATH. Verify that RCL outperforms FCL on simpler tasks while FCL excels on harder tasks, matching paper findings (Tables 3-4).
  2. **Tier ablation**: Train on Low/Medium/High tiers separately for entropy-based metrics (SLE/TLE) on GSM8K and MATH. Confirm that high-entropy tier benefits easier tasks while low-entropy tier helps harder tasks (Figure 2 pattern).
  3. **Internal-state validation**: After training with FCL/RCL/shuffled baseline, measure TLP and LG on held-out problems. Verify that FCL < RCL < SHUF pattern emerges, confirming calibration modulation independent of accuracy (Figure 3 replication).

## Open Questions the Paper Calls Out

- How do online or self-paced curricula that dynamically adjust during training compare to offline curricula in terms of reasoning performance and sample efficiency? The paper focuses on offline CL with predefined data ordering and leaves dynamic adaptation unexplored.

- How do curricula affect intermediate reasoning steps and solution trajectories, beyond final accuracy and internal state metrics? The study focused on accuracy and internal states but not step-by-step reasoning quality.

## Limitations

- Limited model diversity: Only three 7-8B models evaluated, which may not generalize to larger or multimodal models.
- Metric computation reliability: Internal-state metrics depend on stochastic sampling (20 completions at temperature 0.7) without reported variance.
- Task domain specificity: Results confined to mathematical reasoning; effectiveness across different domains remains untested.

## Confidence

**High confidence**: Claims about metric-dependent effectiveness, no single CL strategy dominating, and internal-state metrics modulating convergence dynamics while task-aligned metrics affect final generalization.

**Medium confidence**: Claims about FCL producing more calibrated models and RCL producing more confident models—calibration effects are demonstrated but mechanistic explanations are somewhat speculative.

**Low confidence**: Claims about prioritizing decision-uncertain samples enhancing learning outcomes—mentioned in abstract but not deeply analyzed or statistically validated.

## Next Checks

1. **Cross-domain metric validation**: Apply the same five-metric framework to a non-mathematical task (e.g., code generation or commonsense reasoning) to test whether the same metric effectiveness patterns hold across domains.

2. **Calibration transfer analysis**: Train with FCL and RCL on MMQA, then test on OOD benchmarks while specifically measuring calibration error (ECE). This would validate whether observed internal-state differences persist and affect real-world reliability.

3. **Sample size sensitivity**: Repeat metric computation with 5, 10, and 20 samples per problem to quantify how sampling noise affects difficulty rankings and subsequent curriculum performance. This would establish the robustness of the metric-based ordering.