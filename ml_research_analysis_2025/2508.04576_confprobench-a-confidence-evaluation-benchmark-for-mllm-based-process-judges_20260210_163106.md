---
ver: rpa2
title: 'ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges'
arxiv_id: '2508.04576'
source_url: https://arxiv.org/abs/2508.04576
tags:
- confidence
- mpjs
- reasoning
- robustness
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConfProBench, the first benchmark specifically
  designed to evaluate the reliability of step-level confidence scores produced by
  multimodal large language model-based process judges (MPJs). The benchmark addresses
  a critical gap in existing MPJ evaluations by focusing on confidence robustness,
  sensitivity, and calibration under adversarial perturbations such as synonym substitution,
  syntactic transformation, and image perturbation.
---

# ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges

## Quick Facts
- **arXiv ID**: 2508.04576
- **Source URL**: https://arxiv.org/abs/2508.04576
- **Reference count**: 12
- **Key outcome**: ConfProBench introduces three novel metrics to evaluate confidence reliability in MPJs, revealing that high classification accuracy does not guarantee reliable confidence estimation.

## Executive Summary
ConfProBench is the first benchmark specifically designed to evaluate the reliability of step-level confidence scores produced by multimodal large language model-based process judges (MPJs). The benchmark addresses a critical gap in existing MPJ evaluations by focusing on confidence robustness, sensitivity, and calibration under adversarial perturbations such as synonym substitution, syntactic transformation, and image perturbation. Experiments on 14 state-of-the-art MPJs reveal that while some models achieve high classification accuracy, their confidence reliability remains limited. Notably, Gemini-2.5-flash achieved the highest average score (53.33) across all three confidence metrics, while GPT-4.1 excelled in confidence robustness (73.62). The results highlight significant room for improvement in MPJs' confidence estimation and provide strong baselines for future research.

## Method Summary
ConfProBench evaluates MPJs using 1,200 problems sampled from ProJudgeBench across 3 difficulty levels, 4 disciplines, 3 modalities, and 7 error types. The dataset is divided into 3 subsets (400 each) for different perturbation types: Synonym Substitution, Syntactic Transformation, and Image Perturbation. The benchmark introduces three novel metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS). MPJs are prompted to output binary labels and P(correct)∈(0,1) for each reasoning step. GPT-4o generates 5 perturbation variants per step, with 1 selected randomly and validated through manual filtering to ensure semantic preservation. Confidence scores are extracted from verbalized outputs, and the three metrics are computed using specified formulas with hyperparameters.

## Key Results
- Gemini-2.5-flash achieved the highest average score (53.33) across all three confidence metrics
- GPT-4.1 excelled in confidence robustness with a score of 73.62
- ECE for incorrect steps was consistently much higher than for correct steps, indicating imbalanced calibration
- The benchmark reveals a disconnect between high classification accuracy and reliable confidence estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic-preserving adversarial perturbations expose the fragility of confidence estimations in MPJs.
- **Mechanism:** By applying Synonym Substitution, Syntactic Transformation, and Image Perturbation to reasoning steps without altering their semantic meaning, the benchmark tests whether an MPJ's confidence score is grounded in the underlying logic or merely sensitive to surface-level tokens.
- **Core assumption:** The generated perturbations strictly preserve semantic equivalence and do not introduce ambiguity that would justify a confidence change.
- **Evidence anchors:** [Abstract] Mentions constructing three types of adversarial perturbations; [Section 3.2] Details perturbation generation using GPT-4o and manual filtering; [Corpus] ProJudge focuses on error classification but lacks this specific focus on adversarial confidence stability.
- **Break condition:** If perturbation generation fails (e.g., alters numerical values or logical structure), observed confidence changes may be valid reactions to semantic drift rather than evidence of robustness failure.

### Mechanism 2
- **Claim:** Differentiating confidence scores between correct steps and specific error types quantifies a model's "awareness" of its own limitations.
- **Mechanism:** The Confidence Sensitivity Score (CSS) measures the gap between average confidence assigned to correct steps versus steps containing specific errors. A wider gap indicates the model successfully lowers its confidence when it is likely to be wrong.
- **Core assumption:** A reliable judge should ideally have high confidence in correct steps and distinctively lower confidence in incorrect steps.
- **Evidence anchors:** [Section 3.3] Defines CSS mathematically; [Table 4] Shows proprietary models exhibit higher sensitivity; [Corpus] Overconfidence in LLM-as-a-Judge highlights necessity of distinguishing accuracy from calibration.
- **Break condition:** If a model is consistently overconfident across both correct and incorrect steps, the CSS metric will approach zero or become negative, failing to distinguish valid reasoning from errors.

### Mechanism 3
- **Claim:** Decomposing calibration errors by class reveals systematic biases in MPJ reliability that single-metric averages miss.
- **Mechanism:** The Confidence Calibration Score (CCS) integrates Expected Calibration Error (ECE) with the gap between ECE for correct and incorrect classes. This penalizes models that are well-calibrated on one class but poorly calibrated on another.
- **Core assumption:** Calibration should be balanced; high accuracy on one class should not mask severe miscalibration on another.
- **Evidence anchors:** [Section 3.3] Defines CCS using both ECE and ΔECE; [Table 5] Shows ECE for incorrect steps is consistently much higher than for correct steps; [Section 2.1] Discusses confidence estimation methods.
- **Break condition:** If the dataset has extreme class imbalance, the ECE calculation might be dominated by the majority class, obscuring the true calibration performance on the minority class.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** ECE is the foundational component of the CCS metric. Without understanding that calibration measures the alignment between predicted probability and actual accuracy, the results in Table 5 are uninterpretable.
  - **Quick check question:** If a model predicts 100 steps with 0.9 confidence and 90 are correct, is it perfectly calibrated? What if only 50 are correct?

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - **Why needed here:** The paper evaluates Process Judges (MPJs), which assess every step of a reasoning chain. This distinguishes the task from simply checking a final answer (ORM), which is critical for understanding why confidence must be evaluated at the step level.
  - **Quick check question:** Why might a model give a correct final answer but still contain a reasoning step that deserves a low confidence score?

- **Concept: Adversarial Robustness**
  - **Why needed here:** The CRS metric relies on the principle that a robust model should not change its output for inputs that are semantically equivalent but syntactically different.
  - **Quick check question:** If a model changes its prediction from "Correct" to "Incorrect" solely because you swapped "big" for "large," does it have high or low adversarial robustness?

## Architecture Onboarding

- **Component map:** ProJudgeBench (Meta Data) + Perturbation Engine (GPT-4o + Image Transforms) -> Target MPJ (e.g., GPT-4o, InternVL3) receiving (Problem, Image, Perturbed Step) -> Verbalized Probability Extraction (parsing the MPJ's text output for P(correct)) -> CRS Calculator (Perturbed vs. Original), CSS Calculator (Error vs. Correct), CCS Calculator (ECE & ΔECE)

- **Critical path:** The Perturbation Construction & Validation phase. As noted in Section 3.2, strict manual filtering is required to ensure perturbations do not alter technical terms or mathematical expressions. If this fails, the CRS metric measures noise rather than robustness.

- **Design tradeoffs:**
  - Verbalized vs. Logit Confidence: The paper opts for verbalized confidence, trading the precision of logit access for model-agnostic compatibility (essential for evaluating proprietary models like GPT-4.1).
  - Perturbation Isolation: The dataset is split into three subsets for different perturbations. This prevents interference but reduces the sample size for any single perturbation type to 400.

- **Failure signatures:**
  - High Accuracy, Low CCS: The model is often right but for the wrong reasons, or it is overconfident in its errors (Table 10 highlights this disconnect).
  - Negative CSS: The model is more confident in incorrect steps than correct ones (e.g., MiniCPM-V-2 6 on Question Understanding Error in Table 4).
  - High CCR (Confidence Change Rate) under Synonym Substitution: Indicates the model relies heavily on specific token patterns rather than semantic understanding.

- **First 3 experiments:**
  1. **Establish Baseline Sensitivity:** Run a target MPJ on the unperturbed "Reasoning Error" subset to calculate baseline CSS and verify if the model can distinguish correct logic from reasoning flaws.
  2. **Syntactic Stress Test:** Apply the Syntactic Transformation subset and calculate CRS. The paper (Appendix C) suggests this is the hardest perturbation type; verify if your model fails here first.
  3. **Calibration Diagnosis:** Calculate ECE separately for correct and incorrect classes to compute CCS. Check if the model is "hedging" (low confidence everywhere) or "hallucinating" (high confidence on errors).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do MPJ verbalized confidence scores align with human expert uncertainty judgments?
- **Basis in paper:** [explicit] The authors state in the Conclusion: "First, conducting human confidence annotations and introducing new consistency metrics to assess the alignment between MPJ confidence and expert judgments."
- **Why unresolved:** ConfProBench currently evaluates confidence against binary ground-truth labels or perturbation stability, but lacks a comparison against the distribution of human confidence or inter-annotator agreement.
- **What evidence would resolve it:** A dataset featuring human expert confidence ratings for reasoning steps, along with a new consistency metric correlating MPJ scores with these human judgments.

### Open Question 2
- **Question:** How can the confidence robustness of MPJs be improved specifically against syntactic transformations?
- **Basis in paper:** [inferred] Appendix C notes that "MPJs exhibit the lowest confidence robustness scores (CRS) under syntactic transformations," and the text calls for "designing targeted strategies to enhance the confidence robustness."
- **Why unresolved:** The results demonstrate that models are brittle to changes in sentence structure despite semantic equivalence, but the paper does not propose a method to fix this specific vulnerability.
- **What evidence would resolve it:** Demonstrating improved CRS scores on the Syntactic Transformation subset after applying specific data augmentation or structure-aware training interventions.

### Open Question 3
- **Question:** Can the "thinking" process in reasoning models be optimized to maintain high sensitivity without degrading confidence calibration?
- **Basis in paper:** [inferred] Section 4.2 notes that while the thinking mode in Gemini-2.5-flash improves sensitivity (CSS) and robustness (CRS), "its CCS is lower than that of the no-thinking variant, indicating that the thinking process does not necessarily improve confidence calibration quality."
- **Why unresolved:** This suggests a trade-off where reasoning steps improve error detection but introduce miscalibration, a conflict not resolved in current architectures.
- **What evidence would resolve it:** A model capable of "thinking" that achieves a CSS comparable to Gemini-2.5-flash while matching or exceeding the CCS of its no-thinking variant.

### Open Question 4
- **Question:** How do MPJs perform in safety-critical scenarios where reliable confidence estimation is essential?
- **Basis in paper:** [explicit] The Conclusion suggests "extending ConfProBench to encompass safety-critical scenarios where highly reliable confidence estimation is essential."
- **Why unresolved:** ConfProBench focuses on scientific reasoning problems (Math, Physics, etc.); it remains untested whether the observed confidence limitations generalize to high-stakes domains like medical diagnosis or autonomous driving.
- **What evidence would resolve it:** Evaluation results of current MPJs on a new benchmark dataset comprised of safety-critical multimodal reasoning tasks.

## Limitations
- The manual filtering process for perturbation validation introduces potential subjectivity that could affect CRS results
- The benchmark's focus on process judges may limit generalizability to other MLLM applications
- The perturbation generation using GPT-4o, while systematic, may not capture all meaningful adversarial variations

## Confidence

- **High Confidence:** The core methodology for confidence evaluation (Section 3.3) is well-grounded in established calibration literature
- **Medium Confidence:** The perturbation generation process is systematic but relies on manual validation that could introduce variability
- **Low Confidence:** The generalizability of the benchmark to non-process-judge applications remains untested

## Next Checks

1. **Reproduce CRS with Automated Filtering:** Implement automated semantic similarity checks (e.g., embedding distance thresholds) to replace manual filtering and assess impact on robustness scores.
2. **Cross-Format Confidence Extraction:** Test the verbalized confidence extraction method across diverse MPJ output formats to establish reliability and identify parsing failure modes.
3. **Generalization to Other MLLM Tasks:** Apply the CCS metric to confidence outputs from MPJs evaluating non-process tasks (e.g., visual question answering) to assess benchmark applicability.