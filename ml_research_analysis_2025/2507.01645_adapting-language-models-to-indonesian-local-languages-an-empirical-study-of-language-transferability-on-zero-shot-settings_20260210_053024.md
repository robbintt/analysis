---
ver: rpa2
title: 'Adapting Language Models to Indonesian Local Languages: An Empirical Study
  of Language Transferability on Zero-Shot Settings'
arxiv_id: '2507.01645'
source_url: https://arxiv.org/abs/2507.01645
tags:
- language
- languages
- seen
- indonesian
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the transferability of pre-trained language
  models to low-resource Indonesian local languages using sentiment analysis as a
  case study. The study evaluates both zero-shot performance and adapter-based transfer
  across ten local languages using models of different types: a monolingual Indonesian
  BERT, multilingual models (mBERT and XLM-R), and a modular adapter-based approach
  called MAD-X.'
---

# Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings

## Quick Facts
- arXiv ID: 2507.01645
- Source URL: https://arxiv.org/abs/2507.01645
- Reference count: 15
- Primary result: Zero-shot sentiment transfer to Indonesian local languages depends primarily on pre-training language exposure rather than tokenization factors

## Executive Summary
This paper investigates zero-shot cross-lingual transfer from Indonesian to ten local languages using sentiment analysis as a case study. The study compares three model types—monolingual IndoBERT, multilingual mBERT, and XLM-R—and evaluates a modular adapter-based approach called MAD-X. Results show clear performance patterns: multilingual models perform best on languages seen during pre-training, moderately on linguistically related but unseen languages, and poorly on distant unseen languages. MAD-X adapters significantly improve transfer by learning language-specific representations from unlabeled target text while reusing task knowledge from Indonesian data. Tokenization analysis reveals that subword fragmentation and vocabulary overlap weakly correlate with performance but do not explain transfer success, which is primarily driven by pre-training language exposure.

## Method Summary
The study uses zero-shot transfer where XLM-R is fine-tuned on Indonesian sentiment data (500 samples) then directly evaluated on target languages without target-language training. MAD-X adapters train language-specific modules via masked language modeling on target Wikipedia text, while task adapters learn sentiment classification using Indonesian data with Indonesian language adapter. At inference, the Indonesian adapter is swapped for the target language adapter. Models are evaluated on NusaX sentiment dataset with macro F1-score across 5 random seeds. Tokenization analysis examines subword fragmentation and vocabulary overlap with Indonesian as potential predictors of transfer success.

## Key Results
- Zero-shot performance follows clear hierarchy: seen languages > partially seen > unseen languages
- XLM-R outperforms mBERT and IndoBERT across all language categories
- MAD-X adapters close performance gaps, especially for seen and partially seen languages
- Tokenization factors weakly correlate with performance but don't explain transfer success
- Sufficient unlabeled target text is critical for adapter effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot cross-lingual transfer effectiveness correlates with the model's prior exposure to the target language during pre-training, either directly or through linguistically related languages.
- Mechan: Multilingual models like XLM-R develop shared cross-lingual representations during pre-training. When a language appears in the pre-training corpus, the model learns its vocabulary patterns and syntactic structures. For related languages not explicitly seen, transfer occurs through lexical overlap and shared linguistic features with pre-trained languages.
- Core assumption: The learned representations capture transferable linguistic patterns that generalize across related languages, and the multilingual embedding space aligns semantically similar tokens across languages.
- Evidence anchors:
  - [abstract] "the most consistent predictor of transfer success is the model's prior exposure to the language, either directly or through a related language"
  - [section V.A] "For seen languages (Indonesian and Javanese), all models achieve strong performance... For partially seen languages, XLM-R still significantly outperforms other models, but the gap between seen and partially seen is noticeable"
  - [corpus] Related work on model scaling (arXiv:2501.05629) confirms that pre-training exposure significantly impacts seen vs. unseen language performance.
- Break condition: Transfer degrades substantially when the target language is linguistically distant from all pre-training languages (e.g., Toba Batak, Buginese) with F1 scores dropping to 0.23-0.39.

### Mechanism 2
- Claim: MAD-X adapters improve zero-shot transfer by decoupling language-specific knowledge (learned from unlabeled target corpora) from task-specific knowledge (learned from labeled source data).
- Mechan: Language adapters are trained via masked language modeling on target language unlabeled text, learning language-specific representations while keeping the base model frozen. A separate task adapter learns sentiment classification using Indonesian labeled data with an Indonesian language adapter. At inference, the Indonesian adapter is swapped for the target language adapter, allowing the task knowledge to operate on better target language representations.
- Core assumption: Language adapters can capture sufficient linguistic knowledge from limited unlabeled data, and task adapters encode transferable classification logic independent of the specific language adapter used during training.
- Evidence anchors:
  - [section III.B] "the task adapter learns the sentiment analysis task and the language adapter provides the language-specific vocabulary support"
  - [section V.A] "With MAD-X, we observe that XLM-R equipped with a target language adapter and the Indonesian task adapter significantly closes the gap, even surpassing the full fine-tuning baseline for Sundanese, Minangkabau, and Banjarese"
  - [corpus] Typologically Informed Parameter Aggregation (arXiv:2601.16629) demonstrates adapter-based approaches offer parameter-efficient solutions for low-resource language adaptation.
- Break condition: Adapters fail when unlabeled target corpus is extremely limited (Buginese: 1.8 MB Wikipedia), resulting in performance degradation below zero-shot baseline.

### Mechanism 3
- Claim: Subword fragmentation and vocabulary overlap with the source language correlate weakly with transfer performance but do not fully explain performance disparities.
- Mechan: While higher subword fragmentation (more tokens per word) may reduce model understanding and higher vocabulary overlap may aid transfer, these surface-level features do not determine outcomes at the sentence level. The model's learned contextual representations from pre-training dominate over tokenization characteristics.
- Core assumption: Tokenization affects input representation quality, but semantic understanding from pre-trained representations is the primary driver of classification accuracy.
- Evidence anchors:
  - [abstract] "subword fragmentation and vocabulary overlap with Indonesian correlate weakly with prediction quality, they do not fully explain the observed performance"
  - [section V.B] "no consistent correlation is observed at the sentence level: several misclassified sentences had low fragmentation, and others with high fragmentation were correctly classified"
  - [section V.B Fig. 3] "correlations are not strong or linear. Seen languages often perform well regardless of these factors"
  - [corpus] No directly comparable corpus evidence on tokenization-transfer correlation; this appears to be an underexplored area.
- Break condition: Tokenization factors become more predictive for partially seen languages at the aggregate language level, but remain insufficient as standalone predictors.

## Foundational Learning

- Concept: Zero-Shot Cross-Lingual Transfer
  - Why needed here: The entire experimental framework relies on fine-tuning on Indonesian and evaluating on target languages without target-language training data. Understanding this paradigm is essential to interpret the performance disparities.
  - Quick check question: If you fine-tune a multilingual model on English sentiment data and evaluate directly on Swedish test data without further training, what transfer setting is this?

- Concept: Parameter-Efficient Fine-Tuning (Adapters)
  - Why needed here: MAD-X uses adapter modules rather than full model fine-tuning. You must understand how adapters insert small trainable layers into frozen pretrained models to appreciate why this approach enables modular language-task separation.
  - Quick check question: In an adapter-based approach, which parameters are updated during training: the base model weights, the adapter weights, or both?

- Concept: Subword Tokenization (BPE/WordPiece/SentencePiece)
  - Why needed here: The tokenization analysis examines how subword fragmentation affects transfer. Understanding how tokenizers split unknown words into subword units is necessary to interpret why "over-tokenization" might degrade performance.
  - Quick check question: Why might a word like "nggih" (Javanese) be split into more subword tokens than "ya" (Indonesian) by an Indonesian-trained tokenizer?

## Architecture Onboarding

- Component map:
  - **Base Models**: IndoBERT (monolingual Indonesian), mBERT (104 languages, Wikipedia), XLM-R (100 languages, CommonCrawl) — serve as pretrained backbones
  - **Language Adapters (MAD-X)**: Small bottleneck modules inserted after each transformer layer, trained via MLM on target language unlabeled text
  - **Task Adapters**: Trained on Indonesian sentiment data alongside Indonesian language adapter
  - **Inference Pipeline**: Base model (frozen) + target language adapter + task adapter → sentiment prediction

- Critical path:
  1. Verify language adapter availability for target languages (requires sufficient Wikipedia corpus; Buginese failed with only 1.8 MB)
  2. Train language adapters via MLM on unlabeled target text (1 epoch, lr=1e-4)
  3. Train task adapter on Indonesian labeled data using Indonesian language adapter
  4. At inference: swap Indonesian adapter → target language adapter while keeping task adapter fixed

- Design tradeoffs:
  - Full fine-tuning vs. adapters: Full fine-tuning achieves upper-bound performance but requires labeled target data; MAD-X approaches this performance without target labels but depends on unlabeled corpus quality
  - XLM-R vs. mBERT vs. IndoBERT: XLM-R offers best coverage; IndoBERT excels on Indonesian only; mBERT includes some local languages but smaller corpus limits performance
  - Adapter stacking: Paper suggests future work on combining general and language-specific adapters, not yet implemented

- Failure signatures:
  - Unseen languages (Toba Batak, Buginese): F1 drops to 0.23-0.39 regardless of model choice
  - Insufficient unlabeled corpus for adapter training (Buginese): Performance degrades below zero-shot baseline
  - Monolingual models on non-source languages: IndoBERT achieves only 0.39 F1 on Toba Batak despite strong Indonesian performance

- First 3 experiments:
  1. **Baseline zero-shot evaluation**: Fine-tune XLM-R on Indonesian NusaX training split (500 samples), evaluate on all 10 target language test sets. Record F1 by language category (seen/partially seen/unseen) to establish transfer bounds.
  2. **MAD-X adapter training**: Train language adapters for 5-6 target languages with adequate Wikipedia corpora (Javanese, Sundanese, Minangkabau, Acehnese, Banjarese, Balinese). Train task adapter on Indonesian with Indonesian language adapter active. Evaluate adapter swap performance.
  3. **Tokenization correlation analysis**: For each target language, compute average subwords-per-word and vocabulary overlap with Indonesian. Correlate with F1 scores to replicate paper's finding that these factors are weak predictors compared to pre-training exposure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can shared or clustered adapters across linguistically related languages improve transfer efficiency for low-resource languages compared to language-specific adapters?
- Basis in paper: [explicit] "Clustering languages to train shared adapters or stacking general and language-specific adapters might further enhance efficiency."
- Why unresolved: The study only trained individual language adapters; no experiments were conducted on shared adapters across language families or stacked adapter configurations.
- What evidence would resolve it: Experiments training a single adapter on combined data from related languages (e.g., Austronesian family) and comparing performance against individual adapters, especially for languages with limited monolingual data.

### Open Question 2
- Question: What minimum corpus size or quality threshold is required for adapter training to provide positive transfer gains rather than degradation?
- Basis in paper: [explicit] For Buginese, "using an adapter actually results in lower performance. This may be attributed to the extremely limited size of its Wikipedia corpus (Buginese has only 1.8 MB, compared to 50.5 MB for Javanese)."
- Why unresolved: The paper identifies corpus size as a likely factor but does not systematically test the relationship between adapter training data quantity/quality and downstream performance.
- What evidence would resolve it: Controlled experiments varying adapter training corpus sizes across multiple languages to identify thresholds where adapters become beneficial versus harmful.

### Open Question 3
- Question: Do the observed transfer patterns generalize to other NLP tasks beyond sentiment analysis?
- Basis in paper: [explicit] "Extending the evaluation beyond sentiment analysis to other tasks may also help validate the broader applicability of our findings."
- Why unresolved: The entire study uses only sentiment classification; transferability may differ for tasks requiring different linguistic competencies (e.g., named entity recognition, question answering, parsing).
- What evidence would resolve it: Replicating the same experimental setup across diverse tasks such as NER, QA, or POS tagging on the same Indonesian local languages.

### Open Question 4
- Question: Can tokenizer adaptation or vocabulary expansion methods substantially improve performance for high-fragmentation, unseen languages?
- Basis in paper: [explicit] "Future work could explore expanding pre-training corpora to include more local languages and adapting tokenizers to better suit low-resource settings."
- Why unresolved: The tokenization analysis showed that subword fragmentation correlates only weakly with performance; no experiments tested whether modified tokenizers could improve outcomes.
- What evidence would resolve it: Experiments with language-specific or morphologically-aware tokenizers for high-fragmentation languages like Toba Batak and Buginese, comparing against the standard XLM-R tokenizer.

## Limitations
- Limited scope to ten Indonesian local languages and single sentiment analysis task
- MAD-X approach requires sufficient unlabeled target language text for adapter training
- Tokenization analysis reveals weak correlations that suggest incomplete understanding of tokenization's role
- Relatively small training datasets (500 samples per language) may not capture full model potential

## Confidence
- High Confidence:
  - Zero-shot transfer performance correlates with pre-training language exposure
  - XLM-R consistently outperforms other models across language categories
  - MAD-X adapters improve performance for seen and partially seen languages without target language labeled data
  - Tokenization metrics are weak predictors of transfer success at sentence level
- Medium Confidence:
  - The mechanism of shared cross-lingual representations enabling transfer for related but unseen languages
  - The decoupling of language-specific and task-specific knowledge in MAD-X approach
  - The sufficiency of unlabeled target language text for adapter training in most cases
- Low Confidence:
  - The specific thresholds for "sufficient" unlabeled target language text for adapter training
  - The generalizability of findings to other language families beyond Indonesian local languages
  - The exact relationship between tokenization characteristics and sentence-level prediction accuracy

## Next Checks
1. **Cross-Task Validation**: Replicate the zero-shot transfer experiments using different downstream tasks (e.g., named entity recognition, question answering) on the same ten languages to verify whether the performance patterns by language category generalize beyond sentiment analysis.

2. **Adapter Training Data Sensitivity**: Systematically vary the amount of unlabeled Wikipedia text used for adapter training (e.g., 10%, 50%, 100% of available corpus) for each language and measure the impact on MAD-X performance, particularly for languages with limited Wikipedia coverage like Buginese.

3. **Tokenization Ablation Study**: For languages showing strong vs. weak transfer performance, conduct controlled experiments comparing different tokenization strategies (e.g., SentencePiece vs. BPE, varying vocabulary sizes) to isolate the specific impact of tokenization choices on transfer success beyond what surface metrics reveal.