---
ver: rpa2
title: Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control
  for Short Video Search Relevance Modeling
arxiv_id: '2509.16717'
source_url: https://arxiv.org/abs/2509.16717
tags:
- relevance
- data
- query
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of capturing domain-specific
  fine-grained relevance diversity in short video search by proposing a semi-supervised
  synthetic data pipeline (SSRA). The core method idea involves a two-stage process:
  first, enhancing query diversity by re-annotating documents with varying relevance
  labels using a score model, and second, improving the alignment between synthesized
  queries and target relevance labels through iterative refinement.'
---

# Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling

## Quick Facts
- **arXiv ID:** 2509.16717
- **Source URL:** https://arxiv.org/abs/2509.16717
- **Reference count:** 10
- **Primary result:** SSRA pipeline improves nDCG@10 by 1.73% and AP by 2.80% over baseline, with 1.45% CTR increase in online A/B testing.

## Executive Summary
This paper addresses the challenge of capturing domain-specific fine-grained relevance diversity in short video search by proposing a semi-supervised synthetic data pipeline (SSRA). The core method involves a two-stage process: first, enhancing query diversity by re-annotating documents with varying relevance labels using a score model, and second, improving alignment between synthesized queries and target relevance labels through iterative refinement. Extensive experiments show that SSRA outperforms both prompt-based synthesis and vanilla supervised fine-tuning, with significant improvements in both retrieval and pair classification tasks.

## Method Summary
The SSRA pipeline uses a score model to re-annotate unlabeled data with 4-level relevance labels, creating diverse query-document pairs. A query model generates synthetic queries conditioned on documents and target relevance levels. The system then applies iterative refinement using score model filtering and LLM-based pairwise consistency checks to improve label alignment. Finally, an embedding model trained on the synthetic data achieves state-of-the-art performance on short video search relevance tasks.

## Key Results
- SSRA improves nDCG@10 by 1.73% and AP by 2.80% over baseline on domain-specific retrieval and pair classification tasks
- Online A/B testing shows 1.45% CTR increase, 4.9% SRR improvement, and 0.1054% IUPR gain
- Stage 1 reduces query duplication by 20.85% relative, improving diversity
- Stage 2 improves label alignment with 25.43% relative improvement in consistency

## Why This Works (Mechanism)

### Mechanism 1: Score-Based Re-Annotation for Diversity Enhancement
The score model assigns 4-level relevance labels to unlabeled query-document pairs, restructuring the training corpus so each document associates with multiple queries at different relevance levels rather than multiple documents mapping to the same high-frequency query. This prevents the query model from learning degenerate mappings. The core assumption is that the score model's predictions on unlabeled data sufficiently approximate true relevance distributions to serve as training supervision.

### Mechanism 2: Iterative Refinement via Dual Filtering
After initial synthesis, the score model filters samples where predicted labels mismatch target labels. Then, for each document, an LLM compares query pairs with different relevance labels and removes cases where the LLM's relative relevance judgment contradicts the intended labels. The refined data retrains the query model. The core assumption is that LLM pairwise judgments are more reliable for relative relevance ordering than absolute label assignment.

### Mechanism 3: Fine-Grained Relevance Supervision for Embedding Sensitivity
Multi-level labels provide richer supervision signal than binary contrastive learning. The weighted InfoNCE loss (s_i · L_infoNCE) scales gradient contributions by relevance level, teaching the model to rank partially relevant documents appropriately rather than treating them as negatives. The core assumption is that the downstream task benefits from distinguishing intermediate relevance levels.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The embedding model uses weighted InfoNCE loss where relevance labels scale the loss contribution. Understanding how contrastive learning pulls positive pairs closer and pushes negatives apart is essential to see why weighted versions enable ranking-aware embeddings.
  - Quick check question: Given a query-document pair with relevance label 2 (out of 3), how would the weighted InfoNCE loss treat this compared to a pair with label 3?

- **Semi-Supervised Learning with Pseudo-Labels**
  - Why needed here: SSRA uses a score model to label unlabeled data, then trains on these pseudo-labels. Understanding the risks of confirmation bias (where errors reinforce themselves) clarifies why iterative filtering is necessary.
  - Quick check question: If the score model mislabels 20% of unlabeled data as "highly relevant" when they're actually irrelevant, what happens if you train on these pseudo-labels without filtering?

- **4-Level Relevance Taxonomy**
  - Why needed here: The paper defines domain-specific relevance (0-3) for short video search. Understanding the annotation scheme is critical for interpreting results and adapting to other domains.
  - Quick check question: For a query "kids educational cartoons," would a video about "parenting tips for toddlers" deserve label 1 or 0 under the paper's definition?

## Architecture Onboarding

- **Component map:** Score Model -> Query Model -> Embedding Model with Filtering Layer
- **Critical path:**
  1. Collect seed annotated data (4-level labels, ~200K pairs)
  2. Train score model → use to re-annotate unlabeled data (Stage 1)
  3. Train initial query model on re-annotated + annotated data
  4. Generate synthetic queries → filter via score model + LLM (Stage 2)
  5. Retrain query model on filtered synthetic data
  6. Train embedding model on full corpus (annotated + synthetic)

- **Design tradeoffs:**
  - Prompt-based vs. Tuned Synthesis: Prompt-based (SyCL) is faster to implement but underperforms (Table 3 shows SyCL Modified degrades AP by 1.62-2.17 points). Tuned models require annotation investment.
  - Binary vs. Multi-Level Labels: Multi-level improves nDCG and mid-tier AP but may hurt AP@≥3 (Table 4). Choose based on whether downstream emphasizes ranking precision or top-tier recall.
  - Filtering Strictness: Aggressive filtering improves label alignment but reduces data volume. The paper retains only samples where predicted label matches target (Page 5), but doesn't report retention rates.

- **Failure signatures:**
  - Mode collapse in query generation: High duplicate query rate (>10%) indicates query model learned to output safe, generic queries regardless of document. Check frequency distribution (Table 9).
  - Middle-label confusion: If score model achieves <60% accuracy on labels 1 and 2, Stage 1 re-annotation will inject noise. Monitor per-class F1.
  - Retrieval-pairwise mismatch: If nDCG improves but AP degrades, the model learned global ranking but lost fine-grained discrimination. Check if retrieval test set has different label distribution than pair classification set (Table 1).

- **First 3 experiments:**
  1. **Baseline parity check:** Train embedding model on annotated data only (no synthesis). Report nDCG@10 and AP@≥1,2,3. Compare to paper's "Base Model" row (Table 3) to validate your pipeline.
  2. **Stage isolation ablation:** Train query model with Stage 1 only (no iterative filtering). Measure duplicate query rate and label alignment on a held-out document set. Compare to Table 6 to isolate Stage 2's contribution.
  3. **Label granularity sensitivity:** Train embedding models on binary (0/1 only) vs. full 4-level synthetic data. Use the balanced pair classification subset (as in Table 4) to determine if your domain benefits from fine-grained labels or if binary suffices.

## Open Questions the Paper Calls Out

The paper identifies several open questions but does not explicitly call them out as such in a dedicated section. The key unresolved issues include:

1. The specific trade-off between improved intermediate relevance detection and reduced performance in distinguishing highly relevant pairs (AP@≥3) needs mitigation strategies.
2. The extent to which the SSRA pipeline generalizes to text-heavy, data-scarce domains outside of short video search remains unclear.
3. Whether a 4-level relevance schema is optimal granularity for embedding alignment, or if performance scales with higher label resolutions, is unknown.
4. Whether iterative refinement leads to model collapse or reduced query diversity over multiple cycles is unexplored.

## Limitations

- Score model accuracy on middle relevance levels (1, 2) is not reported, creating uncertainty about whether re-annotation injects noise rather than diversity.
- The LLM pairwise filtering design is described abstractly without showing the actual prompt or filtering criteria, making effectiveness domain-dependent.
- Embedding model scalability is not addressed, with no analysis of whether synthetic data benefits transfer to larger models or production-scale deployments.

## Confidence

- **High confidence**: Stage 1's reduction in query duplication (20.85% relative decrease) and the online A/B testing results (1.45% CTR increase, 4.9% SRR improvement) are well-supported by specific metrics and controlled experiments.
- **Medium confidence**: The claim that fine-grained relevance supervision improves embedding sensitivity is supported by retrieval metrics but shows mixed results on pair classification (AP@≥3 degrades), suggesting the benefit is task-dependent rather than universal.
- **Low confidence**: The specific contribution of Stage 2 filtering (pairwise LLM + score model) is harder to isolate, as the paper reports improvements but doesn't provide ablation studies showing what happens with only one filtering method or neither.

## Next Checks

1. **Per-class score model evaluation**: Compute F1 scores for each relevance label (0, 1, 2, 3) on a held-out test set. If labels 1 and 2 fall below 60% F1, the re-annotation process may be unreliable and require model refinement or different sampling strategies.

2. **Label granularity ablation**: Train separate embedding models using binary relevance labels (0/1 only) versus full 4-level labels on the same synthetic data. Compare nDCG@10 and AP@≥1,2,3 on both retrieval and pair classification tasks to determine if your specific domain benefits from fine-grained supervision.

3. **Filtering component isolation**: Create three versions of the final query model: (a) with both Stage 1 and Stage 2 filtering, (b) with only Stage 1 (no iterative filtering), and (c) with only Stage 2 filtering (initial query model + pairwise filtering only). Measure duplicate query rate, label alignment accuracy, and downstream retrieval performance to quantify each stage's independent contribution.