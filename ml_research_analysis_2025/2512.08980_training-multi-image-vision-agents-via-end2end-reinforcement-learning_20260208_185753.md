---
ver: rpa2
title: Training Multi-Image Vision Agents via End2End Reinforcement Learning
arxiv_id: '2512.08980'
source_url: https://arxiv.org/abs/2512.08980
tags:
- visual
- tool
- reasoning
- zhang
- multi-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IMAgent addresses the challenge of enabling vision agents to perform
  multi-image tasks through end-to-end reinforcement learning. It introduces a novel
  multi-agent pipeline to generate challenging multi-image QA pairs and curates the
  MIFG-QA dataset.
---

# Training Multi-Image Vision Agents via End2End Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.08980
- Source URL: https://arxiv.org/abs/2512.08980
- Authors: Chengqi Dong; Chuhuai Yue; Hang He; Rongge Mao; Fenghe Tang; S Kevin Zhou; Zekun Xu; Xiaohan Wang; Jiajun Chai; Wei Lin; Guojun Yin
- Reference count: 40
- Primary result: IMAgent achieves 88.48% on V* HR-Bench and 49.21% on MIFG-QA using end-to-end RL with visual tools

## Executive Summary
IMAgent addresses the challenge of enabling vision agents to perform multi-image tasks through end-to-end reinforcement learning. It introduces a novel multi-agent pipeline to generate challenging multi-image QA pairs and curates the MIFG-QA dataset. The approach employs specialized tools for visual reflection and confirmation, allowing dynamic attention reallocation during reasoning. Using a two-level mask strategy, IMAgent achieves stable tool use behavior without requiring supervised fine-tuning data. Experiments demonstrate that IMAgent outperforms state-of-the-art models on both single-image benchmarks and the proposed multi-image dataset, achieving 88.48% on V* HR-Bench and 49.21% on MIFG-QA, while providing actionable insights for training vision agents.

## Method Summary
IMAgent trains a vision agent for multi-image QA tasks using end-to-end reinforcement learning. The method employs Qwen2.5-VL-7B-Instruct as the base model and uses GRPO for optimization. Two specialized tools are implemented: visual confirmation (crop/zoom) and visual reflection (image reuse/lookback). Training uses a two-level mask strategy that excludes tool outputs from gradients (action-level) and filters invalid trajectories (trajectory-level). The approach combines single-image data (Deepeyes) with multi-image data (MIFG-QA) generated through a multi-agent pipeline. Tool gain is added to the reward function to encourage exploration during Zero RL training. The system caps pixel budget at 4M and uses batch size 256 with 8 rollouts per prompt.

## Key Results
- Achieves 88.48% accuracy on V* HR-Bench benchmark
- Achieves 49.21% accuracy on MIFG-QA multi-image dataset
- Outperforms state-of-the-art models on single-image benchmarks (70.64% on HR-Bench, 73.83% on MME-RealWorld)

## Why This Works (Mechanism)

### Mechanism 1: Visual Attention Reallocation via Tool Integration
If a VLM is equipped with tools that force the re-injection of visual tokens into the context, the model may maintain attention on image content during later reasoning stages, mitigating "attention drift." By invoking a "zoom-in" or "lookback" tool, the system inserts new visual patches or reloads original image tokens into the active context window. This forces the attention mechanism to re-engage with visual data in mid-to-late layers. The core assumption is that the model has learned that invoking these tools correlates with higher reward, and the visual features extracted from tool outputs are sufficiently distinct to trigger re-allocation of attention heads.

### Mechanism 2: Stabilizing RL via Action-Trajectory Masking
If training masks gradients from external tool outputs and filters invalid trajectories, policy optimization remains stable without Supervised Fine-Tuning (SFT). Tool outputs often follow a different distribution than the base VLM's pre-training data. Action-level masking prevents these external tokens from contributing to the loss, ensuring the model learns the decision to use a tool rather than the probability of the tool's output. Trajectory-level masking discards entire rollouts that exceed length limits or fail to answer, preventing "reward hacking" or collapse from low-probability token accumulation.

### Mechanism 3: Exploration Injection via Tool Gain
If a specific reward bonus is granted for tool usage, models are less likely to collapse into "lazy" direct answering strategies during early Zero-RL training. By multiplying the accuracy reward by a factor including Tool_gain, the policy receives a stronger gradient signal for trajectories that successfully integrate tools, encouraging the emergence of agentic behavior before the model converges on simpler direct-answer heuristics. The core assumption is that the base model possesses latent tool-calling capability that requires a stronger incentive than task accuracy alone to activate.

## Foundational Learning

- **Concept: Attention Drift in VLMs**
  - Why needed here: IMAgent is explicitly designed to fix the phenomenon where VLMs "forget" visual inputs as they generate longer reasoning chains.
  - Quick check question: How does the entropy of a VLM's attention distribution over image tokens change from the first generated token to the 100th in a standard CoT?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: This is the specific RL algorithm used (a variant of PPO/RLVR) to train the agent without a separate critic model.
  - Quick check question: How does GRPO estimate the baseline (advantage) for a given prompt, and why does this remove the need for a value function approximator?

- **Concept: Zero-RL / RLVR**
  - Why needed here: The paper emphasizes training "pure RL" without SFT cold-start, relying entirely on verifiable rewards.
  - Quick check question: What is the primary risk of skipping the SFT "cold-start" phase when training an agentic workflow (e.g., format instability)?

## Architecture Onboarding

- **Component map:** User Query + Multi-Image Input -> Initial CoT -> Model generates tool call -> Tool Executor returns Cropped Image/Reloads Image -> Context Update -> Continue Generation -> Final Answer -> Reward Calculation -> Policy Update

- **Critical path:** 1. User Query + Multi-Image Input -> Initial CoT. 2. Model generates tool call (bbox or index) -> Action Mask applied here. 3. Tool Executor returns Cropped Image or Reloads Image. 4. Context Update -> Continue Generation. 5. Final Answer -> Reward Calculation (Accuracy * Tool Gain + Format). 6. Trajectory Mask -> If invalid (timeout/no answer), zero out gradients.

- **Design tradeoffs:** Pixel Budget capped at 4M to save context space for interaction turns, trading off raw resolution for multi-turn capability. Reward Shaping uses Tool_gain, trading "pure" accuracy optimization for behavioral diversity. Dataset Mix combines single-image and multi-image data, which may dilute specific multi-image fine-tuning.

- **Failure signatures:** Tool Abandonment (model generates answer immediately), Infinite Loop (model repeatedly calls tools), Attention Drift (model hallucinates details absent from image).

- **First 3 experiments:** 1. Ablation on Tool Gain: Run training with Tool_gain=0 vs. Tool_gain=1 to verify if tool usage rates drop to zero. 2. Masking Comparison: Train two models, one with and one without Trajectory-Level Masking, to observe stability on MIFG-QA validation set. 3. Attention Visualization: Visualize attention maps for a multi-image prompt to confirm if Lookback tool actually increases attention weights on original image tokens.

## Open Questions the Paper Calls Out
- What is the optimal design for tool use incentives under Zero RL, and how does the incentive magnitude affect the balance between exploration and exploitation of visual tools?
- Why does step-by-step chain-of-thought reasoning degrade performance compared to direct answers on high-resolution benchmarks like HR-Bench?
- How well does IMAgent generalize to multi-image tasks involving more than the 2-4 images typically present in MIFG-QA, and does the tool-use strategy scale gracefully?

## Limitations
- Data Quality Dependency: The multi-agent pipeline for generating MIFG-QA relies on synthetic question-answer pairs filtered by a base model, with filtering criteria not extensively detailed.
- Tool Use Generalization: Performance on MIFG-QA may be partially attributable to learning patterns specific to this dataset's construction rather than developing truly generalizable multi-image reasoning capabilities.
- Reward Shaping Sensitivity: The reliance on tool gain for encouraging exploration during Zero RL training raises questions about the stability of this approach, with limited exploration of alternative reward function formulations.

## Confidence
- **High Confidence**: The architectural framework (visual tools + GRPO training + two-level masking) is well-specified and the reported benchmark results on established datasets appear reproducible.
- **Medium Confidence**: The attention reallocation mechanism's effectiveness is supported by ablation studies and qualitative observations, but direct visualization evidence is limited to a single example.
- **Low Confidence**: The multi-agent pipeline's ability to generate challenging, diverse multi-image QA pairs at scale is demonstrated but not independently verified.

## Next Checks
1. **Attention Pattern Verification**: Conduct systematic attention visualization across 100+ multi-image samples to quantify whether tool use (particularly Lookback) consistently increases attention weights on relevant image regions compared to baseline models without tools.

2. **Cross-Dataset Generalization Test**: Evaluate IMAgent on established multi-image reasoning datasets not used in training (such as MM-REACT or modified versions of existing benchmarks) to verify that tool use behavior generalizes beyond the MIFG-QA distribution.

3. **Zero RL Stability Analysis**: Perform extended training runs (3x the reported duration) with varying Tool_gain values (0.0, 0.5, 1.0, 2.0) to characterize the stability boundary where exploration collapses or reward hacking emerges, and identify optimal reward shaping parameters.