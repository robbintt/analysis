---
ver: rpa2
title: Understanding Mode Connectivity via Parameter Space Symmetry
arxiv_id: '2505.23681'
source_url: https://arxiv.org/abs/2505.23681
tags:
- connected
- linear
- connectivity
- components
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to understanding mode connectivity
  in neural networks through the lens of parameter space symmetry. The authors show
  that continuous and discrete symmetries in the parameter space can explain the number
  of connected components of minima in linear networks and provide explicit expressions
  for low-loss curves connecting symmetric minima.
---

# Understanding Mode Connectivity via Parameter Space Symmetry

## Quick Facts
- arXiv ID: 2505.23681
- Source URL: https://arxiv.org/abs/2505.23681
- Authors: Bo Zhao; Nima Dehmami; Robin Walters; Rose Yu
- Reference count: 40
- Primary result: Introduces parameter space symmetry framework to explain mode connectivity in neural networks, showing how continuous and discrete symmetries determine connected components of minima and enable explicit low-loss curve construction.

## Executive Summary
This paper presents a novel theoretical framework for understanding mode connectivity in neural networks through the lens of parameter space symmetry. The authors establish that the topology of symmetry groups directly determines the number of connected components in the loss landscape of linear networks. They derive explicit expressions for low-loss curves connecting symmetric minima using Lie group theory, and identify precise conditions under which linear mode connectivity fails due to unbounded error barriers in networks with homogeneous activations.

The framework provides new insights into when and why linear interpolation between minima works or fails, complementing existing theories on loss landscape connectivity. By relating the topology of symmetry groups to that of minima, the paper offers a unifying perspective on mode connectivity that applies across different network architectures and activation functions, with implications for optimization dynamics and generalization.

## Method Summary
The method involves analyzing neural network parameter spaces through symmetry groups, identifying continuous symmetries like general linear groups for linear networks and discrete symmetries like permutations for networks with homogeneous activations. The approach constructs explicit low-loss curves using the exponential map of Lie groups and derives conditions for mode connectivity failure. Experiments involve training networks with SGD to find minima, applying symmetry transformations to generate second minima, and comparing loss along symmetry-induced curves versus linear interpolation. The framework is validated on synthetic datasets with controlled architectures to verify theoretical predictions about connected components and error barriers.

## Key Results
- Establishes homeomorphism between minima of linear networks and $(GL_h)^{l-1}$, proving the number of connected components is determined by the symmetry group topology
- Derives explicit low-loss curves connecting symmetric minima using exponential map construction: $\gamma(t) = \exp(t \log(g)) \cdot w$
- Proves mode connectivity up to permutation for linear networks with invertible weights
- Demonstrates unbounded error barriers during linear interpolation between minima related by rescaling symmetries in homogeneous activation networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The connectedness of minima in linear networks is topologically determined by the symmetry group of the parameter space.
- **Mechanism:** The paper establishes a homeomorphism between the minimum set $L^{-1}(0)$ and the symmetry group $(GL_h)^{l-1}$. Since general linear groups have two connected components (positive and negative determinant), the minimum inherits this topology.
- **Core assumption:** The network has full-rank weights and a loss function invariant to the group action.
- **Evidence anchors:** [abstract] "derive the number of connected components of the minima of linear networks"; [section 4.1] "Proposition 4.1. There is a homeomorphism between $L^{-1}(0)$ and $(GL_h)^{l-1}$."; [corpus] "Symmetry in Neural Network Parameter Spaces" supports the general premise that symmetries define equivalence classes in parameter space.
- **Break condition:** The homeomorphism breaks if weights are not invertible or if regularization removes the symmetry.

### Mechanism 2
- **Claim:** Explicit low-loss curves connecting minima can be constructed using the exponential map of Lie group symmetries.
- **Mechanism:** Given a symmetry $g$ mapping weight $w_1$ to $w_2$, the curve $\gamma(t) = \exp(t \log(g)) \cdot w_1$ stays on the minimum level set because group actions preserve loss values.
- **Core assumption:** The two minima belong to the same orbit of the symmetry group.
- **Evidence anchors:** [abstract] "provide explicit expressions for low-loss curves connecting symmetric minima"; [section 6.1] Equation (5): "$\gamma(t, g, w) = \exp(t \log(g)) \cdot w$... Hence, $\gamma$ is a curve that connects the points."; [corpus] Weak direct corpus evidence for this specific Lie group curve construction; mechanism is primarily derived from the paper's theoretical contribution.
- **Break condition:** The curve construction fails if the symmetries are approximate rather than exact, or if the Lie group structure does not apply.

### Mechanism 3
- **Claim:** Linear Mode Connectivity (LMC) fails when interpolating between minima that are distant on unbounded orbits created by rescaling symmetries.
- **Mechanism:** Homogeneous activations (e.g., ReLU) allow rescaling $(W_{l-1}, W_l) \to (cW_{l-1}, c^{-1}W_l)$. A linear path between such scaled minima passes through regions of high loss (unbounded error barrier) because the "valley" is curved.
- **Core assumption:** The network has at least two layers and a homogeneous activation function.
- **Evidence anchors:** [abstract] "demonstrating that error barriers can be unbounded in multi-layer regressions with homogeneous activations"; [section 5.2] "Proposition 5.3... error barrier on the linear interpolation... is unbounded"; [corpus] "Entropic Confinement..." identifies entropic barriers, providing a complementary view on why linear paths might fail.
- **Break condition:** If parameters are bounded or the curvature of the connecting curve is small, LMC may hold approximately (Theorem 6.2).

## Foundational Learning

- **Concept: Topological Connectedness vs. Path Connectedness**
  - **Why needed here:** The paper relies on the fact that continuous maps (like group actions) preserve connectedness to prove that minima are linked.
  - **Quick check question:** If a symmetry group has 2 disconnected components, what is the upper bound on the number of components of its orbit in the loss landscape?

- **Concept: General Linear Group ($GL_n$)**
  - **Why needed here:** This group represents the continuous symmetries in linear networks; its topology (specifically the sign of the determinant) dictates the structure of the minima.
  - **Quick check question:** Why does the determinant sign create two disconnected components in $GL_n(\mathbb{R})$?

- **Concept: Homogeneous Functions**
  - **Why needed here:** Understanding why activations like ReLU allow for rescaling symmetries, which the paper proves can break linear connectivity.
  - **Quick check question:** If $\sigma(cz) = c^k\sigma(z)$, how does scaling the weight $W$ by $c$ affect the pre-activation input?

## Architecture Onboarding

- **Component map:** Parameter Space -> Level Set ($L^{-1}(c)$) -> Symmetry Group -> Orbit

- **Critical path:**
  1. Identify the symmetry group of the architecture (e.g., $GL_h$ for linear, permutations for ReLU).
  2. Determine the topology (connected components) of that group.
  3. Map this topology to the minimum set to count connected components (Prop 4.1).
  4. Use Lie group exponentiation to generate explicit connecting curves (Eq 5).

- **Design tradeoffs:**
  - **Skip Connections:** Reduce the number of disconnected components in the minimum (Prop 4.3), potentially improving connectivity but changing the loss landscape geometry.
  - **Linear Interpolation:** Efficient but prone to unbounded error barriers in homogeneous networks; Symmetry curves are guaranteed low-loss but computationally more complex to derive.

- **Failure signatures:**
  - **Unbounded Loss on Interpolation:** If linearly interpolating between two checkpoints yields exploding loss, check if they differ by a large rescaling factor (Prop 5.3).
  - **Disconnected Minima:** For linear networks, checking the sign of determinants of weight matrices across layers can identify if solutions are in the same connected component (Cor 4.2).

- **First 3 experiments:**
  1. **Verify Component Count:** Train a 3-layer linear network with full-rank data; cluster solutions based on weight determinants to verify the $2^{l-1}$ components prediction (Cor 4.2).
  2. **Symmetry Curve Test:** Implement Equation (5) to connect two minima from the same orbit in a small network; plot the loss along the curve vs. the linear interpolation to verify it remains constant.
  3. **Rescaling Barrier:** Reproduce the unbounded error barrier by taking a trained ReLU network, generating a second minimum via weight rescaling $(cW, c^{-1}W)$, and measuring the loss peak during linear interpolation (Prop 5.3).

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the reduction in the number of connected components of the minimum (induced by skip connections) affect the optimization dynamics and generalization capabilities of neural networks?
  - **Basis in paper:** [explicit] The paper states, "We leave the connection between the topology of the minimum and the optimization and generalization properties of neural networks to future work" following the proof that skip connections reduce connected components.
  - **Why unresolved:** While the paper mathematically proves that architectures like ResNets reduce the number of components compared to linear networks, it does not empirically or theoretically link this topological property to learning performance.
  - **What evidence would resolve it:** Empirical studies correlating the number of connected components (or connectivity width) with convergence rates or generalization gaps, or theoretical bounds linking topology to the "flatness" of minima.

- **Open Question 2:** What are the precise necessary and sufficient conditions for linear mode connectivity (LMC) to hold, particularly regarding the compactness of symmetry groups and parameter bounds?
  - **Basis in paper:** [explicit] The authors note, "Whether the loss barrier on the linear interpolation is bounded can depend on the compactness of the symmetry group and the curvature of the minimum. We leave a systematic investigation of the condition for linear mode connectivity to future work."
  - **Why unresolved:** The paper demonstrates failure cases where the barrier is unbounded due to rescaling symmetries, but it does not provide a unified condition that explains why LMC is frequently observed in practical training scenarios (e.g., SGD solutions).
  - **What evidence would resolve it:** A theoretical framework proving that specific constraints on parameter norms or symmetry group structures guarantee bounded loss barriers during linear interpolation.

- **Open Question 3:** How can the explicit expressions for connecting curves be generalized to non-linear networks where the complete set of symmetries is unknown or difficult to characterize?
  - **Basis in paper:** [explicit] The Discussion section identifies this as a primary challenge: "Extending these results to nonlinear networks is a challenging yet exciting future direction. A full characterization of the minima in non-linear settings requires identifying the complete set of symmetriesâ€”an open problem for many architectures."
  - **Why unresolved:** The paper's derivation of connecting curves and mode connectivity proofs relies heavily on known continuous symmetries (like invertible matrices or homogeneous rescaling), which are harder to define for complex, non-linear activations.
  - **What evidence would resolve it:** Derivation of connecting curves for networks with non-homogeneous activations (e.g., sigmoids) or a method to approximate symmetry-induced curves without exact knowledge of the underlying group structure.

## Limitations

- The theoretical framework assumes exact symmetries that rarely hold perfectly in practical neural networks due to numerical precision, regularization, and optimization constraints.
- The exponential map construction for explicit curves requires finding appropriate symmetry transformations, which is computationally challenging and not addressed for general cases.
- The unbounded error barrier results assume infinite parameter ranges, while practical implementations typically include weight decay or other regularization that bounds parameters.

## Confidence

- **Topology-Dictated Connected Components (Mechanism 1):** High confidence. The mathematical proof connecting the topology of the symmetry group to the connected components of minima is rigorous and well-supported by established group theory.
- **Explicit Low-Loss Curve Construction (Mechanism 2):** Medium confidence. The theoretical construction is mathematically sound, but the practical applicability depends on identifying exact symmetries, which is non-trivial in real networks.
- **Failure of Linear Mode Connectivity (Mechanism 3):** High confidence. The proof of unbounded error barriers for homogeneous activations is rigorous, and the mechanism is well-understood through the rescaling symmetry argument.
- **Approximate Mode Connectivity Conditions (Theorem 6.2):** Medium confidence. The theorem provides conditions under which LMC approximately holds, but the practical relevance depends on quantifying "small" curvature and "bounded" parameters in specific contexts.

## Next Checks

1. **Practical Symmetry Detection:** Implement an algorithm to detect approximate symmetries between trained minima in small neural networks and verify whether the theoretical predictions about connected components match empirical observations.

2. **Error Barrier Scaling:** Systematically vary the rescaling factor in homogeneous activation networks to empirically measure how error barriers scale with the distance between minima, comparing against the theoretical unbounded bound.

3. **Approximate vs. Exact Symmetry Impact:** Train networks with and without explicit symmetry-breaking regularization (e.g., weight decay with different strengths) and measure the impact on mode connectivity to quantify how approximate symmetries affect the theoretical predictions.