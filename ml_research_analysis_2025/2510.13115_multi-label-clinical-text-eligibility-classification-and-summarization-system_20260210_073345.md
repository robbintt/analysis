---
ver: rpa2
title: Multi-Label Clinical Text Eligibility Classification and Summarization System
arxiv_id: '2510.13115'
source_url: https://arxiv.org/abs/2510.13115
tags:
- data
- clinical
- classification
- text
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-label clinical text eligibility classification
  and summarization system using NLP and LLMs. It addresses the challenge of efficiently
  identifying eligible participants for clinical trials by automating the classification
  and summarization of clinical notes.
---

# Multi-Label Clinical Text Eligibility Classification and Summarization System

## Quick Facts
- arXiv ID: 2510.13115
- Source URL: https://arxiv.org/abs/2510.13115
- Reference count: 7
- Primary result: Multi-label classification F1 = 0.83; abstractive summarization ROUGE-L F1 = 0.42

## Executive Summary
This paper presents a system for automating clinical trial eligibility classification and summarization using natural language processing and large language models. The system combines traditional machine learning (Random Forest, SVM) with feature engineering techniques including TF-IDF, Word2Vec embeddings, and Named Entity Recognition to classify clinical notes into eligibility criteria. The approach demonstrates that classifier chains improve performance by capturing label dependencies, achieving micro-averaged F1 scores up to 0.83. The system also provides both extractive and abstractive summarization capabilities to generate clinical justifications for eligibility decisions.

## Method Summary
The system processes clinical notes through preprocessing (NLTK-based cleaning, tokenization, lemmatization) followed by feature engineering using TF-IDF, Word2Vec embeddings (optionally TF-IDF weighted), and NER presence/absence features. Multi-label classification is performed using Binary Relevance, Classifier Chains, and Multi-Output Classifier wrappers around Random Forest and SVM models. For summarization, extractive methods (Luhn, TF-IDF-based) are compared with abstractive summarization using GPT-3 via LangChain refine prompts. The best performance (F1 = 0.83) is achieved with Classifier Chains using Random Forest on TF-IDF weighted embeddings combined with TF-IDF features.

## Key Results
- Micro-averaged F1 score of 0.83 achieved with Classifier Chains + Random Forest using TF-IDF weighted embeddings
- Classifier Chains outperform Binary Relevance (F1 = 0.66) by capturing label dependencies
- Abstractive summarization via GPT-3 shows higher ROUGE-L (F1 = 0.42) than extractive methods but exhibits hallucination risks
- Zero-shot GPT classification achieves F1 = 0.62 without feature engineering, lagging behind engineered approaches

## Why This Works (Mechanism)

### Mechanism 1
Classifier chains improve multi-label clinical eligibility classification by capturing label dependencies. Each classifier in the chain receives both the original features and predictions from preceding classifiers, allowing subsequent classifiers to condition on earlier label decisions. This contrasts with Binary Relevance, which treats each label independently. The approach achieved F1 = 0.83 versus Binary Relevance's F1 = 0.66, demonstrating the value of modeling label correlations in clinical eligibility criteria.

### Mechanism 2
TF-IDF weighted Word2Vec embeddings improve classification by combining semantic representation with term importance. Word2Vec provides 100-dimensional semantic vectors while TF-IDF weighting prioritizes terms that are frequent in documents but rare across the corpus. The weighted average produces document vectors reflecting both semantics and discriminative importance, contributing to the best classification performance.

### Mechanism 3
Named Entity Recognition provides compact, clinically meaningful features that reduce redundancy. The Clinical-AI-Apollo/Medical-NER transformer extracts 18 entity types (Sign_Symptom, Disease_disorder, lab_value, etc.) with entity presence/absence encoded as binary features. This compresses clinical narratives into structured signals that are more discriminative than raw term frequency for eligibility classification.

## Foundational Learning

- **Multi-label vs. Multi-class Classification**: Clinical patients may satisfy multiple eligibility criteria simultaneously, requiring multi-label methods rather than standard multi-class which assumes mutual exclusivity. Quick check: If a patient meets criteria A, does that make them ineligible for criteria B? If no, this is multi-label.

- **Classifier Chains and Label Dependency**: Classifier chains pass predictions forward, allowing subsequent classifiers to condition on earlier decisions. This captures conditional dependence between labels. Quick check: Does knowing a patient has advanced CAD make it more or less likely they have diabetes-related eligibility? If yes, label dependency exists.

- **ROUGE Metrics for Summarization Evaluation**: ROUGE-1, ROUGE-2, and ROUGE-L measure n-gram overlap between generated and reference summaries. Quick check: A ROUGE-1 F1 of 0.42 indicates moderate unigram overlap; content is captured but not exhaustive.

## Architecture Onboarding

- **Component map**: Raw clinical text -> preprocessing -> feature extraction (NER + TF-IDF weighted embeddings) -> Classifier Chains (RF) -> multi-label predictions -> LangChain refine prompts -> GPT-3 abstractive summary -> UI output

- **Critical path**: 1) Clinical text preprocessing and feature engineering, 2) Classifier Chains with Random Forest for eligibility prediction, 3) GPT-3 abstractive summarization via LangChain for clinical justification, 4) Flask-based web interface for results

- **Design tradeoffs**: Classifier Chains capture dependencies but increase inference latency and error propagation risk versus Binary Relevance. Extractive summarization is faithful but less fluent (ROUGE-L F1 = 0.42 max) compared to abstractive (GPT-3) which is fluent but may hallucinate. Zero-shot GPT classifier requires no feature engineering but lags in performance (F1 = 0.62).

- **Failure signatures**: Low recall on minority classes due to class imbalance, LLM hallucination generating plausible but incorrect justifications, and NER out-of-domain failures when clinical notes use terminology outside Medical-NER training.

- **First 3 experiments**: 1) Replicate TF-IDF + Classifier Chains (RF) baseline to verify F1 â‰ˆ 0.83, 2) Perform ablation study removing NER features then TF-IDF weighting to measure individual component contributions, 3) Audit abstractive summarization on held-out records to establish hallucination rate baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can abstractive summarization hallucinations be reliably suppressed without compromising clinical justification relevance? The Discussion section documents GPT-3 generating convincing but factually incorrect summaries, such as inventing kidney damage history to justify creatinine eligibility. This "hallucinating nature" is identified but no mitigation strategies are evaluated. Resolution would require quantitative analysis of factual consistency scores or hallucination rate reduction using retrieval-augmented generation.

### Open Question 2
Does Random Forest Classifier Chain maintain F1 = 0.83 performance when scaled to all 13 n2c2 eligibility criteria? The authors restricted to 4 criteria "based on available data," leaving the system's ability to handle full dataset complexity untested. It's unclear if current feature engineering suffices for remaining criteria requiring different semantic nuances. Resolution requires micro-averaged F1 scores across all 13 labels using the same architecture.

### Open Question 3
Would advanced deep learning models (e.g., BERT) outperform traditional machine learning in this low-data regime? The authors note their basic Neural Network Perceptron performed poorly and suggest advanced deep learning could improve performance. However, the conclusion favoring Random Forest is based on comparison with a simple neural network rather than modern transformers. Resolution requires comparison against fine-tuned clinical LLMs.

## Limitations
- Class imbalance effects on micro-F1 scores are not explicitly addressed; per-class performance metrics are absent
- GPT-3 hallucination cases are documented but not quantified; hallucination rate is unknown
- No ablation studies shown for individual feature types (NER, embeddings, TF-IDF weighting)

## Confidence

- **High**: Classifier chains mechanism for capturing label dependencies is well-established in multi-label literature and aligns with experimental results
- **Medium**: TF-IDF weighted embeddings approach is plausible and shows strong results but lacks cross-study comparison
- **Low**: Impact of NER features is demonstrated but not rigorously isolated from other methods; hallucination rates in abstractive summarization are qualitative only

## Next Checks

1. **Per-class performance analysis**: Compute precision, recall, and F1 for each eligibility criterion to assess class imbalance impact and identify underperforming labels

2. **Hallucination quantification**: Annotate a held-out set of abstractive summaries for factual consistency; report hallucination rate and identify common error patterns

3. **Ablation study**: Systematically remove NER, TF-IDF weighting, and embeddings to isolate each component's contribution to the F1 = 0.83 result and quantify their individual impact