---
ver: rpa2
title: 'How to Capture and Study Conversations Between Research Participants and ChatGPT:
  GPT for Researchers (g4r.org)'
arxiv_id: '2503.18303'
source_url: https://arxiv.org/abs/2503.18303
tags:
- interface
- data
- researcher
- will
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers need standardized tools to study how people interact
  with large language models like ChatGPT, but existing methods are ad hoc and non-reproducible.
  The study introduces GPT for Researchers (G4R), a free website (g4r.org) that allows
  researchers to easily create customized GPT interfaces, integrate them into studies
  (e.g., Qualtrics surveys), and capture all participant-GPT interactions.
---

# How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)

## Quick Facts
- arXiv ID: 2503.18303
- Source URL: https://arxiv.org/abs/2503.18303
- Reference count: 1
- Primary result: A free, standardized tool (g4r.org) that enables researchers to create customizable ChatGPT interfaces, capture participant interactions, and merge data with survey responses for reproducible human-AI interaction research.

## Executive Summary
Researchers studying human-AI interactions lack standardized tools, leading to methodological fragmentation and difficulties in comparing or replicating results. The paper introduces GPT for Researchers (G4R), a free web-based platform that allows researchers to easily create customized ChatGPT interfaces and integrate them into studies (e.g., Qualtrics surveys). G4R captures all participant-GPT interactions, generates participant IDs for data merging, and provides downloadable message logs. This standardized approach enables reproducible and scalable research on human-AI interactions.

## Method Summary
G4R is a web application built with PHP, JavaScript, and HTML that interfaces with the OpenAI API. Researchers configure their study through a form specifying parameters like system prompts, message limits, temperature, and labels. The tool generates a JavaScript snippet that embeds a chat interface in Qualtrics surveys, creating a unique participant ID (g4r_pid) stored in an embedded data field. All messages, responses, and timestamps are logged and can be downloaded as CSV files. Researchers merge these logs with survey responses using the participant ID as a key.

## Key Results
- G4R provides 12 customizable parameters including system prompts, temperature (0.0-2.0), message limits, and interface labels
- The tool automatically generates participant IDs and captures complete conversation logs with timestamps
- Researchers can merge conversational data with survey responses using provided R scripts and the g4r_pid field

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A pre-built, configurable interface reduces methodological fragmentation in human-AI interaction research by replacing ad hoc custom platforms with a standardized tool.
- Mechanism: G4R provides a consistent API-connected chat interface with researcher-configurable parameters (system prompts, message limits, labels) that generates comparable interaction data across studies, enabling replication and cross-study comparison.
- Core assumption: Researchers will adopt a shared platform rather than continue building custom solutions; comparability across studies is valued over maximum flexibility.
- Evidence anchors:
  - [abstract] "researchers lack standardized tools for systematically studying people's interactions with LLMs"
  - [section] "The diverse ways to study human-AI interactions may have made it difficult to compare findings across different research papers, replicate results from those papers, or efficiently scale research efforts."
  - [corpus] Related work (Costello et al., Nie et al., Jelson et al.) built separate custom platforms—empirical validation that fragmentation exists, but no direct evidence yet that G4R resolves it.
- Break condition: If researchers require interface features G4R cannot support (e.g., multimodal inputs, custom retrieval-augmented generation), they will revert to custom builds.

### Mechanism 2
- Claim: Participant ID linkage via embedded data fields enables reliable merging of conversational data with survey responses, solving the data integration problem in multi-platform behavioral studies.
- Mechanism: JavaScript in Qualtrics generates a random `g4r_pid`, stores it in an embedded data field, and includes it in G4R message logs. Researchers later join datasets on this key using provided R scripts.
- Core assumption: Participants complete the survey in a single session without clearing cookies; the embedded data field is initialized before assignment.
- Evidence anchors:
  - [section] "Putting this Embedded Data field at the top of the Survey Flow ensures that this field will be initialized before any value will later be assigned to it."
  - [section] "Participants can merge (1) the message data downloaded from G4R with (2) the survey response data from Qualtrics using the participant IDs"
  - [corpus] No direct corpus evidence on this specific mechanism; related papers do not discuss data integration methods.
- Break condition: If `g4r_pid` fails to persist (e.g., iframe restrictions, cross-origin policies in embedded mode), linkage breaks and data cannot be merged.

### Mechanism 3
- Claim: System prompts and parameter controls (temperature, message limits, prepended/appended text) shape participant-GPT interactions to align with specific research objectives.
- Mechanism: The OpenAI API accepts a system message that sets behavioral constraints; temperature controls token sampling randomness. G4R exposes these as form fields, allowing non-technical researchers to configure GPT behavior without writing API calls.
- Core assumption: Researchers understand how system prompts and temperature affect LLM outputs; default values (temperature=1.0) produce acceptable baseline behavior.
- Evidence anchors:
  - [section] "A system prompt lets researchers guide ChatGPT's responses by setting things like the tone, context, or constraints for the study participants' interactions with GPT."
  - [section] "Lower values (e.g., values closer to 0.0) make responses more focused and deterministic, while higher values (e.g., values closer to 2.0) make responses more random, diverse, or creative"
  - [corpus] Adjacent literature (e.g., emotional prompts affecting AI behavior) supports that LLM responses are manipulable via prompt design, but this is general LLM knowledge, not G4R-specific validation.
- Break condition: If researchers set extreme temperatures or poorly designed system prompts without testing, GPT may produce incoherent or off-topic responses, compromising study validity.

## Foundational Learning

- Concept: **Qualtrics Survey Flow and Embedded Data**
  - Why needed here: G4R integration requires adding an embedded data field (`g4r_pid`) and positioning it correctly in Survey Flow to capture participant IDs before they're assigned.
  - Quick check question: Can you explain why an embedded data field must be placed at the top of the Survey Flow before any logic blocks?

- Concept: **LLM Parameters (System Prompts and Temperature)**
  - Why needed here: G4R exposes these as configuration options; understanding their effects is necessary to design appropriate experimental manipulations.
  - Quick check question: If you want GPT to give highly consistent answers across participants, should you set temperature closer to 0.0 or 2.0?

- Concept: **Data Merging via Join Keys**
  - Why needed here: The core value of G4R is producing analyzable linked datasets; researchers must understand how to join message logs to survey responses using `g4r_pid`.
  - Quick check question: Given a message dataset with multiple rows per participant and a survey dataset with one row per participant, what join type preserves all survey responses while attaching message history?

## Architecture Onboarding

- Component map: G4R Web App -> Qualtrics Survey -> GPT Interface -> Data Pipeline
- Critical path:
  1. Create G4R account → configure interface (system prompt, temperature, message limits)
  2. Copy generated JavaScript to target Qualtrics question
  3. Add `g4r_pid` embedded data field at top of Survey Flow
  4. Deploy survey, collect data
  5. Download message CSV from Researcher Home; merge with Qualtrics export using provided R script

- Design tradeoffs:
  - **New tab vs. embedded mode**: New tab avoids iframe restrictions but may increase participant dropoff; embedded mode keeps participants in survey but risks display/CSS issues and cross-origin cookie problems
  - **Default vs. custom temperature**: Defaults (1.0) are safer for exploratory work; customization requires pilot testing
  - **G4R-hosted API key vs. researcher's own key**: Currently optional; future API usage limits may force researcher-provided keys, adding onboarding friction

- Failure signatures:
  - `g4r_pid` column in Qualtrics export is empty → embedded data field not at top of Survey Flow
  - Message CSV has no data → participants never loaded the GPT interface (check if JS executed)
  - GPT responses off-topic or incoherent → system prompt poorly specified or temperature misconfigured
  - Merge produces too few matches → participant IDs truncated or corrupted during export

- First 3 experiments:
  1. **Pilot with default settings**: Create a G4R interface, accept all defaults, embed in a test Qualtrics survey, complete as a participant, download and merge data. Verify end-to-end data flow.
  2. **Test temperature variation**: Create two interfaces (temperature 0.2 vs. 1.8), ask identical questions, compare response variability. Assess whether the parameter is working as expected.
  3. **Embedded vs. new tab comparison**: Deploy the same interface in both modes across small participant groups (n=5 each). Compare data capture rates and any technical issues (e.g., blocked iframes, pop-up blockers).

## Open Questions the Paper Calls Out

- Question: How can the long-term operational sustainability of G4R be ensured if user demand exceeds the authors' API cost allowances?
- Question: Does the G4R interface induce behavioral changes in participants compared to the native ChatGPT interface?
- Question: Does the use of a centralized tool like G4R effectively improve the replicability and comparability of human-AI interaction studies?

## Limitations
- No empirical evidence yet on whether researchers actually adopt G4R over custom solutions
- Technical dependencies on Qualtrics Survey Flow behavior and browser restrictions may vary across institutional setups
- API key management creates uncertainty about long-term accessibility

## Confidence
- **High**: Technical feasibility of G4R configuration options and integration steps
- **Medium**: Data merging mechanism is technically sound but depends on Qualtrics' embedded data field behavior
- **Low**: Empirical evidence of adoption and impact on research reproducibility not yet presented

## Next Checks
1. **End-to-end pilot deployment**: Deploy G4R in an actual Qualtrics study with at least 10 participants; verify that all participant IDs are captured, message data is complete, and merging produces correct results without data loss.
2. **Parameter sensitivity test**: Systematically vary temperature (0.0, 0.5, 1.0, 1.5, 2.0) across participant groups in the same study; measure response variability and coherence to validate that G4R's temperature control works as specified.
3. **Embedded vs. new tab performance comparison**: Deploy identical interfaces in both modes across two participant groups; measure completion rates, data capture completeness, and any technical failures to determine which mode is more reliable in practice.