---
ver: rpa2
title: 'MM-THEBench: Do Reasoning MLLMs Think Reasonably?'
arxiv_id: '2601.22735'
source_url: https://arxiv.org/abs/2601.22735
tags:
- reasoning
- mllms
- arxiv
- step
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MM-THEBench, a comprehensive benchmark designed
  to evaluate hallucinations in the intermediate chain-of-thoughts (CoTs) of reasoning
  multimodal large language models (MLLMs). The authors address the lack of systematic
  tools to monitor and analyze hallucinations within these reasoning steps, which
  often remain overlooked in existing evaluations.
---

# MM-THEBench: Do Reasoning MLLMs Think Reasonably?
## Quick Facts
- arXiv ID: 2601.22735
- Source URL: https://arxiv.org/abs/2601.22735
- Reference count: 40
- **Primary result**: Introduces MM-THEBench to systematically evaluate hallucinations in intermediate Chain-of-Thought (CoT) reasoning steps of multimodal models, finding reasoning hallucinations more detrimental to final accuracy than perception errors.

## Executive Summary
This paper introduces MM-THEBench, a comprehensive benchmark designed to evaluate hallucinations in the intermediate chain-of-thoughts (CoTs) of reasoning multimodal large language models (MLLMs). The authors address the lack of systematic tools to monitor and analyze hallucinations within these reasoning steps, which often remain overlooked in existing evaluations. To tackle this, they propose a fine-grained hallucination taxonomy grounded in three cognitive dimensions—knowledge, perception, and reasoning—each further subdivided into subcategories. Alongside this taxonomy, they develop a multi-level automated evaluation framework that uses rubric-based scoring and LLM-as-a-judge to assess both the quality and hallucination patterns in reasoning steps. Experiments on 14 state-of-the-art reasoning MLLMs reveal that the correctness of intermediate CoTs lags behind final answer accuracy, with reasoning hallucinations having a stronger negative impact on final outputs than perception hallucinations. The results highlight the need for detailed monitoring of intermediate reasoning processes to guide the development of more reliable and capable reasoning MLLMs.

## Method Summary
The paper constructs MM-THEBench by curating 1,340 multimodal questions from 8 existing datasets and annotating them with atomic reasoning steps and detailed rubrics. These rubrics are organized under a three-level cognitive taxonomy: knowledge, perception, and reasoning. The evaluation framework employs an automated LLM-as-a-judge pipeline using Qwen-3-32B with specific decoding parameters (temperature=0.6, top_p=0.95, max_tokens=32768). The pipeline consists of four stages: answer extraction, step segmentation, step matching, and rubric scoring. This approach enables fine-grained analysis of hallucinations at the step level, providing insights beyond final answer accuracy.

## Key Results
- Intermediate CoT correctness (78.3%) lags significantly behind final answer accuracy (92.3%).
- Reasoning hallucinations have a stronger negative impact on final outputs (81.4% accuracy) compared to perception hallucinations (90.8% accuracy).
- The automated LLM-as-a-judge evaluation achieves high correlation (0.74-0.95) with human annotations.

## Why This Works (Mechanism)
The framework works by decomposing multimodal reasoning into fine-grained steps with detailed rubrics, allowing systematic identification of hallucination types. The three-level cognitive taxonomy provides a structured approach to categorizing errors, while the LLM-as-a-judge automation enables scalable evaluation across multiple models. The ground-truth annotation process ensures evaluation quality, and the multi-stage pipeline captures both semantic understanding and step-level accuracy.

## Foundational Learning
- **Multimodal reasoning**: Understanding how MLLMs process both visual and textual information in reasoning tasks.
  - Why needed: Essential for interpreting hallucination patterns in multimodal contexts.
  - Quick check: Can identify knowledge vs perception vs reasoning errors in a given reasoning trace.
- **Chain-of-Thought (CoT) evaluation**: Techniques for analyzing intermediate reasoning steps rather than just final outputs.
  - Why needed: Reveals where models fail during the reasoning process.
  - Quick check: Can segment a reasoning trace into individual atomic steps.
- **LLM-as-a-judge methodology**: Using LLMs to evaluate other models' outputs with rubric-based scoring.
  - Why needed: Enables scalable, consistent evaluation of complex multimodal reasoning.
  - Quick check: Can implement the four-stage evaluation pipeline with proper prompt engineering.
- **Hallucination taxonomy**: Categorizing errors into knowledge, perception, and reasoning subcategories.
  - Why needed: Provides structured understanding of failure modes.
  - Quick check: Can classify a given error according to the three-level taxonomy.
- **Rubric-based evaluation**: Creating detailed scoring criteria for step-by-step assessment.
  - Why needed: Enables fine-grained measurement beyond binary correctness.
  - Quick check: Can design rubrics that capture both positive evidence and hallucination indicators.
- **Multimodal dataset curation**: Selecting and annotating datasets suitable for reasoning evaluation.
  - Why needed: Ensures evaluation covers diverse reasoning challenges.
  - Quick check: Can identify appropriate datasets for different reasoning modalities.

## Architecture Onboarding

### Component Map
Data Sources (8 datasets) -> Question Collection (1,340) -> Step Annotation -> Rubric Generation -> LLM-as-Judge (Qwen-3-32B) -> Evaluation Pipeline (4 stages) -> Results Aggregation

### Critical Path
Dataset curation → Ground truth annotation → Rubric generation → LLM evaluation → Result analysis

### Design Tradeoffs
- **Single vs multi-judge**: Uses one judge model for consistency but may miss complementary perspectives
- **Automated vs human evaluation**: Balances scalability with evaluation fidelity
- **Fine-grained vs coarse evaluation**: Provides detailed insights but requires more complex annotation

### Failure Signatures
- JSON parsing errors from judge model output
- Context window overflow with long CoTs
- Incorrect rubric-step matching
- Inconsistent ground truth annotation

### Exactly 3 First Experiments
1. Run the complete 4-stage evaluation pipeline on a small subset of 10-20 questions to verify implementation
2. Test the rubric generation process with sample reasoning steps to ensure proper categorization
3. Validate the context window handling by processing an intentionally long CoT response

## Open Questions the Paper Calls Out
- **Open Question 1**: Can a multi-judge evaluation framework combining LLMs (for reasoning-focused evaluation) and MLLMs (for perception-sensitive evaluation) significantly reduce bias compared to a single-judge approach?
- **Open Question 2**: How do hallucination patterns in intermediate CoTs differ for interactive tasks (e.g., GUI navigation, agent-based tool calling) compared to the QA-centric tasks currently evaluated?
- **Open Question 3**: Can targeted interventions specifically reduce reasoning hallucinations (which show stronger correlation with incorrect final answers) without degrading beneficial reasoning behaviors?

## Limitations
- The framework relies on a single LLM-as-a-judge, which may introduce evaluation bias
- Limited to QA paradigm tasks, not covering interactive or embodied multimodal reasoning
- Requires detailed ground truth annotation, making dataset construction resource-intensive

## Confidence
- **High Confidence**: Intermediate CoT correctness lags behind final answer accuracy; reasoning hallucinations have stronger negative impact than perception errors
- **Medium Confidence**: The novel hallucination taxonomy provides structured error categorization; existing evaluations overlook intermediate step quality
- **Low Confidence**: Universal need for detailed intermediate CoT monitoring across all reasoning MLLM applications

## Next Checks
1. Reconstruct the rubric generation pipeline from the procedural description and validate generated rubrics
2. Execute the complete LLM-as-a-judge pipeline on a small controlled dataset with known ground truth
3. Test context window handling by processing a deliberately lengthy multimodal response through the pipeline