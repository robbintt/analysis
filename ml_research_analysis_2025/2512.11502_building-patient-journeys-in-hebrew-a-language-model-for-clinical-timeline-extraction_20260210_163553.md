---
ver: rpa2
title: 'Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline
  Extraction'
arxiv_id: '2512.11502'
source_url: https://arxiv.org/abs/2512.11502
tags:
- language
- medical
- hebrew
- clinical
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HeMed, the first Hebrew medical language
  model designed for extracting structured clinical timelines from electronic health
  records (EHRs). Built on DictaBERT 2.0 and continuously pre-trained on over five
  million de-identified Hebrew clinical notes, the model addresses the scarcity of
  Hebrew medical NLP resources.
---

# Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction

## Quick Facts
- arXiv ID: 2512.11502
- Source URL: https://arxiv.org/abs/2512.11502
- Reference count: 10
- Primary result: HeMed, a Hebrew medical language model, achieves strong performance on temporal relation classification across clinical domains using continual pre-training on de-identified EHRs.

## Executive Summary
This paper introduces HeMed, the first Hebrew medical language model designed for extracting structured clinical timelines from electronic health records (EHRs). Built on DictaBERT 2.0 and continuously pre-trained on over five million de-identified Hebrew clinical notes, the model addresses the scarcity of Hebrew medical NLP resources. Two novel datasets—Med-TRC and Onc-TRC—were manually annotated for temporal relation classification (TRC) across different clinical domains. Experiments show that continual pre-training improves downstream performance, with vocabulary adaptation enhancing token efficiency. Importantly, de-identification does not compromise model effectiveness, supporting privacy-conscious development. The model achieves strong results on both datasets, demonstrating its ability to structure patient journeys and support time-aware clinical NLP applications in a low-resource setting.

## Method Summary
HeMed is developed through a two-stage pipeline: (1) continual MLM pre-training on a large corpus of de-identified Hebrew clinical notes using a vocabulary adapted from DictaBERT 2.0, and (2) fine-tuning with the ESS (event start state) architecture for temporal relation classification. The model leverages a de-identification process based on metadata-informed regex with smart replacements, ensuring patient privacy without sacrificing performance. Two novel datasets, Med-TRC and Onc-TRC, are manually annotated for pairwise temporal relation classification across clinical domains. Vocabulary adaptation is explored via Simple (union-based) and AdaLM (iterative) methods, improving token efficiency and compression rates. The model is evaluated using weighted F1 and relaxed F1 metrics, demonstrating its effectiveness in structuring patient journeys.

## Key Results
- HeMed outperforms DictaBERT 2.0 on both Med-TRC and Onc-TRC datasets, validating the benefits of continual pre-training.
- Vocabulary adaptation (Simple and AdaLM) enhances token efficiency and improves downstream performance.
- De-identification does not compromise model effectiveness, supporting privacy-conscious development.
- Strong results on both datasets demonstrate HeMed’s ability to structure patient journeys and support time-aware clinical NLP applications.

## Why This Works (Mechanism)
HeMed’s effectiveness stems from continual pre-training on domain-specific clinical notes, which adapts the model to medical terminology and context. Vocabulary adaptation ensures efficient tokenization of domain-specific terms, reducing context overflow and improving compression rates. The ESS architecture is specifically designed for temporal relation classification, enabling accurate extraction of clinical timelines. De-identification preserves privacy while maintaining the semantic integrity of the data, allowing the model to learn from sensitive clinical notes without exposing patient information.

## Foundational Learning
- **Continual Pre-training**: Fine-tuning a pre-trained model on domain-specific data to adapt it to new contexts. *Why needed*: Standard pre-trained models lack domain-specific knowledge. *Quick check*: Compare performance on domain-specific tasks before and after continual pre-training.
- **Temporal Relation Classification (TRC)**: Classifying the temporal relationship between pairs of events (e.g., before, after, equal, vague). *Why needed*: Essential for constructing clinical timelines and understanding patient journeys. *Quick check*: Verify class distribution and ensure balanced evaluation.
- **Vocabulary Adaptation**: Expanding the tokenizer’s vocabulary to include domain-specific terms. *Why needed*: Improves tokenization efficiency and reduces context overflow. *Quick check*: Measure compression rate and token count before and after adaptation.
- **De-identification**: Removing or masking identifiable patient information from clinical notes. *Why needed*: Ensures privacy compliance while retaining data utility. *Quick check*: Spot-check de-identified outputs for retained medical content.
- **ESS Architecture**: A neural architecture designed for temporal relation classification. *Why needed*: Optimized for extracting structured clinical timelines. *Quick check*: Verify implementation matches the original paper.
- **Class Imbalance Handling**: Techniques like clipping the majority class to prevent skewed evaluation. *Why needed*: Ensures fair assessment of model performance. *Quick check*: Compare per-class F1 scores and ensure majority class is clipped.

## Architecture Onboarding
- **Component Map**: De-identified EMR Corpus -> Continually Pre-trained HeMed Model -> ESS Architecture -> TRC Fine-tuning -> Med-TRC/Onc-TRC Datasets
- **Critical Path**: De-identification -> Continually Pre-trained HeMed Model -> ESS Architecture -> TRC Fine-tuning
- **Design Tradeoffs**: Continual pre-training vs. computational cost; vocabulary adaptation vs. tokenizer complexity; de-identification vs. data utility.
- **Failure Signatures**: Poor compression rates indicate inadequate vocabulary adaptation; low TRC F1 suggests insufficient domain adaptation or class imbalance issues.
- **First Experiments**:
  1. Train HeMed on a small subset of de-identified clinical notes and evaluate TRC F1 on Med-TRC.
  2. Compare Simple vs. AdaLM vocabulary adaptation on token efficiency and downstream performance.
  3. Test de-identification’s impact on TRC performance by training with and without de-identification.

## Open Questions the Paper Calls Out
None

## Limitations
- The 5M-note Hebrew EMR corpus and TRC datasets are not publicly available, limiting exact reproduction.
- De-identification methodology is described but lacks detailed implementation specifics.
- Vocabulary adaptation strategies (Simple vs. AdaLM) are not fully isolated to determine their individual contributions.
- The ESS architecture details are referenced but not fully specified in this paper.

## Confidence
- **High confidence** in the claim that continual pre-training improves TRC performance, supported by clear experimental comparisons.
- **Medium confidence** in the de-identification claim, as the approach is reasonable but lacks quantitative validation.
- **Medium confidence** in the superiority of vocabulary adaptation, as the paper shows improvement but does not isolate the contributions of Simple vs. AdaLM.

## Next Checks
1. Obtain or approximate a Hebrew clinical corpus of comparable size and de-identify it using metadata-informed regex to replicate preprocessing.
2. If the HeMed checkpoint is unavailable, pre-train a Hebrew tokenizer with WordPiece on 1M sampled notes and merge into DictaBERT via Simple or AdaLM; measure CTC and compression rate.
3. Reimplement the ESS architecture from Yanko et al. (2023) and fine-tune on synthetic or proxy Hebrew clinical data to verify the impact of continual pre-training on TRC F1 scores.