---
ver: rpa2
title: 'CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable
  Robot Learning'
arxiv_id: '2505.17006'
source_url: https://arxiv.org/abs/2505.17006
tags:
- motion
- arxiv
- learning
- latent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMo, a method to learn continuous latent
  motion representations from internet videos for scalable robot learning. The key
  challenge addressed is that existing discrete latent action methods suffer from
  information loss and struggle with complex, fine-grained dynamics.
---

# CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning

## Quick Facts
- **arXiv ID:** 2505.17006
- **Source URL:** https://arxiv.org/abs/2505.17006
- **Reference count:** 40
- **Primary result:** Continuous latent motion learned from action-less internet videos enables zero-shot pseudo-action generation for scalable robot policy learning

## Executive Summary
This paper introduces CoMo, a method to learn continuous latent motion representations from internet videos for scalable robot learning. The key challenge addressed is that existing discrete latent action methods suffer from information loss and struggle with complex, fine-grained dynamics. CoMo uses an early temporal feature difference mechanism to prevent model collapse and suppress static appearance noise, effectively avoiding shortcut learning. Guided by the information bottleneck principle, it constrains the latent motion embedding dimensionality to balance action-relevant information and background noise. Critically, CoMo exhibits strong zero-shot generalization, generating continuous pseudo actions for previously unseen video domains. This enables unified policy learning using pseudo actions from various action-less video datasets combined with limited labeled robot data.

## Method Summary
CoMo learns continuous latent motion representations from action-less videos using a Motion-Enhanced Inverse Dynamics Model (ME-IDM) that predicts latent motion embeddings from current and future frames. The method computes early temporal feature differences (current-future frame features) before encoding, preventing model collapse by removing direct future frame access. The learned continuous embeddings serve as pseudo actions for video data, enabling joint training with real robot actions in a unified policy architecture. The approach balances action-relevant information preservation against background noise injection through dimensionality constraints informed by information bottleneck theory.

## Key Results
- CoMo achieves 80.8% success rate on LIBERO with only 1× robot data versus 89.2% with 5× robot data
- Linear probing MSE (LP-MSE) shows effective action prediction capability from latent motion embeddings
- S-PCFC metric demonstrates high similarity between past-to-current and future-to-current motion embeddings, validating temporal consistency
- Strong zero-shot generalization enables pseudo action generation for unseen video domains

## Why This Works (Mechanism)

### Mechanism 1: Early Temporal Feature Difference Prevents Model Collapse
Removing future frame features from encoder input and using only current frame features plus feature differences suppresses static appearance shortcuts that cause continuous latent motion models to collapse into future-frame predictors. By computing temporal feature difference Dt = Ft+n - Ft early, then feeding only [Ft, Dt] to the encoder, the model cannot access raw future appearance and must encode motion dynamics to enable future frame reconstruction in the decoder.

### Mechanism 2: Information Bottleneck Constrains Dimensionality Trade-off
Constraining latent motion embedding dimensionality (paper uses 128) balances action-relevant information preservation against action-irrelevant noise injection. Following Information Bottleneck theory, higher-dimensional embeddings have capacity to encode background appearance along with motion. This noise degrades policy learning when the policy must jointly predict low-dimensional robot actions and latent motion. Lower dimensions force compression, but too few lose fine-grained motion detail.

### Mechanism 3: Continuous Latent Space Enables Unified Policy Co-training
Continuous pseudo actions from CoMo share distributional structure with continuous robot actions, enabling seamless joint training in a single policy architecture without multi-stage pretraining or discrete-to-continuous mapping. Discrete latent actions create a categorical distribution that requires separate handling from continuous robot action spaces—often necessitating multi-stage training or planning-then-action pipelines. CoMo's continuous embeddings allow a diffusion policy to denoise both modalities in a shared continuous space.

## Foundational Learning

- **Concept: Inverse Dynamics Modeling**
  - **Why needed here:** CoMo's Motion-Enhanced Inverse Dynamics Model (ME-IDM) predicts the latent motion z that would cause transition from frame Ot to Ot+n. This is the core self-supervised learning objective.
  - **Quick check question:** Given state s_t and s_{t+1}, can you define the inverse dynamics problem and contrast it with forward dynamics (predicting s_{t+1} from s_t and action a)?

- **Concept: Information Bottleneck Principle**
  - **Why needed here:** The paper explicitly uses this to justify embedding dimensionality constraints. The principle trades off compression (removing irrelevant noise) against preservation (keeping task-relevant information).
  - **Quick check question:** Given a representation Z that must predict target Y from input X, can you explain why minimizing I(X;Z) while maximizing I(Z;Y) might improve generalization?

- **Concept: Diffusion Policy / Autoregressive Policy Architectures**
  - **Why needed here:** CoMo evaluates on both architectures. Diffusion policies denoise action sequences iteratively; autoregressive policies predict actions token-by-token. Understanding conditioning mechanisms is critical for implementing joint training.
  - **Quick check question:** In a diffusion policy, how are observation and language conditions typically injected into the denoising network? How does this differ from autoregressive policy conditioning?

## Architecture Onboarding

- **Component map:** MAE-pretrained ViT-Large -> Feature Extractor -> Early Temporal Difference -> Motion Q-former -> FDM Decoder -> Motion Embedding Zt
- **Critical path:** Pre-train ME-IDM + FDM on large-scale action-less video datasets using reconstruction + perceptual loss. Freeze ME-IDM; extract pseudo action labels Zt for all video frames in target domain. Train unified policy on combined dataset: robot trajectories with ground-truth actions + video trajectories with pseudo actions Zt.
- **Design tradeoffs:**
  - **Embedding dimension (128 vs. higher):** Paper shows 128 optimal; higher dimensions increase S-PCFC (more background noise) without LP-MSE gains. Lower dimensions may lose fine motion.
  - **RGB difference vs. Feature difference:** RGB difference better for in-domain data (low-level motion signals); feature difference better for out-of-domain internet video (preserves abstract motion patterns).
  - **Frame interval:** Varies by dataset (10 for LIBERO/SAM-V/EgoVid, 5 for CALVIN, 20 for Droid) to normalize motion magnitude across frame rates.
- **Failure signatures:**
  - **Model collapse:** S-PCFC ≈ 1.0 indicates encoder is passing future frame features directly; check if Dt computation is implemented correctly and future features Ft+n are removed from encoder input.
  - **Poor transfer to robot domain:** High LP-MSE (>2.0) on held-out robot data indicates latent motion lacks action-relevant information; consider increasing embedding dimension or adding temporal supervision.
  - **Policy degradation with co-training:** Success rate drops when adding video data; check that pseudo action distribution aligns with robot action distribution (visualize t-SNE of z vs. a embeddings).
- **First 3 experiments:**
  1. **Reproduce S-PCFC baseline:** Implement O2-Fea variant (future frame features as latent motion). Confirm S-PCFC ≈ 1.0 and LP-MSE > 2.5. This validates your metric implementation and establishes the shortcut learning baseline.
  2. **Ablate embedding dimension:** Train CoMo with dimensions [32, 64, 128, 256] on LIBERO. Plot LP-MSE vs. S-PCFC vs. success rate. Confirm 128 shows best balance. This validates the information bottleneck claim.
  3. **Zero-shot pseudo action extraction:** Train CoMo on SAM-V + EgoVid + Droid only (no robot data). Extract pseudo actions for LIBERO or CALVIN videos. Train diffusion policy on pseudo actions only (no robot data). Measure LP-MSE to ground-truth robot actions. This probes cross-domain generalization capacity.

## Open Questions the Paper Calls Out

- **Question:** How can the performance gap between policies trained with CoMo pseudo-actions versus those trained with abundant ground-truth robot data be effectively closed?
  - **Basis in paper:** Section 6 notes that despite improvements, "there is still a gap between the latent motion and actual robot action," referencing Table 2 where 5× robot data outperforms CoMo (89.2% vs. 80.8%).
  - **Why unresolved:** The unsupervised nature of the pseudo-action generation may fail to capture the full kinematic fidelity of labeled robot actions.
  - **What evidence would resolve it:** A method achieving statistical parity with the "5× robot data" baseline without requiring additional labeled trajectories.

- **Question:** What forms of extra temporal supervision can be incorporated to improve the temporal sensitivity and skill-centricity of the learned motion representations?
  - **Basis in paper:** The authors state in Section 6 that "incorporating extra temporal supervision to obtain more temporally sensitive, skill-centric motion representations" is a direction for future work.
  - **Why unresolved:** The current method relies on visual reconstruction losses (pixel/perceptual), which do not explicitly enforce consistency with physical temporal dynamics or skill boundaries.
  - **What evidence would resolve it:** Demonstrating that adding explicit temporal losses (e.g., velocity, flow) improves the linear probing MSE and policy success rates on long-horizon tasks.

- **Question:** Can the latent motion embedding dimensionality be adaptively scaled to match task complexity without introducing action-agnostic noise?
  - **Basis in paper:** Finding 4 discusses the trade-off in dimensionality scaling (Fig. 3a), noting that increasing dimensions improves detail but also introduces static noise (increasing S-PCFC), forcing a fixed dimension of 128.
  - **Why unresolved:** A fixed dimension represents a compromise; complex tasks may require higher capacity, but the current architecture lacks a mechanism to increase capacity without noise.
  - **What evidence would resolve it:** A variable-dimension model that maintains low S-PCFC while scaling up dimensions for difficult tasks, leading to higher success rates than the fixed-dimension baseline.

## Limitations

- The evaluation focuses on simulated and real-world manipulation tasks with limited diversity in object types and scene complexity
- Zero-shot generalization claims rely on quantitative metrics (LP-MSE, S-PCFC) that are proxies for action relevance rather than direct task performance
- The 128-dimensional bottleneck is tuned for specific datasets and tasks studied, but may not generalize to more complex manipulation scenarios
- Ablation studies focus on dimensionality and input preprocessing choices but do not explore architectural alternatives or pretraining strategies

## Confidence

- **High Confidence:** The core mechanism of using early temporal feature differences to prevent model collapse is well-supported by the experimental comparison with O2-Fea baseline (S-PCFC ≈ 1.0 indicates collapse)
- **Medium Confidence:** The unified policy co-training approach shows promising results, but the evaluation relies on a limited set of tasks and does not compare against alternative multi-stage training approaches in detail
- **Low Confidence:** The optimal dimensionality of 128 is presented as a finding, but the analysis shows relatively stable LP-MSE for dimensions 128-256 while S-PCFC improves

## Next Checks

1. **Cross-Domain Generalization Test:** Train CoMo on internet videos only (SAM-V + EgoVid + Droid), extract pseudo actions for CALVIN or LIBERO videos, and evaluate LP-MSE against ground truth robot actions. This directly tests the zero-shot transfer capability claimed in the paper.

2. **Dimensionality Scaling with Task Complexity:** Systematically vary embedding dimension [32, 64, 128, 256, 512] and evaluate policy success rates on increasingly complex manipulation tasks (e.g., long-horizon pick-and-place with multiple objects). This would validate whether 128 remains optimal for more challenging scenarios.

3. **Ablation of Motion Query Design:** Replace the learnable motion queries in the Motion Q-former with standard positional embeddings or no queries, and measure the impact on LP-MSE and S-PCFC. This tests whether the specific query mechanism is critical to CoMo's performance or if simpler attention mechanisms suffice.