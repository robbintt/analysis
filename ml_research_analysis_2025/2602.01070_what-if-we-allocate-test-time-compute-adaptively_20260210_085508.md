---
ver: rpa2
title: What If We Allocate Test-Time Compute Adaptively?
arxiv_id: '2602.01070'
source_url: https://arxiv.org/abs/2602.01070
tags:
- reasoning
- compute
- problem
- selection
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adaptive test-time compute framework for
  mathematical reasoning that dynamically allocates inference-time computation by
  selecting tools and strategies per problem, guided by a process reward model (PRM)
  that evaluates intermediate reasoning steps. The agent generates multiple reasoning
  trajectories across iterations, using PRM scores to guide step-level selection and
  final answer choice.
---

# What If We Allocate Test-Time Compute Adaptively?

## Quick Facts
- arXiv ID: 2602.01070
- Source URL: https://arxiv.org/abs/2602.01070
- Authors: Ahsan Bilal; Ahmed Mohsin; Muhammad Umer; Ali Subhan; Hassan Rizwan; Ayesha Mohsin; Dean Hougen
- Reference count: 40
- One-line primary result: Adaptive test-time compute allocation improves mathematical reasoning accuracy while reducing FLOPs vs. uniform scaling

## Executive Summary
This paper introduces an adaptive test-time compute framework for mathematical reasoning that dynamically allocates inference-time computation by selecting tools and strategies per problem, guided by a process reward model (PRM) that evaluates intermediate reasoning steps. The agent generates multiple reasoning trajectories across iterations, using PRM scores to guide step-level selection and final answer choice. Across MATH-500, AIME24, and AMO-Bench, the approach significantly improves accuracy over uniform test-time scaling—e.g., from 43.8% to 65.4% on MATH-500 for Llama-3.1-8B-Instruct and from 71.2% to 81.4% for Qwen-2.5-7B-Instruct—while achieving better compute efficiency by concentrating resources on high-utility reasoning paths rather than uniform search expansion.

## Method Summary
The framework implements a four-stage agent pipeline: planning (AP) to generate reasoning plans, tool selection (AT) to choose from CoT, self-reflection, verifiers, reframer, and summarizer, compute selection (AC) to pick between Best-of-N, beam search, and lookahead strategies with exploration parameters, and answer extraction (AF). For each problem, K=10 independent iterations generate trajectories with PRM-guided pruning at step boundaries, then select the final answer from the highest-R_mean trajectory. The PRM evaluates step transitions and assigns validity scores used for both intra-trajectory pruning and inter-trajectory selection. No training/finetuning is required for the heuristic controller; base models are Llama-3.1-8B-Instruct or Qwen-2.5-7B-Instruct with standard decoding parameters.

## Key Results
- Dynamic+PRM improves MATH-500 accuracy from 43.8% to 65.4% (Llama-3.1-8B-Instruct) and 71.2% to 81.4% (Qwen-2.5-7B-Instruct)
- Achieves better compute efficiency via lower SCI scores by concentrating FLOPs on high-utility trajectories
- Fixed configurations reach ~81% accuracy, suggesting adaptivity's advantage is efficiency, not capability ceiling
- PRM-guided selection alone shows limited gains without adaptive tool/strategy selection

## Why This Works (Mechanism)

### Mechanism 1: PRM-Guided Intra-Trajectory Pruning
Step-level PRM scores evaluate each transition (s_{t-1} → s_t) and assign validity scores v_t ∈ [0,1]. During beam search or lookahead, candidates are ranked by mean accumulated scores R(τ_{1:t}) = (1/t)Σv_u, and low-scoring branches are pruned at step boundaries. Core assumption: PRM correctness signals at individual steps correlate with overall trajectory quality and final answer correctness. Break condition: When PRM is miscalibrated, pruning may discard correct branches; observed degradation on MATH-500 Level 5 supports this.

### Mechanism 2: Adaptive Per-Problem Configuration
A controller maps (problem, plan) → (tool subset T, compute strategy c, exploration parameter m). Easy problems get direct generation; hard problems get beam search with higher m. Core assumption: Problem characteristics can be reliably mapped to optimal tool-strategy combinations via heuristic rules. Break condition: Fixed ablations show some configurations reach ~81% on MATH-500, suggesting adaptivity's advantage is efficiency, not raw capability ceiling.

### Mechanism 3: Inter-Trajectory Selection via Aggregated Rewards
Generate K=10 trajectories with potentially different tool/strategy configurations. Score each completed trajectory by R_mean(τ) = (1/T)Σv_t. Select y* from argmax_i R(τ_i). Core assumption: Higher mean step validity correlates with correct final answers across diverse reasoning paths. Break condition: On hardest benchmarks (AIME24, AMO-Bench), no fixed configuration exceeds 10% accuracy, suggesting selection alone insufficient—trajectory diversity from multi-tool selection is essential.

## Foundational Learning

- Concept: Process Reward Models (PRMs)
  - Why needed here: The entire adaptive framework depends on PRM scores for both intra-trajectory pruning and inter-trajectory selection.
  - Quick check question: Given a reasoning step "x² = 4 → x = 2", would a PRM flag this as incomplete? (Answer: Yes—misses x = -2)

- Concept: Test-Time Compute Scaling Strategies (Best-of-N, Beam Search, Lookahead)
  - Why needed here: The compute selector must choose among these strategies based on problem structure; understanding their tradeoffs is essential.
  - Quick check question: When would Lookahead (small depth exploration) be preferred over Best-of-N (independent samples)? (Answer: When intermediate step quality is critical and errors compound)

- Concept: Compute Efficiency Metrics (FLOPs, Compute Intensity)
  - Why needed here: The paper's efficiency claims require understanding how F_theo and SCI capture raw compute vs. effective utilization.
  - Quick check question: Why can Dynamic+PRM have higher FLOPs but lower SCI than fixed configurations? (Answer: SCI penalizes wasted computation on low-utility trajectories; adaptivity concentrates compute effectively)

## Architecture Onboarding

- Component map: Problem x → Planning Agent A_P → Plan π → Tool Selector A_T → Tools T → Compute Selector A_C → Strategy c, Parameter m → Reasoning Execution → Trajectory τ (with PRM-guided pruning) → Answer Extractor A_F → Answer y (Repeat K=10 iterations; select final answer via argmax R_mean)

- Critical path: PRM scoring is invoked at every step transition. Any latency in PRM inference directly impacts total generation time.

- Design tradeoffs:
  - Higher K (iterations) → more exploration but linear FLOPs increases
  - Larger m (beam width/lookahead depth) → better pruning but more PRM calls per step
  - Controller can be heuristic (current) or learned (future)—trade-off between implementation complexity and optimality

- Failure signatures:
  - Level 5 MATH problems: PRM misranks trajectories → selection degrades
  - AIME24 with high exploration (p=10): Accuracy drops from 10% to 3.3% → excessive branching introduces noise
  - Missing numeric verifier on arithmetic-heavy problems: Errors propagate undetected

- First 3 experiments:
  1. **Ablate PRM guidance**: Compare Dynamic+PRM vs. Dynamic+random selection to isolate PRM contribution on MATH-500 Levels 1–5.
  2. **Fixed vs. adaptive compute budget**: Hold F_theo constant across fixed Best-of-N and adaptive configurations; measure accuracy/FLOPs ratio.
  3. **PRM calibration audit**: On a held-out set, compare PRM step scores against human annotations to quantify miscalibration rate and correlate with accuracy degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learned, budget-aware policies outperform the heuristic controller for tool selection, compute allocation, and stopping decisions under explicit constraints?
- Basis in paper: [explicit] "the controller is heuristic by design...this motivates future work on learned, budget-aware policies that optimize tool use, compute allocation, and stopping decisions under explicit constraints"
- Why unresolved: The current approach uses deterministic, role-conditioned prompt templates without any learned policy component; the authors isolate test-time effects but leave policy learning unexplored.
- What evidence would resolve it: Comparative experiments training a policy network to jointly select tools, strategies, and parameters under fixed compute or latency budgets, measuring accuracy vs. the heuristic baseline.

### Open Question 2
- Question: Does targeted supervision on hard instances or curriculum-based PRM training improve verification reliability on the most difficult problems (e.g., MATH-500 Level 5)?
- Basis in paper: [explicit] "its effectiveness diminishes on the hardest problems, where the PRM can misrank plausible but incorrect trajectories...This observed degradation motivates the need for difficulty-aware verification"
- Why unresolved: The paper uses a pre-trained PRM (Qwen2.5-Math-PRM-7B) without modification; the misranking issue on hard problems is documented but not addressed.
- What evidence would resolve it: Train or fine-tune PRMs with difficulty-stratified data and compare selection accuracy on Level 5 problems against the current PRM.

### Open Question 3
- Question: How does the adaptive framework perform on non-mathematical reasoning domains such as code generation, long-context QA, or safety-critical decision-making with domain-specific verifiers?
- Basis in paper: [explicit] "while results are strong for mathematical reasoning, generalization to other domains remains open and will likely require domain-adaptive verification signals and broader evaluation"
- Why unresolved: The paper focuses exclusively on mathematical reasoning benchmarks; the domain-agnostic claim is not empirically validated.
- What evidence would resolve it: Apply the framework to code benchmarks (e.g., HumanEval) with a code verifier and compare adaptive vs. uniform scaling.

### Open Question 4
- Question: Can latency-aware scheduling and batching strategies reduce wall-clock overhead from multi-branch search and verification without sacrificing accuracy gains?
- Basis in paper: [explicit] "multi-branch search and verification introduce latency and systems overhead, suggesting co-design with batching and latency-aware scheduling"
- Why unresolved: The paper reports hardware-agnostic compute metrics (FLOPs, SCI) but does not address real-time latency constraints or systems-level optimization.
- What evidence would resolve it: Implement latency-aware scheduling for parallel trajectory evaluation and measure accuracy vs. wall-clock time trade-offs.

## Limitations

- PRM miscalibration on hard problems (Level 5 MATH, AIME24) causes incorrect trajectory selection and degrades accuracy
- Controller uses deterministic heuristic prompts rather than learned decision-making, limiting potential performance gains
- Fixed configurations achieve similar accuracy levels (~81%) to adaptive approaches, suggesting adaptivity's primary advantage is efficiency rather than capability ceiling

## Confidence

**High Confidence**: Compute efficiency improvements (SCI metrics showing better utilization) and basic accuracy gains over uniform scaling baselines are well-supported by ablation studies and consistent across multiple benchmarks.

**Medium Confidence**: The claim that PRM-guided pruning improves accuracy requires careful interpretation—while the mechanism is sound, PRM miscalibration on harder problems suggests the correlation between step validity and final answer correctness is weaker than assumed, particularly for complex mathematical reasoning.

**Low Confidence**: The assertion that dynamic per-problem configuration selection is essential for top performance is weakened by evidence that some fixed configurations achieve similar accuracy levels, suggesting adaptivity may primarily optimize compute usage rather than unlock new capability frontiers.

## Next Checks

1. **PRM Calibration Audit**: On a held-out validation set with human-annotated step correctness, compare PRM step scores against ground truth to quantify miscalibration rates. Correlate miscalibration frequency with accuracy degradation on MATH-500 Levels 4-5 and AIME24 to establish whether PRM quality is the primary limiting factor.

2. **Fixed Budget Efficiency Test**: Hold total theoretical FLOPs constant across fixed Best-of-N configurations and the adaptive framework. Measure accuracy per unit FLOPs (efficiency ratio) to isolate whether adaptivity provides genuine efficiency gains or simply reallocates fixed computational budgets.

3. **Controller Generalization Test**: Replace the heuristic controller with a learned model trained on a subset of problems to predict optimal tool-strategy combinations. Compare accuracy and efficiency against the current prompt-based controller to determine if the performance gains stem from adaptivity itself or the specific heuristic rules implemented.