---
ver: rpa2
title: 'Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training'
arxiv_id: '2507.12507'
source_url: https://arxiv.org/abs/2507.12507
tags:
- training
- reasoning
- entropy
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates prolonged reinforcement learning for reasoning-focused
  language models, addressing key challenges such as entropy collapse, exploration-exploitation
  balance, and performance plateaus. The authors introduce several critical components:
  decoupled clipping and dynamic sampling from DAPO to maintain exploration, controlled
  KL regularization to stabilize training from a pretrained model, and periodic reference
  policy resets to overcome stagnation.'
---

# Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training

## Quick Facts
- arXiv ID: 2507.12507
- Source URL: https://arxiv.org/abs/2507.12507
- Reference count: 35
- Models trained for 100K+ steps achieve +14.7% math, +13.9% coding, +54.8% logic puzzles, +25.1% STEM, and +18.1% instruction following over strong baselines

## Executive Summary
This work investigates prolonged reinforcement learning for reasoning-focused language models, addressing key challenges such as entropy collapse, exploration-exploitation balance, and performance plateaus. The authors introduce several critical components: decoupled clipping and dynamic sampling from DAPO to maintain exploration, controlled KL regularization to stabilize training from a pretrained model, and periodic reference policy resets to overcome stagnation. Training on diverse verifiable reward tasks (math, coding, STEM, logic puzzles, instruction following), their approach yields substantial improvements across all domains compared to a strong baseline. The results demonstrate that even small-scale models can achieve strong reasoning performance through carefully designed RL techniques and prolonged training.

## Method Summary
The authors train reasoning-focused language models using a prolonged reinforcement learning pipeline built on Group Relative Policy Optimization (GRPO) with DAPO enhancements. Key components include decoupled clipping with asymmetric bounds (ε_low=0.2, ε_high=0.4), dynamic sampling that filters prompts with 0% or 100% accuracy, KL regularization (β=0.0001) with periodic reference policy resets, and high-temperature rollouts (T=1.2). The training uses 5 diverse verifiable reward datasets totaling 136K prompts across math, coding, STEM, logic puzzles, and instruction following. Models are trained in 8 stages with reference policy resets triggered by validation degradation or plateaus, starting from DeepSeek-R1-Distill-Qwen-1.5B and extending context from 8K to 16K tokens.

## Key Results
- Math reasoning: +14.7% improvement on AIME and AMC problems
- Coding: +13.9% improvement on Codeforces and LiveCodeBench
- Logic puzzles: +54.8% improvement on graph coloring and reasoning gym tasks
- STEM reasoning: +25.1% improvement on GPQA-diamond problems
- Instruction following: +18.1% improvement on IFEval strict tasks

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Clipping for Entropy Preservation
Decoupled clipping bounds prevent premature mode collapse during prolonged RL by allowing upward probability adjustments on low-probability tokens while still constraining downward updates. Standard PPO clipping uses symmetric bounds (1−ε, 1+ε). DAPO decouples these into ε_low and ε_high. Setting ε_high > ε_low (0.4 vs 0.2) enables "clip-higher" behavior—tokens with positive advantage can have their probabilities increased beyond what symmetric clipping allows, maintaining output diversity. Core assumption: exploration remains valuable even late in training; mode collapse is caused by overly aggressive probability suppression, not just insufficient temperature. Evidence: increasing the upper threshold to ε_high=0.4 led to further improvements, mitigating entropy collapse and yielding the highest validation performance overall.

### Mechanism 2: Dynamic Sampling for Signal Density
Filtering prompts where current policy achieves 0% or 100% accuracy improves sample efficiency by concentrating gradient updates on examples that provide learning signal. At each training step, evaluate current policy on prompt batch. Remove prompts with perfect success (no positive examples to learn from) or perfect failure (no gradient signal in advantage computation). This maintains intermediate difficulty distribution. Core assumption: the advantage estimator in GRPO (group-relative normalization) requires variance in outcomes within each batch to produce meaningful gradients. Evidence: dynamic sampling led to faster improvements than static sampling by filtering out prompts with zero advantage. This increases reward signal density in each batch.

### Mechanism 3: Reference Policy Reset with KL Regularization
A small KL penalty (β=0.0001) stabilizes training from strong pretrained models, but periodic reference policy resets prevent the KL term from eventually blocking further learning. KL regularization anchors the policy to a reference, preventing catastrophic drift. However, as the policy improves, the original reference becomes a ceiling. Hard-resetting π_ref to current π_θ (and clearing optimizer state) re-anchors the KL constraint around improved behavior while maintaining its stabilizing effect. Core assumption: the pretrained model already has useful reasoning patterns; the goal is refinement rather than exploration from scratch. Evidence: extending Run 1 causes a sudden degradation on Codeforces validation scores... We perform a hard reset of training, resetting the reference policy... as well as optimizer states.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO removes the critic model from PPO, estimating advantages from group reward statistics. Essential for understanding why dynamic sampling matters—advantages are normalized within each prompt group.
  - Quick check question: Given 16 responses to a prompt with rewards [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], what is the advantage of a response with reward 1?

- Concept: **Entropy Collapse in Language Models**
  - Why needed here: The paper explicitly identifies this as the core failure mode. Without understanding why entropy drops (and why it's bad), the clipping/temperature/KL interventions seem arbitrary.
  - Quick check question: If model entropy drops from 3.0 to 0.5 bits/token during training, what behaviors would you expect to see in rollouts?

- Concept: **Verifiable Rewards vs. Learned Reward Models**
  - Why needed here: The paper's entire training pipeline depends on programmatic verification. Without this, the reward hacking problem makes prolonged RL unreliable.
  - Quick check question: For a code generation task, what are three ways to construct a verifiable reward signal? What failure modes does each have?

## Architecture Onboarding

- Component map: Prompt Pool -> Dynamic Filter -> Rollout Engine -> Policy π_θ <- GRPO + KL + Decoupled Clip <- Sandbox Reward Servers <- Reference π_ref (Periodic hard reset)

- Critical path:
  1. Initialize from DeepSeek-R1-Distill-Qwen-1.5B (not raw base model—KL penalty requires strong starting point)
  2. Build sandboxed reward servers with multiprocess execution (code timeout: 5s, parallel evaluation)
  3. Implement GRPO with group size 16, decoupled clipping (0.2/0.4), KL coefficient 1e-4
  4. Add dynamic sampling filter and high-temperature rollouts (1.2)
  5. Monitor entropy, KL divergence, and validation scores—trigger reference reset on degradation

- Design tradeoffs:
  - **Higher temperature (1.2) vs. exploitation**: Paper shows 0.6 causes early instability, but 1.2 may be suboptimal for late-stage training—requires per-model tuning
  - **KL penalty presence vs. removal**: Recent work removes KL entirely; this paper argues it's necessary when starting from capable checkpoints. Assumption: your base model quality determines this choice
  - **Context window (8K → 16K)**: Extended late in training (Run 8). Marginal math gains, broader domain improvements. Consider memory/performance tradeoff

- Failure signatures:
  - **Entropy collapse**: Entropy drops below ~1.0, validation plateaus → increase ε_high or temperature
  - **KL spike + validation drop**: KL divergence suddenly increases while performance degrades → trigger reference reset immediately
  - **Response length explosion + no EOS**: Model loops without terminating → add reward shaping penalty for non-termination
  - **Format mismatch**: Model uses \boxed{} when expecting <answer> tags → easier to learn than core reasoning; continue training

- First 3 experiments:
  1. **Temperature ablation on single domain**: Train on math-only data with T∈{0.6, 1.0, 1.2} for 500 steps. Monitor entropy trajectory and AIME2024 scores. Replicate Figure 4 pattern before scaling to full pipeline.
  2. **Clipping coefficient sweep**: Fix temperature, test (ε_low, ε_high) ∈ {(0.1, 0.2), (0.2, 0.2), (0.2, 0.3), (0.2, 0.4)}. Measure entropy preservation vs. reward improvement. Validate Section 5.2 claim that 0.2/0.4 is optimal.
  3. **Reference reset timing**: Train for 2000 steps, then compare: (a) no reset, (b) reset at step 1000, (c) reset at first KL spike detection. Use Codeforces as early warning signal based on Figure 6. Establish operational trigger for your infrastructure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the finding that high sampling temperatures (e.g., 1.2) stabilize training and prevent entropy collapse generalize to larger language models or different architectures?
- Basis in paper: The authors state in Section 5.1: "We note that this observation may be model-dependent, and transferring it to other models may require further analysis."
- Why unresolved: The ablation studies were conducted exclusively on a specific 1.5B parameter model (DeepSeek-R1-Distill-Qwen-1.5B). It is unclear if larger models with more stable initial distributions would still benefit from such high temperatures or if this is a necessity specific to smaller capacity models.
- What evidence would resolve it: Conducting the same temperature ablation studies on larger variants of the model (e.g., 7B or 70B) and analyzing the correlation between temperature, entropy maintenance, and final validation scores.

### Open Question 2
- Question: Can "cold start" limitations in complex domains (e.g., ARC, games) be resolved through RL alone, or is specific background knowledge injection via finetuning a prerequisite?
- Basis in paper: Section 4.4 notes that failures in tasks like ARC and games stem from a "lack of core reasoning skills" or "insufficient background knowledge," and states they leave "these enhancements to future work."
- Why unresolved: The current study shows that while formatting issues are easily learned via RL, deeper reasoning failures in specific domains persisted. The paper does not determine if prolonged RL can eventually overcome the lack of domain-specific pre-existing knowledge.
- What evidence would resolve it: An experiment comparing the current approach against a setup where the model is first finetuned on domain-specific background data (e.g., game rules or ARC patterns) before undergoing prolonged RL.

### Open Question 3
- Question: Is there a principled, automated heuristic for determining the optimal timing for reference policy resets to replace manual monitoring?
- Basis in paper: The paper describes the reset strategy in Section 3.3.1 as occurring "periodically" or "especially when validation metrics significantly degrade," implying a manual or reactive intervention.
- Why unresolved: Reliance on manual monitoring of validation spikes or KL divergence is not scalable or robust. The paper does not offer a theoretical or automated framework for predicting the exact moment a reset maximizes the "unlocks" of performance gains.
- What evidence would resolve it: A study defining and testing automated triggers for resets (e.g., based on the second derivative of KL divergence or gradient norms) and comparing their efficiency and final performance against the manual schedule used in the paper.

## Limitations
- The exact timing and quantitative triggers for reference policy resets remain unspecified, making it difficult to reproduce the claimed benefits systematically
- The evaluation benchmarks, while diverse, lack systematic analysis of whether improvements generalize to truly out-of-distribution reasoning tasks or merely represent overfitting to specific verification protocols
- The paper assumes verifiable reward signals are available, which limits applicability to domains where such signals cannot be programmatically constructed

## Confidence
- **High confidence**: The core empirical finding that prolonged RL with the proposed DAPO enhancements yields measurable improvements across all tested reasoning domains (+14.7% math, +13.9% coding, +54.8% logic puzzles, +25.1% STEM, +18.1% instruction following)
- **Medium confidence**: The mechanism explanations for why specific components work (entropy preservation through decoupled clipping, signal density through dynamic sampling, stability through KL regularization with periodic resets)
- **Low confidence**: The generalizability claims beyond the specific verifiable reward tasks used

## Next Checks
1. **Independent replication of core components**: Implement the decoupled clipping (ε_low=0.2, ε_high=0.4) and dynamic sampling mechanisms on a single reasoning domain (e.g., math) with a different base model and dataset. Compare entropy trajectories and performance improvements against standard PPO to validate the claimed mechanisms.

2. **Component isolation experiment**: Design an ablation study that systematically removes each DAPO enhancement (dynamic sampling, decoupled clipping, KL regularization, reference resets) while keeping others constant. This would quantify the marginal contribution of each component rather than treating them as a unified package.

3. **Cross-domain generalization test**: After training on the full diverse task set, evaluate performance on reasoning problems from domains not included in training (e.g., medical diagnosis, legal reasoning, or novel puzzle types). This would test whether the improvements represent genuine reasoning capability enhancement versus task-specific optimization.