---
ver: rpa2
title: TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource
  Languages
arxiv_id: '2506.05057'
source_url: https://arxiv.org/abs/2506.05057
tags:
- tall
- language
- low-resource
- languages
- hebrew
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TALL, a trainable architecture that enhances
  LLM performance in low-resource languages by integrating translation models and
  dimension alignment adapters. TALL transforms low-resource inputs into high-resource
  representations while preserving linguistic features.
---

# TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages

## Quick Facts
- arXiv ID: 2506.05057
- Source URL: https://arxiv.org/abs/2506.05057
- Reference count: 6
- Primary result: Achieves 5.59% accuracy on Hebrew missing word prediction vs 2.93% for next best baseline

## Executive Summary
TALL introduces a trainable architecture that enhances large language model (LLM) performance on low-resource languages by integrating translation models and dimension alignment adapters. The system transforms low-resource inputs into high-resource representations while preserving linguistic features through dimension alignment layers and custom transformers. Experiments on Hebrew demonstrate significant improvements over baselines, with TALL achieving up to 5.59% accuracy in predicting missing words compared to 2.93% for the next best method. The approach is parameter-efficient, training only 14-15% of parameters while freezing pre-trained components.

## Method Summary
TALL uses frozen Marian MT models for translation and a frozen LLM, with trainable dimension alignment adapters and custom transformer modules. The architecture processes low-resource language input through a translation pipeline, aligns dimensions between the translation model and LLM embeddings using two-layer MLPs with layer normalization and GELU activations, then uses cross-attention in custom transformers to preserve source features. Training employs teacher forcing with loss computed only on the final missing token position, while inference uses temperature sampling with T=0.7, top-k=50, and top-p=0.95.

## Key Results
- TALL achieves 5.59% accuracy on missing word prediction vs 2.93% for next best baseline (MaChIAto)
- Parameter-efficient design trains only 14-15% of parameters while freezing 86%
- Outperforms fine-tuning baseline (0.16% accuracy) and direct LLM use (0.63% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Representation Alignment via Adapter Projections
- The architecture uses trainable two-layer MLP adapters with layer normalization and GELU activations to project between incompatible embedding spaces (Marian MT's 1024 dimensions to LLM's 1024 or 896 dimensions).
- This creates a "bridge" allowing semantic content to flow across languages while maintaining task-relevant features.

### Mechanism 2: Source Feature Preservation via Cross-Attention
- Custom transformers receive LLM-tokenized translated sentence embeddings as queries but cross-attend to dimension-aligned low-resource encoder states as keys/values.
- This ensures predictions incorporate linguistic patterns from the original input that pure translation might distort or lose.

### Mechanism 3: Parameter-Efficient Frozen-Component Transfer
- Translation models (Marian MT) and the main LLM remain frozen (~86% of parameters), with only dimension alignment adapters and custom transformers training (~14-15%).
- This preserves pre-trained knowledge while learning the cross-lingual mapping function.

## Foundational Learning

- **Cross-Attention in Sequence-to-Sequence Models**: TALL's custom transformers use cross-attention to fuse information from translated high-resource tokens (queries) and original low-resource encoder states (keys/values). Quick check: In TALL's LR-HR custom transformer, which representation serves as the Query and which serves as Key/Value for cross-attention?

- **Adapter-Based Parameter-Efficient Transfer**: TALL builds on the adapter paradigm (Houlsby et al., 2019) by inserting small trainable modules into frozen models rather than fine-tuning entire networks. Quick check: Why does the paper show fine-tuning degrades performance (0.16%) compared to direct use (0.63%), and how does adapter training avoid this?

- **Teacher Forcing with Token-Focused Loss**: TALL uses teacher forcing during training but computes loss only on the final missing token. Quick check: What are the tradeoffs of computing loss only on the final token versus all tokens, and why does the architecture require this constraint?

## Architecture Onboarding

- **Component map**:
  LR-HR Encoder (frozen, Marian MT) → Dim Alignment Adapter 1 → Custom Transformer LR-HR → Main LLM (frozen, bloomz-560m or Qwen) → Dim Alignment Adapter 2 → Custom Transformer HR-LR → HR-LR Decoder (frozen, Marian MT) → Output: Predicted Hebrew token

- **Critical path**:
  1. Verify Marian MT models exist for your language pair
  2. Identify dimensional mismatches: encoder (1024), LLM (1024 bloomz / 896 Qwen), decoder (512)
  3. Design adapter MLPs to bridge each gap (see Tables 3-4 for exact configs)
  4. Implement cross-attention in custom transformers
  5. Configure training to compute loss only on final token position

- **Design tradeoffs**:
  - Single-token loss vs. multi-token: Architecture constrains loss to final token only—limits gradient signal but simplifies cross-lingual alignment
  - Inference speed vs. accuracy: Multi-stage pipeline with cross-attention breaks standard KV caching—slower but more accurate
  - Adapter size vs. efficiency: Larger MLPs capture more complex alignments but reduce parameter-efficiency gains

- **Failure signatures**:
  - Fine-tuning degradation: Paper shows fine-tuning drops accuracy (0.16% vs 0.63% direct)—indicates catastrophic forgetting on limited low-resource data
  - Translation error propagation: Errors from Marian MT compound through all 7 stages
  - Domain mismatch: Trained on news, tested on Wikipedia/lyrics—generalization shown but not guaranteed for all domains

- **First 3 experiments**:
  1. **Cross-attention ablation**: Replace cross-attention with simple concatenation—quantifies contribution of source feature preservation mechanism
  2. **Adapter capacity sweep**: Test adapter hidden dimensions [256, 512, 1024, 2048]—finds accuracy/efficiency sweet spot
  3. **Multi-token loss extension**: Modify to predict last N tokens (N=1,2,3,5)—measures whether single-token constraint is the primary performance limiter

## Open Questions the Paper Calls Out

### Open Question 1
Can the TALL architecture generalize to other low-resource languages, particularly those with dissimilar syntax or lower-quality translation models? The Conclusion states: "Future work will extend TALL to additional low-resource languages." Additionally, Section 6 notes: "For extremely low-resource languages where quality translation models are unavailable, this dependency could limit applicability."

### Open Question 2
Can the training protocol be modified to optimize for all tokens in a sequence rather than just the final missing token? Section 3.2 and Section 6 state that the architecture constrains training to the final token and suggest: "Future iterations could explore curriculum learning strategies or multi-stage training protocols with additional per-token predictions."

### Open Question 3
Can partial caching techniques or optimized implementations resolve the inference latency issues caused by the architecture's incompatibility with standard key-value caching? Section 6 states: "The integration of multiple translation and alignment modules complicates efficient inference, particularly for key-value caching... Future work could explore partial caching techniques or optimized implementations to address this limitation."

### Open Question 4
Do more expressive adapter architectures or non-linear alignment techniques improve performance by mitigating the information bottleneck in dimension alignment layers? Section 6 states: "The dimension alignment adapters, while necessary for integration, could introduce information bottlenecks... Future refinements may explore more expressive adapter architectures or non-linear alignment techniques."

## Limitations

- Architecture specification gaps: Exact number of layers, attention heads, and hidden dimensions for custom transformer modules are not fully specified
- Translation model dependency: Performance is inherently limited by the quality of the Marian MT translation models
- Single-token constraint: Architecture restricts training to final token only, limiting gradient signal and semantic coherence

## Confidence

- **High Confidence**: TALL achieves 5.59% accuracy vs 2.93% for next best baseline on Hebrew missing word prediction; Parameter efficiency claim (14-15% trained) is verifiable; Fine-tuning degradation (0.16% vs 0.63% direct) is directly demonstrated
- **Medium Confidence**: Cross-attention effectively preserves source features (supported by architecture but not directly ablated); Dimension alignment adapters successfully bridge embedding space gaps (demonstrated through end-to-end performance but not component-level validation); Frozen component transfer prevents catastrophic forgetting (reasonable given evidence but not systematically tested)
- **Low Confidence**: Adapter bottleneck is the primary performance limiter (identified as limitation but not empirically quantified); Generalization to other low-resource languages (demonstrated only for Hebrew); Scalability to morphologically complex or distant language pairs (not tested)

## Next Checks

1. **Component-Level Ablation Study**: Remove cross-attention and replace with simple concatenation to quantify its contribution to performance; Test with and without dimension alignment adapters; Compare full TALL vs. translation-only vs. LLM-only baselines to isolate architectural benefits

2. **Adapter Capacity Scaling Analysis**: Systematically vary adapter hidden dimensions (256, 512, 1024, 2048) while measuring accuracy on validation set, parameter count and memory usage, and training convergence speed; Plot accuracy vs. parameter count to identify diminishing returns and optimal efficiency point

3. **Cross-Lingual Generalization Testing**: Apply TALL architecture to a different low-resource language pair (e.g., English-Swahili or English-Bengali); Test on multiple domains beyond news (medical, legal, social media); Measure performance degradation relative to Hebrew results to quantify architectural robustness