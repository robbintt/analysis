---
ver: rpa2
title: Gradient Descent as a Shrinkage Operator for Spectral Bias
arxiv_id: '2504.18207'
source_url: https://arxiv.org/abs/2504.18207
tags:
- activations
- singular
- relu
- components
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates spectral bias in shallow neural networks
  by establishing explicit connections between activation functions, gradient descent
  (GD) hyperparameters, and implicit regularization. The key contributions are: (1)
  Activation functions can be related to spline regression regularization, where the
  activation choice controls the order of gradient smoothing; (2) GD acts as a shrinkage
  operator on the singular values of a network''s Jacobian, with learning rate and
  iterations controlling which principal components are preserved; (3) An explicit
  relationship is derived between GD hyperparameters and the number of active principal
  components, with monotonic activations (Heaviside, tanh) being significantly more
  iteration-efficient than non-monotonic ones (ReLU, GELU, SiLU); (4) Non-monotonic
  activations like sinc and Gaussian are shown to be better suited for scale-based
  regularization rather than GD hyperparameter tuning.'
---

# Gradient Descent as a Shrinkage Operator for Spectral Bias

## Quick Facts
- arXiv ID: 2504.18207
- Source URL: https://arxiv.org/abs/2504.18207
- Reference count: 40
- Primary result: Gradient descent acts as a shrinkage operator on network Jacobian singular values, with monotonic activations being significantly more iteration-efficient than non-monotonic ones.

## Executive Summary
This paper establishes explicit connections between activation functions, gradient descent hyperparameters, and implicit regularization in shallow neural networks. The key insight is that GD can be reinterpreted as a shrinkage operator that masks singular values of the network's Jacobian, effectively selecting which principal components (frequency bands) are learned. The choice of activation function determines whether regularization is best controlled through GD iterations or through scaling parameters, with monotonic activations (Heaviside, tanh) being significantly more iteration-efficient than non-monotonic ones (ReLU, GELU, SiLU).

## Method Summary
The paper analyzes 1D signal reconstruction using shallow networks $f(x) = \sum w_m \eta(x - b_m)$ with $M=1024$ fixed biases and trained weights. The Jacobian matrix $A$ is constructed where $A_{ij} = \eta(x_i - b_j)$, and its singular values determine spectral bias behavior. Training uses full-batch GD with learning rate $\alpha = s_{max}^{-2}$ (inverse square of largest singular value). For monotonic activations, iterations $q$ control bandwidth; for non-monotonic activations, scaling parameter $\sigma$ controls bandwidth instead. Experiments use row 100 from the standard "peppers" image as input data.

## Key Results
- GD acts as a shrinkage operator with masking function $m(s; \alpha, q) = [1 - \exp(-\alpha \cdot q \cdot s^2)]$ on Jacobian singular values
- Monotonic activations (Heaviside, tanh) show rapid singular value drop-off, making them highly iteration-efficient
- Non-monotonic activations (sinc, Gaussian) have flat spectral distributions requiring scale-based rather than iteration-based regularization
- The paper derives explicit relationships between GD hyperparameters and number of active principal components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient descent implicitly acts as a shrinkage operator on the singular values of the network's Jacobian, selectively masking frequency components based on learning rate and iteration count.
- **Mechanism:** The paper derives that the GD update path effectively applies a masking function $m(s; \alpha, q) = [1 - \exp(-\alpha \cdot q \cdot s^2)]$ to the singular values $s$ of the feature matrix $A$. This behaves similarly to PCA regularization, where low-frequency (high singular value) components are learned first, while high-frequency components are suppressed until later iterations.
- **Core assumption:** The network is sufficiently wide ($M > N$) and the loss landscape allows the gradient flow approximation (linearized network dynamics) to hold.
- **Evidence anchors:**
  - [abstract] "GD can be reinterpreted as a shrinkage operator that masks the singular values..."
  - [section 4] Proposition 4.1 and Eq. 15 explicitly define the masking function derived from Neumann series/gradient flows.
  - [corpus] Related work on spectral bias confirms that non-linearities shape learning regimes, but this paper specifically models the *operator* aspect of GD.
- **Break condition:** If the learning rate $\alpha > s_{max}^{-2}$, the discrete iteration process may diverge or exhibit oscillatory behavior not captured by the continuous ODE approximation.

### Mechanism 2
- **Claim:** The spectral distribution of singular values is determined by the choice of activation function, which dictates whether GD hyperparameters or scaling factors are the effective regularization mechanism.
- **Mechanism:** Monotonic activations (e.g., Heaviside, tanh) produce a rapid initial drop-off in singular values, creating a distinct separation that allows the GD mask to select a specific bandwidth $K$. In contrast, non-monotonic activations (e.g., sinc, Gaussian) have a flat spectrum that drops precipitously only after index $k=\sigma$; this geometry renders the GD mask ineffective for selection, shifting the regularization burden to the scaling parameter $\sigma$.
- **Core assumption:** The singular value decomposition (SVD) of the activation matrix $A$ accurately reflects the spectral bias properties of the network during training.
- **Evidence anchors:**
  - [section 6] "In (b) one can see that the non-monotonic activations have a strikingly different spectral distribution... tepid then rapid drop-off."
  - [section 5] Discusses how sinc/Gaussian activations relate to sampling theory and bandwidth, unlike monotonic ReLU/Heaviside.
  - [corpus] "The Spectral Bias of Shallow Neural Network Learning..." supports the general premise that non-linearity shapes spectral bias, aligning with the paper's specific findings on monotonic vs. non-monotonic spectra.
- **Break condition:** If a monotonic activation is used with incorrect scaling (specifically tanh with low $\sigma$), it may revert to poor "nearest neighbor" interpolation properties, losing its spectral basis advantage.

### Mechanism 3
- **Claim:** Activation functions implicitly enforce spline smoothing regularization, where the order of the activation polynomial relates to the order of the derivative penalty in spline regression.
- **Mechanism:** The paper establishes that a shallow network minimizes the $r$-th order gradient energy of the function $f(x)$, where the activation $\eta(x)$ takes the form $x^{r-1} \cdot [x > 0]$. For example, ReLU ($r=2$) corresponds to minimizing the second derivative (curvature), while Heaviside ($r=1$) minimizes the first derivative.
- **Core assumption:** The width $M$ is large enough to approximate the continuous integral formulation of spline smoothing.
- **Evidence anchors:**
  - [section 3] Proposition 3.1 derives the equivalence between the network formulation and spline regression.
  - [section 1] "Activation functions can be related to spline regression regularization..."
  - [corpus] Evidence is weak in the specific corpus neighbors regarding spline regression; this connection is primarily internal to the paper's theoretical contribution.
- **Break condition:** The connection strictly holds for the derived polynomial forms (ReLU, Heaviside) but is an approximation for smooth activations like GELU or SiLU.

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) and Spectral Analysis
  - **Why needed here:** The paper reinterprets the network's learning dynamics not as weight updates, but as operations on the singular values ($s$) of the Jacobian matrix. Understanding how "energy" is distributed across these values is essential to grasp the "shrinkage operator" concept.
  - **Quick check question:** Can you explain why a rapid drop-off in singular values might make a system easier to regularize than a flat distribution?

- **Concept:** Gradient Flows (ODE approximation of GD)
  - **Why needed here:** The derivation of the masking function relies on treating discrete gradient descent steps as a continuous Ordinary Differential Equation (ODE). This allows the authors to model the "trajectory" of the solution towards the minimum norm solution.
  - **Quick check question:** How does the learning rate $\alpha$ and iteration count $q$ relate to the time variable $t$ in the continuous gradient flow ODE?

- **Concept:** Spline Regression/Smoothing
  - **Why needed here:** This provides the theoretical bridge between the *choice* of activation function and the *smoothness* of the output. It explains why certain activations (like ReLU) naturally produce smoother results than others.
  - **Quick check question:** If an activation function corresponds to a 2nd-order spline penalty ($r=2$), what physical property of the signal curve is being minimized?

## Architecture Onboarding

- **Component map:** Input $x$ -> Biases $b$ (fixed grid) -> Activation $\eta(x-b)$ -> Weights $w$ (trained) -> Output $f(x) = \sum w_m \eta(x-b_m)$ -> Jacobian $A$ (SVD analysis)

- **Critical path:**
  1. Select **Activation** ($\eta$) based on desired regularization strategy (GD-tunable vs. Scale-tunable)
  2. Determine **Spectrum** of $A$ to identify $s_{max}$ and the drop-off profile
  3. Set **Hyperparameters**:
     - For Monotonic: Set learning rate $\alpha \approx s_{max}^{-2}$ and tune iterations $q$ for bandwidth
     - For Non-monotonic: Set scaling $\sigma$ to desired bandwidth $K$; iterations matter less

- **Design tradeoffs:**
  - **Monotonic (e.g., Tanh):**
    - *Pro:* Can tune spectral bandwidth simply by stopping training early (controlling $q$). Strict frequency basis.
    - *Con:* Performance is sensitive to scaling factor $\sigma$.
  - **Non-Monotonic (e.g., Sinc):**
    - *Pro:* Iteration efficient; bandwidth is set explicitly by scaling $\sigma$.
    - *Con:* Cannot regularize effectively by stopping early (GD mask doesn't work well on flat spectra).
  - **ReLU variants:** Smooth basis but less strictly aligned with frequency basis than Heaviside/Tanh.

- **Failure signatures:**
  - **"Grey Zone" Dominance:** If the singular value drop-off is too slow (e.g., ReLU2), the GD mask transition is shallow, causing interference between components and poor generalization.
  - **Scaling Mismatch:** Using tanh with poor scaling results in "nearest neighbor" artifacts rather than smooth interpolation.
  - **Misapplied Regularization:** Attempting to control spectral bias via iteration count $q$ for a Gaussian/Sinc activation (Fig 6a shows this fails).

- **First 3 experiments:**
  1. **Spectral Profile Verification:** Construct the matrix $A$ for Heaviside, ReLU, and Sinc activations on a 1D domain. Plot the normalized singular values (log scale) to confirm the "rapid drop-off" vs. "flat then drop" hypothesis described in Section 6.
  2. **GD Mask Validation:** Train a shallow Tanh network on a 1D signal. Vary iterations $q$ while keeping $\alpha$ fixed. Plot the PSNR vs. active components $K$ to verify that $q$ explicitly controls the retained bandwidth (matching the DST baseline as in Fig 6a).
  3. **Scale vs. Iteration Ablation:** Train a Sinc network. In run A, fix scaling $\sigma$ and vary iterations $q$. In run B, fix iterations $q$ and vary scaling $\sigma$. Confirm that run B tracks the DST baseline effectively while run A fails (validating Fig 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the shrinkage operator interpretation of Gradient Descent generalize to deep neural networks?
- Basis in paper: [explicit] The authors conclude that viewing GD as a shrinkage operator "could open up new avenues of investigation for regularizing deep networks."
- Why unresolved: The theoretical derivations and experiments are confined to 1D shallow networks, leaving the behavior of deep architectures uncharacterized.
- What evidence would resolve it: Empirical validation showing that the singular values of a deep network's Jacobian are masked by the function $m(s; \alpha, q)$ during training.

### Open Question 2
- Question: Can scaling-based regularization of non-monotonic activations lead to architectures that are significantly more training-efficient?
- Basis in paper: [explicit] The paper states that non-monotonic activations regularized via scaling (rather than iterations) "gives new hope for designing architectures that are super-efficient to train."
- Why unresolved: While the paper shows scaling controls bandwidth, it does not benchmark the actual training speed/efficiency trade-offs against standard monotonic networks.
- What evidence would resolve it: Comparative benchmarks demonstrating that tuning $\sigma$ achieves target generalization with fewer gradient descent steps than tuning $\alpha$ and $q$.

### Open Question 3
- Question: Does the ordering of singular values correspond to signal smoothness in high-dimensional input spaces?
- Basis in paper: [inferred] Appendix E notes that for 2D signals, "the index of the principal component no longer have any relationship to bandwidth," unlike the strict 1D frequency basis.
- Why unresolved: The core theory relies on principal components matching a Discrete Sine Transform (frequency) basis, a property that appears to degrade in dimensions $D > 1$.
- What evidence would resolve it: A theoretical or empirical characterization of the principal components of $A$ for $x \in \mathbb{R}^D$ to determine if lower-index components still represent smoother functions.

## Limitations
- The theoretical framework relies on strong assumptions about network width and gradient flow approximations that may not hold in practical scenarios
- Experiments are limited to 1D signal reconstruction from a single image row, limiting generalizability to higher-dimensional or more complex tasks
- The spline regression connection for non-polynomial activations (GELU, SiLU) is approximate rather than exact

## Confidence

- **High:** The gradient descent as shrinkage operator mechanism (Proposition 4.1 and Eq. 15) - this is the core theoretical contribution with clear mathematical derivation
- **Medium:** The spectral distribution characterization for monotonic vs non-monotonic activations - supported by Figure 6 but relies on specific experimental conditions
- **Medium:** The activation-spline regularization connection for ReLU and Heaviside - well-derived but weaker empirical support in the corpus
- **Low:** Generalization claims to deeper networks or other architectures - the paper explicitly focuses on shallow networks

## Next Checks

1. **Robustness to Activation Scaling:** Reproduce Figure 6a for tanh with multiple scaling factors (σ = 10, 100, 1000) to verify the claim that scaling must be "sufficiently large" for the discrete sine transform basis to emerge. Document the exact threshold where performance degrades.

2. **Cross-Architecture Transfer:** Apply the same analysis framework (SVD of Jacobian, GD mask characterization) to a 2-layer MLP trained on CIFAR-10. Compare whether the monotonic/non-monotonic activation patterns persist in higher-dimensional feature spaces.

3. **Stochastic Gradient Perturbations:** Repeat the GD mask experiments with SGD (mini-batches of size 32) instead of full-batch GD. Measure how noise affects the singular value trajectory and whether the theoretical α = s_max⁻² still prevents divergence.