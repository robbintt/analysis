---
ver: rpa2
title: 'From nuclear safety to LLM security: Applying non-probabilistic risk management
  strategies to build safe and secure LLM-powered systems'
arxiv_id: '2505.17084'
source_url: https://arxiv.org/abs/2505.17084
tags:
- strategies
- risk
- system
- formal
- config
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of managing safety and security
  risks in Large Language Model (LLM) powered systems, which are difficult to handle
  using conventional probabilistic risk analysis due to novelty, complexity, and adaptive
  adversaries. The authors propose applying over 100 non-probabilistic risk management
  strategies drawn from fields like nuclear engineering, medicine, and military science,
  organized into five categories: structural, reactive, formal, counter-adversarial,
  and multi-stage strategies.'
---

# From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems

## Quick Facts
- arXiv ID: 2505.17084
- Source URL: https://arxiv.org/abs/2505.17084
- Reference count: 0
- Primary result: A practical framework applying over 100 non-probabilistic risk strategies from high-reliability fields to LLM security, enabling systematic risk reduction where probabilistic data is scarce.

## Executive Summary
This paper addresses the challenge of managing safety and security risks in LLM-powered systems, which are difficult to handle using conventional probabilistic risk analysis due to novelty, complexity, and adaptive adversaries. The authors propose applying over 100 non-probabilistic risk management strategies drawn from fields like nuclear engineering, medicine, and military science, organized into five categories: structural, reactive, formal, counter-adversarial, and multi-stage strategies. They present a mapping of these strategies to LLM security and AI safety, and introduce an LLM-powered workflow for applying them. The primary result is a practical framework and toolkit that enables solution architects to systematically reduce risks in LLM-powered systems, particularly where data on risks is scarce.

## Method Summary
The method involves curating a catalog of 100+ non-probabilistic risk strategies (RDOT) from high-reliability fields and categorizing them into five types. An LLM-powered workflow uses a reasoning model to map these generic strategies to specific system components based on a project description. The approach replaces quantitative probability estimation with qualitative, field-agnostic design principles to improve system robustness and resilience without requiring enumerated event frequencies.

## Key Results
- The framework successfully identifies 15% of strategies as "highly promising and not previously considered" for LLM security
- 31% of strategies were already utilized in a RAG hallucination-reduction case study
- The LLM-powered workflow demonstrates feasibility for strategy mapping, though human review remains necessary

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If probabilistic data is unavailable, applying qualitative strategies from high-stakes physical engineering may reduce LLM system risk.
- **Mechanism:** The framework replaces quantitative probability estimation (PRA) with qualitative, field-agnostic design principles (RDOT)—such as spatial separation or fail-safe designs. These methods function by improving system robustness and resilience without requiring enumerated event frequencies.
- **Core assumption:** Principles of robustness derived from physical systems (e.g., nuclear) transfer validly to stochastic software systems.
- **Evidence anchors:**
  - [abstract] Notes that conventional PRA is impractical for LLMs due to novelty and adaptive adversaries, proposing 100+ non-probabilistic strategies instead.
  - [section 2] Defines the RDOT catalog's five categories, arguing that generic strategies like "robust designs" are field-agnostic.
  - [corpus] Related work on *TRiSM for Agentic AI* supports the need for structured Trust, Risk, and Security Management in complex LLM systems, though it does not validate the nuclear-engineering origin of specific strategies.
- **Break condition:** If the specific failure modes of LLMs (e.g., semantic hallucinations) are fundamentally incompatible with physical safety abstractions like "safety margins."

### Mechanism 2
- **Claim:** Multi-layered defenses (structural, reactive, formal) likely provide better coverage against adaptive adversaries than single-point solutions.
- **Mechanism:** By combining structural barriers (e.g., sandboxing), reactive measures (e.g., anomaly detection), and formal methods (e.g., event-tree analysis), the system creates overlapping coverage. This "defense in depth" ensures that if one layer fails, others remain active.
- **Core assumption:** Complexity introduced by multiple layers does not introduce new, unforeseen failure modes that outweigh the safety benefits.
- **Evidence anchors:**
  - [section 3.1] Discusses "multi-echelon or multi-layer defense" to decrease the risk of unexpected gaps.
  - [section 3.4] Highlights that reactive strategies like intrusion detection offer protections against adversaries when combined with structural methods.
  - [corpus] *A Safety and Security Framework for Real-World Agentic Systems* similarly frames safety as an emergent property of dynamic interactions, implicitly supporting multi-layered approaches.
- **Break condition:** If the defensive layers share a common mode of failure or if the complexity creates unmanageable operational overhead (as warned in the Limitations section regarding Three Mile Island).

### Mechanism 3
- **Claim:** An LLM with reasoning capabilities can function as a contextual engine to map generic risk strategies to specific architectural components.
- **Mechanism:** The authors propose a workflow where an LLM (e.g., DeepSeek-R1) ingests the RDOT catalog and a project description. It then performs semantic matching to suggest relevant mitigations (e.g., applying "sacrificial parts" to a specific API gateway).
- **Core assumption:** The LLM possesses sufficient reasoning capability to correctly interpret abstract safety strategies and map them to novel technical contexts without hallucinating invalid applications.
- **Evidence anchors:**
  - [section 3.6] Describes the LLM-powered workflow using a prompt with the strategy list and project description to "report strategies that could reduce the risk."
  - [corpus] *AgentAuditor* highlights the difficulty of evaluating agent safety step-by-step, suggesting that using LLMs to *generate* safety strategies relies on the same reasoning capabilities that are currently difficult to evaluate.
- **Break condition:** If the LLM misinterprets the project architecture or suggests strategies that are technically infeasible or detrimental to the system's primary function.

## Foundational Learning

- **Concept:** **Knightian Uncertainty (Radical Uncertainty)**
  - **Why needed here:** The paper explicitly rejects Probabilistic Risk Assessment (PRA) because LLM risks are novel and cannot be easily enumerated or quantified. Understanding the difference between "calculable risk" and "uncertainty" is prerequisite to accepting the paper's qualitative approach.
  - **Quick check question:** Can you explain why estimating a probability distribution for a novel adversarial prompt is considered "radical uncertainty" rather than just "missing data"?

- **Concept:** **Defense in Depth (Multi-echelon Defense)**
  - **Why needed here:** This is the core structural mechanism proposed. It moves beyond "fixing bugs" to creating resilient architectures where multiple independent barriers protect the system.
  - **Quick check question:** If the primary input filter fails, what is the next structural barrier in your current design that prevents a data exfiltration event?

- **Concept:** **Safety Culture**
  - **Why needed here:** The paper argues that technical controls are insufficient without organizational processes like training, questioning anomalies, and delegation of authority—common in nuclear/aviation but often lacking in software.
  - **Quick check question:** Does your organization have a process for developers to question safety requirements without slowing down release cycles?

## Architecture Onboarding

- **Component map:**
  - RDOT Catalog (100+ strategies) -> Strategy Matcher (LLM) -> Defense Layers (Implementation)

- **Critical path:**
  1. Profile the system's **Impact** (value/consequence) and **Adversarial Exposure**
  2. Filter the RDOT catalog from 100+ strategies to a relevant subset (e.g., focusing on Counter-adversarial for public-facing apps)
  3. Implement **Structural** strategies first (robustness), then **Reactive** (detection)

- **Design tradeoffs:**
  - **Complexity vs. Safety:** As noted in the paper (referencing Three Mile Island), adding multiple overlapping safety systems can introduce new failure modes and operational complexity.
  - **Qualitative vs. Quantitative:** Adopting these strategies trades the precision of probabilistic metrics for the broader coverage of qualitative heuristics.

- **Failure signatures:**
  - **Brittle Overlap:** Multiple defensive layers that all fail simultaneously due to a shared underlying dependency (e.g., all filters relying on the same embedding model).
  - **Safety Bypass:** A sophisticated adversary finding a gap between the "layers" of defense (e.g., an input that bypasses the filter but triggers an unintended action in the agent).

- **First 3 experiments:**
  1. **Catalog Filtering:** Select a low-risk internal tool. Apply the filtering criteria (Impact/Resources) to select 3 "Structural" strategies from Table A1 and implement them.
  2. **LLM Strategy Generation:** Run the proposed LLM workflow (Section 3.6) on a high-level design document. Manually evaluate the feasibility of the top 5 suggested strategies.
  3. **Red Teaming vs. RDOT:** Pick one "Counter-adversarial" strategy (e.g., "Randomization of moves" or "Deception") and simulate an adversarial attack to see if the strategy changes the success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multiple non-probabilistic strategies be combined efficiently without introducing excessive complexity or new modes of system failure?
- Basis in paper: [explicit] Section 5 states that "multiple overlapping safety systems can create cost and complexity, and even introduce new modes of failure," citing the Three Mile Island accident.
- Why unresolved: The authors suggest iterative refinement but do not provide a systematic method for optimizing the interaction between different strategies (e.g., structural vs. reactive).
- What evidence would resolve it: Empirical data from LLM deployments showing failure rates in systems with single strategies versus those with combined, multi-layered strategies.

### Open Question 2
- Question: How effective are the proposed LLM-powered workflows in accurately identifying and mapping relevant risk strategies to specific system components?
- Basis in paper: [inferred] Section 3.6 describes an LLM workflow where the "raw output is usually infeasible or too complex," relying on human engineers for final selection.
- Why unresolved: The paper proposes the workflow as a tool but does not quantify the precision or reliability of the LLM in suggesting valid risk mitigations compared to human experts.
- What evidence would resolve it: A benchmark study evaluating the relevance and correctness of strategies suggested by the LLM workflow against a gold-standard set determined by security architects.

### Open Question 3
- Question: To what extent do strategies borrowed from high-reliability fields (e.g., FMECA, checkrides) actually reduce failure rates in LLM systems compared to standard software engineering practices?
- Basis in paper: [explicit] Section 3 states that strategies like FMECA and spiral development are "uncommon in AI" and "might be quite useful," but acknowledges lacking space to evaluate individual applications.
- Why unresolved: The paper provides a theoretical mapping but offers no quantitative results demonstrating that these specific nuclear/aerospace strategies outperform existing AI safety baselines.
- What evidence would resolve it: Controlled experiments measuring the reduction in hallucinations or security breaches in systems developed using these specific borrowed strategies versus controls.

### Open Question 4
- Question: How robust are structural and reactive strategies against sophisticated adaptive adversaries specifically targeting LLM vulnerabilities?
- Basis in paper: [inferred] Section 3.4 mentions applying strategies against adaptive adversaries but notes that offensive AI technologies are "rapidly advancing" and often overmatch current defenses.
- Why unresolved: While the paper maps counter-adversarial strategies, it does not validate how well static structural defenses (like spatial separation) hold up against dynamic, learning-based attacks.
- What evidence would resolve it: Red-teaming exercises measuring the success rate of adaptive attacks against systems fortified specifically with the proposed counter-adversarial strategies.

## Limitations
- **Transferability uncertainty:** The core assumption that physical engineering safety principles transfer validly to stochastic software systems remains unproven.
- **Implementation complexity:** Multiple overlapping safety systems can introduce new failure modes and operational complexity, as warned by the Three Mile Island example.
- **LLM workflow reliability:** The LLM-powered strategy mapping tool is presented as a useful aid, but its precision and reliability compared to human experts is not quantified.

## Confidence
- **High Confidence:** The identification of the gap in conventional PRA for LLM systems is well-supported by the literature and the novelty/adaptivity of LLM risks.
- **Medium Confidence:** The categorization of 100+ strategies from RDOT into five coherent categories is plausible, but the specific relevance of many strategies to LLM security requires deeper validation.
- **Low Confidence:** The LLM-powered workflow's ability to accurately and usefully map abstract strategies to specific architectural components is the most speculative claim, as it depends on the LLM's reasoning capabilities and the quality of its semantic matching.

## Next Checks
1. **Transferability Validation:** For a selected set of RDOT strategies (e.g., "defense in depth" or "fail-safe design"), explicitly map their original context in nuclear engineering to their proposed application in LLM systems. Evaluate if the underlying failure modes are analogous.
2. **LLM Workflow Prompt Testing:** Reconstruct and test the proposed LLM workflow using a reasoning model on a small, well-defined LLM application (e.g., a customer service chatbot). Compare the LLM's strategy suggestions to a manual expert review.
3. **Red Teaming with RDOT:** Implement a small subset of counter-adversarial strategies from the RDOT catalog (e.g., "randomization of moves" or "deception") in a test LLM system. Conduct a red team exercise to measure the strategies' impact on the system's vulnerability to adversarial attacks.