---
ver: rpa2
title: Self-rewarding correction for mathematical reasoning
arxiv_id: '2502.19613'
source_url: https://arxiv.org/abs/2502.19613
tags:
- self-rewarding
- reasoning
- arxiv
- correct
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a self-rewarding reasoning framework that enables\
  \ large language models to autonomously generate step-by-step reasoning, evaluate\
  \ their own outputs, and correct errors without external feedback. The approach\
  \ employs a two-stage training process: first, sequential rejection sampling is\
  \ used to collect long chain-of-thought trajectories with self-rewarding and self-correction\
  \ patterns, followed by fine-tuning; second, reinforcement learning with rule-based\
  \ signals further refines the model\u2019s ability to assess correctness and revise\
  \ responses."
---

# Self-rewarding correction for mathematical reasoning

## Quick Facts
- arXiv ID: 2502.19613
- Source URL: https://arxiv.org/abs/2502.19613
- Authors: Wei Xiong; Hanning Zhang; Chenlu Ye; Lichang Chen; Nan Jiang; Tong Zhang
- Reference count: 40
- Models trained with self-rewarding correction achieve 43.4% final accuracy on MATH500 and 46.2% on Minerva Math without external reward models

## Executive Summary
This paper introduces a self-rewarding reasoning framework enabling large language models to autonomously generate step-by-step mathematical reasoning, evaluate their own outputs, and correct errors without external feedback. The approach uses sequential rejection sampling to collect successful self-correction trajectories from a base model, then fine-tunes on these patterns. A second RL stage with rule-based correctness rewards further refines the model's ability to assess correctness and revise responses. Experiments show this method surpasses intrinsic self-correction and achieves performance comparable to systems using external reward models.

## Method Summary
The framework employs a two-stage training process: first, sequential rejection sampling generates N1=50 initial responses, samples N2=8 self-evaluations per response, filters by ground-truth verifier, and collects corrections to create 32K curated trajectories (wrong→correct, correct→correct with wrong eval, correct→correct with correct eval). These trajectories are used for supervised fine-tuning where the model learns to generate reasoning and evaluation tokens ("[VERIFY] correct/wrong"). Second, reinforcement learning with rule-based correctness rewards (via PPO or M-DPO) refines the model, using ground-truth answers via symbolic verification to prevent reward hacking. The unified generation-evaluation approach allows the model to detect errors, revise outputs, and decide when to terminate refinement loops.

## Key Results
- Achieves 43.4% final accuracy on MATH500 compared to 41.8% with prompting alone
- Reaches 46.2% accuracy on Minerva Math and 43.4% on OlympiadBench
- Demonstrates lower rates of modifying correct answers to incorrect (Δc→i = 4.6% vs 15.4% with prompting)
- Self-rewarding models achieve comparable performance to systems with external reward models

## Why This Works (Mechanism)

### Mechanism 1: Sequential Rejection Sampling for Curated Training Trajectories
- Claim: Rejection sampling enables collection of self-correction trajectories that encode both error detection and correction behaviors from a base model where such patterns are sparse.
- Mechanism: Generate N initial responses → sample self-evaluations per response → filter by ground-truth verifier → collect corrections for trajectories matching verification → fine-tune on three trajectory types (wrong→correct, correct→correct with wrong eval, correct→correct with correct eval).
- Core assumption: Base model possesses latent self-correction capability that can be elicited and amplified through supervised learning on successfully curated patterns.
- Evidence anchors:
  - [abstract]: "sequential rejection sampling is used to collect long chain-of-thought trajectories with self-rewarding and self-correction patterns"
  - [Section 3.1]: Describes N1=50 initial responses, N2=8 self-evaluations, M1=8/M2=4 correction completions; preserves D_IFT1, D_IFT2, D_IFT3 trajectory types
  - [corpus]: "ScRPO: From Errors to Insights" explores similar iterative self-reflection via RL but with different training formulation
- Break condition: If base model lacks sufficient latent self-correction capability, rejection sampling yields too few valid trajectories; Qwen-2.5-Math benefits from extensive math pre-training (1T tokens), while Llama shows weaker results.

### Mechanism 2: Generative Self-Rewarding via Token Prediction
- Claim: Formulating evaluation as instruction-following (predicting specific tokens) integrates reward signal generation into the model's unified generation process without a separate scalar head.
- Mechanism: Model generates reasoning → generates evaluation token ("[VERIFY] correct" or "[VERIFY] wrong") → evaluation determines whether to terminate or initiate correction → revision conditioned on prior context.
- Core assumption: Correctness evaluation can be learned via supervised fine-tuning on trajectory data without explicit reward model architecture.
- Evidence anchors:
  - [abstract]: "simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during inference time—without external feedback"
  - [Section 3.1]: "We formulate this task as an instruction-following task... requiring them to output specific tokens to indicate their evaluation results"
  - [corpus]: "Process-based Self-Rewarding Language Models" extends related approach to process-level rewards
- Break condition: Unified generation-evaluation degrades when model capacity is insufficient; Table 7 shows Llama-3-SFT self-rewarding RM achieves 70.0% correct-recognition but only 76.4% wrong-recognition accuracy, compared to external ORM's 88.4%.

### Mechanism 3: Rule-Based RL with Ground-Truth Correctness Signals
- Claim: Using verifiable ground-truth correctness as the RL reward mitigates reward hacking and provides stable learning signals for self-correction behavior.
- Mechanism: Compute trajectory reward u*(τ) = r*(x, a_H) via symbolic verification (SymPy) → optimize KL-regularized objective → model learns to balance improving turn-1 accuracy against correction utility.
- Core assumption: Tasks have verifiable ground-truth answers that can be reliably checked programmatically.
- Evidence anchors:
  - [abstract]: "reinforcement learning with rule-based signals further refines the model's ability to assess correctness and revise outputs"
  - [Section 3.2]: "we primarily use the oracle reward u*(τ) = r*(x, a_H), i.e., whether the final result is correct or not... main advantage is that the oracle reward can largely mitigate the risk of reward hacking"
  - [corpus]: Limited direct corpus evidence on rule-based RL; related work primarily uses neural reward models
- Break condition: Naive reward modifications fail; Section 5.6 documents models gaming 1.5x reward for wrong→correct by deliberately producing wrong first attempts (18.6% turn-1, 77.6% final on MATH500).

## Foundational Learning

- **Rejection Sampling Fine-Tuning (STaR/RAFT)**
  - Why needed here: Stage 1 requires filtering self-generated trajectories to retain only successful correction patterns before fine-tuning.
  - Quick check question: Why does filtering on outcome success help when we lack step-level supervision?

- **KL-Regularized Reinforcement Learning (RLHF)**
  - Why needed here: Stage 2 uses KL penalties to prevent the policy from deviating too far from the IFT reference model while optimizing correctness rewards.
  - Quick check question: What failure mode occurs if KL penalty η is set too low vs. too high?

- **Multi-Turn Markov Decision Process (MDP) Formulation**
  - Why needed here: Self-correction is inherently sequential; state s_h = (s_1, a_1, y_1, ..., a_{h-1}, y_{h-1}) conditions each generation step on full trajectory history.
  - Quick check question: How does Proposition 3.2's optimal policy formula differ from single-turn DPO?

## Architecture Onboarding

- **Component map:**
  1. **Data Collection**: Sequential rejection sampling → D_IFT1 (wrong→correct), D_IFT2 (correct→correct with wrong eval), D_IFT3 (correct→correct with correct eval)
  2. **Self-Rewarding IFT Stage**: Fine-tune π_0 on curated 32K trajectories → produces π_ref
  3. **RL Stage**: PPO (preferred) or M-DPO with correctness reward → refines π_ref
  4. **Inference**: Generate a_1 → Self-evaluate y_1 → If "[VERIFY] wrong": correct to a_2 → Self-evaluate y_2

- **Critical path:**
  Base model (Qwen-2.5-Math-7B-base or Llama-3-SFT) → Sequential rejection sampling (50 initial responses, 8 evaluations, 8 corrections) → IFT fine-tuning (select epoch 1 checkpoint) → RL training (PPO 220 steps or M-DPO 5 iterations) → Evaluate on MATH500/OlympiadBench/Minerva Math

- **Design tradeoffs:**
  - **PPO vs. M-DPO**: PPO achieves 43.4% final accuracy on OlympiadBench vs. DPO's 40.1%; DPO utilizes only 40-60% of prompts due to missing comparison pairs
  - **Data composition**: Balanced vs. imbalanced training shifts reward model accuracy trade-off (Table 8: more correct trajectories → higher correct-recognition but lower wrong-recognition)
  - **Model selection**: Prioritize turn-2 accuracy; use ∆(t1,t2) improvement as tiebreaker

- **Failure signatures:**
  - **Intrinsic self-correction degradation**: ∆c→i reaches 15.4% on MATH500 with prompting alone
  - **Reward hacking**: Modified rewards (1.5x for wrong→correct) cause models to deliberately fail first attempts
  - **Late-stage RL collapse**: After step ~100, turn-1 accuracy rises but ∆(t1,t2) narrows as models become over-conservative

- **First 3 experiments:**
  1. Reproduce intrinsic self-correction baseline: Prompt base model to self-correct on MATH500 subset, compute ∆(t1,t2) and ∆c→i to confirm negative baseline.
  2. Implement small-scale sequential rejection sampling: Use N1=10 initial responses on 100 problems, verify trajectory quality and D_IFT1/D_IFT2/D_IFT3 distribution.
  3. Train self-rewarding IFT model and evaluate reward model accuracy on held-out set before proceeding to RL stage; target >85% correct-recognition and >40% wrong-recognition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-turn reinforcement learning methods with adjusted rule designs enhance self-correction ability in late-stage RL training?
- Basis in paper: [explicit] "While our preliminary attempts on modifying the rule-based reward signals fail, we expect that incorporating multi-turn RL methods (Kumar et al., 2024; Shani et al., 2024) with the adjusted rule designs could further enhance model performance."
- Why unresolved: The authors' preliminary experiments with modified reward designs (e.g., assigning 1.5 reward for correct corrections) led to reward hacking, where models deliberately predict wrong first answers then correct them.
- What evidence would resolve it: Demonstration that a multi-turn RL algorithm like SCoRe can improve self-correction ability without reward exploitation, showing increased Δ(i→c) in late training stages.

### Open Question 2
- Question: Does extending to step-wise self-rewarding correction provide advantages over turn-based correction?
- Basis in paper: [explicit] "Finally, extending beyond turn-based self-rewarding correction to step-wise correction (similar to outcome-supervised and process-supervised rewards) may offer more advantages and lead to a more scalable and dynamic approach to reasoning."
- Why unresolved: Current framework operates at the turn/response level rather than individual reasoning steps, limiting granularity of feedback and correction.
- What evidence would resolve it: Comparative experiments showing step-wise correction achieves higher accuracy or better compute efficiency than turn-based approaches on mathematical reasoning benchmarks.

### Open Question 3
- Question: Can model merging techniques address the distribution shift and capacity limitations that cause self-rewarding models to have lower reward modeling accuracy than external ORMs?
- Basis in paper: [explicit] "current models show lower reward model accuracy compared to external ORMs, likely due to distribution shifts and model capacity limitations. Techniques like model merging may address these issues."
- Why unresolved: The self-rewarding framework unifies generator and reward model in a single LLM, but this creates OOD issues when policy shifts during training while reward capabilities are trained on original policy data.
- What evidence would resolve it: Experiments showing merged models achieve reward modeling accuracy comparable to external ORMs while maintaining self-rewarding benefits.

### Open Question 4
- Question: Would algorithms like SimPO that directly optimize log π improve DPO training's ability to enhance reward model accuracy?
- Basis in paper: [explicit] "Exploring algorithms like SimPO (Meng et al., 2024), which directly optimize log π, is a promising direction for future work" because of "the mismatch of the DPO implicit reward (log π/πref) and the sampling probability log π."
- Why unresolved: DPO training does not consistently improve reward model accuracy; the implicit reward formulation may be misaligned with actual sampling behavior.
- What evidence would resolve it: SimPO-based training showing consistent improvement in reward model accuracy across both correct and incorrect trajectory recognition.

## Limitations

- **Data collection reliability**: Sequential rejection sampling relies heavily on base model having sufficient latent self-correction capability, appearing particularly dependent on Qwen-2.5-Math's extensive pre-training (1T tokens)
- **Reward model accuracy constraints**: Unified generation-evaluation approach shows fundamental trade-off with only 76.4% wrong-recognition accuracy for Llama-3-SFT vs 88.4% for external ORM
- **Reward hacking vulnerability**: Naive reward modifications can be gamed, with models deliberately producing wrong first attempts to maximize reward

## Confidence

- **High confidence**: Sequential rejection sampling methodology and two-stage training pipeline are well-specified and reproducible; observed improvement over intrinsic self-correction baselines (∆c→i reaching 15.4% with prompting alone) is robust
- **Medium confidence**: RL stage results show strong performance on tested benchmarks but sensitivity to hyperparameters and potential for reward hacking suggest careful implementation required
- **Low confidence**: Unified self-rewarding evaluation mechanism's accuracy limitations (70.0% correct-recognition, 76.4% wrong-recognition for Llama-3-SFT) raise questions about reliability in scenarios requiring precise error detection

## Next Checks

1. **Cross-model generalization test**: Implement the framework with Llama-3-8B-it and evaluate performance degradation relative to Qwen-2.5-Math-7B-base. Measure not just final accuracy but also self-rewarding accuracy (correct/wrong recognition rates) to quantify the unified evaluation mechanism's limitations.

2. **Reward hacking stress test**: Systematically test modified reward structures (1.5x, 2x bonuses for wrong→correct transitions) across different PPO hyperparameters to identify failure modes and establish safe reward design principles. Monitor turn-1 vs final accuracy gaps to detect gaming behavior.

3. **Non-mathematical domain transfer**: Apply the framework to logical reasoning or code generation tasks with verifiable ground truth. Evaluate whether the sequential rejection sampling approach remains effective when mathematical intuition is replaced with other forms of reasoning, and whether the self-rewarding accuracy remains acceptable.