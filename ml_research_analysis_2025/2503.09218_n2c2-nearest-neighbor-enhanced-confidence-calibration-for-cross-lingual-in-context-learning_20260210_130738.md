---
ver: rpa2
title: 'N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual In-Context
  Learning'
arxiv_id: '2503.09218'
source_url: https://arxiv.org/abs/2503.09218
tags:
- n2c2
- language
- calibration
- cross-lingual
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of poor accuracy and high calibration
  errors in cross-lingual in-context learning (ICL) for sentiment classification.
  The proposed N2C2 method improves ICL by retrieving and leveraging supporting examples
  from the source language through a k-nearest neighbors augmented classifier.
---

# N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual In-Context Learning

## Quick Facts
- arXiv ID: 2503.09218
- Source URL: https://arxiv.org/abs/2503.09218
- Reference count: 33
- Improves cross-lingual ICL accuracy by 4.2% and reduces ECE by 10.53% on average

## Executive Summary
N2C2 addresses the challenge of poor accuracy and calibration in cross-lingual in-context learning for sentiment classification. The method augments traditional ICL with a k-nearest neighbors classifier that retrieves semantically similar examples from a source-language datastore. By interpolating the model's native predictions with a kNN-based distribution, N2C2 significantly improves both accuracy and calibration across multiple languages. The approach incorporates a learned retrieval representation, confidence-aware distribution construction, and adaptive neighbor combination to effectively leverage limited supporting instances.

## Method Summary
N2C2 enhances cross-lingual ICL by retrieving and leveraging supporting examples from the source language through a k-nearest neighbors augmented classifier. The method employs a semantically consistent retrieval representation via a learned linear projection, a confidence-aware distribution that reweights neighbors based on model confidence, and an adaptive combination mechanism that interpolates multiple kNN distributions. The approach is trained on a split of the source data, with one portion used to populate the retrieval datastore and the other to train the projection, confidence, and combination modules. Experiments use XLM-RoBERTa-base as the backbone model.

## Key Results
- Outperforms traditional ICL, fine-tuning, prompt tuning, and recent state-of-the-art methods
- Average improvements of 4.2% accuracy and 10.53% reduction in Expected Calibration Error (ECE)
- Consistent performance gains across all language pairs and b-shot settings (2,4,8,16,32 examples per class)

## Why This Works (Mechanism)

### Mechanism 1: kNN-Augmented Distribution Interpolation
Interpolating a k-nearest neighbor distribution with the base model's distribution improves prediction accuracy and calibration. The final prediction distribution combines the model's native output with a kNN-based distribution generated by retrieving semantically close source-language examples, grounding predictions in explicit similar examples.

### Mechanism 2: Semantically Consistent Retrieval Representation
A task-specific projection layer improves retrieval quality by transforming MASK token representations into a space optimized for semantic similarity. This learned linear layer ensures representations belonging to the same class are closer in the embedding space, improving neighbor relevance.

### Mechanism 3: Confidence-Aware Distribution Construction
Reweighting retrieved neighbors based on the model's prediction confidence improves calibration. The confidence-aware module calculates dynamic weights that incorporate the model's predicted probability for both the test query and retrieved examples, upweighting high-confidence neighbors.

## Foundational Learning

- **In-Context Learning (ICL)**: The baseline paradigm where examples provided in a prompt guide model predictions. Why needed: Understanding how prompts guide predictions is essential to see how N2C2 augments this process. Quick check: Given a prompt with English examples and a Chinese test sentence, how does standard ICL make a prediction?

- **Model Calibration**: A calibrated model's confidence should match its accuracy. Why needed: The core problem is miscalibration, and ECE measures the gap between confidence and accuracy. Quick check: If a model makes 100 predictions, each with 80% confidence, and 80 are correct, is the model perfectly calibrated?

- **k-Nearest Neighbors (kNN) for Classification**: A classifier that makes predictions based on the labels of nearby training examples. Why needed: The primary augmentation is a kNN classifier built on top of the MPLM's representations. Quick check: How does the choice of k affect the bias-variance tradeoff in a kNN classifier?

## Architecture Onboarding

- **Component map**: Input text -> MPLM -> MASK Representation -> Retrieval Projection Layer -> kNN Retrieval -> CD Module (reweighted kNN dist) -> DWE Module (interpolation weights) -> Final Distribution -> Prediction

- **Critical path**: The MASK token representation flows through the retrieval projection layer, then kNN retrieval finds similar examples, the CD module reweights these neighbors based on confidence, the DWE module determines interpolation weights, and the final distribution combines these signals for prediction.

- **Design tradeoffs**: The backbone is frozen to preserve general knowledge and prevent overfitting in low-data regimes, at the cost of limited adaptation. The training data must be split between populating the datastore and training the projection, CD, and DWE modules.

- **Failure signatures**: High variance or accuracy collapse indicates overfitting of trainable modules. Weak kNN signal suggests poor cross-lingual alignment or sparse datastore for the target domain.

- **First 3 experiments**: 1) Reproduce main results for a single language pair and run ablation study by removing modules. 2) Visualize t-SNE plots of source data in raw vs. learned projection space to verify clustering improvement. 3) Vary temperature Ï„ and Kmax values to plot accuracy and ECE sensitivity, finding robust settings.

## Open Questions the Paper Calls Out

- **Decoder-only models**: The method was not tested on decoder-only multilingual models like XGLM, BLOOM, or ChatGPT due to computational resource limitations. How does N2C2 perform on these architectures?

- **Generation and multi-choice tasks**: Extending to tasks without fixed verbalizers is difficult because the method relies on a fixed set of label mappings. Can the framework work for multilingual generation or multi-choice QA?

- **Demonstration retrieval integration**: The study used random 1-shot selection due to length constraints. How would performance change with longer demonstrations or integration with demonstration-based retrieval methods?

## Limitations

- The method assumes semantically similar source-language examples can reliably guide target-language predictions, which may fail with poor cross-lingual alignment or sparse datastores.
- The confidence-aware module's reliance on the MPLM's predicted probabilities introduces a potential feedback loop where miscalibrated base predictions could be amplified.
- Generalizability of the learned retrieval projection to unseen languages and domains is uncertain, as the paper doesn't test transfer to entirely new languages or tasks.

## Confidence

- **High confidence**: kNN-augmented ICL improves accuracy and ECE over vanilla ICL (supported by ablation study and consistent cross-language improvements)
- **Medium confidence**: The confidence-aware distribution module's specific mechanism is plausible but lacks deep analysis of its interaction with base model calibration issues
- **Low confidence**: The learned retrieval projection's generalizability to new languages and domains is uncertain without transfer testing

## Next Checks

1. **Datastore Quality Analysis**: Conduct ablation studies varying the size and diversity of the source-language datastore, measuring accuracy and ECE as a function of unique source examples per class to quantify sensitivity to sparsity.

2. **Cross-Lingual Alignment Evaluation**: Visualize the retrieval space using t-SNE or UMAP for multiple language pairs, plotting source examples with shared labels to assess cross-lingual semantic alignment quality and identify failure modes.

3. **Base Model Calibration Stress Test**: Evaluate N2C2 on a miscalibrated MPLM (fine-tuned to be overconfident), measuring whether the confidence-aware module mitigates or exacerbates calibration errors to validate its interaction with base model confidence signals.