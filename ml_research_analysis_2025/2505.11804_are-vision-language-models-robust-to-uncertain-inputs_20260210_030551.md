---
ver: rpa2
title: Are vision language models robust to uncertain inputs?
arxiv_id: '2505.11804'
source_url: https://arxiv.org/abs/2505.11804
tags:
- inputs
- uncertainty
- level
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether vision language models (VLMs)
  can reliably quantify uncertainty when faced with ambiguous or anomalous inputs.
  The authors evaluate VLMs on two classic uncertainty quantification tasks: anomaly
  detection (identifying inputs outside predefined categories) and classification
  under inherently ambiguous conditions.'
---

# Are vision language models robust to uncertain inputs?

## Quick Facts
- arXiv ID: 2505.11804
- Source URL: https://arxiv.org/abs/2505.11804
- Reference count: 40
- Primary result: VLMs show improved robustness to corrupted inputs but still require explicit rejection prompting to reliably express uncertainty, with caption diversity offering a novel uncertainty signal

## Executive Summary
This paper investigates whether vision language models can reliably quantify uncertainty when faced with ambiguous or anomalous inputs. The authors evaluate VLMs on two classic uncertainty quantification tasks: anomaly detection (identifying inputs outside predefined categories) and classification under inherently ambiguous conditions. They find that while newer and larger VLMs show improved robustness compared to earlier models, they still tend to hallucinate confident responses even when faced with unclear inputs. A key finding is that simply prompting VLMs to abstain from uncertain predictions enables significant reliability gains, achieving near-perfect robustness for natural images like ImageNet. However, for domain-specific tasks like galaxy morphology classification, VLMs fail to reliably estimate uncertainty due to insufficient specialized knowledge.

## Method Summary
The study evaluates black-box VLMs (GPT4o-mini, Llama 3.2, Qwen 2/2.5 variants) on two uncertainty quantification tasks: anomaly detection and classification with rejection under ambiguous/corrupted inputs. The key intervention is appending a rejection instruction to prompts (e.g., "if you find an image very ambiguous and cannot confidently classify it, return 'unknown'"). For caption diversity analysis, the authors generate 20 independent captions per image using stochastic decoding (temperature=0.6, top-P=0.95, top-K=50), embed them with sentence transformers, and compute 1 - mean pairwise cosine similarity. Evaluation datasets include ImageNet-C (corrupted images), CIFAR-10 vs. non-CIFAR-10 images, PTB-XL ECG signals, and Galaxy Zoo 2 subset.

## Key Results
- Newer and larger VLMs exhibit improved baseline robustness to corrupted inputs compared to earlier models
- Explicit rejection prompting enables near-perfect accuracy on classified samples in natural images like ImageNet
- Caption diversity correlates with model uncertainty, with rejected samples showing higher diversity than classified samples
- VLMs fail to reliably estimate uncertainty in domain-specific tasks like galaxy morphology due to insufficient specialized knowledge

## Why This Works (Mechanism)

### Mechanism 1: Explicit Rejection Prompting Activates Latent Uncertainty Awareness
VLMs possess implicit awareness of input ambiguity but require explicit instruction to express uncertainty through rejection rather than forced classification. The model's instruction-following tendency (trained via RLHF/instruction tuning) causes it to comply with classification requests even when internally uncertain. Adding a rejection option to the prompt "releases" this constraint, allowing the model to abstain when internal confidence is low. This fails when models lack domain-specific knowledge (galaxy morphology: caption diversity remains flat, rejection doesn't improve accuracy).

### Mechanism 2: Caption Diversity Reveals Internal Uncertainty
The semantic diversity of independently-generated captions under stochastic decoding correlates with model uncertainty about an input. When a model is uncertain about visual content, stochastic sampling explores multiple plausible interpretations, yielding diverse captions. When confident, sampling converges on similar descriptions. This mechanism breaks down when domain knowledge is absent, as galaxy images with high annotator disagreement still produce low-diversity captions, indicating the model doesn't perceive the ambiguity.

### Mechanism 3: Scaling Improves Robustness Baseline But Not Uncertainty Expression
Larger and newer VLMs show improved robustness to corrupted inputs, but all models still require explicit rejection prompting to reliably express uncertainty. Scaling improves visual recognition and generalization but does not inherently change instruction-following behavior that suppresses uncertainty expression. Scale alone cannot compensate for missing domain expertise (galaxy morphology, specialized ECG analysis).

## Foundational Learning

- **Out-of-Distribution (OOD) / Anomaly Detection**: The paper frames VLM uncertainty through classic OOD detection tasks—identifying inputs outside predefined categories. Quick check: If a model trained on CIFAR-10 receives an SVHN digit, should it (a) classify into nearest CIFAR-10 class, (b) output low confidence, or (c) reject classification entirely? (Correct: c or b, not a)

- **Calibration vs. Rejection (Selective Classification)**: The paper doesn't evaluate continuous calibration (confidence proportional to correctness) but binary rejection (classify vs. abstain). These are distinct uncertainty quantification approaches. Quick check: A model achieves 90% accuracy on classified samples and rejects 30% of inputs. Is this well-calibrated? (Insufficient info—calibration requires confidence scores, not just rejection rates)

- **Instruction Tuning and Compliance Bias**: The paper's core claim is that VLMs' instruction-following training causes them to suppress uncertainty expression unless explicitly permitted to abstain. Quick check: Why might a model classify a random noise image as "cat" rather than saying "unknown"? (Instruction-following bias encourages task completion over uncertainty acknowledgment)

## Architecture Onboarding

- **Component map**: Input layer (Image + text prompt) -> VLM backbone (black-box models) -> Caption diversity module (stochastic sampling + embedding) -> Output parsing (label vs. "unknown")
- **Critical path**: Construct prompt with category list + rejection option -> Query VLM with deterministic or stochastic decoding -> Parse response for label vs. rejection -> For caption diversity: sample 20 captions, embed, compute diversity score -> Correlate diversity score with rejection behavior
- **Design tradeoffs**: Prompting strategy (Simple vs. Direct vs. Caption & Answer), temperature tuning (0.6 compromise), domain applicability (natural images work well; specialized domains fail)
- **Failure signatures**: Over-rejection (GPT4o-mini on ECG: recall=1.0 but precision=0.36), under-rejection (Llama 3.2 on ECG: recall=0.31), flat caption diversity under ambiguity (galaxy images), hallucination without rejection prompt (accuracy collapse under corruption)
- **First 3 experiments**: 1) Baseline corruption robustness: Prompt VLM to classify 1,000 ImageNet-C images without rejection option, plot accuracy vs. corruption level. 2) Rejection prompt ablation: Repeat with rejection instruction, compare accuracy on non-rejected samples and rejection rates. 3) Caption diversity correlation: For same ImageNet-C images, sample 20 captions, compute diversity scores, correlate with rejection behavior and corruption level.

## Open Questions the Paper Calls Out

- How can VLMs be augmented with domain-specific expert knowledge to enable reliable uncertainty quantification in specialized fields (e.g., galaxy morphology, medical imaging)?
- What mechanisms underlie the observed difference in prompt-sensitivity between older VLMs (Llama 3.2, Qwen 2) and newer models (GPT4o-mini, Qwen 2.5 72B)?
- Does caption diversity generalize as a reliable uncertainty signal for other VLM architectures, modalities, or beyond classification tasks?
- What are the failure modes of rejection-based uncertainty quantification when VLMs over-reject valid inputs versus hallucinate on anomalous ones?

## Limitations

- Domain knowledge dependency: VLMs fail to reliably quantify uncertainty in specialized domains where they lack sufficient domain-specific knowledge
- Black-box methodology constraints: Reliance on prompt engineering and output analysis without model internals limits mechanistic understanding
- Dataset construction ambiguity: Exact 1,000-image subsets for ImageNet-C experiments not specified, affecting reproducibility
- Prompt sensitivity: Older and smaller models show significant prompt sensitivity, raising questions about whether observed behavior reflects true uncertainty awareness

## Confidence

- **High confidence**: Newer and larger VLMs show improved baseline robustness to corrupted inputs compared to earlier models
- **Medium confidence**: Explicit rejection prompting enables significant reliability gains across natural images
- **Low confidence**: Caption diversity reliably reveals internal uncertainty across all domains and model types

## Next Checks

1. Cross-domain caption diversity validation: Test whether caption diversity correlates with annotator disagreement in specialized domains beyond galaxy morphology
2. Internal state probing: Use available API features to probe whether caption diversity reflects genuine uncertainty or sampling artifacts
3. Human uncertainty alignment: Compare VLM rejection decisions with human uncertainty ratings on the same ambiguous inputs across different corruption types and domains