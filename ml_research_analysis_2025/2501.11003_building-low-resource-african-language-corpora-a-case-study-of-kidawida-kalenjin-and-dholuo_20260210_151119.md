---
ver: rpa2
title: 'Building low-resource African language corpora: A case study of Kidawida,
  Kalenjin and Dholuo'
arxiv_id: '2501.11003'
source_url: https://arxiv.org/abs/2501.11003
tags:
- language
- languages
- data
- african
- kalenjin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study developed linguistic corpora for three under-resourced\
  \ Kenyan languages\u2014Kidaw'ida, Kalenjin, and Dholuo\u2014using selective crowdsourcing\
  \ to collect text and speech data from native speakers. The project produced 30,000\
  \ text sentences per language, translated into Kiswahili to create parallel corpora,\
  \ and gathered speech recordings (56 hours for Kidaw'ida, 92 hours for Kalenjin,\
  \ and 120 hours for Dholuo) on Mozilla Common Voice."
---

# Building low-resource African language corpora: A case study of Kidawida, Kalenjin and Dholuo

## Quick Facts
- arXiv ID: 2501.11003
- Source URL: https://arxiv.org/abs/2501.11003
- Reference count: 18
- Primary result: Developed 30K text sentences + 56-120 hours of speech per language for three Kenyan languages using selective crowdsourcing

## Executive Summary
This study presents a methodology for building linguistic corpora for three under-resourced Kenyan languages—Kidaw'ida, Kalenjin, and Dholuo—through selective crowdsourcing. The approach recruited proficient native speakers and paid Data Collection Leads to ensure quality control during text collection and translation into Kiswahili to create parallel corpora. Speech data was gathered on Mozilla Common Voice after extensive platform localization. The resulting datasets are publicly available on Zenodo and Mozilla Common Voice, aiming to support NLP applications and promote linguistic diversity in AI systems for African languages.

## Method Summary
The methodology involved recruiting Data Collection Leads (DCLs) with high language proficiency who then identified qualified contributors from their networks. Contributors wrote original sentences or transcribed recorded conversations, which DCLs reviewed for spelling, grammar, fluency, and translation accuracy. Each language collected 30,000 text sentences, translated into Kiswahili for parallel corpus creation. For speech data, the team completed Mozilla Common Voice platform localization (translating 1,000+ technical strings) before uploading sentences and gathering voice recordings from diverse speakers, targeting gender balance. All data was released under CC0 license.

## Key Results
- Created 30,000 text sentences per language with Kiswahili translations for parallel corpora
- Collected 56 hours of speech for Kidaw'ida, 92 hours for Kalenjin, and 120 hours for Dholuo on Mozilla Common Voice
- Achieved gender balance with 7:5 female:male ratio among Data Collection Leads
- Published datasets on Zenodo and Mozilla Common Voice under CC0 license

## Why This Works (Mechanism)

### Mechanism 1: Selective Crowdsourcing with Proficiency-Gated Quality Assurance
Recruiting known, proficient contributors through trusted networks balances data generation velocity with quality control. Data Collection Leads identify qualified contributors and review all submissions for spelling, grammar, fluency, and translation accuracy before data release.

### Mechanism 2: Bridge-Language Parallel Corpora for Cross-Lingual Transfer
Translating indigenous language text into Kiswahili creates parallel corpora enabling machine translation and cross-lingual transfer learning. Contributors proficient in both languages translate sentences bidirectionally to create aligned sentence pairs suitable for NMT training.

### Mechanism 3: Platform Piggybacking with High Upfront Localization Cost
Leveraging established open platforms (Mozilla Common Voice) despite steep localization requirements provides long-term infrastructure, discoverability, and community contribution pathways. This requires translating over 1,000 technical strings into each language, including inventing terminology for concepts without existing translations.

## Foundational Learning

- **Low-resource language constraints in NLP**: Understanding why standard approaches (web crawling, large pretrained models) fail for these languages contextualizes methodology choices. *Quick check*: Can you name two reasons web crawling fails for Kidaw'ida?
- **Crowdsourcing vs. selective recruitment tradeoffs**: The paper's core methodological innovation is "selective" crowdsourcing—understanding when this breaks is critical for scaling decisions. *Quick check*: What quality risk does open crowdsourcing introduce that selective recruitment mitigates?
- **Parallel corpora and pivot languages**: The Kiswahili translation strategy assumes cross-lingual transfer is viable; understanding this assumption helps evaluate downstream application feasibility. *Quick check*: Why translate to Kiswahili rather than English directly?

## Architecture Onboarding

- **Component map**: Contributors → Google Sheets → DCL Review → GitHub → Zenodo (CC0); Text Corpus → Pontoon Localization → Mozilla Common Voice → Voice Recordings → MCV Download
- **Critical path**: 1) Recruit DCLs, 2) Collect 30K sentences/language, 3) Translate → Kiswahili, 4) Complete Pontoon localization, 5) Upload sentences to MCV → Begin voice recording, 6) Release on Zenodo + MCV
- **Design tradeoffs**: Selective vs. open crowdsourcing (quality + trust vs. scale + speed); MCV vs. custom platform (high localization cost + existing infrastructure vs. control + flexibility); Kiswahili vs. English bridge (contributor accessibility vs. international developer accessibility)
- **Failure signatures**: Contributor enthusiasm doesn't translate to sustained participation; DCL review bottleneck as data volume grows; Pontoon localization incomplete → MCV launch blocked; Gender imbalance in voice data → biased ASR
- **First 3 experiments**: 1) Train sequence-to-sequence model on Kidaw'ida↔Kiswahili parallel corpus; report BLEU scores, 2) Train ASR on 56-120 hours/language; analyze error patterns by speaker demographics, 3) Track contributor participation over 3 months; identify dropout points and test intervention effectiveness

## Open Questions the Paper Calls Out

- **Scalability for LLM training**: Is selective crowdsourcing scalable for creating the massive datasets (trillions of tokens) required for modern Large Language Models? The approach ensures quality but may not be sustainable for large projects needing data at the scale of Llama 2 (2 trillion tokens).
- **Baseline accuracy levels**: What baseline accuracy levels can NLP models achieve when trained exclusively on the 30,000 sentences and limited hours of speech collected? The paper states datasets should establish a baseline but does not include results from training or evaluating models.
- **Code-switching impact**: How does the removal of code-switched terms during transcription affect real-world applicability of resulting speech models? By removing code-switching, the corpus may not represent natural speech patterns, potentially limiting ASR system robustness in actual conversational contexts.

## Limitations

- Selective crowdsourcing methodology creates fundamental scalability bottlenecks due to reliance on DCLs' personal networks for contributor recruitment
- Translation quality into Kiswahili and utility of resulting parallel corpora for downstream NLP tasks remain unvalidated
- Effectiveness of Kiswahili as bridge language for cross-lingual transfer remains theoretical without empirical validation

## Confidence

- **High Confidence**: Corpus collection methodology successfully implemented; produced measurable outputs (30K sentences, 56-120 hours of speech per language); Mozilla Common Voice integration process is detailed and technically sound
- **Medium Confidence**: Claim that corpora will support NLP applications is plausible but unverified; gender balance achievement reported but not independently verified
- **Low Confidence**: Scalability of selective crowdsourcing beyond these three languages or to larger dataset sizes is speculative; effectiveness of Kiswahili as bridge language remains theoretical

## Next Checks

1. **Machine Translation Baseline Evaluation**: Train sequence-to-sequence model on Kidaw'ida↔Kiswahili parallel corpus; report BLEU scores to establish practical NLP application utility
2. **ASR Error Analysis by Demographics**: Train automatic speech recognition models on 56-120 hours of speech data per language; analyze error patterns across different speaker demographics to validate gender balance translates to unbiased recognition performance
3. **Community Retention Cohort Analysis**: Track contributor participation patterns over 3-month period post-corpus completion; identify dropout points and test whether targeted interventions can improve sustained engagement for future data collection efforts