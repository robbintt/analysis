---
ver: rpa2
title: 'PolicyPad: Collaborative Prototyping of LLM Policies'
arxiv_id: '2509.19680'
source_url: https://arxiv.org/abs/2509.19680
tags:
- policy
- prototyping
- experts
- policies
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PolicyPad enables domain experts to collaboratively prototype\
  \ LLM policies by integrating UX prototyping methods like rapid iteration, scenario-based\
  \ testing, and real-time collaboration. In workshops with 22 mental health and legal\
  \ experts, PolicyPad led to 4\xD7 more novel policy statements than a baseline system,\
  \ with 51.9% novelty versus 18.2%."
---

# PolicyPad: Collaborative Prototyping of LLM Policies

## Quick Facts
- arXiv ID: 2509.19680
- Source URL: https://arxiv.org/abs/2509.19680
- Reference count: 40
- Key outcome: 4× more novel policy statements (51.9% novelty vs 18.2%) in workshops with mental health and legal experts

## Executive Summary
PolicyPad is a collaborative system that enables domain experts to prototype LLM policies through rapid iteration, scenario-based testing, and real-time collaboration. The system combines a collaborative editor with a policy-informed model, allowing experts to draft policies and immediately observe their effects on model behavior. In workshops with 22 mental health and legal experts, PolicyPad generated 4× more novel policy statements than a baseline system, with experts valuing the ability to experiment directly with model responses and collaborate through shared scenarios.

## Method Summary
The system was evaluated through a within-subjects study with 22 domain experts (10 mental health, 12 legal) in 8 groups. Participants used PolicyPad and a baseline system across two 90-minute sessions, with conditions counterbalanced. The system features a collaborative editor (TipTap), scenario gallery, policy-informed model (Llama 3.3 70B), automated heuristics evaluation (o4-mini), and policy suggestion generation. Novelty was measured by comparing generated policies against existing standards using GPT-4.1 with human verification.

## Key Results
- PolicyPad produced 4× more novel policy statements (51.9% novelty vs 18.2% baseline)
- 100% of policy suggestions were accepted during studies, even imperfect ones that sparked further ideas
- Real-time collaboration amplified experts' abilities to draw upon expertise and iteratively refine policies
- Novel policies offered specific guidance on deferring to human experts, crisis procedures, and proactive information elicitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scenario-based grounding surfaces latent expert knowledge that would otherwise remain tacit during policy design.
- Mechanism: Concrete user-AI conversation examples serve as boundary objects that provoke domain-specific reasoning and pattern recognition. Experts identify recurring problems in model responses and translate them into explicit policy statements through discussion.
- Core assumption: Experts possess domain knowledge that is activated by realistic scenarios but difficult to articulate in abstract policy terms without such grounding.
- Evidence anchors:
  - [section 3.2.3]: "Exploring scenarios helped experts spot recurring problems in model responses and turn them into clear policies... 'I keep seeing this thing over and over and it's incorrect, so that needs to be a rule.'"
  - [section 7.1.3]: Experts could "immediately observe the impact their edits had on the response" when testing against scenarios.

### Mechanism 2
- Claim: Tight feedback loops between policy drafting and model experimentation reduce the abstraction distance between expert intent and model behavior.
- Mechanism: Experts draft policy statements, immediately test against a policy-informed model, and observe behavioral changes. This iterative cycle allows rapid validation or invalidation of policy hypotheses.
- Core assumption: The policy-informed model provides sufficiently representative behavior for experts to evaluate policy effectiveness.
- Evidence anchors:
  - [abstract]: "PolicyPad enables domain experts to collaboratively prototype LLM policies by integrating UX prototyping methods like rapid iteration, scenario-based testing, and real-time collaboration."
  - [section 3.2.1]: Experts in the observational study "failed to verify whether and how those visions actually came into fruition because they did not have a 'policy-informed' model to interact with."

### Mechanism 3
- Claim: Collaborative affordances (spotlight scenarios, shared editing) amplify individual contributions through peer feedback and collective sensemaking.
- Mechanism: Features like scenario spotlighting allow experts to share observations, co-edit model responses, and converge on policy statements. Real-time collaboration enables rapid resolution of disagreements and builds consensus.
- Core assumption: Small groups (2-4 experts) provide sufficient diversity of perspective without overwhelming individual voices.
- Evidence anchors:
  - [section 7.1.4]: "Real-time collaboration amplified participants' abilities to draw upon their expertise, challenge assumptions, and iteratively refine policies."
  - [section 7.1.2]: "100% of the policy suggestions were accepted during our studies... P7 shared why they accepted policy suggestions even when they seemed imperfect: 'I liked the policy suggestions, even if they weren't necessarily dead on. It gave us ideas for other [policy statements].'"

## Foundational Learning

- Concept: **Prototyping fidelity spectrum**
  - Why needed here: PolicyPad focuses on low-fidelity policy prototypes (high-level guidance) rather than production-ready policies. Understanding this distinction prevents premature optimization of wording or implementation details.
  - Quick check question: Can you explain why experts should focus on "what behavior is desirable" rather than "how to phrase it perfectly" during prototyping?

- Concept: **Policy-informed model behavior**
  - Why needed here: The system uses a frontier LLM (Llama 3.3 70B) with the drafted policy as a system prompt. This approach assumes the model can follow policy instructions sufficiently to demonstrate behavioral intent.
  - Quick check question: What are the limitations of using a base model with system prompts versus a model fine-tuned on policy-aligned data?

- Concept: **Heuristic evaluation in policy design**
  - Why needed here: PolicyPad introduces heuristics (e.g., "clarity," "incorporate real-world practices") that persist across policy iterations. These guide quality without constraining content.
  - Quick check question: How do heuristics differ from policy statements in their function during prototyping?

## Architecture Onboarding

- Component map:
  1. Collaborative editor (TipTap) -> Scenario gallery/sidebar -> Policy-informed model endpoint (Llama 3.3 70B) -> Heuristics evaluator (o4-mini) -> Policy suggestion generator (o4-mini)

- Critical path:
  1. Expert browses scenarios in gallery → identifies behavioral issues
  2. Expert drafts/edits policy statements in collaborative editor
  3. Expert tests changes via regenerate or conversation extension in sidebar
  4. Expert may spotlight scenario for group discussion and co-editing
  5. Group saves policy snapshot → heuristics evaluation runs → responses update
  6. Iterate until session ends

- Design tradeoffs:
  - **Private sidebar vs. shared spotlight**: Balances individual exploration with group alignment. Too much privacy fragments understanding; too much sharing constrains independent testing.
  - **Automated heuristics vs. human judgment**: Automated checks draw attention but may miss nuance. Overridden by group consensus.
  - **Low-fidelity policy focus**: Prioritizes breadth of exploration over depth of specification. May require translation to high-fidelity policies for deployment.

- Failure signatures:
  - **Stale responses**: Policy changes don't propagate to scenario responses (check: snapshot triggered? model endpoint reachable?)
  - **Collaboration deadlock**: Experts avoid disagreeing openly (observed in study: "participants tended to agree with others in their group and rarely challenged")
  - **Heuristic misalignment**: Experts reject heuristics as irrelevant (e.g., legal experts questioning applicability of professional ethics to AI)

- First 3 experiments:
  1. **Scenario diversity test**: Vary scenario realism (synthetic vs. log-derived) and measure novelty of resulting policy statements. Hypothesis: More realistic scenarios yield higher novelty.
  2. **Collaboration size test**: Compare policy novelty across group sizes (2, 3, 4, 5+). Hypothesis: Novelty peaks at 3 and declines as group dynamics shift.
  3. **Feedback loop latency test**: Introduce artificial delay (0s, 5s, 30s) in policy-to-response updates and measure expert frustration and iteration count. Hypothesis: Latency >5s disrupts flow and reduces iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can low-fidelity policy prototypes be effectively translated into high-fidelity policies suitable for RLHF or RLAIF pipelines?
- Basis in paper: [explicit] Section 4 states the work focuses on low-fidelity prototypes and "leave[s] the translation of low- to high-fidelity policies to future work."
- Why unresolved: PolicyPad currently prioritizes rapid ideation and perspective-sharing over creating production-ready artifacts with polished legal wording or implementation details.
- What evidence would resolve it: A study measuring the alignment performance of models trained on policies converted from PolicyPad prototypes versus expert-written benchmarks.

### Open Question 2
- Question: How do policy outputs differ when generated by laypeople compared to domain experts using collaborative prototyping tools?
- Basis in paper: [explicit] Section 9 notes future work should "run policy prototyping sessions with the members of the general public to pinpoint key similarities and differences."
- Why unresolved: The current evaluation is limited to specialized experts, yet deployed LLMs must serve a broader public with potentially conflicting values or risk tolerances.
- What evidence would resolve it: A comparative analysis of policy statements generated by expert groups versus lay groups for the same high-stakes scenarios.

### Open Question 3
- Question: To what extent do expert-generated policies using PolicyPad generalize to non-Western cultural and legal contexts?
- Basis in paper: [explicit] Section 9 states the perspectives are "heavily influenced by the American... systems" and "may not generalize to other countries."
- Why unresolved: 20 of the 22 participants were U.S.-based, potentially embedding Western-centric assumptions about mental health and legal advice into the resulting policies.
- What evidence would resolve it: Replicating the study with non-Western experts to identify cultural divergences in the policy statements regarding safety and advice.

## Limitations
- Generalizability limited to mental health and legal domains with U.S.-based experts
- Novelty measurement may underestimate true novelty due to finite comparison corpus
- Results dependent on frontier LLM performance and may not replicate with smaller models

## Confidence
- High confidence in collaborative design benefits: Strong evidence from multiple sources and consistent qualitative feedback
- Medium confidence in novelty measurement: Methodology sound but comparison corpus may not capture all relevant policies
- Medium confidence in scenario effectiveness: Well-supported mechanism but primarily from single study

## Next Checks
1. Conduct domain transferability study: Replicate core experiments with experts from three additional high-stakes domains (healthcare, finance, education) to assess generalizability.
2. Implement cross-validation of novelty measurement: Have domain experts blind to the study evaluate novelty scores on a held-out dataset.
3. Test model dependency: Compare PolicyPad's effectiveness using different model families (small vs. large, open vs. closed source).