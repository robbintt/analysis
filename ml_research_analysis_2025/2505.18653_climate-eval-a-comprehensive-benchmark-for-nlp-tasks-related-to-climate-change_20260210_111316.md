---
ver: rpa2
title: 'Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change'
arxiv_id: '2505.18653'
source_url: https://arxiv.org/abs/2505.18653
tags:
- climate
- classification
- tasks
- dataset
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ClimateEval, a comprehensive benchmark for
  evaluating NLP models on climate change-related tasks. The benchmark aggregates
  13 datasets to create 25 tasks spanning text classification, question answering,
  and information extraction.
---

# Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change

## Quick Facts
- **arXiv ID:** 2505.18653
- **Source URL:** https://arxiv.org/abs/2505.18653
- **Reference count:** 9
- **Primary result:** Introduces ClimateEval, a unified benchmark of 25 tasks from 13 datasets, showing few-shot prompting generally improves LLM performance on climate-related NLP tasks, though some tasks remain challenging.

## Executive Summary
This paper introduces ClimateEval, a comprehensive benchmark for evaluating NLP models on climate change-related tasks. The benchmark aggregates 13 datasets to create 25 tasks spanning text classification, question answering, and information extraction. A new dataset of climate news articles from The Guardian is also introduced. The authors evaluate open-source LLMs (2B-70B parameters) in zero-shot and few-shot settings, finding that few-shot prompting generally improves performance but tasks like climate-specific NER and claim detection remain challenging. The unified benchmark enables standardized evaluation of LLMs across diverse climate-related tasks.

## Method Summary
ClimateEval provides a unified evaluation benchmark that standardizes task formulations, label sets, and prompts across 13 datasets to create 25 tasks. The evaluation uses the LM Evaluation Harness framework with zero-shot and 5-shot prompting. Classification tasks use log-likelihood ranking of label strings, while NER tasks generate JSON output. Models ranging from 2B to 70B parameters are evaluated, with larger models using 4-bit quantization. The benchmark includes a new Guardian Climate News Corpus with 10 classification labels covering 40,000 articles.

## Key Results
- Few-shot prompting consistently improves performance on classification tasks, with the most significant gains observed in Climate Stance detection
- Tasks requiring external knowledge like Climate-FEVER (fact verification) and PIRA (multiple-choice QA) show no improvement from few-shot examples
- NER and claim detection tasks remain challenging even with few-shot prompting
- The unified benchmark provides standardized evaluation across diverse climate-related tasks

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning via Few-Shot Prompting
- Claim: Few-shot prompting improves model performance on climate-related classification tasks by providing examples that illustrate the task's schema, but its effectiveness is constrained by the task's reliance on external knowledge.
- Mechanism: The prompt includes labeled examples that guide the model's generation toward the correct output space, reducing ambiguity in the classification task.
- Core assumption: The model possesses sufficient prior knowledge to generalize from a small number of examples to unseen instances without updating its weights.
- Evidence anchors:
  - [abstract] "finding that few-shot prompting generally improves performance"
  - [section 4.1] "few-shot prompting consistently improves performance (Figure 2). The most significant gains are observed in Climate Stance... Another set of tasks that do not benefit from in-context learning includes Climate-FEVER (fact verification) and PIRA (multiple-choice QA)... which show no improvement."
  - [corpus] A related paper, "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries," discusses instruction tuning for a similar domain.
- Break condition: The mechanism fails for tasks requiring specific external knowledge (e.g., Climate-FEVER), as few-shot examples cannot provide the necessary factual information.

### Mechanism 2: Unified Benchmark for Standardized Evaluation
- Claim: Aggregating diverse datasets into a single framework with standardized task formulations enables reproducible and comparable assessment of LLMs.
- Mechanism: The ClimateEval benchmark unifies 13 datasets into 25 tasks, providing a consistent evaluation suite via the LM Evaluation Harness, which addresses inconsistencies in prior work.
- Core assumption: The chosen prompt templates and task formulations are sufficiently neutral and do not introduce systematic bias favoring specific model architectures.
- Evidence anchors:
  - [abstract] "This results in a benchmark of 25 tasks... Our benchmark provides a standardized evaluation suite for systematically assessing the performance of large language models (LLMs)"
  - [section 3.1] "ClimateEval provides a unified evaluation benchmark that standardizes task formulations, label sets, and prompts."
  - [corpus] Corpus context shows this builds directly on prior efforts like ClimaBench, which lacked this unification.
- Break condition: The mechanism assumes the selected datasets are representative of "climate discourse," which may not hold if important genres or languages are underrepresented.

### Mechanism 3: Constrained Decoding for Classification
- Claim: Formulating classification tasks as a choice between label strings, ranked by log-likelihood, provides more stable evaluation than open-ended generation.
- Mechanism: For each input, the model computes the probability of each possible label string (e.g., "supports", "refutes") and selects the highest-scoring one, eliminating variability in free-form text.
- Core assumption: The model's token probabilities are a reliable proxy for its semantic understanding of the label categories.
- Evidence anchors:
  - [section 4] "For classification tasks, the log-likelihoods of each possible label are calculated, and the label with the highest likelihood is selected as the model's prediction."
  - [corpus] No direct corpus evidence was found for this specific evaluation strategy.
- Break condition: This method may fail if labels have different token lengths or if the model's tokenizer affects likelihoods unevenly, requiring careful normalization.

## Foundational Learning

- Concept: **Zero-shot vs. Few-shot Learning**
  - Why needed here: The paper's core experimental comparison evaluates models under these two settings. Understanding the difference is essential to interpret the reported performance gains.
  - Quick check question: Does providing examples in the prompt (few-shot) improve performance on all types of tasks, or just some?

- Concept: **Log-Likelihood Evaluation**
  - Why needed here: This is the specific method used to score models on most tasks in the benchmark, distinguishing it from generative evaluation.
  - Quick check question: When evaluating a model on a multiple-choice question, do you ask it to generate an answer or to score each option?

- Concept: **Domain-Specific Benchmarking**
  - Why needed here: The paper's primary contribution is creating a specialized benchmark. Understanding why general benchmarks are insufficient for climate tasks motivates the work.
  - Quick check question: Why might a model's performance on general NLP tasks not translate to performance on climate-specific text?

## Architecture Onboarding

- Component map: Source Datasets (13 raw collections) -> Task Configurations (25 YAML files defining prompt templates and label mappings) -> Evaluation Harness (lm-eval engine) -> Metrics (Macro-F1, accuracy, Span/Type F1)

- Critical path: 1) Install lm-evaluation-harness. 2) Clone the ClimateEval repository containing task YAML files. 3) Execute the one-line lm-eval command, specifying the model and the climatEval tasks.

- Design tradeoffs: The design prioritizes breadth of coverage (25 tasks) over depth of generative evaluation, as most tasks are classification-based. It also trades evaluation control (via constrained decoding) for ecological validity of how a user might prompt a model in the wild.

- Failure signatures: Be alert for poor performance on tasks with many labels (e.g., SciDCC with 20 labels), where likelihood scores may become indistinguishable. Also, watch for tasks like PIRA "without context," where high zero-shot performance may suggest data contamination or insufficient difficulty.

- First 3 experiments:
  1. **Baseline Validation**: Run evaluation on a single task (climatext_sentences) with a small model (gemma-2-2b) in both 0-shot and 5-shot modes to confirm the pipeline works and reproduce a result from Table 2.
  2. **Ablation on Knowledge**: Compare performance on pira_w_ctx (with context) vs. pira_wo_ctx (without context) to quantify how much a model relies on retrieved context versus internal knowledge.
  3. **Domain Adaptation Check**: Compare ClimateGPT-7b against its base model Llama-2-7b on the exeter_claim task to test the impact of climate-specific pre-training on a misinformation detection task.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark primarily focuses on English language tasks, limiting its applicability to multilingual climate discourse
- The constrained decoding approach may not fully capture a model's ability to generate nuanced responses in open-ended scenarios
- The selection of datasets may not comprehensively represent all aspects of climate discourse, potentially introducing bias

## Confidence
High confidence in the benchmark's methodology and results, based on the clear experimental design and standardized evaluation framework.

## Next Checks
1. Verify the Guardian dataset can be accessed via the HuggingFace path and reproduces the reported classification results
2. Test the lm-eval pipeline with a 7B model on 2-3 tasks to confirm the evaluation framework works as specified
3. Compare few-shot performance on pira_w_ctx vs pira_wo_ctx to validate the claimed knowledge-dependence of these tasks