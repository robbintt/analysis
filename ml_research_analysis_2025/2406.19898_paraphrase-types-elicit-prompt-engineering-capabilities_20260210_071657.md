---
ver: rpa2
title: Paraphrase Types Elicit Prompt Engineering Capabilities
arxiv_id: '2406.19898'
source_url: https://arxiv.org/abs/2406.19898
tags:
- prompt
- link
- tasks
- task
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates how linguistic variations
  in prompts affect large language models (LLMs) across 120 tasks and five models.
  Using 26 paraphrase types across six categories (morphology, syntax, lexicon, lexico-syntax,
  discourse, others), the study measures performance changes when prompts are linguistically
  perturbed.
---

# Paraphrase Types Elicit Prompt Engineering Capabilities

## Quick Facts
- arXiv ID: 2406.19898
- Source URL: https://arxiv.org/abs/2406.19898
- Reference count: 40
- 26 paraphrase types across 6 categories systematically improve LLM performance by 5.5-6.7% median gain

## Executive Summary
This paper systematically investigates how linguistic variations in prompts affect large language models (LLMs) across 120 tasks and five models. Using 26 paraphrase types across six categories (morphology, syntax, lexicon, lexico-syntax, discourse, others), the study measures performance changes when prompts are linguistically perturbed. Results show significant potential for performance improvement through prompt adaptation: median gains of 6.7% for Mixtral 8x7B and 5.5% for LLaMA 3 8B were observed. Morphological and lexical changes showed the most promise in improving prompts, with gains ranging from 1.26% to 26% depending on task type. The study found that smaller models benefit more from prompt tuning than larger ones, and that performance gains can occur independent of prompt length, lexical diversity, or proximity to training data. These findings suggest that prompt engineering through controlled linguistic manipulation can substantially improve LLM task performance across diverse domains.

## Method Summary
The study evaluated 26 paraphrase types across 120 tasks using five different LLM models (Mixtral 8x7B, LLaMA 3 8B/70B, Mistral 7B, Qwen 7B). For each task, original prompts were paraphrased using a fine-tuned GPT-3.5-turbo-16k model trained on the ETPC dataset. Performance was measured using ROUGE-L scores on few-shot examples (3-6 per task) from the Super-NaturalInstructions dataset. The analysis examined performance differences across paraphrase types, model scales, task domains, and controlled for confounding factors including prompt length, lexical diversity, training data proximity, and temperature settings.

## Key Results
- Morphological and lexical paraphrase types yielded median performance gains of 6.7% (Mixtral 8x7B) and 5.5% (LLaMA 3 8B)
- Smaller models showed higher sensitivity to prompt variations, with LLaMA 3 8B gaining 21.1% to potentially outperform its 70B counterpart
- Performance improvements were task-dependent: sentiment classification benefited from polarity substitutions, while vocabulary-intensive tasks improved with lexical changes
- Gains occurred independent of prompt length, lexical diversity, and training data proximity, suggesting genuine capability enhancement

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Precision Enhancement
- Claim: Specific linguistic changes in prompts can unlock latent model capabilities without architectural changes.
- Mechanism: Targeted paraphrase types reduce ambiguity or better align instructions with patterns the model has learned during training, making task requirements more accessible to the model's inference process.
- Core assumption: Models have latent capacity for better performance that is constrained by suboptimal instruction phrasing rather than fundamental capability limits.
- Evidence anchors:
  - [abstract] "changes in morphology and lexicon, i.e., the vocabulary used, showed promise in improving prompts"
  - [section 4, Q1] "Morphology changes include changing modal verbs, which helps LLMs to follow instructions more clearly"
  - [corpus] Weak corpus support; related work (Same Question, Different Words) focuses on robustness rather than capability enhancement
- Break condition: Performance gains disappear when controlling for semantic equivalence, or when gains are fully explained by proximity to training data

### Mechanism 2: Model Scale Sensitivity
- Claim: Smaller models exhibit higher sensitivity to prompt linguistic variations, enabling them to match or exceed larger model performance with optimized prompting.
- Mechanism: Smaller models have more constrained decision boundaries, making them more responsive to linguistic perturbations that reduce task ambiguity. Larger models have more robust internal representations that are less dependent on precise phrasing.
- Core assumption: The sensitivity differential is due to architectural and training scale differences, not simply different training data distributions.
- Evidence anchors:
  - [abstract] "smaller models are more sensitive to prompt variations and can outperform larger ones when prompts are optimally adjusted"
  - [section 4, Q3] "LLaMA 3 8B has a lower baseline performance of 0.58, while the 70B model's baseline is at 0.65... LLaMA 3 8B could gain 21.1%, making the model markedly better than its 70B counterpart"
  - [corpus] No direct corpus evidence on scale-sensitivity relationships in prompting
- Break condition: Sensitivity differences disappear when controlling for training data and architecture family, or when larger models consistently outperform smaller ones regardless of prompt optimization

### Mechanism 3: Task-Domain-Linguistic Alignment
- Claim: Different task domains respond differentially to specific paraphrase types based on their linguistic requirements.
- Mechanism: Tasks have inherent linguistic structures that align better with certain paraphrase types (e.g., sentiment analysis requires polarity sensitivity, summarization benefits from discourse coherence markers, vocabulary-intensive tasks benefit from lexical precision).
- Core assumption: The observed task-type interactions are not random noise but reflect genuine task-linguistic dependencies.
- Evidence anchors:
  - [section 4, Q2] "sentiment classification seems to benefit from polarity substitutions and is sensitive to negation"
  - [section 4, Q2] "Lexical changes improve performance in vocabulary-intensive tasks such as named entity recognition and text entailment"
  - [corpus] Green Prompt Engineering paper suggests linguistic complexity affects model behavior systematically
- Break condition: Task-type interactions show no consistent patterns across multiple task families or are fully explained by prompt length/complexity rather than linguistic type

## Foundational Learning

- Concept: **Paraphrase Typology** (Vila et al., 2014)
  - Why needed here: Understanding the 26 paraphrase types across 6 families (morphology, syntax, lexicon, lexico-syntax, discourse, others) is essential for systematically generating and analyzing prompt variations.
  - Quick check question: Can you distinguish between a morphological change (modal verb: "should" → "must") and a lexico-syntactic change (synthetic/analytic substitution)?

- Concept: **Model Sensitivity vs. Robustness**
  - Why needed here: The paper shows smaller models are more sensitive to prompt changes (higher variance, higher gain potential). Understanding this tradeoff is crucial for choosing when to invest in prompt engineering vs. model scaling.
  - Quick check question: Given a 7B model with baseline 0.55 and a 70B model with baseline 0.65, under what conditions might the smaller model be preferable?

- Concept: **Confounding Factors in Prompt Evaluation**
  - Why needed here: The paper controls for prompt length, lexical diversity, training data proximity, and temperature. Understanding these controls is necessary to interpret whether observed gains are genuine or artifacts.
  - Quick check question: If you observe a 5% performance gain from a paraphrase, what three factors should you check before concluding the linguistic change caused the improvement?

## Architecture Onboarding

- Component map:
  Original prompt → GPT-3.5-turbo-16k (fine-tuned on ETPC dataset) → 26 paraphrase variants per prompt → Super-NaturalInstructions dataset (120 tasks, 24 families) → few-shot examples (3-6 per task) → ROUGE-L evaluation → performance difference computation, correlation analysis, training data proximity analysis

- Critical path: (1) Select task and original prompt → (2) Generate 26 paraphrase variants with type labels → (3) Run inference on task examples → (4) Compute ROUGE-L scores → (5) Calculate performance deltas by paraphrase type → (6) Aggregate across tasks/models

- Design tradeoffs:
  - **Median vs. Maximum gain reporting**: Median is more realistic but understates potential; maximum shows upper bound but is rarely achievable in practice
  - **Single paraphrase vs. ensemble**: Testing individual types is cleaner but may miss combinatorial effects
  - **ROUGE-L vs. task-specific metrics**: ROUGE-L enables cross-task comparison but may not capture quality nuances in generative tasks

- Failure signatures:
  - **Semantic drift**: Paraphrase generation changes meaning, invalidating the comparison (addressed via fine-tuned model but not eliminated)
  - **Type confusion**: Paraphrase model misclassifies change types, leading to incorrect attribution (acknowledged in Limitations)
  - **Task refusal**: Safety filters cause models to refuse classification tasks involving sensitive content (observed for toxic language detection tasks)
  - **Temperature confounding**: High temperature can mask or exaggerate paraphrase effects (controlled via low temperature experiments)

- First 3 experiments:
  1. **Reproduction baseline**: Select 3 diverse tasks (sentiment analysis, summarization, question answering), generate paraphrases for their prompts, and verify the morphology/lexicon advantage pattern holds for your target model.
  2. **Scale sensitivity check**: Compare paraphrase sensitivity for two models of different sizes from the same family (e.g., LLaMA 3 8B vs. 70B) on 5 tasks to confirm smaller models show higher variance and gain potential.
  3. **Task-type interaction mapping**: For a single model, systematically test all 6 paraphrase families across 3 task families to build a preliminary task-type alignment map before broader deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanisms that cause language models to exhibit variable performance when instructions are paraphrased?
- Basis in paper: [explicit] The authors ask, "why such different behaviors can be achieved by changing the instructions" in the discussion regarding how smaller models can outperform larger ones via prompt tuning.
- Why unresolved: The study quantifies the performance shifts but does not investigate the internal model states or attention mechanisms that might interpret specific linguistic structures differently.
- What evidence would resolve it: Mechanistic interpretability analysis (e.g., probing attention heads) linking specific paraphrase types to shifts in internal activation patterns.

### Open Question 2
- Question: How can the optimal paraphrase type for a new, unseen task be determined without exhaustive search?
- Basis in paper: [explicit] The Limitations section states, "Finding the most successful type of change in a given setting is non-trivial, and more research needs to be done on successfully perturbed prompts for new and unseen tasks."
- Why unresolved: The paper identifies aggregate trends but notes that no single type works universally across all domains, leaving the challenge of a priori selection open.
- What evidence would resolve it: The development of a classifier or heuristic that successfully predicts the most effective paraphrase family based solely on task definitions or few-shot examples.

### Open Question 3
- Question: To what extent do generation errors in automated paraphrase models bias the observed performance results?
- Basis in paper: [explicit] The authors acknowledge that the models used to generate paraphrases "only have a certain accuracy, leading to models sometimes confusing one type for another."
- Why unresolved: The noise in the generation pipeline creates a margin of error, making it difficult to isolate whether performance shifts are strictly due to the intended linguistic change or unintended artifacts.
- What evidence would resolve it: Comparative experiments using human-verified, gold-standard paraphrase datasets to validate if the trends (e.g., morphology gains) hold under zero-error conditions.

## Limitations

- ROUGE-L evaluation may not capture task-specific quality nuances for generative tasks where semantic fidelity matters more than n-gram overlap
- Paraphrase generation cannot guarantee semantic equivalence across all 26 types, potentially introducing confounding through meaning drift
- Temperature sensitivity findings suggest some improvements may not be robust across different inference settings

## Confidence

- **High Confidence**: Smaller models show higher sensitivity to prompt variations and can outperform larger models when prompts are optimized (Section 4, Q3)
- **Medium Confidence**: Specific performance gains by paraphrase type (morphology/lexicon showing the most promise) due to potential confounding from paraphrase generation quality
- **Medium Confidence**: Task-type interactions (sentiment analysis benefiting from polarity changes, vocabulary tasks from lexical changes) as these could be influenced by task-specific prompt formulations
- **Low Confidence**: Temperature independence claim, given the study only tested low vs. high temperature extremes without exploring intermediate values

## Next Checks

1. **Semantic Equivalence Validation**: For each paraphrase type, conduct human evaluation on a random sample of 50 paraphrased prompts to verify semantic preservation. Calculate the correlation between semantic drift and performance changes to determine if observed gains are confounded by meaning changes rather than linguistic optimization.

2. **Real-World Application Testing**: Select 5 high-impact tasks from the study and implement the top-performing paraphrase types in a real conversational context with prompt history. Compare performance against the original prompts in a dynamic setting where the model can use conversation context, measuring both task accuracy and response quality.

3. **Cross-Model Family Generalization**: Extend the experiment to include models from different architectural families (e.g., Mistral, Gemma, Claude) and training paradigms (instruction-tuned vs. base models). Test whether the morphology/lexicon advantage pattern holds across diverse model architectures or if it's specific to the tested models' training approaches.