---
ver: rpa2
title: 'AnyUp: Universal Feature Upsampling'
arxiv_id: '2510.12764'
source_url: https://arxiv.org/abs/2510.12764
tags:
- feature
- upsampling
- features
- anyup
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnyUp, a universal feature upsampling method
  that generalizes to any vision feature type and resolution without requiring encoder-specific
  training. The core innovation is a feature-agnostic layer that processes features
  of varying dimensionality by convolving them with a learned filter basis, followed
  by a window attention mechanism that simplifies the upsampling problem by restricting
  attention to local regions.
---

# AnyUp: Universal Feature Upsampling

## Quick Facts
- arXiv ID: 2510.12764
- Source URL: https://arxiv.org/abs/2510.12764
- Authors: Thomas Wimmer; Prune Truong; Marie-Julie Rakotosaona; Michael Oechsle; Federico Tombari; Bernt Schiele; Jan Eric Lenssen
- Reference count: 18
- Primary result: Universal feature upsampling method achieving state-of-the-art performance across semantic segmentation, depth estimation, and surface normal estimation while generalizing to any vision encoder without retraining

## Executive Summary
AnyUp introduces a universal feature upsampling method that can process features from any vision encoder regardless of their dimensionality or architecture. The core innovation is a feature-agnostic layer that processes varying dimensionalities by convolving features with a learned filter basis, followed by local window attention to simplify the upsampling problem. The method achieves state-of-the-art performance on multiple downstream tasks while requiring no encoder-specific training, making it the first truly universal upsampling solution.

## Method Summary
AnyUp employs a three-stage approach: (1) a feature-agnostic layer that reduces input features to a canonical dimensionality through learned filter basis convolution and averaging, (2) local window attention that restricts high-resolution queries to attend only to nearby low-resolution features, and (3) crop-based training with consistency regularization to preserve semantic fidelity without requiring expensive full-resolution ground truth features.

## Key Results
- Achieves mIoU up to 62.16 on COCO semantic segmentation
- Reaches RMSE down to 0.4755 on depth estimation
- Achieves RMSE down to 31.17 on surface normal estimation
- Generalizes to features from SigLIP and DINOv3 without retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model generalizes to feature extractors with varying dimensionalities (e.g., DINO vs. CLIP) without retraining.
- **Mechanism:** A **Feature-Agnostic Layer** processes input features by convolving each channel independently with a learned filter basis $\{\psi_j\}$, applying softmax, and averaging the results across all channels. This results in a fixed-dimensional output regardless of input channel count $N$.
- **Core assumption:** The model assumes that local structural information required for upsampling is shared across channels and can be compressed into a canonical representation via averaging.
- **Evidence anchors:**
  - [abstract] "processes features of varying dimensionality by convolving them with a learned filter basis"
  - [section 4.1] Equation 1 defines the layer output as an average over $N$ input channels, ensuring dimension invariance.
  - [corpus] The need for this is highlighted by [LoftUp] and other baselines which generally require encoder-specific retraining or fixed projection layers.
- **Break condition:** If the semantic information in a new encoder is encoded entirely in inter-channel relationships rather than spatial patterns per channel, the averaging operation might discard critical data.

### Mechanism 2
- **Claim:** Restricting attention to local windows improves upsampling accuracy and reduces artifacts.
- **Mechanism:** **Local Window Attention** limits the set of low-resolution features a high-resolution query can attend to. This prevents the model from "cheating" by attending to semantically similar but spatially distant image regions, thereby simplifying the optimization objective.
- **Core assumption:** Upsampling is primarily a local texture transfer problem where high-res pixels derive from nearby low-res patches, not global context.
- **Evidence anchors:**
  - [abstract] "simplifies the upsampling problem by restricting attention to local regions"
  - [section 4.2] Authors state global attention sometimes uses "vastly unrelated and distant image areas."
  - [corpus] Weak direct evidence in corpus; related works like [LoftUp] or [SRMambaV2] focus on coordinate or global attention, contrasting with AnyUp's local restriction.
- **Break condition:** If an object requires global context to resolve local ambiguity (e.g., a disconnected part requiring whole-object shape knowledge), windowing may cause fragmentation.

### Mechanism 3
- **Claim:** High-fidelity upsampling can be trained without expensive high-resolution ground-truth features.
- **Mechanism:** **Crop-Based Training** computes "ground truth" features only for small random crops of a high-res image. The model learns to upsample the full image by matching the features in the corresponding crop location, avoiding the OOM risk of full high-res feature extraction.
- **Core assumption:** The model can learn global upsampling by optimizing on local dense supervision; global coherence will emerge from overlapping contexts.
- **Evidence anchors:**
  - [abstract] "trained using a crop-based strategy... to preserve feature semantics"
  - [section 4.3.1] Describes supervising only on smaller local crops $I'$ rather than the full high-res map.
- **Break condition:** If the training crop size is too small, the model may fail to capture large-scale structural continuity, leading to disjoint features.

## Foundational Learning

- **Concept:** **Attention Mechanisms (Query/Key/Value)**
  - **Why needed here:** The core of AnyUp is a cross-attention layer where high-res image pixels (Queries) attend to low-res features (Keys/Values). Understanding this interaction is vital for debugging feature transfer.
  - **Quick check question:** Can you explain how the "Value" matrix determines the content of the output feature, while the "Key" determines *where* that content comes from?

- **Concept:** **Resolution/Semantic Trade-off in ViTs**
  - **Why needed here:** The problem exists because Vision Transformers lower resolution to manage quadratic attention costs. AnyUp attempts to reverse this locally.
  - **Quick check question:** Why does increasing the input resolution of a standard ViT (without windowing) often lead to out-of-memory (OOM) errors or distribution shifts?

- **Concept:** **Consistency Regularization**
  - **Why needed here:** The paper uses $L_{self-consistency}$ and $L_{input-consistency}$ to prevent the upsampler from hallucinating features or drifting from the original semantic space.
  - **Quick check question:** What would happen to the upsampled features if the consistency loss were removed and only the crop-reconstruction loss was used? (Hint: Think about distribution drift).

## Architecture Onboarding

- **Component map:** Low-res Feature Map ($p$) -> Feature-Agnostic Layer -> Processed Features -> Window Attention (with $I^{hr}$ Guidance) -> Upsampled Features

- **Critical path:** The flow of semantic information relies on the **Feature-Agnostic Layer** successfully projecting diverse inputs into a space where the **Window Attention** can correlate them with RGB guidance. If the projection (Step 2) fails to standardize features, the attention (Step 4) queries misaligned keys.

- **Design tradeoffs:**
  - **Window Size:** Larger windows capture more context but increase compute and risk reintroducing "distant attention" artifacts. Small windows are efficient but may miss global structure.
  - **Filter Basis Size ($M$):** A larger $M$ in the agnostic layer captures more nuance but reduces the "agnostic" nature by overfitting to training distributions.

- **Failure signatures:**
  - **Texture Copying:** If the RGB guidance encoder dominates, the output might look like the RGB image rather than the semantic feature map.
  - **Halo Artifacts:** Mentioned in the paper regarding baselines; usually caused by misalignment in attention or filtering.
  - **Distribution Shift:** If regularization is too weak, upsampled features may drift outside the original feature space, breaking downstream linear probes.

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run inference on a DINO feature and a CLIP feature. Does the model output have the same shape? Does it crash if you remove the "Feature-Agnostic Layer" and use a standard Linear layer?
  2. **Attention Visualization:** Visualize the attention maps for a high-res pixel on an object boundary. Does it attend to the correct low-res patch, or does it "bleed" into the background (testing the Window Attention mechanism)?
  3. **Probe Consistency:** Train a linear probe on original low-res features, then evaluate it on AnyUp's high-res output *without* fine-tuning. A significant drop indicates a failure in "Feature Space Preservation."

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes local structure dominates upsampling requirements, potentially failing for tasks requiring global context
- Filter basis averaging could lose critical inter-channel relationships in some encoder designs
- Limited qualitative analysis of failure cases and edge conditions

## Confidence
- **High confidence:** Crop-based training strategy and consistency regularization are well-validated
- **Medium confidence:** Feature-agnostic layer successfully handles dimensional variation
- **Low confidence:** Window attention universally simplifies upsampling without sacrificing global semantic coherence

## Next Checks
1. **Distribution Drift Test:** Systematically measure feature space drift by training linear probes on original vs. upsampled features across multiple encoder families, quantifying KL divergence between distributions
2. **Window Size Sensitivity:** Evaluate performance degradation when reducing window size below 8×8 and increasing beyond 32×32 for boundary-heavy vs. texture-heavy images
3. **Cross-Encoder Stress Test:** Apply AnyUp to features from novel architectures (e.g., Hyena, Mamba-based encoders) not represented in training, measuring absolute performance drop and qualitative artifacts