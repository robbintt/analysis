---
ver: rpa2
title: 'Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese
  K-12 Mathematics Learning'
arxiv_id: '2506.18330'
source_url: https://arxiv.org/abs/2506.18330
tags:
- training
- data
- learning
- confucius3-math
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Confucius3-Math is a 14B-parameter open-source LLM optimized for
  Chinese K-12 mathematics education. It achieves state-of-the-art performance on
  mathematics reasoning tasks, outperforming larger models through targeted post-training
  reinforcement learning.
---

# Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning

## Quick Facts
- arXiv ID: 2506.18330
- Source URL: https://arxiv.org/abs/2506.18330
- Reference count: 11
- Achieves 96.2% accuracy on internal K-12 mathematics benchmarks while costing only $26K to train

## Executive Summary
Confucius3-Math is a 14B-parameter open-source LLM optimized for Chinese K-12 mathematics education. It achieves state-of-the-art performance on mathematics reasoning tasks, outperforming larger models through targeted post-training reinforcement learning. The model was developed using three technical innovations: Targeted Entropy Regularization to stabilize training, Recent Sample Recovery to improve data efficiency, and Policy-Specific Hardness Weighting to enhance learning adaptation. Trained at a cost of $26K on a single GPU, it reaches 96.2% accuracy on internal K-12 benchmarks and demonstrates 15.8× faster inference than comparable models. The work shows that domain-specific, high-performance reasoning models can be built cost-effectively with focused data and advanced RL techniques.

## Method Summary
The model uses DeepSeek-R1-Distill-Qwen-14B as base model, trained on ~540K samples through three-stage pure RL: Stage 1 (4K ctx, GRPO + Targeted Entropy Regularization), Stages 2-3 (8K/16K ctx, DAPO + RSR + PSHW). Key innovations include entropy regularization with bounds (target=0.55), recent sample recovery for data efficiency, and advantage weighting by question difficulty. The reward model combines format/repetition filtering with rule-based or LLM-as-Judge scoring. Training cost was 13,109 H800 GPU-hours ($26K).

## Key Results
- Achieves 96.2% accuracy on proprietary CK12-MATH internal K-12 benchmark
- Outperforms larger models including Qwen2.5-32B-Instruct and DeepSeek-Coder-V2-16B on mathematics reasoning tasks
- Demonstrates 15.8× faster inference than comparable models while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Targeted Entropy Regularization stabilizes RL training and prevents language mixing artifacts.
- **Mechanism**: Adds loss term |entropy_loss − entropy_target| * entropy_coeff to penalize deviation from target entropy (0.55), constraining entropy from both above and below unlike standard approaches.
- **Core assumption**: High entropy correlates with mixed language problem where models substitute tokens across languages; stability requires entropy to remain within narrow band rather than simply staying above floor.
- **Evidence anchors**: Abstract states it "stabilize training"; Figure 3 shows GRPO+TER maintains stable entropy around target while baseline GRPO oscillates; authors explicitly state "the mixed language problem also goes away".
- **Break condition**: If entropy_target is set inappropriately for different model or domain, training may destabilize or exploration may be insufficient.

### Mechanism 2
- **Claim**: Recent Sample Recovery improves data efficiency and final model quality by reusing legitimate samples discarded during dynamic sampling.
- **Mechanism**: Saves overflow samples that passed filtering but exceeded batch capacity during DAPO's dynamic sampling, then injects them into next batch. Samples are never stale—they're from immediately prior policy step.
- **Core assumption**: Adjacent policy steps produce sufficiently similar distributions that samples remain valid without importance sampling corrections; blending effect may smooth training.
- **Evidence anchors**: Abstract states it "improve data efficiency"; Figure 4a shows RSR+DAPO achieves higher accuracy with less raw data consumption; Figure 4b shows ~3 percentage point improvement at same data volume.
- **Break condition**: If policy updates are too aggressive (large step sizes), recovered samples become too off-policy and may introduce distribution shift.

### Mechanism 3
- **Claim**: Policy-Specific Hardness Weighting improves learning by adapting advantage estimation to each problem's difficulty relative to current policy.
- **Mechanism**: Advantage estimator weighted by D(q) = α × μ + 1.256 where μ is mean score across sampled outputs for question q. Lower mean (harder problems) yields higher weight. Removes length normalization from DAPO to prevent bias toward brief correct answers and long incorrect ones.
- **Core assumption**: Per-question mean reward serves as reliable proxy for difficulty relative to current policy; harder problems deserve higher gradient signals.
- **Evidence anchors**: Abstract states it "enhance learning adaptation"; authors describe incorporating relative difficulty into advantage estimation; α = -0.256 set empirically.
- **Break condition**: If batch composition is highly heterogeneous, mean-based difficulty estimates may be noisy; problems with inherently low success rates may dominate training.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO) and Dynamic sAmpling Policy Optimization (DAPO)**
  - Why needed here: Confucius3-Math builds on these RL frameworks for LLM post-training. Understanding how they sample outputs, compute advantages, and update policies is prerequisite to grasping the modifications (PSHW, RSR).
  - Quick check question: Can you explain why DAPO filters prompts with 0% or 100% accuracy during dynamic sampling?

- **Concept: Entropy Regularization in Reinforcement Learning**
  - Why needed here: The paper's first technical innovation modifies standard entropy regularization. Without understanding why entropy matters for exploration and how it's typically implemented (e.g., in PPO), the targeted variant won't make sense.
  - Quick check question: What problem does standard entropy regularization solve, and what failure mode can it cause in LLM training?

- **Concept: Chain-of-Thought and Test-Time Compute**
  - Why needed here: The paper explicitly positions itself relative to o1-style TTC and measures CoT growth via response length expansion across training stages. The base model (DeepSeek-R1-Distill-Qwen-14B) was selected partly for its CoT capabilities.
  - Quick check question: How does staged context window expansion (4K → 8K → 16K) relate to CoT development?

## Architecture Onboarding

- **Component map**: Base Model (DeepSeek-R1-Distill-Qwen-14B) -> Data Pipeline (curated math datasets → deduplication → type filtering) -> Reward Model (hybrid filter → scoring) -> Training (3-stage RL with progressive context expansion) -> Deployment (vLLM with FP8 quantization)

- **Critical path**: 1) Data curation (deduplication, type filtering critical for reward signal quality) 2) Base model selection (entropy and CoT output richness as selection criteria) 3) Stage 1 training (GRPO + TER to establish stability) 4) Stages 2-3 (DAPO + RSR + PSHW for efficiency and performance) 5) Evaluation on K-12 benchmarks (CK12-MATH internal, GAOKAO, MathBench)

- **Design tradeoffs**: RL vs. Distillation: Authors chose RL despite distillation being easier, betting on emergent reasoning; corpus neighbor (Qwen3) uses distillation instead. Model size (14B): Balanced expressivity against deployment on consumer hardware. Entropy bounds vs. floor: TER constrains both directions unlike Skywork-OR1's lower-bound approach.

- **Failure signatures**: Mixed language output: Check if entropy regularization is too weak or target is too high. Text repetition in thinking/answer: Verify repetition filter covers both regions; early-stage repetition can spread. Entropy collapse or explosion: Monitor policy entropy curve; should stabilize near target. Poor data efficiency: Verify RSR buffer is being populated and consumed correctly.

- **First 3 experiments**: 1) Reproduce Stage 1 with and without TER on small data subset; plot entropy curves and check for language mixing. 2) Compare DAPO vs. DAPO+RSR with fixed data budget; measure accuracy per raw tokens consumed. 3) Ablate PSHW by training with standard advantage estimation; evaluate on hardest benchmark subset (AIME) to isolate difficulty-weighting effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the principled theoretical basis for setting the target entropy value in Targeted Entropy Regularization?
- Basis in paper: [explicit] "The targeted entropy regulation method used in this work is effective. However, it lacks a principled treatment."
- Why unresolved: The target entropy of 0.55 was set empirically without theoretical justification; the authors acknowledge the method addresses exploration-exploitation but hasn't been analyzed using established frameworks.
- What evidence would resolve it: A theoretical analysis connecting entropy targets to convergence properties, or empirical studies systematically varying the target across domains to establish principled selection criteria.

### Open Question 2
- Question: Why does Recent Sample Recovery (RSR) improve model quality beyond just data efficiency gains?
- Basis in paper: [explicit] "RSR not only improves data efficiency, but also produces better results, even with the same amount of effective data... We speculate that this may be due to certain smoothing effect, since RSR blends two distributions that are not too far away. We will give this issue a more rigorous treatment in another paper."
- Why unresolved: The mechanism behind RSR's quality improvement remains speculative; the distribution-blending hypothesis hasn't been formally tested.
- What evidence would resolve it: Ablation studies isolating the smoothing effect, or theoretical analysis of how mixing nearby policy distributions affects gradient estimates.

### Open Question 3
- Question: Can policy entropy serve as a reliable predictor of a base model's suitability for RL post-training?
- Basis in paper: [inferred] "We postulate that the policy entropy could be an importance metric when determining whether a model can serve as an initial model for RL optimization."
- Why unresolved: This observation came from exploratory experiments across four models but lacks systematic validation; correlation was observed but causation and predictive power remain unestablished.
- What evidence would resolve it: Large-scale studies across diverse model families measuring entropy before training and correlating with final RL performance, controlling for confounding factors.

## Limitations
- Primary performance claims rely on proprietary benchmarks (CK12-MATH) whose composition and protocols are not disclosed
- Three technical innovations show promising results but their generalizability to other domains or model scales remains untested
- Optimal hyperparameter settings (entropy_target=0.55, α=-0.256) were empirically determined and may require re-tuning for different applications

## Confidence

**High Confidence**: Technical implementation details (three-stage training pipeline, progressive context expansion, reward model architecture) are clearly specified and reproducible. Cost analysis and inference speed comparisons appear methodologically sound.

**Medium Confidence**: Core performance claims (96.2% on CK12-MATH, 15.8× faster inference) are supported by internal benchmarks but lack full independent verification. Claimed advantages of three technical innovations are demonstrated within paper's controlled experiments.

**Low Confidence**: Generalizability of three innovations to other domains or model architectures has not been established. Optimal hyperparameter settings may not transfer to different tasks.

## Next Checks

1. **Independent Benchmark Reproduction**: Replicate training pipeline on publicly available mathematics benchmarks (GSM8K, MATH) to verify three technical innovations provide consistent performance improvements across different evaluation datasets.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary entropy_target (0.4-0.7), PSHW coefficient α (-0.4 to -0.1), and RSR buffer size to quantify their impact on training stability and final performance, establishing robustness of reported settings.

3. **Cross-Domain Transfer Test**: Apply three innovations (TER, RSR, PSHW) to non-mathematical reasoning task (e.g., code generation or commonsense reasoning) to evaluate whether improvements generalize beyond K-12 mathematics domain where developed.