---
ver: rpa2
title: Unifying Vision-Language Latents for Zero-label Image Caption Enhancement
arxiv_id: '2510.12931'
source_url: https://arxiv.org/abs/2510.12931
tags:
- image
- vizer
- vizerg
- vizergt
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViZer, a zero-label training framework for
  vision-language models that enhances image captioning without requiring text annotations.
  ViZer actively aligns visual and language representations by mapping latent features
  between modalities, enabling models to self-improve using only raw images.
---

# Unifying Vision-Language Latents for Zero-label Image Caption Enhancement

## Quick Facts
- **arXiv ID**: 2510.12931
- **Source URL**: https://arxiv.org/abs/2510.12931
- **Reference count**: 40
- **Primary result**: ViZer improves image captioning without text annotations by aligning visual and language latents, showing gains in CLIPScore and qualitative grounding.

## Executive Summary
This paper introduces ViZer, a zero-label training framework that enhances vision-language models (VLMs) by aligning visual and textual representations in latent space. ViZer employs a lightweight mapper network to transform text embeddings into the visual feature space, enabling the model to self-improve using only raw images. Applied to SmolVLM-Base and Qwen2-VL, ViZer generates more grounded and descriptive captions than baselines, outperforming in CLIPScore and qualitative evaluations. The method is modular, scalable, and demonstrates that unlabeled data can effectively enhance multimodal alignment. Notably, automated metrics often fail to capture these gains, highlighting the need for better evaluation methods.

## Method Summary
ViZer introduces a zero-label training framework that aligns visual and textual representations in latent space to improve image captioning. The method employs a lightweight mapper network (a 2-layer MLP) to transform text embeddings into the visual feature space. Two variants exist: ViZerGT uses ground-truth captions for mapper training, while ViZerG is fully unsupervised, using the VLM's own generated captions. The mapper is trained to minimize cosine distance between visual features and mapped text embeddings. The aligned mapper is then frozen, and the VLM is adapted using LoRA with a consistency loss between visual features and mapped text features from generated captions. Training uses small datasets (10k-40k samples) to prevent overfitting and improve generalization.

## Key Results
- ViZer achieves improved CLIPScore on COCO and OpenImages, outperforming baselines in grounding and descriptive quality.
- Smaller mapper training datasets (10k-40k samples) consistently outperform larger ones, preventing overfitting to specific captioning patterns.
- Automated metrics like BLEU and CIDEr often fail to capture qualitative improvements, highlighting the need for better evaluation methods.
- ViZerG, the fully unsupervised variant, demonstrates that self-generated captions can serve as effective alignment targets when combined with consistency enforcement.

## Why This Works (Mechanism)

### Mechanism 1
Aligning visual and textual representations in latent space improves caption grounding without labeled data. ViZer trains a lightweight mapper network (MLP) to minimize cosine distance between visual features from a frozen encoder and mapped text embeddings. This bidirectional alignment creates a shared semantic space where caption generation can be guided by visual similarity signals rather than explicit labels. Core assumption: Visual features contain sufficient semantic structure that, when properly aligned with language representations, can serve as a training signal for caption generation. Evidence: [abstract] "ViZer actively aligns vision and language representation features during training, enabling existing VLMs to generate improved captions without requiring text labels." Break condition: Fails if visual encoder produces features lacking semantic discrimination for caption-relevant concepts.

### Mechanism 2
Self-generated captions can serve as alignment targets for zero-label training when combined with consistency enforcement. ViZerG variant uses the VLM's own generated captions as text inputs to the mapper. The model is trained to minimize discrepancy between caption-derived embeddings and visual features via the aligned space, creating a self-consistency loop where better alignment produces better captions, which in turn improve alignment. Core assumption: Generated captions, while imperfect, contain enough semantic signal to create useful alignment targets when the model has been pre-trained on similar image distributions. Evidence: [section 3.1] "ViZerG mapper is aligned on the hidden features of VLM to be trained... only ViZerG may be considered truly unsupervised and zero-label." Break condition: Fails if base VLM produces systematically biased or low-quality captions that reinforce errors through the self-consistency loop.

### Mechanism 3
Smaller mapper training datasets prevent overfitting to specific captioning patterns and improve generalization. The paper empirically finds that limiting mapper training data (40k samples for ViZerGT, 10k for ViZerG) outperforms larger datasets. Smaller datasets force the mapper to learn looser, more flexible alignment rather than memorizing caption-style patterns specific to the training corpus. Core assumption: Moderate underfitting of the mapper is preferable to overfitting, as it preserves transferability to diverse images and caption styles. Evidence: [section 4.3] "Smaller training dataset sizes are consistently preferred for both ViZerGT and ViZerG... Increasing the sample size beyond these ranges yields diminishing returns and, in several cases, even leads to measurable performance degradation." Break condition: Fails if training data is too small to capture necessary semantic diversity, or too large/narrow, causing mapper to overfit to corpus-specific phrasing.

## Foundational Learning

- **Contrastive Learning in Multimodal Spaces**
  - Why needed here: ViZer's loss function directly uses cosine similarity contrastive objectives. Understanding how negative sampling, temperature scaling, and embedding normalization affect alignment quality is essential for debugging training.
  - Quick check question: Can you explain why cosine similarity is preferred over Euclidean distance for normalizing text-visual feature alignment across varying caption lengths?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: ViZer uses LoRA to adapt VLMs without full retraining. Understanding rank, alpha, and target modules determines whether you preserve pretrained capabilities while enabling alignment learning.
  - Quick check question: If LoRA rank is set too high, what specific failure mode would you expect in terms of catastrophic forgetting vs. insufficient adaptation?

- **Vision-Language Model Architectures (Encoder-Decoder vs. Decoder-Only)**
  - Why needed here: ViZer requires understanding where to inject the mapper—between frozen vision encoder outputs and VLM hidden states. Different architectures (SmolVLM vs. Qwen2-VL) have different integration points.
  - Quick check question: Given a VLM with a Q-Former bridge (like BLIP-2), would ViZer's mapper go before or after the Q-Former, and why?

## Architecture Onboarding

- **Component map**:
  - Frozen Vision Encoder (Vθ) -> Visual features FI
  - Text Encoder/Tokenizer (Eϕ) -> Text embeddings
  - ViZer Mapper (Mτ) -> Mapped text features
  - VLM Backbone (fψ) -> Caption generation
  - Integration Point: Mapper output compared to visual features via cosine similarity loss

- **Critical path**:
  1. Train mapper first on image-text pairs (ViZerGT) or generated captions (ViZerG)
  2. Freeze mapper, enable LoRA on VLM attention layers
  3. Feed unlabeled images through vision encoder
  4. Generate captions with current VLM
  5. Pass generated captions through mapper to get aligned text features
  6. Compute cosine similarity loss between visual features and mapped text features
  7. Backpropagate through LoRA parameters only

- **Design tradeoffs**:
  - Mapper width vs. overfitting: Width of 256 optimal; larger widths (512-1024) show degradation due to memorization
  - Dataset size vs. generalization: 10k-40k samples preferred over 100k+; larger datasets cause corpus-specific overfitting
  - ViZerGT vs. ViZerG: ViZerGT uses ground-truth labels (not truly zero-label but stronger signal); ViZerG is fully unsupervised but depends on base VLM quality
  - LoRA rank 32/alpha 64: Balances adaptation capacity with preservation of pretrained knowledge

- **Failure signatures**:
  - Generated captions become verbose or hallucinate objects not in image: Mapper may be over-aligning to text features at expense of visual grounding
  - Captions unchanged from baseline: LoRA rank too low or learning rate insufficient to enable adaptation
  - Catastrophic forgetting of non-captioning tasks: LoRA applied too broadly or training too long without task interleaving
  - Mapper loss decreases but caption quality doesn't improve: Alignment learned on wrong semantic level (surface features vs. semantic concepts)

- **First 3 experiments**:
  1. **Mapper-only baseline**: Train mapper on COCO/CC3M with frozen VLM. Measure CLIPScore on held-out images before any VLM adaptation to validate mapper learns meaningful alignment.
  2. **Ablation on mapper width and data size**: Grid search width (128, 256, 512) × data size (10k, 40k, 100k) on SmolVLM-Base. Track both CLIPScore and qualitative grounding to reproduce paper's optimal 256/40k finding.
  3. **Cross-domain transfer test**: Train mapper on COCO (natural images), evaluate on domain-shifted images (medical, satellite, or UI screenshots). Measure alignment quality degradation to understand robustness limits mentioned in Section 5.

## Open Questions the Paper Calls Out

### Open Question 1
Can ViZer's alignment approach be effectively extended to visual question answering (VQA), where models must align answers with localized visual regions rather than global scene features? Basis: The limitations section states: "Currently, ViZer is limited to image captioning tasks, as extending the visual feature alignment to visual question answering (VQA) remains a challenging endeavor. In VQA, models often focus on localized regions or specific objects rather than the entire visual scene, making direct alignment between answers and global visual features a non-trivial task." Why unresolved: VQA requires grounding textual answers to specific image regions, whereas ViZer currently aligns full caption embeddings with global visual features. What evidence would resolve it: A modified ViZer architecture that incorporates region-level visual features and demonstrates improved VQA performance on standard benchmarks without text annotations.

### Open Question 2
How robust is ViZer to domain shift when applied to out-of-distribution image types such as medical imagery, satellite photos, or noisy web-scraped data? Basis: Section 5 states: "Another limitation is the uncertain behavior of ViZer on highly diverse or out-of-distribution images... Domains with distinct visual statistics, such as medical or satellite imagery, or noisy web data, may challenge the robustness of zero-label alignment." Why unresolved: The experiments use OpenImages and common pretraining datasets that share similar visual statistics. Performance on substantially different visual domains remains untested. What evidence would resolve it: Systematic evaluation of ViZer-trained models on domain-specific datasets with analysis of alignment quality degradation patterns.

### Open Question 3
Why does increasing the mapper training dataset size beyond 10k-40k samples cause performance degradation, and what does this reveal about optimal alignment in latent space? Basis: Section 4.3 documents counterintuitive findings: "smaller training dataset sizes are consistently preferred... best performance emerges with roughly 40k labeled samples for ViZerGT and around 10k samples for ViZerG. Increasing the sample size beyond these ranges yields diminishing returns and, in several cases, even leads to measurable performance degradation." Why unresolved: The authors attribute this to overfitting (mapper memorizing captioning patterns), but the mechanism is not fully characterized. Whether this is a fundamental property of cross-modal alignment or an artifact of the specific architecture remains unclear. What evidence would resolve it: Ablation studies varying mapper capacity alongside dataset size, analysis of learned representations at different scales, and comparison with regularization techniques.

## Limitations
- ViZer is currently limited to image captioning and faces challenges extending to tasks requiring localized visual attention like VQA.
- The method's robustness to domain shift is uncertain, particularly for out-of-distribution images like medical or satellite imagery.
- The self-consistency mechanism in ViZerG may reinforce existing model biases when using self-generated captions for alignment.

## Confidence
- **High Confidence**: The core claim that latent space alignment between visual and textual features can improve caption grounding is well-supported by experimental results and aligns with established contrastive learning principles.
- **Medium Confidence**: The claim that zero-label training can match or exceed supervised caption quality is supported but limited by the absence of true zero-label baselines and dependence on pre-trained VLM quality.
- **Low Confidence**: The claim that ViZerG represents truly unsupervised learning is questionable, as it depends on the base VLM's pre-training quality and distribution match between training and inference data.

## Next Checks
1. **Self-consistency validation test**: Train ViZerG on one image distribution (e.g., COCO) and evaluate on a completely disjoint distribution (e.g., medical images or satellite imagery) to measure whether the self-consistency mechanism generalizes or overfits to specific visual patterns.

2. **True zero-label ablation**: Implement a variant that trains from scratch without any text supervision, comparing performance against ViZer variants that use pre-trained VLMs to isolate the contribution of alignment versus pre-training.

3. **Mapper architecture sensitivity**: Systematically vary mapper depth (1-4 layers) and activation functions (ReLU, GELU, Swish) while keeping width fixed at 256 to determine whether the current 2-layer MLP is optimal or merely sufficient.