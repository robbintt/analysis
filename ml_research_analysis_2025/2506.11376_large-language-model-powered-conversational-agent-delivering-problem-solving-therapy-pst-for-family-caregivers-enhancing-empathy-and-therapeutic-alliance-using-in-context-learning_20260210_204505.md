---
ver: rpa2
title: 'Large Language Model-Powered Conversational Agent Delivering Problem-Solving
  Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance
  Using In-Context Learning'
arxiv_id: '2506.11376'
source_url: https://arxiv.org/abs/2506.11376
tags:
- participants
- caregivers
- empathy
- health
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a large language model (LLM)-powered chatbot
  to deliver Problem-Solving Therapy (PST) for family caregivers, integrating Motivational
  Interviewing (MI) and Behavioral Chain Analysis (BCA) to enhance empathy and therapeutic
  alliance. A within-subject experiment with 28 caregivers tested four LLM configurations
  using Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques.
---

# Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning

## Quick Facts
- arXiv ID: 2506.11376
- Source URL: https://arxiv.org/abs/2506.11376
- Reference count: 0
- Primary result: LLM-powered chatbot delivered PST for caregivers with mean empathy scores of 1.89 (interpretation) and 1.77 (exploration), and therapeutic alliance scores of 8.15 (relationship) and 8.36 (goals)

## Executive Summary
This study developed and tested a large language model (LLM)-powered chatbot to deliver Problem-Solving Therapy (PST) for family caregivers of individuals with chronic conditions. The intervention integrated Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA) techniques to enhance empathy and therapeutic alliance. Using a within-subject experimental design with 28 caregivers, the study evaluated four LLM configurations employing Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques. The best-performing model, combining all techniques with the Llama 3 model, demonstrated promising results in therapeutic constructs, though statistical significance was limited by variance in outcome measures.

## Method Summary
The study employed a within-subject experimental design where 28 family caregivers interacted with four different LLM configurations in randomized order. Each configuration used different combinations of Few-Shot learning and RAG prompting techniques with Llama 3. Caregivers engaged in simulated conversations about caregiving challenges, with interactions limited to 8 turns. The study measured empathy through Humanness and Warmth scores, evaluated therapeutic alliance using the Session Rating Scale (SRS-3), and collected qualitative feedback. All conversations were processed through an automated empathy classifier and reviewed by human evaluators for validation.

## Key Results
- Best-performing model achieved mean empathy scores of 1.89 (interpretation) and 1.77 (exploration) on 0-2 scales
- Therapeutic alliance scores reached 8.15 (relationship) and 8.36 (goals and topics) on 0-10 scales
- Qualitative feedback indicated the model effectively validated emotions and provided actionable strategies, though some participants preferred more direct advice

## Why This Works (Mechanism)
The LLM-powered PST intervention works by leveraging the model's ability to process complex emotional and behavioral patterns through integrated therapeutic techniques. The combination of Few-Shot learning allows the model to rapidly adapt to therapeutic contexts, while RAG provides access to relevant PST knowledge bases. The integration of MI techniques enhances the model's capacity for empathetic responses, and BCA enables systematic exploration of behavioral patterns. This multi-modal approach allows the system to provide both emotional validation and practical problem-solving strategies within the constraints of conversational AI.

## Foundational Learning
- **Problem-Solving Therapy (PST)**: A structured intervention that helps individuals identify problems and develop concrete solutions. Why needed: Provides the therapeutic framework for the chatbot's problem-solving capabilities. Quick check: Model consistently identifies problems and generates actionable solutions within 8-turn conversations.
- **Motivational Interviewing (MI)**: A counseling approach that enhances intrinsic motivation for change through empathetic engagement. Why needed: Improves the chatbot's ability to build therapeutic rapport and encourage caregiver self-reflection. Quick check: Conversations demonstrate open-ended questions and reflective listening.
- **Behavioral Chain Analysis (BCA)**: A technique for examining sequences of behaviors leading to specific outcomes. Why needed: Enables systematic exploration of caregiving challenges and their underlying patterns. Quick check: Model successfully traces behavioral sequences across multiple conversation turns.
- **Few-Shot Learning**: A machine learning approach where models learn from limited examples. Why needed: Allows rapid adaptation of the LLM to therapeutic contexts without extensive retraining. Quick check: Model performance improves across conversation turns with minimal examples.
- **Retrieval-Augmented Generation (RAG)**: A technique combining retrieval of relevant information with generative capabilities. Why needed: Provides access to PST knowledge bases while maintaining conversational flow. Quick check: Responses incorporate relevant PST concepts and strategies.

## Architecture Onboarding

**Component Map:**
User -> Conversation Interface -> LLM (Llama 3) -> Few-Shot Module -> RAG Module -> PST/MI/BCA Integration -> Response Generator

**Critical Path:**
User input → Prompt Engineering → Few-Shot Context → RAG Retrieval → Therapeutic Response Generation → User Feedback

**Design Tradeoffs:**
The system balances thorough assessment (BCA) with efficient advice delivery, constrained by 8-turn interaction limits. The tradeoff between deep exploration and immediate practical suggestions reflects real-world caregiving time constraints while maintaining therapeutic effectiveness.

**Failure Signatures:**
- Model fails to validate emotions when caregivers express distress
- Responses become repetitive or generic after multiple turns
- System struggles to transition from assessment to actionable advice within time limits
- Empathy scores drop when handling complex multi-issue caregiving scenarios

**First Experiments:**
1. Test empathy classifier accuracy on multi-turn conversations with varied emotional complexity
2. Evaluate response quality when transitioning from assessment to solution generation
3. Measure user satisfaction when varying the ratio of exploration to advice-giving

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based interventions dynamically balance the need for thorough behavioral assessment (via BCA) with caregivers' preferences for efficient, immediate advice?
- Basis in paper: The authors state that "balancing thorough assessment with efficient advice delivery remains a challenge," noting that the time-consuming nature of BCA prevented some participants from receiving suggestions within the limited interaction turns.
- Why unresolved: The study design used a fixed 8-turn limit and a static protocol, which failed to satisfy users who preferred direct advice over deep exploration.
- What evidence would resolve it: A study comparing user satisfaction and therapeutic alliance scores in conditions where the model adaptively shortens assessment based on real-time user intent.

### Open Question 2
- Question: Does explicitly incorporating user preferences regarding therapeutic style (e.g., assessment depth vs. directiveness) improve therapeutic alliance scores?
- Basis in paper: The discussion suggests that "future iterations... may consider user preference" and "offering options for users to choose from with varied emphasis on assessment vs. advice-giving."
- Why unresolved: The current within-subject experiment applied the same therapeutic configurations to all participants without tailoring the interaction style to individual preferences.
- What evidence would resolve it: A randomized trial showing higher Session Rating Scale scores for a preference-adaptive model compared to a static PST/MI model.

### Open Question 3
- Question: Can automated metrics specifically trained on multi-turn dialogue accurately evaluate therapeutic constructs like empathy and alliance in LLM-driven therapy?
- Basis in paper: The authors identify a "lack of dialogue corpora or automated metrics specifically for evaluating empathy in multi-turn conversations" and note the current algorithm was trained only on single-turn data.
- Why unresolved: Existing automated empathy classifiers fail to capture the sequential context required to measure therapeutic alliance, forcing reliance on self-reports which showed limited variance.
- What evidence would resolve it: The development and validation of a multi-turn classifier that correlates strongly with human expert ratings of empathy in caregiver chatbot logs.

## Limitations
- Small sample size (N=28) limits statistical power and generalizability
- Simulated conversations rather than real therapeutic interactions reduce ecological validity
- Lack of longitudinal assessment prevents evaluation of sustained therapeutic outcomes
- Limited variance in outcome measures makes statistical significance difficult to achieve

## Confidence

**High Confidence:** Technical implementation of LLM-powered chatbot using Llama 3, Few-Shot learning, and RAG techniques is well-documented and technically sound.

**Medium Confidence:** Qualitative feedback from caregivers regarding empathy and therapeutic alliance appears consistent with technical implementation, though limited by hypothetical scenario design.

**Low Confidence:** Clinical efficacy claims are limited by lack of real-world therapeutic interactions and absence of control groups or comparative effectiveness data.

## Next Checks
1. Conduct a randomized controlled trial with larger sample size comparing LLM-powered PST intervention against traditional in-person PST and waitlist control groups, measuring both quantitative outcomes and real-world therapeutic alliance.

2. Implement longitudinal follow-up assessments (3, 6, and 12 months) to evaluate sustained impact on caregiver mental health outcomes and problem-solving capabilities in real caregiving scenarios.

3. Develop and validate standardized metrics for measuring empathy and therapeutic alliance specifically in AI-powered mental health interventions, including assessment of potential risks and adverse events in actual therapeutic use.