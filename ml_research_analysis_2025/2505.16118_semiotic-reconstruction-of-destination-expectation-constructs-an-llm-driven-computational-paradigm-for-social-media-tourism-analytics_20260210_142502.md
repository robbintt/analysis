---
ver: rpa2
title: Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven
  Computational Paradigm for Social Media Tourism Analytics
arxiv_id: '2505.16118'
source_url: https://arxiv.org/abs/2505.16118
tags:
- expectation
- social
- tourism
- expectations
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study pioneers a dual-method LLM framework to analyze tourist\
  \ expectations from social media UGC, combining unsupervised expectation extraction\
  \ with survey-informed supervised fine-tuning. The methodology successfully quantifies\
  \ five core expectation categories\u2014Emotional, Natural, Exotic Cultural, Leisure,\
  \ and Social\u2014demonstrating LLMs' capacity to decode nuanced psychological constructs\
  \ from vernacular expressions."
---

# Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics

## Quick Facts
- **arXiv ID**: 2505.16118
- **Source URL**: https://arxiv.org/abs/2505.16118
- **Reference count**: 40
- **Primary result**: LLM framework extracts and quantifies tourist expectations from social media, showing Leisure/Social factors drive engagement more than Natural/Emotional factors

## Executive Summary
This study introduces a dual-method LLM framework that computationally extracts and quantifies tourist expectations from Chinese social media UGC. The methodology combines unsupervised GPT-4 extraction with survey-informed supervised fine-tuning to identify five core expectation categories: Emotional, Natural, Exotic Cultural, Leisure, and Social. Validation against human-labeled data shows strong model-user agreement, with empirical findings revealing that Leisure and Social Expectations drive engagement more effectively than foundational Natural/Emotional factors, particularly in pandemic-era tourism contexts.

## Method Summary
The framework processes 12,843 social media posts through a dual-path pipeline: unsupervised GPT-4 zero-shot extraction identifies expectation categories from vernacular text, while supervised fine-tuning with LoRA adapts the model to 7-point Likert scale intensity scores using 100 human-labeled examples. Model outputs are validated through Bland-Altman analysis and used to predict engagement via Random Forest regression. The approach addresses scalability limitations of traditional content analysis while maintaining semantic accuracy in decoding nuanced psychological constructs.

## Key Results
- Dual-method LLM framework successfully extracts five core expectation categories from vernacular social media text
- Strong model-human agreement demonstrated through Bland-Altman analysis (Krippendorff's α = 0.81)
- Leisure and Social Expectations show stronger positive impact on engagement than Natural/Emotional factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs extract latent psychological constructs from unstructured vernacular text via zero-shot prompting
- Mechanism: GPT-4's self-attention layers disambiguate polysemous terms and cultural slang through contextual embeddings rather than rigid lexicons, mapping vernacular to high-level semantic categories
- Core assumption: Semantic representation of "expectations" in model's pre-training space aligns with psychometric definitions of tourism motivation
- Evidence anchors: Abstract demonstrates capacity to decode nuanced psychological constructs; section 3.2.1 describes unsupervised paradigm capitalizing on emergent capability
- Break condition: Mechanism fails if input slang falls outside model's training distribution, causing semantic drift or hallucinated categories

### Mechanism 2
- Claim: Supervised fine-tuning with sparse survey data aligns LLM output scores with human-defined intensity scales
- Mechanism: LoRA updates only 0.3% of model weights, minimizing MSE between predicted scores and human questionnaire responses
- Core assumption: Relationship between text features and 7-point Likert scale is consistent across crowd-sourced raters and learnable via regression on small sample
- Evidence anchors: Abstract describes combining unsupervised extraction with survey-informed fine-tuning; section 3.2.2 details training data comprising <text, score, rationale> triples
- Break condition: Mechanism degrades if human labels contain subjective bias or 100-shot training set fails to represent variance, leading to overfitting

### Mechanism 3
- Claim: Non-linear interactions of expectation scores predict user engagement better than linear associations
- Mechanism: Random Forest captures complex dependencies where Leisure and Social expectations provide positive marginal contributions to likes via SHAP analysis
- Core assumption: Likes function as valid behavioral proxy for successful activation of specific psychological expectations
- Evidence anchors: Abstract reveals Leisure and Social Expectations drive engagement more effectively; section 4.3 shows Leisure Expectation and Social Expectation show most prominent positive contributions
- Break condition: If engagement driven by platform algorithms favoring visual content rather than textual expression, causal link between expectation score and likes is spurious

## Foundational Learning

- **Self-Attention & Contextual Embeddings**: Essential for understanding how model handles vernacular expressions and neologisms without fixed dictionary. Quick check: How does model distinguish "adventure" as physical challenge vs. emotional catharsis?

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**: Explains how system adapts to tourism expectation domain using only 100 samples without catastrophic forgetting. Quick check: Why modify only 0.3% of parameters rather than full model weights?

- **SHAP (Shapley Additive Explanations)**: Required to interpret Random Forest results and understand why high feature importance doesn't always mean high positive impact on likes. Quick check: If feature has high importance but mixed SHAP value distribution, what does that imply about its influence?

## Architecture Onboarding

- **Component map**: Input (UGC Text) -> Preprocessor (Noise Removal/Slang Normalization) -> [Dual Path] 1. Unsupervised (GPT-4 Zero-Shot) -> Category Extraction 2. Supervised (GPT-4 + LoRA) -> Intensity Scoring (1-7) -> Validator (Bland-Altman) -> Predictor (Random Forest) -> Engagement Prediction

- **Critical path**: Generation of <text, score, rationale> triples in section 3.2.2 is bottleneck; if rationales incoherent, fine-tuning fails

- **Design tradeoffs**: Trades cost/latency of large model (GPT-4) for accuracy gained over rule-based systems, accepts small training set (100 samples) to ensure model stability

- **Failure signatures**:
  - Hallucination: Model invents expectations not supported by text in unsupervised phase
  - Agreement Failure: Bland-Altman plot shows systematic bias (mean difference ≠ 0)
  - Semantic Drift: Model confuses Exotic Cultural with Natural expectations due to ambiguous inputs

- **First 3 experiments**:
  1. Slang Robustness Test: Input 50 posts with heavy dialect/slang to verify if unsupervised extractor maintains semantic coherence
  2. Inter-Rater Reliability Check: Compare model scores against 3 independent human annotators on held-out set to verify Bland-Altman limits of agreement
  3. Ablation on Engagement: Retrain Random Forest without Social Expectation to quantify drop in predictive power for likes

## Open Questions the Paper Calls Out

- **Cross-cultural validation of expectation hierarchies through multilingual analysis**: The conclusion explicitly lists this as first priority for subsequent research, acknowledging expectation constructs may be culturally bound

- **Multimodal integration of visual-textual data streams**: Authors identify this as key pathway for future inquiry, noting current methodology focuses on text-based semiotic reconstruction potentially missing non-verbal tourist gaze cues

- **Longitudinal tracking of expectation evolution in post-pandemic contexts**: Paper calls for this to validate crisis-era findings, noting data scope captures acute pandemic phases but ends before stabilization of travel behaviors

## Limitations
- Reliance on GPT-4 with LoRA fine-tuning creates significant reproducibility barriers for standard API users
- Translation from Chinese social media text to English introduces potential semantic loss for region-specific slang
- Validation limited to Chinese tourism contexts and specific platforms, raising questions about generalizability

## Confidence
- **High Confidence**: Dual-method LLM framework successfully extracts and quantifies destination expectations from UGC
- **Medium Confidence**: Finding that Leisure and Social expectations drive engagement more effectively than Natural/Emotional factors
- **Low Confidence**: Exact mechanism by which GPT-4's self-attention layers disambiguate cultural slang and neologisms

## Next Checks
1. Apply framework to tourism UGC from different cultural contexts to test generalizability of five expectation categories
2. Conduct controlled experiment comparing model-predicted expectations against actual behavioral outcomes rather than likes
3. Systematically test unsupervised extractor with intentionally ambiguous or culturally-specific inputs to quantify hallucination rates