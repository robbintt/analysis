---
ver: rpa2
title: Can Large Language Models Match Tutoring System Adaptivity? A Benchmarking
  Study
arxiv_id: '2504.05570'
source_url: https://arxiv.org/abs/2504.05570
tags:
- llms
- tutoring
- adaptivity
- student
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a prompt variation framework to evaluate large
  language models' (LLMs) adaptivity and pedagogical soundness in tutoring scenarios.
  Using real-world tutoring data from an intelligent tutoring system (ITS), the researchers
  systematically removed key context components (e.g., student errors, knowledge components)
  from prompts and generated 1,350 instructional moves using three LLMs (Llama3-8B,
  Llama3-70B, GPT-4o).
---

# Can Large Language Models Match Tutoring System Adaptivity? A Benchmarking Study

## Quick Facts
- **arXiv ID:** 2504.05570
- **Source URL:** https://arxiv.org/abs/2504.05570
- **Reference count:** 35
- **Primary result:** Current LLMs show limited adaptivity to student errors compared to ITS systems

## Executive Summary
This study evaluates whether large language models (LLMs) can match the adaptivity of intelligent tutoring systems (ITS) in educational contexts. Using a novel prompt variation framework applied to real-world tutoring data from an ITS focused on linear equations, the researchers systematically removed context components from prompts and generated instructional moves using three LLMs. The study found that while some LLMs showed basic pedagogical soundness, their ability to adapt to student-specific context was significantly limited compared to traditional ITS systems. These findings suggest that current LLM-based tutoring approaches are unlikely to achieve the learning benefits of established ITS without substantial improvements.

## Method Summary
The researchers developed a prompt variation framework to systematically evaluate LLM adaptivity in tutoring scenarios. They collected 450 instructional moves from a real ITS (Lynnette) that provides step-by-step tutoring for linear equations. For each move, they created seven prompt variations by removing one of seven context components (student errors, knowledge components, correct actions, hints, etc.) while keeping other elements constant. This generated 1,350 instructional moves across three LLMs (Llama3-8B, Llama3-70B, GPT-4o). They measured adaptivity using text embeddings and randomization tests to detect sensitivity to removed context, and assessed pedagogical soundness using a validated tutor-training classifier. The framework allowed systematic comparison of how different models respond to changes in tutoring context.

## Key Results
- Llama3-70B showed the only statistically significant adaptivity to student errors among tested LLMs
- Llama3-8B achieved higher pedagogical soundness scores but struggled with instruction-following behaviors
- GPT-4o adhered closely to instructions but provided overly direct feedback that reduced learning opportunities
- Overall, LLM-generated instructional moves demonstrated significantly lower adaptivity compared to traditional ITS systems

## Why This Works (Mechanism)
The study demonstrates that LLMs can generate coherent tutoring responses but struggle to adapt their responses based on specific student context information. The mechanism underlying this limitation appears to be that LLMs, trained on general web data, lack the fine-grained pedagogical reasoning capabilities that ITS systems develop through targeted design for educational adaptation. While LLMs can follow general tutoring patterns, they fail to detect and respond to subtle cues about student knowledge states, error patterns, and learning progress that are crucial for effective adaptive instruction.

## Foundational Learning
- **Text embeddings for semantic comparison**: Used to measure similarity between instructional moves with different context components - needed to quantify adaptivity statistically
- **Randomization tests for statistical significance**: Applied to determine if observed differences in embeddings were meaningful - needed to validate adaptivity measurements
- **Tutor-training classifiers for pedagogical assessment**: Employed to evaluate the educational quality of generated responses - needed to ensure moves meet tutoring standards
- **Prompt engineering with systematic variations**: Created controlled experiments by removing specific context components - needed to isolate factors affecting adaptivity
- **Intelligent tutoring system architecture**: Provided the benchmark dataset and pedagogical standards - needed as the gold standard for comparison
- **Linear equation problem-solving pedagogy**: Served as the domain for testing - needed to create a well-defined, measurable educational context

## Architecture Onboarding
**Component map:** Prompt variation framework -> LLM generation -> Text embedding comparison -> Statistical testing -> Pedagogical classification

**Critical path:** Context component removal → Prompt generation → LLM response → Embedding analysis → Adaptivity measurement

**Design tradeoffs:** The study prioritized systematic evaluation over real-time performance, choosing controlled prompt variations over naturalistic conversation simulation. This provided clearer causal insights but may not fully capture interactive tutoring dynamics.

**Failure signatures:** LLMs showed consistent patterns of providing generic feedback regardless of student error type, failing to reference specific knowledge components, and not adjusting scaffolding based on prior hints provided.

**First experiments:** (1) Test whether adding explicit error detection prompts improves LLM adaptivity to student mistakes. (2) Compare LLM performance when given summarized versus full student attempt histories. (3) Evaluate whether fine-tuning LLMs on tutoring-specific data improves pedagogical soundness scores.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does selecting targeted subsets of context data, rather than providing full student attempt histories, affect the quality of LLM-generated instructional moves?
- Basis in paper: [explicit] The authors state in the Limitations section that "the impact of context window length in LLM-based tutoring remains an open question" and suggest that "selecting targeted subsets of context data may enhance LLM generations."
- Why unresolved: The current study provided full student attempt histories to the models and did not test data selection or summarization strategies.
- What evidence would resolve it: A comparative study using the same benchmarking framework that evaluates adaptivity and pedagogical soundness scores when prompts contain full histories versus algorithmically selected or summarized context subsets.

### Open Question 2
- Question: To what extent does tuning hyperparameters, such as model temperature, influence the tradeoff between scaffolding specificity and generality in LLM-based tutoring?
- Basis in paper: [explicit] The Discussion notes that model selection influences the "degree of assistance provided" and explicitly calls for future research to "systematically explore the effect of tuning these parameters [like model temperature] on instructional quality."
- Why unresolved: The study compared different model architectures (Llama3 vs. GPT-4o) but did not isolate the effects of generation parameters like temperature within those models.
- What evidence would resolve it: Experimental results where the same model is tested across a range of temperature settings using the authors' benchmarking method to measure changes in pedagogical soundness and adaptivity.

### Open Question 3
- Question: Do the limitations in LLM adaptivity observed in algebraic equation solving persist in open-ended problem-solving domains or non-mathematical subjects?
- Basis in paper: [explicit] The authors acknowledge that the findings "may not generalize to other educational settings, such as open-ended problem-solving or non-mathematical subjects" and call for applying the approach to "diverse disciplines."
- Why unresolved: The dataset was restricted to a single tutoring system (Lynnette) focused on linear equations, limiting the scope of the conclusions.
- What evidence would resolve it: Replication of the prompt variation framework on datasets from different domains (e.g., coding, essay writing) to determine if LLMs show similarly low adaptivity to context components like errors or knowledge components.

### Open Question 4
- Question: Does LLM instructional adaptivity degrade in languages that are underrepresented in web training corpora compared to English?
- Basis in paper: [explicit] The Limitations section notes the study relied on "tutoring scenarios from an American sample encoded in English" and suggests future research could "expand our benchmarking approach to multilingual data sets."
- Why unresolved: The models were evaluated exclusively on English data, leaving their ability to detect and adapt to nuances in other languages untested.
- What evidence would resolve it: Benchmarking results from multilingual tutoring datasets comparing the statistical significance of adaptivity (sensitivity to removed context) in English versus low-resource languages.

## Limitations
- The prompt variation framework only removed one context component at a time, potentially missing interactions between multiple contextual factors
- Text embedding and randomization test methodology may not capture all dimensions of pedagogical adaptivity that human tutors employ
- The evaluation focused exclusively on mathematics tutoring from one ITS platform, limiting generalizability to other subjects

## Confidence
- **High confidence:** Current LLMs show limited adaptivity compared to ITS systems, given systematic testing methodology and clear statistical differences
- **Medium confidence:** Specific comparative performance of different LLM models, as this depends on particular evaluation metrics and prompts used
- **Low confidence:** Broader claims about potential for LLM-based tutoring systems, as these require further research with more complex and varied contexts

## Next Checks
1. Test the prompt variation framework with multiple context components removed simultaneously to better simulate real tutoring complexity
2. Conduct human expert evaluations alongside automated classifiers to validate the assessment of pedagogical soundness across different tutoring scenarios
3. Extend the evaluation to multiple subject domains and ITS platforms to assess generalizability of the findings