---
ver: rpa2
title: Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations
  using BoolQ
arxiv_id: '2601.16508'
source_url: https://arxiv.org/abs/2601.16508
tags:
- question
- anchor
- related
- length
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined whether conversation length affects the veracity
  of LLM responses using the BoolQ dataset under varying length and scaffolding conditions.
  The research tested three LLMs across four scaffolds (meta, semantic, underspecified,
  misleading) at five conversation lengths, tracking accuracy, incorrect responses,
  and "I don't know" responses.
---

# Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations using BoolQ

## Quick Facts
- arXiv ID: 2601.16508
- Source URL: https://arxiv.org/abs/2601.16508
- Reference count: 17
- Multi-turn length degrades LLM accuracy differently across models, with Qwen2.5's confident misinformation being most concerning

## Executive Summary
This study reveals that conversation length significantly affects LLM response veracity in ways invisible to single-turn evaluations. Using the BoolQ dataset, researchers tested three models across five conversation lengths and four scaffold types, discovering model-specific vulnerabilities that scale with interaction length. The most concerning finding is Qwen2.5's pattern of declining accuracy paired with declining abstention under misleading scaffolding—producing confident misinformation. These results demonstrate that static evaluations fundamentally miss critical safety failures that emerge only in extended conversational contexts.

## Method Summary
The researchers evaluated three LLMs (Phi-4, DeepSeek, Qwen2.5) on BoolQ yes/no questions across five conversation lengths (L=1, 6, 11, 16, 21 turns) × four scaffolds (meta, semantic, underspecified, misleading). Each conversation used rolling context windows with the final turn being the target BoolQ question. Model responses were classified as Correct, Incorrect, or "I don't know" against ground truth. Multinomial logistic regression with scaffold × length interactions examined whether conversation length affected accuracy and confidence calibration differently across scaffold types.

## Key Results
- Most scaffold effects were neutral or beneficial, but misleading scaffolding degraded Qwen2.5's accuracy (β=-0.015, p=0.009) while also reducing IDK responses (β=-0.069, p<0.001)
- Phi-4 showed the safest pattern: stable accuracy with increasing IDK responses under misleading conditions
- DeepSeek required appropriate context to function well, showing poor baseline but improvement with meta scaffolding
- Length-dependent scaffold effects were model-specific and invisible under single-turn testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended conversational context produces scaffold-specific effects on model veracity that scale with interaction length
- Mechanism: Prior conversational turns function as implicit priming—semantic scaffolds activate related knowledge structures (potentially beneficial), while misleading scaffolds introduce conflicting signals (potentially harmful). Effects accumulate across turns rather than remaining localized.
- Core assumption: Models weight prior conversational context when generating responses, and this weighting interacts with content type.
- Evidence anchors:
  - [abstract] "length-dependent and scaffold-specific effects we observed demonstrate a fundamental limitation of static evaluations"
  - [section] "Models assign disproportionately high attention weights to the earliest tokens in a sequence, while under-using information positioned in the middle of long prompts"
  - [corpus] Related work on multi-turn risk quantification (arXiv:2510.03969) confirms that "fixed attack prompt sequences" fail to reveal vulnerabilities visible in extended interactions
- Break condition: If models ignore prior turns or weight them uniformly regardless of content, scaffold effects would be noise rather than systematic.

### Mechanism 2
- Claim: Declining accuracy paired with declining abstention produces the most harmful failure mode—confident misinformation
- Mechanism: Under misleading scaffolding, Qwen2.5 showed accuracy decrease (β=-0.015, p=0.009) while IDK responses also decreased (β=-0.069, p<0.001). This double-decline means models become more confident as they become more wrong.
- Core assumption: Confident errors cause greater harm than uncertain errors because users are less likely to correct confident-seeming answers.
- Evidence anchors:
  - [abstract] "combination of declining accuracy with inappropriate confidence in Qwen2.5's response to misleading information represents the most concerning safety failure"
  - [section] "When models commit to incorrect answers rather than abstaining, they produce the edge cases most likely to cause real-world harm: errors delivered with false confidence preventing user correction"
  - [corpus] Limited direct corpus evidence on confidence calibration in multi-turn settings; neighboring papers focus on attack success rates rather than abstention behavior
- Break condition: If models increase abstention as accuracy declines (like Phi-4's IDK increase under misleading conditions), the safety profile changes fundamentally.

### Mechanism 3
- Claim: Model-specific safety strategies produce divergent responses to identical scaffolding conditions
- Mechanism: The three models implement distinct approaches—Phi-4 prioritizes caution (IDK increases across all scaffolds), DeepSeek requires appropriate context to function well, Qwen2.5 prioritizes answering but is vulnerable to misleading information. Same input produces different failure modes.
- Core assumption: These differences reflect underlying training/architectural choices rather than random variation.
- Evidence anchors:
  - [abstract] "model-specific vulnerabilities that were invisible under single-turn testing"
  - [section] "The three models implemented distinct safety strategies... Only Qwen2.5 exhibited the dangerous pattern of declining accuracy paired with declining abstention"
  - [corpus] SAGE framework paper (arXiv:2504.19674) notes benchmarks "failing to capture the conversational dynamics of real-world usage and the application-specific nature of safety concerns"
- Break condition: If all models showed qualitatively similar scaffold × length interactions, the mechanism would be general rather than model-architecture dependent.

## Foundational Learning

- Concept: **Attention Distribution in Context Windows**
  - Why needed here: The paper's length effects depend on understanding that models don't attend uniformly to all tokens—early tokens receive higher weights, middle content is underused.
  - Quick check question: If a model has a 21-turn conversation, would you expect Turn 1 or Turn 10 to have more influence on the final response?

- Concept: **Confidence Calibration vs. Accuracy**
  - Why needed here: The paper's central safety insight is that two models with identical accuracy can have vastly different harm profiles depending on whether wrong answers come with confidence or uncertainty.
  - Quick check question: Model A is 70% accurate and never says "I don't know." Model B is 65% accurate but says "I don't know" on 20% of questions it's uncertain about. Which is safer for medical advice?

- Concept: **Scaffold as Experimental Control**
  - Why needed here: The four scaffolds isolate different aspects of conversation (pure length, semantic priming, ambiguity, misinformation) to identify what drives performance changes.
  - Quick check question: If you wanted to test whether conversation length itself (not content) degrades performance, which scaffold would you use?

## Architecture Onboarding

- Component map:
  Stimulus source -> Enrichment pipeline -> Scaffold generator -> Conversation controller -> Model inference -> Response classifier -> Regression analysis

- Critical path:
  1. Enrich BoolQ items with semantic metadata (primary topic + 4 related terms)
  2. For each question × length × scaffold combination, generate conversation template
  3. Execute full conversation with target model, capture final response
  4. Classify response against ground truth
  5. Fit regression model examining Correct vs. Incorrect and IDK vs. Incorrect contrasts

- Design tradeoffs:
  - **Artificial scaffolds** → experimental control over semantic relationships, but may not reflect natural conversation patterns
  - **Three-way response options** → reveals confidence calibration, but deployment scenarios often pressure models to always answer
  - **Small model sample (3 models)** → enables deep analysis, but limits generalizability to architectural conclusions
  - **Single task domain (BoolQ)** → controls for task complexity, but unknown whether patterns hold for open-ended generation

- Failure signatures:
  - **Confident misinformation**: Accuracy↓ + IDK↓ (Qwen2.5 under misleading) — most dangerous
  - **Appropriate caution**: Accuracy stable + IDK↑ (Phi-4 under misleading) — safest pattern
  - **Context dependency**: Poor baseline but improvement with scaffolding (DeepSeek with meta) — needs context to function
  - **Stable performance**: No length or scaffold effects — static evaluation would be sufficient

- First 3 experiments:
  1. **Cross-model generalization**: Test 5-10 additional models spanning different architectures/training approaches to identify whether scaffold vulnerabilities cluster by model family
  2. **Length boundaries**: Extend to L=31, 41, 51 to identify where effects plateau or accelerate—critical for understanding real-world deployment limits
  3. **Natural conversation patterns**: Replace artificial scaffolds with human-generated multi-turn conversations (from existing dialogue datasets) to validate that lab-constructed effects appear in organic interactions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the observed model-specific vulnerabilities, such as Qwen2.5's susceptibility to confident misinformation, caused by architectural differences or training data characteristics?
- Basis in paper: [explicit] The Discussion states that "Investigation of the mechanisms producing model-specific vulnerabilities, whether architectural, training-related, or both, would inform the development of more robust conversational AI systems."
- Why unresolved: This study only compared three models, preventing the isolation of causal factors behind the divergent safety strategies (e.g., Phi-4's caution vs. Qwen2.5's confidence).
- What evidence would resolve it: Ablation studies controlling for model architecture and fine-tuning datasets to identify which component drives the decline in accuracy paired with declining abstention.

### Open Question 2
- Question: Do the failure patterns identified in open-source models generalize to closed, commercial LLMs?
- Basis in paper: [inferred] The Limitations section notes that "Resource constraints precluded testing closed commercial models" and that the sample size "prevents strong conclusions about architectural or training factors."
- Why unresolved: It remains unknown whether the "confident misinformation" failure mode is a quirk of smaller open models or a fundamental risk in state-of-the-art proprietary systems.
- What evidence would resolve it: Replicating the multi-turn scaffold protocol on major commercial APIs (e.g., GPT-4, Claude) to verify if similar length-dependent degradation occurs.

### Open Question 3
- Question: Does the "confident misinformation" effect persist when using naturally-occurring conversational patterns instead of synthetic scaffolds?
- Basis in paper: [inferred] The authors note in Limitations that future research should examine "additional scaffold types including naturally-occurring conversational patterns."
- Why unresolved: The current synthetic scaffolds may trigger specific attention failures that differ from how users naturally introduce misleading context in real-world dialogues.
- What evidence would resolve it: Evaluating models using human-generated multi-turn conversations to determine if the accuracy-to-confidence ratio trends hold outside of template-based inputs.

## Limitations
- Testing only three models limits generalizability across architectural families
- BoolQ's binary yes/no format may not generalize to open-ended generation tasks
- Artificial scaffolding may not reflect natural human conversation patterns

## Confidence
**High confidence (95%+)**: Model-specific scaffold effects are real and measurable
**Medium confidence (70-95%)**: The mechanism linking attention distribution to scaffold effects is plausible but not directly measured
**Low confidence (30-70%)**: Claims about real-world harm from confident misinformation remain hypothetical without empirical validation

## Next Checks
1. Test 5-10 additional models spanning different training approaches to identify whether scaffold vulnerabilities cluster by architectural family
2. Replace artificial scaffolds with human-generated multi-turn conversations to validate that lab-constructed effects appear in organic interactions
3. Conduct user studies comparing model outputs under different confidence profiles to measure actual user behavior and harm