---
ver: rpa2
title: Language Model Personalization via Reward Factorization
arxiv_id: '2503.06358'
source_url: https://arxiv.org/abs/2503.06358
tags:
- user
- reward
- responses
- preferences
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of personalizing large language
  model (LLM) responses to individual user preferences. It introduces a framework
  that learns user-specific reward functions as linear combinations of base reward
  functions, allowing efficient personalization without training separate models per
  user.
---

# Language Model Personalization via Reward Factorization

## Quick Facts
- arXiv ID: 2503.06358
- Source URL: https://arxiv.org/abs/2503.06358
- Authors: Idan Shenfeld; Felix Faltings; Pulkit Agrawal; Aldo Pacchiano
- Reference count: 35
- Primary result: Achieves 67% win rate over GPT-4o using only 10-20 pairwise comparisons per user

## Executive Summary
This work introduces a novel approach to personalizing large language model responses to individual user preferences through reward factorization. The key innovation lies in learning user-specific reward functions as linear combinations of base reward functions, which enables efficient personalization without the need to train separate models for each user. By leveraging active learning and pairwise comparisons, the method can infer user preference weights from as few as 10-20 interactions, significantly reducing the data requirements compared to traditional personalization approaches.

The framework demonstrates substantial performance improvements over standard RLHF and default LLM responses, with experiments showing 10-25% gains in response quality. The method achieves a 67% win rate over GPT-4o in human evaluations while requiring 25x less data than training individual models per user. This efficiency makes it particularly promising for scaling personalization to large user bases without prohibitive computational costs.

## Method Summary
The method learns user-specific reward functions as linear combinations of base reward functions, where the base functions capture different aspects of response quality (helpfulness, coherence, etc.). The personalization process uses active learning to select which responses to show users for pairwise comparison, strategically gathering information to infer the user's preference weights. The learned weights are then used to score and select responses from the LLM. The framework employs an explore-exploit algorithm to balance gathering new information about user preferences with optimizing for known preferences.

## Key Results
- Achieves 67% win rate over default GPT-4o responses in human evaluations
- Outperforms standard RLHF by 10-25% in response quality
- Requires only 10-20 pairwise comparisons per user versus 25x more data for training individual models
- Maintains performance across synthetic and real user experiments

## Why This Works (Mechanism)
The approach works by decomposing the personalization problem into learning a set of interpretable base reward functions that capture different dimensions of response quality, then inferring each user's unique weighting of these dimensions. This factorization allows the system to generalize across users - instead of learning an entirely new model for each user, it learns how to combine the same base functions differently. The active learning component ensures that each user interaction provides maximum information about their preferences, making the data efficiency possible.

## Foundational Learning
- **Reward factorization**: Breaking down complex preference functions into interpretable components - needed to enable efficient learning and generalization across users; quick check: verify base functions capture orthogonal aspects of quality
- **Active learning for preference elicitation**: Strategically selecting which comparisons to show users - needed to minimize the number of interactions required; quick check: measure information gain per comparison
- **Linear reward combination**: Representing user preferences as weighted sums of base rewards - needed to enable efficient personalization without separate models; quick check: test performance with non-linear combinations
- **Explore-exploit tradeoff**: Balancing preference discovery with optimization - needed to efficiently learn preferences while providing good responses; quick check: analyze exploration rate vs. performance
- **Pairwise comparison interface**: Collecting preference data through relative judgments - needed for efficient data collection and noise reduction; quick check: validate that pairwise judgments correlate with absolute quality ratings
- **Preference weight inference**: Estimating user-specific weights from limited comparisons - needed to personalize without extensive interaction; quick check: test weight estimation accuracy with varying numbers of comparisons

## Architecture Onboarding

**Component map:**
User -> Pairwise Comparison Interface -> Active Learning Selector -> Base Reward Functions -> Weight Inference -> Response Scoring -> LLM

**Critical path:**
Active Learning Selector -> Weight Inference -> Response Scoring -> LLM response generation

**Design tradeoffs:**
The framework trades model expressiveness (separate models per user would be more flexible) for data efficiency and scalability. The linear combination assumption simplifies learning but may not capture all preference nuances. Pairwise comparisons are more data-efficient than absolute ratings but require more user interaction steps.

**Failure signatures:**
- Degraded performance when preference weights become negative or extreme
- Poor generalization when base reward functions inadequately capture user preference dimensions
- Computational bottlenecks when scoring responses with multiple reward functions
- Active learning inefficiency when user preferences are highly idiosyncratic

**First experiments:**
1. Test weight inference accuracy with synthetic users having known preference weights
2. Evaluate base reward function coverage by measuring performance gaps for users with extreme preferences
3. Benchmark computational overhead of reward scoring across different response lengths and numbers of base functions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades with negative or extreme preference weights
- Heavy reliance on synthetic user data limits generalizability
- Computational overhead of computing multiple reward functions per response
- Pairwise comparison interface may not capture full complexity of user preferences

## Confidence
- **High confidence**: Core technical contribution of reward factorization is sound and well-supported by experimental results
- **Medium confidence**: Performance improvements over RLHF and default responses are demonstrated but evaluation setup may not represent real-world conditions
- **Low confidence**: Scalability claims and robustness to diverse preference distributions are less well-established

## Next Checks
1. Deploy the personalization system with a diverse user base (500+ users) over an extended period to evaluate performance across varied preference distributions, measuring both response quality and computational overhead in production conditions.

2. Systematically test the method's performance across artificially generated user preference distributions ranging from uniform to highly skewed, including cases with negative preference weights and extreme values, to quantify robustness boundaries.

3. Assess the personalization method's performance across multiple task types (creative writing, factual Q&A, code generation) to verify whether base reward functions learned for one task type generalize effectively to others, or if task-specific reward functions are required.