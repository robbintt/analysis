---
ver: rpa2
title: 'Memorization in 3D Shape Generation: An Empirical Study'
arxiv_id: '2512.23628'
source_url: https://arxiv.org/abs/2512.23628
tags:
- training
- memorization
- shapes
- data
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an evaluation framework to quantify memorization\
  \ in 3D generative models using a nearest-neighbor retrieval metric (LFD) and the\
  \ Mann-Whitney z-score. The framework distinguishes memorization from low-quality\
  \ generation by controlling for generation quality with Fr\xB4echet Distance."
---

# Memorization in 3D Shape Generation: An Empirical Study

## Quick Facts
- arXiv ID: 2512.23628
- Source URL: https://arxiv.org/abs/2512.23628
- Reference count: 40
- This paper introduces an evaluation framework to quantify memorization in 3D generative models using nearest-neighbor retrieval and Mann-Whitney z-score while controlling for generation quality

## Executive Summary
This paper addresses the critical issue of memorization in 3D generative models, where models may simply reproduce training data rather than creating novel shapes. The authors introduce a comprehensive evaluation framework that distinguishes memorization from low-quality generation by controlling for generation quality using Fr´echet Distance. Through extensive experiments across multiple model architectures and datasets, the study reveals that while earlier models trained on small datasets exhibit strong memorization, recent large-scale models demonstrate effective generalization. The research identifies key factors influencing memorization including data modality, diversity, conditioning granularity, and guidance scale, while proposing practical mitigation strategies that reduce memorization without compromising generation quality.

## Method Summary
The authors propose a memorization evaluation framework that combines nearest-neighbor retrieval metrics (specifically Light Field Descriptor distance) with the Mann-Whitney U test to quantify memorization while controlling for generation quality using Fr´echet Distance. The framework compares generated shapes against training and held-out datasets to determine whether similarities indicate memorization or genuine generalization. The study evaluates multiple state-of-the-art 3D generative models including Point-E, Zero-123, and Genie, examining their behavior across different data modalities (3D shapes vs images), dataset scales, and modeling strategies. Controlled experiments systematically vary factors such as data diversity, conditioning granularity, guidance scale, Vecset length, and augmentation techniques to understand their impact on memorization.

## Key Results
- Recent large-scale 3D generative models demonstrate effective generalization compared to earlier models trained on small datasets
- Images exhibit stronger memorization tendencies than 3D shapes when used as conditioning data
- Higher data diversity increases memorization, while finer-grained conditioning and moderate guidance scales also contribute to memorization
- Longer Vecset lengths and rotation augmentation effectively reduce memorization while maintaining generation quality

## Why This Works (Mechanism)
The framework works by establishing a statistical baseline for expected similarities between generated shapes and both training and held-out datasets. By controlling for generation quality through Fr´echet Distance, the Mann-Whitney U test can determine whether observed similarities exceed what would be expected from low-quality generation alone. This approach distinguishes true memorization (overly faithful reproduction of training examples) from the natural tendency of high-quality generators to produce shapes similar to their training distribution. The nearest-neighbor retrieval using LFD provides a quantitative measure of shape similarity that can be statistically analyzed across large sample sets.

## Foundational Learning

**Light Field Descriptor (LFD)**: A shape descriptor that captures the light field of 3D objects from multiple viewpoints. Why needed: Provides a view-invariant representation for comparing 3D shapes regardless of orientation. Quick check: Should produce similar values for rotated versions of the same shape.

**Fréchet Distance**: A metric for comparing probability distributions, used here to measure generation quality. Why needed: Enables quality-controlled memorization detection by distinguishing poor generation from true memorization. Quick check: Lower values indicate better match between generated and reference distributions.

**Mann-Whitney U Test**: A non-parametric statistical test comparing two independent samples. Why needed: Determines whether generated shapes are significantly more similar to training data than held-out data, indicating memorization. Quick check: Significant z-scores suggest non-random similarity patterns.

**Vecset**: A set of latent vectors used in diffusion models for generation conditioning. Why needed: Longer Vecsets provide more diverse guidance signals, potentially reducing memorization by preventing over-reliance on specific training examples. Quick check: Should correlate with generation diversity metrics.

## Architecture Onboarding

**Component Map**: Training Data → 3D Generative Model → Generated Shapes → LFD Computation → Mann-Whitney U Test → Memorization Score

**Critical Path**: The evaluation pipeline consists of: 1) Generate shapes using target model, 2) Compute LFD distances between generated shapes and both training/held-out datasets, 3) Calculate Fréchet Distance to ensure quality control, 4) Perform Mann-Whitney U test to determine statistical significance of similarity differences, 5) Report memorization score as z-score.

**Design Tradeoffs**: The choice of LFD as the sole similarity metric provides computational efficiency and view-invariance but may miss certain aspects of shape similarity. The framework balances between sensitivity to memorization and robustness to generation quality variations. Using statistical testing adds rigor but requires sufficient sample sizes for reliable results.

**Failure Signatures**: High memorization scores with low Fréchet distances indicate genuine memorization rather than poor generation quality. Conversely, high memorization scores with high Fréchet distances may indicate the model is producing low-quality outputs that happen to resemble training data. Inconsistent results across different similarity metrics may suggest metric-specific artifacts.

**First 3 Experiments**:
1. Baseline evaluation: Compare LFD-based memorization scores against human perceptual judgments of shape similarity
2. Ablation study: Test memorization detection using alternative similarity metrics (Chamfer distance, Earth Mover's distance) to validate LFD robustness
3. Cross-modal validation: Evaluate whether models trained on 3D shapes but conditioned on images show different memorization patterns than image-to-image models

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations

- The LFD metric may not fully capture all aspects of 3D shape similarity, potentially limiting the comprehensiveness of memorization detection
- Controlled experiments were primarily conducted on single-category datasets, which may not generalize to multi-category scenarios
- Findings are based exclusively on diffusion models, leaving uncertainty about whether memorization patterns generalize to other generative approaches
- The study does not explore alternative memorization detection metrics or validate LFD's effectiveness across different shape types

## Confidence

- **High confidence**: Recent large-scale models demonstrate effective generalization compared to earlier small-dataset models
- **Medium confidence**: Controlled experiment results regarding data modality, diversity, conditioning granularity, and guidance scale effects
- **Medium confidence**: Proposed mitigation strategies (longer Vecset lengths and rotation augmentation) effectiveness

## Next Checks

1. Validate the LFD-based memorization detection across multiple complementary metrics (e.g., Chamfer distance, Earth Mover's distance) to ensure findings are not metric-dependent

2. Extend controlled experiments to multi-category datasets and diverse 3D shape domains beyond chairs to assess whether identified factors exhibit consistent effects

3. Test proposed memorization mitigation strategies (longer Vecset lengths and rotation augmentation) on state-of-the-art large-scale diffusion models trained on multi-category datasets