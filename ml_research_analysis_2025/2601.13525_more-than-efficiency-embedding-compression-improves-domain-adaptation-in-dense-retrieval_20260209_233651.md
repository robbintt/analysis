---
ver: rpa2
title: 'More Than Efficiency: Embedding Compression Improves Domain Adaptation in
  Dense Retrieval'
arxiv_id: '2601.13525'
source_url: https://arxiv.org/abs/2601.13525
tags:
- compression
- query
- retrieval
- domain
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that applying PCA to query embeddings alone
  can effectively improve dense retrieval performance across diverse domains, with
  NDCG@10 gains in 75.4% of model-dataset pairs. PCA compression refocuses retrieval
  on domain-relevant semantic features by removing low-variance components, offering
  a training-free adaptation method that outperforms query+document compression and
  matches more complex domain adaptation techniques.
---

# More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval

## Quick Facts
- arXiv ID: 2601.13525
- Source URL: https://arxiv.org/abs/2601.13525
- Reference count: 28
- Applying PCA compression to query embeddings alone improves dense retrieval NDCG@10 in 75.4% of model-dataset pairs without requiring labeled data or model fine-tuning.

## Executive Summary
This work challenges the conventional view of embedding compression as merely an efficiency tool by demonstrating its effectiveness for domain adaptation in dense retrieval. The authors show that applying Principal Component Analysis (PCA) to query embeddings alone can significantly improve retrieval performance across diverse domains. Unlike previous approaches that compress both queries and documents (potentially diluting semantic features), this method preserves query intent while filtering out domain-irrelevant noise, achieving gains that outperform more complex adaptation techniques.

## Method Summary
The approach applies PCA compression post-hoc to pre-trained dense retrievers without any model fine-tuning. First, all query embeddings from the target domain are mean-centered and used to compute a covariance matrix. PCA is then fitted to extract the top d' principal components (retaining a specified percentage of variance). Both queries and documents are projected into this reduced subspace using the learned projection matrix. Retrieval proceeds via standard cosine similarity ranking in the compressed space. The key innovation is fitting PCA exclusively on target-domain queries rather than on both queries and documents, which empirically yields more consistent improvements across diverse datasets.

## Key Results
- Query-only PCA compression improves NDCG@10 in 75.4% of model-dataset pairs versus 56.3% for query+document compression
- Moderate compression (50-90% variance retention) yields optimal performance, with aggressive compression degrading results
- The method achieves competitive results against more complex domain adaptation techniques without requiring labeled data or model updates

## Why This Works (Mechanism)

### Mechanism 1: Noise Filtering via Variance Thresholding
If domain-specific signals in pre-trained embeddings are encoded in high-variance dimensions, PCA projection may improve retrieval by filtering out low-variance noise associated with the source domain. PCA identifies orthogonal axes of maximum variance in the target query embeddings. By retaining only the top d' components, the method effectively projects embeddings into a subspace where domain-specific distinctions are magnified, assuming these distinctions align with the principal components. The core assumption is that semantic variations critical to the target domain manifest as directions of high variance, while source-domain artifacts or noise lie in low-variance dimensions.

### Mechanism 2: Intent Alignment via Query-Only Projection
Fitting PCA solely on target-domain queries aligns the embedding space with user intent, preventing the "dilution" of semantic features caused by including the document corpus in the fitting process. The projection matrix W is computed exclusively from query covariance. When applied to both queries and documents, it forces the entire retrieval space to conform to the "view" of the user's semantic structure, rather than the broader, potentially irrelevant variance of the document corpus. The core assumption is that queries provide a cleaner, more task-relevant signal of domain semantics than the full document set.

### Mechanism 3: Implicit Regularization via Power-Law Decay
Moderate compression (50-90% retention) acts as a regularizer by truncating the "long tail" of embedding dimensions that contribute little to semantic distinction. Embedding spectra typically exhibit power-law decay, meaning most information is concentrated in the first few components. Retaining 50-90% of components preserves this structure while discarding the noisy tail, reducing overfitting to spurious correlations in the pre-trained space. The core assumption is that the eigenvalue spectrum of the embeddings follows a power-law distribution where the tail represents noise.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA)**
  - Why needed here: This is the core operation; you must understand how covariance matrices and eigenvectors define a new basis for the data to interpret why compression helps.
  - Quick check question: If you retain 90% of components, does that mean you keep 90% of the original vector dimensions or 90% of the variance?

- **Concept: Dense Retrieval (Bi-Encoder Architecture)**
  - Why needed here: The method assumes a fixed encoder and operates post-hoc on the vectors; understanding the independence of query and document encoding is vital.
  - Quick check question: Why does applying a transformation matrix W to the query vector q and document vector d preserve their cosine similarity ranking properties?

- **Concept: Domain Shift / Distribution Mismatch**
  - Why needed here: The paper frames compression as a solution to the "mismatch" between training and target data; you need to grasp why a model trained on Wikipedia might fail on Biomedical data.
  - Quick check question: Does domain shift affect the syntax (grammar) or the semantics (word usage/relationships) more in this context?

## Architecture Onboarding

- **Component map:** Unlabeled Target Domain Queries → Fixed Pre-trained Encoder → PCA Module (Fit on Q only) → Projection Matrix W → Projected embeddings Q' → Approximate Nearest Neighbor search → Retrieved documents

- **Critical path:** Fitting the PCA matrix. You must collect enough query embeddings from the target domain to form a stable covariance matrix. If N_queries < d_embedding, the covariance is rank-deficient.

- **Design tradeoffs:**
  - **Retention Ratio (r):** Higher r (e.g., 0.9) is safer but offers less noise reduction; lower r (e.g., 0.5) maximizes adaptation effect but risks information loss.
  - **Fitting Data:** Query-only is empirically superior (75.4% success) vs. Query+Document (56.3% success), but Query+Document might be necessary if the query set is extremely small.

- **Failure signatures:**
  - **Catastrophic Drop:** Occurs if retention is too low (e.g., 10%) or if fitting on Q+D dilutes query intent (e.g., on Apps/GerDa datasets).
  - **Low-Query Instability:** If the query sample size is <50 (e.g., ChemNQ in Appendix E), PCA cannot robustly estimate domain axes, leading to inconsistent results.

- **First 3 experiments:**
  1. **Baseline Validation:** Run retrieval on a target dataset with raw embeddings vs. 90% query-only PCA compression to replicate the 75.4% improvement trend.
  2. **Ablation (Q vs. Q+D):** Fit PCA on queries-only and again on Q+D. Compare NDCG@10 to verify that query-only fitting yields higher gains.
  3. **Retention Sweep:** Test retention ratios r ∈ {0.5, 0.7, 0.9, 0.95} to find the "sweet spot" where NDCG peaks before dropping off due to information loss.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the optimal retention ratio for PCA compression be predicted theoretically without empirical testing? The authors state in the Limitations section that "We leave the criteria for choosing the optimal retention ratio to future works." The optimal ratio varies significantly depending on the specific model and dataset, preventing a universal prescriptive guideline.

- **Open Question 2:** Can nonlinear dimensionality reduction methods improve upon the domain adaptation performance of linear PCA? The paper notes that "PCA may inadequately capture nonlinear semantic relationships in embedding spaces, suggesting potential for exploring nonlinear dimensionality reduction methods." PCA is strictly linear, but dense embeddings often reside on complex manifolds where linear projection might discard useful semantic curvature.

- **Open Question 3:** Why does the correlation between a model's pre-existing domain familiarity and performance gain from PCA differ across architectures? The analysis of "Domain Familiarity" reveals mixed results, with strong positive correlations for BGE/Instructor but negative or insignificant correlations for Sent-T5/MiniLM, a phenomenon the authors describe as "complex."

## Limitations
- Performance is highly sensitive to dataset characteristics, with consistent gains for hierarchical datasets but mixed results for flatter datasets
- Method relies on unlabeled query data for PCA fitting, raising scalability concerns for domains with very small query sets
- Paper doesn't comprehensively compare against other domain adaptation techniques like vocabulary expansion or explicit query reformulation

## Confidence

- **High Confidence:** The core observation that PCA compression can improve retrieval (positive results exist and are statistically significant)
- **Medium Confidence:** The mechanism explanations (noise filtering, intent alignment) - these are plausible but not definitively proven
- **Medium Confidence:** The superiority of query-only vs query+document fitting - results are consistent but the sample size of datasets is limited

## Next Checks
1. **Dataset Characteristic Analysis:** Systematically categorize datasets by query structure (hierarchical vs flat) and verify whether the performance pattern (consistent gains for hierarchical, mixed results for flat) holds across a broader sample.
2. **Query Set Size Sensitivity:** Conduct controlled experiments varying query set sizes from 10 to 1000+ to establish the minimum viable sample size for stable PCA fitting and identify the scaling relationship.
3. **Comparison to Query Expansion:** Implement a simple query expansion baseline (e.g., Rocchio feedback) on the same datasets to determine whether the gains come from the compression mechanism itself or simply from query-side adaptation.