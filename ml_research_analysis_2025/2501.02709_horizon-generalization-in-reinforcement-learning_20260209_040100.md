---
ver: rpa2
title: Horizon Generalization in Reinforcement Learning
arxiv_id: '2501.02709'
source_url: https://arxiv.org/abs/2501.02709
tags:
- generalization
- horizon
- planning
- learning
- quasimetric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies horizon generalization in goal-conditioned\
  \ reinforcement learning, where policies trained on nearby goals should succeed\
  \ at reaching distant goals. The authors connect this property to planning invariance\u2014\
  the idea that a policy should take similar actions when navigating directly to a\
  \ distant goal versus navigating to intermediate waypoints along the way."
---

# Horizon Generalization in Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2501.02709
- **Source URL:** https://arxiv.org/abs/2501.02709
- **Reference count:** 40
- **Primary result:** Quasimetric policies exhibit planning invariance, leading to stronger horizon generalization in goal-conditioned RL

## Executive Summary
This paper addresses the challenge of horizon generalization in goal-conditioned reinforcement learning, where policies trained on nearby goals must succeed at reaching distant goals. The authors establish a theoretical connection between horizon generalization and planning invariance - the property that a policy should take similar actions whether navigating directly to a distant goal or through intermediate waypoints. They prove that quasimetric policies, which measure asymmetric distances between states, are planning invariant and that this invariance leads to horizon generalization. Empirical results on both tabular and high-dimensional continuous control tasks demonstrate that quasimetric methods exhibit stronger horizon generalization compared to standard approaches, opening new directions for achieving powerful generalization properties in RL through geometric constraints.

## Method Summary
The authors propose using quasimetric architectures that enforce the triangle inequality to achieve planning invariance. The core method involves training a quasimetric distance function d(s,g) that satisfies d(s,g) ≤ d(s,w) + d(w,g) for all states s, goals g, and waypoints w. A greedy policy acts to minimize this distance. The theoretical analysis proves that if a quasimetric policy is optimal for short horizons, it guarantees optimality for arbitrarily long horizons. The practical generalization is measured by a horizon generalization parameter η, where η ≥ 0.5 enables unbounded reach. Experiments use Contrastive RL (CRL) and Contrastive Metric Distillation (CMD-1) with quasimetric architectures like Metric Residual Networks.

## Key Results
- Quasimetric policies are proven to be planning invariant (Theorem 1)
- Planning invariance leads to horizon generalization (Theorem 2)
- Empirical results show quasimetric methods achieve higher η values, with CMD-1 exceeding the critical η=0.5 threshold
- Success rates degrade more gracefully with distance for quasimetric approaches compared to standard MLPs

## Why This Works (Mechanism)

### Mechanism 1: Planning Invariance via Quasimetric Constraints
A quasimetric architecture enforces the triangle inequality (d(s,g) ≤ d(s,w) + d(w,g)). A greedy policy selecting actions to minimize this distance automatically "satisfices" the path via intermediate waypoints, making explicit planning redundant. This works when the distance function accurately reflects temporal dynamics and the policy acts greedily with respect to this distance.

### Mechanism 2: Recursive Horizon Extension
If a policy is planning-invariant and optimal for short horizons, it theoretically guarantees optimality for arbitrarily long horizons. The proof uses induction: optimality over distance c allows "stitching" two optimal segments of length c to form an optimal trajectory of length 2c, doubling repeatedly to cover the reachable space. This requires perfect optimality on the short-horizon training distribution.

### Mechanism 3: Success Attenuation
In practice, horizon generalization operates on a spectrum measured by η, the ratio of success at distance 2H vs H. If η ≥ 0.5, the cumulative "Reach" (area under the success curve) theoretically becomes unbounded. This assumes success probability monotonically decreases with distance, and each doubling of horizon degrades success by factor η.

## Foundational Learning

- **Concept: Quasimetrics & The Triangle Inequality**
  - **Why needed here:** The entire theoretical argument hinges on using a distance metric that satisfies the triangle inequality but allows asymmetry. Without this, you cannot guarantee trajectory "stitching."
  - **Quick check question:** Why is asymmetry (d(A,B) ≠ d(B,A)) important for navigation tasks, and how does the triangle inequality ensure logical consistency when concatenating paths?

- **Concept: Goal-Conditioned RL (GCRL)**
  - **Why needed here:** This is the base formalism. You must understand that the policy takes a state s and a goal g as input, and the objective is to reach g, typically measured by a sparse reward or distance metric.
  - **Quick check question:** How does a goal-conditioned policy differ from a standard policy in a fixed-horizon MDP, and what role does the "distance" play in defining the loss?

- **Concept: Invariance as Generalization**
  - **Why needed here:** The paper frames generalization not as "more data" but as an architectural property (invariance). Understanding this link is key to seeing why specific network layers matter more than just scaling parameters.
  - **Quick check question:** If a network is invariant to "planning," does it mean it can plan or that it doesn't need to plan? (Answer: It acts as if it has already planned).

## Architecture Onboarding

- **Component map:** State s, Goal g -> Quasimetric Distance Network d(s,g) -> Policy π(a|s,g) acts greedily: argmin_a d(s,a,g)
- **Critical path:** Replacing standard MLP critics with Quasimetric Architectures (e.g., MRN). A standard MLP does not enforce the triangle inequality and lacks the "self-consistency" required for planning invariance.
- **Design tradeoffs:** Quasimetric nets constrain the hypothesis space (enforce geometry), potentially reducing flexibility on the training set but enforcing generalization (higher η) on longer horizons. Contrastive losses (like InfoNCE) are preferred to estimate distances/probabilities.
- **Failure signatures:** High Bellman Error (architecture cannot represent distance accurately), Planning Discrepancy (adding explicit planner significantly boosts performance), The Random Policy Trap (perfect invariance but fails base case).
- **First 3 experiments:**
  1. Tabular Validation (Figure 5 Top): Train on tabular maze with short trajectories. Test if quasimetric version successfully reaches distant goals where standard metric regression fails.
  2. Planning Invariance Test (Figure 5 Right): Compare success rate when conditioned on final goal vs intermediate waypoint. If they differ significantly, architecture lacks invariance.
  3. Estimate η (Section 5.4): Train on goals within distance c. Evaluate success at 2c and 4c. Calculate ratio of success retention to determine if above η=0.5 threshold.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can planning-invariant geometry be extended to task specifications beyond goal-conditioning, such as rewards, language, or preferences?
- **Open Question 2:** Do planning invariance and horizon generalization properties hold in complex decision-making environments like robotic manipulation and language-based agents?
- **Open Question 3:** How does the violation of the "base case" assumption (optimality on all nearby goals) affect the degradation of horizon generalization?

## Limitations

- Theoretical results require perfect quasimetric constraints and exact optimality on short horizons, conditions rarely met in practice
- Connection between planning invariance and actual planning efficiency in stochastic environments not fully explored
- Empirical evaluation limited to simple goal-reaching tasks; unclear how principles extend to hierarchical planning

## Confidence

- **High Confidence:** The theoretical connection between quasimetric properties and planning invariance (Theorems 1 and 2) is mathematically sound
- **Medium Confidence:** Empirical evidence showing quasimetric methods achieve better horizon generalization is convincing but limited to specific domains
- **Low Confidence:** Claim that quasimetric architectures are the only viable path to planning invariance is overstated

## Next Checks

1. **Stochastic Environment Validation:** Test quasimetric policies in stochastic goal-reaching environments with noisy action outcomes. Measure whether planning invariance still correlates with horizon generalization under non-deterministic trajectories.

2. **Multi-Step Planning Efficiency:** Implement explicit planner using quasimetric distance to find optimal waypoint sequences for long-horizon goals. Compare performance and planning efficiency (number of planning steps) against greedy quasimetric policy.

3. **Architecture Ablation Study:** Systematically vary quasimetric architecture's constraint strength (soft vs hard triangle inequality enforcement). Measure how changes in constraint strength affect both base-case performance and horizon generalization parameter η.