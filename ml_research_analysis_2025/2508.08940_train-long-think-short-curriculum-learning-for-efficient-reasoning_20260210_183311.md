---
ver: rpa2
title: 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning'
arxiv_id: '2508.08940'
source_url: https://arxiv.org/abs/2508.08940
tags:
- reasoning
- accuracy
- length
- budget
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a curriculum learning approach for training
  efficient reasoning models using Group Relative Policy Optimization (GRPO). The
  method progressively tightens token budgets during training, enabling models to
  first explore solution strategies and then compress them into concise reasoning
  traces.
---

# Train Long, Think Short: Curriculum Learning for Efficient Reasoning

## Quick Facts
- **arXiv ID:** 2508.08940
- **Source URL:** https://arxiv.org/abs/2508.08940
- **Reference count:** 4
- **Primary result:** Curriculum learning with GRPO outperforms fixed-budget training, achieving 86.2% accuracy on GSM8K vs 82.71% for fixed budget while reducing average length from 258.4 to 88.8 tokens.

## Executive Summary
This paper introduces a curriculum learning approach for training efficient reasoning models using Group Relative Policy Optimization (GRPO). The method progressively tightens token budgets during training, enabling models to first explore solution strategies and then compress them into concise reasoning traces. Experiments with Q WEN -2.5-7B show that curriculum learning consistently outperforms fixed-budget baselines, achieving higher accuracy (e.g., 86.2% on GSM8K vs. 82.71% for fixed budget) and improved token efficiency (e.g., reducing average length from 258.4 to 88.8 tokens). The gains hold across multiple datasets including GSM8K, MATH500, and SV AMP, and are robust to reward weighting and decay schedule design. The study demonstrates that gradual budget tightening serves as a powerful inductive bias for training efficient reasoning models.

## Method Summary
The approach trains reasoning models using GRPO with progressively tightening token budgets. Starting with generous token budgets (256 tokens), the model explores long chain-of-thought reasoning. An exponential decay schedule then gradually reduces the budget to the target (87 tokens) over training. The method combines three reward signals: correctness (via math-verify), length adherence (using triangular reward functions), and formatting compliance. Group sampling (G=8) enables relative advantage normalization without requiring a value function. The curriculum design encourages models to discover effective reasoning strategies before compressing them into concise traces.

## Key Results
- Curriculum training achieves 86.2% accuracy on GSM8K at 88.8 average tokens vs. 82.71% for fixed budget at 87.0 tokens
- Triangular length rewards outperform flat-band rewards (86.2% vs 84.6% on GSM8K)
- Exponential decay benefits easy tasks while linear decay improves harder tasks (MATH500: 37.4%→42.8%)
- Consistent improvements across multiple datasets including SVAMP, College Math, and GSM+

## Why This Works (Mechanism)

### Mechanism 1
Progressive budget tightening serves as an inductive bias that enables models to first discover reasoning strategies, then compress them. Starting with generous token budgets allows exploration of long reasoning paths, while exponential decay forces distillation into concise traces. This prevents premature truncation that occurs with fixed budgets.

### Mechanism 2
Triangular length rewards preserve accuracy better than flat-band rewards by encouraging full-budget exploration before compression. The ramp-up, plateau, ramp-down shape creates gradient incentives to use allocated budget rather than settling for minimally short correct answers.

### Mechanism 3
Group-relative advantage normalization in GRPO stabilizes learning from sparse correctness signals without requiring a value function. By normalizing rewards across groups of sampled responses, the method reinforces responses that outperform cohort averages rather than just achieving high absolute rewards.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The core RL algorithm that enables training without value functions through group sampling and relative advantage normalization. *Quick check:* Given rewards [0, 0, 0, 1, 0, 0, 0, 0], what is the advantage of the correct response? (μ=0.125, σ≈0.35, A≈2.5)

- **Curriculum Learning**: The principle of starting with easy conditions and gradually increasing difficulty. *Quick check:* Why might starting with a tight budget prevent learning? (The model never discovers successful strategies to compress.)

- **KL-Regularized Reinforcement Learning**: The approach includes KL penalties to prevent catastrophic forgetting. *Quick check:* If training improves reasoning but degrades general language abilities, what hyperparameter should you adjust? (Increase β to strengthen KL penalty.)

## Architecture Onboarding

- **Component map:**
  Prompt Template → Model (QWEN-2.5-7B) → Response Sampling (G=8) → Per-response Rewards (Correctness, Length, Formatting) → GRPO Loss → Policy Update (+ KL penalty)

- **Critical path:**
  1. Implement reward decomposition first (correctness, length, formatting)
  2. Set up exponential budget schedule with configurable γ and T
  3. Integrate GRPO training loop with group sampling (G=8)
  4. Add KL penalty tracking to monitor policy drift

- **Design tradeoffs:**
  - Exponential vs. Linear decay: Exponential favors efficiency; linear improves performance on harder tasks
  - Decay speed: Fast/moderate decay gives best average accuracy; slow decay helps easy tasks but fails on hard ones
  - Reward weights: Length-heavy maximizes compression; correctness-heavy maximizes accuracy at small token cost

- **Failure signatures:**
  - Model generates well-formatted but incorrect answers: Increase λ_c or check verifier quality
  - Model ignores budget constraint: Check length reward implementation
  - Accuracy collapses mid-training: KL penalty may be too weak; increase β
  - All responses in a group get identical rewards: Problem may be too easy/hard

- **First 3 experiments:**
  1. Reproduce GSM8K curriculum vs. fixed-budget comparison with B₀=256, B_f=87, γ=0.7, T=150
  2. Ablate decay schedule (exponential vs. linear) on MATH500
  3. Test reward function shape (triangular vs. band) on GSM8K

## Open Questions the Paper Calls Out

- How does curriculum-based length control behave at different model scales (1.3B, 3B, 13B, 70B)?
- Does curriculum learning for length-controlled reasoning transfer effectively to non-mathematical domains?
- What is the optimal interaction between decay schedule design and task complexity?

## Limitations
- Missing critical hyperparameters (learning rate, batch size, reward scaling factors) affect reproducibility
- Limited evaluation across different model scales; results may not generalize beyond QWEN-2.5-7B
- Performance on harder tasks (MATH500) is significantly worse than on easier tasks (GSM8K)
- No systematic analysis of reward weight sensitivity or universal parameter selection

## Confidence

**High Confidence Claims:**
- Curriculum learning outperforms fixed-budget training on GSM8K
- Progressive budget tightening provides effective inductive bias
- GRPO with triangular rewards enables stable training without value functions
- Method achieves consistent token efficiency improvements across datasets

**Medium Confidence Claims:**
- Exponential decay optimal for easy tasks while linear better for harder tasks
- Triangular length rewards consistently outperform flat-band rewards
- Specific decay intervals (I=75, I=150) represent optimal timing

**Low Confidence Claims:**
- Approach generalizes effectively to all mathematical reasoning tasks without tuning
- Same curriculum design applies equally well to non-mathematical domains
- Method scales seamlessly across different model sizes

## Next Checks

1. **Ablation of Decay Schedule Across Difficulty Spectrum**: Test exponential, linear, and hybrid decay schedules on curated problems spanning easy to very hard reasoning tasks, measuring learning stability and convergence speed.

2. **Reward Weight Sensitivity Analysis**: Systematically vary λ_c and λ_ℓ across a wider range (0.1 to 0.9) while holding other parameters constant to map the Pareto frontier between accuracy and token efficiency.

3. **Cross-Model Generalization Study**: Apply the curriculum approach to at least two additional model scales (e.g., 3B and 13B parameter versions) to compare optimal decay schedules and reward weights across model sizes.