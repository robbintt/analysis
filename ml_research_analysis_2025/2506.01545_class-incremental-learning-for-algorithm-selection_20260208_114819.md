---
ver: rpa2
title: Class Incremental Learning for Algorithm Selection
arxiv_id: '2506.01545'
source_url: https://arxiv.org/abs/2506.01545
tags:
- training
- data
- learning
- accuracy
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Class Incremental Learning (CIL) to algorithm
  selection for optimization, addressing the challenge of updating models when new
  solvers (classes) arrive in a data stream without forgetting past knowledge. Using
  a bin-packing dataset with four deterministic heuristics, the study benchmarks eight
  CIL methods across six possible solver-pair streams, each with five training set
  sizes.
---

# Class Incremental Learning for Algorithm Selection

## Quick Facts
- arXiv ID: 2506.01545
- Source URL: https://arxiv.org/abs/2506.01545
- Reference count: 22
- Primary result: Replay and Gradient Episodic Memory (GEM) achieve 82.3% final accuracy in streaming algorithm selection, with only 6% loss vs. cumulative training

## Executive Summary
This paper applies Class Incremental Learning (CIL) to algorithm selection for optimization, addressing the challenge of updating models when new solvers arrive in a data stream without forgetting past knowledge. Using a bin-packing dataset with four deterministic heuristics, the study benchmarks eight CIL methods across six possible solver-pair streams. Rehearsal-based methods—particularly Replay and GEM—significantly outperform other CIL categories, achieving final accuracy of 82.3% across all four classes. The order of solver presentation affects performance, suggesting some solver pairs are harder to distinguish. These results demonstrate that rehearsal-based CIL is a viable approach for streaming algorithm selection, balancing new solver adaptation with preservation of past solver knowledge.

## Method Summary
The study uses a bin-packing dataset with 4000 instances and 4 deterministic heuristics (FF, BF, NF, WF). It benchmarks 8 CIL methods (EWC, MAS, SI, GEM, AGEM, Feature Replay, Experience Replay, LwF) across 6 possible class-pair streams, each with 5 training set sizes. The model architecture is an MLP, with rehearsal methods storing 100 exemplars from previous tasks. Performance is measured by per-task accuracy before/after updates, final 4-class accuracy, and comparison to Oracle and Cumulative baselines.

## Key Results
- Replay and GEM achieve 82.3% final accuracy, significantly outperforming regularization-based methods (46% final accuracy)
- Buffer size of 100 exemplars results in only 6% accuracy loss compared to cumulative training scenario
- Solver presentation order affects performance, with some pairs being inherently harder to distinguish
- Training set size has minimal impact on final accuracy
- Parameter regularization methods (EWC, MAS, SI, LwF) fail to retain knowledge of previous tasks

## Why This Works (Mechanism)

### Mechanism 1: Rehearsal-Based Gradient Anchoring
If rehearsal-based strategies are used, the model mitigates catastrophic forgetting by anchoring gradient updates to a sparse history of past data distributions. Methods like Replay and Gradient Episodic Memory (GEM) store a small buffer of exemplars (100 instances in this study). During the update for a new solver (Task $D_2$), these exemplars are reintroduced to the loss function. Replay interleaves them with new data, while GEM uses them as inequality constraints to prevent gradient updates from increasing the loss on previous tasks. The exemplars stored in memory remain representative of the feature distribution of the historical tasks, and the storage budget (buffer size) is sufficient to capture class boundaries.

### Mechanism 2: Solver Distinctiveness and Order Sensitivity
The achieved accuracy is conditional on the pairwise distinctiveness of the solver classes; the learning mechanism struggles when sequential classes require similar decision boundaries. The paper suggests that the order of presentation impacts performance because distinguishing between certain heuristics (e.g., BF vs. WF) is inherently harder. If a model learns a difficult boundary later ($D_2$), it may struggle to recalibrate the feature space without disrupting the boundary learned for $D_1$. The deterministic heuristics (FF, BF, NF, WF) have overlapping performance profiles on subsets of instances, creating non-trivial decision boundaries.

### Mechanism 3: Regularization Insufficiency in Algorithm Selection
Parameter regularization methods (e.g., EWC, SI) likely fail because the weight importance calculated during $D_1$ does not effectively preserve the decision boundaries required for $D_2$ in this domain. Regularization methods penalize changes to "important" weights. The paper observes near-zero retention of $D_1$ knowledge with these methods. This implies that the feature space for algorithm selection requires significant restructuring to accommodate new solvers, and penalizing weight changes restricts the plasticity needed to learn $D_2$ while falsely assuming the old weights are sufficient for old classes.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: This is the central problem the paper addresses. A standard model trained on $D_2$ (new solvers) loses almost all predictive power for $D_1$ (old solvers) unless specific CIL mechanisms are applied.
  - Quick check question: If I train a model on Class A and then train it solely on Class B, what happens to its accuracy on Class A? (Answer: It typically drops to near zero/random guessing).

- **Concept: Class Incremental Learning (CIL)**
  - Why needed here: This distinguishes the scenario from "Task Incremental Learning." In CIL, the model must predict the correct class label among *all* seen classes (Solver 1, 2, 3, 4) at inference time, without being told which task the instance belongs to.
  - Quick check question: At inference time, does the model receive a "task ID" to help narrow down the class prediction? (In CIL: No).

- **Concept: Algorithm Selection (AS)**
  - Why needed here: This is the application domain. The goal isn't just image classification, but mapping optimization instance features to the best performing heuristic (solver) from a portfolio.
  - Quick check question: What acts as the "label" in this machine learning setup? (Answer: The identity of the solver that yields the best performance/Falkenauer metric).

## Architecture Onboarding

- **Component map**: Data Stream -> Model (MLP) -> Strategy (CIL method) -> Buffer (exemplars) -> Evaluation
- **Critical path**:
  1. Train model on $D_1$ (2 solvers)
  2. "Task boundary" detected: New solvers ($D_2$) arrive
  3. **Update Head**: The output layer is modified to accommodate new class labels
  4. **Replay/Update**: Train model on $D_2$ data + Buffer data (if Replay)
  5. **Evaluation**: Test on instances from all 4 solvers

- **Design tradeoffs**:
  - **Replay vs. Regularization**: The paper clearly demonstrates a tradeoff where Replay requires storage (privacy concerns mentioned in intro) but works (~82% acc), while Regularization is storage-free but fails (~46% acc)
  - **Buffer Size vs. Performance**: The paper notes 100 exemplars resulted in only 6% loss compared to Cumulative training
  - **Order Dependency**: The system is sensitive to the order in which solvers are introduced (Table 3)

- **Failure signatures**:
  - **Collapse to New Task**: Accuracy on old classes ($D_1$) drops to <5% while accuracy on new classes ($D_2$) is >90%. (Observed in EWC, LwF)
  - **High Variance**: Standard deviation spikes in methods like AGEM (0.117 final accuracy), indicating unstable convergence depending on the stream order

- **First 3 experiments**:
  1. **Establish Upper Bound**: Train a "Cumulative" model where $D_1$ data is retained and combined with $D_2$ to verify the maximum achievable accuracy on the dataset (expected ~84-88%)
  2. **Verify Forgetting**: Train a Naive (Fine-tuning) baseline. Train on $D_1$, then train on $D_2$ without any CIL strategy. Confirm that $D_1$ accuracy approaches zero
  3. **Order Sensitivity Check**: Run the Replay experiment on two different stream orders (e.g., [BF, FF] → [NF, WF] vs. [BF, NF] → [FF, WF]) to verify that accuracy fluctuates based on solver similarity as suggested by Table 3

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the efficacy of rehearsal-based CIL methods scale when applied to data streams containing significantly more than two tasks ($B > 2$)?
  - Basis in paper: [inferred] The methodology explicitly states, "In all experiments, we set $B = 2$," limiting the validation to a single incremental step.
  - Why unresolved: It is unclear if the 7% forgetting loss remains stable or compounds as the model undergoes successive updates over longer time periods.
  - What evidence would resolve it: Benchmark results on streams with $B=5, 10, 20$ tasks, measuring accuracy degradation per step.

- **Open Question 2**: Do the findings generalize to algorithm selection scenarios involving larger portfolios or continuous domains like SAT and TSP?
  - Basis in paper: [explicit] The study uses a specific "bin-packing dataset... associated with 4 deterministic heuristic solvers" and acknowledges the need to evaluate on streaming data generally.
  - Why unresolved: The 82.3% accuracy is achieved on a small, discrete classification task; results may differ with high-dimensional feature spaces or noisy stochastic solvers.
  - What evidence would resolve it: Application of the Replay and GEM methods to standard ASlib scenarios with larger solver sets.

- **Open Question 3**: Can the difficulty of distinguishing specific solver pairs be theoretically characterized to predict catastrophic forgetting?
  - Basis in paper: [explicit] The conclusion notes that "the order in which classes appear in a stream can impact results, suggesting that some pairs of classes are more difficult to distinguish than others."
  - Why unresolved: The paper observes the variance in performance based on order but does not define the properties that make specific pairs (e.g., BF, WF) harder to separate.
  - What evidence would resolve it: A theoretical analysis correlating inter-class feature distance (or solver behavioral similarity) with forgetting rates.

## Limitations

- The study lacks explicit architecture and hyperparameter details, blocking exact replication
- The superiority of rehearsal methods (82.3% vs 46% accuracy) is contingent on specific buffer size (100 exemplars) and may not generalize to other capacities
- Solver order effects are observed but not systematically analyzed to identify what makes specific pairs harder to distinguish
- Privacy constraints prohibiting exemplar storage are mentioned but not explored with alternatives like pseudo-rehearsal

## Confidence

- **High Confidence**: Rehearsal-based methods (Replay, GEM) significantly outperform regularization-based methods (EWC, MAS, SI, LwF) in this bin-packing setting; buffer size of 100 exemplars yields only 6% accuracy loss versus cumulative training
- **Medium Confidence**: Solver presentation order impacts final accuracy due to inherent pairwise distinctiveness; however, the exact drivers of difficulty (e.g., decision boundary complexity) are not measured
- **Medium Confidence**: Parameter regularization methods fail here because the feature space requires significant restructuring for new solvers, making weight importance metrics inadequate—this is inferred from observed failure rather than proven

## Next Checks

1. **Architectural Sensitivity Test**: Vary the MLP depth/width and retraining epochs to verify that observed superiority of Replay/GEM is robust to architectural changes
2. **Buffer Size Sweep**: Repeat the Replay experiment with buffer sizes {50, 200, 500} to confirm that the 6% accuracy loss at size 100 is not a local optimum
3. **Solver Pair Similarity Analysis**: Compute pairwise overlap in Falkenauer metric distributions for each solver to correlate with observed order-dependent accuracy drops, quantifying the link between feature similarity and forgetting susceptibility