---
ver: rpa2
title: Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings
arxiv_id: '2505.16877'
source_url: https://arxiv.org/abs/2505.16877
tags:
- kgcp
- prediction
- coverage
- condkgcp
- covgap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  in Knowledge Graph Embedding (KGE) methods, specifically aiming to achieve predicate-conditional
  coverage guarantees. The authors propose COND KGCP, a novel method that approximates
  predicate-conditional coverage while maintaining compact prediction sets.
---

# Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings

## Quick Facts
- **arXiv ID:** 2505.16877
- **Source URL:** https://arxiv.org/abs/2505.16877
- **Reference count:** 40
- **Primary result:** COND KGCP achieves coverage gaps closest to the empirical lower bound while maintaining compact prediction sets, with efficiency rates consistently lower than competing approaches.

## Executive Summary
This paper addresses the challenge of uncertainty quantification in Knowledge Graph Embedding (KGE) methods, specifically aiming to achieve predicate-conditional coverage guarantees. The authors propose COND KGCP, a novel method that approximates predicate-conditional coverage while maintaining compact prediction sets. The key innovation involves merging predicates with similar vector representations to increase calibration data and augmenting the calibration process with rank information to reduce prediction set sizes. Theoretical guarantees are provided, showing that COND KGCP achieves conditional coverage probabilities tightly centered around the desired confidence level. Empirically, COND KGCP outperforms five baseline methods across six KGE models and two benchmark datasets (WN18 and FB15k), demonstrating superior trade-offs between conditional coverage probability and prediction set size.

## Method Summary
COND KGCP extends marginal conformal prediction to achieve predicate-conditional coverage for KGE link prediction. The method first merges predicates with similar vector representations to create calibration subgroups, then applies dual calibration using both rank and score information. For each predicate group, it computes a rank threshold to prefilter entities and a score threshold for final prediction set selection. The approach uses a dual error budget split between rank and score calibration, controlled by hyperparameter γ, to balance coverage tightness and prediction set size.

## Key Results
- COND KGCP achieves coverage gaps closest to the empirical lower bound across all model-dataset combinations
- The method maintains compact prediction sets with consistently lower efficiency rates than competing approaches
- Predicate merging and rank calibration both contribute to improved performance, as shown in ablation studies
- Optimal hyperparameters are γ ∈ {0.01, 0.1} and φ ∈ {20, 50, 100, 200} depending on dataset

## Why This Works (Mechanism)

### Mechanism 1: Predicate Merging for Subgroup Enrichment
Grouping predicates with similar vector representations increases calibration set sizes per subgroup, enabling more stable threshold estimation while preserving conditional coverage properties. Predicates with sufficient data form their own partitions; predicates with few triples are assigned to the most similar well-populated partition using negative Manhattan distance similarity. The core assumption is that predicates with similar vector representations have similar nonconformity score distributions, justifying shared calibration thresholds.

### Mechanism 2: Rank-Based Prefiltering
Applying a rank threshold before score calibration filters out low-confidence entities, reducing prediction set size while preserving coverage guarantees. For each predicate group, the method computes the miscoverage error for different rank thresholds and selects the smallest rank threshold that maintains coverage below the error rate. The core assumption is that the joint probability of (score ≤ threshold AND rank ≤ k̂) is bounded by the score-only probability.

### Mechanism 3: Dual Calibration Error Budget
The hyperparameter γ controls an error budget split between rank and score calibration, allowing flexible trade-offs between coverage tightness and prediction set size. The rank calibration accounts for γ fraction of the total error budget, leaving (1−γ) fraction for score calibration. Coverage bounds show γ controls asymmetry between upper and lower deviation.

## Foundational Learning

- **Concept: Marginal vs. Conditional Coverage in Conformal Prediction**
  - **Why needed here:** The entire motivation hinges on the limitation of marginal coverage vs. conditional coverage. Without understanding this distinction, the value proposition of COND KGCP is unclear.
  - **Quick check question:** If a medical KGE has 99% coverage on "treats_disease" but only 60% on "contraindicated_for," would marginal coverage at 90% detect this problem?

- **Concept: Nonconformity Scores from KGE Models**
  - **Why needed here:** The method depends on converting KGE plausibility scores to nonconformity scores. Understanding how different KGE scoring functions produce score distributions affects threshold selection.
  - **Quick check question:** Why does TransE use −||h+r−t|| while the SOFTMAX nonconformity score uses 1 − exp(score)/Σexp(scores)?

- **Concept: Quantile-Based Threshold Selection**
  - **Why needed here:** Equation 1 defines the empirical quantile with a finite-sample correction. Understanding this correction is essential for interpreting why small calibration sets produce unstable thresholds.
  - **Quick check question:** For a calibration set of 10 triples and target coverage 90%, what rank position determines the threshold?

## Architecture Onboarding

- **Component map:**
Training Data T_tr → KGE Model M_θ → Entity/Predicate Embeddings → Predicate Merging via Algorithm 1 → Partition P → Per-partition Rank Calibration → k̂(g) for each g ∈ P → Per-partition Score Calibration → ŝ_ϵ′(g)(T_g) → Test Query q → Compute scores for all entities → Filter by rank ≤ k̂(g) → Filter by score ≤ ŝ_ϵ′(g) → Prediction Set Ĉ(q)

- **Critical path:**
1. Pre-train KGE model (shared with baseline; most expensive)
2. Compute predicate embeddings and run Algorithm 1 to form partitions (one-time, O(|R|²) similarity computations)
3. For each partition g: compute rank thresholds via validation set, then compute score thresholds (calibration step)
4. At inference: score all entities (O(|E|·d)), sort (O(|E| log|E|)), apply dual filters

- **Design tradeoffs:**
  - **ϕ (minimum partition size):** Higher ϕ → coarser groupings → smaller CovGap reduction but smaller AveSize
  - **γ (error budget allocation):** Lower γ → tighter coverage lower bound → potentially larger sets
  - **Similarity function:** Paper uses negative Manhattan distance; cosine similarity or learned similarity metrics are unexplored alternatives

- **Failure signatures:**
  - **CovGap not improving:** Check if partitions are too coarse (increase ϕ) or if predicate embeddings don't reflect score distribution similarity
  - **AveSize exploding:** Check if rank threshold k̂(g) is too large (rank calibration failing to filter) or if base model has poor Hits@K performance
  - **Conditional coverage violated for specific predicates:** Examine calibration set size per partition

- **First 3 experiments:**
1. **Reproduce Table 2 ablation:** Train any KGE model (e.g., TransE) on WN18, run COND KGCP with and without predicate merging, with and without rank calibration. Verify that merging reduces CovGap and rank calibration reduces AveSize.
2. **Hyperparameter sensitivity sweep:** For a single model-dataset pair, sweep ϕ ∈ [20, 50, 100, 200] and γ ∈ [0.01, 0.1, 0.5]. Plot CovGap vs. AveSize to visualize the trade-off frontier.
3. **Coverage calibration check:** For target coverage 0.9, verify that empirical conditional coverage Cov_r falls within the bounds of Proposition 1 for each predicate group. Identify any predicates where coverage deviates significantly from 1−ϵ.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the conformal prediction guarantees of COND KGCP be extended to handle covariate shift where the distribution of calibration and test queries differs?
- Basis in paper: The authors state in the Limitations section that they are "working on extending our approach to handle covariate shift, where only the input distribution changes while the conditional distribution remains unchanged."
- Why unresolved: The current theoretical guarantees rely on the assumption that triples are drawn i.i.d. (or are exchangeable), which is often violated in dynamic real-world graphs.
- What evidence would resolve it: A theoretical modification of the coverage bounds that remains valid under defined distribution shifts, or empirical demonstration of robust coverage on temporal knowledge graph datasets where the test set is chronologically shifted from the calibration set.

### Open Question 2
- Question: Can incorporating semantic or structural features (beyond vector proximity) improve the predicate merging process and enhance conditional coverage?
- Basis in paper: The authors note: "future work could explore incorporating additional features, such as the semantic meaning of predicates, to further enhance the merging process and improve robustness in diverse scenarios."
- Why unresolved: The current method relies solely on the similarity of vector representations to merge predicates, which may fail to capture functional similarities if the embedding geometry is distorted or the vectors are uninformative.
- What evidence would resolve it: Experiments comparing the current merging strategy against a semantic-aware strategy on datasets with rich metadata, showing improved coverage gaps for minority predicates.

### Open Question 3
- Question: Is the assumption that "similarity of vector representations corresponds to the similarity of the distribution of nonconformity scores" valid in non-Euclidean embedding spaces?
- Basis in paper: The paper utilizes negative Manhattan distance for predicate merging. However, the Related Work mentions "hyperbolic" spaces, and the experiments are restricted to Euclidean/Complex models.
- Why unresolved: In curved spaces (e.g., hyperbolic geometry), standard distance metrics behave differently; vectors close in distance may not share the same structural role, potentially invalidating the merging heuristic used to increase calibration set size.
- What evidence would resolve it: An ablation study applying COND KGCP to hyperbolic KGE models, analyzing if merging predicates based on standard distance metrics maintains the link between vector similarity and score distribution similarity.

## Limitations
- The core assumption that predicates with similar vector representations have similar nonconformity score distributions is weakly supported empirically and may fail for certain predicate groups
- The method inherits KGE model limitations - poor base model performance directly impacts prediction set efficiency
- The rank calibration mechanism assumes monotonic relationship between scores and ranks, which may not hold for all KGE scoring functions

## Confidence

- **High confidence:** The core algorithm implementation and empirical evaluation methodology are well-specified. The theoretical guarantees follow standard conformal prediction arguments. The ablation studies provide strong evidence for the effectiveness of predicate merging and rank calibration.
- **Medium confidence:** The hyperparameter sensitivity analysis suggests robustness, but optimal settings may be dataset-dependent. The theoretical bounds are asymptotic and may not reflect finite-sample behavior in all cases.
- **Low confidence:** The assumption about predicate embedding similarity correlating with score distribution similarity is weakly supported empirically. The paper doesn't explore alternative similarity metrics or predicate grouping strategies.

## Next Checks

1. **Predicate similarity validation:** For each merged predicate group, compute the empirical correlation between vector similarity and score distribution similarity. Identify groups where this correlation is low to test the core assumption.

2. **Coverage calibration verification:** For each predicate group, verify that empirical conditional coverage falls within the theoretical bounds of Proposition 1 across multiple coverage levels (ε ∈ {0.1, 0.2, 0.3}). Flag any systematic violations.

3. **Rank-score correlation analysis:** For correct answers in the test set, compute the correlation between nonconformity scores and rank positions. Identify predicates where this correlation is negative or near-zero to assess rank calibration effectiveness.