---
ver: rpa2
title: 'SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction
  Using Large Language Models'
arxiv_id: '2503.06475'
source_url: https://arxiv.org/abs/2503.06475
tags:
- knowledge
- relationships
- data
- stroke
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduced SKG-LLM, a knowledge graph constructed from
  stroke-related articles using mathematical and large language models. The model
  utilized GPT-4 for data preprocessing and embedding extraction throughout the KG
  construction process.
---

# SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models

## Quick Facts
- arXiv ID: 2503.06475
- Source URL: https://arxiv.org/abs/2503.06475
- Reference count: 40
- This study introduced SKG-LLM, a knowledge graph constructed from stroke-related articles using mathematical and large language models.

## Executive Summary
This paper presents SKG-LLM, a stroke knowledge graph built from 1,286 biomedical articles using GPT-4 for preprocessing and embedding extraction. The model combines Bayesian networks with tensor decomposition to probabilistically model entity relationships, achieving high precision (0.906→0.923) and recall (0.923→0.918) after expert review. The resulting KG contains 2,692 nodes, 5,012 edges, 13 node types, and 24 edge types, outperforming baselines like Wikidata and WN18RR on precision and recall metrics.

## Method Summary
The SKG-LLM pipeline uses GPT-4 for text cleaning, normalization, and embedding extraction from PubMed articles (2020-2024). A data tensor combines entities, actions, and attributes, which is processed through mutual information calculations and CP tensor decomposition. Bayesian networks and LDA extract relationships, while the EM algorithm optimizes parameters. Final validation includes GPT-4 evaluation and expert review to improve precision and recall metrics.

## Key Results
- Achieved precision of 0.906 and recall of 0.923, improving to 0.923 precision and 0.918 recall after expert review
- KG contains 2,692 nodes, 5,012 edges, 13 node types, and 24 edge types
- Outperformed Wikidata and WN18RR baselines, particularly in precision and recall metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4-based preprocessing improves entity-relationship extraction quality for domain-specific KGs.
- Mechanism: GPT-4 generates high-dimensional embeddings from cleaned text, capturing semantic features that feed into tensor construction. The LLM's linguistic understanding enables more accurate entity disambiguation and probability estimation in subsequent Bayesian network modeling.
- Core assumption: GPT-4 embeddings preserve domain-specific semantics for biomedical stroke literature better than traditional embedding methods.
- Evidence anchors: [abstract] "GPT-4 was used for data pre-processing, and the extraction of embeddings was also done by GPT-4 in the whole KG construction process." [section 2.2.2] "GPT-4 was used as a language model to extract these meaningful vectors... These extracted vectors can capture the complex semantic features of the text."
- Break condition: If GPT-4 embeddings fail to capture stroke-specific terminology (e.g., confuses "ischemic" vs "hemorrhagic" contexts), downstream tensor decomposition will propagate errors.

### Mechanism 2
- Claim: Combining Bayesian networks with tensor decomposition enables probabilistic modeling of entity interactions.
- Mechanism: Bayesian networks compute conditional probabilities P(R|E1,E2) between entities, while CP tensor decomposition factorizes the data tensor into entity-action-attribute components. The mutual information calculation (Equation 4) quantifies entity dependencies before probability matrix construction.
- Core assumption: Entity relationships in stroke literature follow probabilistic patterns learnable via joint probability distributions.
- Evidence anchors: [section 2.3.1] "Mutual information I(X;Y) between two entities... is used to show how much knowledge of one entity reduces uncertainty about another." [section 2.4.2] "The probability of the relationship R according to the data tensor Tdata can be estimated" via decomposed component vectors.
- Break condition: If entity relationships are highly sparse or non-stationary across the 2020-2024 literature window, probability matrices become unreliable.

### Mechanism 3
- Claim: Expert review corrects LLM extraction errors and improves precision more than recall.
- Mechanism: Expert review identifies false positives (incorrect relationships) that GPT-4 accepted, shifting the precision-recall tradeoff. Cohen's kappa quantifies inter-rater agreement during validation.
- Core assumption: Domain experts can reliably identify spurious relationships that probabilistic models miss.
- Evidence anchors: [abstract] "Expert reviews further improved the results and increased precision to 0.923 and recall to 0.918" (vs. 0.906 precision, 0.923 recall pre-review). [section 4.3] "In this evaluation, experts evaluate the relationships and present the ranking as a consolidated weighted average."
- Break condition: If expert availability is limited or domain expertise varies, scalability of this correction mechanism degrades.

## Foundational Learning

- Concept: **Knowledge Graph Construction Pipeline (NER → RE → Link Prediction)**
  - Why needed here: SKG-LLM uses Named Entity Recognition to identify stroke-related entities, Relation Extraction to connect them, and link prediction algorithms (TransE, RotatE, etc.) for evaluation. Understanding this sequence is prerequisite to debugging extraction failures.
  - Quick check question: Can you trace how a "tPA treats ischemic stroke" relationship flows from raw PubMed text through tensor decomposition to a graph edge?

- Concept: **Mutual Information for Entity Dependency Quantification**
  - Why needed here: Equation 4 computes I(X;Y) to measure how much one entity reveals about another. This drives probability matrix construction and determines which entity pairs merit relationship edges.
  - Quick check question: Given two entities "hypertension" and "stroke," what would high vs. low mutual information imply for KG edge weight?

- Concept: **CP Tensor Decomposition**
  - Why needed here: Equation 10 factorizes the data tensor (entities ⊗ actions ⊗ attributes) into rank-R components. This reduces dimensionality while preserving relational structure for probability estimation.
  - Quick check question: If rank R is too low, what information loss occurs in the reconstructed tensor?

## Architecture Onboarding

- Component map: Data Layer → Embedding Layer → Probability Layer → Extraction Layer → Optimization Layer → Validation Layer
- Critical path: Cleaned text → Embeddings → Tensor construction → Probability estimation → Extraction → Expert validation. Failures in tensor construction (step 3) cascade to all downstream steps.
- Design tradeoffs:
  - GPT-4 preprocessing: Higher quality embeddings vs. API cost and latency
  - Expert review: Higher precision vs. scalability bottleneck
  - CP decomposition rank R: Higher R captures more relationships vs. computational cost and overfitting risk
  - EM iterations: More iterations improve convergence vs. diminishing returns
- Failure signatures:
  - Low precision despite high recall: Indicates false positive relationships—check expert review gaps
  - Inconsistent probability matrices: May indicate spurious mutual information from noisy text cleaning
  - Poor link prediction MRR: Tensor decomposition rank may be insufficient or entity embeddings misaligned
- First 3 experiments:
  1. **Baseline comparison**: Run pipeline without GPT-4 preprocessing (use standard BERT embeddings) to quantify embedding quality impact on final precision/recall.
  2. **Rank sensitivity analysis**: Vary CP decomposition rank R (e.g., 10, 50, 100) and measure MRR/P@K on held-out relationships to identify optimal dimensionality.
  3. **Expert review ablation**: Remove expert validation step and compare consistency scores (Table 5, 0.93 vs. baseline) to quantify human correction value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would integrating advanced deep learning architectures impact the accuracy of parameter estimation compared to the current Expectation-Maximization (EM) algorithm?
- Basis in paper: [explicit] The conclusion states, "Integration of deep learning and machine learning approaches can improve the model."
- Why unresolved: The current study relies on probabilistic methods (Bayesian networks, LDA) and EM for optimization, leaving potential improvements from deep learning unexplored.
- What evidence would resolve it: A comparative analysis measuring precision and recall gains when swapping the EM step for a deep learning-based optimizer.

### Open Question 2
- Question: To what extent does using GPT-4 for both construction and evaluation introduce a circularity bias or "self-agreement" hallucinations?
- Basis in paper: [inferred] The methodology uses GPT-4 for preprocessing, extraction, and "further validation" (Table 5), without acknowledging the risk that an LLM might validate its own hallucinations.
- Why unresolved: The paper reports high "Consistency (GPT-4)" scores but does not verify if GPT-4 is merely confirming its own generated context rather than objective truth.
- What evidence would resolve it: A validation study using a distinct, non-LLM-based evaluator or a human-annotated ground truth dataset not derived from GPT-4.

### Open Question 3
- Question: How does the model's computational efficiency and graph quality scale when expanding the dataset beyond 1,286 articles?
- Basis in paper: [explicit] The authors note, "Using better keywords and extracting more articles allows for presenting a better model."
- Why unresolved: The study limits its scope to a specific set of papers; the feasibility of running the proposed tensor decomposition and LLM pipeline on the full volume of biomedical literature is unknown.
- What evidence would resolve it: Performance benchmarks (latency, cost, and F1-score) measured across incrementally larger datasets (e.g., 10k and 100k documents).

## Limitations
- Heavy reliance on GPT-4's proprietary embeddings without external validation of their domain-specific quality for stroke literature
- Expert review scalability remains unproven for larger knowledge graphs
- Comparison methodology with Wikidata and WN18RR lacks detail on dataset overlap and validation approach

## Confidence
- **High Confidence**: The mathematical framework (mutual information calculation, CP tensor decomposition, Bayesian networks) is well-specified and theoretically sound
- **Medium Confidence**: The GPT-4 preprocessing pipeline likely improves entity extraction quality, but exact prompts and parameters are unspecified
- **Low Confidence**: Claims of outperforming Wikidata and WN18RR require independent verification due to insufficient methodological detail

## Next Checks
1. **Reproducibility Test**: Implement the pipeline without GPT-4 (using BERT embeddings) to quantify the actual contribution of LLM-based preprocessing to final metrics
2. **Robustness Analysis**: Vary CP decomposition rank R and measure MRR/P@K stability across different literature subsets to assess overfitting risk
3. **Expert Validation Scalability**: Conduct time-motion studies on expert review process to determine practical limits for KG size before human validation becomes infeasible