---
ver: rpa2
title: 'Bias in Large Language Models Across Clinical Applications: A Systematic Review'
arxiv_id: '2504.02917'
source_url: https://arxiv.org/abs/2504.02917
tags:
- bias
- clinical
- were
- gender
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review identified pervasive bias in large language
  models (LLMs) across clinical applications, with 38 studies revealing significant
  issues stemming from both data-related and model-related sources. Bias manifested
  as allocative harm (differential treatment recommendations), representational harm
  (stereotypical associations, biased image generation), and performance disparities,
  affecting multiple attributes including race/ethnicity, gender, age, disability,
  and language.
---

# Bias in Large Language Models Across Clinical Applications: A Systematic Review

## Quick Facts
- arXiv ID: 2504.02917
- Source URL: https://arxiv.org/abs/2504.02917
- Reference count: 0
- This systematic review identified pervasive bias in large language models (LLMs) across clinical applications, with 38 studies revealing significant issues stemming from both data-related and model-related sources.

## Executive Summary
This systematic review of 38 studies reveals that bias in large language models is pervasive across clinical applications, affecting diagnostic recommendations, treatment suggestions, and patient representation. Bias manifests through allocative harm (differential treatment recommendations), representational harm (stereotypical associations, biased image generation), and performance disparities across demographic groups. Both data-related sources (biased training data reflecting historical inequities) and model-related sources (architecture and training procedures) contribute to these biases, which affect multiple attributes including race/ethnicity, gender, age, disability, and language. The review found that current bias evaluation methods are heterogeneous and inconsistent, making meaningful comparison across studies difficult.

## Method Summary
The review followed PRISMA protocol, screening 1,065 records from PubMed, OVID, and EMBASE databases (inception to January 27, 2025) using specified search strings. Two independent reviewers screened titles/abstracts and full-texts, resolving conflicts with a third reviewer. Data extraction captured LLM type, clinical task, bias evaluation methods, and outcomes. A modified ROBINS-I tool assessed risk of bias. The review included studies evaluating bias in LLMs for clinical tasks with sufficient data, excluding case reports, reviews, and animal studies. Article summarization was assisted by Gemini during data extraction.

## Key Results
- 38 studies revealed pervasive bias in LLMs across clinical applications, with both data-related and model-related sources contributing
- Bias manifested as allocative harm (differential treatment recommendations), representational harm (stereotypical associations, biased image generation), and performance disparities across demographic groups
- Risk of bias assessment indicated mostly low risk across most domains, though outcome measurement and selective reporting showed moderate risk in several studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data containing historical and societal biases causes LLMs to encode and perpetuate these biases in clinical outputs.
- Mechanism: LLMs learn statistical patterns from training corpora. When those corpora reflect historical healthcare inequities (e.g., differential treatment by race, gender), the model internalizes these as predictive patterns rather than systemic artifacts, then reproduces them in generated recommendations.
- Core assumption: The model cannot distinguish between clinically valid patterns and historically discriminatory practices without explicit intervention.
- Evidence anchors: "Both data-related bias (from biased training data) and model-related bias (from model training) were significant contributors"; "Historical bias may explain this phenomenon. Medical data, by its very nature, reflects the history of medical practice, which has often been marked by disparities and inequities."
- Break condition: If training data were perfectly representative and scrubbed of historical inequities, this mechanism would not operate—but this is empirically unattainable per the paper.

### Mechanism 2
- Claim: Model architecture, training objectives, and optimization processes create unique bias profiles independent of input data distribution.
- Mechanism: Different LLMs trained on similar data can produce divergent outputs for identical clinical prompts (e.g., Gemini vs. GPT-4 opioid recommendations). This suggests model-specific factors—tokenization, attention patterns, fine-tuning procedures, RLHF alignment—shape how biases are amplified or suppressed.
- Core assumption: Observed output differences between models under controlled conditions stem from architectural and training differences rather than stochastic variation alone.
- Evidence anchors: "model-related bias (from model training) were significant contributors"; "even when presented with identical inputs, different LLMs can exhibit varying degrees of bias... Gemini was more likely than GPT-4 to recommend strong opioids"
- Break condition: If all models converged to identical outputs given identical data and prompts, model-related bias would be negligible—but the paper shows this does not occur.

### Mechanism 3
- Claim: Encoded biases translate into three distinct harm categories—allocative, representational, and performance disparities—that directly impact clinical care quality and equity.
- Mechanism: Bias in latent representations surfaces differently depending on task type. Allocative harm emerges in resource-allocation decisions (treatment recommendations, diagnostic pathways). Representational harm emerges in content generation (stereotypical associations, image generation biases). Performance disparities emerge in accuracy differentials across demographic subgroups.
- Core assumption: These three harm categories capture the majority of clinically relevant bias manifestations, though others may exist.
- Evidence anchors: "Biases manifested as: allocative harm (e.g., differential treatment recommendations); representational harm (e.g., stereotypical associations, biased image generation); and performance disparities (e.g., variable output quality)"; "Allocative Harm was frequently seen in studies examining diagnostic pathways and treatment recommendations... Representational Harm manifested as the reinforcement of harmful stereotypes"
- Break condition: If bias existed but never manifested in clinically actionable outputs, harm would be theoretical only—the paper documents this is not the case.

## Foundational Learning

- Concept: **Allocative vs. Representational Harm**
  - Why needed here: The paper uses this distinction to categorize bias manifestations; understanding it is prerequisite to designing targeted mitigation.
  - Quick check question: When an LLM recommends more invasive procedures for one racial group, is this allocative or representational harm?

- Concept: **Historical Bias in Medical Data**
  - Why needed here: The paper identifies historical bias as a primary mechanism; recognizing that "accurate" historical records can encode discrimination is essential for data curation.
  - Quick check question: If a training dataset accurately reflects past prescribing patterns that were racially discriminatory, is the data "unbiased"?

- Concept: **Model-Specific Bias Profiles**
  - Why needed here: The review demonstrates different models exhibit different bias patterns under identical conditions; assuming all LLMs behave similarly is incorrect.
  - Quick check question: If GPT-4 and Gemini produce different opioid recommendations for identical patient cases, what does this imply about model-agnostic bias mitigation?

## Architecture Onboarding

- Component map: Data layer (training corpus) -> Model layer (architecture, training procedure) -> Evaluation layer (bias detection methods) -> Deployment layer (real-world usage context)

- Critical path: 1) Audit training data for representation gaps and historical artifacts; 2) Test model outputs across demographic attributes using standardized vignettes; 3) Measure all three harm types (allocative, representational, performance); 4) Compare against ground truth or expert consensus; 5) Monitor in deployment context

- Design tradeoffs:
  - Fine-tuning on local data vs. generalization: Paper shows LLMs not fine-tuned to local EHR data performed poorly on external validation, but fine-tuning may amplify local biases
  - Multiple models vs. single model: Using multiple LLMs provides comparative bias detection but increases integration complexity
  - Standardized vignettes vs. real-world data: Vignettes enable controlled bias testing but may not reflect deployment complexity

- Failure signatures:
  - Differential treatment recommendations by race/gender without clinical justification (allocative harm)
  - Stereotypical demographic associations in generated text or images (representational harm)
  - Accuracy gaps across demographic subgroups on same clinical task (performance disparity)
  - Consistent underrepresentation of minority groups in professional depictions

- First 3 experiments:
  1. Baseline bias audit: Run standardized clinical vignettes with demographic permutations through target LLM; measure output differences across race, gender, age, SES attributes
  2. Comparative model profiling: Test identical clinical scenarios across multiple LLMs (e.g., GPT-4, Gemini) to map model-specific bias profiles
  3. Local validation test: Evaluate pre-trained LLM performance on institution-specific EHR data before deployment; compare against fine-tuned versions for bias-performance tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do user interactions and deployment contexts introduce or amplify bias in LLMs within real-world clinical settings?
- Basis in paper: The authors state that "None of the included studies directly assessed deployment bias" and explicitly call for future studies to "investigate how LLMs are used in real-world clinical settings... and whether these interactions introduce or amplify bias."
- Why unresolved: Current literature relies heavily on simulated clinical vignettes rather than studying live deployments, leaving a critical gap in understanding how clinician behavior and context influence bias.
- What evidence would resolve it: Observational studies or randomized trials in active clinical environments that measure bias manifestations before and after LLM integration into clinical workflows.

### Open Question 2
- Question: Which bias mitigation strategies are most effective for reducing allocative and representational harms in clinical LLMs without compromising clinical accuracy?
- Basis in paper: The review concludes that "Future research needs to go beyond identifying bias and develop, test and validate tools for mitigation and reduction of bias across the different categories of bias."
- Why unresolved: While the authors note that mitigation strategies must address data curation and model development, they describe current approaches as "nascent" and lacking rigorous evaluation in the included studies.
- What evidence would resolve it: Comparative studies testing specific interventions (e.g., data re-weighting, adversarial debiasing) that demonstrate a statistically significant reduction in bias metrics while maintaining diagnostic accuracy.

### Open Question 3
- Question: Can a standardized framework for bias evaluation be developed to facilitate meaningful comparisons across diverse LLM architectures and demographic groups?
- Basis in paper: The authors identify a methodological limitation where "heterogeneity in bias evaluation methods across studies... presents a challenge for direct comparison" and conclude that "Standardizing bias evaluation methods... are crucial."
- Why unresolved: The review found that studies utilized varied subjective and objective metrics, making it difficult to synthesize results or track progress across the field.
- What evidence would resolve it: The development and adoption of a consensus-based set of fairness metrics and reporting guidelines (similar to CONSORT) specific to clinical AI evaluation.

## Limitations

- The review relies on published studies which may underrepresent negative results or unpublished bias assessments
- Most studies used synthetic or standardized test cases rather than real-world clinical data
- Limited understanding of bias evolution during LLM deployment in actual clinical settings
- Risk of bias assessment focused on study methodology rather than bias measurement accuracy

## Confidence

- High Confidence: Pervasive bias exists across multiple clinical LLM applications (supported by 38 studies)
- Medium Confidence: Data-related and model-related sources both contribute significantly (inferred from study reports, though causality not directly tested)
- Medium Confidence: Three harm categories (allocative, representational, performance) capture primary manifestations (framework consistent across studies but not independently validated)

## Next Checks

1. Conduct controlled experiments comparing identical clinical prompts across multiple LLMs to verify model-specific bias profiles reported in the literature
2. Test bias mitigation strategies (data curation, fine-tuning approaches) on a standardized clinical bias benchmark
3. Implement longitudinal bias monitoring during LLM deployment in clinical settings to validate study findings under real-world conditions