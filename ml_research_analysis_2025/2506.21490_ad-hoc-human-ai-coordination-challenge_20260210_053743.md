---
ver: rpa2
title: Ad-Hoc Human-AI Coordination Challenge
arxiv_id: '2506.21490'
source_url: https://arxiv.org/abs/2506.21490
tags:
- card
- human
- clue
- agents
- play
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Ad-Hoc Human-AI Coordination Challenge
  (AH2AC2) to evaluate how well AI agents can coordinate with humans in the cooperative
  card game Hanabi. The authors address the difficulty of human evaluation by developing
  human proxy agents trained on a large-scale dataset of human gameplay combined with
  regularised reinforcement learning to maintain human-like behaviour.
---

# Ad-Hoc Human-AI Coordination Challenge

## Quick Facts
- **arXiv ID:** 2506.21490
- **Source URL:** https://arxiv.org/abs/2506.21490
- **Reference count:** 40
- **Primary result:** OBL (L4) achieves highest human-AI coordination performance (21.04 mean score) without using any human data

## Executive Summary
This paper introduces the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to evaluate how well AI agents can coordinate with humans in the cooperative card game Hanabi. The authors address the difficulty of human evaluation by developing human proxy agents trained on a large-scale dataset of human gameplay combined with regularised reinforcement learning to maintain human-like behaviour. They open-source a limited dataset to encourage data-efficient methods and host proxy agents via an API to ensure fair, reproducible evaluation. The challenge includes two evaluation regimes: coordination with human proxies and action prediction on unseen human data.

## Method Summary
The challenge uses human proxy agents trained via behavioral cloning on 147K human games followed by KL-regularized reinforcement learning to maintain human-like behavior. The public dataset contains 3,079 games (1,858 two-player, 1,221 three-player) from which participants can develop methods. Evaluation occurs through an API that runs agents against proxies over 1,000 games, with leaderboards for two-player and three-player settings. The method employs LSTM-based policies with cross-entropy loss for BC and IPPO with KL regularization (weight λ∈[0.1,0.3]) for fine-tuning.

## Key Results
- Zero-shot coordination methods like OBL outperform data-dependent approaches when limited human data is available
- HDR-IPPO with KL regularization maintains human-like behavior while improving robustness over pure BC
- Even strong LLMs like DeepSeek-R1 without fine-tuning fall short of human-compatible performance
- BC alone suffers from high zero-score rates (>70%) in self-play, demonstrating need for regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human proxy agents trained with behavioral cloning followed by KL-regularized RL produce robust, human-like evaluation partners that outperform pure imitation learning.
- Mechanism: BC initializes policies on human data → IPPO with KL regularization (Equation 2) improves robustness while constraining deviation from human conventions → proxies maintain human-like behavior metrics (IPP, Communicativeness) while reducing zero-score games from 70.92% to 0.27% in 3P setting.
- Core assumption: The KL regularizer weight λ sufficiently balances policy improvement against convention drift; the hanab.live dataset captures representative human conventions.
- Evidence anchors: [abstract]: "human proxy agents trained on a large-scale dataset of human gameplay combined with regularised reinforcement learning to maintain human-like behaviour"; [section 4.2]: "LHDR-IPPO_t(θ) = (1 − λ) · LIPPO_t(θ) + λ · DKL[πBC_θ′(·|τ^i_t)||πHP_θ(·|τ^i_t)]"

### Mechanism 2
- Claim: Zero-shot coordination methods like Off-Belief Learning achieve strong human-AI coordination without any human data by learning grounded conventions.
- Mechanism: OBL trains agents to assume partners follow fixed anchor policies at each reasoning level → prevents overfitting to arbitrary self-play conventions → produces policies that generalize to human partners by learning universally interpretable strategies.
- Core assumption: Grounded policies learned without human data transfer to human coordination; the OBL L4 checkpoint captures sufficiently generic conventions.
- Evidence anchors: [section 6]: "OBL (L4) achieves the highest performance without using any human data... 21.04 mean score"

### Mechanism 3
- Claim: Deliberately limiting human data exposure (3,079 games) while withholding proxy agents forces development of data-efficient coordination methods.
- Mechanism: Small public dataset → participants must develop few-shot/transfer approaches → evaluation API prevents direct overfitting to proxy behavior → leaderboard reveals which methods generalize.
- Core assumption: 3,079 games provide sufficient signal for data-efficient methods without enabling pure imitation; API access model prevents gradient leakage.
- Evidence anchors: [abstract]: "open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data"

### Mechanism 4
- Claim: LLMs without fine-tuning exhibit foundational coordination capability but significantly underperform specialized methods.
- Mechanism: Prompted LLMs receive game state + rules + conventions in natural language → chain-of-thought reasoning → action selection via structured JSON output.
- Core assumption: Prompt engineering can elicit strategic reasoning; evaluation on 100 games (vs. 1000 for other baselines) provides representative performance estimates.
- Evidence anchors: [section 6]: "DeepSeek-R1 H-Group... 9.91 mean score" vs OBL's 21.04 in 2P; "significant improvements are still needed"

## Foundational Learning

- Concept: Dec-POMDPs (Decentralized Partially Observable Markov Decision Processes)
  - Why needed here: Hanabi is formalized as a Dec-POMDP where each agent receives only local observations and must coordinate without shared state.
  - Quick check question: Can you explain why joint policy π cannot be decomposed into independent optimization of each agent's local policy?

- Concept: Behavioral Cloning with Distribution Shift
  - Why needed here: BC policies trained on limited human data suffer from compounding errors when encountering states outside the training distribution (e.g., 70.92% zero-score games in 3P SP).
  - Quick check question: Why does BC performance degrade in self-play compared to cross-play with stronger partners?

- Concept: Zero-Shot Coordination vs. Ad-Hoc Teamplay
  - Why needed here: AH2AC2 evaluates ad-hoc teamplay (coordination with agents trained by different algorithms, specifically human-like proxies), distinct from ZSC which assumes same-algorithm partners.
  - Quick check question: What's the difference between training for ZSC (e.g., OBL) versus training for coordination with a fixed population (e.g., BR-BC)?

## Architecture Onboarding

- Component map: Human Data (147K games) → BC Training → πBC_θ′ → HDR-IPPO (KL-regularized) → πHP_θ (Human Proxies) → Public Data (3K games) → Challenge Baselines (BC, BR-BC, HDR-IPPO, OBL, FCP) → Evaluation API → 1000 games vs Proxies → Leaderboard

- Critical path:
  1. Understand Hanabi game rules and H-Group conventions (Appendix A.1, A.10.2)
  2. Study the HDR-IPPO training procedure (Section 4.2, Equation 2)
  3. Implement a baseline agent using provided codebase
  4. Register for API access and run evaluation protocol
  5. Iterate on method design targeting specific failure modes

- Design tradeoffs:
  - **Data-dependent vs. zero-shot**: BR-BC/HDR-IPPO use human data but may overfit; OBL generalizes without data but lacks 3P support.
  - **Architecture choice**: LSTM-based (BC, HDR-IPPO) captures temporal dependencies vs. feed-forward (IPPO, BR-BC, OBL) for simpler optimization.
  - **Regularization strength λ**: Higher values (0.2-0.3) maintain human-likeness; lower values (0.0-0.01) risk convention drift.

- Failure signatures:
  - **BC collapse**: High zero-score rate (>70%) in SP; median scores much higher than mean indicates brittle policies.
  - **Convention divergence**: Large gap between SP and cross-play scores (e.g., λ=0.0 agents) indicates overfitting to self-play conventions.
  - **LLM hallucination**: Ignoring mid-prompt evidence ("lost-in-the-middle" error) when game state is complex.

- First 3 experiments:
  1. Reproduce the BC baseline on the 3K public dataset; validate test set cross-entropy loss matches reported values (0.86-0.87 for 2P).
  2. Run ablation on KL weight λ using the two-player setting; plot cross-play matrix against BC policy to confirm convention preservation at λ≥0.1.
  3. Implement a simple ensemble method combining OBL (zero-shot) with BC initialization; evaluate whether limited human data can improve upon OBL's 21.04 baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does HDR-IPPO with KL regularisation provide theoretical guarantees on human-likeness and coordination quality, beyond empirical validation?
- Basis in paper: [explicit] "While our experiments and previous works provide strong empirical evidence for the effectiveness of regularised RL in generating human-like agents, a deeper theoretical understanding of the methodology is crucial."
- Why unresolved: The paper relies on empirical validation (cross-play scores, action prediction accuracy, behavioural metrics) but offers no formal analysis of why the regularisation term preserves human-compatible conventions during RL fine-tuning.
- What evidence would resolve it: A theoretical framework connecting KL regularisation strength to bounds on policy divergence from human behaviour, or formal guarantees on cross-play performance with human-like partners.

### Open Question 2
- Question: Can methods that efficiently leverage limited human data outperform zero-shot coordination algorithms like OBL on AH2AC2?
- Basis in paper: [explicit] "There is a pressing need for new techniques that efficiently utilise limited human data to improve human-AI teamwork." The baseline results show OBL achieves 21.04 mean score without human data, while BR-BC achieves 19.41 with data.
- Why unresolved: Current data-dependent approaches (BC, BR-BC, HDR-IPPO) underperform compared to ZSC methods when limited to the open-sourced 3,079 games, revealing a gap in methodology.
- What evidence would resolve it: Novel algorithms that use the limited dataset to achieve mean scores exceeding OBL's 21.04 on the two-player leaderboard.

### Open Question 3
- Question: Do human proxy agents trained via HDR-IPPO provide ecologically valid substitutes for actual human partners in Hanabi?
- Basis in paper: [explicit] "The ultimate validation of our human proxy agents requires direct human-AI play. Future work should involve conducting play experiments with human participants, comparing their experiences and performance when playing with human proxies versus playing with other humans."
- Why unresolved: The paper validates human-likeness through proxy-to-BC cross-play, action prediction, and behavioural metrics (IPP, communicativeness), but no human subjects study confirms whether humans experience proxy agents as human-like.
- What evidence would resolve it: Human participant studies showing comparable coordination scores, subjective ratings of partner predictability, and qualitative feedback when paired with proxies versus human teammates.

### Open Question 4
- Question: Can LLMs with Hanabi-specific fine-tuning or more sophisticated prompting achieve human-compatible coordination on AH2AC2?
- Basis in paper: [explicit] "Our preliminary evaluation of DeepSeek-R1, though limited by resource constraints to fewer games than other baselines, provides initial insights into LLM capabilities for human-AI coordination." The paper calls for "a more extensive evaluation of LLMs and explore more sophisticated methods."
- Why unresolved: DeepSeek-R1 achieved only 9.91 (with H-conventions) vs OBL's 21.04 in two-player settings, evaluated on just 100 games, leaving the upper bound of LLM capabilities unclear.
- What evidence would resolve it: Systematic evaluation of fine-tuned LLMs or chain-of-thought reasoning approaches achieving mean scores competitive with OBL or BR-BC baselines on the full 1,000-game evaluation.

## Limitations

- The evaluation depends entirely on the quality and representativeness of the human proxy agents, which cannot be fully validated without access to the full 147K-game dataset and proxy model weights.
- LLM evaluation is limited to only 100 games per condition due to resource constraints, which may not provide statistically robust performance estimates.
- The KL regularization weight λ requires careful tuning; the paper shows λ≥0.1 is needed to prevent convention drift, but optimal values likely depend on dataset size and domain specifics.

## Confidence

- **High**: Mechanism 1 (BC+KL-regularized RL produces human-like proxies) - supported by quantitative results and ablation studies
- **High**: Mechanism 2 (OBL outperforms data-dependent methods with limited human data) - clear performance gap demonstrated
- **Medium**: Mechanism 3 (Dataset limitation strategy forces data-efficient methods) - theoretically sound but effectiveness depends on challenge outcomes
- **Low**: Mechanism 4 (LLM performance estimates) - limited evaluation scope and resource constraints

## Next Checks

1. **Ablation Study Replication**: Run HDR-IPPO training with multiple λ values (0.0, 0.1, 0.2, 0.3) on the public 3K dataset and measure zero-score rates and cross-play performance to confirm the KL regularization threshold effect.
2. **Dataset Sufficiency Test**: Train pure BC agents on the 3K public dataset and evaluate whether they achieve reasonable performance without KL regularization, which would suggest the dataset may be too large for the intended challenge constraints.
3. **LLM Prompt Engineering Evaluation**: Conduct a systematic prompt ablation study on the 100-game LLM evaluation to identify which prompt components most affect performance and whether results are statistically robust.