---
ver: rpa2
title: 'Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via
  Attention Lens'
arxiv_id: '2508.02419'
source_url: https://arxiv.org/abs/2508.02419
tags:
- attention
- lvlms
- visual
- hallucination
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a previously overlooked issue in large vision-language
  models (LVLMs) called "modality bias," where models disproportionately focus on
  either visual or textual information during object hallucination. The authors propose
  TV AI, a training-free method that intervenes in attention weights during inference
  to balance cross-modal attention and align responses with user instructions.
---

# Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens

## Quick Facts
- **arXiv ID**: 2508.02419
- **Source URL**: https://arxiv.org/abs/2508.02419
- **Reference count**: 40
- **Primary result**: TV AI reduces object hallucination in LVLMs via attention-weight intervention, improving CHAIR sentence-level hallucination from 47.6% to 22.4% for LLaVA-1.5 while preserving general capabilities.

## Executive Summary
This paper identifies "modality bias" as a fundamental cause of object hallucination in large vision-language models (LVLMs), where models disproportionately rely on either visual or textual information during generation. The authors propose TV AI, a training-free inference-time method that intervenes in cross-modal attention weights to balance information from both modalities. Through experiments on four popular LVLMs (LLaVA-1.5, Qwen-VL-Chat, MiniGPT-4, Shikra), TV AI significantly reduces hallucinations on benchmarks like CHAIR and POPE while maintaining general capability, achieving state-of-the-art performance without requiring architectural modifications.

## Method Summary
TV AI operates by hooking into the self-attention layers of LVLMs during inference and augmenting attention weights before the softmax operation. For each token, the method scales attention weights differently for text and visual tokens using learned coefficients (α for text, β for vision), effectively amplifying cross-modal attention. Additionally, a contrastive decoding strategy combines the original and modified probability distributions using a mixing parameter γ. The method is applied to specific decoder layers identified as "attention sink emergence" points, where modality bias becomes most pronounced. Hyperparameters are tuned per model architecture and dataset, with different values required for LLaVA-1.5, MiniGPT-4, Shikra, and Qwen-VL-Chat.

## Key Results
- TV AI reduces sentence-level hallucination on CHAIR from 47.6% to 22.4% for LLaVA-1.5
- On POPE, TV AI achieves 75.5% accuracy for LLaVA-1.5 (vs 67.9% baseline)
- Maintains general capability across MMBench, preserving F1 scores while reducing hallucination
- Outperforms state-of-the-art inference-time intervention methods across all tested benchmarks

## Why This Works (Mechanism)
The method works by directly intervening in the attention mechanism where modality bias manifests. LVLMs tend to develop a preference for either visual or textual information during hallucination, creating imbalanced attention distributions. By scaling attention weights differently for each modality before softmax, TV AI forces the model to consider information from both sources more equally. The contrastive decoding component further ensures that the final output incorporates both the original model behavior and the balanced attention distribution, reducing overreliance on parametric knowledge while preserving the model's general capabilities.

## Foundational Learning
- **Attention sink emergence**: The phenomenon where certain attention heads develop strong preferences for specific token types during generation; needed to identify optimal intervention points
- **Cross-modal attention**: How LVLMs combine visual and textual information through attention mechanisms; critical for understanding where modality bias occurs
- **Softmax manipulation**: Modifying attention weights before softmax to influence token distribution; fundamental to TV AI's intervention approach
- **Contrastive decoding**: Combining multiple probability distributions to produce final outputs; used to balance original and modified attention states

## Architecture Onboarding

**Component Map**
Vision Encoder -> Cross-Encoder Connector -> Decoder with Attention Hooks -> Output Generator

**Critical Path**
Visual features → Cross-modal attention → Attention weight augmentation → Contrastive probability mixing → Token generation

**Design Tradeoffs**
Static vs dynamic hyperparameters (manual tuning vs adaptability), intervention strength vs capability preservation, computational overhead vs performance gain

**Failure Signatures**
Over-correction destroys general capability (F1 drops sharply), under-correction leaves hallucination unchanged, wrong hyperparameters per architecture cause inconsistent results

**First Experiments**
1. Apply TV AI with default hyperparameters to LLaVA-1.5 on CHAIR; verify hallucination reduction
2. Test TV AI with varying α values (0.8-1.2) to find optimal balance for a given model
3. Evaluate TV AI on POPE with "Is there a <object> in the image?" prompt to measure accuracy

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Can a dynamic, real-time attention manipulation mechanism outperform the static hyperparameter approach used in TV AI?
- **Basis in paper**: The Limitations section states that "a dynamic attention distribution manipulation method would better adapt to diverse model architectures and domains... which constitutes our future research direction."
- **Why unresolved**: The current TV AI method relies on static hyperparameters (α, β, γ) that must be manually tuned and fixed for specific datasets or models, limiting adaptability.
- **What evidence would resolve it**: A modified TV AI framework that continuously assesses the attention distribution of the current token and dynamically adjusts intervention strength without pre-defined constants, showing superior or equal performance without manual tuning.

**Open Question 2**
- **Question**: What specific training-time interventions are required to fundamentally eliminate modality bias rather than merely mitigating it at inference?
- **Basis in paper**: The authors note in the Limitations section that "improved multimodal data preparation pipelines and training paradigms will be needed to fundamentally resolve this problem," acknowledging TV AI is only a partial, inference-time fix.
- **Why unresolved**: The paper focuses exclusively on inference-time intervention and does not explore how modifying the loss functions, data ratios, or architectural connectors during training could prevent the bias from forming.
- **What evidence would resolve it**: An ablation study comparing standard LVLM training against a training regime designed to balance modality attention, demonstrating that the resulting model requires significantly less (or zero) inference-time intervention to avoid hallucination.

**Open Question 3**
- **Question**: What architectural features determine the optimal intervention layers and strength (α, β) for different LVLMs?
- **Basis in paper**: The ablation study shows LLaVA-1.5 requires α=0.93 while MiniGPT-4 requires α=0.8. The paper attributes this to "architectural/training/data differences" but does not isolate which specific factors drive this sensitivity.
- **Why unresolved**: The selection of intervention layers and strengths is currently determined empirically for each model, lacking a theoretical framework to predict these needs based on architecture.
- **What evidence would resolve it**: A comparative analysis across diverse LVLM architectures correlating specific structural components (e.g., projector types, visual encoder resolution) with the magnitude of modality bias, yielding a predictive rule for optimal intervention settings.

## Limitations
- Method effectiveness heavily depends on correct hyperparameter tuning (α, β, γ) per architecture
- Scope limited to four specific models and three benchmarks, raising generalizability concerns
- Requires careful calibration to avoid over-correction that destroys general capabilities

## Confidence

**High Confidence**: The identification of modality bias as a distinct phenomenon in LVLMs, supported by quantitative evidence showing attention divergence between visual and textual information. The general effectiveness of TV AI in reducing hallucinations on CHAIR and POPE benchmarks is well-established through comparative experiments.

**Medium Confidence**: The claim that TV AI preserves general capabilities while reducing hallucinations, as this depends heavily on careful hyperparameter tuning and the specific evaluation benchmarks used. The computational efficiency claim of "minimal overhead" lacks quantitative timing data.

**Low Confidence**: The assertion that TV AI is a "plug-and-play" solution requiring no architectural modifications, as practical implementation requires non-trivial attention hook integration and careful parameter tuning per model.

## Next Checks

1. **Hyperparameter Robustness**: Systematically vary α, β, and γ values across a wider range for each model to establish sensitivity curves and identify optimal operating regions that balance hallucination reduction with capability preservation.

2. **Cross-Architecture Generalizability**: Test TV AI on additional LVLM architectures (e.g., InternVL, LLaVA-NeXT, BLIP-2) and diverse datasets beyond the current benchmarks to validate the method's broader applicability.

3. **Dynamic Thresholding Implementation**: Develop and evaluate an automated method for determining intervention thresholds (attention head selection, intervention layers) rather than relying on manual identification of "attention sink emergence" patterns.