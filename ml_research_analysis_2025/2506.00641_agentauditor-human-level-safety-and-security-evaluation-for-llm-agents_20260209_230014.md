---
ver: rpa2
title: 'AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents'
arxiv_id: '2506.00641'
source_url: https://arxiv.org/abs/2506.00641
tags:
- agent
- safety
- agentauditor
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of reliably evaluating the safety
  and security of large language model (LLM)-based agents, which are increasingly
  autonomous decision-makers. Existing rule-based or LLM-based evaluators often miss
  dangers in step-by-step actions, overlook subtle meanings, fail to see how small
  issues compound, and get confused by unclear safety or security rules.
---

# AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents

## Quick Facts
- arXiv ID: 2506.00641
- Source URL: https://arxiv.org/abs/2506.00641
- Authors: Hanjun Luo; Shenyu Dai; Chiming Ni; Xinfeng Li; Guibin Zhang; Kun Wang; Tongliang Liu; Hanan Salam
- Reference count: 40
- Primary result: Introduces AgentAuditor, achieving human-level accuracy in LLM agent safety and security evaluation through memory-augmented reasoning

## Executive Summary
AgentAuditor addresses critical limitations in current LLM-based safety and security evaluation methods by introducing a training-free, memory-augmented reasoning framework. The system constructs experiential memory through adaptive semantic feature extraction and chain-of-thought reasoning traces, enabling dynamic retrieval of relevant past interactions to guide evaluation of new cases. A novel benchmark called ASSEBench is introduced to comprehensively assess evaluators across 15 risk types and 29 application scenarios. Experimental results demonstrate that AgentAuditor consistently improves evaluation performance across all benchmarks and achieves human-level accuracy, establishing a new state-of-the-art for LLM-as-a-judge approaches in agent safety and security.

## Method Summary
AgentAuditor operates as a training-free framework that builds experiential memory by adaptively extracting structured semantic features and generating chain-of-thought reasoning traces from past interactions. The system employs a multi-stage, context-aware retrieval-augmented generation process that dynamically retrieves the most relevant reasoning experiences to guide LLM evaluator assessments of new cases. The approach leverages memory augmentation to overcome limitations of existing rule-based or LLM-based evaluators that often miss dangers in sequential actions, overlook subtle meanings, fail to recognize how small issues compound, and get confused by ambiguous safety or security rules. The framework's design enables it to maintain comprehensive awareness of both immediate and cumulative risks across agent interactions.

## Key Results
- AgentAuditor achieves human-level accuracy in safety and security evaluation of LLM agents
- The system demonstrates consistent performance improvements across all evaluation benchmarks
- ASSEBench establishes the first comprehensive benchmark covering 15 risk types across 29 application scenarios
- AgentAuditor sets a new state-of-the-art for LLM-as-a-judge approaches in agent safety and security

## Why This Works (Mechanism)
AgentAuditor succeeds by addressing fundamental weaknesses in existing LLM evaluation approaches through memory-augmented reasoning. The system's experiential memory captures both explicit safety violations and subtle security threats that might be missed by rule-based systems. By maintaining chain-of-thought reasoning traces, AgentAuditor can trace the evolution of risks across sequential actions rather than evaluating each step in isolation. The retrieval-augmented generation process ensures that evaluations benefit from relevant historical experiences, allowing the system to recognize patterns and compound effects that single-pass evaluations would miss. This comprehensive approach enables detection of both obvious and nuanced safety and security issues that arise in complex agent behaviors.

## Foundational Learning

1. **Experiential Memory Construction** - Adaptive extraction of structured semantic features from past interactions
   - Why needed: Captures context and reasoning patterns that inform future evaluations
   - Quick check: Verify semantic features preserve critical context while remaining generalizable

2. **Chain-of-Thought Reasoning Traces** - Systematic documentation of evaluation logic and decision pathways
   - Why needed: Enables traceability and consistent application of evaluation criteria
   - Quick check: Ensure traces capture both explicit conclusions and implicit reasoning steps

3. **Context-Aware Retrieval** - Dynamic selection of relevant historical experiences based on current evaluation context
   - Why needed: Provides evaluation guidance tailored to specific risk scenarios
   - Quick check: Validate retrieval accuracy across diverse risk types and scenarios

4. **Multi-Stage Evaluation Process** - Sequential refinement of assessments through multiple reasoning stages
   - Why needed: Allows progressive deepening of analysis and risk identification
   - Quick check: Confirm each stage adds meaningful evaluation value without redundancy

5. **Risk Type Classification** - Systematic categorization of safety and security threats into 15 distinct types
   - Why needed: Enables targeted evaluation approaches for different risk categories
   - Quick check: Verify classification covers the full spectrum of agent safety and security concerns

## Architecture Onboarding

**Component Map**: User Input -> Context Extraction -> Memory Retrieval -> Reasoning Augmentation -> Risk Assessment -> Output

**Critical Path**: The evaluation pipeline follows a sequential flow where user inputs are first processed for context extraction, then matched against experiential memory through retrieval, followed by reasoning augmentation using chain-of-thought traces, culminating in comprehensive risk assessment generation.

**Design Tradeoffs**: The system prioritizes evaluation comprehensiveness over speed by employing multi-stage reasoning and extensive memory retrieval, accepting increased computational overhead for improved accuracy. The training-free approach sacrifices the potential performance gains of fine-tuned models in favor of broader generalizability across diverse safety and security scenarios.

**Failure Signatures**: Performance degradation may occur when encountering novel risk types not represented in experiential memory, when context extraction fails to capture critical nuances in agent behavior, or when retrieval mechanisms cannot identify sufficiently relevant historical experiences. The system may also struggle with highly ambiguous safety/security definitions that lack clear precedent in training data.

**First 3 Experiments**: 
1. Evaluate AgentAuditor on ASSEBench benchmark to establish baseline performance across all 15 risk types
2. Compare retrieval accuracy against baseline methods using precision-recall metrics for historical experience matching
3. Conduct ablation studies removing individual components (memory, retrieval, reasoning traces) to quantify their contribution to overall performance

## Open Questions the Paper Calls Out

None

## Limitations

- ASSEBench benchmark reliability and real-world generalizability remain untested despite comprehensive scope
- "Training-free" characterization may be misleading given reliance on engineered prompts and retrieval mechanisms
- System lacks validation against specialized security tools and human expert panels beyond claimed human-level accuracy
- No assessment of adversarial scenarios where malicious actors might circumvent evaluation framework

## Confidence

- Human-level accuracy claim: Medium confidence (based on ASSEBench comparisons with human evaluators, but benchmark validation lacking)
- State-of-the-art performance claim: Medium confidence (demonstrated against existing LLM evaluators only, not broader assessment methods)
- Technical approach validity: High confidence (memory-augmented reasoning and retrieval-augmented generation are established techniques with clear implementation)

## Next Checks

1. **Benchmark External Validation**: Have independent researchers evaluate ASSEBench's quality and coverage by attempting to create new test cases that reveal potential blind spots in the benchmark design.

2. **Real-World Deployment Testing**: Deploy AgentAuditor in controlled environments with actual LLM agents performing real tasks to assess whether the benchmark performance translates to practical safety and security improvements.

3. **Adversarial Robustness Assessment**: Conduct systematic testing where researchers attempt to craft inputs specifically designed to fool AgentAuditor's evaluation process, measuring the framework's resilience to adversarial attacks.