---
ver: rpa2
title: 'PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real
  Transfer with Few Demonstrations'
arxiv_id: '2504.20520'
source_url: https://arxiv.org/abs/2504.20520
tags:
- reward
- object
- policy
- simulation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning robust robotic policies
  from few demonstrations that generalize across variations in robot initial positions
  and object poses. The authors propose PRISM, a real-to-sim-to-real pipeline that
  constructs simulation environments from expert demonstrations by identifying scene
  objects from images and retrieving their 3D models from existing libraries.
---

# PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations

## Quick Facts
- arXiv ID: 2504.20520
- Source URL: https://arxiv.org/abs/2504.20520
- Reference count: 40
- One-line primary result: 82% average success rate across six manipulation tasks with few demonstrations

## Executive Summary
This paper addresses the challenge of learning robust robotic manipulation policies from few demonstrations that generalize across variations in robot initial positions and object poses. PRISM proposes a real-to-sim-to-real pipeline that constructs simulation environments from expert demonstrations by identifying scene objects from images and retrieving their 3D models from existing libraries. The key innovation is a projection-based reward model for RL policy training that uses human-guided object projection relationships as prompts to a vision-language model, with policies further fine-tuned using expert demonstrations.

PRISM achieves an average success rate of 82% across six manipulation tasks with randomized robot initial poses and object configurations, outperforming baseline methods by 68%. The action feasibility predictor derived from the projection-based reward model improves task success rates by 20% in real-world scenarios. The method successfully transfers policies learned in simulation to real-world execution, demonstrating robustness to variations in both robot initial positions and object poses while requiring minimal manual reward engineering and expert demonstrations.

## Method Summary
PRISM constructs simulation environments from expert demonstrations by identifying scene objects from images and retrieving their 3D models from existing libraries. The core innovation is a projection-based reward model for RL policy training that uses human-guided object projection relationships as prompts to a vision-language model (VLM). The VLM evaluates whether projected objects satisfy spatial relationships defined by the task, generating rewards that guide policy learning. Policies are then fine-tuned using expert demonstrations, enabling transfer from simulation to real-world execution while maintaining robustness to variations in robot initial positions and object poses.

## Key Results
- 82% average success rate across six manipulation tasks with randomized robot initial poses and object configurations
- Outperforms baseline methods by 68% in terms of success rates
- Action feasibility predictor derived from projection-based reward model improves task success rates by 20% in real-world scenarios

## Why This Works (Mechanism)
The projection-based reward model leverages human-provided object projection relationships as structured prompts to a vision-language model, enabling semantic understanding of task requirements without extensive manual reward engineering. By evaluating whether projected objects satisfy spatial relationships defined by the task, the VLM generates semantically meaningful rewards that guide RL policy learning toward task completion. The real-to-sim-to-real pipeline ensures that policies trained in simulation can generalize to real-world variations in robot initial positions and object poses, while expert demonstration fine-tuning provides additional robustness and refinement.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Needed for semantic understanding of object relationships without manual reward engineering; quick check: evaluate VLM performance on object relationship tasks
- **Reinforcement Learning in Simulation**: Enables safe and efficient policy training before real-world deployment; quick check: measure learning curves and convergence rates
- **Real-to-Sim-to-Real Transfer**: Allows policies trained in simulation to generalize to real-world variations; quick check: compare performance on sim-to-real transfer tasks
- **3D Model Retrieval**: Critical for accurate scene reconstruction from demonstrations; quick check: evaluate retrieval accuracy for diverse object categories
- **Action Feasibility Prediction**: Improves real-world task success by filtering invalid actions; quick check: measure improvement in success rates with and without feasibility predictor

## Architecture Onboarding

Component Map: Demonstrations -> Scene Reconstruction -> 3D Model Retrieval -> Projection-based Reward Model -> VLM Reward Evaluation -> RL Policy Training -> Expert Demonstration Fine-tuning -> Real-world Execution

Critical Path: The core workflow follows Demonstrations through Scene Reconstruction, 3D Model Retrieval, Projection-based Reward Model, VLM Reward Evaluation, RL Policy Training, and Real-world Execution. Expert Demonstration Fine-tuning provides an optional refinement step.

Design Tradeoffs: The method trades computational complexity of VLM-based reward evaluation for reduced manual reward engineering. The real-to-sim-to-real pipeline requires accurate scene reconstruction but enables safe policy training. The reliance on existing 3D model libraries limits object diversity but simplifies scene construction.

Failure Signatures: Poor 3D model retrieval leads to inaccurate scene reconstruction and degraded policy performance. Insufficient human guidance on object projection relationships results in semantically meaningless rewards. VLM reward evaluation errors propagate to policy training. Real-world execution failures indicate sim-to-real transfer limitations.

First Experiments:
1. Evaluate VLM performance on object relationship tasks with varying human-provided projection relationships
2. Measure learning curves and convergence rates for RL policies with different reward formulations
3. Compare policy performance on sim-to-real transfer tasks with and without expert demonstration fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Projection-based reward model relies heavily on human-provided object projection relationships as VLM prompts, introducing potential subjectivity and scalability concerns
- Quality of 3D model retrieval from existing libraries may limit performance for objects with limited representations or when precise geometric correspondence is required
- Real-to-sim-to-real pipeline assumes accurate scene reconstruction from demonstrations, which may degrade with complex environments or occlusions

## Confidence
- High: Empirical performance metrics (82% average success rate) and policy transfer capabilities are directly measured in experiments
- Medium: Generalizability claims beyond tested manipulation tasks given limited task diversity
- Medium: Assertion that minimal manual reward engineering is required, as human input is still needed for object projection relationships

## Next Checks
1. Test PRISM on manipulation tasks involving novel object categories not represented in the 3D model libraries to evaluate robustness to model retrieval failures
2. Conduct ablation studies isolating the contribution of the action feasibility predictor to task success rates in varied environmental conditions
3. Evaluate policy performance degradation when demonstration quality varies (e.g., partial demonstrations, noisy sensor data) to assess robustness to imperfect expert inputs