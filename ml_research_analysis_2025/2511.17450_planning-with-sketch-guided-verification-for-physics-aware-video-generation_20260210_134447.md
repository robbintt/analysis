---
ver: rpa2
title: Planning with Sketch-Guided Verification for Physics-Aware Video Generation
arxiv_id: '2511.17450'
source_url: https://arxiv.org/abs/2511.17450
tags:
- video
- motion
- frame
- planning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SketchVerify, a test-time planning framework
  that improves video generation by iteratively refining motion plans through lightweight
  sketch-based verification rather than costly full video synthesis. Given a text
  prompt and initial image, it samples multiple candidate trajectories for moving
  objects, renders each as a video sketch, and scores them with a multimodal vision-language
  verifier for semantic alignment and physical plausibility (Newtonian consistency,
  penetration avoidance, gravity coherence, and shape stability).
---

# Planning with Sketch-Guided Verification for Physics-Aware Video Generation

## Quick Facts
- **arXiv ID**: 2511.17450
- **Source URL**: https://arxiv.org/abs/2511.17450
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art physics-aware video generation through test-time planning with sketch-based verification, outperforming open-source models in instruction following, physical realism, and efficiency.

## Executive Summary
This paper introduces SketchVerify, a test-time planning framework that improves physics-aware video generation by iteratively refining motion plans through lightweight sketch-based verification rather than costly full video synthesis. The system samples multiple candidate trajectories for moving objects, renders each as a video sketch, and scores them with a multimodal vision-language verifier for semantic alignment and physical plausibility. Evaluated on WorldModelBench and PhyWorldBench, SketchVerify significantly outperforms state-of-the-art open-source models in instruction following, physical realism, and commonsense consistency while reducing planning cost by nearly an order of magnitude compared to iterative refinement baselines.

## Method Summary
SketchVerify operates entirely at inference without fine-tuning, using external MLLMs for planning and verification. Given a text prompt and initial image, the framework decomposes the prompt into sub-instructions, identifies moving objects, and samples multiple candidate trajectories. Each trajectory is rendered as a lightweight video sketch by compositing object crops onto a static background, then scored by a multimodal verifier for semantic alignment and physical plausibility (Newtonian consistency, penetration avoidance, gravity coherence, and shape stability). The highest-scoring trajectory is selected and merged into a unified plan passed to a trajectory-conditioned diffusion model for final synthesis.

## Key Results
- Achieves instruction following scores of 2.08 on WorldModelBench vs 1.49-1.68 for open-source baselines
- Improves physics coherence scores from 4.55-4.76 to 4.81
- Reduces planning time from 660-720s to 72.5s per sub-instruction
- Sketch-based verification provides ~10× efficiency gains while maintaining comparable performance to full video synthesis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sketch-based verification provides ~10× efficiency gains while maintaining verification quality comparable to full video synthesis.
- **Mechanism**: By cropping segmented objects from the initial frame and compositing them onto a static background according to trajectory coordinates, the system creates lightweight video sketches that preserve essential spatial-temporal structure without invoking expensive diffusion-based rendering.
- **Core assumption**: The verifier can assess physical plausibility and semantic alignment from rough motion sketches without photorealistic rendering.
- **Evidence anchors**: Abstract states sketch verification "bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance"; section 3.3 describes sketches as "layout-preserving approximation of the planned motion."

### Mechanism 2
- **Claim**: Multimodal verification strengthens spatial and physical reasoning compared to language-only feedback.
- **Mechanism**: The VLM directly observes motion trajectories as video sketches, enabling assessment of Newtonian consistency, penetration violations, gravitational coherence, and deformation through visual grounding rather than textual descriptions.
- **Core assumption**: The verifier's visual reasoning capabilities generalize to simplified composite sketches and produce reliable scalar scores via structured prompting.
- **Evidence anchors**: Table 3 shows language-only verifier scores 1.49 instruction follow vs 2.08 for multimodal; physics scores 4.76 vs 4.81.

### Mechanism 3
- **Claim**: Scaling the number of sampled trajectory candidates correlates with improved motion quality, with diminishing returns beyond moderate sampling.
- **Mechanism**: By sampling K candidates and ranking via verifier scores, the system explores a broader trajectory space, increasing the probability of selecting a high-quality plan that satisfies both semantic and physical constraints.
- **Core assumption**: The planner can generate sufficiently diverse candidates, and verifier scores reliably correlate with downstream generation quality.
- **Evidence anchors**: Figure 6 shows instruction score increases from 1.46 (K=0) to 2.08 (K=5); physics score from 4.55 to 4.81.

## Foundational Learning

- **Concept: Test-time planning vs. training-time conditioning**
  - Why needed here: SketchVerify operates entirely at inference without fine-tuning, relying on external MLLMs for planning and verification.
  - Quick check question: Can you explain why the framework requires no gradient updates to the video diffusion model?

- **Concept: Trajectory-conditioned video generation**
  - Why needed here: The verified motion plan is converted to dense trajectories that modulate the diffusion denoising process.
  - Quick check question: How does the ATI-14B model inject trajectory latents during generation?

- **Concept: Compositional sketch rendering**
  - Why needed here: Understanding how object sprites are composited onto static backgrounds clarifies what information is preserved vs. lost.
  - Quick check question: What are the trade-offs between sketch-based and full-video verification in terms of information fidelity?

## Architecture Onboarding

- **Component map**: Prompt → Sub-instruction decomposition → Object segmentation → Background extraction → Trajectory sampling → Sketch rendering → Verification → Top-K selection → Trajectory interpolation → Diffusion synthesis
- **Critical path**: Prompt → Sub-instruction decomposition → Object segmentation → Background extraction → Trajectory sampling → Sketch rendering → Verification → Top-K selection → Trajectory interpolation → Diffusion synthesis
- **Design tradeoffs**:
  - Sketch vs. full-video verification: ~10× faster but may miss fine-grained physics (friction, collision response)
  - Candidate count K: Higher K improves selection but increases API costs and latency
  - Verifier model size: Larger models (Gemini) outperform smaller VLMs (Qwen-3B) in spatial reasoning
  - Sub-instruction granularity: Finer decomposition enables localized correction but increases planning overhead
- **Failure signatures**:
  - Floating objects without gravity coherence → verifier physics score consistently low
  - Incorrect object tracking across sub-instructions → context frame update fails
  - Malformed planner outputs (invalid JSON, missing fields) → re-sampling loop triggered
  - Penetration artifacts → background inpainting leaves residual overlap regions
- **First 3 experiments**:
  1. **Verifier modality ablation**: Run SketchVerify with text-only vs. multimodal verification on 20 WorldModelBench prompts to quantify the visual grounding contribution.
  2. **Candidate scaling study**: Vary K ∈ {1, 3, 5, 10} and plot instruction-following scores to identify saturation point for your compute budget.
  3. **Cross-domain robustness**: Test on 10 PhyWorldBench prompts spanning fluid dynamics, rigid-body collisions, and gravity to characterize verifier limitations on fine-grained physics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the framework effectively integrate quantitative physical constraints (e.g., friction, elastic collisions) beyond the current qualitative assessment provided by the MLLM verifier?
- **Basis in paper**: Appendix A.5 notes that while the framework handles high-level physical plausibility, it struggles with "fine-grained physics, such as frictional forces... or other continuous dynamics that typically require differentiable simulation."
- **Why unresolved**: The current verifier relies on prompt-based reasoning ("Is this consistent with gravity?") rather than calculating physical parameters, leaving a gap in enforcing strict Newtonian accuracy for complex interactions.
- **What evidence would resolve it**: Demonstration of the framework successfully integrating a differentiable physics engine or numerical reward signal into the verification loop to handle high-fidelity collision response and friction.

### Open Question 2
- **Question**: How can the 2D bounding box representation be extended to handle fine-grained 3D object interactions or non-rigid fluid dynamics?
- **Basis in paper**: Appendix A.5 states that because motion is represented via 2D boxes, "the framework can struggle with fine-grained 3D interactions such as detailed affordances or fluid-like behavior."
- **Why unresolved**: The planning and verification modules operate on 2D projections, lacking the depth and volumetric information necessary to model complex 3D articulation or fluid flow.
- **What evidence would resolve it**: Implementation of 3D trajectory planning (e.g., using depth maps or meshes) or non-rigid sketch rendering that improves performance on fluid/articulated object benchmarks without significant latency costs.

### Open Question 3
- **Question**: Can the pipeline maintain efficiency and verification accuracy in dynamic scenes with significant camera motion?
- **Basis in paper**: Section 3.2 explicitly describes removing objects and filling them with a "static background image B" for compositing, and the trajectory sampling assumes a stationary canvas.
- **Why unresolved**: The reliance on background inpainting and static-compositing suggests the method may break down if the "stage" itself moves (camera ego-motion), as the verification sketches would misalign with the scene's perspective shift.
- **What evidence would resolve it**: Evaluation on benchmarks featuring moving cameras, showing that the sketch rendering and verification logic can handle homography or background flow.

## Limitations

- **Model Dependencies**: Heavy reliance on external APIs (GPT-4.1, Gemini 2.5-Flash, ATI-14B) creates potential availability, cost, and version compatibility issues.
- **Physics Fidelity Gap**: Simplified compositing may miss subtle physical interactions like friction, complex collision responses, or fluid dynamics.
- **Planner-Generator Alignment**: No explicit validation that verifier-optimized trajectories produce high-quality final videos from ATI-14B.

## Confidence

- **High Confidence**: Sketch-based verification provides substantial efficiency gains without sacrificing semantic alignment quality. Quantitative improvements over open-source baselines are well-supported.
- **Medium Confidence**: Multimodal verification provides meaningful improvements over language-only verification for spatial reasoning tasks, though corpus evidence is limited.
- **Medium Confidence**: Scaling trajectory candidates improves selection quality with diminishing returns, though optimal candidate count depends on planner diversity and verifier discrimination.

## Next Checks

1. **Cross-Model Verification Study**: Implement SketchVerify with three different verifier models (Gemini 2.5-Flash, Qwen-3B-VL, and a smaller VLM) to quantify the impact of verifier capacity on both efficiency and quality.
2. **Fine-Grained Physics Analysis**: Conduct detailed comparison between sketch-based and full-video verification on tasks with complex physics (fluid dynamics, multiple collisions, elastic deformation) to measure correlation between verifier scores and human judgments.
3. **Planner-Generator Alignment Experiment**: Generate videos using both top-ranked and bottom-ranked trajectories (from verifier scores) and evaluate actual generation quality to quantify how well verifier optimization translates to final video quality.