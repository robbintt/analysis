---
ver: rpa2
title: 'R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology
  Report Generation'
arxiv_id: '2508.03426'
source_url: https://arxiv.org/abs/2508.03426
tags:
- report
- graph
- knowledge
- generation
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating accurate and clinically
  relevant radiology reports from X-ray images, specifically tackling issues like
  hallucination and weak disease diagnostic capabilities in existing large language
  model (LLM) approaches. The authors propose a novel framework, R2GenKG, that incorporates
  a large-scale multi-modal medical knowledge graph (M3KG) constructed using GPT-4o,
  containing 2477 entities, 3 relation types, 37424 triples, and 6943 disease-aware
  vision tokens for the CheXpert Plus dataset.
---

# R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation

## Quick Facts
- arXiv ID: 2508.03426
- Source URL: https://arxiv.org/abs/2508.03426
- Reference count: 40
- Primary result: R2GenKG achieves BLEU-4 scores of 0.181 and 0.106 on IU-Xray and CheXpert Plus datasets respectively, with improved clinical efficacy F1 score of 0.292 on CheXpert Plus.

## Executive Summary
R2GenKG introduces a hierarchical multi-modal knowledge graph approach to address hallucination and weak disease diagnostic capabilities in LLM-based radiology report generation. The framework constructs a large-scale medical knowledge graph using GPT-4o, containing 2477 entities, 3 relation types, 37424 triples, and 6943 disease-aware vision tokens specifically for the CheXpert Plus dataset. By integrating visual features from Swin-Transformer with knowledge graph representations via relational graph convolutional networks and cross-attention mechanisms, R2GenKG significantly outperforms state-of-the-art methods on two benchmark datasets.

## Method Summary
The R2GenKG framework combines visual and knowledge graph features through a hierarchical multi-granularity approach. It constructs a multi-modal medical knowledge graph (M3KG) using GPT-4o for entity/relation extraction and Grad-CAM for visual token extraction. The system uses Swin-Transformer for visual feature extraction, R-GCN for encoding multi-granular knowledge graph representations at different scales (100-500 nodes), and bi-directional cross-attention mechanisms (V2KG and KG2V) to fuse visual and knowledge features. The fused representations are then fed to a Llama2-7B LLM for report generation.

## Key Results
- R2GenKG achieves BLEU-4 scores of 0.181 on IU-Xray and 0.106 on CheXpert Plus datasets
- Clinical efficacy F1 score improves from 0.260 to 0.292 on CheXpert Plus
- Performance peaks at 300 entity nodes in the knowledge graph, with degradation at 500 nodes
- Ablation study shows disease visual graph (DVG) improves BLEU-4 from 0.101 to 0.103 and F1 from 0.260 to 0.262

## Why This Works (Mechanism)

### Mechanism 1: Visual Grounding via Disease-Aware Tokens
- **Claim:** Injecting pre-constructed multi-modal knowledge graph containing disease-aware visual patches grounds LLM generation, reducing hallucinations.
- **Mechanism:** GPT-4o extracts entities/relations and Grad-CAM extracts visual patches for 14 disease labels. During inference, specific "disease-aware vision tokens" are retrieved to augment input image features.
- **Core assumption:** Grad-CAM activations generalize well to test images and accurately represent visual manifestations without spurious correlations.
- **Evidence anchors:** [Page 2, Section 1] notes existing KGs neglect multi-modal information; [Page 7, Table 4] shows DVG improves BLEU-4 from 0.101 to 0.103 and F1 from 0.260 to 0.262.
- **Break condition:** If Grad-CAM activations are noisy or fail to localize diseases accurately, retrieved vision tokens become irrelevant visual noise, confusing the LLM.

### Mechanism 2: Hierarchical Multi-Granularity Graph Representation
- **Claim:** Hierarchical multi-granularity graph representation preserves semantic detail better than single-scale graphs.
- **Mechanism:** Large M3KG is sampled into subgraphs of varying sizes (100-500 nodes) representing coarse-to-fine semantics, encoded separately by R-GCN and fused using self-attention.
- **Core assumption:** Node count directly correlates with semantic granularity, and optimal 300-node count is a robust structural property.
- **Evidence anchors:** [Page 4, Section 3.3] describes multi-scale construction; [Page 7, Table 9] shows performance peaks at 300 entities (BLEU-4 0.106) and drops at 500 entities (BLEU-4 0.101).
- **Break condition:** If self-attention fusion fails to distinguish noise from signal, the model suffers information overload, explaining performance drop at 500 nodes.

### Mechanism 3: Bi-Directional Cross-Attention Feature Alignment
- **Claim:** Bi-directional cross-attention (KG2V and V2KG) facilitates better feature alignment than single-direction attention.
- **Mechanism:** V2KG queries graph using visual features while KG2V queries visual features using graph concepts, creating tighter coupling between Swin-Transformer and R-GCN embeddings.
- **Core assumption:** Embedding spaces of Swin-Transformer and Bio ClinicalBERT/R-GCN are compatible enough for alignment via simple projection and attention layers.
- **Evidence anchors:** [Page 5, Section 3.4] details KG2V and V2KG cross-attention; [Page 7, Table 3] shows full model outperforms partial variants.
- **Break condition:** If visual encoder fails to detect subtle anomalies, V2KG has nothing to query and KG2V might hallucinate findings by attending to unrelated regions based on graph priors.

## Foundational Learning

- **Concept:** Relational Graph Convolutional Networks (R-GCN)
  - **Why needed here:** Standard GCNs treat all edges identically. R2GenKG relies on distinct medical relations (*located at*, *modify*, *suggestive of*) to encode semantic meaning. Without R-GCN, the model would lose distinction between "effusion *located at* pleural" and "effusion *suggestive of* heart failure."
  - **Quick check question:** If you replaced R-GCN with a standard GCN, would the model still know the difference between an anatomical location and a diagnostic suggestion?

- **Concept:** Q-Former (Querying Transformer)
  - **Why needed here:** The architecture uses Q-Former to bridge Swin-Transformer and LLM, acting as bottleneck that compresses dense visual feature map into fixed number of learnable queries (14 visual concepts), making input manageable for LLM.
  - **Quick check question:** How does the Q-Former prevent the LLM from being overwhelmed by high-resolution raw output of Vision Transformer?

- **Concept:** Grad-CAM for Visual Grounding
  - **Why needed here:** M3KG construction relies on Grad-CAM to extract "disease-aware vision tokens" that highlight regions most relevant to specific classification decisions, effectively cropping X-ray to show just "lung" or "effusion" area for graph.
  - **Quick check question:** Why is Grad-CAM necessary instead of just using raw image patches? (Answer: It filters for relevance/disease evidence).

## Architecture Onboarding

- **Component map:** Chest X-ray Image + Ground Truth Reports -> GPT-4o (Entity/Relation Extraction) -> ReXKG -> Grad-CAM (Visual Token Extraction) -> R-GCN Encoder; Swin Transformer -> Q-Former; Cross-Attention (V2KG & KG2V) + Multi-scale Self-Attention -> Llama2-7B (LLM) takes concatenated embeddings.

- **Critical path:** The most critical interaction occurs at the Fusion Module. An engineer should trace how CrossAttention(Fv, Kv, Kv) retrieves visual knowledge and how Fg2n and Fn2g are projected into Llama2 embedding space (dimension 4096). If these projections are misaligned, the LLM receives garbage input.

- **Design tradeoffs:**
  - **Static vs. Dynamic KG:** M3KG is pre-constructed (static), which is efficient but might fail on novel symptom combinations not present in training corpus or CheXpert labels.
  - **Complexity:** Model uses multiple heavy components (Swin, R-GCN, Llama2). Paper notes "significant computational costs" (Page 9).
  - **Modality Gap:** Paper acknowledges (Page 9, Limitation) that alignment between visual disease features and textual graphs is currently "limited," a known bottleneck.

- **Failure signatures:**
  - **Hallucination via KG:** If KG contains relation "A suggests B" but image only shows A, model might report B because graph "primes" the LLM.
  - **Attention Decay:** If visual features are weak, KG2V attention weights might flatline, causing model to rely solely on language priors (generating generic reports).

- **First 3 experiments:**
  1. **Zero-KG Baseline:** Run pipeline with KG branch disabled (zeroed out) to isolate performance gain strictly attributable to knowledge graph.
  2. **Node Count Sensitivity:** Reproduce ablation in [Page 7, Table 9] on different dataset (e.g., IU-Xray if using CheXpert for training) to verify if 300 nodes is universal optimum or dataset-dependent.
  3. **Visual Retrieval Quality:** Visualize "disease-aware vision tokens" retrieved for specific input. Do they actually highlight diseased area or retrieve generic lung shapes? (Validates Mechanism 1).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can deep alignment mechanisms between visual disease features and textual graphs improve cross-modal fusion performance?
- **Basis in paper:** [explicit] Authors state in Section 4.8 that current model "lacks deep alignment mechanisms in terms of structural hierarchy and semantic representation," limiting performance.
- **Why unresolved:** Current framework relies on cross-attention modules which may not fully bridge semantic gap between visual tokens and graph nodes.
- **What evidence would resolve it:** Improved Clinical Efficacy (CE) metrics or qualitative visualizations demonstrating more precise mapping of visual anomalies to specific graph entities.

### Open Question 2
- **Question:** How can computational overhead of hierarchical multi-modal knowledge graphs be reduced for resource-constrained deployment?
- **Basis in paper:** [explicit] Section 4.8 highlights framework requires "training and inference on high-performance GPUs," limiting deployment in clinical scenarios with restricted resources.
- **Why unresolved:** Integration of R-GCNs, multiple cross-attention modules, and 7B parameter LLM (Llama2) creates heavy computational burden.
- **What evidence would resolve it:** Lighter variant achieving comparable BLEU/ROUGE scores on consumer-grade hardware or dedicated efficiency analysis reducing 33.29s/iter speed.

### Open Question 3
- **Question:** What strategies can mitigate performance degradation caused by excessive entity nodes in knowledge graph?
- **Basis in paper:** [inferred] Ablation study in Section 4.5 shows performance degrades when entity nodes increase from 300 to 500, attributed to "redundant or noisy information."
- **Why unresolved:** Paper identifies noise threshold but does not propose method to filter or denoise graph to allow larger, more comprehensive knowledge bases without accuracy loss.
- **What evidence would resolve it:** Dynamic pruning mechanism or graph denoising module maintaining/improving performance with >500 entity nodes.

## Limitations
- M3KG construction from GPT-4o on CheXpert Plus labels creates potential dataset bias
- Performance gains primarily demonstrated on two datasets (IU-Xray and CheXpert Plus) with relatively small scale
- Significant computational overhead due to multi-component architecture
- Limited alignment between visual features and textual graph representations

## Confidence
- **High confidence**: Experimental methodology is sound with appropriate baselines and ablation studies; performance improvements over SOTA are statistically significant and well-documented.
- **Medium confidence**: Mechanism of visual grounding via Grad-CAM-extracted tokens is plausible but depends on quality of disease localization not directly validated.
- **Medium confidence**: Multi-granularity graph design shows performance optimization at 300 nodes, but this may be dataset-specific rather than universal principle.

## Next Checks
1. **Cross-dataset robustness test**: Evaluate R2GenKG on third, independently collected radiology dataset (e.g., MIMIC-CXR) to verify if 300-node optimization and performance gains generalize beyond CheXpert Plus.
2. **Visual token quality audit**: Manually inspect 50 random cases where model retrieves "disease-aware vision tokens" - verify whether Grad-CAM correctly highlights pathological regions versus generic anatomical structures.
3. **Static KG limitation analysis**: Introduce controlled experiment with synthetic "novel disease combinations" not present in training corpus to quantify how static knowledge graph constrains model's ability to generalize beyond pre-defined knowledge.