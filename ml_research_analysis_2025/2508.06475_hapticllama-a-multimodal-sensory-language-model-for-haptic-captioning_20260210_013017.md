---
ver: rpa2
title: 'HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning'
arxiv_id: '2508.06475'
source_url: https://arxiv.org/abs/2508.06475
tags:
- haptic
- hapticllama
- human
- captions
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HapticLLaMA, the first multimodal sensory
  language model for haptic captioning. The model translates vibration signals into
  sensory, emotional, and associative natural language descriptions.
---

# HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning

## Quick Facts
- arXiv ID: 2508.06475
- Source URL: https://arxiv.org/abs/2508.06475
- Reference count: 17
- Primary result: First LLM for haptic captioning, achieving 59.98 METEOR, 32.06 BLEU-4, and >61% human ratings above 3.5

## Executive Summary
This paper introduces HapticLLaMA, the first multimodal sensory language model for haptic captioning that translates vibration signals into sensory, emotional, and associative natural language descriptions. The model employs two distinct haptic tokenizers—a frequency-based tokenizer and an EnCodec-based tokenizer—to convert raw vibration signals into discrete tokens suitable for integration with the LLaMA architecture. Trained in two stages (supervised fine-tuning with LoRA adaptation, followed by RLHF via DPO), the model demonstrates strong capability in interpreting haptic signals, achieving automated metric scores of 59.98 METEOR and 32.06 BLEU-4, with over 61% of generated captions receiving human ratings above 3.5 on a 7-point scale.

## Method Summary
HapticLLaMA converts raw vibration signals into discrete token sequences using either a frequency-based tokenizer (FFT-based with logarithmic frequency spacing) or an EnCodec neural codec (using residual vector quantization). These tokens are integrated into LLaMA-3.2-3B via LoRA-based adaptation, with only low-rank matrices inserted into query and value projection layers. The model is trained in two stages: supervised fine-tuning using human-annotated haptic captions, followed by reinforcement learning from human feedback using direct preference optimization (DPO) based on 7-point Likert scale ratings. Preference pairs are constructed by matching captions rated ≥3.5 with those rated <3.5.

## Key Results
- Achieved 59.98 METEOR and 32.06 BLEU-4 on automated metrics
- Over 61% of generated captions received human ratings above 3.5 on 7-point scale
- RLHF improved overall ratings by 10% compared to supervised learning alone
- EnCodec tokenizer outperformed frequency-based tokenizer by ~2 BLEU-4 points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete tokenization of continuous haptic signals enables LLM integration.
- Mechanism: Raw vibration signals are converted to discrete token sequences via FFT-based frequency domain binning with logarithmic spacing aligned to human just-noticeable differences, producing compact FREQ_X_AMP_Y tokens, or via EnCodec neural codec using encoder-latent-quantizer-decoder pipeline with residual vector quantization, producing richer fixed-length sequences. These tokens are added to the LLaMA vocabulary as special tokens with randomly initialized embeddings updated during training.
- Core assumption: Haptic signals share sufficient structure with language (discrete compositional units) that tokenization paradigms transfer; EnCodec's audio compression captures vibration-relevant features despite being trained on audio.
- Evidence anchors:
  - [abstract]: "We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units."
  - [section 3.3.1]: "The tokenizer then assigns a unique token (e.g., FREQ_3_AMP_2) to each frequency-amplitude pair, encoding the signal's spectral content into a form interpretable by LLMs."
  - [corpus]: HapticCap paper (Hu et al., 2025) provides the foundational dataset but does not address tokenization for LLMs—this is a novel contribution here.
- Break condition: If haptic perception requires continuous temporal dynamics that discrete tokenization fundamentally destroys (e.g., sub-token temporal microstructures), token-based approaches will hit a ceiling.

### Mechanism 2
- Claim: LoRA adaptation enables efficient haptic-text alignment without full model fine-tuning.
- Mechanism: Low-rank matrices B* and A* are inserted into query (Q) and value (V) projection layers. The adapted weight becomes W_lora = W + BA, where only A and B are trainable. This allows the pretrained LLaMA-3.2-3B to learn haptic-text associations through ~2-4% of total parameters while preserving linguistic competencies.
- Core assumption: Haptic-text alignment is a low-rank transformation of existing language representations; the embedding space has sufficient plasticity to accommodate haptic tokens without catastrophic interference.
- Evidence anchors:
  - [abstract]: "HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation."
  - [section 3.4.1]: "For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA; Hu et al., 2022), which inserts trainable low-rank matrices ΔW* into the model weights."
  - [corpus]: Related multimodal work (DiaDem, Touch100k) uses similar PEFT strategies but for vision/audio modalities—haptic-specific adaptation remains underexplored.
- Break condition: If haptic representations require modifying deeper layers or attention patterns that LoRA doesn't reach, alignment quality will plateau.

### Mechanism 3
- Claim: DPO-based RLHF improves alignment with human haptic perception beyond supervised learning.
- Mechanism: After Stage 1 SFT, generated captions are rated by humans on a 1-7 scale. Preference pairs are constructed by matching captions rated ≥3.5 (preferred) with those rated <3.5 (rejected). DPO trains the policy model to assign higher likelihood to preferred captions using a classification objective with the SFT model as reference, incorporating human perceptual judgments that n-gram metrics miss.
- Core assumption: Human ratings capture perceptual alignment that BLEU/METEOR do not; the binary threshold (3.5) meaningfully separates good/bad captions without losing signal from nuanced middle ratings.
- Evidence anchors:
  - [abstract]: "RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception."
  - [section 5.2]: "There is a visible shift in the distribution toward higher rating intervals... about 10% increase in ratings above 3.5."
  - [corpus]: No directly comparable haptic RLHF work exists; this extends DPO methodology from text/vision domains to sensory signals.
- Break condition: If preference pairs are noisy (raters disagree significantly on haptic perception) or the 3.5 threshold discards useful gradient signal from borderline cases, DPO may amplify annotation noise rather than perceptual alignment.

## Foundational Learning

- Concept: **Time-domain vs. frequency-domain signal representation**
  - Why needed here: The frequency-based tokenizer operates in the frequency domain (FFT output), while EnCodec processes raw time-domain waveforms. Understanding this distinction explains why EnCodec captures temporal/rhythmic features that frequency-only tokenization misses.
  - Quick check question: Given a rhythmic pulse signal, would you expect frequency-domain binning to capture the rhythm pattern, or would you need time-domain processing? Why?

- Concept: **Residual Vector Quantization (RVQ)**
  - Why needed here: EnCodec uses RVQ to discretize latent representations. RVQ iteratively quantizes residuals from previous quantization steps, enabling high-fidelity reconstruction with multiple codebooks. This explains EnCodec's larger vocabulary (1,024 tokens) and fixed-length outputs.
  - Quick check question: If RVQ uses 4 codebooks of 256 entries each, what is the effective vocabulary size, and how does this differ from a single codebook of equivalent capacity?

- Concept: **Direct Preference Optimization (DPO) vs. PPO-based RLHF**
  - Why needed here: HapticLLaMA uses DPO rather than PPO. DPO bypasses explicit reward model training by reframing RLHF as a classification problem on preference pairs. This is computationally simpler but requires carefully constructed pairs.
  - Quick check question: In DPO, what role does the reference model (frozen SFT model) play, and what happens if the reference model is too close or too far from the policy model during optimization?

## Architecture Onboarding

- Component map:
Raw Vibration Signal → [Haptic Tokenizer] → Discrete Token Sequence → [Vocabulary Expansion + Embedding Lookup] → [LLaMA-3.2-3B + LoRA (Q,V layers)] → Generated Caption → [Stage 1: SFT Loss] → Caption + Human Rating → [Stage 2: DPO Preference Pairs] → Final HapticLLaMA

- Critical path:
  1. Tokenizer choice determines input granularity (Frequency: 47.5 avg tokens, compact; EnCodec: 1,379 fixed tokens, rich).
  2. LoRA rank and target layers (Q,V only) control adaptation capacity.
  3. Preference pair construction (3.5 threshold, pairing strategy) directly shapes RLHF signal quality.

- Design tradeoffs:
  - **Frequency vs. EnCodec tokenizer**: Frequency offers 29x compression (47.5 vs. 1,379 tokens) suitable for real-time/edge deployment; EnCodec captures temporal dynamics but requires more compute. Paper shows EnCodec outperforms by ~2 BLEU-4 points.
  - **DPO pairing strategy**: Paper tested two approaches—(A) high/low split at 3.5 threshold vs. (B) all pairwise comparisons within same signal. Approach A performed better; approach B caused performance drop (Table 6), likely due to noise from near-boundary pairs.
  - **Human evaluation scale**: 7-point Likert with 3.5 midpoint split; alternative would be pairwise preference collection (more expensive but cleaner signal).

- Failure signatures:
  - **Tokenizer mismatch**: If vibration signals contain meaningful sub-token temporal patterns (e.g., amplitude envelopes within single frequency bins), frequency tokenizer will lose this information. Symptom: generated captions miss rhythmic or transient descriptions.
  - **LoRA under-capacity**: If LoRA rank is too low or applied to insufficient layers, haptic embeddings won't integrate with language representations. Symptom: captions are fluent but generic/unrelated to input signal (similar to signal-agnostic baseline).
  - **DPO reward hacking**: If preference pairs contain annotation noise or systematic biases (e.g., longer captions preferred regardless of accuracy), model may optimize for spurious features. Symptom: automatic metrics improve but human ratings stagnate or degrade.

- First 3 experiments:
  1. **Tokenizer ablation on temporal patterns**: Create synthetic vibrations with identical frequency content but different rhythms (single pulse vs. repeated pulses). Compare Frequency vs. EnCodec tokenizer outputs and generated caption quality. Hypothesis: EnCodec captures rhythm; Frequency tokenizer produces identical tokens for both.
  2. **LoRA layer sweep**: Train variants with LoRA applied to (a) Q,V only (paper default), (b) all attention projections (Q,K,V,O), (c) FFN layers. Measure BLEU-4 and human ratings. Hypothesis: haptic alignment may benefit from broader adaptation if signal features require non-attention transformations.
  3. **DPO threshold sensitivity**: Retrain Stage 2 with preference pair thresholds at 2.5, 3.5 (default), 4.5. Track both automatic metrics and human evaluation distribution shifts. Hypothesis: higher threshold creates cleaner but smaller preference signal; optimal point balances pair count vs. annotation quality.

## Open Questions the Paper Calls Out
None

## Limitations
- Tokenizer domain mismatch: EnCodec was originally trained on audio signals, not vibration haptics, raising questions about optimal feature extraction for haptic-specific patterns.
- Human annotation reliability: The 7-point human rating scale and 3.5 threshold for preference pair construction may oversimplify nuanced perceptual judgments and amplify rater-specific biases.
- Limited generalization: All evaluations are conducted on the HapticCap dataset, with unverified performance on real-world haptic signals from consumer or industrial devices.

## Confidence
- **HapticLLaMA achieves strong performance on HapticCap dataset**: High confidence (consistent automatic metrics and human ratings across multiple evaluations)
- **EnCodec tokenizer outperforms frequency-based tokenization**: Medium confidence (automatic metrics show advantage but domain mismatch raises questions)
- **RLHF via DPO improves alignment with human perception**: Medium confidence (10% improvement documented but binary threshold construction may oversimplify human judgment)

## Next Checks
1. **Cross-dataset generalization test**: Evaluate HapticLLaMA on vibration signals from consumer electronics or automotive haptic systems not present in HapticCap to establish real-world applicability and identify domain adaptation needs.

2. **Temporal pattern ablation study**: Create synthetic haptic signals with identical frequency spectra but varying temporal structures to systematically compare Frequency vs. EnCodec tokenizer outputs and test whether EnCodec's superior performance stems from temporal feature capture.

3. **DPO threshold sensitivity analysis**: Retrain Stage 2 DPO models using multiple preference pair thresholds (2.5, 3.5, 4.5) and compare both automatic metrics and human rating distributions to quantify the impact of threshold choice on preference pair quality.