---
ver: rpa2
title: 'POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office
  Automation'
arxiv_id: '2601.11816'
source_url: https://arxiv.org/abs/2601.11816
tags:
- policy
- typed
- planning
- execution
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLARIS introduces a governed orchestration framework for enterprise
  back-office automation that enforces typed planning, policy-aware selection, and
  validator-gated execution over LLM agents. By treating workflows as typed DAGs with
  structured plan diversity and compliance-aware reasoning, it achieves micro-F1 of
  0.81 on SROIE and 0.95-1.00 precision on anomaly routing while producing auditable
  execution traces.
---

# POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation

## Quick Facts
- arXiv ID: 2601.11816
- Source URL: https://arxiv.org/abs/2601.11816
- Reference count: 8
- Primary result: Achieves micro-F1 of 0.81 on SROIE and 0.95-1.00 precision on anomaly routing while producing auditable execution traces.

## Executive Summary
POLARIS introduces a governed orchestration framework for enterprise back-office automation that enforces typed planning, policy-aware selection, and validator-gated execution over LLM agents. By treating workflows as typed DAGs with structured plan diversity and compliance-aware reasoning, it achieves strong precision on document extraction and anomaly routing while producing auditable execution traces. The approach reduces human intervention through bounded repair loops and compiled policy guardrails that block side effects before they occur, establishing a benchmark for auditable, policy-aligned Agentic AI in regulated settings.

## Method Summary
POLARIS implements a plan-select-act loop where a CoAPlanner generates K=5 structurally diverse, type-checked DAGs of agents, which are then scored and selected by a ReasoningAgent using a rubric that prioritizes compliance, sequencing, and parsimony. Execution is guarded by validator-gated checks and a bounded repair loop (L_max=3) that iteratively refines parsing outputs, with policy guardrails blocking unsafe side effects. The system uses DSPy for few-shot planning priors and Autogen ConversableAgent for agent instantiation.

## Key Results
- SROIE micro-F1 of 0.81 on 4-field extraction (Company, Address, Date, Total)
- Synthetic invoice suite extraction F1 ~0.96 and policy violation precision ~0.82
- Anomaly routing precision of 0.95-1.00 with complete auditable traces

## Why This Works (Mechanism)

### Mechanism 1: Typed DAG Planning Constrained by I/O Contracts
Enforcing type-compatible edges and policy-ordering rules during plan synthesis produces structurally valid workflows that are more likely to be executable and compliant. The CoAPlanner generates candidate DAGs where each agent declares typed inputs/outputs. The system filters plans for feasibility: type-compatibility (parse output matches validate input) and compliance ordering (parse→validate→risk/approval). A rubric-guided selector then scores these pre-validated plans on compliance, sequencing, parsimony, and prior. This shifts validation "left" into the planning stage, reducing runtime failures.

### Mechanism 2: Bounded Validator-Gated Repair Loop for Extraction Robustness
A targeted, iterative repair loop focused on failing fields can improve document extraction quality (F1) compared to single-pass or open-ended retry approaches. After an initial document parse, a DataValidatorAgent checks for missing fields, low confidence, or rule violations. If validation fails, a targeted re-parsing is triggered with specific feedback (e.g., ROI hints, schema prompts) focused only on the failing aspects. This loop is bounded (L_max=2-3) to control latency.

### Mechanism 3: Policy-Aware Selection and Guardrails for Compliance
Injecting policy checks at both the plan selection stage and execution stage improves compliance precision and provides auditable decision rationales. A ReasoningAgent scores candidate plans using a rubric that includes a `compliance(p)` term. Additionally, a compiled policy guardrail layer (e.g., threshold rules, currency checks) gates side effects during execution (e.g., blocking an approval). This dual-layer approach ensures only compliant plans are selected and their execution remains compliant.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) with Type-Checking**
  - Why needed here: POLARIS represents workflows as DAGs of agents. Understanding that edges represent data flow with specific types is critical for debugging why a plan was rejected or how agents are sequenced.
  - Quick check question: Can you explain why a plan with `AnomalyDetection` before `DocumentParser` would be considered infeasible?

- **Concept: z-Score Anomaly Detection with MAD**
  - Why needed here: The system uses Median Absolute Deviation (MAD) to detect amount anomalies. This is a statistical technique more robust to outliers than standard deviation.
  - Quick check question: Why might MAD be preferred over standard deviation when calculating a z-score for financial amounts that could contain extreme outliers?

- **Concept: Rubric-Based Reasoning**
  - Why needed here: Plan selection is not a simple "best-of-N" sampling but a weighted scoring against a fixed rubric (compliance, sequencing, parsimony). Understanding this is key to interpreting why a specific plan was chosen.
  - Quick check question: If two plans are equally compliant, what is the next most important factor in the rubric for selecting between them?

## Architecture Onboarding

- **Component map:**
  Input Normalizer -> CoAPlanner -> ReasoningAgent -> Scheduler -> DocumentParser -> DataValidator -> (parallel) PolicyRetrieval, RiskControl, AnomalyDetection -> (serial sink) Approval, ReportGenerator

- **Critical path:** Input Normalizer → CoAPlanner → ReasoningAgent → Scheduler dispatches DocumentParser → Validator Loop → (parallel) Policy, Risk, Anomaly agents → (serial sink) Approval/Report

- **Design tradeoffs:**
  - Plan Diversity (K) vs. Latency: Higher K offers better plan options but increases planning time. The paper uses K=5.
  - Repair Budget (L_max) vs. Latency/Reliability: Higher repair budget improves extraction accuracy at the cost of latency. Set to 2-3.
  - Parallelism vs. Dependency Safety: The scheduler allows parallel checks but enforces strict dependencies (e.g., Parser first). This balances speed with correctness.

- **Failure signatures:**
  - Plan Rejection: ReasoningAgent returns an error or no valid plan. Check if all K plans violate a hard constraint (e.g., type mismatch).
  - Repair Loop Timeout: Parser-Validator loop exceeds L_max. Check validator rules or document quality.
  - Policy Violation: RiskControl blocks execution. Inspect the specific triggered rule (e.g., unknown vendor, threshold breach).

- **First 3 experiments:**
  1. Reproduce Synthetic Baseline: Run the system on the 40-invoice synthetic suite provided in the paper. Verify that extraction F1 (~0.96) and policy violation precision (0.82) match reported figures. This validates the core pipeline.
  2. Ablate Repair Loop: Disable the validator-gated repair loop (L_max=1). Measure the drop in extraction F1 on noisy documents (VL scenario). This quantifies the loop's contribution.
  3. Vary Plan Diversity: Change K (number of candidate plans) from 1 to 5. Measure the selection success rate and quality of the chosen plan (based on rubric score) on complex tasks. This tests the diversity constraint mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the POLARIS architecture generalize to other regulated domains such as supply chain, HR, and compliance, and how does it integrate with standard agent benchmarks?
- Basis in paper: The Conclusion states that "Broader cross-domain validation and integration with standard AgentBench tasks remain areas of future work."
- Why unresolved: The empirical evaluation is restricted to document-centric finance tasks (SROIE and synthetic invoices), leaving performance in other complex domains untested.
- What evidence would resolve it: Benchmark results on non-finance datasets (e.g., supply chain logs or HR compliance documents) showing comparable precision, recall, and policy adherence.

### Open Question 2
- Question: What specific dimensions constitute a "standardized governance metric" for enterprise Agentic AI, and can they be unified across different frameworks?
- Basis in paper: Section 5.4 notes that "Future benchmarks may expand these dimensions with standardized governance metrics and multi-agent reproducibility protocols."
- Why unresolved: The paper introduces specific primitives (typed DAG traces, repair logs) but lacks a universal schema for comparing governance across different agent systems.
- What evidence would resolve it: A formalized, cross-framework evaluation schema that quantifies audit-trace completeness and policy alignment, validated on multiple agent architectures.

### Open Question 3
- Question: Can the rubric weights used for plan selection be dynamically learned or adapted, rather than remaining fixed heuristic constants?
- Basis in paper: Section 4.4 specifies that the weights $(w_1, w_2, w_3)$ for the utility function $U(p;\tau)$ are "fixed in this work," suggesting they were manually tuned.
- Why unresolved: Static weights may not optimize the trade-off between compliance, sequencing, and parsimony across varying enterprise contexts or data drifts.
- What evidence would resolve it: A comparative study showing that an adaptive or learned weighting strategy yields higher F1 scores or lower repair rates than the fixed configuration.

## Limitations
- Evaluation relies on synthetic invoice data and a modified SROIE benchmark rather than production enterprise datasets, limiting generalizability.
- Critical hyperparameters (rubric weights, confidence thresholds, repair budget) are not specified, and the exemplar bank for few-shot planning priors is not provided.
- The absence of published datasets, policies, and detailed experimental protocols hinders independent validation.

## Confidence
- **High Confidence:** The typed DAG planning mechanism and validator-gated repair loop represent valid architectural approaches for enforcing structural correctness and improving extraction robustness.
- **Medium Confidence:** The reported F1=0.81 on SROIE and precision metrics (0.95-1.00) are promising but require reproduction on independently generated synthetic data and real enterprise documents to confirm.
- **Low Confidence:** Without access to the actual policy database, exemplar bank, and detailed hyperparameters, exact replication of the results is not feasible.

## Next Checks
1. Reproduce Synthetic Baseline: Run the system on the 40-invoice synthetic suite provided in the paper. Verify that extraction F1 (~0.96) and policy violation precision (0.82) match reported figures.
2. Ablate Policy Guardrails: Remove the RiskControl policy guardrails and measure the increase in policy violations. This quantifies the contribution of the compiled policy checks to compliance precision.
3. Stress Test Type Constraints: Systematically inject type mismatches and invalid sequencing into plans and measure the planner's ability to reject them. This validates the robustness of the typed DAG planning and type-checking mechanism.