---
ver: rpa2
title: Does higher interpretability imply better utility? A Pairwise Analysis on Sparse
  Autoencoders
arxiv_id: '2510.03659'
source_url: https://arxiv.org/abs/2510.03659
tags:
- steering
- interpretability
- features
- score
- saes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether higher interpretability of Sparse
  Autoencoders (SAEs) translates into better steering utility for controlling large
  language models (LLMs). To answer this, the authors train 90 SAEs across three model
  sizes, five architectures, and six sparsity levels, then measure interpretability
  using SAEBENCH and steering performance using AXBENCH.
---

# Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders

## Quick Facts
- arXiv ID: 2510.03659
- Source URL: https://arxiv.org/abs/2510.03659
- Reference count: 40
- Primary result: Only weak positive correlation (τ_b ≈ 0.298) between SAE interpretability and steering utility, with interpretability not a reliable proxy for steering performance.

## Executive Summary
This paper investigates whether higher interpretability of Sparse Autoencoders (SAEs) translates into better steering utility for controlling large language models (LLMs). Through extensive experimentation across 90 SAEs spanning three model sizes, five architectures, and six sparsity levels, the authors find only a weak positive correlation between interpretability and utility. They introduce Delta Token Confidence, a novel feature selection criterion that improves steering performance by 52.52% by identifying features that maximally change next-token distribution confidence. Critically, after selecting high-utility features using this method, the interpretability-utility correlation vanishes or becomes negative, demonstrating that the most effective steering features are not necessarily the most interpretable.

## Method Summary
The authors train 90 SAEs (30 per model across three base LLMs) using five different architectures and six sparsity levels. They evaluate interpretability using SAEBENCH's AutoInterp score, which measures an LLM-judge's ability to predict latent activations from human-generated descriptions. Steering utility is assessed via AXBENCH using harmonic mean of concept, instruction, and fluency scores. The key innovation is Delta Token Confidence, which selects features based on their impact on next-token distribution confidence when amplified. This method identifies features that improve steering performance by 52.52% compared to output-score baselines.

## Key Results
- Weak positive correlation (τ_b ≈ 0.298) between interpretability and steering utility across 90 SAEs
- Delta Token Confidence feature selection improves steering by 52.52% over output-score baselines
- After selecting high-utility features, interpretability-utility correlation becomes negligible (τ_b ≈ -0.069) or negative
- Moderate sparsity levels (L0 ≈ 80-160) show the strongest interpretability-utility relationship

## Why This Works (Mechanism)

### Mechanism 1: Weak Coupling Between Interpretability and Steering Utility
SAE training objectives optimize reconstruction-sparsity trade-offs, producing human-readable features that need not correspond to causally influential directions in activation space. The AutoInterp scoring pipeline measures a distinct quality from causal control over generation, explaining the weak correlation.

### Mechanism 2: ΔToken Confidence Identifies Causally Salient Features
Ranking features by their impact on next-token distribution (ΔC_k) selects features that improve steering by 52.52% over output-score baselines. Features with large |ΔC_k| are those whose decoder atoms lie in directions the model is sensitive to at the intervention layer.

### Mechanism 3: High-Utility Features Decouple from Interpretability
After selecting features with high ΔToken Confidence, the interpretability-utility correlation vanishes (τ_b ≈ 0) or becomes negative. Features that maximally shift token distributions tend to be low-level or distributed patterns rather than high-level human-interpretable concepts.

## Foundational Learning

**Concept: Sparse Autoencoder (SAE) Decomposition**
- Why needed: SAEs decompose LLM activations into sparse high-dimensional features; understanding encoder-decoder structure is essential for interpreting steering intervention.
- Quick check: Given an SAE with encoder W_E and decoder W_D, what is the reconstruction of activation x, and which component provides the steering direction?

**Concept: Kendall's Rank Correlation (τ_b)**
- Why needed: The paper uses τ_b to quantify agreement between interpretability and utility rankings across SAEs, handling ties and ordinal comparisons.
- Quick check: If SAE A has higher interpretability but lower utility than SAE B, does this pair contribute +1, -1, or 0 to the τ_b numerator?

**Concept: Token Entropy and Confidence**
- Why needed: ΔToken Confidence builds on entropy H(p) and top-k confidence C_k(p); distinguishing these is necessary to understand the method.
- Quick check: For a distribution p over vocabulary V, does higher token confidence C_k correspond to sharper or flatter top-k predictions?

## Architecture Onboarding

**Component map:**
SAE Training Pipeline -> Interpretability Evaluation (SAEBENCH) -> Steering Evaluation (AXBENCH) -> ΔToken Confidence Selection

**Critical path:**
1. Train SAEs with target L_0 across architectures and sparsity levels
2. Score 1,000 latents per SAE with SAEBENCH; sample CONCEPT100
3. Compute Steering Score per SAE using AXBENCH on CONCEPT100
4. Compute Kendall's τ_b between Interpretability Score and Steering Score
5. Rank features by |ΔC_k|; select top-K per direction; re-evaluate steering

**Design tradeoffs:**
- Sparsity vs. Steering: Higher L_0 improves reconstruction but weakens interpretability-utility correlation
- Architecture Selection: BatchTopK shows most stable steering gains, but Gated/JumpReLU also perform well
- Intervention Layer: Fixed mid-layers (12/17/20) balance concept abstraction and causal influence

**Failure signatures:**
- Low Steering Score despite high interpretability: SAE features are human-readable but lie in low-causal-influence directions
- Negative ΔC_k features decrease fluency: Over-amplification or selection of features tied to syntactic patterns
- High variance in steering gains across SAEs: Architectural sensitivity at small model scales

**First 3 experiments:**
1. Replicate τ_b correlation on a single model: Train 10 SAEs on Gemma-2-2B, verify τ_b ≈ 0.2-0.3
2. Validate ΔToken Confidence selection: For one SAE, rank features by |ΔC_k|, compare steering scores against baselines
3. Test correlation collapse post-selection: After selecting high-ΔC_k features across all SAEs, recompute τ_b

## Open Questions the Paper Calls Out

**Open Question 1**: Can SAE training objectives be fundamentally redesigned to optimize for controllability and utility without relying on post-hoc feature selection? The paper concludes the need for "fundamentally new, utility-oriented SAE training paradigms" to bridge the interpretability-utility gap.

**Open Question 2**: Do task-general utility indicators exist that reliably predict steerability across different architectures and model scales? The authors call for development of "task-general utility indicators that reliably predict steerability across models."

**Open Question 3**: Does the negative correlation between interpretability and utility persist or intensify as model scale increases beyond 9B parameters? The study is limited to 2B-9B models, but results suggest this may be a fundamental property of deep representation learning.

## Limitations

- Interpretability evaluation reliability depends on LLM-judge predictions, which may inherit biases from the judge model's understanding
- Feature selection generalizability may be architecture- and layer-dependent, assuming single-feature amplification captures causal influence
- Steering task representativeness limited to 10 Alpaca-Eval instructions and CONCEPT100 latents, potentially not capturing full diversity of scenarios

## Confidence

**High Confidence**: The weak positive correlation between interpretability and steering utility (τ_b ≈ 0.298) is robustly demonstrated across 90 SAEs and three model scales.

**Medium Confidence**: The 52.52% improvement from Delta Token Confidence selection is well-supported within the experimental framework, though absolute steering scores remain modest.

**Low Confidence**: The claim that interpretability and utility become negatively correlated after feature selection is based on limited statistical evidence (τ_b ≈ -0.069).

## Next Checks

1. **Cross-Architecture Validation**: Replicate the full analysis pipeline on a fourth model architecture (e.g., Llama-3 or Mistral) to test generalizability beyond the original three model families.

2. **Multi-Layer Intervention Study**: Extend Delta Token Confidence selection to evaluate feature impact across multiple intervention layers and compare performance against single-layer selection.

3. **Human Expert Correlation Study**: Commission human experts to independently rate SAE features for interpretability and correlate these judgments with both AutoInterp scores and steering performance.