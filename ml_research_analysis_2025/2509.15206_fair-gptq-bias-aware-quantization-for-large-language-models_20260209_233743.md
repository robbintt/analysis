---
ver: rpa2
title: 'Fair-GPTQ: Bias-Aware Quantization for Large Language Models'
arxiv_id: '2509.15206'
source_url: https://arxiv.org/abs/2509.15206
tags:
- quantization
- bias
- fair-gptq
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fair-GPTQ, the first quantization method
  designed to reduce bias in large language models during compression. It modifies
  the GPTQ optimization objective by adding a group-fairness constraint that penalizes
  differences in likelihood between stereotypical and anti-stereotypical sentence
  pairs.
---

# Fair-GPTQ: Bias-Aware Quantization for Large Language Models

## Quick Facts
- **arXiv ID**: 2509.15206
- **Source URL**: https://arxiv.org/abs/2509.15206
- **Reference count**: 39
- **Primary result**: First quantization method that reduces bias in LLMs during compression by adding group-fairness constraints to GPTQ

## Executive Summary
This paper introduces Fair-GPTQ, a novel post-training quantization method that reduces social bias in large language models while compressing them to 4-bit precision. The method modifies the standard GPTQ optimization objective by adding a group-fairness constraint that penalizes differences in likelihood between stereotypical and anti-stereotypical sentence pairs. Fair-GPTQ consistently reduces stereotype scores across multiple benchmarks while preserving at least 90% of zero-shot performance, making it practical for real-world deployment.

## Method Summary
Fair-GPTQ modifies the GPTQ quantization objective by adding a group-fairness constraint that penalizes differences in representation between stereotypical and anti-stereotypical inputs. The method applies this bias-aware quantization only to attention output projection and fully connected output matrices, which are most influential for bias and token generation. During compression, the approach separates a debiasing update from the standard quantization loop, enabling computational efficiency identical to GPTQ while reducing stereotype scores by 1-3 points across benchmarks.

## Key Results
- Reduces CrowS-Pairs stereotype score from 65.95 to 63.92 for Mistral-7B while preserving >90% zero-shot accuracy
- Maintains <2× baseline perplexity on WikiText-2 across all experiments
- Outperforms iterative debiasing baselines with faster runtime and better fairness-accuracy tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding group-fairness constraint to quantization objective reduces stereotype likelihood differences between paired inputs during compression
- Mechanism: Modifies GPTQ objective to minimize reconstruction error plus penalty for differences between stereotypical and anti-stereotypical representations via α||W'(X⁰ - X¹)||²₂ term
- Core assumption: Bias signal from paired inputs generalizes to broader stereotype generation patterns during inference
- Evidence: Mathematical derivation in Section 3.3 with Eq. 2 showing modified objective; empirical results show consistent stereotype reduction

### Mechanism 2
- Claim: Applying bias-aware quantization only to attention output projection and FFN output matrices achieves most debiasing effect with minimal performance loss
- Mechanism: These matrices directly influence residual stream and token generation; weight updates show largest relative changes under Fair-GPTQ
- Core assumption: Bias predominantly flows through these output projections rather than input projections
- Evidence: Section 4.2 ablation showing matrix contributions; Figure 4 visualization of weight updates

### Mechanism 3
- Claim: Separating debiasing update from quantization loop enables computational efficiency identical to standard GPTQ
- Mechanism: Debiasing pre-update W ← W - (H⁻¹H_bias W^T)^T applied once before column-wise quantization, reusing same H⁻¹ computed for reconstruction
- Core assumption: Hessian inverse accurately captures both reconstruction sensitivity and bias sensitivity
- Evidence: Algorithm 1 in Section 3.4 showing two-phase structure; theoretical analysis of computational complexity

## Foundational Learning

- **Optimal Brain Surgeon (OBS) framework**: Why needed - Fair-GPTQ extends OBS by relaxing local-minimum assumption; understanding second-order Taylor expansion and Hessian-based weight saliency is essential. Quick check - In standard OBS, why is gradient term J assumed zero, and what changes when Fair-GPTQ introduces paired inputs?

- **Group fairness via likelihood parity**: Why needed - Paper formalizes bias as |μ_Y(M_θ, G_a) - μ_Y(M_θ, G_b)| < ε where μ measures log-likelihood differences on stereo/anti-stereo pairs. Quick check - How does likelihood-based fairness metric differ from demographic parity in classification?

- **Weight-only post-training quantization**: Why needed - Method operates on weights only (not activations), isolating fairness effects from activation-level modifications. Quick check - Why does weight-only quantization allow cleaner isolation of bias effects compared to activation-aware methods like AWQ?

## Architecture Onboarding

- **Component map**: StereoSet dev pairs → X⁰ (stereotypical), X¹ (anti-stereotypical) representations → Hessian computation (H, H_bias) → Debiasing pre-update (target matrices only) → Column-wise quantization with error compensation → 4-bit quantized weights with fairness constraint

- **Critical path**: 1) Load model and StereoSet dev (4,212 pairs) 2) For each layer, identify attention.out_proj and FC2/down_proj 3) Compute H and H_bias from calibration activations 4) Apply debiasing pre-update with α (0.1 ALL, 0.5 subset) 5) Run standard GPTQ quantization on updated weights 6) Quantize other matrices with vanilla GPTQ

- **Design tradeoffs**: α parameter (higher = more debiasing but higher perplexity); layer selection (lower layers stronger debiasing, upper layers better task performance); group size (32 for instruction-tuned, 128 for base models); target matrices vs all matrices (applying to all increases perplexity without proportional gain)

- **Failure signatures**: Perplexity explosion (>100) - likely too-high α or wrong layer subset; no bias reduction - verify pairs correctly matched; numerical instability - H_bias addition may cause Cholesky failure; zero-shot collapse (>50% drop) - reduce α, switch to lower layer strategy, or increase group size

- **First 3 experiments**: 1) Reproduce Table 1 baseline: OPT-6.7B with Fair-GPTQ (ALL, α=0.1), verify CrowS-Pairs drops from ~67.98 to ~67.26 while preserving >90% zero-shot accuracy 2) Layer ablation: Compare LOWER (10%), UPPER (10%), ALL on Mistral-7B, expect LOWER to achieve best CP score (~63.09) with acceptable perplexity (~7.33) 3) Matrix ablation: On OPT-1.3B, apply Fair-GPTQ to single matrix types and measure stereo score reduction vs perplexity increase

## Open Questions the Paper Calls Out

- **Multilingual bias mitigation**: The paper notes that "multilingual calibration data would be expected to improve performance for multilingual models" but current experiments are strictly monolingual English. Evidence needed: Experiments applying Fair-GPTQ to multilingual LLMs using calibration data in target languages.

- **Long-context narrative calibration**: Section F states future work plans to "introduce an extended dataset that provides additional context, forming story-like narratives" as current calibration is limited to short sequences. Evidence needed: Evaluation on newly curated dataset of story-like narratives with minimal stereotypical differences.

- **Performance recovery with outlier channels**: The Conclusion suggests exploring "half-precision outlier channels (Lee et al., 2024) to recover the performance of debiased models" as Fair-GPTQ preserves 90% but not 100% of baseline accuracy. Evidence needed: Hybrid quantization experiments measuring zero-shot accuracy alongside stereotype scores.

## Limitations

- **Empirical validation gaps**: Ablation studies limited to 4-bit symmetric quantization only; effectiveness with 3-bit, 5-bit, or mixed-precision quantization unknown; calibration set may not capture all bias dimensions in real-world usage.

- **Generalization concerns**: Effectiveness on bias dimensions beyond gender/race/religion (age, disability, intersectional identities) or domain-specific biases (medical, legal, technical) not evaluated; assumption that calibration pair differences generalize to all inference scenarios not empirically validated.

- **Technical implementation risks**: Cholesky decomposition may encounter numerical instability with high α values or ill-conditioned Hessians; computational overhead claim assumes efficient implementation that may vary across hardware architectures.

## Confidence

- **High confidence**: Core mechanism of adding group-fairness constraints to quantization objective; applying debiasing only to attention output projection and FCN output matrices
- **Medium confidence**: Computational efficiency claim showing identical runtime to GPTQ; α parameter tuning strategy (0.1 for ALL layers, 0.5 for subset)
- **Low confidence**: Assumption that bias signal from calibration pairs generalizes to all inference scenarios; numerical stability across different model scales and quantization bit-widths

## Next Checks

1. **Cross-bias generalization test**: Apply Fair-GPTQ to models trained on datasets with different bias characteristics (biomedical literature, legal documents) and evaluate performance on bias benchmarks specific to those domains to validate whether stereo/anti-stereo calibration approach generalizes beyond social stereotypes.

2. **Ablation of calibration pair diversity**: Systematically vary number and diversity of calibration pairs (100, 500, 1000 pairs from different bias categories) to determine minimum effective sample size and identify saturation points where additional pairs provide diminishing returns on bias reduction.

3. **Numerical stability stress test**: Intentionally push α to high values (1.0, 2.0, 5.0) and measure failure rates for Cholesky decomposition across different model sizes and layer types; document error recovery procedures and establish safe operating ranges for practical deployment.