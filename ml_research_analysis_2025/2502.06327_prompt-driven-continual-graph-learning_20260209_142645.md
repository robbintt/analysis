---
ver: rpa2
title: Prompt-Driven Continual Graph Learning
arxiv_id: '2502.06327'
source_url: https://arxiv.org/abs/2502.06327
tags:
- graph
- prompts
- learning
- prompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROMPT CGL introduces prompt-based learning to continual graph
  learning to address scalability and privacy issues in evolving graphs. Unlike replay-based
  methods that require storing historical data, PROMPT CGL learns a separate prompt
  for each incoming task while keeping the underlying graph neural network fixed,
  naturally avoiding catastrophic forgetting.
---

# Prompt-Driven Continual Graph Learning

## Quick Facts
- arXiv ID: 2502.06327
- Source URL: https://arxiv.org/abs/2502.06327
- Reference count: 40
- Primary result: Achieves state-of-the-art CGL accuracy with minimal prompts (2-3) and constant memory usage

## Executive Summary
PROMPT CGL introduces prompt-based learning to continual graph learning to address scalability and privacy issues in evolving graphs. Unlike replay-based methods that require storing historical data, PROMPT CGL learns a separate prompt for each incoming task while keeping the underlying graph neural network fixed, naturally avoiding catastrophic forgetting. The method employs hierarchical prompting to address feature and topology variations between tasks, and a personalized prompt generator to create tailored prompts for individual nodes while maintaining constant memory usage regardless of graph scale. Experiments on four benchmarks show that PROMPT CGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption.

## Method Summary
PROMPT CGL operates by first pre-training a graph neural network backbone on an initial task, then freezing its parameters for all subsequent tasks. For each new task, the method learns task-specific prompts that modify the input features and intermediate representations through a hierarchical two-stage process. Node-level prompts address feature discrepancies while subgraph-level prompts handle structural variations. A personalized prompt generator creates node-specific prompt vectors through a query-based aggregation mechanism, enabling adaptation with constant memory regardless of graph size. The approach maintains a prompt bank for inference, retrieving the appropriate prompts based on task identity.

## Key Results
- Achieves superior performance against existing CGL approaches on CoraFull, OGB-Arxiv, Reddit, and OGB-Products
- Maintains constant memory usage with 2-3 prompts per task regardless of graph scale
- Demonstrates robust performance across different GNN architectures (GCN, GAT, SAGE)
- Reduces memory consumption significantly compared to replay-based methods

## Why This Works (Mechanism)

### Mechanism 1: Parameter Isolation via Frozen Backbone
- Claim: Fixing the GNN backbone after initial pre-training while learning only task-specific prompts preserves prior knowledge without requiring historical data storage.
- Mechanism: After pre-training on task T0, the backbone parameters Θ are frozen. For each subsequent task Tt, only the prompt parameters P (node-level Pn and subgraph-level Ps) and prediction layer Φ are optimized via the objective in Eq. (7). Since no weight updates occur on the backbone, representations learned for earlier tasks remain intact.
- Core assumption: The initial pre-training task T0 produces a sufficiently general feature extractor that can be adapted to new task distributions through additive prompt vectors alone.
- Evidence anchors:
  - [abstract]: "learns a separate prompt for each incoming task while keeping the underlying graph neural network fixed, naturally avoiding catastrophic forgetting"
  - [section III.B.1]: "we first pre-train the backbone gΘ and the prediction layer fΦ on the initial task T0 without prompts and then freeze the backbone parameters in subsequent tasks"
  - [corpus]: AL-GNN paper (arxiv 2512.18295) similarly proposes "replay-free continual graph learning," suggesting parameter isolation is an emerging strategy, though validation remains limited to benchmark settings.

### Mechanism 2: Hierarchical Prompting for Feature-Topology Decoupling
- Claim: Applying prompts at two distinct stages—before and after the first GNN layer—separately addresses feature-level and topology-level variations between task graphs.
- Mechanism: Node-level prompts Pn are added to raw features X0 (Eq. 2), shifting the input distribution. These prompted features pass through GNN layer 1 to obtain topology-enriched representations X1. Subgraph-level prompts Ps are then added to X1 (Eq. 4), adjusting for structural differences. This two-stage design captures both node attribute shifts and neighborhood pattern shifts.
- Core assumption: Feature and topology discrepancies in CGL are separable enough that two additive interventions suffice; complex interactions between them do not require joint modeling.
- Evidence anchors:
  - [abstract]: "hierarchical prompting to address feature and topology variations between tasks"
  - [section III.B.2]: "node-level prompts to address feature discrepancies and subgraph-level prompts to handle structural variations"
  - [Table III]: Ablation shows removing either NP or SP causes AP drops of 4-27%, with NP removal causing the largest degradation on CoraFull (-26.7%).
  - [corpus]: Related work on evolving domain shifts (GCAL, arxiv 2505.16860) addresses "continuous domain shifts" but does not explicitly separate feature vs. topology prompting—this decomposition appears novel here but under-tested across diverse graph types.

### Mechanism 3: Personalized Prompt Generator for Memory-Efficient Adaptation
- Claim: A query-based aggregation mechanism generates node-specific prompts from a small shared prompt pool, achieving personalization with constant O(k·d) memory regardless of graph size.
- Mechanism: For node i, the personalized prompt pp_i is computed as a weighted sum of k prompt vectors (Eq. 5): pp_i = Σ αj·pj, where α = Softmax(Qx_i). The query matrix Q is decomposed into low-rank vectors u ⊗ v (Eq. 6). This allows infinite node-level variation while storing only k vectors per prompt level.
- Core assumption: Node representations x_i contain enough signal for the query mechanism to meaningfully differentiate prompt aggregation weights; k=2-3 basis vectors span the necessary variation.
- Evidence anchors:
  - [abstract]: "personalized prompt generator to create tailored prompts for individual nodes while maintaining constant memory consumption"
  - [section III.B.3]: Equations (5-6) define the generator; "leading to a total space complexity of O(k·d)"
  - [Table IV]: Performance plateaus at k=2-3, with 3 prompts yielding 95.4% AP on CoraFull and 96.7% on Arxiv—adding a 4th prompt slightly decreases performance.
  - [corpus]: No direct corpus comparison for this specific PG design; memory-efficient prompt personalization appears unique to this framework.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Neural Networks**
  - Why needed here: PROMPT CGL's core motivation is avoiding forgetting without replay buffers. Understanding why sequential gradient updates overwrite prior knowledge clarifies why parameter isolation is effective.
  - Quick check question: If a model trained on task A is fine-tuned on task B, what happens to accuracy on task A, and why does freezing weights prevent this?

- Concept: **Graph Neural Network Message Passing**
  - Why needed here: The frozen backbone is a multi-layer GNN that aggregates neighbor information. Hierarchical prompts operate before and after this aggregation—comprehending how topology enters representations is essential.
  - Quick check question: In a 2-layer GCN, what does the output of layer 1 represent compared to layer 2, and where does the subgraph-level prompt intervene?

- Concept: **Prompt Learning in Foundation Models**
  - Why needed here: This work adapts the prompt tuning paradigm from NLP/Vision to graphs. The shift from "tune weights" to "learn task-specific input modifications" is the conceptual foundation.
  - Quick check question: In prefix-tuning for language models, what parameters are learned vs. frozen, and how does PROMPT CGL's approach parallel this?

## Architecture Onboarding

- Component map:
  - Backbone gΘ: 2-layer GNN (GCN/GAT/SAGE), pre-trained on T0, frozen thereafter. Outputs node representations.
  - Prediction Layer fΦ: Shared across all tasks, updated with low learning rate (5e-4).
  - Node-level Prompts Pn: k vectors in R^{k×df}, added to raw features via PG.
  - Subgraph-level Prompts Ps: k vectors in R^{k×dh}, added to layer-1 outputs via PG.
  - Personalized Prompt Generator (PG): Query mechanism with low-rank decomposition (u, v), generates pp_i per node.
  - Prompt Bank: Storage for learned (Pn, Ps, u, v) per task, retrieved by task ID at inference.

- Critical path:
  1. Pre-train gΘ + fΦ on T0 (standard supervised GNN training).
  2. Freeze Θ; initialize Pn, Ps, u, v for task Tt.
  3. For each node: compute pp_i = PG(X0, Pn), add to features → Xp_0.
  4. Pass through GNN layer 1 → X1 (topology-enriched).
  5. Compute subgraph pp_i via PG(X1, Ps), add → Xp_1.
  6. Pass through remaining layers → predictions.
  7. Update only Pn, Ps, u, v, Φ via cross-entropy loss.
  8. Store prompt parameters to bank; repeat for next task.

- Design tradeoffs:
  - Prompt count k: Table IV shows k=3 is optimal; k=1 severely underperforms (78.3% AP on Arxiv vs. 96.7%). k=4 adds memory without gain.
  - Prompt dimension: Table V shows 32-dim prompts match higher dimensions (94-96% AP), suggesting PG compensates for capacity.
  - Learning rates: Section III.B.4 notes prediction layer uses 5e-4 (lower) vs. 0.01 for prompts. Figure 7 shows AP is insensitive to β but sensitive to α—prompt tuning drives adaptation.
  - Backbone choice: Table VI shows GCN slightly outperforms GAT and SAGE (95.4% vs. 93-94.4% on CoraFull), but all variants remain viable.

- Failure signatures:
  - High forgetting (AF > 5%): Indicates prediction layer learning rate may be too high, or prompts are not sufficiently task-specific. Check α/β ratio.
  - AP saturation with increased k: If k>3 shows no improvement, node heterogeneity may be low—consider reducing k to save memory.
  - Poor transfer to new domains: If T0 pre-training distribution is too narrow, frozen backbone cannot extract useful features—verify pre-training task diversity.
  - Inference task ID errors: System relies on correct task identifier retrieval; misidentification causes wrong prompt retrieval.

- First 3 experiments:
  1. Ablation on hierarchical components: Replicate Table III on your target dataset. Remove PG, NP, and SP individually to confirm which components are critical for your graph's feature vs. topology complexity.
  2. Prompt count sweep: Run k ∈ {1, 2, 3, 4, 5} and plot AP vs. memory. Identify the knee point for your data—it may differ from the paper's k=3 if your graph has higher node diversity.
  3. Backbone substitution test: Swap the default GCN for your preferred GNN architecture (e.g., GAT for attention interpretability or SAGE for sampling scalability). Compare AP and runtime to validate architecture-agnostic claims in Table VI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework perform inference without explicit task identifiers (Task-Agnostic setting)?
- Basis in paper: [Explicit] Section III-B.5 describes the inference process, stating that the model retrieves the corresponding prompt parameters from the bank "based on the task identifier."
- Why unresolved: In realistic continual learning streams, the model rarely knows the specific task ID of the test instance a priori (Class-Incremental Learning setting), yet the current method requires this information to select the correct prompt.
- What evidence would resolve it: A modification allowing the model to automatically select or aggregate prompts based on the input sample's features, tested in a task-agnostic evaluation setting.

### Open Question 2
- Question: How does the fixed backbone pre-trained on the initial task (T0) limit performance on future tasks with significantly different feature distributions?
- Basis in paper: [Explicit] Section III-B.1 states that the backbone is pre-trained on the initial task T0 and then "freeze[s] the backbone parameters in subsequent tasks."
- Why unresolved: If the feature distribution of T0 differs drastically from later tasks, the frozen representation may lack the plasticity to encode new structures effectively, potentially capping performance regardless of prompt tuning.
- What evidence would resolve it: Experiments varying the domain similarity between T0 and subsequent tasks, or analyzing the feature space alignment between the frozen backbone and later task graphs.

### Open Question 3
- Question: Why does increasing the number of prompts (k) lead to performance degradation?
- Basis in paper: [Explicit] Section IV-F.2 and Table IV note that increasing prompts from 3 to 4 results in a minor performance drop, which is counter-intuitive for capacity increases.
- Why unresolved: While the authors suggest 2-3 prompts are optimal, the mechanism causing performance decay with more prompts (e.g., optimization difficulty or overfitting) is not fully explained.
- What evidence would resolve it: An ablation study analyzing the gradient updates or feature variance of the prompt vectors as k increases to determine if prompts begin to interfere with one another.

### Open Question 4
- Question: Can the method handle evolving graphs where new nodes connect to nodes from previous tasks?
- Basis in paper: [Inferred] The methodology (Section III-A) defines the setting as a sequence of datasets Dt with graph Gt, implying a disjoint task structure rather than a single monolithic graph that grows over time.
- Why unresolved: In many real-world CGL scenarios (e.g., citation networks), new nodes form edges with historical nodes. The current frozen backbone and task-specific prompts may not support message passing across these temporal task boundaries.
- What evidence would resolve it: Testing the framework on a dynamic graph setting where edges exist between the nodes of the current task Tt and the frozen nodes of previous tasks T_{<t}.

## Limitations
- The claim that k=2-3 prompts suffice for all graph types remains unverified across diverse real-world datasets beyond the four benchmarks tested.
- The separation of feature vs. topology prompting assumes these shifts are independent, which may not hold in graphs with correlated node attribute and structural changes.
- Memory savings depend on task oracle availability; if task inference is required at test time, additional complexity may be needed.

## Confidence
- **High confidence**: Parameter isolation mechanism (freezing backbone), memory efficiency claims, and ablation results for prompt count and dimensions.
- **Medium confidence**: Hierarchical prompting effectiveness across diverse graph types, generalization to non-class-incremental scenarios, and scalability to larger graphs.
- **Low confidence**: Task inference protocol without oracle, performance on graphs with correlated feature-topology shifts, and robustness to noisy or missing node features.

## Next Checks
1. Cross-dataset robustness test: Apply PROMPT CGL to at least two additional graph datasets (e.g., citation networks with different scales, social networks) to verify hierarchical prompting generalizes beyond the four benchmarks.
2. Feature-topology correlation analysis: Design experiments where node attributes and structural roles are explicitly correlated, then measure whether hierarchical prompts still outperform unified prompting approaches.
3. Task inference without oracle: Implement a task identifier network and evaluate performance degradation compared to oracle-based task selection to validate real-world deployment readiness.