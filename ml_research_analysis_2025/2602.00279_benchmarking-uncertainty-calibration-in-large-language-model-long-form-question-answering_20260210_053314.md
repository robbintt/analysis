---
ver: rpa2
title: Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question
  Answering
arxiv_id: '2602.00279'
source_url: https://arxiv.org/abs/2602.00279
tags:
- accuracy
- invalid
- label
- probabilities
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first large-scale benchmark for evaluating\
  \ uncertainty quantification (UQ) methods in scientific long-form question answering,\
  \ a domain where reliable uncertainty estimates are critical due to complex reasoning\
  \ demands and the potential for hallucinations. The study evaluates four representative\
  \ UQ approaches\u2014token-level confidences, verbalized uncertainty, P(True), and\
  \ answer frequency\u2014across 20 large language models (base, instruction-tuned,\
  \ and reasoning variants) on 685,000 responses from seven scientific QA datasets."
---

# Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering

## Quick Facts
- arXiv ID: 2602.00279
- Source URL: https://arxiv.org/abs/2602.00279
- Reference count: 40
- Primary result: First large-scale benchmark showing token-level probability polarization in instruction-tuned LLMs renders them unreliable as uncertainty signals for long-form scientific QA

## Executive Summary
This paper presents the first comprehensive benchmark for evaluating uncertainty quantification (UQ) methods in scientific long-form question answering using large language models. The study systematically evaluates four UQ approaches across 20 LLMs on 685,000 responses from seven scientific QA datasets. Key findings reveal that instruction-tuned models exhibit pronounced polarization in token-level probability distributions, making them unreliable for uncertainty estimation. The research demonstrates that answer frequency based on semantic consistency provides the most reliable calibration despite high computational cost, while highlighting that standard metrics like Expected Calibration Error can be misleading when confidence scores collapse to extreme values.

## Method Summary
The benchmark evaluates four UQ methods—token-level confidences, verbalized uncertainty, P(True), and answer frequency—across 20 LLMs including base, instruction-tuned, and reasoning variants. Experiments use 685,000 responses from seven scientific QA datasets (4 MCQA and 3 Arithmetic) with 250 items each. The study employs APriCoT counterfactual prompting for MCQA and Chain-of-Thought for Arithmetic QA. Token-level analysis uses vLLM with structured decoding to extract and normalize probabilities, while sequence-level methods generate 10 samples per prompt for frequency-based approaches. Calibration is evaluated using ECE, AUROC, and calibration plots, with semantic consistency computed through specialized clustering prompts and judging models for complex datasets.

## Key Results
- Instruction-tuned models show pronounced polarization in token-level probability distributions, rendering them unreliable as uncertainty signals
- Reasoning models exhibit similar polarization patterns, though the reasoning process can mitigate it depending on the provider
- Answer frequency based on semantic consistency demonstrates the most reliable calibration but requires 10x inference cost
- Expected Calibration Error is misleading as a performance metric when confidence scores collapse into narrow regions

## Why This Works (Mechanism)
The benchmark succeeds by creating controlled experimental conditions that isolate uncertainty quantification behavior across different model families and training paradigms. By using structured decoding and counterfactual prompting, the study captures systematic differences in how base, instruction-tuned, and reasoning models handle uncertainty. The large-scale evaluation across 685,000 responses provides statistical power to detect subtle calibration differences, while the semantic clustering approach for answer frequency enables meaningful comparison of diverse long-form responses beyond simple string matching.

## Foundational Learning
- **Expected Calibration Error (ECE)**: Measures the difference between predicted confidence and actual accuracy across confidence bins. Why needed: Provides a standardized metric for comparing calibration quality across models and methods. Quick check: Verify that confidence-binned accuracy matches average confidence within each bin.

- **Semantic Consistency Clustering**: Groups semantically equivalent answers across multiple samples using specialized prompts and judging models. Why needed: Enables evaluation of answer frequency methods on complex long-form responses where exact string matching fails. Quick check: Confirm that semantically equivalent responses receive similar consistency scores.

- **Structured Decoding**: Forces generation to valid answer tokens using regex patterns (e.g., "[ABCD]") for controlled probability extraction. Why needed: Ensures clean probability distributions for token-level uncertainty analysis without extraneous generation. Quick check: Verify that all generated tokens match the specified regex pattern.

- **Counterfactual Prompting (APriCoT)**: Presents multiple-choice questions with carefully constructed distractors to probe model reasoning. Why needed: Enables systematic evaluation of model uncertainty across diverse reasoning paths. Quick check: Confirm that distractors are semantically plausible and cover common misconception patterns.

- **Token-level Entailment**: Uses NLI models to assess semantic relationships between generated tokens and target answers. Why needed: Provides fine-grained uncertainty signals at the token level for sequence-level aggregation. Quick check: Verify that entailment scores align with human judgment of semantic similarity.

## Architecture Onboarding

Component Map: Datasets -> Prompts -> Models -> UQ Methods -> Metrics -> Analysis

Critical Path: Dataset → Prompt → Generation → UQ Extraction → Calibration Evaluation → Provider Analysis

Design Tradeoffs: The benchmark prioritizes calibration accuracy over computational efficiency, using 10 samples per question for frequency methods despite 10x cost, to ensure reliable semantic clustering. This design choice reveals that computational overhead is justified for applications requiring reliable uncertainty estimates, though it limits real-time applicability.

Failure Signatures: Verbalized uncertainty and P(True) methods consistently collapse to extreme scores (0 or 1), failing to discriminate between different uncertainty levels. Token-level polarization manifests as high probability mass concentrated on single tokens regardless of question difficulty. CCP suffers from vanishing sequence-level scores due to multiplicative aggregation across long generations.

First Experiments:
1. Reproduce token-level polarization by running structured decoding on Llama-3.3-70B-Instruct vs. base Llama-3.3-70B on MMLU, comparing probability distributions and ECE scores.
2. Implement verbalized uncertainty prompting on GPQA subset and analyze score distribution to verify collapse to extreme values.
3. Test semantic clustering thresholds on SciBench by varying the judging model's confidence cutoff and measuring calibration quality trade-offs.

## Open Questions the Paper Calls Out

Do specific reasoning model training pipelines (e.g., DeepSeek-R1 vs. Magistral) contain distinct fine-tuning stages that actively prevent the probability mass polarization observed in other instruction-tuned models? While the paper identifies provider-specific differences in polarization mitigation, it does not isolate the specific architectural or data-driven components responsible for the effect.

How can calibration metrics be mathematically reformulated to evaluate uncertainty quality independent of model accuracy when confidence scores collapse to extreme values? The paper critiques ECE as misleading in polarized distributions but does not propose alternative summary statistics that disentangle calibration from accuracy.

Can the aggregation strategy for Claim-Conditioned Probability (CCP) be adjusted to prevent the vanishing of sequence-level scores during long-form generation? The current benchmark identifies this failure mode but leaves the modification of the aggregation mechanism for future work.

## Limitations

The study's findings may not generalize beyond scientific domains to humanities or social sciences due to the specialized nature of the datasets. The benchmark requires exact prompt templates and judging criteria that are referenced but not fully disclosed, creating reproducibility challenges for independent researchers. The 10x computational overhead for frequency-based methods, while yielding better calibration, limits practical applicability in real-time or resource-constrained scenarios.

## Confidence

High Confidence: Token-level polarization in instruction-tuned models is empirically observed across multiple datasets and model families with clear visualizations.
Medium Confidence: Reasoning models show similar polarization but with provider-dependent mitigation effects that require further investigation.
Medium Confidence: Answer frequency provides most reliable calibration but depends on unspecified semantic clustering parameters.

## Next Checks

1. Reproduce token-level polarization by implementing structured decoding