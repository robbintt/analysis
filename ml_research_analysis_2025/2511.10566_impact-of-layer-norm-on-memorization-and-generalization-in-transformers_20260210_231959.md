---
ver: rpa2
title: Impact of Layer Norm on Memorization and Generalization in Transformers
arxiv_id: '2511.10566'
source_url: https://arxiv.org/abs/2511.10566
tags:
- memorization
- learning
- post-ln
- pre-ln
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Layer Normalization (LayerNorm) affects
  memorization and learning in transformer models. The authors find that in Pre-LayerNorm
  transformers, LayerNorm is essential for stable learning, and removing its parameters
  disrupts learning and exacerbates overfitting.
---

# Impact of Layer Norm on Memorization and Generalization in Transformers

## Quick Facts
- **arXiv ID:** 2511.10566
- **Source URL:** https://arxiv.org/abs/2511.10566
- **Reference count:** 40
- **Primary result:** LayerNorm plays opposite roles in Pre-LN (essential for learning) vs Post-LN (essential for memorization) transformers, with early-layer LN parameters being most critical

## Executive Summary
This paper investigates how Layer Normalization (LayerNorm) affects memorization and learning in transformer models. The authors find that in Pre-LayerNorm transformers, LayerNorm is essential for stable learning, and removing its parameters disrupts learning and exacerbates overfitting. In contrast, for Post-LayerNorm transformers, LayerNorm plays a key role in memorization, and its removal mitigates memorization by recovering true labels without harming learning. The study further identifies that early-layer LayerNorm parameters are the most critical in shaping these effects. Through gradient analysis, the authors explain why LayerNorm has divergent impacts in the two architectures, highlighting its distinct roles in memorization and learning. The findings are validated across 13 models and 6 datasets spanning vision and language tasks.

## Method Summary
The study fine-tunes 13 transformer models (Pre-LN and Post-LN variants) on datasets with 1% randomly flipped labels. Models are trained until 100% training accuracy is reached to force memorization. The key intervention involves removing LayerNorm learnable parameters (setting weights w=1 and biases b=0) while keeping normalization active. The authors evaluate learning via test accuracy, memorization via prediction accuracy on noisy labels, and recovery via prediction accuracy on true labels. Gradient norm analysis quantifies LayerNorm influence across layers, with systematic ablation experiments testing layer-wise importance.

## Key Results
- Pre-LN transformers: LN parameter removal destabilizes learning and exacerbates overfitting, while Post-LN transformers: LN removal suppresses memorization without harming generalization
- Early-layer LayerNorm parameters are most critical for both learning and memorization across architectures
- Learning gradient norms significantly exceed memorization gradient norms in Pre-LN models, while being comparable in Post-LN models
- Findings validated across 13 models (BERT, RoBERTa, DeBERTa, ELECTRA, Longformer, DistilBERT, GPT-2, GPT-Neo, Qwen2, ViT variants) and 6 datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: In Pre-LN transformers, LayerNorm parameters are essential for stable learning; their removal destabilizes training and exacerbates overfitting.
- **Mechanism**: Learning gradient norms (∥g^learn_x∥₂) significantly exceed memorization gradient norms (∥g^mem_x∥₂) in Pre-LN models. The ratio ∥g^learn_x∥₂/∥g^mem_x∥₂ ≫ 1 across all layers indicates LN primarily facilitates learning. When LN learnable parameters (w, b) are removed, the dominant learning signal is disrupted while memorization persists.
- **Core assumption**: The gradient norm ratio reflects the functional role of LN parameters in separating learning from memorization processes.
- **Evidence anchors**:
  - [abstract] "In Pre-LayerNorm transformers, LayerNorm is essential for stable learning, and removing its parameters disrupts learning and exacerbates overfitting."
  - [Section 4.1 & Figure 2a] Shows test accuracy degradation in Pre-LN models (Qwen2) when LN parameters removed, with learning not recovering through training.
  - [Section 6.1 & Eq. 5] Formalizes: ∥g^learn_x∥₂/∥g^mem_x∥₂|_Pre-LN ≫ ∥g^learn_x∥₂/∥g^mem_x∥₂|_Post-LN

### Mechanism 2
- **Claim**: In Post-LN transformers, LayerNorm parameters primarily govern memorization; their removal suppresses memorization and enables recovery of genuine labels without harming learning.
- **Mechanism**: Learning and memorization gradient norms have comparable magnitudes in Post-LN models. LN parameter removal affects both similarly, but the suppression of memorization is more pronounced because the memorization signal was the dominant contributor to overfitting. Later-layer LNs can compensate for early-layer LN removal due to gradual gradient decay.
- **Core assumption**: Comparable gradient magnitudes for learning and memorization allow selective suppression of the overfitting component without destabilizing generalization.
- **Evidence anchors**:
  - [abstract] "For Post-LayerNorm transformers, LayerNorm plays a key role in memorization, and its removal mitigates memorization by recovering true labels without harming learning."
  - [Figure 2e & 2f] Shows decreasing memorization scores and overfitting gap in Post-LN models (ELECTRA) after LN removal.
  - [Section 6.2] "In Post-LN models, later LNs can compensate for the absence of the early ones, recovering learning, while mitigating memorization, due to their comparable gradient norms."

### Mechanism 3
- **Claim**: Early-layer LayerNorm parameters are most critical for both learning and memorization across both architectures.
- **Mechanism**: The upper bound of gradient norms decreases monotonically from early to later layers: UB(∥g_x1∥₂) ≥ UB(∥g_x2∥₂) ≥ ... ≥ UB(∥g_xN∥₂). This theoretical bound, combined with empirical observations of high early-layer gradient norms, explains why early LN removal has the strongest impact.
- **Core assumption**: The gradient norm upper bound correlates with functional importance of LN parameters at each layer.
- **Evidence anchors**:
  - [abstract] "The study further identifies that early-layer LayerNorm parameters are the most critical in shaping these effects."
  - [Theorem 3 & Eq. 8] Formal proof of gradient norm decay across layers.
  - [Figure 4 & Appendix G.3] Empirical validation showing higher gradient norms in early layers for both Pre-LN (GPTNeo) and Post-LN (DeBERTa) models.

## Foundational Learning

- **Concept: Pre-LN vs Post-LN Architecture**
  - Why needed here: The divergent effects of LN depend entirely on whether normalization is applied before (Pre-LN) or after (Post-LN) the residual addition. Without this distinction, the findings appear contradictory.
  - Quick check question: In a transformer block, does LN come before MHSA/FFN (Pre-LN) or after the residual connection (Post-LN)?

- **Concept: Label Memorization vs Learning**
  - Why needed here: The paper separates memorization (overfitting to noisy labels without generalization) from learning (capturing patterns that generalize to test data). Key metrics (memorization score, recovery score, test accuracy) measure these separately.
  - Quick check question: When a model achieves 100% training accuracy on mislabeled data, is it learning or memorizing?

- **Concept: Gradient Norm as Sensitivity Measure**
  - Why needed here: The paper uses ∥∇_x L∥₂ to quantify how much each layer's LN influences loss for memorization vs learning. Higher norms indicate greater influence.
  - Quick check question: What does a high gradient norm at a layer indicate about that layer's contribution to the loss?

## Architecture Onboarding

- **Component map**:
  ```
  Pre-LN Block:  x → LN1 → MHSA → + → LN2 → FFN → + → y
                          ↑_________|          ↑_______|
  
  Post-LN Block: x → MHSA → + → LN1 → FFN → + → LN2 → y
                    ↑_______|          ↑_______|
  ```

- **Critical path**: Early layers (first N/3 layers) → LN parameters → gradient flow → learning/memorization balance

- **Design tradeoffs**:
  - Pre-LN: Stable gradient flow (standard in modern LLMs like GPT, Llama) but LN removal catastrophically harms learning
  - Post-LN: May have gradient instability issues but LN removal offers a memorization control mechanism without harming generalization

- **Failure signatures**:
  - Pre-LN with LN removed: Test accuracy drops and never recovers; train-test gap widens; high random prediction scores
  - Post-LN with LN removed: Should maintain test accuracy; memorization decreases; recovery score increases

- **First 3 experiments**:
  1. **Baseline establishment**: Train your transformer (identify if Pre/Post-LN) on dataset with 1% random label noise until 100% train accuracy. Record test accuracy, memorization score, recovery score.
  2. **LN parameter removal**: Set all LN weights w=1 and biases b=0 (keep normalization operation). Continue training and observe: Does test accuracy drop (Pre-LN) or stay stable (Post-LN)? Does memorization decrease (Post-LN) or persist (Pre-LN)?
  3. **Layer-wise analysis**: Remove LN parameters from only early layers (first 1/3), only middle layers, or only later layers. Compare which removal has strongest effect to validate early-layer importance claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do residual paths, attention heads, and feed-forward networks interact with LayerNorm to shape memorization, specifically in cases where LayerNorm removal is insufficient to suppress it?
- Basis in paper: [explicit] Appendix K states that "other components—like residual paths, attention, and feed-forward layers—also influence memorization and merit further investigation," noting the study focused only on LN.
- Why unresolved: The paper isolates LayerNorm to establish its role but does not perform ablation studies on other architectural components to determine if they drive memorization independently or synergistically with LN.
- What evidence would resolve it: Ablation studies combining LN removal with modifications to residual connections or attention heads to observe changes in the Memorization Score.

### Open Question 2
- Question: Why does the removal of LayerNorm parameters fail to mitigate memorization in specific Post-LN models like DistilBERT, and which architectural features drive this anomaly?
- Basis in paper: [explicit] Appendix G.1.2 notes that DistilBERT (a Post-LN model) did not show memorization suppression upon LN removal, suggesting "other components... might have a more profound impact... making it an interesting future work."
- Why unresolved: The study identifies DistilBERT as an outlier to the general Post-LN trend but does not analyze the internal mechanism or distillation history that might cause this resistance to memorization suppression.
- What evidence would resolve it: A comparative gradient analysis or layer-wise probing of DistilBERT against standard Post-LN models (e.g., BERT) to identify distinct memorization pathways.

### Open Question 3
- Question: Can memorization be suppressed in Pre-LN transformers without destabilizing learning, given that removing LayerNorm parameters exacerbates overfitting in this architecture?
- Basis in paper: [inferred] The paper concludes that LN removal is effective for Post-LN models but "destabilizes learning" and "exacerbates overfitting" in Pre-LN models, leaving the problem of efficient memorization mitigation for Pre-LN architectures unresolved.
- Why unresolved: The proposed intervention (LN parameter removal) harms the generalization capabilities of Pre-LN models, implying a need for a different approach to separate memorization from learning in these dominant architectures.
- What evidence would resolve it: A method (e.g., selective regularization or targeted parameter freezing) that successfully lowers the Memorization Score in Pre-LN models without reducing the Learning (Test) Accuracy.

## Limitations
- The findings rely on controlled experimental conditions (fixed 1% label noise, training until 100% accuracy) that may not generalize to real-world scenarios with varying noise levels
- The gradient norm analysis assumes the ratio ∥g^learn∥₂/∥g^mem∥₂ directly reflects functional importance, but this relationship hasn't been validated across different loss landscapes
- The early-layer criticality claim depends on the theoretical gradient norm bound, which may not hold for all architectures or initialization schemes

## Confidence

- **High Confidence:** The empirical observation that LN removal has opposite effects in Pre-LN vs Post-LN models (validated across 13 models and 6 datasets)
- **Medium Confidence:** The gradient norm analysis explaining why these divergent effects occur, as this relies on specific theoretical assumptions about gradient flow
- **Medium Confidence:** The early-layer criticality claim, supported by gradient norm bounds and empirical data, but the monotonic decay pattern needs validation across more architectures

## Next Checks

1. Test whether the Pre-LN vs Post-LN divergence persists with different noise injection levels (0.1%, 5%, 10%) to assess robustness beyond the fixed 1% condition
2. Verify the gradient norm ratio relationship by computing actual gradient contributions to loss rather than just norms, to confirm the functional interpretation
3. Evaluate whether the early-layer criticality holds when using different initialization schemes or normalization variants (RMSNorm, InstanceNorm) to test the generality of the gradient norm bound