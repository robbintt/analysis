---
ver: rpa2
title: 'SkillGen: Learning Domain Skills for In-Context Sequential Decision Making'
arxiv_id: '2511.14670'
source_url: https://arxiv.org/abs/2511.14670
tags:
- action
- rate
- task
- progress
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SkillGen, a skill-based in-context learning
  (ICL) framework for sequential decision-making. SkillGen addresses the challenge
  of designing ICL prompts that are focused, granular, and label-efficient by extracting
  and applying domain-level skills.
---

# SkillGen: Learning Domain Skills for In-Context Sequential Decision Making

## Quick Facts
- **arXiv ID**: 2511.14670
- **Source URL**: https://arxiv.org/abs/2511.14670
- **Reference count**: 40
- **Key outcome**: Introduces SkillGen, a skill-based in-context learning framework that improves sequential decision-making performance by 5.9%–16.5% through domain-level skill extraction and fine-grained prompt generation.

## Executive Summary
SkillGen addresses the challenge of designing effective in-context learning prompts for sequential decision-making by introducing a skill-based framework. The approach constructs an action-centric domain graph from sampled trajectories, identifies high-utility actions using temporal-difference credit assignment, and retrieves step-wise skills to generate focused, granular prompts. Evaluated across three environments (ALFWorld, BabyAI, ScienceWorld) with multiple LLMs, SkillGen achieves consistent performance improvements through enhanced prompt specificity and domain-relevant skill incorporation.

## Method Summary
SkillGen builds an action-centric domain graph from sampled trajectories to capture domain-level skills. It employs temporal-difference credit assignment to identify high-utility action segments, then retrieves step-wise skills to construct fine-grained, context-aware in-context learning prompts. The framework is evaluated on ALFWorld, BabyAI, and ScienceWorld using both open-source and proprietary LLMs, demonstrating improved task performance through more focused and label-efficient prompt design compared to standard ICL approaches.

## Key Results
- Average progress rate improvement of 5.9%–16.5% across tested models and environments
- Consistent performance gains across ALFWorld, BabyAI, and ScienceWorld environments
- Ablation studies confirm contributions of both golden segment identification and step-wise skill retrieval
- Theoretical analysis supports that high-utility segments enhance task identifiability

## Why This Works (Mechanism)
SkillGen improves in-context learning by transforming generic prompts into domain-specific, skill-focused instructions. The temporal-difference credit assignment identifies which action sequences are most critical for task success, allowing the system to prioritize these high-utility segments in prompt construction. By breaking down complex tasks into step-wise skills and retrieving them contextually, the framework creates prompts that are both more granular and more aligned with the specific requirements of the target environment.

## Foundational Learning
- **Temporal-Difference Credit Assignment**: Assigns credit to actions based on their contribution to reward prediction errors; needed to identify high-utility segments in trajectories; quick check: verify TD error correlates with actual task success
- **Domain Graph Construction**: Builds action-centric graphs from sampled trajectories; needed to capture relationships between actions and skills; quick check: ensure graph captures all necessary action dependencies
- **Skill Extraction from Trajectories**: Identifies reusable action patterns from successful trajectories; needed to create transferable knowledge units; quick check: validate extracted skills improve task completion rates
- **In-Context Learning Prompt Design**: Crafts prompts that guide LLM decision-making without fine-tuning; needed to leverage LLM capabilities efficiently; quick check: measure prompt specificity vs. performance correlation

## Architecture Onboarding

**Component Map**: Trajectory Sampling -> Domain Graph Construction -> Temporal-Difference Credit Assignment -> High-Utility Segment Identification -> Step-wise Skill Retrieval -> In-Context Prompt Generation -> LLM Decision Making

**Critical Path**: The temporal-difference credit assignment stage is critical, as it determines which segments become "golden" and drive the subsequent skill retrieval and prompt generation. Errors here propagate through the entire pipeline.

**Design Tradeoffs**: Skill granularity vs. prompt complexity (finer skills provide more specific guidance but increase prompt length); domain graph coverage vs. computational cost (more comprehensive graphs improve skill quality but require more resources); credit assignment accuracy vs. real-time applicability (more sophisticated methods improve segment identification but may be computationally expensive).

**Failure Signatures**: Poor task performance due to generic prompts suggests issues in skill extraction or credit assignment; suboptimal skill retrieval indicates problems with domain graph construction; computational bottlenecks suggest the graph construction or skill identification stages need optimization.

**First Experiments**:
1. Compare performance using standard ICL prompts versus SkillGen-generated prompts on a simple environment
2. Ablate the temporal-difference credit assignment component to test its impact on skill selection quality
3. Vary the granularity of extracted skills to find the optimal balance between specificity and prompt length

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may not generalize beyond tested environments and LLM variants
- Computational overhead from domain graph construction and skill extraction may scale poorly with task complexity
- Effectiveness of temporal-difference credit assignment in continuous state spaces remains to be validated
- Theoretical claims about task identifiability require more rigorous mathematical formalization

## Confidence

**High**: Empirical performance improvements across tested environments and models; effectiveness of skill-based prompt generation compared to standard ICL.

**Medium**: Theoretical analysis supporting task identifiability; generalizability of the framework to unseen domains.

**Low**: Scalability of the domain graph construction; robustness of temporal-difference credit assignment in noisy environments.

## Next Checks
1. Conduct experiments on additional RL environments with varying state and action space complexities to test generalizability.
2. Measure and report the computational overhead introduced by the domain graph construction and skill extraction pipeline.
3. Perform ablation studies isolating the contribution of temporal-difference credit assignment versus alternative credit assignment methods.