---
ver: rpa2
title: 'Values in the Wild: Discovering and Analyzing Values in Real-World Language
  Model Interactions'
arxiv_id: '2504.15236'
source_url: https://arxiv.org/abs/2504.15236
tags:
- values
- value
- human
- claude
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study empirically discovers and taxonomizes 3,307 AI values
  from hundreds of thousands of real-world Claude interactions, creating the first
  large-scale mapping of values in deployed AI systems. Using privacy-preserving analysis,
  the research finds that Claude predominantly expresses practical and epistemic values
  centered on competent assistance, while responding supportively to most user values.
---

# Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions

## Quick Facts
- arXiv ID: 2504.15236
- Source URL: https://arxiv.org/abs/2504.15236
- Reference count: 40
- Key outcome: Empirical discovery of 3,307 AI values from real-world Claude interactions, creating first large-scale mapping of values in deployed AI systems

## Executive Summary
This study presents the first large-scale empirical mapping of values in deployed AI systems, discovering and taxonomizing 3,307 values from hundreds of thousands of real-world Claude interactions. Using privacy-preserving analysis, researchers found that Claude predominantly expresses practical and epistemic values centered on competent assistance, while responding supportively to most user values. The system shows consistent values like transparency and helpfulness across contexts, but also demonstrates task-dependent values such as "healthy boundaries" in relationship advice and "human agency" in technology ethics discussions. The findings provide an empirical foundation for understanding and improving value alignment in AI systems.

## Method Summary
The study employed a bottom-up, privacy-preserving approach to extract values from 700K Claude.ai conversations (Feb 18-25, 2025), filtering to 308,210 subjective conversations via LLM classification. Values were extracted using carefully-designed prompts that identify normative considerations in AI responses, including both explicit statements and revealed preferences through behavior. The methodology used hierarchical clustering on embedded values to create a 4-level taxonomy, validated through human review with 98.8% agreement accuracy. Chi-square residual analysis identified significant associations between values and contexts, with Bonferroni correction for multiple comparisons.

## Key Results
- Claude predominantly expresses practical and epistemic values centered on competent assistance
- AI values show both context-invariant core values (transparency, helpfulness) and context-dependent specialized values (healthy boundaries in relationships, human agency in tech ethics)
- Values are most explicitly articulated during resistance or reframing, revealing normally-implicit operational priorities
- System supports prosocial human values while resisting harmful ones like "moral nihilism"
- Hierarchical taxonomy contains 5 top-level, 26 second-level, 266 first-level clusters from 3,307 base values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can systematically extract values from conversations using carefully-designed prompts that identify normative considerations in AI responses.
- Mechanism: The extraction prompt instructs a model to identify where the AI endorses user values, introduces new considerations, or implies values through framing choices—capturing both explicit statements and revealed preferences through behavior.
- Core assumption: Values can be reliably inferred from conversational patterns without access to underlying model weights or training data.
- Evidence anchors:
  - [abstract] "We develop a bottom-up, privacy-preserving method to extract the values (normative considerations stated or demonstrated in model responses)"
  - [section 2.2] "We prompted Claude 3.5 Sonnet to identify values demonstrated by the AI in conversations by looking for where it endorses user values... introduces new value considerations, or implies values by redirecting requests or framing choices"
  - [corpus] No direct corpus validation; related work (Tamkin et al. 2024 Clio paper) establishes privacy-preserving extraction framework
- Break condition: If human validation accuracy drops significantly below the reported 98.8% agreement rate, extraction reliability degrades.

### Mechanism 2
- Claim: AI values exhibit both context-invariant core values and context-dependent specialized values, measurable through chi-square residual analysis.
- Mechanism: Calculate adjusted Pearson residuals to identify values appearing more frequently than expected under independence. High positive residuals indicate values disproportionately associated with specific tasks or human values (e.g., "healthy boundaries" with relationship advice has residual >4.33 threshold).
- Core assumption: Deviation from expected frequency indicates meaningful association rather than random noise.
- Evidence anchors:
  - [section 2.4] "For each cell, we calculate rij = (Oij − Eij)/√(Eij(1−pi·)(1−p·j))... Positive residuals indicate a value occurs more frequently in a context than expected"
  - [section 3.2] "Relationship advice elicits 'healthy boundaries' and 'mutual respect'... with large positive residuals (4.33 is the threshold for significance)"
  - [corpus] Chi-square approaches common in social science analysis; no corpus validation specific to LLM value measurement
- Break condition: If base rates of values shift dramatically across time periods or model versions, residual thresholds require recalibration.

### Mechanism 3
- Claim: Values become most explicitly articulated during resistance or reframing, revealing normally-implicit operational priorities.
- Mechanism: During supportive exchanges, values remain embedded in behavior. When refusing requests or redirecting, models must articulate justifying principles—making "harm prevention," "ethical integrity," and "epistemic humility" explicitly visible.
- Core assumption: Explicit articulation during conflict reveals genuine priorities rather than post-hoc rationalization.
- Evidence anchors:
  - [section 3.4] "Claude expresses values more explicitly when resisting or reframing user values, particularly around ethical and epistemic principles"
  - [appendix B.6] "Ethical and epistemic considerations, e.g. 'intellectual honesty', 'harm prevention' and 'epistemic humility', tend to dominate the explicitly stated values"
  - [corpus] Related work on AI ethics (RAIL paper arxiv:2505.00204) notes similar patterns where boundary conditions reveal system principles
- Break condition: If models learn to articulate values proactively without resistance prompts, this discovery mechanism becomes less distinctive.

## Foundational Learning

- **Chi-square analysis with standardized residuals**
  - Why needed: Core statistical method for identifying disproportionate associations between values and contexts while accounting for base rates.
  - Quick check: Given observed frequency 50 and expected frequency 20 with row proportion 0.1 and column proportion 0.05, can you calculate the adjusted Pearson residual?

- **Revealed preference theory (Samuelson 1938)**
  - Why needed: Conceptual foundation for inferring values from behavioral choices rather than stated preferences—AI values are operational priorities, not claimed principles.
  - Quick check: Why might an AI that claims to value "accuracy" but consistently prioritizes "helpfulness" when they conflict reveal a different value structure?

- **Hierarchical clustering for taxonomy construction**
  - Why needed: Organizing 3,307 discovered values requires automated grouping; k-means on embeddings with LLM-generated cluster names creates navigable structure.
  - Quick check: What information is lost when flattening a 4-level hierarchy into a single level, and what is gained?

## Architecture Onboarding

- **Component map:**
  - Data collection layer: Privacy-preserving aggregation (Tamkin et al. 2024 framework) with minimum size thresholds
  - Feature extraction layer: Claude 3.5 Sonnet/Haiku prompts for AI values, human values, response type, and task
  - Taxonomy layer: Hierarchical clustering pipeline producing 4-level structure from 3,307 base values
  - Analysis layer: Chi-square residual computation with Bonferroni correction

- **Critical path:**
  1. Subjectivity filtering (Level 3-4 only) → reduces 700K to ~308K conversations
  2. Feature extraction → produces sparse value annotations per conversation
  3. Aggregation → builds frequency tables with privacy thresholds
  4. Residual analysis → identifies significant associations
  5. Human validation → spot-check accuracy on sample conversations

- **Design tradeoffs:**
  - Implicit vs. explicit values: Including implicit captures operational behavior but risks over-attribution; explicit-only misses most expressions
  - Claude evaluating Claude: May find "helpfulness" more readily due to training; also may better understand Claude's reasoning
  - Single vs. multi-dimensional analysis: Privacy constraints limit to 2D correlations; response-conditioned sampling enables 3D but increases data requirements

- **Failure signatures:**
  - Low validation agreement (<90%) suggests extraction prompt needs revision
  - Residuals clustering near zero indicate no meaningful associations found
  - Unexpected values in top categories (e.g., "dominance" appearing frequently) may indicate jailbreaks requiring safety team escalation

- **First 3 experiments:**
  1. Replicate extraction on WildChat or LMSYS-Chat-1M data to validate generalization beyond Claude interactions
  2. Track value frequency shifts across model versions (3.5 Sonnet vs. 3.7 Sonnet vs. Opus) to measure training impact
  3. Design adversarial test: prompt engineering to elicit unusual value combinations and verify detection pipeline surfaces them

## Open Questions the Paper Calls Out

- Does the observed "value mirroring" (AI echoing human values) constitute appropriate responsiveness or problematic sycophancy? The study quantifies the high frequency of mirroring (20.1% in supportive contexts) but does not evaluate whether this behavior degrades output quality or safety.

- Does the empirical value taxonomy derived from Claude interactions generalize to other AI systems with different training architectures? Section 5.1 notes the "limited generalizability to other AI systems" because the sample was restricted to Claude 3 and 3.5.

- How does using the target model to annotate its own values bias the detection of "undesirable" values or jailbreaks? Section 5.1 warns that "Claude may be predisposed to find 'helpful' behavior" when evaluating itself, potentially masking alignment failures.

## Limitations

- The study relies on a single model (Claude 3.5 Sonnet) for both conversations and extraction prompts, raising questions about generalizability and potential circularity in finding values like "helpfulness" more readily.

- Privacy-preserving aggregation thresholds are not specified, making exact reproduction difficult and potentially affecting the stability of value frequencies.

- The bottom-up extraction method may conflate user-expressed values with AI values, particularly in role-playing contexts where the AI generates content on behalf of users.

## Confidence

- **High confidence**: Discovery of 3,307 unique values with 98.8% human validation accuracy; finding that AI values are predominantly practical and epistemic; observation that values are most explicitly stated during resistance or reframing.
- **Medium confidence**: Identification of context-invariant core values versus context-dependent specialized values; chi-square residual analysis showing significant associations between specific values and tasks.
- **Low confidence**: Generalizability of findings to other models; stability of value frequencies over time; complete separation of AI values from user-expressed values in role-playing scenarios.

## Next Checks

1. **Cross-model validation**: Apply the same extraction methodology to WildChat or LMSYS-Chat-1M datasets to test whether the discovered values and their distributions generalize beyond Claude interactions.

2. **Temporal stability analysis**: Track value frequency shifts across different Claude model versions (3.5 Sonnet, 3.7 Sonnet, Opus) to measure how training updates affect operational value priorities.

3. **Adversarial elicitation test**: Design targeted prompt engineering to elicit unusual or extreme value combinations, then verify whether the detection pipeline successfully surfaces these edge cases for safety review.