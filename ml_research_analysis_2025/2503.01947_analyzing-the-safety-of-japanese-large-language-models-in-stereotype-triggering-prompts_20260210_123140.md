---
ver: rpa2
title: Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering
  Prompts
arxiv_id: '2503.01947'
source_url: https://arxiv.org/abs/2503.01947
tags:
- japanese
- llm-jp
- qwen
- responses
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study directly evaluates stereotypes in Japanese LLMs using
  3,612 stereotype-triggering prompts across 301 social groups and 12 templates. Responses
  from three foundational models (Japanese, English, and Chinese) were analyzed for
  refusal rates, toxicity, and sentiment.
---

# Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts

## Quick Facts
- arXiv ID: 2503.01947
- Source URL: https://arxiv.org/abs/2503.01947
- Reference count: 40
- Three Japanese, English, and Chinese LLMs show significantly different safety behaviors when prompted with stereotype-triggering content

## Executive Summary
This study directly evaluates stereotypes in Japanese LLMs using 3,612 stereotype-triggering prompts across 301 social groups and 12 templates. Responses from three foundational models (Japanese, English, and Chinese) were analyzed for refusal rates, toxicity, and sentiment. Results show the Japanese model (LLM-jp) had the lowest refusal rate (0.3% vs. 12.2% and 29.3%) and generated more toxic and negative responses than other models. Prompt format significantly influenced outputs, with question and negative-opinion templates producing higher toxicity. Certain social groups received disproportionately harmful responses. These findings reveal insufficient ethical safety mechanisms in Japanese LLMs and emphasize the need for bias mitigation strategies beyond linguistic boundaries.

## Method Summary
The study used 3,612 prompts generated from 301 social groups across 9 categories, combined with 12 Japanese templates (Statement, Question, Positive-opinion, Negative-opinion formats). Three models were evaluated: Gemma-2-27B, Qwen2.5-14B, and LLM-jp-3-13B, generating 10 response options per prompt. Responses were classified as invalid/refusal/valid using rule-based detection and language validation. Toxicity was measured using Google's Perspective API, and sentiment was analyzed using a BERT-based Japanese sentiment classifier. The evaluation focused on refusal rates, maximum toxicity per prompt, and sentiment scores across different social groups and template formats.

## Key Results
- Japanese model (LLM-jp) had the lowest refusal rate (0.3% vs. 12.2% for Gemma and 29.3% for Qwen)
- LLM-jp generated more toxic and negative responses than other models across all template types
- Question and negative-opinion templates produced higher toxicity than statement and positive-opinion templates
- Cross-model toxicity correlations were significant, with Gemma and Qwen showing stronger correlation (r=0.513) than either with LLM-jp

## Why This Works (Mechanism)

### Mechanism 1: Template Format Modulates Safety Bypass
- **Claim:** Prompt format directly influences whether model safety mechanisms activate.
- **Mechanism:** Question and Negative-opinion templates linguistically frame the task as information-seeking rather than harm-generation, reducing the likelihood that refusal triggers fire. The paper shows Negative-opinion templates produced higher toxicity while Positive-opinion templates showed lower toxicity (approximately 0.05 for LLM-jp).
- **Core assumption:** Safety classifiers may weight intent signals from prompt structure more heavily than content analysis.
- **Evidence anchors:**
  - [abstract] "Question and negative-opinion templates producing higher toxicity."
  - [section IV-B] "While Question and Negative-opinion templates consistently exhibit higher toxicity across models, Statement and Positive-opinion templates show relatively lower scores."
  - [corpus] Weak direct evidence; related work on robustness (FLEX benchmark) suggests format sensitivity is a known vulnerability but not mechanism-specific.
- **Break condition:** If models implement content-level toxicity filtering independent of syntactic framing, template effects would diminish.

### Mechanism 2: Refusal Rate Reflects Safety Alignment Investment
- **Claim:** Lower refusal rates indicate weaker safety fine-tuning rather than linguistic capability gaps.
- **Mechanism:** Qwen (29.3% refusal) and Gemma (12.2%) show progressively stricter refusal behavior, correlating with investment in alignment. LLM-jp's 0.3% refusal rate suggests minimal safety fine-tuning. The paper notes Qwen applies "the strictest safety mechanism" while LLM-jp shows "minimal content moderation."
- **Core assumption:** Refusal behavior is primarily a product of alignment training rather than architectural differences.
- **Evidence anchors:**
  - [abstract] "LLM-jp had the lowest refusal rate (0.3% vs. 12.2% and 29.3%)."
  - [section IV-A] "Qwen exhibits the highest refusal rate (29.3%), followed by Gemma (12.2%), while LLM-jp has an extremely low refusal rate (0.3%)."
  - [corpus] JMedEthicBench and AnswerCarefully datasets indicate Japanese LLM safety is an active area requiring improvement, supporting underinvestment hypothesis.
- **Break condition:** If refusal rates were purely capacity-driven, performance benchmarks (not alignment) would predict refusal behavior.

### Mechanism 3: Cross-Model Toxicity Correlation Indicates Shared Training Data Biases
- **Claim:** Toxicity patterns correlate across models trained on different primary languages when exposed to similar social group prompts.
- **Mechanism:** Gemma and Qwen show higher toxicity correlation (r=0.513) than either does with LLM-jp (r≈0.39), suggesting shared training corpus biases despite language differences. Williams tests confirm Gemma-Qwen correlation is significantly stronger.
- **Core assumption:** Training data for major LLMs contains overlapping web-scale corpora with similar stereotype distributions.
- **Evidence anchors:**
  - [section IV-D] "The correlation coefficients are as follows: r = 0.513 for Gemma vs. Qwen; r = 0.387 for Gemma vs. LLM-jp."
  - [section IV-D] "Williams tests reveal... the correlation between Gemma and Qwen is significantly stronger."
  - [corpus] Limited; corpus papers focus on evaluation methods rather than training data overlap.
- **Break condition:** If correlations were purely linguistic (same-language transfer), same-language pairs would show strongest correlations regardless of model origin.

## Foundational Learning

- **Concept: Direct vs. Indirect Bias Evaluation**
  - **Why needed here:** The paper uses direct evaluation (open-ended responses) rather than indirect (sentence pair selection). Understanding this distinction is essential for interpreting why refusal rates matter.
  - **Quick check question:** Can you explain why indirect evaluation (e.g., BBQ, StereoSet) may miss harms that direct evaluation captures?

- **Concept: Refusal Rate as Safety Proxy**
  - **Why needed here:** The study uses refusal rate as a primary safety metric alongside toxicity. Engineers must understand what refusal measures—and what it doesn't (e.g., a model may refuse harmful prompts but still produce biased content when it does respond).
  - **Quick check question:** A model with 100% refusal rate would score perfectly on this safety metric—what harms might this obscure?

- **Concept: Toxicity Scoring via Perspective API**
  - **Why needed here:** The paper relies on Perspective API for toxicity scoring. Understanding API limitations (language coverage, annotation biases) is critical for reproducibility.
  - **Quick check question:** How might Perspective API's training on primarily English data affect toxicity scores for Japanese text?

## Architecture Onboarding

- **Component map:** Prompt Generator (301 groups × 12 templates) -> Model Interface (Gemma-2-27B, Qwen2.5-14B, LLM-jp-3-13B) -> Response Classifier (Invalid/Refusal/Valid) -> Evaluation Pipeline (Perspective API, BERT sentiment classifier)

- **Critical path:** Prompt construction → Model inference (10 options per prompt) → Response classification → Toxicity/sentiment scoring → Correlation analysis

- **Design tradeoffs:**
  - Maximum toxicity per prompt (Equation 2) vs. average—paper uses max, amplifying worst-case harms
  - Rule-based refusal detection vs. classifier-based—paper uses patterns; may miss implicit refusals
  - Single model per language vs. multiple—limits generalizability (acknowledged by authors)

- **Failure signatures:**
  - 84 invalid responses (0.8%), including 12 non-Japanese outputs from Japanese models—indicates tokenization or instruction-following failures
  - LLM-jp required temperature ≥1.0 for valid outputs; lower temperatures caused incomplete responses

- **First 3 experiments:**
  1. Replicate refusal rate analysis on a subset (100 prompts) to validate rule-based refusal detection against human annotation
  2. Test whether translating the same prompts to English causes Gemma's refusal behavior to match Qwen's (disentangling language vs. model effects)
  3. Add a fourth model (e.g., Claude or GPT-4) to establish whether toxicity correlations extend beyond the three models tested

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are the observed discrepancies in refusal rates and toxicity attributable to the linguistic nature of Japanese versus English/Chinese, or to specific model characteristics such as parameter size and general performance benchmarks?
- **Basis in paper:** [inferred] The authors note in the Limitations section that because they selected only one model per language with different performance levels, "it remains unclear whether the observed differences arise from language variations or model-specific characteristics."
- **Why unresolved:** The study design isolated language by model but could not control for architectural differences or base performance capabilities across the three distinct models (LLM-jp, Gemma, Qwen).
- **What evidence would resolve it:** A controlled comparison of multiple LLMs within the same language family and a cross-lingual analysis of models with identical architectures but different pre-training corpora.

### Open Question 2
- **Question:** What specific debiasing algorithms or safety fine-tuning methods are effective for mitigating stereotypes in Japanese LLMs without degrading their linguistic capabilities?
- **Basis in paper:** [explicit] The Conclusion states, "it is crucial to investigate why these stereotypes emerge in LLMs and how they can be mitigated," noting that comprehensive analyses are currently lacking.
- **Why unresolved:** This study focused exclusively on evaluating and diagnosing the presence of bias rather than implementing or testing mitigation strategies.
- **What evidence would resolve it:** Pre- and post-intervention evaluations of Japanese LLMs subjected to specific debiasing techniques, such as cross-lingual continual pre-training or safety-specific instruction tuning.

### Open Question 3
- **Question:** Do larger parameter Japanese models (e.g., 172B parameters) exhibit different refusal or toxicity behaviors compared to the mid-sized (10B–30B) models evaluated in this study?
- **Basis in paper:** [explicit] The authors note in the Limitations that "larger models may behave differently," specifically citing that Japanese-based LLMs are still developing and utilizing larger sizes.
- **Why unresolved:** Practical constraints limited the evaluation to models with 10B–30B parameters, leaving the safety profile of state-of-the-art large-scale Japanese models untested.
- **What evidence would resolve it:** Applying the same 3,612 stereotype-triggering prompts to the latest large-scale Japanese models (e.g., LLM-jp-3-172b) and comparing the resulting toxicity and refusal rates.

## Limitations
- Data Scope Constraints: The study evaluates only three models (one Japanese, one English, one Chinese), limiting generalizability to other Japanese LLMs or cross-lingual safety patterns.
- Evaluation Metric Dependencies: Toxicity scores rely on Google's Perspective API, which has documented limitations in non-English contexts and may not capture culturally-specific forms of harm.
- Methodological Constraints: The study uses direct evaluation rather than indirect methods, which may overestimate harmful response rates.

## Confidence
- **High Confidence:** Template format effects on toxicity (well-supported by consistent pattern across all three models)
- **Medium Confidence:** Cross-model toxicity correlations (statistically significant but based on limited model sample size)
- **Low Confidence:** Refusal rate as safety proxy (may conflate safety alignment with linguistic capability or instruction-following ability)

## Next Checks
1. **Cross-linguistic validation:** Translate Japanese prompts to English and test whether Gemma's refusal behavior aligns more closely with Qwen, disentangling language effects from model-specific safety training
2. **Human annotation benchmark:** Have native Japanese speakers manually classify 200 randomly selected responses to validate automated toxicity scoring and refusal detection accuracy
3. **Template robustness test:** Generate prompts using only the question template across all social groups and measure toxicity distribution to isolate template effects from other prompt variations