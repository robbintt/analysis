---
ver: rpa2
title: Solving Word-Sense Disambiguation and Word-Sense Induction with Dictionary
  Examples
arxiv_id: '2503.04328'
source_url: https://arxiv.org/abs/2503.04328
tags:
- sentences
- sense
- task
- dictionary
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to leverage LLMs for generating
  training data for word-sense disambiguation (WSD) and word-sense induction (WSI)
  tasks in less-resourced languages, using Slovene as a case study. The method involves
  expanding short dictionary usage examples into complete sentences using GPT-3.5,
  then using these sentences to train models for the word-in-context (WiC) task.
---

# Solving Word-Sense Disambiguation and Word-Sense Induction with Dictionary Examples

## Quick Facts
- **arXiv ID:** 2503.04328
- **Source URL:** https://arxiv.org/abs/2503.04328
- **Reference count:** 0
- **Primary result:** LLM-generated sentences from dictionary snippets enable WiC-based WSD/WSI models for less-resourced languages, achieving 93% WSD accuracy and 75% WSI accuracy in non-OOV scenarios.

## Executive Summary
This paper introduces a methodology for leveraging large language models to generate training data for word-sense disambiguation (WSD) and word-sense induction (WSI) in less-resourced languages, using Slovene as a case study. The approach addresses the scarcity of annotated datasets by expanding short dictionary usage examples into complete sentences using GPT-3.5, then training WiC (Word-in-Context) models on these expanded sentences. The WiC classifiers are subsequently adapted to solve WSD and WSI tasks without requiring pre-constructed sense inventories. Results show that models trained on LLM-generated sentences significantly outperform those trained on unexpanded dictionary snippets, with classification accuracies of 93% for WSD and 75% for WSI in non-out-of-vocabulary scenarios.

## Method Summary
The methodology involves three main steps: First, dictionary usage examples (snippets) are expanded into complete sentences using GPT-3.5 with the prompt "Razširi [snippet] v polno poved." Second, WiC dataset pairs are constructed by labeling sentence pairs as same-sense (1) or different-sense (0), limiting to top 6 examples per sense and 100 pairs per lemma. Third, SloBERTa is fine-tuned for 20 epochs on the WiC task, then adapted for WSD by selecting the sense with highest similarity score and for WSI by inducing new senses when confidence scores fall below a calibrated threshold (1.2× average probability). The approach is evaluated across Pure-OOV, Partial-OOV, and Non-OOV scenarios using the Elexis-WSD dataset.

## Key Results
- Models trained on LLM-generated sentences achieved 93% accuracy for WSD compared to baseline methods
- WSI accuracy reached 75% in non-OOV scenarios using threshold-based induction
- WiC models trained on expanded sentences significantly outperformed those trained on unexpanded dictionary snippets
- The methodology proved effective for out-of-vocabulary words in Partial-OOV and Non-OOV evaluation scenarios

## Why This Works (Mechanism)

### Mechanism 1: Contextual Density Enhancement via Synthetic Expansion
Short dictionary snippets are often too brief to provide sufficient context for transformer-based disambiguation models. GPT-3.5 expands these snippets into complete sentences, generating the surrounding semantic context required for effective contextual embedding while preserving the original word sense with >98% fidelity.

### Mechanism 2: Inventory-Free Disambiguation via Binary Comparison
The WiC binary classification task serves as a functional proxy for WSD without requiring a rigid sense inventory. Instead of mapping words to specific IDs in a fixed database, the model asks "Is the sense of Word A in Sentence 1 the same as Word A in Sentence 2?" This simplifies the task to semantic equivalence detection.

### Mechanism 3: Threshold-Based Open-Set Recognition for Induction
WSI is achieved by applying a calibrated probability threshold to WiC classifier outputs. When the confidence score for "same sense" falls below 1.2× the average prediction probability for all known examples, the system flags the input as belonging to a "New Sense," enabling open-set recognition.

## Foundational Learning

### Concept: Word-in-Context (WiC) vs. WSD
**Why needed:** The paper hinges on replacing multi-class WSD with binary WiC comparison. **Quick check:** "Given two sentences with 'bank', does the model output a specific sense ID (WSD) or a boolean 'same/different' flag (WiC)?"

### Concept: Sense Inventory
**Why needed:** These are the bottleneck for low-resource languages. **Quick check:** "Why can't we just use a standard lookup table for word senses in Slovene?"

### Concept: Out-of-Vocabulary (OOV) Evaluation
**Why needed:** Evaluation splits (Pure-OOV vs. Non-OOV) prove the model's utility for dictionary-scale deployment. **Quick check:** "If trained on 'apple' and 'orange', can the model disambiguate 'banana' without retraining? (Pure-OOV scenario)"

## Architecture Onboarding

### Component map:
Dictionary Snippets + Definitions -> GPT-3.5 (Expansion) -> SloBERTa (Embeddings) -> Binary Classifier (WiC) -> K-NN Matching (WSD) & Thresholding (WSI)

### Critical path:
The prompt engineering for GPT-3.5 is critical. If the prompt fails to maintain the target sense during expansion, the resulting dataset creates a negative feedback loop where the WiC model learns to conflate senses.

### Design tradeoffs:
The tradeoff is between data precision (human-curated but scarce) vs. data volume (LLM-generated and noisy). The paper argues that volume and sentence completeness outweigh the small error rate (1.5%) in sense fidelity.

### Failure signatures:
- **Topic Drift:** Classifier confuses sentences about similar topics with sentences containing the specific word sense
- **Snippet Failure:** High loss during training because short snippets provide insufficient gradient signal compared to expanded sentences

### First 3 experiments:
1. **Sanity Check (Human Eval):** Sample 50 dictionary snippets and GPT-generated expansions; manually verify sense preservation
2. **WiC Baseline:** Train SloBERTa on unexpanded snippets vs. expanded sentences; compare binary classification accuracy
3. **Threshold Sweep:** Run WSI evaluation using different multiples (1.0x, 1.2x, 1.5x) to find optimal trade-off between discovering new senses and over-fragmenting existing ones

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scale with larger LLM-generated datasets? The authors note they limited data to 9,096 pairs due to resource constraints, but the potential gains from quadratic pair expansion remain untested.

### Open Question 2
Can this methodology construct dictionary senses automatically in lexicography? The current work disambiguates existing senses but hasn't tested generating dictionary entries from raw corpora.

### Open Question 3
Can the model detect emerging word senses in diachronic linguistic analyses? The experiments used static datasets rather than temporal corpora tracking semantic change over time.

### Open Question 4
On which specific types of word senses or linguistic structures do LLMs fail when generating Slovene sentences? The manual evaluation was limited to 200 sentences, identifying only isolated issues like single-letter words or technical terminology errors.

## Limitations

- **LLM reliability uncertainty:** The 98.5% sense preservation rate for GPT-3.5 expansions is not independently verified and may vary for different lemma types
- **Threshold heuristic limitations:** The 1.2× threshold for WSI induction lacks rigorous calibration across different lemma categories and domains
- **Dictionary dependency:** Methodology's effectiveness depends on dictionary quality and completeness, limiting applicability to languages with less standardized lexicographic resources

## Confidence

- **High Confidence:** WiC classification can substitute for traditional WSD in inventory-free scenarios
- **Medium Confidence:** LLM expansion is effective for creating usable training data, but requires validation across other less-resourced languages
- **Low Confidence:** WSI threshold mechanism (1.2× multiplier) and its generalizability to different domains and language families remains inadequately tested

## Next Checks

1. **Sense Preservation Verification:** Conduct blind human evaluation on 200 randomly sampled GPT-generated sentences to independently verify the 98.5% sense preservation rate, with particular attention to polysemous words in technical domains

2. **Cross-Lingual Generalization:** Replicate the entire pipeline on a second less-resourced language (e.g., Basque or Maltese) using the same GPT-3.5 expansion approach to test methodology's broader applicability beyond Slovene

3. **Threshold Calibration Study:** Systematically test WSI performance across a range of threshold multipliers (0.8× to 2.0×) on a held-out validation set to establish whether the 1.2× heuristic is optimal or merely sufficient