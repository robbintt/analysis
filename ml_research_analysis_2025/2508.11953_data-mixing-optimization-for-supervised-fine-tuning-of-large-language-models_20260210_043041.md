---
ver: rpa2
title: Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models
arxiv_id: '2508.11953'
source_url: https://arxiv.org/abs/2508.11953
tags:
- data
- domain
- weights
- domains
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for optimizing data mixtures
  in supervised fine-tuning (SFT) of large language models (LLMs). The method frames
  data mixing as an optimization problem to minimize validation loss, using a novel
  parameterization that models scaling effects of SFT data and domain interactions.
---

# Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models

## Quick Facts
- **arXiv ID:** 2508.11953
- **Source URL:** https://arxiv.org/abs/2508.11953
- **Authors:** Yuan Li; Zhengzhong Liu; Eric Xing
- **Reference count:** 40
- **Primary result:** Novel method for optimizing data mixtures in SFT using perturbation experiments and scaling law parameterization, achieving competitive performance with only 0.66% higher loss than optimal grid search.

## Executive Summary
This paper introduces a novel method for optimizing data mixtures in supervised fine-tuning (SFT) of large language models (LLMs). The method frames data mixing as an optimization problem to minimize validation loss, using a novel parameterization that models scaling effects of SFT data and domain interactions. By conducting perturbation experiments on small-scale data mixtures, the approach fits parameters and derives optimal domain weights. The method ensures balanced performance across all domains, as supported by mathematical proofs and empirical results. Experiments across various scenarios show that models trained with optimized weights perform on par with those using optimal weights from grid search, with per-domain loss only 0.66% higher on average. The method also improves validation loss and downstream performance when reweighting popular SFT datasets. Additionally, it generalizes to guide data selection for domain-specific models and provides insights into SFT.

## Method Summary
The method optimizes data mixtures for SFT by fitting scaling parameters through perturbation experiments. For each domain, the approach trains small models on datasets where one domain's size varies (0.5×, 2×, 3×) while others remain fixed. These experiments capture how loss scales with data size and how domains interact through transfer learning. The fitted parameters are then used to predict validation loss for any data mixture, which is optimized using constrained optimization (SLSQP) to find the optimal domain weights. The method ensures balanced performance across all domains by leveraging the convexity of the objective function and the diminishing returns property of scaling laws.

## Key Results
- Models trained with optimized weights achieve validation loss only 0.66% higher than optimal grid search results
- The method improves validation loss and downstream performance when reweighting popular SFT datasets
- Provides insights into SFT and generalizes to guide data selection for domain-specific models
- Ensures balanced performance across all domains through mathematical guarantees

## Why This Works (Mechanism)

### Mechanism 1: Transfer-Enhanced Loss Parameterization
The method predicts validation loss for a target data budget by modeling data scaling and domain interactions, rather than relying on heuristics. It parameterizes the loss function for each domain using a power law that combines in-domain data with "effective data transferred" from other domains. This allows the model to account for cross-domain benefits (e.g., general instruction data improving math reasoning). The core assumption is that parameters fitted on small-scale perturbation experiments generalize to larger data budgets and follow established scaling laws for fine-tuning.

### Mechanism 2: Convexity-Driven Balanced Weighting
The optimization algorithm naturally prevents domain starvation ("No Domains Left Behind") without explicit constraints for every domain. The objective function is proven convex, and because the loss curve follows a power law, the marginal return is high when a domain's weight is small and diminishes as weight increases. The optimizer is mathematically driven to allocate data to under-represented domains where the loss reduction gradient is steepest.

### Mechanism 3: Small-Scale Perturbation Fitting
Global optimal weights can be approximated by training only on small, perturbed versions of the dataset. Instead of grid search on the full model, the method runs training runs with data sizes scaled by factors, then fits the scaling parameters using Huber loss on these results. The core assumption is that small proxy models used for perturbation reflect the relative domain difficulties and transfer dynamics of the final target model.

## Foundational Learning

- **Scaling Laws (Power Law)**: Why needed here: The entire parameterization assumes loss decreases as a power function of data size. Quick check: If you double the data, does the loss halve? (Answer: No, it decreases non-linearly).

- **Transfer Learning (Effective Data)**: Why needed here: The paper relies on the concept that training on domain A effectively counts as training data for domain B, scaled by a transfer coefficient. Quick check: Can training on general instructions reduce perplexity on a medical validation set? (Answer: Yes, per the paper's "cocktail effect" and transfer mechanism).

- **Constrained Optimization (SLSQP)**: Why needed here: To find the best weights, one must minimize the predicted loss subject to the constraint that weights sum to 1. Quick check: Why can't we just pick the domain with the steepest loss curve? (Answer: Because of the simplex constraint and the need to minimize total aggregated loss).

## Architecture Onboarding

- **Component map**: Data Sampler -> Proxy Trainer -> Parameter Fitter -> Weight Optimizer (SLSQP)
- **Critical path**: The Perturbation Loop. You cannot optimize weights without first fitting the parameters via multiple training runs.
- **Design tradeoffs**: Using a smaller model for fitting is faster but risks misestimating transfer coefficients for the target model. Smaller perturbation ratios save compute but may fail to capture the tail of the scaling law where loss plateaus.
- **Failure signatures**: Unbalanced Weights if a domain yields extremely high loss reduction, the optimizer might still saturate it, potentially ignoring others if the transfer term is underestimated. Validation/Downstream Mismatch: optimizing for validation PPL does not guarantee downstream task improvement if the validation set distribution diverges from test tasks.
- **First 3 experiments**:
  1. Run perturbation for a single domain and plot the fitted loss curve against real loss points to verify the power-law fit.
  2. Compare optimal weights calculated with the transfer term vs without to quantify the impact of domain interaction.
  3. Fit parameters on a 1M token budget and predict weights for a 10M budget, then validate against a grid search at 10M to test generalization.

## Open Questions the Paper Calls Out

- Can the method's performance be improved by integrating synthetic data generation rather than repeated upsampling of existing data? The authors speculate that generating new data for domains rather than repeatedly upsampling existing data can fully reveal the potential of their approach.

- Can the parameterization of validation loss be extended to directly model and optimize downstream task performance? The authors propose extending their parameterization to directly model downstream performance, thereby optimizing weights for targeted tasks.

- Can parameters or optimal weights estimated from small proxy models be reliably transferred to larger models within the same family? The authors note that models within the same class exhibit similar performance patterns and suggest estimating loss parameters using smaller models and then applying optimized weights to larger models.

## Limitations
- The fitted scaling parameters are derived from small-scale perturbation experiments, and the paper assumes these generalize to larger budgets, but the extrapolation range is substantial.
- The method uses small proxy models for perturbation experiments, but scaling laws and transfer dynamics may differ between small and large models.
- The transfer term assumes additive cross-domain benefits, but in practice, some domains may interfere negatively with others.

## Confidence
- **High Confidence**: The core methodology of using perturbation experiments to fit scaling parameters is well-specified and mathematically sound.
- **Medium Confidence**: The empirical results showing competitive performance are demonstrated but only across a limited set of experiments.
- **Low Confidence**: The assumption that small-scale perturbation results perfectly predict large-scale behavior is not empirically validated across multiple orders of magnitude.

## Next Checks
1. **Scale Extrapolation Test**: Fit parameters using perturbation experiments at 1M token budget and validate whether the predicted optimal weights for 50M token budget achieve similar performance to weights found via grid search at that scale.

2. **Model Size Sensitivity**: Repeat the perturbation experiments and parameter fitting using a 7B model instead of the 3B model. Compare whether the resulting optimal weights differ significantly and whether the method still produces competitive results when applied to the larger model.

3. **Negative Transfer Validation**: Construct a scenario where training on domain A is known to hurt performance on domain B (e.g., code data potentially interfering with general instruction following). Test whether the method detects and mitigates this negative transfer by appropriately weighting the domains.