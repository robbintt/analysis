---
ver: rpa2
title: Guiding Retrieval using LLM-based Listwise Rankers
arxiv_id: '2501.09186'
source_url: https://arxiv.org/abs/2501.09186
tags:
- retrieval
- documents
- ranking
- reranking
- listwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bounded recall in LLM-based
  listwise reranking systems, where relevant documents not initially retrieved are
  permanently excluded. The authors propose SlideGar, an adaptive retrieval method
  that integrates corpus graph exploration with sliding window listwise ranking to
  dynamically expand the retrieval set during reranking.
---

# Guiding Retrieval using LLM-based Listwise Rankers

## Quick Facts
- arXiv ID: 2501.09186
- Source URL: https://arxiv.org/abs/2501.09186
- Authors: Mandeep Rathee; Sean MacAvaney; Avishek Anand
- Reference count: 40
- Key outcome: SlideGar improves nDCG@10 by up to 13.23% and recall by 28.02% over standard LLM rankers with minimal computational overhead (0.02% additional latency)

## Executive Summary
This paper addresses the challenge of bounded recall in LLM-based listwise reranking systems, where relevant documents not initially retrieved are permanently excluded. The authors propose SlideGar, an adaptive retrieval method that integrates corpus graph exploration with sliding window listwise ranking to dynamically expand the retrieval set during reranking. By alternating between initial results and graph-neighborhood documents, SlideGar enables effective feedback without requiring additional LLM inferences.

Experiments show that SlideGar significantly improves retrieval effectiveness across diverse datasets and ranker types, with gains particularly pronounced in low-budget settings. The method is robust across different retrievers, rankers, and datasets, demonstrating strong generalization while adding only 2-7ms of computational overhead.

## Method Summary
SlideGar is an adaptive retrieval method that addresses bounded recall in LLM-based listwise reranking by integrating corpus graph exploration with sliding window ranking. The algorithm maintains two pools of documents: the initial retrieval results (R0) and a frontier of graph-neighborhood documents (F). It processes documents in sliding windows of size w, ranking each window with an LLM ranker, then alternately drawing the next window from whichever pool wasn't used in the previous iteration. The frontier is populated with neighbors of top-ranked documents, prioritized using reciprocal rank scores. This approach enables discovery of relevant documents missed by the initial retriever without additional LLM inference calls.

## Key Results
- Improves nDCG@10 by up to 13.23% over standard LLM rankers
- Increases recall by 28.02% while adding only 0.02% computational overhead
- Gains are particularly pronounced in low-budget settings (c=50), where SlideGarTCT achieves up to 48.72% recall improvement
- Works effectively across multiple datasets (DL19, TREC-COVID, MQ2007, MQ2008) and ranker types (RankZephyr, RankGPT, RankVicuna)
- SlideGarTCT consistently outperforms SlideGarBM25, showing that semantic graph neighbors better complement LLM ranking than lexical neighbors

## Why This Works (Mechanism)

### Mechanism 1: Alternating Pool Strategy for Bounded Recall Recovery
- **Claim:** Alternating between initial retrieval results and graph neighbors allows discovery of relevant documents missed by the first-stage retriever without increasing LLM inference calls.
- **Mechanism:** After each window is ranked, the algorithm draws the next batch from whichever pool (initial R0 or graph frontier F) was not used in the previous iteration. The frontier is populated with neighbors of top-ranked documents, creating a dynamic expansion signal.
- **Core assumption:** The Cluster Hypothesis holds—documents similar to relevant documents are also likely to be relevant.
- **Evidence anchors:**
  - [abstract]: "By alternating between initial results and graph-neighborhood documents, SlideGar enables effective feedback without requiring additional LLM inferences"
  - [section 3.2, Algorithm 1]: "Alternate between initial ranking and frontier" with explicit alternation logic at line 11
  - [corpus]: Weak direct support. Neighbor paper "On Listwise Reranking for Corpus Feedback" references graph-aware adaptive retrieval, confirming this technique family.
- **Break condition:** If the corpus graph has poor neighbor quality or the domain violates the cluster hypothesis (e.g., adversarial documents), recall gains will not materialize.

### Mechanism 2: Listwise-to-Pointwise Adaptation via Rank-Based Pseudo-Scores
- **Claim:** Converting listwise rank outputs to pseudo-scores via reciprocal rank enables adaptive retrieval methods designed for pointwise scoring to work with LLM listwise rankers.
- **Mechanism:** Listwise LLM rankers output only permutations without scores. SlideGar assigns reciprocal rank (1/rank) as a pseudo-score to each document, enabling prioritization of which documents' neighbors to explore—higher-ranked documents' neighbors get priority in the frontier.
- **Core assumption:** Rank position correlates sufficiently with relevance to serve as a score proxy for neighbor selection.
- **Evidence anchors:**
  - [abstract]: "our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point"
  - [section 3.2]: "Since the ranker does not provide any score, we use the reciprocal of the rank as a pseudo-score to the documents in B"
  - [corpus]: No direct corpus support found for this specific rank-to-score conversion technique.
- **Break condition:** If the LLM ranker produces systematically incorrect rankings, wrong documents' neighbors will be prioritized, potentially degrading rather than improving results.

### Mechanism 3: Pre-Computed Corpus Graph for Constant-Time Neighbor Lookup
- **Claim:** Offline-constructed document similarity graphs enable retrieval of candidate relevant documents with negligible runtime overhead compared to index re-querying.
- **Mechanism:** A corpus graph stores each document's k nearest neighbors (k=16 in experiments). During reranking, neighbor lookups are O(1) graph traversals rather than expensive similarity searches. This avoids the query drift and re-execution costs of traditional PRF methods like RM3.
- **Core assumption:** Document-document similarity relationships are stable and transfer to query relevance patterns.
- **Evidence anchors:**
  - [abstract]: "minimal computational overhead (0.02% additional latency)"
  - [section 2.2]: "By constructing the corpus graph offline, overheads of applying AR (as compared to traditional reranking) are minimal, since lookups can be conducted in a constant and minimal time"
  - [corpus]: Weak support. Related papers discuss embedding-based reranking but don't validate graph construction depth choices.
- **Break condition:** Dynamic corpora with frequent additions/updates will cause graph staleness; highly sparse or domain-specific corpora may have weak neighbor relationships.

## Foundational Learning

- **Concept: Cascading Retrieve-and-Rerank Pipeline**
  - **Why needed here:** SlideGar operates within this two-stage paradigm; the bounded recall problem is inherent to any system where the reranker only sees pre-filtered documents.
  - **Quick check question:** Why can a reranker improve precision but never recall in a standard cascading pipeline?

- **Concept: Listwise vs. Pointwise Ranking Paradigms**
  - **Why needed here:** The core contribution is adapting adaptive retrieval (designed for pointwise scorers like MonoT5) to listwise LLM rankers that output permutations without scores.
  - **Quick check question:** What output does a pointwise ranker produce that a listwise ranker explicitly does not?

- **Concept: Pseudo-Relevance Feedback (PRF) and Query Drift**
  - **Why needed here:** SlideGar is a form of PRF but avoids query rewriting; understanding why traditional PRF methods like RM3 cause query drift clarifies SlideGar's efficiency advantage.
  - **Quick check question:** Why does expanding the query with terms from top-retrieved documents risk retrieving off-topic results?

## Architecture Onboarding

- **Component map:** Initial Retriever (BM25 or TCT) -> Corpus Graph (pre-computed, k=16 neighbors) -> Sliding Window Controller -> Listwise LLM Ranker -> Frontier Manager -> Pseudo-Score Generator

- **Critical path:**
  1. Initial retrieval → R0 (top-c documents)
  2. First window (w docs from R0) → LLM ranks → split into carry-forward (top b) and finalized
  3. Ranked docs' neighbors added to frontier F with priority from pseudo-scores
  4. Next window = carry-forward + next batch from alternate pool (F if previous was R0)
  5. Repeat until c-b documents finalized; append remaining carry-forward

- **Design tradeoffs:**
  - Window size vs. context: Larger windows (w=20) provide more ranking context but approach LLM context limits
  - Graph depth (k): Paper shows k=10-16 optimal; deeper graphs add noise without benefit
  - Graph type: TCT (dense) graphs outperform BM25 (sparse) graphs—semantic neighbors complement lexical retrieval better
  - Ranking budget (c): Lower budgets (c=50) show larger relative recall gains; higher budgets (c=100) show diminishing returns

- **Failure signatures:**
  - Recall unchanged: Verify graph construction; check if initial retrieval already has high recall (saturated regime)
  - nDCG drops despite recall gains: Newly introduced neighbors may be ranked poorly by LLM; consider ranker quality
  - Latency spikes: SlideGar overhead should be ~2-7ms regardless of budget; if higher, check graph data structure
  - No significant difference between SlideGarBM25 and SlideGarTCT: Corpus may lack semantic structure; verify dense embeddings quality

- **First 3 experiments:**
  1. Establish baseline: Run `BM25»RankZephyr` on DL19 with c=50; record nDCG@10, Recall@50
  2. Validate SlideGar: Run `BM25»RankZephyr w/ SlideGarTCT` same settings; expect ~20% recall improvement, ~10% nDCG improvement
  3. Ablate graph depth: Test k ∈ [2, 4, 8, 16] to find optimal neighbor count for your corpus; plot recall vs. k to identify saturation point

## Open Questions the Paper Calls Out
None

## Limitations
- Temporal validity of corpus graphs is not addressed; static graphs may become stale in dynamic information environments
- Generalizability to non-English corpora is unknown; all experiments use English datasets
- Performance gains are tightly coupled to specific LLM rankers, with effectiveness varying based on ranker characteristics

## Confidence
- **High Confidence:** The core claim that alternating between initial results and graph neighbors enables recall improvement without additional LLM calls is well-supported by experimental methodology and results
- **Medium Confidence:** The assertion that SlideGar adds only 0.02% computational overhead is supported by reported latency increases, though may vary with configurations
- **Low Confidence:** The claim about effectiveness being particularly pronounced in low-budget settings is based on observed trends but lacks comprehensive testing across wider budget ranges

## Next Checks
1. **Dynamic corpus evaluation:** Test SlideGar on a time-evolving corpus where documents are added periodically. Measure recall degradation over time and evaluate different graph update strategies (incremental vs. full reconstruction).

2. **Cross-lingual generalization study:** Implement SlideGar using the same methodology on non-English corpora (e.g., CLEF datasets in multiple languages). Compare recall and nDCG improvements against the English baselines to identify language-specific performance patterns.

3. **Ranker diversity benchmarking:** Evaluate SlideGar with LLM rankers trained on different objectives (pointwise vs. listwise), using different context window sizes, and with varying permutation quality metrics. Systematically measure how ranker characteristics impact SlideGar's effectiveness.