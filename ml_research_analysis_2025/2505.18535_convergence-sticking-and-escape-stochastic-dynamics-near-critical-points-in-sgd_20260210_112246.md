---
ver: rpa2
title: 'Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points
  in SGD'
arxiv_id: '2505.18535'
source_url: https://arxiv.org/abs/2505.18535
tags:
- stochastic
- have
- convergence
- time
- critical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies convergence and escape dynamics of stochastic
  gradient descent (SGD) in one-dimensional non-convex landscapes, separately analyzing
  infinite- and finite-variance noise regimes. The authors prove that SGD converges
  to the basin's minimum unless initialized near a local maximum, in which case it
  may linger or escape to neighboring basins depending on noise characteristics and
  function geometry.
---

# Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD

## Quick Facts
- arXiv ID: 2505.18535
- Source URL: https://arxiv.org/abs/2505.18535
- Reference count: 40
- One-line primary result: SGD converges to basin minima unless near sharp maxima, where escape probabilities depend on noise and geometry

## Executive Summary
This paper provides a theoretical analysis of stochastic gradient descent (SGD) dynamics near critical points in non-convex optimization landscapes. The authors distinguish between infinite and finite-variance noise regimes and prove that SGD typically converges to basin minima unless initialized near local maxima. For sharp maxima, they derive explicit formulas for escape probabilities to neighboring basins. The work offers nuanced insights into why SGD tends to avoid sharp minima and prefer flat ones, providing time scale estimates for convergence and escape phenomena.

## Method Summary
The paper employs probabilistic techniques to analyze SGD dynamics in one-dimensional non-convex landscapes, separately treating infinite- and finite-variance noise regimes. The analysis establishes conditions for convergence to basin minima and derives escape probability formulas for sharp maxima. The authors extend their results to multi-dimensional settings and provide time scale estimates for both convergence and escape phenomena. The theoretical framework connects SGD's transition behavior between critical points to fundamental properties of the loss landscape geometry and noise characteristics.

## Key Results
- SGD converges to basin minima under typical initialization unless near local maxima
- For sharp maxima, escape probabilities to neighboring minima can be explicitly calculated
- Time scales for convergence and escape phenomena are identified and characterized
- SGD's tendency to avoid sharp minima and reach flat ones is theoretically justified

## Why This Works (Mechanism)
SGD's behavior near critical points is governed by the interplay between gradient noise and landscape geometry. In infinite-variance noise regimes, SGD exhibits heavy-tailed behavior that can facilitate escape from sharp maxima. In finite-variance regimes, the escape probability depends on the sharpness of the maximum and the local noise characteristics. The convergence to flat minima occurs because these regions provide more stable dynamics and lower effective noise amplification, while sharp maxima create unstable equilibria that SGD tends to escape from given sufficient noise or poor initialization.

## Foundational Learning
- Critical point classification: Sharp vs. flat minima/maxima - needed to understand different dynamical behaviors; quick check: examine Hessian eigenvalues
- Noise regimes: Infinite vs. finite variance - determines escape dynamics; quick check: compute noise kurtosis
- Basin of attraction: Geometric regions around minima - defines convergence targets; quick check: visualize level sets
- Escape probability: Likelihood of transitioning between basins - quantifies SGD's exploration; quick check: compare theoretical vs empirical transition rates
- Time scale separation: Different rates for convergence vs escape - identifies distinct dynamical phases; quick check: monitor convergence metrics over training

## Architecture Onboarding
**Component map:** Initialization -> Critical point approach -> Convergence/sticking phase -> Escape phase -> Basin transition
**Critical path:** The sequence from initialization through approach to critical points, determining whether convergence or escape occurs based on noise characteristics and local geometry
**Design tradeoffs:** Sharp maxima provide informative gradients but create escape-prone dynamics; flat minima offer stability but potentially slower convergence
**Failure signatures:** Getting stuck near maxima, premature escape to suboptimal basins, oscillatory behavior around unstable equilibria
**First experiments:**
1. Vary initialization distance from maxima to test convergence vs escape predictions
2. Compare escape rates under different noise distributions (Gaussian vs heavy-tailed)
3. Measure time scales for convergence to flat vs sharp minima under controlled noise

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily focused on one-dimensional landscapes with only brief multi-dimensional extensions
- Assumes distinct infinite vs finite variance noise regimes that may not capture real-world complexity
- Sharp maximum assumption may not reflect the full diversity of critical point geometries
- Limited consideration of how noise properties might vary dynamically during optimization

## Confidence
- High confidence: Convergence to basin minima under typical initialization conditions
- Medium confidence: Escape probability formulas for sharp maxima
- Medium confidence: Time scale estimates for convergence and escape phenomena
- Low confidence: Direct applicability to high-dimensional, complex loss landscapes

## Next Checks
1. Verify escape probability formulas through numerical simulations across different noise distributions and function geometries
2. Test convergence and escape predictions in multi-dimensional optimization problems with varying critical point structures
3. Evaluate theoretical predictions against empirical SGD behavior on real machine learning tasks with known landscape properties