---
ver: rpa2
title: Auditability and the Landscape of Distance to Multicalibration
arxiv_id: '2509.16930'
source_url: https://arxiv.org/abs/2509.16930
tags:
- multicalibration
- distance
- which
- have
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how to measure multicalibration of predictors.
  It argues that a good multicalibration metric should (1) reflect how much a predictor
  would need to change to become perfectly multicalibrated, and (2) be auditable with
  finite samples.
---

# Auditability and the Landscape of Distance to Multicalibration

## Quick Facts
- arXiv ID: 2509.16930
- Source URL: https://arxiv.org/abs/2509.16930
- Reference count: 40
- Primary result: Proposes two equivalent metrics for measuring multicalibration error that are both Lipschitz in ground truth and have all local minima as global minima, while analyzing auditability challenges.

## Executive Summary
This paper examines how to measure multicalibration of predictors through distance-based metrics. The authors argue that a good multicalibration metric should reflect how much a predictor would need to change to become perfectly multicalibrated and be auditable with finite samples. They analyze two natural extensions of the distance to calibration framework - worst-group distance and distance to multicalibration - and show that each fails one of these properties. The paper proposes two equivalent metrics that satisfy both requirements and proves they have desirable theoretical properties, while also examining auditability limitations.

## Method Summary
The paper proposes two equivalent metrics for measuring multicalibration error: a continuized variant of distance to multicalibration (]dMC) and distance to intersection multicalibration (dIMC). These metrics are computed by finding the minimum ℓ1 distance from a predictor f to the nearest multicalibrated predictor. The dIMC metric is calculated by decomposing the predictor space into intersections of subgroups and computing weighted distance to calibration error across this partition. The paper also analyzes distance to multiaccuracy (dMA), which requires unbiased predictions on all subgroups rather than calibrated ones. The authors prove these metrics are Lipschitz in the ground truth and have the property that all local minima are global minima.

## Key Results
- wdMC fails auditability because it only optimizes calibration within each subgroup independently, potentially requiring exponentially large predictor changes
- dIMC = ]dMC and both satisfy Lipschitz continuity in ground truth and have all local minima as global minima
- Auditability of ]dMC requires exponentially many samples in general, but can be efficient when the number of subgroups is small or under structural conditions
- dMA can be computed via linear programming but its auditability remains an open question

## Why This Works (Mechanism)
The paper works by analyzing the geometric structure of multicalibration constraints and how distance metrics interact with this structure. The key insight is that measuring multicalibration requires considering the intersection structure of subgroups rather than treating each subgroup independently. The continuized metric ]dMC handles the continuum of possible calibrations by taking a supremum over all possible calibration functions, while dIMC achieves the same result through decomposition into intersections. This intersection-based approach ensures that local optimization steps move toward global optimality.

## Foundational Learning
- **Distance to Calibration**: Measures ℓ1 distance from predictor to nearest perfectly calibrated predictor; needed to quantify how far a predictor is from being multicalibrated.
- **Intersection Closure**: The set J(C) of all intersections of subgroups in C; needed because multicalibration requires consistent calibration across overlapping subgroups.
- **Lipschitz Continuity**: Property ensuring small changes in ground truth lead to small changes in the metric; needed for statistical stability and auditability.
- **Statistical Hardness**: The requirement of exponentially many samples to distinguish between reducible and irreducible error; needed to understand fundamental limits of auditing.
- **Local vs Global Minima**: Property where all local minima are global minima; needed to ensure optimization algorithms don't get stuck in suboptimal solutions.
- **Linear Programming Formulation**: Method for computing dMA via optimization; needed to make the metric computationally tractable.

## Architecture Onboarding

**Component Map**: Subgroups C -> Intersection closure J(C) -> Partition D -> Weighted dCE computation

**Critical Path**: Predictor f -> Compute dCE on each S in J(C) -> Sum weighted dCE values -> Output dIMC

**Design Tradeoffs**: wdMC is computationally simpler but has non-global local minima and poor auditability; dIMC/dMC are more complex to compute but have better theoretical properties and auditability.

**Failure Signatures**: Exponential blowup in |J(C)| when subgroups have many intersections; statistical hardness when ground truth distributions are complex; non-convexity issues with wdMC.

**First Experiments**:
1. Implement Proposition 5 counterexample with X={x₁,x₂,x₃}, C={S₁,S₂} to verify wdMC has non-global local minima
2. Compute dIMC on synthetic data with varying numbers of subgroups to verify the decomposition works correctly
3. Test Lipschitz continuity empirically by perturbing ground truth and measuring changes in dIMC

## Open Questions the Paper Calls Out

**Open Question 1**: Can restricting ground truth distributions to those with low VC dimension or bounded complexity enable efficient auditability of continuized distance to multicalibration (]dMC)?

**Open Question 2**: Can distance to multiaccuracy (dMA) be audited with statistically and computationally efficient algorithms?

**Open Question 3**: Can higher-order predictors (trained with k-snapshots) bypass the information-theoretic barriers to distinguishing aleatoric from epistemic uncertainty in multicalibration auditing?

## Limitations
- The complexity of dIMC scales exponentially with the number of subgroups due to the need to compute dCE on all intersections in J(C)
- Statistical hardness results assume worst-case distributions, which may not reflect real-world data structure
- The continuized distance ]dMC requires careful implementation for proper handling of the continuum of possible calibrations
- Lack of empirical validation on real-world datasets to complement theoretical analysis

## Confidence
- High confidence: Theoretical proofs of Proposition 5 (wdMC has non-global local minima), Proposition 7 (dIMC = ]dMC), and Proposition 10 (Lipschitz continuity)
- Medium confidence: Statistical hardness results for auditability (Lemma 25, Theorem 28) due to dependence on worst-case distributions
- Medium confidence: Extension to multiaccuracy (Proposition 36) as it follows similar structure but with additional constraints

## Next Checks
1. Implement the linear program from Proposition 36 on synthetic data with varying numbers of subgroups to verify tractability bounds and check that the LP correctly computes dMA
2. Conduct empirical evaluation on a real dataset (e.g., COMPAS or Adult) comparing wdMC, dIMC, and dMA metrics to assess their practical differences and computational feasibility
3. Perform sensitivity analysis on ]dMC by testing different discretization schemes for the continuum of possible calibrations to ensure numerical stability and convergence