---
ver: rpa2
title: 'Position: Require Frontier AI Labs To Release Small "Analog" Models'
arxiv_id: '2510.14053'
source_url: https://arxiv.org/abs/2510.14053
tags:
- analog
- safety
- frontier
- arxiv
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a regulatory policy requiring frontier AI labs
  to publicly release small, distilled "analog" models alongside their large proprietary
  systems. These analog models would be trained on the same data and objectives but
  constrained to 0.5-5% of the frontier model's size.
---

# Position: Require Frontier AI Labs To Release Small "Analog" Models

## Quick Facts
- arXiv ID: 2510.14053
- Source URL: https://arxiv.org/abs/2510.14053
- Authors: Shriyash Upadhyay; Chaithanya Bandi; Narmeen Oozeer; Philip Quirke
- Reference count: 12
- Primary result: Proposes mandatory release of small distilled "analog" models (0.5-5% of frontier size) to enable safety research while preserving competitive advantages

## Executive Summary
This paper proposes a regulatory policy requiring frontier AI labs to publicly release small, distilled "analog" models alongside their large proprietary systems. These analog models would be trained on the same data and objectives but constrained to 0.5-5% of the frontier model's size. The authors argue this approach enhances safety oversight and interpretability research while preserving innovation and competitive advantages. Recent research demonstrates that safety interventions developed using small models successfully transfer to larger systems, validating the technical foundation of this proposal. The policy would incur minimal costs—estimated at about 0.1% of frontier model training budgets—while creating significant public goods.

## Method Summary
The paper proposes creating analog models through distillation from frontier models, where these smaller models (0.5-5% of the original size) would be trained on identical data and objectives. The core validation involves testing whether safety interventions like steering vectors, when developed on analog models, successfully transfer to frontier-scale systems. This requires measuring intervention effectiveness across model scales and quantifying the representational alignment between analog and frontier models using metrics like embedding isometry (Pearson correlation > 0.9) and intervention transfer fidelity (<5% performance degradation).

## Key Results
- Recent research demonstrates that safety and interpretability methods developed using smaller models generalize effectively to frontier-scale systems
- The policy would incur minimal additional costs, estimated at about 0.1% of frontier model training budgets
- Cross-scale transfer of safety interventions has been validated in recent studies with trigger rate reductions below 5%

## Why This Works (Mechanism)

### Mechanism 1: Cross-Scale Transfer of Safety Interventions
- Claim: Safety interventions discovered on small models reliably transfer to frontier-scale systems
- Core assumption: Feature directions representing safety-relevant concepts remain stable across scales
- Evidence: Oozeer et al. [2025] transferred steering vectors from 0.5B to 3B models with <5% trigger rates; Lee et al. [2025] showed embedding spaces are nearly isometric across 1B-70B (Pearson r > 0.9)
- Break condition: Emergent capabilities appearing only at frontier scale would limit transfer fidelity

### Mechanism 2: Representational Convergence via Shared Training
- Claim: Models trained on identical data/objectives converge to shared internal geometries regardless of scale
- Core assumption: Training data and objectives being truly identical is sufficient for representational alignment
- Evidence: Platonic Representation Hypothesis [Huh et al., 2024]; superposition research shows stable feature vectors across 70M-1B models [Elhage et al., 2022, Bricken et al., 2023]
- Break condition: Proprietary data mixtures or objectives not replicable in analogs would degrade convergence

### Mechanism 3: Cost Efficiency Through Infrastructure Reuse
- Claim: Producing analog models costs ~0.1% of frontier training budgets
- Core assumption: Labs already maintain reusable assets; distillation quality remains acceptable at 0.5-5% scale
- Evidence: Estimated $92,759 for an 8B analog (~0.1% of frontier training); includes training, distillation, safety fine-tuning, and hosting
- Break condition: If distillation fails to capture complex reasoning or emergent behaviors, analog utility drops while costs remain fixed

## Foundational Learning

- **Knowledge Distillation**: Why needed? Analog models are produced via distillation from frontier models; understanding teacher-student dynamics is prerequisite to evaluating fidelity claims. Quick check: Can you explain why a small student model trained on a large teacher's outputs might retain core capabilities while losing surface-level memorization?

- **Representation Geometry**: Why needed? The transfer mechanism assumes embedding spaces share structure across scales; geometric intuition (orthogonality, isometry, projection) underpins the technical argument. Quick check: If two models have "isometric" embedding spaces, what does that imply about transferring a steering vector between them?

- **Scaling Laws**: Why needed? The policy assumes smooth, predictable capability scaling; understanding power-law relationships helps assess where analog-to-frontier extrapolation is valid. Quick check: If model loss scales as a power law with compute, what does this suggest about capability jumps at frontier scale?

## Architecture Onboarding

- **Component map**: Frontier Training Pipeline → Frontier Model → Distillation Module → Analog Model (0.5-5% params) → Release Infrastructure (weights, model card, data description, scripts) → External Research Community → Safety/Interpretability Interventions → Transfer Mechanisms → Back to Frontier Model

- **Critical path**: 1. Validate representational alignment between analog and frontier (intervention transfer tests) 2. Confirm distillation preserves safety-relevant features (benchmark comparison) 3. Complete security evaluation before release (dual-use mitigation)

- **Design tradeoffs**: Size cap (0.5% = stronger IP protection but weaker fidelity; 5% = better transfer but higher substitution risk), Release lag (1 month = faster oversight but less security vetting; 3 months = safer but delays public benefit), License permissiveness (Apache 2.0 = maximum research utility; restrictive terms = misuse mitigation but friction)

- **Failure signatures**: Intervention transfer success rate <70% on safety benchmarks, Emergent frontier behaviors (e.g., sophisticated deception) absent in analog, Capability extraction attacks enabling frontier reconstruction from analog, Commercial substitution exceeding 5% of frontier API revenue

- **First 3 experiments**: 1. Replicate cross-scale steering vector transfer (0.5B→3B) using cited methodology; measure trigger rate reduction 2. Benchmark analog vs. frontier on safety-critical tasks (refusal, toxicity, hallucination) to quantify fidelity gap 3. Attempt adversarial reconstruction: given only the analog, estimate upper bound on extractable frontier capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Core hypothesis about representational convergence not empirically validated across diverse model families
- Cost analysis assumes distillation quality remains acceptable but doesn't address potential degradation of complex multi-step reasoning
- Security evaluation timeline lacks specificity about what constitutes adequate dual-use mitigation

## Confidence

- **High confidence**: Technical feasibility of producing analog models via distillation (established methodology, well-understood cost structure)
- **Medium confidence**: Transfer mechanism working for safety interventions (supported by recent studies but limited in scope)
- **Low confidence**: Assumption that representational alignment holds across all frontier architectures and that distillation preserves emergent reasoning capabilities

## Next Checks

1. **Cross-architecture transfer validation**: Test steering vector transfer between model families with different architectures to assess whether geometric similarity assumptions hold beyond narrow cases.

2. **Emergent capability gap analysis**: Systematically compare analog and frontier models on tasks requiring complex reasoning, long-horizon planning, and multi-step problem solving to quantify capability gaps that could limit intervention transfer.

3. **Dual-use risk quantification**: Conduct red-team exercises attempting to reconstruct frontier capabilities from released analog models, measuring the fidelity of extracted capabilities against different size caps and release timing windows.