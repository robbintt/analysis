---
ver: rpa2
title: 'What Layers When: Learning to Skip Compute in LLMs with Residual Gates'
arxiv_id: '2510.13876'
source_url: https://arxiv.org/abs/2510.13876
tags:
- gate
- gateskip
- arxiv
- compute
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GateSkip introduces residual-stream gating for token-wise layer
  skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear
  gate that condenses the branch's output before it re-enters the residual stream.
---

# What Layers When: Learning to Skip Compute in LLMs with Residual Gates

## Quick Facts
- **arXiv ID**: 2510.13876
- **Source URL**: https://arxiv.org/abs/2510.13876
- **Reference count**: 40
- **Key outcome**: GateSkip saves up to 15% compute while retaining >90% accuracy on long-form reasoning tasks

## Executive Summary
GateSkip introduces a novel approach to token-wise layer skipping in decoder-only LLMs through residual-stream gating. Each Attention and MLP branch is equipped with a sigmoid-linear gate that compresses the branch's output before it re-enters the residual stream. During inference, tokens are ranked by gate values and low-importance ones are skipped using a per-layer budget. The method achieves up to 15% compute savings on long-form reasoning while maintaining over 90% of baseline accuracy, and improves accuracy at full compute for instruction-tuned models.

## Method Summary
GateSkip modifies transformer architectures by adding vector gates (W_G ∈ R^(H×H), b ∈ R^H) after each Attention/MLP output projection. The gate multiplies module output before residual addition using h^(ℓ+1) = h^ℓ + o^ℓ ⊙ σ(W_G h^ℓ + b). During fine-tuning, a sparsity loss encourages gate values to differentiate between important and unimportant tokens. At inference, tokens are ranked by gate values and skipped below quantile thresholds, with KV caches copied upward for skipped tokens. The method is evaluated on Llama-3.2, Llama-3.1, and Gemma-2 models across reasoning and instruction-following tasks.

## Key Results
- Saves up to 15% compute while retaining >90% accuracy on long-form reasoning tasks
- Improves accuracy at full compute for instruction-tuned models
- Matches baseline quality near 50% compute savings

## Why This Works (Mechanism)
GateSkip works by learning token-specific importance scores through smooth, differentiable gates that operate on the residual stream. Unlike hard routing in Mixture-of-Depths, these gates allow fine-tuning stability on pretrained models. The post-module gate placement ensures that each branch's contribution is evaluated before residual addition, allowing the model to learn which tokens benefit from deeper processing. The sparsity loss encourages the model to make confident decisions about token importance, while the per-layer budget ensures computational savings without catastrophic accuracy drops.

## Foundational Learning
- **Residual Stream Gating**: Adding learnable gates to control information flow through residual connections; needed for token-wise importance scoring; quick check: verify gate values are between 0 and 1
- **Sigmoid-Linear Activation**: Using σ(x) as a gating function that preserves pretrained behavior when initialized properly; needed for smooth, differentiable skipping; quick check: confirm initial gate outputs are near 1
- **Per-Layer Budget Scheduling**: Decaying the fraction of tokens processed at each layer over training; needed to balance accuracy and compute savings; quick check: verify budget decreases from 100% to target value
- **Quantile-Based Skipping**: Selecting tokens to skip based on percentile ranking of gate values; needed for consistent compute savings across layers; quick check: measure actual compute savings matches target budget
- **KV Cache Copying**: Maintaining consistency when skipping tokens by copying hidden states upward; needed to preserve autoregressive generation; quick check: verify skipped tokens have correct KV entries
- **L2 Sparsity Loss**: Using L2 regularization on gate values to encourage differentiation; needed for stable training compared to KL-divergence; quick check: monitor gate value distribution for separation

## Architecture Onboarding
- **Component Map**: Input tokens → Embedding → Attention/MLP with gates → Gate value computation → Token ranking → KV copying → Next layer (or output)
- **Critical Path**: Token embedding → Gate computation → Token ranking → Conditional skipping → KV cache management → Final prediction
- **Design Tradeoffs**: Post-module vs pre-module gate placement (25.5% vs 5.5% accuracy); L2 vs KL sparsity loss (L2 more stable under compute constraints); smooth gates vs hard routing (fine-tuning stability vs potential efficiency)
- **Failure Signatures**: Uniform gate values across tokens; training instability or divergence; accuracy collapse at low compute savings
- **First Experiments**: 1) Verify gate initialization preserves pretrained behavior (b = 5 → σ(5) ≈ 1); 2) Check gate value histograms show differentiation between tokens; 3) Measure actual vs target compute savings at various budget levels

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on non-reasoning tasks is less pronounced than on long-form reasoning
- Missing key hyperparameters (peak learning rate, exact training data composition)
- Token-level inconsistency in computation patterns could affect deterministic execution requirements

## Confidence
**High confidence**: Technical feasibility of residual-stream gating is well-supported by mathematical formulation and ablation studies; architectural modifications are clearly described.

**Medium confidence**: Compute savings of 10-15% on long-form reasoning while maintaining >90% accuracy is supported by experimental results; improvement at full compute for instruction-tuned models is validated but sensitive to dataset specifics.

**Low confidence**: Generalization to extremely long sequences and interaction with other efficiency methods lacks extensive validation; claims about gate values providing insight into transformer information flow need rigorous analysis.

## Next Checks
1. **Budget schedule sensitivity analysis**: Systematically vary budget decay schedule to identify optimal Pareto frontier between accuracy and compute savings.

2. **Cross-task transferability validation**: Fine-tune on one task and evaluate zero-shot performance on structurally similar tasks to assess learned gating generalization.

3. **Stability under extreme compute constraints**: Evaluate performance at very aggressive compute savings (50% and below) to identify practical limits of the approach.