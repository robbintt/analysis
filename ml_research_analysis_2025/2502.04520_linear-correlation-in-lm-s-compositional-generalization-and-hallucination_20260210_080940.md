---
ver: rpa2
title: Linear Correlation in LM's Compositional Generalization and Hallucination
arxiv_id: '2502.04520'
source_url: https://arxiv.org/abs/2502.04520
tags:
- correlation
- knowledge
- linear
- city
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization ability of language
  models by uncovering a phenomenon of linear correlations in next token prediction
  logits. The authors find that certain related knowledge pairs exhibit a linear transformation
  between their output logits, which mirrors the linearity in human knowledge composition.
---

# Linear Correlation in LM's Compositional Generalization and Hallucination

## Quick Facts
- **arXiv ID:** 2502.04520
- **Source URL:** https://arxiv.org/abs/2502.04520
- **Reference count:** 40
- **Primary result:** Linear transformations between related knowledge pairs' logits correlate with compositional generalization and hallucination in LLMs

## Executive Summary
This paper uncovers a phenomenon where certain related knowledge pairs exhibit linear correlations in next-token prediction logits across arbitrary inputs. The authors demonstrate that a linear transformation W can map logits from one prompt (e.g., "X lives in the city of") to another related prompt (e.g., "X lives in the country of"). This correlation is resilient to fine-tuning and emerges primarily from vocabulary representations rather than complex transformer mechanisms. The paper shows that high correlation intensity and precision enable successful compositional generalization, while imprecise transformations lead to systematic hallucinations.

## Method Summary
The authors analyze logit pairs from related knowledge prompts across multiple families (attribute, cross-language, simile, math) in LLaMA-3-8B. They fit linear transformations W ∈ ℝ^|V|×|V| using partial data and evaluate on held-out inputs. The methodology includes label-wise Pearson correlation to eliminate bias terms, Hit@Top-N metrics to measure transformation precision, and gradient correlation analysis to link correlation to generalization. An ablation study replaces all transformer components except vocabulary embeddings with a minimal mean-pooling + feedforward network trained on 1024 paired examples.

## Key Results
- Linear transformations exist between related knowledge prompts (City→Country correlation: 0.89)
- These correlations persist through fine-tuning across model versions
- High W precision (Hit@Top-5 ~0.67 for City→Country) enables successful generalization
- Low W precision causes compositional hallucinations (CEO→Company: Hit@Top-5 ~0.05)
- Minimal architecture with only vocabulary representations achieves comparable generalization (97.66% vs 97.70% for full model)

## Why This Works (Mechanism)

### Mechanism 1: Linear Logit Mapping Between Related Knowledge Prompts
- **Claim:** For compositionally related prompts (e.g., "X lives in the city of" → "X lives in the country of"), a linear transformation (W, b) can map next-token prediction logits from source to target across arbitrary inputs X.
- **Mechanism:** The transformation matrix W captures statistical relationships between vocabulary subdomains. High W weights correspond to real-world pairs (Paris→France), while counterfactual weights (Indianapolis→India) create systematic errors.
- **Core assumption:** The linearity emerges from how knowledge is organized in parameter space, specifically that relational transformations can be approximated by key-value matching in feedforward layers.
- **Evidence anchors:**
  - [abstract] "There exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another"
  - [Section 3.4] Label-wise Pearson correlations show high values (0.89 for City→Country) and low values for unrelated pairs (Gender with most attributes)
  - [corpus] "Representational Homomorphism Predicts and Improves Compositional Generalization" introduces Homomorphism Error to predict generalization failures at the representational level
- **Break condition:** Linearity degrades when semantic completeness of tokens is low (subwords: Hit@Top-5 = 0.00 vs. whole semantics: 0.49).

### Mechanism 2: Gradient Correlation Enables Knowledge Propagation
- **Claim:** When linear correlation between knowledge pairs is high, the same transformation W can estimate gradients on target knowledge from source gradients.
- **Mechanism:** During fine-tuning, gradient updates to source knowledge (e.g., City(X)=Shanghai) propagate through the learned W to target knowledge (Country(X)=China). This causes compositional generalization when W is precise, and hallucination when W is imprecise.
- **Core assumption:** The linear structure is resilient to gradient descent, meaning W remains stable across training updates.
- **Evidence anchors:**
  - [Section 4.1] Gradient correlations closely track logit correlations (e.g., City→Country: 0.79 gradient vs. 0.89 logit)
  - [Section 4.2] W fitted before post-training still estimates behavior after post-training across LLaMA-3-8B → LLaMA-3-8B-Instruct
  - [corpus] Limited direct corpus support; related work on representation-based approaches notes they "fail to generalize out of distribution"
- **Break condition:** Low correlation pairs (CEO→Company: 0.47) show poor gradient propagation and minimal generalization.

### Mechanism 3: Vocabulary Representations as the Compositional Substrate
- **Claim:** The linear correlation phenomenon originates primarily from pre-trained vocabulary representations, not from the transformer's attention mechanisms.
- **Mechanism:** When replacing all intermediate layers (attention, position embeddings, layer norms) with a single mean-pooling + feedforward network trained on only 1024 paired examples, the model achieves comparable generalization to the full transformer.
- **Core assumption:** Vocabulary embeddings encode relational structure (Paris is geometrically close to France) that can be exploited by even minimal architectures.
- **Evidence anchors:**
  - [Section 6, Table 7] Simplified architecture achieves 97.66% generalization on City→Country with correct mappings, but only 22.66% with shuffled mappings
  - [Section 6] When city-country correspondence is broken or reduced to first letters, generalization collapses
  - [corpus] "Quantifying Compositionality of Classic and State-of-the-Art Embeddings" examines whether embeddings exploit compositional meanings, relevant but not confirming the causality claim
- **Break condition:** Assumption: This causal attribution is based on ablation, not direct mechanistic proof of how embeddings encode relations.

## Foundational Learning

- **Next Token Prediction (NTP) as Knowledge Unit**
  - Why needed here: All analysis operates on logits from NTP tasks; understanding how prompts define "knowledge subdomains" is prerequisite.
  - Quick check question: Given prompt "X works for the company of," what vocabulary tokens constitute the meaningful output domain?

- **Linear Transformation Fitting (Regression)**
  - Why needed here: The paper fits W ∈ ℝ^|V|×|V| using partial logit pairs and evaluates on held-out pairs; understanding train/test split methodology is essential.
  - Quick check question: If you have 10K logit pairs (source, target), how would you fit W to minimize reconstruction error on held-out data?

- **Pearson Correlation as Distributional Similarity**
  - Why needed here: The paper uses label-wise (not instance-wise) Pearson correlation to eliminate bias term effects and evaluate W directly.
  - Quick check question: Why would instance-wise correlation be dominated by global bias b, and how does label-wise aggregation address this?

## Architecture Onboarding

- **Component map:** Input X → Prompt Template → Full Transformer OR Mean-Pool + FFN → Context Vector C ∈ ℝ^d → Logits = C · V^T (tied embeddings) → Fit (W, b) between logit pairs

- **Critical path:** Vocabulary embeddings V → Context encoding → Logit computation → W fitting → Correlation evaluation → Gradient analysis

- **Design tradeoffs:**
  - Using logits (not hidden states) for W fitting improves interpretability (weights map to vocabulary tokens directly) but discards information before the LM head
  - Label-wise correlation ignores instance-level bias but may miss input-specific patterns
  - Subdomain restriction (~100 tokens per relation) focuses analysis but excludes long-tail tokens

- **Failure signatures:**
  - High correlation + low W precision → compositional hallucination (e.g., X+1=3 → X+2=3, not 4)
  - Low W weight on ground-truth pair → failed generalization despite high correlation
  - Subword tokens → W precision collapses to near-zero

- **First 3 experiments:**
  1. **Reproduce correlation matrix:** Take LLaMA-3-8B, extract logits for 111 prompts across 4 families (attribute, cross-language, simile, math), fit W on 50% of X inputs, compute label-wise Pearson on held-out 50%. Verify City→Country correlation >0.85.
  2. **Test W precision:** For high-correlation pairs, compute Hit@Top-N (N=1,3,5) for whether correct target tokens receive highest W weights. Confirm City→Country ~0.42-0.67 vs. CEO→Company ~0.05-0.09.
  3. **Minimal architecture ablation:** Replace LLaMA-3-8B internals with mean-pooling + single FFN (random init), train on 1024 paired texts, evaluate generalization on 128 held-out X values. Expect >90% for City→Country with correct mappings, <25% with shuffled mappings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the formal theoretical explanation for why resilient linear correlations emerge in language models?
- **Basis in paper:** [Explicit] Appendix B states the authors do not provide a formal theory explaining why these correlations emerge, suggesting future work explore model architectures, optimization dynamics, and linguistic structures.
- **Why unresolved:** The current study is primarily empirical, characterizing the phenomenon and its connection to hallucination and generalization without deriving the mechanism from first principles.
- **What evidence would resolve it:** A theoretical framework that derives the existence of these linear transformations from the properties of transformer architectures or the gradient descent optimization process.

### Open Question 2
- **Question:** How do specific properties of the training data influence the formation of linear correlations?
- **Basis in paper:** [Explicit] Appendix B notes the study lacks a systematic analysis of how training data influences correlation formation and suggests investigating which data properties contribute to their emergence.
- **Why unresolved:** The paper analyzes pre-trained models (e.g., Llama 3) to discover existing correlations but does not track how specific data regimes during pre-training cause these correlations to form or degrade.
- **What evidence would resolve it:** Experiments that control the composition, frequency, and relational structure of the pre-training corpus and measure the resulting impact on the intensity and precision of the linear transformation W.

### Open Question 3
- **Question:** Can we develop a method to predict which arbitrary knowledge pairs will exhibit linear correlation before model inference?
- **Basis in paper:** [Explicit] Appendix B highlights that the authors "do not establish a general method to predict what knowledge pairs exhibit this property" and suggests developing theoretical or empirical criteria for identification.
- **Why unresolved:** The paper detects correlations post-hoc by fitting W using logit pairs, rather than proposing a predictive heuristic based on semantic or structural features of the relation types.
- **What evidence would resolve it:** A model or set of criteria that can accurately forecast high linear correlation for unseen relation types (e.g., distinct from Attribute, Math, or Simile families) based solely on the definition of the knowledge pair.

## Limitations

- **Scale and Generality:** Findings based on single model family (LLaMA-3-8B); unclear if phenomenon generalizes across architectures and scales
- **Precision Threshold Ambiguity:** Paper identifies need for "high correlation intensity and precision" but doesn't quantify minimum thresholds for successful generalization
- **Correlation vs. Causation:** Ablation shows vocabulary embeddings are sufficient but doesn't prove they are the sole or primary cause of compositional generalization

## Confidence

**High Confidence:**
- Linear correlations exist between related knowledge pairs in LLM logits (supported by multiple correlation measurements across different prompt families)
- These correlations persist through fine-tuning (demonstrated across model versions)
- W fitting works better on high-correlation pairs (quantitative comparison of Hit@Top-N scores)

**Medium Confidence:**
- Vocabulary representations are the primary substrate for compositional generalization (supported by ablation but lacks alternative architectural comparisons)
- Low W precision causes compositional hallucinations (mechanism proposed but not exhaustively validated across failure modes)
- Gradient propagation through W enables compositional generalization (correlation demonstrated but causal link not definitively proven)

**Low Confidence:**
- The phenomenon generalizes across all LLM architectures and scales (based on single model family)
- Linear correlations are the optimal or only way LMs handle composition (no comparison with alternative mechanisms)
- The minimal architecture truly isolates vocabulary representation effects (no control for pre-training data overlap or other confounds)

## Next Checks

1. **Cross-Model Generalization Test:** Apply the same correlation analysis to at least three different model families (e.g., LLaMA, GPT, BERT) across different scales (1B, 8B, 70B parameters). Verify whether City→Country correlations remain >0.85 and whether W precision patterns replicate. This would test whether the phenomenon is architecture-independent.

2. **Tokenization Impact Study:** Compare correlation and precision metrics between subword tokenization (BPE, SentencePiece) and whole-word tokenization on a controlled dataset where all tokens are whole words. If whole-word tokens show significantly higher W precision (>0.3 vs. <0.01), this would validate the subword limitation claim and suggest architectural modifications.

3. **Alternative Mechanism Comparison:** Implement and compare against at least two alternative compositional generalization mechanisms: (a) attention-based relational composition where attention heads learn to route between related concepts, and (b) sparse expert routing where different experts handle different knowledge subdomains. Measure whether these outperform or complement the linear correlation approach on the same benchmark tasks.