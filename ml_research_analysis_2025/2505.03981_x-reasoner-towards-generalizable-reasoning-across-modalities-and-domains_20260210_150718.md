---
ver: rpa2
title: 'X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains'
arxiv_id: '2505.03981'
source_url: https://arxiv.org/abs/2505.03981
tags:
- reasoning
- easoner
- multimodal
- medical
- text-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: X-Reasoner demonstrates that reasoning capabilities can be effectively
  transferred across modalities and domains through general-domain text-only post-training.
  The approach combines supervised fine-tuning on general-domain reasoning traces
  with reinforcement learning on mathematical tasks, enabling strong performance on
  multimodal and specialized benchmarks without requiring in-domain multimodal data.
---

# X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains

## Quick Facts
- **arXiv ID**: 2505.03981
- **Source URL**: https://arxiv.org/abs/2505.03981
- **Reference count**: 40
- **Primary result**: State-of-the-art performance on MMMU-Pro and MathVista through text-only post-training of VLMs

## Executive Summary
X-Reasoner demonstrates that reasoning capabilities can be effectively transferred across modalities and domains through general-domain text-only post-training. The approach combines supervised fine-tuning on general-domain reasoning traces with reinforcement learning on mathematical tasks, enabling strong performance on multimodal and specialized benchmarks without requiring in-domain multimodal data. X-Reasoner achieves state-of-the-art results on multiple challenging multimodal reasoning benchmarks including MMMU-Pro and MathVista, and its medical variant X-Reasoner-Med sets new records on medical text and multimodal tasks. The work shows that text-based reasoning training alone provides the majority of learning needed for effective multimodal reasoning, though domain-specific text data further improves specialized performance.

## Method Summary
X-Reasoner uses a two-stage post-training pipeline applied to instruction-tuned VLMs (Qwen2.5-VL-7B-Instruct). Stage 1 employs supervised fine-tuning with OpenThoughts-114k, a general-domain dataset of long chain-of-thought traces distilled from stronger models. Stage 2 applies reinforcement learning with verifiable rewards using Orz-math-57k, a math-only dataset. The approach leverages the hypothesis that structured reasoning patterns learned from text transfer effectively to multimodal tasks. A variant, X-Reasoner-Med, uses medical text data for domain adaptation. Training uses GRPO algorithm with binary accuracy rewards, token-level loss normalization, and forced-exiting mechanisms to prevent endless thinking.

## Key Results
- Achieves 54.9% on MMMU-Pro (new SOTA), surpassing prior methods by 2.8%
- Sets new record on MathVista at 73.2% with 5-shot inference
- X-Reasoner-Med achieves 68.1% on MMLU-Pro-Health (new SOTA) and 67.9% on OmniMedVQA
- Text-only training provides majority of reasoning capability, with domain-specific data providing additional gains
- Combines SFT+RL outperforms either method alone across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: General-domain text-only post-training can impart reasoning patterns that transfer across modalities and specialized domains.
- **Mechanism**: Long chain-of-thought (CoT) supervision during SFT teaches structured reasoning patterns (planning, self-verification, error correction) that are modality-agnostic. When applied to multimodal inputs at inference, the model applies these same reasoning templates while incorporating visual tokens as additional context.
- **Core assumption**: Reasoning is fundamentally a symbolic process that can be decoupled from perceptual encoding; vision-language models already possess sufficient visual grounding from pretraining.
- **Evidence anchors**:
  - [abstract] "General-domain text-based post-training can enable such strong generalizable reasoning."
  - [Section 2.1] SFT with OpenThoughts yields consistent gains across "cross-domain (X-domain), cross-modality (X-modality) and the intersection of the two axes."
  - [corpus] Related work LMM-R1 (arXiv:2503.07536) similarly finds two-stage rule-based RL empowers 3B LMMs, suggesting the pattern generalizes across model scales.

### Mechanism 2
- **Claim**: Mathematics serves as an "anchor domain" whose reasoning structure is sufficiently rich to support cross-domain and cross-modality transfer.
- **Mechanism**: Math tasks naturally require long, structured chains of symbolic manipulation with verifiable endpoints. This forces models to learn robust planning and verification behaviors that generalize to other domains requiring multi-step inference (e.g., medical diagnosis, scientific reasoning).
- **Core assumption**: The cognitive operations required for mathematical reasoning (decomposition, verification, backtracking) share sufficient structure with reasoning in other domains.
- **Evidence anchors**:
  - [Section 2.2, Table 3] Models trained on math data improve on both math benchmarks (GSM8K: +2.8%, MathVision: +3.4%) and out-of-domain medical tasks (MMLU-Pro-Health: +2.9%, OmniMedVQA: +1.0%). Conversely, MedQA training improves medical tasks but not math.
  - [Section 2.2] "Math-trained models consistently generate longer responses across benchmarks," suggesting learned persistence in reasoning.

### Mechanism 3
- **Claim**: Combining SFT with RL yields stronger generalization than either method alone.
- **Mechanism**: SFT efficiently acquires rich CoT patterns through supervised distillation, establishing structured reasoning templates. RL then refines these patterns through reward-guided exploration, improving accuracy while mitigating SFT-specific pathologies (endless thinking).
- **Core assumption**: SFT provides a stable initialization that makes RL exploration more efficient; RL's reward signal can improve upon distilled patterns without catastrophic forgetting.
- **Evidence anchors**:
  - [Section 2.2, Table 4] SFT+RL achieves 53.3% on MMLU-Pro vs. 50.4% (SFT only) and 50.3% (RL only). Similar patterns hold across cross-domain and cross-modality settings.
  - [Section 2.1] SFT alone causes "endless thinking" in ~17% of generations; RL training reduces this (Figure 5 shows decreasing clip ratio).

## Foundational Learning

- **Chain-of-Thought (CoT) Distillation**:
  - Why needed here: The SFT stage relies on distilling long CoT traces from stronger models (QwQ-32B, DeepSeek-R1). Understanding how to generate, filter, and format these traces is essential.
  - Quick check question: Can you explain the difference between CoT prompting at inference vs. CoT distillation during training?

- **Reinforcement Learning with Verifiable Rewards (RLVR)**:
  - Why needed here: The RL stage uses GRPO with binary accuracy rewards rather than learned reward models. Understanding why verifiable rewards reduce reward hacking is critical.
  - Quick check question: Why might a learned reward model be more susceptible to "reward hacking" than verifiable rewards?

- **Cross-Modal Transfer**:
  - Why needed here: The paper's central claim depends on reasoning transferring from text to vision. Understanding modality-agnostic vs. modality-specific representations helps interpret results.
  - Quick check question: What evidence would suggest a model is using visual information vs. text-only shortcuts on a multimodal benchmark?

## Architecture Onboarding

- **Component map**:
  Base VLM (Qwen2.5-VL-7B-Instruct) -> SFT (OpenThoughts-114k, 4 epochs, LR 1e-5) -> RLVR (Orz-math-57k, GRPO, 8 rollouts/query) -> Inference (forced-exiting at 4096 tokens)

- **Critical path**:
  1. Start with instruction-tuned VLM (not base)
  2. Apply SFT with long CoT traces (enables reasoning patterns)
  3. Apply RL with math data (refines patterns, reduces endless thinking)
  4. (Optional) Domain adaptation via continued text-only training (X-Reasoner-Med)

- **Design tradeoffs**:
  - Text-only vs. multimodal training data: Text-only is compute-efficient and avoids curation complexity but may underperform on tasks requiring visual reasoning patterns not present in text.
  - Math vs. domain-specific RL: Math provides broader generalization; domain data provides higher in-domain performance (Table 3).
  - Instruction-tuned vs. base VLM: Paper acknowledges base models may better incentivize reasoning emergence but uses instruction-tuned due to computational constraints and availability.

- **Failure signatures**:
  - **Endless thinking**: Model generates indefinitely without termination (~17% post-SFT). Fix: forced-exiting mechanism at token threshold.
  - **Text-only shortcuts**: Model solves multimodal tasks without using visual input. Detection: mask images and compare performance (Table 6 ablation).
  - **RL instability**: GRPO training may diverge. Mitigations: clip-higher, token-level loss normalization, reduced KL penalty.

- **First 3 experiments**:
  1. **Reproduce SFT-only baseline**: Train on OpenThoughts-114k, evaluate on MMMU-Pro and MMLU-Pro. Confirm ~2-3% gains over baseline. Check endless thinking rate.
  2. **Ablate training data domain**: Compare RL on math (Orz) vs. medical (MedQA) data. Verify math training improves both domains while medical training only improves medical (replicate Table 3).
  3. **Test cross-modal transfer authenticity**: Mask visual inputs on MathVista/MMMU, identify text-solvable examples, and verify X-Reasoner maintains gains on remaining examples (replicate Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would initializing from a pretrained base model (rather than an instruction-tuned VLM) yield stronger or more robust reasoning capabilities with X-Reasoner's training recipe?
- Basis in paper: [explicit] Section 5 states: "Prior studies have emphasized the importance of starting from pretrained base checkpoints, rather than instruction-tuned models... However, in our case, we are constrained by computational resources and the lack of base VLMs in the recent open-source releases, limiting our ability to empirically validate these claims."
- Why unresolved: The authors could not test this due to lack of available base VLMs and computational constraints.
- What evidence would resolve it: Train X-Reasoner from both base and instruction-tuned checkpoints with identical hyperparameters and compare performance across benchmarks.

### Open Question 2
- Question: Does X-Reasoner's reasoning generalization hold for open-ended generation, interactive dialogue, and instruction-following tasks beyond multiple-choice QA?
- Basis in paper: [explicit] Section 5 notes: "Our evaluation primarily targets mathematical questions and multiple-choice questions across general and medical domains... In particular, we have not tested our approach on open-ended generation, interactive dialogue, or instruction-following scenarios."
- Why unresolved: The benchmarks used focus on verifiable, structured outputs; free-form reasoning tasks remain untested.
- What evidence would resolve it: Evaluate X-Reasoner on open-ended benchmarks (e.g., MT-Bench, open-domain dialogue) and report performance relative to baselines.

### Open Question 3
- Question: Would continued in-domain multimodal SFT or RL further enhance X-Reasoner's domain-specific performance beyond text-only domain adaptation?
- Basis in paper: [explicit] Conclusion states: "We leave to future work the exploration of continued in-domain multimodal SFT/RL to further strengthen domain-specific and multimodal reasoning capabilities."
- Why unresolved: X-Reasoner-Med uses only text-only medical data; the incremental benefit of multimodal domain data is unknown.
- What evidence would resolve it: Train variants with medical multimodal data (e.g., medical images with CoT) and compare against X-Reasoner-Med.

### Open Question 4
- Question: Why does mathematics specifically serve as such an effective "anchor domain" for promoting generalizable reasoning across domains and modalities?
- Basis in paper: [inferred] Section 2.2 observes that math-trained models generalize better than domain-specific (e.g., MedQA) training, but the underlying mechanism is hypothesized rather than rigorously explained.
- Why unresolved: The paper demonstrates the phenomenon empirically but does not isolate which properties of math data (structured reasoning, answer verifiability, chain length) drive transfer.
- What evidence would resolve it: Conduct controlled ablations varying reasoning structure length, verifiability, and domain specificity to identify causal factors.

## Limitations

- **Computational constraints**: Used instruction-tuned VLMs rather than base models, potentially limiting reasoning capability emergence.
- **Benchmark scope**: Evaluation focused on mathematical and multiple-choice tasks, leaving open-ended reasoning capabilities untested.
- **Text-only domain adaptation**: Medical variant uses only text data, missing potential benefits of multimodal domain-specific training.

## Confidence

- **High Confidence**: SFT+RL consistently outperforms either method alone across benchmarks (Table 4)
- **Medium Confidence**: Math as "anchor domain" shows directional transfer benefits but lacks systematic comparison with other domains
- **Medium Confidence**: Cross-modal transfer claims rely on comparative performance rather than direct attribution analysis
- **Low Confidence**: Claim that 94.6% of MMMU examples can be solved with text-only information seems unusually high and needs validation

## Next Checks

1. **Direct Attribution Analysis**: Mask visual inputs during inference on MathVista and MMMU-Pro, then analyze whether X-Reasoner's gains persist on examples that genuinely require visual information (not just text-solvable problems).

2. **Domain Transfer Systematic Study**: Conduct controlled experiments comparing math vs. medical vs. scientific vs. legal RL training data to systematically identify which domains provide optimal transfer benefits and under what conditions.

3. **Base Model Investigation**: Replicate the full pipeline starting from base VLMs rather than instruction-tuned models to test whether reasoning capabilities emerge more naturally from base models.