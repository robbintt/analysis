---
ver: rpa2
title: 'AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated
  Question Generation and Interactive Assessment'
arxiv_id: '2509.23811'
source_url: https://arxiv.org/abs/2509.23811
tags:
- learning
- dataset
- bloom
- difficulty
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AnveshanaAI, a multimodal platform designed
  for adaptive AI/ML education through automated question generation and interactive
  assessment. It addresses the challenge of fragmented learning resources by integrating
  dynamic question generation, adaptive assessment, and simulation-driven features
  within a unified ecosystem.
---

# AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment

## Quick Facts
- arXiv ID: 2509.23811
- Source URL: https://arxiv.org/abs/2509.23811
- Reference count: 5
- Primary result: Automated question generation with 1.3 validation perplexity and BERTScore F1=0.427

## Executive Summary
AnveshanaAI is a multimodal platform designed to address fragmented AI/ML learning resources through automated question generation, adaptive assessment, and interactive simulation features. The system constructs a dataset of over 10,000 annotated QA pairs across 26 categories and employs fine-tuned language models to generate educational content. Experiments demonstrate stable model convergence, broad dataset coverage, and measurable improvements in learner engagement compared to traditional static repositories.

## Method Summary
The platform uses Mistral-7B v0.1 with 4-bit quantization and LoRA adapters for fine-tuning on a dataset of 10,845 QA pairs annotated with Bloom's taxonomy levels and difficulty metadata. Training employs HuggingFace Trainer with batch_size=2/device, learning rate=2e-5, weight_decay=0.01, and runs for 5 epochs on a single 20GB GPU. The model achieves validation perplexity of 1.3 and BERTScore F1=0.427, with evaluation metrics showing 97% Bloom's taxonomy utilization and semantic similarity between 0.6-0.8.

## Key Results
- Dataset construction achieved 97% coverage across all six Bloom's taxonomy levels
- Model fine-tuning stabilized with validation perplexity of 1.3
- BERTScore evaluation yielded F1=0.427 with Precision=0.289 and Recall=0.818
- Learner engagement metrics showed measurable improvements over static repositories

## Why This Works (Mechanism)
The platform leverages fine-tuned language models to generate pedagogically structured questions that align with cognitive learning objectives through Bloom's taxonomy annotations. The multimodal approach integrates text generation with simulation and gamification features to create an adaptive learning environment that responds to individual learner needs and preferences.

## Foundational Learning
- Bloom's Taxonomy Levels (Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation): Why needed - provides structured framework for educational objectives; Quick check - verify all six levels are represented in dataset with appropriate question types
- LoRA (Low-Rank Adaptation): Why needed - enables efficient fine-tuning of large models on limited hardware; Quick check - confirm adapter parameters are correctly applied during training
- BERTScore Evaluation: Why needed - measures semantic similarity between generated and reference answers; Quick check - validate that evaluation uses same prompt format as training

## Architecture Onboarding
**Component Map:** Dataset -> Fine-tuning Pipeline -> Adaptive Assessment -> Simulation/Gamification -> Learner Analytics
**Critical Path:** QA Pair Annotation → Model Fine-tuning → Question Generation → Adaptive Delivery → Engagement Tracking
**Design Tradeoffs:** Single GPU constraint necessitated 4-bit quantization and LoRA, sacrificing some model capacity for accessibility
**Failure Signatures:** OOM errors indicate need for sequence length reduction; perplexity divergence suggests learning rate or LoRA configuration issues
**First Experiments:** 1) Load and validate dataset schema integrity, 2) Test model loading with 4-bit quantization, 3) Run single batch training to verify pipeline functionality

## Open Questions the Paper Calls Out
### Open Question 1
Does the integration of simulation and competitive features in AnveshanaAI result in measurable improvements in long-term learner retention compared to static repositories? The authors pose RQ3 regarding how these features enhance "long-term engagement," and the abstract claims "measurable gains in learner engagement," but the results section focuses solely on dataset metrics and model convergence.

### Open Question 2
Does the low precision (0.289) and high recall (0.818) of the fine-tuned model indicate pedagogically beneficial elaboration or harmful verbosity and hallucination? The authors interpret the low precision as "extended explanatory richness" beneficial for education, but automated metrics cannot distinguish between helpful context and incoherent "noise."

### Open Question 3
How robust is the multimodal "Viva Mode" in handling domain-specific jargon and varying student accents during oral assessments? The methodology mentions a "Viva Mode" using Whisper-based ASR, but the experimentation section restricts evaluation to text-based metrics, ignoring the speech-to-text error rates.

## Limitations
- Critical LoRA configuration parameters (rank, alpha, dropout, target modules) are unspecified
- BERTScore evaluation protocol and checkpoint selection not detailed
- Single CSV file format may complicate standard dataset loading procedures
- Significant precision-recall gap in evaluation metrics raises data quality concerns

## Confidence
- **High confidence**: Core methodology of using fine-tuned LLMs for automated question generation with Bloom's taxonomy annotations
- **Medium confidence**: Training setup with Mistral-7B v0.1, 4-bit quantization, and LoRA parameters are reasonable but lack specific configuration details
- **Low confidence**: Specific metric values and their interpretation due to significant precision-recall gap and unclear validation protocols

## Next Checks
1. Reconstruct the exact prompt template and tokenization strategy used during training to ensure generated questions match expected format, then evaluate reproduction of BERTScore F1=0.427
2. Implement and test multiple LoRA configurations (varying rank, alpha, dropout, target modules) to identify configuration achieving validation perplexity closest to 1.3
3. Validate Bloom's taxonomy utilization metric by analyzing distribution of generated questions across all six taxonomy levels and comparing against claimed 97% coverage