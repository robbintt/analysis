---
ver: rpa2
title: 'Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study
  for developing Natural Language-Based BIM Information Retrieval Systems'
arxiv_id: '2508.05676'
source_url: https://arxiv.org/abs/2508.05676
tags:
- 'true'
- query
- information
- building
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two NLP approaches for BIM information retrieval:
  domain-specific fine-tuning and prompt-based learning. The authors construct a BIM-specific
  dataset with 1,740 annotated queries across 69 models and evaluate both methods
  using a two-stage framework (intent recognition and table-based QA).'
---

# Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems

## Quick Facts
- arXiv ID: 2508.05676
- Source URL: https://arxiv.org/abs/2508.05676
- Reference count: 0
- Primary result: Hybrid approach (XLNet + GPT-4o) achieves 90.80% accuracy for BIM information retrieval, outperforming either method alone

## Executive Summary
This paper compares domain-specific fine-tuning versus prompt-based learning for natural language BIM information retrieval systems. The authors construct a BIM-specific dataset with 1,740 annotated queries across 69 models and evaluate both approaches using a two-stage framework (intent recognition and table-based QA). Results demonstrate that domain-specific fine-tuning excels at rigid classification tasks while prompt-based learning performs better at flexible reasoning tasks. A hybrid architecture combining XLNet for intent recognition with GPT-4o for QA achieves the highest overall accuracy (90.80%), though performance degrades significantly with large-scale BIM models due to transformer context limitations.

## Method Summary
The study constructs a BIM-specific dataset with 1,740 annotated queries across 69 BIM models, mapping natural language queries to structured IFC sub-databases (CSV tables). Two approaches are evaluated: domain-specific fine-tuning using XLNet for intent classification and prompt-based learning using GPT-4o for table-based QA. A two-stage framework processes queries through intent recognition (mapping to specific BIM sub-databases) followed by table-based question answering. The hybrid approach routes intent classification through XLNet and QA through GPT-4o, leveraging each method's strengths. Case studies with BIM models of varying complexity reveal scalability challenges when processing large tabular contexts.

## Key Results
- XLNet achieves 99.14% accuracy for intent recognition, significantly outperforming GPT-4o (47.13%)
- GPT-4o achieves 91.95% accuracy for table-based QA, outperforming TAPAS (86.49%)
- Hybrid approach (XLNet + GPT-4o) achieves highest overall accuracy of 90.80%
- Performance degrades with large-scale models (>200 rows) due to transformer context limitations
- Large-scale teaching facility model shows 60% failure rate due to context window truncation

## Why This Works (Mechanism)

### Mechanism 1: Task-Decoupled Hybridization
A hybrid architecture combining fine-tuned intent classification with prompt-based reasoning optimizes accuracy better than uniform application. Fine-tuning (XLNet) creates robust decision boundaries for rigid classification, while prompt-based learning (GPT-4o) leverages latent reasoning for flexible generation. The assumption is that fine-tuned representations for intent are more robust than LLM in-context learning for classification.

### Mechanism 2: Attention Saturation in Long Contexts
Transformer-based QA performance degrades non-linearly with tabular input size due to quadratic attention complexity. When BIM sub-databases exceed effective context windows (>200 rows), models attend to partial segments rather than aggregating across full datasets. The assumption is that models prioritize local coherence over global correctness.

### Mechanism 3: Supervision for Semantic Disambiguation
Domain-specific fine-tuning resolves entity ambiguity better than few-shot prompting when label overlap exists. Explicit negative sampling during fine-tuning teaches models to distinguish overlapping database labels, while few-shot learning relies on shallow pattern matching. The assumption is that ambiguity is systematic enough to be learned via weights.

## Foundational Learning

- **Concept: Two-Stage NLI Pipeline (Intent â†’ QA)**
  - Why needed: Treating "understanding what to look for" and "finding the answer" as separate problems allows optimizing distinct models, avoiding the "jack of all trades" penalty
  - Quick check: Can you explain why a generic LLM might fail to distinguish between querying a "Door" entity vs. "Space" entity if the Door table contains Space IDs?

- **Concept: IFC Data Structure (Sub-databases)**
  - Why needed: The mechanism relies on mapping natural language to structured IFC classes. Understanding that BIM models are relational databases, not just 3D geometry, is prerequisite to designing retrieval logic
  - Quick check: How does the segmentation of a BIM model into CSV-style sub-databases constrain the types of queries the system can handle?

- **Concept: Context Window vs. Data Volume**
  - Why needed: The failure in Case Study 3 highlights the hard constraint of transformer context length. Distinguishing between "reasoning capability" and "context capacity" is vital for diagnosing scalability
  - Quick check: Why does increasing the rows in a BIM database cause an LLM to return "incomplete subsets" rather than just slowing down?

## Architecture Onboarding

- **Component map:** User Text Query -> Intent Classifier (XLNet) -> Retriever (loads table subset) -> Table QA (GPT-4o/TAPAS) -> Answer

- **Critical path:**
  1. Intent Accuracy: If XLNet misclassifies intent, subsequent QA step fails (Garbage In)
  2. Table Size: If retrieved table > ~200 rows, QA accuracy drops precipitously

- **Design tradeoffs:**
  - XLNet vs GPT-4o for Intent: XLNet offers +50% accuracy but requires labeled data; GPT-4o is zero-effort but fails on semantic ambiguity
  - TAPAS vs GPT-4o for QA: TAPAS is lighter for strict aggregation; GPT-4o is robust for comparative reasoning but expensive and capped by context length

- **Failure signatures:**
  - Intent Failure: Queries wrong table (e.g., looks in Door instead of Space table)
  - Context Overflow: Answers correct for partial view but wrong for whole building
  - Incomplete List: Returns only first 5 items found in context window

- **First 3 experiments:**
  1. Intent Threshold Test: Train XLNet and verify confusion matrix for overlapping entities to establish baseline for hybrid router
  2. Token Limit Stress Test: Feed GPT-4o synthetic BIM table with increasing rows (50, 100, 200, 500) to measure exact break point
  3. Hybrid Integration Test: Build XLNet-to-GPT-4o pipeline and test on medium-sized model to validate error isolation

## Open Questions the Paper Calls Out

- Can automatic table partitioning strategies effectively mitigate LLM performance degradation when processing large-scale BIM tabular data?
- How can the proposed two-stage framework be extended to support advanced BIM tasks such as automated building code compliance checking?
- What architectural or methodological improvements are required to ensure robustness of NLI systems in large-scale BIM projects?

## Limitations

- Context Window Constraints: Exact threshold and scaling behavior of GPT-4o's degradation with table size remain unquantified
- Generalization Gap: Dataset comprises only 69 models; effectiveness across diverse architectural typologies and regional standards unverified
- Fine-tuning Data Dependency: XLNet's superior performance relies on 1,740 annotated queries; scaling and performance with reduced data unaddressed

## Confidence

- High Confidence: Comparative performance metrics between XLNet and GPT-4o for intent recognition (99.14% vs 47.13%) and hybrid approach's overall accuracy (90.80%)
- Medium Confidence: Claim about transformer limitations with long tabular contexts based on Case Study 3 observations but lacks systematic degradation curve measurement
- Low Confidence: Assertion that prompt-based models "struggle to determine whether to retrieve information" based on qualitative observations rather than quantitative error analysis

## Next Checks

1. Context Window Breakpoint Analysis: Systematically test GPT-4o's QA accuracy across BIM tables with 50, 100, 200, 500, and 1000 rows to establish precise relationship between table size and performance degradation

2. Alternative Retrieval Architecture Validation: Implement chunking + aggregation strategy where large tables are divided into manageable segments with coordinating agent, then compare accuracy and latency against single-context baseline

3. Cross-Schema Generalization Test: Evaluate hybrid pipeline on BIM models from different countries using varying IFC standards (IFC2x3 vs IFC4) and architectural typologies to measure performance variance and identify schema-specific failure modes