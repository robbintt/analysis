---
ver: rpa2
title: Strong Duality and Dual Ascent Approach to Continuous-Time Chance-Constrained
  Stochastic Optimal Control
arxiv_id: '2511.19451'
source_url: https://arxiv.org/abs/2511.19451
tags:
- problem
- control
- dual
- optimal
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a continuous-time, continuous-space chance-constrained
  stochastic optimal control (SOC) problem where the probability of failure to satisfy
  state constraints is explicitly bounded. The authors leverage the notion of exit
  time from continuous-time stochastic calculus to formulate the chance-constrained
  SOC problem without any conservative approximation.
---

# Strong Duality and Dual Ascent Approach to Continuous-Time Chance-Constrained Stochastic Optimal Control

## Quick Facts
- arXiv ID: 2511.19451
- Source URL: https://arxiv.org/abs/2511.19451
- Reference count: 40
- Primary result: Strong duality holds between primal chance-constrained SOC and its dual under structural assumptions, enabling efficient path integral solution via dual ascent

## Executive Summary
This paper presents a novel approach to continuous-time chance-constrained stochastic optimal control that avoids conservative approximations by reformulating the chance constraint using exit-time theory. The key innovation is leveraging the notion of exit time from continuous-time stochastic calculus to transform the chance constraint into an exact expectation of an indicator function, which can be incorporated into the cost function through a dual formulation. Under specific structural assumptions on system dynamics and cost function, strong duality holds between the primal constrained problem and its dual unconstrained problem, enabling efficient numerical solution via path integral sampling.

## Method Summary
The method formulates a continuous-time, continuous-space chance-constrained stochastic optimal control problem where the probability of failure to satisfy state constraints is explicitly bounded. The chance constraint is transformed into an expectation of an indicator function using exit-time theory, which can be incorporated into the cost function by considering a dual formulation. The dual function is expressed in terms of the solution to a Hamilton-Jacobi-Bellman (HJB) partial differential equation parameterized by the dual variable. Under a certain assumption on the system dynamics and cost function, strong duality holds between the primal chance-constrained problem and its dual. The path integral approach is utilized to numerically solve the dual problem via gradient ascent using open-loop samples of system trajectories.

## Key Results
- Strong duality holds between primal chance-constrained SOC and its dual under Assumptions 1-3
- Path integral approach solves dual via gradient ascent using only uncontrolled trajectory samples
- Simulation studies show effective chance-constrained motion planning for mobile robot navigation
- Solution accuracy compared favorably with finite difference method while offering better scalability

## Why This Works (Mechanism)

### Mechanism 1: Dual Formulation Removes Conservative Approximation
The chance constraint is incorporated into the objective without conservative bounding (e.g., Boole's inequality), preserving optimality. The probability of failure is exactly rewritten as an expectation of an indicator function via exit-time theory, joining the terminal cost in a unified boundary condition. Core assumption: exit time is well-defined (bounded safe set, regular dynamics).

### Mechanism 2: Strong Duality Enables Soft-Constraint Equivalence
Under structural conditions, the primal constrained problem and the dual unconstrained problem have equal optimal values. Assumption 1 aligns noise covariance with control authority, linearizing the HJB PDE. Assumptions 2-3 ensure Slater-like conditions and continuity of failure probability, guaranteeing strong duality.

### Mechanism 3: Path Integral Sampling Solves Dual via Uncontrolled Trajectories
The dual objective and optimal control can be evaluated from samples of uncontrolled dynamics, avoiding global PDE discretization. The Feynman-Kac formula represents the linearized HJB solution as an expectation over uncontrolled trajectories. GPU-parallel Monte Carlo replaces global grid-based solvers.

## Foundational Learning

- **Stochastic Differential Equations (Itô Calculus):**
  - Why needed: Dynamics are formulated as Itô SDEs; Dynkin's formula underpins verification theorem and risk estimation
  - Quick check question: Can you derive the Itô differential of J(x(t), t) for a given value function?

- **Dynamic Programming & HJB PDEs:**
  - Why needed: The dual subproblem is solved via the HJB PDE; linearization via exponential transform is central
  - Quick check question: What boundary condition does the value function satisfy at the safe set boundary?

- **Duality Theory in Optimization:**
  - Why needed: Understanding Lagrangian relaxation, Slater's condition, and complementary slackness is essential to interpret strong duality
  - Quick check question: If the primal is non-convex, under what conditions can strong duality still hold?

## Architecture Onboarding

- **Component map:** Primal Formulation -> Dual Problem -> HJB Solver -> Risk Estimator -> Dual Ascent
- **Critical path:**
  1. Verify Assumption 1 for your system (noise-control alignment)
  2. Initialize η = 0; compute P_fail via sampling
  3. If P_fail > Δ, iterate dual ascent (Algorithm 1)
  4. Return optimal control u*(·; η*) from path integral formula
- **Design tradeoffs:**
  - Finite Difference vs. Path Integral: FDM provides smooth global solutions but scales poorly with dimension; path integral is local, noisy, but parallelizable
  - Sample count vs. accuracy: More samples reduce variance but increase computation
  - Step size γ in dual ascent: Too large causes oscillation; too small slows convergence
- **Failure signatures:**
  - Non-convergence of η: Likely violation of Assumption 1 or discontinuous P_fail(η)
  - High-variance control estimates: Insufficient samples or degenerate importance weights
  - Conservative solutions: If duality gap is non-zero, the dual may underachieve primal optimum
- **First 3 experiments:**
  1. 2D input velocity model with simple obstacle (as in Fig. 2); verify η* vs. Δ trend
  2. Ablation on Assumption 1: Deliberately mismatch Σ and G and observe duality gap
  3. Scalability test: Compare path integral and FDM runtime on 2D vs. 5D car model; profile GPU vs. CPU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what mild conditions is the function mapping the dual variable to the probability of failure, η → P_fail(x_0, t_0, u*(x, t; η)), continuous?
- Basis in paper: Section IV.B states regarding Assumption 3: "We conjecture that this assumption is valid under mild conditions; a formal analysis is postponed as future work."
- Why unresolved: This continuity is necessary for proving complementary slackness and strong duality in the non-convex setting, but is currently assumed rather than derived.
- What evidence would resolve it: A formal mathematical proof establishing continuity of the failure probability function for general classes of system dynamics and cost functions.

### Open Question 2
- Question: How can strong duality be achieved for system dynamics that do not satisfy the restrictive noise-control coupling condition?
- Basis in paper: The Conclusion notes: "In future work, we plan to find alternatives in order to get rid of this restrictive assumption (Assumption 1)."
- Why unresolved: The current proof of strong duality relies on Assumption 1, which forces control cost to be tuned based on noise variance, limiting generality.
- What evidence would resolve it: A derivation of strong duality property or modified algorithmic framework that functions without the specific relationship between diffusion and control cost matrices.

### Open Question 3
- Question: What is the sample complexity of the proposed path integral control method?
- Basis in paper: The Conclusion states the intention to "conduct the sample complexity analysis of the path integral control in order to investigate how the accuracy of Monte Carlo sampling affects the solution."
- Why unresolved: While the algorithm is shown to be effective in simulations, the theoretical relationship between number of Monte Carlo samples and accuracy of dual variable/optimal policy is undetermined.
- What evidence would resolve it: Theoretical bounds quantifying estimation error and convergence rate of dual ascent algorithm as function of number of trajectory samples.

### Open Question 4
- Question: Can the proposed numerical framework be extended to systems with inherently unstable uncontrolled dynamics?
- Basis in paper: Section V states: "Extending the proposed path integral and HJB–Feynman–Kac–based framework to systems with inherently unstable uncontrolled dynamics would require additional care..."
- Why unresolved: The derivation and numerical stability of the Feynman-Kac representation rely on the behavior of the uncontrolled process, which is assumed stable in the provided examples.
- What evidence would resolve it: A modification of the control cost function design or the numerical integration scheme that guarantees stability for unstable uncontrolled drift dynamics.

## Limitations
- The requirement that noise and control channels be aligned (Assumption 1) is restrictive and may not hold in many practical systems
- Strong duality relies on Assumptions 1-3; violations may lead to non-zero duality gap with limited discussion of failure modes
- Path integral approach may suffer from high variance in trajectory rewards, especially for systems with rapid escape from safe regions

## Confidence

- **High confidence**: Path integral methodology for solving linearized HJB equations and exact reformulation of chance constraints via exit-time theory are well-grounded in stochastic calculus and dynamic programming literature
- **Medium confidence**: Strong duality result under Assumptions 1-3 is mathematically sound but practical applicability is limited by restrictive nature of these assumptions
- **Medium confidence**: Simulation results demonstrate the approach on simple 2D and 5D navigation problems, but scalability to high-dimensional systems and complex constraints remains to be thoroughly validated

## Next Checks

1. **Assumption 1 sensitivity analysis**: Systematically vary the noise-to-control ratio λ and quantify the impact on duality gap and solution quality
2. **High-variance regime testing**: Evaluate algorithm performance on systems with rapid exit times or high noise levels to assess sample complexity and importance weight degeneracy
3. **Assumption violation study**: Intentionally construct systems where Assumption 1 fails and measure the resulting duality gap to understand robustness limits