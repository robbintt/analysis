---
ver: rpa2
title: How AI Agents Follow the Herd of AI? Network Effects, History, and Machine
  Optimism
arxiv_id: '2512.11943'
source_url: https://arxiv.org/abs/2512.11943
tags:
- agents
- network
- participation
- historical
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language model (LLM)-based agents\
  \ navigate network-effect games where individual payoffs depend on peer participation.\
  \ Using a conference-attendance framework, we test three Qwen models (max, turbo,\
  \ 1.5B) across static and repeated game settings with four price trajectories (fixed,\
  \ ascending, descending, random) and two network-effect strengths (\u03B2=0.25,\
  \ \u03B2=0.75)."
---

# How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism

## Quick Facts
- arXiv ID: 2512.11943
- Source URL: https://arxiv.org/abs/2512.11943
- Authors: Yu Liu; Wenwen Li; Yifan Dou; Guangnan Ye
- Reference count: 2
- Key outcome: This study investigates how large language model (LLM)-based agents navigate network-effect games where individual payoffs depend on peer participation. Using a conference-attendance framework, we test three Qwen models (max, turbo, 1.5B) across static and repeated game settings with four price trajectories (fixed, ascending, descending, random) and two network-effect strengths (β=0.25, β=0.75). Without historical data, agents fail to converge toward theoretical equilibrium, showing significant expectation dispersion. Under weak network effects (β=0.25), ordered price trajectories enable partial convergence to theoretical participation levels. However, strong network effects (β=0.75) trigger persistent "AI optimism" - agents overestimate participation despite contradictory historical evidence. Random price sequences prevent convergence entirely, revealing that temporal coherence in historical data critically shapes LLM reasoning. These findings demonstrate that equilibrium outcomes in AI-mediated systems depend not just on incentives but on how history is structured, highlighting a fundamental divergence from human reasoning patterns.

## Executive Summary
This study examines how LLM-based agents coordinate in network-effect games where individual payoffs depend on collective participation. Using a conference attendance framework with three Qwen model variants, the research reveals that agents fail to converge to theoretical equilibrium without historical data, showing significant expectation dispersion. Under weak network effects, ordered price trajectories enable partial learning and convergence, while strong network effects trigger persistent "AI optimism" causing agents to overestimate participation despite contradictory evidence. Critically, random price sequences prevent convergence entirely, demonstrating that temporal coherence in historical data is essential for LLM reasoning—a fundamental divergence from human coordination patterns.

## Method Summary
The study simulates a 6-agent network-effect game where LLM agents decide whether to participate (attend a conference) based on individual standalone values (θ=1-6) and network effect strength (β=0.25 or 0.75). The manager agent sets prices and aggregates participation decisions, sharing only total historical participation counts. Experiments run in two phases: static (10 one-shot iterations without history) and repeated (iterative rounds with evolving prices). Four price trajectories are tested: fixed, ascending, descending, and random. Each trajectory is independently repeated 10 times. Agent expectations of peer participation (N) are recorded and compared to theoretical Fulfilled Expectation Equilibrium (FEE) predictions.

## Key Results
- Without historical data, agents show significant expectation dispersion and fail to converge toward theoretical equilibrium
- Under weak network effects (β=0.25), ordered price trajectories enable partial convergence to theoretical participation levels
- Strong network effects (β=0.75) trigger persistent "AI optimism," causing agents to systematically overestimate participation despite contradictory historical evidence
- Random price sequences prevent convergence entirely, demonstrating that temporal coherence in historical data critically shapes LLM reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ordered historical trajectories enable LLM agents to infer price-participation relationships under weak network effects, partially converging toward theoretical equilibrium.
- Mechanism: LLMs leverage in-context learning from sequentially structured price-participation data to update beliefs about peer behavior. When prices follow coherent patterns (ascending/descending), agents extract causal relationships between cost and expected participation, aligning expectations with the fulfilled expectation equilibrium (FEE).
- Core assumption: LLMs can perform implicit Bayesian-like belief updating when historical data exhibits temporal regularity.
- Evidence anchors:
  - [abstract]: "ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects"
  - [section 5.3]: "as rounds progress and historical participation data accumulates, their mean expectations... slope downward, aligning with the theoretical trend"
  - [corpus]: Weak corpus support—neighbor paper "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems" addresses peer influence but not temporal history effects specifically.
- Break condition: Convergence fails when network effect strength (β) exceeds ~0.5, triggering optimism bias that overrides historical learning.

### Mechanism 2
- Claim: Strong network effects induce persistent "AI optimism," causing agents to systematically overestimate participation even when historical evidence contradicts their beliefs.
- Mechanism: High β values amplify perceived collective benefits in the utility function, making agents overweight potential gains from coordination. This creates a bias where agents interpret ambiguous signals optimistically, discounting negative historical feedback.
- Core assumption: LLMs' training on human coordination narratives biases them toward pro-social, optimistic equilibria.
- Evidence anchors:
  - [abstract]: "strong effects (β=0.75) triggered persistent 'AI optimism'—agents overestimated participation despite contradictory evidence"
  - [section 5.2]: "LLMs inherently favor coordination optimism... amplified network benefits (β) appear to override rational inference from past outcomes"
  - [corpus]: "Financial Stability Implications of Generative AI" finds AI agents make more rational decisions than humans—but this paper finds the opposite under strong network effects, suggesting context-dependence.
- Break condition: Optimism persists even at extreme prices (p=5.99, where FEE predicts N=1, agents expect N≈4); only breaks when prices drop substantially combined with high β creating coincidental alignment.

### Mechanism 3
- Claim: LLM agents require temporally coherent historical sequences to learn equilibrium behavior; randomized histories eliminate convergence capacity.
- Mechanism: Unlike humans who maintain causal models independent of presentation order, LLMs process history as token sequences where ordering signals relationships. Randomized sequences obscure the price→participation causal link, leaving agents without extractable patterns.
- Core assumption: LLM reasoning depends on surface-level sequential patterns rather than abstracted causal models.
- Evidence anchors:
  - [abstract]: "Randomized price histories disrupted convergence entirely, demonstrating that temporal coherence in data shapes LLM reasoning unlike humans"
  - [section 5.5]: "random price fluctuations obscure causal relationships between cost and participation, leaving agents unable to generalize patterns from disjointed historical snapshots"
  - [corpus]: Limited direct corpus support for temporal coherence specifically in LLM game-theoretic contexts.
- Break condition: Any history curation that disrupts monotonicity or clear directional trends prevents learning, regardless of information completeness.

## Foundational Learning

- Concept: **Fulfilled Expectations Equilibrium (FEE)**
  - Why needed here: The theoretical benchmark against which agent behavior is measured; assumes agents coordinate on shared expectations of participation that exactly match realized outcomes.
  - Quick check question: Can you explain why FEE requires homogeneous expectations among rational agents?

- Concept: **Network Effects (β coefficient)**
  - Why needed here: The core structural parameter that determines whether coordination is easy (weak β) or prone to optimism bias (strong β); drives the utility function U = θ + βN - p.
  - Quick check question: If β increases from 0.25 to 0.75, how should rational agents adjust their participation threshold at a given price?

- Concept: **In-Context Learning in LLMs**
  - Why needed here: The mechanism by which LLMs incorporate historical data without weight updates; explains why history formatting and ordering critically shape outcomes.
  - Quick check question: What distinguishes in-context learning from gradient-based learning, and why might ordering matter more for the former?

## Architecture Onboarding

- Component map:
  - Manager agent -> sets prices, aggregates participation, broadcasts historical counts
  - Scholar agents (6) -> each with standalone value θ∈{1,2,3,4,5,6}, receives price + aggregate history, outputs participation expectation N
  - Utility calculator -> applies U = θ + βN - p, determines binary participation decision
  - History buffer -> stores price-participation trajectories, formatted for prompt injection

- Critical path:
  1. Configure β (0.25 or 0.75) and assign θ values
  2. Select price trajectory type (fixed/ascending/descending/random)
  3. For each round: manager sets p → agents form N expectations → manager aggregates → broadcast historical count
  4. Repeat 6 price steps × 10 iterations per trajectory

- Design tradeoffs:
  - **Temperature=0.7**: Introduces stochasticity to capture behavioral variance; lower values would reduce dispersion but mask optimism effects
  - **Aggregate vs. individual history**: Aggregate prevents direct peer inference but mirrors real-world information asymmetry; individual-level data might accelerate convergence but reduce ecological validity
  - **Price trajectory length**: 6 steps covers theoretical thresholds (N=1-6); fewer steps risk incomplete learning, more steps increase token costs

- Failure signatures:
  - **Persistent high variance in expectations across iterations** → agents not learning from history; check history prompt formatting
  - **Expectations consistently above FEE under strong β** → AI optimism active; expected behavior per paper, not a bug
  - **No difference between ordered and random trajectories** → possible temperature too high or model lacks in-context capacity

- First 3 experiments:
  1. Replicate static baseline (no history, β=0.25): Verify agents show dispersed expectations without convergence, matching Figure 1 baseline.
  2. Test fixed-price convergence (β=0.25 vs. β=0.75): Confirm weak β enables learning while strong β triggers optimism, per Figure 2.
  3. Compare ascending vs. random trajectories (β=0.25): Validate that temporal coherence drives learning, per Figure 3 vs. Figure 5 comparison.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do strategic expectations and convergence behaviors change in hybrid human-AI networks compared to pure AI systems?
- Basis in paper: [explicit] The Conclusion explicitly states, "Future work should explore hybrid human-AI networks and real-world data noise."
- Why unresolved: The current study isolated AI-to-AI interactions to establish a baseline for "machine optimism" and history reliance, excluding the variable of human behavioral noise and reasoning.
- What evidence would resolve it: Experiments running the same network-effect game with mixed groups of human subjects and AI agents to observe if AI optimism spreads to humans or if human rationality tempers AI behavior.

### Open Question 2
- Question: Does the reliance on temporally coherent history persist when agents face real-world data noise and incomplete information?
- Basis in paper: [explicit] The Conclusion suggests future work should explore "real-world data noise."
- Why unresolved: The experiments used stylized, clean utility functions and controlled price trajectories, whereas real-world environments contain contradictory signals or ambiguous data that might disrupt the "history design" effect.
- What evidence would resolve it: Testing agents in simulations with stochastic utility functions, missing historical data points, or "noisy" price signals to determine if structured history remains an effective coordination tool.

### Open Question 3
- Question: Is "AI optimism" (overestimating participation under strong network effects) a result of training data bias or a structural failure of recursive reasoning?
- Basis in paper: [inferred] Sections 5.2 and 5.3 document persistent "AI optimism" where agents prioritize collective gains over evidence, but the paper only speculates on the cause (e.g., "perceived value of collective action").
- Why unresolved: The paper establishes the existence of the bias but does not isolate whether it stems from the model's pre-training on positive texts or a limitation in its ability to compute negative recursive loops.
- What evidence would resolve it: Ablation studies using models fine-tuned on pessimistic vs. optimistic datasets, or chain-of-thought analysis to see if agents explicitly dismiss contradictory historical data.

### Open Question 4
- Question: Do the findings regarding history dependence and network effects generalize to non-Qwen LLM architectures?
- Basis in paper: [inferred] The Methodology (Section 3) restricts experiments to the Qwen family (Max, Turbo, 1.5B), noting detailed results for other models were omitted due to space constraints.
- Why unresolved: LLMs often exhibit different reasoning capabilities based on architecture and alignment; it remains unconfirmed if the specific "AI optimism" and sensitivity to randomization are universal traits or specific to the Qwen models used.
- What evidence would resolve it: Replicating the exact experimental protocol using diverse frontier models (e.g., GPT, Claude, Llama) to verify if the convergence failures are systemic to LLMs.

## Limitations
- Findings are limited to Qwen model variants and may not generalize to other LLM architectures or coordination domains
- The temperature parameter (0.7) introduces stochasticity that may obscure or amplify learning effects
- Aggregate-only history format prevents individual-level peer inference, potentially constraining convergence compared to real-world scenarios
- Six-round trajectory length may be insufficient for some models to fully extract patterns, particularly in random sequence conditions

## Confidence

- **High confidence**: Mechanism 1 (weak β + ordered history → partial convergence) and Mechanism 2 (strong β → optimism bias) - these effects are directly observable in reported figures and align with theoretical expectations
- **Medium confidence**: Mechanism 3 (temporal coherence requirement) - while the random trajectory results support this, the causal interpretation relies on comparison with human reasoning patterns not empirically tested here
- **Medium confidence**: The claim that optimism is "inherent" to LLM training rather than a parameter or prompt artifact - this requires ablation studies varying these factors

## Next Checks

1. Replicate the random trajectory condition with temperature=0.1 to test whether optimism persists without stochastic interference
2. Implement a "peaked" price sequence (e.g., 2.24→5.99→2.24) to test whether LLMs can learn non-monotonic relationships between price and participation
3. Run the same experimental protocol with a transformer-based coordination game (e.g., stag hunt) to test whether optimism is specific to the network-effect utility form or represents a broader LLM coordination bias