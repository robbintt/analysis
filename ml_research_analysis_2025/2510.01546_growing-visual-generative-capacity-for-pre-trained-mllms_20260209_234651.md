---
ver: rpa2
title: Growing Visual Generative Capacity for Pre-Trained MLLMs
arxiv_id: '2510.01546'
source_url: https://arxiv.org/abs/2510.01546
tags:
- arxiv
- generation
- visual
- tokens
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bridge, a pure autoregressive unified multimodal
  large language model that augments pre-trained visual understanding models with
  generative capacity through a Mixture-of-Transformers architecture, enabling both
  image understanding and generation within a single next-token prediction framework.
  To enhance visual generation fidelity, the authors propose a semantic-to-pixel discrete
  representation that integrates compact semantic tokens with fine-grained pixel tokens,
  achieving strong language alignment and precise visual detail reconstruction with
  only a 7.9% increase in sequence length.
---

# Growing Visual Generative Capacity for Pre-Trained MLLMs

## Quick Facts
- arXiv ID: 2510.01546
- Source URL: https://arxiv.org/abs/2510.01546
- Reference count: 32
- This paper introduces Bridge, a unified multimodal large language model that augments pre-trained visual understanding models with generative capacity through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework.

## Executive Summary
This paper introduces Bridge, a unified multimodal large language model that augments pre-trained visual understanding models with generative capacity through a Mixture-of-Transformers architecture. The key innovation is enabling both image understanding and generation within a single next-token prediction framework, addressing the challenge of unifying multimodal capabilities. Bridge achieves competitive performance across diverse benchmarks while requiring less training data and time compared to prior approaches.

## Method Summary
Bridge employs a Mixture-of-Transformers (MoT) architecture that integrates pre-trained visual encoders with a unified multimodal decoder. The model uses a novel semantic-to-pixel discrete representation that combines compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment with only a 7.9% increase in sequence length. The framework operates through pure autoregressive next-token prediction, enabling both understanding and generation tasks. Training efficiency is improved through selective parameter updates and the MoT architecture, which reduces computational overhead compared to traditional transformer approaches.

## Key Results
- Achieves 85.51 overall score on DPG Bench and 0.82 on GenEval for text-to-image generation
- Scores 3.39 overall on ImgEdit for image editing tasks
- Demonstrates reduced training data requirements and training time compared to prior unified MLLMs
- Shows competitive or superior results across diverse multimodal benchmarks

## Why This Works (Mechanism)
The Bridge model works by combining a pre-trained visual encoder with a unified multimodal decoder through a Mixture-of-Transformers architecture. This allows the model to leverage existing visual understanding capabilities while adding generative capacity through next-token prediction. The semantic-to-pixel discrete representation provides a compact yet expressive way to represent images, enabling efficient generation while maintaining visual fidelity. The autoregressive framework ensures consistent processing for both understanding and generation tasks.

## Foundational Learning
- **Mixture-of-Transformers (MoT)**: A modular architecture that combines different transformer components for specialized tasks - needed for efficient integration of understanding and generation capabilities; quick check: verify component interactions through ablation studies
- **Discrete Tokenization**: Converting continuous visual data into discrete tokens for autoregressive prediction - needed for unified next-token framework; quick check: measure compression ratio vs visual quality trade-off
- **Semantic-to-Pixel Representation**: Hybrid approach combining semantic and pixel-level tokens - needed for balancing language alignment with visual detail; quick check: evaluate reconstruction quality at different semantic compression levels
- **Unified Autoregressive Framework**: Single framework for both understanding and generation - needed for consistency and efficiency; quick check: test task-specific performance degradation
- **Pre-trained Model Fine-tuning**: Leveraging existing visual understanding models - needed for reduced training requirements; quick check: compare with from-scratch training on same data

## Architecture Onboarding

**Component Map**: Visual Encoder -> Mixture-of-Transformers -> Discrete Tokenizer -> Unified Decoder -> Output Generation

**Critical Path**: Input Image -> Visual Encoder -> MoT Processing -> Discrete Token Generation -> Next-Token Prediction -> Output

**Design Tradeoffs**: The MoT architecture trades some parameter sharing for efficiency gains, while the semantic-to-pixel representation balances compression with visual fidelity. The unified framework sacrifices some task-specific optimization for generality.

**Failure Signatures**: Performance degradation on out-of-distribution visual concepts, reduced generation diversity with semantic token compression, and potential misalignment between semantic and pixel representations.

**First 3 Experiments**:
1. Ablation study removing MoT components to quantify efficiency gains
2. Testing semantic token compression levels to find optimal visual quality trade-off
3. Zero-shot transfer to novel visual domains to assess generalization

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Architecture generalization uncertainty on out-of-distribution data and long-horizon reasoning tasks
- Semantic-to-pixel representation trade-offs not fully quantified for generation diversity
- Training data composition ambiguity regarding dataset diversity and potential biases
- Evaluation benchmark gaps in real-world multimodal interaction scenarios
- Computational efficiency claims lack absolute training time and resource metrics

## Confidence
- **High Confidence**: Performance on established multimodal benchmarks (DPG Bench, GenEval, ImgEdit)
- **Medium Confidence**: Unified autoregressive framework effectiveness and MoT architecture benefits
- **Medium Confidence**: Semantic-to-pixel discrete representation improvements
- **Low Confidence**: Real-world deployment readiness and cross-domain generalization

## Next Checks
1. Conduct human preference studies comparing Bridge generations against established models across diverse visual domains to validate benchmark performance in practical scenarios

2. Evaluate cross-domain generalization by testing the model on zero-shot tasks involving novel visual concepts not present in training data, including culturally diverse imagery and specialized technical domains

3. Perform ablation studies on the semantic-to-pixel representation to quantify the impact of semantic token compression on generation diversity, visual fidelity, and cross-modal alignment quality