---
ver: rpa2
title: What Matters in Deep Learning for Time Series Forecasting?
arxiv_id: '2512.22702'
source_url: https://arxiv.org/abs/2512.22702
tags:
- time
- forecasting
- series
- architectures
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the effectiveness of current deep learning
  benchmarks for time series forecasting by examining overlooked design choices that
  impact performance. The authors identify four key design dimensions: model configuration
  (local vs global), preprocessing and exogenous variables, temporal processing, and
  spatial processing.'
---

# What Matters in Deep Learning for Time Series Forecasting?

## Quick Facts
- arXiv ID: 2512.22702
- Source URL: https://arxiv.org/abs/2512.22702
- Reference count: 40
- Key outcome: Simple, well-designed architectures can match state-of-the-art performance when properly configured; overlooked implementation details in existing architectures can fundamentally change their behavior and affect empirical results

## Executive Summary
This paper identifies four key design dimensions that impact deep learning performance for time series forecasting: model configuration (local vs global), preprocessing and exogenous variables, temporal processing, and spatial processing. Through extensive experiments on four real-world datasets, the authors demonstrate that streamlined architectures achieve competitive results against complex state-of-the-art models when confounding design choices are controlled. They propose an auxiliary forecasting model card to standardize documentation of key design choices, aiming to improve transparency and reproducibility in the field.

## Method Summary
The authors conduct controlled experiments comparing baseline SOTA architectures (PatchTST, DLinear, TimeMixer, iTransformer, Crossformer, ModernTCN, Linear global/local) against reference architectures with standardized design choices. Reference architectures use RevInv normalization, optional patching-style convolutions, optional local embeddings (to create hybrid models), temporal modules (MLP, TCN, RNN, Transformer, Pyraformer), optional spatial attention, and linear decoders. Experiments are run on four datasets (Electricity, Weather, Traffic, Solar) with varying window/horizon settings, using MSE/MAE metrics averaged over 3 runs.

## Key Results
- Simple reference architectures (MLP, TCN) achieve competitive performance against complex SOTA models when design dimensions are controlled
- Model configuration (global vs hybrid vs local) fundamentally determines model class and performance, with hybrid models often outperforming pure variants
- Adding calendar covariates improves performance for most models but effects vary by dataset and architecture
- Removing spatial attention from iTransformer often improves or maintains performance, questioning its utility in current benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model configuration choice (global vs local vs hybrid) fundamentally determines model class and performance, yet is often treated as an implementation detail rather than a design decision.
- Mechanism: Local models learn series-specific parameters (θ_i), global models share parameters (θ) across all series, and hybrid models combine shared parameters with series-specific components (θ, φ_i). The paper demonstrates that "channel independence" in architectures like PatchTST corresponds to the well-established global modeling paradigm, while architectures using learnable local embeddings or position encodings implicitly create hybrid models.
- Core assumption: Performance differences between architectures can be attributed to their sequence modeling operators only when model configuration is held constant.
- Evidence anchors:
  - [abstract] "overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method"
  - [section D1, Table 1] Shows hybrid models with local embeddings outperform pure global variants on Electricity (e.g., Transformer: 0.136 MSE hybrid vs. 0.151 global), while Traffic shows the opposite pattern (0.392 global vs. 0.417 hybrid), indicating the configuration choice is dataset-dependent.
  - [corpus] TimeRecipe (arxiv 2506.06482) similarly evaluates "module level effectiveness" but doesn't isolate model configuration as a primary dimension.

### Mechanism 2
- Claim: Preprocessing and exogenous variables account for significant but unmeasured performance variance across benchmarks.
- Mechanism: Models compared in benchmarks often use different preprocessing (scaling, normalization) and exogenous inputs (calendar features, time encodings). The paper shows adding calendar covariates improves PatchTST on Electricity (0.128 vs. 0.134 MSE) and Traffic (0.355 vs. 0.383), but the effect varies by dataset and model—DLinear sees minimal benefit.
- Core assumption: Fair comparison requires all models receive identical inputs and preprocessing; performance differences should be attributed only to architectural components being evaluated.
- Evidence anchors:
  - [abstract] "overlooked implementation details in existing architectures... drastically affect the observed empirical results"
  - [section D2, Table 2] Covariates reduce MSE for Transformer (0.136 vs. 0.155 on Electricity), PatchTST (0.128 vs. 0.134), and iTransformer (0.154 vs. 0.167), but Crossformer and DLinear show mixed results.
  - [corpus] ARIES (arxiv 2509.06060) links model performance to data properties like seasonality, suggesting preprocessing choices should be data-driven.

### Mechanism 3
- Claim: Simple temporal processing architectures (MLP, TCN) match complex SOTA transformers when other design dimensions are properly controlled, indicating that sequence modeling operators may be less critical than currently assumed.
- Mechanism: The paper's reference architectures—using standard MLPs, TCNs, RNNs, and basic transformers with patching—achieve competitive performance against PatchTST, TimeMixer, and DLinear when all models use the same covariates and hybrid configuration. Table 3 shows MLP achieving 0.129 MSE on Electricity vs. PatchTST's 0.125.
- Core assumption: Benchmarking practices have failed to isolate the contribution of architectural innovations from confounding design choices.
- Evidence anchors:
  - [abstract] "simple, well-designed architectures can often match the state of the art"
  - [section D3, Table 3] No single model consistently wins; reference MLP, TCN, and Pyraformer all achieve competitive results across datasets.
  - [section D4, Table 4] Removing spatial attention from iTransformer improves or maintains performance on all datasets (e.g., Solar: 0.194 vs. 0.208 MSE with attention).
  - [corpus] "The Power of Architecture" (arxiv 2507.13043) similarly questions which transformer architectures work best, but focuses on encoder/decoder variants rather than controlling for confounders.

## Foundational Learning

- **Concept: Global vs. Local Forecasting Paradigm**
  - Why needed here: The paper frames recent "channel independence" as a rediscovery of global forecasting—a decades-old principle where one model learns from all series in a collection. Without this context, performance differences appear mysterious.
  - Quick check question: Given 1000 sensors measuring the same quantity at different locations, would you train 1000 separate models (local) or one model on all data (global)? What if sensors measure different quantities?

- **Concept: Hybrid Global-Local Models**
  - Why needed here: Many SOTA architectures use shared backbones with series-specific embeddings (e.g., TimeMixer's normalization parameters, Crossformer's position embeddings). This hybridization is rarely explicit in model descriptions but significantly impacts results.
  - Quick check question: If a model uses a shared encoder but per-series learnable embeddings, can it make predictions for a new series not seen during training?

- **Concept: Transductive vs. Inductive Inference**
  - Why needed here: Global models can generalize to unseen series (inductive, cold-start capable); local models cannot. Hybrid models' inductive capability depends on which components are shared. This has deployment implications.
  - Quick check question: Your production system needs to forecast demand for new products with no historical data. Which model configuration enables this?

## Architecture Onboarding

- **Component map:**
  Input → [Preprocessing: RevInv normalization] → [Feature encoding: temporal covariates + optional patching convolution] → [Local embeddings (optional, creates hybrid model)] → [Temporal processing: MLP/TCN/RNN/Transformer/Pyraformer] → [Spatial processing (optional): attention across series] → [Decoder: linear projection to horizon] → [Denormalization]

- **Critical path:** Start with D1 (model configuration). Decide global/local/hybrid based on: (a) series homogeneity, (b) cold-start requirements, (c) data volume per series. Then D2 (preprocessing/covariates)—standardize across all compared models. D3 (temporal) and D4 (spatial) are secondary; simple operators often suffice.

- **Design tradeoffs:**
  - Global models: Better sample efficiency, cold-start capable, but may underfit heterogeneous series
  - Local parameters: Better fit per-series patterns, but reduce effective training data per parameter and prevent cold-start
  - Spatial attention: Theoretically models inter-series dependencies, but Tables 4 and 14-17 show removal often *improves* performance, suggesting limited benefit in current benchmarks
  - Patching: Reduces sequence length for attention-based models; appears beneficial across architectures

- **Failure signatures:**
  - Model performs well on benchmarks but fails on new series → likely using local parameters, not truly global
  - Complex SOTA model underperforms simple baseline → check if baseline has hidden local parameters or different preprocessing
  - Adding spatial attention degrades performance → inter-series dependencies may not exist or be learnable in your data
  - Performance varies wildly across runs → hyperparameter sensitivity may dominate architectural choices

- **First 3 experiments:**
  1. **Ablate configuration:** Take your chosen architecture. Compare global vs. hybrid (add learnable series embeddings) vs. local (separate models per series). Report which configuration works best and why this matches your data characteristics.
  2. **Standardize inputs:** Select your strongest baseline and your proposed model. Ensure identical preprocessing and covariates. Compare before/after standardization to quantify the "unfair advantage" from implementation differences.
  3. **Simplify temporal processing:** Replace your temporal module with a 2-layer MLP with residual connections. If performance remains competitive, your complexity isn't justified. Document this using the paper's model card template.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can controlled synthetic benchmarks be developed to effectively isolate the individual contributions of temporal versus spatial processing modules?
- Basis in paper: [explicit] In the Discussion, the authors propose that "introducing benchmarks specifically designed to isolate distinct dimensions, possibly by relying on synthetic datasets, can help measure the isolated effects of different design choices."
- Why unresolved: Current real-world benchmarks conflate temporal and spatial dependencies, and the authors show that simple models often match SOTA, making it difficult to discern if spatial modules are actually learning useful inter-series dependencies or just overfitting noise.
- What evidence would resolve it: The creation and adoption of a synthetic dataset with known ground-truth causal structures where models with explicit spatial modules consistently and verifiably outperform channel-independent baselines.

### Open Question 2
- Question: Do the identified benchmarking pitfalls regarding model configuration and preprocessing persist when evaluating probabilistic forecasting methods?
- Basis in paper: [explicit] The Limitations section states that the analysis is restricted to point forecasting and notes, "the discussion can be expanded to include probabilistic forecasting and the choice of metrics to quantify forecasting accuracy."
- Why unresolved: It is unknown if the "streamlined" architectures that match SOTA performance on point metrics (MSE/MAE) would similarly match the performance of complex models in capturing distributional uncertainty.
- What evidence would resolve it: A reproduction of the study's comparative analysis using probabilistic metrics (e.g., CRPS, NLL) to see if the relative ranking of simple vs. complex architectures remains unchanged.

### Open Question 3
- Question: Under what specific conditions does explicit spatial attention provide benefits that outweigh the computational costs in long-range forecasting?
- Basis in paper: [inferred] The paper empirically demonstrates in Table 4 that removing spatial attention from the iTransformer model often improves performance on standard benchmarks, leading the authors to question the operator's utility in this context.
- Why unresolved: The authors show spatial attention is currently detrimental or redundant on the tested datasets, but they do not define the theoretical data characteristics (e.g., specific types of correlation structures) where this complex operator would be necessary.
- What evidence would resolve it: An ablation study on a dataset designed with high, non-local inter-series correlation, showing that spatial attention mechanisms provide a statistically significant reduction in error over simple global or local models.

## Limitations

- The paper's findings are based on four specific datasets that may not represent the full diversity of time series forecasting scenarios
- The study does not extensively explore datasets with very long-range dependencies or those requiring specialized temporal reasoning
- The paper focuses on univariate forecasting and does not address multivariate or panel data scenarios where inter-series relationships might be more critical

## Confidence

- **High confidence**: The core claim that implementation details (preprocessing, covariates, model configuration) significantly impact empirical results is well-supported by controlled ablations across multiple datasets and architectures
- **Medium confidence**: The assertion that simple temporal processing architectures can match complex SOTA models when other design dimensions are controlled, while demonstrated empirically, may not generalize to all forecasting scenarios or more challenging datasets
- **Medium confidence**: The recommendation to use model cards for standardized documentation is sound but lacks evaluation of whether this practice will actually improve reproducibility in practice

## Next Checks

1. **Dataset diversity test**: Replicate the controlled experiments on datasets with different characteristics (e.g., longer horizons, stronger seasonality, different sampling rates) to verify the generalizability of the "simple architectures suffice" claim
2. **Implementation audit**: Independently reimplement one complex SOTA architecture (e.g., PatchTST) with all design choices explicitly documented and controlled, comparing against the reference architecture to quantify the "unfair advantage" from implementation details
3. **Cold-start validation**: Systematically evaluate the inductive capabilities of global vs. hybrid vs. local configurations on held-out series not seen during training to verify the practical implications of model configuration choices