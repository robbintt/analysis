---
ver: rpa2
title: 'Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs'
arxiv_id: '2510.00861'
source_url: https://arxiv.org/abs/2510.00861
tags:
- search
- answer
- reasoning
- arxiv
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Erasable Reinforcement Learning (ERL) addresses the fragility of
  search-augmented LLMs in multi-hop reasoning by introducing a targeted error correction
  mechanism. Instead of treating the entire reasoning trajectory as a monolithic process,
  ERL detects errors in decomposition, retrieval, or reasoning, erases faulty steps,
  and regenerates reasoning from the last correct state.
---

# Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs

## Quick Facts
- arXiv ID: 2510.00861
- Source URL: https://arxiv.org/abs/2510.00861
- Reference count: 40
- Primary result: ERL achieves SOTA on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with 3B model gaining +8.48% EM and +11.56% F1, and 7B model gaining +5.38% EM and +7.22% F1

## Executive Summary
Search-augmented LLMs often struggle with multi-hop reasoning due to error propagation through reasoning chains. Erase to Improve (ERL) addresses this fragility by introducing a targeted error correction mechanism that detects and erases faulty reasoning steps, then regenerates from the last correct state. This approach transforms brittle reasoning into a more resilient process by preventing defective logic from cascading through the chain.

ERL operates by segmenting reasoning trajectories and applying erasure operators at different granularities - sub-answer, search, or plan level - based on intermediate reward signals. The method demonstrates substantial improvements over existing search-augmented models, achieving new state-of-the-art results on multiple multi-hop reasoning benchmarks. The framework shows particular strength in correcting reasoning errors, which are more frequent and have greater downstream impact than retrieval or planning errors.

## Method Summary
ERL treats search-augmented reasoning as a Markov Decision Process where the policy generates structured actions for search queries, observations, sub-answers, and final answers. The core innovation is a trajectory segmentation mechanism that uses three erasure operators to selectively remove faulty reasoning segments while preserving correct prefixes. The system computes dense stepwise rewards for search coverage and sub-answer quality, enabling early error detection before terminal evaluation. When errors are detected via reward thresholds, the trajectory is truncated at the error point and regenerated from the last valid state. The entire process is trained using PPO with clipping, where rewards are attributed at the token level to guide policy updates.

## Key Results
- ERL achieves new SOTA on HotpotQA, MuSiQue, 2Wiki, and Bamboogle benchmarks
- 3B model gains +8.48% EM and +11.56% F1 over previous SOTA results
- 7B model gains +5.38% EM and +7.22% F1 over previous SOTA results
- Ablation studies show sub-answer erasure is most effective, correcting 9.6% of errors vs 6.53% for retrieval and 2.01% for decomposition

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Segmentation via Erasure Operators
Selectively removing faulty reasoning segments while preserving correct prefixes prevents error propagation through multi-hop chains. An erasure operator truncates trajectories at step t and regenerates from the last valid state, with three erasure types operating at different granularities: sub-answer, search, and plan. Core assumption: Intermediate rewards reliably signal which step contains the error. Evidence: [abstract] explicitly states the mechanism prevents defective logic from propagating, and [section 3.4] formalizes the truncation logic. Break condition: If reward signals misattribute blame, erasure removes wrong segments and compounds errors.

### Mechanism 2: Dense Stepwise Rewards for Early Error Detection
Per-step rewards for search coverage and sub-answer quality enable error detection before terminal evaluation, replacing sparse outcome-only signals. Rsearch computes coverage gain minus redundancy penalty using TF-IDF similarity, while Rsub_answer measures F1 improvement over previous sub-answers. Core assumption: TF-IDF similarity and F1 overlap are sufficient proxies for evidence relevance. Evidence: [section 3.3] defines the reward computation, and [table 3] shows ablation results where disabling sub-answer erasure causes -1.80% F1. Break condition: If gold evidence or sub-answers are noisy, reward signals become unreliable, causing erratic erasure triggers.

### Mechanism 3: Hierarchical Correction Priority
Correcting reasoning errors yields larger gains than correcting retrieval or planning errors, establishing an effectiveness hierarchy. Sub-answer erasure corrects 9.6% of errors vs 6.53% for retrieval and 2.01% for decomposition. Core assumption: Error types are sufficiently separable that sequential correction doesn't cause interference. Evidence: [section 5] states the clear hierarchy with sub-answer erasure > search > plan, and [table 4] shows sub-answer-only approaches full ERL performance. Break condition: If multiple heterogeneous errors co-occur, sequential correction may require many iterations without guaranteed convergence.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: ERL models search-reasoning as state transitions st = (Q, Ht); understanding the tuple (S, A, P, R, γ) is prerequisite for following the method.
  - Quick check question: How does the state st encode both the original question and accumulated interaction history?

- Concept: Proximal Policy Optimization (PPO) with clipping
  - Why needed here: ERL uses PPO for policy updates; understanding the clipped objective and KL regularization explains training stability.
  - Quick check question: What role does the clipping parameter ε play in preventing destructive policy updates?

- Concept: Credit assignment in multi-step reasoning
  - Why needed here: Dense rewards distribute credit across steps; understanding why sparse rewards fail explains ERL's design motivation.
  - Quick check question: Why might a correct final answer receive low reward if an intermediate step is flawed?

## Architecture Onboarding

- Component map: Policy model (Qwen2.5-3B/7B) -> Search tool (E5 retriever + Wiki-18 corpus) -> Reward calculator -> Erasure controller -> PPO trainer

- Critical path:
  1. Question Q input → policy samples (ot, rt, qt)
  2. Search tool returns et
  3. Reward calculator evaluates Rsearch, Rsub_answer against gold annotations
  4. Erasure controller checks thresholds; if triggered, truncate to τ₀:t and regenerate
  5. Repeat until ⟨Afinal⟩ or max steps reached
  6. Compute Ranswer, aggregate rewards, update policy via PPO

- Design tradeoffs:
  - Threshold sensitivity: Lower α increases erasure frequency (higher compute, potential over-correction); higher α may miss errors. Paper uses separate α (local) and β (plan-level) but doesn't specify exact values.
  - Top-K retrieval: k=3 is optimal; k=1 lacks multi-hop evidence, k=5 introduces distracting noise (table 6).
  - Iteration limits: More erasure iterations improve correction rates but increase latency; paper shows diminishing returns beyond 5 iterations.

- Failure signatures:
  - Premature observations: Model outputs sub_answer without fully consuming evidence (appendix E, table 8).
  - Entity misalignment: Correct document retrieved but wrong entity extracted from it (appendix E, figure 9).
  - Spelling-based entity confusion: "Banir" matched to "Banagher" instead of correct entity, cascading to wrong downstream answers (figure 10).

- First 3 experiments:
  1. Reproduce single-erasure ablation on MuSiQue validation set to validate hierarchy claim (sub-answer > search > plan).
  2. Sweep α ∈ {0.2, 0.4, 0.6} while fixing β, measuring erasure trigger rate and F1 to calibrate threshold sensitivity.
  3. Compare k=1,3,5 retrieval on Bamboogle to confirm noise-vs-coverage tradeoff reported in table 6.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the ERL framework be adapted to identify and resolve multiple heterogeneous errors simultaneously rather than sequentially?
  - Basis in paper: [explicit] Section 7 states the framework "may struggle when multiple heterogeneous errors occur simultaneously within a reasoning trajectory," often requiring repeated iterations for separate repairs.
  - Why unresolved: Current design relies on sequential erasure operators which cannot handle compound failures in a single corrective pass.
  - What evidence would resolve it: Modified architecture demonstrating "coordinated error mitigation" that reduces the number of regeneration steps required for complex trajectories containing mixed error types.

- **Open Question 2**: How can the computational overhead introduced by the sequential identify-erase-regenerate cycle be reduced to ensure scalability?
  - Basis in paper: [explicit] Section 7 notes that the strength of the framework "inherently increases computational overhead" which "limits scalability and efficiency."
  - Why unresolved: While paper demonstrates accuracy improvements, it acknowledges the "erase to improve" paradigm trades off inference speed for robustness.
  - What evidence would resolve it: Implementation details of an optimized ERL variant that minimizes latency, or rigorous analysis of wall-clock time compared to standard PPO/GRPO baselines on long-horizon tasks.

- **Open Question 3**: Can the ERL mechanism be effectively transferred to generative tasks outside of multi-hop question answering?
  - Basis in paper: [explicit] Conclusion (Section 8) explicitly lists "extending this mechanism to a broader range of generative tasks" as a direction for future work.
  - Why unresolved: Current study validates ERL exclusively on multi-hop QA datasets; utility for tasks like summarization or code generation remains untested.
  - What evidence would resolve it: Successful application of erasure-recovery logic to open-ended generation tasks, showing improvements in factual consistency or logical coherence.

## Limitations
- The method relies on oracle annotations (gold sub-answers and evidence) that may not be available in real-world deployment
- Error localization assumes errors are detectable via intermediate rewards, which becomes significantly harder without gold evidence in open-domain settings
- Computational overhead of multiple erasure iterations could limit practical applicability, though the paper doesn't quantify the cost-benefit tradeoff

## Confidence

- **High confidence**: The empirical results showing SOTA performance across four benchmarks (HotpotQA, MuSiQue, 2Wiki, Bamboogle) are well-supported by the data in tables 1-4.
- **Medium confidence**: The mechanism explanations (error hierarchy, dense rewards) are plausible given the ablation results, but rely on assumptions about error separability and reward signal reliability that weren't extensively tested.
- **Medium confidence**: The architectural design choices (threshold values, Top-K=3) are validated within the paper's scope but may not generalize across different domains or retriever qualities.

## Next Checks

1. Test ERL's performance on multi-hop reasoning tasks where gold evidence is unavailable, using only retrieval-based evidence matching to simulate realistic conditions.
2. Measure the computational overhead (latency, cost) of multiple erasure iterations across different dataset complexities and quantify the accuracy-cost tradeoff.
3. Evaluate error correction effectiveness when multiple heterogeneous errors occur simultaneously in a single trajectory, testing the model's ability to escape local minima through sequential corrections.