---
ver: rpa2
title: 'Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective
  Step-aware Formal Verification'
arxiv_id: '2506.04592'
source_url: https://arxiv.org/abs/2506.04592
tags:
- step
- formal
- reasoning
- language
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Safe, a retrospective, step-aware formal
  verification framework that enhances mathematical reasoning in large language models
  by combining formal theorem proving with process reward models. Safe automatically
  generates formal Lean 4 statements for each reasoning step and uses automated theorem
  provers to validate correctness, producing interpretable and verifiable evidence.
---

# Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification

## Quick Facts
- arXiv ID: 2506.04592
- Source URL: https://arxiv.org/abs/2506.04592
- Reference count: 32
- Key outcome: Safe achieves 2-3% accuracy improvement over PRMs/ORMs on mathematical reasoning tasks

## Executive Summary
Safe introduces a retrospective, step-aware formal verification framework that enhances mathematical reasoning in large language models by combining formal theorem proving with process reward models. The system automatically generates formal Lean 4 statements for each reasoning step and uses automated theorem provers to validate correctness, producing interpretable and verifiable evidence. Evaluated across multiple LLMs and mathematical datasets (GSM8K, MATH-500, CollegeMath), Safe achieves state-of-the-art performance, improving accuracy by 2-3% over existing reward models. The authors also release FormalStep, a benchmark of 30,809 formal statements for step-level theorem proving, demonstrating that single-step verification is computationally tractable with appropriate sample budgets.

## Method Summary
Safe works by generating multiple reasoning trajectories from an LLM, decomposing them into individual steps, and then auto-formalizing each step transition into Lean 4 theorem statements. These statements are verified using automated theorem provers (DeepSeek-Prover-V1.5) with a sample budget of 16 attempts per statement. The verification produces four discrete states (Proof Failed, Proof Successful, Failed Formalization, Formalization Failed) which are aggregated using a tiny LSTM to produce a retrospective verification score. This score is combined with a prospective PRM score via weighted multiplication to select the best trajectory. The framework requires ~4-8x more inference cost than standard PRMs but provides interpretable proof evidence for reasoning correctness.

## Key Results
- Achieves 2-3% accuracy improvement over PRMs and ORM ensembles on GSM8K, MATH-500, and CollegeMath datasets
- Single-step theorem proving succeeds on over 80% of FormalStep benchmark statements with 16 proof attempts
- Outperforms naive PRM+ORM ensemble methods, demonstrating complementary benefits of retrospective and prospective scoring
- Shows effectiveness across multiple LLM architectures (GPT-4o, Llama-3 variants, DeepSeek-Math)

## Why This Works (Mechanism)

### Mechanism 1: Single-Step Tractability
Decomposing complex mathematical reasoning into individual steps reduces the computational burden of automated theorem proving, making formal verification feasible at inference time. By isolating step transitions into standalone Lean 4 theorems, the search space is drastically reduced compared to full problem formalization.

### Mechanism 2: Retrospective-Prospective Score Complementarity
Formal verification provides "retrospective" correctness signals that are orthogonal to the "prospective" value estimation of Process Reward Models. This complementarity ensures that logical hallucinations missed by PRMs are caught by formal verification, and vice versa.

### Mechanism 3: Noise-Resistant State Aggregation
An LSTM filters out noise from imperfect auto-formalization and failed proof attempts better than strict binary filtering. The model learns to weigh the criticality of specific failure patterns in the sequence of verification states.

## Foundational Learning

- **Concept: Lean 4 & Theorem Proving**
  - Why needed: Safe relies on Lean 4 as the formal verification engine; understanding theorem statements vs. proofs is essential
  - Quick check: Can you explain why a prover might fail to prove a true statement (e.g., due to missing premises or timeout)?

- **Concept: Auto-formalization**
  - Why needed: Bridges natural language reasoning and formal verification; translation is a potential point of failure
  - Quick check: If an LLM translates "the graph goes up" into Lean, what formal definition might it need?

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed: Safe is evaluated by augmenting PRMs; need to distinguish step-level vs. answer-level scoring
  - Quick check: Why would a step be logically sound (pass Safe verification) but still receive a low PRM score?

## Architecture Onboarding

- **Component map:** Generator -> Decomposer -> Auto-Formalizer -> Formal Verifier -> State Aggregator -> Ensembler
- **Critical path:** Auto-Formalizer â†’ Formal Verifier loop; this is where 4-8x inference cost overhead occurs
- **Design tradeoffs:** Compute vs. Accuracy (increasing sample budget raises proof success but linearly increases latency); LSTM vs. Heuristic (learned aggregator requires small training set but handles noise better)
- **Failure signatures:** Formalization Drift (valid but semantically distinct Lean theorems); Saturation Effect (LSTM struggles when almost all samples are correct)
- **First 3 experiments:** 1) Unit Test the Verifier: Run Step Verifier on FormalStep benchmark to reproduce "Proof Rate" vs. "Sample Budget" curve; 2) Ablate the Aggregator: Replace LSTM with simple Mean/Median aggregation on MATH-500; 3) Stress Test Scaling: Reproduce Figure 4 comparing Safe vs. high-N sampling for Llama 3.0 vs. 3.1

## Open Questions the Paper Calls Out

1. **Question:** How can the computational resource demands of the retrospective verification process be reduced to make it viable for reinforcement learning (RL) training pipelines?
   - Basis: Authors identify "alleviating the computational resource demands" and integrating verification into RL as explicit areas for future work
   - Why unresolved: Current method requires 4-8x more LLM queries than standard PRMs, creating "friction" that prevents direct application in RL scenarios

2. **Question:** Can the "black box" nature of the LSTM aggregator be replaced or refined to provide perfectly interpretable evidence for reasoning correctness?
   - Basis: Authors acknowledge the "scoring mechanism based on LSTM neural network parameters still lacks perfect interpretability"
   - Why unresolved: Although inputs are formal, aggregation into final score relies on neural network which introduces noise and reduces transparency

3. **Question:** Does formal verification provide a consistent advantage over simpler scaling strategies (increasing sample count) for lower-capacity language models?
   - Basis: Paper notes that for weaker models like Llama 3.0, "increasing the sample count with a weaker... RM may remain an effective strategy"
   - Why unresolved: Performance/cost trade-off appears variable across model capabilities, leaving optimal verification strategy for smaller models undefined

## Limitations
- Requires 4-8x more inference cost due to formal verification pipeline, limiting practical deployment
- Auto-formalization introduces additional failure point where semantic meaning may be lost
- LSTM aggregator trained on relatively small dataset (~2,000 trajectories), raising generalizability questions
- Geometric reasoning and visualization concepts are explicitly excluded from formal verification capabilities

## Confidence

**High Confidence:**
- Safe improves accuracy by 2-3% over standalone PRMs and ORM ensembles on tested datasets
- Formal verification of single steps is computationally feasible with appropriate sample budgets
- 4-state classification system for theorem proving outcomes is well-defined and reproducible

**Medium Confidence:**
- Claimed orthogonal benefits of retrospective formal verification and prospective PRM scoring
- Assertion that Safe works better than naive ensemble methods due to complementary error detection
- Generalizability of results across different LLM architectures and mathematical problem types

**Low Confidence:**
- Specific claim that 16 proof attempts per step is optimal sample budget across all scenarios
- Assertion that Safe will maintain advantage on more complex mathematical domains beyond tested datasets
- Scalability of approach to larger models or more diverse mathematical libraries

## Next Checks

1. **Ablation Study of State Aggregation:** Replace LSTM aggregator with simple heuristic methods (mean, median, or threshold-based) to quantify specific contribution of learned noise filtering to overall performance.

2. **Cross-Domain Generalization Test:** Evaluate Safe on geometry problems or mathematical domains requiring visual/spatial reasoning to assess limitations of algebra/number theory-focused Lean 4 formalization.

3. **Compute-Accuracy Tradeoff Analysis:** Systematically vary ATP sample budget (8, 12, 16, 20 attempts) and measure marginal returns in accuracy versus computational cost to identify optimal deployment parameters.