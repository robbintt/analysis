---
ver: rpa2
title: Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning
  in VLMs
arxiv_id: '2509.24640'
source_url: https://arxiv.org/abs/2509.24640
tags:
- video
- reasoning
- videos
- clips
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPLICE, a human-curated benchmark derived
  from the COIN instructional video dataset to assess visual reasoning in vision-language
  models (VLMs). SPLICE consists of 3,381 human-filtered videos across 12 categories
  and 180 sub-categories, segmented into 11,423 clips.
---

# Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs

## Quick Facts
- arXiv ID: 2509.24640
- Source URL: https://arxiv.org/abs/2509.24640
- Reference count: 24
- Human-curated benchmark shows VLMs significantly underperform humans on visual reasoning tasks, with best models achieving only 23-51% accuracy versus human 85%

## Executive Summary
This paper introduces SPLICE, a human-curated benchmark derived from the COIN instructional video dataset to assess visual reasoning in vision-language models (VLMs). SPLICE consists of 3,381 human-filtered videos across 12 categories and 180 sub-categories, segmented into 11,423 clips. The task requires models to reorder shuffled clips into coherent sequences, testing temporal, causal, spatial, contextual, and general knowledge reasoning. Results show VLMs significantly underperform humans, with best models achieving only 23-51% accuracy versus human 85%. Even with textual annotations, models show limited improvement, suggesting reliance on language priors rather than visual understanding.

## Method Summary
SPLICE benchmark construction involved human curation of 3,381 videos from COIN dataset, filtering for instructional content with clear step-by-step progression. Videos were segmented into clips representing distinct events, then shuffled for the reordering task. Three evaluation modalities: video-only, text-only (human-annotated descriptions), and video+text. Models tested include Qwen2-VL (7B/72B), Gemini-Flash, InternVL2.5-78B, and LLaVA-OneVision-72B with specified frame sampling rates. Binary accuracy (exact sequence match) serves as primary metric, with Hamming and position-wise accuracy as secondary metrics. No training was performed - this is an evaluation-only benchmark.

## Key Results
- VLMs achieve 23-51% binary accuracy versus human baseline of 85%, with performance degrading significantly for sequences longer than 5 clips
- Text annotations improve model performance but not human performance, indicating models rely on language priors over visual understanding
- Models perform significantly better on causal/temporal reasoning tasks (68.37% on "Make" tasks) versus spatial/contextual tasks (36.96% on "Spatial" tasks)
- Open-source models (Qwen2-VL) lag behind closed-source models (Gemini) despite similar parameter counts
- Models exhibit "visual similarity bias," frequently placing first and last clips (often visually similar states) adjacently

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs rely on language priors over visual signals for sequencing, evidenced by disproportionate performance gains when text is added.
- **Mechanism:** Models likely use text descriptions as a "scaffolding" logic, bypassing the harder task of extracting temporal causality from raw pixels. When text is absent, they lack the robust visual grounding to infer order.
- **Core assumption:** The performance gain is due to reasoning via language rather than multimodal synergy.
- **Evidence anchors:**
  - [Abstract] "While human-annotated textual descriptions improve model accuracy, they do not affect human performance, suggesting that models rely more on language priors than on visual understanding."
  - [Section 7.3] "This improvement suggests that language models are still more capable of reasoning... as they were able to benefit from information that was not necessary for humans."
  - [Corpus] Related work (e.g., TRAVL, VRBench) highlights similar struggles in deep visual reasoning, supporting the "language shortcut" hypothesis.
- **Break condition:** If a model achieves human-parity on video-only input, this mechanism is invalidated.

### Mechanism 2
- **Claim:** Models exhibit a "Visual Similarity Bias," treating visually analogous frames (e.g., start vs. end states) as sequential rather than distinct.
- **Mechanism:** In tasks like "replacing a toner cartridge," the starting state (closed door) and ending state (closed door) are visually similar. Models aggregate these into a temporal cluster rather than recognizing the causal loop.
- **Core assumption:** The model's attention mechanism weights visual similarity higher than causal sequence position.
- **Evidence anchors:**
  - [Section 7.5] "Models frequently predicted the first and last clips as being sequential... indicating a model bias towards ordering visually similar clips adjacently."
  - [Section 7.5] "In the 'change/replace' group... Gemini-2.0-Flash scored 26.83%... but 53.5% on other videos."
  - [Corpus] *Assumption: Related papers on VLM spatiotemporal awareness (e.g., VLM4D) suggest this is a pervasive architectural limitation.*
- **Break condition:** If models succeed at "change/replace" tasks (where initial/final states are identical) without text.

### Mechanism 3
- **Claim:** Performance correlates with reasoning type; models handle causal/temporal tasks better than spatial/contextual ones.
- **Mechanism:** "Make" tasks (e.g., cooking) involve visible state changes (dough formation) that serve as strong causal cues. "Change" or "Spatial" tasks often require maintaining context or spatial orientation over time, which current architectures appear to lose.
- **Core assumption:** State-change detection is more robust in current vision encoders than spatial tracking or context maintenance.
- **Evidence anchors:**
  - [Abstract] "VLMs perform relatively better on videos where temporal and causal reasoning are dominant, compared to those where contextual and spatial reasoning are dominant."
  - [Section 7.5] Table 4 shows high performance on "Make" (68.37%) vs low on "Spatial" (36.96%) sub-domains.
  - [Corpus] *Limited direct evidence in corpus for this specific split; corpus focuses more on general physics/reasoning limits.*
- **Break condition:** If spatial reasoning scores match or exceed causal reasoning scores on this dataset.

## Foundational Learning

- **Concept: Multi-Video Context Window**
  - **Why needed here:** Unlike single-image QA, SPLICE requires the model to ingest 2-7 separate clips simultaneously and maintain distinct representations for each before comparing them.
  - **Quick check question:** Can your architecture accept a list of video tensors `[Clip A, Clip B, ...]` and reference them individually in the output, or does it merge them into a single context stream?

- **Concept: Event-Based Segmentation**
  - **Why needed here:** The benchmark splits videos based on *events* (semantic steps), not uniform time chunks. Models must handle variable-length clips.
  - **Quick check question:** Does your sampling strategy (e.g., uniform 16 frames) fail when a clip is very short (2s) or very long (30s) compared to event boundaries?

- **Concept: Reasoning Taxonomy (Causal vs. Contextual)**
  - **Why needed here:** To diagnose failures, one must distinguish between failing to see *what happened* (causal) vs. *where/when it fits* (contextual/spatial).
  - **Quick check question:** If a model orders "open door" and "close door" adjacently, is it a temporal failure or a visual similarity failure?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Projector -> LLM Backbone -> Output Head
- **Critical path:** The **Projector/LLM Interface**. If visual features are compressed too aggressively here, the subtle differences between "start state" and "end state" (Contextual Reasoning) are lost, leading to the "Visual Similarity Bias."
- **Design tradeoffs:**
  - **FPS vs. Uniform Frames:** The paper notes Qwen uses 1 FPS (good for long context) while InternVL uses uniform 16 frames (denser sampling). Lower FPS risks missing rapid actions; fixed frame counts risk truncating long clips.
  - **Open vs. Closed Source:** Closed models (Gemini) handle long context better; Open models (Qwen-7B) degrade faster with clip count.
- **Failure signatures:**
  - **The "Loopback" Error:** Consistently predicting Clip 1 and Clip N (start/end) as neighbors.
  - **Random Walk on >5 Clips:** Performance dropping to near-random baselines when sequence length exceeds 5 due to combinatorial explosion ($N!$) and context dilution.
- **First 3 experiments:**
  1. **Sanity Check (Visual Only):** Input 3 shuffled clips with distinct visual states (e.g., "egg cracking," "frying," "plating"). If this fails, the visual encoder is not grounding the events.
  2. **Text-Only Baseline:** Input only the human annotations (provided in dataset). This isolates the LLM's logical reasoning capability from vision limitations.
  3. **Modality Ablation:** Run Video+Text on the "Electrical Appliance" domain to measure reliance on text for resolving the "Change/Replace" ambiguity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the integration of audio modality enhance cross-modal alignment and improve performance on complex event-ordering tasks?
- **Basis in paper:** [explicit] The authors state in the Conclusion, "In the future, we aim to incorporate voice to enhance cross-modal alignment and assess how models integrate audio with visual reasoning."
- **Why unresolved:** The current benchmark explicitly excludes audio due to limited model support, leaving the potential benefits of auditory cues (e.g., sounds of actions preceding visual changes) untested.
- **What evidence would resolve it:** A re-evaluation of state-of-the-art models on the SPLICE benchmark with audio tracks included, measuring the delta in accuracy compared to video-only and video+text settings.

### Open Question 2
- **Question:** How can VLMs be trained to overcome the "visual similarity" bias where they incorrectly group sequential clips based on pixel-level resemblance rather than causal logic?
- **Basis in paper:** [inferred] Section 7.5 reveals a specific failure mode where models frequently predict the first and last clips (which often look similar, e.g., a closed door) are adjacent, suggesting a reliance on visual shortcuts over contextual reasoning.
- **Why unresolved:** The paper identifies this bias (associating visually similar clips as sequential) but does not propose or test methods to mitigate this shortcut learning.
- **What evidence would resolve it:** Ablation studies or training interventions that penalize reliance on simple visual feature matching, showing improved performance on "change/replace" tasks where initial and final states are visually similar.

### Open Question 3
- **Question:** What architectural or training paradigm shifts are required to translate increased Language Model (LLM) scale into improved visual reasoning capabilities?
- **Basis in paper:** [inferred] Section 7.1 notes that Qwen2-VL-7B performs on par with Qwen2-VL-72B in visual-only settings, suggesting that simply scaling the language model size does not currently enhance visual reasoning.
- **Why unresolved:** The disconnect between parameter count and visual task performance suggests current scaling laws do not efficiently apply to multimodal reasoning, but the paper stops short of identifying the specific architectural bottlenecks.
- **What evidence would resolve it:** Comparative analysis of models with varying vision encoder sizes or specialized multimodal pre-training objectives to isolate which components (other than LLM size) drive visual reasoning improvements.

### Open Question 4
- **Question:** What specific mechanisms are required to elevate VLM performance on spatial and contextual reasoning tasks to match their proficiency in temporal and causal reasoning?
- **Basis in paper:** [inferred] The analysis in Section 7.5 and Table 4 shows a significant performance disparity: models score significantly higher on "make" (causal/temporal) tasks than "change/replace" (contextual) or sports (spatial) tasks.
- **Why unresolved:** The paper quantifies the gap but leaves open the question of why current architectures favor temporal cues over spatial or contextual integration.
- **What evidence would resolve it:** Development and testing of models with explicit spatial-awareness modules or context-tracking memory mechanisms, demonstrating narrowed performance gaps between these reasoning categories.

## Limitations

- The reliance on human-curated text annotations may inadvertently encode temporal cues that models can exploit without genuine visual understanding
- Performance gap between open and closed-source models may reflect architectural differences (context length, pretraining data) rather than fundamental reasoning capabilities
- The binary accuracy metric is particularly stringent, potentially masking meaningful partial ordering abilities that models might possess

## Confidence

**High confidence**: The benchmark construction methodology and dataset statistics are well-documented and reproducible. The observation that models underperform humans by a substantial margin (23-51% vs 85%) is robust across evaluation conditions. The correlation between reasoning type and performance (temporal/causal > contextual/spatial) is supported by clear quantitative evidence.

**Medium confidence**: The interpretation that models rely on language priors over visual signals is plausible but not definitively proven. While text annotations improve performance and don't help humans, this could also reflect models' better ability to integrate multimodal information rather than pure language shortcutting. The "visual similarity bias" mechanism is observed but may be partially explained by other factors like limited context windows.

**Low confidence**: Claims about architectural limitations (e.g., specific failure modes with >5 clips) are based on observed patterns but lack systematic ablation studies to isolate causes. The comparative performance between model families could be influenced by unreported factors like inference optimization or prompt engineering variations.

## Next Checks

1. **Cross-validation with synthetic perturbations**: Apply systematic visual perturbations (motion blur, color shifts) to clips that models order correctly with text. If performance drops disproportionately compared to humans, this would confirm reliance on visual shortcuts rather than genuine reasoning.

2. **Ablation on annotation quality**: Create multiple versions of text annotations with varying levels of temporal specificity (from very detailed to highly abstract). Measure how performance scales with annotation informativeness to distinguish between language shortcutting and multimodal integration.

3. **Human-in-the-loop error analysis**: Have human evaluators attempt the task under conditions that systematically limit either visual information (low resolution) or contextual information (viewing clips in isolation). Compare failure patterns to model failures to identify whether models share human cognitive limitations or exhibit distinct architectural biases.