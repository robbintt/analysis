---
ver: rpa2
title: Can LLMs Understand What We Cannot Say? Measuring Multilevel Alignment Through
  Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels
arxiv_id: '2512.13142'
source_url: https://arxiv.org/abs/2512.13142
tags:
- stigma
- abortion
- gpt-5
- mini
- secrecy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly used in sensitive
  health contexts but lack systematic evaluation of their understanding of complex
  psychological constructs. This study investigates whether LLMs coherently represent
  abortion stigma across cognitive, interpersonal, and structural levels using the
  validated ILAS scale with 627 demographically diverse personas across five leading
  LLMs.
---

# Can LLMs Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels

## Quick Facts
- arXiv ID: 2512.13142
- Source URL: https://arxiv.org/abs/2512.13142
- Reference count: 40
- Key outcome: LLMs fail to coherently represent abortion stigma across cognitive, interpersonal, and structural levels, producing fragmented and contradictory outputs that could compound harm in sensitive health contexts

## Executive Summary
This study investigates whether large language models can coherently represent complex psychological constructs by evaluating their understanding of abortion stigma across cognitive, interpersonal, and structural levels. Using the validated ILAS scale with 627 demographically diverse personas across five leading LLMs, the research reveals critical failures: models underestimate cognitive stigma while overestimating interpersonal stigma, introduce demographic biases absent from human data, and produce internal contradictions. These findings demonstrate that current alignment approaches ensure appropriate surface-level language but fail to guarantee coherent internal representations across theoretically related constructs. The work establishes that understanding what people cannot say requires more than surface-level appropriateness—it demands representational coherence across levels that current models lack.

## Method Summary
The study employs the validated 20-item ILAS scale to measure abortion stigma across four subscales: worries about judgment, isolation, self-judgment, and community condemnation. Researchers generated 627 demographically diverse personas matching the original ILAS validation study's marginal distributions, applying plausibility constraints to prevent implausible combinations. Five LLMs (GPT-5 mini, OSS-20B, Llama-3.1-8B-Instruct, Gemma-3-4B-IT, Llama-3.1-70B-Instruct) were prompted with these personas to complete the ILAS questionnaire. Model outputs were compared to human validation data through Welch's t-tests, multivariable OLS regression to detect introduced biases, and logistic regression to test coherence between stigma scales and behavioral outcomes like secrecy.

## Key Results
- Models underestimate cognitive stigma (self-judgment) while overestimating interpersonal stigma (worries about judgment, isolation)
- Demographic biases were introduced: higher stigma predicted for younger, less educated, and non-White personas compared to human data
- Internal contradictions observed: models simultaneously overestimated isolation yet predicted isolated individuals are less secretive
- OSS-20B showed zero variance in self-judgment responses across all personas
- No "Never" responses for secrecy despite 36% of humans reporting no secrecy about abortion

## Why This Works (Mechanism)

### Mechanism 1
Current alignment approaches ensure appropriate surface-level language but not coherent internal representations across theoretical levels. The paper demonstrates this through a validated psychometric instrument (ILAS) measuring stigma across cognitive, interpersonal, and structural dimensions simultaneously. Models pass language appropriateness checks while failing internal coherence tests—specifically, they overestimate interpersonal stigma while underestimating cognitive stigma, and show contradictory isolation-secrecy relationships. Assumption: Coherence across theoretically related constructs indicates genuine understanding, while fragmentation indicates superficial pattern-matching. Evidence: Models underestimate cognitive stigma while overestimating interpersonal stigma [abstract]; behavioral science frameworks needed for auditing [section 7.4].

### Mechanism 2
Demographic persona prompting reveals systematically amplified biases absent from human validation data. When LLMs are prompted with demographic characteristics, they introduce associations not present in original human study—specifically, assigning higher stigma to younger, less educated, and non-White personas. The paper uses multivariable OLS regression comparing model coefficients to human baseline coefficients to detect these introduced biases. Assumption: Validated psychometric scales with human comparison data provide reliable ground truth for detecting model bias introduction. Evidence: Models introduced biases absent from human data for age and race [section 5.1.1, 5.1.6].

### Mechanism 3
Models fail to capture cyclical feedback processes that characterize complex psychological phenomena. Stigma operates through feedback loops where concealment reinforces stigma, but models treat related constructs (isolation, secrecy, disclosure) as independent. The paper shows models simultaneously overestimate isolation yet predict isolated individuals are less secretive—a logical contradiction revealing fragmented representations. Assumption: Psychological constructs with theoretical interdependencies should show coherent statistical relationships in model outputs. Evidence: Three models showed negative associations between isolation and secrecy, contradicting both human patterns and their own predictions [section 6.0.2].

## Foundational Learning

- **Psychometric validation and subscale structure**: Why needed here: The ILAS scale has four subscales that must remain theoretically independent while correlating appropriately with overall stigma. Understanding this structure is essential for detecting incoherence. Quick check: Can you explain why finding a negative correlation between isolation and secrecy contradicts the theoretical model of stigma?

- **Multilevel theoretical frameworks**: Why needed here: The paper's core insight is that stigma operates simultaneously at cognitive (internal), interpersonal (social), and structural (normative) levels. Each level requires different evaluation approaches and has different failure modes. Quick check: Why would underestimating self-judgment while overestimating social judgment create different harms than a uniform bias?

- **Demographic persona generation with plausibility constraints**: Why needed here: The paper generated 627 personas matching marginal distributions from the original study while applying logical constraints (e.g., preventing 15-18 year-olds with abortions 40 years ago). Poorly constrained personas could introduce confounds. Quick check: What types of implausible demographic combinations would most threaten validity in reproductive health contexts?

## Architecture Onboarding

- **Component map**: Persona Generator -> ILAS Questionnaire Engine -> Baseline Comparator -> Regression Analyzer -> Coherence Tester
- **Critical path**: Generate personas → Run baseline condition → Run demographic-prompted condition → Calculate subscale scores → Compare to human validation data via regression → Test cross-construct coherence (stigma-secrecy relationships) → Identify contradictions and introduced biases
- **Design tradeoffs**: Using marginal distributions rather than joint distributions may create implausible combinations (addressed via plausibility checks); low temperature (0.1) reduces variance but may underrepresent natural response diversity; three runs per model balances computational cost against variance estimation; dichotomizing secrecy responses avoids class imbalance but loses granularity
- **Failure signatures**: Level imbalance (cognitive underestimated while interpersonal overestimated); introduced bias coefficients (model shows significant associations for demographics where human data showed none); cross-construct contradiction (negative isolation-secrecy associations); universal response patterns (no "Never" selections for secrecy); zero variance responses (OSS-20B assigned identical self-judgment scores)
- **First 3 experiments**: 1. Baseline comparison: Run models on ILAS with and without demographic prompting using Welch's t-tests to establish whether demographics shift representations; 2. Demographic regression: Fit multivariable OLS models with demographics as predictors, compare coefficients to human validation data to detect introduced biases; 3. Stigma-secrecy coherence: Fit logistic regression predicting secrecy from each stigma scale, check for internal contradictions and universal response patterns

## Open Questions the Paper Calls Out

- **Open Question 1**: Can validated psychometric scale-based evaluation methods detect multilevel representational failures in LLMs across other stigmatized health contexts beyond abortion? Basis: Authors state "Future work should extend this methodology using validated psychometric scales to other stigmatized public health contexts (mental health, HIV, eating disorders)." Unresolved because this study only examined abortion stigma; generalizability to other conditions with different theoretical structures remains unknown.

- **Open Question 2**: What are the mechanistic sources within model internals that produce incoherent representations across stigma levels? Basis: Authors call to "use model internals to investigate mechanistic sources of these failures." Unresolved because the study identified failures empirically through output analysis but did not examine internal representations or attention mechanisms that produce fragmented, contradictory stigma representations.

- **Open Question 3**: Can prompting strategies like Graph of Thought improve models' ability to represent cyclical, recursive psychological processes that stigma entails? Basis: Authors state "Graph of Thought prompting could potentially reveal which stigma dimensions models consider, though our findings suggest models may lack representations of recursive processes regardless of prompting method." Unresolved because current evaluation relies on Chain-of-Thought prompting which forces linear explanations.

- **Open Question 4**: Would interventions that improve multilevel representational coherence generalize across model architectures and training paradigms? Basis: Authors call to "test interventions that improve multilevel coherence." Unresolved because the study documented failures across five models but did not test remediation approaches; whether targeted fine-tuning, curriculum learning, or architectural modifications can address these failures is unknown.

## Limitations

- Use of marginal distributions rather than joint distributions may create implausible demographic combinations despite plausibility constraints
- Exclusive focus on abortion stigma limits generalizability to other sensitive domains with different theoretical structures
- Deterministic behavior of OSS-20B (zero variance in self-judgment scores) suggests some models may not capture natural response diversity
- Simplified secrecy measure (dichotomized) may lose important granularity in understanding stigma-secrecy relationships

## Confidence

- **High Confidence**: Level imbalance finding (underestimating cognitive stigma while overestimating interpersonal stigma) is well-supported by statistical comparisons to human validation data
- **Medium Confidence**: Introduced demographic biases finding is moderately supported, though lack of joint distribution generation introduces uncertainty
- **Medium Confidence**: Cross-construct contradictions (negative isolation-secrecy associations) are statistically evident but require careful interpretation

## Next Checks

1. **Replication with Joint Distributions**: Generate personas using joint demographic distributions rather than marginal distributions to test whether implausible combinations drive the observed biases and contradictions

2. **Alternative Sensitive Domains**: Apply the same psychometric evaluation framework to other psychological constructs (e.g., mental health stigma, trauma responses) to test generalizability of the coherence failure findings

3. **Dynamic Response Generation**: Test whether increasing temperature or using alternative sampling strategies affects the universal response patterns and deterministic behaviors observed in some models