---
ver: rpa2
title: A Study on Regularization-Based Continual Learning Methods for Indic ASR
arxiv_id: '2508.06280'
source_url: https://arxiv.org/abs/2508.06280
tags:
- learning
- naive
- languages
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates continual learning for multilingual Automatic\
  \ Speech Recognition (ASR) in low-resource Indian languages, focusing on the problem\
  \ of catastrophic forgetting when sequentially learning new languages. The study\
  \ evaluates three regularization-based continual learning strategies\u2014Elastic\
  \ Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without\
  \ Forgetting (LwF)\u2014applied to a Conformer-based hybrid RNN-T/CTC model trained\
  \ incrementally across nine Indian languages."
---

# A Study on Regularization-Based Continual Learning Methods for Indic ASR

## Quick Facts
- arXiv ID: 2508.06280
- Source URL: https://arxiv.org/abs/2508.06280
- Authors: Gokul Adethya T; S. Jaya Nirmala
- Reference count: 40
- One-line primary result: LwF and MAS outperform EWC and baseline in mitigating forgetting for multilingual ASR, but all methods face stability-plasticity trade-offs with persistent forgetting under sequential learning.

## Executive Summary
This paper investigates continual learning for multilingual Automatic Speech Recognition (ASR) in low-resource Indian languages, focusing on catastrophic forgetting when sequentially learning new languages. The study evaluates three regularization-based continual learning strategies—Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF)—applied to a Conformer-based hybrid RNN-T/CTC model trained incrementally across nine Indian languages. Using a subset of the IndicSUPERB benchmark with 3,000 low-resource utterances per language, the research compares these methods against naive fine-tuning. Key results show that LwF and MAS outperform EWC and the baseline in mitigating forgetting, particularly under noisy conditions, with LwF achieving the most stable Word Error Rates (WERs) below 1.0 and strong Backward Transfer scores. However, all methods exhibit a stability-plasticity trade-off: longer training reduces WER but increases forgetting. Notably, RNN-T models yield lower WERs than CTC but amplify forgetting. The study concludes that while regularization-based continual learning methods improve knowledge retention, achieving practical deployment-level performance remains challenging due to persistent forgetting and suboptimal WERs during new language acquisition.

## Method Summary
The study employs a Conformer-based hybrid RNN-T/CTC model (~130M parameters) pretrained on Hindi from the NeMo toolkit, fine-tuned incrementally on nine Indian languages (Hindi → Bengali → Marathi → Telugu → Tamil → Urdu → Gujarati → Kannada → Odia) without data replay. Three regularization-based continual learning strategies are evaluated: Elastic Weight Consolidation (EWC) with λ∈{5,10}, Memory Aware Synapses (MAS) with λ=1 and α_ctx∈{0.3,1.0}, and Learning without Forgetting (LwF) with α_KD∈{0.1,0.5} and α_ctx=0.3. Training uses Adam optimizer (lr=1e-4) with base loss L_base = 0.7×L_RNNT + 0.3×L_CTC. Each language task is trained for 1, 2, 5, or 10 epochs using 3,000 utterances per language (2,000 clean + 1,000 noisy) from a subset of the IndicSUPERB benchmark. Word Error Rate (WER) is measured on clean/noisy splits for both RNN-T and CTC paths, alongside Backward Transfer (BWT) to quantify forgetting. The code is available at https://github.com/FrozenWolf-Cyber/Indic-CL-ASR.

## Key Results
- LwF and MAS outperform EWC and naive fine-tuning in mitigating catastrophic forgetting, with LwF achieving the most stable WERs below 1.0.
- All methods exhibit a stability-plasticity trade-off: longer training (e.g., 10 epochs) reduces WER but increases forgetting, particularly in RNN-T paths.
- RNN-T models yield lower WERs than CTC but amplify forgetting, highlighting a trade-off between accuracy and knowledge retention.

## Why This Works (Mechanism)
The regularization-based methods work by constraining parameter updates to preserve knowledge from previously learned languages. EWC uses Fisher information to identify important parameters and penalizes their modification, while MAS computes parameter importance based on sensitivity to output changes. LwF employs knowledge distillation, forcing the model to maintain performance on earlier languages while learning new ones through KL-divergence between old and new model outputs. This prevents catastrophic forgetting by balancing plasticity (learning new languages) with stability (retaining old knowledge), though the trade-off manifests as increased forgetting with longer training or when using RNN-T paths that prioritize accuracy over retention.

## Foundational Learning
- **Continual Learning**: Training models on sequential tasks without forgetting previous knowledge. Why needed: Addresses catastrophic forgetting in dynamic ASR deployment. Quick check: Verify sequential training order and absence of data replay.
- **Catastrophic Forgetting**: Degradation of performance on earlier tasks when learning new ones. Why needed: Central problem this study aims to mitigate. Quick check: Monitor WER and BWT across tasks.
- **Elastic Weight Consolidation (EWC)**: Regularization method that penalizes changes to important parameters. Why needed: Prevents overwriting knowledge critical for earlier tasks. Quick check: Confirm Fisher matrix computation and λ_EWC tuning.
- **Memory Aware Synapses (MAS)**: Computes parameter importance based on sensitivity to output changes. Why needed: Alternative to EWC for preserving task-specific knowledge. Quick check: Validate MAS importance scores and λ_MAS impact.
- **Learning without Forgetting (LwF)**: Distillation-based method that preserves outputs for old tasks while learning new ones. Why needed: Maintains performance on previous languages during new language acquisition. Quick check: Ensure KL-divergence distillation loss is correctly implemented.

## Architecture Onboarding
- **Component Map**: Pretrained Conformer → Sequential Language Tasks → CL Method (EWC/MAS/LwF) → WER/BWT Evaluation
- **Critical Path**: Hindi pretraining → Sequential fine-tuning (9 languages) → CL regularization → WER and BWT computation
- **Design Tradeoffs**: RNN-T vs. CTC paths (RNN-T: lower WER, higher forgetting; CTC: higher WER, lower forgetting). LwF vs. MAS vs. EWC (LwF: best stability, MAS: moderate performance, EWC: weakest).
- **Failure Signatures**: High WER (>1.0) with LwF when α_KD=0.5 (model barely learns new tasks); EWC shows good WER but very negative BWT on RNN-T (excessive forgetting of earlier languages).
- **First Experiments**: 1) Train with LwF (α_KD=0.1, α_ctx=0.3, 5 epochs) and evaluate WER/BWT. 2) Compare RNN-T vs. CTC WERs after 10 epochs. 3) Test EWC with λ=5 and λ=10 to assess sensitivity.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are constrained by the limited training data (3,000 utterances per language) and specific subset of IndicSUPERB, which may not fully represent Indian language diversity.
- Sequential training order could introduce biases, with earlier languages benefiting more from pretraining while later ones face more forgetting.
- The study does not explore data augmentation or replay-based methods, which might mitigate forgetting more effectively.

## Confidence
- **High Confidence**: LwF and MAS outperform EWC and naive fine-tuning in reducing forgetting; stability-plasticity trade-off and higher forgetting in RNN-T vs. CTC are consistently observed.
- **Medium Confidence**: Achieving practical deployment-level performance remains challenging, but depends on specific deployment criteria not explicitly defined; LwF effectiveness under noisy conditions is supported but could vary with noise profiles.
- **Low Confidence**: Generalizability to other low-resource language settings or larger datasets is uncertain due to study limitations to IndicSUPERB subset and sequential training order.

## Next Checks
1. Reproduce with varied hyperparameters (e.g., α_KD=0.1 vs. 0.5, λ_EWC=5 vs. 10) to confirm sensitivity of LwF and EWC to these parameters.
2. Expand to additional Indian languages or other low-resource language families to assess generalizability of findings.
3. Compare regularization-based methods against data replay strategies (e.g., episodic memory) to determine if they offer superior mitigation of catastrophic forgetting.