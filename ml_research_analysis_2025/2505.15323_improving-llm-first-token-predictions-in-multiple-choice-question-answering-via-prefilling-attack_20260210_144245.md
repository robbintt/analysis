---
ver: rpa2
title: Improving LLM First-Token Predictions in Multiple-Choice Question Answering
  via Prefilling Attack
arxiv_id: '2505.15323'
source_url: https://arxiv.org/abs/2505.15323
tags:
- prefilling
- answer
- accuracy
- token
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a key limitation in evaluating Large Language
  Models (LLMs) on multiple-choice question answering (MCQA) tasks using first-token
  probability (FTP). While FTP is efficient, it can fail when the model's first token
  is misaligned (e.g., a full sentence instead of a label) or misinterpreted (e.g.,
  a valid label used grammatically rather than as the answer).
---

# Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack

## Quick Facts
- arXiv ID: 2505.15323
- Source URL: https://arxiv.org/abs/2505.15323
- Reference count: 30
- This paper introduces a "prefilling attack" that improves first-token probability (FTP) evaluation in MCQA by prepending a natural-language prefix to steer models toward generating valid answer tokens as the first token.

## Executive Summary
This paper addresses a key limitation in evaluating Large Language Models (LLMs) on multiple-choice question answering (MCQA) tasks using first-token probability (FTP). While FTP is efficient, it can fail when the model's first token is misaligned (e.g., a full sentence instead of a label) or misinterpreted (e.g., a valid label used grammatically rather than as the answer). To address this, the authors introduce a "prefilling attack," a simple natural-language prefix (e.g., "The correct option is:") prepended to the model output to steer it toward generating a valid answer as the first token. This approach improves accuracy, calibration, and output consistency across diverse LLMs and benchmarks, often matching or surpassing the performance of more expensive open-ended generation methods without requiring model modifications. It significantly reduces misalignment and misinterpretation errors, making FTP-based evaluation more reliable and trustworthy.

## Method Summary
The method involves prepending a natural-language prefix immediately after the assistant start token in the model's chat template. This prefix, such as "Given the question and the possible options, my answer is: ", steers the model's next-token distribution toward generating a valid answer label (A/B/C/D) as the first token. The approach works by conditioning the model's generation on the injected context, exploiting instruction-tuned models' tendency to complete declarative statements with categorical answers. No model parameters are modified—the prefix is added to the prompt structure before inference.

## Key Results
- FTP+Prefilling consistently outperforms standard FTP across all tested models and benchmarks
- Accuracy improvements range from +3.6% to +7.7% on MMLU, with similar gains on other datasets
- First-Token Validity Rate (FTVR) increases from near-zero to 100% for most models when using prefilling
- Calibration metrics (ECE, Brier score) improve significantly, indicating better confidence-accuracy alignment
- Prefilling reduces misalignment errors (generating non-answer tokens) and misinterpretation errors (using valid tokens grammatically)

## Why This Works (Mechanism)

### Mechanism 1: Output Distribution Steering via Context Conditioning
- Claim: Prepending a natural-language prefix shifts the model's next-token probability distribution toward symbolic answer tokens.
- Mechanism: Instruction-tuned LLMs treat the injected prefix as already-generated content, conditioning subsequent generation on this context. The phrase "The correct option is:" creates strong expectation for a symbolic label (A/B/C/D) as the immediate continuation.
- Core assumption: The model's training on instruction-following data includes patterns where declarative statements precede categorical answers.
- Evidence anchors: [abstract] "prepended to the model output to steer the model to respond with a clean, valid option, without modifying its parameters"; [Section 3.2] "the model treats this injected text as previously generated content, naturally continuing its response from the provided context"; [corpus] Limited direct corpus support; related work on directive interference (OI-Bench) suggests LLMs are susceptible to directive signals, but this specific steering mechanism via prefilling is not extensively studied externally.
- Break condition: Models whose training data lacks answer-declaration patterns may show reduced or no benefit from prefilling.

### Mechanism 2: Reduction of First-Token Misalignment
- Claim: Prefilling constrains the model's valid first-token vocabulary space, reducing the probability of generating non-answer tokens (e.g., "The", "I", "Given").
- Mechanism: By syntactically completing a sentence that demands a direct object (the option label), prefilling makes grammatically valid continuations with non-answer tokens far less probable. The model's token predictions shift from "what would I say first?" to "what completes this sentence?"
- Core assumption: The prefilling template is linguistically structured to create strong syntactic expectation for a short, categorical completion.
- Evidence anchors: [Section 3.1] "the top-ranked token may lie outside the valid answer set (i.e., misalignment)"; [Section 4.4, Table 2] Full-vocabulary evaluation shows FTVR (First-Token Validity Rate) increasing from 2.4% to 100.0% for Llama-3.1-8B on SciQ with prefilling; [corpus] "Right Answer, Wrong Score" paper confirms LLM evaluation inconsistencies in MCQA, supporting the fragility claim, but does not directly validate this specific mitigation mechanism.
- Break condition: Templates that are syntactically open-ended (e.g., "I think...") may not provide sufficient constraint.

### Mechanism 3: Calibration Improvement via Output Format Regularization
- Claim: Prefilling improves the alignment between model confidence scores and actual correctness by reducing format-driven noise.
- Mechanism: When models generate verbose preambles, their first-token probabilities reflect uncertainty about *how to begin* rather than *which answer is correct*. Prefilling eliminates this uncertainty source, making the probability signal more directly reflective of answer confidence.
- Core assumption: Miscalibration in FTP is partially driven by format uncertainty rather than purely reasoning uncertainty.
- Evidence anchors: [Section 4.5, Table 3] ECE drops from 20.6% to 13.0% for Llama-3.1-8B on MMLU; Brier scores decrease across all tested models; [Section 4.5] "prefilling consistently improves model calibration...lowering the expected calibration error across all tested models"; [corpus] No direct external validation of calibration improvement via prefilling in related literature.
- Break condition: If a model is already well-calibrated on FTP tasks, prefilling may yield diminishing returns on calibration metrics.

## Foundational Learning

- **Autoregressive Next-Token Prediction**
  - Why needed here: FTP evaluation assumes the first token's probability distribution reflects the model's answer choice. Understanding how LLMs predict tokens sequentially is essential to grasp why prefilling alters this distribution.
  - Quick check question: Given the prompt "The capital of France is", which token would an autoregressive model assign highest probability to as its next token?

- **Instruction-Tuned Chat Formats**
  - Why needed here: Prefilling exploits the structured dialogue format (user/assistant turns with special tokens) used by instruction-tuned models. The prefix is injected after the assistant role marker.
  - Quick check question: In Llama's chat format, where would you inject the prefilling prefix—before or after the `<|start_header_id|>assistant<|end_header_id|>` sequence?

- **Calibration in Classification**
  - Why needed here: The paper argues prefilling improves calibration (confidence-accuracy alignment). Understanding ECE and Brier scores helps evaluate whether confidence estimates are trustworthy.
  - Quick check question: If a model makes predictions with 80% confidence and is correct 60% of the time, is it overconfident, underconfident, or well-calibrated?

## Architecture Onboarding

- **Component map**: Prompt Formatter -> Prefilling Injector -> FTP Scorer -> Calibration Evaluator
- **Critical path**: 1) Format question with options using target model's chat template; 2) Inject prefilling prefix (e.g., "Given the question and the possible options, my answer is:") after assistant marker; 3) Forward pass to obtain next-token logits; 4) Extract probabilities for valid answer tokens only; 5) Compare predicted vs. ground-truth label
- **Design tradeoffs**: Template selection (longer templates provide stronger steering but risk introducing bias; shorter templates are more neutral but may be less effective); Token filtering (restricting to A/B/C/D tokens improves validity but may hide misalignment issues; full-vocabulary evaluation reveals true first-token behavior but requires post-hoc validation)
- **Failure signatures**: Zero FTVR on standard FTP (model consistently generates non-answer first tokens—indicating severe misalignment); High Continuation Diversity (CD) (valid first token followed by inconsistent second tokens—suggesting misinterpretation); Persistent miscalibration after prefilling (ECE remains high despite accuracy gains—suggesting deeper confidence estimation issues)
- **First 3 experiments**: 1) Baseline FTP vs. FTP+Prefilling comparison on a held-out benchmark subset (e.g., 500 MMLU examples) using the default template; measure accuracy gain and FTVR improvement; 2) Template robustness test with 3-5 alternative prefilling phrases (varying length and formality) to assess sensitivity; report mean accuracy and standard deviation; 3) Full-vocabulary decoding analysis on 100 examples to inspect top-5 tokens with and without prefilling; quantify how often the correct answer appears in top-5 vs. top-1

## Open Questions the Paper Calls Out

- **Open Question 1**: How does prefilling interact with different decoding strategies such as beam search, nucleus sampling, or varying temperature settings? [explicit] The authors state in the Limitations section: "we do not examine how other decoding regimes, such as beam search or temperature sampling, might interact with these failure modes." Why unresolved: The study focuses exclusively on greedy decoding; other decoding strategies may amplify, diminish, or qualitatively alter prefilling's effectiveness. What evidence would resolve it: Controlled experiments comparing prefilling performance across multiple decoding regimes, measuring accuracy, calibration, and validity rate under each configuration.

- **Open Question 2**: Does the prefilling strategy generalize to multilingual MCQA benchmarks and out-of-distribution domain tasks? [explicit] The Limitations section notes: "The scope of our study is also limited to a subset of English-language benchmarks and does not yet consider multilingual or domain-shifted tasks." Why unresolved: Language-specific tokenization and instruction-tuning practices may cause prefilling prefixes to behave differently across languages or specialized domains. What evidence would resolve it: Experiments applying the same prefilling templates to multilingual benchmarks (e.g., multilingual MMLU) and domain-shifted datasets, with analysis of cross-lingual template transferability.

- **Open Question 3**: Can prefilling mitigate misalignment effects in non-symbolic generation tasks such as open-ended QA, summarization, or dialogue? [explicit] The authors acknowledge: "our study does not explore whether similar misalignment effects occur in open-ended QA, summarization, or dialogue tasks, where symbolic constraints are looser or absent." Why unresolved: The prefilling mechanism was designed for constrained symbolic output; its utility in tasks requiring flexible generation remains unknown. What evidence would resolve it: Studies applying prefilling prefixes to open-ended generation tasks and measuring output quality, relevance, and format consistency using task-appropriate metrics.

- **Open Question 4**: What are the mechanistic explanations for why prefilling shifts the next-token distribution toward symbolic outputs? [inferred] The paper demonstrates empirical effectiveness but does not investigate the underlying attention patterns, representation changes, or token probability dynamics that cause prefilling to work. Why unresolved: Understanding the mechanism could enable principled prefix design rather than trial-and-error template selection. What evidence would resolve it: Probing experiments analyzing attention heads, intermediate layer representations, and logit distributions with and without prefilling to identify causal mechanisms.

## Limitations

- Prefilling's effectiveness depends on model familiarity with declarative instruction patterns and chat-format conventions, with uncertain robustness to adversarial or domain-shifted prompts
- The approach assumes closed-label formats (A/B/C/D), limiting applicability to open-ended or numerical-response questions
- Reliance on token-level analysis may obscure semantic errors where grammatically valid tokens are contextually inappropriate

## Confidence

- **High Confidence**: The prefilling attack reliably improves first-token validity rates (FTVR) and reduces misalignment errors across tested models
- **Medium Confidence**: Claims about improved calibration (lower ECE, Brier scores) are supported by quantitative evidence but lack external validation or theoretical grounding
- **Low Confidence**: The assertion that prefilling "often matches or surpasses" open-ended generation performance is overstated; the paper shows parity in accuracy but does not benchmark against sampling-based methods on calibration or robustness metrics

## Next Checks

1. **Adversarial Prompt Robustness**: Test prefilling on prompts containing negation ("Which option is NOT correct?") or complex reasoning steps to assess whether the prefix consistently steers toward valid answer tokens or breaks under syntactic ambiguity.

2. **Cross-Domain Generalization**: Apply prefilling to numerical-answer MCQA (e.g., mathematics benchmarks) and open-ended classification tasks to quantify degradation in accuracy when the closed-label assumption fails.

3. **Calibration Transferability**: Compare prefilling's calibration gains against a control intervention (e.g., temperature scaling or label smoothing) on the same model/dataset pairs to isolate whether improvements stem from format regularization or mere output consistency.