---
ver: rpa2
title: Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery
  Rate Control
arxiv_id: '2511.11711'
source_url: https://arxiv.org/abs/2511.11711
tags:
- features
- feature
- knockoff
- interpretability
- selected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Model-X knockoffs to control false discovery\
  \ rate (FDR) in sparse autoencoder (SAE) feature selection, addressing the problem\
  \ of distinguishing real computational patterns from spurious correlations in mechanistic\
  \ interpretability. The core method uses equi-correlated Gaussian knockoffs combined\
  \ with \u21131-regularized logistic regression to select features with provable\
  \ FDR guarantees, achieving 129 genuine features from 512 candidates at target FDR\
  \ q=0.1 for sentiment classification."
---

# Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control

## Quick Facts
- **arXiv ID**: 2511.11711
- **Source URL**: https://arxiv.org/abs/2511.11711
- **Authors**: Tsogt-Ochir Enkhbayar
- **Reference count**: 4
- **One-line result**: Introduces Model-X knockoffs to control FDR in SAE feature selection, achieving 129 genuine features from 512 candidates at target FDR q=0.1 for sentiment classification

## Executive Summary
This paper addresses the critical problem of distinguishing real computational patterns from spurious correlations in mechanistic interpretability of sparse autoencoders (SAEs). The core contribution is applying Model-X knockoffs to SAE feature selection, providing provable false discovery rate (FDR) control when identifying features associated with task performance. Using equi-correlated Gaussian knockoffs combined with ℓ1-regularized logistic regression, the method achieves 5.40× signal-to-noise separation between selected and rejected features, identifying 25% of highly active SAE features as carrying task-relevant signal versus 75% being noise. This demonstrates that naive feature analysis approaches are dominated by false positives and provides a rigorous statistical framework for reliable feature discovery in interpretability research.

## Method Summary
The method applies Model-X knockoffs to SAE feature selection for sentiment classification. First, SAE activations are reduced from 32,768 dimensions to the top 512 features by energy (mean absolute activation). A ridge-regularized covariance matrix is estimated from n=4,096 training samples. Equi-correlated Gaussian knockoffs are constructed using the estimated covariance, then ℓ1-regularized logistic regression is fit on the augmented design matrix [X|X̃]. Knockoff statistics W_j = |β_j| - |β̃_j| are computed for each feature, and the knockoff+ threshold τ at FDR level q=0.1 selects genuine features. The approach provably controls FDR under standard Model-X assumptions (Gaussian surrogate for latent distribution) with finite-sample guarantees.

## Key Results
- Achieved 129 genuine features from 512 candidates at target FDR q=0.1 for sentiment classification
- Demonstrated 5.40× signal-to-noise separation between selected and rejected features
- Showed 25% of highly active SAE features carry task-relevant signal versus 75% being noise
- Validated that naive feature analysis approaches are dominated by false positives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic "knockoff" features provide a valid null distribution for testing feature importance while preserving correlation structure.
- **Mechanism**: Knockoffs are constructed to satisfy pairwise exchangeability with original features under swap operations, and conditional independence from the response Y given original features X. The Gaussian construction uses Ẋ = X(I - Σ⁻¹S) + U·Chol(2S - SΣ⁻¹S)ᵀ where S is a diagonal matrix with 0 ⪯ S ⪯ 2Σ.
- **Core assumption**: Feature distribution can be adequately approximated by a Gaussian surrogate; covariance Σ is estimable from n samples where n > p (after reduction).
- **Evidence anchors**: [abstract] "using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions"; [Section 3.3, Algorithm 2] Details ridge-regularized covariance estimation and equi-correlated knockoff sampling.
- **Break condition**: If SAE features deviate substantially from multivariate Gaussian (e.g., heavy tails, discrete modes), knockoff validity degrades; if n ≤ p, covariance estimation fails without aggressive regularization.

### Mechanism 2
- **Claim**: L1-regularized logistic regression coefficients provide discriminative importance scores that separate genuine features from knockoffs.
- **Mechanism**: The augmented design matrix [X|Ẋ] doubles the feature space. L1 penalty induces sparsity, so only informative features receive non-zero coefficients. For genuine features, |β_j| > |β̃_j| on average because X_j has true association with Y while Ẋ_j does not.
- **Core assumption**: Regularization strength C is appropriately tuned; the logistic model captures the feature-label relationship sufficiently; features are not perfectly collinear.
- **Evidence anchors**: [Section 3.4, Eq. 5-6] Explicitly defines the L1-regularized objective and knockoff statistic W_j = |β_j| - |β̃_j|; [Section 4.2.2] "SNR = E[W_j | W_j ≥ τ] / E[|W_j| | W_j < τ] = 0.363/0.067 = 5.40" demonstrating separation.
- **Break condition**: If regularization is too weak, both original and knockoff features get non-zero coefficients; if too strong, genuine features are zeroed out. SAGA solver convergence issues with ill-conditioned data.

### Mechanism 3
- **Claim**: The knockoff+ threshold τ provides finite-sample FDR control regardless of feature dependence structure.
- **Mechanism**: The threshold τ = min{t : (1 + #{j: W_j ≤ -t}) / max{1, #{j: W_j ≥ t}} ≤ q} counts features where knockoffs outperform originals (empirical null proxy). The "+1" in numerator provides conservative bias ensuring E[false discoveries / discoveries] ≤ q.
- **Core assumption**: Knockoffs were correctly constructed (Mechanism 1 holds); exchangeability under the null.
- **Evidence anchors**: [Section 3.5, Theorem 1 citation] References Barber & Candès (2015) for finite-sample guarantee; [Section 4.2.1] At q=0.1, threshold τ=0.158 selected 129/512 features; negative statistics (Table 2) empirically validate that some features are indeed null.
- **Break condition**: If Model-X assumption (knowing true X distribution) is violated, FDR control is no longer guaranteed. The Gaussian surrogate is an approximation.

## Foundational Learning

- **Concept: Multiple Testing and False Discovery Rate**
  - Why needed here: Testing 32,000+ SAE features creates massive multiple comparison problems; uncorrected approaches yield ~75% false positives per this paper's findings.
  - Quick check question: If you test 1,000 null features at α=0.05 without correction, how many false positives do you expect? (Answer: ~50. FDR controls the expected proportion among discoveries, not per-test error.)

- **Concept: L1 Regularization and Sparsity**
  - Why needed here: Provides both feature selection and importance weights; critical for the knockoff statistic computation.
  - Quick check question: Why does L1 produce sparse solutions while L2 does not? (Answer: L1 constraint region has corners/vertices at sparse points; L2 is spherical.)

- **Concept: Covariance Estimation in High Dimensions**
  - Why needed here: Knockoff construction requires accurate Σ estimation; when p approaches n, sample covariance is singular.
  - Quick check question: With n=500 samples and p=400 features, what goes wrong with unregularized covariance estimation? (Answer: Eigenvalues become unreliable; matrix may not be positive definite. Ridge regularization λI stabilizes.)

## Architecture Onboarding

- **Component map**: SAE Activations (n×m) → Energy-based reduction (n×k, k=512) → Covariance estimation (ridge-regularized) → Knockoff sampling (Gaussian, equi-correlated) → L1 Logistic Regression on [X|Ẋ] (n×2k) → Knockoff statistics W_j = |β_j| - |β̃_j| → Knockoff+ threshold τ at FDR level q → Selected features Ŝ = {j: W_j ≥ τ}

- **Critical path**: Covariance estimation → positive definite check → Cholesky decomposition. If Cholesky fails, knockoff construction aborts.

- **Design tradeoffs**:
  - Energy-based reduction: Retains high-activity features but may miss rare-but-informative features. k=512 chosen for n>p stability.
  - Equi-correlated knockoffs (sI): Simpler than full S matrix optimization; may be suboptimal for heterogeneous correlations.
  - L1 logistic vs. other importance measures: Coefficient magnitudes are interpretable but sensitive to regularization strength.

- **Failure signatures**:
  - Cholesky failure on Σ_knockoff = 2S - SΣ⁻¹S → s_max too large or Σ estimation poor.
  - All W_j near zero → regularization too strong or signal genuinely weak.
  - Many negative W_j with large magnitude → features capturing dataset-specific artifacts, not generalizable patterns.
  - Selected set >> expected at FDR q → knockoff construction violated exchangeability.

- **First 3 experiments**:
  1. **Sanity check**: Apply to synthetic data where ground truth features are known. Verify FDR ≤ q across multiple random seeds.
  2. **Ablation on k**: Test k=256, 512, 1024 with fixed n=4096. Observe how covariance estimation quality affects selection stability.
  3. **Cross-task validation**: Features selected for sentiment (SST-2) should transfer poorly to a different task (e.g., NER) if they are genuinely task-specific. This tests whether selection captures task-relevant signal vs. dataset artifacts.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the knockoff framework be adapted to handle the full SAE dictionary (p ≫ n) without energy-based pre-filtering that risks discarding rare but important features?
  - Basis in paper: [explicit] The authors note the necessity of feature reduction because n > p is required for stable covariance estimation, and suggest future work explore "approximate knockoff methods that handle p ≫ n settings."
  - Why unresolved: Current covariance estimation becomes intractable or unstable when the number of features vastly exceeds the number of data samples.
  - What evidence would resolve it: Successful application of a modified knockoff procedure to an unreduced SAE dictionary of 32,000+ features while maintaining FDR control.

- **Open Question 2**: Do SAE features validated as "real" for one specific task (e.g., sentiment classification) generalize to distinct downstream tasks, or are they largely task-specific statistical artifacts?
  - Basis in paper: [explicit] Section 5.2 asks future work to investigate "whether features that are 'real' for one task generalize to others."
  - Why unresolved: The current study validates features solely against sentiment labels (SST-2), leaving the generalizability of these "genuine" features to other behaviors unknown.
  - What evidence would resolve it: A cross-task study measuring the retention rate of knockoff-selected features when the target variable Y is changed to a different behavioral benchmark.

- **Open Question 3**: Is statistical significance via knockoffs predictive of causal importance when measured by intervention techniques like activation patching?
  - Basis in paper: [explicit] The authors state that their "method identifies features that are statistically associated with task performance, but this does not necessarily imply causal importance."
  - Why unresolved: High knockoff statistics confirm a feature contains task-relevant information, but this correlation differs from verifying that the feature is a mechanistic cause of the output.
  - What evidence would resolve it: Empirical validation showing a strong correlation between high knockoff statistics (W_j) and large causal effect sizes measured by activation patching or ablation.

## Limitations

- The Gaussian knockoff construction assumes feature distributions can be well-approximated by a multivariate Gaussian, which may not hold for SAE activations with heavy tails or discrete structure.
- The ℓ1-regularized logistic regression is sensitive to regularization strength and may miss genuine features if the penalty is too strong or include noise if too weak.
- Feature reduction via energy-based selection (k=512) retains high-activity features but may exclude rare-but-informative features, potentially biasing toward common patterns.

## Confidence

- **High confidence**: The knockoff+ methodology itself (FDR control theory) and the empirical finding that naive feature analysis yields ~75% false positives.
- **Medium confidence**: The specific SNR calculation (5.40× separation) and the Gaussian approximation for SAE features, which may vary across architectures.
- **Low confidence**: The generalizability of the 25% signal rate across different SAE models, tasks, and layer locations.

## Next Checks

1. Apply the method to a synthetic dataset where ground truth features are known to verify FDR ≤ q across multiple random seeds.
2. Perform ablation studies varying k (256, 512, 1024) with fixed n=4096 to assess stability of covariance estimation and selection.
3. Test cross-task transferability: features selected for sentiment classification should perform poorly on unrelated tasks like NER if they capture task-specific rather than general computational patterns.