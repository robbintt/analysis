---
ver: rpa2
title: Improving Value-based Process Verifier via Low-Cost Variance Reduction
arxiv_id: '2508.10539'
source_url: https://arxiv.org/abs/2508.10539
tags:
- value
- distribution
- state
- variance
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies high variance in Monte Carlo (MC) estimation
  as a key bottleneck limiting the performance of value-based process verifiers for
  LLM reasoning. The authors show that MC estimation is a Minimum Variance Unbiased
  Estimator (MVUE), meaning variance cannot be reduced without additional information.
---

# Improving Value-based Process Verifier via Low-Cost Variance Reduction

## Quick Facts
- **arXiv ID:** 2508.10539
- **Source URL:** https://arxiv.org/abs/2508.10539
- **Reference count:** 40
- **One-line primary result:** ComMCS reduces variance in value-based process verification, improving Best-of-32 accuracy by 2.2-2.8 points on MATH-500 and GSM8K benchmarks.

## Executive Summary
This paper addresses high variance in Monte Carlo estimation as a bottleneck for value-based process verifiers in LLM mathematical reasoning. The authors prove that Monte Carlo estimation is a Minimum Variance Unbiased Estimator (MVUE), meaning variance cannot be reduced without additional information. They propose ComMCS (Compound Monte Carlo Sampling), which constructs an unbiased estimator by linearly combining MC estimators from current and subsequent steps, reducing variance without extra LLM inference cost. Experiments show ComMCS outperforms regression-based and return-distribution modeling baselines by 2.2-2.8 points on Best-of-32 sampling across MATH-500 and GSM8K benchmarks.

## Method Summary
ComMCS constructs an unbiased estimator by linearly combining MC estimators from current and subsequent steps to reduce variance without additional LLM inference. The method assumes one-step value distributions are Gaussian, enabling variance estimation from current and future states. Coefficients for combining estimators are found through dynamic heuristic search rather than analytical solutions. The approach trains verifiers with 9-bin categorical distributions representing value ranges and applies ComMCS during value annotation. Theoretical analysis proves unbiasedness and variance reduction under conditions where future step variance is smaller than current step variance.

## Key Results
- ComMCS achieves 2.2-2.8 point improvements over baselines on Best-of-32 sampling accuracy
- Outperforms VBCE, VMSE, and VCE without ComMCS across MATH-500 and GSM8K benchmarks
- Reduces variance by 10-15% in most samples while maintaining unbiasedness
- Generalizes across different base models (Qwen2.5-Math-7B, Deepseek-math-7B) and search strategies

## Why This Works (Mechanism)
ComMCS works by recognizing that while MC estimation is MVUE (cannot reduce variance without extra information), combining current-step and future-step MC estimates creates a new unbiased estimator with lower variance. The key insight is that future step values contain information about current step values through the Markov property, and this correlation can be exploited to reduce estimation variance. The Gaussian assumption enables practical variance computation, and dynamic coefficient selection ensures variance reduction occurs only when beneficial.

## Foundational Learning
- **Monte Carlo estimation:** Random sampling method for estimating expected values; needed because it's the baseline for value annotation in process verification
- **Minimum Variance Unbiased Estimator (MVUE):** Statistical estimator with lowest possible variance among unbiased estimators; needed to prove MC estimation cannot be improved without additional information
- **Categorical distribution modeling:** Representing values as discrete probability distributions across bins; needed to handle uncertainty in value estimation for training verifiers
- **Variance reduction techniques:** Methods to decrease estimator variance while maintaining unbiasedness; needed to improve training signal quality for verifiers
- **Markov property:** Future states depend only on current states, not past history; needed to justify combining current and future step estimates
- **Gaussian distribution assumption:** Modeling one-step value distributions as normal; needed for practical variance estimation and coefficient selection

## Architecture Onboarding

**Component map:** Dataset construction -> ComMCS implementation -> Verifier training -> Evaluation with BoN/beam search

**Critical path:** The core path involves generating solutions, computing MC rollouts for value annotation, applying ComMCS to construct unbiased low-variance estimators, training the verifier with categorical distributions, and evaluating using search strategies. The most critical components are the variance estimation (Eq. 8, 16, 21) and coefficient selection algorithm.

**Design tradeoffs:** The method trades analytical optimality for practical applicability by using heuristic coefficient search instead of deriving closed-form solutions. This makes the approach more flexible but potentially suboptimal. The Gaussian assumption simplifies implementation but may not hold for all domains.

**Failure signatures:** Coefficient search fails to reduce variance in ~15% of samples when future step variance exceeds current step variance. Distribution assumption violations occur when one-step values are multimodal or heavily skewed. Poor performance on non-mathematical reasoning tasks suggests domain-specific limitations.

**First experiments:** 1) Verify variance reduction percentage across coefficient search space; 2) Test Gaussian distribution assumption on GSM8K values; 3) Compare ComMCS against simple moving average baseline for coefficient selection.

## Open Questions the Paper Calls Out
- **Generalization to code generation:** Can ComMCS be effectively applied to code generation domains where value distributions may differ from mathematical reasoning?
- **Robustness to non-Gaussian distributions:** How well does ComMCS perform when value distributions deviate significantly from Gaussian assumptions?
- **Analytical coefficient optimization:** Can the heuristic search for combination coefficients be replaced by an analytical solution to maximize variance reduction?

## Limitations
- The variance reduction condition (future variance < current variance) is not always satisfied, limiting effectiveness in ~15% of cases
- Relies on Gaussian distribution assumption that may not hold for all reasoning domains or tasks
- Experimental validation limited to 7B parameter models without testing scaling effects to larger models
- Only compared against categorical distribution baselines, not regression-based or other variance reduction techniques

## Confidence
- **High confidence:** Theoretical proof of unbiasedness and variance reduction conditions; core ComMCS algorithm implementation
- **Medium confidence:** Effectiveness of dynamic coefficient selection; validity of Gaussian distribution assumption across domains
- **Low confidence:** Generalization to larger models and non-mathematical reasoning tasks; performance in extreme sample efficiency scenarios

## Next Checks
1. **Coefficient search robustness test:** Implement systematic grid search over coefficient values and measure percentage of samples with successful variance reduction compared to heuristic approach
2. **Distribution assumption validation:** Empirically verify Gaussian assumption on GSM8K by plotting one-step value distributions and performing normality tests; evaluate alternative approximations if distributions are non-Gaussian
3. **Scaling experiment:** Train ComMCS on larger verifier model (34B parameters) and compare variance reduction magnitude and accuracy gains against 7B baseline to test scaling effects