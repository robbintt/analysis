---
ver: rpa2
title: Distributional encoding for Gaussian process regression with qualitative inputs
arxiv_id: '2506.04813'
source_url: https://arxiv.org/abs/2506.04813
tags:
- encoding
- kernel
- where
- qualitative
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating qualitative
  (categorical) inputs into Gaussian process (GP) regression models, which is essential
  for many engineering applications where observations are expensive. The core contribution
  is a "distributional encoding" (DE) method that represents each category level as
  a probability distribution over the output variable, enabling the use of kernel
  methods on distributions (like Wasserstein or MMD distances) to measure similarity
  between categories.
---

# Distributional encoding for Gaussian process regression with qualitative inputs

## Quick Facts
- arXiv ID: 2506.04813
- Source URL: https://arxiv.org/abs/2506.04813
- Authors: Sébastien Da Veiga
- Reference count: 16
- Primary result: Distributional encoding (DE) achieves state-of-the-art GP regression performance with categorical inputs by representing each category level as a probability distribution over outputs, enabling efficient kernel methods while matching or surpassing latent variable models.

## Executive Summary
This paper addresses the challenge of incorporating qualitative (categorical) inputs into Gaussian process regression models, which is essential for many engineering applications where observations are expensive. The core contribution is a "distributional encoding" (DE) method that represents each category level as a probability distribution over the output variable, enabling the use of kernel methods on distributions (like Wasserstein or MMD distances) to measure similarity between categories. This approach generalizes target encoding by using all output samples per category rather than just summary statistics.

The method is validated through extensive numerical experiments on synthetic and real-world datasets, demonstrating state-of-the-art predictive performance compared to existing approaches like LVGP (latent variable GP) and covariance parameterization. Notably, DE achieves similar accuracy to LVGP but with significantly lower computational cost. The paper also presents extensions to classification, multi-task learning, and incorporation of auxiliary data, showing practical benefits in scenarios with limited high-fidelity data but available low-fidelity simulations or related datasets.

## Method Summary
The distributional encoding method transforms categorical inputs into kernel-compatible representations by mapping each category level to the empirical distribution of observed outputs for that level. For a categorical variable u with level l, the encoding is the collection of all output samples Y observed with u=l. A kernel function on these distributions (either MMD or Wasserstein distance) then defines similarity between levels. The GP model combines this categorical kernel with standard kernels for continuous inputs, enabling end-to-end training with standard GP frameworks. Unlike LVGP approaches that learn latent embeddings jointly with hyperparameters, DE calculates embeddings a priori based on empirical data, improving computational efficiency while maintaining predictive accuracy.

## Key Results
- Distributional encoding achieves competitive performance with LVGP on engineering test cases (Beam bending, Borehole, OTL circuit, Piston) while being computationally more efficient
- The method outperforms traditional target encoding (mean encoding) by preserving higher-order moments of the output distribution
- DE successfully extends to classification, multi-task learning, and scenarios with auxiliary data (low-fidelity simulations or related datasets)
- In material design applications, DE shows improved generalization to unseen levels when auxiliary data is available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing categorical levels as probability distributions preserves higher-order moments of the output, enabling the kernel to distinguish between levels with similar means but different variances or shapes.
- **Mechanism:** Standard target encoding reduces a level l to ȳₗ (the mean). DE instead represents l as P̂ᵧ|ᵤ₌ₗ. When the GP kernel (e.g., MMD or Wasserstein) compares two levels, it computes similarity based on the geometric "shape" of these distributions, not just their central tendency.
- **Core assumption:** The conditional distribution P(Y|u=l) contains signal relevant to the regression task that is lost in the mean.
- **Evidence anchors:**
  - [abstract] "DE associates each level with an empirical distribution... makes use of all samples of the target variable."
  - [section 2.3] "...mean or the standard deviation are crude summaries of the response, that may fail to capture complex relationships..."
- **Break condition:** If there are very few observations per level (small Nₜₗ), the empirical distribution is a poor approximation of the true conditional distribution, leading to noisy distance estimates.

### Mechanism 2
- **Claim:** Defining kernels via Maximum Mean Discrepancy (MMD) or Wasserstein distances allows categorical inputs to be integrated into standard GP frameworks while satisfying the requirement for positive semi-definiteness.
- **Mechanism:** Standard kernels (e.g., RBF) cannot handle categorical inputs directly. DE transforms the categorical problem into a distributional comparison problem. The paper leverages theoretical results ensuring that distance-substitution kernels like k(P,Q) = exp(-γ MMD²(P,Q)) are valid positive semi-definite kernels, ensuring the resulting covariance matrix is invertible and the GP is well-defined.
- **Core assumption:** The chosen distributional distance (MMD or W₂) effectively captures the "dissimilarity" between categories in a way that correlates with the target function's behavior.
- **Evidence anchors:**
  - [section 3.1] "Song (2008) showed that kMMD... is positive semi-definite whatever the dimension."
  - [section 3.1] Defines the valid kernel for 1D Wasserstein: kᵂ₂(P,Q) = exp(-γ W₂ᵝ(P,Q)).
- **Break condition:** In the multivariate output case, the standard Wasserstein distance does not guarantee a positive semi-definite kernel; the Sliced Wasserstein distance must be used instead.

### Mechanism 3
- **Claim:** Decoupling the embedding calculation from the GP hyperparameter optimization improves computational efficiency and stability compared to Latent Variable Gaussian Processes (LVGP).
- **Mechanism:** LVGP learns latent coordinates for categorical levels jointly with GP hyperparameters via maximum likelihood, creating a difficult non-convex optimization landscape. DE calculates the distributional embeddings (and thus the similarities between levels) a priori based on the empirical data. This transforms the problem into a standard GP regression with fixed kernel structures, reducing the parameter space.
- **Core assumption:** The empirical distribution provides a sufficiently accurate "fixed" embedding such that the loss of joint optimization flexibility is outweighed by the gains in stability and speed.
- **Evidence anchors:**
  - [abstract] "...matching or surpassing latent variable models while being computationally more efficient."
  - [section 2.2] Describes LVGP limitations: "likelihood landscape can be multimodal... optimization can stall."
- **Break condition:** If the auxiliary information is sparse or the training set is tiny, the "fixed" embedding might be suboptimal compared to a jointly learned manifold that could borrow strength across levels.

## Foundational Learning

- **Concept: Maximum Mean Discrepancy (MMD)**
  - **Why needed here:** This is the core mathematical tool used to quantify the distance between the probability distributions of two categorical levels. Without understanding MMD, the mechanism for how the kernel defines "similarity" is opaque.
  - **Quick check question:** How does MMD measure the distance between two sets of samples (distributions) using a kernel function?

- **Concept: Target Encoding (Mean Encoding)**
  - **Why needed here:** The paper positions its method (Distributional Encoding) as a generalization of this baseline. Understanding the standard approach (and its tendency to lose variance information) clarifies the value proposition of DE.
  - **Quick check question:** Why might replacing a category with the mean of the target variable lead to suboptimal performance compared to using the full distribution?

- **Concept: Reproducing Kernel Hilbert Space (RKHS)**
  - **Why needed here:** The paper relies on "kernel mean embedding," which maps distributions into an RKHS. This ensures the distance metrics used are mathematically valid for building covariance matrices.
  - **Quick check question:** What property must a kernel satisfy to be a valid covariance function in a Gaussian Process?

## Architecture Onboarding

- **Component map:** Input Layer (continuous inputs x + Categorical inputs u) -> Encoding Layer (categorical level l → Empirical Distribution P̂ᵧ|ᵤ₌ₗ) -> Kernel Layer (continuous: Matérn/RBF, categorical: Distance Substitution Kernel) -> Inference Engine (Standard Gaussian Process)

- **Critical path:** The implementation of the Distributional Distance Metric. For 1D regression, the Wasserstein distance is efficient (sorting quantiles). For multi-output or high-dimensional settings, the Sliced Wasserstein or MMD is required.

- **Design tradeoffs:**
  - **Efficiency vs. Stability:** DE is faster than LVGP but less flexible if the initial empirical distributions are noisy (small sample size).
  - **MMD vs. Wasserstein:** MMD is valid in any dimension but requires tuning the base kernel. Wasserstein (W₂) considers geometry but is only strictly valid for 1D (requiring Sliced approximations for higher dims).
  - **Auxiliary Data:** Adding low-fidelity data improves generalization (especially for unseen levels) but adds complexity to data preprocessing.

- **Failure signatures:**
  - **Small Sample per Level:** If Nₜₗ ≈ 1 for many levels, the distribution is a point mass, degrading DE to noisy target encoding.
  - **Strong Interactions:** If the output depends heavily on the interaction between u and continuous x, encoding solely based on the marginal P(Y|u) might fail. (See Appendix A for interaction detection strategies).

- **First 3 experiments:**
  1. **Sanity Check (1D):** Replicate the "Beam Bending" or "Borehole" experiment. Compare Mean Encoding vs. Distributional Encoding (W₂) vs. LVGP. Look for DE matching LVGP accuracy with lower compute time.
  2. **Auxiliary Data Test:** Simulate a "multi-fidelity" scenario (e.g., using the Borehole low-fidelity function). Train a GP on high-fidelity points only vs. a GP using DE constructed from low-fidelity points for the categorical encoding.
  3. **Unseen Level Generalization:** Hold out one categorical level from training. Attempt to predict it using only auxiliary data (if available) or analyze the failure mode if no auxiliary data exists.

## Open Questions the Paper Calls Out

- **Question:** Are the distributional kernels (Wasserstein-based, MMD-based) universal in the sense of approximating any continuous function on the space of categorical inputs?
  - **Basis in paper:** [explicit] The conclusion states "there is still room for improvement in the theoretical understanding of these kernels, including their universality, convergence under empirical approximation."
  - **Why unresolved:** The paper relies on existing theoretical results for characteristic kernels on distributions but does not prove universality guarantees specific to the GP regression setting with categorical inputs.
  - **What evidence would resolve it:** A theoretical proof establishing universal approximation properties, or counterexamples showing cases where DE fails to approximate certain function classes.

- **Question:** Can hybrid models combining distributional encoding with latent variable embeddings achieve consistently better predictive performance than either approach alone?
  - **Basis in paper:** [explicit] The conclusion proposes that "hybrid models that combine distributional and latent embeddings could offer the best of both worlds: data-driven flexibility and compact representation."
  - **Why unresolved:** The paper only briefly suggests initializing latent variable optimization with distributional encodings via multi-dimensional scaling, but provides no empirical evaluation of such hybrid approaches.
  - **What evidence would resolve it:** Systematic experiments comparing standalone DE, standalone LVGP, and hybrid models across diverse benchmarks, showing whether hybrids yield significant and consistent improvements.

- **Question:** How can distributional encoding be extended to handle sparse or high-dimensional auxiliary data without degradation in performance or interpretability?
  - **Basis in paper:** [explicit] The conclusion identifies the need for "generalization to sparse or high-dimensional auxiliary data" as an open theoretical challenge.
  - **Why unresolved:** The auxiliary data experiments (Section 4.3) use low-dimensional settings, and the multivariate extension via sliced Wasserstein distance may struggle with high-dimensional outputs due to the curse of dimensionality in Monte Carlo sampling.
  - **What evidence would resolve it:** Theoretical bounds on estimation error as dimension grows, and empirical validation on problems with high-dimensional auxiliary variables (e.g., functional outputs, image data).

- **Question:** Does the interaction-aware extension of distributional encoding scale effectively to large datasets with multiple interacting qualitative and quantitative inputs?
  - **Basis in paper:** [explicit] Appendix A states: "We plan to investigate further its potential on large datasets in future work" regarding the strategy for handling interactions via partitioned distributional encodings.
  - **Why unresolved:** The proposed strategy requires partitioning quantitative inputs and computing second-order Sobol' indices, which increases problem dimensionality and requires larger training sets—but this was not empirically validated.
  - **What evidence would resolve it:** Experiments on large-scale engineering or materials design problems with many interacting variables, comparing the interaction-aware DE against standard DE and competing methods in both accuracy and computational cost.

## Limitations

- The method requires sufficient observations per categorical level to form meaningful empirical distributions; with small Nₜₗ, performance degrades to noisy target encoding
- The approach is validated primarily on engineering test cases with discrete, well-defined categorical levels rather than open-world categorical inputs
- While computational efficiency is claimed, specific runtime comparisons with LVGP are not provided
- The multivariate extension via sliced Wasserstein distance may struggle with high-dimensional outputs due to Monte Carlo sampling requirements

## Confidence

- **Predictive Performance Claims:** High - consistent improvements over mean encoding and competitive performance with LVGP across multiple test cases with robust statistical evidence
- **Computational Efficiency Claims:** Medium - theoretical argument for efficiency is sound but lacks empirical runtime data for validation
- **Auxiliary Data Benefits:** Low-Medium - demonstrated in material design case study but not systematically explored across synthetic test cases

## Next Checks

1. **Sample Size Sensitivity Analysis:** Systematically vary Nₜₗ (number of observations per categorical level) and measure the degradation in DE performance relative to LVGP to quantify minimum viable sample size

2. **Runtime Benchmarking:** Implement both DE and LVGP on identical hardware and datasets, measuring training time and hyperparameter optimization convergence to validate computational efficiency claims

3. **Open-World Categorical Extension:** Test the approach on datasets where new categorical levels can appear during prediction, measuring how well distributional encoding generalizes to unseen levels compared to LVGP's learned manifold