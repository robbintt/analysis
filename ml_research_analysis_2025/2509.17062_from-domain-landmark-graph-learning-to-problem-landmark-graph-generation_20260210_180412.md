---
ver: rpa2
title: From domain-landmark graph learning to problem-landmark graph generation
arxiv_id: '2509.17062'
source_url: https://arxiv.org/abs/2509.17062
tags:
- landmarks
- planning
- lifted
- landmark
- p-lgg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to learn probabilistic landmark ordering
  relationships from multiple planning tasks within a domain, creating a reusable
  probabilistic lifted ordering graph (p-LOG). For a new planning task, the approach
  instantiates this domain-level information into a probabilistic problem-landmark
  graph (p-LGG) by combining insights from both initial and goal states.
---

# From domain-landmark graph learning to problem-landmark graph generation

## Quick Facts
- **arXiv ID:** 2509.17062
- **Source URL:** https://arxiv.org/abs/2509.17062
- **Authors:** Cristian Pérez-Corral; Antonio Garrido; Laura Sebastia
- **Reference count:** 19
- **One-line primary result:** A method learns probabilistic landmark orderings from multiple planning tasks and instantiates them for new tasks, improving landmark discovery with high recall and F1-scores across 11 IPC domains.

## Executive Summary
This paper proposes a method to learn probabilistic landmark ordering relationships from multiple planning tasks within a domain, creating a reusable probabilistic lifted ordering graph (p-LOG). For a new planning task, the approach instantiates this domain-level information into a probabilistic problem-landmark graph (p-LGG) by combining insights from both initial and goal states. Evaluation on 11 IPC domains shows that the p-LGG captures more true landmarks than classical methods (DS1, DS2), with recall improvements up to 1.0 in some domains and overall F1-scores exceeding 0.9 in 9 of 11 domains. The method is faster than DS2 (which can take days) and adds less than 2 seconds compared to DS1. The main strength lies in generalization, enabling landmark discovery beyond traditional methods, though some domains exhibit lower precision due to probabilistic instantiation.

## Method Summary
The approach involves two main phases: training and inference. During training, the system ingests multiple planning tasks for a domain, extracts Landmark Generation Graphs (LGGs), lifts them by replacing objects with variables, and merges them into a probabilistic lifted ordering graph (p-LOG) where edge weights represent the frequency of ordering relationships across the dataset. For inference on a new task, the system generates two graphs—one rooted in the initial state and one in the goal state—by instantiating the p-LOG with task-specific objects and constraints. These graphs are then unified through equivalence search to produce the final probabilistic problem-landmark graph (p-LGG), which captures both the actionable paths from the start and the necessary paths to the goal.

## Key Results
- The p-LGG method achieves recall improvements up to 1.0 in some domains compared to classical methods.
- F1-scores exceed 0.9 in 9 out of 11 tested IPC domains.
- The method is significantly faster than DS2 (complete landmark extraction) and only adds less than 2 seconds compared to the faster DS1 method.
- The approach generalizes landmark discovery across tasks, enabling the detection of landmarks beyond what traditional methods can find.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Statistical aggregation of lifted ordering relationships across a domain allows for the reusable encoding of likely causal structures.
- **Mechanism:** The system generates a Landmark Generation Graph (LGG) for individual tasks, then "lifts" them by replacing objects with variables (?x0, ?x1). These are merged into a weighted graph (w-LOG) where edge weights represent the frequency of occurrence across the dataset. This frequency is normalized into a probability ($\mu$), creating the **probabilistic Lifted Ordering Graph (p-LOG)**.
- **Core assumption:** Valid structural patterns (e.g., holding(?x0) requires clear(?x1)) recur across instances in a domain, and their frequency correlates with their general validity.
- **Evidence anchors:**
  - [abstract] "...learns probabilistic landmark ordering relationships... creating a reusable probabilistic lifted ordering graph (p-LOG)."
  - [section III] "The probability of an edge $e = (L'_1, L'_2)$ is calculated as the proportion it appears in the graphs..."
  - [corpus] The paper "Revisiting Landmarks" (FMR 0.51) supports the viability of learning generalized landmarks from previous plans.
- **Break condition:** If the training dataset is non-representative or too small, the calculated probabilities may not reflect the true domain constraints, leading to low-confidence edges being instantiated.

### Mechanism 2
- **Claim:** Instantiating landmarks bidirectionally (from initial state $I$ and goal state $G$) constrains the search space to paths that are both actionable and relevant.
- **Mechanism:** The system generates two graphs: $p\text{-}LGG_I$ (forward from $I$) and $p\text{-}LGG_G$ (backward from $G$). During instantiation, it enforces **distinct constraints** ($V_{csts}$) where variables must differ from specific objects or other variables (e.g., a block cannot be stacked on itself).
- **Core assumption:** A valid plan requires intermediate states that are reachable from the start *and* necessary for the goal; paths satisfying neither are pruned.
- **Evidence anchors:**
  - [section IV] "This instantiation operates in two phases... former instantiating information from the initial state and the latter from the goal state."
  - [section IV] "Vcsts[xi].objects contains the objects that xi cannot take... Vcsts[xi].variables contains the variables that xi cannot equal."
  - [corpus] Weak/No direct corpus support for this specific bidirectional constraint mechanism in planning; corpus contains unrelated keyword matches (dental/visual landmarks).
- **Break condition:** If the constraints ($V_{csts}$) are too loose, the graph may explode with irrelevant lifted landmarks; if too tight, valid but complex causal chains may be severed.

### Mechanism 3
- **Claim:** Unifying the bidirectional graphs via "equivalence search" resolves variable bindings to produce a coherent landmark graph.
- **Mechanism:** Algorithm 3 iterates through lifted landmarks and attempts to instantiate them by finding "equivalent predicates" (matching predicate names and compatible parameters) present in the grounded $I$ and $G$ sets. It minimizes a "distance" metric (count of uninstantiated parameters) to select the best variable bindings.
- **Core assumption:** The intersection of the forward ($I$-rooted) and backward ($G$-rooted) graph projections contains the "true" problem landmarks.
- **Evidence anchors:**
  - [section IV] "...combines these two graphs into one unified graph by searching equivalences to extract landmark orderings."
  - [section IV] "We introduce a simple measure of distance... counts the different parameters in the equivalence... choose the top-n candidates with the highest probability."
  - [corpus] Weak/No corpus support for this specific unification algorithm.
- **Break condition:** If multiple distinct lifted landmarks map to the same grounded predicate (ambiguity), the "best equivalent" heuristic may select a suboptimal ordering, degrading precision.

## Foundational Learning

- **Concept: Lifted Planning (Parameterization)**
  - **Why needed here:** The entire p-LOG architecture depends on converting specific objects (e.g., `block_a`) into variables (`?x0`) to transfer knowledge between tasks.
  - **Quick check question:** How does replacing `on(a,b)` with `on(?x0,?x1)` enable cross-instance learning?

- **Concept: Landmark Ordering (Greedy Necessary)**
  - **Why needed here:** The paper focuses specifically on "greedy necessary orders" (Landmark A must be true immediately before Action achieving B). Understanding this is required to interpret the edges in the p-LOG.
  - **Quick check question:** Does an edge $A \to B$ in the p-LOG mean A must happen *sometime before* B, or *immediately before* B?

- **Concept: Precision vs. Recall Trade-offs**
  - **Why needed here:** The paper explicitly trades precision (correctness) for recall (completeness) to find more landmarks. Evaluating the success of this method requires understanding this balance.
  - **Quick check question:** If the system generates a "probabilistic" landmark that is false (low precision), how might that negatively impact a planner using this graph as a heuristic?

## Architecture Onboarding

- **Component map:** Training Module (Ingests domain tasks → Extracts LGGs → Lifts/L merges into p-LOG) → Inference Module (Takes new task ⟨I, G⟩ + p-LOG → Generates p-LGG_I and p-LGG_G → Unifies into final p-LGG) → Validation Layer (Compares generated p-LGG against classical baselines using F1-scores and α-metrics).

- **Critical path:** The instantiation process in Algorithm 3 (`search best equiv`) is the bottleneck. It determines how abstract probabilistic relations map to concrete objects. Errors here propagate as false landmarks.

- **Design tradeoffs:**
  - **Recall > Precision:** The system is designed to find *more* landmarks (high recall, up to 1.0) even if some are false (lower precision), which is useful for heuristics but risky for strict validity.
  - **Speed vs. Completeness:** It is significantly faster than complete methods (DS2), but approximates the truth via probabilities rather than exhaustive proofs.

- **Failure signatures:**
  - **Precision Drop:** Occurs in domains with few landmarks or many open parameters (e.g., Driverlog, ZenoTravel) where the probabilistic instantiation matches incorrect variable bindings.
  - **Over-generalization:** A probability of 1.0 in the p-LOG does not guarantee truth; it only guarantees frequency in the training set (Section III).

- **First 3 experiments:**
  1. **Baseline Reproduction:** Implement the p-LOG generation for BlocksWorld using 4 training instances and verify the probability weights match the paper's description (e.g., `clear(?x0)` probability).
  2. **Ablation on Unification:** Run the instantiation on a new task using *only* the goal-based graph ($p\text{-}LGG_G$) vs. the unified graph ($p\text{-}LGG$) to quantify the contribution of the initial state constraints.
  3. **Sensitivity Analysis:** Vary the training set size (e.g., 2 vs. 10 instances) to determine if the probabilistic weights stabilize or fluctuate, impacting the final p-LGG quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can greedy necessary orders be safely combined with other order types, such as reasonable orders, within a probabilistic lifted ordering graph?
- **Basis in paper:** [explicit] The authors state in the Future Work section that they "want to analyze how to safely combine greedy necessary orders with other orders, such as reasonable ones, in a probabilistic way."
- **Why unresolved:** The current implementation is restricted to greedy necessary orders. Reasonable orders represent preferences rather than strict necessities, and it is unclear how the probabilistic weighting scheme used for necessary orders should be adapted to capture the nuance of "reasonable" advice without misleading the planner.
- **What evidence would resolve it:** An extension of the p-LOG generation algorithm that incorporates reasonable orders and an evaluation showing the resulting impact on planning efficiency and solution quality.

### Open Question 2
- **Question:** Does using the instantiated p-LGG as a search heuristic improve planner performance, or do incorrect probabilistic landmarks negatively impact the search process?
- **Basis in paper:** [explicit] The authors note they are "exploring the use of instantiated p-LGGs in a heuristic-like manner" but need to "analyze when a planner can benefit from the generated information and if an incorrect probabilistic landmark has a negative impact on the search."
- **Why unresolved:** While the paper demonstrates that the p-LGG improves recall in landmark detection, it does not test whether this larger set of landmarks (some of which are probabilistic and potentially incorrect) actually helps a planner solve problems faster or degrades performance due to misinformation.
- **What evidence would resolve it:** Empirical data from integrating the p-LGG into a planner's heuristic calculation, comparing node expansions and solve times against standard landmark heuristics like LAMA.

### Open Question 3
- **Question:** Can extracting lifted conjunctive and mutex relationships between landmarks from the input dataset enhance the quality of the generated graphs?
- **Basis in paper:** [explicit] The authors mention they are "investigating methods to extract lifted conjunctive and mutex relationships between orderings based on the input dataset" as a way to "potentially enhance the quality of results."
- **Why unresolved:** The current method relies on individual landmark orderings derived from DS1 and DS2. It does not account for complex interactions, such as mutex constraints (mutually exclusive landmarks), which could prevent the instantiation of invalid combinations that lower the precision of the current approach.
- **What evidence would resolve it:** A modified learning algorithm that identifies and encodes these complex relationships, evaluated by an increase in precision and F1-score compared to the baseline p-LGG results reported in the paper.

## Limitations

- The method shows low precision (0.23 and 0.29) in ZenoTravel and Driverlog domains, attributed to probabilistic instantiation but not systematically explained.
- The lifting mechanism for mapping objects to consistent variables across tasks is not explicitly defined, impacting reproducibility.
- The "equivalence search" in Algorithm 3 is underspecified, making it difficult to fully validate the unification process.

## Confidence

- **High Confidence:** The overall performance improvement in recall (up to 1.0) and F1-score (>0.9 in 9/11 domains) is well-supported by the experimental results. The speed advantage over DS2 is also clearly demonstrated.
- **Medium Confidence:** The claim that bidirectional instantiation (from initial and goal states) improves landmark discovery is plausible given the results, but the specific mechanism of the "equivalence search" in Algorithm 3 is underspecified, making it difficult to fully validate.
- **Low Confidence:** The explanation for why certain domains (ZenoTravel, Driverlog) exhibit low precision is incomplete. The paper attributes this to "probabilistic instantiation" but does not explore alternative causes such as domain-specific structural complexity or training data quality.

## Next Checks

1. **Ablation Study on Unification:** Implement and compare the p-LGG generation using only the goal-based graph ($p\text{-}LGG_G$) versus the unified graph ($p\text{-}LGG$) to quantify the contribution of initial state constraints to the final landmark set.
2. **Sensitivity Analysis on Training Size:** Vary the number of training instances (e.g., 2 vs. 10) to determine if the probabilistic weights in the p-LOG stabilize, and how this affects precision in low-performing domains.
3. **Lifting Mechanism Validation:** Design a controlled experiment where the object-to-variable mapping is varied systematically to test whether the consistency of this mapping affects the quality of the p-LOG and downstream p-LGGs.