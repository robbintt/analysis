---
ver: rpa2
title: Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge
arxiv_id: '2508.14916'
source_url: https://arxiv.org/abs/2508.14916
tags:
- speech
- system
- arxiv
- multilingual
- yang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multilingual automatic speech recognition
  (ASR) system for the MLC-SLM 2025 Challenge. The system uses a frozen Whisper-large-v3
  encoder for acoustic feature extraction, a trainable adaptor with Linear-ReLU-Linear
  transformation for speech-text alignment, and a frozen Qwen2.5-7B-Instruct LLM with
  trainable LoRA for linguistic decoding.
---

# Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge

## Quick Facts
- arXiv ID: 2508.14916
- Source URL: https://arxiv.org/abs/2508.14916
- Reference count: 0
- Achieved 9.83% WER/CER on evaluation set, ranking third globally

## Executive Summary
This paper presents a multilingual automatic speech recognition (ASR) system for the MLC-SLM 2025 Challenge, employing a frozen Whisper-large-v3 encoder for acoustic feature extraction, a trainable adaptor with Linear-ReLU-Linear transformation for speech-text alignment, and a frozen Qwen2.5-7B-Instruct LLM with trainable LoRA for linguistic decoding. The system was trained on 1,500 hours of multilingual conversational speech across 11 languages plus an external MSR-86K subset. The architecture achieved a word/character error rate of 9.83% on the evaluation set, securing third place among global participants.

## Method Summary
The system uses a hybrid approach combining frozen speech encoder (Whisper-large-v3) with frozen LLM (Qwen2.5-7B-Instruct) connected through trainable components. The acoustic encoder extracts features from multilingual speech, which pass through a trainable adaptor with Linear-ReLU-Linear transformation for alignment, then feed into the LLM with LoRA adapters for linguistic decoding. The model was trained on 1,500 hours of multilingual conversational speech across 11 languages plus an external MSR-86K subset, employing data augmentation techniques including SpecAugment, speed perturbation, Mixup, and noise addition.

## Key Results
- Achieved 9.83% word/character error rate (WER/CER) on evaluation set
- Ranked third place among global participants in MLC-SLM 2025 Challenge
- Trained on 1,500 hours of multilingual conversational speech across 11 languages

## Why This Works (Mechanism)
The system leverages frozen pre-trained models (Whisper encoder and Qwen2.5 LLM) to benefit from their strong cross-lingual capabilities and large-scale knowledge, while trainable adaptors bridge the modality gap and LoRA adapters enable efficient adaptation. The frozen encoder captures robust acoustic features across languages, the adaptor handles speech-text alignment, and the LLM decodes linguistic content. Data augmentation techniques improve robustness to acoustic variations and domain shifts.

## Foundational Learning

1. **Speech-LLM Integration** - Why needed: Enables leveraging large language models for ASR while maintaining speech-specific feature extraction; Quick check: Verify frozen components maintain performance while trainable modules learn alignment

2. **Cross-lingual Feature Extraction** - Why needed: Supports multilingual recognition across 11 languages with shared acoustic representations; Quick check: Test performance consistency across language subsets

3. **LoRA Adaptation** - Why needed: Enables efficient fine-tuning of large LLMs without full parameter updates; Quick check: Compare performance with full fine-tuning vs LoRA

4. **Modality Alignment** - Why needed: Bridges gap between continuous speech features and discrete text tokens; Quick check: Validate alignment accuracy on matched speech-text pairs

5. **Data Augmentation Strategies** - Why needed: Improves robustness to acoustic variations and domain shifts; Quick check: Measure performance degradation without augmentation

6. **Multilingual Training** - Why needed: Enables single model to handle diverse languages efficiently; Quick check: Assess performance balance across all 11 languages

## Architecture Onboarding

**Component Map**: Speech input -> Frozen Whisper-large-v3 encoder -> Trainable adaptor (Linear-ReLU-Linear) -> Frozen Qwen2.5-7B-Instruct LLM with LoRA -> Text output

**Critical Path**: Speech features flow from frozen encoder through trainable adaptor to frozen LLM, with LoRA adapters enabling task-specific adaptation while maintaining base model capabilities.

**Design Tradeoffs**: Freezing encoder and LLM leverages pre-trained capabilities but limits domain adaptation; LoRA provides efficient adaptation but may constrain learning capacity; trainable adaptor bridges modalities but adds complexity.

**Failure Signatures**: Performance degradation in specific languages suggests imbalanced training data; high WER in noisy conditions indicates insufficient acoustic robustness; alignment errors point to adaptor inadequacy.

**3 First Experiments**:
1. Evaluate individual language performance to identify potential disparities
2. Test system with unfrozen components to quantify freezing impact
3. Analyze error types to identify dominant failure modes

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset composition uncertainty: No details about language distribution, domain balance, or data quality characteristics
- Architecture evaluation limitations: No ablation studies or alternative architecture comparisons
- Training methodology constraints: Lacks hyperparameter optimization details and convergence analysis

## Confidence

**High confidence**: System architecture description and component selection are clearly specified and technically sound
**Medium confidence**: Reported WER/CER of 9.83% is likely accurate but requires baseline comparison for interpretation
**Low confidence**: Claims about robustness and superiority lack empirical support due to absent comparative analyses

## Next Checks

1. Evaluate system performance disaggregated by language to identify potential performance disparities
2. Systematically unfreeze and fine-tune individual components to quantify impact of freezing decisions
3. Perform detailed error analysis to categorize failure modes and implement targeted corrections