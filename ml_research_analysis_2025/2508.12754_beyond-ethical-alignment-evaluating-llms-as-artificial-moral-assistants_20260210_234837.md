---
ver: rpa2
title: 'Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants'
arxiv_id: '2508.12754'
source_url: https://arxiv.org/abs/2508.12754
tags:
- moral
- reasoning
- task
- llms
- precept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) as Artificial
  Moral Assistants (AMAs) by assessing their ability to perform explicit moral reasoning,
  including both deductive and abductive forms. To do this, the authors develop a
  novel formal framework and corresponding benchmark (AMAeval) that tests models'
  ability to derive context-specific moral precepts from abstract values and evaluate
  the moral consistency of actions and consequences.
---

# Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants

## Quick Facts
- arXiv ID: 2508.12754
- Source URL: https://arxiv.org/abs/2508.12754
- Reference count: 40
- Key outcome: Evaluates LLMs as Artificial Moral Assistants through abductive and deductive moral reasoning tasks, revealing that while larger models generally perform better, abductive reasoning remains challenging and static/dynamic evaluation capture distinct capabilities.

## Executive Summary
This paper introduces AMAeval, a novel benchmark framework for evaluating large language models as Artificial Moral Assistants (AMAs). Unlike traditional alignment benchmarks that focus on final ethical verdicts, AMAeval tests models' ability to produce explicit moral reasoning chains through both abductive reasoning (deriving context-specific precepts from abstract values) and deductive reasoning (evaluating action-consequence consistency). The benchmark includes both static tasks (evaluating given reasoning chains) and dynamic tasks (generating reasoning chains). Results across popular open models show persistent limitations in abductive reasoning capabilities and reveal that static and dynamic evaluation measure related but separable abilities.

## Method Summary
The authors developed a formal framework testing LLMs' moral reasoning through two tasks: Task 1 (abductive) requires deriving context-specific precepts from abstract values, while Task 2 (deductive) evaluates action-consequence consistency against given precepts. The benchmark uses 40 morally charged scenarios based on Moral Foundations Theory, generating 200 Task 1 samples and 1,090 Task 2 samples. Human annotators labeled the data with acceptable reliability (Krippendorff's α = 0.68-0.70). For dynamic evaluation, a Qwen-2.5-3B classifier was fine-tuned on human annotations using LoRA with specific hyperparameters. Models were evaluated using five-shot prompting, with a composite AMA score combining static and dynamic metrics.

## Key Results
- Larger models consistently outperform smaller ones, with Gemma-3-12B (61.94) and Qwen-2.5-32B (62.19) achieving the highest scores
- All models perform significantly worse on abductive reasoning (Task 1) compared to deductive evaluation (Task 2)
- Static and dynamic evaluation show moderate correlation (ρ = 0.756) but reveal distinct capability patterns, with models like Phi-4 ranking highly in static but poorly in dynamic evaluation
- Task 1 binary classification (threshold ≥3 = correct) achieved only 34.3% accuracy even for the best classifier, confirming abductive difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating moral reasoning chains separately from final verdicts reveals capabilities hidden by classification-only benchmarks
- Mechanism: The framework decomposes moral reasoning into abductive (deriving context-specific precepts) and deductive (evaluating consistency) components, requiring explicit reasoning chains rather than just verdicts to detect whether correct answers stem from valid inference or spurious correlations
- Core assumption: Moral competence requires transparent reasoning, not just aligned outputs
- Evidence anchors: Abstract notes existing benchmarks remain superficial by measuring alignment based on final ethical verdicts rather than explicit moral reasoning

### Mechanism 2
- Claim: Abductive moral reasoning (deriving precepts from values) is harder than deductive evaluation for current LLMs
- Mechanism: Abduction requires generating plausible antecedents that imply assumed facts, which is underconstrained compared to deduction's application of given precepts to consequences
- Core assumption: The difficulty gap reflects genuine capability differences rather than benchmark design artifacts
- Evidence anchors: Static Task 2 accuracy/F1 significantly higher than Task 1 across all models, with all models yielding worse performance in Task 1 than Task 2

### Mechanism 3
- Claim: Static evaluation (verifying reasoning) and dynamic evaluation (generating reasoning) capture related but separable abilities
- Mechanism: Models must both judge correctness of given chains and produce valid chains themselves, with moderate Spearman correlation (ρ = 0.756) plus outlier models indicating verification and generation draw on overlapping but non-identical representations
- Core assumption: The divergence reflects genuine capability separation rather than evaluation noise
- Evidence anchors: Several models deviate notably from the correlation trend, reinforcing the asymmetry between verification and generation capabilities

## Foundational Learning

- Concept: **Abductive reasoning** (inference to best explanation)
  - Why needed here: Core to Task 1; models must derive specific precepts that would justify abstract values in context. Unlike deduction, abduction produces plausible explanations with no single correct answer.
  - Quick check question: Given "fairness" and a whistleblowing scenario, can you generate a context-specific precept and explain why it instantiates that value?

- Concept: **Moral Foundations Theory**
  - Why needed here: Provides the value set V (Care, Fairness, Loyalty, Authority, Sanctity) used in benchmark generation. Understanding these helps interpret why certain precepts are judged more or less convincing.
  - Quick check question: Name the five moral foundations and give an example precept for each in a workplace conflict scenario.

- Concept: **Static vs. Dynamic benchmark evaluation**
  - Why needed here: The paper shows these probe different capabilities. Static tasks use human-annotated ground truth; dynamic tasks require trained classifiers to assess generated reasoning.
  - Quick check question: If a model scores 90% on static evaluation but 60% on dynamic, what does this suggest about its deployment readiness as an AMA?

## Architecture Onboarding

- Component map: Scenario q + value set V → Function R (extract actions A_q) → Function F (predict consequences C_{α,q}) → Function ⇓ (Task 1: abductive precept generation) OR Function E (Task 2: deductive evaluation) → Classifier C (for dynamic scoring)

- Critical path:
  1. Generate/annotate dataset with all components (40 scenarios → 200 Task 1 samples, 1090 Task 2 samples)
  2. Train classifier C on 80/20 split by scenario to avoid leakage
  3. Run static evaluation: 5-shot prompting, compare model ratings to human ground truth
  4. Run dynamic evaluation: models generate reasoning chains, classifier C scores quality
  5. Compute AMA score = 0.5·F1_static + 0.5·Acc_dynamic − λ·MAE_static

- Design tradeoffs:
  - Binary vs. 5-class Task 1: Binary threshold (≥3 = correct) used because 5-class classification showed <35% accuracy even for large classifiers
  - Classifier size: 3B chosen despite stagnation above 3B—larger models add cost without accuracy gains
  - LoRA fine-tuning: Full fine-tuning might improve classifier but increases resource requirements

- Failure signatures:
  - High MAE on static Task 1 (>1.1): Model produces uncalibrated precept quality ratings
  - Large static/dynamic rank gap: Asymmetric capability (e.g., Phi-4 verifies well but generates poorly)
  - Largest model in family underperforms second-largest: Potential distillation artifacts

- First 3 experiments:
  1. Run baseline evaluation on Qwen-2.5-7B across both static and dynamic tasks to establish correlation with reported results
  2. Ablate the pre-trained classifier C by replacing with majority-class baseline to quantify its contribution to dynamic scoring
  3. Test whether chain-of-thought prompting before precept derivation improves Task 1 dynamic accuracy for a mid-size model (e.g., Gemma-3-12B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training methodologies or architectural modifications are required to improve abductive moral reasoning in LLMs to a level comparable with their deductive capabilities?
- Basis in paper: Authors conclude most models struggle with abductive reasoning, especially in generation, and emphasize the need for dedicated strategies to separately improve both forms of moral reasoning
- Why unresolved: Results show persistent performance gap where even top-performing models fail to generate valid abductive reasoning chains despite succeeding at deductive evaluation
- What evidence would resolve it: A study demonstrating a training regime that results in statistically significant increase in "Dynamic Task 1" accuracy, closing the gap with Task 2 performance

### Open Question 2
- Question: Why do static evaluation capabilities (verifying reasoning) and dynamic capabilities (generating reasoning) diverge significantly in certain model families, such as Phi and Llama?
- Basis in paper: Authors note evaluation and generation emerge as distinct capabilities, observing models like Phi 4 rank highly in static evaluation but drop significantly in dynamic generation
- Why unresolved: While the paper identifies the asymmetry (high Spearman correlation but distinct outlier behaviors), it does not determine if this is caused by specific architectural constraints, instruction-tuning datasets, or inherent limits in models' generative plasticity
- What evidence would resolve it: A causal analysis mapping specific pre-training or fine-tuning data characteristics to the observed disconnect between ability to critique vs. produce moral arguments

### Open Question 3
- Question: Does the observed performance drop in the largest model variants (e.g., Qwen 72B vs. 32B) result specifically from the loss of reasoning fidelity during knowledge distillation processes used to train smaller models?
- Basis in paper: Authors hypothesize that all model families yield improved performance up to the second largest model, which consistently outperforms the largest model likely due to increasing use of knowledge distillation
- Why unresolved: Regression analysis confirms the anomaly at largest scales, but specific mechanism—whether data quality, over-fitting, or distillation artifacts—remains a hypothesis
- What evidence would resolve it: A controlled comparison evaluating a base model against its distilled counterparts on the AMAeval benchmark, isolating the "quality" of synthetic reasoning data used in distillation as the independent variable

## Limitations

- The human annotation process involves subjective moral judgments that may not generalize across cultural contexts, despite achieving acceptable inter-annotator reliability
- The classifier used for dynamic evaluation may not perfectly capture human judgment criteria, with performance stagnation above 3B parameters suggesting potential evaluation methodology limitations
- The framework assumes explicit moral reasoning chains are necessary and sufficient for AMA evaluation, potentially missing other aspects of moral competence
- The binary threshold approach for Task 1 (≥3 = correct) is a simplification that may obscure important gradations in precept quality

## Confidence

- **High confidence**: Larger models generally perform better across both tasks, consistent with established scaling laws; moderate correlation between static and dynamic performance (ρ = 0.756) is reliably demonstrated
- **Medium confidence**: Abductive reasoning is inherently harder than deductive reasoning is supported by performance gap but could reflect training data distribution; separation between verification and generation abilities is observed but practical implications remain unclear
- **Low confidence**: The assertion that abductive-deductive decomposition provides superior diagnostic capability compared to classification-only benchmarks relies heavily on theoretical arguments rather than direct empirical comparison with alternative frameworks

## Next Checks

1. **Cultural generalization test**: Replicate the evaluation framework with human annotators from different cultural backgrounds to assess whether performance patterns hold across diverse moral frameworks, particularly for Moral Foundations Theory values used

2. **Alternative classifier validation**: Replace the Qwen-2.5-3B classifier with both a larger model (testing stagnation claim) and a simpler heuristic baseline to determine whether classifier's judgments meaningfully differ from simpler approaches

3. **Direct reasoning quality assessment**: Conduct a small-scale human evaluation comparing quality and coherence of reasoning chains from top-performing and bottom-performing models to validate whether automated metrics capture meaningful differences in moral reasoning sophistication