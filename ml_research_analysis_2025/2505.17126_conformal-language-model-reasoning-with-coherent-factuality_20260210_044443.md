---
ver: rpa2
title: Conformal Language Model Reasoning with Coherent Factuality
arxiv_id: '2505.17126'
source_url: https://arxiv.org/abs/2505.17126
tags:
- factuality
- claims
- claim
- graph
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to ensure factuality in language
  model reasoning outputs by enforcing "coherent factuality" - requiring each step
  of reasoning to be deducible from prior steps and ground truth. The authors apply
  split conformal prediction to filter subgraphs in a deducibility graph representation
  of reasoning chains, rather than filtering individual claims independently.
---

# Conformal Language Model Reasoning with Coherent Factuality

## Quick Facts
- **arXiv ID:** 2505.17126
- **Source URL:** https://arxiv.org/abs/2505.17126
- **Reference count:** 40
- **Primary result:** 90% factuality while retaining 80% of claims on MATH and FELM datasets using subgraph-based conformal filtering

## Executive Summary
This paper introduces a novel method to ensure factuality in language model reasoning outputs by enforcing "coherent factuality" - requiring each step of reasoning to be deducible from prior steps and ground truth. The authors apply split conformal prediction to filter subgraphs in a deducibility graph representation of reasoning chains, rather than filtering individual claims independently. Their approach achieves 90% factuality while retaining 80% of claims on MATH and FELM datasets, producing more "legible" outputs that are easier for humans to verify.

## Method Summary
The method filters language model reasoning outputs to guarantee coherent factuality by representing reasoning chains as directed acyclic graphs (DAGs) and applying split conformal prediction to subgraph sets. The approach generates approximate deducibility graphs via LLM few-shot prompting, scores claims using self-consistency across multiple generations, and propagates risk scores through graph structure using descendant weighting. A calibrated threshold selects the maximum subgraph below the conformal quantile, ensuring each retained claim has all logical prerequisites present.

## Key Results
- Achieves 90% coherent factuality while retaining 80% of claims on MATH and FELM datasets
- Maintains calibrated coverage within theoretical bounds [1-α, 1-α+1/(n+1)]
- Demonstrates 40% lower false positive rate and 35% lower false negative rate compared to baseline methods
- Produces more "legible" outputs that are easier for humans to verify

## Why This Works (Mechanism)

### Mechanism 1: Graph-Structured Deducibility Constraints
Reasoning correctness depends on claim substantiation, not just individual truth. An approximate deducibility graph (DAG) encodes sufficient dependencies between claims, with filtering preserving ancestor-connected subgraphs to ensure each retained claim has all logical prerequisites present. Core assumption: superstring deducibility holds - adding more valid information doesn't break deducibility of downstream claims.

### Mechanism 2: Risk Scoring with Graph Propagation
Propagating risk scores through graph structure improves filtering decisions over independent scoring. The descendant weighting function σ(v) = (1-β)σ_ind(v) + β·median{σ_ind(v'): v' is descendant of v} incorporates downstream uncertainty into node scoring, penalizing nodes with uncertain descendants even if locally confident. Core assumption: errors tend to cascade - claims derived from uncertain premises should share that uncertainty.

### Mechanism 3: Split Conformal Calibration on Subgraph Sets
Threshold selection over subgraphs (not individual claims) yields calibrated coherent factuality guarantees. The non-conformity score r(X,Y,U_T) = sup{τ: all subgraphs with threshold ≤τ are coherently factual} uses the calibrated quantile from a held-out set to guarantee P[Y_filtered is coherently factual] ≥ 1-α under exchangeability. Core assumption: data points are exchangeable across calibration and test, and graph proxies satisfy approximate deducibility.

## Foundational Learning

- **Concept: Split Conformal Prediction**
  - Why needed here: Core statistical framework that provides distribution-free coverage guarantees by calibrating thresholds on held-out data.
  - Quick check question: Given calibration scores [0.2, 0.4, 0.5, 0.7, 0.9] and α=0.2, what quantile gives the conformal threshold?

- **Concept: Directed Acyclic Graphs (DAGs) and Topological Ordering**
  - Why needed here: Reasoning chains are represented as DAGs; valid outputs require topological sorts that respect dependencies.
  - Quick check question: If edges are (A→B), (B→C), (A→C), what topological orders are valid? Can you have (C, B, A)?

- **Concept: Self-Consistency Scoring for LLM Claims**
  - Why needed here: The heuristic risk function σ_ind uses frequency of claim appearance across multiple model samples as a proxy for confidence.
  - Quick check question: If a claim appears in 4 of 5 sampled responses, what's the self-consistency confidence score? How does this map to risk?

## Architecture Onboarding

- **Component map:**
  Input: (X, Y) → Claim Splitter → Claims {c_1,...,c_n} → Graph Generator (LLM) → DAG G = (V, E) → Risk Scorer → Node scores σ(v) using self-consistency → Subgraph Generator → Set of (subgraph, threshold) pairs → Conformal Calibrator (uses calibration set) → Threshold q̂_α → Filter → Max-threshold subgraph below q̂_α → Topological sort → Y_filtered

- **Critical path:** Graph generation quality directly determines if Definition 4 holds, which is required for the upper bound guarantee. Poor graphs may still satisfy the lower bound but produce incoherent outputs.

- **Design tradeoffs:**
  - Graph complexity: Linear chain graphs (1→2→...→n) are simpler but may miss dependencies; richer graphs capture more structure but require better LLM graph generation
  - Scoring function: Independent scoring vs. descendant weighting - trade-off between simplicity and improved retention at low α
  - Annotation cost: Gold-standard subset annotation is expensive; silver-standard (per-claim) annotation is cheaper but assumes graph validity

- **Failure signatures:**
  - Calibration plots showing factuality below 1-α lower bound → check exchangeability assumption
  - Low claim retention (<50%) at moderate α → graph may be overly dense or scoring function miscalibrated
  - Cyclic graphs from LLM generator → fall back to linear chain; indicates prompt needs refinement
  - High false positive rate in legibility checks → filtered outputs still contain subtle errors not caught by current risk function

- **First 3 experiments:**
  1. Validate calibration on held-out set: Run Algorithm 3 on MATH calibration split (n=50), test on held-out split, verify factuality stays within [1-α, 1-α+1/(n+1)] for α ∈ {0.05, 0.1, 0.15, 0.2}
  2. Ablate graph structure: Compare claim retention and factuality using (a) GPT-4o generated graphs, (b) linear chain graphs, (c) human-constructed ideal graphs on 10 annotated examples
  3. Reprompting utility test: Take filtered outputs at α=0.1, feed back to LLM with "complete this reasoning," measure error rate reduction vs. independently filtered baseline (replicate Table 1)

## Open Questions the Paper Calls Out

### Open Question 1
Can approximate deducibility graphs be generated reliably for complex, long-chain reasoning tasks without relying on proprietary models? The paper notes that GPT-4o struggled with longer reasoning outputs containing many claims, and relying on proprietary models is not ideal. Current experiments show degradation in Llama-generated graphs and context-length limits in GPT-4o for long proofs.

### Open Question 2
Does the coherent factuality framework transfer effectively to non-natural-language reasoning domains like code generation? The paper states code generation is a natural domain where dependency graphs and compilation offer a well-defined notion of coherent substantiation. However, the method has only been evaluated on MATH and FELM, assuming subjective "deducibility" rather than strict compilation dependencies.

### Open Question 3
Do coherently filtered outputs improve human verifiability ("legibility") compared to independent factuality baselines? The paper defers human studies of output legibility to future works, currently relying on LLM-judges as proxies. While claiming improved legibility, the paper validates it only via GPT-4o/Llama judges, not human evaluators.

## Limitations
- The method requires high-quality DAGs to enforce coherent factuality, but LLM-generated graphs may be imperfect or contain cycles, potentially affecting upper bound guarantees
- The annotation protocol is underspecified, using both "silver" (claim-level) and "gold" (subset-level) annotation without clear guidelines on when each is used
- The descendant weighting parameter β is set to 0.5 empirically but not extensively validated across different datasets or graph structures

## Confidence

- **High confidence:** The theoretical framework (split conformal prediction, DAG structure, risk propagation) is sound and well-defined. The experimental results showing 90% factuality with 80% claim retention are reproducible given the same data and models.
- **Medium confidence:** The practical effectiveness of descendant weighting and the robustness claims regarding poor-quality graphs. These depend on empirical assumptions about error correlation that weren't extensively tested.
- **Low confidence:** The exact annotation protocol and its impact on calibration. Without standardized annotation guidelines, results may not generalize across different annotator teams.

## Next Checks

1. **Annotation protocol validation:** Create detailed annotation guidelines for determining coherent factuality, conduct inter-annotator agreement studies on 50 examples, and measure how annotation variability affects calibration results.

2. **Graph quality sensitivity analysis:** Systematically vary graph quality (perfect DAG, cyclic graphs, linear chains, sparse vs. dense graphs) on a fixed set of 20 annotated examples, measuring impact on factuality bounds, claim retention, and output legibility.

3. **β parameter sweep:** Test descendant weighting across β ∈ {0.0, 0.25, 0.5, 0.75, 1.0} on both MATH and FELM datasets, measuring factuality vs. claim retention trade-offs and identifying task-specific optimal values.