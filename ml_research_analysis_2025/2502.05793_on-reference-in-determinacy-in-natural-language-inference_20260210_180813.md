---
ver: rpa2
title: On Reference (In-)Determinacy in Natural Language Inference
arxiv_id: '2502.05793'
source_url: https://arxiv.org/abs/2502.05793
tags:
- premise
- reference
- label
- language
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how the reference determinacy (RD) assumption
  in NLI datasets affects model performance in downstream tasks. The authors introduce
  REFNLI, a benchmark with 1,143 examples where premises are retrieved from Wikipedia
  and may not share context with hypotheses.
---

# On Reference (In-)Determinacy in Natural Language Inference

## Quick Facts
- arXiv ID: 2502.05793
- Source URL: https://arxiv.org/abs/2502.05793
- Reference count: 28
- Key outcome: NLI models trained on RD-assumption datasets fail on REFNLI, producing over 80% false contradictions and 50% false entailments when premises are irrelevant

## Executive Summary
This paper identifies reference determinacy (RD) as a hidden confounder in NLI datasets that causes models to fail when premises may be irrelevant to hypotheses. The authors introduce REFNLI, a benchmark with 1,143 examples where premises are retrieved from Wikipedia and may not share context with hypotheses. They find that NLI models finetuned on standard datasets struggle to recognize context mismatches, producing over 80% false contradictions and 50% false entailments when premises are irrelevant. Human disagreement patterns in NLI also correlate with reference ambiguity, suggesting RD contributes to inherent label disagreements. Filtering training examples with low lexical overlap between premise and hypothesis improves model precision on REFNLI, validating that RD artifacts in training data harm real-world performance.

## Method Summary
The authors create REFNLI by sampling claims from FEVER and VitaminC, retrieving top-10 Wikipedia passages via BM25, and having human annotators label claim-evidence pairs with 4-way labels (Entailment, Contradiction, Ambiguous, Neutral). They finetune T5-large on various NLI dataset mixtures and evaluate on REFNLI, measuring per-label precision, recall, and AUROC. The key intervention is filtering training examples with low lexical overlap (Jaccard ≤ 0.15) between premise and hypothesis, which improves model precision on REFNLI. They also analyze human disagreement patterns in ChaosNLI to show reference ambiguity correlates with label shifts.

## Key Results
- NLI models finetuned on standard datasets produce over 80% false contradictions and over 50% false entailments when premises are irrelevant to hypotheses
- Filtering training examples with Jaccard overlap ≤ 0.15 improves precision on REFNLI, with neutral recall improving most dramatically (from 0.44 to 0.83)
- ~47% of examples in SNLI and MNLI exhibit reference ambiguity, and human disagreement patterns correlate with reference determinacy issues
- T5 models trained on single datasets (SNLI, MNLI, ANLI, FEVER, VitaminC) perform worse on REFNLI than models trained on dataset mixtures

## Why This Works (Mechanism)

### Mechanism 1: Reference Determinacy Bias in Training Data
Standard NLI datasets assume premise and hypothesis refer to same context (RD assumption). Models learn to predict entailment/contradiction based on surface-level semantic conflicts without learning to verify reference relevance. This creates systematic bias where models predict contradictions even when premise is irrelevant to hypothesis.

### Mechanism 2: Lexical Overlap as a Proxy for Reference Determinacy
Low lexical overlap between premise and hypothesis correlates with examples that only receive entailment/contradiction labels due to RD assumption. Removing such examples from training reduces model's tendency to make spurious predictions on irrelevant premises.

### Mechanism 3: Reference Ambiguity Drives Human Label Disagreement
A significant portion of human disagreement in NLI annotations arises from reference ambiguity rather than purely semantic uncertainty. When annotators are not explicitly instructed to assume RD, they vary in whether they interpret ambiguous references as same-context or not.

## Foundational Learning

- **Concept: Reference Determinacy (RD)**
  - Why needed here: Central to understanding the paper's diagnosis of NLI failure modes
  - Quick check question: Given premise "John bought apples" and hypothesis "John bought oranges," would this be labeled contradiction under RD? What if the two Johns are different people?

- **Concept: False Contradiction Rate**
  - Why needed here: The paper's primary metric for evaluating RD bias
  - Quick check question: If a model predicts "contradiction" for a claim about Newton against evidence about Einstein, is this a false contradiction?

- **Concept: Jaccard Similarity**
  - Why needed here: Used as the filtering heuristic in Section 4.4
  - Quick check question: What is the Jaccard similarity between "the cat sat on the mat" and "the dog sat on the log"?

## Architecture Onboarding

- **Component map:** Claims from FEVER/VitaminC → BM25 retrieval from Wikipedia → NLI model prediction → Human annotation (4-way labels) → REFNLI benchmark → T5-large finetuning → Evaluation

- **Critical path:** 1) Sample claims from FEVER/VitaminC → 2) Retrieve top-10 Wikipedia passages via BM25 → 3) Filter examples where NLI model predicts entailment/contradiction but evidence is from irrelevant Wikipedia page → 4) Expert annotation with 4-way labels → 5) Train T5-large on dataset mixtures → 6) Evaluate on REFNLI

- **Design tradeoffs:**
  - Lexical overlap threshold: Too aggressive → loses valid examples; too lenient → fails to filter RD-dependent cases
  - 4-way vs 3-way labels: "Ambiguous" label captures reference ambiguity cases
  - Gemini comparison: 8-shot only, no AUROC available

- **Failure signatures:**
  - High recall + low precision on contradiction predictions (false contradictions)
  - Strong performance on entailment AUROC (>0.85) but weaker on contradiction
  - Neutral predictions drop when context is irrelevant

- **First 3 experiments:**
  1. Replicate REFNLI evaluation: Take an off-the-shelf NLI model, run it on REFNLI, verify high false contradiction rate
  2. Ablate the Jaccard threshold: Test 0.10, 0.15, 0.20, 0.25 thresholds on validation set
  3. Test on in-domain NLI: Evaluate filtered vs unfiltered models on standard MNLI dev set

## Open Questions the Paper Calls Out

- **Question:** Does reference indeterminacy significantly impact model performance in other natural language tasks beyond NLI?
  - Basis: The authors state in conclusion that "it remains to be seen whether the reference or context ambiguity problem exists in other tasks and datasets as well"
  - Why unresolved: Study focused exclusively on NLI and fact verification
  - Evidence needed: Evaluating models on benchmarks for other tasks where context mismatches are artificially introduced

- **Question:** How do large language models (LLMs) specifically react to and resolve input ambiguities compared to fine-tuned models?
  - Basis: Limitations section notes study could be "extended and strengthened with experiments with large language models"
  - Why unresolved: Authors relied primarily on T5 and RoBERTa architectures
  - Evidence needed: Comparative study analyzing output distributions of various LLMs on REFNLI ambiguous subset

- **Question:** Is the observed reference determinacy bias a function of the specific model architecture used?
  - Basis: Authors note in limitations that "experimenting with more models could potentially eliminate model architecture as the confounder"
  - Why unresolved: Experiments predominantly utilized T5-large architecture
  - Evidence needed: Replicating training and evaluation experiments using diverse architectures

## Limitations

- The lexical overlap heuristic (Jaccard ≤ 0.15) is treated as a sufficient proxy for RD-dependency without rigorous ablation or comparison to alternative measures
- The claim that human disagreement stems from reference ambiguity is correlational and doesn't control for other potential confounds like sentence complexity
- The 1,143 examples in REFNLI may not capture the full distribution of real-world reference ambiguity scenarios

## Confidence

**High Confidence**: The empirical observation that NLI models finetuned on standard datasets fail dramatically on REFNLI (over 80% false contradictions on irrelevant premises) is robust and directly measurable.

**Medium Confidence**: The causal mechanism linking RD assumption in training data to poor downstream performance is plausible but not definitively proven. Alternative explanations exist for the filtering improvements.

**Low Confidence**: The generalizability of the lexical overlap threshold across different NLI datasets and model architectures. The paper doesn't test whether different thresholds work better for different model sizes or training regimes.

## Next Checks

1. **Ablation Study on Jaccard Threshold**: Systematically vary the lexical overlap threshold (0.10, 0.15, 0.20, 0.25) and measure both REFNLI performance and in-distribution NLI performance.

2. **Cross-Dataset Generalization Test**: Apply the lexical overlap filtering approach to entirely different NLI datasets (e.g., XNLI, QNLI) and evaluate whether the same improvements hold.

3. **Controlled Human Annotation Study**: Design a new annotation task where reference determinacy is explicitly varied while holding other factors constant. Have annotators label the same premise-hypothesis pairs twice—once assuming RD and once not assuming RD.