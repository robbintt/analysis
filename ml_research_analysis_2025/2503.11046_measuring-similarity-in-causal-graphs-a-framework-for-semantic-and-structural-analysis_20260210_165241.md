---
ver: rpa2
title: 'Measuring Similarity in Causal Graphs: A Framework for Semantic and Structural
  Analysis'
arxiv_id: '2503.11046'
source_url: https://arxiv.org/abs/2503.11046
tags:
- causal
- graphs
- graph
- similarity
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates metrics for comparing causal graphs by integrating\
  \ semantic and structural approaches. Nine metrics\u2014four semantic similarity\
  \ measures (BLEU, Fuzzy Matching, Cosine Similarity, Negative Euclidean Distance)\
  \ and five graph comparison kernels (Pyramid Match, Shortest Path, Subgraph Matching,\
  \ WL Vertex/Edge Histograms)\u2014were tested on a synthetic dataset of 2,000 causal\
  \ loop diagrams generated via LLM from the Limits to Growth model."
---

# Measuring Similarity in Causal Graphs: A Framework for Semantic and Structural Analysis

## Quick Facts
- **arXiv ID:** 2503.11046
- **Source URL:** https://arxiv.org/abs/2503.11046
- **Reference count:** 40
- **Key outcome:** Nine metrics—four semantic (BLEU, Fuzzy Matching, Cosine Similarity, Negative Euclidean Distance) and five graph kernels (Pyramid Match, Shortest Path, Subgraph Matching, WL Vertex/Edge Histograms)—were tested on synthetic causal loop diagrams, showing that combining multiple metrics is essential for comprehensive causal graph comparison.

## Executive Summary
This study evaluates metrics for comparing causal graphs by integrating semantic and structural approaches. Nine metrics were tested on a synthetic dataset of 2,000 causal loop diagrams generated via LLM from the Limits to Growth model. Results show that semantic metrics like Cosine Similarity capture conceptual relationships, while BLEU relies on exact word matching. Graph metrics vary in sensitivity to structural differences, with Subgraph Matching excelling at partial matches and WL-based metrics emphasizing node or edge labels. The findings highlight that no single metric suffices; combining multiple metrics is essential for comprehensive causal graph comparison, advancing tools for analyzing complex systems.

## Method Summary
The study compared nine metrics—four semantic (BLEU, Fuzzy Matching, Cosine Similarity, Negative Euclidean Distance) and five graph kernels (Pyramid Match, Shortest Path, Subgraph Matching, WL Vertex/Edge Histograms)—on synthetic causal loop diagrams. Variable names were embedded using Sentence-BERT, and structural similarity was computed via graph kernels. The synthetic dataset of 2,000 variations was generated from the Limits to Growth model using GPT-4o-mini. Distributions and sensitivities of metrics were analyzed to assess their effectiveness in capturing semantic and structural similarities.

## Key Results
- Semantic metrics like Cosine Similarity capture conceptual relationships between variable names, while BLEU relies on exact word matching.
- Graph metrics vary in sensitivity to structural differences, with Subgraph Matching excelling at partial matches and WL-based metrics emphasizing node or edge labels.
- No single metric suffices; combining multiple metrics is essential for comprehensive causal graph comparison.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional semantic embeddings capture conceptual relationships between variable names even when lexical forms differ.
- Mechanism: Word vectors encode semantic meaning in high-dimensional space; Cosine Similarity measures angular alignment while Euclidean Distance measures spatial proximity. Cosine Similarity achieved 0.95 scores for rephrased variables (e.g., "student enrollment" → "student demand") while BLEU dropped to 0.29.
- Core assumption: Variable names share conceptual relationships that persist across lexical variation; embedding models trained on general corpora transfer to domain-specific causal terminology.
- Evidence anchors:
  - [abstract]: "semantic metrics like Cosine Similarity capture conceptual relationships"
  - [Page 14, Figure 3]: M3 (Cosine Similarity) shows "strong concentration of higher scores, capturing conceptual relationships between variable names"
  - [corpus]: Weak direct evidence—corpus papers focus on code similarity (CSSG) and RAG systems, not causal graph semantics specifically
- Break condition: Highly technical domain vocabulary with no representation in pre-trained embeddings; variable names with ambiguous or context-dependent meanings.

### Mechanism 2
- Claim: Learning graph kernels compare structural similarity without requiring node correspondence.
- Mechanism: Graph kernels embed graphs into feature spaces where similarity is computed via: (1) Subgraph Matching counts bounded-size matching subgraphs; (2) WL Histograms iteratively aggregate neighbor label information; (3) Shortest Path compares path-length distributions. Subgraph Matching achieved 0.94 for graphs with minor structural changes while WL Vertex Histogram dropped to 0.64 when node labels differed.
- Core assumption: Structural features (subgraphs, paths, label distributions) are sufficient signatures for similarity; graph size differences don't invalidate comparison.
- Evidence anchors:
  - [Page 8, Table 2]: Learning graph kernels have "independence from node correspondence, along with their support for labeled nodes, edges attributes, and directed edges"
  - [Page 15, Figure 4]: G3 (Subgraph Matching) shows "more dispersed distribution" capturing "partial structural overlaps, even when the overall graph alignment is imperfect"
  - [corpus]: Graph2Region paper addresses graph similarity learning but focuses on neural approximations for MCS/GED, not kernel methods
- Break condition: Graphs with drastically different sizes (>10× node count); dense graphs where subgraph enumeration becomes computationally prohibitive.

### Mechanism 3
- Claim: No single metric suffices—combining semantic and structural metrics is essential for comprehensive causal graph comparison.
- Mechanism: Semantic metrics (M1-M4) assess variable name meaning; structural metrics (G1-G5) assess topology and edge attributes. Their divergence signals different types of mismatches: high semantic + low structural = naming preserved but causal mechanisms altered; low semantic + high structural (G5 only) = edge patterns preserved but variables renamed.
- Core assumption: Different similarity dimensions are independent and complementary; their combination provides fuller assessment than any single metric.
- Evidence anchors:
  - [abstract]: "no single metric suffices; combining multiple metrics is essential for comprehensive causal graph comparison"
  - [Page 16, Table 5]: Case 3 shows high semantic (M3=0.96) but low structural (G4=0.05) when variables retained meaning but structure diverged; G5 remained high (0.99) despite missing nodes, demonstrating misleading single-metric reliance
  - [corpus]: No corpus papers directly validate multi-metric combination for causal graphs
- Break condition: Application domains where one dimension (semantic OR structural) dominates decision-making; metrics producing conflicting signals without clear weighting guidance.

## Foundational Learning

- Concept: Word Embeddings and Vector Space Models
  - Why needed here: All four semantic metrics (M1-M4) rely on representing variable names as vectors; understanding embedding spaces is prerequisite for interpreting Cosine Similarity and Euclidean Distance results.
  - Quick check question: Given two variable names "Net Increase" and "Growth Factor," would you expect high Cosine Similarity? (Paper shows M3=0.65—moderate, capturing shared growth concept but different implications)

- Concept: Graph Kernel Methods
  - Why needed here: Five of nine metrics (G1-G5) are learning graph kernels; understanding how kernels encode structural features (subgraphs, paths, label histograms) is essential for selecting appropriate metrics.
  - Quick check question: Why might WL Edge Histogram (G5) score 0.99 for graphs with completely different node labels? (Answer: G5 ignores node labels entirely, focusing only on edge labels)

- Concept: Causal Loop Diagrams and Feedback Structure
  - Why needed here: The paper uses CLDs (cyclic, with edge polarities) as testbed; understanding feedback loops (reinforcing R, balancing B) informs why Shortest Path kernel is highlighted for cyclic structures.
  - Quick check question: In the Limits to Growth model, what structural feature would Shortest Path kernel specifically capture? (Answer: Causal paths between endpoints, avoiding "tottering" in cyclic graphs—Page 8, Table 2)

## Architecture Onboarding

- Component map:
  - Input: Two causal graphs (reference + comparison) with node labels and directed, labeled edges
  - Preprocessing: Variable name embedding via Sentence-BERT; graph structure extraction (nodes, edges, polarities)
  - Semantic layer: Four metrics (BLEU, Fuzzy, Cosine, Euclidean) compute [0,1] or [-∞,0] scores
  - Structural layer: Five graph kernels compute [0,1] scores via GraKeL or similar library
  - Aggregation: No unified metric—current framework requires manual interpretation of 9 scores

- Critical path: (1) Define comparison task (semantic focus vs. structural focus vs. both) → (2) Select subset of metrics based on task requirements → (3) Compute selected metrics → (4) Interpret divergence patterns (e.g., high semantic + low structural signals mechanism disagreement)

- Design tradeoffs:
  - Precision vs. robustness: BLEU requires exact word sequences (high precision, low robustness to rephrasing); Cosine Similarity captures synonyms but may conflate distinct concepts
  - Node vs. edge emphasis: WL Vertex (G4) vs. WL Edge (G5) force choice between node-label and edge-label focus; no single metric captures both
  - Computational cost: Subgraph Matching (G3) most precise but "computationally expensive when number of subgraphs grows rapidly" (Page 8, Table 2)

- Failure signatures:
  - All semantic scores low but G5 high: Variables renamed but causal link patterns preserved (potentially misleading—Page 16, Table 5 Case 4)
  - High semantic but near-zero G4: Node labels match but structure completely differs
  - BLEU near 0 for all comparisons: Dataset uses non-overlapping vocabulary (wrong metric choice)

- First 3 experiments:
  1. Replicate simple example analysis (Table 3-4) with own causal graphs to validate metric implementation before scaling
  2. Test metric sensitivity to single-edge polarity flips on small graphs (5-7 nodes) to establish detection thresholds
  3. Benchmark computational scaling: Generate graphs of increasing size (5, 10, 20, 50 nodes) and time each of the 9 metrics to identify bottlenecks before production deployment

## Open Questions the Paper Calls Out

- Can semantic and structural metrics be integrated into a single measure that accounts for both semantic and structural characteristics? [explicit] The Discussion section explicitly asks, "Can the two types of metrics be integrated into a single measure that accounts for both semantic and structural characteristics?"
- Do the selected metrics maintain computational feasibility and reliability when applied to large-scale causal graphs? [explicit] The Discussion states that "the computational feasibility of these metrics for large-scale causal graphs remains an open question."
- How do the selected metrics perform on other causal graph types, such as Directed Acyclic Graphs (DAGs)? [explicit] The authors note that the study "exclusively examines CLDs" and suggest that "Future research should also consider other types of causal graphs."

## Limitations
- No explicit validation for multi-metric aggregation approach or optimal combination strategy
- Synthetic dataset generation via LLM paraphrasing may not represent real-world causal graph variations
- Lack of ground truth for causal relationship accuracy limits validation of high structural similarity

## Confidence
- **High Confidence:** Relative performance rankings of individual metrics are well-supported by controlled synthetic variations
- **Medium Confidence:** Claim that no single metric suffices is demonstrated through case examples, but optimal combination strategy remains heuristic
- **Low Confidence:** Transferability of findings to real-world causal graphs beyond the Limits to Growth model and synthetic variations has not been established

## Next Checks
1. Apply the framework to validate causal graphs from multiple domains (economics, biology, social systems) to test generalizability beyond the Limits to Growth model
2. Conduct expert evaluation studies where domain specialists assess whether high-scoring graph pairs (according to the framework) represent genuinely similar causal structures
3. Implement and test automated metric weighting algorithms that optimize combinations based on specific task requirements (e.g., mechanism preservation vs. variable naming consistency)