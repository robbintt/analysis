---
ver: rpa2
title: Dynamically Scaled Activation Steering
arxiv_id: '2512.03661'
source_url: https://arxiv.org/abs/2512.03661
tags:
- dsas
- steering
- blurred
- lineas
- heavily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamically Scaled Activation Steering (DSAS) addresses the problem
  of indiscriminate activation steering that degrades model performance when applied
  uniformly. The core method idea is to adaptively modulate steering strength based
  on context, learning when to steer rather than applying fixed policies.
---

# Dynamically Scaled Activation Steering

## Quick Facts
- **arXiv ID**: 2512.03661
- **Source URL**: https://arxiv.org/abs/2512.03661
- **Reference count**: 40
- **Primary result**: Dynamically Scaled Activation Steering (DSAS) improves the Pareto front between toxicity mitigation and capability retention by adaptively modulating steering strength based on context, learning when to steer rather than applying fixed policies.

## Executive Summary
Dynamically Scaled Activation Steering (DSAS) addresses the problem of indiscriminate activation steering that degrades model performance when applied uniformly. The core method idea is to adaptively modulate steering strength based on context, learning when to steer rather than applying fixed policies. DSAS trains logistic regressors to predict per-token intervention strength, allowing it to intervene strongly only when undesired behavior is detected. When combined with existing steering methods, DSAS consistently improves the Pareto front across toxicity mitigation and capability retention. For example, on Qwen 2.5 (1.5B), DSAS-enhanced CAA reduced toxicity from 12.1% to 8.64% while maintaining MMLU accuracy at 59.84%, outperforming both vanilla steering and recent conditional methods like CAST and MERA. DSAS also generalizes to text-to-image diffusion models, selectively blurring target concepts while preserving non-target content. The approach introduces minimal computational overhead and provides interpretability by pinpointing which tokens require steering.

## Method Summary
DSAS trains logistic regressors per layer to predict intervention strength by separating source (undesired) and control (neutral) activations. At inference, it interpolates between original and steered activations using classifier confidence as weights. The method can be applied offline (post-hoc classifier training) or end-to-end (joint optimization with steering maps). DSAS assumes linear separability of source and control activations in representation space, using PCA for dimensionality reduction. The approach adds minimal computational overhead compared to vanilla steering while providing interpretability through token-level confidence scores.

## Key Results
- DSAS consistently improves the Pareto front across toxicity mitigation and capability retention
- On Qwen 2.5 (1.5B), DSAS-enhanced CAA reduced toxicity from 12.1% to 8.64% while maintaining MMLU accuracy at 59.84%
- DSAS generalizes to text-to-image diffusion models, selectively blurring target concepts while preserving non-target content
- The method introduces minimal computational overhead compared to vanilla steering

## Why This Works (Mechanism)

### Mechanism 1: Per-Layer "When-to-Steer" Classifiers
DSAS trains logistic regressors per layer to separate source (undesired behavior) and control (neutral behavior) activations. The classifier probability becomes the adaptive steering strength for each token/feature at layer ℓ. This assumes linear separability in activation space—the Linear Representation Hypothesis. Evidence shows DSAS produces higher average activation magnitudes for toxic vs. non-toxic sentences and maintains higher cosine similarity to original activations for non-toxic sentences while increasing divergence for toxic sentences. If classifier accuracy approaches random (≈0.5), DSAS falls back to uniform steering at half strength.

### Mechanism 2: Decoupled Interpolation Between Original and Steered Activations
DSAS applies linear interpolation between original activation and steered activation, weighted by classifier confidence: T_DSAS_ℓ(t_ℓ,k) = (1 - h_ℓ(t_ℓ,k)) · t_ℓ,k + h_ℓ(t_ℓ,k) · T_ℓ(t_ℓ,k; λ). This preserves model capabilities on neutral inputs to the extent the classifier assigns low steering probabilities to control-set-like inputs. Evidence shows DSAS maintains higher cosine similarity to original activations for non-toxic sentences while increasing divergence for toxic sentences. If h_ℓ misclassifies neutral inputs as source-like, those inputs will be unnecessarily perturbed, degrading fluency/accuracy.

### Mechanism 3: End-to-End Joint Optimization (E2E-DSAS)
E2E-DSAS jointly learns classifier parameters with steering maps using Wasserstein distance (source→target) and L2 control regularization. Results are architecture- and hyperparameter-sensitive—E2E-DSAS improves Pareto front on Gemma 2B but shows mixed results on Qwen models. Small γ → insufficient control preservation; large γ → weak steering signal. The paper uses γ=1 as default but notes sensitivity to this hyperparameter.

## Foundational Learning

- **Concept: Activation Steering**
  - **Why needed here:** DSAS is a wrapper around existing steering methods (CAA, ITI, LinEAS). You must understand what these base methods do before understanding what DSAS adds.
  - **Quick check question:** Given two contrastive activations a_pos and a_neg, can you sketch how CAA constructs a steering vector?

- **Concept: Linear Separability in Representation Space**
  - **Why needed here:** DSAS's per-layer classifiers assume source and control activations are linearly separable. If they're not, the classifier provides noisy guidance.
  - **Quick check question:** Why does the paper apply PCA before training logistic regressors? What happens if you skip PCA and have |S|, |C| ≪ d_ℓ?

- **Concept: Pareto Front Trade-offs**
  - **Why needed here:** The paper's central claim is improving the Pareto front between toxicity reduction and capability retention. You need to understand that this is a multi-objective problem where improving one metric often degrades another.
  - **Quick check question:** If method A achieves 5% toxicity at 55% MMLU and method B achieves 3% toxicity at 53% MMLU, which lies on the Pareto front? Which is "better"?

## Architecture Onboarding

- **Component map:** Source set S + Control set C → forward pass to collect activations → PCA projection → logistic regression per layer → (optionally) accuracy threshold τ to disable low-performing layers → inference with interpolation
- **Critical path:**
  1. Curate high-quality source/control sets (32 samples each worked, but quality matters more than quantity)
  2. Choose PCA components (5 is default; fewer underfits, more doesn't help much)
  3. Set accuracy threshold τ OR use adaptive accuracy scaling
  4. Choose global strength λ (DSAS often works better with λ > 1)
- **Design tradeoffs:**
  - PCA components vs. training time: More components → longer training (40-57x slower without PCA)
  - Threshold τ vs. coverage: Higher τ → more conservative, may miss steering opportunities; lower τ → more intervention but potential false positives
  - Control set definition: For broad target distributions, C = T; for narrow targets, C ≠ T
- **Failure signatures:**
  - Classifier accuracy ≈ 0.5 across layers → DSAS provides no benefit, falls back to vanilla steering
  - Toxicity doesn't decrease even with high λ → check if source set actually represents target behavior
  - MMLU/perplexity degrades significantly → control set may overlap poorly with actual neutral inputs
- **First 3 experiments:**
  1. Baseline sanity check: Replicate toxicity mitigation on Qwen 2.5 1.5B with vanilla CAA/ITI/LinEAS. Sweep λ ∈ [0, 10] to establish Pareto front.
  2. Ablation on control set quality: Train DSAS with random control sentences vs. carefully curated controls. Measure impact on classifier accuracy and Pareto front.
  3. Cross-dataset generalization: Train DSAS on RTP toxicity data, evaluate on a different toxicity benchmark (e.g., Toxigen). Test whether "when-to-steer" classifiers generalize or overfit to training distribution.

## Open Questions the Paper Calls Out

- **Question:** Can non-linear regressors (e.g., MLPs) improve DSAS performance when the Linear Representation Hypothesis fails, and what computational trade-offs do they introduce?
- **Question:** How can DSAS achieve precise spatial localization of concepts in diffusion models rather than diffusing interventions across the image?
- **Question:** Can the global strength parameter λ be fully eliminated through adaptive normalization, removing the need for Pareto front tuning?
- **Question:** Does token-level supervision improve DSAS precision over the current sentence-level averaging approach for training per-layer classifiers?

## Limitations

- The method relies on the Linear Representation Hypothesis, which may fail for non-linear structures in activation space
- Results are primarily demonstrated on decoder-only models (Qwen 2.5, Gemma 2), with limited testing on other architectures
- The paper acknowledges that E2E-DSAS performance is sensitive to architecture and hyperparameter choices
- Limited experimental validation of DSAS generalization to text-to-image diffusion models, with no quantitative results provided

## Confidence

**High Confidence:**
- DSAS reduces computational overhead compared to vanilla steering (40-57x speedup with PCA)
- DSAS provides interpretability by highlighting which tokens require steering (cosine similarity analysis)
- The interpolation mechanism (Equation 3.4) is mathematically sound and correctly implemented

**Medium Confidence:**
- DSAS consistently improves the Pareto front across different steering methods on tested models
- DSAS generalizes to diffusion models (limited experimental support)
- E2E-DSAS can outperform offline DSAS (mixed results, architecture-dependent)

**Low Confidence:**
- DSAS performance generalizes to models outside the Qwen/Gemma family
- The 32-sample training set size is universally sufficient
- DSAS works equally well for all types of undesired behaviors beyond toxicity

## Next Checks

- **Cross-Architecture Robustness:** Test DSAS on diverse models including encoder-decoder, multimodal, and smaller models to verify scalability and identify architecture-dependent performance patterns.
- **Dataset Transferability Analysis:** Train DSAS classifiers on one toxicity dataset and evaluate on multiple others to measure classifier accuracy degradation and test whether "when-to-steer" patterns generalize versus dataset-specific memorization.
- **Systematic Linear Separability Testing:** For each model/dataset combination, measure per-layer classifier accuracy, distribution overlap before/after PCA, and performance degradation with varying PCA components to establish quantitative bounds on when the method should be expected to work.