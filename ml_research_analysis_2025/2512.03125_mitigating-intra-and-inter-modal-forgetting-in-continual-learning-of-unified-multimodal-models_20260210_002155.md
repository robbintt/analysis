---
ver: rpa2
title: Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified
  Multimodal Models
arxiv_id: '2512.03125'
source_url: https://arxiv.org/abs/2512.03125
tags:
- image
- forgetting
- multimodal
- generation
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses inter-modal catastrophic forgetting
  in unified multimodal generative models (UMGMs), where models forget image generation
  capabilities while learning multimodal understanding tasks. The authors propose
  Modality-Decoupled Experts (MoDE), which decouples text and image updates through
  modality-specific adapters (T-MoE for text, V-Adapter for images) and uses knowledge
  distillation to preserve image generation.
---

# Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models

## Quick Facts
- arXiv ID: 2512.03125
- Source URL: https://arxiv.org/abs/2512.03125
- Reference count: 40
- Key outcome: MoDE achieves 33.47% accuracy on multimodal understanding while maintaining image generation quality (FID 53.74)

## Executive Summary
This paper identifies and addresses inter-modal catastrophic forgetting in unified multimodal generative models (UMGMs), where models forget image generation capabilities while learning multimodal understanding tasks. The authors propose Modality-Decoupled Experts (MoDE), which decouples text and image updates through modality-specific adapters (T-MoE for text, V-Adapter for images) and uses knowledge distillation to preserve image generation. Experiments on Chameleon and Janus-Pro backbones show MoDE achieves 33.47% accuracy on multimodal understanding while maintaining image generation quality (FID 53.74), outperforming baselines like SeqLoRA (28.43% accuracy, FID 56.12) and Model Tailor (32.62% accuracy, FID 55.47). Theoretical analysis proves MoDE reduces gradient conflict between modalities, validating its effectiveness in mitigating both inter- and intra-modal forgetting.

## Method Summary
MoDE introduces modality-decoupled fine-tuning for UMGMs by separating text and image updates into distinct adapter modules. The method uses T-MoE (a mixture of text experts with soft routing) for text tokens and V-Adapter (single LoRA module) for image tokens, with knowledge distillation anchoring image generation capability. During sequential training on multimodal understanding tasks, text tokens are routed to T-MoE while image tokens go to V-Adapter, with each module updated independently. The V-Adapter additionally performs knowledge distillation from the frozen pre-trained UMGM to preserve generation capabilities. This architecture eliminates first-order gradient interference between modalities while allowing task-specific adaptation.

## Key Results
- MoDE achieves 33.47% average accuracy on multimodal understanding tasks
- Image generation quality maintained at FID 53.74
- Outperforms SeqLoRA (28.43% accuracy, FID 56.12) and Model Tailor (32.62% accuracy, FID 55.47)
- Theoretical proof shows MoDE bounds visual loss drift to O(η²) by eliminating gradient conflict
- Ablation confirms both modality decoupling and knowledge distillation are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Modality Gradient Conflict Drives Inter-modal Forgetting
When UMGMs share parameters across modalities, sequential training causes gradient conflict between text and image objectives, directly degrading the untrained modality. The paper defines modality gradient conflict via the inner product ⟨g_t, g_v⟩ of text and visual gradients. When this is negative, a gradient step optimizing text generation increases visual loss by −η⟨g_t, g_v⟩ (first-order Taylor expansion). This creates directional interference in shared parameter space. Evidence shows significant conflict in modality-coupled MoE-LoRA with many parameters having cosine < 0.5.

### Mechanism 2: Parameter Decoupling Eliminates First-Order Interference
Separating text and image updates into disjoint trainable subspaces (ϕ for T-MoE, ψ for V-Adapter) bounds inter-modal drift to O(η²), eliminating the dominant first-order conflict. By construction, ∂L_v/∂ϕ = 0 and ∂L_t/∂ψ = 0, making gradient supports orthogonal (⟨g_t, g_v⟩ = 0). The first-order term in the Taylor expansion vanishes, leaving only second-order Hessian effects. Empirical results show all parameters have zero cosine disagreement in MoDE, confirming orthogonal gradients.

### Mechanism 3: Knowledge Distillation Anchors Visual Generation
Logit-level distillation from the frozen pre-trained UMGM to the V-Adapter preserves image generation capability while allowing visual understanding adaptation. The V-Adapter loss combines cross-entropy (L_CE) with KL divergence (L_KD) via L_V-Adapter = L_CE + λL_KD (λ = 0.3). Distillation uses a small LAION-5B reference set, forcing student logits to match teacher soft predictions on image tokens. Evidence shows MoDE w/o KD drops to FID 54.61 vs. 53.74 with KD; understanding accuracy also drops slightly (33.07% → 33.47%).

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: Both T-MoE and V-Adapter build on LoRA. Understanding that W → W + BA with rank r ≪ min(d_in, d_out) enables efficient fine-tuning without modifying frozen weights.
  - Quick check: Can you explain why LoRA's low-rank constraint limits expressivity while enabling efficient adaptation?

- **Concept: Mixture of Experts (MoE) with Routing**
  - Why needed: T-MoE uses soft routing g_j(x) = softmax(xW_g)_j to select among n expert LoRAs. This enables task-specific adaptation within the text modality.
  - Quick check: How does a gating network decide which expert to activate for a given token, and what does "sparse" routing mean?

- **Concept: Gradient Geometry (Inner Product as Alignment)**
  - Why needed: The core insight relies on interpreting ⟨g_t, g_v⟩ < 0 as conflict. You must understand that negative inner product means gradients pull in opposite directions.
  - Quick check: If two loss gradients have cosine similarity of -0.8, what happens to loss A when you take a gradient step minimizing loss B?

- **Concept: Knowledge Distillation (Logit-level)**
  - Why needed: The V-Anchor uses KL divergence between softened teacher/student logits to transfer generation capability.
  - Quick check: Why soften logits with temperature β before computing KL divergence, rather than using hard labels?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed: The paper distinguishes intra-modal (same output type, different tasks) from inter-modal forgetting (different output modalities). Understanding this taxonomy is essential.
  - Quick check: Why does sequential fine-tuning cause forgetting, and how does MoDE differ from replay-based approaches?

## Architecture Onboarding

- **Component map:**
  Input sequence [text tokens, image tokens] → Token-type routing → T-MoE (n experts) for text tokens + V-Adapter (single LoRA) for image tokens → Reassembled sequence → Frozen UMGM backbone → Output

- **Critical path:**
  1. Token identification: Image tokens must be correctly identified (typically special tokens or position ranges in interleaved sequences)
  2. Modality routing: T-Router computes g_j(x) for text tokens; image tokens bypass routing
  3. Disjoint updates: T-MoE parameters ϕ and V-Adapter parameters ψ never interact during backprop
  4. KD forward pass: For V-Adapter, a parallel teacher forward pass computes reference logits from frozen UMGM

- **Design tradeoffs:**
  - Number of experts (n): Table 7 shows 4 experts balance performance (33.47%) vs. cost; 6 experts add modest gains (34.90%) but increase routing complexity
  - Distillation weight (λ): Table 5 shows λ = 0.3 is optimal; λ = 0.5 improves FID but drops accuracy (31.53%)
  - LoRA rank (r): Fixed at 8 across experiments; higher rank increases capacity but also interference risk
  - Reference data for KD: Small LAION-5B subset; too little data → weak anchor, too much → computational overhead

- **Failure signatures:**
  - Decoupling without KD (Table 3, "MoDE w/o KD"): FID degrades to 54.61, understanding drops to 33.07% → V-Adapter drifts without anchor
  - KD without decoupling (Table 8): Adding KD to SeqLoRA improves FID (56.12 → 52.83) but understanding still lags (28.43% → 30.81%); MoELoRA+KD hurts understanding (33.01% → 30.07%) → KD "locks in" conflicting features when parameters are shared
  - High λ (Table 5, λ = 1.0): FID improves (51.72) but accuracy drops (32.10%) → over-constrained adaptation
  - DualPrompt baseline (Table 1): Low forgetting (6.82%) but low accuracy (31.92%) → insufficient capacity to learn hard tasks

- **First 3 experiments:**
  1. Gradient conflict measurement: Train baseline MoE-LoRA on a single understanding task; compute cosine similarity between text and image gradients across all shared parameters. Expect distribution similar to Figure 5 (high conflict).
  2. Decoupling ablation: Implement MoDE without KD (just T-MoE + V-Adapter with L_CE). Compare forgetting metrics vs. SeqLoRA. Expect Table 9 results: decoupling alone gives modest gains (FID 54.74 vs. 56.12).
  3. Lambda sweep with monitoring: Train full MoDE with λ ∈ {0.0, 0.3, 0.5, 1.0}. Plot both FID and accuracy. Identify the Pareto frontier and verify λ = 0.3 is near-optimal. Monitor training curves for signs of over-anchoring (flat accuracy improvement despite low forgetting).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MoDE architecture be effectively adapted for unified models that utilize non-autoregressive backbones or diffusion-based decoders?
- Basis in paper: [explicit] The paper states the method is designed for "transformer-based UMGMs... built on this autoregressive architecture," explicitly distinguishing it from "diffusion+MLLM" designs.
- Why unresolved: The theoretical proof relies on gradient conflicts within an autoregressive framework; it is unclear if modality decoupling via LoRA functions similarly in denoising diffusion processes or other generative paradigms.
- What evidence would resolve it: Experiments applying MoDE to a unified model with a diffusion-based decoder (e.g., Show-o or similar hybrid architectures) to test if decoupling mitigates forgetting in non-autoregressive generation.

### Open Question 2
- Question: Does the binary decoupling strategy (T-MoE vs. V-Adapter) scale effectively to UMGMs incorporating three or more modalities, such as audio or video?
- Basis in paper: [inferred] The paper introduces a strict dichotomy between "T-MoE" for text and "V-Adapter" for images, leaving the architectural configuration for additional modalities undefined.
- Why unresolved: Adding a third modality (e.g., audio) would require determining if it needs its own adapter, shares an existing one, or requires a new MoE, potentially reintroducing gradient conflicts.
- What evidence would resolve it: Implementation of MoDE on a tri-modal model (e.g., Image-Audio-Text) to observe if isolating a third modality preserves performance or if inter-modal conflicts re-emerge.

### Open Question 3
- Question: How sensitive is the knowledge distillation effectiveness to the composition and size of the reference dataset used for the V-Adapter?
- Basis in paper: [inferred] The method relies on "a small subset of images sampled from the LAION-5B dataset" for distillation, but does not analyze the impact of data selection.
- Why unresolved: If the reference data does not cover the semantic diversity of the pre-trained knowledge, the distillation might preserve some capabilities while failing to protect others ("partial" forgetting).
- What evidence would resolve it: Ablation studies varying the domain and size of the reference dataset (e.g., using only specific classes vs. diverse data) to measure the impact on FID and CLIP scores.

### Open Question 4
- Question: Does the T-MoE routing mechanism suffer from expert collapse or capacity saturation in scenarios with significantly longer task sequences?
- Basis in paper: [inferred] The experiments limit the continual learning sequence to 5-6 datasets; while the architecture is scalable, the routing behavior over hundreds of sequential tasks is untested.
- Why unresolved: In continual learning, router functions can converge to selecting only a few experts, reducing plasticity; MoDE's specific routing might be susceptible to this over extended horizons.
- What evidence would resolve it: Evaluation on a longer stream of diverse tasks (e.g., >20 tasks) to analyze the entropy of the router distribution and expert utilization rates.

## Limitations
- Architecture scope: MoDE is validated only on Chameleon and Janus-Pro backbones, limiting generalization claims
- Sequential-only evaluation: Method tested only on sequential task orders, not shuffled or non-sequential scenarios
- Token identification: Paper assumes reliable image token identification but doesn't specify the mechanism

## Confidence

**High Confidence (Empirical + Theoretical Support):**
- MoDE reduces inter-modal forgetting (FID improvement from 56.12→53.74)
- Gradient conflict between modalities exists in UMGM fine-tuning
- Knowledge distillation alone cannot fully mitigate forgetting without decoupling

**Medium Confidence (Theoretical Support + Limited Empirical):**
- Gradient conflict is the primary driver of inter-modal forgetting
- O(η²) bound on visual loss drift in MoDE is tight enough for practical significance
- Optimal λ = 0.3 is universally applicable across tasks

**Low Confidence (Single Point Evidence or Unverified Claims):**
- MoDE's superiority over all baselines in all metrics across all conditions
- The routing mechanism's robustness to ambiguous token types
- Generalization to task orders beyond the sequential evaluation

## Next Checks

**Check 1: Gradient Conflict Characterization** - Train a modality-coupled baseline (SeqLoRA) on ScienceQA→TextVQA, then measure and visualize the distribution of ⟨g_t, g_v⟩ across shared parameters. Verify negative inner products are prevalent and correlate with FID degradation.

**Check 2: Token Routing Robustness Test** - Create ambiguous interleaved sequences where text and image tokens are difficult to distinguish. Evaluate whether MoDE's routing mechanism correctly identifies and separates them, or if misclassification occurs.

**Check 3: Non-Sequential Task Order** - Train MoDE on the same five tasks but in a shuffled order (e.g., ImageNet→VizWiz→ScienceQA→GQA→TextVQA). Compare accuracy and forgetting metrics against the sequential baseline to assess robustness to task ordering.