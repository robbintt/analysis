---
ver: rpa2
title: Fine-tuning Flow Matching Generative Models with Intermediate Feedback
arxiv_id: '2510.18072'
source_url: https://arxiv.org/abs/2510.18072
tags:
- learning
- critic
- policy
- flow
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AC-Flow, an actor-critic framework for fine-tuning
  flow matching generative models with intermediate feedback. The method addresses
  the credit assignment problem in text-to-image generation by learning value estimates
  for intermediate states rather than relying solely on outcome rewards.
---

# Fine-tuning Flow Matching Generative Models with Intermediate Feedback

## Quick Facts
- arXiv ID: 2510.18072
- Source URL: https://arxiv.org/abs/2510.18072
- Authors: Jiajun Fan; Chaoran Cheng; Shuaike Shen; Xiangxin Zhou; Ge Liu
- Reference count: 40
- Key outcome: AC-Flow improves CLIP Score from 29.37 to 32.93 on Stable Diffusion 3 while maintaining sample diversity

## Executive Summary
This paper introduces AC-Flow, an actor-critic framework for fine-tuning flow matching generative models using intermediate feedback. The method addresses the credit assignment problem in text-to-image generation by learning value estimates for intermediate states rather than relying solely on outcome rewards. AC-Flow introduces three key innovations: reward shaping for stable intermediate value learning, a dual-stability mechanism combining advantage clipping with warm-up strategies, and a generalized critic weighting scheme with Wasserstein regularization to maintain diversity.

## Method Summary
AC-Flow proposes an actor-critic framework that fine-tunes flow matching generative models by incorporating intermediate feedback signals during the generation process. The approach uses a flow matching model with conditional diffusion processes, where the actor network generates samples conditioned on intermediate rewards, while the critic network estimates value functions for these intermediate states. The framework introduces reward shaping to stabilize value learning, dual-stability mechanisms to prevent overfitting and maintain robustness, and Wasserstein regularization to preserve output diversity. The method is evaluated on Stable Diffusion 3, demonstrating significant improvements in alignment metrics while maintaining generation quality.

## Key Results
- Improves CLIP Score from 29.37 to 32.93 on Stable Diffusion 3
- Maintains sample diversity while improving alignment with text prompts
- Demonstrates strong generalization to unseen human preference metrics

## Why This Works (Mechanism)
AC-Flow works by solving the credit assignment problem in flow matching models through temporal value estimation. Traditional approaches only use outcome rewards, making it difficult to propagate feedback to intermediate generation steps. By introducing a critic network that estimates values for intermediate states, AC-Flow enables more precise gradient updates throughout the generation process. The dual-stability mechanism prevents the critic from becoming over-confident in early training stages while the advantage clipping prevents extreme policy updates. The Wasserstein regularization ensures the fine-tuned model maintains coverage of the original output distribution, preventing mode collapse.

## Foundational Learning

**Flow Matching Models**
- Why needed: Understanding the base architecture for generative modeling
- Quick check: Can you explain how flow matching differs from diffusion models?

**Actor-Critic Reinforcement Learning**
- Why needed: Core framework for credit assignment across intermediate states
- Quick check: How does advantage estimation improve policy updates?

**Reward Shaping**
- Why needed: Stabilizes value learning in sparse reward environments
- Quick check: What's the difference between potential-based and heuristic reward shaping?

**Wasserstein Distance**
- Why needed: Metric for measuring distribution similarity and maintaining diversity
- Quick check: Why is Wasserstein distance preferred over KL divergence for diversity preservation?

## Architecture Onboarding

**Component Map**
Flow Matching Model -> Actor Network -> Critic Network -> Reward Shaper -> Wasserstein Regularizer

**Critical Path**
Input text prompt → Flow matching generation → Intermediate state extraction → Critic value estimation → Advantage calculation → Actor policy update → Output image generation

**Design Tradeoffs**
The dual-network architecture (actor + critic) provides better credit assignment but increases computational overhead and training complexity. Reward shaping improves stability but requires careful hyperparameter tuning. Wasserstein regularization preserves diversity but adds computational cost to each training iteration.

**Failure Signatures**
- Vanishing gradients if advantage values become too small
- Critic collapse if reward shaping is too aggressive
- Diversity loss if Wasserstein regularization weight is insufficient
- Mode collapse if the dual-stability mechanism fails to constrain updates

**3 First Experiments**
1. Baseline: Fine-tune Stable Diffusion 3 with only outcome rewards
2. Ablation: Remove dual-stability mechanism to measure its impact on stability
3. Sensitivity: Vary Wasserstein regularization weight to observe diversity effects

## Open Questions the Paper Calls Out
None

## Limitations

- Limited evaluation scope focused only on Stable Diffusion 3 without cross-architecture validation
- Computational overhead from maintaining dual-network architecture not quantified
- Limited ablation studies on individual component contributions (reward shaping, dual-stability, Wasserstein regularization)

## Confidence

- **High Confidence**: Mathematical formulation and theoretical grounding in reinforcement learning are sound
- **Medium Confidence**: Generalization claims to human preference metrics need more extensive validation
- **Low Confidence**: Universal applicability claims across all flow matching models lack sufficient cross-model validation

## Next Checks

1. Conduct systematic ablation studies removing each of the three key innovations to quantify their individual contributions to performance improvements.

2. Evaluate AC-Flow across multiple flow matching architectures beyond Stable Diffusion 3 to establish broader applicability.

3. Perform computational complexity analysis comparing training and inference times of AC-Flow against baseline fine-tuning methods.