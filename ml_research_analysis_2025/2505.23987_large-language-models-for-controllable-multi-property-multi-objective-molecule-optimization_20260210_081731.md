---
ver: rpa2
title: Large Language Models for Controllable Multi-property Multi-objective Molecule
  Optimization
arxiv_id: '2505.23987'
source_url: https://arxiv.org/abs/2505.23987
tags:
- shot
- tasks
- llms
- properties
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-MuMOInstruct, the first large-scale instruction-tuning
  dataset for controllable multi-property molecule optimization with property-specific
  objectives. It enables models to selectively improve specific molecular properties
  while maintaining others at desirable levels, reflecting real-world drug design
  needs.
---

# Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization

## Quick Facts
- arXiv ID: 2505.23987
- Source URL: https://arxiv.org/abs/2505.23987
- Reference count: 40
- Primary result: GeLLM4O-Cs achieve up to 126% higher success rate on multi-property molecule optimization compared to strong baselines

## Executive Summary
This paper introduces C-MuMOInstruct, the first large-scale instruction-tuning dataset for controllable multi-property molecule optimization with property-specific objectives. The dataset enables models to selectively improve specific molecular properties while maintaining others at desirable levels, reflecting real-world drug design needs. Leveraging this dataset, the authors develop GeLLM4O-Cs, a family of instruction-tuned LLMs including specialist and generalist variants. Experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLM4O-Cs consistently outperform strong baselines, achieving up to 126% higher success rate.

## Method Summary
The authors construct C-MuMOInstruct from molecule pairs satisfying similarity, improvement, and stability constraints, creating 256,185 pairs across 28,266 tasks with 30 instruction templates per task. They fine-tune Mistral-7B-Instruct-v0.3 or Llama-3.1-8B-Instruct using LoRA (rank=16, α=16, dropout=0.05) on all projection layers and LM head. Specialist models train on specific property combinations (10 epochs, batch 32) while generalist models train on diverse property combinations (1800 steps, batch 128). At inference, they use 0-shot evaluation with beam search (width=20) to generate 20 candidates per input, validating SMILES and checking property constraints against thresholds.

## Key Results
- GeLLM4O-Cs achieve up to 126% higher success rate compared to strong baselines on multi-property molecule optimization
- Generalist models outperform specialist ones on 4 out of 5 in-distribution tasks, with up to 26% improvement in success rate
- Generalist GeLLM4O-Cs demonstrate strong 0-shot generalization to out-of-distribution tasks, outperforming baselines by 27% on average
- The models show robust performance with high validity rates (>90%) and structural similarity preservation (Tanimoto > 0.6)

## Why This Works (Mechanism)

### Mechanism 1: Property-Specific Objective Encoding via Threshold-Based Instruction Design
- **Claim:** The model learns to distinguish between sub-optimal properties requiring improvement versus near-optimal properties requiring maintenance through explicit threshold-based objectives in training data.
- **Mechanism:** C-MuMOInstruct encodes a binary classification per property: if p(Mx) < Θp, the property is labeled sub-optimal and the task requires improvement by at least ∆p; if p(Mx) ≥ Θp, the property is labeled near-optimal and the task requires maintenance within ∆p. This creates 28,266 distinct task configurations where the model must attend to property-specific instructions rather than applying uniform improvement strategies.
- **Core assumption:** The model can associate instruction tokens (e.g., "improve BBBP to ≥0.9", "maintain QED") with the appropriate constraint satisfaction behavior during generation. Assumption: property names and direction words serve as control signals rather than mere description.
- **Evidence anchors:** [abstract] "explicitly incorporates controllable property-specific objectives – specifying which properties must be improved up to a user-defined property-specific threshold, and which must be maintained within acceptable bounds" [section 2] "A C-MuMO task optimizing Mx to My aims to improve all sub-optimal properties Pi = {p ∈ P| p(Mx) ≺ Θp} while maintaining all near-optimal properties Ps = {p ∈ P | p(Mx) ≻= Θp}" [corpus] Weak direct evidence; related work (AgentDrug, M^4olGen) addresses multi-property optimization but not explicit improve-vs-maintain distinction via thresholds.
- **Break condition:** If property tokens are permuted or replaced with synonyms the model hasn't seen (e.g., "BBBP" → "blood-brain barrier permeation" without training exposure), performance may degrade significantly. Table A13 shows specialist models drop >5% SR on unseen instructions for 2/5 tasks.

### Mechanism 2: Cross-Task Transfer via Generalist Training on Diverse Property Combinations
- **Claim:** Generalist models trained across multiple property combinations and objectives transfer shared chemical semantics and modification strategies to novel property combinations.
- **Mechanism:** Generalist GeLLM4O-C-P(10) is jointly trained on tasks involving all 10 properties in diverse combinations (single to decuple-property). This exposes the model to overlapping structural modification patterns (e.g., adding lipophilic groups improves PlogP and BBBP) while varying which properties are targeted. During inference on novel combinations, the model composes learned patterns.
- **Core assumption:** Structural modification strategies are partially compositional across properties—learning that aromatic chlorides increase lipophilicity transfers from PlogP tasks to BBBP tasks. Assumption: training diversity enables generalization without memorizing task-specific solutions.
- **Evidence anchors:** [section 5.1] "Generalist GeLLM4O-Cs outperform specialist ones on 4 out of 5 IND tasks...where generalist models outperform specialist ones by up to 26% in SR. Limited training pairs in these tasks hinder the specialist models to learn robust modification strategies." [section 5.2] "Generalist GeLLM4O-Cs exhibit strong 0-shot generalization to OOD tasks, outperforming strong baselines by 27% on average." [corpus] GeLLMO (Dey et al. 2025) demonstrated generalization without property-specific control; this work extends with controllable objectives. M^4olGen uses multi-agent approach for precise constraints but requires different architecture.
- **Break condition:** Conflicting objectives across training tasks may degrade specialized performance. Section 5.1 notes GeLLM4O-C-P(10) underperforms on BDPQ (895 pairs) compared to specialists, potentially because "tasks with competing or conflicting objectives weaken its ability to specialize."

### Mechanism 3: Pairwise Structure-Property Association Learning
- **Claim:** Training on molecule pairs satisfying similarity, improvement, and stability constraints enables implicit learning of how specific structural modifications map to multi-property changes.
- **Mechanism:** Each training example presents (Mx, My) where Tanimoto similarity > 0.6, sub-optimal properties improve by ≥∆p, and near-optimal properties change within ∆p. The model learns the mapping from (instruction + Mx structure) to (My structure) that satisfies constraints, implicitly encoding structure-property relationships without explicit property predictors.
- **Core assumption:** The SMILES string representation plus instruction provides sufficient signal for the model to identify which substructures to modify. Assumption: the model can perform counterfactual reasoning—"if I change this substructure, properties change this way"—from paired examples alone.
- **Evidence anchors:** [section 2.1] "C-MuMOInstruct is constructed from molecule pairs that satisfy similarity, property improvement, and stability constraints. This enables models to effectively associate targeted structural modifications with property changes." [section F.1] Case study shows GeLLM4O-C replacing morpholine with para-chlorophenyl improves AMP (+0.29) and PlogP (+0.85) while maintaining CARC and hERG. [corpus] AgentDrug uses iterative feedback from property predictors; this approach learns from paired data without explicit predictor integration.
- **Break condition:** If test molecules have scaffolds poorly represented in training pairs, the model may generate invalid SMILES or apply inappropriate modifications. Validity drops to 88.8% for GeLLM4O-C-P(10)Llama on some tasks (Table A3).

## Foundational Learning

- **Concept: SMILES (Simplified Molecular Input Line Entry System)**
  - **Why needed here:** Input and output representations are SMILES strings. Understanding canonical SMILES, tokenization, and validity is essential for debugging generation failures.
  - **Quick check question:** Can you identify why "C1=CC=CC=C1" and "c1ccccc1" represent the same benzene ring but may tokenize differently?

- **Concept: ADMET Properties in Drug Discovery**
  - **Why needed here:** The 10 properties (AMP, BBBP, hERG, etc.) represent absorption, distribution, metabolism, excretion, and toxicity factors. Understanding which direction is desirable (e.g., lower hERG is better) is required to interpret results.
  - **Quick check question:** For a CNS-targeting drug, should BBBP be maximized or minimized? What about hERG?

- **Concept: Multi-Objective Optimization Trade-offs**
  - **Why needed here:** Properties often conflict (e.g., increasing lipophilicity for BBBP may increase hERG toxicity). The model must navigate Pareto-optimal solutions. Understanding threshold-based success versus scalarized rewards clarifies evaluation.
  - **Quick check question:** Why might a model succeed at improving BBBP and DRD2 but fail at maintaining hERG?

## Architecture Onboarding

- **Component map:** Base LLM (Mistral-7B-Instruct-v0.3 or Llama-3.1-8B-Instruct) -> LoRA fine-tuning (rank=16, α=16, dropout=0.05) on all projection layers + LM head -> C-MuMOInstruct dataset (256,185 pairs, 28,266 tasks) -> 0-shot inference with beam search (width=20)
- **Critical path:** Load preprocessed molecule pairs with property labels (sub-optimal vs. near-optimal) -> Format instruction using template: general instruction + SMILES input + property-specific adjustment phrases -> Fine-tune with LoRA, ensuring held-out instruction per task for generalization testing -> At inference: format test instruction, generate 20 candidates via beam search, validate SMILES, compute property scores, check constraints
- **Design tradeoffs:** Specialist vs. Generalist: Specialist models excel on specific property combinations with sufficient data; generalist models transfer better to novel combinations but may underperform on specialized tasks with conflicting objectives. Training epochs: Generalist P(10) trained for 1,800 steps vs. specialists for 10 epochs—balances cost vs. overfitting risk. Beam width: 20 candidates provide diversity but increase inference cost; no sampling temperature tuning reported.
- **Failure signatures:** Low Validity (<90%): Model generates syntactically invalid SMILES; check tokenizer handling of special characters. High Similarity but Low RI: Model is too conservative, making minimal changes that don't satisfy improvement thresholds. Low Similarity and Low Success: Model changes scaffold excessively, violating similarity constraint and failing property objectives. OOD performance collapse: Model overfits to training property combinations; check training diversity and held-out instruction performance.
- **First 3 experiments:** Reproduce IND baseline comparison: Train GeLLM4O-C-N (specialist) on a single property combination (e.g., BPQ with 700 pairs), evaluate SR and RI against Mistral-1-shot baseline. Verify ~71% SR matches reported specialist performance. Ablate instruction diversity: Train on only 5 instruction templates instead of 30, evaluate on held-out instructions. Expect >5% SR drop per Table A13 pattern. Test OOD generalization with reduced training scope: Train generalist on only 5-property combinations (exclude all 4-property tasks), then evaluate on BDEQ (4-property OOD). Quantify transfer degradation vs. full P(10) model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can GeLLM4O-C be extended to support iterative, multi-step optimization using feedback mechanisms to ensure molecules reach specific pharmaceutical thresholds?
- **Basis in paper:** [explicit] The authors explicitly state their framework is limited to "single-step optimization" and that real-world optimization often requires multiple iterations, noting that designing a "feedback mechanism... is non-trivial."
- **Why unresolved:** The current architecture outputs a single modified molecule based on the input instruction; it lacks a loop to evaluate the output against constraints and trigger further refinement.
- **What evidence would resolve it:** A modified GeLLM4O-C framework utilizing reinforcement learning or a multi-turn dialogue setup that successfully iterates on molecules until all $\Theta_p$ thresholds are reached.

### Open Question 2
- **Question:** How can the negative transfer effects of competing objectives be mitigated when training foundational models on diverse multi-property tasks?
- **Basis in paper:** [explicit] The authors observe that the generalist GeLLM4O-C-P(10) model underperforms on specific tasks like BDPQ, hypothesizing this is due to "tasks with competing or conflicting objectives."
- **Why unresolved:** Simply scaling up training data with multi-task tuning introduces conflicts that degrade performance on specialized tasks, and the paper does not propose a solution for this specific interference.
- **What evidence would resolve it:** An architectural modification (e.g., Mixture-of-Experts) or training strategy that improves the P(10) model's performance on conflicting tasks to match or exceed the specialist models.

### Open Question 3
- **Question:** What is the performance discrepancy between GeLLM4O-C optimizations based on computational predictors versus those validated by experimental wet-lab data?
- **Basis in paper:** [explicit] The authors note they "rely on computational predictors" (like ADMET-AI), which may "introduce inaccuracies" and not reflect exact experimental outcomes.
- **Why unresolved:** The model's success is defined entirely by computational metrics ($p(My) \succ \Delta_p$), which are proxies that may diverge from biological reality.
- **What evidence would resolve it:** Benchmarking GeLLM4O-C generated molecules using in vitro assays to correlate the reported computational "Success Rate" (SR) with actual biological activity and safety profiles.

## Limitations
- Evaluation relies entirely on computational property predictors rather than experimental validation, which may not capture complex structure-activity relationships
- Dataset construction depends on thresholds (Θp at 60th percentile, ∆p improvements) that may not generalize across different molecular scaffolds or property distributions
- SMILES-based representation may limit the model's ability to generate structurally diverse molecules compared to graph-based approaches

## Confidence

- **High Confidence:** The instruction-tuning methodology and dataset construction approach are well-specified and reproducible. The performance improvements over baselines on in-distribution tasks are robust across multiple metrics (SR, SRΘ, RI, Sim). The LoRA fine-tuning configuration is clearly documented with specific hyperparameters.
- **Medium Confidence:** The out-of-distribution generalization claims are supported by results but may be influenced by the specific property combinations chosen. The specialist vs. generalist trade-offs are observed but could depend on the particular tasks and data distribution used. The failure mode analysis provides useful diagnostic guidance but may not capture all edge cases.
- **Low Confidence:** The biological relevance of computational property improvements is not validated experimentally. The claim that property-specific threshold encoding creates distinct improve-vs-maintain behaviors is plausible but not directly verified through ablation studies. The long-term generalization to truly novel molecular scaffolds or property combinations remains untested.

## Next Checks

1. **Instruction Template Ablation:** Train models with systematically reduced instruction template diversity (e.g., 5, 10, 15 templates) to quantify the contribution of instruction variation to generalization performance. Measure SR drop per template reduction and identify minimum viable template count.

2. **Scaffold Diversity Stress Test:** Evaluate models on test sets with systematically varied scaffold similarity to training data (e.g., Tanimoto < 0.3 vs. > 0.8). Quantify performance degradation and identify scaffold types where the model fails to generate valid or effective modifications.

3. **Property Predictor Consistency Validation:** Cross-validate property predictions using multiple independent computational tools (e.g., compare ADMET-AI vs. DeepChem for same molecules). Measure correlation between predictor pairs and quantify impact of predictor disagreement on SR measurements.