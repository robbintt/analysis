---
ver: rpa2
title: Text Prompt Injection of Vision Language Models
arxiv_id: '2510.09849'
source_url: https://arxiv.org/abs/2510.09849
tags:
- text
- attack
- image
- injection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates text prompt injection attacks on vision\
  \ language models (VLMs), where adversarial text is embedded in images to mislead\
  \ the model's output. The authors propose an algorithm that identifies high color-consistency\
  \ areas in images and perturbs pixels to create visible text prompts while maintaining\
  \ an l\u221E constraint."
---

# Text Prompt Injection of Vision Language Models
## Quick Facts
- **arXiv ID:** 2510.09849
- **Source URL:** https://arxiv.org/abs/2510.09849
- **Reference count:** 38
- **Primary result:** Proposed algorithm for text prompt injection achieves 41.2% untargeted and 37.6% targeted attack success rates on Llava-Next-72B with l∞=8/255 constraint

## Executive Summary
This paper investigates text prompt injection attacks on vision language models (VLMs), where adversarial text is embedded in images to mislead the model's output. The authors propose an algorithm that identifies high color-consistency areas in images and perturbs pixels to create visible text prompts while maintaining an l∞ constraint. Experiments on the Llava-Next-72B model using the Oxford-IIIT Pet Dataset show that this approach achieves significantly higher success rates than gradient-based transfer attacks, with untargeted ASR reaching 41.2% and targeted ASR reaching 37.6% under tight constraints (ϵ=8/255). The method is effective for large VLMs, requires fewer computational resources, and remains covert to human observers.

## Method Summary
The proposed method identifies high color-consistency areas in images and perturbs pixels to embed adversarial text prompts while maintaining an l∞ constraint. The algorithm leverages the observation that text prompt injection is particularly effective for large VLMs, requiring fewer computational resources compared to traditional gradient-based transfer attacks. The approach focuses on maintaining the covertness of the attack to human observers while achieving high attack success rates on the Llava-Next-72B model using the Oxford-IIIT Pet Dataset.

## Key Results
- Untargeted attack success rate reaches 41.2% under tight l∞ constraint (ϵ=8/255)
- Targeted attack success rate reaches 37.6% with the same constraint
- Method outperforms gradient-based transfer attacks significantly
- Attack remains covert to human observers while being effective

## Why This Works (Mechanism)
The effectiveness stems from exploiting the way VLMs process visual and textual information simultaneously. By embedding adversarial text in regions of high color consistency, the attack leverages the model's tendency to attend to visually coherent regions while simultaneously processing embedded textual information. The l∞ constraint ensures that pixel perturbations remain minimal and visually imperceptible to human observers, while still being sufficient to trigger the desired model behavior. The method's efficiency comes from avoiding computationally expensive gradient calculations required by traditional transfer attacks.

## Foundational Learning
- **Vision Language Models (VLMs):** Multimodal models that process both visual and textual information. Needed to understand the target system and attack surface.
- **Adversarial Attacks:** Techniques to manipulate model inputs to produce incorrect outputs. Essential for understanding the attack methodology and threat model.
- **l∞ Constraint:** Limits the maximum pixel-wise perturbation in adversarial attacks. Critical for maintaining attack covertness while ensuring effectiveness.
- **Color Consistency:** Regions in images with similar color values. Important for identifying optimal locations to embed adversarial text.
- **Attack Success Rate (ASR):** Metric measuring the effectiveness of adversarial attacks. Used to evaluate and compare different attack methods.

## Architecture Onboarding
**Component Map:** Image Processing -> Color Consistency Analysis -> Text Embedding -> VLM Input
**Critical Path:** Image analysis → High color-consistency region identification → Pixel perturbation for text embedding → VLM processing → Output manipulation
**Design Tradeoffs:** Balance between attack success rate and covertness through l∞ constraint tuning; computational efficiency vs. attack strength
**Failure Signatures:** Low attack success rate when color consistency regions are insufficient; visible artifacts when l∞ constraint is too loose; reduced effectiveness on smaller VLMs
**3 First Experiments:** 1) Test on different VLM architectures (not just Llava-Next-72B), 2) Evaluate on diverse datasets beyond Oxford-IIIT Pet Dataset, 3) Measure attack effectiveness against various l∞ constraint values

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single VLM model (Llava-Next-72B) and specific dataset (Oxford-IIIT Pet Dataset)
- No thorough exploration of attack robustness against potential defenses
- Human evaluation of covertness not detailed
- Claims about computational efficiency based on limited comparisons

## Confidence
- Effectiveness claims: **Medium** - Strong results on tested setup but limited generalization
- Computational efficiency claims: **Medium** - Based on comparisons with specific baseline methods
- Covertness claims: **Medium** - Human evaluation details not provided

## Next Checks
1. Validate attack effectiveness across multiple VLM architectures and scales
2. Test robustness against common adversarial defense mechanisms
3. Conduct comprehensive human studies to verify covertness claims across different image types and text prompts