---
ver: rpa2
title: 'WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot
  Personalization'
arxiv_id: '2508.19544'
source_url: https://arxiv.org/abs/2508.19544
tags:
- gaze
- estimation
- face
- head
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WebEyeTrack, a novel browser-compatible eye-tracking
  framework that addresses key limitations in existing webcam-based gaze estimation
  methods. The framework combines metric head pose estimation through 3D face reconstruction
  with a lightweight CNN model called BlazeGaze, which uses few-shot learning to adapt
  to new users with as few as nine calibration samples.
---

# WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization

## Quick Facts
- **arXiv ID:** 2508.19544
- **Source URL:** https://arxiv.org/abs/2508.19544
- **Reference count:** 19
- **Primary result:** Browser-compatible eye-tracking with 2.32 cm error on GazeCapture, adapting to new users with just 9 calibration samples

## Executive Summary
WebEyeTrack introduces a novel browser-compatible eye-tracking framework that addresses key limitations in existing webcam-based gaze estimation methods. The system combines metric head pose estimation through 3D face reconstruction with a lightweight CNN model called BlazeGaze, which uses few-shot learning to adapt to new users with minimal calibration data. The framework achieves state-of-the-art performance while maintaining real-time inference speeds on consumer devices, successfully bridging the gap between high-accuracy appearance-based gaze estimation and scalable webcam-based eye-tracking solutions.

## Method Summary
WebEyeTrack employs a two-stage training approach combining metric head pose estimation via iris-based scaling with a few-shot learning adaptation mechanism. The system uses MediaPipe for 3D face mesh reconstruction, deriving metric-scale head pose from monocular webcam input by applying an iris diameter constant (1.2 cm) to resolve scale ambiguity. A lightweight CNN architecture called BlazeGaze, built with BlazeBlocks optimized for browser environments, performs the gaze estimation. The model undergoes representation learning followed by meta-learning (MAML) where the encoder is frozen and only the gaze estimator is adapted to new users with minimal calibration samples.

## Key Results
- Achieves 2.32 cm error margin on GazeCapture dataset
- Real-time inference speeds of 2.4 milliseconds on iPhone 14
- Adapts to new users with as few as nine calibration samples
- Demonstrates strong robustness to temporal drift compared to WebGazer

## Why This Works (Mechanism)

### Mechanism 1: Metric Head Pose Estimation via Iris Scaling
The system mitigates temporal drift and improves robustness to head movement by deriving metric-scale head pose from monocular webcam input rather than relying on relative 2D landmarks. Using MediaPipe for 3D face mesh reconstruction, it applies the average human iris diameter (1.2 cm) to estimate face scale, converting normalized face landmarks into metric coordinates. A "radial Procrustes" analysis iteratively refines depth estimation to align projected 3D points with 2D observations.

### Mechanism 2: Frozen-Encoder Few-Shot Personalization (MAML)
The system enables rapid adaptation to new users with limited data (k â‰¤ 9) by pre-training a robust encoder and only meta-learning the gaze estimation head. BlazeGaze uses a two-stage training process: representation learning with an autoencoder trained on reconstruction, gaze, and embedding consistency losses, followed by MAML applied only to the lightweight MLP gaze estimator while keeping the encoder frozen.

### Mechanism 3: Browser-Native Inference via BlazeBlocks
The system achieves real-time browser performance by utilizing a specialized CNN architecture (BlazeGaze) optimized for web environments. The model replaces standard heavy convolutions with "BlazeBlocks" that use depthwise separable convolutions and channel splitting to reduce FLOPs, allowing the entire pipeline to execute directly in the browser client via TensorFlow.js.

## Foundational Learning

- **Model-Agnostic Meta-Learning (MAML)**
  - *Why needed:* Standard transfer learning would require more data and compute; MAML specifically optimizes the initialization for rapid gradient descent
  - *Quick check:* If you run 5 gradient steps on a new user and the loss explodes, did the meta-training fail, or is the inner-loop learning rate too high?

- **Monocular Scale Ambiguity**
  - *Why needed:* A single camera cannot distinguish between a small face close to the camera and a large face far away; understanding this limitation is required to appreciate why the paper uses an "iris diameter constant" heuristic
  - *Quick check:* Why does projecting 2D landmarks to 3D metric space require a known physical reference length (like iris width)?

- **Knowledge Distillation vs. Architecture Design**
  - *Why needed:* The paper doesn't use a massive model and distill it; it designs a small model (BlazeBlocks) from the start, showing efficiency can come from architecture rather than compression
  - *Quick check:* How does a "BlazeBlock" reduce computational cost compared to a standard Conv2D layer?

## Architecture Onboarding

- **Component map:** Webcam Frame -> MediaPipe Face Mesh -> Extracts Eye Patches + UVZ Landmarks -> Custom JS geometric solver -> Metric Head Pose Vector -> BlazeGaze (TF.js) -> CNN Encoder -> MLP Regressor -> 2D Point of Gaze

- **Critical path:** The Face Mesh detection is the bottleneck. If MediaPipe drops frames, both Pose and Gaze inputs become stale. The Iris scaling calculation is the most fragile geometric step.

- **Design tradeoffs:** Trades raw accuracy (SOTA ViTs are slightly better) for temporal stability by explicitly modeling head pose; generalizes by freezing the encoder to ensure the meta-learner doesn't overfit to specific user features during quick adaptation.

- **Failure signatures:** "Snapping" Gaze indicates head pose estimation failure (check Iris detection); slow calibration (>2 seconds) suggests device falling back to CPU; center bias indicates encoder features not activating (input normalization issue).

- **First 3 experiments:**
  1. Place face at measured distances (40cm, 60cm, 80cm) from camera; compare reported metric head pose Z-depth against ground truth to validate iris-scaling assumption
  2. Run system with k=3, k=9, and k=15 calibration points on new user; plot error reduction curve to verify marginal gain of 9 samples
  3. Run inference loop on low-end mobile device in Chrome; monitor FPS and RAM usage to identify if BlazeGaze causes memory leaks or thermal throttling

## Open Questions the Paper Calls Out
- How does gaze estimation accuracy vary across demographic groups, and can fairness-aware training mitigate systematic performance disparities?
- What is the energy consumption profile during continuous real-time inference on battery-powered mobile devices, and can optimizations reduce power draw?
- How does the fixed iris diameter assumption systematically affect metric head pose estimation accuracy across individuals with varying anatomical proportions?
- How does temporal drift evolve beyond 20-minute sessions tested, and what recalibration strategies are needed for extended-duration use cases?

## Limitations
- Scale estimation relies on fixed iris diameter assumption (1.2 cm) that may not hold across diverse populations
- Calibration data sensitivity not adequately addressed for users with poor motor control or suboptimal conditions
- Browser environment constraints create uncertainty about true cross-platform scalability
- Dataset generalization limited by evaluation primarily on GazeCapture and MPIIFaceGaze

## Confidence
- **High Confidence:** Core architectural claims and error metrics are well-specified and technically sound
- **Medium Confidence:** Real-time performance claims depend on unverified browser-specific optimizations and device-specific hardware capabilities
- **Low Confidence:** Biological scaling assumption lacks empirical validation across diverse populations; temporal drift robustness not extensively validated beyond WebGazer comparison

## Next Checks
1. Test framework across range of devices (iPhone 12, Samsung Galaxy A53, older iPad) and browsers (Chrome, Safari, Firefox) to verify 2.4ms inference claim and identify performance cliffs
2. Collect ground truth iris measurements from 50 diverse participants and compare against framework's inferred scale estimates to quantify error distribution and demographic correlations
3. Design study where participants perform calibration under varying conditions (controlled environment, moving vehicle, with/without glasses) to measure relationship between calibration precision, sample count, and final accuracy