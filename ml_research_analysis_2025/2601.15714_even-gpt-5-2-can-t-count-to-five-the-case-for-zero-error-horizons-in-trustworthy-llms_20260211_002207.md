---
ver: rpa2
title: 'Even GPT-5.2 Can''t Count to Five: The Case for Zero-Error Horizons in Trustworthy
  LLMs'
arxiv_id: '2601.15714'
source_url: https://arxiv.org/abs/2601.15714
tags:
- qwen2
- b-instruct
- arxiv
- computational
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Zero-Error Horizon (ZEH) as a metric for
  evaluating the reliability of large language models (LLMs) in safety-critical applications.
  ZEH measures the maximum problem size a model can solve without any errors, providing
  concrete failure examples (ZEH limiters) that reveal surprising limitations even
  in advanced models like GPT-5.2.
---

# Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs

## Quick Facts
- arXiv ID: 2601.15714
- Source URL: https://arxiv.org/abs/2601.15714
- Authors: Ryoma Sato
- Reference count: 40
- Primary result: Introduces Zero-Error Horizon (ZEH) metric showing even GPT-5.2 fails on basic tasks like binary parity of 11000

## Executive Summary
This paper introduces Zero-Error Horizon (ZEH) as a rigorous metric for evaluating LLM reliability in safety-critical applications. ZEH measures the maximum problem size a model can solve without any errors, providing concrete failure examples that reveal surprising limitations even in advanced models like GPT-5.2. The authors demonstrate that ZEH effectively captures the transition from memorization to algorithmic reasoning as model size increases, and propose four optimization techniques achieving up to 10x speedup in ZEH computation through trie-based prefix sharing and online softmax.

## Method Summary
The method involves exhaustive evaluation of all problem instances at each size level, starting from size 1 and continuing until the first error occurs. For multiplication tasks, instances are generated as all pairs (a,b) where max(a,b) ≤ n. The paper evaluates Qwen2.5-Instruct models (0.5B to 72B) using greedy decoding with temperature=0. Four speedup techniques are implemented: teacher forcing for parallel verification, batching across sizes, prompt KV cache sharing, and FlashTree trie-based attention with online softmax. ZEH limiters (first failure examples) are collected as concrete evidence of model limitations.

## Key Results
- GPT-5.2 cannot compute parity of binary string 11000 or determine if (((())))) is balanced
- Qwen2.5 models show systematic transition from memorization (high corpus correlation) to algorithmic reasoning (structured errors) as size increases
- Four optimization techniques achieve 6-11x speedup in ZEH computation
- Structured error rates increase from 58% to 90% as models grow from 0.5B to 72B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZEH detects capability boundaries by exhaustive search, revealing failure modes that sampling-based metrics miss.
- Mechanism: ZEH iterates through problem sizes n=1,2,3... and tests ALL instances at each size. When any error occurs at size n+1, ZEH=n is returned. This exhaustive approach guarantees no "holes" exist within the certified range, unlike accuracy which can miss rare failures in sampled evaluations.
- Core assumption: The task has a well-defined problem size metric (e.g., string length, max operand) where difficulty scales monotonically.
- Evidence anchors:
  - [abstract] "ZEH measures the maximum range that a model can solve without any errors"
  - [Section 2.2.1] "ZEH ensures no oversights through exhaustive verification"
  - [corpus] "Can ChatGPT Learn to Count Letters?" shows LLMs struggle on simple counting—consistent with the parity/parentheses failures here
- Break condition: If problem size doesn't correlate with difficulty, or if the task space at each size is too large to enumerate exhaustively, ZEH becomes computationally infeasible.

### Mechanism 2
- Claim: The transition from memorization to algorithmic reasoning manifests as structured error patterns detectable via ZEH analysis.
- Mechanism: Smaller models show high correlation between training corpus frequency and accuracy (Table 4, ρ=0.27 for 0.5B)—suggesting reliance on memorization. Larger models show lower correlation (ρ=0.04 for 72B) but higher "structured error" rates (Table 5, 90% for 72B vs 58% for 0.5B), where errors are multiples of 10—consistent with carry mistakes in learned algorithms.
- Core assumption: Training data frequency approximates memorization exposure; structured errors indicate algorithm execution rather than recall.
- Evidence anchors:
  - [Section 3.2] "Small models have high correlation [with corpus frequency], indicating that they memorize training data"
  - [Table 5-6] Structured error rate increases from 58% to 90%; carry presence negatively interacts with model size in logistic regression
  - [corpus] Related work (Nikankin et al.) investigated heuristics in arithmetic—paper notes this extends that scaling analysis
- Break condition: If structured errors arise from other causes (tokenization artifacts, systematic biases), the memorization→algorithm narrative weakens.

### Mechanism 3
- Claim: Tree-structured KV cache sharing with online softmax achieves ~10x speedup by exploiting prefix overlaps across problem instances.
- Mechanism: FlashTree organizes all evaluation strings into a trie. Shared prefixes (e.g., "1×" in "1×1=", "1×2=") share KV cache computation. A Triton kernel computes attention without materializing the full attention mask, using online softmax to maintain numerical stability incrementally.
- Core assumption: The task has significant prefix overlap across instances (multiplication with shared operands, prompts with shared instructions).
- Evidence anchors:
  - [Section 4.4] "By managing the strings to be evaluated with a trie, we can share the KV cache for shared prefixes"
  - [Table 8] FlashTree achieves 6-11x speedup over naive autoregression across Qwen models
  - [corpus] Weak/no direct corpus evidence for FlashTree specifically; draws on Tree Attention from speculative decoding literature (cites Miao et al., SpecInfer)
- Break condition: If prefix overlap is minimal or attention patterns require dense computation, trie-based speedup degrades toward standard teacher forcing.

## Foundational Learning

- Concept: **Exhaustive vs. Sampled Evaluation**
  - Why needed here: ZEH requires testing every instance up to the boundary. Understanding why sampling misses "holes" is essential to appreciating ZEH's value for safety-critical domains.
  - Quick check question: If a model has 99% accuracy on n≤100 multiplication, why might its ZEH still be <10?

- Concept: **Memorization vs. Algorithmic Reasoning**
  - Why needed here: The paper claims ZEH growth signals algorithmic capability acquisition. You need to understand what evidence supports this transition (correlation decay, error structuring, carry sensitivity).
  - Quick check question: What three statistical analyses does the paper use to argue for memorization→algorithm transition?

- Concept: **Teacher Forcing and KV Cache Sharing**
  - Why needed here: The speedup techniques (teacher forcing verification, prefix sharing, trie structures) require understanding how autoregressive inference differs from parallel verification and how attention masking enables prefix sharing.
  - Quick check question: Why might teacher forcing misjudge correctness, and how does the paper's fallback procedure address this?

## Architecture Onboarding

- Component map:
  ```
  ZEH Evaluation Pipeline:
  ├── Problem Generator (creates B_n instances for each size n)
  ├── Verifier (checks model correctness)
  │   ├── Naive: Autoregressive decoding per instance
  │   ├── TF: Teacher forcing with causal mask (single forward pass)
  │   └── FlashTree: Trie-organized batch with shared KV cache
  ├── Boundary Detector (returns max n where all T_n ⊆ C)
  └── Limiter Collector (stores first-failure examples as evidence)
  ```

- Critical path:
  1. Define problem size metric (e.g., max(a,b) for multiplication)
  2. Generate all instances at each size level
  3. Verify all instances using FlashTree (preferred) or TF with fallback
  4. Return ZEH when first error encountered; record ZEH limiter

- Design tradeoffs:
  - **Teacher forcing speed vs. accuracy**: TF is faster but may misjudge alternative tokenizations (e.g., "1,204" vs "1204"). Paper uses fallback to autoregressive for ambiguous cases.
  - **Look-ahead batching vs. early termination**: Batching across sizes improves GPU utilization but wastes compute if ZEH is very small (see 0.5B model in Table 8).
  - **Exhaustive verification cost vs. guarantee strength**: ZEH provides hard guarantees but scales as O(|T_n|); accuracy samples scale as O(k) for k samples but provides only probabilistic bounds.

- Failure signatures:
  - ZEH=0 on simple tasks indicates fundamental capability gap (e.g., 0.5B model answering "2" for 1×1)
  - ZEH limiter with unstructured error (e.g., random digit) suggests memorization
  - ZEH limiter with structured error (e.g., off by 20, correct ones digit) suggests algorithmic execution failure

- First 3 experiments:
  1. **Baseline ZEH measurement**: Run naive Algorithm 1 on a small model (e.g., Qwen2.5-0.5B) for multiplication n≤10. Verify you can reproduce the ZEH=0 result with limiter 1×1→"2".
  2. **Speedup validation**: Implement teacher forcing verification with fallback. Compare runtime against autoregressive decoding on the 99×99 multiplication suite. Target: ~2x speedup (Table 7).
  3. **Memorization correlation analysis**: For a model family (e.g., Qwen2.5), compute Spearman's ρ between C4 corpus frequency and correctness. Confirm the paper's finding that correlation decreases with model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mathematical or formal methods fundamentally solve the computational explosion problem in ZEH evaluation as model performance improves?
- Basis in paper: [explicit] The authors state: "However, these techniques are not a panacea, and when model performance improves, the explosion in the number of instances |Tn| that must be evaluated is unavoidable. Fundamentally solving this problem will likely require mathematical and formal methods. This direction is left for future work."
- Why unresolved: Current speedup techniques (teacher forcing, batching, FlashTree) achieve up to 10x speedup but cannot fundamentally address the combinatorial growth in instances as ZEH increases.
- What evidence would resolve it: Development of formal verification methods or mathematical proofs that can establish ZEH bounds without exhaustive enumeration.

### Open Question 2
- Question: How should ZEH be adapted for systems that integrate external tools, given that tool-augmented models may have effectively infinite ZEH on some tasks?
- Basis in paper: [explicit] "In safety-critical domains, errors can occur in the judgment of whether to call or in integrating the results of the call... systems without tool calls are simpler and easier to maintain. In short, it is preferable to solve problems without tool calls when possible."
- Why unresolved: The paper evaluates LLMs without tool calls but acknowledges that real-world systems often use tools, creating a gap between evaluated and deployed configurations.
- What evidence would resolve it: A systematic study of ZEH measurement methodologies for tool-augmented agents, including error modes in tool invocation and result integration.

### Open Question 3
- Question: Does ZEH on simple algorithmic tasks (parity, balanced parentheses, multiplication) reliably predict failure modes in complex downstream tasks that depend on these capabilities?
- Basis in paper: [inferred] The paper claims "such tasks can serve as building blocks for complex problems" and ZEH provides "warning signals," but does not empirically validate this transfer relationship.
- Why unresolved: The paper demonstrates ZEH on isolated tasks but does not show whether ZEH limiters on simple tasks correlate with errors on compositional tasks where these operations appear as subroutines.
- What evidence would resolve it: Experiments showing that models with low ZEH on multiplication make cascading errors on mathematical reasoning tasks at points where multiplication subproblems exceed ZEH.

## Limitations
- Computational cost scales exponentially with problem size, making exhaustive evaluation impractical for large problem spaces
- Interpretation of structured errors as algorithmic reasoning is correlational rather than definitively causal
- Paper doesn't address whether ZEH captures the full capability boundary given potential jagged boundaries
- GPT-5.2 failures might represent tokenization artifacts rather than fundamental limitations

## Confidence

- **High Confidence**: The computational feasibility of ZEH as a metric (exhaustive vs. sampled evaluation tradeoff), the speedup achieved by teacher forcing with fallback verification, and the basic correlation between model size and ZEH values across the Qwen2.5 family.

- **Medium Confidence**: The interpretation of structured error patterns as evidence of algorithmic reasoning over memorization, the claim that ZEH provides meaningful safety guarantees for safety-critical applications, and the effectiveness of prefix-sharing optimizations across different task types.

- **Low Confidence**: The generalizability of ZEH to complex reasoning tasks beyond arithmetic and string manipulation, the claim that GPT-5.2's failures represent fundamental limitations rather than prompt sensitivity, and the assertion that ZEH reveals "surprising" limitations without comparison to established benchmarks.

## Next Checks

1. **Cross-task generalization**: Apply ZEH evaluation to a reasoning task with different structural properties (e.g., logical inference or code generation) to test whether structured error patterns persist outside arithmetic domains.

2. **Prompt sensitivity analysis**: Systematically vary prompts for GPT-5.2's "11000" parity failure to determine whether the limitation is fundamental or artifactual, testing alternative tokenizations and instruction formats.

3. **Benchmark comparison**: Compare ZEH results against established capability metrics (BIG-Bench, MMLU) for the same model families to assess whether ZEH captures complementary or redundant information about model limitations.