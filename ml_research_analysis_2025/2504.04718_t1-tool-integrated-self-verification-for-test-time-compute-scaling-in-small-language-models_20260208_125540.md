---
ver: rpa2
title: 'T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small
  Language Models'
arxiv_id: '2504.04718'
source_url: https://arxiv.org/abs/2504.04718
tags:
- toolv
- verification
- scaling
- zhang
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether small language models (sLMs) can
  reliably perform self-verification under test-time scaling. While test-time scaling
  has proven effective for improving sLM reasoning, prior work relied on large verifiers,
  leaving self-verification largely unexplored.
---

# T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models

## Quick Facts
- arXiv ID: 2504.04718
- Source URL: https://arxiv.org/abs/2504.04718
- Reference count: 40
- One-line primary result: Llama-3.2-1B with T1 outperforms Llama-3.1-8B on MATH benchmarks through tool-integrated self-verification

## Executive Summary
This paper investigates whether small language models (sLMs) can reliably perform self-verification under test-time scaling. While test-time scaling has proven effective for improving sLM reasoning, prior work relied on large verifiers, leaving self-verification largely unexplored. The authors identify that sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking, even after distillation from larger verifiers.

To address this, they propose Tool-integrated Self-verification (T1), which delegates memorization-heavy verification steps to external tools like code interpreters or retrievers. Theoretical analysis shows that using tools reduces memorization demands and improves test-time scaling performance. Experiments demonstrate that with T1, a Llama-3.2 1B model outperforms the significantly larger Llama-3.1 8B model on MATH benchmarks, and T1 generalizes effectively to both mathematical and multi-domain knowledge-intensive tasks. The findings highlight that tool integration substantially enhances sLM self-verification abilities under test-time scaling.

## Method Summary
The authors propose T1, a two-stage verification framework for small language models that delegates memorization-heavy tasks to external tools. First, a base sLM generates multiple candidate solutions. The ToolV adapter then filters these candidates by generating code (for math) or queries (for facts) that are executed by external tools. Surviving candidates are scored by an RM adapter trained via knowledge distillation from a larger teacher model. The system uses multi-LoRA adapters for specialized generation and verification modes, allowing the model to switch between roles without permanently altering base capabilities.

## Key Results
- Llama-3.2-1B with T1 outperforms Llama-3.1-8B on MATH benchmarks
- ToolV improves Best-of-N scaling slope compared to baseline verification
- T1 generalizes to both mathematical and multi-domain knowledge-intensive tasks
- Tool integration reduces memorization demands and improves test-time scaling performance

## Why This Works (Mechanism)

### Mechanism 1: Memorization Offloading via Externalization
- **Claim:** Replacing internal parametric verification with external tool execution reduces the memorization capacity required by small models.
- **Mechanism:** Verification tasks often require exact recall (e.g., arithmetic facts, specific knowledge triples), which scale with parameter count. By delegating these to a code interpreter or retriever, the model's role shifts from "knowing the answer" to "knowing how to check," converting a memory retrieval problem into a procedural generation problem.
- **Core assumption:** The sLM has sufficient capacity to learn the syntax and logic for tool invocation (e.g., writing Python code) even if it lacks the capacity to store the knowledge itself.
- **Evidence anchors:**
  - [abstract] "Theoretical analysis shows that tool integration reduces memorization demands..."
  - [section 5.1] Theorem 5.2 proves $I(X; \theta | P) = 0$ when tools are used, compared to $\Omega(M^3)$ without.
  - [corpus] Related work "ReVeal" supports the need for reliable signals in verification, though "T1" specifically focuses on the offloading aspect.
- **Break condition:** If the verification logic is too complex for the sLM to express as a tool query (e.g., subtle semantic nuance that cannot be coded or retrieved), the mechanism fails.

### Mechanism 2: Two-Stage Filtering and Scoring
- **Claim:** Decoupling verification into a binary tool-based filter (ToolV) and a semantic reward model (RM) improves Best-of-N scaling by aggressively pruning hard errors before semantic ranking.
- **Mechanism:** The ToolV stage provides high precision on objective errors (calculation/factual), filtering candidates to a smaller set. The RM stage then ranks this cleaner set based on logical coherence. This prevents the RM from being "distracted" by candidates that look logically sound but contain numerical hallucinations.
- **Core assumption:** The ToolV stage has high precision (few false positives), even if recall is imperfect.
- **Evidence anchors:**
  - [abstract] "...T1, which delegates memorization-heavy verification steps to external tools..."
  - [section 4.1] Eq. (3) formalizes the multiplication of the binary tool function $f(x, y)$ and the verifier score $r(x, y)$.
  - [corpus] Corpus papers like "Sample, Scrutinize and Scale" corroborate the general scaling trends, but T1's specific two-stage decoupling is the distinct mechanism here.
- **Break condition:** If ToolV generates false negatives (rejecting correct solutions due to bad code generation), the final accuracy caps at the recall of the filter.

### Mechanism 3: Multi-LoRA Distillation for Specialization
- **Claim:** Distilling verification capabilities from a larger teacher into specialized Low-Rank Adapters (LoRA) allows a single sLM to switch between generation and verification modes.
- **Mechanism:** Instead of fine-tuning the entire model, separate adapters ($\Delta\theta_{tool}$, $\Delta\theta_{reward}$) are trained on data synthesized by a large teacher (GPT-4o). This allows the sLM to temporarily "borrow" reasoning patterns for verification without permanently altering its base generation capabilities.
- **Core assumption:** The student model can compress the teacher's verification logic into low-rank matrices.
- **Evidence anchors:**
  - [section 4.2] "We employ knowledge distillation... adopting a multi-LoRA approach... for each verifier."
  - [figure 4] Shows "Distilled GenRM" outperforming "Zero-shot GenRM," validating the distillation mechanism.
  - [corpus] Evidence is weak regarding multi-LoRA specifically in the provided corpus summaries; this appears to be a specific implementation detail of T1.
- **Break condition:** If the distillation dataset is noisy or the teacher model's reasoning is too complex, the adapters may overfit or fail to generalize.

## Foundational Learning

- **Concept: Best-of-N Sampling (Parallel Scaling)**
  - **Why needed here:** This is the base upon which T1 operates. You must understand that scaling works by sampling $N$ candidates and selecting the best one using a verifier.
  - **Quick check question:** If I sample 64 solutions but my verifier is random, does scaling work? (Answer: No).

- **Concept: Process Reward Models (PRM) vs. Generative Reward Models (GenRM)**
  - **Why needed here:** T1 integrates with both. You need to distinguish between a model that outputs a scalar score per step (PRM) and a model that generates a "Yes/No" token with reasoning (GenRM).
  - **Quick check question:** Which verifier type provides interpretable reasoning traces? (Answer: GenRM).

- **Concept: Program-Aided Language Models (PAL)**
  - **Why needed here:** T1's ToolV is essentially a PAL approach applied to verification. You need to understand that the model generates code, and an external interpreter executes it.
  - **Quick check question:** Does the model perform the arithmetic, or does the Python interpreter? (Answer: The interpreter).

## Architecture Onboarding

- **Component map:** Generator ($\pi$) -> ToolV adapter -> Code Interpreter/Retriever -> RM adapter -> Solution selection

- **Critical path:**
  1. **Generation:** Sample $N$ solutions (Chain-of-Thought) from the base model.
  2. **Tool-Filtering:** For each solution, prompt the ToolV adapter. Execute code. If code asserts `False`, discard the solution.
  3. **RM-Scoring:** Score the surviving solutions using the RM adapter.
  4. **Selection:** Return the solution with the highest RM score.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Executing code for every candidate adds significant latency compared to a single forward pass for scoring.
  - **Recall vs. Precision:** Strict tool filtering improves precision (removes bad answers) but risks recall (discarding good answers if the generated verification code is buggy).

- **Failure signatures:**
  - **Syntax Errors in ToolV:** The sLM generates invalid Python, causing the tool to crash or return nothing.
  - **False Negatives:** The solution is correct, but the verification code has a logic bug (e.g., comparing floating point strings) and returns `False`.
  - **Empty Candidate Set:** If ToolV is too aggressive, all $N$ candidates might be filtered out, requiring a fallback strategy.

- **First 3 experiments:**
  1. **Baseline Scaling:** Implement Best-of-N using only the distilled RM adapter (no tools) to establish a baseline on MATH500.
  2. **ToolV Integration:** Add the ToolV stage. Compare the "Best-of-N" curve against the baseline to verify that the slope improves (better scaling).
  3. **Ablation on N:** Run the full T1 pipeline with $N=16$ vs $N=64$ to determine the compute-efficient operating point for your specific sLM size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tool-based reasoning be integrated into the verification step to recover correct solutions that are mistakenly rejected (false negatives) by the current filter-based approach?
- Basis in paper: [explicit] The authors state in the Limitations section that "ToolV acts only as a rejection filter and cannot recover from false negatives," suggesting integration could mitigate this.
- Why unresolved: The current T1 implementation uses a binary function $f(x, y) \in \{0, 1\}$, meaning once a solution is filtered out, it is lost regardless of its actual correctness.
- What evidence would resolve it: A modified T1 framework where tool outputs (e.g., code execution results) provide corrective feedback or lower confidence scores rather than hard rejection, demonstrating improved recall rates on mathematical benchmarks.

### Open Question 2
- Question: Does Tool-integrated Self-verification (T1) effectively enhance performance in sequential test-time scaling strategies, such as step-level search or long reasoning chains?
- Basis in paper: [explicit] The authors note their work "focuses on best-of-N (parallel) test-time scaling," but "tools can also benefit other test-time scaling strategies... Exploring these directions presents a promising avenue for future work."
- Why unresolved: The study restricts its evaluation to parallel scaling (Best-of-N), leaving the interaction between tool use and sequential information accumulation untested.
- What evidence would resolve it: Experimental results applying T1 to sequential methods (e.g., Tree of Thoughts or beam search) showing consistent performance improvements similar to those seen in the parallel Best-of-N setting.

### Open Question 3
- Question: Why does tool-based verification fail to improve performance in geometric reasoning domains?
- Basis in paper: [inferred] The analysis of Figure 7 notes that while ToolV improves Algebra and Number Theory, "Geometry sees a drop, likely due to ToolV being less effective in that domain."
- Why unresolved: The paper identifies the performance degradation in geometry but does not investigate the specific failure modes of code interpreters or retrieval tools in handling spatial or geometric logic.
- What evidence would resolve it: An error analysis of ToolV on geometry tasks revealing whether failures stem from translation errors (natural language to code) or the limitations of standard libraries (e.g., SymPy) in handling geometric constraints.

## Limitations
- The theoretical advantage assumes perfectly reliable tool execution, which may not hold in practice
- The evaluation is heavily focused on mathematical reasoning tasks, leaving generalization to other domains untested
- The distillation process from GPT-4o-mini to specialized LoRA adapters has not been independently verified

## Confidence
- **High Confidence:** The core mechanism of offloading verification to external tools is technically sound and theoretically justified
- **Medium Confidence:** The experimental results showing Llama-3.2-1B outperforming Llama-3.1-8B are compelling but require independent replication
- **Low Confidence:** The claim that this approach "substantially enhances" sLM self-verification abilities across all test-time scaling scenarios is overstated

## Next Checks
1. **Tool Reliability Stress Test:** Implement a version where the code interpreter randomly introduces controlled errors (5-15% failure rate) and measure the degradation in T1 performance
2. **Domain Generalization Study:** Apply T1 to non-mathematical reasoning tasks such as commonsense reasoning (HellaSwag) or logical deduction (LogiQA) to test whether tool integration provides similar benefits outside the mathematical domain
3. **Teacher Distillation Ablation:** Train T1 using multiple teacher models (GPT-4, Claude, open-source alternatives) and compare performance to isolate whether improvements come from the tool integration mechanism itself versus specific qualities of the GPT-4o-mini teacher