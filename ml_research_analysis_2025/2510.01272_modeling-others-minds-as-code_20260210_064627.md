---
ver: rpa2
title: Modeling Others' Minds as Code
arxiv_id: '2510.01272'
source_url: https://arxiv.org/abs/2510.01272
tags:
- agent
- rote
- behavior
- human
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROTE is a novel algorithm that models human behavior as executable
  programs rather than through complex mental state inference. It uses LLMs to generate
  candidate behavioral programs from observed trajectories, then applies Bayesian
  inference to select the most likely ones.
---

# Modeling Others' Minds as Code

## Quick Facts
- arXiv ID: 2510.01272
- Source URL: https://arxiv.org/abs/2510.01272
- Reference count: 40
- Models human behavior as executable programs using LLMs and Bayesian inference, achieving up to 50% improvement in action prediction accuracy.

## Executive Summary
ROTE introduces a novel approach to modeling human behavior by representing agents as executable programs rather than inferring complex mental states. The method uses LLMs to generate candidate behavioral programs from observed trajectories, then applies Bayesian inference to select the most likely ones. This programmatic representation enables efficient multi-step predictions and provides interpretable decision-making processes, outperforming traditional behavior cloning and inverse planning baselines across gridworld and embodied robotics environments.

## Method Summary
The method generates candidate Python programs implementing finite state machines from observed trajectories using LLMs, then applies Bayesian inference with Sequential Monte Carlo to weight and update these hypotheses. Programs are executed against observation histories to compute likelihoods, with posterior probabilities maintained through resampling and rejuvenation steps. The top-k programs are used to predict actions on current observations, enabling efficient multi-step forecasting without repeated LLM calls.

## Key Results
- ROTE outperforms behavior cloning and inverse planning baselines by up to 50% in action prediction accuracy
- Achieves human-level accuracy in predicting human behavior in gridworld environments
- Demonstrates superior zero-shot generalization to novel environments with orders-of-magnitude efficiency gains in multi-step predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral programs provide a compact, executable representation that captures decision-making logic without requiring explicit goal/belief inference.
- Mechanism: LLMs generate candidate Python programs implementing FSM-like agents. Each program λ maps observations to actions through explicit conditional logic and state transitions. Programs are weighted by posterior probability p(λ|h) ∝ p(h|λ)·p(λ), where the prior favors shorter programs (Solomonoff induction principle).
- Core assumption: Observed agents follow deterministic or near-deterministic "scripts" with finite internal states, even if ground truth behavior is generated by more complex processes.
- Evidence anchors: [abstract] "We propose modeling these routines as behavioral programs instantiated in computer code rather than policies conditioned on beliefs and desires." [Section 3.1] "Encouraging concise program synthesis is not just a matter of engineering preference but is theoretically grounded in the foundations of algorithmic probability and inductive inference."
- Break condition: If agents exhibit highly stochastic, context-dependent, or belief-driven planning that cannot be compressed into finite-state decision rules, program hypotheses will underfit and prediction accuracy will degrade.

### Mechanism 2
- Claim: Bayesian inference over program hypotheses enables robust uncertainty handling and online refinement as more observations accumulate.
- Mechanism: Sequential Monte Carlo maintains a distribution over candidate programs. At each timestep, likelihood p(h|λ) is computed by executing each program against observed history. Low-probability programs are replaced via rejuvenation (resampling from LLM). Final prediction aggregates top-k programs weighted by posterior.
- Core assumption: The true behavioral program exists within the LLM's generative capability, and sufficient hypothesis diversity can be achieved with reasonable sample counts.
- Evidence anchors: [abstract] "probabilistic inference for reasoning about uncertainty over that space" [Section 3.2] "we estimate the posterior probability of a candidate agent program λ given the observed history using the relationship: p(λ|h₀:t₋₁) ∝ p(h₀:t₋₁|λ)p(λ)"
- Break condition: If the LLM fails to generate the correct program within the hypothesis budget, or if rejuvenation cannot escape local optima, posterior mass will concentrate on incorrect programs.

### Mechanism 3
- Claim: Code-based representations enable efficient multi-step prediction and zero-shot generalization by decoupling inference from execution.
- Mechanism: Once programs are inferred, future predictions require only program execution (milliseconds) rather than repeated LLM calls. Programs transfer to novel environments because they encode decision logic abstracted from specific state configurations.
- Core assumption: Behavioral scripts are environment-agnostic—"patrol counterclockwise" logic applies regardless of wall positions.
- Evidence anchors: [abstract] "Code-based representations enable efficient multi-step predictions and provide interpretable decision-making processes." [Section 5, Figure 6] "ROTE's test-time compute costs scale orders of magnitude more efficiently with the number of predictions"
- Break condition: If behavioral scripts are tightly coupled to specific environmental features (e.g., "go to the kitchen" requires re-mapping in new layouts), transferred programs will mispredict.

## Foundational Learning

- Concept: **Bayesian Inference / Sequential Monte Carlo**
  - Why needed here: Core algorithm for weighting and updating program hypotheses as observations arrive. Without this, you'd have no principled way to combine uncertain program predictions.
  - Quick check question: Can you explain why p(λ|h) ∝ p(h|λ)·p(λ) and how particle filtering maintains diversity?

- Concept: **Finite State Machines and Program Synthesis**
  - Why needed here: ROTE represents agents as FSMs synthesized via LLMs. Understanding state transitions, determinism, and program structure is essential for debugging generated code.
  - Quick check question: Given a sequence of (observation, action) pairs, can you sketch an FSM that might generate them?

- Concept: **Solomonoff Induction / Occam's Razor**
  - Why needed here: Provides theoretical justification for preferring shorter programs. The paper explicitly grounds this in algorithmic information theory.
  - Quick check question: Why should shorter programs receive higher prior probability, and when might this assumption fail?

## Architecture Onboarding

- Component map:
  - LLM Program Generator -> Observation Parser (optional) -> Likelihood Evaluator -> Bayesian Inference Engine -> Action Predictor

- Critical path:
  1. Observation history h₀:t₋₁ + current observation oₜ → LLM prompt
  2. LLM generates N program candidates (with retry on compilation errors)
  3. Compute likelihood for each program against h₀:t₋₁
  4. Update posterior, rejuvenate low-probability programs
  5. Execute top-k programs on oₜ, aggregate predictions

- Design tradeoffs:
  - **Hypothesis count (N)**: More hypotheses → better coverage but slower inference. Paper uses N=30.
  - **Structure constraints (Light/Moderate/Severe)**: Stricter FSM structure improves efficiency on scripted agents but degrades on goal-directed humans. Figure 13 shows environment-dependent optima.
  - **Two-stage parsing**: Helps in Construction (abstracts grid coordinates) but hurts in Partnr (removes rich scene graph details). Figure 11.
  - **Top-k vs. single best**: Paper finds minimal accuracy difference (Figure 15), so k can be tuned for speed vs. uncertainty quantification.

- Failure signatures:
  - **Compilation errors**: Generated programs fail to run. Mitigation: retry with error trace feedback (paper found full resampling more effective than repair).
  - **Hallucinated states/actions**: "Severe" structure condition can introduce non-existent states (e.g., "CHARGE" state in Listing 6). Mitigation: validate against action space.
  - **Posterior collapse**: All probability mass on wrong program. Check per-timestep accuracy curves; if early timesteps are wrong, initial hypotheses may be insufficient.
  - **Overfitting to trajectory quirks**: Programs exploit spurious patterns. Paper found low correlation between accuracy and state/action repetition (0.303, 0.064), suggesting this is mitigated.

- First 3 experiments:
  1. **Reproduce Construction baseline**: Implement ROTE on the 10 FSM agents. Verify ~50% accuracy improvement over BC and that multi-step prediction time scales sublinearly (Figure 6). This validates core mechanism on controlled data.
  2. **Ablate structure constraints**: Compare Light/Moderate/Severe prompting on human gameplay data. Confirm that Moderate outperforms on human behavior while Severe works on scripted agents (Figure 13). This tests the "script vs. goal-directed" gradient hypothesis.
  3. **Test transfer to novel Construction environments**: After inferring programs in one grid layout, evaluate zero-shot prediction in a different layout without updating program weights. Target >baseline accuracy (Figure 4). This validates generalization claims.

## Open Questions the Paper Calls Out

- Can ROTE be extended to high-dimensional, continuous control settings (e.g., raw video feeds for assistive robots) by integrating vision-language models?
  - Basis in paper: [explicit] "future research should explore ROTE in high-dimensional, continuous control settings. In those cases, ROTE might need to be integrated with vision-language models (VLMs) to parse pixel-based inputs"
  - Why unresolved: Current evaluation was limited to text-based observations and discrete action spaces (6 actions in Construction, 19 tools in Partnr).
  - What evidence would resolve it: Demonstration of ROTE operating on raw pixel/video inputs in continuous control domains with comparable accuracy to discrete settings.

- Does the structure of ROTE's inferred programs align with how humans intuitively reason about others' mental states?
  - Basis in paper: [explicit] "further research is needed to empirically validate whether the structure of ROTE's inferred programs aligns with how humans intuitively reason about others' mental states"
  - Why unresolved: ROTE focuses on action prediction rather than explicit belief/goal inference; the cognitive plausibility of its program representations remains untested.
  - What evidence would resolve it: Cognitive studies comparing ROTE's inferred program structures against human explanations of observed behavior.

- How does LLM scale impact ROTE's prediction quality in sophisticated multi-agent scenarios?
  - Basis in paper: [explicit] "explore how the size of LLMs used for behavioral program inference impacts prediction quality in more sophisticated scenarios, such as modeling team coordination in workplaces or norm enforcement on social platforms"
  - Why unresolved: All experiments used 8B-16B parameter models on relatively simple domains; scaling effects on complex social reasoning are unknown.
  - What evidence would resolve it: Systematic evaluation across LLM scales on multi-agent coordination and norm enforcement tasks.

## Limitations

- The method's performance is highly sensitive to the choice of structure constraints, with optimal settings varying dramatically between scripted agents and human behavior.
- Success depends fundamentally on the LLM's ability to generate correct program hypotheses, with no systematic analysis of failure modes or limits of behavioral complexity.
- Claims about superior interpretability lack quantitative validation—while programs are readable, there's no evidence they actually reveal decision-making logic rather than just reproducing behavior.

## Confidence

- **High confidence**: The core Bayesian inference mechanism over program hypotheses is well-grounded in algorithmic probability theory. The multi-step prediction efficiency gains and zero-shot generalization results on Construction agents are clearly demonstrated.
- **Medium confidence**: The comparison to human-level accuracy on human gameplay data is compelling but relies on a single human study. The Partnr results show lower absolute performance (~40-50% accuracy) compared to Construction (~70%+), suggesting the method may struggle with richer, more complex observation spaces.
- **Low confidence**: Claims about superior interpretability lack quantitative validation. While programs are readable, the paper doesn't measure whether they actually reveal decision-making logic or merely reproduce behavior without explanatory value.

## Next Checks

1. **Stress test structure constraints**: Systematically vary constraint levels across a gradient of agent types (from purely scripted to fully goal-directed) and measure accuracy breakdown points. This would identify the method's limits for behavioral complexity.

2. **LLM failure analysis**: Design experiments where known difficult observation histories are fed to the LLM program generator. Measure compilation failure rates, hallucinated state/action introduction, and posterior collapse frequency across different agent types and environments.

3. **Interpretability validation**: Conduct human studies where participants attempt to predict agent behavior from the generated programs versus raw trajectories. Measure whether programs actually improve understanding of decision-making logic, not just prediction accuracy.