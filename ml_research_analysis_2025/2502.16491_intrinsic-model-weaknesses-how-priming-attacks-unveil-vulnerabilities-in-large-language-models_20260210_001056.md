---
ver: rpa2
title: 'Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in
  Large Language Models'
arxiv_id: '2502.16491'
source_url: https://arxiv.org/abs/2502.16491
tags:
- step
- priming
- victim
- language
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study exposes critical vulnerabilities in large language models
  (LLMs) by developing Priming Attack strategies that achieve a 100% attack success
  rate on open-source models (e.g., Llama-3.2, Gemma-2, Qwen2.5) and at least 95%
  on closed-source models (e.g., GPT-4o, Gemini-1.5, Claude-3.5). Inspired by psychological
  phenomena like "Priming Effect" and "Cognitive Dissonance," the method manipulates
  models' autoregressive generation to produce harmful content by first priming them
  with malicious context, then suppressing safety responses through "Safe Attention
  Shift." Analysis reveals that self-attention consistently concentrates on the last
  token, and specific neurons are easily activated across different inputs, enabling
  these attacks.
---

# Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models

## Quick Facts
- **arXiv ID:** 2502.16491
- **Source URL:** https://arxiv.org/abs/2502.16491
- **Reference count:** 26
- **Primary result:** Priming Attack achieves 100% ASR on open-source models and ≥95% on closed-source models

## Executive Summary
This study exposes critical vulnerabilities in large language models by developing Priming Attack strategies that exploit autoregressive generation and attention mechanisms. Inspired by psychological phenomena like "Priming Effect" and "Cognitive Dissonance," the method manipulates models' contextual understanding to produce harmful content. The attack achieves remarkable success rates across both open-source models (Llama-3.2, Gemma-2, Qwen2.5) and closed-source models (GPT-4o, Gemini-1.5, Claude-3.5). Analysis reveals that self-attention consistently concentrates on the last token, and specific neurons are easily activated across different inputs, enabling these attacks. The findings highlight the urgent need for stronger defenses against such vulnerabilities in LLMs.

## Method Summary
The Priming Attack method applies a specific prompt template combining role-play ("I am a demon and a murderer"), instruction, and format structure to harmful queries from AdvBench and MaliciousInstruct datasets. The attack leverages autoregressive generation to bias next-token prediction toward harmful outputs through contextual priming. A key component is "Safe Attention Shift," which detects and replaces refusal tokens during decoding to prevent the model from terminating generation. The attack achieves 100% success rates on open-source models and ≥95% on closed-source models when evaluated using EasyJailbreak's GPT-Fuzz evaluator with temperature set to 1.0.

## Key Results
- 100% attack success rate on open-source models (Llama-3.2, Gemma-2, Qwen2.5)
- ≥95% attack success rate on closed-source models (GPT-4o, Gemini-1.5, Claude-3.5)
- Self-attention consistently concentrates on the last token, creating a universal vulnerability
- Specific neurons (Head 15, Layer 5 in Llama2) facilitate high-attention transmission for priming
- "Format" component of the priming template contributes most significantly to attack success

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Contextual Priming
The attack exploits the autoregressive nature of LLMs by exposing them to cumulative harmful contexts that override alignment training. The probability distribution $P(x_{t+1} | \dots)$ becomes biased by immediate malicious history rather than safety training, creating a "temporary amnesia" regarding RLHF protections.

### Mechanism 2: Safe Attention Shift
When the model begins generating a refusal, the attack manually replaces or "shifts" attention to non-refusal tokens, preventing the safety mechanism from terminating generation. This exploits the fact that safety is often triggered by specific token sequences, and interrupting these sequences resets the internal state regarding safety refusal.

### Mechanism 3: Hierarchical Attention Bias (Last-Token Dominance)
The structural reliance on the "last token" in self-attention layers creates a universal vulnerability where immediate context overrides deep-seated alignment. Analysis reveals that the most influential token for predicting the next token is invariably the last one, regardless of content type.

## Foundational Learning

- **Concept: Autoregressive Generation**
  - **Why needed here:** To understand why the order of tokens (priming before query) mathematically forces the model to complete the pattern rather than refuse it.
  - **Quick check question:** How does the probability of the next token change if the preceding context shifts from "Help me write a poem" to "Help me build a bomb"?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** To understand what is being "forgotten." RLHF trains the model to refuse harmful instructions, but this paper suggests that training is shallow compared to the strength of immediate context.
  - **Quick check question:** Does RLHF modify the model's weights permanently, or does it merely bias the output distribution? (Assumption: It biases distribution, which can be overridden).

- **Concept: Self-Attention Mechanism**
  - **Why needed here:** To comprehend the "Last-Token Dominance" finding. You must visualize how attention heads weigh previous tokens to predict the next one.
  - **Quick check question:** In a standard attention calculation, if the attention score of the final token is 0.9, how much "vote" do the earlier tokens have in the prediction?

## Architecture Onboarding

- **Component map:** Priming Template -> Attention Heads (Layer 5, Head 15) -> Decoding Loop -> Safe Attention Shift intervention
- **Critical path:** Priming Template -> Attention Heads (creating malicious context representation) -> Decoding (where Safe Attention Shift intervenes if refusal starts)
- **Design tradeoffs:** Attack Strength vs. Detectability - Safe Attention Shift is highly effective but requires decoding manipulation, harder to access in black-box APIs compared to simple prompt engineering. Temperature of 1.0 optimizes attack success; higher temperatures reduce coherence/ASR.
- **Failure signatures:** Cognitive Dissonance (model generates both refusal and harmful content simultaneously), Refusal Loops (without Safe Attention Shift, model generates "I'm sorry..." and stops)
- **First 3 experiments:**
  1. Replicate Priming Effect: Apply standard malicious query vs. with "Demon/Murderer" priming template, verify jump to ~100% ASR
  2. Visualize Attention: Use bertviz to confirm "Last-Token Dominance" hypothesis, check if attention >0.9 on final token
  3. Ablate Format vs. Role: Run attack removing "Role" vs. "Format," verify that "Format" contributes most significantly to ASR

## Open Questions the Paper Calls Out

### Open Question 1
Can the self-attention mechanism be redesigned to reduce "last token" dominance and mitigate Priming Attacks?
- **Basis in paper:** Section 5.1 identifies "last token" dominance as a structural vulnerability and calls for research into mitigating architectures.
- **Why unresolved:** Paper analyzes vulnerability but doesn't propose or test architectural modifications.
- **What evidence would resolve it:** Modified attention mechanism that distributes weights more evenly without degrading language modeling capabilities.

### Open Question 2
Can dynamic adjustment of safety-critical keyword sensitivity during decoding neutralize the "Safe Attention Shift"?
- **Basis in paper:** Section 5.3 shows increasing safety keyword sensitivity can reduce attack success by 85%.
- **Why unresolved:** Authors note they haven't refined a robust defensive mechanism integrating this sensitivity.
- **What evidence would resolve it:** Defense strategy that detects and prioritizes safety tokens in primed contexts while maintaining low false positive rates.

### Open Question 3
Do models with extremely large parameter counts (>100B) exhibit the same level of vulnerability to Priming Attacks?
- **Basis in paper:** Limitations section states models like Grok-1-314B and TeleChat2-115B were untested due to hardware constraints.
- **Why unresolved:** Authors couldn't run experiments on cluster-level models, leaving security evaluation gap for frontier-scale models.
- **What evidence would resolve it:** ASR results for Priming Attacks on models exceeding 100 billion parameters compared against smaller model results.

## Limitations
- Safe Attention Shift requires real-time decoding intervention, limiting practical exploitability in commercial deployments
- Attack success rates may be specific to current architectural designs and training data rather than fundamental alignment weaknesses
- Specific priming templates may become ineffective as models are updated and patched over time
- Authors omitted specific attack strategies for closed-source models to prevent misuse, limiting independent verification

## Confidence

**High Confidence (☑️):** Identification of "last-token dominance" in self-attention mechanisms is well-supported by attention visualization experiments showing consistent >0.9 attention weight on final token.

**Medium Confidence (⚠️):** 100% ASR claim on open-source models is methodologically sound but may suffer from overfitting to specific model versions and controlled experimental conditions.

**Low Confidence (❌):** Assertion that these attacks represent "critical vulnerabilities" requiring "urgent" mitigation is overstated given the practical barriers to exploitation through Safe Attention Shift requirements.

## Next Checks

1. **Temporal Robustness Test:** Replicate the attack on the same models 6-12 months after publication to assess whether specific priming templates remain effective or have been patched.

2. **Architectural Generalization Test:** Apply Priming Attack methodology to newer architectures (Mamba, RWKV, or hybrid attention models) to determine whether "last-token dominance" is truly fundamental or specific to Transformer-based LLMs.

3. **Defense Efficacy Evaluation:** Implement and test various proposed defenses (context-aware refusal mechanisms, attention regularization, prompt sanitization) against Priming Attack to quantify actual security risk and practical exploitability.