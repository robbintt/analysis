---
ver: rpa2
title: Do Code LLMs Do Static Analysis?
arxiv_id: '2505.12118'
source_url: https://arxiv.org/abs/2505.12118
tags:
- code
- llms
- static
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether code LLMs perform static analysis
  tasks during code intelligence tasks. We conducted experiments with three static
  analysis tasks (callgraph, AST, and dataflow generation) and three code intelligence
  tasks (code summarization, translation, and generation) using two open-source (CodeLlaMA,
  jam) and two closed-source models (GPT-4o, Gemini).
---

# Do Code LLMs Do Static Analysis?
## Quick Facts
- arXiv ID: 2505.12118
- Source URL: https://arxiv.org/abs/2505.12118
- Reference count: 40
- Code LLMs perform poorly on static analysis tasks, and finetuning on these tasks does not improve code intelligence performance.

## Executive Summary
This study investigates whether code large language models (LLMs) perform static analysis tasks to aid code intelligence tasks like summarization, translation, and generation. The authors conducted experiments with three static analysis tasks (callgraph, AST, and dataflow generation) and three code intelligence tasks using both open-source (CodeLlaMA, jam) and closed-source models (GPT-4o, Gemini). The results show that LLMs perform poorly on static analysis tasks, with in-context learning only improving AST generation. Finetuning on static analysis tasks did not significantly improve code intelligence task performance, suggesting that LLMs use different reasoning processes than human programmers for these tasks.

## Method Summary
The study evaluated LLMs on static analysis tasks (AST, callgraph, and dataflow generation) and code intelligence tasks (summarization, translation, and generation) using datasets in Java and C/C++. Gold standard outputs were generated using srcML, Doxygen, and Joern. The researchers tested both closed-source models (GPT-4o, Gemini) and open-source models (JAM-350M, CodeLlaMA-13B) with and without finetuning. For finetuning experiments, they employed QLoRA for CodeLlaMA and full finetuning for JAM, using a sequential approach where models were first trained on static analysis tasks and then on code intelligence tasks.

## Key Results
- LLMs performed poorly on static analysis tasks, with AST generation showing only marginal improvement through in-context learning
- Finetuning on static analysis tasks did not significantly improve code intelligence task performance
- Closed-source models (GPT-4o, Gemini) showed near-zero chain accuracy on dataflow tasks, indicating inability to maintain multi-step logic

## Why This Works (Mechanism)
The paper does not provide a specific mechanism for why LLMs fail at static analysis tasks. Instead, it suggests that LLMs use fundamentally different reasoning processes than human programmers, potentially relying on surface-level patterns rather than semantic understanding.

## Foundational Learning
- **Static Analysis Tasks:** Understanding callgraph, AST, and dataflow generation - needed to evaluate LLM performance on graph-based code representations; quick check: verify gold standard generation using srcML/Doxygen/Joern
- **Code Intelligence Tasks:** Familiarity with code summarization, translation, and generation metrics - needed to assess LLM capabilities on practical coding tasks; quick check: review BLEU/METEOR/USE metric implementations
- **Graph Serialization:** Knowledge of text-based graph formats (edge lists, DOT) - needed to convert graph outputs into LLM-readable formats; quick check: test graph serialization with sample callgraph data
- **Prompt Engineering:** Understanding of in-context learning techniques - needed to evaluate prompting effects on AST generation; quick check: design prompt templates with varying levels of detail
- **Sequential Finetuning:** Understanding of multi-stage training protocols - needed to replicate the finetuning experiments; quick check: verify checkpoint loading between finetuning stages
- **Evaluation Metrics:** Familiarity with Levenshtein distance and Jaccard similarity - needed to measure AST and graph generation accuracy; quick check: implement metric calculations on sample outputs

## Architecture Onboarding
- **Component Map:** Datasets (Java/C++) -> Gold Generation (srcML/Doxygen/Joern) -> LLM Inference (GPT-4o/Gemini/CodeLlaMA/JAM) -> Metric Calculation (Levenshtein/Jaccard/BLEU) -> Results Analysis
- **Critical Path:** Data preparation → Gold standard generation → Model inference → Metric computation → Statistical analysis
- **Design Tradeoffs:** Using closed-source models provides state-of-the-art performance but limits experimental control; open-source models allow full experimentation but may have lower baseline capabilities
- **Failure Signatures:** OOM errors during graph generation indicate context length issues; malformed XML suggests serialization problems; zero accuracy values indicate fundamental capability gaps
- **First Experiments:**
  1. Run baseline static analysis tasks with GPT-4o to establish poor performance benchmark
  2. Test in-context learning on AST generation with varying prompt templates
  3. Implement sequential finetuning on CodeLlaMA-13B and evaluate transfer learning

## Open Questions the Paper Calls Out
- How can model architectures or training curricula be modified so that static analysis capabilities transfer effectively to downstream code intelligence tasks?
- If LLMs do not rely on static analysis for comprehension, what specific internal mechanisms or representations do they utilize to perform code intelligence tasks?
- Can specific prompting strategies, such as chain-of-thought reasoning, improve LLM performance on complex static analysis tasks requiring multi-step logic?

## Limitations
- The study did not test alternative training methodologies that might enable transfer from static analysis to code intelligence tasks
- The graph serialization format used for training/inference is not detailed, which could affect performance measurements
- The sequential finetuning protocol lacks statistical significance testing for the observed effects

## Confidence
- **High confidence:** LLMs' inability to perform static analysis tasks as measured by quantitative metrics
- **Medium confidence:** LLMs use fundamentally different reasoning processes than human programmers
- **Low confidence:** In-context learning only helps AST generation without specific prompt details

## Next Checks
1. Run a prompt ablation study on AST generation using different serialization formats (DOT vs. edge lists) to isolate format effects
2. Implement gradient-based attribution on the sequential finetuning checkpoint to verify if static analysis features are being reused
3. Test whether increasing context window size beyond 10k tokens improves callgraph generation accuracy, controlling for truncation effects