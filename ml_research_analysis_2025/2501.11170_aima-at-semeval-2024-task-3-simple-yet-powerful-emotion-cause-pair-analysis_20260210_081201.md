---
ver: rpa2
title: 'AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis'
arxiv_id: '2501.11170'
source_url: https://arxiv.org/abs/2501.11170
tags:
- emotion
- extraction
- cause
- task
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents AIMA''s approach to SemEval-2024 Task 3, focusing
  on emotion-cause pair extraction in conversational contexts. The team developed
  a three-component model: embedding extraction using EmoBERTa, cause-pair extraction
  with emotion classification, and cause extraction via question-answering post-pair
  detection.'
---

# AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis

## Quick Facts
- arXiv ID: 2501.11170
- Source URL: https://arxiv.org/abs/2501.11170
- Reference count: 4
- Primary result: 10th place out of 23 teams in Subtask 1, 6th in Subtask 2 of SemEval-2024 Task 3

## Executive Summary
This paper presents AIMA's approach to SemEval-2024 Task 3, focusing on emotion-cause pair extraction in conversational contexts. The team developed a three-component model: embedding extraction using EmoBERTa, cause-pair extraction with emotion classification, and cause extraction via question-answering post-pair detection. Despite supporting multimodal inputs, the model focused on textual data for Subtask 1. The approach achieved 10th place out of 23 teams in Subtask 1 and 6th in Subtask 2. Key performance metrics included F1 scores across six emotion categories, with weighted averages accounting for class imbalances. The model demonstrated strong emotion classification capabilities while identifying opportunities for refining causal factor identification within conversations.

## Method Summary
AIMA's approach employs a three-stage pipeline for emotion-cause pair extraction. First, EmoBERTa embeddings are extracted from conversational utterances and fine-tuned for emotion classification. Second, a causality matrix is constructed using attention weights from a transformer encoder that combines embeddings with emotion logits. Third, for detected emotion-cause pairs, a question-answering model (DeBERTa-v3-base-squad2) extracts causal spans using templated prompts. The architecture supports multimodal inputs but focuses on text-only processing for Subtask 1.

## Key Results
- Achieved 10th place out of 23 teams in Subtask 1
- Weighted F1 score of 0.2584 (below baseline of 0.3000)
- Strict Match F1 of 0.0217 and Proportional Match F1 of 0.2049 for span extraction
- Strong emotion classification with diagonal dominance in confusion matrix
- Text-only approach underperformed multimodal baseline MC-ECPE-2steps

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning EmoBERTa on conversation-specific emotion data enables emotion classification that aligns with the target task's emotion taxonomy. EmoBERTa encodes speaker information and dialogue context through segment representations; fine-tuning adapts pretrained emotional knowledge to the ECF dataset's seven-class schema (neutral, joy, surprise, anger, sadness, disgust, fear). Core assumption: The pretrained emotional representations transfer effectively to the conversational emotion-cause domain without catastrophic forgetting.

### Mechanism 2
Concatenating classification logits with embeddings before transformer encoding enriches utterance representations with emotion-specific signals, improving causality detection. The combined representation `combined = [s1, s2, s3, logits]` injects explicit emotion predictions into the attention computation; the final transformer layer's attention weights form the causality matrix `Cm = AN(lencoder_1:N-1(input))`, capturing inter-utterance causal dependencies. Core assumption: Attention weights from the final encoder layer meaningfully encode causal relationships rather than merely linguistic correlations.

### Mechanism 3
Framing cause span extraction as a QA task with templated prompts enables precise localization of causal sub-texts within utterances. For each detected emotion-cause pair, a structured query is generated: "Which part of the text {target_utterance} is the reason for {speaker}'s feeling of {emotion} when {main_utterance} is said?"; the DeBERTa-v3-base-squad2 model (fine-tuned on SQuAD2 then task data) extracts answer spans. Core assumption: The prompt template generalizes across emotion categories and conversation structures without requiring task-specific prompt engineering.

## Foundational Learning

- **Transformer attention as relation extractor**
  - Why needed here: The causality matrix is derived from attention weights; understanding how self-attention encodes token-to-token and utterance-to-utterance relationships is essential for debugging spurious causality predictions.
  - Quick check question: Given a 4-utterance dialogue, can you manually trace how multi-head attention might assign higher weights between an emotion expression and its triggering utterance?

- **QA span extraction with SQuAD-style training**
  - Why needed here: The cause extraction phase uses a QA formulation; understanding start/end position prediction and handling of unanswerable questions (SQuAD2) is critical for interpreting model outputs.
  - Quick check question: How does a QA model handle cases where the answer span crosses token boundaries or contains partial words?

- **Multi-task emotion and cause learning**
  - Why needed here: The model jointly considers emotion classification logits and causality extraction; error propagation from emotion misclassification to causality prediction must be understood.
  - Quick check question: If an utterance is misclassified as "neutral" instead of "joy," how would this affect the downstream causality matrix?

## Architecture Onboarding

- **Component map**: Input utterances → EmoBERTa embeddings → Concatenate with logits → Add positional embeddings → Transformer encoder → Causality matrix (attention weights) → Filter to candidate pairs → Generate QA prompts → Extract cause spans
- **Critical path**: Input utterances → EmoBERTa embeddings → Concatenate with logits → Add positional embeddings → Transformer encoder → Causality matrix (attention weights) → Filter to candidate pairs → Generate QA prompts → Extract cause spans
- **Design tradeoffs**:
  - Text-only vs. multimodal: Architecture supports audio/video embeddings but used only text; paper notes this was for Subtask 1 focus—implies multimodal fusion was not prioritized for performance gain.
  - Two-stage vs. end-to-end: Causality matrix extraction is separate from QA-based span extraction; errors from stage 1 propagate to stage 2 without gradient feedback.
  - DeBERTa-v3-base vs. larger QA models: Chose base model for efficiency; unclear if larger model would improve proportional match F1 (0.2049 vs. strict 0.0217).
- **Failure signatures**:
  - Low strict match, higher proportional match (Table 1): Suggests cause span boundaries are imprecise—QA model captures relevant region but not exact annotation boundaries.
  - Weighted F1 (0.2584) below baseline (0.3000): Indicates text-only approach underperforms multimodal baseline MC-ECPE-2steps; possible multimodal cues are necessary for some causes.
  - Confusion matrix shows emotion classification is strong (diagonal dominance), but pair extraction lags—bottleneck is causality detection, not emotion recognition.
- **First 3 experiments**:
  1. Ablate logits from transformer input: Run with `combined = [s1, s2, s3]` only (no logits) to quantify contribution of emotion signal enrichment to causality matrix quality.
  2. Evaluate QA prompt variants: Test alternative prompt templates (e.g., simpler "What caused {emotion}?" vs. full template) on validation set to assess prompt sensitivity.
  3. Multimodal pilot: Add audio embeddings (even simple features like MFCCs) to the concatenation for Subtask 2 data to measure delta vs. text-only baseline.

## Open Questions the Paper Calls Out

- **Would integrating actual multimodal cues (audio and video) improve performance on emotion-cause pair extraction beyond text-only approaches, and to what extent?**
  - Basis in paper: [explicit] The authors state: "Although our architecture supports multimodal data—including text, audio, and video through concatenations of the embeddings of these modalities... this study specifically harnesses textual data." Additionally, their conclusion mentions "explore the integration of lightweight multimodal data processing techniques."
  - Why unresolved: The architecture was designed for multimodality but was tested only on text, leaving the potential benefit of multimodal cues unexplored.
  - What evidence would resolve it: Experiments comparing the current text-only approach against the same architecture with multimodal embeddings on both Subtask 1 and Subtask 2.

- **What specific refinements to the causal factor identification component would improve emotion-cause pair extraction accuracy?**
  - Basis in paper: [explicit] The error analysis states: "There is a valuable opportunity to refine the identification of causal factors within conversations for further improvement."
  - Why unresolved: The model excels at emotion classification but shows weakness in cause extraction. The paper provides no specific analysis of what aspects of cause identification fail most frequently.
  - What evidence would resolve it: Detailed error analysis categorizing failure modes in cause extraction; comparative experiments with alternative cause extraction methods.

- **How would an end-to-end architecture compare to the current pipeline approach in terms of both performance and computational efficiency?**
  - Basis in paper: [inferred] The model uses three sequential components, potentially allowing error propagation between stages. The conclusion emphasizes simplicity and efficiency, but the pipeline approach may introduce redundancy.
  - Why unresolved: No comparison is made between the pipeline approach and joint/ end-to-end alternatives.
  - What evidence would resolve it: Comparative experiments between the current pipeline and end-to-end architectures measuring F1 scores and computational costs.

## Limitations

- Several architectural details are underspecified, including the number of transformer encoder layers, hidden dimensions, dropout rates, and the method for deriving "s1, s2, s3" embeddings from EmoBERTa
- The text-only approach underperformed multimodal baselines, suggesting multimodal cues may be necessary for capturing some causal factors
- Reliance on attention weights to form the causality matrix assumes these weights meaningfully encode causal relationships rather than linguistic correlations, without direct corpus validation

## Confidence

- **High Confidence**: The use of EmoBERTa for emotion classification and the overall two-stage approach (emotion classification → causality matrix → QA-based cause extraction) are well-supported by the paper's results and methodology
- **Medium Confidence**: The mechanism of enriching utterance representations with emotion-specific signals through concatenated logits is plausible but lacks direct validation
- **Low Confidence**: The generalizability of the QA prompt template for cause extraction across diverse conversation structures and emotion categories is uncertain due to limited corpus support

## Next Checks

1. Ablate logits from transformer input: Run the causality matrix model with `combined = [s1, s2, s3]` only (no logits) to quantify the contribution of emotion signal enrichment to causality detection quality
2. Evaluate QA prompt variants: Test alternative prompt templates (e.g., simpler "What caused {emotion}?" vs. the full template) on the validation set to assess prompt sensitivity
3. Multimodal pilot: Add audio embeddings (even simple features like MFCCs) to the concatenation for Subtask 2 data to measure the delta vs. the text-only baseline