---
ver: rpa2
title: Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on
  Ill-Conditioned Problems
arxiv_id: '2506.04126'
source_url: https://arxiv.org/abs/2506.04126
tags:
- epoch
- function
- small
- component
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the convergence behavior of Incremental Gradient\
  \ Descent (IGD) in the small epoch regime, where the number of epochs K is smaller\
  \ than the condition number \u03BA. The authors establish lower bounds showing that\
  \ IGD can exhibit surprisingly slow convergence even when all component functions\
  \ are strongly convex."
---

# Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems

## Quick Facts
- **arXiv ID**: 2506.04126
- **Source URL**: https://arxiv.org/abs/2506.04126
- **Reference count**: 40
- **Primary result**: IGD can be slower than uniform-sampling SGD by a factor of n in the small epoch regime (K ≲ κ)

## Executive Summary
This paper establishes tight convergence bounds for Incremental Gradient Descent (IGD) in the small epoch regime, where the number of epochs K is smaller than the condition number κ. The authors prove that IGD can exhibit surprisingly slow convergence even when all component functions are strongly convex, with rates as poor as G²/(µK) - slower than uniform-sampling SGD by a factor of n. They also show that when some component functions are nonconvex, the convergence rate can degrade significantly, with the optimality gap potentially growing exponentially with κ/K. The paper demonstrates that a well-designed permutation strategy can enable permutation-based SGD to outperform uniform-sampling SGD in the small epoch regime.

## Method Summary
The paper analyzes IGD's convergence by constructing lower bound examples using dimension aggregation techniques. It examines both the small epoch regime (K ≲ κ) and large epoch regime (K ≳ κ), with and without nonconvex component functions. The analysis covers different permutation strategies including IGD (fixed ordering), Random Reshuffling (RR), and with-replacement SGD. The theoretical framework uses µ-strongly convex and L-smooth assumptions, with gradient error bounds characterized by parameters G and G*. The paper also explores Herding-based permutation strategies that could theoretically outperform with-replacement SGD.

## Key Results
- IGD converges at rate G²/(µK) in the small epoch regime (K ≲ κ), which is slower than uniform-sampling SGD by a factor of n
- When some component functions are nonconvex, the optimality gap can grow as G²/L · (1 + L/2µnK)^n ≈ exp(κ/2K)
- A well-designed permutation computed via Herding can achieve Õ(H²L²G²*/µ³n²K²), outperforming with-replacement SGD when n ≥ H²κ²/K
- In the large epoch regime, the convergence rate gap between convex and nonconvex component functions is only a factor of κ

## Why This Works (Mechanism)

### Mechanism 1: Small Epoch Convergence Degradation via Step Size Conflict
- Claim: In the small epoch regime (K ≲ κ), IGD converges at rate G²/(µK), which is slower than uniform-sampling SGD by a factor of n.
- Mechanism: A fundamental tension exists between step size requirements: (1) step size must be small (≲ 1/nL) to control cumulative error within a single epoch by limiting iterate movement; (2) step size must be large (≳ 1/K) to make meaningful progress toward the optimum when epochs are few. When K < κ, these requirements cannot be simultaneously satisfied.
- Core assumption: Assumption 2.4 (bounded gradient errors with P=0) and Assumption 2.3 (strong convexity and smoothness). The lower bound construction uses component functions sharing the same Hessian.
- Evidence anchors:
  - [section 2.4] "Why Do Existing Bounds Require Large Epochs?" explicitly describes this step size conflict
  - [Theorem 3.1] Establishes Ω(G²/µK) lower bound for K ≤ κ/2 with shared-Hessian components
  - [corpus] Weak direct corpus support for this specific mechanism; related work on anti-correlated SGD noise (arXiv:2306.05300) discusses epoch-based training dynamics but not the step size conflict
- Break condition: When K ≳ κ, the conflict resolves and IGD achieves tight bounds matching upper bounds (Theorem 4.1, Proposition 4.2).

### Mechanism 2: Nonconvex Component-Induced Exponential Divergence
- Claim: When some component functions are nonconvex, the optimality gap can grow as G²/L · (1 + L/2µnK)^n ≈ exp(κ/2K), exhibiting exponential dependence on κ/K rather than polynomial.
- Mechanism: Component functions that are concave in particular directions can systematically push iterates away from the global optimum. The construction uses n/2 strongly convex components and n/2 concave components, where the concave components amplify the gap exponentially with each epoch.
- Core assumption: Assumption 2.4 with P=3 (bounded gradient errors allowing larger deviation relative to overall gradient). Theorem 3.5 requires n ≥ 4, κ ≥ 4, K ≤ κ/4.
- Evidence anchors:
  - [Theorem 3.5] Establishes Ω(min{µD², G²/L·(1+L/2µnK)^n}) lower bound for 2D function with nonconvex components
  - [section 3.1] "F2(x) = L/8 x² with component functions... the first n/2 component functions are strongly convex, while the remaining component functions are concave"
  - [corpus] No direct corpus evidence for nonconvex component effects in small-epoch regime
- Break condition: In the large epoch regime (K ≳ κ³/n², κ^(3/2)), the rate gap between convex and nonconvex components reduces to only a factor of κ (Theorems 4.3, 4.4).

### Mechanism 3: Herding-Based Permutation for Optimal Gradient Balancing
- Claim: A permutation computed via Herding at the global optimum (requiring oracle access to ∇fi(x*)) can achieve convergence rate Õ(H²L²G²*/µ³n²K²), which outperforms with-replacement SGD when n ≥ H²κ²/K.
- Mechanism: The Herding algorithm (Lemma 3.6) finds a permutation σ such that the cumulative sum of component gradients at x* remains bounded: max_i ||Σ_{j=1}^i ∇f_σ(j)(x*)|| ≤ HG*. This bounds the "gradient drift" error that accumulates during epoch processing, replacing the worst-case O(nG*) drift with Õ(√d·G*).
- Core assumption: Components are µ-strongly convex. Requires oracle access to x* (impractical) OR identical-Hessian components where ∇fi(x*) - ∇F(x*) = ∇fi(x₀) - ∇F(x₀) can be computed at initialization.
- Evidence anchors:
  - [Theorem 3.7] "Herding at Optimum" proves existence of permutation achieving Õ(H²L²G²*/µ³n²K²)
  - [section 3.2] "We demonstrate that a well-designed permutation can enable permutation-based SGD to achieve a faster convergence rate than with-replacement SGD"
  - [corpus] GraB algorithm (Lu et al. 2022a, cited in paper) uses adaptive Herding for faster convergence in large-epoch regime
- Break condition: Theorem 3.7 is not implementable without x* knowledge; requires special structure (identical Hessians) for practical computation of permutation.

## Foundational Learning

- **Concept: Condition Number κ = L/µ**
  - Why needed here: The entire analysis hinges on whether K ≲ κ (small epoch regime) or K ≳ κ (large epoch regime). Understanding κ as the ratio of smoothness to strong convexity parameter is essential.
  - Quick check question: For a problem with L=100 and µ=0.1, what is κ? If you run K=500 epochs, are you in the small or large epoch regime?

- **Concept: Permutation-Based SGD Variants (IGD, RR, SS, GraB)**
  - Why needed here: The paper's contribution is specifically about IGD's worst-case behavior and contrasting it with other permutation strategies. Understanding the difference—IGD uses fixed ordering, RR shuffles each epoch, SS shuffles once and reuses—is critical.
  - Quick check question: Which permutation strategy would you expect to be most vulnerable to adversarial data ordering: IGD, RR, or SS?

- **Concept: Lower Bound Construction via Dimension Aggregation**
  - Why needed here: The proofs use a "dimension-aggregating technique" (Appendix B) where separate 1D or 2D functions are constructed for different step-size regimes, then combined. Understanding this technique helps interpret why the lower bounds hold "for any constant step size η."
  - Quick check question: Why does constructing separate functions for small, moderate, and large step-size regimes strengthen the lower bound result?

## Architecture Onboarding

- **Component map:**
  - Objective Function: F(x) = (1/n)Σᵢfᵢ(x) with Assumptions 2.3 (µ-strongly convex, L-smooth) and 2.4/2.5 (gradient bounds)
  - Algorithm Variants: Algorithm 1 (general permutation-based SGD) → IGD (identity permutation), RR (independent shuffle each epoch), SS (single shuffle), GraB (adaptive)
  - Regime Classification: K ≲ κ/n (very small), κ/n ≲ K ≲ κ (small), K ≳ κ (large)
  - Rate Components: G (gradient noise bound), G* (gradient at optimum), H (Herding constant ~√d), n (number of components), K (epochs)

- **Critical path:**
  1. Identify regime: Compute κ = L/µ, compare with K
  2. Check component assumptions: Are all fᵢ strongly convex? Are there nonconvex components?
  3. Select permutation strategy: IGD for deterministic, RR for practical, GraB/Hered (if identical Hessians available) for optimal
  4. Set step size: η ≈ 2/(µnK·log(...)) for theoretical bounds; practical choices differ

- **Design tradeoffs:**
  - **IGD vs RR**: IGD is deterministic but can be O(n) slower than RR in small-epoch regime; RR has variance but better worst-case rates
  - **Identical Hessian assumption**: Enables Õ(1/n²K²) rates via Herding but is restrictive; without it, rates degrade to Õ(1/K²) for IGD
  - **Small vs Large epoch**: Small epochs are practically relevant for LLM training (κ large, K small due to compute limits) but have worse convergence guarantees

- **Failure signatures:**
  - **Circular trajectory failure** (Theorem 3.3 construction): Iterates maintain constant distance from optimum due to rotational symmetry in component functions; detect via ||x^k_0 - x*|| not decreasing
  - **Exponential divergence** (Theorem 3.5 construction): Loss increases with epochs when nonconvex components dominate; detect via F(x^K_n) > F(x^k_n) for k < K
  - **Step size sensitivity**: For η < 1/(µnK), progress too slow; for η > 2/L, divergence; for η ∈ (2/L, ∞), iterate oscillates/diverges (Lemma F.1 shows ||x^K_n|| ≥ |x_0|)

- **First 3 experiments:**
  1. **Validate lower bound on Theorem 3.3 function**: Use parameters µ=1.0×10⁰, L=1.0×10⁴, G=1.0×10⁰, n=1.0×10³; initialize at x*=(0,0); plot trajectory to confirm circular motion and ||x^k_0|| ≈ constant (see Figure 2)
  2. **Compare IGD vs RR vs SGD on Theorem 3.5 function**: Use µ=1.0×10⁰, L=1.0×10⁴, G=1.0×10⁰, n=1.0×10²; measure optimality gap over varying K; expect IGD gap to increase exponentially as K decreases while RR/SGD remain stable (see Figure 4)
  3. **Real-world validation on MNIST/CIFAR binary classification**: Use natural data ordering (all class 0 then all class 1); compare IGD (deterministic sequential), RR, and with-replacement SGD; expect IGD to show slower convergence in early epochs (see Figures 5, 6)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tight convergence bounds be established for Random Reshuffling (RR) in the small epoch regime (K ≲ κ) under standard strongly convex assumptions?
- Basis in paper: [explicit] Appendix A.4 highlights the gap between the existing upper bound (Õ(L²/µ³nK²)) and the lower bound (Ω(1/µnK)) as a clear direction for future exploration.
- Why unresolved: Current tight bounds for small epochs are restricted to quadratic objectives with commuting Hessians; generalizing beyond this requires more advanced analytical techniques.
- What evidence would resolve it: A matching upper and lower bound proof for RR that applies to general smooth, strongly convex functions without the commuting Hessian requirement.

### Open Question 2
- Question: Can an efficient and implementable permutation-based SGD method be designed that guarantees faster convergence than with-replacement SGD in the small epoch regime?
- Basis in paper: [explicit] The Conclusion identifies "develop[ing] an efficient and practical permutation-based SGD method" as a future direction. Section 3.2 notes that Theorem 3.7, while proving such speedup exists, is not implementable because it requires oracle knowledge of gradients at the optimum x*.
- Why unresolved: The only current theoretical proof of acceleration relies on a "Herding at Optimum" strategy that is impossible to execute without knowing x* a priori.
- What evidence would resolve it: An algorithm specification and convergence proof that achieves a rate of Õ(1/n²K²) without prior knowledge of x*.

### Open Question 3
- Question: Do the pessimistic lower bounds for Incremental Gradient Descent (IGD) in the small epoch regime remain tight when using varying step sizes?
- Basis in paper: [inferred] Section 3.1 states that extending Theorem 3.1 to a varying step size scheme is left for "future work," noting that the current comparison to with-replacement SGD involves a subtle difference in step size regimes.
- Why unresolved: The paper's lower bound construction specifically partitions the domain of the constant step size η to ensure slow convergence for any fixed choice.
- What evidence would resolve it: A theoretical lower bound that applies to standard diminishing step size schedules (e.g., η_t ∝ 1/t) or an upper bound showing improved convergence under such schedules.

## Limitations

- The Herding-based permutation strategy (Theorem 3.7) requires oracle access to the global optimum x*, making it practically infeasible for most applications
- The exponential divergence result for nonconvex components (Theorem 3.5) relies on a specific 2D construction that may not generalize to higher dimensions
- The lower bound analysis for IGD in the small epoch regime may not hold for varying step size schedules

## Confidence

- **High Confidence**: The small epoch regime lower bounds (Theorem 3.1, 3.3) are well-supported by the mathematical framework and have clear empirical validation through trajectory visualization
- **Medium Confidence**: The large epoch regime bounds (Theorems 4.1-4.4) follow logically from the small epoch analysis but rely on several intermediate results whose practical implications are less clear
- **Low Confidence**: The Herding-based permutation claim (Theorem 3.7) has significant practical limitations due to the x* oracle requirement, and the exponential divergence for nonconvex components may be an artifact of the specific 2D construction

## Next Checks

1. **Implement Herding Permutation without Oracle**: Test whether the permutation strategy works when component gradients are estimated at initialization rather than requiring exact x* access. This would bridge the gap between theoretical optimality and practical feasibility.

2. **Generalize Nonconvex Component Analysis**: Extend the nonconvex component construction to 3D or higher dimensions to verify whether the exponential divergence phenomenon persists or if it's specific to the 2D case presented.

3. **Empirical Validation on Real LLM Training**: Run IGD on a small language model training task (e.g., next-token prediction on a subset of Wikipedia) to verify whether the predicted convergence degradation in the small epoch regime manifests in practical deep learning settings.