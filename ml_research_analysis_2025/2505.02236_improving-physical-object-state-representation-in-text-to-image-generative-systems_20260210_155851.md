---
ver: rpa2
title: Improving Physical Object State Representation in Text-to-Image Generative
  Systems
arxiv_id: '2505.02236'
source_url: https://arxiv.org/abs/2505.02236
tags:
- object
- data
- states
- synthetic
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a fully automatic pipeline to generate synthetic
  data for training text-to-image models to better understand and represent object
  states such as absence or emptiness. The method uses prompts describing objects
  in various states, generates synthetic images, filters them using a vision-language
  model, and recaps the prompts for diversity.
---

# Improving Physical Object State Representation in Text-to-Image Generative Systems

## Quick Facts
- arXiv ID: 2505.02236
- Source URL: https://arxiv.org/abs/2505.02236
- Reference count: 40
- Key outcome: Fine-tuning text-to-image models on synthetic data for object absence/emptiness states improves GPT scores by 8% and VQA scores by 24% on public benchmarks, with no degradation in visual quality.

## Executive Summary
This paper addresses a fundamental failure in text-to-image generative models: their inability to accurately represent object states like absence or emptiness. The authors develop a fully automatic pipeline that generates synthetic training data, filters it using a vision-language model, and recaps prompts for diversity. When fine-tuned on this data, models show significant improvements on state representation benchmarks while maintaining performance on general prompts. The approach generalizes to unseen objects and demonstrates that targeted synthetic data generation can correct specific semantic failures in diffusion models.

## Method Summary
The method uses GPT-4o-mini to generate 3,000 object names and template prompts describing absence states. SD 1.5 generates multiple candidate images per prompt, which are filtered by GPT-4o-mini using VQA-style prompts to ensure accurate state representation. An LLM then recaps prompts into diverse natural language forms. The resulting ~7,600 image-text pairs are used to fine-tune various models via LoRA (rank 4-16 depending on model). The pipeline specifically targets the gap where CLIP-based text encoders struggle with negation and absence concepts.

## Key Results
- 8% improvement in GPT-4o-mini evaluation on GenAI-Object-State benchmark
- 24% improvement in VQA-score on Object-State-Bench benchmark
- No degradation in visual quality on non-state prompts (GPT 40%→39%, VQA 68%→69%)
- Generalizes to unseen objects (5% performance gap between seen/unseen benchmarks)
- Qualitatively fixes failure modes like generating bottles on an "empty table"

## Why This Works (Mechanism)

### Mechanism 1: VLM-Guided Filtering Creates Clean Training Signal
The pipeline generates multiple candidate images per prompt and uses GPT-4o-mini to answer "Does this image accurately reflect both conditions?" — keeping only images that pass. This removes examples where the base model hallucinates absent objects.

### Mechanism 2: Recaptioning Breaks Template Overfitting
An LLM transforms template-like prompts into varied natural language (e.g., "an empty table" → "a table without any bottles on it"), introducing syntactic diversity while preserving semantic intent.

### Mechanism 3: LoRA Fine-Tuning Preserves Base Capabilities
Low-rank adaptation updates only a small subset of weights, constraining deviation from pre-trained representations while still adapting to the new data distribution.

## Foundational Learning

- Concept: **Diffusion model text conditioning via CLIP encodings**
  - Why needed here: Understanding why CLIP struggles with negation clarifies why the baseline fails and why explicit training data is required.
  - Quick check: Can you explain how a text encoder's representation of "without a bottle" might collapse toward "with a bottle" due to co-occurrence statistics?

- Concept: **Distribution shift and synthetic data augmentation**
  - Why needed here: The paper's core hypothesis is that training data lacks absence-state examples; synthetic data explicitly fills this gap.
  - Quick check: Why would filtering real datasets (COCO, VidOSC) for absence captions yield lower-quality training data than synthetic generation?

- Concept: **Low-Rank Adaptation (LoRA) mechanics**
  - Why needed here: All fine-tuning uses LoRA; understanding rank, scaling, and module targeting is necessary for reproduction.
  - Quick check: If LoRA rank is too low, what failure mode would you expect when learning new semantic concepts?

## Architecture Onboarding

- Component map: Prompt Generation (GPT-4o-mini) → Image Synthesis (SD 1.5) → 3000 objects → Multiple seeds/prompt → VLM Filter (GPT-4o-mini) → Yes/No classification → Recaptioning (LLM) → 7,600 final pairs → LoRA Fine-Tuning (SD 1.5, 2.1, SDXL, Flux, OmniGen)

- Critical path: The VLM filtering step is the bottleneck — the paper notes baseline models "struggle to generate images that represent object states correctly," so without filtering, >50% of synthetic examples may be mislabeled.

- Design tradeoffs:
  - CFG scale (5.0 vs. default 7.5): Lower CFG increases diversity but reduces prompt adherence
  - Filtering strictness: Aggressive filtering yields cleaner data but reduces dataset size
  - Recaptioning aggressiveness: Over-specific prompts may create compositional complexity the model can't satisfy

- Failure signatures:
  - Over-correction: Model over-represents emptiness, making objects themselves poorly formed
  - Partial convergence: Model moves toward correct state but doesn't fully capture all semantic details
  - Commonsense degradation: 3-4% drop on CommonsenseT2I suggests some interference with non-state knowledge

- First 3 experiments:
  1. Reproduce filtering rate: Generate 100 prompts through pipeline, measure pass fraction
  2. Ablate filter entirely: Fine-tune on unfiltered synthetic data and compare to filtered version
  3. Test on held-out state types: Apply approach to fullness or presence states without explicit training data

## Open Questions the Paper Calls Out
- Can the proposed synthetic data pipeline be extended to rectify other specific failure modes like inaccurate object counts or misspelled text?
- How do direct architectural modifications compare against data-centric fine-tuning approaches?
- How can fine-tuning be optimized to prevent over-representation of emptiness while maintaining visual fidelity?

## Limitations
- Generalization to unseen objects shows 5% performance gap, suggesting less complete transfer than claimed
- Limited ablation studies prevent isolating contributions of VLM filtering versus recaptioning
- Choice of LoRA ranks (4-16) is not empirically justified and may be too low for complex semantic concepts

## Confidence
- High: 24% improvement on Object-State-Bench is well-supported by controlled evaluation
- Medium: "No degradation" claim holds for non-state prompts but CommonsenseT2I shows 3-4% drop
- Low: Recaptioning necessity lacks direct evidence; no comparison to unfiltered synthetic data

## Next Checks
1. Ablation of synthetic data components: Fine-tune models using unfiltered synthetic data and filtered data without recaptioning to quantify marginal contributions.
2. Generalization to non-absence states: Apply pipeline to "fullness" or "presence" states to test semantic transfer.
3. VLM filter reliability assessment: Manually audit 100 filtered examples to measure false positive/negative rates and calculate actual filtering pass rate.