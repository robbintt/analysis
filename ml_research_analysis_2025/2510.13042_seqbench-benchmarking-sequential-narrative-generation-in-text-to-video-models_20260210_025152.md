---
ver: rpa2
title: 'SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models'
arxiv_id: '2510.13042'
source_url: https://arxiv.org/abs/2510.13042
tags:
- evaluation
- narrative
- generation
- temporal
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeqBench introduces a benchmark for evaluating narrative coherence
  in text-to-video generation, addressing the gap where existing T2V benchmarks focus
  on visual quality but ignore sequential storytelling. The authors designed a dataset
  of 320 prompts across four content categories and four difficulty levels, generating
  2,560 videos from eight state-of-the-art T2V models.
---

# SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models

## Quick Facts
- **arXiv ID**: 2510.13042
- **Source URL**: https://arxiv.org/abs/2510.13042
- **Reference count**: 26
- **Key outcome**: Introduces DTG-based metric for evaluating narrative coherence in text-to-video models, revealing fundamental limitations in sequential reasoning

## Executive Summary
SeqBench addresses a critical gap in text-to-video (T2V) evaluation by introducing a benchmark focused specifically on narrative coherence across sequential frames. While existing T2V benchmarks primarily assess visual quality, SeqBench evaluates how well models maintain consistent object states, handle multi-object interactions, and preserve realistic timing and ordering relationships between sequential actions. The benchmark consists of 320 prompts across four content categories and four difficulty levels, generating 2,560 videos from eight state-of-the-art T2V models.

The study introduces a Dynamic Temporal Graphs (DTG) metric that captures long-range dependencies and temporal ordering while maintaining computational efficiency. Human evaluation showed strong correlation with DTG scores (ρ=0.857), validating the metric's effectiveness. Kling 2.0 emerged as the top performer with an average coherence score of 0.252, significantly outperforming other models. The benchmark reveals that current T2V models fundamentally struggle with maintaining object states across multi-action sequences and handling physically plausible multi-object scenarios.

## Method Summary
The authors constructed SeqBench by first creating a dataset of 320 prompts across four content categories (human actions, object interactions, animal movements, and complex scenes) and four difficulty levels. These prompts were designed to test various aspects of sequential narrative generation, from simple single-object actions to complex multi-object scenarios with temporal dependencies. Eight state-of-the-art T2V models were used to generate 2,560 videos from these prompts.

To enable automatic evaluation, the researchers developed the Dynamic Temporal Graphs (DTG) metric, which constructs temporal graphs from video frames and analyzes the consistency of object states and action sequences over time. The metric captures long-range dependencies and temporal ordering while remaining computationally efficient compared to frame-by-frame analysis. Human evaluation of 200 randomly selected videos was conducted to validate the automatic metric, showing strong correlation (ρ=0.857) between human judgments and DTG scores.

## Key Results
- Kling 2.0 achieved the highest average coherence score of 0.252, significantly outperforming other T2V models
- Current T2V models struggle with maintaining consistent object states across multi-action sequences (average scores below 0.3)
- Models showed particular difficulty with multi-object scenarios, often producing physically implausible results with inconsistent spatial relationships
- The DTG metric demonstrated strong correlation with human evaluation (ρ=0.857), validating its effectiveness for automatic assessment

## Why This Works (Mechanism)
The benchmark works by systematically testing T2V models' ability to maintain narrative coherence across sequential frames, which requires tracking object states, preserving temporal ordering, and handling multi-object interactions consistently. The DTG metric captures these requirements by constructing temporal graphs that represent object relationships and action sequences, then analyzing graph properties that indicate narrative consistency. This approach effectively measures whether models can maintain logical story progression rather than just producing visually appealing individual frames.

## Foundational Learning

**Temporal Graph Construction** - Why needed: To represent the evolving relationships between objects and actions across video frames. Quick check: Verify that graph nodes correctly capture object states and edges represent temporal transitions.

**Object State Tracking** - Why needed: To ensure models maintain consistent object properties (position, appearance, state) across sequential frames. Quick check: Confirm that object tracking algorithms correctly handle occlusions and reappearances.

**Action Sequence Analysis** - Why needed: To evaluate whether the temporal ordering of actions follows logical narrative progression. Quick check: Validate that action recognition algorithms correctly identify and sequence events across frames.

**Multi-object Interaction Modeling** - Why needed: To assess how well models handle physically plausible interactions between multiple objects. Quick check: Test that spatial relationships between objects remain consistent with physical constraints.

**Coherence Scoring** - Why needed: To quantify the degree of narrative consistency in generated videos. Quick check: Verify that the scoring function appropriately weights different types of coherence violations.

## Architecture Onboarding

**Component Map**: Prompt Generator -> Video Generation Models -> Frame Extraction -> Object Detection -> Temporal Graph Construction -> Coherence Scoring -> Human Evaluation Validation

**Critical Path**: Prompt generation → video generation → frame processing → temporal graph construction → coherence scoring → human validation

**Design Tradeoffs**: The DTG metric prioritizes computational efficiency over frame-by-frame analysis, potentially missing fine-grained motion inconsistencies. The benchmark focuses on narrative coherence at the expense of spatial consistency and object permanence evaluation.

**Failure Signatures**: Common failure modes include object state inconsistencies (objects appearing/disappearing), physically implausible multi-object interactions (objects passing through each other), and incorrect temporal ordering of sequential actions.

**First 3 Experiments**:
1. Test DTG metric on simple single-object action videos to establish baseline performance
2. Evaluate multi-object interaction scenarios to identify specific failure modes
3. Compare DTG scores against human evaluation on a diverse subset of generated videos

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

- Benchmark focuses on narrative coherence but does not evaluate spatial consistency, object permanence, or fine-grained motion quality
- Limited content category diversity (only four categories) may not represent full complexity of real-world scenarios
- Dataset size of 320 prompts may not provide sufficient statistical power for detecting subtle performance differences
- DTG metric has not been validated across diverse video generation tasks beyond sequential narratives

## Confidence

**High Confidence**: Benchmark dataset construction methodology is well-documented and reproducible; human evaluation correlation results strongly validate DTG metric within tested domain.

**Medium Confidence**: Generalization of DTG metric to other video generation tasks remains untested; computational efficiency claims require independent verification.

**Low Confidence**: Long-term reliability for tracking model improvements over time as architectures continue to evolve rapidly.

## Next Checks

1. **Cross-Domain Validation**: Test DTG metric's effectiveness on non-narrative video generation tasks to establish generalizability beyond sequential storytelling.

2. **Longitudinal Study**: Re-evaluate same models after 6-12 months to assess benchmark's effectiveness for tracking improvements in sequential reasoning capabilities.

3. **Alternative Metric Comparison**: Systematically compare DTG against other coherence metrics on a subset of benchmark to identify potential complementary evaluation approaches.