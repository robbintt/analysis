---
ver: rpa2
title: Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time
  Adaptation
arxiv_id: '2512.11458'
source_url: https://arxiv.org/abs/2512.11458
tags:
- action
- cache
- skeleton-cache
- zero-shot
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Skeleton-Cache introduces a training-free test-time adaptation\
  \ framework for skeleton-based zero-shot action recognition, addressing the challenge\
  \ of distribution shifts between seen and unseen actions during inference. The method\
  \ constructs a non-parametric cache storing structured skeleton descriptors\u2014\
  combining global, spatial, and temporal features\u2014and reformulates recognition\
  \ as a retrieval task."
---

# Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation

## Quick Facts
- arXiv ID: 2512.11458
- Source URL: https://arxiv.org/abs/2512.11458
- Reference count: 40
- Key outcome: Training-free test-time adaptation framework for skeleton-based zero-shot action recognition that achieves up to 7 percentage points improvement in zero-shot accuracy and 7 points in generalized zero-shot harmonic mean.

## Executive Summary
Skeleton-Cache introduces a training-free test-time adaptation framework for skeleton-based zero-shot action recognition, addressing the challenge of distribution shifts between seen and unseen actions during inference. The method constructs a non-parametric cache storing structured skeleton descriptors—combining global, spatial, and temporal features—and reformulates recognition as a retrieval task. Large language models guide descriptor fusion by assigning class-specific importance weights, enabling dynamic adaptation without retraining. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II show consistent performance boosts across diverse SZAR backbones, achieving up to 7 percentage points improvement in zero-shot accuracy and 7 points in generalized zero-shot harmonic mean, while maintaining minimal computational overhead suitable for real-time deployment.

## Method Summary
Skeleton-Cache is a plug-and-play test-time adaptation module for frozen skeleton-based zero-shot action recognition models. The method constructs a non-parametric cache that stores structured skeleton descriptors (global, spatial body regions, and temporal phases) extracted from high-confidence predictions. During inference, test samples are represented as 8 descriptors and compared against cached entries using cosine similarity with learned temperature scaling. Large language models (GPT-4o) provide class-specific importance weights for descriptor fusion based on semantic understanding of which body parts and motion phases are most relevant for each action. The method combines base model predictions with cache-retrieved predictions using a learnable fusion coefficient, achieving adaptation without gradient updates.

## Key Results
- Achieves up to 7 percentage points improvement in zero-shot accuracy over baselines on NTU RGB+D 60/120
- Improves generalized zero-shot harmonic mean by up to 7 points compared to state-of-the-art methods
- Maintains minimal computational overhead suitable for real-time deployment
- Consistent performance boosts across diverse SZAR backbones including PURLS, SA-DVAE, SMIE, and SynSE
- Cache size of 8 samples per class provides optimal balance between diversity and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Structured Multi-Scale Descriptor Retrieval
Decomposing skeleton sequences into global, spatial (body part), and temporal descriptors enables more precise matching for retrieval-based classification than holistic embeddings. Each test sample is represented as 8 descriptors (1 global + 4 spatial body regions + 3 temporal phases). Cache retrieval computes per-descriptor similarity scores against stored entries, producing descriptor-wise class logits that capture complementary discriminative cues. Fine-grained local patterns remain consistent across same-class samples and are more reliable for distinguishing similar actions than global features alone. Evidence: ablation shows full 8-part descriptors (+6.24 pp) outperforms global-only (+3.38 pp).

### Mechanism 2: LLM-Guided Semantic Prior for Descriptor Fusion
LLM-derived class-specific importance weights improve fusion of descriptor-wise predictions by encoding which body parts and motion phases are semantically relevant per action. GPT-4o is prompted once per class to output spatial weights for 4 body regions, temporal weights for 3 phases, and global-vs-local preference γ. These form a normalized weight vector applied to descriptor-wise logits before fusion with base model predictions. LLM semantic knowledge about action semantics (e.g., "kicking" emphasizes legs and end phase) transfers to skeleton feature importance. Evidence: LLM weights (+6.24 pp) outperform uniform weights (+4.45 pp) and random weights (-1.91 pp).

### Mechanism 3: Entropy-Gated Cache Population
Selectively caching high-confidence predictions creates a reliable memory that improves over time without gradient updates. For each test sample, entropy of the prediction distribution is computed. Low-entropy (high-confidence) samples replace the highest-entropy entry within their predicted class block, up to capacity K per class. High-confidence predictions are more likely correct, so cached entries become increasingly representative of true class distributions. Evidence: performance plateaus at K=8, indicating moderate cache suffices; AdaNPC and TDA support non-gradient adaptation validity.

## Foundational Learning

- **Concept: Zero-Shot Learning (ZSL) and Generalized ZSL (GZSL)**
  - Why needed here: The method operates on unseen classes using semantic embeddings; understanding the train/test class split and harmonic mean metric is essential.
  - Quick check question: Can you explain why GZSL is harder than standard ZSL?

- **Concept: Test-Time Adaptation (TTA)**
  - Why needed here: Skeleton-Cache is a training-free TTA method that modifies inference behavior without gradient updates.
  - Quick check question: What is the key difference between gradient-based TTA (e.g., Tent) and training-free TTA (e.g., T3A)?

- **Concept: Skeleton Sequence Representation (C×T×V×M)**
  - Why needed here: Feature extraction and descriptor computation depend on understanding joint indices, temporal frames, and multi-person handling.
  - Quick check question: What do the dimensions C, T, V, and M represent in a skeleton tensor?

## Architecture Onboarding

- **Component map**: Frozen SZAR Backbone (ST-GCN encoder + semantic alignment head) → Descriptor Extractor → Non-Parametric Cache → Similarity Module → LLM Prior Matrix → Logit Fusion

- **Critical path**: Load frozen SZAR model and precomputed LLM weights → For each test sample: extract descriptors → compute cache similarity → retrieve descriptor-wise logits → fuse with LLM weights → combine with base logits → If prediction entropy below threshold, update cache

- **Design tradeoffs**:
  - Cache size K: Larger K captures more diversity but increases memory and latency (optimal ~8)
  - Temperature β: Higher β sharpens similarity distribution (optimal ~3.0)
  - Fusion coefficient αs: Controls cache influence vs. base model (optimal ~5.0)
  - LLM choice: GPT-4o used; smaller models may produce less reliable priors

- **Failure signatures**:
  - Cache accuracy plateaus or drops → check for low-confidence predictions polluting cache
  - Per-class accuracy variance high → inspect LLM weights for semantic mismatches
  - Latency spike → reduce K or batch similarity computation

- **First 3 experiments**:
  1. Baseline validation: Replicate PURLS+SC on NTU60 55/5 split; verify reported 85.46% ZSL accuracy with K=8, β=3.0, αs=5.0
  2. Ablation sweep: Vary descriptor configurations (global-only, spatial-only, temporal-only, full) to confirm 8-part design benefit
  3. Weight strategy comparison: Test random, uniform, and LLM weights to validate semantic prior contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Skeleton-Cache's retrieval mechanism be improved for complex, multi-limb coordinated actions (e.g., "juggle") where high-confidence exemplars are scarce?
- Basis in paper: The per-class analysis shows "juggle exhibits a significant drop (13.37% to 0.87%), likely due to its complex, multi-limb coordination, which challenges the cache's retrieval when high-confidence exemplars are scarce."
- Why unresolved: The current cache relies on high-confidence samples, but complex actions may not produce confident predictions initially, creating a cold-start problem for those classes.
- What evidence would resolve it: Demonstrating improved performance on complex coordination actions through modified cache initialization, relaxed confidence thresholds, or multi-stage retrieval strategies.

### Open Question 2
- Question: How robust is Skeleton-Cache under severe occlusions and significant pose noise?
- Basis in paper: The authors state: "In challenging scenarios with severe occlusions or pose noise, performance may be moderately affected."
- Why unresolved: The paper evaluates on standard benchmarks (NTU, PKU-MMD) without systematic stress-testing under degraded input quality conditions.
- What evidence would resolve it: Experiments on datasets with synthetic or natural occlusions/noise, measuring accuracy degradation curves as input quality decreases.

### Open Question 3
- Question: Can Skeleton-Cache be effectively combined with gradient-based TTA methods to leverage both structured retrieval and parameter adaptation?
- Basis in paper: Table 7 shows gradient-based methods (TPT, DiffTPT) fail on skeleton data, while Skeleton-Cache succeeds. The complementary strengths suggest potential hybrid approaches were not explored.
- Why unresolved: The paper positions training-free and gradient-based methods as alternatives rather than complementary components.
- What evidence would resolve it: Experiments combining Skeleton-Cache's retrieval with lightweight gradient updates, measuring whether gains are additive or conflicting.

## Limitations
- Dataset split specificity: Paper references standard splits but doesn't explicitly list which 55/5 classes are used for NTU60
- LLM dependency: Performance relies on GPT-4o's semantic understanding; smaller models may produce less reliable weights
- Cache initialization bias: Early predictions heavily influence cache content; systematic miscalibration can accumulate incorrect prototypes
- Computational overhead: Real-time cache updates and similarity computations add latency that may impact deployment

## Confidence
- **High confidence**: Multi-scale descriptor retrieval improves discrimination (supported by ablation showing +6.24 pp improvement); cache-based non-parametric adaptation is valid (supported by AdaNPC and TDA references)
- **Medium confidence**: LLM-guided fusion provides meaningful semantic priors (supported by weight ablation, but limited direct corpus evidence for skeleton-specific LLM weighting)
- **Medium confidence**: Entropy-gated cache population improves over time (supported by intuition and related work, but no ablation on cache update strategy)

## Next Checks
1. Replicate the 55/5 split for NTU60 using referenced prior protocols and verify baseline PURLS+SC performance matches reported 85.46% ZSL accuracy.
2. Conduct ablation studies varying cache size K (1-16) and fusion coefficient αs (1-10) to confirm optimal hyperparameters and assess sensitivity.
3. Test the method with alternative LLM models (e.g., GPT-3.5, open-source alternatives) to evaluate the robustness of semantic prior contribution and identify the minimum viable model size.