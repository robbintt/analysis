---
ver: rpa2
title: 'Time to Split: Exploring Data Splitting Strategies for Offline Evaluation
  of Sequential Recommenders'
arxiv_id: '2507.16289'
source_url: https://arxiv.org/abs/2507.16289
tags:
- test
- validation
- data
- target
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares data splitting strategies for evaluating sequential
  recommender systems, focusing on leave-one-out and global temporal splits with various
  target selection methods. Results show that leave-one-out splits inflate performance
  by allowing temporal leakage and produce unrealistic test horizons.
---

# Time to Split: Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders

## Quick Facts
- arXiv ID: 2507.16289
- Source URL: https://arxiv.org/abs/2507.16289
- Authors: Danil Gusak; Anna Volodkevich; Anton Klenitskiy; Alexey Vasilev; Evgeny Frolov
- Reference count: 40
- This study compares data splitting strategies for evaluating sequential recommender systems, focusing on leave-one-out and global temporal splits with various target selection methods.

## Executive Summary
This study investigates the impact of different data splitting strategies on the evaluation of sequential recommender systems. The authors systematically compare leave-one-out (LOO) splits against global temporal splits (GTS) with different target selection methods, revealing that LOO splits can artificially inflate performance through temporal leakage. The research demonstrates that GTS with Last or Random target selection provides more realistic evaluation conditions that better align with the next-item prediction task, while maintaining computational efficiency compared to more comprehensive evaluation approaches.

## Method Summary
The authors evaluate sequential recommender systems using five different data splitting strategies across four datasets (ML-1M, ML-25M, Gowalla, and Amazon-Books). They compare leave-one-out (LOO) splits with global temporal splits (GTS) using different target selection methods: Last, First, Random, and All. The evaluation employs two standard metrics - Hit Rate at 10 (HR@10) and Normalized Discounted Cumulative Gain at 10 (NDCG@10). To establish a more comprehensive baseline, they also implement a Successive evaluation protocol that requires predicting each item in the test sequence sequentially. The study tests four sequential recommendation models: FPMC, Fossil, Caser, and SASRec.

## Key Results
- Leave-one-out splits inflate performance by allowing temporal leakage and produce unrealistic test horizons
- Global temporal splits with Last or Random targets align better with real-world next-item prediction and yield high correlation with more comprehensive but computationally expensive Successive evaluation
- Model rankings vary significantly across splits, highlighting the need for careful protocol selection to ensure fair and reproducible comparisons
- GTS Last and Random are recommended as practical alternatives, while GTS All and First targets show poor alignment with the task

## Why This Works (Mechanism)
The effectiveness of different splitting strategies depends on their ability to create realistic evaluation scenarios that reflect the sequential nature of recommendation tasks. LOO splits fail because they allow future information to leak into training, creating an artificial performance boost. GTS splits with appropriate target selection create temporal barriers that prevent information leakage while maintaining realistic prediction horizons. The correlation between GTS Last/Random and Successive evaluation suggests that these methods capture the essential characteristics of sequential prediction without requiring computationally expensive full-sequence evaluation.

## Foundational Learning
**Temporal leakage** - When future information is inadvertently included in training data, artificially inflating model performance
- Why needed: Critical for understanding why LOO splits give misleading results
- Quick check: Verify that test items are not present in training sequences for any user

**Global temporal split** - Dividing data based on a global timestamp rather than per-user boundaries
- Why needed: Ensures consistent temporal separation across all users
- Quick check: Confirm that all training items have timestamps before the split point

**Target selection methods** - Different strategies for choosing which items become prediction targets in test sets
- Why needed: Determines what the model is actually being evaluated on
- Quick check: Verify that selected targets align with intended evaluation goals

**Successive evaluation** - Comprehensive evaluation where each item in a sequence must be predicted sequentially
- Why needed: Provides gold standard for evaluation despite computational cost
- Quick check: Ensure each prediction uses only preceding items as context

**Evaluation horizon** - The temporal distance between training data and test items
- Why needed: Affects the difficulty and realism of the prediction task
- Quick check: Measure average time gaps between training and test items

## Architecture Onboarding

Component map: Data -> Preprocessing -> Splitting -> Model Training -> Evaluation -> Metrics

Critical path: Raw interaction data flows through preprocessing (filtering, formatting) into splitting strategy selection, then to model training where temporal constraints are enforced, followed by evaluation using selected metrics, and finally aggregation of results for comparison.

Design tradeoffs: LOO offers simplicity and high performance but unrealistic evaluation conditions; GTS provides realistic temporal separation but requires careful parameter tuning; Successive offers most comprehensive evaluation but at high computational cost.

Failure signatures: Performance inflation in LOO indicates temporal leakage; poor correlation between splits suggests misalignment with evaluation goals; computational bottlenecks in Successive indicate scalability issues.

First experiments: 1) Verify temporal separation in GTS splits, 2) Compare performance distributions across different target selection methods, 3) Measure correlation between simplified and comprehensive evaluation approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on specific dataset domains (mostly e-commerce) which may not generalize to all recommendation scenarios
- More recent large language model-based sequential recommenders were not included in the evaluation
- The study relies on specific evaluation metrics (HR@10, NDCG@10) that may not capture all aspects of recommendation quality
- Different sequence lengths and user activity patterns were not explored for their impact on split performance

## Confidence
- High confidence: LOO splits enable temporal leakage and unrealistic test horizons
- Medium confidence: GTS Last/Random correlation with Successive evaluation as validation proxy
- Medium confidence: Model ranking variability across splits indicates need for careful protocol selection

## Next Checks
1. Test the correlation between GTS Last/Random and Successive evaluation across a broader range of dataset types, including those with different temporal dynamics and user behaviors
2. Evaluate whether the observed performance differences persist when using additional metrics such as precision, recall, and coverage
3. Assess the impact of sequence length distribution and user activity patterns on the relative performance of different splitting strategies