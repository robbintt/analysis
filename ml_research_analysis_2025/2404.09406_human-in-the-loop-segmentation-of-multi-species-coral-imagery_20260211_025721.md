---
ver: rpa2
title: Human-in-the-Loop Segmentation of Multi-species Coral Imagery
arxiv_id: '2404.09406'
source_url: https://arxiv.org/abs/2404.09406
tags:
- point
- labels
- label
- segmentation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently labeling complex
  underwater coral imagery for semantic segmentation. Coral images are difficult to
  annotate due to overlapping species, intricate textures, and unclear boundaries,
  and labeling is time-consuming for marine scientists.
---

# Human-in-the-Loop Segmentation of Multi-species Coral Imagery

## Quick Facts
- arXiv ID: 2404.09406
- Source URL: https://arxiv.org/abs/2404.09406
- Reference count: 40
- Primary result: Denoised DINOv2 features + KNN improves mIoU by 19.7% over prior work with 5 labels per image

## Executive Summary
This paper tackles the challenge of labeling complex underwater coral imagery for semantic segmentation, where traditional methods struggle due to overlapping species and unclear boundaries. The authors propose a two-stage pipeline that propagates sparse human point labels into dense segmentation masks using denoised foundation model features, achieving significant improvements in annotation efficiency. The method demonstrates that combining denoised DINOv2 features with K-Nearest Neighbors and a human-in-the-loop point selection strategy can generate high-quality training data with as few as 5-25 labels per image.

## Method Summary
The approach uses a two-stage pipeline: Stage 1 extracts denoised DINOv2 features and propagates sparse point labels into dense masks using K-Nearest Neighbors (k=1), while Stage 2 trains a DeepLabv3+ segmentation model on these augmented masks. The human-in-the-loop component guides label placement by maximizing spatial diversity and feature uncertainty, significantly improving annotation efficiency. The method is evaluated on the UCSD Mosaics dataset with 8 coral species plus an "unknown" class, using standard semantic segmentation metrics (mIoU, PA, mPA).

## Key Results
- 19.7% improvement in mIoU over prior state-of-the-art when using 5 human-labeled points per image
- 5.8% improvement in mIoU over prior work without human-in-the-loop labeling for 5 points per image
- 13.5% improvement in mIoU when integrated into DeepLabv3+ pipeline compared to prior approaches for 5 point labels
- Human-in-the-loop labeling consistently outperforms grid and random point placement strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Denoised foundation features provide cleaner semantic boundaries than raw ViT features in low-data regimes.
- **Mechanism:** Raw ViT features contain grid-like artifacts from positional embeddings; denoising removes these artifacts so pixels cluster by semantic similarity rather than spatial proximity.
- **Core assumption:** DINOv2 feature space is sufficiently general to separate coral species without fine-tuning.
- **Evidence:** Visual comparison (Fig. 11) shows artifacts in raw features vs. denoised; Table III quantifies performance gain.

### Mechanism 2
- **Claim:** Active point selection optimizes labeling efficiency by maximizing spatial diversity and feature uncertainty.
- **Mechanism:** System guides annotator to pixels that are spatially distant from existing labels and exhibit low cosine similarity to current label embeddings.
- **Core assumption:** A single labeled pixel can represent the feature distribution of an entire semantic class instance.
- **Evidence:** Table I shows HIL labeling outperforms Grid/Random significantly in sparse (5-25 points) settings.

### Mechanism 3
- **Claim:** k=1 Nearest Neighbor is sufficient for propagation if feature embedding space is well-structured.
- **Mechanism:** In normalized denoised feature space, semantic similarity is encoded as Euclidean/Cosine proximity; propagating the label of the single nearest neighbor effectively segments the image.
- **Core assumption:** Intra-class feature variance is smaller than inter-class feature variance for coral species.
- **Evidence:** Fig. 13 shows k=1 consistently yields highest accuracy, especially for sparse labels.

## Foundational Learning

- **Concept: Vision Transformers & Positional Embeddings**
  - **Why needed here:** Method relies on DINOv2 (a ViT); understanding why "denoising" is needed to remove grid artifacts from positional embeddings.
  - **Quick check question:** Why would a standard ViT feature map show a checkerboard pattern on a constant-color image?

- **Concept: k-Nearest Neighbors in High Dimensions**
  - **Why needed here:** Core propagation logic is geometric lookup in feature space, not neural network prediction.
  - **Quick check question:** If you have 5 labeled points in a 768-dimensional space, how does KNN decide the class of an unlabeled pixel?

- **Concept: Semantic Segmentation vs. Instance Segmentation**
  - **Why needed here:** Paper distinguishes its goal (class per pixel) from models like "Segment Anything" (grouping pixels into objects without class).
  - **Quick check question:** Does a semantic segmentation model output a list of bounding boxes or a dense mask of class probabilities?

## Architecture Onboarding

- **Component map:** Image -> Denoised DINOv2 -> KNN -> Dense Mask -> DeepLabv3+ -> Inference Model
- **Critical path:** Correctness of Stage One propagation depends entirely on the Denoising step; using standard DINOv2 features corrupts KNN clustering with positional artifacts.
- **Design tradeoffs:**
  - HIL vs. Grid: HIL provides ~20% mIoU gain over Grid but requires iterative human presence; Grid is parallelizable, HIL is sequential.
  - Two-Stage Pipeline: Stage 1 creates training data for Stage 2; computationally expensive at training but allows lightweight models (MobileNet) at inference.
- **Failure signatures:**
  - Grid Artifacts: Masks showing checkerboard patterns indicate Denoised DINOv2 was not used correctly.
  - Class Collapse: If 5 points are all on same species, KNN will map entire image to that class.
  - Unknown Domination: In sparse settings, "Unknown" class dominates if points not placed strategically (HIL prevents this).
- **First 3 experiments:**
  1. Extract features from coral image using raw vs. denoised DINOv2; visualize via PCA to confirm artifacts removed.
  2. Run KNN propagation (k=1) with 5 random points vs. 5 grid points; compare mask quality against ground truth.
  3. Vary Î» weight in HIL selector to see if balance between spatial distance and feature uncertainty impacts next point suggestion quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the two-stage pipeline be replaced by a single-stage approach that generates segmentation masks at inference time?
- Basis in paper: Section VI-B states developing a single-stage approach to overcome cross-image feature dissimilarity is left for future work.
- Why unresolved: DINOv2 features encode global image context, making pixels of same class in different images not spatially close in embedding space.
- What evidence would resolve it: A method that aligns feature spaces across images or utilizes a memory bank for zero-shot segmentation on new images.

### Open Question 2
- Question: How does inter-observer variability among marine scientists affect the performance of the proposed human-in-the-loop labeling regime?
- Basis in paper: Section VI-C suggests testing with multiple domain experts to assess interaction and variability.
- Why unresolved: Current results assume perfect label selection based on ground truth; real-world expert annotation introduces noise and subjective boundaries.
- What evidence would resolve it: User study with multiple marine biologists comparing simulated performance against actual labeling times and segmentation accuracies.

### Open Question 3
- Question: Can the method be adapted to detect and segment novel species through active learning during real-time robotic deployment?
- Basis in paper: Section VI-C proposes extending framework to treat human-in-the-loop as active learning for detecting novel species.
- Why unresolved: Current implementation relies on fixed set of classes and static propagation, lacking capability to identify and incorporate unseen classes on the fly.
- What evidence would resolve it: Deployed system that identifies out-of-distribution anomalies, prompts expert for label, and immediately integrates new class.

## Limitations
- Performance depends heavily on quality of denoised DINOv2 features and assumption that single point represents entire species instance
- Human-in-the-loop component introduces sequential bottleneck that may not scale to very large datasets
- Method may degrade with high intra-class variance or significant lighting variations not captured during DINOv2 training

## Confidence
- **High Confidence:** Denoised features improve KNN propagation accuracy (supported by Fig. 11 and Table III)
- **Medium Confidence:** k=1 is optimal for sparse labels (plausible given feature space structure, but limited k-value testing)
- **Low Confidence:** Long-term stability of human-in-the-loop guidance (not addressed; noisy initial labels may cause pipeline failure)

## Next Checks
1. Extract features from coral image using raw vs. denoised DINOv2; visualize via PCA to confirm artifacts removed and semantic clusters form.
2. Run KNN propagation (k=1) with 5 random points vs. 5 grid points; compare visual quality of resulting mask against ground truth to isolate impact of point placement strategy.
3. Simulate human-in-the-loop process with synthetic errors (e.g., 10% mislabeled points) to assess robustness of propagation and downstream segmentation to annotation noise.