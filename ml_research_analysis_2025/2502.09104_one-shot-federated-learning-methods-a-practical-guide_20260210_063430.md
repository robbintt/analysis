---
ver: rpa2
title: 'One-shot Federated Learning Methods: A Practical Guide'
arxiv_id: '2502.09104'
source_url: https://arxiv.org/abs/2502.09104
tags:
- learning
- methods
- data
- federated
- one-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of one-shot federated
  learning (OFL), a distributed machine learning paradigm that limits client-server
  communication to a single round to address privacy and communication overhead issues
  in traditional federated learning. The paper identifies two major challenges in
  OFL: data heterogeneity (non-iid data across clients) and model heterogeneity (different
  model architectures or resources among clients).'
---

# One-shot Federated Learning Methods: A Practical Guide

## Quick Facts
- arXiv ID: 2502.09104
- Source URL: https://arxiv.org/abs/2502.09104
- Reference count: 2
- Primary result: Comprehensive survey categorizing one-shot federated learning techniques into four groups addressing privacy and communication overhead issues

## Executive Summary
This paper provides a comprehensive survey of one-shot federated learning (OFL), a distributed machine learning paradigm that limits client-server communication to a single round to address privacy and communication overhead issues in traditional federated learning. The authors identify two major challenges in OFL: data heterogeneity (non-iid data across clients) and model heterogeneity (different model architectures or resources among clients). They propose a novel taxonomy categorizing current OFL techniques into four main groups: Parameter Learning, Knowledge Distillation, Generative Models, and Ensemble Methods.

## Method Summary
The survey analyzes OFL techniques through a four-category taxonomy, examining parameter learning methods like FedAvg-style aggregation, knowledge distillation approaches for model compression, generative models for synthetic data creation, and ensemble methods for combining client models. The authors evaluate each category's advantages, limitations, and trade-offs, with particular emphasis on how these methods handle the unique challenges of single-round communication. They also discuss hybrid methods that combine multiple techniques and provide insights into future research directions for OFL development.

## Key Results
- Generative models outperform knowledge distillation methods due to direct data approximation rather than compressed model knowledge
- Adaptive ensemble methods with server-side optimization iterations yield better accuracy than static ensembles
- Integrating multiple techniques and performing multiple local updates on the server side can enhance OFL performance
- Prototype learning within parameter learning shows particular promise for handling heterogeneous data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Limiting communication to a single round reduces privacy attack surface and communication overhead while enabling distributed training.
- Mechanism: Clients train local models on private data, then upload parameters/knowledge exactly once. The server aggregates these into a global model without iterative feedback loops. This eliminates repeated exposure of model gradients that could be exploited in multi-round FL attacks.
- Core assumption: Local models capture sufficient statistical information about client data distributions in one training pass to enable meaningful server-side aggregation.
- Evidence anchors:
  - [abstract] "constrains client-server communication to a single round, addressing privacy and communication overhead issues"
  - [section 1] "communication between server and clients is limited to a single round... potentially achiev[ing] even stronger security due to its single-round setting"
  - [corpus] Related work (arXiv:2602.01186) confirms OFL "alleviates these limitations by reducing communication" compared to classical FL's multi-round process
- Break condition: If local datasets are extremely small or highly skewed, single-pass local training may fail to capture representative statistics, causing severe global model degradation.

### Mechanism 2
- Claim: Generative models outperform knowledge distillation for OFL because they directly approximate local data distributions rather than compressed model knowledge.
- Mechanism: Generative approaches (GANs, VAEs, Diffusion models) learn to synthesize data matching client data statistics. The server trains on synthetic data that reflects the union of client distributions, mitigating out-of-distribution (OOD) issues inherent in single-round aggregation.
- Core assumption: Synthetic data quality is sufficient to capture class-conditional distributions across heterogeneous clients.
- Evidence anchors:
  - [section 3.3] "generative models can address the inefficiencies associated with knowledge distillation methods, often leading to superior performance"
  - [section 4.1] "Generative models tend to perform better compared to knowledge distillation methods, primarily because they typically operate directly on the data rather than the model"
  - [corpus] arXiv:2502.08488 demonstrates classifier-free diffusion models for OFL, supporting generative approaches
- Break condition: If generative models produce low-fidelity samples or mode collapse occurs, synthetic data will mislead global model training, worse than direct parameter aggregation.

### Mechanism 3
- Claim: Adaptive ensemble methods with multiple server-side optimization iterations yield better accuracy than static ensembles while maintaining single-upload constraints.
- Mechanism: Rather than simple averaging, adaptive methods (e.g., MoE, weight refinement) iteratively adjust aggregation weights based on sample-level or model-level differences. Methods like Co-Boosting and FuseFL perform multiple local updates on the server using uploaded statistics, maximizing information extraction from the single client upload.
- Core assumption: Server-side computational resources are available for iterative refinement, and uploaded information contains extractable invariant features.
- Evidence anchors:
  - [section 3.4] "adaptive ensemble methods typically yield better results"
  - [section 4.1] "uploading once but updating multiple iterations... a multi-shot approach should be favored over a one-shot approach on the server while keeping the same communication costs"
  - [corpus] Evidence is limited in neighboring papers on adaptive ensembles specifically; most focus on foundational OFL setup
- Break condition: If ensemble weights are mis-calibrated or local models have conflicting feature representations, adaptive methods may amplify errors rather than correct them.

## Foundational Learning

- Concept: **Federated Learning Fundamentals (FedAvg, non-IID data)**
  - Why needed here: OFL inherits FL's distributed optimization framework but removes iterative correction. Understanding how FedAvg handles heterogeneity illuminates why OFL needs different techniques.
  - Quick check question: Can you explain why FedAvg degrades under label skew and how multiple rounds normally compensate?

- Concept: **Knowledge Distillation (teacher-student compression)**
  - Why needed here: Distillation is a core OFL technique for transferring knowledge without raw data. Understanding compression-accuracy tradeoffs is essential for selecting between data vs. model distillation.
  - Quick check question: What information is lost when distilling a large ensemble into a single student model?

- Concept: **Generative Models (GANs, VAEs, Diffusion)**
  - Why needed here: The paper identifies generative models as superior to distillation. Understanding distribution matching and synthetic data quality is critical for implementation.
  - Quick check question: Why might a diffusion model capture data distribution better than a GAN for OFL purposes?

## Architecture Onboarding

- Component map:
  - Client-side: Local model trainer → Statistics extractor (parameters/embeddings/synthetic data generator) → Privacy filter (DP noise/encryption)
  - Communication: Single upload round (parameters, distillates, or generative statistics)
  - Server-side: Aggregator → Global model trainer (potentially with synthetic data) → Optional: Iterative refinement loop (server-side only)
  - Taxonomy decision: Parameter Learning (direct aggregation) | Knowledge Distillation (data/model compression) | Generative Models (synthetic data creation) | Ensemble Methods (static/adaptive combining)

- Critical path:
  1. Characterize heterogeneity type (quantity skew vs. feature skew vs. label skew; model architecture differences)
  2. Select primary technique based on heterogeneity profile and privacy requirements
  3. Design client-side statistics extraction to maximize single-upload information
  4. Implement server-side aggregation with optional refinement iterations
  5. Validate on held-out global test set representing target distribution

- Design tradeoffs:
  - Parameter Learning: Fast, theoretically grounded, but fails on model heterogeneity; weaker privacy
  - Knowledge Distillation: Better privacy, supports heterogeneous models, but compression loses information
  - Generative Models: Highest accuracy potential, handles both heterogeneity types, but computationally expensive and requires good generative quality
  - Ensemble (Static): Simple but suboptimal under heterogeneity
  - Ensemble (Adaptive): Best accuracy, but requires server-side computation and careful weight calibration

- Failure signatures:
  - Global model accuracy significantly below average local model accuracy → Aggregation failing to combine knowledge
  - High variance across client-specific test subsets → Label skew not addressed
  - Global model fails on classes absent from some clients → Open-set/OOD problem (consider FedOV-style outlier generation)
  - Synthetic data visibly unrealistic or mode-collapsed → Generative model insufficient; fall back to parameter learning or distillation

- First 3 experiments:
  1. Baseline comparison: Implement FedAvg-style parameter averaging in single-round setting on CIFAR-10 with Dirichlet-distributed label skew (α=0.1) to establish heterogeneity impact
  2. Technique ablation: Compare static ensemble vs. adaptive ensemble (weight by local data quantity vs. weight by model similarity) to quantify adaptive method gains
  3. Generative validation: Train a simple VAE on client data distributions, generate synthetic samples, measure classifier trained on synthetic vs. real held-out data to assess distribution fidelity before full OFL integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can OFL methods achieve competitive accuracy without relying on additional public datasets, foundation models, or transmitted data labels?
- Basis in paper: [explicit] The authors state: "OFL ideally should be data-free; using additional datasets might lead the global model to learn biased information that does not align with the target dataset."
- Why unresolved: Current methods like FedKT, FedKD, FedBiP, and FedDISC depend on external data sources, yet data-free operation remains underexplored.
- What evidence would resolve it: A data-free OFL method matching or exceeding the accuracy of public-data-dependent baselines on standard benchmarks.

### Open Question 2
- Question: How can OFL methods scale to train or fine-tune large language models (LLMs) across geo-distributed clients?
- Basis in paper: [explicit] "In the future, the FL community could explore OFL methods to enable more practical training of LLMs with different parties."
- Why unresolved: Current OFL experiments focus on small models (LeNet, VGGNet, ResNet) and simple datasets; LLMs introduce massive parameter sizes even for single-round communication.
- What evidence would resolve it: Demonstrating OFL-based LLM fine-tuning with communication costs comparable to one round of parameter exchange and accuracy approaching multi-round FL.

### Open Question 3
- Question: How can adaptive ensemble methods dynamically adjust weights based on individual data samples rather than only model-level differences?
- Basis in paper: [inferred] The authors note adaptive methods like FENS and Co-boosting refine ensemble weights but "still struggle to adjust according to specific data samples" due to data heterogeneity. They highlight IntactOFL's MoE approach as promising but incomplete.
- Why unresolved: Data heterogeneity causes sample-level disparities across clients, requiring per-sample weight adjustments not yet achieved.
- What evidence would resolve it: An adaptive ensemble method demonstrating improved accuracy through instance-specific weighting, validated across heterogeneous data partitions.

### Open Question 4
- Question: What theoretical guarantees can ensure the convergence and privacy of hybrid OFL methods combining multiple techniques?
- Basis in paper: [inferred] The authors observe that "integrating multiple techniques can lead to better results" but note parameter learning provides convergence proofs while hybrid methods lack unified theoretical foundations.
- Why unresolved: Hybrid methods combine distillation, generative models, and ensembles, making theoretical analysis complex.
- What evidence would resolve it: Formal convergence bounds and differential privacy guarantees for a hybrid OFL method combining at least two technique categories.

## Limitations

- Quantitative performance comparisons across OFL techniques under standardized conditions are lacking
- Specific hyperparameter configurations for individual methods remain unspecified, making direct reproduction challenging
- The claimed superiority of generative models over knowledge distillation requires empirical validation across diverse heterogeneity types

## Confidence

- **High Confidence**: The taxonomy structure (four main categories) and identification of core challenges (data/model heterogeneity) are well-supported by the corpus
- **Medium Confidence**: Claims about adaptive ensemble superiority and generative model performance need empirical verification across different data regimes
- **Low Confidence**: Specific superiority rankings between techniques (e.g., "generative models always outperform knowledge distillation") lack rigorous comparative analysis

## Next Checks

1. **Controlled Comparison**: Implement three baseline methods (FedAvg-style parameter learning, basic knowledge distillation, and VAE-based generative approach) on CIFAR-10 with Dirichlet-distributed label skew (α=0.1) to empirically compare performance

2. **Heterogeneity Sensitivity**: Systematically vary non-IID distribution types (quantity skew vs. label skew vs. feature skew) and measure method robustness to identify which techniques excel under specific heterogeneity profiles

3. **Server-Side Computation Tradeoff**: Compare single aggregation iteration vs. multiple refinement iterations on the server while maintaining single client upload to quantify the accuracy-cost tradeoff of adaptive ensemble methods