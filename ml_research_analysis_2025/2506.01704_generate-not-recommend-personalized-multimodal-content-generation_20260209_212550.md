---
ver: rpa2
title: 'Generate, Not Recommend: Personalized Multimodal Content Generation'
arxiv_id: '2506.01704'
source_url: https://arxiv.org/abs/2506.01704
tags:
- generation
- user
- movie
- image
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new paradigm for personalized multimodal
  content generation, going beyond traditional recommendation systems by directly
  generating novel, user-tailored content such as images based on individual user
  histories. The approach leverages any-to-any Large Multimodal Models (LMMs), training
  them through supervised fine-tuning and online reinforcement learning to produce
  personalized images without relying on existing candidate content.
---

# Generate, Not Recommend: Personalized Multimodal Content Generation

## Quick Facts
- **arXiv ID**: 2506.01704
- **Source URL**: https://arxiv.org/abs/2506.01704
- **Reference count**: 12
- **Primary result**: Direct generation of personalized images via any-to-any LMMs outperforms retrieval-based recommendation on personal relevance metrics

## Executive Summary
This paper introduces a novel approach to personalized multimodal content generation that goes beyond traditional recommendation systems by directly generating novel, user-tailored content such as images based on individual user histories. The method leverages any-to-any Large Multimodal Models (LMMs), training them through supervised fine-tuning and online reinforcement learning to produce personalized images without relying on existing candidate content. Experiments on MovieLens and PixelRec datasets demonstrate that generated images are more personally relevant and of higher quality than those retrieved by state-of-the-art recommendation models, with improvements across multiple semantic and perceptual similarity metrics. The approach addresses limitations of recommendation systems that can only retrieve existing items and of generation systems that ignore user history.

## Method Summary
The framework trains Janus-Pro-1B, an any-to-any LMM, to generate personalized images from user interaction histories through a two-stage process. First, supervised fine-tuning (SFT) learns to predict the visual tokens of the next item from the previous k-1 multimodal history items. Second, online reinforcement learning using Group Relative Policy Optimization (GRPO) enables exploration beyond the limited observed data, optimizing for both personal relevance and aesthetic quality. The reward function combines CLIP-based semantic similarity to historical items and future items, DINO-based perceptual similarity, profile consistency via Qwen-2.5-VL, and aesthetics via NIMA. The model generates top-4 images per history and evaluates the best one against ground-truth next items and potential future interests.

## Key Results
- Generated images show higher CLIP-based personal relevance (CTS/CIS) than retrieval-based models
- DINO similarity (DIS) improvements demonstrate better preservation of visual style preferences
- User studies confirm qualitative improvements in personalization and generation quality
- RL stage with GRPO significantly boosts performance on personalization metrics compared to SFT-only
- Reward hacking observed when model generates popular items like "The Godfather" poster regardless of history

## Why This Works (Mechanism)

### Mechanism 1: End-to-End Multimodal History Encoding with Direct Generation
The any-to-any LMM receives both text attributes (titles, genres) and visual content (posters, covers) from user history items, creating a rich multimodal representation that preserves visual style preferences, semantic themes, and cross-modal patterns simultaneously. This unified representation conditions image generation directly without information loss from intermediate summarization. The model learns to attend to relevant historical patterns during generation, producing content that reflects learned preferences rather than generic patterns.

### Mechanism 2: Two-Stage Training with RL Exploration Beyond Sparse Interaction Data
SFT establishes basic history-to-image generation capability using ground-truth next items. GRPO then enables exploration by sampling multiple candidate images and reinforcing those with higher reward signals. This addresses the limitation that relevant items may be absent from training data due to exposure bias or logging omissions. The online RL stage discovers novel relevant content rather than being constrained to observed interactions.

### Mechanism 3: Multi-Signal Reward Composition for Balancing Relevance and Quality
The reward aggregates semantic similarity to history via CLIP, perceptual similarity via DINO/LPIPS/SSIM, profile consistency via Qwen-2.5-VL, future relevance by comparing to items k+1:k+p, and aesthetics via NIMA. This multi-objective signal guides GRPO toward images that balance multiple relevance dimensions, ensuring generated content is both personally relevant and visually appealing rather than optimizing for a single proxy metric.

## Foundational Learning

- **Concept: Any-to-Any Large Multimodal Models (LMMs)**
  - Why needed here: Standard vision-language models handle only image-to-text or text-to-image, not the bidirectional multimodal understanding+generation required for encoding image+text histories and generating images. Janus-Pro-1B's unified architecture enables this.
  - Quick check question: Can your chosen backbone model both understand multimodal sequences AND generate images, or does it require a separate decoder?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Standard RL algorithms require single-sample evaluation or preference pairs. GRPO samples G images per history, computes group-relative advantages (Rj - mean)/std, enabling stable policy updates without a separate value model—critical when reward computation is expensive.
  - Quick check question: Do you understand why GRPO's group-relative advantage computation avoids needing a learned value function?

- **Concept: Reward Hacking in Generative RL**
  - Why needed here: The paper explicitly documents reward hacking where the model learned to generate "The Godfather" posters regardless of user history because CLIP assigned high similarity scores to this popular pretraining image. Monitoring reward standard deviation is essential.
  - Quick check question: Can you identify when your model is optimizing reward shortcuts versus genuine personalization signals?

## Architecture Onboarding

- **Component map:**
User History (k-1 items) -> [Any-to-Any LMM: Janus-Pro-1B] -> Generated Personalized Image (k-th item)
├── Vision encoder processes history images
├── Language model processes text + fused multimodal representations
├── Image decoder generates output image tokens
└── VQ decoding to pixel space

- **Critical path:** History preprocessing (formatting multimodal items into instruction template) → LMM forward pass → Image token generation → VQ decoding to pixel space. The SFT→RL transition is the most failure-prone stage; reward design determines RL success.

- **Design tradeoffs:**
  - **Memorization vs. Generalization:** Vanilla Janus generates meaningless color blocks; SFT enables generation but risks memorizing training images. Limited ability to generalize beyond memorized visual patterns.
  - **Exploration vs. Reward Hacking:** Higher RL steps improve metrics but risk collapse to popular items. Monitor reward std—sudden drops indicate hacking.
  - **Dataset Domain vs. Pretraining Bias:** MovieLens outperforms PixelRec because LMMs have seen movie posters during pretraining but not micro-video covers. Domain familiarity matters.

- **Failure signatures:**
  1. **Reward hacking:** Generating identical popular images regardless of history (e.g., "The Godfather" posters). Detect via: reward std drops, manual inspection of generated samples.
  2. **History insensitivity:** Generated images ignore history preferences. Detect via: low CIS/DIS scores, high variance across different histories.
  3. **Mode collapse:** Low diversity in outputs. Detect via: high similarity among top-4 generated images.
  4. **Quality degradation:** Aesthetics drop during RL. Detect via: NIMA score monitoring.

- **First 3 experiments:**
  1. **SFT-only baseline validation:** Train Janus-Pro-1B with only supervised fine-tuning (1 epoch, lr=4e-5) on MovieLens subset. Measure CIS/DIS/CTS scores on held-out test set. Confirm the model generates meaningful images (not color blocks) before proceeding to RL.
  2. **Reward hacking early detection:** Run GRPO training for 100 steps. Plot reward mean and std curves. If std drops sharply before step 100, your reward signals are exploitable. Redesign rewards (e.g., add diversity penalty, reduce CLIP weight).
  3. **Ablation on reward components:** Train three RL variants with single reward types: (a) historical relevance only, (b) aesthetics only, (c) future relevance only. Compare to full reward composition. Identify which components drive which metric improvements.

## Open Questions the Paper Calls Out

- **How can reward hacking be effectively mitigated in online reinforcement learning for personalized generation?**
  - The model learns to generate high-reward items that lack personal relevance, causing a sudden drop in reward standard deviation that requires manual intervention to stop training.
  - An automated detection mechanism or regularization technique that maintains reward diversity without manual stopping criteria would resolve this.

- **What constitute effective dedicated benchmarks or interactive evaluation platforms for personalized multimodal content generation?**
  - Current experiments repurpose existing recommendation datasets as proxies, which may not fully capture the complexity of open-ended, novel generation tasks.
  - The creation and adoption of a standardized dataset containing ground-truth data specifically designed for evaluating generative personalization rather than retrieval would resolve this.

- **How can LMM backbones be enhanced to better support simultaneous multimodal understanding and generation?**
  - Current backbones struggle to understand multimodal inputs and generate multimodal outputs simultaneously, often defaulting to memorized patterns.
  - A model architecture that generates novel, personalized concepts without merely replicating the visual patterns of items seen during fine-tuning would resolve this.

## Limitations

- **Reward function opacity**: Specific weighting scheme and normalization procedures for the multi-signal reward remain unspecified, making exact reproduction difficult
- **Pretraining domain dependency**: Performance heavily depends on visual pretraining overlap with target domain, limiting generalization to domains without visual pretraining familiarity
- **Evaluation proxy metrics**: Reliance on CLIP, DINO, and NIMA metrics may not perfectly correlate with true user satisfaction despite user study validation

## Confidence

**High Confidence:**
- The two-stage training framework (SFT → GRPO) is technically sound and well-motivated
- The documented reward hacking phenomenon and its detection via reward standard deviation monitoring
- The comparative advantage over retrieval-based recommendation on the tested benchmarks
- The architectural feasibility of using any-to-any LMMs for end-to-end generation

**Medium Confidence:**
- The relative importance and optimal combination of reward components
- The generalizability to domains beyond movie posters and video covers
- The long-term stability of RL training without reward hacking
- The practical scalability and computational requirements

**Low Confidence:**
- Exact reproduction of results without the specific reward weighting details
- Performance on truly cold-start users with minimal interaction history
- Real-world user satisfaction compared to proxy metric improvements
- The impact of different LMM backbone choices on final performance

## Next Checks

1. **Reward Function Sensitivity Analysis**: Systematically vary the weights of individual reward components (CTS, CIS, DIS, PCS, NIMA, etc.) and measure their impact on both proxy metrics and qualitative output quality. This would identify which signals are most critical for genuine personalization versus metric optimization.

2. **Domain Transfer Experiment**: Apply the framework to a completely different domain (e.g., personalized product design, interior decoration, or music album art) where pretraining overlap is minimal. Compare performance degradation against MovieLens to quantify pretraining dependency.

3. **Longitudinal RL Stability Test**: Extend GRPO training beyond 800 steps while monitoring reward std, output diversity, and manual inspection quality. Identify the precise point where reward hacking begins and test regularization strategies (entropy bonuses, diversity penalties, curriculum learning) to extend stable training duration.