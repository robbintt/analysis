---
ver: rpa2
title: Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused
  Preference Optimization
arxiv_id: '2501.17295'
source_url: https://arxiv.org/abs/2501.17295
tags:
- hallucination
- translation
- alma-7b-r
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of translation hallucinations
  in large language model (LLM)-based machine translation systems. The authors propose
  a method that intrinsically mitigates hallucinations during the model training phase,
  rather than relying on post-hoc detection and re-translation.
---

# Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization

## Quick Facts
- arXiv ID: 2501.17295
- Source URL: https://arxiv.org/abs/2501.17295
- Reference count: 40
- One-line primary result: Fine-tuning LLMs on hallucination-focused preference datasets reduces hallucination rates by 96% on average across five language pairs while preserving translation quality.

## Executive Summary
This paper addresses the problem of translation hallucinations in large language model (LLM)-based machine translation systems. The authors propose a method that intrinsically mitigates hallucinations during the model training phase, rather than relying on post-hoc detection and re-translation. Their approach involves creating a hallucination-focused preference dataset by translating monolingual data, detecting hallucinations, and mitigating them using existing post-hoc methods. They then fine-tune the LLM using Contrastive Preference Optimization (CPO) on this dataset. The results show that fine-tuning LLMs on the hallucination-focused preference dataset reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. Additionally, the approach generalizes well in a cross-lingual zero-shot setting, achieving an 89% reduction in hallucination rates across three previously unseen target languages.

## Method Summary
The method involves three key steps: (1) generating a preference dataset by translating monolingual English data with a baseline LLM, detecting hallucinations using BLASER 2.0-QE, and mitigating them through post-hoc strategies (epsilon sampling with LaBSE re-ranking); (2) fine-tuning the LLM using scaled CPO loss (combining NLL and preference loss) with LoRA adapters on the hallucination-focused dataset; and (3) mixing this dataset with the general translation quality dataset to preserve overall translation performance. The approach is evaluated across five language pairs (Czech, German, Icelandic, Russian, Chinese) using COMET scores and hallucination rate metrics.

## Key Results
- Fine-tuning reduces hallucination rate by 96% average across five language pairs
- General translation quality preserved (COMET scores remain at ~81.8)
- Cross-lingual zero-shot generalization achieves 89% hallucination reduction on three unseen languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive Preference Optimization (CPO) with hallucination-focused preference pairs directly reduces the model's likelihood of generating hallucinations during inference.
- **Mechanism:** The CPO loss function combines a Negative Log-Likelihood (NLL) term (encouraging the model to produce the preferred, mitigated translation) and a preference loss term (specifically increasing the probability gap between the preferred and dispreferred, hallucinated translations). This guides the model to explicitly reject its own erroneous outputs.
- **Core assumption:** The model can generalize from the provided contrastive examples to identify and avoid hallucinations in unseen contexts.
- **Evidence anchors:** [abstract] "Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96%". [section] Table 6 ablation shows the full CPO loss (NLL + preference loss) achieves a 0.005% hallucination rate, outperforming NLL-only (0.078%) and preference-only (3.556%) losses.
- **Break condition:** If the preference dataset is of low quality (e.g., includes non-hallucinated translations as "dispreferred"), the model may learn to reject valid outputs, degrading general translation quality.

### Mechanism 2
- **Claim:** A hallucination-focused preference dataset, created by offline post-hoc mitigation, provides the necessary training signal for intrinsic hallucination avoidance.
- **Mechanism:** The framework translates monolingual data with the baseline model to capture its own hallucinations (dispreferred). These are then corrected using a post-hoc mitigation strategy (e.g., re-ranking with LaBSE) to generate hallucination-free alternatives (preferred). This automated pipeline creates a scalable, task-specific dataset without human annotation.
- **Core assumption:** The post-hoc mitigation strategies (Fallback, Candidate Generation/Selection) used to create the "preferred" translations are highly effective at generating hallucination-free examples, thus providing a clean supervision signal.
- **Evidence anchors:** [abstract] "...creating a hallucination-focused preference dataset by translating monolingual data, detecting hallucinations, and mitigating them using existing post-hoc methods." [section] Table 3 shows re-ranking using LaBSE achieving an average mitigation rate of 99.6% on the development set, validating that preferred translations in the dataset are high-quality.
- **Break condition:** The approach will fail if the model's hallucinations are too rare to build a sufficiently large and diverse preference dataset, making the training signal weak and ineffective.

### Mechanism 3
- **Claim:** Mixing the hallucination-focused dataset with a general translation quality dataset preserves overall translation performance while reducing hallucinations.
- **Mechanism:** Training solely on the hallucination dataset (D_train^p) reduces hallucinations but hurts general translation quality (COMET score drops by 1.0 point). Combining it with the original ALMA preference dataset (D_train^alma), which focuses on general quality, provides a balanced training signal that maintains broad translation capabilities while specializing in hallucination avoidance.
- **Core assumption:** The optimization landscape of the two objectives (hallucination avoidance vs. general translation quality) can be navigated without catastrophic forgetting or interference.
- **Evidence anchors:** [abstract] "...reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality." [section] Table 4 shows model M_p (trained only on D_train^p) has COMET 80.8. The combined model M_p+a recovers to 81.6, nearly matching baseline ALMA-7B-R (81.8), while maintaining low hallucination rate.
- **Break condition:** If the two datasets contain conflicting examples (e.g., a translation preferred for general quality in one set but flagged as a hallucination in another), training could become unstable or yield suboptimal results on both fronts.

## Foundational Learning

- **Concept:** Direct Preference Optimization (DPO) / Contrastive Preference Optimization (CPO)
  - **Why needed here:** The core learning algorithm used to fine-tune the LLM. It moves beyond simple supervised fine-tuning by directly optimizing a policy to satisfy preferences, which is key to teaching the model to "reject" its own hallucinations.
  - **Quick check question:** How does CPO differ from standard Supervised Fine-Tuning (SFT) on only the preferred examples?

- **Concept:** Hallucination Detection (BLASER 2.0-QE)
  - **Why needed here:** This is the automatic metric used to label the data. Understanding that it measures cross-lingual semantic similarity is crucial for interpreting the "hallucination score" threshold used to construct the preference dataset.
  - **Quick check question:** What does a BLASER score of 1 versus 5 signify, and how is it converted to a hallucination score?

- **Concept:** Post-hoc Hallucination Mitigation (Re-ranking, Fallback)
  - **Why needed here:** These are the tools used to generate the "preferred" translations in the dataset. Knowing how they work (e.g., generating multiple candidates, using a quality metric) helps understand the quality and potential biases of the training data.
  - **Quick check question:** Why is the "Fallback" strategy (using NLLB-3.3B) less effective than candidate generation strategies in this context?

## Architecture Onboarding

- **Component map:** Data Creation Pipeline (Monolingual Source → LLM Translation → BLASER 2.0-QE Detector → Post-hoc Mitigator → Preference Dataset) → Model Training Pipeline (LLM + CPO Loss) → Fine-tuned LLM
- **Critical path:** The most critical component is the Hallucination Detector (BLASER 2.0-QE). Its accuracy dictates the quality of the preference pairs; false positives will degrade translation quality, and false negatives will reduce hallucination mitigation efficacy.
- **Design tradeoffs:** A key tradeoff is between data quantity and quality. Using a lower hallucination score threshold for the detector yields more training samples but increases noise (false positives). The paper (Table 7) shows that higher quality (stricter threshold) is more important than quantity. Another tradeoff is performance vs. general quality, addressed by mixing datasets.
- **Failure signatures:** A major failure mode would be a significant drop in general translation quality (e.g., COMET scores). This would indicate the model has overfit to the hallucination-focused dataset and forgotten its broader translation competence.
- **First 3 experiments:**
  1. **Baseline & Detector Threshold Calibration:** Establish the hallucination rate of your base LLM and manually validate different thresholds for the chosen hallucination detector to balance precision and recall.
  2. **Post-hoc Mitigation Ablation:** Compare different mitigation strategies (Fallback vs. Re-ranking with LaBSE/COMET) to identify the most effective method for generating high-quality "preferred" translations.
  3. **Ablation on Dataset Mixing:** Train separate models on the hallucination-only dataset, the general-quality dataset, and a mixture of both to quantify the tradeoff between hallucination reduction and translation quality preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed mitigation framework generalize effectively to translation directions beyond English-to-X, such as X-to-English or non-English-centric language pairs?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "In this work we explored only en→X language pairs... We leave the exploration of other directions as a future work."
- **Why unresolved:** Resource and time constraints restricted the experiments to five specific target languages with English as the source.
- **What evidence would resolve it:** Applying the same preference optimization pipeline to X→En and X→Y directions and measuring the resulting hallucination rates.

### Open Question 2
- **Question:** Do the specific source sentences that trigger hallucinations after fine-tuning fail due to underlying data quality issues or modeling limitations?
- **Basis in paper:** [explicit] In Section 6.4, the authors ask: "It would be valuable to further investigate whether the high proportion of source sentences that still result in hallucinations after fine-tuning are due to underlying data quality issues, limitations in the modeling technique, or a combination of both."
- **Why unresolved:** While the authors observed an overlap in source sentences causing hallucinations, they did not isolate the root cause of these persistent failures.
- **What evidence would resolve it:** A detailed error analysis of the remaining hallucinations, controlling for data noise versus model capacity.

### Open Question 3
- **Question:** How sensitive is the method to the choice of hallucination detector and the specific threshold used for data creation?
- **Basis in paper:** [inferred] The Limitations section notes the approach "depends on a hallucination detector" and implies sensitivity by stating "analysis might be required to decide hallucination detector threshold."
- **Why unresolved:** The study relies on a specific detector (BLASER 2.0-QE) and a fixed threshold (0.5), leaving the impact of detector bias on the preference dataset unexplored.
- **What evidence would resolve it:** Experiments utilizing different detection metrics (e.g., COMET-QE) and threshold values to observe the variance in hallucination reduction performance.

## Limitations
- Reliance on post-hoc hallucination mitigation strategies introduces potential biases that aren't fully characterized
- Zero-shot cross-lingual generalization results are based on a limited evaluation set of only three target languages
- Ablation study focuses primarily on loss function components without exploring different hallucination detection thresholds or candidate generation strategies in depth
- Doesn't address potential domain adaptation issues that might arise when applying the fine-tuned model to different translation domains

## Confidence

**High Confidence:** The core experimental results showing 96% average hallucination reduction across five language pairs while maintaining COMET scores are well-supported by the presented data and methodology. The ablation study demonstrating the superiority of the full CPO loss over its components is methodologically sound.

**Medium Confidence:** The zero-shot cross-lingual generalization results (89% hallucination reduction across three unseen languages) are promising but based on a limited evaluation set. The assumption that mixing hallucination-focused and general quality datasets optimally balances the two objectives is supported by evidence but could benefit from more extensive hyperparameter exploration.

**Low Confidence:** The long-term effectiveness of this approach in real-world deployment scenarios, particularly regarding its ability to handle domain shifts and emerging hallucination patterns not present in the training data, remains uncertain without extended validation.

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate the fine-tuned model on multiple translation domains (e.g., medical, legal, conversational) beyond news to assess whether the hallucination mitigation generalizes across different translation contexts and whether domain-specific hallucination patterns emerge.

2. **Adversarial Hallucination Detection:** Create a systematic set of test cases designed to trigger specific hallucination types (entity fabrication, fact hallucination, etc.) to measure the model's robustness against targeted attacks and identify any residual vulnerabilities the training approach didn't address.

3. **Longitudinal Performance Monitoring:** Deploy the fine-tuned model in a real-world translation system for an extended period, monitoring both hallucination rates and general translation quality over time to assess whether the benefits persist and whether new hallucination patterns emerge that weren't captured in the initial training data.