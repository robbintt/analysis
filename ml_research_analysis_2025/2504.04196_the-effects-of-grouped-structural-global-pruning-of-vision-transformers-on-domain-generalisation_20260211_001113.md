---
ver: rpa2
title: The Effects of Grouped Structural Global Pruning of Vision Transformers on
  Domain Generalisation
arxiv_id: '2504.04196'
source_url: https://arxiv.org/abs/2504.04196
tags:
- pruning
- accuracy
- network
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of grouped structural pruning
  on vision transformers (ViT, BeiT, and DeiT) for domain generalisation tasks. The
  method employs dependency graph analysis to identify and prune redundant groups
  of neurons, filters, or attention heads, using metrics like Hessian, Taylor, L1/L2
  norms, and random selection.
---

# The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation

## Quick Facts
- arXiv ID: 2504.04196
- Source URL: https://arxiv.org/abs/2504.04196
- Reference count: 35
- Primary result: Grouped structural pruning of ViT/BeiT/DeiT achieves up to 23× speedup with minimal accuracy loss for domain generalization tasks

## Executive Summary
This paper investigates the impact of grouped structural pruning on vision transformers for domain generalisation tasks. The method employs dependency graph analysis to identify and prune redundant groups of neurons, filters, or attention heads, using metrics like Hessian, Taylor, L1/L2 norms, and random selection. Pruning ratios of 50%, 75%, and 95% were applied, followed by fine-tuning on PACS and Office-Home benchmarks. Results show significant speed improvements (up to 23×) with minimal accuracy losses, especially using the Hessian metric. For instance, BeiT with 50% pruning retained 94.23% accuracy with a 1.81× speedup. The study reveals that structured pruning balances model efficiency and domain generalisation, with Hessian-based pruning excelling at moderate levels and random pruning surprisingly effective at extreme pruning.

## Method Summary
The study uses dependency graph analysis to perform grouped structural global pruning on pre-trained vision transformers (ViT, BeiT, DeiT). The method identifies parameter groups (filters, channels, attention heads) and their inter-dependencies through layer connections, then prunes globally across the entire network based on importance metrics. Five importance metrics were evaluated: Hessian (second-order), Taylor (first-order), L1 norm, L2 norm, and random selection. After pruning, models were fine-tuned for 300 epochs using AdamW optimizer with specific hyperparameters, then evaluated on PACS and Office-Home domain generalisation benchmarks.

## Key Results
- BeiT with 50% pruning retained 94.23% accuracy with 1.81× speedup using Hessian metric
- DeiT with 95% pruning using random selection achieved 64.13% accuracy, outperforming Hessian's 54.41%
- Hessian-based pruning showed minimal accuracy drops (-2.94%, -1.42%, -1.72%) at 50% pruning across all models
- Speed improvements reached up to 23× for highly pruned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dependency graph analysis enables structural pruning without breaking layer-to-layer connections.
- **Mechanism:** The network is represented as a graph G=(V,E) where nodes are filters/channels/attention heads and edges represent dependencies (fully connected layers, skip connections, residual connections). When a node is selected for pruning, all dependent nodes are pruned simultaneously, preserving structural consistency.
- **Core assumption:** Pre-trained Vision Transformers contain redundant parameter groups that can be removed without destroying learned representations.
- **Evidence anchors:**
  - [abstract] "Our method uses dependency graph analysis to identify and remove redundant groups of neurons, weights, filters, or attention heads"
  - [section III.A] "The parameters within each group are inter-dependent because of layer-to-layer connections, therefore they must be pruned simultaneously to maintain the model's structural integrity"
  - [corpus] DepGraph (Fang et al., 2023) provides the foundational dependency-based pruning framework
- **Break condition:** If the dependency graph construction misses inter-layer connections, pruning may create dimension mismatches during inference.

### Mechanism 2
- **Claim:** Hessian-based importance scoring outperforms magnitude-based methods at moderate pruning levels because it captures loss sensitivity directly.
- **Mechanism:** Hessian (second-order) pruning computes curvature information to estimate how each parameter's removal would affect the loss function. This identifies parameters that appear small in magnitude but are actually critical for model output, while flagging large-magnitude parameters that may be redundant.
- **Core assumption:** Parameters with low second-order derivative impact on loss are genuinely redundant, not just under-activated in calibration data.
- **Evidence anchors:**
  - [abstract] "Hessian metric resulted in accuracy drops of only -2.94%, -1.42%, and -1.72%" at 50% pruning
  - [section V] "Hessian-based is better at preserving performance at moderate pruning levels" while "L1-Norm and L2-Norm pruning perform badly at all levels because they rely on magnitude"
  - [corpus] Limited direct comparison—corpus papers focus on latency-aware and unstructured pruning rather than importance metric comparisons
- **Break condition:** At extreme pruning (95%), Hessian's advantage degrades severely (e.g., DeiT: -42.15% accuracy), suggesting curvature information becomes insufficient when most parameters must be removed.

### Mechanism 3
- **Claim:** Random pruning at extreme ratios can outperform sophisticated metrics by avoiding systematic bias in importance estimation.
- **Mechanism:** At 95% pruning, random selection preserves a distributed subset of parameters across all layers, whereas importance-based methods may over-concentrate pruning in specific architectural components. The paper hypothesizes this relates to "grokking"—emergent generalization in compressed models.
- **Core assumption:** Pre-trained weights contain multiple viable sub-networks, and random sampling can accidentally discover performant ones at high sparsity.
- **Evidence anchors:**
  - [section V] "Random pruning is less successful at lower pruning levels however it has surprising resilience at high pruning (95%)"
  - [Table IV] DeiT at 95%: Random achieves 64.13% vs. Hessian's 54.41% and Taylor's 32.02%
  - [corpus] Related work (Týr-the-Pruner, IntraSlice) does not examine random baseline effectiveness at extreme sparsity
- **Break condition:** Random pruning fails catastrophically at moderate levels (50%): DeiT drops -13.58% vs. Hessian's -1.72%.

## Foundational Learning

- **Concept: Domain Generalization vs. Domain Adaptation**
  - **Why needed here:** The evaluation uses DG benchmarks where models must generalize to unseen domains without target-domain training data. Confusing this with domain adaptation (which uses target data) would misinterpret the experimental setup.
  - **Quick check question:** If you had access to target domain samples during training, would the PACS benchmark setup still apply?

- **Concept: Structural vs. Unstructured Pruning**
  - **Why needed here:** Structural pruning removes entire groups (filters, heads) and yields hardware speedups; unstructured pruning creates sparse masks requiring specialized runtime support. This distinction explains why speedup metrics are meaningful here.
  - **Quick check question:** Why does a 50% unstructured pruning ratio not typically yield a 2× speedup on standard hardware?

- **Concept: Dependency Graphs in Computation Graphs**
  - **Why needed here:** Understanding how tensor dimensions propagate through layers (especially residual connections, concatenation) is prerequisite to implementing the grouping logic correctly.
  - **Quick check question:** If you prune a residual branch output, what must also happen to the layer that receives the summed residual?

## Architecture Onboarding

- **Component map:** Dependency Graph Builder -> Importance Scorer -> Global Pruner -> Fine-tuning Loop
- **Critical path:**
  1. Load pre-trained ViT/BeiT/DeiT weights
  2. Build dependency graph from layer connectivity
  3. Compute importance scores for all parameter groups
  4. Sort groups globally, prune bottom-k% by importance (respecting dependencies)
  5. Save pruned architecture + remaining weights
  6. Fine-tune on source domains of PACS/Office-Home
  7. Evaluate on held-out target domain

- **Design tradeoffs:**
  - **Hessian vs. Taylor vs. L1/L2:** Hessian most accurate but computationally expensive; L1/L2 fast but poor accuracy retention
  - **Global vs. Local pruning:** Global (used here) may unevenly prune layers; Local preserves layer balance but may retain redundant layers
  - **Pruning ratio selection:** 50% gives best accuracy-efficiency balance; 75%+ acceptable only for BeiT; 95% requires domain-specific validation

- **Failure signatures:**
  - **Dimension mismatch errors:** Dependency graph missed a connection; check skip/residual paths
  - **Accuracy collapses beyond expected:** Importance metric scores are corrupted or threshold misapplied
  - **No speedup despite pruning:** Pruning unstructured weights instead of structured groups; verify filter/head removal

- **First 3 experiments:**
  1. Replicate Table II entry: Prune ViT-Base at 50% using Hessian, fine-tune on PACS, verify ~2.5× speedup and <3% accuracy drop
  2. Ablation test: Apply same 50% Hessian pruning but use local (per-layer) instead of global threshold—compare accuracy retention
  3. Edge case validation: Test BeiT at 95% with Random vs. Hessian scoring on Office-Home—confirm whether random outperforms on this benchmark too

## Open Questions the Paper Calls Out
- The paper explicitly suggests exploring dynamic training strategies as future work, as the current study only examines static pruning of pre-trained models followed by fine-tuning.

## Limitations
- Hessian-based pruning superiority is demonstrated primarily at 50% pruning, with degraded performance at 95%, suggesting limitations in curvature-based importance estimation under extreme sparsity
- The dependency graph construction is based on architectural analysis but lacks ablation studies showing what happens when connections are missed
- Domain generalisation evaluation assumes test-time domain shift but does not examine whether pruned models generalize better or worse than unpruned models across domains

## Confidence
- **High confidence:** Speed measurements and structural pruning methodology
- **Medium confidence:** Importance metric comparisons at moderate pruning levels
- **Low confidence:** Claimed superiority of Hessian at extreme pruning and unexpected random pruning results at 95%

## Next Checks
1. Replicate the DeiT 95% pruning experiment comparing Hessian, L1, and random methods on Office-Home to confirm whether random truly outperforms importance-based methods
2. Implement ablation test where dependency graph misses a residual connection—measure resulting accuracy drop to quantify sensitivity to graph completeness
3. Evaluate whether pruned models show different domain generalization gaps (source-target accuracy differences) compared to unpruned baselines, testing if pruning affects generalization behavior