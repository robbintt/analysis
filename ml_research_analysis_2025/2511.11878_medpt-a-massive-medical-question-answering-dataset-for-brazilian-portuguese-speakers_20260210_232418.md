---
ver: rpa2
title: 'MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese
  Speakers'
arxiv_id: '2511.11878'
source_url: https://arxiv.org/abs/2511.11878
tags:
- medical
- dataset
- question
- questions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedPT, the first large-scale, real-world
  corpus for Brazilian Portuguese in the medical domain, containing 384,095 authentic
  question-answer pairs from patient-doctor interactions. To address the linguistic
  and cultural gap in existing medical NLP datasets, the authors curated a native
  Portuguese corpus capturing local clinical realities and unique linguistic properties,
  such as the natural asymmetry in patient-doctor communication.
---

# MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers
## Quick Facts
- arXiv ID: 2511.11878
- Source URL: https://arxiv.org/abs/2511.11878
- Reference count: 0
- Primary result: First large-scale, real-world Brazilian Portuguese medical Q&A corpus with 384,095 authentic patient-doctor pairs

## Executive Summary
MedPT introduces the first large-scale, real-world corpus for Brazilian Portuguese in the medical domain, containing 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset addresses the linguistic and cultural gap in existing medical NLP datasets by curating a native Portuguese corpus that captures local clinical realities and unique linguistic properties, such as the natural asymmetry in patient-doctor communication. The authors validated its utility by fine-tuning a 1.7B parameter model on a medical specialty routing task, achieving a 94% F1-score on a 20-class setup.

## Method Summary
The authors curated MedPT from authentic patient-doctor interactions, applying a multi-stage cleaning protocol that included column-specific curation and LLM-driven annotation to classify questions into seven semantic types. The corpus covers over 3,200 medical topics and 104 professional specializations. To validate utility, they fine-tuned a 1.7B parameter model on a medical specialty routing task, achieving a 94% F1-score on a 20-class setup. Qualitative error analysis revealed misclassifications reflected genuine clinical ambiguities, highlighting the dataset's semantic richness.

## Key Results
- 384,095 authentic question-answer pairs from patient-doctor interactions
- Covers over 3,200 medical topics and 104 professional specializations
- 94% F1-score on medical specialty routing task with 1.7B parameter model

## Why This Works (Mechanism)
The dataset's effectiveness stems from its authentic, real-world patient-doctor interactions that capture the natural linguistic asymmetry and cultural context of Brazilian Portuguese medical communication. The multi-stage cleaning protocol ensures data quality while the LLM-driven semantic classification enables practical application in medical specialty routing. The dataset's coverage of 3,200+ topics and 104 specializations provides sufficient diversity for robust model training.

## Foundational Learning
- **Brazilian Portuguese medical terminology** - Why needed: Ensures cultural and linguistic relevance; Quick check: Native speaker validation of medical terms
- **Patient-doctor communication asymmetry** - Why needed: Captures realistic question patterns; Quick check: Compare question lengths and complexity between patients and doctors
- **Medical specialty classification** - Why needed: Enables effective routing; Quick check: Verify specialty assignments match question content

## Architecture Onboarding
**Component map:** Raw patient-doctor interactions -> Cleaning pipeline -> LLM annotation -> Semantic classification -> Model training -> Specialty routing
**Critical path:** Data collection → Cleaning protocol → Semantic annotation → Model fine-tuning → Evaluation
**Design tradeoffs:** Authenticity vs. data quality (real conversations vs. curated), LLM efficiency vs. human annotation accuracy
**Failure signatures:** Misclassifications reflecting clinical ambiguities, underrepresentation of rare specialties, LLM annotation errors
**3 first experiments:**
1. Fine-tune on 10-class subset to verify basic routing capability
2. Human validation of LLM-annotated semantic types
3. Specialty distribution analysis to identify coverage gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Representativeness across Brazil's diverse medical specialties and regional linguistic variations is uncertain
- Limited validation of LLM-driven annotation reliability and potential biases
- Focus on patient-doctor interactions may not capture all medical query types

## Confidence
- Dataset utility for fine-tuning medical NLP models: High
- Dataset generalizability and semantic richness: Medium

## Next Checks
1. Conduct detailed analysis of question distribution across 104 medical specialties to identify underrepresented domains
2. Perform human-in-the-loop validation of LLM-annotated semantic classifications to measure inter-annotator agreement
3. Evaluate dataset performance on downstream medical NLP tasks beyond specialty routing to assess broader utility