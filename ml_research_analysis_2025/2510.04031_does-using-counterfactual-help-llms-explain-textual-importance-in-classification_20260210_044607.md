---
ver: rpa2
title: Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?
arxiv_id: '2510.04031'
source_url: https://arxiv.org/abs/2510.04031
tags:
- review
- words
- movie
- classification
- produce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether incorporating counterfactuals into
  large language model (LLM) reasoning improves their ability to identify the top
  words influencing classification decisions. The researchers introduce a framework
  called the decision-changing rate to quantify word importance and compare three
  approaches: direct prompting, counterfactual-parallel, and counterfactual-sequential.'
---

# Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?

## Quick Facts
- arXiv ID: 2510.04031
- Source URL: https://arxiv.org/abs/2510.04031
- Authors: Nelvin Tan; James Asikin Cheung; Yu-Ching Shih; Dong Yang; Amol Salunkhe
- Reference count: 9
- Primary result: Incorporating counterfactuals improves LLM ability to identify top words influencing classification decisions.

## Executive Summary
This study investigates whether incorporating counterfactuals into LLM reasoning improves their ability to identify the top words influencing classification decisions. The researchers introduce a decision-changing rate (DCR) metric to quantify word importance and compare three approaches: direct prompting, counterfactual-parallel, and counterfactual-sequential. Experiments on Amazon, SST2, and IMDB datasets show that using counterfactuals generally improves identification of important words compared to direct prompting alone, with counterfactual-parallel consistently outperforming or matching direct prompting across all datasets.

## Method Summary
The study compares three approaches for identifying important words in LLM classification: Direct Prompting (DP) where the LLM directly identifies top-k words, Counterfactual-Parallel (CFP) where the LLM generates both original and counterfactual inputs before identifying words, and Counterfactual-Sequential (CFS) where the LLM first identifies words via DP, then refines after seeing the counterfactual. The Decision-Changing Rate (DCR) metric evaluates word importance by masking identified words and testing whether sentiment can be flipped through LLM modifications. The method is tested on 100 random samples each from Amazon (short), SST2 (medium), and IMDB (long) datasets using GPT-4o and LLaMA3-70B models.

## Key Results
- Counterfactual-parallel consistently outperformed or matched direct prompting across all datasets
- Decision-changing rate metric effectively measured how top words influence LLM classification decisions
- Increasing k values generally increased DCR, indicating more opportunities for sentiment flips
- Counterfactual-sequential showed inconsistency, underperforming DP on short Amazon dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual comparison improves word importance identification by providing contrastive reasoning cues.
- Mechanism: When an LLM generates both an original input x and a minimally-modified counterfactual x′ that flips the classification, the contrast between x and x′ highlights which words are decision-critical. The LLM then identifies top-k words by analyzing differences rather than reasoning in isolation.
- Core assumption: LLMs can reliably produce valid counterfactuals (ŷ′ ≠ ŷ) and can use contrastive pairs to isolate important features.
- Evidence anchors: Abstract states the study examines how counterfactuals affect LLM's ability to identify top words; related work shows LLMs can produce counterfactuals with high success rate for few-class problems.

### Mechanism 2
- Claim: Parallel presentation of original and counterfactual (CFP) is more consistent than sequential refinement (CFS).
- Mechanism: CFP presents x and x′ together and asks for top-k words in a single inference step, allowing direct comparison. CFS first generates words via DP, then asks the LLM to refine after seeing x′—introducing potential anchoring bias from initial (possibly incorrect) selections.
- Core assumption: Parallel comparison reduces reasoning complexity compared to sequential self-correction.
- Evidence anchors: Section 4.2 shows CFP consistently outperforms or matches DP, while CFS underperforms DP on Amazon dataset for DCR1.

### Mechanism 3
- Claim: Decision-Changing Rate (DCR) validates word importance by testing whether masked replacement flips classification.
- Mechanism: After identifying top-k words, each is replaced with {MASK}. The LLM fills masks to flip sentiment. If classification flips, score = 1; else 0. Average yields DCR. Higher DCR indicates identified words truly influence decisions.
- Core assumption: If removing/replacing identified words enables sentiment flip, those words were genuinely important.
- Evidence anchors: Section 4.1 introduces DCR as the average of decision-changing scores; increasing k increases DCR since there are more opportunities for sentiment change.

## Foundational Learning

- Concept: **Counterfactual Explanations**
  - Why needed here: Core mechanism for improving word importance identification through contrastive reasoning.
  - Quick check question: Can you explain why a counterfactual must satisfy ŷ′ ≠ ŷ to be valid for this method?

- Concept: **Black-box LLM Explainability**
  - Why needed here: The method assumes no access to weights/gradients; explainability must work via input-output queries only.
  - Quick check question: Why are SHAP/LIME impractical for LLMs in this setting (hint: consider LLM call costs)?

- Concept: **Prompt Engineering for Classification + Explanation**
  - Why needed here: Success depends on carefully structured prompts that constrain output format and prevent off-target modifications.
  - Quick check question: What output format does DP require, and why might this matter for evaluation?

## Architecture Onboarding

- Component map:
  - Input Layer: Raw text review (x)
  - Classification Module: LLM classifies x → ŷ
  - Counterfactual Generator: LLM produces x′ such that classification flips
  - Word Selector: Three paths—DP (direct), CFP (parallel comparison), CFS (sequential refinement)
  - Evaluator (DCR): Masks selected words, requests sentiment-flip fill, reclassifies, scores

- Critical path:
  1. Classify input x
  2. Generate counterfactual x′ (if using CFP/CFS)
  3. Verify ŷ′ ≠ ŷ (fallback to DP if failed)
  4. Extract top-k words via chosen method
  5. Compute DCR for validation

- Design tradeoffs:
  - CFP vs CFS vs DP: CFP most consistent; CFS occasionally better but unreliable; DP cheapest (1 call) but lowest quality
  - k selection: Higher k increases DCR but identifies less precise features; proportion of k to text length matters (short texts: higher proportional impact)
  - LLM choice: Weaker models (LLaMA3-70B) show higher DCR but lower absolute accuracy; stronger models (GPT-4o) more robust but harder to flip

- Failure signatures:
  - Counterfactual generation fails (ŷ′ = ŷ) → falls back to DP
  - LLM modifies non-masked words during DCR evaluation → noisy scores
  - CFS produces worse results than DP (observed on short Amazon dataset)
  - Context length exceeded (IMDB with LLaMA3-70B) → requires manual truncation

- First 3 experiments:
  1. **Baseline DP on single dataset**: Run direct prompting on 100 Amazon reviews with k=1,2,3; measure DCR to establish baseline performance and understand k-sensitivity.
  2. **CFP vs DP comparison**: Implement counterfactual-parallel on same dataset; compare DCR at each k; verify counterfactual success rate (how often ŷ′ ≠ ŷ).
  3. **Cross-dataset validation**: Run CFP on SST2 (medium) and IMDB (long) with appropriate k values; observe how text length and k proportion affect DCR patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the counterfactual-based framework remain effective when extended to multi-class classification tasks?
- Basis in paper: The authors propose reducing multi-class problems to binary ones (e.g., "positive" vs. "not positive") but state, "We leave the exploration of this idea as a future work."
- Why unresolved: The study exclusively validates the proposed approaches on binary sentiment classification datasets.
- What evidence would resolve it: Experimental results applying the decision-changing rate framework to datasets with more than two classes, utilizing the proposed binary reduction strategy.

### Open Question 2
- Question: Is the performance of the counterfactual-parallel approach consistent across a wider variety of LLM architectures and parameter sizes?
- Basis in paper: The Limitations section acknowledges, "we did not exhaustively evaluate a large selections of LLMs," restricting tests to LLaMA3-70B and GPT-4o.
- Why unresolved: It is uncertain if smaller or less capable models can generate sufficiently high-quality counterfactuals to aid in explanation without hallucination or error.
- What evidence would resolve it: Benchmarking the decision-changing rate across a diverse set of open-source models (e.g., 7B or 13B parameter ranges) and different model families.

### Open Question 3
- Question: How can the evaluation metric be refined to strictly isolate the importance of selected words without interference from unintended model edits?
- Basis in paper: The authors note a methodological limitation where "the LLM sometimes (but rarely) replace[s] some other non-mask words," injecting noise into the DCR calculation.
- Why unresolved: The validity of the "decision-changing rate" depends on the LLM modifying *only* the masked words during evaluation, a constraint current prompting fails to enforce perfectly.
- What evidence would resolve it: A modified evaluation protocol using constrained decoding or post-hoc verification to ensure no tokens outside the top-k selection are altered during the sentiment flip step.

## Limitations
- The study only tested binary sentiment classification, leaving multi-class extension as future work
- DCR evaluation can be noisy when LLMs modify non-masked words during sentiment-flip attempts
- Limited evaluation to only two LLM models (LLaMA3-70B and GPT-4o) without broader architectural testing

## Confidence
- **High confidence**: The core finding that CFP consistently outperforms or matches DP across all datasets. The experimental setup is well-defined and results are reproducible.
- **Medium confidence**: The observation that CFP is more consistent than CFS. While supported by results, this requires further validation with additional models and datasets to confirm it's not model-specific.
- **Low confidence**: The claim that higher k always increases DCR. While generally true, the relationship between k and DCR precision isn't thoroughly explored, and the optimal k varies significantly by dataset and text length.

## Next Checks
1. **Failure rate analysis**: Measure and report the frequency of counterfactual generation failures (ŷ′ = ŷ) across all datasets and models to quantify how often the method falls back to DP.
2. **DCR noise quantification**: Implement logging of modified reviews during DCR evaluation to measure the percentage of samples where non-masked words are changed, and assess impact on DCR scores.
3. **Cross-model consistency**: Test the CFP approach with additional LLM architectures (Claude, Gemini) and smaller models (Mistral-7B) to determine if the performance advantage generalizes beyond the tested models.