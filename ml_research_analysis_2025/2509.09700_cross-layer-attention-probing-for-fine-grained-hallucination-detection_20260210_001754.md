---
ver: rpa2
title: Cross-Layer Attention Probing for Fine-Grained Hallucination Detection
arxiv_id: '2509.09700'
source_url: https://arxiv.org/abs/2509.09700
tags:
- responses
- clap
- hallucination
- detection
- sampled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-Layer Attention Probing (CLAP), a novel
  method for detecting hallucinations in large language models by leveraging activations
  across the entire residual stream. CLAP constructs a sequence of layer-wise activations,
  projects them to a lower-dimensional space, and applies a transformer encoder to
  learn discriminative features of hallucinations using a CLS token and linear classifier.
---

# Cross-Layer Attention Probing for Fine-Grained Hallucination Detection

## Quick Facts
- arXiv ID: 2509.09700
- Source URL: https://arxiv.org/abs/2509.09700
- Reference count: 40
- Primary result: CLAP improves hallucination detection accuracy on both greedy and sampled responses, and generalizes better out-of-distribution than single-layer probes.

## Executive Summary
This paper introduces Cross-Layer Attention Probing (CLAP), a novel method for detecting hallucinations in large language models by leveraging activations across the entire residual stream. CLAP constructs a sequence of layer-wise activations, projects them to a lower-dimensional space, and applies a transformer encoder to learn discriminative features of hallucinations using a CLS token and linear classifier. Evaluated across five LLMs and three tasks, CLAP improves hallucination detection over baselines on both greedy and sampled responses. It also enables fine-grained detection among different sampled responses, enhancing hallucination mitigation when combined with strategies like DoLa, reducing both hallucination and abstention rates. Out-of-distribution tests show CLAP generalizes better than probes at individual layers, maintaining high reliability even when prompts fall outside training domains.

## Method Summary
CLAP extracts activations from all decoder layers when generating the last token (EOS) of LLM outputs, projects them to a lower-dimensional space, and processes them through a transformer encoder to detect hallucinations. The method constructs a sequence of layer activations (prepended with a learnable CLS token), applies down-projection to d_model=128, and uses a small transformer encoder to aggregate information before classification. The approach is trained on both greedy and sampled responses, enabling fine-grained detection that can distinguish between hallucinated and non-hallucinated responses to the same prompt. The model is trained with BCE loss using prompt-wise batching for sampled responses, with linear warmup and cosine annealing scheduling.

## Key Results
- CLAP-g (greedy) and CLAP-s (sampled) outperform linear and non-linear baseline probes on hallucination detection AUC across multiple LLMs and datasets.
- CLAP-s achieves fine-grained detection capability, distinguishing between correct and hallucinated sampled responses with 2-3% AUC improvement over baselines.
- Out-of-distribution generalization tests show CLAP consistently outperforms single-layer probes (Last Layer, Most Accurate Layer, etc.) and fixed-layer concatenation methods.
- When combined with DoLa abstention strategy, CLAP reduces hallucination rates by 2-3% while maintaining lower abstention rates compared to other detection methods.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attending across all layers captures task-dependent hallucination signatures better than any single layer.
- **Mechanism:** A transformer encoder learns to weight layer contributions dynamically via self-attention over the sequence of projected layer activations. The CLS token aggregates into a single embedding for classification.
- **Core assumption:** Hallucination-relevant information is distributed unevenly across layers and varies by task/domain.
- **Evidence anchors:**
  - [abstract] "processes the LLM activations across the entire residual stream as a joint sequence"
  - [Section 3.1] describes projecting layer activations and applying transformer encoder with CLS token
  - [Table 5] CLAP outperforms Project+Concat and Maxpool on out-of-distribution transfer
  - [corpus] Related work (ICR Probe, HuDEx) also explores hidden state dynamics across layers, suggesting broader validity of cross-layer signals

### Mechanism 2
- **Claim:** Training on both greedy and sampled responses enables fine-grained discrimination within the sampled space.
- **Mechanism:** Sampling at high temperature creates varied responses per prompt (some hallucinated, some not). Training with per-response labels forces the probe to learn subtle differences rather than just prompt-level uncertainty.
- **Core assumption:** Correct information exists in the residual stream even when greedy decoding fails.
- **Evidence anchors:**
  - [abstract] "enabling fine-grained detection, i.e. the ability to disambiguate hallucinations and non-hallucinations among different sampled responses to a given prompt"
  - [Section 3.2] describes sampling K additional responses and labeling independently
  - [Table 1] CLAP-s outperforms baselines on sampled test responses

### Mechanism 3
- **Claim:** Learned attention over layers improves out-of-distribution generalization versus fixed layer selection.
- **Mechanism:** Rather than committing to a single "best" layer (which may be domain-specific), CLAP adaptively re-weights layers at inference time based on learned attention patterns.
- **Core assumption:** The distribution of relevant signals across layers shifts across domains, but cross-layer patterns remain learnable.
- **Evidence anchors:**
  - [Section 5] describes OOD tests with twenty train-test pairs across five datasets
  - [Table 3] CLAP shows positive % gain over Last Layer, Most Accurate Layer, Most Confident Layer, Majority Voting, and Semantic Entropy Probes
  - [Table 5] CLAP (1.1M params) outperforms Project+Concat (1M params) on TQA→City transfer

## Foundational Learning

- **Concept: Residual Stream in Transformers**
  - Why needed here: CLAP treats layer outputs as a sequence; understanding that residual connections keep activations in a shared representational space is essential.
  - Quick check question: Why can activations from different transformer layers be meaningfully compared or processed together?

- **Concept: Activation Probing (Open-Box Methods)**
  - Why needed here: The paper positions CLAP against prior probes (linear, non-linear, attention-head-based); understanding what probes extract from internals is prerequisite.
  - Quick check question: What distinguishes "open-box" activation probing from "grey-box" uncertainty estimation?

- **Concept: Temperature Sampling and Hallucination Variance**
  - Why needed here: Fine-grained detection exploits that high-temperature sampling yields mixed hallucination/non-hallucination outputs for the same prompt.
  - Quick check question: Why does sampling at temperature > 1 create responses with varying factual correctness?

## Architecture Onboarding

- **Component map:** LLM forward pass → extract L layer activations → project to d_model → [CLS; x'_1; ...; x'_L] → encoder → CLS output → linear classifier → hallucination probability

- **Critical path:** Prompt → LLM forward pass → extract L layer activations (d_LLM each) → project to d_model → [CLS; x'_1; ...; x'_L] → encoder → CLS output → linear classifier → hallucination probability

- **Design tradeoffs:**
  - d_model=128 keeps parameters low (~15K for 2B LLM) but may lose fine-grained signals
  - n_enc=2 slightly improves performance (Table 4, STR task) at cost of compute
  - Training on sampled responses improves fine-grained detection but increases labeling cost
  - Project+Concat matches CLAP in-distribution but underperforms OOD (Table 5)

- **Failure signatures:**
  - Very high hallucination rates in training data may imbalance learning
  - Refusal responses ("I don't know") conflated with hallucinations
  - Large LLMs may incur prohibitive memory if projecting to high d_model

- **First 3 experiments:**
  1. Implement linear probe (LP) and non-linear probe (NLP) on last-layer activations; verify CLAP-g outperforms on greedy responses (target: Alpaca-7B TQA/NQ ~87-89% AUC)
  2. Sample K=10 responses per prompt at T=1.0, top_p=0.95; train CLAP-s and compare to LP-s/NLP-s on sampled test set (target: ~2-3% AUC gain)
  3. Train on TQA, test on Wikidata "city-country" relation; compare CLAP vs Last-Layer and Project+Concat (target: CLAP >57% AUC vs Project+Concat ~56-57%)

## Open Questions the Paper Calls Out
- **Can CLAP maintain performance when scaled to larger LLMs?** The authors note that their ablation study shows hallucinations can still be detected after projecting to lower dimensions, providing evidence for scaling CLAP to larger LLMs - they leave this to future work.

- **What is the role of each layer within CLAP?** While CLAP takes input from all layers, the investigation of the role of each layer within CLAP is left to future work.

- **Is probing the final token sufficient?** The paper probes the activation of the final generated token (EOS) but leaves open whether this is sufficient for detecting hallucinations that occur in the middle of long-form generated sequences.

## Limitations
- Memory constraints for large LLMs with high-dimensional activations may limit practical deployment
- ROUGE-1 cutoffs for labeling introduce potential noise, particularly for tasks where surface-level overlap doesn't capture semantic correctness
- Transformer encoder architecture details (particularly positional encoding) are underspecified, which could affect reproducibility

## Confidence
- **Mechanism 1:** High confidence - OOD generalization results and comparison with Project+Concat provide strong evidence
- **Mechanism 2:** Medium confidence - While Table 1 shows CLAP-s outperforms baselines, fine-grained discrimination needs more detailed analysis
- **Mechanism 3:** High confidence - Consistent performance improvements across multiple OOD transfer pairs support this claim

## Next Checks
1. **Verify positional encoding in layer sequence:** Test whether adding learned positional embeddings to the layer activation sequence improves CLAP performance compared to no positional encoding, particularly for OOD generalization.

2. **Analyze attention weight distributions:** Examine the learned attention weights across layers for different datasets/tasks to confirm that CLAP is indeed learning domain-specific layer importance rather than memorizing fixed patterns.

3. **Stress-test fine-grained detection:** Generate more extreme temperature sampling scenarios (T > 1.5) to verify that CLAP-s maintains discrimination capability when hallucination variance is maximized, and measure actual abstention rates under various confidence thresholds.