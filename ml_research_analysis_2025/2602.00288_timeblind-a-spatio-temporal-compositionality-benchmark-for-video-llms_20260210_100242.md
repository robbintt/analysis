---
ver: rpa2
title: 'TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs'
arxiv_id: '2602.00288'
source_url: https://arxiv.org/abs/2602.00288
tags:
- video
- temporal
- timeblind
- benchmark
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeBlind is a benchmark that tests how well multimodal models
  understand fine-grained temporal dynamics in videos. Unlike existing benchmarks,
  it uses minimal video pairs that differ only in temporal structure, paired with
  complementary questions that prevent models from relying on static or linguistic
  shortcuts.
---

# TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs

## Quick Facts
- arXiv ID: 2602.00288
- Source URL: https://arxiv.org/abs/2602.00288
- Reference count: 29
- 20+ state-of-the-art models tested, including GPT-5 and Gemini 3 Pro, achieved average Instance Accuracy of just 48.2% versus human performance of 98.2%

## Executive Summary
TimeBlind is a benchmark that tests how well multimodal models understand fine-grained temporal dynamics in videos. Unlike existing benchmarks, it uses minimal video pairs that differ only in temporal structure, paired with complementary questions that prevent models from relying on static or linguistic shortcuts. The benchmark is organized around a cognitive taxonomy spanning Events, Event Attributes, and Structural Event Logic, including 11 fine-grained categories.

Evaluations of 20+ state-of-the-art models, including GPT-5 and Gemini 3 Pro, show average Instance Accuracy of just 48.2%—far below human performance of 98.2%—revealing a substantial gap in genuine temporal reasoning despite high standard accuracy scores. The benchmark reveals systematic deficiencies in physics-related event attributes and highlights cases where smaller models outperform much larger ones through effective design choices.

## Method Summary
The benchmark uses minimal video pairs that differ only in temporal structure, paired with complementary questions that prevent models from relying on static or linguistic shortcuts. It employs a cognitive taxonomy spanning Events, Event Attributes, and Structural Event Logic, organized into 11 fine-grained categories. Each video pair contains videos that are nearly identical except for a single temporal manipulation, with questions designed to require understanding of that temporal difference. The evaluation protocol includes Instance Accuracy and a more rigorous Spatio-Temporal Consistency metric that pairs questions about each video in a pair.

## Key Results
- State-of-the-art models including GPT-5 and Gemini 3 Pro achieved only 48.2% average Instance Accuracy on TimeBlind
- Human performance on the benchmark was 98.2%, revealing a substantial gap in genuine temporal reasoning
- Models showed systematic deficiencies in physics-related event attributes (speed, force, magnitude) and struggled particularly with fine-grained temporal compositionality
- Smaller models like Molmo2-8B outperformed much larger models like Qwen3-VL-235B by 5.4%, suggesting effective design choices can overcome scale limitations

## Why This Works (Mechanism)
The benchmark works by isolating temporal reasoning from other video understanding capabilities through minimal video pairs that differ only in temporal structure. This controlled approach prevents models from relying on visual or linguistic shortcuts, forcing them to engage with genuine temporal compositionality. The complementary question design ensures models cannot succeed by memorizing static features or surface patterns.

## Foundational Learning
- **Temporal compositionality**: Understanding how events unfold over time and how their sequence affects meaning. Why needed: Video understanding requires more than recognizing static frames. Quick check: Can the model distinguish between "before" and "after" relationships in event sequences.
- **Minimal pair methodology**: Creating stimuli that differ by only one feature to isolate specific cognitive abilities. Why needed: To ensure models cannot succeed through general visual or linguistic reasoning. Quick check: Are video pairs truly distinguishable only by temporal structure.
- **Cognitive taxonomy of events**: Organizing temporal understanding into hierarchical categories from basic events to complex logical structures. Why needed: Provides systematic framework for evaluating different aspects of temporal reasoning. Quick check: Does the taxonomy cover the full range of temporal reasoning tasks.
- **Spatio-temporal consistency**: Evaluating whether models maintain coherent understanding across temporally related stimuli. Why needed: Tests genuine comprehension rather than isolated correct answers. Quick check: Do model responses to paired questions show logical consistency.
- **Physics-based event attributes**: Understanding physical properties like speed, force, and magnitude in video sequences. Why needed: Critical for real-world reasoning but particularly challenging for current models. Quick check: Can models distinguish "gentle" from "forceful" actions based on temporal dynamics.

## Architecture Onboarding

**Component Map**: Video Encoder -> Temporal Reasoning Module -> Language Decoder -> Question Answer Generation

**Critical Path**: The most important pathway is Video Encoder -> Temporal Reasoning Module, as this determines whether the model can extract and process temporal dynamics from video input.

**Design Tradeoffs**: The benchmark highlights a fundamental tradeoff between model scale and effective temporal reasoning design. Larger models don't automatically achieve better temporal understanding, suggesting architectural choices and training objectives may be more important than parameter count for this specific capability.

**Failure Signatures**: Models show characteristic failures in physics-based event attributes (speed, force, magnitude) and struggle with fine-grained temporal compositionality even when performing well on standard video understanding tasks. These failures persist across different model architectures and scales.

**First 3 Experiments**:
1. Test baseline performance on TimeBlind to establish temporal reasoning capabilities
2. Evaluate model responses on paired questions to assess spatio-temporal consistency
3. Analyze performance across the 11 taxonomy categories to identify specific weakness patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training strategies enable smaller models (e.g., Molmo2-8B) to outperform much larger models in fine-grained temporal understanding?
- Basis in paper: Section 5.1 observes that Molmo2-8B surpasses Qwen3-VL-235B by 5.4%, suggesting "effective design choices" can overcome scale limitations.
- Why unresolved: The paper identifies the phenomenon but does not isolate whether data curation, architecture, or training objectives are the primary drivers.
- What evidence would resolve it: Ablation studies comparing Molmo2's specific training pipeline against standard VideoLLM training on temporal benchmarks.

### Open Question 2
- Question: Does the poor performance on controlled minimal pairs translate directly to failure in diverse, uncontrolled real-world scenarios?
- Basis in paper: The Impact Statement acknowledges the benchmark uses "controlled settings" and encourages expanding evaluation to "more diverse contexts."
- Why unresolved: The diagnostic nature of TimeBlind uses synthetic or curated minimal pairs, leaving the correlation with messy, naturalistic video robustness unproven.
- What evidence would resolve it: Evaluating the same models on a "TimeBlind-Wild" dataset featuring natural videos with similar temporal discriminators.

### Open Question 3
- Question: How can models be improved to perceive physics-related event attributes (speed, force, magnitude) where they currently perform near random chance?
- Basis in paper: Section 5.2 highlights a "systematic deficiency" in Event Attributes (specifically Force and Speed), where models fail to distinguish physical nuances like "gentle vs. forceful."
- Why unresolved: The paper identifies the performance gap but does not propose methods to inject physical dynamics understanding into current architectures.
- What evidence would resolve it: Training models with physics-grounded video datasets or auxiliary losses targeting motion dynamics and observing improvements on the "Event Attributes" subset.

## Limitations
- The benchmark uses controlled minimal video pairs that may not fully capture real-world video understanding complexity
- Performance comparisons assume benchmark questions are equally clear to both humans and models
- The cognitive taxonomy employed has not been independently verified for validity in assessing temporal reasoning
- Results may not generalize to all video domains or naturalistic scenarios

## Confidence

**High**: Benchmark construction and design methodology, reported model performance scores
**Medium**: Human baseline comparison, real-world applicability of results
**Low**: Theoretical framework validity

## Next Checks
1. Conduct inter-rater reliability tests with multiple human annotators on a subset of benchmark questions to verify consistency in human performance measurements
2. Test model performance on temporally augmented versions of the same videos (e.g., with added noise, frame drops, or altered frame rates) to assess robustness
3. Compare performance across different video domains (e.g., human actions, natural phenomena, mechanical processes) to identify potential domain-specific weaknesses in temporal reasoning