---
ver: rpa2
title: 'Read the Scene, Not the Script: Outcome-Aware Safety for LLMs'
arxiv_id: '2510.04320'
source_url: https://arxiv.org/abs/2510.04320
tags:
- risk
- safety
- what
- semantic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental problem in safety-aligned
  large language models: consequence-blindness, where models over-rely on superficial
  semantic cues rather than reasoning about real-world outcomes. To address this,
  the authors introduce a new alignment dataset, CS-Chain-4k, which provides consequence-aware
  supervision for safety alignment.'
---

# Read the Scene, Not the Script: Outcome-Aware Safety for LLMs

## Quick Facts
- arXiv ID: 2510.04320
- Source URL: https://arxiv.org/abs/2510.04320
- Reference count: 40
- Models fine-tuned on CS-Chain-4k show improved robustness against jailbreaks and reduced over-refusal while maintaining performance on other safety benchmarks

## Executive Summary
This paper identifies consequence-blindness as a fundamental problem in safety-aligned LLMs: models over-rely on superficial semantic cues rather than reasoning about real-world outcomes. This leads to both jailbreak vulnerabilities (accepting harmful requests with benign phrasing) and over-refusal (rejecting harmless requests with sensitive keywords). The authors address this by introducing CS-Chain-4k, a consequence-aware alignment dataset that provides explicit supervision for outcome reasoning, and CB-Bench, a benchmark that reveals these failures by testing matched and mismatched semantic/outcome risk cases. Models fine-tuned on CS-Chain-4k show significant improvements: they are more robust against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining performance on other safety and utility benchmarks. Interpretability analyses demonstrate that CS-Chain-4k shifts models' decision focus from background cues to actual consequences.

## Method Summary
The approach constructs CS-Chain-4k by first creating a balanced prompt pool from harmful/adversarial/benign sources, then using Qwen3-275B-Thinking to generate 5 consequence-reasoning CoT responses per prompt. These are filtered by ft-mistral-7b-instruct-v0.2-sorry-bench-202406 to retain safe responses, with 4,000 pairs selected for diversity. Qwen2.5-7B-Instruct is fine-tuned using full-parameter SFT with specific hyperparameters (lr=1e-5, 4 epochs, batch size 16) on these consequence-aware pairs. The CB-Bench evaluation tests models across 4 risk scenarios (Q1-Q4) with LLM-as-a-judge scoring for refusal/helpfulness/harmfulness. Interpretability analyses include linear probing of hidden states to detect outcome risk encoding and gradient-based token attribution to analyze attention shifts between background and question components.

## Key Results
- CS-Chain-4k fine-tuning significantly reduces both jailbreak vulnerability and over-refusal rates on CB-Bench
- Models show improved robustness against semantic-camouflage jailbreaks while maintaining utility on external benchmarks (StrongReject, XSTest, MMLU, HellaSwag)
- Interpretability analysis reveals that consequence-chain models shift attention from background tokens to question tokens during safety decisions
- Linear probes show models internally encode outcome risk independently of semantic risk but fail to route this information to output decisions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Consequence-blindness is the shared root cause of both jailbreak vulnerability and over-refusal.
- **Mechanism:** Current alignment methods supervise at the surface level—labels or rewards on final decisions—rather than modeling downstream effects. This trains models to attend to lexical/stylistic cues (semantic risk) that do not encode actual consequences (outcome risk), creating a systematic bias.
- **Core assumption:** Assumption: Surface-form supervision necessarily decouples safety decisions from consequence reasoning; this is not formally proven but empirically observed across mainstream models.
- **Evidence anchors:**
  - [abstract] "We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on surface-form signals, lexical or stylistic cues that do not encode consequences."
  - [Section 2.1] Defines (s, o) risk pairs where matched (s=o) cases are handled correctly but mismatched (s≠o) cases produce jailbreaks (0,1) or over-refusal (1,0).
  - [corpus] Related work on secondary risks of LLMs (arXiv:2506.12382) notes non-adversarial failures emerge during normal operation, supporting that surface-level alignment misses deeper reasoning gaps.
- **Break condition:** If models receive explicit consequence-level supervision during pretraining or if architecture enforces outcome-grounded decision pathways, the mechanism weakens.

### Mechanism 2
- **Claim:** CS-Chain-4k shifts attention from background tokens to question tokens during safety decisions.
- **Mechanism:** The dataset provides explicit consequence-reasoning chains that train the model to compute outcome risk from the core query rather than react to contextual/semantic signals. Gradient-based token attribution shows consequence-chain models assign higher positive attribution to question components vs. background.
- **Core assumption:** Assumption: The shift in attention patterns causally improves safety decisions; attribution analysis shows correlation but does not definitively prove causation.
- **Evidence anchors:**
  - [abstract] "Interpretability analyses demonstrate that CS-Chain-4k shifts models' decision focus from background cues to actual consequences."
  - [Section 4.3.2] "Only the consequence-chain model assigns significantly higher positive attribution to the question component... base, without-chain, and safe-chain models show more balanced or even negative attribution between background and question."
  - [corpus] No direct corpus evidence for this specific attention-shift mechanism; related work on probing alignment failures (arXiv:2508.08243) focuses on generating failure examples rather than attention dynamics.
- **Break condition:** If consequence-reasoning chains contain spurious correlations or if models learn to pattern-match chain structure without internalizing consequence evaluation, gains may not generalize.

### Mechanism 3
- **Claim:** Models internally encode outcome risk independently of semantic risk but fail to route this information to output decisions.
- **Mechanism:** Linear probes trained on matched cases achieve high accuracy distinguishing outcome risk in middle layers—even for mismatched cases—but accuracy drops in later layers. This indicates representations exist but are not utilized during generation.
- **Core assumption:** Assumption: The accuracy drop in later layers reflects a decision-process failure rather than probe limitation; the probe methodology may not capture all relevant representational structure.
- **Evidence anchors:**
  - [Section 4.3.1] "In the middle layers, the representations become highly accurate, nearly perfectly distinguishing outcome risk even in mismatched cases. However, this accuracy drops again in later layers."
  - [Section 4.3.1] "The probe achieves relatively high accuracy on cases with inconsistent risks, indicating that the model internally encodes the distinction... but this information is not consistently reflected in the model's output decisions."
  - [corpus] Weak corpus support; related work on refusal mechanisms (not in corpus summary) has explored directional representations, but no direct evidence on outcome vs. semantic risk encoding separation.
- **Break condition:** If the probe is learning dataset artifacts rather than genuine outcome representations, the mechanism does not hold.

## Foundational Learning

- **Concept:** Semantic risk vs. outcome risk separation
  - **Why needed here:** The entire framework depends on distinguishing what a request *sounds like* (sensitive keywords, phrasing) from what fulfilling it would *actually cause* (real-world harm). Without this distinction, you cannot construct CB-Bench or interpret results.
  - **Quick check question:** Given "How to kill a Python process?", what is the semantic risk label and what is the outcome risk label?

- **Concept:** Chain-of-Thought (CoT) supervision for safety
  - **Why needed here:** CS-Chain-4k uses consequence-reasoning chains to provide explicit supervision. Understanding how CoT changes model behavior is essential for interpreting why this approach works differently from direct refusal training.
  - **Quick check question:** What is the difference between training a model to refuse harmful requests vs. training it to reason through consequences before deciding?

- **Concept:** Linear probing and representation analysis
  - **Why needed here:** The paper uses probing to show models encode outcome risk but don't use it. Understanding what probes measure—and their limitations—is critical for interpreting these claims.
  - **Quick check question:** If a linear probe achieves 90% accuracy on hidden states for outcome risk classification, what can and cannot you conclude about the model's internal representations?

## Architecture Onboarding

- **Component map:** Prompt pool construction -> CS-Chain-4k generation -> CB-Bench evaluation -> Interpretability layer (probing + attribution)
- **Critical path:**
  1. Construct balanced prompt pool with emphasis on mismatched semantic/outcome risk cases
  2. Generate consequence-reasoning chains with explicit outcome evaluation
  3. Filter for safe responses to ensure training signal quality
  4. Fine-tune with full-parameter SFT (lr=1e-5, 4 epochs, batch size 16)
  5. Evaluate on CB-Bench + external benchmarks (StrongReject, XSTest, MMLU, HellaSwag)
- **Design tradeoffs:**
  - **Dataset size vs. quality:** 4000 samples selected for diversity and filtered quality; larger datasets may introduce noise
  - **Prompting vs. training for consequence focus:** Inference-time "consequence" prompting improves defense but underperforms CS-Chain fine-tuning (Table 1 vs. Table 3)
  - **CoT inclusion:** WithoutChain (no CoT) and SafeChain (safety-focused CoT) both underperform CS-Chain (consequence-focused CoT), suggesting content of reasoning chain matters, not just its presence
- **Failure signatures:**
  - **Jailbreaked (high semantic risk accepted):** Model attends to benign background framing and misses harmful outcome in question component
  - **Over-refusal (low outcome risk rejected):** Model reacts to sensitive keywords in background despite safe actual request
  - **Reasoning models worse:** DeepSeek-R1-Distill and QwQ show higher CB-Scores, suggesting reasoning enhancement may amplify semantic attention without improving consequence evaluation
- **First 3 experiments:**
  1. **Reproduce CB-Bench baseline on your target model:** Generate Q1-Q4 for 5 subtopics, evaluate with LLM-as-judge, compute Jailbreaked, Over-refusal, and CB-Score to establish consequence-blindness presence
  2. **Ablate consequence-reasoning vs. safety-reasoning:** Compare CS-Chain, SafeChain, and WithoutChain fine-tuning on identical prompt pool to isolate what aspect of supervision drives improvement
  3. **Probing replication:** Train linear probes on matched cases at layers 10, 20, 27; evaluate on mismatched cases. Confirm whether your model encodes outcome risk independently and where the accuracy drop occurs

## Open Questions the Paper Calls Out

- **Question:** Can reinforcement learning (RL) with outcome-based reward modeling internalize consequence-aware behavior more effectively than supervised fine-tuning (SFT)?
  - **Basis in paper:** [explicit] The authors explicitly state in Section 7 that a key limitation is the reliance on SFT, and they identify "outcome-based reward modeling" as a necessary next step to "directly optimize for consequence-aware behavior."
  - **Why unresolved:** Designing scalable and reliable reward signals for nuanced safety scenarios remains a major challenge that the current SFT approach does not address.
  - **What evidence would resolve it:** Comparative experiments showing that models trained with outcome-based RL outperform CS-Chain-SFT models on CB-Bench, specifically by reducing the gap between internal risk encoding and refusal behavior.

- **Question:** Does reasoning enhancement inherently increase consequence-blindness by over-weighting surface semantic features?
  - **Basis in paper:** [inferred] Section 3.3 notes that reasoning-enhanced models exhibit universally higher CB-Scores driven by over-refusal, suggesting current reasoning methods may "over-strengthen models' attention to surface semantic features" rather than actual consequences.
  - **Why unresolved:** The paper identifies this "potential defect" but does not determine if this is an artifact of current training data or a fundamental limitation of reasoning architectures.
  - **What evidence would resolve it:** Ablation studies isolating the reasoning component (e.g., removing Chain-of-Thought) to see if over-refusal rates drop, or attention analysis showing disproportionate weight on background tokens ($T_B$) versus question tokens ($T_Q$) in reasoning models.

- **Question:** What are the specific neural circuits that decouple outcome risk encoding in middle layers from safety decisions in later layers?
  - **Basis in paper:** [inferred] Section 4.3.1 reveals that while models accurately encode outcome risk in middle layers, this accuracy drops in later layers, indicating a failure to "consistently reflect [this information] in the model's output decisions."
  - **Why unresolved:** The authors note that current probing is limited to "surface-level correlations" and cannot explain the mechanism of this information loss (Section 7).
  - **What evidence would resolve it:** Causal tracing or activation patching interventions that restore high accuracy in the final layers by forcing the propagation of outcome-risk signals from the middle layers.

## Limitations

- The semantic vs. outcome risk framework may not capture all safety-relevant dimensions
- CB-Bench evaluation depends heavily on LLM-as-a-judge reliability, introducing uncertainty about true model behavior
- The 4,000-sample CS-Chain-4k dataset may not capture the full complexity of real-world consequence reasoning
- Interpretability analyses show correlations but cannot definitively prove causation between attention shifts and safety improvements

## Confidence

- **High Confidence:** The empirical demonstration that mainstream models show both jailbreak vulnerability and over-refusal patterns; the effectiveness of CS-Chain-4k fine-tuning in reducing both failure modes on CB-Bench
- **Medium Confidence:** The claim that consequence-blindness is the shared root cause of jailbreaks and over-refusal; the interpretability evidence showing attention shifts and representation patterns
- **Low Confidence:** The generalizability of findings to models substantially different from Qwen2.5-7B; the long-term stability of fine-tuned safety improvements; whether the semantic/outcome risk framework captures all relevant safety dimensions

## Next Checks

1. **Cross-Model Generalization Test:** Apply CS-Chain-4k fine-tuning to at least three different base models (e.g., Llama, Mistral, and Gemma) and evaluate on CB-Bench. This would validate whether the approach works beyond Qwen2.5-7B and whether improvements stem from general consequence reasoning rather than model-specific patterns.

2. **Adversarial Stress Test with Novel Jailbreaks:** After CS-Chain-4k fine-tuning, test models against entirely new jailbreak patterns not represented in the training data or CB-Bench. This would reveal whether the model has learned general consequence reasoning or simply memorized patterns from the training distribution.

3. **Long-Term Stability and Distribution Shift Analysis:** Fine-tune a model on CS-Chain-4k, then evaluate it immediately and after 1, 2, and 4 weeks of continued pretraining or other fine-tuning tasks. This would reveal whether consequence-aware alignment is stable or degrades when exposed to new data distributions.