---
ver: rpa2
title: 'APCE: Adaptive Progressive Context Expansion for Long Context Processing'
arxiv_id: '2510.12051'
source_url: https://arxiv.org/abs/2510.12051
tags:
- apce
- chunks
- chunk
- baseline
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces APCE, an adaptive method for efficient long-context
  processing in transformers. The key idea is to select the most relevant input chunks
  using semantic similarity matching with the current query, rather than processing
  all tokens.
---

# APCE: Adaptive Progressive Context Expansion for Long Context Processing

## Quick Facts
- arXiv ID: 2510.12051
- Source URL: https://arxiv.org/abs/2510.12051
- Reference count: 40
- Primary result: Achieves comparable or superior summarization performance to dense baseline while using 50-70% of input chunks, with 32.8% memory efficiency improvement

## Executive Summary
This paper introduces APCE, an adaptive method for efficient long-context processing in transformers that addresses quadratic memory growth from self-attention and context length degradation (ContextRot). The key innovation is selecting the most relevant input chunks using semantic similarity matching with the current query, rather than processing all tokens. APCE achieved comparable or superior performance to the full dense baseline on BookSum summarization while using only 50-70% of input chunks, with memory efficiency improvements of 32.8% for KV-cache and 55.6% for prefill attention. The approach is hardware-agnostic and compatible with existing transformer implementations.

## Method Summary
APCE works by partitioning input sequences into chunks (default 800 tokens), projecting each chunk into low-dimensional embeddings using a SentenceTransformer (all-MiniLM-L6-v2), then selecting the top-k chunks most similar to the current query via cosine similarity. During generation, reprioritization occurs every 50 tokens to update chunk selection based on the enhanced query (blending original instruction with recent generation tokens). The method supports asynchronous generation, allowing token generation to start after an initial subset of chunks is loaded, progressively improving quality as more context is acquired. This approach reduces O(N²) attention complexity to O(k²m²) while maintaining task performance.

## Key Results
- With 70% chunk selection, APCE improved BERTScore by 0.4% over dense baseline while maintaining ROUGE-L within 0.5%
- Memory efficiency improvements reached 32.8% for KV-cache and 55.6% for prefill attention
- Asynchronous generation improved time-to-first-token without requiring full prefill completion
- Evaluated on BookSum long-context summarization with Llama-3.2-3B-Instruct model

## Why This Works (Mechanism)

### Mechanism 1: Input-Level Semantic Chunk Selection
- **Claim:** Selecting input chunks via semantic similarity to the query preserves task-relevant information while reducing memory and compute.
- **Mechanism:** During prefill, the input sequence is partitioned into chunks, each projected once into a d=384 embedding via a SentenceTransformer. The query is similarly projected. Cosine similarity selects the top-k chunks before any self-attention.
- **Core assumption:** The semantic similarity between a static chunk embedding and the query embedding is a sufficient proxy for attention-level relevance across all layers.
- **Evidence anchors:** [abstract] "select the most important input chunks through low-dimensional semantic similarity matching with the current query."

### Mechanism 2: Reprioritization via Dynamic Query Enhancement
- **Claim:** Updating the query embedding with recent generation tokens and re-scoring chunk relevance mitigates context drift and recovers previously evicted relevant chunks.
- **Mechanism:** Every R reprioritization interval tokens, the query embedding is enhanced by blending the original instruction with the last N generated tokens. All cached chunk embeddings are re-scored.
- **Core assumption:** Relevance is dynamic; chunks irrelevant at prefill may become critical as generation context evolves.
- **Evidence anchors:** [abstract] "supports reprioritization to update chunk selection."

### Mechanism 3: Asynchronous Generation for TTFT Reduction
- **Claim:** Initiating generation from a sparse, partially-prefilled context reduces time-to-first-token without fully compromising output quality.
- **Mechanism:** After an initial subset of chunks is loaded (e.g., 4 chunks), generation begins. As additional chunks are progressively loaded and attention is updated, output quality improves.
- **Core assumption:** Early tokens can be meaningfully generated with partial context, and quality degradation is acceptable or recoverable.
- **Evidence anchors:** [abstract] "allows for asynchronous generation to improve time-to-first-token."

## Foundational Learning

- **Concept: Self-Attention Quadratic Complexity**
  - **Why needed here:** APCE's core value proposition is reducing O(N²) attention memory/compute to O(k²m²). Understanding baseline complexity clarifies why input-level sparsity matters.
  - **Quick check question:** For a 30k-token sequence with dense attention, what is the attention matrix size? (Answer: 30k × 30k = 900M elements.)

- **Concept: KV-Cache and Its Scaling**
  - **Why needed here:** APCE reduces KV-cache by loading only selected chunks. Understanding KV-cache linear scaling (seq_len × 2 × d_emb) is necessary to interpret the 32.8% efficiency claim.
  - **Quick check question:** If d_KV=1024, seq_len=30k, FP16, what is KV-cache size? (Answer: ~116.89 MB per layer, per Table 4.)

- **Concept: ContextRot / Context Length Degradation**
  - **Why needed here:** The paper frames APCE as mitigating ContextRot—performance degradation as context grows. Understanding this phenomenon contextualizes the motivation.
  - **Quick check question:** Why might a model trained on 4k contexts perform poorly on 30k inputs even with no architectural changes? (Answer: Distribution shift, attention dilution, position embedding extrapolation issues.)

## Architecture Onboarding

- **Component map:** Input → Chunker → Embedder (once, cached) → Selector → Prefill (sparse) → Reprioritization Loop → Async Controller → Output
- **Critical path:** 1. Input → Chunker → Embedder (once, cached). 2. Query embedding → Selector → Prefill (sparse). 3. If async: generate after chunk 4, continue loading. 4. During decode: every R tokens, reprioritize → recompute K/V if needed → continue.
- **Design tradeoffs:** Chunk size (m): 800 tokens optimal; Selection ratio (k): 50-70% balances performance and efficiency; Reprioritization interval (R): 25 tokens outperforms baseline; Recomputation: enabled by default adds overhead but mitigates attention inconsistencies.
- **Failure signatures:** Performance drop spikes at specific chunk sizes (Fig. 4, 30k at 1600 tokens); Total generation time exceeds dense baseline; TTFT gains but quality degrades.
- **First 3 experiments:** 1. Reproduce 30k BookSum with 70% chunk selection, m=800, R=50. 2. Ablate reprioritization interval: R ∈ {1, 5, 10, 25, 50, 100, 200}. 3. Stress-test async generation: vary initial chunk threshold (2, 4, 6).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can APCE be effectively adapted for long-context reasoning or multi-turn dialogue tasks, and what specific mechanisms are required to preserve semantic understanding during input sparsification?
- **Basis in paper:** The authors state they hope findings inspire research for "other relevant long-context tasks" and hypothesize that "another level of sophistication is needed to preserve semantic understanding" beyond summarization.
- **Why unresolved:** The current evaluation is restricted to summarization, which has different dependency requirements than reasoning or dialogue.

### Open Question 2
- **Question:** Does incorporating context-boundary awareness into the semantic similarity matching function improve chunk selection accuracy compared to the current fixed-size approach?
- **Basis in paper:** In "Conclusion and Future Directions," the authors suggest that "More sophisticated implementations can consider context-boundary awareness for semantic similarity matching."
- **Why unresolved:** The current vanilla implementation uses fixed chunk sizes (e.g., 800 tokens), which risks splitting coherent semantic units and degrading the relevance score.

### Open Question 3
- **Question:** To what extent can optimized compute kernels eliminate the current trade-off between reduced Time-to-First-Token (TTFT) and increased total generation time?
- **Basis in paper:** The limitations section notes the "vanilla implementation restricts understanding the full potential... forcing a –potentially unnecessary– trade-off" and suggests "supporting, customized kernel implementations" could improve total generation time.
- **Why unresolved:** The current implementation shows APCE increases total time due to reprioritization overhead, masking potential speedups from reduced attention complexity.

## Limitations
- The paper claims APCE mitigates ContextRot but does not explicitly measure or report ContextRot baselines or its severity
- Semantic similarity selection may break down for tasks requiring fine-grained token-level reasoning or when query semantics diverge significantly from chunk-level embeddings
- Asynchronous generation may introduce hallucinations if early tokens critically depend on missing context

## Confidence
- **High confidence**: Claims about memory efficiency improvements (32.8% KV-cache reduction, 55.6% prefill attention reduction) are directly supported by quantitative measurements
- **Medium confidence**: Claims about comparable or superior summarization performance depend on the semantic similarity assumption holding across diverse scenarios
- **Low confidence**: Claims about mitigating ContextRot are not directly validated - the paper shows APCE performs well on long contexts but does not establish whether or when ContextRot occurs without APCE

## Next Checks
1. **ContextRot validation**: Measure baseline context length degradation by comparing 4k-trained model performance on 4k vs 30k inputs without any APCE intervention
2. **Semantic similarity failure modes**: Design test cases where chunk-level embeddings clearly misrepresent task relevance and measure whether APCE's selection accuracy degrades
3. **Factual accuracy under async generation**: Generate summaries using varying initial chunk thresholds and measure factual consistency scores to quantify hallucination risk