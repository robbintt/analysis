---
ver: rpa2
title: Large Language Models for Oral History Understanding with Text Classification
  and Sentiment Analysis
arxiv_id: '2508.06729'
source_url: https://arxiv.org/abs/2508.06729
tags:
- prompt
- oral
- sentiment
- history
- concise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a scalable framework using large language\
  \ models (LLMs) to automatically annotate Japanese American oral histories for semantic\
  \ classification and sentiment analysis. A high-quality dataset of 558 sentences\
  \ was manually labeled, and four prompt strategies\u2014zero-shot, few-shot, and\
  \ retrieval-augmented generation\u2014were evaluated across three models (ChatGPT,\
  \ Llama, and Qwen)."
---

# Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis

## Quick Facts
- arXiv ID: 2508.06729
- Source URL: https://arxiv.org/abs/2508.06729
- Reference count: 40
- Primary result: LLM framework achieves 88.71% F1 for semantic classification and 82.87% for sentiment analysis on Japanese American oral histories

## Executive Summary
This study introduces a scalable framework using large language models (LLMs) to automatically annotate Japanese American oral histories for semantic classification and sentiment analysis. A high-quality dataset of 558 sentences was manually labeled, and four prompt strategies—zero-shot, few-shot, and retrieval-augmented generation—were evaluated across three models (ChatGPT, Llama, and Qwen). ChatGPT achieved the highest F1 score (88.71%) for semantic classification, while Llama slightly outperformed for sentiment analysis (82.87%). The best prompt configurations were applied to annotate 92,191 sentences from 1,002 interviews. Results show LLMs can effectively preserve contextual nuance and narrative integrity in large-scale, culturally sensitive archival analysis. This framework supports ethical, scalable NLP use in digital humanities and collective memory preservation.

## Method Summary
The framework combines expert-annotated gold-standard data (558 sentences from 15 narrators) with systematic prompt engineering evaluation across three LLMs and four prompting strategies. The approach tests zero-shot, few-shot, and retrieval-augmented generation methods using four sentiment prompt variants and five semantic variants. Performance is measured by F1 score across five repetitions to assess stability. The optimal configurations are then applied to annotate 92,191 sentences from 1,002 interviews. The pipeline includes preprocessing, gold-standard creation, prompt evaluation, model selection, and large-scale annotation.

## Key Results
- ChatGPT achieved highest semantic F1 score of 88.71% with refined few-shot prompt
- Llama achieved highest sentiment F1 score of 82.87% with concise few-shot prompt
- Refined prompt strategy significantly outperformed foundational and comprehensive variants for semantic classification
- Llama demonstrated superior stability with lowest standard deviation across runs

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Prompt Engineering for Nuanced Classification
Carefully designed, concise, few-shot prompts can guide LLMs to perform semantic and sentiment classification on culturally sensitive oral histories with high, stable accuracy. Prompts serve as high-level instructions that align a model's pre-trained capabilities with specific domain requirements. For semantic classification, which requires fine-grained thematic understanding, prompts enriched with domain-specific definitions, keywords, and background context help the model disambiguate categories. For sentiment analysis, which is more general, concise prompts focusing on essential instructions reduce cognitive load and improve generalization.

### Mechanism 2: Model Selection and Prompt Transferability Trade-offs
The optimal choice of LLM depends on the task complexity, with proprietary models excelling in nuanced semantic understanding and open-source models offering competitive, more stable performance for general sentiment tasks. Different LLM architectures and instruction-tuning paradigms respond differently to prompts. A complex, multi-class semantic task (6 categories) leverages the deeper reasoning and broader knowledge base of advanced models like ChatGPT. A simpler, three-class sentiment task is handled comparably well by robust open-source models, which also exhibit lower output variance.

### Mechanism 3: Scalable Human-in-the-Loop Annotation Pipeline
A pipeline combining expert-created gold-standard data, iterative prompt evaluation, and final automated annotation can produce a large-scale, reliable labeled dataset for niche historical archives. A small, expert-annotated dataset (558 sentences) serves as a benchmark for rigorous evaluation. Multiple prompt strategies and models are systematically compared against this benchmark using F1 scores. The best-performing configuration is then deployed to annotate the full corpus (92,191 sentences), transforming an intractable manual task into a scalable computational one.

## Foundational Learning

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - **Why needed here:** The entire framework's performance hinges on understanding these strategies. Zero-shot relies on the model's pre-existing knowledge, while few-shot provides labeled examples in the prompt to guide the model.
  - **Quick check question:** If you have no labeled data for a new oral history archive, which strategy is your only option? If you have a small set of labeled examples, how might you leverage them?

- **Concept: F1 Score as a Harmonic Mean of Precision and Recall**
  - **Why needed here:** The paper uses F1 as the primary metric to compare models and prompts. Understanding it balances false positives and false negatives is crucial for interpreting why certain configurations are selected.
  - **Quick check question:** Why is F1 a better metric than accuracy for this multi-class classification task where some categories (like "Peace and Justice") are much smaller than others?

- **Concept: Sentiment Analysis in Historical and Cultural Context**
  - **Why needed here:** Standard sentiment analysis tools often fail on historical narratives where emotional expression is subdued or culturally specific. This paper addresses this by using LLMs and culturally-aware prompt design.
  - **Quick check question:** Why might a sentence like "We didn't say much about it" be difficult for a generic sentiment model to classify correctly in the context of this paper's corpus?

## Architecture Onboarding

- **Component map:** Data Preprocessing -> Gold-Standard Creation -> Prompt Design & Evaluation Engine -> Model Selector -> Large-Scale Annotator -> Downstream Analysis Modules
- **Critical path:** Secure a small, high-quality, expert-annotated dataset. This is the single most important step. Build the evaluation loop to programmatically test prompt/model combinations against this dataset. Conduct rigorous evaluation runs (e.g., 5 times per configuration) to identify not just the highest average F1, but also the most stable setup.
- **Design tradeoffs:** Token Cost vs. Detail: "Refined" prompts with definitions and background improve semantic accuracy but cost more in tokens. "Concise" prompts are cheaper and effective for sentiment. Proprietary vs. Open-Source Models: ChatGPT offers peak accuracy for complex tasks but with cost and higher variance. Llama/Qwen offer excellent stability and lower cost, suitable for scalable production. Retrieval-Augmented Generation (RAG): Can boost context but introduces noise sensitivity and instability, as seen in the paper's results.
- **Failure signatures:** Semantic Drift: RAG with poorly formulated prompts leads to dramatically low F1 scores (e.g., 13.08% for zero-shot RAG foundational). Unstable Outputs: High standard deviation in F1 across runs indicates the model/prompt combination is unreliable, even if the average score is good. Poor Generalization: A prompt optimized for ChatGPT performs significantly worse when transferred to Llama for semantic classification.
- **First 3 experiments:** Establish Baseline: Using a basic zero-shot prompt, run sentiment and semantic classification on the gold-standard sample with ChatGPT, Llama, and Qwen to establish baseline F1 scores and variances. Prompt Variant Ablation: For one model (e.g., Llama), systematically test the progression of prompt complexity for semantic classification: Foundational → Structured → Comprehensive. Measure the F1 and stability gains at each step to understand which components (definitions, background, keywords) contribute most. Few-Shot Example Analysis: Test the effect of example quality and quantity. For sentiment analysis, compare a few-shot prompt with 2 random examples vs. 2 carefully selected, clear-cut examples. Evaluate the impact on F1 score and consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the LLM-based annotation framework generalize to oral history corpora from diverse cultural, linguistic, and historical backgrounds beyond Japanese American incarceration narratives?
- Basis in paper: [explicit] Future work section states: "we plan to evaluate the framework on additional oral history corpora from diverse cultural, linguistic, and historical backgrounds. This will allow us to test the adaptability of our methods and improve generalizability across underrepresented narratives."
- Why unresolved: The current study validated the framework only on the JAIOH corpus, which has specific historical and cultural specificity. Different narrative styles, linguistic features, or cultural contexts may affect performance.
- What evidence would resolve it: Systematic evaluation of the same annotation pipeline on multiple oral history collections from different cultural contexts (e.g., Holocaust testimonies, Indigenous oral histories, refugee narratives) with comparable benchmark metrics.

### Open Question 2
- Question: Can automated prompt optimization techniques outperform human-crafted prompts for semantic classification in culturally sensitive domains?
- Basis in paper: [explicit] Future work section states: "Future efforts will explore automated prompt optimization techniques, retrieval-augmented prompting, and hybrid pipelines that incorporate human feedback to improve annotation precision."
- Why unresolved: The current study relied on iterative human refinement of prompts with LLM assistance. The relative effectiveness of fully automated prompt optimization remains untested for this domain.
- What evidence would resolve it: Comparative experiments pitting automated prompt optimization methods (e.g., gradient-free prompt tuning, evolutionary prompt search) against the human-crafted refined prompts on the same oral history classification tasks.

### Open Question 3
- Question: How does LLM annotation accuracy vary across narrative subtypes and speaker demographics within oral history collections?
- Basis in paper: [inferred] The paper aggregates F1 scores across all 558 sentences but does not analyze whether performance differs based on narrator characteristics (e.g., age, gender, generation), narrative complexity, or emotional intensity. Section 4.3.1 notes that "High performing models still risk errors when emotional tone is implicit or culturally nuanced."
- Why unresolved: The evaluation reports aggregate performance metrics without disaggregating by speaker or narrative characteristics that may systematically affect annotation quality.
- What evidence would resolve it: Stratified evaluation reporting F1 scores by narrator demographics, sentence length, sentiment intensity, or thematic category density to identify systematic performance variations.

## Limitations
- Limited generalizability to oral histories from different cultural or linguistic contexts without significant prompt adaptation
- Evaluation dataset represents only 558 sentences from 15 narrators, which may not capture full diversity of narrative styles
- RAG implementation details are incompletely specified, making it difficult to reproduce mixed results observed
- Does not address potential biases in LLM outputs or ethical implications of automated annotation for culturally sensitive materials

## Confidence

- **High Confidence:** The core finding that well-designed few-shot prompts significantly outperform zero-shot approaches for both semantic and sentiment classification tasks. The systematic evaluation methodology and reproducible results across multiple models support this claim.
- **Medium Confidence:** The assertion that Llama offers superior stability and cost-effectiveness compared to ChatGPT for sentiment analysis, and that ChatGPT excels at nuanced semantic classification. While supported by the data, these conclusions are based on a single dataset and may vary with different content or model versions.
- **Low Confidence:** The scalability claim that the framework can maintain consistent annotation quality across the full 92,191-sentence corpus without drift or degradation over time. This was not directly validated through longitudinal testing or cross-validation with the full dataset.

## Next Checks

1. **Cross-Domain Transferability Test:** Apply the best-performing prompt configurations to a different oral history collection (e.g., from another cultural or historical context) and measure performance degradation or adaptation requirements.
2. **Full Corpus Consistency Analysis:** Implement a sampling-based validation where the automated annotations on the full 92,191 sentences are randomly spot-checked against expert review to quantify any drift or systematic errors that may have emerged during large-scale application.
3. **RAG Implementation Replication:** Recreate the retrieval-augmented generation setup with full specification of the embedding model, retrieval parameters, and context formatting to isolate whether performance variations were due to implementation choices or inherent limitations of the RAG approach for this task.