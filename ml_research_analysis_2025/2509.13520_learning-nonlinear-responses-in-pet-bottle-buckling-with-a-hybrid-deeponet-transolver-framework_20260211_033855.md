---
ver: rpa2
title: Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver
  Framework
arxiv_id: '2509.13520'
source_url: https://arxiv.org/abs/2509.13520
tags:
- bottle
- design
- neural
- learning
- designs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid DeepONet-Transolver framework for
  learning the mechanical responses of PET bottle buckling under top compression.
  The framework addresses the computational cost of traditional FEA by learning a
  surrogate that predicts both nodal displacement fields and time-dependent reaction
  forces across varying bottle geometries.
---

# Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework

## Quick Facts
- arXiv ID: 2509.13520
- Source URL: https://arxiv.org/abs/2509.13520
- Reference count: 38
- Hybrid DeepONet-Transolver framework achieves 2.5-13% mean relative L2 errors for displacement fields and ~2.4% for reaction forces in PET bottle buckling simulations.

## Executive Summary
This study presents a hybrid DeepONet-Transolver framework for learning mechanical responses of PET bottle buckling under top compression. The framework addresses computational costs of traditional FEA by learning a surrogate that predicts nodal displacement fields and time-dependent reaction forces across varying bottle geometries. The method integrates Transolver for spatial displacement prediction with DeepONet for temporal reaction force prediction, using geometry features from Transolver as input to DeepONet's branch network. Evaluated on two-parameter and four-parameter bottle design families, the framework demonstrates mean relative L2 errors of 2.5-13% for displacement fields and approximately 2.4% for reaction forces in the four-parameter family.

## Method Summary
The framework combines Transolver, a geometry-aware transformer architecture, with DeepONet to predict both spatial displacement fields and temporal reaction forces. Transolver uses physics-driven token slicing to group mesh nodes by physical similarity, enabling geometry-aware feature learning. The output embeddings are max-pooled to create geometry-specific representations that serve as input to DeepONet's branch network, while the trunk network receives time coordinates. Both networks share Transolver's feature extraction pathway and are optimized simultaneously using L1 loss. The method was evaluated on 228 training samples (90-10% train-test split) from two-parameter and four-parameter PET bottle design families, with mesh representations containing approximately 20,000 nodes.

## Key Results
- Mean relative L2 errors of 2.5-13% for displacement fields (ux, uy, uz) across test geometries
- Reaction force prediction accuracy of approximately 2.4% mean error for four-parameter designs
- Point-wise absolute displacement errors on the order of 10^-4 to 10^-3, with larger errors localized to bottle neck and rib-surface transitions
- Model successfully captures buckling behavior including gradient reversal in reaction force curves
- Four-parameter designs show 2-10× higher uz errors compared to two-parameter designs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Physically-consistent token slicing enables geometry-aware feature learning across non-Euclidean domains.
- **Mechanism:** Transolver groups mesh nodes into S latent "slices" based on proximity and feature similarity via softmax-weighted aggregation. Token embeddings represent weighted contributions from nodal points with similar physical features, allowing the model to learn correlations between regions under similar physical states.
- **Core assumption:** Points with similar output values can be grouped into consistent latent slices that align during backpropagation via dot-product attention.
- **Evidence anchors:** Section 4.1 describes the slicing approach; R² values exceeding 0.99 across displacement components indicate the surrogate captures significant variance in target fields.
- **Break condition:** If geometric variations produce fundamentally different physical response patterns not represented in training slices, token alignment fails.

### Mechanism 2
- **Claim:** Latent geometry embeddings from Transolver provide sufficient functional representations for DeepONet's branch network to predict time-dependent quantities.
- **Mechanism:** Transolver's output embedding undergoes max pooling across nodal points to create a geometry-specific function representation that serves as input to DeepONet's branch network, while the trunk network receives time coordinates.
- **Core assumption:** Max-pooled latent embeddings capture sufficient geometry-specific information to encode the functional mapping to temporal outputs without explicit geometric parameters.
- **Evidence anchors:** Section 4.3 describes the max pooling approach; Table 1 shows reaction force mean error of ~0.30% (2-parameter) and ~2.4% (4-parameter).
- **Break condition:** If geometry-to-temporal-response mapping requires information lost during max pooling, the DeepONet branch lacks sufficient input representation.

### Mechanism 3
- **Claim:** Concurrent multi-task training with shared feature extraction improves generalization across spatial and temporal predictions.
- **Mechanism:** Both networks share Transolver's feature extraction pathway and are optimized simultaneously via combined L1 loss. The displacement prediction task provides supervised signals that align token embeddings, which simultaneously inform the DeepONet branch's geometry representation for reaction force prediction.
- **Core assumption:** Spatial displacement fields and temporal reaction forces share underlying physical dependencies that benefit from joint representation learning.
- **Evidence anchors:** Section 5.1 describes simultaneous optimization; Section 5.2 shows the model captures buckling events despite not being explicitly trained on buckling detection.
- **Break condition:** If spatial and temporal outputs require conflicting feature representations, shared training degrades both tasks.

## Foundational Learning

- **Concept: Operator Learning and DeepONet Architecture**
  - **Why needed here:** The framework treats PDE solution mapping as learning operators between function spaces rather than discretized inputs/outputs. DeepONet's branch-trunk decomposition enables discretization-invariant predictions.
  - **Quick check question:** Can you explain why DeepONet's output is computed as a dot product between branch and trunk embeddings, and what property this enables for mesh refinement?

- **Concept: Transformer Attention for Physical Systems**
  - **Why needed here:** Transolver adapts self-attention to group mesh points by physical similarity rather than spatial adjacency. Understanding attention weights as slice assignments is critical for interpreting model behavior.
  - **Quick check question:** How does the softmax operation along the slice dimension differ from standard attention, and what does w_hjs represent physically?

- **Concept: Nonlinear FEA and Buckling Response**
  - **Why needed here:** The surrogate learns from displacement-driven simulations that capture post-buckling softening. Understanding why displacement (not force) boundary conditions are used clarifies the training data characteristics.
  - **Quick check question:** Why does prescribing displacement ensure complete load-displacement curves including peak reaction force and post-buckling behavior?

## Architecture Onboarding

- **Component map:** Input geometry (x_j, n_j) ∈ R^6 → Transolver (8 heads, 128 hidden, 4 blocks, 32 slices) → y* embedding → [fork] → de-slicing for displacements AND max-pool → DeepONet branch → dot product with trunk → reaction force

- **Critical path:** Input geometry → Transolver attention blocks (4 stacked) → y* embedding → [fork] → de-slicing for displacements AND max-pool → DeepONet branch → dot product with trunk → reaction force

- **Design tradeoffs:**
  - L1 loss chosen heuristically over L2 for better prediction results (not rigorously compared)
  - 90-10 train-test split; 80-20 gave 2-3% higher error, suggesting data scarcity sensitivity
  - Batch size of 1 limits training speed but may improve stability for variable geometry sizes
  - 32 slices balance granularity vs computational cost; no ablation study provided

- **Failure signatures:**
  - Max errors ~0.67-0.71 for uy displacement in outliers (Table 1) indicate extrapolation to unseen response patterns
  - Point-wise errors concentrated at bottle neck and rib-surface transitions (Figure 5)
  - Four-parameter designs show 2-10× higher uz errors vs two-parameter, suggesting design space coverage issues
  - Under-represented regions in D ∈ R^4 produce higher variance in test errors

- **First 3 experiments:**
  1. **Baseline validation:** Train Transolver-only model for displacement prediction; verify L2 errors align with ~2.5-13% range before adding DeepONet complexity.
  2. **Ablation on pooling strategy:** Compare max pooling vs mean pooling vs learned aggregation for y* → DeepONet branch input; measure reaction force error sensitivity.
  3. **Design space coverage test:** Train on 150 samples (vs 228) and evaluate error increase to quantify data efficiency; check if Adaptive DOE sampling strategy provides measurable benefit over uniform sampling in 2-parameter case.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the hybrid DeepONet-Transolver framework maintain accuracy (e.g., <15% mean relative L2 error) when scaling to industrial-scale bottle designs with 7 or more geometric parameters?
- **Basis in paper:** The authors state: "Our preliminary experiments with seven-parameter bottle designs with a smaller sample size revealed that the model's accuracy decreases notably under such conditions. Hence, extending this methodology to handle higher-dimensional parametric representations is necessary."
- **Why unresolved:** Only 2-parameter and 4-parameter design families were evaluated; 7-parameter experiments showed degraded performance but were not systematically investigated.
- **What evidence would resolve it:** Systematic evaluation on 7+ parameter bottle families with varying sample sizes, reporting mean L2 errors for displacement and reaction forces.

### Open Question 2
- **Question:** What training sample selection strategies minimize prediction error in sparse, high-dimensional design spaces for non-parametric geometries?
- **Basis in paper:** "Selecting training cases from sparser or more diverse regions of the design space can improve the effectiveness of such surrogates. However, this strategy is less feasible in non-parametric settings, where the design space is unknown or difficult to characterize."
- **Why unresolved:** Random sampling was used in this study; no systematic comparison of active learning or adaptive sampling strategies for non-parametric geometry-aware surrogates was conducted.
- **What evidence would resolve it:** Comparative study of sampling strategies (random vs. adaptive vs. diversity-based) on test error distributions across varying design space sparsity levels.

### Open Question 3
- **Question:** What architectural modifications or physics-informed constraints can reduce localized prediction errors in regions with strong nonlinear deformation (e.g., bottle neck, ribbed transition zones)?
- **Basis in paper:** The authors report point-wise absolute errors of 10^-4 to 10^-3 concentrated in "specific regions rather than uniformly spread," particularly near the bottle neck and ribbed regions where strong nonlinear deformation occurs during buckling.
- **Why unresolved:** The framework treats all nodal points equally in the loss function; no targeted mechanisms address high-deformation regions specifically.
- **What evidence would resolve it:** Ablation studies incorporating spatially-weighted loss functions or attention mechanisms biased toward high-strain regions, with point-wise error analysis comparing localization patterns.

## Limitations

- Data scarcity: Only 228 training samples for complex 4-parameter designs may limit generalization to extreme or novel geometries
- Lack of architectural optimization: L1 loss choice and hyperparameters (32 slices, batch size 1) were not rigorously optimized through ablation studies
- Physical interpretability gaps: Token slicing mechanism's physical consistency lacks direct empirical validation for heterogeneous material responses

## Confidence

- **High confidence**: Operator learning framework and basic DeepONet architecture (established literature); displacement prediction accuracy (R² > 0.99); correct implementation of physics-driven simulation setup.
- **Medium confidence**: Hybrid Transolver-DeepONet integration effectiveness; multi-task learning benefits; reaction force prediction accuracy (~2.4% error); point-wise error localization patterns.
- **Low confidence**: Specific token slicing mechanism physical interpretability; generalization to design spaces beyond training coverage; optimal architectural hyperparameters; scalability to more complex geometries or materials.

## Next Checks

1. **Design space extrapolation test**: Systematically evaluate model performance on geometrically extreme samples (e.g., rtop=35mm, drib=10mm combinations) to quantify extrapolation limits and identify failure regions.

2. **Attention mechanism interpretability**: Visualize and analyze the softmax-weighted slice assignments (w_hjs) across different geometries to verify physical consistency and identify when token alignment breaks down.

3. **Architectural ablation study**: Compare max-pooling vs alternative aggregation strategies (mean pooling, learned attention) for Transolver-to-DeepONet feature transfer, measuring impact on reaction force prediction accuracy.