---
ver: rpa2
title: 'TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs'
arxiv_id: '2510.06878'
source_url: https://arxiv.org/abs/2510.06878
tags:
- code
- refinement
- training
- tree
- tgpr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of effective exploration in iterative
  code refinement for large language models (LLMs). While iterative refinement has
  proven promising for resolving complex programming tasks, existing methods struggle
  with the exploration-exploitation dilemma due to reliance on fixed heuristics or
  inefficient on-policy exploration.
---

# TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs

## Quick Facts
- **arXiv ID:** 2510.06878
- **Source URL:** https://arxiv.org/abs/2510.06878
- **Reference count:** 10
- **Primary result:** TGPR achieves up to +4.2 percentage points absolute improvement in pass@1 and up to +12.51 percentage points absolute improvement in pass@10 compared to a competitive GRPO baseline on HumanEval, MBPP, and APPS benchmarks.

## Executive Summary
The paper addresses the challenge of effective exploration in iterative code refinement for large language models (LLMs). While iterative refinement has proven promising for resolving complex programming tasks, existing methods struggle with the exploration-exploitation dilemma due to reliance on fixed heuristics or inefficient on-policy exploration. The authors introduce Tree-Guided Policy Refinement (TGPR), a novel framework that integrates Thompson Sampling-guided tree search as a training-time data augmentation engine with Group Relative Policy Optimization (GRPO). This combination enables strategic exploration of both successful and failed refinement paths, generating high-quality and diverse training trajectories. On benchmarks including HumanEval, MBPP, and APPS, TGPR achieves significant improvements over a competitive GRPO baseline, demonstrating a principled approach to combining learned policies with structured search.

## Method Summary
TGPR combines Thompson Sampling-guided tree search with GRPO to improve iterative code refinement. The method uses a Qwen-7B policy model to generate code refinements and a fine-tuned Qwen-7B reward model to evaluate trajectories using a hybrid reward combining CodeBLEU and test pass ratios. During training, an iterative refinement tree expands via Thompson Sampling-based node selection, generating diverse trajectories from both successful and failed refinement paths. These trajectories train the policy through GRPO without requiring a value function. The tree search operates only during training, not inference, allowing the learned policy to internalize exploration patterns while avoiding computational overhead at test time. The system runs on 256 parallel environments with 8 rollouts per task across 128 tasks over 5 epochs.

## Key Results
- Up to +4.2 percentage points absolute improvement in pass@1 on HumanEval, MBPP, and APPS benchmarks
- Up to +12.51 percentage points absolute improvement in pass@10 on APPS benchmark
- Significant performance gains demonstrate effectiveness of training-time exploration for learned policies

## Why This Works (Mechanism)

### Mechanism 1
Thompson Sampling-guided tree search provides more effective exploration than on-policy sampling alone. Each program node maintains Beta distribution parameters (α_ρ, β_ρ) updated by reward R(ρ) and failed refinement count N_ρ. Selection samples θ_ρ ~ Beta(α_ρ, β_ρ) and picks max, dynamically balancing uncertain paths vs. promising ones. The hybrid reward R(ρ) = CodeBLEU + |T_passed|/|T| correlates meaningfully with eventual correctness. TGPR achieves +12.51 pp pass@10 on APPS vs. GRPO, suggesting exploration gains compound at higher k. If reward model correlates poorly with ground-truth, tree guidance becomes noisy; if N_ρ grows unbounded without reset, exploration collapses to exploitation.

### Mechanism 2
Using tree search exclusively at training-time transfers strategic exploration patterns to the policy. Tree generates diverse trajectories (successes + informative failures) during rollouts. GRPO policy internalizes selection patterns, enabling single-shot confident decisions at test time without search overhead. The policy can generalize exploration heuristics from training trajectories to unseen problems. TGPR achieves significant gains while restricting tree search to training, avoiding inference-time costs. If distribution shift between training and test problems is large, learned exploration patterns may not transfer; if trajectory diversity is insufficient, policy overfits to narrow refinement paths.

### Mechanism 3
Dense hybrid reward enables finer-grained credit assignment than sparse pass/fail. CodeBLEU provides continuous semantic similarity signal; test pass ratio gives partial credit. Together they create smoother gradient signal for both reward model training and GRPO advantage computation. Partial correctness (some tests passing, high CodeBLEU) is predictive of refinability to full correctness. Custom reward shows stable convergence with higher average rewards than full set. If CodeBLEU rewards syntactic similarity that doesn't correlate with semantic fixability, reward hacking occurs; if test cases are non-representative, partial credit misleads.

## Foundational Learning

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** Core RL algorithm; computes advantages via group-normalized rewards without value function.
  - **Quick check question:** Can you explain how GRPO differs from PPO in advantage estimation?

- **Concept:** Thompson Sampling for bandits
  - **Why needed here:** Underlies tree node selection; maintains posterior over arm quality.
  - **Quick check question:** How does Thompson Sampling differ from UCB in handling uncertainty?

- **Concept:** MDP formulation for multi-turn code refinement
  - **Why needed here:** Frames debugging as sequential decision process with states (code + feedback) and actions (modifications).
  - **Quick check question:** What constitutes a "state" vs. an "action" in the code debugging MDP?

## Architecture Onboarding

- **Component map:** Policy Model (Qwen-7B) -> Reward Model (fine-tuned Qwen-7B) -> Iterative Refinement Tree -> GRPO Optimizer -> Parallel Environments
- **Critical path:** 1) Sample task → Policy generates initial code → Add to tree root 2) Tree expands via refinements; Thompson Sampling selects nodes 3) Reward model scores trajectories → Compute advantages → GRPO update 4) Repeat across 5 epochs, 128 tasks/epoch
- **Design tradeoffs:** Tree depth vs. compute (deeper trees explore more but increase training time); Temperature 1.0 (rollout) vs. 0.6 (eval) (higher for exploration during training, lower for deterministic inference); KL divergence term removed (simplifies pipeline but may reduce stability)
- **Failure signatures:** Pass@1 improves but pass@10 stagnates (insufficient trajectory diversity); Reward model MSE rises (overfitting; check dropout, early stopping); Training reward unstable (check reward model correlation; verify C hyperparameter tuning)
- **First 3 experiments:** 1) Ablate tree guidance: Replace Thompson Sampling with random node selection; expect pass@10 drop 2) Reward validation: Correlate r_φ predictions with ground-truth on held-out set; confirm r > 0.85 3) Trajectory diversity audit: Measure unique refinement paths per problem; compare TGPR vs. vanilla GRPO

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the immediate scope of the TGPR framework. The work focuses on demonstrating the effectiveness of combining Thompson Sampling-guided tree search with GRPO for iterative code refinement, without extending to broader applications or theoretical guarantees.

## Limitations
- Reliance on CodeBLEU as part of hybrid reward function lacks external validation of correlation with actual fixability
- Hyperparameter C for Thompson Sampling confidence calibration is unspecified, potentially affecting exploration-exploitation balance
- Performance on longer, more complex programs (beyond 1-10 line programs in standard benchmarks) remains unknown

## Confidence
- **High confidence**: Pass@1 and pass@10 improvements over GRPO baseline on standard benchmarks; core mechanism of using Thompson Sampling for training-time exploration is clearly described and implemented
- **Medium confidence**: Claim that dense hybrid rewards enable better credit assignment - while logical, lacks direct empirical validation beyond training curves
- **Low confidence**: Generalization to complex, real-world debugging scenarios - benchmarks used are relatively constrained

## Next Checks
1. **Reward model validation**: Correlate r_φ predictions with ground-truth rewards on a held-out validation set; confirm Pearson r > 0.85
2. **Breakage test**: Replace Thompson Sampling with random node selection in the tree; verify pass@10 performance degrades significantly
3. **Transfer test**: Evaluate TGPR on longer, more complex programming problems beyond the standard benchmarks to assess real-world applicability