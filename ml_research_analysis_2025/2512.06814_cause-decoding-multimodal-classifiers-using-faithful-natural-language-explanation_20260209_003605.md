---
ver: rpa2
title: 'CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation'
arxiv_id: '2512.06814'
source_url: https://arxiv.org/abs/2512.06814
tags:
- cause
- causal
- explanations
- because
- meme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CAuSE is a framework for generating faithful natural language\
  \ explanations for multimodal classifiers. It uses interchange intervention training\
  \ to enforce causal alignment between a frozen classifier and a fine-tuned language\
  \ model, ensuring the explanations reflect the model\u2019s actual decision-making\
  \ process."
---

# CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation

## Quick Facts
- arXiv ID: 2512.06814
- Source URL: https://arxiv.org/abs/2512.06814
- Reference count: 30
- Primary result: CAuSE generates faithful natural language explanations for frozen multimodal classifiers using causal alignment, outperforming baselines on F1 and CCMR metrics.

## Executive Summary
CAuSE is a framework that generates faithful natural language explanations (NLEs) for frozen multimodal classifiers by enforcing causal alignment between the classifier and an explainer model. It combines interchange intervention training (IIT) with a teacher-student setup where a fine-tuned language model mimics the classifier's behavior. The framework was evaluated on e-SNLI-VE, Hateful Memes, and VQA-X datasets, showing superior faithfulness compared to baselines including fine-tuned VLMs and LLM prompting. Ablation studies confirm that the causal alignment loss is critical for achieving high faithfulness scores.

## Method Summary
CAuSE uses interchange intervention training to align a frozen multimodal classifier with a fine-tuned language model that generates explanations. The explainer consists of an MLP ψ, a GPT-2-small ϕ, an aggregator A, and a classifier C2. Training involves minimizing a composite loss including causal language modeling, KL divergence teacher-student loss, IIT loss, and Frobenius regularization. At inference, only ψ and ϕ are used to generate explanations. The framework evaluates faithfulness using F1 scores between C2 and C1 predictions, and CCMR metrics that measure alignment through counterfactual optimization.

## Key Results
- CAuSE achieves higher F1 and CCMR faithfulness scores compared to fine-tuned VLMs and LLM prompting baselines across three multimodal datasets.
- Ablation studies show that removing the causal alignment loss (IIT) reduces faithfulness scores, confirming its importance.
- Qualitative analysis reveals CAuSE explanations better reflect classifier reasoning, even when less aligned with human-written explanations.

## Why This Works (Mechanism)
CAuSE enforces causal alignment between the frozen classifier and explainer through interchange intervention training, where the explainer's internal representations are made to match the classifier's representations at randomly sampled neurons. This ensures the generated explanations faithfully represent the classifier's actual decision-making process rather than introducing confabulations.

## Foundational Learning
- Interchange Intervention Training (IIT): A technique to align model representations by intervening on internal states; needed to enforce causal alignment between classifier and explainer; quick check: verify neuron sampling and intervention implementation.
- Counterfactual faithfulness metrics (CCMR): Measures explanation faithfulness by finding minimal input perturbations that flip predictions; needed to evaluate explanation quality beyond simple F1; quick check: test counterfactual optimization convergence and feasibility rates.
- Teacher-student training with KL divergence: Used to train the explainer to mimic the frozen classifier's outputs; needed for transferring classifier knowledge to the explainer; quick check: monitor KL divergence loss during training.

## Architecture Onboarding
- Component map: Text/Image → Encoder E → Classifier C1 (frozen) → Aggregator A → MLP ψ → GPT-2-small ϕ → Explanation
- Critical path: Input → E → C1 → ψ → ϕ → Explanation generation
- Design tradeoffs: Uses frozen classifier for faithfulness vs. trainable classifier for flexibility; requires correct prediction filtering to avoid propagating errors
- Failure signatures: Confabulation (repetitive tokens), abstention (label-only outputs), low CCMR feasibility (out-of-distribution counterfactuals)
- First experiments: 1) Train CAuSE on correctly predicted samples only; 2) Evaluate F1 between C2 and C1 outputs; 3) Compute CCMR scores with counterfactual optimization

## Open Questions the Paper Calls Out
None

## Limitations
- High sensitivity to implementation details including learning rate, batch size, optimizer choice, and IIT sampling procedure.
- Reliance on frozen classifiers and filtering to correct predictions may limit applicability when classifier accuracy is low.
- Absence of systematic human evaluation of explanation quality.

## Confidence
- Faithfulness improvements: Medium (dependent on unreported hyperparameters)
- Baseline comparisons: Medium (strong dependence on training details)
- Qualitative examples: Medium (cherry-picked, no human evaluation)

## Next Checks
1. Implement CAuSE with specified datasets and classifiers, but experiment with learning rate schedules and batch sizes to determine sensitivity of F1 and CCMR scores.
2. Run CCMR counterfactual optimization with different solvers (e.g., gradient-based vs. grid search) to test stability of generation feasibility percentages.
3. Conduct ablation without IIT loss to quantify its contribution to faithfulness gains relative to training without causal alignment.