---
ver: rpa2
title: 'EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language
  Models'
arxiv_id: '2511.09880'
source_url: https://arxiv.org/abs/2511.09880
tags:
- safety
- utility
- unsafe
- alignment
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ENCHTABLE introduces a unified framework to preserve safety alignment
  in fine-tuned LLMs without requiring retraining. It uses Neural Tangent Kernel (NTK)-based
  safety vector distillation to decouple safety knowledge from task-specific reasoning,
  combined with interference-aware merging to balance safety and utility.
---

# EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models

## Quick Facts
- arXiv ID: 2511.09880
- Source URL: https://arxiv.org/abs/2511.09880
- Reference count: 40
- Primary result: EnchTable reduces unsafe responses (e.g., 0.009 unsafe rate on medical tasks) while maintaining high utility (0.738 score on medical QA) without retraining.

## Executive Summary
ENCHTABLE introduces a unified framework to preserve safety alignment in fine-tuned LLMs without requiring retraining. It uses Neural Tangent Kernel (NTK)-based safety vector distillation to decouple safety knowledge from task-specific reasoning, combined with interference-aware merging to balance safety and utility. Evaluated across three domains (code, math, medical) and three model architectures, ENCHTABLE significantly reduces unsafe responses while maintaining high utility, outperforming existing methods in safety and robustness against jailbreaking attacks.

## Method Summary
ENCHTABLE preserves safety alignment in fine-tuned LLMs by extracting a safety vector from a surrogate model via NTK-constrained harmful fine-tuning, then merging it into downstream models with interference-aware scaling. The surrogate model undergoes harmful fine-tuning using unsafe prompts to capture safety knowledge in a linearized NTK regime. The resulting safety vector is merged with the task vector from SFT using coarse scaling based on vector magnitudes and fine-grained SVD-based interference scoring to minimize utility degradation. The framework supports both full fine-tuning and LoRA methods.

## Key Results
- Significantly reduces unsafe responses across three domains (code, math, medical) while maintaining high utility
- Outperforms existing methods (TSVM, SafeLoRA, TIES) in both safety and utility metrics
- Demonstrates robustness against jailbreaking attacks while preserving task performance

## Why This Works (Mechanism)

### Mechanism 1: NTK-Linearized Safety Vector Distillation
The surrogate model undergoes "harmful fine-tuning" using harmful prompts and responses to "remove" safety capability. Instead of standard fine-tuning, the objective is linearized as f(x; θ*) ≈ f(x; θ_sur) + (θ* - θ_sur)^T ∇_θ_sur f(x; θ_sur). This approximation constrains updates to the tangent space around θ_sur, yielding a safety vector Δ = θ_sur - θ* with improved transferability. The core assumption is that parameter updates remain sufficiently small for the first-order Taylor expansion to hold and safety knowledge forms a relatively isolated subspace.

### Mechanism 2: Dual-Layer Interference-Aware Merging
Two-stage scaling minimizes utility degradation: (1) Coarse-grained: α = β · ||Δ_SFT|| / ||Δ|| normalizes safety vector magnitude to task vector magnitude. (2) Fine-grained: SVD decomposes both vectors, computes interference score s_i = ||(U_a^T U_a - I) Σ_a (V_a^T V_a - I)||_1, then scales each layer as Δ*_i = e^{-s_i} Δ_i. Higher overlap → higher s_i → smaller weight. The core assumption is that overlapping singular vectors between safety and task weights indicate shared features that cause interference when merged.

### Mechanism 3: Component-Selective Safety Injection
The framework is partitioned into variants targeting different components. FFN layers are characterized as key-value memory structures for knowledge encoding; attention heads encode linguistic features and contextual dependencies. By targeting intermediate FFN layers (excluding first/last 12 layers), memory constraints are met without sacrificing effectiveness. The core assumption is that safety-relevant knowledge has non-uniform distribution across layers and middle FFN layers contain most safety-critical information.

## Foundational Learning

- **Concept: Task Arithmetic**
  - Why needed here: ENCHTABLE builds on task arithmetic, treating safety alignment as a "task" with its own vector. Understanding that θ_new = θ_pre + Σ_t α_t Δ_t enables additive model modification.
  - Quick check question: If you merge two task vectors Δ_A and Δ_B, what assumption must hold for the merged model to perform well on both tasks?

- **Concept: Neural Tangent Kernel (NTK) Regime**
  - Why needed here: The core innovation uses NTK linearization. Understanding that f_lin(x; θ*) ≈ f(x; θ) + (θ* - θ)^T ∇_θ f(x; θ) approximates training dynamics when updates are small is essential.
  - Quick check question: Why does the NTK approximation break down for large learning rates or many training steps?

- **Concept: Singular Value Decomposition for Interference Detection**
  - Why needed here: Fine-grained scaling uses SVD to detect overlapping features. The interference score s_i measures deviation from orthogonality in singular vector spaces.
  - Quick check question: If U_a^T U_a = I (identity matrix), what does that imply about the relationship between safety and task singular vectors?

## Architecture Onboarding

- **Component map:**
  Surrogate Model (well-aligned, same architecture as target) → NTK-constrained harmful fine-tuning on D_harmful → Safety Vector Δ → Target SFT Model (fine-tuned on downstream task) → Compute task vector Δ_SFT = θ_SFT - θ_pre → Coarse scaling: α = β · ||Δ_SFT|| / ||Δ|| → Fine scaling: SVD-based interference scoring per layer → Aligned Model θ* = θ_SFT + α · Δ*

- **Critical path:**
  1. Surrogate model selection (must be well-aligned Instruct model, not Base) — Table 11 shows Base surrogate yields 0.727 unsafe rate vs 0.029 for Instruct
  2. Harmful fine-tuning steps T — Figure 3 shows T ≈ 200-300 is optimal; too few fails to capture safety direction, too many causes overfitting
  3. Scaling coefficient β — Figure 3 shows β ∈ (0.1, 0.14) keeps unsafe rate below bound; default β = 0.1

- **Design tradeoffs:**
  - Attention-only vs FFN-only vs Both: Attention-only is ~4× faster (fewer parameters) with competitive results. FFN-only achieves slightly better utility on some tasks
  - Full-FT vs LoRA compatibility: Framework supports both; Table 5 shows LoRA results comparable to full fine-tuning
  - Per-architecture vs universal safety vectors: Safety vector must be recomputed per architecture but can be reused across tasks on that architecture

- **Failure signatures:**
  - Degenerate outputs (repeated symbols, invalid characters): Indicates excessive merging weight or incompatible vectors. See Appendix G case study for examples with TSVM/SafeLoRA/TIES
  - High unsafe rate despite alignment: Check surrogate model alignment quality (Instruct vs Base) and fine-tuning steps T
  - Catastrophic utility drop: Check β value; if too high, safety vector dominates task vector

- **First 3 experiments:**
  1. **Sanity check on surrogate selection**: Train safety vector using Base vs Instruct surrogate on same architecture. Measure unsafe rate on a held-out safety benchmark. Expect Instruct to yield significantly lower unsafe rates (per Table 11)
  2. **Scaling coefficient sweep**: On a single task (e.g., Code), sweep β from 0.04 to 0.16. Plot utility score (HE/HEP) vs unsafe rate. Identify the Pareto-optimal region where unsafe rate < bound (0.03) and utility drop < 5%
  3. **Component ablation**: Run ENCHTABLE in three modes (Attention-only, FFN-only, Both) on Medical task. Compare unsafe rate and utility score to establish whether your compute budget allows full coverage or requires selective targeting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ENCHTABLE's safety-utility trade-off scale to larger LLM architectures (e.g., 70B, 175B parameters)?
- Basis in paper: [explicit] Section 7 states: "ENCHTABLE achieves excellent performance on downstream LLMs ranging from 7B to 8B parameters. However, computational constraints limit its evaluation on larger models (e.g., 70B, 175B)."
- Why unresolved: All experiments were conducted on 7B-8B models due to computational constraints; larger models may exhibit different knowledge compartmentalization patterns
- What evidence would resolve it: Evaluation results on 70B+ models showing unsafe rates and utility scores across the same three domains

### Open Question 2
- Question: Can ENCHTABLE generalize to high-stakes domains beyond code, math, and medical (e.g., cross-lingual legal reasoning, financial risk assessment)?
- Basis in paper: [explicit] Section 7 states: "We aim to adapt it to cross-lingual legal reasoning, financial risk assessment, and scientific literature analysis."
- Why unresolved: Current evaluation covers three domains; domain-specific knowledge structures and safety requirements may differ significantly in legal and financial contexts
- What evidence would resolve it: Experiments on legal and financial datasets measuring both safety alignment transfer and domain-specific utility preservation

### Open Question 3
- Question: Is ENCHTABLE compatible with PEFT methods beyond LoRA (e.g., prefix-tuning, adapter-tuning, QLoRA)?
- Basis in paper: [explicit] Section 7 states: "its compatibility with other PEFT methods—such as prefix-tuning, adapter-tuning, and QLoRA—remains unexplored."
- Why unresolved: Only Full-FT and LoRA were validated; different parameter-efficient methods modify different model components, potentially affecting safety vector extraction
- What evidence would resolve it: Comparative experiments across multiple PEFT paradigms showing consistent unsafe rate reductions and utility preservation

## Limitations

- The NTK linearization assumption is fragile and breaks down with excessive fine-tuning steps or aggressive learning rates, yielding noisy safety vectors
- Safety vectors must be recomputed for each architecture, limiting scalability and requiring significant computational resources
- Claims about safety knowledge localization in middle FFN layers are based on "empirical observations" without strong external validation

## Confidence

- **High Confidence**: Basic merging framework functionality, utility preservation on standard benchmarks, superiority over SafeLoRA and TIES on combined metrics
- **Medium Confidence**: NTK-based safety vector extraction effectiveness, interference-aware scaling mechanism, component-selective targeting benefits
- **Low Confidence**: Specific claims about safety knowledge localization in middle FFN layers, universality of SVD interference scoring across architectures, long-term robustness against adaptive adversarial attacks

## Next Checks

1. **NTK Linearization Breakpoint Analysis**: Systematically vary fine-tuning steps T (50, 100, 200, 300, 500) and learning rates η (5e-6, 1e-5, 2e-5) on Code-LLaMA3-8B. Plot unsafe rate and utility score against T and η to empirically identify the region where the first-order Taylor approximation breaks down. Verify that step sizes remain in the linear regime.

2. **Interference Score Sensitivity Test**: On the same Code task, sweep the SVD rank fraction γ from 0.1 to 0.9 in increments of 0.1. For each γ, compute the layer-wise interference scores and measure the resulting unsafe rate and utility. Determine if γ=0.5 is optimal or if performance is robust across a range of values.

3. **Architecture Transferability Experiment**: Using the safety vector extracted from LLaMA3-8B, attempt to merge it into Qwen2.5-Coder-7B (different architecture). Measure unsafe rate and utility compared to using the correct Qwen2.5 safety vector. This tests whether safety vectors are truly architecture-specific or if partial transfer is possible, which would improve scalability.