---
ver: rpa2
title: Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems
arxiv_id: '2508.11689'
source_url: https://arxiv.org/abs/2508.11689
tags:
- threshold
- energy
- spiking
- accuracy
- spike
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASPEN, a novel energy-aware technique for
  neuromorphic systems designed to enhance the feasibility of ultra-low-power, always-on
  wearables. The core idea leverages stochastic perturbations to neuron firing thresholds
  during training, enabling robust adaptation to varying energy constraints at inference
  time without retraining or complex pruning.
---

# Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems

## Quick Facts
- arXiv ID: 2508.11689
- Source URL: https://arxiv.org/abs/2508.11689
- Reference count: 40
- One-line primary result: Stochastic threshold perturbations during SNN training enable runtime energy-accuracy trade-off control, achieving ~120 µW on SynSense Xylo hardware.

## Executive Summary
ASPEN introduces a novel energy-aware technique for neuromorphic systems that enables runtime control over energy consumption without retraining. The method applies stochastic perturbations to neuron firing thresholds during training, creating models that generalize across varying energy constraints at inference time. This bio-inspired approach mimics intrinsic plasticity, allowing dynamic adjustment of spiking activity to balance accuracy and power consumption. Evaluated on SynSense Xylo hardware, ASPEN achieves approximately 1000× lower power than traditional von Neumann systems while maintaining competitive accuracy.

## Method Summary
ASPEN trains spiking neural networks with stochastic firing thresholds sampled from uniform distributions (e.g., U(1.0, 1.5)) during each training batch, using surrogate gradient learning with BPTT. The network learns robust representations that generalize across threshold variations. At inference, the firing threshold becomes a tunable parameter that directly controls spike count and energy consumption. The approach is validated on IMU-based human activity recognition using datasets like UCI-HAR, KU-HAR, and UniMiB-SHAR ADL, with a SynNet-style architecture (15→48→48→48→C) deployed on SynSense Xylo hardware.

## Key Results
- Achieves ~120 µW power consumption on SynSense Xylo hardware
- Provides 1000× lower power than traditional von Neumann systems
- Enables 2× better energy efficiency than state-of-the-art neuromorphic systems
- Maintains graceful accuracy degradation across threshold ranges without catastrophic drops

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Threshold Training as Regularization
Injecting noise into neuronal firing thresholds during training acts as a regularizer, forcing neurons to learn robust representations that generalize across varying energy constraints. This lowers expected spike probability via Jensen's inequality on the cumulative distribution function, suppressing weak activations. The network can generalize robustness from training distribution to inference-time threshold shifts without retraining.

### Mechanism 2: Inference-Time Energy-Accuracy Tradeoff Control
Treating the firing threshold as a tunable runtime parameter enables dynamic control over the energy-accuracy tradeoff. Increasing the threshold at inference raises the bar for spike generation, directly reducing spike counts and dynamic power consumption. Because the model was exposed to threshold variance during training, it degrades gracefully rather than catastrophically as thresholds rise.

### Mechanism 3: Discrete Sampling for Hardware Compatibility
Discrete threshold sampling during training offers superior regularization and better compatibility with digital neuromorphic hardware. Training with discrete levels aligns the model with hardware constraints and acts as a stronger regularizer by reducing degrees of freedom, leading to better spike reduction at minimal accuracy loss.

## Foundational Learning

- **Leaky Integrate-and-Fire (LIF) Neurons**: ASPEN modifies the core LIF equation parameter (threshold). Without understanding that LIF neurons accumulate voltage and reset upon spiking, the mechanism of "raising the threshold to save energy" is opaque.
  - Quick check: If you increase the firing threshold θ in an LIF neuron, does the spike rate increase or decrease, assuming input current remains constant?

- **Intrinsic vs. Extrinsic Plasticity**: The paper distinguishes between changing weights (extrinsic/synaptic) and changing neuron excitability (intrinsic/threshold). ASPEN is an implementation of intrinsic plasticity.
  - Quick check: Does "intrinsic plasticity" refer to changing the connection strength between neurons or the excitability parameters of the neuron itself?

- **Surrogate Gradient Learning**: Training SNNs requires backpropagation through non-differentiable spikes. The paper uses surrogate gradients to learn weights that support the stochastic thresholding strategy.
  - Quick check: Since the spiking function is non-differentiable, what mathematical trick is typically used to calculate gradients during backpropagation?

## Architecture Onboarding

- **Component map**: Encoder (IMU → 15-channel spikes) -> SNN Core (15→48→48→48→C) -> Threshold Adapter (stochastic/inference control) -> Hardware Backend (SynSense Xylo)

- **Critical path**:
  1. Define threshold distribution (e.g., U(1.0, 1.5))
  2. Train model using BPTT while sampling thresholds per batch
  3. Deploy weights to hardware
  4. At runtime, monitor battery/energy state
  5. Adjust global threshold register on hardware to modulate power draw vs. accuracy

- **Design tradeoffs**:
  - Single-Model vs. Model Switching: Single model uses less memory but has narrower operating range; model switching covers extreme energy regimes but requires storing multiple weight sets
  - Distribution Width: Wider distribution (U(1.0, 3.0)) allows lower power operation but sacrifices peak accuracy; narrow distribution retains accuracy but limits energy savings

- **Failure signatures**:
  - Catastrophic Accuracy Drop: Occurs in fixed-threshold models when inference threshold drifts from training value; ASPEN prevents this via robustness training
  - Silent Neurons: If inference threshold is set too high, the network enters a "silence" state where no information propagates

- **First 3 experiments**:
  1. Train baseline fixed-threshold model and ASPEN model; sweep inference thresholds from 0.6 to 2.4; plot accuracy vs. spike count to verify ASPEN's curve is smoother
  2. Train three ASPEN models with different distribution widths (1.0-1.5, 1.0-2.0, 1.0-3.0); identify which offers best Pareto front
  3. Deploy model to Xylo HDK; measure actual power consumption (µW) at different threshold settings to validate linear relationship between spike count and power

## Open Questions the Paper Calls Out

- **Theoretical Framework for Non-linear Relationships**: Can a mathematical model be developed to predictably characterize the non-linear relationship between firing threshold modulation, spike rates, and accuracy? The authors note this relationship is "difficult to characterize/model" and currently rely on empirical sensitivity analysis.

- **Latency Impact of Reduced Spiking**: Does threshold modulation to reduce spiking activity incur latency penalties that impact real-time usability? The paper emphasizes energy reduction but doesn't analyze if higher thresholds delay time-to-decision, critical for always-on interactive wearables.

- **Robustness to Domain Shifts and Sensor Noise**: How does stochastic threshold training interact with robustness to severe domain shifts or sensor noise? While ASPEN shows generalization across datasets, extensive testing under adversarial noise conditions or significant sensor drift is needed.

## Limitations

- Energy savings claims rely on specific SynSense Xylo hardware characteristics and may not generalize to other neuromorphic platforms
- Optimal threshold distribution width appears narrow (U(1.0, 1.5) to U(1.0, 2.0)), with wider distributions degrading accuracy
- Limited comparison to other adaptive techniques like pruning or quantization; "2× better than state-of-the-art" lacks clear definition

## Confidence

- **High Confidence**: Core mechanism of stochastic threshold training improving generalization across inference thresholds is well-supported by experimental data
- **Medium Confidence**: Energy savings claims supported by Xylo measurements but generalization to other platforms requires validation; discrete vs. continuous sampling advantage demonstrated but lacks extensive ablation
- **Low Confidence**: Relationship between training distribution width and maximum achievable energy savings not thoroughly explored; interactions with other optimization techniques not addressed

## Next Checks

1. Deploy ASPEN-trained models to at least two different neuromorphic hardware platforms (e.g., Intel Loihi and IBM TrueNorth) to verify energy savings generalize beyond SynSense Xylo

2. Systematically train models with threshold distributions ranging from U(1.0, 1.2) to U(1.0, 3.0) in 0.2 increments, measuring both peak accuracy and minimum achievable power consumption to map the full Pareto front

3. Implement and compare ASPEN against combined approaches: ASPEN + weight pruning, ASPEN + quantization, and traditional pruning + quantization baselines to quantify whether ASPEN provides additive benefits