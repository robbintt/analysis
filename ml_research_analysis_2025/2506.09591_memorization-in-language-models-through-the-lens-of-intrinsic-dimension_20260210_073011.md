---
ver: rpa2
title: Memorization in Language Models through the Lens of Intrinsic Dimension
arxiv_id: '2506.09591'
source_url: https://arxiv.org/abs/2506.09591
tags:
- memorization
- uni00000013
- uni00000011
- intrinsic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how the intrinsic dimension (ID) of text\
  \ sequences\u2014measured as the effective number of latent directions required\
  \ to represent data\u2014affects memorization in language models. The study controls\
  \ for confounding factors like duplication frequency and context length, then estimates\
  \ ID using contextualized embeddings and the TwoNN method."
---

# Memorization in Language Models through the Lens of Intrinsic Dimension

## Quick Facts
- arXiv ID: 2506.09591
- Source URL: https://arxiv.org/abs/2506.09591
- Authors: Stefan Arnold
- Reference count: 11
- Primary result: Low-ID sequences are more prone to memorization than high-ID sequences, especially under sparse exposure and in overparameterized models.

## Executive Summary
This paper investigates how the intrinsic dimension (ID) of text sequences modulates memorization in language models. The study controls for confounding factors like duplication frequency and context length, then estimates ID using contextualized embeddings and the TwoNN method. Memorization is measured by checking if a model's generated continuation matches the original suffix of a prompt. Results show that low-ID sequences are more prone to memorization, especially under sparse exposure and in overparameterized models, while high-ID sequences are less frequently memorized unless heavily duplicated. This suppressive effect of ID on memorization is strongest for large models and diminishes with repeated exposure, suggesting that model scale, exposure frequency, and data complexity jointly determine memorization risk.

## Method Summary
The study samples 1,000 sequences from The Pile dataset, stratified by duplication frequency across three ranges ([1,10), [10,100), [100,1000)), and truncates all to 150 tokens. For each sequence, BERT contextualized embeddings are extracted and ID is estimated using the TwoNN method. Each sample is split into prefix and suffix, and the GPT-Neo model family (0.1B, 1.3B, 2.7B, 6.0B parameters) is prompted with the prefix to generate a continuation via greedy decoding. Memorization rate is computed as the proportion of exact string matches between the continuation and the original suffix. The analysis aggregates results by ID quantiles and duplication bins to assess how structural complexity influences memorization.

## Key Results
- Low-ID sequences are significantly more prone to memorization than high-ID sequences under sparse exposure conditions.
- The suppressive effect of ID on memorization is strongest in overparameterized models and diminishes with repeated exposure.
- High-ID sequences require heavy duplication to be memorized, suggesting that model scale, exposure frequency, and data complexity jointly determine memorization risk.

## Why This Works (Mechanism)
The paper demonstrates that intrinsic dimension acts as a geometric proxy for structural complexity in text sequences, where low-ID sequences (simpler, more redundant structures) are easier for models to memorize than high-ID sequences (complex, diverse structures). This relationship holds even after controlling for duplication frequency and context length, indicating that ID captures an inherent property of data that influences memorization beyond simple repetition. The effect is amplified in larger models, suggesting that overparameterization makes models more sensitive to the geometric properties of training data.

## Foundational Learning
- **Intrinsic Dimension (ID)**: The effective number of latent directions required to represent data. Needed because it quantifies the geometric complexity of text sequences, which the paper shows correlates with memorization susceptibility. Quick check: Verify ID estimates using different embedding methods (e.g., PCA-based vs. TwoNN) on synthetic low/high-ID data.
- **TwoNN Method**: A nearest-neighbor-based algorithm for estimating intrinsic dimension from point cloud data. Needed because it provides a robust, non-parametric way to measure the dimensionality of contextualized embeddings. Quick check: Test TwoNN on high-dimensional Gaussian data where the true dimension is known.
- **Contextualized Embeddings**: Vector representations of text that capture meaning based on surrounding context. Needed because they encode the semantic and syntactic structure of sequences in a way that reflects their intrinsic dimension. Quick check: Compare ID estimates using different layers of BERT to see which captures sequence complexity best.
- **Greedy Decoding**: A deterministic text generation strategy that selects the most likely next token at each step. Needed because it provides a consistent, reproducible way to measure memorization across sequences. Quick check: Verify that greedy decoding produces stable outputs for the same prefix across multiple runs.

## Architecture Onboarding
- **Component Map**: BERT embeddings -> TwoNN ID estimation -> GPT-Neo prefix prompting -> Greedy decoding -> Memorization rate calculation -> ID quantile aggregation
- **Critical Path**: Sequence extraction → BERT embedding → TwoNN ID computation → Prefix/suffix split → Model prompting → Continuation generation → Exact match verification → Aggregation by ID and duplication
- **Design Tradeoffs**: The study uses greedy decoding for reproducibility but acknowledges this is atypical in practice, potentially underestimating memorization under stochastic decoding. The choice of 150-token sequences balances computational feasibility with sufficient context for ID estimation.
- **Failure Signatures**: Unstable ID estimates on short sequences, no matches from greedy decoding (indicating tokenization issues), or inconsistent memorization rates across model scales suggesting implementation errors.
- **First Experiments**: 1) Test ID estimation stability on sequences of varying lengths; 2) Verify prefix/suffix split doesn't artificially inflate memorization; 3) Compare memorization rates using different GPT-Neo checkpoints to ensure model loading is correct.

## Open Questions the Paper Calls Out
- Does the suppressive effect of intrinsic dimension on memorization persist under stochastic decoding strategies? The authors note their reliance on greedy decoding as a limitation, and it remains unknown if high-ID sequences remain robust against temperature sampling or top-k sampling.
- How does intrinsic dimension influence memorization in the presence of near-duplicates? The study controls only for exact duplicates, explicitly omitting near-duplicates which are known to account for the majority of memorized content.
- Does intrinsic dimension act as a suppressive signal for approximate or semantic memorization? The paper restricts its definition to verbatim memorization, which the authors admit can give a "false sense of privacy."

## Limitations
- Unknown prefix/suffix split ratio introduces ambiguity in memorization measurement, potentially affecting rate calculations.
- Unclear BERT embedding specifications (model variant, layer, representation type) could affect ID estimates and their relationship to memorization.
- Missing TwoNN hyperparameters and tokenization artifact handling may lead to unstable ID estimates and inconsistent results across implementations.

## Confidence
- High: The core finding that low-ID sequences are more prone to memorization than high-ID sequences under sparse exposure conditions.
- Medium: The suppressive effect of ID on memorization is strongest for large models and diminishes with repeated exposure.
- Low: The exact quantification of memorization rate differences across ID quantiles and duplication frequency bins due to reproduction uncertainties.

## Next Checks
1. Replicate ID estimation using publicly available TwoNN implementations on a small test set with known dimensionality to verify stability.
2. Test multiple prefix/suffix splits (e.g., 50/100, 100/50 tokens) to assess sensitivity of memorization rates to sequence partitioning.
3. Compare memorization rates using different BERT variants (base vs. large) and embedding layers to establish robustness of ID estimates.