---
ver: rpa2
title: 'RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG
  with Human and LLM Feedback'
arxiv_id: '2508.05512'
source_url: https://arxiv.org/abs/2508.05512
tags:
- arxiv
- retrieval
- evaluation
- rankarena
- reranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RankArena is a unified platform for evaluating retrieval, reranking,
  and RAG systems using both human and LLM-based feedback. It addresses the challenge
  of scalable, user-centric evaluation by providing five complementary modes: pairwise
  battles, direct reranker inspection, manual annotation, LLM-as-a-judge evaluation,
  and end-to-end RAG assessment.'
---

# RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback

## Quick Facts
- arXiv ID: 2508.05512
- Source URL: https://arxiv.org/abs/2508.05512
- Authors: Abdelrahman Abdallah; Mahmoud Abdalla; Bhawna Piryani; Jamshid Mozafari; Mohammed Ali; Adam Jatowt
- Reference count: 40
- Primary result: Unified platform evaluating retrieval, reranking, and RAG systems with 74.2% human-LLM agreement, using ELO-style rankings and BEIR benchmarks.

## Executive Summary
RankArena addresses the challenge of scalable, user-centric evaluation for retrieval, reranking, and RAG systems by integrating human and LLM-based feedback. The platform offers five complementary evaluation modes—pairwise battles, direct reranker inspection, manual annotation, LLM-as-a-judge, and end-to-end RAG assessment—to capture diverse feedback signals. With support for 84 reranker models, 5 retrievers, and structured LLM prompting, RankArena enables comprehensive system comparison and dataset generation for training reward models and retrieval agents.

## Method Summary
The platform is built on Rankify, Gradio, and PyTorch, deployed on 2× NVIDIA A40 GPUs with 250GB RAM. It integrates 24 reranking methods (84 models) and 5 retrievers (DPR, Colbert, Contreiver, BGE, BM25) to process queries from BEIR benchmark datasets. Evaluations use pairwise preferences aggregated into ELO-style ratings via the formula R_i = 1200 + 32 × (w_i - 0.5) × min(log(total_votes_i + 1), 5.0), with LLM-as-a-judge using GPT-4 and structured prompts for pairwise voting. The system captures fine-grained interaction metadata for training rerankers and reward models.

## Key Results
- Average BEIR score of 52.8 for the best method (twolar), with strong correlations between BEIR benchmarks.
- Human-LLM agreement reached 74.2% overall across evaluation modes.
- The platform captures diverse feedback signals suitable for training rerankers, reward models, and retrieval agents.

## Why This Works (Mechanism)

### Mechanism 1
Structured LLM-as-a-judge prompting can approximate human relevance judgments in retrieval tasks, provided the prompting constraints are enforced. The system forces LLMs (e.g., GPT-4) to evaluate outputs based on explicit criteria—relevance, ranking order, and answer faithfulness—using a strict output format (Model A, Model B, or Tie). This mimics human evaluation of "usefulness" rather than keyword overlap. Core assumption: the LLM has internalized human preferences sufficiently well. Evidence: 74.2% human-LLM agreement reported. Break condition: if LLM exhibits length bias or fails to follow strict output schema.

### Mechanism 2
Aggregating pairwise preferences via an ELO-style rating transforms noisy relative comparisons into a stable global ranking. The platform calculates win rate w_i for each model and transforms it into an ELO rating using the formula R_i = 1200 + 32 × (w_i - 0.5) × min(log(total_votes_i + 1), 5.0). The logarithmic term acts as a confidence dampener. Core assumption: transitivity of preference holds and user votes are i.i.d. Evidence: ELO formula explicitly detailed in the paper. Break condition: if "rock-paper-scissors" dynamics occur among top models.

### Mechanism 3
Capturing fine-grained interaction metadata (e.g., annotation time, movement metrics) creates a reusable asset for training reward models, distinct from simple text logs. Beyond recording the winner, the system logs "auxiliary metadata" and allows full-list manual reordering. This converts implicit user behavior into explicit listwise supervision signals. Core assumption: effort (time) and adjustments (movements) correlate with difficulty or quality. Evidence: platform stores interactions for training rerankers. Break condition: if annotators exhibit high variance in attention or speed.

## Foundational Learning

- **Learning-to-Rank (LTR) & Pointwise vs. Pairwise**
  - Why needed: The platform distinguishes between listwise annotation and pairwise battles. Understanding this distinction is crucial for selecting the right evaluation mode.
  - Quick check: Does the platform support pointwise scoring for individual documents, or strictly comparative pairwise/listwise ranking? (Answer: Pairwise battles and listwise annotation).

- **ELO Rating System**
  - Why needed: The leaderboard is a dynamic rating derived from win rates, not a simple average. Understanding the "confidence" term is crucial for interpretation.
  - Quick check: Why does the formula include a min(log(total_votes + 1), 5.0) term? (Answer: To cap the impact of win rate until sufficient votes are reached).

- **RAG Faithfulness vs. Relevance**
  - Why needed: The LLM judge explicitly separates "relevance of retrieved documents" from "faithfulness of the answer."
  - Quick check: In the RAG evaluation prompt, does the judge prioritize the answer's correctness or its alignment with the provided context? (Answer: Alignment/Faithfulness).

## Architecture Onboarding

- **Component map**: Frontend (Gradio) -> Backend (Rankify/PyTorch) -> Evaluation Engine (LLM-as-a-judge + ELO calculation) -> Data Store (structured storage for preferences and metadata)

- **Critical path**: Query Input (User or Benchmark) -> Retrieval & Reranking (Inference on GPUs) -> UI Presentation (Blind or Named) -> Feedback Capture (Vote + Metadata) -> Rating Update (ELO Calculation)

- **Design tradeoffs**: Blind vs. Named (defaults to blind to reduce brand bias, offers named for debugging); LLM vs. Human (LLMs scale better but show ~25% disagreement); Simplicity vs. Granularity (ELO ratings are easy to read but hide domain-specific failures).

- **Failure signatures**: Cold Start (new models have volatile ratings due to low vote counts); Judge Drift (historical ELO ratings may not be comparable if LLM judge version changes); Annotation Fatigue (long sessions may degrade human label quality).

- **First 3 experiments**: 1) Calibration Run: 50 blind pairwise comparisons between twolar and UPR to verify ELO implementation. 2) Judge Consistency Check: Run 20 queries through both Human annotation and LLM-as-a-judge, calculate local agreement rate. 3) Metadata Analysis: Perform "Full-list Annotation" task and analyze "movement metrics" to see if top-ranked documents require the most human adjustment.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific linguistic or semantic features characterize instances where human preferences and LLM-as-a-judge decisions significantly diverge? While the paper reports 74.2% agreement, it does not provide error analysis explaining LLM judge failure modes. Evidence to resolve: qualitative categorization of divergence cases or correlation analysis between disagreement rates and specific query types.

- **Open Question 2**: How does the heavy-tailed distribution of user engagement affect the statistical confidence and stability of the ELO-style leaderboard? Figure 7 shows clear skew in user engagement, where many methods receive few or zero votes. Evidence to resolve: confidence interval analysis for ELO scores based on sample size or simulation showing ranking volatility with sparse votes.

- **Open Question 3**: To what extent does fine-tuning rerankers on RankArena's collected preference datasets improve performance on out-of-distribution tasks compared to training on static benchmarks? The paper claims the platform stores interactions for training but focuses on evaluation metrics rather than training outcomes. Evidence to resolve: comparative experiment showing reranker performance before and after fine-tuning on collected RankArena preference data.

## Limitations

- The 74.2% human-LLM agreement rate may not generalize across different domains or judge models.
- The ELO rating system assumes transitive preferences, which may break down in "rock-paper-scissors" scenarios among top-performing models.
- The exact human annotation protocol (number of annotators, qualification criteria, inter-annotator agreement thresholds) remains unspecified, limiting reproducibility.

## Confidence

- **High Confidence**: The platform's technical architecture (Gradio frontend, PyTorch backend, GPU deployment) and the ELO rating formula are well-specified and reproducible.
- **Medium Confidence**: The 74.2% human-LLM agreement rate is supported by the abstract but requires independent verification with the full human annotation protocol.
- **Low Confidence**: Generalization of the platform's performance metrics across different judge models (e.g., switching from GPT-4 to Claude) without recalibration.

## Next Checks

1. **Independent Human-LLM Agreement**: Run 50 pairwise comparisons through both human annotators and LLM judges using the exact prompt from Figure 3, then calculate the local agreement rate to verify the 74.2% claim.

2. **ELO Calibration Test**: Perform 100 blind pairwise comparisons between twolar and weak ones (UPR), then verify that the ELO ratings separate them significantly according to the formula.

3. **Cross-Domain Consistency**: Evaluate the same models on both BEIR and NFCorpus datasets to check if the reported 0.27 correlation holds in your local setup, identifying any domain-specific performance gaps.