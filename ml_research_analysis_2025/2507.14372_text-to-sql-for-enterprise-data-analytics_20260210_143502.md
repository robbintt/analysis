---
ver: rpa2
title: Text-to-SQL for Enterprise Data Analytics
arxiv_id: '2507.14372'
source_url: https://arxiv.org/abs/2507.14372
tags:
- query
- tables
- table
- user
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: An enterprise Text-to-SQL chatbot was built to enable non-expert
  users to query a large, dynamic data lake. The system integrates a knowledge graph
  containing metadata, historical query logs, documentation, and domain knowledge,
  combined with a multi-stage retrieval and ranking pipeline that improves table and
  column identification.
---

# Text-to-SQL for Enterprise Data Analytics

## Quick Facts
- arXiv ID: 2507.14372
- Source URL: https://arxiv.org/abs/2507.14372
- Reference count: 40
- Enterprise Text-to-SQL chatbot achieving 53% expert-rated correct responses with 78% table recall

## Executive Summary
This paper presents an enterprise Text-to-SQL chatbot that enables non-expert users to query a large, dynamic data lake using natural language. The system integrates a knowledge graph containing metadata, historical query logs, documentation, and domain knowledge, combined with a multi-stage retrieval and ranking pipeline that improves table and column identification. The chatbot supports multiple user intents and provides interactive query explanations, validation, and debugging. In expert evaluation, 53% of responses were rated correct or close to correct, with 78% table recall and 96% successful compilation rate.

## Method Summary
The system builds a knowledge graph from metadata, query logs, documentation, code, and domain knowledge, then applies Independent Component Analysis (ICA) to cluster tables based on historical user access patterns. A multi-stage retrieval pipeline uses embedding-based retrieval (K_ret=20) followed by LLM-based ranking (K_rnk=7) to identify relevant tables and columns. The Query Writer LLM generates SQL with ranked context, validated through Trino EXPLAIN and hallucination detection. A Researcher Agent fixes hallucinations by searching for correct tables, with iterative refinement through a Query Fixer.

## Key Results
- 53% of responses rated correct or close to correct by expert reviewers
- 78% table recall and 56% column recall on internal benchmark
- 96% successful compilation rate with 99% valid tables and columns
- Multi-stage retrieval and knowledge graph components show significant performance gains in ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Multi-Source Knowledge Graph for Semantic Grounding
Aggregating metadata, query logs, documentation, code, and domain knowledge into a unified knowledge graph substantially improves Text-to-SQL accuracy over schema-only approaches. The knowledge graph links tables to semantic context (descriptions, usage patterns, jargon, example queries), helping the LLM disambiguate business terms and identify correct tables/columns that share similar names but have different meanings across product areas.

### Mechanism 2: User-Dataset Clustering via Access Pattern Analysis
Clustering tables based on historical user access patterns improves retrieval relevance by personalizing the candidate table set. ICA is applied to the user-table access matrix to identify 200 soft clusters. Users and product areas are associated with top clusters, defining personalized candidate tables. This reduces the search space and surfaces tables the user's team commonly queries.

### Mechanism 3: Multi-Stage Retrieval with LLM-Based Context Ranking
Combining embedding-based retrieval (EBR) for high recall with LLM-based ranking for precision improves table/column identification compared to EBR alone. Stage 1 retrieves K_ret=20 candidate tables via EBR, example queries, and user mentions. Stage 2 uses an LLM ranker to score table relevance (1-10) and select top K_rnk=7 tables with relevant columns. The ranker reasons across all retrieved context rather than just embedding similarity.

## Foundational Learning

- **Embedding-Based Retrieval (EBR) and Vector Indexes**
  - Why needed here: Used for initial table/column retrieval and example query matching. Understanding EBR helps debug retrieval failures and tune K_ret.
  - Quick check question: Can you explain why the system retrieves K_ret=20 tables but only passes K_rnk=7 to the query writer?

- **Knowledge Graph Construction and Indexing**
  - Why needed here: The KG is central to the architecture. Understanding node types (tables, columns, users, product areas) and index refresh cadence helps maintain and extend the system.
  - Quick check question: What are the five indexes used to store the knowledge graph, and which one supports instant refresh?

- **LLM Hallucination in Code Generation**
  - Why needed here: The Query Writer includes a dedicated Researcher Agent to fix table/column hallucinations. Understanding why LLMs hallucinate non-existent schemas helps design validation loops.
  - Quick check question: How does the system detect table/column hallucinations, and what two validation mechanisms are used?

## Architecture Onboarding

- **Component map:**
  Knowledge Graph → Query Writer Agent → Researcher LLM Agent → Multi-Agent Framework → UI Layer
  (Knowledge Graph: Five indexes refreshed weekly to instant → Query Writer: Retrieve→Rank→Write→Fix → Researcher: Tool-equipped search → Intent classifier routes to specific agents → Chat interface with feedback collection)

- **Critical path:**
  Question input → Intent classification → Query Writer Agent → Candidate table initialization (user/product clusters) → EBR retrieval (K_ret=20) → LLM table ranking → LLM column ranking → Query writing → Validation (Trino EXPLAIN + hallucination check) → Researcher Agent (if hallucination) → Query fixer → Response rendering

- **Design tradeoffs:**
  - K_ret=20 vs K_rnk=7: Higher K_ret improves recall but increases ranking latency and token costs
  - Full KG vs minimal: Example queries, table clusters, and attributes provide most gains; domain knowledge surprisingly decreased 4+ scores in ablation
  - Multi-agent vs single agent: Intent-specific agents improve response quality but add routing complexity
  - C.4 configuration (A.4, B.3): Similar performance to full model with fewer LLM calls, suggesting some components can be pruned

- **Failure signatures:**
  - Low table recall (<70%): Check clustering quality, EBR index freshness, or whether user's product area is misconfigured
  - Hallucination errors: Researcher Agent failing to find correct tables; check if tables exist in index
  - "Filter is incorrect" (24% of errors): Most common quality issue per expert review; may indicate ambiguous time ranges or business logic gaps
  - Compilation failure (4%): Syntax or function usage errors; check Trino dialect compatibility

- **First 3 experiments:**
  1. **Baseline establishment:** Run the ablation configuration C.1 (schemas only, EBR + writer, no rankers) on a sample of 20 questions to establish baseline table recall and compilation rate
  2. **Example query impact:** Add the example query index and measure change in table recall (paper shows 45%→60%) and score (9%→24%)
  3. **Cluster personalization:** Enable user-dataset clustering and measure table recall improvement for users with well-established access history vs. new users

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of unstructured domain knowledge introduce noise that degrades Text-to-SQL performance, and can filtering strategies mitigate this?
- Basis in paper: Section 3.3.1 states, "Surprisingly, adding domain knowledge decreases the percentage of answers scoring 4+; perhaps irrelevant domain knowledge records lower performance."
- Why unresolved: The authors observe a counter-intuitive negative impact from a component designed to help, but the specific mechanism (noise vs. distraction) is unverified, and no solution is proposed.
- What evidence would resolve it: A follow-up ablation study measuring performance after applying relevance thresholds or semantic similarity filters to the domain knowledge records prior to retrieval.

### Open Question 2
- Question: How can the system be extended to detect and correct hallucinations in column values (literals), rather than just table and column names?
- Basis in paper: Section 2.2.3 notes, "For hallucination, we focus on invalid tables and columns, which are easy to detect... and leave invalid column values to future work."
- Why unresolved: The current validation loop successfully reduces structural schema errors but fails to catch semantic errors in filtering values, which the evaluation identifies as the most common issue (24% of errors).
- What evidence would resolve it: The integration of a value validation step (e.g., comparing generated constants against column statistics/top values) resulting in a reduced "Filter is incorrect" error rate.

### Open Question 3
- Question: Why does explicit query decomposition (task planning) lower performance in this enterprise setting, and is this failure mode dependent on the specific LLM used?
- Basis in paper: Appendix A.5 reports that adding a query planner "lowered recall and quality metrics" and caused "overly nested queries," leading the authors to hypothesize it is unnecessary for GPT-4o.
- Why unresolved: This finding contrasts with other literature that promotes decomposition, suggesting the technique's utility may be brittle or model-dependent in ways not fully explored by the authors.
- What evidence would resolve it: A comparative analysis of decomposition success rates across different model sizes (e.g., GPT-4o vs. GPT-3.5) and query complexity levels on the internal benchmark.

## Limitations
- Knowledge graph construction details and domain knowledge quality assessment are not fully specified
- Internal benchmark of 133 questions lacks public availability for independent validation
- Domain knowledge integration showed unexpected negative performance in ablation studies without clear explanation
- ICA clustering effectiveness for new users without historical access patterns remains unclear

## Confidence
- **High confidence**: Multi-stage retrieval with LLM ranking improves precision over EBR alone (96% vs 66% compilation success when rankers removed)
- **Medium confidence**: Knowledge graph components provide substantial gains (48% vs 9% 4+ score in ablation, but exact prompt quality and KG maintenance requirements unknown)
- **Medium confidence**: User-dataset clustering improves relevance (45%→56% table recall with clusters, but cluster stability and cold-start handling unclear)
- **Low confidence**: Domain knowledge integration decreases performance (ablation shows 15%→8% 4+ score, but domain knowledge quality or integration method may be at fault rather than the concept itself)

## Next Checks
1. **Reproduce ablation results**: Implement configurations C.1 (schemas only) and C.4 (full) on a sample dataset to verify the claimed 9%→48% improvement in 4+ scores
2. **Domain knowledge investigation**: Conduct targeted experiments to determine whether poor domain knowledge quality or the integration method caused performance degradation
3. **Cold-start analysis**: Test the clustering approach with synthetic new users to measure performance degradation and identify fallback strategies when historical access patterns are unavailable