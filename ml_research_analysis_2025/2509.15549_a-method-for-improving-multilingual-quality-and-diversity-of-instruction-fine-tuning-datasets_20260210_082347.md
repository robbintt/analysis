---
ver: rpa2
title: A method for improving multilingual quality and diversity of instruction fine-tuning
  datasets
arxiv_id: '2509.15549'
source_url: https://arxiv.org/abs/2509.15549
tags:
- multilingual
- data
- quality
- languages
- m-daq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving multilingual instruction
  fine-tuning (IFT) for large language models (LLMs) by tackling data scarcity and
  quality issues across diverse languages. The proposed Multilingual Data Quality
  and Diversity (M-DaQ) method uses a Quality Scoring Model (QSM) to assess and select
  high-quality IFT samples in a language-agnostic manner, combined with an unsupervised
  clustering algorithm to ensure semantic diversity.
---

# A method for improving multilingual quality and diversity of instruction fine-tuning datasets

## Quick Facts
- arXiv ID: 2509.15549
- Source URL: https://arxiv.org/abs/2509.15549
- Reference count: 0
- This work introduces M-DaQ, a method using quality scoring and semantic clustering to improve multilingual IFT datasets, achieving over 60% win rate on benchmarks across 18 languages.

## Executive Summary
This paper addresses the challenge of improving multilingual instruction fine-tuning (IFT) for large language models (LLMs) by tackling data scarcity and quality issues across diverse languages. The proposed Multilingual Data Quality and Diversity (M-DaQ) method uses a Quality Scoring Model (QSM) to assess and select high-quality IFT samples in a language-agnostic manner, combined with an unsupervised clustering algorithm to ensure semantic diversity. Evaluations across 18 languages show that models fine-tuned with M-DaQ significantly outperform vanilla baselines, achieving over 60% win rate on standard benchmarks. Human evaluations confirm these gains and highlight improvements in culturally relevant responses. The method also provides the first systematic investigation of the Superficial Alignment Hypothesis in multilingual settings, demonstrating that a small set of high-quality samples suffices for effective alignment. M-DaQ code is released to support future research.

## Method Summary
M-DaQ improves multilingual IFT by first scoring candidate samples using a Quality Scoring Model (QSM) trained with triplet loss to distinguish high-quality from low-quality responses in a language-agnostic manner. It then clusters samples by semantic embeddings and selects the highest-quality sample from each cluster above a complexity threshold, ensuring both quality and diversity. The resulting dataset is used to fine-tune LLMs, achieving significant performance gains across 18 languages while using fewer samples than traditional approaches.

## Key Results
- M-DaQ achieves over 60% win rate on standard benchmarks across 18 languages compared to vanilla baselines
- Models fine-tuned with M-DaQ show significant improvements in human evaluations, particularly in culturally relevant responses
- The Superficial Alignment Hypothesis holds in multilingual settings: smaller high-quality datasets outperform larger noisy ones, with scaling from 1K to 52K samples reducing win rate by 10.1% and 6.2%
- M-DaQ code is released to support future research in multilingual instruction tuning

## Why This Works (Mechanism)

### Mechanism 1: Triplet-Based Quality Scoring for Language-Agnostic Selection
- Claim: A fine-tuned XLM-RoBERTa model can learn to rank IFT sample quality across languages without language-specific heuristics.
- Mechanism: The Quality Scoring Model (QSM) is trained on paired high-quality (human-revised) and low-quality (noisy/machine-generated) responses using triplet loss. This forces the model to learn a shared embedding space where high-quality (instruction, response) pairs cluster closer together, enabling relative quality ranking independent of language surface features.
- Core assumption: Quality signals in IFT data are partially universal across languages (e.g., coherence, relevance) rather than language-specific.
- Evidence anchors:
  - [abstract] "uses a Quality Scoring Model (QSM) to assess and select high-quality IFT samples in a language-agnostic manner"
  - [section 3.1] "training objective is to maximize the score assigned to sp while minimizing the score for sn, effectively learning a relative quality ranking"
  - [corpus] TACOS (arXiv:2507.03673) similarly uses comparative scoring for IFT selection, suggesting cross-validation of the scoring-based selection paradigm, though not specifically multilingual.
- Break condition: If quality is fundamentally language- or culture-specific (e.g., culturally appropriate phrasing cannot be captured by cross-lingual embeddings), QSM scores will be misaligned with human judgments for underrepresented languages.

### Mechanism 2: Semantic Clustering for Diversity Preservation
- Claim: Unsupervised clustering in a multilingual embedding space selects diverse samples that cover the semantic distribution of instructions.
- Mechanism: DSA encodes all samples using XLM-RoBERTa, applies PCA for dimensionality reduction, then k-means clustering. By selecting the highest-quality sample from each cluster (with a complexity threshold), the method ensures broad semantic coverage rather than over-representing frequent instruction types.
- Core assumption: Semantic diversity in the instruction space correlates with improved generalization across tasks and linguistic formulations.
- Evidence anchors:
  - [abstract] "unsupervised clustering algorithm to ensure semantic diversity"
  - [section 3.2] "By sampling the highest samples from each cluster equally, we enhance the semantic diversity of the resulting dataset"
  - [corpus] Evidence is limited—TACOS uses tagging and scoring but not clustering specifically. Corpus papers do not directly validate clustering for multilingual diversity.
- Break condition: If clusters do not align with meaningful semantic categories (e.g., PCA discards important dimensions, or k-means assumes spherical clusters that don't fit the data manifold), selected samples may be redundant or unrepresentative.

### Mechanism 3: Superficial Alignment Hypothesis in Multilingual Settings
- Claim: For multilingual IFT, a small set of high-quality samples is sufficient for effective alignment, provided the base model acquired sufficient multilingual knowledge during pretraining.
- Mechanism: The authors show that scaling from 1K to 52K samples reduces win rate, suggesting that larger noisy datasets degrade alignment. High-quality samples "unlock" existing multilingual knowledge rather than imparting new knowledge.
- Core assumption: The base model (Llama 3 8B) has sufficient multilingual capability from pretraining; IFT only formats outputs.
- Evidence anchors:
  - [abstract] "demonstrating that a small set of high-quality samples suffices for effective alignment"
  - [section 4.3] "Scaling IFT dataset from 1K to 10K and 52K samples reduces the win rate by 10.1% and 6.2%"
  - [corpus] LIMA (Zhou et al., 2023) originally proposed SAH for English; this paper extends it to multilingual, but corpus papers do not yet cite this extension (recent arXiv preprint).
- Break condition: If the base model lacks sufficient pretraining in a target language (e.g., low-resource languages like Tagalog), SAH may not hold—IFT cannot align knowledge that doesn't exist. The paper notes language-dependent sensitivity (Arabic vs. French), supporting this boundary.

## Foundational Learning

- Concept: Triplet Loss for Representation Learning
  - Why needed here: Core to how QSM learns quality discrimination without explicit labels.
  - Quick check question: Given anchor vi, positive vp, and negative vn, what does the margin ε enforce in the triplet loss?

- Concept: Cross-Lingual Embedding Spaces (XLM-RoBERTa)
  - Why needed here: Both QSM and DSA rely on shared multilingual representations.
  - Quick check question: Why can XLM-RoBERTa encode semantically similar sentences in different languages into nearby vectors?

- Concept: k-Means Clustering Assumptions
  - Why needed here: DSA uses k-means for diversity; understanding its limits prevents misapplication.
  - Quick check question: What shape of clusters does k-means assume, and what happens if data doesn't match?

## Architecture Onboarding

- Component map:
  1. QSM (XLM-RoBERTa + triplet loss fine-tuning) → scores each IFT sample
  2. DSA (XLM-RoBERTa embeddings → PCA → k-means → per-cluster selection) → ensures diversity
  3. IFT fine-tuning (Llama 3 8B) on selected subset → M-DaQ Model

- Critical path:
  1. Prepare positive/negative response pairs from MIDB (or similar curated multilingual data)
  2. Train QSM with triplet loss
  3. Score all candidate IFT samples
  4. Cluster by semantic embeddings, select top-scoring sample per cluster above complexity threshold
  5. Fine-tune base LLM on selected subset

- Design tradeoffs:
  - Quality vs. diversity: Higher nquality reduces diversity if clusters are under-sampled; higher ndiversity may include lower-quality samples.
  - Complexity threshold c_thres: Too low → noisy samples; too high → over-pruning, loss of coverage.
  - Number of clusters: Too few → semantic gaps; too many → sparse clusters, redundancy.

- Failure signatures:
  - QSM systematically misranks certain languages (check per-language score distributions vs. human judgments).
  - Clusters collapse into language-specific groups rather than semantic groups (inspect cluster composition).
  - Win rate improves for high-resource but degrades for low-resource languages (possible pretraining gap).

- First 3 experiments:
  1. Replicate QSM training on MIDB subset; validate correlation between QSM scores and human quality ratings per language.
  2. Vary number of clusters (k) in DSA; measure downstream win rate and cluster semantic coherence (manual inspection of sample diversity).
  3. Ablate complexity threshold: train models with c_thres = 0, avg×0.1, avg×0.2; compare win rates and response length/informativeness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does language-specific pretraining readiness cause the observed variance in Superficial Alignment Hypothesis (SAH) sensitivity?
- Basis in paper: [inferred] Page 4 observes that sensitivity to IFT scale varies by language (e.g., Arabic vs. French) and hypothesizes this stems from "differences in language-specific pretraining readiness," but does not verify this causality.
- Why unresolved: The paper establishes the correlation between language type and SAH sensitivity but lacks a controlled ablation on pretraining data volumes to confirm the hypothesis.
- What evidence would resolve it: A study controlling pretraining corpus sizes across different languages while measuring the resulting efficiency of IFT data scaling.

### Open Question 2
- Question: How does M-DaQ perform when applied to real-world deployment scenarios compared to standard academic benchmarks?
- Basis in paper: [explicit] Page 5 states, "Future directions include extending M-DaQ to support more languages, and applied in real world scenarios."
- Why unresolved: The current evaluation is restricted to Alpaca-Eval and MT-Bench benchmarks, which may not reflect the complexity of user interactions in production environments.
- What evidence would resolve it: A/B testing results or user satisfaction metrics derived from deploying M-DaQ-tuned models in a live production setting.

### Open Question 3
- Question: Can automated evaluation methods be adapted to reliably detect the culturally grounded improvements identified by human experts?
- Basis in paper: [inferred] Page 4 notes that human experts identified qualitative improvements in "culturally appropriate phrasing... aspects that may lie beyond the current discriminative capacity of LLM-based judges."
- Why unresolved: There is a discrepancy between the paper's automated win rates and the cultural nuances perceived by humans, suggesting a "blind spot" in current LLM-as-judge methods.
- What evidence would resolve it: The development of a specialized, culture-specific evaluation benchmark where automated judge scores correlate strongly with human anthropological assessments.

## Limitations

- The complexity threshold formula (average token length × 0.2) appears arbitrary without ablation studies showing sensitivity to this parameter.
- The clustering methodology relies on PCA dimensionality reduction without discussing how many principal components were retained or whether this adequately preserves semantic information across languages.
- The evaluation framework relies heavily on HELM benchmarks that may not fully capture instruction-following capabilities across diverse linguistic and cultural contexts.

## Confidence

**High Confidence**: The core methodology of using triplet loss for quality scoring is well-established in representation learning literature. The observation that larger noisy datasets can degrade performance (supporting the Superficial Alignment Hypothesis) aligns with prior findings in English-only settings.

**Medium Confidence**: The extension of the Superficial Alignment Hypothesis to multilingual settings is plausible but requires more rigorous testing across truly low-resource languages where pretraining coverage is limited. The quality-diversity tradeoff mechanism appears sound but lacks comprehensive ablation studies.

**Low Confidence**: The specific parameter choices (complexity threshold, number of clusters, quality cutoff) and their claimed optimality are not well-supported by empirical evidence in the paper.

## Next Checks

1. **Parameter Sensitivity Analysis**: Conduct comprehensive ablation studies varying the complexity threshold (0.1× to 0.3× average length), number of clusters (25 to 100), and quality score cutoffs to establish robust parameter ranges and identify potential overfitting to specific choices.

2. **Low-Resource Language Validation**: Test M-DaQ's effectiveness on genuinely low-resource languages (e.g., Swahili, Hausa, or regional dialects) where the base model's pretraining coverage is minimal, to rigorously test the boundaries of the Superficial Alignment Hypothesis across the full multilingual spectrum.

3. **Bias and Fairness Audit**: Perform detailed analysis of QSM scoring patterns across languages to identify systematic biases, and conduct fairness audits comparing responses for culturally equivalent instructions across different language groups to ensure the quality-diversity balance doesn't inadvertently favor high-resource language patterns.