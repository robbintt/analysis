---
ver: rpa2
title: Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral,
  and ChatGPT
arxiv_id: '2502.16428'
source_url: https://arxiv.org/abs/2502.16428
tags:
- reasoning
- janus
- multimodal
- accuracy
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses limitations in multimodal LLM evaluation by
  introducing a benchmark that assesses multi-image reasoning, rejection-based evaluation,
  and reasoning consistency. We employ entropy as a novel metric to quantify stability
  across reordered answer variants, revealing models that rely on positional heuristics
  versus genuine comprehension.
---

# Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT

## Quick Facts
- arXiv ID: 2502.16428
- Source URL: https://arxiv.org/abs/2502.16428
- Reference count: 40
- Key outcome: Introduces a benchmark revealing that positional bias and reasoning consistency vary dramatically across multimodal models, with ChatGPT-o1 achieving 82.5% accuracy while Janus models show high entropy (0.787-0.8392) indicating unstable reasoning.

## Executive Summary
This study introduces a novel benchmark for evaluating multimodal large language models on visual reasoning tasks, addressing limitations in current evaluation methods by incorporating rejection-based assessment and entropy metrics for reasoning consistency. The benchmark exposes models that rely on positional heuristics versus genuine comprehension by presenting questions with reordered answer choices and unanswerable scenarios. Results show significant variation in model performance, with ChatGPT-o1 achieving the highest overall accuracy (82.5%) and rejection accuracy (70.0%), while Janus models exhibit poor consistency and high entropy scores indicating positional bias. The study demonstrates that model size does not guarantee performance and that entropy effectively identifies reasoning inconsistencies.

## Method Summary
The study evaluates multimodal LLMs using a benchmark of 120 questions and 376 images curated from MUIRBench, spanning 8 visual reasoning tasks with 3 variants per question (answerable, reordered answers, unanswerable). Models are assessed using four metrics: overall accuracy, rejection accuracy (performance on unanswerable questions), abstention rate (ideal ≈0.33), and entropy measuring answer consistency across reordered variants. Evaluation uses zero-shot prompting with temperature=1.0 and new sessions per question. Janus models are deployed locally on 2× RTX 4090 GPUs while other models use official web interfaces.

## Key Results
- ChatGPT-o1 achieves highest overall accuracy (82.5%) and rejection accuracy (70.0%)
- Janus models show poor consistency with high entropy scores (Janus 7B: 0.8392, Janus 1B: 0.787), indicating positional bias
- Model size does not guarantee performance (Janus 1B vs 7B show similar poor results)
- Qwen models blocked by content filtering on Cartoon Understanding task (0% accuracy)
- Grok 3 shows over-rejection tendency with abstention rate 0.375 (>0.33 ideal threshold)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reordering answer choices reveals positional bias by exposing models that rely on option order heuristics rather than visual reasoning.
- **Mechanism:** A question is presented multiple times with its multiple-choice options (A-D) shuffled. If a model's answers change inconsistently across these reordered variants (yielding high entropy), it indicates unstable reasoning driven by positional heuristics.
- **Core assumption:** A model with genuine comprehension will consistently identify the same semantic answer, regardless of its position in the list.
- **Evidence anchors:**
  - [abstract] "revealing models that rely on positional heuristics versus genuine comprehension" via "stability across reordered answer variants."
  - [section V-D] High entropy scores for Janus 7B (0.8392) and Janus 1B (0.787) are explicitly linked to "susceptibility to positional bias and unstable reasoning."
  - [corpus] Weak/missing direct corpus evidence on positional bias in multimodal benchmarks.
- **Break condition:** If a model's accuracy remains consistent across reordered variants, this mechanism will not detect positional bias (resulting in low entropy, even if the consistent answer is wrong).

### Mechanism 2
- **Claim:** Entropy (Shannon entropy) applied to answer distributions quantifies reasoning consistency, a dimension missed by accuracy alone.
- **Mechanism:** For each question group (one original + reordered variants), the distribution of the model's selected options is converted to probabilities. Entropy is calculated on this distribution. Low entropy implies consistent selection; high entropy implies randomness.
- **Core assumption:** Consistency in answer selection across superficial changes (reordering) is a valid proxy for the stability and robustness of the underlying reasoning process.
- **Evidence anchors:**
  - [abstract] "entropy as a novel metric to quantify reasoning consistency across reordered answer variants."
  - [section IV-B-7, Eq. 4 & 5] Formally defines the entropy H(Qi) calculation and Mean Entropy m.
  - [corpus] Corpus references entropy in unrelated contexts (e.g., affective chains); no direct evidence for this application in VQA.
- **Break condition:** Low entropy does not guarantee correctness. A model could consistently select the same incorrect answer (deterministic failure).

### Mechanism 3
- **Claim:** Rejection accuracy and abstention rate measure a model's uncertainty calibration and ability to recognize unanswerable questions.
- **Mechanism:** The benchmark includes unanswerable questions where the correct choice is removed. A well-calibrated model should select "None of the provided options." Rejection accuracy measures correctness on these specific questions, while abstention rate tracks the overall tendency to use this option.
- **Core assumption:** A reliable model should "know when it doesn't know"—abstaining when no valid answer exists but engaging when one does.
- **Evidence anchors:**
  - [abstract] "assesses multi-image reasoning, rejection-based evaluation, and reasoning consistency."
  - [section V-B] Reports QVQ-72B-Preview's high rejection accuracy (85.5%) vs. Janus models' low scores (<15%).
  - [corpus] Weak/missing corpus evidence on rejection-based evaluation in multimodal contexts.
- **Break condition:** A high abstention rate is not inherently good. Over-rejection (abstaining on answerable questions) is a failure mode, as noted with Grok 3 (abstention rate 0.375 > 0.33 threshold).

## Foundational Learning

- **Concept: Entropy (Shannon Entropy)**
  - **Why needed here:** This is the paper's core novel metric for quantifying reasoning stability. Interpreting it is key to understanding the results.
  - **Quick check question:** If a model answers A, B, A, B for four reordered variants of the same question, would its entropy be high or low?
    *(Answer: High. The uniform distribution across two answers indicates maximal instability/randomness in this binary case.)*

- **Concept: Positional Bias in Multiple-Choice**
  - **Why needed here:** A central problem the benchmark is designed to detect. Models may exploit the order of options rather than reasoning.
  - **Quick check question:** A model gets 90% accuracy on a test. After you shuffle the answer choices for the same questions, its accuracy drops to 25%. What happened?
    *(Answer: The model was using a positional heuristic, likely always favoring a specific option like 'B' or 'C'.)*

- **Concept: Uncertainty Calibration (Abstention)**
  - **Why needed here:** The paper argues that accuracy is insufficient; models must also know when to refuse to answer.
  - **Quick check question:** A model has a 0% abstention rate. What does this imply about its decision-making process?
    *(Answer: The model is likely overconfident, attempting to answer every question even when it lacks sufficient information or when no correct option exists.)*

## Architecture Onboarding

- **Component map:**
  1. Dataset (Curated from MUIRBench): 120 questions, 376 images across 8 tasks. Includes three variant types per question: answerable, reordered, and unanswerable.
  2. Model Interface: Standardized prompt format for querying MLLMs (via web API or local deployment).
  3. Validator: Strict parser to extract the model's choice (A-D or "None") and validate it against provided options.
  4. Entropy Engine: Groups answers by question ID (original + reorderings) and calculates entropy scores.
  5. Metrics Calculator: Computes overall accuracy, rejection accuracy, and abstention rate.

- **Critical path:**
  Question Generation -> Model Inference -> Answer Parsing -> **Entropy & Metrics Calculation** -> Analysis. The entropy step, requiring multiple variant runs per question, is the critical bottleneck and key differentiator.

- **Design tradeoffs:**
  - Dataset Size (120 questions) vs. Cost: A smaller subset was chosen for feasibility across many models, sacrificing exhaustive coverage.
  - Strict Parsing: The rule marking ambiguous or extra-verbal answers as incorrect (Sec IV-B-2) reduces noise but may penalize models with verbose or conversational output styles.
  - Zero-shot, T=1.0: Designed to test intrinsic reasoning without prompting tricks, but results may vary with different sampling strategies.

- **Failure signatures:**
  - High Entropy (>0.7) + Low Accuracy: Random guessing or strong positional bias (e.g., Janus models).
  - Abstention Rate >> 0.33: Over-rejection; the model is too conservative (e.g., Grok 3).
  - Low Rejection Accuracy (<20%): Overconfidence; model fails to identify unanswerable scenarios (e.g., Janus models).
  - Zero Accuracy on Specific Task: Likely due to content safety filters rather than reasoning failure (e.g., Qwen on Cartoon Understanding).

- **First 3 experiments:**
  1. Baseline Run: Evaluate your target model on the 120 base questions to establish raw accuracy.
  2. Bias Probe: Run the reordered variants of the same 120 questions. Calculate entropy per question. A high average entropy score signals positional bias.
  3. Calibration Test: Run the 40 unanswerable questions. Compute rejection accuracy and compare the abstention rate to the 0.33 ideal threshold to diagnose over- or under-confidence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can content moderation strategies be refined in models like Qwen-VL to prevent the rejection of non-hazardous visual inputs (e.g., cartoons) without compromising safety?
- Basis in paper: [explicit] Section VI.C states that Qwen's "overly aggressive" content filtering prevented it from engaging with conventional content, severely limiting its real-world applicability.
- Why unresolved: The paper identifies the symptom (over-filtering) but does not propose or test methods to balance safety with reasoning capability in open-source models.
- What evidence would resolve it: A study showing Qwen models successfully processing and reasoning about "edgy" or humorous visual data after specific safety-layer fine-tuning.

### Open Question 2
- Question: Does training on reordered answer variants reduce positional bias and entropy in smaller multimodal models, or is this instability inherent to their architecture?
- Basis in paper: [inferred] Section VI.G notes that high entropy in Janus models suggests reliance on heuristics rather than content-based reasoning, implying a need to investigate if data augmentation can mitigate this.
- Why unresolved: The study evaluates existing models but does not conduct ablation studies to determine if specific training protocols (like shuffling options during training) would lower entropy scores.
- What evidence would resolve it: Pre- and post-training evaluation of a model like Janus, showing significantly reduced entropy scores after fine-tuning with augmented, reordered datasets.

### Open Question 3
- Question: Can architectural or fine-tuning adjustments correct the "over-rejection" tendency in massive models like Grok 3, or is this uncertainty calibration issue a persistent scaling inefficiency?
- Basis in paper: [explicit] Section VI.D highlights Grok 3's "unusually high abstention rate" and "overly conservative approach," questioning if its massive scale translates to effective decision-making.
- Why unresolved: The paper evaluates the beta version of Grok 3 but does not determine if the observed over-abstention is a fixable calibration error or a fundamental drawback of its specific training mixture.
- What evidence would resolve it: Evaluation of subsequent Grok iterations showing a stabilized abstention rate (closer to the 0.33 threshold) without a loss in overall accuracy.

## Limitations

- Dataset representativeness: The specific 120-question subset selection criteria remain unspecified, potentially limiting generalizability to broader multimodal reasoning tasks
- Entropy threshold interpretation: The paper establishes 0.7 as a high entropy threshold but lacks theoretical justification for this cutoff or comparison to random baseline entropy values
- Cross-model prompting parity: Different models accessed via varying interfaces (local deployment vs. web APIs) may receive subtly different prompt treatments, introducing evaluation confounds

## Confidence

- **High confidence**: Accuracy and rejection accuracy metrics are standard, well-defined, and directly comparable across models
- **Medium confidence**: Entropy as a consistency metric is novel and theoretically sound, but its interpretation as "genuine comprehension" vs. "positional bias" requires additional validation
- **Low confidence**: Claims about model size irrelevance (Janus 1B vs 7B performance) are based on limited sample size and may not generalize to other architectures

## Next Checks

1. Baseline entropy calculation: Compute entropy scores for a random baseline model and perfect consistency baseline to establish theoretical bounds for the metric
2. Prompt sensitivity analysis: Test the same model across different prompt formulations to isolate whether entropy differences stem from reasoning instability or prompt interpretation variations
3. Expanded task coverage: Evaluate the benchmark across additional visual reasoning domains (spatial reasoning, causal inference) to verify the framework's applicability beyond the current 8-task scope