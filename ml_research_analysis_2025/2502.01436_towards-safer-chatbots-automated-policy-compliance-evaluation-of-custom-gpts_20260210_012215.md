---
ver: rpa2
title: 'Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs'
arxiv_id: '2502.01436'
source_url: https://arxiv.org/abs/2502.01436
tags:
- gpts
- policy
- compliance
- custom
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a fully automated method for evaluating the
  policy compliance of user-configured chatbots (Custom GPTs) in the GPT Store. The
  approach combines large-scale GPT discovery, policy-driven red-teaming prompts,
  and LLM-as-a-judge compliance assessment.
---

# Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs

## Quick Facts
- arXiv ID: 2502.01436
- Source URL: https://arxiv.org/abs/2502.01436
- Authors: David Rodriguez; William Seymour; Jose M. Del Alamo; Jose Such
- Reference count: 40
- Primary result: Automated evaluation method achieves F1=0.975 for detecting policy violations in Custom GPTs, revealing 58.7% non-compliance rate.

## Executive Summary
This study introduces a fully automated method for evaluating the policy compliance of user-configured chatbots (Custom GPTs) in the GPT Store. The approach combines large-scale GPT discovery, policy-driven red-teaming prompts, and LLM-as-a-judge compliance assessment. Validated against human annotations, the method achieves an F1 score of 0.975 for detecting policy violations. Applied to 782 Custom GPTs, it reveals that 58.7% exhibit at least one policy violation, with most violations stemming from base model behaviors rather than customization. The method highlights gaps in current review mechanisms and demonstrates scalable, behavior-based policy compliance evaluation for large ecosystems of user-configured chatbots.

## Method Summary
The automated compliance evaluation method operates through three main phases: GPT discovery and interaction using web automation to retrieve metadata and collect responses to red-teaming prompts, prompt generation using GPT-4o to create tailored test prompts based on each GPT's description, and compliance assessment using another LLM (GPT-4o) as a judge to classify prompt-response pairs against operationalized policies. The approach validates its compliance assessment component against human-annotated ground-truth data, achieving an F1 score of 0.975. The method uses direct prompts rather than deceptive ones to ensure reliable evaluation, given the low inter-annotator agreement for deceptive prompt scenarios.

## Key Results
- The automated compliance assessment achieves F1=0.975 when validated against human annotations for binary policy violation detection.
- Among 782 analyzed Custom GPTs, 58.7% exhibited at least one policy violation, with 27.1% violating romantic policy, 31.7% violating cybersecurity policy, and 24.2% violating academic policy.
- Comparison with base models shows that 93.02% of prompts yielded the same compliance classification, indicating most violations originate from model-level behavior rather than customization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Operationalized policies enable reliable automated compliance detection.
- Mechanism: Abstract platform policies are expanded into detailed specifications with explicit criteria and examples, then embedded as context in LLM judge prompts to reduce ambiguity in classification.
- Core assumption: LLM judges can consistently apply policy criteria when they are sufficiently concrete and examples are representative of violation patterns.
- Evidence anchors:
  - [abstract] "We validate our compliance assessment component against a human-annotated ground-truth dataset, achieving an F1 score of 0.975 for binary policy violation detection."
  - [Section 5.2] "The module achieved a precision of 0.976. Accuracy, recall, and F1 score all reached 0.9752."
  - [corpus] Weak direct evidence; corpus neighbors focus on GPT vulnerabilities rather than evaluation methodology validation.
- Break condition: If policies are ambiguous even after operationalization, or if violation categories shift faster than operationalized definitions are updated.

### Mechanism 2
- Claim: Black-box behavior testing via tailored red-teaming reveals compliance gaps that static review misses.
- Mechanism: Prompts are generated dynamically based on each GPT's described purpose, then submitted through the ChatGPT web interface; responses are judged against policies without accessing internal configuration.
- Core assumption: Direct prompts (as opposed to adversarial/deceptive prompts) are sufficient to surface most policy violations in customized chatbots.
- Evidence anchors:
  - [abstract] "The method combines large-scale GPT discovery, policy-driven red-teaming prompts, and automated compliance assessment using an LLM-as-a-judge."
  - [Section 6.1] "Inclusion of deceptive prompts did not alter the final compliance classification for any GPT already deemed compliant based on direct prompts alone."
  - [corpus] Neighbor paper "A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities" supports vulnerability scanning approaches but doesn't validate this specific prompt design.
- Break condition: If GPTs are specifically tuned to recognize and resist evaluation-style prompts, or if violations only emerge in multi-turn conversations.

### Mechanism 3
- Claim: Base model behavior largely determines Custom GPT compliance outcomes.
- Mechanism: Identical red-teaming prompts are re-submitted to base models (GPT-4, GPT-4o); comparison reveals whether violations originate from inherited behaviors or customization-specific amplification.
- Core assumption: Base model responses under controlled prompting are representative of the behaviors that manifest in customized deployments.
- Evidence anchors:
  - [abstract] "A comparison with the base models (GPT-4 and GPT-4o) indicates that most violations originate from model-level behavior, while customization tends to amplify these tendencies rather than create new failure modes."
  - [Section 7.3] "When compared against GPT-4, 93.02% of prompts yielded the same compliance classification as their corresponding Custom GPT evaluations."
  - [corpus] No direct corpus validation of this comparative methodology.
- Break condition: If customization includes knowledge files or actions that fundamentally alter behavior beyond system prompts, base model comparison becomes less predictive.

## Foundational Learning

- Concept: **LLM-as-a-judge paradigm**
  - Why needed here: The entire compliance assessment depends on using an LLM (GPT-4o) to classify prompt-response pairs against operationalized policies.
  - Quick check question: Can you explain why an LLM might be more consistent than human annotators when given structured evaluation criteria, and under what conditions this advantage disappears?

- Concept: **Policy operationalization**
  - Why needed here: Abstract policies ("don't foster romantic companionship") must be converted into concrete, testable criteria for automated evaluation to work.
  - Quick check question: Given the operationalized romantic policy in Appendix C, what behaviors would fall into the gray zone between "neutral relationship advice" and "encouraging emotional dependency"?

- Concept: **Red-teaming vs. jailbreaking distinction**
  - Why needed here: The paper explicitly differentiates its approach (testing under plausible interactions) from adversarial jailbreaking (bypassing guardrails).
  - Quick check question: If a GPT refuses a direct policy-violating request but complies when the same request is framed as role-play, which testing approach would surface this vulnerability?

## Architecture Onboarding

- Component map: GPT Collector & Interactor -> Red-Teaming Prompts Generator -> Compliance Assessment -> Orchestrator
- Critical path: Keyword search → metadata extraction → prompt generation → interaction → response collection → compliance judgment → aggregation → storage
- Design tradeoffs:
  - Direct vs. deceptive prompts: Direct prompts chosen for reliability (α=0.826 inter-annotator agreement) despite deceptive prompts potentially revealing more violations
  - Single-turn vs. multi-turn: Single-turn evaluation prioritizes soundness (flagged violations are real) over completeness (some violations missed)
  - Black-box constraint: No access to system prompts/knowledge files limits attribution precision
- Failure signatures:
  - Rate-limiting at 50 messages/3 hours (Orchestrator pauses and resumes)
  - Backend errors during interaction (13 GPTs excluded)
  - Incomplete compliance evaluations (7 GPTs excluded)
  - Rare JSON output anomalies (1 instance with 9 duets instead of 10)
- First 3 experiments:
  1. Validate the Compliance Assessment module on the released ground-truth dataset (40 direct prompt-response pairs) using a different judge model (e.g., Claude, Gemini) to measure cross-model reliability.
  2. Test whether deceptive prompts surface additional violations in a subset of GPTs already classified as compliant under direct prompting, using improved annotation protocols.
  3. Extend to a new policy domain (e.g., medical advice) with domain-expert annotation to validate whether the operationalization and evaluation approach generalizes beyond the three tested domains.

## Open Questions the Paper Calls Out

- Can the automated compliance evaluation method be effectively extended to highly regulated domains requiring specialized expertise, such as medical, financial, or legal advice?
  - Basis in paper: [explicit] The authors state in the Limitations section that "Extending its applicability to other policy areas (e.g., medical, financial, or legal GPTs) would require engagement with domain-specific experts to validate whether the generated responses adhere to the intended compliance standards."
  - Why unresolved: The current study validated the method only on Romantic, Cybersecurity, and Academic domains where the researchers possessed relevant expertise; the operationalized policies for complex regulated fields have not been developed or tested.
  - What evidence would resolve it: A validation study involving domain experts (e.g., lawyers, doctors) annotating GPT responses in their respective fields, resulting in F1 scores comparable to the 0.975 achieved in the current study.

- Can annotation protocols and deceptive prompt designs be refined to achieve sufficient inter-annotator agreement for reliable automated compliance assessment?
  - Basis in paper: [explicit] The Conclusion notes that "While deceptive prompts were excluded from the large-scale evaluation due to poor annotation agreement, refining their design and improving annotation protocols may allow their inclusion in future studies."
  - Why unresolved: The current study found inter-annotator agreement for deceptive prompts was near chance (α=0.126), making reliable automated judgment or ground-truth creation impossible with the current methodology.
  - What evidence would resolve it: A new dataset of deceptive prompts where human annotators achieve a Krippendorff's Alpha significantly above chance, and an LLM-as-a-judge that can replicate these assessments with high precision.

- Do multi-turn interactions and adversarial prompting strategies reveal compliance risks that remain undetected by single-turn, direct prompting?
  - Basis in paper: [explicit] Section 10 states that "extending the method to multi-turn interactions and more adversarial prompting strategies may uncover further compliance risks that remain undetected under single-turn evaluation."
  - Why unresolved: The study prioritized soundness over completeness by using direct, single-turn prompts; it acknowledges that nuanced queries or sophisticated bypass techniques might evade the current detection method.
  - What evidence would resolve it: A comparative evaluation showing that multi-turn conversational scenarios or jailbreaking techniques elicit policy violations in GPTs previously classified as compliant under single-turn testing.

## Limitations

- The reliance on direct prompts rather than deceptive ones may underestimate the true prevalence of policy violations, though this approach provides more reliable results with higher inter-annotator agreement.
- The black-box nature of evaluation means attribution of violations to customization versus base model behavior is necessarily approximate, as the method cannot access system prompts or knowledge files.
- The operationalized policies, while validated with excellent performance, may not capture all edge cases or evolving policy interpretations that emerge as GPT Store usage patterns mature.

## Confidence

- **High confidence** in the automated evaluation methodology's reliability and performance metrics, given the rigorous validation against human-annotated ground truth and the clear mechanism for policy operationalization.
- **Medium confidence** in the finding that 58.7% of GPTs violate platform policies, as this depends on the completeness of the prompt generation and the decision to exclude deceptive prompts which might surface additional violations.
- **Medium confidence** in the conclusion that base model behavior drives most violations, as the comparative analysis provides strong evidence but cannot account for all customization dimensions.

## Next Checks

1. Validate cross-model reliability by running the Compliance Assessment module on the ground-truth dataset using different judge models (Claude, Gemini) to confirm the LLM-as-a-judge approach is not model-specific.
2. Conduct targeted testing with improved annotation protocols on a subset of GPTs classified as compliant under direct prompts to determine whether deceptive prompts reveal additional violations that the current methodology misses.
3. Extend the operationalization and evaluation approach to new policy domains (e.g., medical advice) with domain-expert annotations to test generalizability beyond the three tested domains.