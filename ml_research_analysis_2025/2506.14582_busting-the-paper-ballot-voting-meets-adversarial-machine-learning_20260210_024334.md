---
ver: rpa2
title: 'Busting the Paper Ballot: Voting Meets Adversarial Machine Learning'
arxiv_id: '2506.14582'
source_url: https://arxiv.org/abs/2506.14582
tags:
- adversarial
- ballot
- attacks
- examples
- voting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that machine learning classifiers used in
  US election tabulators are vulnerable to adversarial examples. The authors train
  six models (SVM, SimpleCNN, VGG-16, ResNet-20, CaiT, and Twins) on four new ballot
  datasets and show that standard white-box attacks fail due to gradient masking caused
  by numerical instability.
---

# Busting the Paper Ballot: Voting Meets Adversarial Machine Learning

## Quick Facts
- arXiv ID: 2506.14582
- Source URL: https://arxiv.org/abs/2506.14582
- Reference count: 40
- Primary result: ML classifiers in US election tabulators vulnerable to adversarial examples; physical attacks using commodity printers/scanners can flip close elections

## Executive Summary
This study reveals critical vulnerabilities in machine learning classifiers used in US election tabulators through adversarial attacks. The researchers trained six different models (SVM, SimpleCNN, VGG-16, ResNet-20, CaiT, and Twins) on four new ballot datasets and discovered that standard white-box attacks fail due to gradient masking caused by numerical instability. By modifying the difference of logits ratio loss function, they successfully overcome this barrier and demonstrate that physical attacks using commodity printers and scanners can achieve misclassification rates high enough to alter election outcomes. The findings expose significant risks in deploying ML-based systems for critical infrastructure like voting and call for increased transparency and standardized datasets in election technology.

## Method Summary
The researchers developed a comprehensive testing framework for ML-based election tabulators by creating four new ballot datasets and training six different classifier architectures. They initially attempted standard white-box adversarial attacks but discovered these failed due to gradient masking from numerical instability in floating-point computations. To address this, they modified the difference of logits ratio loss function to improve gradient signal stability. The study then progressed to physical attacks, printing adversarial examples using commodity printers and scanning them with standard equipment to evaluate real-world attack effectiveness. The methodology systematically tested attack success across different perturbation budgets (ε values) and model architectures to understand vulnerability patterns.

## Key Results
- Standard white-box adversarial attacks fail on election ML classifiers due to gradient masking from numerical instability
- Modified difference of logits ratio loss function successfully overcomes gradient masking, enabling effective attacks
- Physical attacks using commodity printers/scanners achieve high attack success rates (e.g., 0.962 for ResNet-20 at ε = 16/255)
- Attack success rates are sufficient to potentially flip close election outcomes

## Why This Works (Mechanism)
The vulnerability stems from the combination of numerical instability in floating-point computations used by election ML classifiers and the inherent sensitivity of neural networks to small input perturbations. When computing gradients for adversarial examples, the classifiers' floating-point operations introduce noise that masks the true gradient signal, preventing standard attack methods from finding effective adversarial examples. This gradient masking effect is particularly problematic in election systems where precise numerical computations are critical for accurate ballot interpretation. The researchers' modified loss function addresses this by stabilizing the gradient computation, allowing attackers to craft effective adversarial examples that exploit the classifier's learned decision boundaries while maintaining apparent legitimacy as valid ballots.

## Foundational Learning

**Adversarial Machine Learning**: Why needed - Understanding how ML models can be deceived by carefully crafted inputs; Quick check - Can you explain the difference between white-box and black-box attacks?

**Gradient Masking**: Why needed - Critical vulnerability where models hide their true gradients, preventing standard attacks; Quick check - What numerical conditions cause gradient masking in floating-point computations?

**Difference of Logits Ratio**: Why needed - Loss function that helps overcome gradient masking by comparing relative confidence between classes; Quick check - How does modifying this loss function improve gradient signal stability?

**Physical Adversarial Examples**: Why needed - Real-world attacks that bridge the digital-to-physical gap through printing and scanning; Quick check - What degradation factors must physical attacks account for?

**Election Tabulation Systems**: Why needed - Understanding the specific ML deployment context and constraints; Quick check - What preprocessing steps might election systems apply that could affect attack transferability?

## Architecture Onboarding

**Component Map**: Dataset Creation -> Model Training -> Gradient Analysis -> Loss Function Modification -> White-Box Attack Testing -> Physical Attack Implementation

**Critical Path**: The most vulnerable point is the numerical instability in gradient computation during white-box attacks, which the modified loss function must successfully address to enable effective attacks.

**Design Tradeoffs**: The study prioritizes attack effectiveness over stealth, focusing on achieving high misclassification rates rather than creating imperceptible perturbations. This tradeoff reflects the goal of demonstrating vulnerability feasibility rather than developing production-ready attack methods.

**Failure Signatures**: Gradient masking manifests as consistently low attack success rates despite theoretically sound attack implementations, indicating numerical instability rather than fundamental attack limitations.

**First Experiments**: 1) Test modified loss function across additional model architectures beyond the six examined; 2) Conduct physical attacks using election equipment from different manufacturers; 3) Evaluate attack effectiveness when ML classifiers include preprocessing or quality control mechanisms.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The modified loss function's effectiveness may not generalize across all model architectures and attack scenarios
- Physical attack methodology relies on commodity equipment that may differ from actual election infrastructure
- Four newly created datasets may not fully represent the diversity of real US election ballots across jurisdictions

## Confidence

**High Confidence**: Demonstration that standard white-box attacks fail due to gradient masking is well-supported; numerical instability identification as root cause is technically sound.

**Medium Confidence**: Modified loss function effectiveness is demonstrated but may not generalize; physical attack success rates are compelling but may not capture full real-world complexity.

**Low Confidence**: Broader election security implications rely on assumptions about ML deployment variations across jurisdictions.

## Next Checks

1. Test modified loss function and attack methodology across additional model architectures beyond the six examined, particularly those commonly used in commercial election systems.

2. Conduct physical attacks using election equipment from different manufacturers and scanning systems to assess attack transferability across hardware variations.

3. Evaluate attack effectiveness when ML classifiers are deployed with additional preprocessing steps or quality control mechanisms commonly used in election systems.