---
ver: rpa2
title: Empirical Evaluation of Large Language Models in Automated Program Repair
arxiv_id: '2506.13186'
source_url: https://arxiv.org/abs/2506.13186
tags:
- repair
- llms
- bugs
- software
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show promise for automated program
  repair (APR), but their effectiveness across diverse programming languages, bug
  scenarios, and model configurations remains underexplored. This study systematically
  evaluates four representative open-source LLMs (CodeLlama, LLaMA, StarCoder, DeepSeek-Coder)
  spanning 7B-33B parameters across six benchmarks covering enterprise-grade and algorithmic
  bugs in Java, C/C++, and Python.
---

# Empirical Evaluation of Large Language Models in Automated Program Repair

## Quick Facts
- **arXiv ID:** 2506.13186
- **Source URL:** https://arxiv.org/abs/2506.13186
- **Reference count:** 40
- **Primary result:** Model specialization (CodeLlama) can outperform larger general-purpose models (LLaMA) in automated program repair

## Executive Summary
This systematic evaluation benchmarks four open-source LLMs (CodeLlama, LLaMA, StarCoder, DeepSeek-Coder) across six benchmarks covering enterprise-grade and algorithmic bugs in Java, C/C++, and Python. The study reveals that fine-tuned code-specialized models like CodeLlama can outperform larger general-purpose models despite smaller parameter counts, that repair performance does not scale linearly with model size, and that most correct patches emerge within the first 30 generations. Prompt engineering significantly influences repair effectiveness, with analysis-augmented prompts helping weaker models but potentially harming stronger ones when analysis is flawed.

## Method Summary
The study evaluated four representative LLMs spanning 7B-33B parameters using method-level perfect fault localization. Four prompt strategies were tested: zero-shot, one-shot, two-shot, and analysis-augmented. For each bug, models generated 30-200 patches, which were deduplicated, compiled, and tested. Plausible patches passing all tests were manually inspected for semantic equivalence to developer fixes. The evaluation spanned six benchmarks including Defects4J, BugsCpp, IntroClass, and ConDefects across Java, C/C++, and Python, measuring repair rate and precision metrics.

## Key Results
- Fine-tuned code-specialized models (CodeLlama) outperform larger general-purpose models (LLaMA) despite smaller parameter sizes
- Repair performance does not scale linearly with model size; doubling parameters does not double repair effectiveness
- Most correct patches appear within the first 30 generations, enabling aggressive sampling budget reduction
- Prompt engineering significantly impacts repair effectiveness, with analysis-augmented prompts having differential effects on model strengths

## Why This Works (Mechanism)

### Mechanism 1: Code-specialization outperforms raw parameter scaling
Domain-adapted training exposes models to code syntax patterns, common bug-fix transformations, and language-specific idioms that general-purpose pretraining distributes sparsely. Fine-tuning on repair-relevant corpora teaches the model bug-fix attention patterns without requiring proportional parameter increases. This breaks when target bugs use languages or frameworks poorly represented in the code-specialization corpus.

### Mechanism 2: Correct patches concentrate in early generation ranks
When models have strong code priors, semantically correct repairs receive higher log-probability scores and appear earlier in beam/sampling outputs. The distribution follows a power-law-like decay rather than uniform distribution. This breaks for complex multi-hunk bugs or those requiring non-local reasoning, where correct patches may appear at later ranks.

### Mechanism 3: In-context bug analysis has differential effects
Weaker models lack robust internal bug-reasoning chains; explicit analysis provides scaffolding. Stronger models have better internal representations that can be corrupted when prompted to follow incorrect diagnostic paths. This context override effect occurs when flawed analysis dominates the model's reasoning. This breaks when bug analysis quality is guaranteed through symbolic verification.

## Foundational Learning

- **Plausible vs. Correct Patches**: The study distinguishes patches that pass all tests (plausible) from those semantically equivalent to developer fixes (correct). Misunderstanding this leads to overestimating repair capability. Quick check: If a patch passes 100% of test cases but changes the return value for an untested edge case, is it correct or merely plausible?

- **Decoder-Only Transformer Architectures**: All evaluated models use decoder-only architectures; understanding causal masking and autoregressive generation explains why context ordering affects repair success. Quick check: Why might a decoder-only model struggle with bugs requiring bidirectional code understanding compared to an encoder-decoder model?

- **In-Context Learning (Few-Shot Prompting)**: The study compares zero-shot, one-shot, and two-shot prompting; knowing how examples shape the model's output distribution is essential for interpreting the 206.7% improvement findings. Quick check: When providing a repair example in the prompt, why must the example's programming language match the target bug's language?

## Architecture Onboarding

- **Component map:** Prompt Constructor → LLM Inference Engine → Patch Deduplicator → Test Validator → Manual Correctness Checker
- **Critical path:** 1. Receive localized buggy function 2. Construct prompt (select template) 3. Generate N patches (30 recommended) 4. Deduplicate → Compile check → Run test suite → Filter plausibles 5. Manual review plausibles against ground truth for correctness
- **Design tradeoffs:** Model selection: Code-specialized 7B vs. general-purpose 13B vs. large code-specialized 33B; Generation budget: 30 patches (~76%+ correct coverage) vs. 200 patches (marginal gains, 6.6× cost); Prompt complexity: Zero-shot (no dependencies) vs. one-shot (requires example curation) vs. analysis-augmented (requires analysis generation)
- **Failure signatures:** C/C++ long functions (>100 LOC): 76% of DeepSeek patches modify wrong locations; 20% fail compilation; Strong model + flawed analysis: 46.6% drop in correct repairs (DeepSeek on ConDefects-Java); Algorithmic vs. enterprise gap: 45.45% repair rate on IntroClass vs. 5.66% on BugsCpp for best model
- **First 3 experiments:** 1. Baseline replication: Select 50 bugs from Defects4J v1.2; run CodeLlama-7B and LLaMA-2-13B with one-shot prompting; measure C/P ratio and verify CodeLlama outperforms despite smaller size 2. Generation budget validation: For 20 bugs, generate 200 patches each; plot cumulative correct patches by rank; confirm 30-patch threshold captures >75% of correct repairs 3. Prompt sensitivity stress test: On ConDefects-Py subset, compare zero-shot vs. one-shot vs. analysis-augmented for CodeLlama and DeepSeek; quantify the crossover point where analysis helps vs. harms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal training objectives and architectural designs for Large Language Models specifically specialized for Automated Program Repair (APR)?
- Basis in paper: The authors state, "Future research should explore how to construct more effective APR-specialized LLMs, including the design of training objectives, model architectures, and the use of repair-specific datasets."
- Why unresolved: Even the best code-specialized models performed unsatisfactorily on complex datasets such as BugsCpp, indicating that general code pre-training is insufficient for robust repair capabilities.
- What evidence would resolve it: Successful training and evaluation of an LLM using novel APR-specific objectives that consistently outperforms current state-of-the-art code-specialized models across diverse enterprise benchmarks.

### Open Question 2
- Question: Can model compression or distillation techniques be applied to APR models to significantly reduce inference costs without degrading repair accuracy?
- Basis in paper: The paper suggests, "Future research should prioritize methods to reduce inference cost, such as model compression, distillation, or efficient sampling strategies, while maintaining or improving repair accuracy."
- Why unresolved: The study highlights that doubling model size does not yield proportional gains, yet smaller models currently lack the robustness of larger ones. Effective compression is needed to balance the high cost of running large models.
- What evidence would resolve it: Empirical data demonstrating that a compressed or distilled model can achieve repair rates comparable to 33B parameter models while utilizing significantly fewer computational resources.

### Open Question 3
- Question: How do current LLMs perform on broader and more complex real-world software bugs that extend beyond single-function logic errors?
- Basis in paper: The authors note that "real-world bugs may be more complex than those in the datasets," and explicitly call for "further research... to assess LLM repair performance in broader and more complex scenarios."
- Why unresolved: The experiments were limited to single-function bugs, and the models struggled significantly with longer code. The capability of LLMs to handle multi-function or system-level bugs remains unknown.
- What evidence would resolve it: A comprehensive evaluation of LLMs on new benchmarks featuring complex, cross-function bugs in enterprise systems, showing successful repair rates for defects currently classified as "out of scope" for single-function models.

## Limitations
- Assumed perfect fault localization, unrealistically simplifying the APR pipeline
- Focused on method-level bugs with limited test suites, potentially inflating repair rates
- Relied on manual correctness verification, introducing potential subjectivity
- Prompt engineering strategies tested with fixed templates without exploring full design space

## Confidence

- **High confidence**: Model specialization benefits (CodeLlama vs. LLaMA) - supported by multiple benchmarks and consistent performance gaps across languages
- **Medium confidence**: Generation rank concentration effect - while the 30-generation threshold shows strong empirical support, the power-law decay assumption requires further validation across diverse bug types
- **Medium confidence**: Prompt engineering impacts - clear performance differences observed, but the degradation of strong models with flawed analysis warrants additional controlled experiments
- **Low confidence**: Cross-language generalization - limited evidence beyond the initial observations; more systematic evaluation needed

## Next Checks
1. **Localization robustness test**: Evaluate the same models and prompts with noisy/incomplete fault localization to measure real-world performance degradation
2. **Multi-file bug benchmark**: Replicate key experiments on a benchmark containing multi-hunk, cross-file bugs to validate whether generation rank concentration holds for complex repairs
3. **Analysis quality control**: Systematically vary the accuracy of bug analysis prompts to identify the threshold where strong models begin to degrade versus benefit