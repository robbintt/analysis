---
ver: rpa2
title: Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning
  on One Problem
arxiv_id: '2506.03295'
source_url: https://arxiv.org/abs/2506.03295
tags:
- reasoning
- one-shot
- arxiv
- training
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that Critique Fine-Tuning (CFT) on a single
  math problem can significantly improve the reasoning performance of pre-trained
  large language models (LLMs) across both mathematical and logical tasks. CFT constructs
  a dataset by generating diverse candidate solutions to a single problem, obtaining
  detailed critiques from teacher models, and fine-tuning the LLM to critique these
  solutions.
---

# Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem

## Quick Facts
- arXiv ID: 2506.03295
- Source URL: https://arxiv.org/abs/2506.03295
- Reference count: 26
- Key result: CFT on a single math problem improves reasoning performance by 15% on math and 16% on logic tasks, matching one-shot RLVR with 20x less compute

## Executive Summary
This paper introduces Critique Fine-Tuning (CFT), a method that fine-tunes pre-trained LLMs to critique solutions rather than generate them, using data constructed from a single math problem. By collecting diverse candidate solutions from multiple generators and obtaining detailed critiques from teacher models, CFT achieves significant improvements across mathematical and logical reasoning benchmarks. The approach is computationally efficient, requiring only 5 GPU hours compared to 120+ for one-shot RLVR, while delivering comparable or superior performance. The method demonstrates that modern LLMs possess latent reasoning capabilities that can be efficiently unlocked through critique-based supervision rather than traditional reinforcement learning.

## Method Summary
CFT constructs training data by selecting a single math problem, generating 100 diverse candidate solutions from 10 different open-source models, and obtaining detailed critiques from 7 proprietary teacher models. After filtering for quality, 600 critique examples are retained per seed problem. The model is then fine-tuned using full-parameter instruction tuning on (problem, candidate solution) → critique pairs with learning rate 5e-6, batch size 512, and cosine schedule. The method is evaluated on 6 math benchmarks and 3 logic reasoning tasks, showing substantial improvements over base models while requiring minimal computational resources.

## Key Results
- 15% average improvement across six math benchmarks (MATH-500, Minerva, OlympiadBench, AIME24, AIME25, AMC23)
- 16% improvement on three logic reasoning tasks (BBEH subtasks)
- Matches or exceeds one-shot RLVR performance while using 20x less compute (5 vs 120+ GPU hours)
- Demonstrates strong cross-task generalization from math to logical reasoning domains

## Why This Works (Mechanism)

### Mechanism 1: Error Pattern Diversity from Multi-Generator Sampling
Training on critiques of diverse incorrect solutions exposes the model to a broader distribution of reasoning errors than learning from correct solutions alone. By collecting 100 candidate solutions from 10 different generators, the dataset captures varied failure modes, and teacher critiques annotate these errors, providing dense supervision on why solutions fail, not just that they fail. Mixed-generator training achieves 42.2 average accuracy versus 37.6-38.7 for single-generator training, demonstrating that solution diversity correlates with better outcomes.

### Mechanism 2: Critique Objective Recruits Verification Rather Than Imitation
Training the model to generate critiques shifts the learning objective from pattern-matching correct answers to analyzing reasoning validity. Standard SFT trains on (problem) → solution, while CFT trains on (problem, solution) → critique, forcing the model to evaluate intermediate reasoning steps, identify errors, and explain corrections. This process may strengthen metacognitive verification abilities that transfer to solving.

### Mechanism 3: Efficient Activation of Pre-existing Reasoning Representations
Modern pre-trained LLMs possess latent reasoning capabilities inherited from pre-training on mathematical corpora, code, and other reasoning-rich data. CFT provides dense, direct supervision from expert critiques, achieving similar gains in 5 GPU hours versus 120+ for RLVR. The rapid training success suggests activation of existing capabilities rather than learning new ones, though this remains an inference rather than direct evidence.

## Foundational Learning

- **Concept: Critique Fine-Tuning (CFT) vs. Supervised Fine-Tuning (SFT)**
  - Why needed: The entire method hinges on understanding that CFT trains models to critique solutions rather than generate them
  - Quick check: Given a training example (problem, candidate_solution), what does SFT predict as output? What does CFT predict?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: The paper positions CFT as a more efficient alternative to one-shot RLVR
  - Quick check: Why does RLVR require many more GPU hours than CFT for comparable gains? (Hint: Consider reward sparsity vs. dense supervision)

- **Concept: Cross-Task and In-Domain Generalization**
  - Why needed: The paper evaluates CFT on both mathematical reasoning (in-domain) and logical reasoning (cross-domain)
  - Quick check: If CFT is trained on one math problem, why does it improve performance on logic tasks like DisambiguationQA?

## Architecture Onboarding

- **Component map**: Seed Problem → Candidate Solution Generation → Teacher Critique Annotation → Training → Evaluation
- **Critical path**: Data quality hinges on teacher critique quality. If critiques are superficial or incorrect, CFT gains degrade. The paper filters "incorrect or inconsistent critiques" but does not detail the filtering criteria
- **Design tradeoffs**:
  - Seed difficulty: Moderate-difficulty seeds (π1, score=49/100) yield better results than easy (π2, score=93/100) or hard (π1209, score=10/100) seeds
  - Generator diversity: 10 mixed generators outperform single generators (+3.5-4.6 avg points) but increase data collection complexity
  - Teacher model selection: 7 high-capacity proprietary models used; weaker teachers may reduce critique quality
- **Failure signatures**:
  - No improvement on already-aligned models: CFT yields "mixed results" on strong reasoning-oriented LLMs
  - Overfitting risk with insufficient diversity: Single-generator training shows degraded performance
- **First 3 experiments**:
  1. Reproduce single-seed CFT: Train Qwen2.5-Math-7B on dsr-cft-p0 (π1, 600 examples). Evaluate on MATH-500. Expected: ~77%
  2. Ablate generator diversity: Train with single-generator data vs. mixed-generator data. Confirm ~3-4 point gap in average accuracy
  3. Cross-domain transfer test: Train on one math problem, evaluate on BBEH logic tasks. Confirm ~10-20 point gains over base

## Open Questions the Paper Calls Out
- How can one-shot CFT be adapted to consistently improve reasoning capabilities of already strong or extensively aligned models?
- Does the quality or identity of the teacher model significantly impact the effectiveness of one-shot CFT?
- Does one-shot CFT generalize to procedural reasoning domains like code generation or scientific reasoning?

## Limitations
- Data quality dependency: Method success hinges on high-quality teacher critiques, with unclear performance when using weaker teachers
- Transfer mechanism ambiguity: Unclear whether improvements reflect activation of pre-existing reasoning or learning new capabilities
- Evaluation scope limitations: All evaluations use established benchmarks; no testing on novel reasoning domains

## Confidence
- High: Empirical results showing 15% math improvement and 16% logic improvement are well-supported
- Medium: Mechanism explanation is plausible but not conclusively proven through isolation studies
- Low: Claim that CFT "unleashes latent reasoning potential" assumes pre-training creates reasoning capabilities that can be activated rather than learned

## Next Checks
1. Mechanism isolation test: Run CFT with identical error type distributions but generated by single generators versus mixed generators
2. Novel domain transfer: Apply CFT-trained models to reasoning tasks completely absent from pre-training
3. Teacher model ablation: Train CFT using progressively weaker teacher models to determine minimum quality threshold for performance gains