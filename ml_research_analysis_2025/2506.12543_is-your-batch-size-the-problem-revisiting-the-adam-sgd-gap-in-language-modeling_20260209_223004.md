---
ver: rpa2
title: Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling
arxiv_id: '2506.12543'
source_url: https://arxiv.org/abs/2506.12543
tags:
- adam
- batch
- training
- figure
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits the Adam-SGD performance gap in language modeling
  by systematically studying the effects of batch size, momentum, and gradient clipping.
  Through extensive experiments on transformer-based models, the authors find that
  SGD with momentum can perform similarly to Adam when using small batch sizes, if
  hyperparameters are tuned correctly.
---

# Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling

## Quick Facts
- arXiv ID: 2506.12543
- Source URL: https://arxiv.org/abs/2506.12543
- Reference count: 31
- Language models trained with SGD can match Adam performance at small batch sizes when properly tuned

## Executive Summary
This work systematically studies the performance gap between Adam and SGD optimizers in language modeling, challenging the prevailing assumption that Adam is inherently superior. Through extensive experiments on transformer models ranging from 160M to 1B parameters, the authors demonstrate that SGD with momentum can match or even outperform Adam when using small batch sizes with appropriate hyperparameter tuning. The key insight is that batch size critically determines optimizer viability—SGD performs well at small batches (BS < 64) while Adam excels at larger batches (BS ≥ 256). This finding is supported by both empirical results and theoretical analysis using stochastic differential equations.

## Method Summary
The authors conduct large-scale experiments comparing Adam and SGD across various batch sizes using transformer-based language models. The main experimental setup involves a 160M parameter GPT-style model trained on SlimPajama for 1.3B tokens, with extensive hyperparameter sweeps for both optimizers. SGD uses momentum β ∈ {0.95, 0.98} with learning rates in {0.25, 0.5, 1.0}, while Adam uses β₁=β₂=0.95 with learning rates in {1e-3, 2e-3, 4e-3}. Global gradient norm clipping at threshold 1.0 is applied for all SGD runs. The theoretical analysis employs stochastic differential equation approximations to explain why signed gradient methods (Adam, SignSGD) benefit from larger batches through batch-size-dependent drift terms, while SGD's progress depends primarily on iteration count.

## Key Results
- SGD with momentum matches Adam performance at small batch sizes (BS < 64) when hyperparameters are properly tuned
- Adam significantly outperforms SGD at large batch sizes (BS ≥ 256), but this gap narrows when comparing at fixed token budgets rather than wall-clock time
- Gradient grafting experiments show that SGD's failure at large batches stems from poor update direction rather than magnitude
- The critical batch size for SGD is approximately 1, meaning its progress scales with iterations rather than batch size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Signed gradient methods (Adam, SignSGD) exhibit batch-size-dependent acceleration in their drift term, while SGD's early-training progress is iteration-count dependent only.
- Mechanism: The SDE approximation for SignSGD yields drift term $-\text{erf}(\sqrt{B/2} \cdot \Sigma^{-1/2} \nabla f(x_t))dt$, which scales with $\sqrt{B}$ until erf saturates. SGD's SDE drift is simply $-\nabla f(x_t)dt$ with no batch-size dependence—noise only affects diffusion, not drift.
- Core assumption: Gradient noise is approximately Gaussian with diagonal covariance (though authors note heavy-tailed extensions exist).
- Evidence anchors:
  - [Section 4.3, Theorem 1]: Derivation showing erf-based drift for signed updates vs. standard drift for SGD.
  - [Figure 13]: Demonstrates SGD progress is noise-independent in early training, while SignSGD accelerates with more noise (larger effective batch).
  - [corpus]: Related work "Small Batch Size Training for Language Models" (FMR=0.619) supports small-batch SGD viability but doesn't provide the SDE mechanism.
- Break condition: If gradient noise distribution deviates strongly from conditions where erf-linear approximation holds (extreme heavy tails beyond t-student), batch-size acceleration may weaken.

### Mechanism 2
- Claim: SGD's failure at large batch sizes stems primarily from poor update direction, not magnitude.
- Mechanism: Gradient grafting experiments (SGD magnitude + Adam direction) recover Adam-like performance, while reverse grafting (Adam magnitude + SGD direction) performs like SGD. This isolates direction as the culprit.
- Core assumption: Grafting preserves meaningful optimizer properties when learning rates are retuned.
- Evidence anchors:
  - [Section 4.1, Figure 10]: SGD#Adam (SGD magnitude, Adam direction) matches Adam; Adam#SGD matches SGD.
  - [Section 4.2]: Adaptive clipping of top momentum coordinates partially bridges the gap, suggesting specific large-coordinate directions are problematic.
- Break condition: If magnitude-scaling interactions dominate (e.g., extreme learning rate regimes), direction-isolation may not hold.

### Mechanism 3
- Claim: SGD can match or exceed Adam performance at small batch sizes given sufficient iterations, because SGD's critical batch size is approximately 1.
- Mechanism: Under fixed token budget, SGD underperforms at large batches because it cannot convert batch size into acceleration—progress scales with iteration count. Adam converts larger batches into faster per-step progress up to its critical batch size.
- Core assumption: Token budget (data exposure) is the constraint of interest, not wall-clock time.
- Evidence anchors:
  - [Section 2.3, Figure 3]: Left panel shows gap widens with batch size at fixed tokens; right panel shows gap shrinks with more steps at fixed batch.
  - [Section 2.3, Figure 4]: At batch=16 with many steps, SGD outperforms Adam.
  - [Figure 5]: 410M and 1B models confirm SGD can outperform Adam at scale with small batches.
- Break condition: If memory constraints force gradient accumulation (effectively increasing batch), the small-batch advantage disappears; this is impractical for distributed training.

## Foundational Learning

- Concept: **Stochastic Differential Equations (SDEs) for optimization**
  - Why needed here: The theoretical argument relies on interpreting discrete optimizers as continuous-time processes to isolate drift vs. diffusion contributions.
  - Quick check question: Can you explain why drift terms affect convergence rate while diffusion terms affect variance?

- Concept: **Critical batch size**
  - Why needed here: The paper hinges on different optimizers having different critical batch sizes—the batch size beyond which per-step progress saturates.
  - Quick check question: If an optimizer has critical batch size B_c = 64, what happens to per-iteration progress when increasing batch from 32 to 128?

- Concept: **Gradient grafting**
  - Why needed here: Experimental technique to isolate direction vs. magnitude contributions to optimizer behavior.
  - Quick check question: If optimizer A's direction with optimizer B's magnitude performs like A, what does this imply?

## Architecture Onboarding

- Component map:
  - Optimizer core -> SGD with momentum (β=0.98 recommended), Adam (β₁=β₂=0.95), SignSGD (for theoretical comparison)
  - Clipping -> Global gradient norm clipping (threshold=1.0) before momentum; adaptive momentum clipping as alternative
  - Learning rate schedule -> Cosine with 10% warmup or WSD scheduler
  - Model -> Pre-LN Transformer with RMSNorm, RoPE, SwiGLU (standard GPT-style)

- Critical path:
  1. Choose batch size first—this determines optimizer viability
  2. For batch < 64: SGD viable; tune β ∈ {0.95, 0.98}, lr ∈ {0.5, 1.0}
  3. For batch ≥ 256: Adam strongly preferred; tune lr ∈ {1e-3, 5e-3}
  4. Enable global norm clipping (threshold=1.0) for all SGD runs

- Design tradeoffs:
  - Small batch + SGD: Lower memory, more iterations, can match Adam but slower wall-clock
  - Large batch + Adam: Faster convergence per step, higher memory, standard practice
  - No weight decay in experiments—isolate optimizer effects first

- Failure signatures:
  - SGD diverges at large batch without clipping → enable global norm clipping
  - SGD plateauing at large batch despite tuning → direction problem inherent; switch optimizers or reduce batch
  - Normalization layers show most clipping in adaptive scheme → monitor per-layer gradient statistics

- First 3 experiments:
  1. Replicate Figure 1 sweep at your scale: batch sizes {64, 256, 1024}, lr grid, β ∈ {0.9, 0.95, 0.98} for both optimizers on 1.3B tokens.
  2. Ablate clipping: run SGD at batch=1024 with and without global norm clipping to confirm divergence without it.
  3. Token budget test: train at batch=16 for 160k steps vs batch=128 for 20k steps (same tokens) to verify SGD improves with iterations, not batch size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical SDE framework for signed gradient methods be rigorously extended to heavy-tailed gradient noise distributions?
- Basis in paper: [explicit] "While Compagnoni et al. [2025b] provide a similar result for the Heavy-tail noise setting, the Gaussian case already highlights a crucial distinction..." and "Gaussianity is not strictly needed for our insights... a similar expression can hold even for distributions with heavier tails, such as the t-student."
- Why unresolved: The paper provides theory for Gaussian noise and mentions heavy tails, but does not provide formal proofs or empirical validation for non-Gaussian settings common in language modeling.
- What evidence would resolve it: Formal theoretical analysis of SignSGD/Adam SDEs under heavy-tailed noise assumptions, combined with empirical validation on models exhibiting such noise structures.

### Open Question 2
- Question: Does the small-batch SGD competitiveness finding persist at model scales beyond 1B parameters and across diverse architectural variants?
- Basis in paper: [explicit] "We believe that future theoretical work might be able to better explain the Adam-SGD gap as a function of the batch size" and experiments only go up to 1B parameters.
- Why unresolved: The paper demonstrates SGD competitiveness up to 1B parameters but does not explore larger scales or architectural variants (e.g., mixture-of-experts, different attention mechanisms).
- What evidence would resolve it: Systematic experiments with SGD vs Adam at 7B+ parameter scales and across architectural variants, using the small-batch regime identified in the paper.

### Open Question 3
- Question: Can the insights about batch-size-dependent drift terms lead to hybrid optimizers that combine SGD's iteration efficiency with Adam's batch-size scaling?
- Basis in paper: [inferred] The theoretical analysis shows fundamentally different batch-size dependencies between SGD and signed gradient methods, suggesting hybrid approaches might be possible.
- Why unresolved: The paper identifies the mechanistic difference but does not explore whether this understanding can inform the design of new optimization algorithms.
- What evidence would resolve it: Development and empirical testing of novel hybrid optimizers that explicitly incorporate batch-size-aware drift components.

## Limitations
- The theoretical framework assumes Gaussian gradient noise, though heavy-tailed distributions are common in language modeling
- Empirical validation focuses on GPT-style transformers with specific normalization (RMSNorm), limiting generalization to other architectures
- The token-budget framing assumes data exposure is the primary constraint rather than wall-clock time in practical settings

## Confidence

**High confidence**: SGD can match Adam at small batch sizes when properly tuned (β≥0.95, lr∈[0.5,1.0], global norm clipping). This is directly demonstrated across multiple batch sizes and model scales with clear performance reversals.

**Medium confidence**: The theoretical mechanism explaining why signed gradient methods benefit from larger batches (erf-based drift term) while SGD does not. While the SDE derivation is sound under stated assumptions, empirical validation of the continuous-time approximation is limited.

**Medium confidence**: Gradient direction being the primary factor in large-batch SGD failure, based on grafting experiments. The isolation of direction vs. magnitude is convincing, but interactions at extreme learning rates remain unexplored.

## Next Checks

1. **Extend SDE validation**: Test the erf-scaling prediction by measuring early-training progress across a finer grid of batch sizes (e.g., 2, 4, 8, 16) for both SGD and SignSGD variants, quantifying whether progress follows √B scaling until saturation.

2. **Cross-architecture generalization**: Replicate the batch-size sweep on a different architecture (e.g., BERT-style with LayerNorm or a vision transformer) to determine if the SGD-critical-batch-size ≈ 1 finding is architecture-dependent.

3. **Memory-constrained scenario**: Evaluate SGD with gradient accumulation (to simulate large effective batch) on the same hardware budget as large-batch Adam to quantify the practical wall-clock tradeoff, validating whether the small-batch advantage holds under realistic constraints.