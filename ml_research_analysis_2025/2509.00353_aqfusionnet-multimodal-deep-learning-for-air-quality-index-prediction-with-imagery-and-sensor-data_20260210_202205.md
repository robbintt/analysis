---
ver: rpa2
title: 'AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with
  Imagery and Sensor Data'
arxiv_id: '2509.00353'
source_url: https://arxiv.org/abs/2509.00353
tags:
- sensor
- prediction
- data
- learning
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AQFusionNet is a multimodal deep learning framework that integrates
  atmospheric imagery with sensor data for robust AQI prediction. It uses lightweight
  CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0) and a dual-objective learning
  approach to fuse visual and sensor features.
---

# AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data

## Quick Facts
- arXiv ID: 2509.00353
- Source URL: https://arxiv.org/abs/2509.00353
- Reference count: 25
- Primary result: 92.02% classification accuracy and RMSE of 7.70 with EfficientNet-B0 backbone, 18.5% improvement over unimodal baselines

## Executive Summary
AQFusionNet is a multimodal deep learning framework that integrates atmospheric imagery with sensor data for robust AQI prediction. It uses lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0) and a dual-objective learning approach to fuse visual and sensor features. The model achieves 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone, representing an 18.5% improvement over unimodal baselines. Its lightweight design (2.41–4.2 million parameters) supports edge deployment, and its robustness under partial sensor availability makes it suitable for resource-constrained regions.

## Method Summary
The model combines pretrained CNN backbones with multimodal fusion for AQI prediction. Images pass through truncated ImageNet-pretrained backbones (MobileNetV2, ResNet18, or EfficientNet-B0) with projection heads to 128D embeddings. Sensor data (6 pollutant values) passes through an MLP to 128D embeddings. These are concatenated and fused through a nonlinear layer, feeding dual heads: AQI prediction and sensor reconstruction. The composite loss (1-α)L_AQI + αL_sensor (α=0.4) enables cross-modal alignment. Training uses AdamW with cosine annealing, early stopping, and standard augmentations on a stratified 70/15/15 split of the Air Pollution Image Dataset (8,247 samples from 15 Indian/Nepalese cities, 2019-2022).

## Key Results
- EfficientNet-B0 backbone achieves 92.02% classification accuracy and RMSE of 7.70
- 18.5% improvement over unimodal baselines
- 45.8% RMSE reduction compared to Wang et al. [23]
- Lightweight design: 2.41–4.2 million parameters supporting edge deployment

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Learning for Cross-Modal Alignment
The composite loss forces the image encoder to learn visual features that can reconstruct sensor values (PM2.5, PM10, NO2, SO2, CO, O3). This auxiliary task creates shared semantic structure between modalities, so when sensor data is missing, the model can fall back on visually-inferred estimates. The relationship is learnable from the training distribution where atmospheric haze, sky clarity, and other visual cues contain sufficient signal to infer pollutant concentrations. Break condition: If visual conditions decouple from pollutant levels, or if sensor modalities have no visual proxy, reconstruction loss provides noisy signal and may degrade AQI head.

### Mechanism 2: Transferable Visual Features with ImageNet Pretraining
Pretrained backbones provide general visual feature extractors. The projection head maps backbone outputs to 128-dimensional embeddings aligned with sensor embeddings via the fusion layer. This enables the model to leverage complementary information—visual texture for particulate pollution, sensors for precise concentrations. Assumption: ImageNet features transfer meaningfully to atmospheric imagery; the domain gap is bridgeable without extensive domain-specific pretraining. Break condition: If atmospheric imagery differs radically from ImageNet, pretrained features may underperform; from-scratch training or domain-specific pretraining may be needed.

### Mechanism 3: Efficient Concatenation-Based Fusion
Image and sensor embeddings are projected to aligned dimensions (128D each), then concatenated and passed through a fusion layer with ReLU and dropout. This late-fusion approach preserves modality-specific feature extraction while learning cross-modal interactions in the fusion layer weights. Assumption: Late fusion is sufficient; early or intermediate fusion (e.g., cross-attention) is unnecessary for this task. Break condition: If complex cross-modal interactions are critical, simple concatenation may underperform compared to attention-based fusion.

## Foundational Learning

- **Transfer Learning from ImageNet**
  - Why needed here: The image encoder relies on ImageNet-pretrained weights. Understanding feature reuse, fine-tuning strategies, and domain shift is critical for reproducing results or adapting to new regions.
  - Quick check question: What layers would you freeze if adapting to a dataset with vastly different atmospheric conditions (e.g., polar regions)?

- **Multi-Task / Dual-Objective Learning**
  - Why needed here: The model jointly optimizes AQI prediction and sensor reconstruction. Understanding loss weighting (α=0.4), gradient interactions, and task balancing is essential for debugging convergence or adding new auxiliary tasks.
  - Quick check question: If sensor reconstruction loss plateaus but AQI loss continues decreasing, should you adjust α? Why or why not?

- **Multimodal Embedding Alignment**
  - Why needed here: Image and sensor embeddings must be in a shared space for effective fusion. Understanding projection heads, dimensionality choices, and alignment objectives informs architecture modifications.
  - Quick check question: What would happen if image embeddings were 512D and sensor embeddings were 128D without alignment? How would you diagnose misalignment?

## Architecture Onboarding

- **Component map:**
  Input: (Image 224×224×3, Sensor vector d=6) → Image Encoder → Projection Head → h_I (128D)
  Sensor Encoder → h_S (128D) → Fusion: Concatenate [h_I; h_S] → FC + ReLU + Dropout → h_fused
  → Dual Heads: AQI Head (Linear) → ŷ (AQI value), Sensor Estimation Head (Linear) → x̂_S (reconstructed sensors)

- **Critical path:**
  1. Load ImageNet-pretrained backbone, truncate before classification head
  2. Initialize projection, sensor encoder, fusion, and prediction heads randomly
  3. Forward pass: extract image features → project to 128D; encode sensors to 128D
  4. Fuse, compute both outputs, backpropagate composite loss with α=0.4
  5. Use AdamW (lr=3e-4, weight_decay=1e-4), cosine annealing, early stopping (patience=7)

- **Design tradeoffs:**
  - MobileNetV2 (~2.41M params): Best for edge deployment, slightly lower accuracy
  - ResNet18 (~11.27M params): Strong validation performance, best convergence stability
  - EfficientNet-B0 (~4.2M params): Best test generalization (RMSE 7.70, 92.02% accuracy), moderate size
  - Assumption: Late fusion with concatenation is sufficient; more complex fusion may improve performance but increases latency and parameter count

- **Failure signatures:**
  - High validation RMSE with low training RMSE: Overfitting; increase dropout, augment data, or reduce model capacity
  - Sensor reconstruction loss stagnant: Visual features may not capture relevant pollutant signals; check data quality, consider adding weather metadata
  - Large gap between validation and test performance: Distribution shift; evaluate per-city or per-season performance
  - Grad-CAM shows no clear attention patterns: Backbone may be undertrained; verify pretrained weights loaded correctly, increase fine-tuning learning rate

- **First 3 experiments:**
  1. **Baseline reproduction**: Train EfficientNet-B0 variant with given hyperparameters on provided split (70/15/15). Verify RMSE ≤8.0 and accuracy ≥91%. Log validation curves and Grad-CAM visualizations.
  2. **Ablation on dual-objective**: Set α=0 (sensor reconstruction disabled). Compare RMSE and accuracy to full model. If degradation <5%, sensor reconstruction may be optional for this dataset.
  3. **Partial sensor robustness test**: Randomly mask 20-50% of sensor inputs during inference. Measure RMSE degradation. If degradation is severe, explore increasing α or adding explicit sensor imputation module.

## Open Questions the Paper Calls Out

- **How does the integration of temporal attention mechanisms affect the model's ability to capture seasonal and trend-based patterns for long-term AQI forecasting?**
  - The current AQFusionNet processes input as static snapshots without temporal memory or sequence modeling, limiting it to immediate prediction rather than forecasting. Comparative results on multi-step ahead forecasting tasks using a modified AQFusionNet with temporal attention layers versus the current static implementation would resolve this.

- **Can unsupervised domain adaptation techniques enable the framework to maintain high accuracy in new geographic regions without extensive labeled retraining?**
  - The current evaluation is restricted to 15 cities in India and Nepal. Performance metrics from experiments testing the model on out-of-distribution datasets (e.g., US or European cities) with and without domain adaptation layers would resolve this.

- **To what extent does the inclusion of non-ideal imagery (nighttime, rain, winter) degrade or improve the cross-modal alignment and robustness of the visual feature extractor?**
  - The model currently relies on ImageNet pre-training and daylight data. Grad-CAM visualizations and classification accuracy reports specifically evaluated on nighttime and precipitation-heavy samples would resolve this.

- **How does the fusion mechanism handle performance degradation when the visual modality is missing or of significantly lower quality than the sensor modality?**
  - While the paper emphasizes robustness under "partial sensor availability," it does not explicitly evaluate scenarios where the image is missing or corrupted. An ablation study showing test performance when the image input is removed or replaced with noise would resolve this.

## Limitations

- The image-to-sensor reconstruction auxiliary task assumes strong visual-pollutant coupling that may not hold in all atmospheric conditions (e.g., night, rain, colorless gases).
- No ablation studies compare fusion strategies (concatenation vs. attention vs. cross-modal transformers).
- Dataset composition (city distribution, seasonal coverage) is not detailed; results may be biased toward training regions.

## Confidence

- **High**: Dual-objective learning improves accuracy over unimodal baselines (18.5% RMSE reduction demonstrated)
- **Medium**: Cross-modal reconstruction enables graceful degradation under partial sensor unavailability (conceptually sound but not exhaustively validated)
- **Medium**: Lightweight design supports edge deployment (parameter counts provided, but real-world latency/battery tests missing)

## Next Checks

1. Conduct cross-validation across seasons and cities to assess generalization and potential regional bias.
2. Compare simple concatenation fusion against attention-based fusion to quantify potential performance gains.
3. Measure inference latency and memory usage on a representative edge device (e.g., Jetson Nano) to validate deployment claims.