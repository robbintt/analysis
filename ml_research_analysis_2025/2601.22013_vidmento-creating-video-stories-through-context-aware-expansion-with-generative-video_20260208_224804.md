---
ver: rpa2
title: 'Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative
  Video'
arxiv_id: '2601.22013'
source_url: https://arxiv.org/abs/2601.22013
tags:
- video
- generative
- story
- creators
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Vidmento, a video authoring tool that blends
  captured and generative video to expand incomplete stories. Vidmento enables creators
  to visualize narrative gaps, generate contextually connected clips, and refine outputs
  while preserving creative control.
---

# Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video

## Quick Facts
- **arXiv ID**: 2601.22013
- **Source URL**: https://arxiv.org/abs/2601.22013
- **Reference count**: 40
- **Primary result**: Vidmento enables creators to expand incomplete video stories using context-aware generative expansion, preserving creative control while filling narrative gaps.

## Executive Summary
Vidmento is a video authoring tool that integrates captured and generative video to help creators expand incomplete narratives. The system provides a semi-structured canvas where users can visualize story gaps, receive context-aware clip suggestions, and refine outputs while maintaining creative ownership. A user study with 12 creators demonstrated that Vidmento supports expressive storytelling by helping users transform initial footage into compelling narratives through visual story manipulation and grounded generative expansion.

## Method Summary
Vidmento employs a two-stage context-aware generation pipeline: first generating contextual keyframes conditioned on surrounding shots and story context, then animating these keyframes into video using Veo3. The system uses multimodal LLMs for semantic clustering, narrative sequencing, and Socratic-style script suggestions. The interface combines a React Flow node-based canvas with a Tiptap script editor, allowing users to work across visual storyline, scene, and shot levels. The backend orchestrates between Gemini 2.5 Flash for LLM tasks, Gemini Image for keyframes, and Veo3 for video generation.

## Key Results
- 12 creators generated an average of 31 images and 8 videos per session, demonstrating sustained engagement
- Participants valued the unified workspace combining script, timeline, and canvas views for story development
- Users appreciated the ability to generate new perspectives grounded in their existing media while maintaining creative control

## Why This Works (Mechanism)

### Mechanism 1
Context-aware generation likely bridges narrative gaps more effectively than isolated prompting by anchoring visual outputs in existing story constraints. The system employs a two-stage pipeline where generated clips are conditioned on "neighboring shots" (visual continuity) and "story context/scene script" (narrative coherence), reducing the search space for the generative model.

### Mechanism 2
A semi-structured canvas likely aids story exploration by externalizing narrative structure, reducing cognitive load of maintaining complex timelines. Vidmento transforms a linear timeline into a spatial graph where nodes represent shots/scenes, visually surfacing "narrative gaps" as empty edges between nodes.

### Mechanism 3
Socratic-style script suggestions likely preserve creative agency better than auto-generation, encouraging user retention of "authorial voice." Instead of generating full scripts, the system provides prompts based on narrative theory, forcing users to synthesize final text and maintain ownership.

## Foundational Learning

- **Concept: Continuity Editing & Film Grammar**
  - **Why needed here**: The system's prompts are explicitly grounded in "cinematography principles" (shot types, angles). Understanding shot types is necessary to debug why the model generates specific suggestions.
  - **Quick check question**: Can you distinguish between an "establishing shot" and a "reaction shot," and explain why swapping them would alter narrative meaning?

- **Concept: Multimodal Context Window Constraints**
  - **Why needed here**: Vidmento relies on feeding previous/next shots and scripts into the model. You must understand token limits and latency trade-offs when expanding a "context window" to include multiple video frames and long scripts.
  - **Quick check question**: If a user uploads 100 high-resolution photos, how should the system subsample or summarize them to fit into the context window without losing narrative thread?

- **Concept: Graph-Based UI State Management**
  - **Why needed here**: The interface uses a React Flow canvas where "nodes" represent media. Understanding Directed Acyclic Graphs (DAGs) is required to implement features like "visual storyline" logic and edge creation for transitions.
  - **Quick check question**: How do you handle state synchronization between the linear "Script Editor" and the spatial "Canvas" graph when a user deletes a node?

## Architecture Onboarding

- **Component map**: React/TypeScript frontend (React Flow + Tiptap) -> Python/Flask backend orchestration -> Gemini 2.5 Flash (LLM) -> Gemini Image (Keyframes) -> Veo3 (Video Gen)

- **Critical path**: 1) Ingestion: User uploads media -> System generates descriptions (Captioning) using multimodal LLM; 2) Structuring: LLM groups descriptions into Scenes -> Sequences Scenes into narrative graph; 3) Expansion: User selects gap -> System constructs prompt using (Previous Shot, Next Shot, Scene Script) -> Generates Keyframe -> Generates Video

- **Design tradeoffs**: Scaffolding vs. Automation (defaults to suggestion rather than execution to prioritize ownership); Semi-structured Canvas (enforces visual storyline flow but allows freeform placement)

- **Failure signatures**: "Vibe" Drift (generated clips clash with amateur aesthetic of captured footage); Hallucinated Transitions (model suggests scenes contradicting user's actual memories)

- **First 3 experiments**: 1) Context Ablation: Generate shots with only text context vs. only visual neighbors vs. both; 2) Latency Masking: Test user tolerance for ~40s generation time with distraction UI elements; 3) Style Transfer Tuning: Experiment with style extraction from user's footage to match "cinematic reality"

## Open Questions the Paper Calls Out

### Open Question 1
How can generative video systems support spatial or stylistic blending in addition to temporal blending? While Vidmento focused on temporal blending across shots, emerging techniques suggest opportunities for spatial blending within frames and stylistic blending across shots.

### Open Question 2
How can generative models be adapted to match the physical constraints and aesthetic "vibe" of imperfect captured footage? Generated clips often clash with the raw, constrained nature of amateur or ungraded user-captured video, highlighting the need for models to adapt to the "creator's reality."

### Open Question 3
Can AI-assisted authoring systems dynamically sense when a user prefers full automation versus scaffolding? Current tools typically offer fixed assistance levels, but the study found user preference shifts dynamically between exploring (automation) and refining (scaffolding).

## Limitations
- Small sample size (n=12) limits generalizability of user study findings
- Lack of quantitative metrics for narrative quality or coherence
- Style-matching problem between generative outputs and user-captured footage remains unresolved
- Effectiveness depends heavily on prompt engineering details not fully specified

## Confidence

- **High Confidence**: Technical implementation of dual-stage generation pipeline and user interface design
- **Medium Confidence**: User study findings regarding creative agency preservation and feature utility
- **Low Confidence**: Claims about superiority of context-aware generation over isolated prompting, and effectiveness of Socratic-style suggestions

## Next Checks

1. Conduct controlled experiment comparing narrative coherence scores between context-aware generation (using surrounding shots + script) versus text-only prompting across 50+ participants.

2. Implement quantitative metric for style consistency between generated and captured footage, then test whether explicit style conditioning improves user satisfaction scores.

3. Run comparative study measuring creative ownership and satisfaction between Socratic-style script suggestions versus fully auto-generated scripts in larger sample (n=30+).