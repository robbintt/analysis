---
ver: rpa2
title: Horizon Activation Mapping for Neural Networks in Time Series Forecasting
arxiv_id: '2601.02094'
source_url: https://arxiv.org/abs/2601.02094
tags:
- gradient
- sub-series
- causal
- norm
- anti-causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Horizon Activation Mapping (HAM) is introduced as a gradient-based
  interpretability method for time series forecasting models, inspired by grad-CAM
  but adapted for temporal data through causal and anti-causal subseries analysis.
  It uses gradient norm averages to visualize how models attend to different parts
  of the forecasting horizon, enabling cross-model comparisons regardless of architecture
  type.
---

# Horizon Activation Mapping for Neural Networks in Time Series Forecasting

## Quick Facts
- arXiv ID: 2601.02094
- Source URL: https://arxiv.org/abs/2601.02094
- Authors: Hans Krupakar; V A Kandappan
- Reference count: 18
- Key outcome: HAM visualizes how forecasting models attend to different parts of the horizon using gradient norm averages, enabling cross-model comparisons regardless of architecture type.

## Executive Summary
Horizon Activation Mapping (HAM) introduces a gradient-based interpretability method for time series forecasting models, inspired by grad-CAM but adapted for temporal data through causal and anti-causal subseries analysis. By masking the loss function over specific temporal subseries and analyzing gradient norm averages, HAM reveals how different model architectures attend to various parts of the forecasting horizon. Experiments on the ETTm2 dataset across multiple model families demonstrate that HAM captures distinct learning patterns, such as exponential activity in SpaceTime models versus linear combinations in NHITS models, providing granular insights for model selection and validation.

## Method Summary
HAM applies binary masks to the loss function, isolating specific temporal subseries from 0 to h (causal) or h to H (anti-causal). For each timestep h in the horizon H, the method computes gradient norm averages by backpropagating through the masked loss, then aggregates these across the validation set. The resulting curves visualize temporal attention patterns, with areas between curves and proportionality lines indicating emphasis on short vs. long-term forecasting. The intersection point of causal and anti-causal curves identifies the gradient equivariant point, revealing the model's effective temporal decision boundary.

## Key Results
- HAM successfully distinguishes learning patterns across diverse architectures (MLP, Transformer, SSM, diffusion) on ETTm2 dataset
- SpaceTime models show exponential autoregressive activities while NHITS exhibits linear combinations of terms
- Difference plots and interpolated areas highlight model-specific emphasis on short vs. long-term forecasting
- HAM reveals batch size effects, early stopping behavior, and dropout impacts on temporal attention patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masking the loss function over specific temporal subseries isolates the gradient updates relevant to those time steps, revealing how model parameters prioritize different parts of the horizon.
- **Mechanism:** The method applies a binary mask $M_m(\hat{h}, H)$ to the loss before backpropagation. By iteratively shifting the mask endpoint $\hat{h}$ across the horizon $H$, it computes the gradient norm average $\nabla_\theta L$ for "causal" (start-to-current) and "anti-causal" (current-to-end) segments. Higher norm averages indicate greater parameter sensitivity to that specific subseries.
- **Core assumption:** The magnitude of the gradient norm correlates with the "importance" or "attention" the model places on that specific time segment for the prediction task.
- **Evidence anchors:** [abstract] "uses gradient norm averages to study the horizon's subseries"; [section 2] "Given a horizon of size H, the causal mode c includes subseries from 0 to any timestep h... The mask M applied over the loss function... is given by [Eq 1]"
- **Break condition:** If the model architecture involves heavy gradient blocking (e.g., stop-gradient operations) or if the loss surface is extremely flat, the norm averages may fail to reflect actual temporal importance.

### Mechanism 2
- **Claim:** Deviations from a "line of proportionality" (representing uniform attention) serve as a diagnostic signal for architectural bias toward short-term or long-term dependencies.
- **Mechanism:** A linear line connecting $y=0$ to $y=G$ (max gradient norm) represents a hypothetical model that attends uniformly across the horizon. HAM calculates the signed area between the model's actual gradient curve and this line. Positive areas indicate emphasis on the masked region; negative areas indicate neglect relative to a uniform distribution.
- **Core assumption:** A "uniform" gradient distribution is a valid neutral baseline for comparing diverse architectures (MLP, Transformer, SSM).
- **Evidence anchors:** [abstract] "...lines of proportionality signifying uniform distributions of the norm averages."; [section 2] "...areas between the curves and the lines of proportionality are calculated... areas below the lines are negative..."
- **Break condition:** If the dataset inherently requires non-uniform attention (e.g., highly seasonal data where only specific future steps matter), the "proportionality" baseline may be a misleading target for "ideal" performance.

### Mechanism 3
- **Claim:** HAM visualizes the "Gradient Equivariant Point," where the total gradient contribution of the first $h$ timesteps equals that of the remaining $H-h$ timesteps, revealing the effective "memory" or "lookback" behavior of the model.
- **Mechanism:** The causal and anti-causal curves are plotted simultaneously. Their intersection point $h_{eq}$ identifies the timestep where the gradient norm average of the initial segment equals that of the terminal segment. Shifts in $h_{eq}$ during training or across architectures (e.g., NHITS vs. SpaceTime) indicate changes in how the model balances short-term vs. long-term accuracy.
- **Core assumption:** The intersection of causal and anti-causal gradient norms is a stable indicator of the model's temporal decision boundary.
- **Evidence anchors:** [section 2] "The causal and anti-causal mode curves intersect at the gradient equivariant point h in the horizon..."; [section 4.2] "Pyraformer's difference curves... correspond to its largest errors... [while] SpaceTime's gradient equivariant points [are] farthest away..."
- **Break condition:** If the gradient norms are noisy or do not converge smoothly, the intersection point may fluctuate erratically, rendering it uninformative.

## Foundational Learning

- **Concept: Gradient-based Attribution (Grad-CAM)**
  - **Why needed here:** HAM is explicitly inspired by Grad-CAM. You must understand how gradient flows backward from a specific output class (or in this case, a masked loss) to weight the importance of input features (or in this case, temporal subseries).
  - **Quick check question:** How does masking the loss for a specific time step differ from masking the input data directly?

- **Concept: Time Series Forecasting Horizons (H)**
  - **Why needed here:** The entire analysis depends on the decomposition of the prediction window $H$ into subseries. You need to distinguish between the lookback window (input) and the horizon (output), as HAM analyzes the latter.
  - **Quick check question:** If a model predicts $H=720$ steps, does a high gradient norm in the $[0, 100]$ subseries imply the model is focusing on short-term or long-term forecasting?

- **Concept: State Space Models (SSMs) vs. Attention**
  - **Why needed here:** The paper compares architectures (e.g., SpaceTime vs. Transformers). Understanding how SSMs theoretically handle long-range dependencies differently from attention mechanisms helps interpret why their HAM plots (exponential vs. linear) look different.
  - **Quick check question:** Why might an SSM-based model like SpaceTime show "exponential autoregressive activities" compared to a standard Transformer?

## Architecture Onboarding

- **Component map:** Forward Pass -> Masking Module -> Gradient Norm Computer -> Plotting Engine
- **Critical path:** Implementing the masking function $M_m(\hat{h}, H)$ correctly. A bug here (e.g., off-by-one errors in indexing the horizon) will invalidate the interpretability signal entirely, as you would be attributing gradients to the wrong time steps.
- **Design tradeoffs:**
  - **Batch Size vs. Granularity:** The paper notes that batch size changes affect gradient norms (polynomial saturation). Using large batches stabilizes the HAM plot but might obscure fine-grained "online" learning dynamics.
  - **Computational Cost:** Generating a full HAM profile requires $H$ backward passes (or equivalent accumulation) per sample, making it significantly more expensive than a simple validation loss calculation.
- **Failure signatures:**
  - **Flatlines:** If the HAM plot is a flat line at zero, the model is not learning or gradients are vanishing.
  - **Erratic Jumps:** High variance in the curves suggests the model is unstable or the batch size is too small to smooth out gradient noise.
  - **Proportionality Obsession:** If a model perfectly follows the line of proportionality but has high error, it implies the model has learned a trivial "average" forecast rather than useful temporal dynamics.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Train a simple N-Linear or NHITS model on ETTm2 and plot the HAM for a horizon of $H=96$. Verify if the curves are smooth and interpretable.
  2. **Dropout Ablation:** Add dropout to the model and regenerate the HAM. Confirm the observation that dropout increases gradient magnitudes, particularly in longer regions of the horizon (as per Section 3.1).
  3. **Early Stopping Check:** Train a model to convergence and then continue training (overfitting). Plot HAM at early, optimal, and late epochs to visualize the reduction in gradient norm magnitude described in Section 3.3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the observed polynomial trend in gradient norm averages relative to batch size be formalized as a universal approximation law for converged neural networks?
- **Basis:** [explicit] The authors note that Figure 4 shows a "surprising polynomial curve across batch sizes in converged models over a single epoch, motivating future work."
- **Why unresolved:** The paper empirically observes the trend in NHITS, N-Linear, and CycleNet but does not derive the mathematical theory explaining why gradient norms saturate polynomially as batch sizes increase.
- **What evidence would resolve it:** A theoretical proof or extensive empirical regression analysis across diverse architectures establishing a consistent functional relationship between batch size and gradient norm averages.

### Open Question 2
- **Question:** Do the distinct learning patterns identified by HAM, such as SpaceTime's exponential activity, generalize to datasets with different underlying periodicities or noise profiles?
- **Basis:** [inferred] The study relies exclusively on the ETTm2 (Electricity) dataset, while noting that "The model families have yet to find a large enough supervised learning dataset towards comparisons across them."
- **Why unresolved:** It is unclear if the observed causal/anti-causal behaviors (e.g., NHITS neural basis approximation) are intrinsic to the architectures or specific to the spatio-temporal characteristics of electricity load data.
- **What evidence would resolve it:** Application of HAM to diverse benchmarks (e.g., traffic, finance, or forecasting competition datasets) to verify if the "lines of proportionality" and shape signatures remain consistent.

### Open Question 3
- **Question:** Is the characteristic HAM signature of diffusion models—where smaller subseries exhibit larger-than-proportional gradient norms—a fundamental property of the denoising process?
- **Basis:** [inferred] The authors observe specific gradient behaviors in Multi-Resolution DDPM and note that these characteristics "suggest that it could be a property of diffusion."
- **Why unresolved:** The paper posits this connection based on experimental results with one diffusion model but does not isolate the diffusion mechanism as the sole cause.
- **What evidence would resolve it:** Comparative studies layering diffusion processes on non-diffusion backbones, or analyzing HAM on a wider variety of diffusion-based forecasting architectures.

## Limitations
- Computational cost is significant, requiring H backward passes per sample, making it impractical for large horizons or resource-constrained settings.
- The "uniform proportionality" baseline as a neutral reference point may be inappropriate for datasets with inherent temporal biases or seasonality patterns.
- The method's reliability depends on gradient norm magnitude correlating with model importance, which may not hold for architectures with gradient-blocking mechanisms.

## Confidence
- **High confidence:** The HAM mechanism for visualizing gradient distributions across forecasting horizons is technically sound and the empirical observations about batch size effects, early stopping behavior, and dropout impact are reproducible given the same experimental conditions.
- **Medium confidence:** The claim that HAM enables cross-model comparisons is supported by the ETTm2 experiments, but the generalizability to other datasets and forecasting tasks requires further validation.
- **Low confidence:** The interpretability of specific architectural insights (e.g., why SpaceTime shows exponential patterns vs. NHITS linear combinations) relies on understanding underlying model mechanisms that are only partially explained.

## Next Checks
1. **Architecture Transfer Test:** Apply HAM to a completely different time series forecasting dataset (e.g., traffic, energy demand) and verify if the gradient distribution patterns remain consistent across the same model families.
2. **Ablation on Baseline:** Create a simple moving average baseline model and confirm that HAM correctly identifies its uniform gradient distribution (following the proportionality line) while capturing the temporal learning patterns of more sophisticated models.
3. **Gradient Norm Correlation:** For a subset of models, directly correlate HAM-identified "important" time steps with leave-one-out ablation studies where those specific horizon positions are randomly perturbed or masked during inference.