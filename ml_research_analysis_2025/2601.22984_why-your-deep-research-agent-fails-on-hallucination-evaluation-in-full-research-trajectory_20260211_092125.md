---
ver: rpa2
title: Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research
  Trajectory
arxiv_id: '2601.22984'
source_url: https://arxiv.org/abs/2601.22984
tags:
- research
- hallucination
- claim
- trajectory
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first systematic process-aware evaluation
  framework for Deep Research Agents (DRAs), shifting from end-to-end outcome metrics
  to fine-grained auditing of the full research trajectory. It proposes the PIES taxonomy
  to categorize hallucinations along functional components (Planning vs.
---

# Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory

## Quick Facts
- **arXiv ID**: 2601.22984
- **Source URL**: https://arxiv.org/abs/2601.22984
- **Reference count**: 40
- **Primary result**: Introduces the first process-aware evaluation framework for Deep Research Agents (DRAs), revealing no system achieves robust reliability and exposing systemic hallucination propagation and cognitive biases.

## Executive Summary
This work addresses the critical gap in evaluating Deep Research Agents by shifting from end-to-end outcome metrics to fine-grained auditing of the full research trajectory. The authors introduce a systematic framework that tracks hallucination occurrences throughout the entire research process, revealing that current DRAs suffer from severe reliability issues including hallucination propagation and cognitive biases. By constructing DeepHalluBench, a benchmark of 100 hallucination-prone tasks, and applying their evaluation framework to six state-of-the-art DRAs, the study exposes fundamental architectural deficits that go beyond simple retrieval limitations.

The research demonstrates that DRAs consistently fail to produce reliable outputs even on tasks designed to be hallucination-prone, with no system achieving satisfactory performance. The framework's process-aware approach reveals how hallucinations compound through research trajectories, while identifying specific cognitive biases like anchor effects and homogeneity bias that systematically degrade performance. This diagnostic analysis provides a foundation for future architectural optimizations targeting the root causes of unreliability in autonomous research systems.

## Method Summary
The paper proposes a process-aware evaluation framework for Deep Research Agents that moves beyond end-to-end metrics to audit the full research trajectory. The framework employs the PIES taxonomy to categorize hallucinations across functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit), creating a structured approach to hallucination detection. This taxonomy is instantiated into a rigorous evaluation framework that enables fine-grained analysis of hallucination types and their propagation patterns. The methodology is applied to construct DeepHalluBench, a benchmark consisting of 100 tasks specifically designed to be prone to hallucinations, allowing systematic evaluation of six state-of-the-art DRAs under controlled conditions.

## Key Results
- No tested DRA achieves robust reliability on the DeepHalluBench benchmark of 100 hallucination-prone tasks
- Hallucination propagation is identified as a systemic deficit where errors compound through research trajectories
- Cognitive biases including anchor effect and homogeneity bias are exposed as significant performance degraders

## Why This Works (Mechanism)
The framework works by providing a systematic lens to diagnose where and how DRAs fail throughout the entire research process rather than just at the final output. By categorizing hallucinations along functional and error dimensions, it reveals that failures are not random but follow predictable patterns tied to specific architectural components. The process-aware approach captures the temporal dynamics of error propagation, showing how initial mistakes cascade through subsequent reasoning steps, while the bias analysis demonstrates that DRAs exhibit systematic deviations from optimal decision-making similar to human cognitive biases.

## Foundational Learning

1. **PIES Taxonomy**: Classifies hallucinations by functional component (Planning/Summarization) and error property (Explicit/Implicit). Needed to create a structured diagnostic framework; quick check: apply taxonomy to sample hallucinations to verify coverage.

2. **Process-Aware Evaluation**: Shifts from outcome-only metrics to auditing entire research trajectories. Needed to capture error propagation dynamics; quick check: trace hallucination sources through multi-step research tasks.

3. **Hallucination Propagation**: Errors compound through research trajectories as initial mistakes influence subsequent reasoning. Needed to understand systemic reliability issues; quick check: identify cascade patterns in failed research sequences.

4. **Cognitive Biases in DRAs**: Systems exhibit anchor effects and homogeneity bias similar to human decision-making. Needed to explain systematic performance patterns; quick check: test bias effects through randomized task ordering experiments.

5. **Full Research Trajectory Analysis**: Evaluates complete research workflows rather than isolated outputs. Needed to capture contextual factors affecting performance; quick check: compare trajectory-level vs. endpoint-only evaluation metrics.

6. **Benchmark Construction for Failure Modes**: Creates tasks specifically designed to trigger hallucinations. Needed to enable systematic evaluation of failure patterns; quick check: verify benchmark tasks reliably induce target hallucination types.

## Architecture Onboarding

**Component Map**: Task Input -> Planning Module -> Information Retrieval -> Summarization Module -> Output Generation -> Evaluation Framework

**Critical Path**: Planning decisions → Retrieval strategy → Information synthesis → Output generation, where each stage can introduce and propagate hallucinations

**Design Tradeoffs**: Process-aware evaluation provides detailed diagnostics but requires human annotation overhead; taxonomy-based categorization enables systematic analysis but may miss novel hallucination types; benchmark construction enables controlled testing but may not capture all real-world scenarios

**Failure Signatures**: Hallucination propagation patterns show compound error growth; cognitive biases manifest as systematic deviations in task selection and information processing; explicit hallucinations appear as clear factual errors while implicit ones involve subtle reasoning flaws

**First 3 Experiments to Run**:
1. Apply the PIES taxonomy to analyze hallucination patterns in a new DRA to identify dominant failure modes
2. Trace hallucination propagation through multi-step research trajectories to quantify compound error effects
3. Test cognitive bias hypotheses by manipulating task ordering and diversity to observe systematic performance changes

## Open Questions the Paper Calls Out
The paper identifies several open questions including how to develop DRAs that can reliably detect and correct their own hallucinations, whether the identified cognitive biases can be systematically mitigated through architectural changes, and how the evaluation framework can be scaled to handle more complex and diverse research domains. The authors also raise questions about the generalizability of their findings to multilingual DRAs and whether the current taxonomy captures all relevant hallucination types in complex research scenarios.

## Limitations
- Human annotation dependency introduces subjectivity and scalability constraints in hallucination detection
- Taxonomy may not capture all possible hallucination types in complex research trajectories
- Benchmark generalizability to domains outside the selected 100 tasks remains uncertain
- Focus on English-language DRAs limits conclusions about multilingual performance
- Cognitive bias analysis is observational rather than experimentally validated

## Confidence

**High confidence**: Systematic framework design and taxonomy construction based on rigorous methodological approach

**Medium confidence**: Experimental results showing DRAs' unreliability under controlled conditions with specific tasks

**Low confidence**: Cognitive bias analysis requiring deeper psychological validation beyond current observational correlations

## Next Checks

1. Conduct inter-annotator agreement studies to quantify and reduce subjectivity in hallucination detection across PIES taxonomy categories

2. Expand benchmark to include diverse domains and multilingual DRAs to test generalizability beyond current task selection

3. Design controlled experiments to validate cognitive bias hypotheses through randomized task ordering and diversity interventions