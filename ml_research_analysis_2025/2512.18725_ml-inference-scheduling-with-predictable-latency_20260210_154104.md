---
ver: rpa2
title: ML Inference Scheduling with Predictable Latency
arxiv_id: '2512.18725'
source_url: https://arxiv.org/abs/2512.18725
tags:
- batch
- prediction
- interference
- which
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that ML inference serving systems using GPU
  concurrency for better utilization suffer from unpredictable interference that harms
  scheduling accuracy. It evaluates existing coarse-grained interference prediction
  methods, finding that they ignore runtime co-location dynamics and use static models
  that degrade under changing workloads.
---

# ML Inference Scheduling with Predictable Latency

## Quick Facts
- arXiv ID: 2512.18725
- Source URL: https://arxiv.org/abs/2512.18725
- Authors: Haidong Zhao; Nikolaos Georgantas
- Reference count: 32
- Primary result: Static interference predictors in GPU ML inference suffer >60% relative error under changing workloads; online RLS learning reduces MSE to 0.15

## Executive Summary
This paper identifies that ML inference serving systems using GPU concurrency for better utilization suffer from unpredictable interference that harms scheduling accuracy. It evaluates existing coarse-grained interference prediction methods, finding that they ignore runtime co-location dynamics and use static models that degrade under changing workloads. Experiments with real GPU hardware and multiple DNN models show that static interference predictors can have relative errors exceeding 60% and mean squared errors up to 0.55 when workload characteristics shift.

## Method Summary
The paper evaluates GPU kernel interference prediction for concurrent ML inference scheduling. It profiles resource utilization metrics (L2 cache, DRAM, SM throughput) and uses linear regression models to predict interference effects. The study compares offline static models against online learning approaches, particularly recursive least squares (RLS), to assess prediction accuracy under changing workloads and new model deployments.

## Key Results
- Static interference predictors achieve MSE up to 0.55 when workload characteristics shift
- Online RLS learning reduces MSE to as low as 0.15 in heterogeneous settings
- Ignoring runtime co-location dynamics leads to relative errors exceeding 60% in interference prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online learning with recursive least squares (RLS) reduces interference prediction error under changing workloads compared to static offline models.
- Mechanism: The prediction model continuously updates its weight parameters at runtime based on observed prediction errors, allowing it to adapt to data drift and newly deployed models without requiring full retraining cycles.
- Core assumption: Recent serving data is more representative of current workload characteristics than historical training data, and model parameters can be updated efficiently without introducing unacceptable latency.
- Evidence anchors:
  - [abstract]: "Online learning with recursive least squares improves accuracy, reducing MSE to as low as 0.15 in heterogeneous settings."
  - [section 4.3, Table 1]: RLS achieves MSE of 0.1499 on heterogeneous workloads (Test Set 3) versus 0.2065 for offline learning; on unseen models (Test Set 2), RLS achieves 0.3467 versus 0.5480 for offline.
  - [corpus]: Limited direct corpus evidence for RLS-based interference prediction; related papers (MACE, LeMix) discuss co-located retraining/inference but do not evaluate RLS optimizers specifically.
- Break condition: If prediction latency requirements exceed RLS computational overhead, or if workload characteristics change faster than the model's convergence rate, accuracy gains may not translate to scheduling improvements.

### Mechanism 2
- Claim: Tracking runtime co-location dynamics—specifically, which batches overlap and for how long—is necessary to capture actual interference effects.
- Mechanism: Each batch may experience multiple co-location scenarios during execution; interference prediction accuracy depends on correctly attributing resource contention to the appropriate co-located batch(es) based on temporal overlap, not just the state at scheduling time.
- Core assumption: Interference magnitude varies significantly depending on which specific batches co-locate, and coarse-grained approximations (e.g., using only initial co-location state) introduce systematic error.
- Evidence anchors:
  - [abstract]: "coarse-grained methods...ignore runtime co-location dynamics and thus restrict their accuracy in interference prediction."
  - [section 3.1, Figure 4]: Illustrates how Batch 2 briefly co-locates with Batch 1 but then co-locates longer with Batch 3; using only initial state (Batch 1) for prediction introduces error if Batch 3 has different resource demands.
  - [section 4.2]: EWMA smoothing with varying α still yields relative errors exceeding 60% for some predictions, indicating partial consideration of dynamics is insufficient.
  - [corpus]: Weak corpus evidence; neighbor papers do not explicitly address temporal co-location tracking in interference modeling.
- Break condition: If batches have uniform resource demands or co-location durations are relatively stable, coarse-grained approaches may achieve acceptable accuracy with lower overhead.

### Mechanism 3
- Claim: Concurrent batch execution improves GPU utilization and reduces tail latency by mitigating head-of-line (HoL) blocking, but introduces interference that must be predicted.
- Mechanism: Multiple batches execute kernels concurrently on shared GPU resources (L2 cache, DRAM, Tensor Cores). When a single batch does not saturate resources, concurrency fills utilization gaps. However, kernels from different batches contend for the same resources, causing execution time inflation.
- Core assumption: The interference effect is proportional to resource contention intensity and can be estimated from profiled resource throughput metrics (L2 cache, DRAM, SM throughput).
- Evidence anchors:
  - [section 2, Figure 3]: Under stress testing with ResNet-50, concurrent execution achieves lower p99 latency than sequential execution across all co-location scenarios (1-8 co-located tasks).
  - [section 2]: "employing concurrency may mitigate HoL blocking."
  - [section 3]: Interference (ratio of measured to profiled kernel time) can reach 151.66% at the 95th percentile when max batch concurrency is 2.
  - [corpus]: Equinox and BucketServe discuss GPU utilization and batching in LLM serving but do not quantify interference effects from concurrent kernel execution.
- Break condition: If MPS or similar resource partitioning eliminates compute interference but memory contention remains unmodeled, predictions may systematically underestimate delays.

## Foundational Learning

- **Concept: GPU Kernel Execution Model (SIMT)**
  - Why needed: Understanding how batches decompose into kernels and how concurrent kernels contend for shared resources (L2 cache, DRAM, SMs) is prerequisite to interpreting interference mechanisms.
  - Quick check question: On an NVIDIA GPU, what three resource categories does the paper identify as sources of kernel execution interference?

- **Concept: Dynamic Batching and Batch Concurrency**
  - Why needed: The paper's evaluation context assumes familiarity with batching (aggregating requests) and concurrent batch execution (multiple batches running simultaneously).
  - Quick check question: Why might increasing batch size improve throughput for ResNet-50 but not for RoBERTa-B, as shown in Figure 2?

- **Concept: Online Learning with Gradient-Based Optimization**
  - Why needed: The paper compares offline learning, online SGD, and RLS; understanding why RLS outperforms plain SGD requires knowing how optimization algorithms exploit curvature information.
  - Quick check question: What is the key difference between offline learning and online learning in how they handle changing workload characteristics?

## Architecture Onboarding

- **Component map**:
  - Inference Engine (TensorRT) -> Request Scheduler -> Interference Predictor -> GPU Runtime

- **Critical path**:
  1. Request arrives → queued by model
  2. Scheduler evaluates batching opportunity (wait time vs batch size)
  3. Interference Predictor estimates delay from co-located batches
  4. Scheduling decision made (accept/reject/wait)
  5. Batch executed → actual latency observed → online model updated (if adaptive)

- **Design tradeoffs**:
  - Prediction granularity: Coarse-grained (lower overhead, higher error) vs fine-grained (higher accuracy, higher complexity)
  - Model adaptivity: Static (simple deployment, degrades under drift) vs online (adapts, but RLS has higher compute cost than SGD)
  - Concurrency level: Higher concurrency improves utilization but increases interference unpredictability

- **Failure signatures**:
  - Static model deployed on new models: MSE spikes to >0.5 (Table 1, Test Set 2)
  - Ignoring co-location dynamics: Relative errors exceeding 60% even with EWMA smoothing (Figure 5)
  - Underestimated interference: Scheduled batches violate SLOs; p99 latency increases unexpectedly

- **First 3 experiments**:
  1. **Baseline profiling**: Deploy a single model (e.g., ResNet-50), profile kernel execution times and resource throughput metrics (L2 cache, DRAM, SM) at varying batch sizes with no concurrency.
  2. **Interference quantification**: Deploy two models concurrently, measure kernel execution time inflation under co-location, compare against predicted interference from the gpulets-style linear model.
  3. **Online vs offline comparison**: Introduce a new model not in training data, compare MSE trajectories for offline model, online SGD, and online RLS over 1000+ inference requests.

## Open Questions the Paper Calls Out
None

## Limitations

- RLS parameter tuning complexity and its impact on prediction accuracy remain unclear, with the paper only evaluating one RLS configuration (λ = 1, α = 0.98) without sensitivity analysis
- The assumption that observed interference is solely due to resource contention may not hold under GPU thermal throttling or driver-level optimizations that vary across hardware generations
- The evaluation uses TensorRT with synthetic traffic patterns; real-world deployment with diverse client request distributions may exhibit different interference characteristics

## Confidence

- **High confidence**: Interference prediction requires tracking runtime co-location dynamics (supported by direct experimental evidence showing >60% error reduction when tracking actual batch overlap)
- **Medium confidence**: Online RLS learning outperforms offline models (supported by quantitative comparisons, but limited to specific RLS hyperparameters)
- **Medium confidence**: Concurrent execution provides utilization benefits but introduces interference (supported by stress tests, but interference quantification assumes resource contention as sole cause)

## Next Checks

1. **Ablation study on RLS hyperparameters**: Systematically vary forgetting factor λ and learning rate α across multiple workloads to identify optimal configurations and assess sensitivity
2. **Cross-hardware generalization test**: Evaluate the interference prediction model on GPUs from different generations (e.g., NVIDIA A100 vs H100) to verify assumptions about resource contention mechanisms
3. **Real-world deployment monitoring**: Deploy the adaptive interference predictor in a production inference service for one month, tracking prediction accuracy drift, false positive/negative rates in SLO violations, and any unexpected scheduling pathologies