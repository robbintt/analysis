---
ver: rpa2
title: 'Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive
  Reasoning'
arxiv_id: '2509.13352'
source_url: https://arxiv.org/abs/2509.13352
tags:
- reasoning
- layer
- uavs
- agentic
- integration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Agentic UAVs framework, a five-layer architecture
  integrating LLM-driven reasoning with UAV autonomy. The framework addresses limitations
  in current UAV systems by enabling real-time knowledge access, ecosystem integration,
  and collaborative swarm cognition.
---

# Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning

## Quick Facts
- **arXiv ID:** 2509.13352
- **Source URL:** https://arxiv.org/abs/2509.13352
- **Reference count:** 12
- **Primary result:** Agentic UAV framework achieves 91% person detection and 92% action recommendation accuracy using YOLOv11 + GPT-4/Gemma-3 in simulated SAR scenarios, outperforming rule-based baseline (75% detection, 4.5% action recommendation).

## Executive Summary
This paper introduces Agentic UAVs, a five-layer architecture that integrates large language models (LLMs) with UAV autonomy to enable real-time reasoning, tool-calling, and collaborative swarm cognition. The system combines YOLOv11 for perception with GPT-4 or Gemma-3 for contextual analysis and action recommendation in search-and-rescue scenarios. In simulated Hajj pilgrimage environments, the framework demonstrates significantly higher detection confidence (0.79 vs. 0.72) and action recommendation accuracy (92% vs 4.5%) compared to rule-based baselines. Local deployment of a 4B-parameter Gemma-3 model provides 3.34× speedup over cloud-based GPT-4 while maintaining performance. The architecture addresses key limitations in current UAV systems by enabling knowledge access, ecosystem integration, and autonomous decision-making through structured LLM-driven workflows.

## Method Summary
The Agentic UAVs framework implements a five-layer architecture: Perception (YOLOv11 object detection at 30 Hz), Reasoning (GPT-4 API or local Gemma-3 4B with ReAct workflow), Action (UAV control commands), Integration (external tool-calling via MCP/ACP/A2A protocols), and Learning (continuous improvement). The system processes simulated RGB video from Intel RealSense D455 in Gazebo, detecting persons and contextualizing scenes for SAR scenarios. The Reasoning Layer uses structured prompts to generate scene descriptions, confidence scores, and recommended actions, which the Action Layer executes via ROS 2 and PX4 SITL integration. Evaluation uses 44 balanced samples across normal and emergency scenarios, comparing detection confidence, person detection rate, action recommendation rate, and processing time against rule-based YOLO-only baselines.

## Key Results
- **Detection performance:** 91% person detection rate (vs 75% baseline) with higher confidence (0.79 vs 0.72)
- **Action recommendation:** 92% accuracy (vs 4.5% baseline) through LLM-driven contextual analysis
- **Computational efficiency:** Local Gemma-3 4B achieves 3.34× speedup over cloud GPT-4 with comparable accuracy

## Why This Works (Mechanism)
The framework succeeds by combining high-performance perception (YOLOv11) with LLM-driven reasoning that provides contextual understanding beyond object detection. The ReAct workflow enables structured tool-calling and multi-step reasoning, allowing the UAV to analyze scenes, recommend actions, and execute complex missions autonomously. Integration Layer protocols (MCP/ACP/A2A) provide standardized interfaces for ecosystem communication, while the Learning Layer enables continuous adaptation. The architecture balances computational demands by using local models for routine tasks and cloud-based reasoning for complex scenarios.

## Foundational Learning
- **ReAct workflow patterns:** Enables LLMs to reason and act iteratively; needed for structured tool-calling and multi-step decision-making; quick check: verify output schema matches expected JSON format
- **ROS 2 middleware integration:** Provides real-time communication between perception, reasoning, and action layers; needed for synchronized UAV control; quick check: monitor topic timestamps for latency
- **MCP/ACP/A2A protocols:** Standardize external tool integration and agent-to-agent communication; needed for ecosystem interoperability; quick check: validate tool registry format and authentication
- **SITL simulation environment:** Enables safe testing of autonomous behaviors without physical risks; needed for reproducible SAR scenario validation; quick check: verify quadcopter dynamics match PX4 specifications
- **YOLOv11 object detection pipeline:** Provides real-time person detection with confidence scoring; needed for perception layer input to reasoning; quick check: confirm detection accuracy on COCO validation set
- **Prompt engineering for UAV tasks:** Structures LLM inputs for scene analysis and action recommendation; needed to achieve high reasoning accuracy; quick check: test prompt template with sample inputs

## Architecture Onboarding

**Component Map:** Gazebo RealSense -> YOLOv11 -> ROS 2 -> GPT-4/Gemma-3 -> ROS 2 -> PX4 SITL -> Gazebo

**Critical Path:** Perception Layer (YOLOv11) → Reasoning Layer (LLM) → Action Layer (UAV control) → Integration Layer (tool-calling)

**Design Tradeoffs:** Cloud-based GPT-4 provides superior reasoning accuracy (92% action recommendation) but introduces latency and dependency, while local Gemma-3 offers 3.34× speedup with slightly reduced performance, suitable for time-critical missions.

**Failure Signatures:** LLM returns unstructured output breaking downstream processing; ROS 2 message latency causing perception-reasoning desynchronization; tool-calling failures due to missing protocol implementations.

**Three First Experiments:**
1. Deploy YOLOv11 on simulated RealSense video stream and validate person detection accuracy against COCO ground truth
2. Implement basic GPT-4 reasoning node with simplified prompt template and test scene description generation
3. Create mock Integration Layer services to test tool-calling functionality before implementing full MCP/ACP/A2A protocols

## Open Questions the Paper Calls Out
- **Real-world performance gap:** How does the architecture perform under real environmental uncertainties compared to high-fidelity Gazebo simulations? (Basis: sim-to-real gap identified as limitation; validation only in simulation)
- **Hybrid architecture viability:** Can a Tier-1 (rule-based) and Tier-2 (LLM) architecture balance 105× computational overhead while maintaining high contextual reasoning accuracy? (Basis: hybrid designs suggested for speed-accuracy trade-off; experiments compared systems in isolation)
- **Safety assurance:** How can the system guarantee safety and regulatory compliance when Reasoning Layer relies on stochastic LLM outputs? (Basis: safety assurance and regulatory compliance listed as future research requirements; focuses on functional capability rather than formal verification)

## Limitations
- Critical implementation details missing: exact prompt templates, tool definitions, and Gazebo world files prevent exact reproduction
- Performance claims based on proprietary simulated environment, raising questions about real-world generalizability
- Integration Layer protocol implementations lack sufficient technical detail for faithful reproduction
- Local Gemma-3 4B model may be impractical for resource-constrained edge deployment on actual UAVs

## Confidence
- **High confidence:** Architectural design and general methodology are well-documented and reproducible
- **Medium confidence:** Comparative performance metrics are credible but require unavailable simulation assets and prompt templates for exact replication
- **Low confidence:** Ecosystem integration and collaborative swarm cognition claims are primarily conceptual with no quantitative validation

## Next Checks
1. Request exact prompt templates, ReAct workflow specifications, and tool schemas from authors to enable faithful reproduction of Reasoning Layer
2. Reconstruct Hajj pilgrimage simulation environment using crowd simulation tools (SUMO, Carla) to approximate test conditions and validate performance metrics
3. Implement Integration Layer protocol stubs (MCP, ACP, A2A) with mock services to test end-to-end tool-calling functionality before full deployment