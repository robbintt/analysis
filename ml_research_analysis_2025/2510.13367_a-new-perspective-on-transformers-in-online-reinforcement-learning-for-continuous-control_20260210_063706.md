---
ver: rpa2
title: A New Perspective on Transformers in Online Reinforcement Learning for Continuous
  Control
arxiv_id: '2510.13367'
source_url: https://arxiv.org/abs/2510.13367
tags:
- arxiv
- learning
- transformer
- training
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers are rarely used in online model-free RL due to training
  instability and sensitivity to architectural choices. This work systematically evaluates
  how transformers can be effectively applied to continuous control tasks.
---

# A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control

## Quick Facts
- **arXiv ID**: 2510.13367
- **Source URL**: https://arxiv.org/abs/2510.13367
- **Reference count**: 40
- **One-line result**: Transformers can match or exceed MLP, LSTM, CNN, and GTrXL baselines in online RL for continuous control when using EmbedConcat input conditioning, separate backbones, and cross-episode slicing.

## Executive Summary
This work systematically evaluates how transformers can be effectively applied to online model-free reinforcement learning for continuous control tasks. The authors identify and resolve three key challenges: input conditioning in partially observable environments, gradient interference between actor and critic networks, and learning at episode boundaries. Through extensive ablations across MuJoCo, MuJoCo-POMDP, and ManiSkill3 environments, they demonstrate that transformers are competitive with traditional architectures when using EmbedConcat for POMDPs, separate or frozen-backbone actor-critic pairs, and cross-episode data slicing. The findings show transformers can achieve stable, scalable performance in both vector and image-based continuous control settings.

## Method Summary
The method employs a GPT-2-style transformer decoder with pre-layer norm and GELU activation as the backbone for both actor and critic networks. For POMDPs, observations, actions, and rewards are embedded separately and concatenated per timestep (EmbedConcat), while MDPs use observation-only sequences. A critical innovation is preventing gradient interference by either using separate backbones or freezing the shared backbone during critic updates. Training uses cross-episode data slicing where sequences can span episode boundaries, with masking strategies like zero-padding or first-observation padding. The model predicts only from the last token in a fixed context window (typically length 10). Experiments use TD3 (γ=0.99, lr=3e-4, batch=256, buffer=1.5M) and SAC (γ=0.8, lr=3e-4, batch=1024) across vector and image observation spaces.

## Key Results
- Transformers match or exceed MLP, LSTM, CNN, and GTrXL baselines on both MDP and POMDP tasks across MuJoCo and ManiSkill3 environments
- EmbedConcat input conditioning outperforms Interleaved and CrossAttn methods in POMDPs by 15-20% average reward
- Freezing the shared backbone during critic updates prevents gradient explosion and enables stable training
- Cross-episode slicing is essential for early-step learning, improving first-step performance by 30-40% in early-termination environments
- Transformers remain stable when scaling from 51k to 5M parameters, unlike MLPs which degrade without hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding concatenation (EmbedConcat) stabilizes transformer learning in partially observable environments by unifying multimodal inputs into a homogeneous sequence.
- **Mechanism:** The paper suggests that fusing observations, actions, and rewards into a single vector before attention processing allows the model to treat distinct data types as a unified temporal token. This reduces the complexity of aligning heterogeneous modalities (as required in Interleaved or CrossAttn methods), allowing the attention mechanism to focus strictly on temporal dependencies.
- **Core assumption:** Sequential modeling in POMDPs relies on the correlation between past states, actions, and rewards to infer hidden state information.
- **Evidence anchors:**
  - [section 4.1] "EmbedConcat... merges each triplet... into a single embedding, yielding a homogeneous sequence. This allows the attention mechanism to focus purely on temporal dependencies."
  - [section 4.1] Figure 3 shows EmbedConcat outperforming Interleaved and CrossAttn in MuJoCo-POMDP tasks.
  - [corpus] Corpus evidence for this specific embedding mechanism is weak; related papers focus on general sequence modeling rather than input ablation.

### Mechanism 2
- **Claim:** Separating actor and critic backbones (or freezing the shared backbone during critic updates) prevents training collapse caused by conflicting gradient objectives.
- **Mechanism:** In actor-critic setups, the actor maximizes expected reward while the critic minimizes Temporal Difference (TD) error. The paper posits that passing both gradients through a shared transformer backbone creates interference, leading to gradient explosion. Isolating the networks or blocking critic gradients stabilizes the optimization landscape.
- **Core assumption:** The feature representations required for policy optimization (actor) and value estimation (critic) are sufficiently distinct or optimizable via separate pathways to avoid destructive interference.
- **Evidence anchors:**
  - [section 4.2] "Sharing a backbone between actor and critic in off-policy RL causes gradient interference... gradient norms grow continuously to extreme values."
  - [appendix b.2] The authors note this is not unique to transformers; "similar learning instabilities occur when an MLP agent shares an encoder."
  - [corpus] General RL literature (e.g., cited Cobbe et al.) discusses gradient conflict, though specific transformer gradient norms are unique to this text.

### Mechanism 3
- **Claim:** Cross-episode data slicing enables learning at the beginning of episodes by ensuring early-step data occupies the "final token" prediction position.
- **Mechanism:** Transformers predicting from the last token in a fixed context window (Method 1) effectively ignore data that never reaches the end of the sequence. Standard slicing waits for the context to fill, skipping the first C-1 steps. Cross-episode slicing allows the context to bleed backward from previous episodes, placing early-timestep data into the final prediction slot.
- **Core assumption:** Effective policy learning requires training on the initial states of an environment, not just the steady-state tail of episodes.
- **Evidence anchors:**
  - [section 4.3] "The issue is that early observations... never appear at the final token position... Cross-episode slicing... solves this by allowing input sequences to span across episode boundaries."
  - [section 4.3] Figure 10 shows "Within-episode" underperforms specifically in early steps (first 12 steps).
  - [corpus] Not explicitly covered in provided corpus neighbors.

## Foundational Learning

- **Concept: Actor-Critic Methods (e.g., TD3, SAC)**
  - **Why needed here:** The paper’s architectural recommendations (RQ2) are specific to separating the policy (actor) and value (critic) updates. Understanding that one network generates actions and the other grades them is essential to grasp why their gradients conflict.
  - **Quick check question:** Why would the gradient of a policy network (trying to maximize score) interfere with a value network (trying to predict score)?

- **Concept: Partial Observability (POMDP vs MDP)**
  - **Why needed here:** The choice of input conditioning (RQ1) depends entirely on whether the environment is Markovian (MDP) or requires memory (POMDP). You must understand that POMDPs require history to infer state.
  - **Quick check question:** Why does the "ObsOnly" method fail in POMDPs but succeed in MDPs?

- **Concept: Transformer Causal Masking & Context**
  - **Why needed here:** The paper relies on a GPT-style decoder where prediction happens at the final token. Understanding that standard causal masking prevents "seeing the future" explains why the position of data in the context window is critical for training.
  - **Quick check question:** In a transformer predicting only from the last token, why would data from the first 5 steps of an episode never be trained on if you use within-episode slicing with a context length of 10?

## Architecture Onboarding

- **Component map:** Vector or Image (CNN encoder) -> Embedding Layer -> GPT-2 style Transformer Decoder (Pre-LayerNorm, GELU) -> Separate Actor Head (outputs action) and Critic Head (outputs Q-value) -> Replay Buffer with Cross-Episode Slicing logic

- **Critical path:**
  1. Environment Step -> Store in Buffer (Cross-Episode logic)
  2. Sample Batch -> Construct Sequence (EmbedConcat for POMDP, ObsOnly for MDP)
  3. Forward Pass through Transformer
  4. Extract **Last Token** hidden state
  5. Compute Actor/Critic losses (Ensure Critic gradients do **not** flow back into the backbone if sharing)

- **Design tradeoffs:**
  - **Separate vs. Shared Backbone:** Separate is most stable; Shared+Frozen saves parameters but requires careful gradient blocking code
  - **Input Modes:** EmbedConcat is robust but high-dimensional; ObsOnly is efficient but fails POMDPs
  - **Slicing:** Cross-episode adds buffer complexity but is necessary for early-episode performance; "Every token" prediction is an alternative but computationally heavier

- **Failure signatures:**
  - **Runaway Gradient Norms:** Indicates failure to separate actor/critic backbone updates (Section 4.2)
  - **Cold Start Failure:** Agent performs well generally but fails immediately at episode start (Section 4.3)
  - **POMDP Collapse:** Agent fails to solve tasks where velocity/position is masked, indicating use of ObsOnly instead of EmbedConcat

- **First 3 experiments:**
  1. **Gradient Interference Ablation:** Train on HalfCheetah with a shared backbone, once with critic gradients blocked and once without. Plot gradient norms to verify explosion (replicates Figure 8)
  2. **Slicing Validation:** Train on Walker (early termination sensitive) using Within-Episode vs. Cross-Episode slicing. Compare average reward over the first 10-12 steps (replicates Figure 10)
  3. **Conditioning Stress Test:** Train on a POMDP environment (e.g., MuJoCo-POMDP) comparing ObsOnly vs. EmbedConcat to verify the performance gap (replicates Figure 3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What alternative masking strategies can further optimize cross-episode slicing for transformers in online RL?
- **Basis in paper:** [explicit] The authors state regarding cross-episode slicing: "this question warrants further investigation, particularly through exploring alternative masking strategies."
- **Why unresolved:** The paper only tested zero-padding and first-observation masking; optimal handling of episode boundaries in sequence modeling remains empirical.
- **What evidence would resolve it:** A comparative study of learnable masks or specialized attention masks on the proposed benchmarks.

### Open Question 2
- **Question:** Can the proposed unified training setup stabilize transformers when applied to on-policy algorithms like PPO?
- **Basis in paper:** [inferred] Section 5.1 notes that "PPO fails to train both GPT and MLP agents," limiting current findings mostly to off-policy algorithms (TD3, SAC).
- **Why unresolved:** The specific design choices (e.g., backbone freezing, slicing) were validated primarily in off-policy settings; their interaction with on-policy gradient updates is unclear.
- **What evidence would resolve it:** Successful convergence of PPO-GPT on MuJoCo tasks utilizing the recommended input conditioning and slicing techniques.

### Open Question 3
- **Question:** Do the benefits of cross-episode slicing and EmbedConcat persist as context window lengths increase significantly (e.g., 100+ steps)?
- **Basis in paper:** [inferred] Experiments used a fixed context length of 10, while Section 1 highlights transformers' ability to capture "long-range dependencies."
- **Why unresolved:** It is unclear if the "within-episode" training gap widens or if cross-episode slicing induces distinct attention patterns in longer horizons.
- **What evidence would resolve it:** Performance and attention analysis of agents trained with context lengths varying from 10 to 200+ steps.

## Limitations
- Cross-episode slicing implementation details are underspecified (buffer storage vs. sampling time, sequence boundary tracking)
- Exact architecture of embedding MLPs for observations, actions, and rewards in EmbedConcat is not detailed
- Does not conclusively prove transformers are superior to MLPs—only shows competitive performance
- Stability gains from freezing critic backbone lack theoretical grounding about why gradient interference specifically manifests in transformers

## Confidence
- **High Confidence:** The empirical finding that shared actor-critic backbones cause gradient interference, and that freezing the backbone during critic updates resolves this. Directly demonstrated with gradient norm monitoring.
- **Medium Confidence:** The claim that EmbedConcat is the optimal input conditioning for POMDPs. Supported by ablation results but limited to three methods without exploring alternatives.
- **Medium Confidence:** The assertion that transformers are "competitive and scalable" in online RL. Evidence shows parity with strong baselines but doesn't demonstrate clear advantages in sample efficiency or final performance.

## Next Checks
1. **Gradient Interference Isolation:** Replicate the gradient norm monitoring experiment from Section 4.2, training with a shared backbone both with and without critic gradient blocking. Plot actor and critic gradient norms over training steps to verify the explosion pattern described.

2. **Cross-Episode Slicing Impact:** Implement the within-episode vs. cross-episode slicing comparison on a task with early-episode termination sensitivity (like Walker in the paper). Measure and plot per-step rewards for the first 10-12 steps to confirm the performance gap.

3. **Input Conditioning Stress Test:** Create a systematic ablation comparing ObsOnly, Interleaved, CrossAttn, and EmbedConcat on a suite of POMDP variants (different velocity/position masking patterns). Measure not just final performance but training stability and sensitivity to hyperparameters.