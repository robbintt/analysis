---
ver: rpa2
title: 'Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation'
arxiv_id: '2601.21464'
source_url: https://arxiv.org/abs/2601.21464
tags:
- agent
- solution
- critique
- solutions
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoNL, a multi-agent self-play framework for
  training large language models on non-verifiable tasks like creative writing and
  ethical reasoning. The key insight is that critique quality can be measured by whether
  it enables others to improve their solutions, creating explicit supervision for
  meta-evaluation without ground truth.
---

# Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation

## Quick Facts
- arXiv ID: 2601.21464
- Source URL: https://arxiv.org/abs/2601.21464
- Authors: Yuan Sui; Bryan Hooi
- Reference count: 40
- Primary result: 2.7-8.3 percentage point improvements over self-rewarding baselines on non-verifiable tasks

## Executive Summary
This paper introduces CoNL, a multi-agent self-play framework that trains LLMs on non-verifiable tasks like creative writing and ethical reasoning without ground-truth labels. The key insight is that critique quality can be measured by whether it enables others to improve their solutions, creating explicit supervision for meta-evaluation. CoNL uses four rounds of conversation where agents propose, critique, and revise solutions, with rewards based on solution improvement and consensus alignment. Experiments show CoNL achieves strong performance across five benchmarks while maintaining stable training dynamics.

## Method Summary
CoNL implements a 4-round conversation protocol with N=4 agents using distinct personas. Agents generate initial solutions (Round 0), provide blind pairwise rankings and targeted critiques (Round 1), revise solutions or defend originals (Round 2), and provide final rankings (Round 3). Rewards include solution quality (V_final), diagnostic reward for critique effectiveness (Σ max(0, V_final - V_init)), and consensus alignment. The framework uses Bradley-Terry aggregation for quality scores and trains via policy gradient with importance sampling. Context is managed through a memory buffer when exceeding 32k tokens.

## Key Results
- CoNL achieves 2.7-8.3 percentage point improvements over self-rewarding baselines on five benchmarks
- Performance closely matches RL with ground-truth rewards on verifiable tasks
- Maintains stable training dynamics without reward hacking (entropy fluctuations, solution length spikes)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critique quality can be measured by whether it enables solution improvement, creating supervision for meta-evaluation without ground truth.
- Mechanism: Agent i critiques Agent k's solution in Round 1. Agent k revises in Round 2. If V_final_k > V_init_k, Agent i receives diagnostic reward r_diag(i) = Σ max(0, V_final_k - V_init_k).
- Core assumption: Base models possess sufficient initial evaluation capability that critique-improvement correlations signal genuine diagnostic quality.
- Evidence anchors: [abstract] "Our key insight: critique quality can be measured by whether it helps others improve their solutions."

### Mechanism 2
- Claim: Adversarial revision dynamics prevent false-positive critique rewards.
- Mechanism: When Agent i provides invalid critique of Agent k's correct solution, Agent k can defend with counter-arguments in Round 2. If defense succeeds, V_final_k ≈ V_init_k, yielding near-zero reward for Agent i.
- Core assumption: Agents can recognize valid defenses; group consensus reflects genuine quality assessment.
- Evidence anchors: [Table 4] Harm rate 3.1% on DeepMath, 9.4% on AIME 2025—critiques rarely degrade correct solutions.

### Mechanism 3
- Claim: Zero reward for initial ranking tokens prevents gaming the baseline.
- Mechanism: Initial rankings determine V_init baseline. By masking Round 1 ranking tokens (reward = 0), agents have no incentive to manipulate baseline scores.
- Core assumption: Agents would otherwise exploit reward structure; masking eliminates this incentive.
- Evidence anchors: [Section B.3] "Initial ranking tokens receive zero reward... to prevent gaming."

## Foundational Learning

- **Bradley-Terry model for pairwise comparison aggregation**
  - Why needed: Converts conflicting pairwise rankings from multiple agents into unified quality scores V_init and V_final
  - Quick check: Given comparisons "A beats B 3 times, B beats C 2 times, A beats C 4 times," can you sketch how BT would assign relative scores?

- **Policy gradient with importance sampling**
  - Why needed: CoNL trains via policy gradient where sampling distribution (π_old) differs from training distribution (π_θ). Importance sampling corrects this mismatch via ratio p_θ(x)/p_old(x).
  - Quick check: Why does importance sampling matter when the policy changes during training?

- **Token-level credit assignment in RL for language**
  - Why needed: Each agent generates multiple segments (solution, critique, ranking). Assigning the same reward to all tokens prevents the model from learning which behavior caused which outcome.
  - Quick check: If critique tokens receive r_diag but solution tokens receive r_sol, what happens if you assign both rewards to all tokens?

## Architecture Onboarding

- **Component map**: Round 0 (Proposal) -> Round 1 (Evaluation/Critique) -> Round 2 (Revision) -> Round 3 (Final Verdict) -> Reward Computation -> Memory Buffer
- **Critical path**: Round 1 blind ranking → V_init baseline → Round 2 revision → Round 3 ranking → V_final → r_diag = ΔV
- **Design tradeoffs**: N=4 agents balances diversity vs. coordination overhead; w_2=2.0 weighted highest emphasizes meta-evaluation
- **Failure signatures**: Entropy fluctuation + solution length spikes = reward hacking; Pass@1 improving but Rank-ρ declining = generation improving without evaluation calibration
- **First 3 experiments**:
  1. Replicate Table 3 ablation: train with r_diag=0 only. Expect Pass@1 drop of 3-4 points on DeepMath.
  2. Test N=2 vs N=4 agents on AIME subset. Expect 2-3 point Pass@1 gap.
  3. Monitor harm rate during training. If > 10%, inspect Round 2 defenses.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework's effectiveness fundamentally depends on base model's initial evaluation capability
- Evaluation pipeline correlation may not generalize to tasks with different epistemic structures
- Mechanism validation gaps: adversarial revision mechanism not directly validated

## Confidence

**High Confidence**:
- Overall training framework structure and reward formulation
- Baseline comparison results showing 2.7-8.3 percentage point improvements
- Necessity of diagnostic rewards (confirmed by ablation showing Rank-ρ drop from 0.78 to 0.45)

**Medium Confidence**:
- Self-bootstrapping of evaluation capability (relies on base model quality)
- Bradley-Terry aggregation providing reliable quality scores
- Stability of training dynamics without reward hacking

**Low Confidence**:
- Adversarial revision mechanism preventing false-positive critique rewards
- Generalization to tasks with no ground truth whatsoever
- Performance with fewer than 4 agents

## Next Checks

1. **Base Model Sensitivity Test**: Train CoNL with progressively weaker base models and measure at what point the diagnostic reward mechanism breaks down.

2. **Defense Mechanism Isolation**: Design controlled experiment where invalid critiques are deliberately injected, then measure whether agents successfully defend correct solutions in Round 2.

3. **Minimal Agent Configuration**: Train with N=2 agents instead of N=4 and measure the degradation in Pass@1 and Rank-ρ.