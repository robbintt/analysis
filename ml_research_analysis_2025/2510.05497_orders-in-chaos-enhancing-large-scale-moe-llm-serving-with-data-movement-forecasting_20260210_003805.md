---
ver: rpa2
title: 'Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement
  Forecasting'
arxiv_id: '2510.05497'
source_url: https://arxiv.org/abs/2510.05497
tags:
- expert
- data
- systems
- experts
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts the first comprehensive data-movement-centric
  profiling of four large-scale MoE models (200B-1000B parameters) across 24,000 requests
  to understand data movement patterns. The analysis reveals six key insights: prefill-decode
  stage predictability, cross-hierarchy memory management opportunities, expert-placement-aware
  workload distribution, popular expert decentralization, expert-pair separation,
  and workload-aware serving.'
---

# Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting

## Quick Facts
- **arXiv ID**: 2510.05497
- **Source URL**: https://arxiv.org/abs/2510.05497
- **Reference count**: 40
- **Primary result**: First comprehensive data-movement profiling of 200B-1000B MoE models reveals six key insights enabling 5.3× and 3.1× speedups through architectural modifications to wafer-scale GPUs.

## Executive Summary
This paper presents the first comprehensive data-movement-centric profiling of large-scale MoE models (200B-1000B parameters) across 24,000 requests, uncovering six key insights that enable significant performance improvements. The analysis reveals predictable patterns in expert selection during prefill and decode stages, opportunities for cross-hierarchy memory management, and workload-aware serving strategies. Leveraging these insights, the authors demonstrate 5.3× and 3.1× speedups on DeepSeek V3 and Qwen3 respectively through architectural modifications including a two-level command processor and data-driven predictor. The profiling traces and simulation framework are open-sourced with over 1k downloads.

## Method Summary
The authors collect expert selection traces from four large-scale MoE models (DeepSeek V3 671B, Llama4-Maverick 402B, Qwen3-235B, Kimi K2 1000B) using SGLang on 8×H100/H200 clusters, then build a custom event-driven multi-chiplet simulator to analyze temporal and spatial relations. They implement a two-level command processor with task allocation and prediction algorithms, validated against hardware with <5% error. The simulator models wafer-scale GPU architectures with 5×5 or 8×3 mesh topologies, incorporating LLC, local HBM, compute units, and D2D links.

## Key Results
- **5.3× and 3.1× throughput improvements** on DeepSeek V3 and Qwen3 respectively through combined allocation and prediction strategies
- **210×-944× reduction in hop counts** compared to baseline when combining allocation and prediction
- **Strong temporal correlations** identified: prefill-decode stage predictability (Spearman's ρ ≥ 0.7) and cross-hierarchy memory management opportunities

## Why This Works (Mechanism)

### Mechanism 1: Prefill-Decode Stage Predictability
The prefill stage processes all input tokens simultaneously, generating expert selection traces that strongly correlate with decode-stage selections (Spearman's ρ ≥ 0.7). The system captures conditional probability heatmaps during prefill and uses them to prefetch or cache experts before decode begins.

### Mechanism 2: Cross-Hierarchy Memory Management
Layer-level correlations have short reuse distances (consecutive MoE layers execute immediately), making them suitable for fast caches (LLC). Token-level correlations have longer reuse distances (tokens traverse all layers), making them suitable for slower but larger caches (local DRAM).

### Mechanism 3: Expert-Placement-Aware Task Distribution
A two-level command processor tracks expert distribution across dies and assigns request blocks using a cost model that considers DRAM access, compute, and die-to-die communication, preventing hotspots where some dies are overloaded while others idle.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing mechanism**
  - Why needed here: MoE models route each token to a subset of experts via a gating function, creating dynamic, input-dependent data movement
  - Quick check question: Given a 256-expert MoE layer with top-8 routing, what is the theoretical number of possible expert combinations? (Answer: C(256,8) ≈ 4.4 billion)

- **Concept: Temporal vs spatial locality in data movement**
  - Why needed here: The paper categorizes predictability into temporal (time-dependent patterns enabling prefetch) and spatial (distribution patterns enabling load balancing)
  - Quick check question: Which type of locality would inform expert replication across dies vs expert prefetch into local cache?

- **Concept: Memory hierarchy and access latency**
  - Why needed here: The cross-hierarchy memory management insight maps different temporal patterns to LLC vs DRAM
  - Quick check question: Why would token-level correlations (spanning full-layer traversal) be more appropriate for DRAM caching than LLC?

## Architecture Onboarding

- **Component map**: Request arrives → Global CP (wafer-level coordinator) → Local CPs (per-die coordinators) → D2D Controller (with ATU/PDU) → SMs execute MoE kernels → Statistics returned to Global CP

- **Critical path**: Request → Global CP task allocation + predictor → Sub-kernels + prediction guidance to Local CPs → Local CPs configure PDUs → SMs execute with ATU redirecting cached experts → Statistics returned for distribution updates

- **Design tradeoffs**:
  - Prediction granularity vs overhead: Top-n expert selection from heatmap rows balances accuracy against PDU table size
  - Block size in allocation: 50 requests per block trades merge efficiency for allocation precision
  - Host CPU vs GPU CP implementation: GPU CP avoids PCIe transfer overhead (5-51% of execution time depending on model and hardware)

- **Failure signatures**:
  - Stale distribution table: Expert migration without Global CP update causes incorrect ATU translations
  - Cache thrashing: High batch sizes with hot experts exceed local DRAM, causing eviction loops
  - Prediction misses: Out-of-distribution inputs select experts outside top-n predictions, falling back to remote fetch

- **First 3 experiments**:
  1. Validate temporal correlation on your workload: Collect expert selection traces for 100+ requests and compute Spearman's ratio between prefill and decode heatmaps
  2. Measure baseline hop count: Run unmodified allocation on multi-chiplet simulator and record inter-unit communication hops per request batch
  3. Ablate predictor vs allocator: Isolate throughput contributions by running Pred-Only and Allo-Only configurations separately before combining

## Open Questions the Paper Calls Out

### Open Question 1: Architecture Generalization
How do the profiling insights and proposed optimizations generalize to other serving architectures beyond wafer-scale GPUs, such as CXL-based memory disaggregation, multi-GPU clusters, and flash-based systems? The authors state their insights extend to diverse future systems but only validated wafer-scale GPUs.

### Open Question 2: Full Insight Integration
What performance improvements can be achieved by fully integrating all six insights (particularly Insights 4, 5, and 6 on expert decentralization, pair separation, and workload-aware serving) into the wafer-scale GPU design? The case study only implemented Insights 1, 2 (partial), and 3.

### Open Question 3: Extreme Batch Size Scaling
How does prediction accuracy and throughput scale with batch sizes beyond 8,192, particularly approaching the 10,000+ batch sizes that wafer-scale GPUs theoretically support? Evaluation only tested up to batch size 8,192 despite claiming support for >10,000.

## Limitations

- **Simulator fidelity gaps**: While claiming <5% error against 8×H100 DGX hardware, the simulator's exact event scheduling and contention modeling remain underspecified
- **Predictor robustness concerns**: Top-n prediction strategy assumes stable expert popularity distributions but lacks ablation on predictor sensitivity to n or heatmap aggregation windows
- **Cost model transparency**: Algorithm 1's placement cost model mentions DRAM access, computation, and D2D communication but omits explicit weight assignments or decision thresholds

## Confidence

- **High Confidence**: Temporal correlation findings (Spearman's ρ ≥ 0.7) - directly measurable from open-sourced traces; architectural modifications (two-level CP, PDU/ATU) - well-specified with clear hardware extensions
- **Medium Confidence**: 3.1×-5.3× speedup claims - dependent on simulator accuracy and workload representativeness; layer-vs-token caching strategy - theoretically sound but untested across diverse batch sizes
- **Low Confidence**: Generalizability to other MoE architectures (e.g., router-only gating, different top-k values) - not evaluated; performance on non-curriculum datasets - based on limited 24K request corpus

## Next Checks

1. **Correlation Degradation Analysis**: Measure Spearman's ratio degradation when applying prefill-decode prediction on 1,000 held-out requests from datasets not used in profiling (e.g., biomedical literature, legal documents).

2. **Topology Sensitivity Test**: Rerun simulator with alternative mesh topologies (2×8 linear, 4×4, 3×6) while maintaining constant expert count and quantify speedup variance across topologies.

3. **Predictor Ablation Study**: Systematically vary top-n cache size (1, 2, 3, 5, 8) and batch sizes (512, 2048, 8192, 15000) on open-sourced traces and generate throughput vs. cache efficiency curves.