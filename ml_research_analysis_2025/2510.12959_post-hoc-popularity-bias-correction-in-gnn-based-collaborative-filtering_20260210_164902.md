---
ver: rpa2
title: Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering
arxiv_id: '2510.12959'
source_url: https://arxiv.org/abs/2510.12959
tags:
- popularity
- bias
- user
- items
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles popularity bias in graph neural network (GNN)-based
  collaborative filtering, where a few popular items dominate recommendations at the
  expense of niche items. The authors propose Post-hoc Popularity Debiasing (PPD),
  a method that operates directly on pre-trained embeddings without retraining.
---

# Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering

## Quick Facts
- arXiv ID: 2510.12959
- Source URL: https://arxiv.org/abs/2510.12959
- Reference count: 40
- Key outcome: Post-hoc method achieves 186.4% relative improvement in Recall@20 on KuaiRec compared to best baseline

## Executive Summary
This paper addresses popularity bias in GNN-based collaborative filtering where popular items dominate recommendations. The authors propose Post-hoc Popularity Debiasing (PPD), a method that operates directly on pre-trained embeddings without retraining. PPD estimates interaction-level popularity by combining global and personalized preferences, then constructs a popularity direction vector for each node. By projecting embeddings onto this direction and removing the popularity component, the method preserves user preferences while reducing bias. Experiments on three real-world datasets show PPD consistently outperforms state-of-the-art baselines, improving both head and tail item recommendations.

## Method Summary
PPD operates post-hoc on pre-trained GNN embeddings by first estimating interaction-level popularity scores that capture the extent to which each user-item interaction is driven by popularity versus genuine preference. It constructs popularity direction vectors by computing weighted centroids of neighbor embeddings and subtracting preference components. The method then projects each node embedding onto its popularity direction and removes this component to obtain debiased representations. This debiasing is applied at layer 0 before GNN propagation, allowing unbiased information to flow through all layers. The approach requires no retraining and can be applied to any GNN backbone, with hyperparameters controlling the trade-off between popularity removal and preference preservation.

## Key Results
- PPD achieves 186.4% relative improvement in Recall@20 on KuaiRec compared to best baseline
- Consistently outperforms state-of-the-art baselines across all three datasets (KuaiRec, Coat, Yahoo! R3)
- Improves recommendations for both head and tail items, with particularly strong gains for tail items
- Maintains effectiveness across varying GNN layer depths (1-4 layers)
- Generalizes to other GNN backbones like SGL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Popularity bias in GNN-based CF can be decomposed into global and personalized components, allowing targeted removal.
- Mechanism: The method estimates interaction-level popularity scores (b_ui) by computing the difference between an item's global preference (p_i, measured via average similarity to all users in embedding space) and personalized preference (r_ui, measured via similarity to user's historical items with popularity penalty). This isolates popularity-driven interactions from genuine preference signals.
- Core assumption: Popularity and preference occupy distinguishable directions in the learned embedding space.
- Evidence anchors: [abstract] "By estimating interaction-level popularity and removing popularity components from node representations via a popularity direction vector, PPD reduces bias while preserving user preferences." [section 4.1] "The popularity score b_ui for each interaction estimates the extent to which an interaction between user u and item i is influenced by popularity rather than user preference."

### Mechanism 2
- Claim: Popularity direction vectors can be constructed from weighted centroids of neighbor embeddings to capture bias propagation patterns.
- Mechanism: For each node, compute two centroids: popularity-based (weighted by b_ui scores) and preference-based (weighted by 1-b_ui). The difference vector (d_pop = ē_pop - φ·ē_pref) represents the direction along which popularity pulls embeddings away from true preferences. The coefficient φ controls preference signal contribution.
- Core assumption: Popularity-driven interactions cluster in a consistent direction within the embedding space.
- Evidence anchors: [section 4.2] "The popularity direction vector captures how popularity pulls the node embeddings away from preferences. This vector represents the direction from preference to popularity in embedding space."

### Mechanism 3
- Claim: Vector projection can remove popularity components from embeddings while preserving preference signals.
- Mechanism: Project each node embedding onto its popularity direction vector using standard vector projection (Proj = ⟨e, d_pop⟩/||d_pop||² · d_pop). Subtract this component to obtain debiased embeddings (ẽ = e - Proj). Apply this at layer 0 before GNN propagation, allowing unbiased information to flow through all layers.
- Core assumption: The popularity direction is approximately orthogonal to genuine preference directions in the embedding space.
- Evidence anchors: [section 4.2, Eq. 11-12] "By projecting the node embedding onto this direction to get the component of an embedding aligned with that direction. By subtracting this component from the node embedding, we remove the popularity component while preserving user preferences."

## Foundational Learning

- Concept: **GNN Message Passing in Collaborative Filtering**
  - Why needed here: PPD operates on pre-trained GNN embeddings; understanding how neighborhood aggregation works is essential to grasp why popularity bias propagates and amplifies through layers.
  - Quick check question: Can you explain why high-degree (popular) items influence user embeddings more than low-degree items during aggregation?

- Concept: **Vector Projection and Orthogonal Decomposition**
  - Why needed here: The core debiasing mechanism relies on projecting embeddings onto a popularity direction and removing that component. Without understanding projection, you cannot diagnose why debiasing might fail.
  - Quick check question: Given vectors a = [1, 2] and b = [3, 0], compute the projection of a onto b. What does the residual vector represent?

- Concept: **Popularity Bias vs. Exposure Bias in Recommender Systems**
  - Why needed here: PPD specifically targets popularity bias (uneven item distribution) not exposure bias (items not shown). Confusing these leads to wrong problem formulation.
  - Quick check question: A user never interacts with item X because it was never recommended. Is this popularity bias or exposure bias? Would PPD help?

## Architecture Onboarding

- Component map:
  1. Pre-trained GNN backbone (LightGCN/SGL) -> Produces initial embeddings e_u^(0), e_i^(0)
  2. Global preference estimator -> Computes p_i = mean(sim(e_u, e_i)) for all items
  3. Personalized preference estimator -> Computes r_ui for each (u,i) interaction with β penalty
  4. Popularity score calculator -> b_ui = p_i - r_ui (normalized)
  5. Centroid aggregator -> Computes popularity and preference weighted centroids per node
  6. Direction constructor -> d_pop = ē_pop - φ·ē_pref
  7. Projection debiaser -> ẽ = e - Proj_d_pop(e)
  8. Re-propagation module -> Passes debiased layer-0 embeddings through remaining GNN layers

- Critical path:
  1. Load pre-trained embeddings (no training required for PPD)
  2. Compute global preference for all items (O(|U||I|d) - most expensive step)
  3. Compute personalized preferences for all interactions (O(|E|D̄d))
  4. Normalize p_i and r_ui before computing b_ui (min-min normalization per paper)
  5. Construct centroids and direction vectors (tune φ per dataset)
  6. Apply projection at layer 0 only
  7. Re-run forward pass through GNN layers (no backward pass needed)

- Design tradeoffs:
  - **Post-hoc vs. in-training**: PPD avoids retraining but cannot fix bias learned during training; only removes bias from final representations
  - **Layer-0 debiasing only**: Simpler but assumes bias at layer 0 is the primary source; may not capture layer-specific bias amplification
  - **Interaction-level vs. item-level popularity**: Finer granularity but requires more computation; item-level (simple degree) would be faster but less precise
  - **φ tuning**: Low φ (0-0.5) for noisy/skewed datasets (KuaiRec), high φ (0.75-1.0) for reliable preference signals (Coat, Yahoo!R3)

- Failure signatures:
  - Tail item performance degrades head performance significantly: φ too high, preference centroid unreliable
  - No improvement over baseline: β too low, not penalizing popularity in personalized preference
  - Performance varies wildly across random seeds: Embeddings unstable, consider averaging across multiple debiasing runs
  - Recall improves but NDCG degrades: Debiasing helping retrieval but hurting ranking calibration

- First 3 experiments:
  1. **Ablation on β (popularity penalty)**: Test β ∈ {0.0, 0.1, 0.2, 0.3} on validation set. Expect optimal β to vary by dataset skewness. Plot Recall@20 vs. β.
  2. **Ablation on φ (preference centroid coefficient)**: Test φ ∈ {0.0, 0.25, 0.50, 0.75, 1.0}. Compare head vs. tail item performance separately to detect tradeoffs.
  3. **Layer depth sensitivity**: Test GNN layers ∈ {1, 2, 3, 4} with and without PPD. Verify PPD maintains or improves performance at greater depths while baseline degrades (over-smoothing + bias amplification).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the projection-based debiasing framework be generalized to correct for other biases, such as exposure or position bias, without requiring model retraining?
- Basis: [explicit] The conclusion states that future research should "extend post-hoc methods to other forms of bias correction, such as exposure bias or position bias."
- Why unresolved: The current method is mathematically tailored to popularity bias by defining a "popularity direction" based on global item frequency; it is unclear if exposure or position biases can be isolated into a similar linear vector direction in the embedding space.
- What evidence would resolve it: Successful application of a modified PPD framework to datasets with known exposure bias (e.g., biased learning-to-rank datasets), demonstrating statistically significant improvements in unbiased evaluation metrics.

### Open Question 2
- Question: How does PPD perform in dynamic recommendation environments where user preferences and item popularity distributions shift rapidly over time?
- Basis: [explicit] The authors suggest future work should "explore applications beyond CF, including... dynamic recommendation settings."
- Why unresolved: PPD relies on static pre-trained embeddings and a fixed global preference calculation; in dynamic settings, the "popularity direction" derived from historical data may become stale or misleading as trends change.
- What evidence would resolve it: Experiments on temporal datasets (e.g., streaming data) measuring the "refresh rate" required for the popularity direction vector to maintain debiasing efficacy as the interaction graph evolves.

### Open Question 3
- Question: What is the trade-off between recommendation accuracy and computational efficiency when estimating global preference using a sampled subset of users?
- Basis: [inferred] The complexity analysis suggests the cost can be reduced by "randomly sampling a subset of users," but the empirical evaluation appears to use the full user set without testing this approximation.
- Why unresolved: The paper theoretically proposes sampling to manage the $O(|U||I|d)$ complexity but provides no empirical data on how sampling noise affects the precision of the popularity direction vector or final ranking quality.
- What evidence would resolve it: An ablation study comparing the full computation against varying sampling ratios (e.g., 10%, 20% of users) to quantify the resulting variance in Recall and NDCG.

## Limitations
- The method relies heavily on the quality of pre-trained embeddings, and if popularity bias is baked into the training objective, post-hoc correction may be insufficient.
- The evaluation focuses on three datasets with relatively small user bases, limiting generalizability to industrial-scale systems with millions of users.
- The assumption that popularity and preference directions are orthogonal in embedding space lacks empirical validation through sensitivity analysis.

## Confidence
- High confidence: The method's implementation details and experimental results on KuaiRec, Coat, and Yahoo! R3 are well-documented and reproducible.
- Medium confidence: The claim that PPD outperforms all baselines across all metrics is supported by experiments, but the statistical significance of improvements (especially for tail items) is not rigorously tested.
- Low confidence: The assumption that popularity and preference directions are orthogonal in embedding space lacks empirical validation through sensitivity analysis.

## Next Checks
1. **Sensitivity analysis on β and φ**: Systematically vary β ∈ {0.0, 0.1, 0.2, 0.3} and φ ∈ {0.0, 0.25, 0.50, 0.75, 1.0} across all three datasets to quantify robustness to hyperparameter tuning.
2. **Ablation on layer-wise debiasing**: Compare PPD's layer-0 debiasing with layer-wise debiasing (applying at each layer) to test the assumption that layer-0 bias dominates.
3. **Statistical significance testing**: Perform paired t-tests on Recall@20 and NDCG@20 improvements across 10 random seeds to validate that observed gains are statistically significant, especially for tail items where improvements are most dramatic.