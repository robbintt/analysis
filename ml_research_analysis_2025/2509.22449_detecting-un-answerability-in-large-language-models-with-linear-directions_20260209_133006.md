---
ver: rpa2
title: Detecting (Un)answerability in Large Language Models with Linear Directions
arxiv_id: '2509.22449'
source_url: https://arxiv.org/abs/2509.22449
tags:
- direction
- unanswerable
- threshold
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for detecting unanswerable questions
  in large language models (LLMs) by identifying a linear direction in the model's
  activation space that captures unanswerability. The approach uses difference-in-means
  to derive candidate directions and selects the most effective one through activation
  steering, measuring its impact on abstention behavior.
---

# Detecting (Un)answerability in Large Language Models with Linear Directions

## Quick Facts
- arXiv ID: 2509.22449
- Source URL: https://arxiv.org/abs/2509.22449
- Authors: Maor Juliet Lavi; Tova Milo; Mor Geva
- Reference count: 40
- One-line primary result: A linear direction in LLM activation space captures unanswerability, achieving 75.9-96.4% F1 and better cross-dataset generalization than trained classifiers.

## Executive Summary
This paper introduces a method to detect unanswerable questions in LLMs by identifying a linear direction in the model's activation space that captures the concept of unanswerability. The approach uses difference-in-means to derive candidate directions from contrastive example sets and selects the most effective one through activation steering, measuring its impact on abstention behavior. Experiments on two LLMs across four QA datasets show the method outperforms prompt-based baselines and matches trained classifiers, with superior generalization across datasets when threshold calibration is applied.

## Method Summary
The method extracts hidden states at specific layers/positions from LLM residual streams, then computes difference-in-means vectors between unanswerable and answerable example activations to derive candidate directions. Each candidate is evaluated through activation steering—injecting it into the forward pass and measuring the log-odds increase for the "unanswerable" token. The direction maximizing this steering score is selected and used for classification via projection scoring with threshold calibration. This approach identifies linear substructures in activation space that causally influence abstention behavior.

## Key Results
- Achieves F1 scores of 75.9-96.4% across four QA datasets on Llama-3-8B-Instruct and Gemma-3-12B-IT models
- Outperforms prompt-based baselines and matches trained classifier performance
- Shows 8.14% average improvement in generalization across datasets compared to trained classifier
- Causal interventions confirm the direction influences abstention, with 96.8% abstention when added at α=2.0 to answerable examples
- Generalizes to other unanswerability settings including lack of scientific consensus and subjectivity

## Why This Works (Mechanism)

### Mechanism 1: Linear Encoding of Unanswerability in Residual Stream
- Unanswerability is linearly encoded in intermediate layers of the residual stream
- Difference-in-means between unanswerable vs. answerable activations extracts a direction pointing to "unanswerable" regions
- Supported by similar linear structures found for truthfulness and refusal in related work
- Break condition: If unanswerability is non-linearly encoded, single direction would fail

### Mechanism 2: Causal Selection via Steering Score
- The direction that most strongly increases abstention probability when added is selected
- Steering score measures log-odds increase for "un" token vs. all other tokens
- Core assumption: "un" token is reliable proxy for abstention intent under Abstain-aware prompt
- Break condition: If models use varied lexicalizations for abstention, single-token proxy misranks directions

### Mechanism 3: Generalization via Projection Scoring with Threshold Calibration
- Classification uses dot-product projection onto normalized direction with threshold
- Threshold calibrated per evaluation dataset recovers generalization performance
- Score distributions shift across datasets but direction remains informative
- Break condition: If direction captured dataset-specific artifacts rather than unanswerability, threshold calibration wouldn't recover performance

## Foundational Learning

- **Residual Stream and Layer-wise Activations**
  - Why needed here: Method extracts hidden states at specific layers/positions to compute directions and scores
  - Quick check question: Can you identify where in the forward pass the residual stream is modified during steering?

- **Difference-in-Means for Concept Directions**
  - Why needed here: Core technique for deriving candidate directions from contrastive example sets
  - Quick check question: Given two sets of activations A (answerable) and U (unanswerable), how would you compute the difference-in-means direction?

- **Activation Steering (Contrastive Activation Addition)**
  - Why needed here: Used for both direction selection and causal validation
  - Quick check question: What happens to model behavior if you add vs. subtract the unanswerability direction during inference?

## Architecture Onboarding

- **Component map**: Input Layer -> Tokenizer -> Activation Extraction -> Direction Derivation -> Steering Selector -> Classifier
- **Critical path**: 1) Prepare balanced contrast sets, 2) Extract activations at all layers/positions, 3) Compute difference-in-means vectors, 4) Run steering evaluation and score by abstention token probability, 5) Select best layer/position and fit threshold via ROC, 6) At inference: extract activation → project → compare to threshold
- **Design tradeoffs**: Single vs. multi-layer directions (no substantial gains), fixed vs. calibrated threshold (calibration recovers 2.7-23.7%), token proxy selection (assumes consistent abstention lexicalization), layer selection (middle layers consistently selected)
- **Failure signatures**: Low generalization without calibration, high false positives on answerable inputs, steering causes over-abstention, misaligned token proxy
- **First 3 experiments**: 1) Replicate direction derivation on SQuAD at layer 16 position -1, verify ROC-AUC ≥ 0.90, 2) Validate steering causality: add v* at α=1.5 to 50 unanswerable examples, confirm >80% abstention shift, 3) Test generalization without calibration: train on SQuAD, evaluate on NQ with fixed threshold, then recalibrate and measure F1 delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing the simple scalar threshold with a more expressive function significantly improve classification robustness or calibration?
- Basis in paper: Authors state in Limitations that they "use a simple threshold... and do not explore more expressive functions, which could potentially better exploit this signal."
- Why unresolved: Simple scalar threshold may fail to account for variations in score distribution across datasets or input types
- What evidence would resolve it: Comparative study evaluating MLP classifier head or learned calibration function vs. baseline thresholding

### Open Question 2
- Question: Is unanswerability represented in more complex, non-linear patterns within activation space that a single linear direction cannot capture?
- Basis in paper: Authors acknowledge in Limitations that "it is possible that unanswerability is represented in more complex patterns that our method cannot identify."
- Why unresolved: Linear direction method may fail to capture intricate relationships required for difficult edge cases
- What evidence would resolve it: Testing whether non-linear probes or multiple directions yield significant F1 improvements

### Open Question 3
- Question: Can a "universal" unanswerability direction be derived that generalizes robustly across diverse settings like false presuppositions and lack of consensus?
- Basis in paper: Authors note performance on CREPE was lower, suggesting "more effective ways to capture broader unanswerability signal that transfers robustly"
- Why unresolved: Current directions from extractive QA datasets may not align with open-domain false presuppositions
- What evidence would resolve it: Identifying direction trained on mixture of diverse unanswerability types that maintains high performance on all sub-domains

## Limitations
- Generalization without calibration remains limited, requiring dataset-specific threshold tuning
- Steering score relies on "un" token proxy assumption that may not hold across different models or lexicalizations
- Linearity assumption may fail for non-linear or distributed representations of unanswerability
- Method requires labeled contrast sets, creating chicken-and-egg problem for practical deployment

## Confidence
- **High Confidence**: Steering-based direction selection effectively identifies causal directions; projection-based classification achieves strong F1 scores; direction generalizes better than trained classifier with calibration
- **Medium Confidence**: "Better generalization" claim is dataset-dependent and requires calibration; effectiveness on consensus/subjective settings evaluated on limited datasets; comparison to prompt-based baselines may not represent state-of-the-art
- **Low Confidence**: Single direction captures "abstract concept" of unanswerability; steering with "un" token reliably selects best direction across all models; scalability to larger models or different architectures

## Next Checks
1. **Cross-Modal Generalization Test**: Apply method to multimodal LLMs using datasets combining visual and textual unanswerable questions to test transfer to multimodal activation spaces
2. **Ablation on Token Proxy**: Systematically test alternative token sets for steering score computation and test steering using embedding similarity to abstention phrases
3. **Non-Linear Alternative Comparison**: Implement and compare against small MLP classifier on top of frozen LLM embeddings to test whether linearity assumption is necessary