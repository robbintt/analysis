---
ver: rpa2
title: 'NER4all or Context is All You Need: Using LLMs for low-effort, high-performance
  NER on historical texts. A humanities informed approach'
arxiv_id: '2502.04351'
source_url: https://arxiv.org/abs/2502.04351
tags:
- historical
- context
- https
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that readily-available, state-of-the-art
  LLMs can significantly outperform two leading NLP frameworks, spaCy and flair, for
  Named Entity Recognition (NER) in historical documents by 7-22% higher F1-Scores.
  The approach reconceptualizes NER from a purely linguistic task to a humanities-focused
  task, incorporating historical context and persona modelling into prompts.
---

# NER4all or Context is All You Need: Using LLMs for low-effort, high-performance NER on historical texts. A humanities informed approach

## Quick Facts
- arXiv ID: 2502.04351
- Source URL: https://arxiv.org/abs/2502.04351
- Reference count: 0
- Primary result: LLMs outperform spaCy and flair on historical NER by 7-22% higher F1-scores

## Executive Summary
This paper demonstrates that readily-available state-of-the-art LLMs significantly outperform leading NLP frameworks (spaCy and flair) for Named Entity Recognition on historical documents. The key innovation is reconceptualizing NER as a humanities-focused task that incorporates historical context and persona modelling into prompts, rather than treating it purely as a linguistic pattern-matching problem. Surprisingly, zero-shot approaches without examples performed better than few-shot approaches until the number of examples reached 16 or more, challenging conventional wisdom about in-context learning.

## Method Summary
The study evaluates NER performance on a 1921 German Baedeker travel guide corpus using three approaches: spaCy (de_core_news_lg), flair (ner-german-large), and GPT-4o LLM with various prompting strategies. The LLM approach uses context-rich prompts with persona modelling and span-based output format (<<TAG entity text /TAG>>). Evaluation is performed on 25 manually annotated pages with three entity types (PER, ORG, LOC). The key innovation is the humanities-informed prompt engineering that treats NER as a context-driven task rather than purely linguistic pattern recognition.

## Key Results
- GPT-4o with context-rich prompts achieves 0.87 F1-score, outperforming spaCy (0.65) and flair (0.78) by 7-22% absolute points
- Zero-shot prompting (0.87-0.88 F1) surprisingly outperforms few-shot approaches until example count reaches 16 or more
- Context-rich prompts improve recall from 0.75 to 0.84, addressing historians' need to capture all relevant entities
- Span-based output format (<<TAG ... /TAG>>) significantly reduces generation errors compared to token-level labeling

## Why This Works (Mechanism)

### Mechanism 1
Providing explicit historical and source-specific context in prompts activates domain knowledge embedded in LLMs during pretraining, improving NER classification accuracy. Context-rich prompts shift the model from pure next-token prediction on linguistic patterns to retrieval and application of relevant world knowledge. The model mimics domain-expert annotator behavior.

### Mechanism 2
Span-based tag formats (<<TAG text /TAG>>) reduce generation errors compared to token-level labeling schemes. This leverages the LLM's familiarity with structured markup patterns from training data, reducing issues like unclosed tags or token misalignment.

### Mechanism 3
Zero-shot prompting outperforms few-shot prompting until example count reaches â‰¥16 shots. Few-shot examples with random selection may introduce noise or implicit domain mismatch that degrades context activation. Only at sufficient scale does pattern coverage outweigh noise.

## Foundational Learning

- **Named Entity Recognition (NER)**: The entire paper evaluates NER performance; understanding token classification vs. span extraction is essential. Quick check: Can you explain the difference between detecting "Berlin" as a token vs. extracting "Berlin and surroundings" as a span?

- **In-Context Learning (Zero-shot vs. Few-shot)**: The paper's core intervention is prompt engineering without fine-tuning; results hinge on understanding how examples affect model behavior. Quick check: If you provide 4 random annotated examples and performance drops, what hypothesis does this paper suggest?

- **F1-Score, Precision, Recall**: All performance claims are grounded in these metrics; the paper emphasizes recall (historians want to capture all entities) over precision. Quick check: Why might a historian prioritize high recall over high precision for initial corpus screening?

## Architecture Onboarding

- **Component map**: Input Pipeline -> Prompt Constructor -> LLM Inference -> Output Parser -> Evaluation Layer
- **Critical path**: Define entity taxonomy -> Construct context block -> Run 0-shot prompt first -> Post-process with fuzzy matching
- **Design tradeoffs**: 0-shot is cheaper and often outperforms low-shot; German vs. English prompts show language-dependent recall advantages; LLMs trade API cost for lower barrier-to-entry
- **Failure signatures**: LLM "corrects" source text (mitigate with fuzzy matching), unclosed tags (switch to span format), ORGIN class fails (LLM performs better), few-shot degrades performance (check example count)
- **First 3 experiments**: 1) Run spaCy and flair baselines on 5 pages; 2) Test three prompt variants (no context, generic, specific) on same pages; 3) Add 4, 8, 16 random examples to best 0-shot prompt to confirm threshold effect

## Open Questions the Paper Calls Out

- Can context-based prompting maintain high performance on texts from earlier historical periods (16th-18th century) with different linguistic forms?
- Can LLMs reliably perform NER for novel, domain-specific entity classes defined only through natural language descriptions?
- To what extent can an LLM autonomously infer and generate appropriate contextual information for a given historical text?
- How should evaluation frameworks account for genuine ambiguities in historical texts where even expert annotators disagree?

## Limitations
- Evaluation based on single historical corpus (1921 Baedeker guide) with only 25 evaluation pages
- Exact prompt formulations incompletely specified, particularly "Specific Context" and "Full Prompt" versions
- Few-shot threshold finding sensitive to example selection and quality, yet uses random selection without exploring curated examples

## Confidence
- **High Confidence**: LLMs outperform spaCy and flair on this specific corpus (7-22% F1 improvement)
- **Medium Confidence**: Mechanism explanations (context activation, span format benefits, few-shot threshold) are plausible but rely on indirect evidence
- **Low Confidence**: Generalizability claims across historical domains and languages lack empirical support

## Next Checks
1. Test the exact same prompt strategies on at least two additional historical corpora (different languages, periods, or document types) to assess whether the 7-22% performance advantage holds
2. Systematically vary the context block content, persona framing, and instruction wording while keeping all other factors constant to quantify the contribution of each prompt engineering element
3. Compare random example selection against manually curated examples that are specifically chosen to represent the entity taxonomy and historical context, testing whether this shifts the 16-example threshold