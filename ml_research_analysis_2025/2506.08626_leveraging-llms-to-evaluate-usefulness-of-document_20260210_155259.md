---
ver: rpa2
title: Leveraging LLMs to Evaluate Usefulness of Document
arxiv_id: '2506.08626'
source_url: https://arxiv.org/abs/2506.08626
tags:
- usefulness
- document
- search
- clue
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of the traditional Cranfield
  evaluation paradigm in capturing user satisfaction by proposing a new framework
  that leverages large language models (LLMs) for document usefulness judgment. The
  authors introduce CLUE (Cascade LLM-based Usefulness Evaluation), which treats usefulness
  assessment as an ordinal regression problem using a cascading judgment structure.
---

# Leveraging LLMs to Evaluate Usefulness of Document

## Quick Facts
- arXiv ID: 2506.08626
- Source URL: https://arxiv.org/abs/2506.08626
- Reference count: 40
- This paper proposes CLUE, a cascading LLM framework for document usefulness evaluation that outperforms traditional relevance-based approaches

## Executive Summary
This paper addresses the limitations of traditional Cranfield evaluation by proposing CLUE (Cascade LLM-based Usefulness Evaluation), a framework that leverages large language models to assess document usefulness rather than just relevance. The authors treat usefulness assessment as an ordinal regression problem using a cascading judgment structure that incorporates user search context and behavioral data. By designing prompts that capture six key aspects of usefulness identified through user studies, the framework demonstrates superior performance in predicting user satisfaction compared to traditional relevance annotations. Experimental results show GPT-4o achieving a Pearson correlation of 0.38 with user usefulness labels.

## Method Summary
The CLUE framework employs a cascading LLM architecture that performs usefulness assessment as an ordinal regression problem. It uses a carefully designed prompt structure incorporating six aspects (helpful, detailed, related, encyclopedic, specific, comprehensive) identified from user studies, along with user search context and behavioral data. The framework generates usefulness judgments through a reasoning generation step, then applies a linear mapping to predict user satisfaction scores. The approach treats usefulness as a multi-level ordinal variable rather than binary relevance, allowing for more nuanced assessment of document quality and its relationship to user satisfaction.

## Key Results
- CLUE framework outperforms third-party relevance annotations in predicting user satisfaction
- GPT-4o achieves Pearson correlation of 0.38 with user usefulness labels on experimental datasets
- Usefulness judgments from CLUE significantly improve satisfaction prediction models compared to traditional relevance-based approaches
- The cascading structure effectively handles the ordinal nature of usefulness assessment

## Why This Works (Mechanism)
The framework succeeds by recognizing that usefulness is a multi-dimensional concept that cannot be captured through binary relevance judgments alone. By incorporating user context, behavioral signals, and multiple aspects of document quality, CLUE provides a more holistic assessment that better predicts actual user satisfaction. The cascading approach allows for nuanced ordinal judgments while the LLM's reasoning capabilities enable sophisticated evaluation of document characteristics that matter to users.

## Foundational Learning
- **Ordinal regression**: Why needed - usefulness is inherently multi-level rather than binary; Quick check - verify the six-point scale captures meaningful distinctions
- **User context integration**: Why needed - document usefulness depends heavily on user information needs; Quick check - test with varying levels of context detail
- **LLM reasoning capabilities**: Why needed - complex judgment requires understanding multiple document aspects; Quick check - compare with simpler rule-based approaches
- **Cascading judgment structure**: Why needed - allows hierarchical assessment of document quality; Quick check - test alternative judgment architectures
- **Prompt engineering for usefulness**: Why needed - guides LLMs to evaluate specific aspects users care about; Quick check - ablation studies on prompt components
- **Behavioral signal incorporation**: Why needed - past user interactions provide evidence of document utility; Quick check - test with different behavioral features

## Architecture Onboarding

**Component Map**: User query + clicked documents + search context → Prompt Generator → LLM Cascading Engine → Usefulness Scores → Satisfaction Predictor

**Critical Path**: The core evaluation pipeline processes user queries and their clicked documents through the cascading LLM framework, generating usefulness scores that directly predict user satisfaction.

**Design Tradeoffs**: The framework prioritizes prediction accuracy over computational efficiency by using multiple LLM calls in the cascading structure. The six-aspect prompt design provides comprehensive coverage but may introduce complexity.

**Failure Signatures**: Poor performance may occur when user context is insufficient, when documents don't align well with any of the six usefulness aspects, or when the linear mapping between usefulness scores and satisfaction breaks down.

**First Experiments**: 1) Ablation study removing individual usefulness aspects from prompts, 2) Cross-validation on different query-document subsets, 3) Comparison of different LLM models (GPT-4o vs GPT-4) on the same datasets

## Open Questions the Paper Calls Out
- Can Chain-of-Thought prompting further enhance the reasoning capabilities and accuracy of the CLUE framework?
- Can user simulation methodologies be combined with CLUE to create a fully automated, "human-free" evaluation loop?
- How can the usefulness evaluation paradigm be adapted to assess non-clicked documents?

## Limitations
- The cascade approach faces scalability challenges when extending to finer-grained or continuous usefulness assessments
- The framework's performance on specialized domains or non-English content remains untested
- The linear relationship assumed between LLM scores and human satisfaction may not hold across diverse scenarios

## Confidence
- **High Confidence**: The methodology for cascading usefulness judgment and integration of user context features
- **Medium Confidence**: Performance comparisons between different LLM models and their superiority over third-party relevance annotations
- **Low Confidence**: Claims about generalizability across domains, languages, and fine-grained usefulness scales

## Next Checks
1. Conduct ablation studies removing individual aspects (helpful, detailed, etc.) from prompts to quantify their individual contributions to usefulness assessment accuracy
2. Test the framework on specialized domain collections (medical, legal, technical) to evaluate performance outside general web search contexts
3. Implement cross-lingual validation using non-English query-document pairs to assess language dependency of the usefulness judgment approach