---
ver: rpa2
title: System-Mediated Attention Imbalances Make Vision-Language Models Say Yes
arxiv_id: '2601.12430'
source_url: https://arxiv.org/abs/2601.12430
tags:
- attention
- image
- system
- text
- simple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the yes-bias in vision-language models (VLMs),
  a form of hallucination where models respond "yes" regardless of the prompt. The
  study challenges the prevailing image-centric hypothesis, which attributes hallucination
  primarily to insufficient image attention.
---

# System-Mediated Attention Imbalances Make Vision-Language Models Say Yes

## Quick Facts
- arXiv ID: 2601.12430
- Source URL: https://arxiv.org/abs/2601.12430
- Reference count: 31
- One-line primary result: System-mediated attention imbalances, not insufficient image attention, drive yes-bias in VLMs, with late-layer interventions redistributing attention from system tokens to text/image modalities significantly reducing hallucination.

## Executive Summary
This paper challenges the prevailing image-centric hypothesis for VLM hallucination by proposing that redundant system attention—not insufficient image attention—drives the yes-bias. Using causal interventions on LLaVA-1.5 7B, the authors demonstrate that redistributing attention from system tokens (e.g., <BOS>) to image and text modalities significantly reduces yes-bias on compositional benchmarks. The intervention is most effective in late decoder layers (Q4: layers 25-32), improving paired accuracy by up to 95% on Winoground. The study concludes that system attention acts as an "attention sink," promoting coarse representations that fail on compositional tasks, and that addressing this imbalance is key to mitigating hallucination.

## Method Summary
The authors implement an inference-time causal intervention on LLaVA-1.5 7B's text decoder, specifically targeting Q4 layers (25-32). They extract post-softmax attention weights, partition them into system, image, and text modality buckets based on token positions, then zero-ablate source modality weights and redistribute proportionally to recipients using the formula α'_r = α_r + α_s × (α_r / Σα_r'). This intervention is applied to six paired-prompt yes/no benchmarks (BEAF, HallusionBench, MME, NaturalBench, SugarCrepe, Winoground) to measure yes-bias reduction and paired accuracy improvement.

## Key Results
- System-to-text redistribution in Q4 improved paired accuracy by up to 95% on Winoground
- System-to-image redistribution improved paired accuracy by 164% on HallusionBench
- Intervention effects were concentrated in Q4 layers, with global interventions degrading performance
- MME benchmark showed inverse pattern (intervention harmed coarse-representation tasks)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Redundant attention to system tokens (e.g., <BOS>) deprives both image and text modalities, causing attention imbalances that manifest as yes-bias.
- Mechanism: System tokens act as attention sinks, capturing ~70% of total attention despite being semantically uninformative. This leaves insufficient attention for fine-grained processing of image patches and text queries, pushing the model toward default responses.
- Core assumption: The attention distribution observed reflects causal influence on behavior, not merely correlation.
- Evidence anchors:
  - [abstract] "attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs"
  - [section 1] "prior work reports that this modality often accounts for over 70% of the total attention in VLM decoders"
  - [corpus] Related work (PAINT, PAS) similarly identifies attention calibration as key to hallucination mitigation, but focuses on image tokens rather than system tokens; corpus evidence for system-specific mechanisms is weak.
- Break condition: If system attention serves non-redundant functions (e.g., positional anchoring, training stability) beyond coarse aggregation, complete redistribution may degrade performance on tasks requiring those functions.

### Mechanism 2
- Claim: The yes-bias arises specifically from late-layer (Q4: layers 25-32) attention imbalances, not global or early-layer patterns.
- Mechanism: Late decoder layers are where modality-specific information gets consolidated for output generation. Redundant system attention at this stage blocks fine-grained cross-modal integration, while early-layer imbalances don't affect output behavior.
- Core assumption: Quarter-level analysis (8-layer blocks) captures the relevant functional units; per-layer dynamics may differ.
- Evidence anchors:
  - [section A.2.2] "Accuracy gains are almost exclusively concentrated in Q4 (layers 25-32)"
  - [section A.2.1] "global interventions predominantly degrade performance"
  - [corpus] No corpus papers specifically localize hallucination mechanisms to late layers; this is a novel finding requiring validation.
- Break condition: If other architectures distribute the relevant computation differently (not late-layer concentrated), the intervention location must be recalibrated.

### Mechanism 3
- Claim: System attention encourages default reliance on coarse input representations, which works for some tasks but fails on compositional tasks, triggering yes-bias fallback.
- Mechanism: Attention sinks like <BOS> promote coarse cross-modal aggregation. This suffices for high-level matching (landmark recognition) but fails when tasks require discriminating minimal differences. When coarse representations are inadequate, the model defaults to "yes" as a learned response pattern.
- Core assumption: Yes-bias is a fallback behavior triggered by representation uncertainty, not an independent failure mode.
- Evidence anchors:
  - [section 5] "system-mediated attention imbalances in the baseline model encourage the reliance on coarse, aggregated representations"
  - [section 5] Q4 system redistribution degrades MME performance (which permits coarse representations) while improving compositional benchmarks
  - [corpus] Corpus papers do not address the task-dependent nature of intervention effectiveness.
- Break condition: If yes-bias stems from training data imbalance rather than attention-driven representation collapse, attention redistribution alone cannot fully address it.

## Foundational Learning

- Concept: **Attention Sinks in Transformers**
  - Why needed here: The paper's core intervention requires understanding why tokens like <BOS> receive disproportionate attention and what function this serves (or doesn't serve).
  - Quick check question: Can you explain why removing all attention to sink tokens in early layers often causes model collapse, but redistributing them in late layers improves performance?

- Concept: **Modality-Specific Attention in VLMs**
  - Why needed here: The causal intervention operates by manipulating attention weights across three distinct modality regions in the input sequence.
  - Quick check question: Given a VLM input sequence, can you identify which token positions correspond to system, image, and text modalities, and explain how attention flows between them?

- Concept: **Post-Softmax Attention Redistribution**
  - Why needed here: The intervention method (Equations 2-3) requires zeroing source modality weights and proportionally reallocating to recipients while maintaining valid probability distributions.
  - Quick check question: Why must redistribution happen after softmax rather than before, and what numerical stability concerns arise?

## Architecture Onboarding

- Component map:
  - `<BOS> + system_message + <IMAGE_TOKENS> + user_query` -> Text Decoder (32-layer transformer) -> Output
  - System modality: Everything before image tokens (including <BOS>, instructions)
  - Image modality: Patch embeddings projected into text decoder's embedding space
  - Text modality: User query tokens only
  - Q4 = layers 25-32 in 32-layer transformer

- Critical path:
  1. Identify Q4 layers in the text decoder
  2. For each forward pass, extract attention weights after softmax
  3. Partition weights into system/image/text buckets based on token positions
  4. Zero out source modality weights (α'_s = 0)
  5. Redistribute proportionally to recipients: α'_r = α_r + α_s × (α_r / Σ α_r')
  6. Apply modified weights to value aggregation

- Design tradeoffs:
  - **Proportional vs. uniform redistribution**: Proportional preserves existing attention preferences; uniform may introduce bias. Paper uses proportional.
  - **Q4-only vs. global intervention**: Global harms performance; Q4 is the effective intervention zone but may miss inter-layer dynamics.
  - **Complete vs. partial redistribution**: Paper uses 100% for simplicity; graduated transfers (10-30%) could reveal dose-response relationships but were not systematically explored.

- Failure signatures:
  - **MME performance degradation**: Intervention harms tasks relying on coarse representations (landmark/artwork recognition). If your use case prioritizes knowledge extraction over compositional reasoning, this intervention may backfire.
  - **Model collapse at scale**: LLaVA-1.5 13B shows inconsistent gains; baseline already outperforms intervention. Scale changes attention patterns.
  - **Memory issues**: Full-benchmark evaluation on 13B models failed due to memory constraints; intervention itself adds computational overhead.

- First 3 experiments:
  1. **Establish baseline yes-rate**: Run the six paired-prompt benchmarks (BEAF, HallusionBench, MME, NaturalBench, SugarCrepe, Winoground) without intervention. Calculate yes-rate disparity from ground truth. If yes-rate is near 50% for balanced datasets, the model doesn't exhibit yes-bias and this intervention won't help.
  2. **Q4 system-to-text redistribution**: Implement proportional redistribution from system to text modality only in Q4. This was the highest-performing intervention. Compare paired accuracy improvement against the image-boosting baseline (PAI or Image×2.0).
  3. **Layer-sensitivity sweep**: Test redistribution at Q1, Q2, Q3, and Q4 separately to verify late-layer concentration. If gains appear in Q2 or Q3, the model's functional architecture differs from LLaVA-1.5 7B and intervention location must be recalibrated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do system-mediated attention imbalances contribute to autoregressive hallucination (e.g., in image captioning), or is this mechanism specific to yes/no binary tasks?
- Basis in paper: [explicit] The conclusion states: "future work should explore the implications of our findings for autoregressive hallucination."
- Why unresolved: This study restricted scope to single-token yes/no responses; autoregressive generation involves different decoding dynamics and evaluation challenges.
- What evidence would resolve it: Applying comparable attention redistribution interventions to autoregressive tasks (e.g., image captioning, VQA) and measuring hallucination rates using established metrics.

### Open Question 2
- Question: Does redundant system attention drive yes-bias in text-only large language models, or is this phenomenon specific to multimodal architectures?
- Basis in paper: [explicit] The conclusion explicitly calls for investigating "the prospect of redundant system attention driving the yes-bias in text-only LLMs."
- Why unresolved: The current study focused on VLMs where system, image, and text modalities interact; text-only LLMs lack the image modality entirely.
- What evidence would resolve it: Conducting analogous attention redistribution experiments on text-only LLMs using yes/no benchmarks and comparing the effectiveness of system-to-text interventions.

### Open Question 3
- Question: What is the optimal degree of attention redistribution from system tokens, and are the performance improvements monotonic as system attention decreases?
- Basis in paper: [inferred] The Limitations section acknowledges that the use of "zero-ablation with complete redistribution... prevented us from methodically exploring the effects of partial redistributions."
- Why unresolved: Complete (100%) redistribution was used for simplicity, leaving unexplored whether intermediate redistribution levels yield better or more stable improvements.
- What evidence would resolve it: Systematically varying the percentage of redistributed system attention (e.g., 10%, 25%, 50%, 75%, 100%) and measuring performance across benchmarks to identify optimal transfer amounts.

### Open Question 4
- Question: Can the coarse-representation hypothesis be directly validated by probing model internal states to measure representational uncertainty or granularity?
- Basis in paper: [inferred] The Limitations section notes that "claims regarding representation-level mechanisms are based entirely on model performance and attention weights" and should be "further validated through directly probing a model's internal states."
- Why unresolved: The evidence linking system attention to coarse representations is indirect (behavioral performance patterns); no direct neural probing was conducted.
- What evidence would resolve it: Using representational analysis techniques (e.g., probing classifiers, uncertainty quantification, representation similarity analysis) to directly measure whether tasks with high yes-bias correlate with coarser or more uncertain internal representations.

## Limitations

- **Scale-dependent effectiveness**: Intervention shows inconsistent gains on LLaVA-1.5 13B, with memory constraints preventing full evaluation
- **Task-dependent tradeoffs**: Same mechanism that fixes yes-bias on compositional tasks degrades performance on knowledge extraction tasks
- **Assumption of redundancy**: Paper assumes system tokens serve no functional purpose beyond being attention sinks without ablation validation

## Confidence

**High Confidence**: The empirical finding that Q4 system-to-text redistribution improves paired accuracy on compositional benchmarks (Winoground, HallusionBench) is well-supported with specific percentage improvements. The methodology for causal intervention and attention redistribution is clearly specified and reproducible.

**Medium Confidence**: The mechanism that redundant system attention causes yes-bias by encouraging coarse representations is plausible but relies on several assumptions about token functionality and attention flow that aren't fully validated. The task-dependent nature of intervention effects is observed but not deeply explained.

**Low Confidence**: The generalization to LLaVA-1.5 13B is problematic since the intervention shows inconsistent gains and the paper couldn't run full benchmarks due to memory constraints. The claim that system attention serves no functional purpose beyond being an "attention sink" is asserted but not experimentally verified through ablation studies.

## Next Checks

**Check 1: Intervention Location Sensitivity Across Architectures**: Test the same proportional redistribution intervention at different layer blocks (Q1-Q4) for multiple VLM architectures including BLIP-2, Flamingo, and MiniGPT-4. If Q4 concentration varies by architecture, the intervention location cannot be generalized and must be calibrated per model.

**Check 2: System Token Functionality Ablation**: Implement an ablation where system tokens are completely removed from the input sequence (not just their attention redistributed). Compare yes-bias rates between complete removal vs. attention redistribution. If removal causes worse performance, system tokens serve non-redundant functions beyond being attention sinks.

**Check 3: Training Data Imbalance Control**: Train a LLaVA-1.5 7B variant with balanced yes/no pairs in the training data and compare yes-bias rates with the original. If the intervention still improves performance on the balanced model, attention redistribution addresses the mechanism directly. If not, the intervention may be compensating for training data issues rather than fixing attention imbalances.