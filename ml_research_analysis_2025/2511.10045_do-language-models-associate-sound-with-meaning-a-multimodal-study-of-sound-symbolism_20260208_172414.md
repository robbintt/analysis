---
ver: rpa2
title: Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound
  Symbolism
arxiv_id: '2511.10045'
source_url: https://arxiv.org/abs/2511.10045
tags:
- semantic
- words
- word
- audio
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how Multimodal Large Language Models (MLLMs)
  interpret sound-symbolic relationships between phonetic forms and semantic meanings.
  To explore this, researchers constructed LEX-ICON, a large-scale dataset of 10,982
  mimetic words (including 8,052 natural words and 2,930 systematically constructed
  pseudo-words) annotated with up to 25 semantic dimensions across four languages
  (English, French, Japanese, and Korean).
---

# Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism

## Quick Facts
- **arXiv ID:** 2511.10045
- **Source URL:** https://arxiv.org/abs/2511.10045
- **Reference count:** 40
- **Primary result:** MLLMs achieve above-baseline performance in detecting sound-symbolic associations, even for constructed pseudo-words without lexical memorization

## Executive Summary
This study investigates whether Multimodal Large Language Models (MLLMs) can systematically associate phonetic forms with semantic meanings through sound symbolism. Researchers constructed LEX-ICON, a dataset of 10,982 words across four languages, and tested MLLMs on predicting semantic associations across text, IPA, and audio modalities. Results show MLLMs consistently achieve above-baseline performance (0.50 macro-F1) in 84.2% of semantic dimensions for natural words and 68.4% for constructed pseudo-words, indicating genuine phonetic intuition rather than memorization. The study reveals modality-specific processing preferences and identifies attention patterns focused on iconic phonemes in late model layers.

## Method Summary
The study employed a semantic dimension prediction task using the LEX-ICON dataset containing 8,052 natural words and 2,930 constructed CVCV pseudo-words across English, French, Japanese, and Korean. Each word was annotated with up to 25 binary semantic dimensions using a four-LLM consensus method. Three input types (original text, IPA-converted text, and TTS audio) were processed by MLLMs including Qwen2.5-Omni, GPT-4o, and Gemini-2.5-flash. Performance was measured via macro-F1 scores, with attention analysis extracting phoneme-to-semantic feature attention fractions across model layers. Human evaluation validated the pseudo-ground truth on a subset of 50 words.

## Key Results
- MLLMs achieve above-baseline performance (0.50 macro-F1) in 84.2% of semantic dimensions for natural words and 68.4% for constructed pseudo-words
- MLLMs show modality-specific preferences, using audio for acoustically grounded features and text for articulatory/visual features
- Attention analysis reveals higher focus on iconic phonemes in late layers when processing constructed pseudo-words

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MLLMs can systematically associate phonetic forms with semantic meanings, demonstrating a "phonetic intuition" that generalizes beyond memorized vocabulary.
- **Mechanism:** The model processes input modalities into a shared representation space. When presented with binary semantic choices, the model's internal activations align more closely with one semantic prototype than the other. This alignment is quantified via macro-F1 scores on a semantic dimension prediction task.
- **Core assumption:** The pseudo-ground truth data, generated via consensus among multiple LLMs, accurately reflects human-like phonetic iconicity.
- **Evidence anchors:** [abstract] "Results show that MLLMs consistently achieve above-baseline performance (0.50 macro-F1) in 84.2% of semantic dimensions for natural words and 68.4% for constructed pseudo-words..."; [section 4.3] Performance on constructed pseudo-words excludes memorization; [corpus] "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages" provides external context.
- **Break condition:** If performance on constructed pseudo-words drops to random baseline (0.50 macro-F1), the mechanism relies on lexical memorization.

### Mechanism 2
- **Claim:** MLLMs exhibit modality-specific processing preferences, using different input pathways for different types of semantic features.
- **Mechanism:** The model leverages its integrated representation space but weights modalities differently based on the semantic task. For acoustically grounded features like size or speed, the model derives stronger signals from audio modality. For articulatory or visually driven features like beauty, it relies more on text. This is measured by comparing performance deltas between audio and text inputs.
- **Core assumption:** TTS-generated audio and IPA-converted text sufficiently preserve phonetic cues necessary for iconicity detection.
- **Evidence anchors:** [section 4.3] "MLLMs exhibit modality-specific preferences across semantic dimensions, using audio for acoustically grounded features... and text for articulatory or visually driven features..."; [section 5.3] Advantage score analysis provides quantitative evidence; [corpus] "From Monolingual to Bilingual: Investigating Language Conditioning in LLMs" supports input conditioning differences.
- **Break condition:** If performance advantages for specific modalities disappear or reverse across different model architectures.

### Mechanism 3
- **Claim:** MLLMs internally attend to "iconic phonemes" more strongly in late processing layers when analyzing sound-symbolic words.
- **Mechanism:** The model's transformer attention heads assign higher fraction scores to phonemes known to be associated with specific semantic features. This attentional focus is more pronounced in later layers, suggesting reasoning circuitry refines focus on task-relevant phonetic features.
- **Core assumption:** The "attention fraction score"—calculated between phoneme tokens and semantic feature tokens—is a valid proxy for internal reasoning.
- **Evidence anchors:** [section 5.3] "attention fraction to these sound-symbolic phonemes is more prominent in models' late-layers when processing constructed pseudo-words..."; [section 5.3, Figure 7] Layer-wise attention plots show upward trend in attention fraction for IPA text in constructed words.
- **Break condition:** If attention fraction scores do not exceed 0.5 baseline or show no layer-wise trend.

## Foundational Learning

- **Concept: Sound Symbolism (Phonetic Iconicity)**
  - **Why needed here:** This is the core phenomenon under investigation. The paper assumes understanding of the "bouba-kiki" effect—the non-arbitrary link between sound and meaning.
  - **Quick check question:** If you create a new word "glorp," would most people associate it with a sharp or a round shape? Why?

- **Concept: Semantic Differential Method**
  - **Why needed here:** The LEX-ICON dataset is built using this methodology, projecting word meanings onto 25 binary scales (e.g., fast vs. slow, happy vs. sad). Understanding this is key to interpreting results.
  - **Quick check question:** How would you represent the meaning of the word "boulder" using the semantic differential scale for "heavy-light"?

- **Concept: Transformer Attention Heads**
  - **Why needed here:** The paper's internal analysis measures "attention fraction scores" in specific layers and heads. You need to know what attention heads do to interpret Figure 6 and 7.
  - **Quick check question:** In a transformer, what does the attention weight between two tokens represent?

## Architecture Onboarding

- **Component map:** LEX-ICON dataset -> MLLM inference engine -> Semantic prediction head -> Attention extraction module
- **Critical path:**
  1. Data Curation: Convert words to IPA and TTS audio. Annotate with semantic features using LLM ensemble.
  2. Inference: Feed three input types into MLLM with binary-choice prompt.
  3. Analysis: Calculate macro-F1 scores. Extract and normalize attention scores for iconic vs. non-iconic phonemes across layers.

- **Design tradeoffs:**
  - Pseudo vs. Human Ground Truth: LLM ensemble scales efficiently but may inherit model biases; human evaluation done on smaller sample.
  - TTS vs. Real Audio: TTS ensures consistency but may strip away prosodic cues present in human speech.

- **Failure signatures:**
  - Lexical Memorization: High performance on natural words but chance performance on constructed pseudo-words.
  - Attention Baseline: Layer-wise attention fraction scores hover around 0.5 with no upward trend.

- **First 3 experiments:**
  1. Baseline Probe: Run semantic prediction on natural word subset using only original text. Confirm macro-F1 > 0.5 on most dimensions.
  2. Generalization Test: Run same task on constructed pseudo-word subset. Scores should remain above 0.5 to indicate genuine phonetic intuition.
  3. Attention Check: For constructed word with known iconic phoneme (e.g., /k/ for "sharp"), extract attention scores from final layers. Confirm "sharp" semantic token has higher attention than "round" when attending to /k/ phoneme.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does prosodic and intonational variation in naturally produced speech affect MLLMs' sound-symbolic associations compared to TTS-generated audio?
  - **Basis in paper:** [explicit] The conclusion states: "Future work could... investigate modality-specific information such as intonation." Additionally, all audio stimuli were TTS-generated, limiting acoustic naturalness.
  - **Why unresolved:** TTS systems may not capture subtle prosodic features that humans use in sound-symbolic communication, yet the study treats TTS audio as equivalent to natural speech.
  - **What evidence would resolve it:** Compare MLLM performance on matched natural vs. TTS audio stimuli, and systematically vary prosodic features (pitch contours, duration, stress patterns) to measure their impact on semantic dimension prediction.

- **Open Question 2:** Why do MLLMs show discrepancies from human performance on certain semantic dimensions (e.g., simple-complex, strong-weak) despite overall above-baseline performance?
  - **Basis in paper:** [explicit] The authors state: "we reveal that there still remain discrepancies between the semantic dimensions where humans exhibit high scores and those where models perform well."
  - **Why unresolved:** The paper documents this gap but does not investigate its causes—whether due to training data, model architecture, or fundamental differences in how models represent certain semantic features.
  - **What evidence would resolve it:** Analyze attention patterns specifically for low-alignment dimensions; test whether fine-tuning on sound-symbolic data improves alignment; examine training corpora for dimension-specific biases.

- **Open Question 3:** Would sound-symbolic grounding improve MLLM performance on downstream tasks such as language learning or brand name evaluation?
  - **Basis in paper:** [explicit] The conclusion proposes: "extend it to application fields such as language learning or brand effects."
  - **Why unresolved:** The study establishes that MLLMs can detect phonetic iconicity, but does not test whether this capability transfers to practical applications or can be leveraged for task improvement.
  - **What evidence would resolve it:** Design intervention studies where models use sound-symbolic features for vocabulary learning tasks or brand name appropriateness judgments, comparing performance with and without explicit sound-symbolic prompting.

## Limitations

- The reliance on LLM-generated pseudo-ground truth (rather than human annotations for the full dataset) introduces uncertainty about whether the model is detecting true phonetic iconicity or optimizing for internal consistency with its annotator ensemble.
- The use of TTS-generated audio may lack the prosodic and acoustic nuances of natural speech that could influence sound symbolism judgments.
- The attention analysis provides correlational rather than causal evidence—higher attention fraction scores for iconic phonemes don't definitively prove these features drive semantic predictions.

## Confidence

- **High Confidence:** MLLMs demonstrate above-baseline performance on semantic dimension prediction tasks for both natural and constructed words.
- **Medium Confidence:** Modality-specific preferences represent genuine processing strategy rather than architectural artifacts.
- **Low Confidence:** The claim that MLLMs possess genuine "phonetic intuition" that generalizes beyond training data.

## Next Checks

- **Check 1:** Conduct human evaluation on a larger, stratified sample of the LEX-ICON dataset (e.g., 500 words across different semantic dimensions and languages) to validate whether LLM-generated ground truth aligns with human sound-symbolic judgments.
- **Check 2:** Test the same semantic prediction task on a model trained exclusively on text data to determine whether multimodal architecture provides unique advantages for sound symbolism detection.
- **Check 3:** Perform ablation studies on the attention mechanism by masking attention to specific phoneme types during inference. If masking iconic phonemes significantly degrades performance on constructed words but not natural words, this would provide stronger causal evidence that attention to phonetic structure drives sound-symbolic associations.