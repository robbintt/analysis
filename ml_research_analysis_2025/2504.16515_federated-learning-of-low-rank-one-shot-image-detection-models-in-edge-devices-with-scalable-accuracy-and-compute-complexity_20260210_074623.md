---
ver: rpa2
title: Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices
  with Scalable Accuracy and Compute Complexity
arxiv_id: '2504.16515'
source_url: https://arxiv.org/abs/2504.16515
tags:
- lora
- learning
- federated
- edge
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoRa-FL, a federated learning framework that
  uses low-rank adaptation (LoRa) modules to train one-shot image detection models
  on edge devices. By integrating LoRa adapters into Siamese network architectures,
  the method reduces both computational and communication overhead while maintaining
  competitive accuracy.
---

# Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity

## Quick Facts
- **arXiv ID:** 2504.16515
- **Source URL:** https://arxiv.org/abs/2504.16515
- **Reference count:** 40
- **Primary result:** LoRa-FL achieves high accuracy with 31x-35x reduction in FLOPs and communication by training only low-rank LoRA adapter modules in federated one-shot image detection.

## Executive Summary
This paper proposes LoRa-FL, a federated learning framework that uses low-rank adaptation (LoRA) modules to train one-shot image detection models on edge devices. By integrating LoRa adapters into Siamese network architectures, the method reduces both computational and communication overhead while maintaining competitive accuracy. Experiments on MNIST and CIFAR-10 datasets (IID and non-IID) show that LoRa-FL achieves high test accuracy with significantly lower communication bandwidth and compute complexity compared to full-model training.

## Method Summary
LoRa-FL uses a Siamese network with two frozen SqueezeNet backbones pre-trained on ImageNet. Three LoRA adapter layers (two for 128-dimensional embeddings, one for similarity scoring) are trained during federated learning. The central server aggregates only LoRA parameters via FederatedAveraging while keeping backbones frozen. The approach is evaluated across various LoRA ranks (k ∈ {1, 2, 4, 8, 16, 32}) on MNIST and CIFAR-10 datasets under both IID and non-IID settings.

## Key Results
- On MNIST-IID, k=8 LoRa rank attained 97% accuracy while reducing FLOPs by ~31x and communication by ~35x compared to full-rank models
- For CIFAR-10-IID, k≥8 achieved 80-85% accuracy within 4-6 rounds
- Non-IID settings show accuracy degradation but still converge as k increases, demonstrating robustness to data heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing weight matrices into low-rank factorizations reduces trainable parameters and communication bandwidth with controllable accuracy tradeoffs.
- **Mechanism:** A standard weight matrix $W$ of dimensions $(N, M)$ is factorized into two smaller matrices $A$ $(N, k)$ and $B$ $(k, M)$, where rank $k \ll \min(N, M)$. During federated training, only the entries of $A$ and $B$ are updated and transmitted, reducing parameter count from $N \times M$ to $k(N + M)$.
- **Core assumption:** Weight matrices in the adaptation layers exhibit low intrinsic rank sufficient for the one-shot similarity task, meaning critical task information can be captured in a lower-dimensional subspace.
- **Evidence anchors:**
  - [Section III-C]: "For each client i, LoRa modules essentially model a standard fully-connected weight matrix $W_i$ of size $(N, M)$ as a matrix-product between two low-rank matrices $A$ and $B$ of size $(N, k)$ and $(k, M)$ respectively."
  - [Section IV-E]: "Fig. 7 shows that a reduction in the rank k leads to an exponential reduction in edge device compute complexity and communication bandwidth."
  - [corpus]: Neighbor paper FedMHO (arXiv:2502.08518) similarly addresses heterogeneous one-shot FL for resource-constrained devices, corroborating the broader relevance of parameter-efficient FL approaches, though not specifically validating low-rank factorization.
- **Break condition:** If the downstream task requires high-rank feature transformations (e.g., fine-grained discrimination across many similar classes), low $k$ values will underfit and accuracy will plateau below acceptable thresholds regardless of training duration.

### Mechanism 2
- **Claim:** Freezing a pre-trained backbone and training only adapter layers preserves feature extraction capability while drastically reducing local compute.
- **Mechanism:** A Siamese network uses two frozen SqueezeNet backbones (pre-trained on ImageNet) as feature extractors. LoRA adapter layers ($fc\_lora\_1$, $fc\_lora\_2$, $similarity\_lora$) process the backbone outputs for similarity scoring. Only the LoRA parameters are updated during local training.
- **Core assumption:** Pre-trained backbone features are sufficiently generic for the target one-shot image detection task and do not require domain-specific fine-tuning.
- **Evidence anchors:**
  - [Section III-A]: "This backbone is not trained during the FL process to reduce the communication and computational load on the edge devices."
  - [Section IV-A]: "For the MNIST dataset under the IID setting... reaching over 98% for $k \geq 8$," suggesting frozen backbone with LoRA adapters achieves competitive performance.
  - [corpus]: Weak direct evidence. Neighbor papers focus on one-shot FL or edge FL broadly but do not specifically validate frozen-backbone-plus-adapter architectures for one-shot detection.
- **Break condition:** If backbone features are mismatched to the target domain (e.g., ImageNet pre-training on natural images applied to medical imaging or satellite imagery without adaptation), accuracy gains from LoRA adapters alone may be insufficient.

### Mechanism 3
- **Claim:** Applying FederatedAveraging exclusively to low-rank adapter parameters preserves convergence while reducing communication rounds' payload size.
- **Mechanism:** After local training, clients transmit only their $A$ and $B$ matrices to the server. The server aggregates these matrices via FederatedAveraging, producing global adapter weights that are distributed back to clients. Backbone weights remain unchanged throughout.
- **Core assumption:** Adapter parameter aggregation captures sufficient distributed knowledge for global model improvement, and heterogeneity in client data distributions can be absorbed by low-rank updates without catastrophic interference.
- **Evidence anchors:**
  - [Section III-C]: "The central server receives the client updates and merges these updates using the FederatedAveraging algorithm in [5], by aggregating the LoRA adapter parameters only, while not modifying the weights in the Siamese SqueezeNet backbones."
  - [Section IV-C, IV-D]: Non-IID experiments show performance drops compared to IID but still converge as $k$ increases, indicating low-rank aggregation handles moderate heterogeneity.
  - [corpus]: Neighbor paper "Towards One-shot Federated Learning" (arXiv:2505.02426) discusses one-round FL challenges but does not provide direct validation of low-rank adapter aggregation specifically.
- **Break condition:** Under severe non-IID conditions with highly skewed class distributions, low-rank adapters may lack capacity to represent divergent local optima, causing convergence instability or accuracy collapse.

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA)
  - **Why needed here:** Core to reducing parameters trained and transmitted in the FL pipeline.
  - **Quick check question:** Can you explain why factorizing a weight matrix $W$ into $A \times B$ with small $k$ reduces both compute and memory compared to full-rank $W$?

- **Concept:** Siamese Networks for One-Shot Learning
  - **Why needed here:** The architecture uses a Siamese structure to compare target and query images via learned embeddings.
  - **Quick check question:** How does a Siamese network produce a similarity score from two input images using shared weights?

- **Concept:** FederatedAveraging (FedAvg)
  - **Why needed here:** The aggregation mechanism that combines client adapter updates into a global model.
  - **Quick check question:** In FedAvg, what is aggregated from clients and how does the server produce updated global parameters?

## Architecture Onboarding

- **Component map:**
  - Image1 -> Frozen SqueezeNet Backbone -> $x_1$ -> $fc\_lora\_1$ -> $u_1$ (128-dim)
  - Image2 -> Frozen SqueezeNet Backbone -> $x_2$ -> $fc\_lora\_2$ -> $u_2$ (128-dim)
  - $|u_1 - u_2|$ -> $similarity\_lora$ -> Similarity Logit
  - Similarity Logit -> Sigmoid -> $[0, 1]$ Probability

- **Critical path:**
  1. Initialize LoRA layers with rank $k$ (sweep $k \in \{1, 2, 4, 8, 16, 32\}$ during experimentation).
  2. Distribute frozen backbone and initial LoRA weights to $C=5$ clients.
  3. Each selected client ($K=3$ per round) trains locally on one-shot pairs.
  4. Clients upload only $A$ and $B$ matrices.
  5. Server aggregates and broadcasts updated LoRA weights.
  6. Repeat for 10 global rounds.

- **Design tradeoffs:**
  - **Lower rank $k$:** Reduces FLOPs and bandwidth but may underfit on complex datasets (e.g., CIFAR-10 requires $k \geq 8$ for usable accuracy).
  - **Frozen backbone vs. fine-tuning:** Freezing reduces compute but may limit domain adaptation.
  - **IID vs. non-IID:** Non-IID degrades accuracy due to catastrophic interference; higher $k$ partially mitigates.

- **Failure signatures:**
  - Accuracy plateaus well below target with increasing rounds → $k$ too low; increase rank.
  - High variance across folds or rounds → check data imbalance or non-IID severity.
  - Communication savings not realized → verify only LoRA weights are transmitted, not backbone.

- **First 3 experiments:**
  1. **Baseline rank sweep on MNIST IID:** Train LoRa-FL with $k \in \{1, 2, 4, 8, 16, 32\}$ for 10 rounds; confirm accuracy plateaus at $k \geq 8$ (~97–98%) as per Fig. 3.
  2. **CIFAR-10 IID validation:** Repeat rank sweep; verify $k \geq 8$ achieves 80–85% accuracy within 4–6 rounds as per Fig. 4.
  3. **Bandwidth/FLOPs profiling:** For $k=8$ and $k=32$, measure actual transmitted parameter size and FLOPs during local training; confirm ~4.25× bandwidth reduction and ~3.6× FLOPs reduction as per Fig. 7.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degradation in non-IID settings isn't fully quantified with specific comparative metrics across different skew levels
- Experimental setup uses only 5 clients total with 3 selected per round, which is a limited simulation of real-world federated learning scenarios
- Reported computational savings are based on theoretical parameter counting rather than measured runtime performance on actual edge devices

## Confidence
- **High Confidence:** The mechanism of low-rank factorization reducing parameter count and communication overhead is mathematically sound and well-established in the literature
- **Medium Confidence:** The reported accuracy results on MNIST and CIFAR-10, as the experimental setup appears complete though some implementation details are unspecified
- **Low Confidence:** The generalization claims to real-world edge deployment scenarios, as the evaluation is limited to synthetic datasets and small-scale simulations

## Next Checks
1. Implement the same LoRa-FL framework on a more complex dataset (e.g., CIFAR-100 or TinyImageNet) to verify scalability of the approach
2. Conduct ablation studies on backbone freezing by comparing performance when fine-tuning vs. freezing to quantify the tradeoff
3. Measure actual runtime performance (latency, energy consumption) on representative edge hardware to validate theoretical computational savings