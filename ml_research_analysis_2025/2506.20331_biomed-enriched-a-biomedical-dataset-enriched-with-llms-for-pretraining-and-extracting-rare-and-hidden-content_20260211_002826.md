---
ver: rpa2
title: 'Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and
  Extracting Rare and Hidden Content'
arxiv_id: '2506.20331'
source_url: https://arxiv.org/abs/2506.20331
tags:
- clinical
- biomedical
- educational
- content
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Biomed-Enriched, a biomedical text dataset
  constructed from PubMed using a two-stage LLM annotation process. The method involves
  first using a large language model to annotate 400K paragraphs from PubMed with
  scores for document type, domain, educational quality, and language, then distilling
  these annotations into a smaller model to process the full PMC-OA corpus.
---

# Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content

## Quick Facts
- arXiv ID: 2506.20331
- Source URL: https://arxiv.org/abs/2506.20331
- Authors: Rian Touchent; Nathan Godey; Eric de la Clergerie
- Reference count: 32
- Key outcome: Two-stage LLM annotation enables extraction of 2M clinical case paragraphs with 450K high-quality ones under commercial-use licenses; clinical upsampling improves MMLU ProfMed by 5%, educational filtering improves MedQA/MedMCQA by 1%, and combined strategies achieve baseline performance using only one-third of training tokens.

## Executive Summary
This paper introduces Biomed-Enriched, a biomedical text dataset constructed from PubMed using a two-stage LLM annotation process. The method involves first using a large language model to annotate 400K paragraphs from PubMed with scores for document type, domain, educational quality, and language, then distilling these annotations into a smaller model to process the full PMC-OA corpus. The resulting metadata enables extraction of refined subsets including 2M clinical case paragraphs with over 450K high-quality ones under commercial-use licenses. Experimental results show that clinical upsampling improved MMLU ProfMed performance by 5%, educational quality filtering boosted MedQA and MedMCQA by 1%, and combined strategies achieved equivalent performance using only one-third of training tokens compared to standard approaches.

## Method Summary
The method employs a two-stage LLM annotation approach: first, Llama-3.1-70B-Instruct annotates 400K paragraphs from PubMed with multi-dimensional labels (document type, domain, educational quality 1-5, language); second, a smaller XLM-RoBERTa-base model is fine-tuned on these annotations via multi-task learning and applied to the full 133M-paragraph PMC-OA corpus. This produces metadata enabling construction of dataset variants: BE-Base (raw), BE-Educational (score ≥3), BE-Clinical (10× clinical), BE-ClinicalCase (10× cases), BE-French (10× French), and BE-All (combined). The datasets are used for continual pretraining of OLMo2-7B-stage1 models, with evaluation on medical benchmarks.

## Key Results
- Clinical upsampling (10×) boosted MMLU Professional Medicine performance by 4.04 percentage points (63.97% vs 59.93% baseline)
- Educational quality filtering (score ≥3) improved MedMCQA by 1.17 points and Medical Genetics by 2 points
- Combined strategies achieved equivalent performance to BE-Base using approximately one-third of training tokens
- BE-Clinical improved ProfMed but degraded College Biology by 5.5 points, revealing specialization trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two-stage LLM-to-classifier distillation enables scalable, fine-grained annotation of large corpora.
- **Mechanism:** A large LLM (Llama-3.1-70B-Instruct) annotates 400K paragraphs with multi-dimensional labels (type, domain, educational quality, language). A smaller XLM-RoBERTa-base model is fine-tuned on these annotations via multi-task learning and applied to the full PMC-OA corpus, reducing inference cost by orders of magnitude while preserving label quality.
- **Core assumption:** The LLM's annotation patterns generalize to the unannotated majority of the corpus, and the classifier can approximate them without catastrophic degradation.
- **Evidence anchors:**
  - [abstract]: "a large language model annotates 400K paragraphs... These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus."
  - [Section 3.2]: "The distilled model achieved strong performance with 0.805 F1 score for domain classification, 0.854 F1 score for document type classification, and 0.245 MSE on educational quality score prediction."
  - [corpus]: No direct corpus corroboration for this specific distillation setup; weak external validation.
- **Break condition:** Classifier F1 drops below ~0.7 on held-out validation (per dimension), or MSE on quality regression exceeds 0.5—signals that distilled labels no longer reflect LLM judgments.

### Mechanism 2
- **Claim:** Educational-quality filtering improves medical QA performance by preferentially retaining pedagogically coherent, concept-dense paragraphs.
- **Mechanism:** Paragraphs with educational scores ≥3 (introducing key concepts with reasonable coherence or better) are retained; lower-scored content is discarded. This reduces noise (administrative, promotional, incoherent material) and increases the density of learning-relevant signal per token.
- **Core assumption:** Educational coherence and college-level relevance correlate with downstream QA performance, and removal of low-scored paragraphs does not eliminate critical rare facts.
- **Evidence anchors:**
  - [abstract]: "educational quality filtering improving MedQA and MedMCQA by ~1%."
  - [Section 4.1/4.2 + Table 3]: BE-Educational achieves 43.08% on MedMCQA (+1.17 pts vs. BE-Base) and 71.00% on Medical Genetics (+2 pts).
  - [corpus]: No direct corroboration; related work (FineWeb-Edu in Section 2) supports quality filtering for knowledge-intensive benchmarks, but not biomedical-specific.
- **Break condition:** Over-aggressive filtering (threshold >4) degrades performance on rare-domain benchmarks; emergence of knowledge gaps in specialized subtopics.

### Mechanism 3
- **Claim:** Domain-specific upsampling of clinical content improves clinical reasoning benchmarks with minimal token overhead.
- **Mechanism:** Articles with predominantly clinical domain content or clinical case paragraphs are replicated 10× in the training mix. This increases exposure to clinical narratives—typically scarce due to privacy constraints—without collecting new data.
- **Core assumption:** Clinical narratives in PubMed case reports transfer to professional medical reasoning tasks; upsampling does not cause overfitting or distributional collapse within the training window.
- **Evidence anchors:**
  - [abstract]: "clinical upsampling boosting performance by ~5% on MMLU ProfMed."
  - [Section 5]: "BE-Clinical significantly boosted performance on MMLU Professional Medicine benchmark (63.97%, +4.04 pts vs. BE-Base)."
  - [corpus]: No direct external validation of PubMed clinical cases as a substitute for hospital records; remains an assumption.
- **Break condition:** Clinical upsampling improves ProfMed but degrades general biomedical benchmarks (e.g., College Biology drops as in Table 3: BE-Base 70.83% vs. BE-Clinical 65.28%)—monitor for trade-offs.

## Foundational Learning

- **Concept:** Continual Pre-training
  - **Why needed here:** The experiments adapt OLMo2-7B-stage1 to biomedicine via additional training on curated subsets, rather than training from scratch.
  - **Quick check question:** Can you explain why the authors chose an intermediate checkpoint (stage1) instead of a fully trained model for continual pre-training?

- **Concept:** Data-Centric AI / Dataset Engineering
  - **Why needed here:** The core contribution is dataset curation via LLM-guided annotation, filtering, and upsampling—model architecture is held constant.
  - **Quick check question:** What are the trade-offs between quality filtering and upsampling strategies in terms of token efficiency and benchmark coverage?

- **Concept:** Multi-Task Learning for Annotation
  - **Why needed here:** The distilled classifier jointly predicts document type, domain, and educational quality, sharing representations across tasks.
  - **Quick check question:** How might task interference manifest if one annotation dimension (e.g., domain) has much noisier labels than another?

## Architecture Onboarding

- **Component map:** PMC Open Access Subset → 133M paragraphs (filtered to ≥64 tokens) → 400K annotated by Llama-3.1-70B-Instruct → XLM-RoBERTa-base multi-task classifier → full-corpus metadata → dataset variants (BE-Base, BE-Educational, BE-Clinical, BE-ClinicalCase, BE-French, BE-All) → continual pretraining (OLMo2-7B-stage1) → benchmark evaluation

- **Critical path:** LLM annotation (Stage 1) → classifier fine-tuning → full-corpus inference → variant construction → continual pre-training (OLMo2-7B-stage1, 33.6B tokens each) → benchmark evaluation

- **Design tradeoffs:**
  - Paragraph-level vs. article-level annotation: Higher granularity captures isolated high-value content but increases annotation cost and classifier complexity.
  - Threshold selection (score ≥3): Balances retention (mean 3.48, median 4) against noise removal; higher thresholds risk data sparsity.
  - Upsampling factor (10×): Chosen empirically; not systematically tuned. May cause overfitting if training window is too long.

- **Failure signatures:**
  - Classifier degradation: F1 < 0.7 on validation; MSE > 0.5 on quality regression.
  - Benchmark collapse: BE-Clinical improves ProfMed but drops College Biology by 5.5 pts—specialization trade-off.
  - Token inefficiency: If BE-All does not reach baseline performance with ≤1/3 tokens, filtering/upsampling configuration may be suboptimal.

- **First 3 experiments:**
  1. **Validate classifier quality:** Split LLM-annotated 400K into train/val/test; report F1 per dimension and MSE for quality. If domain/type F1 < 0.75 or quality MSE > 0.3, revisit annotation schema or training data balance.
  2. **Ablate filtering thresholds:** Train with score thresholds 2, 3, 4 on a small continual pre-training run (e.g., 5B tokens); plot MedQA/MedMCQA vs. threshold to validate ≥3 as optimal.
  3. **Pilot upsampling factors:** Compare 5× vs. 10× vs. 20× clinical upsampling on MMLU ProfMed and College Biology; identify point where ProfMed gains saturate and biology degradation becomes unacceptable.

## Open Questions the Paper Calls Out

- Would paragraph-level enrichment strategies show the same or greater benefits when applied to larger language models (70B+ parameters)?
- What is the optimal blend ratio between specialized enrichment (clinical/educational) and general biomedical content to balance targeted performance gains against broad knowledge retention?
- How reliable are LLM-generated annotations compared to human expert judgments, particularly for the subjective educational quality scores?
- Would a biology-focused variant (BE-Bio) recover the performance losses on College Biology observed with other enrichment strategies?

## Limitations

- Classifier reliability at scale remains uncertain since validation was performed only on the 400K-annotated sample, not the full 133M-paragraph distribution
- Specialization trade-offs are evident (clinical upsampling improves ProfMed but degrades College Biology by 5.5 points) but not systematically explored
- Token efficiency claims lack supporting learning curves to verify that BE-All truly achieves 3× efficiency

## Confidence

- **High confidence:** The methodology for two-stage annotation and distillation is clearly specified and the classifier performance metrics are directly reported. The trade-off between clinical specialization and general knowledge is observable in the provided results.
- **Medium confidence:** The experimental results for individual strategies (clinical upsampling, educational filtering) are reproducible, but the combined "BE-All" configuration's token efficiency claim lacks supporting learning curves or ablation studies.
- **Low confidence:** The generalizability of clinical case extraction for real-world clinical reasoning tasks, given that PubMed case reports may not capture the full spectrum of clinical scenarios needed for professional medical practice.

## Next Checks

1. **Classifier robustness validation:** Take the XLM-RoBERTa-base classifier and evaluate it on a held-out random sample of 50K paragraphs from the full corpus (not the 400K annotated sample). Report F1 scores per class, with particular attention to clinical case and non-English content detection. If any class falls below 0.7 F1, this indicates systematic bias in the downstream datasets.

2. **Token efficiency learning curves:** For BE-Base, BE-Educational, BE-Clinical, and BE-All, plot validation performance (MMLU ProfMed, MedQA, MedMCQA) against cumulative training tokens from 0 to 50B. This will reveal whether BE-All truly achieves 3× efficiency or if the claim depends on an arbitrary stopping point.

3. **Full-suite MMLU ablation:** Evaluate all pretraining variants on the complete MMLU benchmark (all 57 subjects), not just the medical subset. This will quantify the breadth of knowledge loss/gain for each filtering/upsampling strategy and help identify optimal trade-offs between specialization and general competence.