---
ver: rpa2
title: Effects of structure on reasoning in instance-level Self-Discover
arxiv_id: '2507.03347'
source_url: https://arxiv.org/abs/2507.03347
tags:
- reasoning
- discover
- problem
- structured
- unstructured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares structured JSON reasoning plans with unstructured
  natural language plans in an instance-level adaptation of the Self-Discover framework.
  The study finds that unstructured natural language plans consistently outperform
  structured JSON plans across diverse benchmarks, with up to 18.90% relative improvement
  on the MATH benchmark.
---

# Effects of structure on reasoning in instance-level Self-Discover

## Quick Facts
- arXiv ID: 2507.03347
- Source URL: https://arxiv.org/abs/2507.03347
- Reference count: 40
- Primary result: Unstructured natural language plans outperform structured JSON plans by up to 18.90% relative improvement on MATH benchmark

## Executive Summary
This paper investigates the trade-offs between structured JSON and unstructured natural language reasoning plans in an instance-level adaptation of the Self-Discover framework. The study finds that unstructured plans consistently outperform structured plans across diverse benchmarks, with up to 18.90% relative improvement on the MATH benchmark. The optimal plan generation granularity (instance vs. task-level) depends on the benchmark and model, while few-shot guidance benefits are context-dependent. The results suggest that free-form text reasoning aligns better with model pre-training and question the reliance on structured formats for complex problem-solving in compound systems.

## Method Summary
The study compares structured JSON reasoning plans against unstructured natural language plans using an instance-level adaptation of Self-Discover (iSelf-Discover). The method employs a 3-stage pipeline: SELECT (choosing reasoning modules from 39 predefined options), ADAPT (rewriting modules for the specific task), and REASON (Plan + Follow). The evaluation uses three benchmarks: BBH (27 tasks from bigbenchhard), MATH (200 stratified examples), and T4D (custom dataset). Experiments compare 0-shot and 5-shot settings, task-level versus instance-level granularity, and structured versus unstructured plan formats across LLaMA-3.1-405B-Instruct and Mistral-Large models.

## Key Results
- Unstructured plans achieved up to 18.90% relative improvement on MATH benchmark compared to structured JSON plans
- Instance-level planning benefits diverse tasks (4.30% gain on BBH) but harms coherent tasks (up to 36.56% drop on T4D)
- Few-shot guidance improved T4D by ~9.44% but provided marginal or negative gains on BBH
- 0-shot unstructured plans sometimes outperformed 5-shot structured plans

## Why This Works (Mechanism)

### Mechanism 1: Pre-training Alignment of Unstructured Reasoning
- **Claim:** Unstructured natural language plans are likely more effective than structured JSON plans because they align more closely with the model's pre-training data distribution (unconstrained Chain of Thought traces).
- **Mechanism:** LLMs are optimized for natural language fluency. Forcing a model to generate reasoning within rigid JSON schemas imposes a cognitive and syntactic load that may interfere with the semantic reasoning process, whereas free-form text allows the model to follow "thought processes" seen during pre-training.
- **Core assumption:** The performance drop in structured outputs is primarily caused by the format constraint rather than the content of the reasoning modules.
- **Evidence anchors:** Abstract shows "Free-form text reasoning aligns better with model pre-training"; Section 5.1 shows unstructured plans achieved up to 18.90% relative improvement on MATH, and 0-shot unstructured outperformed 5-shot structured; Corpus supports computing paradigms that natively support unstructured formats.

### Mechanism 2: Granularity-Context Alignment
- **Claim:** The effectiveness of reasoning plan granularity (instance-level vs. task-level) is contingent on the internal diversity of the benchmark.
- **Mechanism:** Instance-level planning optimizes for specific problem features, benefiting diverse datasets where generic plans fail, but harming coherent datasets where specific instance nuances are noise and a robust task-level pattern is superior.
- **Core assumption:** Instance-level plans provide signal for diverse tasks but noise (or overfitting) for coherent tasks.
- **Evidence anchors:** Section 5.2 shows Mistral had 4.30% gain on diverse BBH tasks with instance-level reasoning, while coherent T4D tasks saw performance drops of up to 36.56% compared to task-level baselines; Corpus supports instance-level context in reducing redundant exploration.

### Mechanism 3: Contextual Noise in Few-Shot Guidance
- **Claim:** Providing few-shot examples during instance-level planning helps only when the examples share high structural similarity with the target instance; otherwise, they act as distractors.
- **Mechanism:** In diverse benchmarks, generic few-shot examples introduce conflicting context. In coherent benchmarks, they reinforce the underlying reasoning schema, aiding the adaptation step.
- **Core assumption:** The model attends to the few-shot examples during the SELECT and ADAPT phases, potentially overriding the specific merits of the current instance if the examples are dissimilar.
- **Evidence anchors:** Section 5.3 shows few-shot guidance improved T4D (high consistency) by ~9.44% but provided marginal or negative gains on BBH (high diversity); Section 3 explicitly includes $E^{(k)}_{fs}$ in the SELECT and ADAPT prompts.

## Foundational Learning

- **Concept: Self-Discover Framework (Select, Adapt, Reason)**
  - **Why needed here:** iSelf-Discover modifies the original Self-Discover pipeline. You must understand that the model first *selects* reasoning modules (e.g., "Critical Thinking"), *adapts* them to the task, and then *executes* them.
  - **Quick check question:** Can you distinguish between the "PLANNING" sub-step (generating the plan) and the "FOLLOWING" sub-step (executing it) in the iSelf-Discover architecture?

- **Concept: Constrained Decoding / Structured Outputs**
  - **Why needed here:** The paper pivots around the trade-off between JSON (structured) and Text (unstructured). Understanding *why* engineers typically want JSON (reliability, parsing) vs. what this paper finds (performance loss) is central.
  - **Quick check question:** Why might enforcing a JSON schema with specific keys (e.g., "step_1") hurt a model's ability to perform complex arithmetic reasoning?

- **Concept: Task vs. Instance Granularity**
  - **Why needed here:** The paper defines "Task-level" as one plan for a whole dataset (e.g., MATH) and "Instance-level" as a unique plan for every single question.
  - **Quick check question:** If you have a dataset of 100 diverse logic puzzles, would the paper suggest a Task-level or Instance-level approach? What if you have 100 emails all asking for meeting summaries?

## Architecture Onboarding

- **Component map:** Input (Task Instance + Optional Unlabeled Examples) -> SELECT (choose reasoning modules) -> ADAPT (rewrite modules to task) -> REASON (Plan + Follow)
- **Critical path:** The **Planning prompt** in Stage 3 is the bottleneck. The prompt must explicitly instruct the model to generate *either* a valid JSON object or a natural language plan *without* solving the problem immediately.
- **Design tradeoffs:**
  - **JSON vs. Text:** JSON allows programmatic parsing of reasoning steps (useful for compound systems/tools) but incurs a ~10-19% accuracy tax on complex reasoning. Text maximizes accuracy but is harder to parse programmatically.
  - **Task vs. Instance:** Instance-level requires 2x more inference calls (Plan + Solve) per query. Task-level amortizes the Plan cost but loses precision on diverse inputs.
- **Failure signatures:**
  - **JSON Hallucination:** In Structured mode, the model may generate a valid JSON plan but fail to populate the values correctly during the "Following" phase, or produce a plan that is logically incoherent but syntactically valid.
  - **Overfitting on T4D:** If using Instance-level reasoning on simple, coherent tasks, the model may "overthink" or hallucinate complexity, degrading performance compared to a simple Task-level prompt.
- **First 3 experiments:**
  1. **Format A/B Test:** Run iSelf-Discover on a held-out validation set. Compare "Unstructured 0-shot" vs. "Structured 0-shot". Quantify the accuracy gap vs. the ease of parsing the output.
  2. **Granularity Check:** Run "Task-level" discovery (generate one plan for the whole set) vs. "Instance-level" on your specific data. If your data is homogeneous (like T4D), expect Task-level to win or tie; if heterogeneous (like BBH), expect Instance-level to win.
  3. **Robustness Check:** Specifically evaluate the Structured variant to see how often the generated JSON is malformed or missing required keys versus how often the Unstructured variant produces a final answer format that cannot be regex-parsed.

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific underlying mechanisms cause unstructured natural language to outperform structured JSON in reasoning tasks? The paper explicitly calls for "analyses of model activations [and] studies on the cognitive load" to understand the performance gap, but doesn't conduct mechanistic analysis.
- **Open Question 2:** Can a system be designed to dynamically select the optimal reasoning style and granularity based on input characteristics? The authors suggest developing "adaptive systems that can dynamically select the optimal plan granularity... based on the characteristics of the input query."
- **Open Question 3:** Do these findings regarding the inferiority of structured reasoning generalize to state-of-the-art proprietary models? The Limitations section notes experiments used open-source models and that "further benchmarking on state-of-the-art proprietary models is necessary."

## Limitations
- The study relies on proprietary large models (LLaMA-3.1-405B-Instruct, Mistral-Large) that are not accessible to most researchers
- The evaluation is limited to three specific benchmarks, potentially constraining generalizability
- The comparison assumes standard JSON schemas without exploring alternative structured formats that might better align with model capabilities
- The paper does not investigate hybrid approaches that could combine the accuracy of unstructured text with the parseability of structured formats

## Confidence

| Claim | Confidence |
|-------|------------|
| Unstructured plans outperform structured plans (18.90% relative improvement on MATH) | High |
| Granularity findings (instance vs. task-level) are context-dependent with mixed results | Medium |
| The assertion that structured formats should be abandoned for complex reasoning | Low |

## Next Checks
1. **Prompt fidelity test:** Recreate the exact SELECT-ADAPT-PLAN-FOLLOW pipeline using the visual templates from Appendix B. Compare outputs against published results to identify discrepancies caused by prompt engineering variations.
2. **Hybrid format evaluation:** Design an intermediate format that maintains some structural constraints while allowing natural language flexibility. Test whether this can capture the accuracy benefits of unstructured text while preserving parseability for compound systems.
3. **Generalization across domains:** Apply the iSelf-Discover framework to additional reasoning domains (scientific reasoning, code generation, multi-step decision making) to assess whether the unstructured superiority holds beyond mathematical problem-solving.