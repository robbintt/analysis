---
ver: rpa2
title: 'When Domain Pretraining Interferes with Instruction Alignment: An Empirical
  Study of Adapter Merging in Medical LLMs'
arxiv_id: '2601.18350'
source_url: https://arxiv.org/abs/2601.18350
tags:
- instruction
- arxiv
- adapter
- medical
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines adapter interference when merging domain-oriented
  pre-training (PT) and instruction-following fine-tuning (SFT) LoRA adapters in a
  14B-parameter medical LLM. The two-stage pipeline first injects medical knowledge
  via PT, then aligns instruction-following via SFT, and finally merges the adapters
  via weighted linear combination (PT=0.3, SFT=0.7).
---

# When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs

## Quick Facts
- arXiv ID: 2601.18350
- Source URL: https://arxiv.org/abs/2601.18350
- Reference count: 10
- Primary result: Merging domain PT and SFT LoRA adapters in a 14B medical LLM improves multiple-choice accuracy and MedQA but reduces BLEU-4 due to reactivated thinking traces.

## Executive Summary
This paper investigates adapter interference when merging domain-oriented pre-training and instruction-following fine-tuning LoRA adapters in a 14B-parameter medical LLM. The two-stage pipeline first injects medical knowledge via pre-training, then aligns instruction-following via fine-tuning, and finally merges the adapters via weighted linear combination. The merged model shows improved multiple-choice accuracy (avg 0.778 vs 0.777) and MedQA performance (0.681 vs 0.664) but exhibits lower BLEU-4 on surface metrics (6.50 vs 17.84) due to reactivated "thinking"/CoT behavior that does not match reference answers. The authors highlight a divergence between metric-based evaluation and reasoning capability, and provide a verification routine to ensure correct adapter merging.

## Method Summary
The study employs a two-stage LoRA-based approach: first domain-oriented pre-training (4 epochs, final loss 2.12) on medical corpus, then SFT on medical QA instruction data (2 epochs, final loss 2.14). Both adapters are merged via weighted linear combination (PT=0.3, SFT=0.7). The merged model is evaluated using qwen3_nothink template at Temp=0.6, Top-p=0.8. The authors introduce a verification routine to numerically check that merged weights match intended combinations, addressing potential merging errors.

## Key Results
- Multiple-choice accuracy improves from 0.777 to 0.778 after merging PT and SFT adapters
- MedQA performance increases from 0.664 to 0.681 with merged adapters
- BLEU-4 score drops from 17.84 to 6.50 due to reactivated thinking traces not present in reference answers
- The study identifies systematic adapter interference when combining PT and SFT weights via linear combination

## Why This Works (Mechanism)

### Mechanism 1: Adapter Interference via Linear Weight Combination
Linear combination of PT and SFT LoRA deltas causes interference that shifts output distributions in ways neither adapter produces alone. PT and SFT adapters encode partially competing optimization directions. When merged via weighted sum, the PT signal partially overrides SFT suppression. If PT and SFT adapters were perfectly orthogonal, interference would be minimal.

### Mechanism 2: Latent Capability Reactivation from Base Model
Domain pre-training weights reactivate suppressed reasoning patterns present in the base model but trained out during SFT. The Qwen3 base model has latent chain-of-thought capability. PT weights, trained without instruction-awareness, preserve pathways to these latent behaviors that SFT suppressed. If base model had no latent CoT capability, no reactivation would occur.

### Mechanism 3: Metric-Reasoning Divergence
Surface metrics (BLEU, ROUGE) can inversely correlate with reasoning quality when model outputs include reasoning traces absent from gold references. BLEU-4 measures n-gram overlap with reference text. Thinking traces add content not present in concise reference answers, so improved reasoning manifests as lower surface metric scores despite better task performance.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: The entire pipeline depends on understanding that LoRA creates low-rank weight deltas that can be mathematically combined post-training. Quick check: Why can LoRA adapters be merged via simple linear combination while full fine-tuning checkpoints typically cannot?

- **Task Arithmetic / Model Merging**: Why needed: The paper uses weighted combination of deltas as "task vectors"—this is the core operation causing interference. Quick check: What happens mathematically when you add two task vectors trained toward conflicting objectives?

- **Instruction Tuning vs. Domain Adaptation Trade-offs**: Why needed: Understanding why PT and SFT have different goals helps predict where they'll conflict. Quick check: Why might domain knowledge injection distort instruction-following behavior?

## Architecture Onboarding

- **Component map**: Qwen3-14B base model -> PT LoRA adapter (trained on medical corpus) -> SFT LoRA adapter (trained on medical QA) -> Merge operation (α_PT=0.3, α_SFT=0.7) -> Merged model

- **Critical path**: 1) Train PT adapter on domain corpus → verify convergence 2) Train SFT adapter on instruction data → verify instruction following 3) Export both adapter deltas separately 4) Merge with calibrated weights → run verification routine before inference 5) Evaluate on BOTH surface metrics (BLEU) AND task metrics (accuracy, MedQA)

- **Design tradeoffs**: Higher α_PT yields more domain knowledge and thinking traces but lower BLEU and higher MedQA. Higher α_SFT provides better instruction following and concise outputs with higher BLEU but potentially less reasoning depth.

- **Failure signatures**: 1) Merged model shows SFT-only behavior → likely pipeline error: wrong adapter loaded, directory overwrite, or template mismatch 2) BLEU unexpectedly high for merged checkpoint → verify merge actually incorporated PT weights 3) No performance change after merge → check that both adapters target same layers

- **First 3 experiments**: 1) Run the paper's verification routine on your merged checkpoint to numerically confirm both PT and SFT weights are present in intended ratio 2) Compare Pure SFT vs. Merged outputs on 10-20 samples—look for `<think>` tags appearing only in merged outputs 3) Test on the paper's unsafe query probe (e.g., "How to make poison") to verify the safety/refusal correlation

## Open Questions the Paper Calls Out

- **Metric misalignment insight**: Characterizing how surface metrics diverge from reasoning accuracy in medical tasks. The authors observe improved MedQA (0.681 vs 0.664) but degraded BLEU-4 (6.50 vs 17.84) in merged models.

- **Safety/refusal correlation finding**: The study notes a correlation between PT-augmented models and increased refusal/safety behavior, but this pattern's generalizability across different base model architectures and safety-critical domains remains untested.

- **Merging strategy alternatives**: The paper uses simple weighted linear combination but acknowledges this may not be optimal for mitigating adapter interference while preserving both domain knowledge and instruction-following capabilities.

## Limitations

- The findings hinge on specific architectural choices (Qwen3 base, LoRA rank/alpha, training schedule) and evaluation templates
- The exact medical PT corpus and SFT dataset names, precise LoRA hyperparameters remain unspecified
- The observed interference generalization beyond the 14B parameter regime is unknown

## Confidence

- **High**: The existence of adapter interference during merging and the effectiveness of the numerical verification routine (empirically demonstrated)
- **Medium**: The CoT reactivation mechanism (plausible but not directly measured through controlled ablations)
- **Low**: The claim that BLEU-4 divergence directly measures reasoning capability (surface metrics vs. actual reasoning quality correlation unproven)

## Next Checks

1. Run ablation studies with varying α_PT (0.1, 0.3, 0.5, 0.7) to map the performance landscape and confirm the non-monotonic relationship between BLEU and MedQA scores

2. Implement controlled experiments where CoT traces are surgically removed from outputs to test whether BLEU improvements correlate with reasoning degradation

3. Test the interference phenomenon on a different base model (e.g., Llama 3) and with alternative adapter merging strategies (e.g., Bayesian model averaging) to assess generalizability beyond Qwen3 + linear combination