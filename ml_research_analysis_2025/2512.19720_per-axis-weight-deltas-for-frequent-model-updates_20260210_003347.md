---
ver: rpa2
title: Per-Axis Weight Deltas for Frequent Model Updates
arxiv_id: '2512.19720'
source_url: https://arxiv.org/abs/2512.19720
tags:
- https
- fine-tuned
- arxiv
- base
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple 1-bit delta quantization method for
  efficient storage and serving of fine-tuned LLM variants. The method represents
  weight differences as a binary sign mask combined with per-axis (row/column) FP16
  scaling factors, learned from a small calibration set.
---

# Per-Axis Weight Deltas for Frequent Model Updates

## Quick Facts
- arXiv ID: 2512.19720
- Source URL: https://arxiv.org/abs/2512.19720
- Authors: Stefan Kuyumdzhiev; Radostin Cholakov
- Reference count: 40
- One-line primary result: 1-bit delta quantization with per-axis scaling achieves ~5× storage reduction while maintaining accuracy across five zero-shot benchmarks

## Executive Summary
This paper proposes a simple 1-bit delta quantization method for efficient storage and serving of fine-tuned LLM variants. The method represents weight differences as a binary sign mask combined with per-axis (row/column) FP16 scaling factors, learned from a small calibration set. By capturing axis-specific variations in model weights, the approach improves reconstruction quality over scalar alternatives while maintaining the compactness of 1-bit deltas. Experiments with Llama-3.1-8B show consistent accuracy improvements across five zero-shot benchmarks compared to both uncompressed models and scalar 1-bit delta methods. The delta representation achieves approximately 5× storage reduction compared to full FP16 checkpoints and enables faster cold-start loading, making it practical for serving many fine-tuned variants from a shared base model.

## Method Summary
The method compresses fine-tuned LLM variants by representing weight differences (ΔW = Wf − Wb) as 1-bit sign masks with per-axis FP16 scaling factors. For each linear projection layer, it computes a binary sign mask B = sign(ΔW) and learns per-row or per-column scaling vectors v that minimize layer-output reconstruction error on calibration data. The compressed representation stores only the packed sign mask (1 bit per entry) plus the FP16 scaling vectors. During serving, the compressed deltas are applied to a shared base model to reconstruct fine-tuned variants. The method uses 50 calibration samples per layer for learning scaling vectors, followed by end-to-end joint training on 150 samples to align all layers. Axis selection (row vs column) is determined empirically per layer by comparing reconstruction quality.

## Key Results
- Consistent accuracy improvements across five zero-shot benchmarks (ARC-Challenge, ARC-Easy, HellaSwag, PIQA, Winogrande) compared to scalar 1-bit delta methods
- ~5× storage reduction compared to full FP16 checkpoints for Llama-3.1-8B
- Faster cold-start loading by loading base model once and applying deltas
- Row scaling preferred for q/v/o_proj and down_proj layers; column scaling preferred for gate/up_proj layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-row or per-column scaling vectors improve reconstruction quality over scalar scaling by capturing anisotropic structure in fine-tuning deltas.
- Mechanism: Weight differences ΔW = Wf − Wb exhibit non-uniform magnitude across matrix dimensions. A single scalar assumes isotropy; per-axis scales allow each row/column its own magnitude, matching actual delta distribution. The method computes B = sign(ΔW) and learns v via activation-matching, with reconstruction: cW = v ⊙ B + Wb.
- Core assumption: Fine-tuning deltas have structured anisotropy where magnitude varies systematically across rows or columns.
- Evidence anchors:
  - [abstract] "By capturing axis-specific variations in model weights, the approach improves reconstruction quality over scalar alternatives"
  - [section 1] "they rely on coarse parametrizations that ignore variation in residual scales across rows or columns of weight matrices, leading to reconstruction errors"
  - [corpus] Limited direct corpus support for fine-tuning delta anisotropy; ROSAQ paper supports structure-aware quantization generally
- Break condition: If ΔW is nearly isotropic (uniform magnitude), per-axis scaling offers no advantage over scalar (section 4 explicitly notes this limitation).

### Mechanism 2
- Claim: Minimizing layer-output error rather than weight-space error preserves model function better at extreme compression.
- Mechanism: Instead of recovering exact weight values, the method learns scaling vectors that minimize MSE between compressed layer output Ŷ and fine-tuned layer output Y on calibration data: L_layer = (1/n)||Y − Ŷ||²₂. This accounts for error propagation through the network.
- Core assumption: Weight-space reconstruction is a poor proxy for functional preservation; activation-matching captures what matters downstream.
- Evidence anchors:
  - [section 2.1] "The objective is not to recover the exact parameter values, but to preserve the function the network computes"
  - [section 2.1] Cites Nagel et al. (2020), Frantar et al. (2023), Li et al. (2021) showing round-to-nearest underperforms loss-aware approaches
  - [corpus] Q-Palette and ROSAQ papers similarly use saliency/output-aware quantization
- Break condition: If calibration distribution diverges from deployment distribution, learned scales transfer poorly.

### Mechanism 3
- Claim: Layer-stacked calibration prevents error accumulation by training each layer on inputs from already-compressed predecessors.
- Mechanism: When compressing layer i, input X is the output of layers 0 to i−1 after compression—not original base outputs. Scaling vectors are learned in their actual inference context. After per-layer selection, end-to-end joint training (150 samples, all vectors together) aligns the full stack.
- Core assumption: Layers are coupled; compressing in isolation compounds errors across depth.
- Evidence anchors:
  - [section 2.1] "X is the input that has to be passed to the i layer of the compressed model (i.e., the output of the already-compressed stack up to layer i−1)"
  - [section 2.1] "use an additional set of 150 C4 examples to jointly train all selected vectors on end-to-end objective"
  - [corpus] Corpus evidence weak for this specific caching strategy
- Break condition: If layers operate nearly independently, per-layer calibration without stacking suffices.

## Foundational Learning

- Concept: **Post-Training Quantization (PTQ)**
  - Why needed here: This method applies PTQ to weight deltas, not training-time quantization. Understanding why PTQ needs calibration data and output-matching objectives is essential.
  - Quick check question: Why does PTQ need calibration data if no gradient-based training occurs?

- Concept: **Delta Compression vs. Weight Compression**
  - Why needed here: The method compresses ΔW = Wf − Wb, not weights themselves. Requires maintaining base model and applying deltas on top.
  - Quick check question: What are the latency implications of loading a base model once + applying 5× smaller deltas versus loading full FP16 checkpoints?

- Concept: **Activation-Aware vs. Weight-Aware Quantization**
  - Why needed here: The paper explicitly rejects weight-space reconstruction (section 2.1). This distinction explains why 1-bit deltas work despite aggressive compression.
  - Quick check question: Why might minimizing ||W_quant − W_orig||_F perform worse than minimizing ||f(X_quant) − f(X_orig)||_2 for downstream tasks?

## Architecture Onboarding

- Component map:
  - ΔW computation -> B = sign(ΔW) binary mask -> v scaling vector training -> cW = v ⊙ B + Wb compressed output
  - Base model Wb + compressed deltas -> reconstructed fine-tuned variant
  - Calibration cache (X, Y) pairs -> per-layer vector training -> end-to-end joint optimization

- Critical path:
  1. Load Wb (base) and Wf (fine-tuned); compute ΔW and packed sign mask B for each linear layer in attention and MLP blocks
  2. Build calibration cache: run 50 C4 samples through both models, store (X, Y) pairs via forward hooks (student input X, teacher output Y as BF16). Assumption: Split ~40 train / ~10 validation.
  3. For each layer, instantiate row and column variants with learnable v (initialize v = mean|ΔW| along respective axis). Train each with AdamW, lr=1e-4, 5 epochs on L = ||Y − Ŷ||². Select lower-MSE variant.
  4. Run end-to-end training on all selected v vectors using 150 C4 samples with MSE on final logits. Save masks and vectors.

- Design tradeoffs:
  - **Row vs. Column**: Row uses d_out factors; col uses d_in. Paper selects per-layer empirically. Figure 2 shows q/v/o_proj and down_proj prefer row; gate/up_proj prefer column.
  - **Calibration budget**: 50 samples per-layer, 150 end-to-end. More improves robustness but increases prep time (section 4).
  - **Epochs**: 5 for vector vs. 1 for scalar—more parameters need more optimization (section 3.1).

- Failure signatures:
  - **Isotropic deltas**: Near-uniform ΔW magnitude → scalar equivalent (section 4)
  - **Distribution shift**: C4 calibration ≠ deployment domain → degraded transfer
  - **Small-magnitude entries**: Fixed 1-bit mask cannot represent zeros, propagates noise (section 4)
  - **Bias/embedding changes**: Method only patches linear projections; ignores other components (section 4)

- First 3 experiments:
  1. Reproduce Llama-3.1-8B → Instruct compression (50/150 samples), verify ~70.23 avg accuracy matches Table 1.
  2. Ablation on calibration size: 10, 25, 50, 100, 200 samples; plot accuracy vs. budget.
  3. Axis preference analysis: Count row vs. col selections per layer type; compare to Figure 2 patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- Calibration domain dependence: Heavy reliance on C4-based calibration data with no validation for domains divergent from C4
- Component coverage gap: Only compresses linear projection layers, excluding biases, embeddings, and layer norms
- Anisotropy requirement: Per-axis scaling only outperforms scalar methods when weight deltas exhibit structured anisotropy

## Confidence
**High confidence**: The core mechanism of activation-aware quantization for delta compression is well-supported with reproducible empirical results across five benchmarks.

**Medium confidence**: The claimed ~5× storage reduction and cold-start latency improvements require careful interpretation depending on base model loading architecture and component change patterns.

**Low confidence**: Claims about axis selection patterns lack statistical validation and don't demonstrate generalization across different model architectures.

## Next Checks
1. **Domain transfer robustness**: Evaluate compressed delta performance when calibration data (C4) differs substantially from deployment data (e.g., code, medical, or multilingual domains). Measure accuracy degradation as a function of domain distance.

2. **Component change sensitivity**: Systematically vary which components (biases, embeddings, layer norms) differ between base and fine-tuned models, measuring compressed accuracy as the fraction of changed components increases from 0% to 100%.

3. **Anisotropy prevalence analysis**: Characterize the distribution of weight delta anisotropy across diverse fine-tuning scenarios (LoRA rank 8 vs 64, full fine-tuning, different task types). Quantify what percentage of fine-tuning runs actually benefit from per-axis over scalar scaling.