---
ver: rpa2
title: Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization
  Circuits in Transformers
arxiv_id: '2601.09049'
source_url: https://arxiv.org/abs/2601.09049
tags:
- facts
- grokking
- training
- reasoning
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether grokking (prolonged training beyond
  training-set saturation) yields superior generalization for compositional reasoning
  in transformers. The authors use parameter-sharing transformers trained on synthetic
  two-hop reasoning tasks, where models must compose atomic facts to answer unseen
  compositional queries.
---

# Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers

## Quick Facts
- arXiv ID: 2601.09049
- Source URL: https://arxiv.org/abs/2601.09049
- Authors: Kaiyu He; Zhang Mian; Peilin Wu; Xinya Du; Zhiyu Chen
- Reference count: 33
- Primary result: Grokking does not create new reasoning circuits but integrates memorized facts into pre-established circuits, with limited transferability of second-hop components

## Executive Summary
This paper investigates whether grokking yields superior generalization for compositional reasoning in transformers by analyzing parameter-sharing transformers trained on synthetic two-hop reasoning tasks. The authors demonstrate that grokking and Generalization Circuit formation are dissociable phenomena: models can achieve behavioral grokking without forming correct mechanistic circuits, and mature circuits exhibit limited transferability to novel facts. Critically, the research shows that with sufficient compositional data, models can achieve equivalent performance without prolonged grokking, challenging the view that grokking represents genuine acquisition of generalized reasoning.

## Method Summary
The authors use parameter-sharing transformers trained on synthetic two-hop reasoning tasks where models must compose atomic facts to answer unseen compositional queries. They employ logit lens analysis to trace hidden states across layers, identifying whether models explicitly represent intermediate bridge entities during inference. The experimental design partitions atomic facts into in-distribution (ID) and out-of-distribution (OOD) compositional splits, with varying augmentation strategies (first-hop only, second-hop only, or full augmentation) to control circuit formation conditions. Grokking is measured as the prolonged phase beyond training-set saturation where OOD accuracy improves.

## Key Results
- Grokking integrates memorized atomic facts into pre-established reasoning paths rather than creating new circuits
- Behavioral grokking can occur without correct mechanistic circuit formation ("fake grokking")
- Generalization Circuits exhibit limited transferability, particularly for second-hop components with novel facts
- Full compositional augmentation prevents grokking but yields equivalent circuits faster than sparse augmentation

## Why This Works (Mechanism)

### Mechanism 1: Pre-established Circuit Integration
Grokking does not create new reasoning circuits; it integrates sparsely-supervised facts into circuits that form early in training. The "Generalization Circuit" emerges naturally during early training on in-distribution compositional queries, with the bridge entity recovered in intermediate layers and passed to deeper layers for the second hop. This works because parameter-sharing enables the latent representation of the bridge entity from early layers to serve as valid input to deeper layers.

### Mechanism 2: Data Regime Controls Circuit vs. Memorization
Whether a model forms a true Generalization Circuit or achieves "fake grokking" depends on compositional supervision coverage of OOD facts. First-hop exposure is critical for learning to route bridge entity representations correctly through intermediate layers. When only second-hop augmentation is provided, the model bypasses the circuit and memorizes compositional patterns directly, achieving high accuracy without correct bridge recovery.

### Mechanism 3: Limited Circuit Transferability
A mature Generalization Circuit accelerates assimilation of new facts when they appear in the first hop, but not when they appear in the second hop. The circuit's first-hop and second-hop components have different generalization properties; the second-hop component—the parameters that consume bridge representations to retrieve final answers—does not generalize to new facts without additional grokking or compositional supervision.

## Foundational Learning

- **Concept: Representational Mismatch**
  - Why needed here: Standard transformers resolve atomic facts in shallow layers, producing latent representations that deeper layers were never trained to accept as inputs. This failure mode clarifies why parameter-sharing is necessary.
  - Quick check question: Can you explain why a standard transformer that knows "A→B" and "B→C" cannot answer "A→?" by composing these facts internally?

- **Concept: Logit Lens and Bridge Entity Recovery**
  - Why needed here: The paper's mechanistic analysis relies on decoding hidden states across layers to identify whether the model explicitly represents intermediate entities. This diagnostic tool distinguishes true circuits from memorization.
  - Quick check question: How would you use the logit lens to determine if a model is using a Generalization Circuit versus direct memorization for a compositional query?

- **Concept: In-Distribution vs. Out-of-Distribution Compositional Split**
  - Why needed here: The experimental design partitions atomic facts into ID (used in training compositions) and OOD (reserved for testing). Success on OOD compositions indicates systematic reasoning rather than memorized compositions.
  - Quick check question: If a model sees all atomic facts during training but only ID compositional queries, what must it learn to answer OOD compositional queries correctly?

## Architecture Onboarding

- **Component map:** Input → First-hop retrieval (shallow) → Bridge entity encoding (middle) → Second-hop retrieval (deep) → Answer
- **Critical path:** Parameter-sharing enables consistent representations across depth, with early layers resolving first-hop atomic facts, middle layers recovering bridge entities, and deep layers executing second-hop retrieval
- **Design tradeoffs:** High ϕ ratio (inferred/atomic = 18.0) accelerates grokking but requires more compositional supervision; full augmentation prevents grokking but yields equivalent circuits faster; sparse OOD compositional supervision necessitates prolonged grokking
- **Failure signatures:** Fake grokking (high OOD accuracy without bridge recovery), second-hop transfer failure (bridge correctly recovered but final answer wrong on new-fact compositions), catastrophic forgetting (if finetuning omits retained ID facts)
- **First 3 experiments:**
  1. Verify circuit formation: Train with full augmentation, apply logit lens at each layer on OOD compositions—confirm bridge entity appears in middle layers before final answer
  2. Induce fake grokking: Train with second-hop-only augmentation, compare logit lens traces to natural grokking—confirm behavioral success without mechanistic bridge recovery
  3. Test transfer limits: Take a grokked checkpoint, finetune on new atomic facts in first-hop-only vs. second-hop-only compositions—measure accuracy gap to isolate which circuit component fails to transfer

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise boundary conditions and universal principles that determine whether a specific data distribution triggers "fake grokking" versus true Generalization Circuit formation? The authors state in the Limitations that "the precise boundary conditions and the universal principles governing this transition remain an open question" due to computational infeasibility of exhaustive searching.

### Open Question 2
Do the phenomena of circuit formation and limited transferability observed in parameter-sharing Transformers extend to standard Transformer architectures without shared weights? Section 7 explicitly notes the findings are "specifically constrained to parameter-sharing Transformer architectures" and may not extend to traditional architectures which struggle with representational mismatch.

### Open Question 3
Why is the second-hop component of the Generalization Circuit significantly less transferable to novel facts compared to the first-hop component? Section 5.2 observes that while models can instantly leverage pretraining for the first hop, "when a novel fact is required in the second hop, models fail... indicating that the second component of the Generalization Circuit is less transferable."

## Limitations
- Findings may not generalize to standard transformers or real-world reasoning tasks due to synthetic data and parameter-sharing architecture constraints
- The dissociation between behavioral grokking and mechanistic circuit formation relies on controlled synthetic setup
- Limited transferability of second-hop components could be specific to synthetic task design rather than fundamental limitation

## Confidence
- **High confidence:** The dissociation between behavioral grokking and mechanistic circuit formation is well-supported by experimental evidence
- **Medium confidence:** The claim that parameter-sharing enables circuit formation is plausible but not conclusively proven
- **Medium confidence:** The limited transferability of circuits to new facts in second-hop positions is demonstrated but could be specific to synthetic task structure

## Next Checks
1. **Cross-architecture replication:** Test whether circuit formation and grokking dissociation patterns hold in standard (non-parameter-sharing) transformers, even if circuit formation fails
2. **Natural language reasoning tasks:** Apply same mechanistic analysis (logit lens, bridge entity recovery) to transformers trained on real multi-hop reasoning datasets like HotpotQA or WikiHop
3. **Second-hop component isolation:** Design experiment targeting second-hop circuit component by training models on bridge-to-answer mappings only, then testing transfer to new bridges