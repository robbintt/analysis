---
ver: rpa2
title: Adapting Biomedical Abstracts into Plain language using Large Language Models
arxiv_id: '2501.15700'
source_url: https://arxiv.org/abs/2501.15700
tags:
- language
- evaluation
- available
- plain
- adaptations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to adapt biomedical abstracts into
  plain language using Large Language Models (LLMs). The work addresses the problem
  of low health literacy by automatically simplifying complex biomedical text for
  general public consumption.
---

# Adapting Biomedical Abstracts into Plain language using Large Language Models

## Quick Facts
- arXiv ID: 2501.15700
- Source URL: https://arxiv.org/abs/2501.15700
- Authors: Haritha Gangavarapu; Giridhar Kaushik Ramachandran; Kevin Lybarger; Meliha Yetisgen; Özlem Uzuner
- Reference count: 40
- Primary result: GPT-4-based model ranked first in average simplicity and third in average accuracy for biomedical abstract simplification

## Executive Summary
This paper addresses the challenge of low health literacy by adapting complex biomedical abstracts into plain language using Large Language Models (LLMs). The authors fine-tuned and applied in-context learning with various LLMs including T5, LLaMa2, GPT-3.5, and GPT-4 to transform technical biomedical language into accessible plain language while preserving meaning and readability. Their approach demonstrates that LLMs can effectively bridge the gap between specialized medical knowledge and general public understanding.

## Method Summary
The study fine-tuned various LLMs including T5-large, LLaMa2-13B/70B-chat (using LoRA with rank=16 in 4-bit precision), and GPT-3.5 via API fine-tuning. GPT-4 was used exclusively for in-context learning with distilled annotation guidelines and one-shot examples. The PLABA dataset containing 750 abstracts and 921 manual adaptations was processed into 9,216 sentence-level pairs. Models were evaluated using automatic metrics (BLEU, ROUGE, SARI) and human evaluation on simplicity and accuracy axes.

## Key Results
- GPT-4-based ICL model achieved first place in external manual evaluation for average simplicity
- Fine-tuned T5 and LLaMa2 models showed higher automatic metric scores but performed poorly in human evaluation
- Automatic metrics poorly correlate with human judgment for simplification tasks, often rewarding lexical overlap over genuine adaptation
- In-context learning with detailed annotation guidelines outperformed fine-tuning approaches when using capable models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with detailed annotation guidelines outperforms fine-tuning for plain language adaptation when using the most capable models.
- Mechanism: Providing a distilled version of expert annotation guidelines in the prompt enables the model to internalize domain-specific simplification rules—such as expanding abbreviations, explaining medical jargon in parentheses, and omitting statistical figures—without weight updates.
- Core assumption: The model's pre-trained knowledge includes sufficient biomedical understanding to apply the guidelines correctly with minimal examples.
- Evidence anchors:
  - "Our top performing GPT-4 based model ranked first in the avg. simplicity measure and 3rd on the avg. accuracy measure."
  - "Our final GPT-4-based model included a detailed distilled version of the PLABA annotation guideline with one training example that illustrates a majority of annotation guideline instructions."
- Break condition: If target domain requires terminology not well-represented in pre-training, or if guidelines become too long for context windows, performance may degrade sharply.

### Mechanism 2
- Claim: Automatic metrics (BLEU, ROUGE) correlate poorly with human judgment for simplification tasks because they reward lexical overlap over genuine adaptation.
- Mechanism: Models that copy source sentences with minimal changes achieve high n-gram overlap scores, while models that genuinely simplify (explaining "macular degeneration" as "damage to the central part of the retina") are penalized for deviating from reference wording despite achieving the task goal.
- Core assumption: Human evaluators prioritize understandability and faithfulness over lexical similarity to source.
- Evidence anchors:
  - "Our top performing GPT-4 based model ranked first in the avg. simplicity measure"
  - "While the fine-tuned models T5 and LLaMa show higher performance than the GPT models in automatic evaluation... T5 and LLaMa2 tend to frequently repeat the abstract sentence as is with minimal simplifications, leading to higher ROUGE and BLEU scores."
- Break condition: If evaluation must be fully automated at scale without human annotation, relying on BLEU/ROUGE will systematically select models that fail to simplify.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) on instruction-tuned models provides a practical middle ground but may not match guideline-informed ICL for complex transformation tasks.
- Mechanism: LoRA enables training only 0.05% of parameters in 4-bit precision, making large models trainable in low-resource settings, but the supervised fine-tuning signal may not fully capture the nuanced simplification rules that explicit guidelines convey.
- Core assumption: The fine-tuning dataset quality and instruction format sufficiently represent the target task distribution.
- Evidence anchors:
  - "We compare different fine-tuning approaches including Parameter Efficient Fine-Tuning (PEFT) and In-Context Learning (ICL)"
  - "We fine-tuned Llama2 chat models in 4-bit precision using the Low-Rank Adaptation (LoRA)... LoRA rank set to 16 that allowed training only 0.05% of the entire trainable parameters."
- Break condition: If computational resources permit full fine-tuning with larger datasets, PEFT's efficiency advantage diminishes while potential quality ceiling remains.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The best-performing system used ICL rather than fine-tuning; understanding how examples and instructions in prompts steer model behavior is essential for reproducing results.
  - Quick check question: Can you explain why providing annotation guidelines in the prompt differs from providing task instructions?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA/PEFT)**
  - Why needed here: LLaMa2 experiments used LoRA to train 70B models efficiently; practical deployment requires understanding this technique.
  - Quick check question: What does "training only 0.05% of parameters" mean in terms of what gets updated during backpropagation?

- Concept: **Evaluation Metrics for Text Generation (BLEU, ROUGE, SARI)**
  - Why needed here: The paper demonstrates these metrics can be misleading for simplification; knowing what each measures helps interpret results critically.
  - Quick check question: Why would SARI be more appropriate than BLEU for simplification tasks?

## Architecture Onboarding

- Component map:
  PLABA dataset (750 abstracts → 921 adaptations) → sentence-level pairs (9,216 pairs) → train/val/test split (70/15/15) → Model families (T5-large, LLaMa2-13B/70B-chat, GPT-3.5, GPT-4) → Prompt engineering (short instructions vs. detailed guidelines vs. guidelines + one-shot example) → Evaluation (Automatic + Human)

- Critical path:
  1. Prepare sentence-level abstract-adaptation pairs with context (consumer question from MedlinePlus)
  2. For fine-tuning: format as instruction-response pairs; for ICL: construct prompt with guidelines + example
  3. Generate adaptations on test set
  4. Run automatic metrics for screening, but MUST perform human evaluation for final selection

- Design tradeoffs:
  - **GPT-4 ICL vs. LLaMa2 LoRA**: GPT-4 requires API dependency and per-token costs but achieved best human scores; LLaMa2 offers self-hosting but ranked lower in human evaluation despite higher automatic scores
  - **Dataset repetition**: Abstracts with multiple adaptations require repeating source sentences, creating duplicate inputs with different targets—may affect training stability
  - **Context inclusion**: Adding inter-sentence context did not significantly improve performance, suggesting sentence-level simplification may not require broader document context

- Failure signatures:
  - Model copies source sentence verbatim (high BLEU, failed simplification)
  - Model generates verbose, over-explained output (observed with GPT-3.5 and LLaMa2)
  - Model hallucinates medical explanations not grounded in source (faithfulness failure)
  - Automatic metrics rank model highly but human evaluation reveals poor simplification quality

- First 3 experiments:
  1. **Baseline replication**: Fine-tune T5-large on PLABA sentence pairs; evaluate with BLEU, ROUGE, SARI; perform spot-check human evaluation to confirm the copy-without-simplifying behavior
  2. **ICL guideline ablation**: Test GPT-4 with (a) short instructions only, (b) full annotation guidelines, (c) guidelines + one-shot example; compare human simplicity scores to isolate contribution of each prompt component
  3. **Cross-model comparison**: Fine-tune LLaMa2-13B with LoRA using identical instructions as T5; compare automatic vs. human evaluation divergence to validate paper's finding about metric-human mismatch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic evaluation metrics be developed to effectively penalize copying and align with human judgments of semantic simplification?
- Basis in paper: The authors note that BLEU and ROUGE "are not efficient in capturing the semantics" and misleadingly favored models that "repeat the abstract sentence as is."
- Why unresolved: N-gram overlap metrics fail to distinguish between high-quality abstractive simplification and low-effort extraction.
- What evidence would resolve it: A metric that correlates strongly with human simplicity scores while inversely correlating with source sentence overlap.

### Open Question 2
- Question: What specific fine-tuning interventions are required to prevent open-source LLMs from defaulting to extraction rather than abstractive adaptation?
- Basis in paper: The authors observed that fine-tuned LLaMa2 and T5 models "tend to frequently repeat the abstract sentence" whereas GPT-4 successfully generated plain language.
- Why unresolved: It is unclear if this "copying" behavior is due to model size, architecture, or the specific Parameter Efficient Fine-Tuning (PEFT) configuration used.
- What evidence would resolve it: An open-source training methodology that achieves parity with GPT-4 on the "Simplicity" axis without increasing hallucination.

### Open Question 3
- Question: Is the trade-off between high simplicity (Rank 1) and lower accuracy (Rank 3) an inherent limitation of current LLMs or a result of the specific prompting guidelines used?
- Basis in paper: The top-performing GPT-4 model ranked 1st in average simplicity but only 3rd in average accuracy.
- Why unresolved: The paper does not determine if the prompt instructions for simplification caused the model to omit necessary technical details.
- What evidence would resolve it: Ablation studies showing if modified prompts can improve accuracy scores without statistically degrading simplicity scores.

## Limitations

- Heavy reliance on GPT-4's in-context learning requires per-token API costs and raises reproducibility concerns without exact annotation guidelines
- Automatic metrics poorly correlate with human judgment, creating practical dilemma for scalable evaluation
- Focus on sentence-level simplification without broader document context leaves open questions about coherence and discourse-level quality

## Confidence

**High confidence** in the core finding that automatic metrics (BLEU, ROUGE) systematically mis-rank models for simplification tasks, as this is directly demonstrated through systematic comparison of metric rankings versus human evaluation results.

**Medium confidence** in the superiority of GPT-4 ICL over fine-tuning approaches, given that this conclusion is based on single-track competition results rather than controlled experiments with identical evaluation protocols across all models.

**Medium confidence** in the effectiveness of LoRA for resource-constrained fine-tuning, as the paper provides implementation details but doesn't compare against other PEFT methods or full fine-tuning baselines under identical computational budgets.

## Next Checks

1. **Metric Correlation Validation**: Replicate the study's automatic vs. human evaluation comparison on a held-out test set to verify whether BLEU/ROUGE systematically overrate non-simplifying models across different domains and text types.

2. **Guideline Distillation Effect**: Conduct an ablation study comparing GPT-4 performance using (a) full annotation guidelines, (b) distilled guidelines (as used in paper), and (c) simple task instructions to quantify the impact of guideline distillation on simplification quality.

3. **Cross-Model Controlled Experiment**: Run a controlled comparison where all models (T5, LLaMa2, GPT-3.5, GPT-4) are evaluated using identical human evaluation protocols on the same test set, measuring both simplicity and accuracy metrics to confirm the competition rankings.