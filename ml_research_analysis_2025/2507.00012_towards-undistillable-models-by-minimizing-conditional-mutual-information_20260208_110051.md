---
ver: rpa2
title: Towards Undistillable Models by Minimizing Conditional Mutual Information
arxiv_id: '2507.00012'
source_url: https://arxiv.org/abs/2507.00012
tags:
- cmim
- student
- accuracy
- trained
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of protecting deep neural networks
  (DNNs) from knowledge distillation (KD) attacks, which can be used to steal intellectual
  property. The core method idea is to train DNNs by jointly minimizing the conventional
  cross entropy (CE) loss and the conditional mutual information (CMI) values of all
  temperature-scaled output clusters.
---

# Towards Undistillable Models by Minimizing Conditional Mutual Information

## Quick Facts
- arXiv ID: 2507.00012
- Source URL: https://arxiv.org/abs/2507.00012
- Reference count: 40
- Primary result: CMIM models are undistillable by all tested KD methods, with knockoff students not outperforming label smoothing students

## Executive Summary
This paper addresses the problem of protecting deep neural networks from knowledge distillation attacks that can steal intellectual property. The authors propose training DNNs by jointly minimizing cross-entropy loss and conditional mutual information (CMI) of output probability clusters. By making output distributions within each class highly concentrated, the resulting CMIM models become resistant to distillation attacks while maintaining higher classification accuracy than models trained with CE loss alone.

## Method Summary
The method trains DNNs by combining standard cross-entropy loss with a CMI minimization term. For each class, the approach minimizes the conditional mutual information I(X;Ŷ|Y=y) across power-transformed output distributions, effectively reducing intra-class output diversity. This is achieved through an alternating optimization algorithm that maintains auxiliary distributions Q_{y,i} to estimate class-conditional centroids, enabling GPU-parallelizable CMI estimation. The power transform α = 1/T covers different temperature scalings, making the defense robust against temperature-based attacks.

## Key Results
- CMIM models achieve higher classification accuracy compared to CE-only models across CIFAR-100, TinyImageNet, and ImageNet
- Knockoff students trained from CMIM teachers do not outperform label smoothing students, demonstrating successful undistillability
- The defense is effective against multiple distillation methods including KD, DKD, MKD, and RKD
- Proper hyperparameter tuning (λ, N, β, ω) is critical for balancing accuracy and robustness

## Why This Works (Mechanism)

### Mechanism 1: Output Cluster Concentration via CMI Minimization
The method reduces intra-class output diversity by minimizing I(X;Ŷ|Y=y), making output clusters highly concentrated. Lower CMI means tighter clusters with less "dark knowledge" for distillation to exploit.

### Mechanism 2: Power-Transformed CMI for Temperature-Robust Protection
By minimizing max_α I(X;Ŷ_α|Y=y) over α ∈ [0,β], the defense covers the spectrum of possible attack temperatures. Temperature scaling T is equivalent to power transform α = 1/T.

### Mechanism 3: Alternating Optimization with Auxiliary Distributions
The intractable dependency on class-conditional centroids is replaced by learnable auxiliary distributions Q_{y,i}, updated via exponential moving average. This enables GPU-parallelizable CMI estimation without per-batch centroid dependencies.

## Foundational Learning

- **Conditional Mutual Information (CMI)**: Measures how much knowing X tells you about Ŷ when you already know Y. Why needed: Quantifies output diversity within each class. Quick check: If I(X;Ŷ|Y=y) = 0 for all y, what does that imply? (Answer: All samples of class y produce identical output distributions.)

- **Temperature Scaling in Knowledge Distillation**: Softens probability distributions; higher T reveals more inter-class relationships (dark knowledge). Why needed: Controls the amount of information revealed during distillation. Quick check: Why does T→∞ make KD equivalent to label smoothing? (Answer: All probabilities become uniform.)

- **KL Divergence as Cluster Dispersion Measure**: CMI decomposes as expected KL divergence between individual predictions and their class-conditional mean. Why needed: Measures how spread out predictions are within each class cluster. Quick check: If all q_x for class y are identical to s_y, what is KL(q_x, s_y)? (Answer: Zero.)

## Architecture Onboarding

- **Component map**: Logits -> Softmax -> q_x -> Power transform α -> CMI module -> Q_{y,i} updates -> Combined loss -> Backprop

- **Critical path**:
  1. Forward pass computes q_x for batch
  2. Apply N power transforms α₁...α_N
  3. For each class y in batch, update Q_{y,i} via batch-averaged q^αᵢ
  4. Compute weighted CMI term using current Q
  5. Backpropagate combined loss

- **Design tradeoffs**:
  - Larger N → better max-CMI approximation → ~constant training overhead (parallelizable) but more memory
  - Larger ω → better approximates maximum → risk of numerical overflow (NaNs for ω > 30)
  - Larger β → covers more temperature range → may destabilize optimization
  - Larger λ → stronger undistillability → potential accuracy tradeoff

- **Failure signatures**:
  - NaN loss during training: ω too large; reduce to ≤25
  - Knockoff student still outperforms LS baseline: CMI not minimized enough; try increasing N or λ
  - Protected model accuracy drops: λ too aggressive; reduce λ
  - Class imbalance causing poor Q estimates: Ensure balanced batch sampling per class

- **First 3 experiments**:
  1. **Baseline reproduction**: Train ResNet-50 on CIFAR-100 with CE only; measure CMI vs α curve and knockoff student accuracy using standard KD.
  2. **CMIM ablation on λ**: Train with λ ∈ {0.1, 0.25, 0.5, 1.0}, fix N=50, ω=20, β=2. Measure protected model accuracy, peak CMI, and best knockoff accuracy.
  3. **Cross-architecture transfer test**: Train CMIM ResNet-50 teacher, distill to VGG11 and ShuffleNetV2 students using multiple KD methods (KD, DKD, MKD). Verify knockoff accuracy < LS student accuracy.

## Open Questions the Paper Calls Out

- **Formal theoretical proof**: Can a formal theoretical proof be established to guarantee that minimizing CMI across power-transformed clusters ensures a model is strictly undistillable? The authors acknowledge this remains an open challenge despite empirical success.

- **Generative architecture protection**: How can the CMIM framework be effectively adapted to protect the intellectual property of generative architectures like LLMs, CLIP, and diffusion models? Current method is designed for classification tasks with cross-entropy and softmax outputs.

- **Latent space regularization**: Does integrating CMI minimization into a contrastive learning framework to regularize the latent space provide stronger undistillability than regularizing the output probability space? The paper proposes this as a promising future direction.

## Limitations
- Limited analysis of whether CMI minimization prevents all forms of knowledge extraction beyond tested distillation methods
- No theoretical guarantees that low CMI universally prevents knowledge transfer
- Hyperparameter λ selection appears heuristic with no systematic guidance provided
- Claims extend beyond tested methods without mathematical proof

## Confidence
- **High Confidence**: CMIM models achieve higher classification accuracy than CE-only baselines when properly tuned
- **Medium Confidence**: CMI minimization effectively reduces intra-class output diversity as measured by KL divergence
- **Low Confidence**: CMIM provides robust protection against all possible knowledge distillation attacks, including those not yet developed

## Next Checks
1. **Cross-Attack Validation**: Test CMIM protection against feature-distillation methods (e.g., IFVD, RKD) and attention-based distillation to verify defense generalizes beyond probability-based attacks.

2. **Hyperparameter Sensitivity Analysis**: Systematically map the λ-N-β-ω hyperparameter space to identify optimal configurations and quantify the accuracy-robustness tradeoff curve for practical deployment.

3. **Long-Tail Class Distribution**: Evaluate CMIM performance on datasets with significant class imbalance to verify that Q_{y,i} estimates remain stable and the defense mechanism functions correctly when class frequencies vary widely.