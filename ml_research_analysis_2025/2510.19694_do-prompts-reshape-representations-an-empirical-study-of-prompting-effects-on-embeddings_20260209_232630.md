---
ver: rpa2
title: Do Prompts Reshape Representations? An Empirical Study of Prompting Effects
  on Embeddings
arxiv_id: '2510.19694'
source_url: https://arxiv.org/abs/2510.19694
tags:
- prompt
- bert
- representations
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how prompting affects sentence representations
  in language models. The authors analyze whether relevant prompts lead to better
  task-specific embeddings compared to irrelevant ones, using probing classifiers
  across four classification tasks.
---

# Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings

## Quick Facts
- arXiv ID: 2510.19694
- Source URL: https://arxiv.org/abs/2510.19694
- Authors: Cesar Gonzalez-Gutierrez; Dirk Hovy
- Reference count: 40
- Primary result: Prompting modifies embeddings through contextualization, but prompt relevance does not consistently improve representation quality

## Executive Summary
This paper investigates how prompting affects sentence representations in language models, testing whether relevant prompts lead to better task-specific embeddings compared to irrelevant ones. The authors analyze BERT, RoBERTa, and GPT-2 models across four classification tasks using probing classifiers and task alignment metrics. Results show that prompting does alter representations through contextualization, but changes in probe performance do not consistently correlate with prompt relevance to the task. Different models and datasets show varying patterns, with no clear advantage for task-relevant prompts.

## Method Summary
The study applies prompt templates to dataset samples and generates embeddings from BERT, RoBERTa, and GPT-2 models using mean pooling or [CLS] token extraction. MaxEnt classifiers with L2 regularization are trained on these embeddings to predict dataset labels, with performance compared against no-prompt and random-prompt baselines. Task alignment scores provide an alternative measure of how well embedding space structure matches label structure. Statistical significance is assessed via bootstrap sampling.

## Key Results
- Prompting modifies embeddings through contextualization, but task-relevant prompts do not consistently outperform irrelevant or random prompts
- BERT shows more consistent gains from prompting compared to RoBERTa and GPT-2, which often show degraded performance
- Strong correlation (Pearson's r = 0.7475) between task alignment and probe performance suggests prompt effects manifest as class distribution shifts in embedding space
- Static (non-contextualized) prompts show no significant effect on probe performance, confirming contextualization is required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting modifies sentence representations through contextualization of tokens.
- Mechanism: Instruction tokens influence sample token representations via self-attention during the forward pass. This redistribution of class samples in embedding space can be detected by probing classifiers.
- Core assumption: Contextualization is necessary for prompts to affect representation quality.
- Evidence anchors:
  - [abstract] "prompting affects the quality of representations" through "contextualization"
  - [Section 3.2.4] Static (non-contextualized) prompts show no significant effect on probe performance, confirming contextualization is required
  - [corpus] Related work on in-context learning (e.g., induction heads, function vectors) suggests attention-based mechanisms, but no direct corroboration for this specific embedding-level claim
- Break condition: If using non-contextual embeddings (e.g., averaged static token vectors), prompting effects disappear.

### Mechanism 2
- Claim: Task alignment and probe performance are strongly correlated, suggesting prompt effects manifest as class distribution shifts in embedding space.
- Mechanism: Prompts induce geometric changes in representation space—specifically, redistributing samples relative to class boundaries—which both probing accuracy and task alignment metrics capture.
- Core assumption: Both probing and task alignment reflect the same underlying representational change.
- Evidence anchors:
  - [Section 3.2.2] Pearson's r = 0.7475, Spearman's ρ = 0.8412 between task alignment and probe performance
  - [Section 5] "changes amount to a redistribution of class samples in the embedding space"
  - [corpus] No direct corpus evidence on this correlation; remains an internal finding
- Break condition: If class separability metrics diverge from probe accuracy, this mechanism would not hold.

### Mechanism 3
- Claim: Prompt relevance does NOT predictably improve representation quality.
- Mechanism: Contrary to the hypothesis, relevant prompts do not consistently outperform irrelevant or random prompts. The relationship between prompt-task alignment and representation quality is model- and dataset-dependent.
- Core assumption: If ICL were driven purely by language pattern matching from pre-training, relevant prompts should improve representations.
- Evidence anchors:
  - [abstract] "changes in representation quality do not consistently correlate with the relevance of the prompts"
  - [Section 3.1] "prompts for unrelated tasks can improve probe performance, while relevant prompts may even degrade performance"
  - [corpus] Lu et al. (2024) cited within paper finds similar unexpected performance from irrelevant input changes
- Break condition: If a model shows consistent improvement only with task-relevant prompts across all datasets, this mechanism would be falsified.

## Foundational Learning

- Concept: **Probing classifiers**
  - Why needed here: The entire methodology relies on training linear classifiers on embeddings to measure task-relevant information. Without understanding probing, you cannot interpret the results.
  - Quick check question: If a probe achieves 90% accuracy on sentiment embeddings, does this prove the model "understands" sentiment, or just that sentiment information is linearly separable in that space?

- Concept: **Contextual vs. static embeddings**
  - Why needed here: The paper explicitly shows that only contextualized representations (where tokens attend to each other) are affected by prompting. Static averaging neutralizes prompt effects.
  - Quick check question: What happens to prompt effects if you pre-compute token embeddings and average them without running them through the model together?

- Concept: **Task alignment metric**
  - Why needed here: Used as an alternative measure to validate probing findings. Measures how well embedding space structure matches label structure across clustering granularities.
  - Quick check question: If task alignment is high but probe accuracy is low, what might this indicate about the representation?

## Architecture Onboarding

- Component map:
  - Input layer: Prompt templates applied to raw text (verbalization)
  - Encoder: BERT/RoBERTa (bidirectional MLM) or GPT-2 (autoregressive) processes concatenated prompt + sample
  - Pooling strategies: [CLS] token, mean pooling (µ), weighted mean (w for GPT-2)
  - Layer selection: Final layer (₁), second-to-last (₂)
  - Probe: MaxEnt classifier with L2 regularization trained on embeddings
  - Metrics: Probe accuracy/F1, task alignment scores, bootstrap significance testing

- Critical path:
  1. Apply prompt template to dataset samples
  2. Generate embeddings from specified layer and pooling method
  3. Train probe on training partition embeddings
  4. Evaluate on test partition, compute significance vs. baselines (no-prompt, random-prompt)

- Design tradeoffs:
  - [CLS] vs. mean pooling: [CLS] generally underperforms mean pooling in this study
  - Layer choice: Effects vary; no consistent winner between final and penultimate layers
  - Model choice: BERT shows most consistent gains from prompting; GPT-2 often degrades

- Failure signatures:
  - Random prompts outperforming task-relevant prompts
  - Significant variance across datasets for same prompt/task combination
  - No-prompt baseline matching or exceeding prompted performance

- First 3 experiments:
  1. Replicate toxicity detection probing on Wiki Toxic with BERT mean-pooled final layer, comparing no-prompt, random, and task-relevant prompts. Expect ~1-2% F1 variance with inconsistent significance.
  2. Test static prompt construction: separately embed instructions and sample, then average. Should show no significant differences across conditions (validates contextualization requirement).
  3. Cross-task probe: Train toxicity probe on sentiment-prompted embeddings. Check if "irrelevant" prompts sometimes improve performance on held-out toxicity task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the lack of correlation between prompt relevance and representation quality persist in larger, more extensively pre-trained models?
- Basis in paper: [explicit] The authors state in the Conclusion that one possible explanation for their results is that "the models used... were not sufficiently pre-trained" compared to state-of-the-art models.
- Why unresolved: This study was restricted to smaller architectures (BERT, RoBERTa, GPT-2); it is unknown if scaling parameters or training data volume resolves the observed misalignment.
- What evidence would resolve it: Replicating these probing experiments on modern Large Language Models (LLMs) with significantly larger parameter counts and pre-training corpora.

### Open Question 2
- Question: Is supervised instruction fine-tuning necessary to align representation quality with prompt relevance?
- Basis in paper: [explicit] The Conclusion suggests that "pre-training alone may not be enough" and that "instruction fine-tuning or reinforcement learning... may be necessary."
- Why unresolved: The models tested were standard pre-trained encoders/decoders without the specific supervised adaptation used in conversational assistants.
- What evidence would resolve it: Comparing probe performance between base models and their instruction-tuned variants (e.g., comparing a base model to its Instruct or RLHF-aligned version).

### Open Question 3
- Question: Can analyzing layer-wise dynamics during the forward pass explain in-context learning mechanisms where static embeddings fail?
- Basis in paper: [explicit] The Conclusion hypothesizes that "the embedding-level perspective is too limited" and that "layer dynamics of input processing may play a crucial role."
- Why unresolved: The methodology relied on static, final representations (or single layers), potentially missing transient processing states essential for ICL.
- What evidence would resolve it: A mechanistic analysis tracking how information flows and evolves across all transformer layers during prompt processing, rather than analyzing the final pooled output.

## Limitations

- Representational Measurability: The study relies on linear probes to measure task-relevant information in embeddings, which may underestimate the impact of prompting on representation structure by assuming linear separability is sufficient.
- Model and Dataset Generalizability: Results show significant variation across models and datasets, suggesting findings may not generalize uniformly across the broader landscape of language models or downstream tasks.
- Prompt Template Specificity: The study uses a fixed set of prompt templates per task, which may not capture the full space of effective prompting strategies and could influence the conclusion about prompt relevance.

## Confidence

- High Confidence: The finding that prompting modifies embeddings through contextualization is well-supported by the static prompt construction experiment.
- Medium Confidence: The claim that prompt relevance does not consistently correlate with representation quality has solid empirical support but requires careful interpretation due to underlying mechanism uncertainties.
- Low Confidence: Broader claims about implications for in-context learning mechanisms should be treated cautiously as the paper's findings reveal empirical patterns but do not definitively explain underlying causes.

## Next Checks

1. Replicate key experiments using non-linear classifiers (e.g., small MLPs) to verify whether linear probe limitations are masking meaningful prompt effects on representation structure.

2. Test whether embeddings generated with prompts from one model family (e.g., BERT prompts) transfer effectively to probing tasks designed for another model family (e.g., RoBERTa), to isolate prompt effects from model-specific factors.

3. Systematically vary prompt templates beyond the fixed set used in the study (e.g., different syntactic structures, varying prompt length) to determine whether the observed pattern of inconsistent prompt relevance effects holds across a broader prompting strategy space.