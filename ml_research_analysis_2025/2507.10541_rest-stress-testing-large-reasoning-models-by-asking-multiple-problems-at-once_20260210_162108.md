---
ver: rpa2
title: 'REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at
  Once'
arxiv_id: '2507.10541'
source_url: https://arxiv.org/abs/2507.10541
tags:
- reasoning
- stress
- rest
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REST addresses the challenge of evaluating Large Reasoning Models
  (LRMs) in real-world multi-context scenarios by stress-testing them with multiple
  problems at once, rather than isolated single questions. It transforms existing
  benchmarks by concatenating several questions into one prompt, assessing capabilities
  like contextual priority allocation, cross-problem interference resistance, and
  dynamic cognitive load management.
---

# REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once

## Quick Facts
- arXiv ID: 2507.10541
- Source URL: https://arxiv.org/abs/2507.10541
- Reference count: 40
- Stress-test LRMs by concatenating multiple questions into single prompts to assess contextual priority allocation, cross-problem interference resistance, and cognitive load management.

## Executive Summary
REST (Reasoning Stress Testing) introduces a new paradigm for evaluating Large Reasoning Models by concatenating multiple questions into single prompts, simulating real-world multi-context reasoning scenarios. This approach reveals substantial performance degradation under stress—for example, DeepSeek-R1's accuracy drops 29.1% on AIME24—exposing pronounced gaps among models with similar single-question scores. REST shows that models trained with long2short techniques preserve accuracy better under multi-question settings, providing a cost-efficient, discriminative evaluation method that better reflects real-world reasoning demands.

## Method Summary
REST transforms existing reasoning benchmarks by concatenating multiple problems into single prompts to evaluate how models handle contextual priority allocation, cross-problem interference, and dynamic cognitive load management. The method uses cyclic indexing to create stress prompts where s consecutive questions are combined: p^s_i = Compose(q_i, q_{i+1}, ..., q_{[(i+s-1) mod N]}). Prompts are structured with numbered questions and explicit instructions to answer each within \boxed{} notation. The approach tests 34 SOTA models (1.5B–671B parameters) across 7 benchmarks with varying stress levels, using rule-based extraction via regex on boxed answers and comparing performance against single-question baselines.

## Key Results
- DeepSeek-R1's accuracy drops 29.1% on AIME24 under REST stress testing
- Long2short-trained models preserve accuracy better under multi-question settings
- Models with similar single-question scores show pronounced performance gaps under REST

## Why This Works (Mechanism)
REST works by simulating real-world multi-context reasoning scenarios where models must manage multiple problems simultaneously rather than in isolation. By concatenating questions, it creates genuine interference between problems, forcing models to allocate cognitive resources dynamically and resist cross-contamination between reasoning contexts. The cyclic indexing ensures comprehensive coverage of question combinations while maintaining systematic evaluation. This approach reveals fundamental limitations in how LRMs handle cognitive load and contextual switching that single-question benchmarks cannot expose.

## Foundational Learning
- **Cyclic indexing for stress construction**: Why needed - ensures systematic coverage of question combinations while maintaining reproducibility. Quick check - verify that concatenated question sets cycle through all available questions without repetition bias.
- **Rule-based answer extraction via regex**: Why needed - provides consistent parsing of multi-answer outputs across different model formats. Quick check - manually inspect extracted boxed answers to ensure all intended answers are captured.
- **Stress level parameterization**: Why needed - allows controlled investigation of performance degradation as cognitive load increases. Quick check - plot accuracy decay curve across stress levels to confirm expected monotonic decline.

## Architecture Onboarding

**Component Map**: Benchmarks -> Compose() -> Prompt Generation -> Model Inference -> Regex Extraction -> Accuracy Calculation

**Critical Path**: Compose() (concatenation) → Prompt Generation → Model Inference → Answer Extraction → Accuracy Computation

**Design Tradeoffs**: 
- Higher stress levels provide more stress testing but risk overwhelming models completely
- Longer prompts increase realism but may exceed model context windows
- Rule-based extraction is simpler but may miss nuanced answer formats

**Failure Signatures**: 
- Question omission (model only answers early questions)
- Output truncation at 32K tokens
- Inconsistent boxed answer formatting across models

**First Experiments**:
1. Run REST on GSM8K with s={1,3,6} using DeepSeek-R1-Distill-Qwen-7B to verify accuracy drop at higher stress levels
2. Test answer extraction by manually inspecting extracted boxed answers from sample REST prompts
3. Compare single-question baseline accuracy with REST s=1 to confirm consistency before assessing multi-question degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Exact sampling hyperparameters for each model are not fully specified, making exact replication challenging
- Rule-based answer extraction mechanism is underspecified beyond generic regex patterns
- Benchmark partitions and specific question groupings are not reported, introducing potential variance across runs

## Confidence
- **High** confidence that REST is a valid multi-question evaluation paradigm that reveals performance degradation under multi-context stress
- **Medium** confidence in the magnitude of accuracy drops reported due to unspecified hyperparameters and benchmark splits
- **Low** confidence in reproducing exact per-model rankings without full sampling parameters and deterministic question groupings

## Next Checks
1. Run REST on GSM8K with s={1,3,6} using DeepSeek-R1-Distill-Qwen-7B with documented sampling settings; verify average accuracy at s>1 drops relative to s=1
2. Test answer extraction by manually inspecting extracted boxed answers from sample REST prompts to ensure regex captures all intended answers
3. Compare single-question baseline accuracy with REST s=1 to confirm consistency before assessing multi-question degradation