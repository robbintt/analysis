---
ver: rpa2
title: Efficient Strategy for Improving Large Language Model (LLM) Capabilities
arxiv_id: '2508.04073'
source_url: https://arxiv.org/abs/2508.04073
tags:
- fine-tuning
- base
- performance
- language
- velandia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an efficient strategy for improving large language
  model (LLM) capabilities in resource-constrained environments using a base 1B parameter
  model. The approach combines data selection from academic theses, fine-tuning with
  LoRA, post-training quantization, and Retrieval-Augmented Generation (RAG) to enhance
  quality, response formatting, and efficiency.
---

# Efficient Strategy for Improving Large Language Model (LLM) Capabilities

## Quick Facts
- arXiv ID: 2508.04073
- Source URL: https://arxiv.org/abs/2508.04073
- Reference count: 28
- The paper proposes an efficient strategy for improving LLM capabilities in resource-constrained environments using a 1B parameter base model combined with data selection, fine-tuning, quantization, and RAG.

## Executive Summary
This paper presents an efficient strategy for enhancing a 1B parameter LLM (Llama-3.2-1B-Instruct) in resource-constrained environments. The approach combines domain-specific data selection from academic theses, fine-tuning with LoRA adapters, post-training quantization, and Retrieval-Augmented Generation (RAG). A family of nine model variants was developed and evaluated using an LLM-as-a-judge methodology. The best-performing model, LLM-q-ft-rag (quantized, fine-tuned, with RAG), achieved an average ranking of 2.50 with 26 first-place finishes out of 100 questions, demonstrating that quantization followed by fine-tuning yields better results than the reverse order due to adapter precision preservation.

## Method Summary
The method involves four key components: (1) data preparation using academic theses from UNAL repository, formatted as Q&A pairs (1,910 documents, 75/25 train/test split); (2) model fine-tuning with LoRA (rank=2, alpha=8, dropout=0.2) for 7 hours on a single NVIDIA T4 GPU (16GB VRAM); (3) post-training quantization to 4-bit using Q4_K_M format; and (4) RAG integration using TF-IDF vectorization with cosine similarity retrieval from the thesis corpus. The critical finding is that quantizing the base model before applying LoRA fine-tuning (Q-FT) preserves adapter precision and outperforms fine-tuning-then-quantizing (FT-Q), which degrades performance due to precision loss in the adapter weights.

## Key Results
- The LLM-q-ft-rag model achieved the best performance with an average ranking of 2.50 and 26 first-place finishes out of 100 questions.
- Quantization followed by fine-tuning (Q-FT) outperformed fine-tuning followed by quantization (FT-Q), with rankings of 2.89 vs. 8.56 respectively.
- Fine-tuning significantly improved base model performance (rank 7.47 → 2.89), while RAG provided marginal but consistent improvements.
- Quantized models offered faster inference and lower memory usage, making them practical for constrained deployments.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantizing a base model before applying LoRA fine-tuning preserves adapter precision and outperforms fine-tuning-then-quantizing.
- **Mechanism:** When fine-tuning is applied to a pre-quantized model (Q-FT), the LoRA adapter weights train in full precision (FP16) atop frozen 4-bit base weights. In contrast, quantizing after fine-tuning (FT-Q) subjects the already-trained adapter to precision reduction, degrading learned representations.
- **Core assumption:** LoRA adapters trained on quantized backbones can learn compensatory representations that maintain or improve performance despite base model compression.
- **Evidence anchors:** Table 4 shows LLM-q-ft achieved rank 2.89 with 27 first-place finishes vs. LLM-ft-q at rank 8.56 with zero first-place finishes. Section 3.1 states quantization after fine-tuning degrades performance because it reduces numerical precision across the entire model, including the adapter.

### Mechanism 2
- **Claim:** LoRA fine-tuning with carefully curated academic-domain data improves response structure and domain relevance without full parameter updates.
- **Mechanism:** LoRA injects trainable low-rank decomposition matrices into attention layers, requiring ~0.1-1% of total parameters to be updated. This allows domain adaptation (academic thesis content) while preserving base model capabilities.
- **Core assumption:** The question-answer pairs generated from thesis fragments contain sufficient signal for the model to learn domain-specific formatting and content patterns.
- **Evidence anchors:** Table 2 shows training with rank=2, alpha=8, dropout=0.2 for 7 hours on 1,859 thesis documents. The approach was selected for enhancing structure and formatting of model outputs.

### Mechanism 3
- **Claim:** RAG provides marginal but consistent quality improvements by injecting relevant context at inference time.
- **Mechanism:** TF-IDF vectorization retrieves document fragments from the thesis corpus based on cosine similarity. Retrieved context is prepended to prompts, grounding responses in source material without modifying model weights.
- **Core assumption:** TF-IDF similarity effectively identifies relevant context for arbitrary user queries within the academic domain.
- **Evidence anchors:** Table 4 shows RAG variants consistently outperform non-RAG counterparts (e.g., LLM-q-ft-rag at 2.50 vs. LLM-q-ft at 2.89). Section 1.4 describes RAG system with TF-IDF retrieval, ~2000ms query time, and context window compliance.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The paper uses LoRA to make 1B parameter fine-tuning feasible on limited resources (single T4 GPU, 16GB VRAM).
  - **Quick check question:** Can you explain why LoRA's rank parameter controls the tradeoff between adaptation capacity and compute cost?

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** PTQ enables deployment efficiency; the paper's key finding hinges on when quantization occurs relative to fine-tuning.
  - **Quick check question:** What precision reduction does 4-bit quantization represent compared to FP16, and what information loss does it introduce?

- **Concept: TF-IDF Vectorization for Retrieval**
  - **Why needed here:** The RAG system uses TF-IDF rather than dense embeddings; understanding sparse retrieval helps diagnose RAG failure modes.
  - **Quick check question:** Why might TF-IDF struggle with semantic similarity compared to dense embedding retrieval, and when might it still be preferable?

## Architecture Onboarding

- **Component map:**
  Base Model (LLaMA 3.2 1B) -> [Branch A: Quantize First] [Branch B: Fine-tune First] -> 4-bit Quantized Model -> LoRA Fine-tuning -> [Q-FT Models] [FT-Q Models] -> + RAG Adapter (optional) -> Final Model Variant

- **Critical path:** Dataset curation → Q-A generation → Quantize base → LoRA train → (optional) attach RAG → evaluate with LLM-as-judge

- **Design tradeoffs:**
  - LoRA rank=2: Minimal compute but limited adaptation capacity; higher rank (4-8) may improve quality at memory cost
  - TF-IDF vs. dense retrieval: TF-IDF is faster (~2000ms) but may miss semantic matches; dense embeddings cost more but improve recall
  - 4-bit vs. 8-bit quantization: 4-bit maximizes efficiency; 8-bit preserves more precision but halves compression ratio

- **Failure signatures:**
  - FT-Q models ranking last (8.36-8.56): Adapter precision loss from post-hoc quantization
  - Base model without RAG ranking 7.47: No domain context injection
  - RAG adding no improvement on base model (7.47 vs. 7.63): Context not utilized effectively or retrieval mismatch

- **First 3 experiments:**
  1. **Replicate Q-FT vs. FT-Q comparison:** Train identical LoRA configs on quantized vs. non-quantized base with same data splits; measure ranking gap to validate ordering hypothesis
  2. **Ablate RAG contribution:** Run Q-FT model with RAG disabled vs. enabled on held-out questions; isolate retrieval signal from model improvements
  3. **Test LoRA rank sensitivity:** Train Q-FT models with rank=1, 2, 4, 8; plot performance vs. training time/memory to find optimal efficiency frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the performance gains from the "quantize-then-finetune" (Q-FT) strategy transfer to significantly larger parameter models (e.g., 7B or 70B) or different architectures?
- Basis in paper: The conclusion states the work was limited to a specific dataset and a single base model, explicitly recommending the expansion of experimental scope to multiple datasets and models for generalizability.
- Why unresolved: The study restricts its findings to a 1B parameter model; it is unknown if the observed benefits of applying LoRA to a pre-quantized model scale effectively or if they are an artifact of small model compression.
- What evidence would resolve it: Replicating the specific Q-FT vs. FT-Q experimental pipeline on larger architectures (e.g., LLaMA-3-8B or 70B) and comparing the resulting degradation or improvement deltas.

### Open Question 2
- Question: How does the LLM-as-a-judge evaluation correlate with human expert review within the specific domain of academic theses used for training?
- Basis in paper: The paper relies entirely on GPT-4o to rank model responses ("LLM-as-a-judge") because traditional metrics are unreliable, but it does not validate these rankings against human ground truth.
- Why unresolved: While GPT-4o provides a ranking, it may suffer from specific biases (e.g., preference for length or specific formatting) that differ from human academic standards, potentially misrepresenting the "quality" of the fine-tuned outputs.
- What evidence would resolve it: A comparative study where human subject matter experts rank a subset of the model responses to calculate an inter-annotator agreement score against the GPT-4o rankings.

### Open Question 3
- Question: Can the "quantize-then-finetune" approach be effectively combined with reasoning paradigms like Chain-of-Thought (CoT) or reinforcement learning without destabilizing the low-rank adapters?
- Basis in paper: The Future Work section highlights the "Reasoning Paradigm" and models like DeepSeek-R1, questioning how reasoning capabilities can be enhanced, but notes the current study did not explore prompt structuring or step-by-step verification.
- Why unresolved: The current work focuses on retrieval and formatting; it is unclear if the precision reduction in quantized models hinders the complex multi-step logic required for reasoning paradigms when using LoRA.
- What evidence would resolve it: Benchmarking the Q-FT model variants on reasoning-specific datasets (e.g., GSM8K) using CoT prompting to measure logical consistency against the baseline.

## Limitations
- The study is limited to a 1B parameter model and academic domain data, limiting generalizability to other domains and scales.
- Critical training hyperparameters (learning rate, batch size, epochs) are unspecified, making exact reproduction difficult.
- The LLM-as-a-judge methodology introduces subjectivity and potential benchmark leakage with only 100 evaluation questions.
- RAG's marginal benefit suggests context retrieval quality may be limiting, but TF-IDF vs. dense embeddings was not compared.

## Confidence
- **High Confidence:** The Q-FT > FT-Q ordering effect is clearly demonstrated with direct comparison (rank 2.89 vs. 8.56, zero first-place finishes for FT-Q).
- **Medium Confidence:** LoRA fine-tuning improves base model quality (rank 7.47 → 2.89), though the magnitude depends on data quality and prompt design assumptions.
- **Low Confidence:** RAG provides consistent marginal improvement; TF-IDF retrieval is optimal for this corpus; results generalize beyond academic domain.

## Next Checks
1. **Hyperparameter Sensitivity Sweep:** Re-run Q-FT training with varying LoRA ranks (1, 2, 4, 8) and learning rates to identify optimal efficiency-quality tradeoff and confirm rank=2 is justified.
2. **RAG Retrieval Comparison:** Implement dense embedding retrieval (e.g., sentence transformers) alongside TF-IDF; compare retrieval relevance and downstream ranking impact to validate sparse retrieval choice.
3. **Domain Generalization Test:** Apply the Q-FT-4bit-RAG pipeline to a non-academic corpus (e.g., legal documents, technical manuals) with identical training configuration; measure ranking drop to quantify domain dependence.