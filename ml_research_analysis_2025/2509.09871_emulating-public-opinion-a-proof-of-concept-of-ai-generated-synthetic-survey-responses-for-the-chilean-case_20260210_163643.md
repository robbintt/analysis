---
ver: rpa2
title: 'Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey
  Responses for the Chilean Case'
arxiv_id: '2509.09871'
source_url: https://arxiv.org/abs/2509.09871
tags:
- synthetic
- confidence
- data
- survey
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study benchmarks 128 LLM-prompt-question combinations, generating
  189,696 synthetic survey profiles, to assess whether AI-generated synthetic respondents
  can replicate real-world public opinion in Chile. Models tested include GPT-4o,
  GPT-4o-mini, Llama 4 Maverick, and Qwen 2.5.
---

# Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case

## Quick Facts
- arXiv ID: 2509.09871
- Source URL: https://arxiv.org/abs/2509.09871
- Reference count: 11
- Primary result: LLM-generated synthetic survey profiles achieve F1-scores >0.90 on trust-related items, with highest alignment among 45-59 age group

## Executive Summary
This study benchmarks 128 LLM-prompt-question combinations, generating 189,696 synthetic survey profiles to assess whether AI-generated synthetic respondents can replicate real-world public opinion in Chile. Models tested include GPT-4o, GPT-4o-mini, Llama 4 Maverick, and Qwen 2.5. Synthetic responses closely match human responses on trust-related items, achieving F1-scores and accuracy above 0.90. GPT-4o, GPT-4o-mini, and Llama 4 Maverick perform comparably overall. Highest alignment occurs among respondents aged 45-59. While synthetic samples approximate probabilistic survey responses, item-level heterogeneity remains, highlighting the need for careful calibration and subgroup evaluation to ensure algorithmic fidelity and reduce biases in LLM-generated public opinion data.

## Method Summary
The study benchmarks 128 question–subsample pairs using two prompting strategies (demographics-only and demographics+attitudinal) across four LLM models (GPT-4o, GPT-4o-mini, Llama 4 Maverick, Qwen 2.5) applied to 16 Chilean survey questions from the CEP Survey #92. For each profile–question pair, synthetic responses are generated and compared against ground-truth survey data using accuracy, precision, recall, and F1-score metrics. Random-effects meta-regression decomposes variance in F1-scores across sociodemographic subsamples to identify systematic biases. The analysis reveals that while aggregate alignment is strong for trust-related items, performance varies significantly by question type and demographic subgroup.

## Key Results
- Synthetic responses achieve F1-scores and accuracy above 0.90 on trust-related items
- GPT-4o, GPT-4o-mini, and Llama 4 Maverick show comparable overall performance
- Highest alignment occurs among respondents aged 45-59
- Item-level heterogeneity persists, with economic and political perception questions showing lower performance than trust items
- Subgroup meta-regression reveals positive, significant coefficients only for the 45-59 age group

## Why This Works (Mechanism)

### Mechanism 1
LLMs encode statistical associations between sociodemographic characteristics and survey responses from their training data. When prompted with a profile (e.g., "45-year-old urban female"), the model samples from the conditional distribution of responses associated with that profile, effectively performing pattern retrieval rather than genuine opinion formation. Core assumption: training corpus contains sufficient Chilean public opinion coverage and statistical relationships match target population. Break condition: underrepresented populations or culturally specific attitudes in training data lead to systematic bias.

### Mechanism 2
Aggregate-level alignment occurs because random individual-level errors cancel out across large samples. The law of large numbers applied to synthetic sampling ensures that while any single synthetic response may deviate from its human counterpart, the aggregate distribution converges toward the population distribution if conditional probabilities are approximately calibrated. Core assumption: errors are approximately random rather than systematically directional. Break condition: systematic response biases (e.g., positivity bias) skew aggregate distributions regardless of sample size.

### Mechanism 3
Alignment varies systematically by question type and demographic subgroup because some opinion distributions are more strongly encoded in training data than others. Topics with extensive media coverage, academic discussion, or online discourse have richer representations, enabling more accurate response modeling. Similarly, demographics with more online presence may be better captured than underrepresented groups. Core assumption: salience and frequency of topics in training data correlates with synthetic response accuracy. Break condition: niche topics or populations with minimal digital footprint lack sufficient signal for appropriate conditioning.

## Foundational Learning

- **Concept: F1-Score and Class Imbalance**
  - Why needed here: Survey responses are often imbalanced (e.g., 70% say "no confidence" in political parties). Accuracy alone is misleading in such cases. The F1-score (harmonic mean of precision and recall) provides a more reliable measure of synthetic-human alignment.
  - Quick check question: If 90% of respondents answer "Option A" and your model always predicts "Option A," what is the accuracy? What is the F1-score for the minority class?

- **Concept: Algorithmic Fidelity**
  - Why needed here: The paper evaluates whether synthetic respondents replicate the "nuanced viewpoints of subpopulations." This requires understanding how fidelity differs from raw accuracy—it captures whether the model reproduces subgroup-specific patterns, not just aggregate distributions.
  - Quick check question: A model achieves 85% accuracy overall but systematically underestimates support for a policy among rural respondents. Does it have high algorithmic fidelity?

- **Concept: Random-Effects Meta-Regression**
  - Why needed here: The paper uses meta-regression to decompose variance in F1-scores across question-subsample pairs, separating sampling error from genuine heterogeneity. Understanding τ² (between-study variance) and I² (proportion of total variance due to heterogeneity) is essential for interpreting the bias analysis.
  - Quick check question: If I² = 42.45%, what proportion of variance in F1-scores is attributable to real differences between question-subsample pairs versus random error?

## Architecture Onboarding

- **Component map**: Ground-truth survey data -> Profile extractor -> Prompt templates -> LLM inference layer -> Response parser -> Evaluation metrics calculator -> Bias detection module
- **Critical path**: Ground-truth data → Profile construction → Prompt formulation → LLM inference → Response alignment → Metric computation → Subgroup decomposition. Errors at any stage propagate; the prompt formulation stage is the most failure-prone for cross-cultural applications.
- **Design tradeoffs**: Prompt complexity vs. data requirements (adding attitudinal variables improves performance but requires more input data); model selection vs. cost (closed-source models perform slightly better but cost more); sample size vs. API costs (more profiles reduce variance but increase costs linearly).
- **Failure signatures**: F1-score < 0.60 on specific question types indicates topic is poorly captured in training data; systematically lower F1-scores for specific demographic subgroups indicate representation bias; high variance across prompt formulations indicates sensitivity to prompt engineering.
- **First 3 experiments**: 1) Baseline replication: Select GPT-4o-mini, test on 2-3 trust questions with demographics-only prompting, verify F1-scores >0.80; 2) Subgroup stress test: Stratify evaluation by age and gender, compute F1-scores separately, check whether 45-59 age group shows highest alignment; 3) Topic generalization test: Apply pipeline to economic perception question, compare F1-scores to trust items, quantify item-level heterogeneity gap.

## Open Questions the Paper Calls Out
- How does algorithmic fidelity vary across educational and socioeconomic subgroups? The current study restricted bias analysis to area, gender, and age groups, leaving education and socioeconomic level unexplored.
- To what extent can post-processing calibration techniques correct for item-level heterogeneity in synthetic samples? The paper proposes reweighting or calibrating synthetic samples as future research without testing these adjustments.
- Do advanced reasoning models offer improved accuracy in survey emulation compared to standard LLMs? Reasoning models were excluded from benchmarking due to computational costs, leaving the trade-off between cost and potential performance gains untested.

## Limitations
- The paper does not report calibration curves or confidence intervals for F1-scores, making it difficult to assess statistical significance of subgroup differences.
- High performance for trust items versus lower performance for economic and political items suggests method works best for topics with extensive digital discourse coverage, limiting generalizability.
- Findings hinge on prompt engineering quality, which remains poorly specified and highly sensitive to cultural context.

## Confidence
- **High confidence**: Methodology for benchmarking LLM-generated responses against Chilean survey data is sound and reproducible; finding that alignment varies by demographic subgroup (highest for ages 45-59) is robust.
- **Medium confidence**: Claims about comparable performance across different LLM models are supported, but analysis doesn't fully explore why certain models perform better on specific question types.
- **Low confidence**: Generalizability claim that synthetic sampling "approximates probabilistic survey responses" across diverse topics and populations is not fully supported, given demonstrated topic-dependent performance heterogeneity.

## Next Checks
1. **Topic-generalization stress test**: Apply pipeline to a question outside the trust domain (e.g., economic perception item) and compute the gap in F1-scores between trust items and the new topic to quantify topic-specific performance decay.
2. **Subgroup bias audit**: Stratify evaluation by education level and socioeconomic status, computing F1-scores for each stratum. Compare whether the 45-59 age group advantage persists across all education levels.
3. **Prompt-sensitivity analysis**: Generate synthetic responses using three different prompt formulations for the same profile-question pairs (e.g., adding explicit instructions to avoid bias, changing role-play framing, or varying temperature). Measure variance in F1-scores across formulations to quantify prompt stability.