---
ver: rpa2
title: A Systematic Analysis of Biases in Large Language Models
arxiv_id: '2512.15792'
source_url: https://arxiv.org/abs/2512.15792
tags:
- llms
- bias
- language
- news
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically analyzes biases in four large language
  models (Qwen, DeepSeek, Gemini, GPT) across five dimensions: politics, ideology,
  alliance, language, and gender. The authors design five experiments using news summarization,
  stance classification, UN voting patterns, multilingual story completion, and World
  Values Survey responses.'
---

# A Systematic Analysis of Biases in Large Language Models

## Quick Facts
- arXiv ID: 2512.15792
- Source URL: https://arxiv.org/abs/2512.15792
- Authors: Xulang Zhang; Rui Mao; Erik Cambria
- Reference count: 40
- Primary result: While models are generally politically neutral, they exhibit slight left-leaning tendencies and stronger agreement with peripheral UN delegates than major powers

## Executive Summary
This paper presents a comprehensive systematic analysis of biases across five dimensions in four large language models: Qwen2.5, DeepSeek-V3, Gemini-2.5, and GPT-4o-mini. The authors designed five experiments using news summarization, stance classification, UN voting patterns, multilingual story completion, and World Values Survey responses. Results show that despite alignment efforts, models inherit biases from training data, exhibiting slight left-leaning political tendencies, varying ideological alignments, and stronger agreement with peripheral UN delegates. The study highlights the importance of bias awareness when deploying LLMs in real-world applications.

## Method Summary
The authors analyze biases across five dimensions using five experiments: (1) political bias through cosine similarity of embeddings between model-generated neutral summaries and partisan news coverage; (2) ideological bias via stance classification accuracy on left/center/right articles; (3) alliance bias through Cohen's Kappa comparison of model UN voting patterns with actual delegates; (4) language bias using PCA visualization of multilingual story completions; and (5) gender bias comparing model responses to World Values Survey questions against male/female averages. The study uses consistent prompts across four LLMs with Qwen3-Embedding-4B for all embedding calculations.

## Key Results
- Models show slight left-leaning tendencies in political bias despite alignment efforts
- Ideological biases vary by model: Gemini aligns more with right, GPT with left
- All models show stronger agreement with peripheral UN delegates than major powers
- Language biases reveal diverse thinking across languages without strong anglocentric patterns
- Gender bias analysis indicates models align more with women's values than men's

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit and perpetuate biases from human-generated training corpora despite alignment interventions
- Core assumption: Observed model behaviors reflect statistical patterns in training data rather than emergent properties
- Evidence: Results indicate models show biases despite alignment; biases presumed inherited from pre-training data
- Break condition: If models trained on synthetic/de-biased data still exhibit identical bias patterns

### Mechanism 2
- Claim: Semantic similarity between model outputs and partisan content reveals latent political positioning
- Core assumption: Embedding similarity correlates with ideological alignment
- Evidence: Cosine similarities computed between generated summaries and left/right news reporting
- Break condition: If embedding similarity captures stylistic rather than ideological features

### Mechanism 3
- Claim: Misclassification patterns in stance detection reveal models' own ideological blind spots
- Core assumption: Classification errors reflect asymmetric exposure to ideological language patterns
- Evidence: Asymmetric misclassification rates suggest differential familiarity with rhetorical patterns
- Break condition: If misclassification rates correlate with dataset imbalance rather than model alignment

## Foundational Learning

- Concept: **Cosine similarity in embedding space**
  - Why needed here: Core metric for measuring political bias through semantic distance
  - Quick check: If two texts have cosine similarity of 0.95, are they necessarily semantically equivalent? (No—similarity captures directional alignment, not semantic identity.)

- Concept: **Cohen's Kappa for inter-rater agreement**
  - Why needed here: Quantifies alignment between LLM voting patterns and UN delegate votes
  - Quick check: Why use Kappa instead of raw accuracy? (Kappa corrects for agreement that would occur by random chance.)

- Concept: **Principal Component Analysis (PCA) for cross-lingual comparison**
  - Why needed here: Reduces high-dimensional story embeddings to 2D for visualizing cross-lingual differences
  - Quick check: If all languages cluster tightly in PCA space, what does this suggest? (Likely cross-lingual transfer or shared underlying representations.)

## Architecture Onboarding

- Component map: Center news → LLM summary → Embed → Compare to left/right embeddings → Cosine similarity scores
- Critical path: Prompt engineering is highest-leverage component—small changes in system prompts substantially affect outputs
- Design tradeoffs: Single embedding model ensures consistency but introduces model-specific biases; back-translation enables cross-lingual comparison but introduces noise
- Failure signatures: Summaries clustering in top-right indicates quality not bias; classifying everything as "center" indicates insensitivity to ideological cues
- First 3 experiments:
  1. Replicate political bias experiment with 10-20 events to validate cosine similarity pipeline
  2. Test prompt sensitivity with and without "neutral" framing for ideological outputs
  3. Pilot language bias with 5-10 languages across different resource levels before full deployment

## Open Questions the Paper Calls Out

- Question: To what extent is it desirable to align LLMs to human reasoning patterns if the human "teachers" are intrinsically biased and heterogeneous?
  - Basis: Authors explicitly ask about desirability of aligning LLMs to intrinsically biased human teachers
  - Why unresolved: Current alignment techniques like RLHF often fail to eliminate bias and may amplify it
  - Resolution evidence: Development of alternative alignment paradigms prioritizing abstract neutrality

- Question: Can a theory of intelligence be designed that upholds neutrality and calibrated uncertainty without relying on mimicking human cognition?
  - Basis: Authors suggest LLMs might require distinct theory of intelligence from human cognitive patterns
  - Why unresolved: Current architectures fundamentally built to predict human-generated text
  - Resolution evidence: Architectural innovations allowing non-anthropocentric reasoning frameworks

- Question: What specific factors in training or alignment cause models to align more closely with peripheral UN delegates than with major geopolitical powers?
  - Basis: Consistent finding that models agree more with delegates from Latin America and Africa
  - Why unresolved: Unclear if alignment stems from training data balancing, safety filters, or training corpora artifacts
  - Resolution evidence: Ablation studies on training data composition and safety layer analysis

## Limitations

- Single embedding model (Qwen3-Embedding-4B) introduces model-specific biases into measurements
- Back-translation approach for multilingual analysis introduces additional noise
- API-based model access means results depend on specific model versions and unreported sampling parameters
- Political bias analysis uses limited 1,018 events from Bias Flipper dataset
- Gender bias analysis relies on potentially outdated World Values Survey Wave 7 data

## Confidence

**High Confidence**: Political bias methodology through cosine similarity is technically sound and well-documented
**Medium Confidence**: UN voting patterns analysis is statistically valid but assumptions about ideological alignment introduce uncertainty
**Low Confidence**: Gender bias analysis using absolute distance from survey averages is most indirect measurement

## Next Checks

1. Run ideological bias classification experiment with three different prompt formulations to quantify prompt-dependence
2. Repeat political bias analysis using two different embedding models to test pattern consistency
3. Select 50 political events and run monthly analysis for three months to assess temporal stability