---
ver: rpa2
title: 'Pretrained Battery Transformer (PBT): A battery life prediction foundation
  model'
arxiv_id: '2512.16334'
source_url: https://arxiv.org/abs/2512.16334
tags:
- battery
- data
- learning
- life
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Pretrained Battery Transformer (PBT),
  the first foundation model for battery cycle life prediction. PBT leverages a domain-knowledge-encoded
  mixture-of-experts layer (BatteryMoE) to capture universal representations from
  13 diverse lithium-ion battery datasets, outperforming existing models by an average
  of 19.8%.
---

# Pretrained Battery Transformer (PBT): A battery life prediction foundation model

## Quick Facts
- arXiv ID: 2512.16334
- Source URL: https://arxiv.org/abs/2512.16334
- Reference count: 40
- First foundation model for battery cycle life prediction, achieving 19.8% average improvement over existing models

## Executive Summary
This study introduces PBT, the first foundation model for battery cycle life prediction, leveraging domain-knowledge-encoded mixture-of-experts layers to capture universal representations from 13 diverse lithium-ion battery datasets. PBT significantly outperforms existing models by an average of 19.8% and demonstrates exceptional generalizability across battery chemistries (LIB, Na-ion, Zn-ion) and formats. With transfer learning, PBT maintains state-of-the-art performance even with limited data, reducing data acquisition needs by ~99% while preserving accuracy. This work establishes a pathway toward universal battery lifetime prediction systems, accelerating battery development and deployment across real-world applications.

## Method Summary
PBT uses a BatteryMoE mixture-of-experts layer with both hard (domain-knowledge-based) and soft (LLM-based) encoders to route battery inputs to specialized expert networks. The model tokenizes each charge-discharge cycle's voltage, current, and capacity data, then processes these tokens through transformer-based intra- and inter-cycle encoders with BatteryMoE modules. Pretrained on 13 lithium-ion datasets, PBT achieves transfer learning performance on 15 datasets covering various chemistries through fine-tuning and adapter tuning.

## Key Results
- Achieves 19.8% average improvement over existing models (CPMLP, CPTransformer, BatLiNet) on LIB datasets
- Maintains state-of-the-art performance across 15 datasets with various chemistries (LIB, Na-ion, Zn-ion)
- Reduces data acquisition needs by ~99% while preserving accuracy through transfer learning
- Successfully transfers knowledge to unseen chemistries (Na-ion, Zn-ion) with limited target data

## Why This Works (Mechanism)

### Mechanism 1: Domain-knowledge-encoded routing
PBT captures transferable degradation patterns from heterogeneous battery data by using domain-knowledge-encoded routing in BatteryMoE. The Hard Encoder routes battery inputs to specialized expert networks based on physically meaningful categories (cathode, anode, format, temperature), isolating heterogeneous data and allowing specialized sub-models to learn complex mappings from cycling patterns to lifetime within each group.

### Mechanism 2: LLM-based soft encoding
A Soft Encoder using an LLM provides rich, continuous representations of aging conditions, enabling the model to leverage nuanced interactions between factors. A text prompt describing all 10 aging factors is embedded by Llama-3.1-8b-Instruct, with the resulting vector providing a prior on how factors combine to affect degradation, guiding the Battery gate to assign weights to specialized experts.

### Mechanism 3: CyclePatch tokenization with transformer architecture
PBT models the sequential nature of battery degradation cycles using a transformer-based architecture with BatteryMoE-CyclePatch tokenization. Each charge-discharge cycle's data is tokenized into a single token embedding, then processed by intra- and inter-cycle encoders using transformer self-attention to model degradation progression over time, with standard feed-forward layers replaced by BatteryMoE modules.

## Foundational Learning

- **Foundation Models (FMs)**: PBT is presented as a foundation model, shifting from task-specific training to pretraining on diverse datasets for general-purpose representations. This paradigm is key to achieving universal lifetime prediction.
  - Quick check: Can you explain how a foundation model differs from a standard supervised learning model trained on a single dataset?

- **Mixture-of-Experts (MoE)**: The core innovation is BatteryMoE, adapting MoE concepts from large language models. Understanding MoE (experts, gates, routing) is essential to grasp how PBT handles heterogeneous data.
  - Quick check: In a standard MoE layer, what is the role of the 'gating network'?

- **Transfer Learning**: PBT's primary value is transferring knowledge to new, unseen battery types with limited data. The paper demonstrates this via fine-tuning and adapter tuning.
  - Quick check: What is the difference between 'fine-tuning' and 'adapter tuning' as described in the paper's transfer learning experiments?

## Architecture Onboarding

- **Component map**: Input Data + Prompt -> Hard/Soft Encoding (Gating) -> Cycle Patching (Tokenization) -> Transformer Encoding (MoE Layers) -> Projection Head -> Predicted Cycle Life

- **Critical path**: The flow is: `Input Data + Prompt` -> `Hard/Soft Encoding (Gating)` -> `Cycle Patching (Tokenization)` -> `Transformer Encoding (MoE Layers)` -> `Projection Head` -> `Predicted Cycle Life`. The most critical and novel path is the combined Hard/Soft encoding that controls the mixture-of-experts.

- **Design tradeoffs**:
  - Hard vs. Soft Routing: Hard routing ensures physically grounded expert selection but may lack nuance; soft routing captures interactions but relies on LLM's "prior" being correct. PBT uses both.
  - Data Efficiency vs. Specificity: Foundation model approach trades task-specific simplicity for massive data efficiency and generalization potential.
  - Complexity: Architecture is significantly more complex than previous models, which may make training and debugging harder.

- **Failure signatures**:
  - Poor performance on new dataset: Check if hard-coded factors exist in pretraining data; verify prompt accurately describes new battery.
  - High variance in results: Model performance can be sensitive to data split and hyperparameters; ensure multiple runs with different seeds.
  - Failure under extreme data scarcity: Requires some target-domain data for transfer learning; cannot perform zero-shot prediction on fundamentally different chemistries.

- **First 3 experiments**:
  1. Reproduce Transfer Learning Result: Fine-tune pretrained PBT on small subset (9 batteries) of target dataset to replicate performance gain over baselines.
  2. Ablate Hard/Soft Encoders: Run model with only Hard Encoder, then only Soft Encoder, to understand relative contributions and validate ablation study.
  3. Test on Truly Novel Condition: Introduce battery with cathode material not in pretraining set to probe generalizability limits and soft encoder robustness.

## Open Questions the Paper Calls Out

- **Unified lifetime metric**: Integrating field data with laboratory development data requires a unified lifetime metric because field operations involve varying conditions and partial cycling, making standard cycle life calculations impractical. Currently no broadly accepted metric correlates degradation across variable real-world usage profiles.

- **Battery-specific PEFT techniques**: Exploring battery-specific parameter-efficient transfer learning techniques like prompt tuning or LoRA could potentially achieve superior performance with fewer trainable parameters compared to standard fine-tuning and adapter tuning.

- **Latent proprietary factors**: Incorporating latent proprietary factors (e.g., electrolyte recipes, stacking stress) into the Hard Encoder could resolve performance deviations observed in the CALCE dataset, as these factors are absent from public datasets used in this study.

## Limitations

- Generalizability to truly novel battery chemistries may be limited, as hard-coded routing may fail for chemistries with fundamentally different degradation mechanisms
- Expert network specifications within BatteryMoE are not fully detailed, with exact layer dimensions and activations per expert type inferred rather than explicitly stated
- Soft encoder reliability depends on LLM embeddings capturing battery aging physics, a strong assumption that hasn't been validated against domain experts

## Confidence

- **High Confidence**: PBT significantly outperforms existing models on LIB datasets; achieves strong performance on LIB batteries with limited data; BatteryMoE architecture is novel and effective
- **Medium Confidence**: PBT generalizes well to Na-ion and Zn-ion batteries; soft encoder meaningfully contributes to performance; model reduces data acquisition needs by ~99%
- **Low Confidence**: PBT can be directly applied to battery management systems in production; performance is robust to all data quality issues; hard-coded routing will remain effective as new chemistries emerge

## Next Checks

1. Validate expert network architecture by re-implementing BatteryMoE with exact FFN dimensions and activations per expert type, then running ablation studies comparing different expert architectures on pretraining data

2. Stress-test the soft encoder by conducting controlled experiments where the LLM prompt is deliberately modified to exclude known critical aging factors, measuring degradation in PBT's performance to quantify sensitivity

3. Test on truly novel chemistry by acquiring or simulating data for battery chemistry with cathode/anode material not present in any pretraining datasets, then fine-tuning PBT on small number of cycles to assess hard-coded routing robustness to extreme domain shifts