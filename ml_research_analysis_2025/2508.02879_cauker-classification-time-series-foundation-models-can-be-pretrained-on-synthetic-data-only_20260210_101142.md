---
ver: rpa2
title: 'CauKer: classification time series foundation models can be pretrained on
  synthetic data only'
arxiv_id: '2508.02879'
source_url: https://arxiv.org/abs/2508.02879
tags:
- time
- series
- data
- synthetic
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CauKer, a synthetic data generation pipeline
  for pretraining time series foundation models (TSFMs) on classification tasks. CauKer
  combines Gaussian Process kernel composition with Structural Causal Models to generate
  diverse, causally coherent time series with realistic trends, seasonality, and nonlinear
  interactions.
---

# CauKer: classification time series foundation models can be pretrained on synthetic data only

## Quick Facts
- arXiv ID: 2508.02879
- Source URL: https://arxiv.org/abs/2508.02879
- Reference count: 32
- Pretraining TSFMs on CauKer synthetic data achieves SOTA zero-shot accuracy, matching real-world datasets while using up to 20× less data

## Executive Summary
CauKer is a synthetic data generation pipeline for pretraining time series foundation models (TSFMs) on classification tasks. It combines Gaussian Process kernel composition with Structural Causal Models to produce diverse, causally coherent time series with realistic trends, seasonality, and nonlinear interactions. The approach addresses the challenge of sample-efficient pretraining without requiring large real-world datasets. Experiments show models pretrained on CauKer achieve state-of-the-art zero-shot classification performance, matching accuracy of models trained on real-world datasets while using up to 20× less data.

## Method Summary
CauKer generates synthetic time series by first sampling and composing Gaussian Process kernels to create diverse temporal patterns, then propagating these through a randomly generated Structural Causal Model with nonlinear activations. The process uses a kernel bank of 36 variants (RBF, ExpSineSquared, RationalQuadratic, etc.) combined via additive and multiplicative operations, along with mean functions that preserve discriminative level/trend information. The resulting time series exhibit realistic trends, seasonality, and nonlinear interactions while maintaining classification-relevant structure. This synthetic data enables pretraining of TSFMs that generalize effectively to real-world classification tasks.

## Key Results
- Models pretrained on CauKer achieve SOTA zero-shot accuracy on UCR benchmark, matching real-world dataset performance
- CauKer exhibits clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters)
- CauKer requires up to 20× less data than real-world datasets for equivalent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Composing Gaussian Process kernels creates diverse, temporally coherent patterns that capture realistic seasonality, trends, and periodicity.
- Mechanism: A kernel bank of 36 variants is randomly sampled and combined via additive and multiplicative operations, defining GP priors that generate smooth, structured root signals.
- Core assumption: Time series classification requires models to recognize canonical temporal motifs; exposure to systematically varied synthetic motifs builds transferable representations.
- Evidence anchors:
  - [abstract]: "CauKer combines Gaussian Process (GP) kernel composition... to generate diverse, causally coherent time series with realistic trends, seasonality, and nonlinear interactions."
  - [Section 3.2]: Steps 1-3 describe kernel sampling, composition, and root node generation from GP(μᵢ, κ*ᵢ).
  - [Appendix C.2]: Figure 7 visualizes covariance matrices and sample paths from base kernels.

### Mechanism 2
- Claim: Propagating GP-generated signals through a Structural Causal Model (SCM) introduces nonlinear, causally coherent dependencies that enrich classification-relevant structure.
- Mechanism: A randomly generated DAG assigns each edge a nonlinear activation (ReLU, sigmoid, sinusoidal, modulo, LeakyReLU). Child nodes compute tvⱼ = W × [ϕ(e·)(tu·)] + b, blending parent signals through learned linear combinations and edge-wise nonlinearities.
- Core assumption: Classification benefits from discriminative structure beyond smooth temporal motifs; SCM edges create structured variation that mimics real-world causal dynamics.
- Evidence anchors:
  - [Section 3.2]: Steps 4-5 formalize activation sampling and DAG propagation.
  - [Appendix C.2]: Lists six activation types and their roles in creating complex dependencies.
  - [Section 4.1]: Figure 2 shows hierarchical clustering of CauKer-generated series reveals clear cluster blocks and anomalies.

### Mechanism 3
- Claim: Non-zero mean functions in GP priors provide discriminative cues that improve classification-oriented pretraining.
- Mechanism: Mean bank includes linear, exponential, and anomaly (sparse spike) functions. These are combined with kernels to shift GP samples away from zero-centered baselines, preserving level/trend information as class-relevant features.
- Core assumption: Classification tasks often distinguish series by mean level or trend direction; forecasting-focused generators discard this signal.
- Evidence anchors:
  - [Section 3.2]: "Our task calls for retaining the mean level itself as a discriminative cue."
  - [Table 1]: Mean+KernelSynth (78.20%) outperforms KernelSynth (77.70%) for Mantis; gap larger for MOMENT (72.56% vs 69.31%).

### Mechanism 4
- Claim: Structured synthetic data exhibits predictable scaling laws for both dataset size and model capacity, unlike real-world benchmarks with irregular scaling.
- Mechanism: CauKer's controlled generative process produces data with learnable generative structure. As dataset size (10K→10M) or model capacity (1M→783M params) increases, test accuracy improves monotonically until saturation.
- Core assumption: Real-world time series corpora lack diversity or have domain mismatch with evaluation benchmarks, causing flat/noisy scaling; synthetic data with controlled diversity enables systematic generalization gains.
- Evidence anchors:
  - [abstract]: "CauKer-generated datasets exhibit clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters), unlike real-world datasets."
  - [Figure 3 + Table 3]: CauKer curves show smooth accuracy gains; UEA curves are flat or irregular.

## Foundational Learning

- Concept: **Gaussian Processes (GP)**
  - Why needed here: Core generative engine; kernels define covariance structure (smoothness, periodicity) of synthetic time series.
  - Quick check question: Can you explain how an RBF kernel's length-scale parameter affects the smoothness of sampled paths?

- Concept: **Structural Causal Models (SCM)**
  - Why needed here: Provides the DAG-based propagation mechanism that adds nonlinear, causally structured dependencies to GP root signals.
  - Quick check question: How does a child node's value in an SCM differ from a simple feedforward neural network layer?

- Concept: **Zero-Shot Classification with Frozen Encoders**
  - Why needed here: Evaluation paradigm; TSFM F(x) produces embeddings, a lightweight classifier (RF/SVM) is trained on embeddings without updating F.
  - Quick check question: Why is the encoder frozen during downstream evaluation, and what does this test?

- Concept: **Self-Supervised Learning (Contrastive vs. Masked)**
  - Why needed here: Two TSFMs (Mantis—contrastive, MOMENT—masked) are pretrained on CauKer data; understanding both paradigms is essential for replication.
  - Quick check question: What is the fundamental difference between contrastive loss and masked reconstruction loss?

## Architecture Onboarding

- Component map:
  - Kernel Bank (K) -> GP Sampler -> Root Signals -> DAG Generator -> Propagation Engine -> Synthetic Dataset
  - Mean Bank (M) -> GP Sampler (with K) -> Root Signals
  - Activation Bank (A) -> DAG Propagation Engine

- Critical path:
  1. Sample K kernels → compose via random +/× → define κ*
  2. Sample M mean functions → repeat step 1 for each → define M GP priors
  3. Sample root signals tᵢ ~ GP(μᵢ, κ*ᵢ)
  4. Generate random DAG with M root nodes, E edges
  5. Sample E activations → assign to edges
  6. Propagate: for each non-root node, compute tᵥⱼ = W × [ϕ(e·)(tu·)] + b
  7. Interpolate node outputs to fixed length L → output synthetic dataset

- Design tradeoffs:
  - **Diversity vs. Coherence**: Larger kernel/activation banks increase pattern diversity but may generate harder-to-learn signals, slowing convergence.
  - **DAG Complexity vs. Compute**: Deeper/wider DAGs produce richer interactions but require more forward passes; paper uses random sparse DAGs.
  - **Mean Function Inclusion**: Essential for classification (discriminative level/trend cues) but inappropriate for forecasting (zero-mean assumption).

- Failure signatures:
  - **Mode Collapse**: If kernel composition always yields similar κ*, generated series lack diversity; check pairwise DTW distance distribution.
  - **Over-Smoothness**: Excessive RBF/RationalQuadratic dominance produces overly smooth series; increase WhiteKernel or anomaly mean frequency.
  - **Scaling Law Absence**: If accuracy plateaus before 10M samples, dataset diversity may be insufficient; expand kernel/activation banks.
  - **Training Instability**: If contrastive loss (Mantis) diverges, reduce learning rate or increase temperature T; masked loss (MOMENT) is more stable.

- First 3 experiments:
  1. **Ablation: Mean vs. Zero-Mean**: Generate 100K series with Mean+KernelSynth vs. KernelSynth; train Mantis; compare UCR accuracy to verify Table 1 gains (~0.5% for Mantis, ~3% for MOMENT).
  2. **Scaling Law Validation**: Generate CauKer datasets at 10K, 50K, 100K, 1M, 10M; train Mantis-8M; plot accuracy vs. log(dataset size); expect monotonic increase with diminishing returns.
  3. **DAG Depth Analysis**: Fix dataset size (100K), vary DAG depth (1–5 layers); train Mantis; analyze whether deeper causal structures improve transfer or cause overfitting to synthetic artifacts.

## Open Questions the Paper Calls Out
None

## Limitations
- The kernel bank composition is manually curated; expanding it risks introducing unstable patterns that could break transfer.
- SCM activation diversity (six types) is limited compared to the infinite space of possible causal dynamics.
- No ablation on DAG depth/width or on kernel/activation frequency hyperparameters.
- Zero-shot classification assumes fixed encoder evaluation; few-shot or fine-tuning scenarios are untested.
- Real-world dataset diversity is not quantified; scaling law gaps could stem from dataset mismatch rather than synthetic advantage.

## Confidence
- **High confidence** in kernel composition generating diverse temporal motifs (well-established GP theory + clear visual evidence).
- **Medium confidence** in SCM propagation enriching discriminative structure (SCM concept is sound, but edge activations not deeply validated).
- **Medium confidence** in mean functions improving classification (ablation shows gains, but no comparison to real-world mean-inclusive generators).
- **High confidence** in CauKer scaling laws (consistent monotonic curves, unlike noisy real-world benchmarks).

## Next Checks
1. **DAG Depth Ablation**: Generate 100K samples with DAG depths 1–5; train Mantis; plot accuracy vs. depth to determine optimal causal complexity.
2. **Kernel Bank Expansion**: Add 10 more kernels (e.g., periodic, Matérn) to K; regenerate 100K CauKer; test whether UCR accuracy improves or if synthetic artifacts dominate.
3. **Downstream Task Diversity**: Evaluate zero-shot performance on non-UCR datasets (e.g., medical, sensor) to confirm cross-domain transferability beyond the paper's benchmark scope.