---
ver: rpa2
title: Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via
  LangGraph
arxiv_id: '2502.18465'
source_url: https://arxiv.org/abs/2502.18465
tags:
- code
- langgraph
- large
- generation
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study proposes a LangGraph-based automated bug fixing system
  that combines large language models with vector database memory for iterative code
  repair. The system implements a four-step workflow: code generation using GLM-4-Flash,
  execution validation, bug repair through ChromaDB''s memory search, and iterative
  updates until functional correctness.'
---

# Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph

## Quick Facts
- arXiv ID: 2502.18465
- Source URL: https://arxiv.org/abs/2502.18465
- Reference count: 7
- Primary result: LangGraph-based system combines LLM reasoning with vector database memory to iteratively fix runtime bugs through execution-feedback loops

## Executive Summary
This study presents a LangGraph-based automated bug fixing system that integrates large language models with vector database memory for iterative code repair. The system implements a four-step workflow: code generation using GLM-4-Flash, execution validation, bug repair through ChromaDB's memory search, and iterative updates until functional correctness. Two case studies demonstrate the system's effectiveness: calculating triangle areas (no bugs encountered) and implementing division with zero-divisor error handling (successfully fixed through conditional checks). The approach leverages LangGraph's state management for precise control and GLM-4-Flash's reasoning capabilities for code modification.

## Method Summary
The proposed system follows a LangGraph workflow with nodes for code generation, execution, bug detection, memory operations, and repair. GLM-4-Flash generates Python functions from natural language prompts, which are executed in a sandbox environment. When errors occur, ChromaDB searches for similar historical bug patterns, which contextualize the LLM's repair prompt. The system iteratively refines code through execution-feedback loops until successful execution or a stopping condition. The unified state object tracks progress across nodes, enabling conditional routing between execution and repair cycles.

## Key Results
- Successfully generated functional code for triangle area calculation without encountering bugs
- Automatically fixed ZeroDivisionError in division function through iterative refinement and conditional check addition
- Demonstrated effective state management using LangGraph's graph-based orchestration for multi-step repair workflows

## Why This Works (Mechanism)

### Mechanism 1
Iterative execution-feedback loops enable runtime error resolution through LLM-guided code modification. Generated code executes in a sandbox; errors are captured and fed back to the LLM with context. The model produces a repaired version, which re-enters execution. This cycle repeats until success or a stopping condition. Core assumption: The LLM can correctly interpret error messages and generate valid patches within few iterations.

### Mechanism 2
Vector-based memory retrieval improves repair quality by surfacing similar historical bug patterns. ChromaDB stores bug reports and fix patterns as embeddings. On error, the system performs semantic search to retrieve relevant prior cases, which contextualize the LLM's repair prompt. Core assumption: Prior bug-fix patterns transfer to new cases via semantic similarity.

### Mechanism 3
Graph-based state orchestration provides precise control over multi-step repair workflows. LangGraph compiles a directed graph of nodes with a unified state object passed along edges. Conditional routing determines whether to exit or iterate. Core assumption: The workflow is sufficiently regular that a fixed graph structure captures the needed control flow.

## Foundational Learning

- **Graph-based workflow orchestration (DAGs, state machines)**: Why needed here - LangGraph's value proposition is structuring agent workflows as compiled graphs with shared state; understanding nodes, edges, and conditional routing is prerequisite. Quick check: Can you sketch a 3-node graph with a conditional loop back to node 2 based on an error flag?

- **Vector embeddings and semantic search**: Why needed here - ChromaDB's role depends on embedding-based similarity; without this, memory retrieval becomes opaque. Quick check: Given two error messages, would you expect their embeddings to be similar if they share the same exception type but different variable names?

- **LLM code generation and self-repair limitations**: Why needed here - The system assumes the LLM can patch code from error context; understanding failure modes (syntax errors, logic errors, context window limits) is essential. Quick check: If an LLM receives a ZeroDivisionError traceback, what additional context would improve its patch quality beyond the traceback alone?

## Architecture Onboarding

- **Component map**: User prompt → code_generation → code_execution → (if bug) bug_issue → memory_search → memory_filter → memory_create → memory_update → code_update → code_repair → loop back to code_execution; (if no bug) → end
- **Critical path**: 1) User prompt → code_generation node → GLM-4-Flash generates function. 2) code_execution node → sandbox runs code → success → END, or failure → bug_issue node. 3) bug_issue → memory_search → memory_filter → retrieve similar bugs. 4) Retrieved context + error → GLM-4-Flash produces repair → code_update node. 5) code_repair node updates state → loop back to code_execution. 6) Repeat until execution succeeds or iteration limit.
- **Design tradeoffs**: Single-agent vs. multi-agent (single-agent used; multi-agent could handle complex repos but increases coordination cost); Local vs. cloud LLM (GLM-4-Flash cloud-based; local models reduce latency but may have weaker reasoning); Memory scope (storing all bugs risks retrieval noise; filtering heuristics may improve precision but reduce recall).
- **Failure signatures**: Infinite loop (code never passes execution; repair introduces new bugs each iteration); Silent logic errors (code executes without exceptions but produces incorrect results); Retrieval drift (memory returns irrelevant prior bugs, misleading repair); State corruption (LangGraph state object grows unbounded or contains stale entries).
- **First 3 experiments**: 1) Baseline loop without memory (disable ChromaDB retrieval; measure repair success rate and iterations on division-by-zero case and 5 similar simple functions). 2) Memory ablation (compare full memory vs. search-only vs. no memory; track success rate, time, and iteration count). 3) Error type stratification (test on 3 error categories: syntax, runtime, logic/edge-case; measure which categories system handles best and where it fails most).

## Open Questions the Paper Calls Out
- Can the proposed LangGraph system generalize to repository-level bug fixing with multiple files and dependencies? (The experimental validation is limited to two simple case studies, whereas Related Work discusses SWE-bench involving complex repository contexts)
- How effectively does the agent handle bugs involving multiple error locations in the stack compared to single-point failures? (The case studies only demonstrate fixing single, specific runtime errors)
- Does introducing third-party testing or formal verification functions significantly reduce the recurrence of bugs compared to the current iterative loop? (The current system relies solely on internal execution validation)

## Limitations
- Lacks critical implementation details: exact prompt templates, state schema definitions, and ChromaDB configuration parameters
- Two case studies are minimal (simple arithmetic functions), leaving unclear whether the approach scales to complex, multi-file bugs
- Claim that "vector-based memory retrieval improves repair quality" is weakly supported, as corpus contains no direct evidence of vector memory's effectiveness for bug fixing

## Confidence
- **High**: Iterative execution-feedback loops can resolve simple runtime errors (validated by division-by-zero case)
- **Medium**: Graph-based state orchestration provides precise control (mechanism plausible but not empirically validated against alternatives)
- **Low**: Vector-based memory retrieval improves repair quality (no ablation study provided; similarity between bug patterns and fix applicability not demonstrated)

## Next Checks
1. **Memory Ablation Study**: Compare bug-fixing success rates and iteration counts with full memory (search+filter+create+update), search-only, and no memory on 10+ diverse error cases
2. **Error Type Analysis**: Test on 15 errors across syntax, runtime, and logic/edge-case categories; measure which error types the system handles successfully versus where it fails
3. **Prompt Template Evaluation**: Systematically vary GLM-4-Flash repair prompts (with/without memory context, different error descriptions) to identify optimal prompt structure for different bug categories