---
ver: rpa2
title: 'confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot
  NAS Methods'
arxiv_id: '2507.16533'
source_url: https://arxiv.org/abs/2507.16533
tags:
- search
- darts
- methods
- architecture
- supernet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces confopt, a library for implementing and evaluating
  gradient-based one-shot NAS methods. It addresses the challenges of benchmarking
  and evaluation in NAS, particularly the overreliance on the DARTS benchmark and
  fragmented implementations.
---

# confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot NAS Methods

## Quick Facts
- **arXiv ID**: 2507.16533
- **Source URL**: https://arxiv.org/abs/2507.16533
- **Reference count**: 40
- **Primary result**: Current NAS evaluations are unreliable due to benchmark overfitting; method rankings differ substantially across search space configurations.

## Executive Summary
This paper introduces confopt, a library designed to address key challenges in benchmarking and evaluating gradient-based one-shot Neural Architecture Search (NAS) methods. The authors argue that the current overreliance on the DARTS benchmark and fragmented implementations lead to misleading conclusions about method performance. confopt provides a minimal API for integrating new search spaces and enables the decomposition of NAS optimizers into core modular components. Using confopt, they create DARTS-Bench-Suite, a collection of nine benchmarks derived from the DARTS search space, and evaluate seven NAS optimizers across these benchmarks. The key finding is that the rankings of NAS methods differ substantially across the different benchmarks, highlighting the need for more comprehensive and rigorous evaluation protocols.

## Method Summary
The confopt library implements gradient-based one-shot NAS methods with a focus on modular decomposition and rigorous evaluation. It provides a minimal API to integrate new search spaces and distinguishes between algorithmic components (e.g., sampling strategies) and supernet mutations (e.g., partial connections). The evaluation protocol uses a 50/50 data split: the first half trains the supernet, and the second half retrains the discovered architecture. DARTS-Bench-Suite includes nine variants (Wide, Deep, Single-Cell) of the DARTS search space to test method robustness. The library supports standard metrics and gradient statistics logging to analyze failure modes like discretization gaps and skip-connect dominance.

## Key Results
- Rankings of NAS methods differ substantially across DARTS-Bench-Suite variants, with Wide and Deep benchmarks showing moderate anti-correlation.
- The modular API enables fair comparison by isolating algorithmic contributions from supernet mutations.
- The 50/50 data split evaluation protocol reveals overfitting in standard DARTS evaluation practices.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing NAS optimizers into modular components (samplers, regularizers, pruning) allows for the isolation of algorithmic contributions and fairer comparisons.
- **Mechanism**: The `confopt` library separates "algorithmic components" from "modifications to the supernet," enabling controlled ablation studies by combining specific samplers with supernet mutations.
- **Core assumption**: The performance of a NAS method can be attributed to the specific combination of these modular parts, and the interfaces do not constrain the optimization dynamics.
- **Evidence anchors**: [section 4.1] Distinguishes between algorithmic components and supernet mutations.
- **Break condition**: If a novel NAS method requires tight coupling between the sampler and the weight optimization that cannot be expressed through the discrete API hooks, the decomposition will fail to capture the method's true behavior.

### Mechanism 2
- **Claim**: Splitting the training data into disjoint sets for "search" and "evaluation" creates a more rigorous assessment of an architecture's intrinsic generalization capability.
- **Mechanism**: By strictly partitioning the data (e.g., 50% for search, 50% for retraining), the evaluation protocol forces the discovered architecture to generalize to unseen samples during the retraining phase.
- **Core assumption**: The optimal architecture found on the search-split is representative enough to be worth retraining on the evaluation-split, and the dataset is large enough to survive a 50% reduction without degrading model capacity.
- **Evidence anchors**: [section 5.1] Describes the 50/50 data split for search and retraining.
- **Break condition**: For small datasets, halving the training data may induce significant noise or underfitting, making it impossible to distinguish between a bad architecture and a data-starved model.

### Mechanism 3
- **Claim**: Varying the search space configuration (specifically width, depth, and operation sets) exposes high instability in NAS method rankings, which are hidden when evaluating solely on the standard DARTS benchmark.
- **Mechanism**: The authors introduce DARTS-Bench-Suite, comprising variants like "DARTS-Wide," "DARTS-Deep," and "Single-Cell." When methods are evaluated across these variants, their relative performance rankings change substantially.
- **Core assumption**: The observed ranking instability is due to the optimizer's sensitivity to search space structure rather than just random noise or insufficient training epochs.
- **Evidence anchors**: [section 6.2] Notes that rankings differ substantially across settings, particularly between Wide and Deep benchmarks.
- **Break condition**: If the hyperparameter grid used for retraining is not diverse enough, the observed instability might simply be an artifact of specific hyperparameter biases rather than architectural flaws.

## Foundational Learning

- **Concept: Differentiable Architecture Search (DARTS)**
  - **Why needed here**: The library is explicitly designed for "gradient-based one-shot NAS," with DARTS being the formative method. Understanding the bi-level optimization (weights vs. architecture parameters) is required to configure the `Profile` classes.
  - **Quick check question**: Can you explain why DARTS requires a "validation loss" to update the architecture parameters, distinct from the "training loss" used for weights?

- **Concept: Discretization Gap**
  - **Why needed here**: A primary motivation for the library is addressing "failure modes" like the discretization gap, where a high-performing continuous supernet collapses when discretized (often due to skip connections).
  - **Quick check question**: Why might an operation like a "skip connection" have an artificially high architectural weight during the continuous search phase, and how does this hurt the final discrete model?

- **Concept: Proxy vs. Target Models**
  - **Why needed here**: The paper critiques the standard practice of training a small "proxy" supernet to find an architecture for a larger "target" model. Understanding this distinction is key to using the library's "DARTS-Bench-Suite," which aligns these two.
  - **Quick check question**: What is the "poor rank correlation" problem mentioned in the paper when moving from a proxy supernet to a target network?

## Architecture Onboarding

- **Component map**: `confopt.profile` -> `confopt.train.Experiment` -> Supernet Wrappers
- **Critical path**:
  1. **Select a Profile:** Instantiate a pre-configured profile (e.g., `GDASProfile`) or create a custom one.
  2. **Configure Search:** Set trainer presets (epochs, learning rates) and select the search space (e.g., `SearchSpaceType.DARTS`).
  3. **Run Experiment:** Call `experiment.train_supernet(profile)` to execute the search and logging.
- **Design tradeoffs**:
  - **Minimal API vs. Control**: The library uses a minimal API to lower integration friction, but advanced users must ensure their custom search spaces implement the required "optional APIs" to unlock metrics like gradient statistics.
  - **Standardization vs. Flexibility**: By standardizing the evaluation pipeline (e.g., data splits), the library enforces rigor but may require users to adapt their existing workflows to fit the `Experiment` class structure.
- **Failure signatures**:
  - **Performance Collapse**: Discovered architectures dominated by parameterless skip connections (detected via the library's logging of operation selection frequency).
  - **Rank Mismatch**: A method performing well on standard DARTS but failing on "Deep" or "Single-Cell" variants indicates overfitting to the specific depth/width of the standard benchmark.
- **First 3 experiments**:
  1. **Baseline Reproduction:** Run `DARTSProfile` on the standard DARTS search space to verify the library produces results consistent with the original paper.
  2. **Optimizer Ablation:** Run `DrNASProfile` on the same search space and compare the "gradient statistics" or "operation selection" metrics against DARTS to visualize the difference in search stability.
  3. **Stress Test:** Evaluate a chosen optimizer (e.g., PC-DARTS) across the "Wide" vs. "Deep" variants in the DARTS-Bench-Suite to check if its performance is robust to depth/width changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed inconsistencies in NAS method rankings persist when applied to diverse, real-world datasets beyond CIFAR-10?
- Basis in paper: [explicit] The limitations section states that more generalizable findings could be obtained by introducing "more diverse, real-world datasets" rather than relying solely on standard datasets like CIFAR.
- Why unresolved: The authors' experiments were restricted to the CIFAR dataset to demonstrate the proof of concept within the DARTS-Bench-Suite.
- What evidence would resolve it: Evaluation of the seven NAS optimizers across the DARTS-Bench-Suite using datasets such as ImageNet or domain-specific tabular data.

### Open Question 2
- Question: Can search spaces be designed that are inherently robust to the "overfitting" and "saturation" issues currently observed in the DARTS search space?
- Basis in paper: [explicit] The conclusion explicitly calls on the community to "design more robust search spaces for evaluating NAS methods" to prevent methods from overfitting their recipes to specific challenges.
- Why unresolved: Current benchmarks, including the new DARTS-Bench-Suite, are still derivatives of the original DARTS space, which suffers from discretization gaps and skip-connect dominance.
- What evidence would resolve it: The creation and validation of a search space where method rankings are stable and improvements are statistically significant outside the margin of noise.

### Open Question 3
- Question: What theoretical mechanisms drive the moderate anti-correlation of NAS method rankings between architectural variants (e.g., Wide vs. Deep)?
- Basis in paper: [inferred] Section 6.2 notes that rankings are "moderately anti-correlated between Wide + All Skip and Deep + No Skip" and highlights this lack of stability, but does not propose a theoretical cause.
- Why unresolved: The paper demonstrates the existence of rank disparity through empirical results but leaves the underlying interaction between optimizer dynamics and macro-architecture configurations unexplained.
- What evidence would resolve it: A theoretical analysis or ablation study linking specific optimizer components (e.g., regularization terms, sampling strategies) to performance sensitivity on depth versus width variations.

## Limitations

- The library's modular decomposition may not capture methods requiring tight coupling between sampler and weight optimization.
- The evaluation protocol's 50/50 data split could induce underfitting on small datasets, limiting its applicability.
- The DARTS-Bench-Suite, while more diverse than standard DARTS, is still derived from the original search space, which has known failure modes like discretization gaps.

## Confidence

- **High confidence**: The existence of DARTS-Bench-Suite and the evaluation protocol with disjoint data splits are well-documented and reproducible.
- **Medium confidence**: The claim that rankings differ substantially across benchmarks is supported but lacks statistical rigor.
- **Medium confidence**: The modular API design's ability to fairly compare methods assumes that discrete interfaces do not constrain optimization dynamics.

## Next Checks

1. **Statistical Validation**: Perform significance testing (e.g., paired t-tests) on the ranking differences between Wide and Deep benchmarks to quantify whether observed instabilities are statistically meaningful.
2. **Modular Flexibility Test**: Implement a NAS method requiring tight coupling between sampler and weight updates (e.g., methods with interdependent update rules) to verify whether the modular API can express its dynamics without loss of fidelity.
3. **Data Split Robustness**: Repeat the evaluation protocol with varying data split ratios (e.g., 60/40 instead of 50/50) to confirm that the observed instability in rankings is not an artifact of the specific 50% split used.