---
ver: rpa2
title: Transformer-Based Approach for Automated Functional Group Replacement in Chemical
  Compounds
arxiv_id: '2601.07930'
source_url: https://arxiv.org/abs/2601.07930
tags:
- functional
- group
- chemical
- molecular
- molecule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a transformer-based two-stage method for functional
  group replacement in chemical compounds. By leveraging SMIRKS-based representations
  and ChEMBL data, the model generates transformation strings that specify both the
  removal and addition of functional groups.
---

# Transformer-Based Approach for Automated Functional Group Replacement in Chemical Compounds

## Quick Facts
- arXiv ID: 2601.07930
- Source URL: https://arxiv.org/abs/2601.07930
- Authors: Bo Pan; Zhiping Zhang; Kevin Spiekermann; Tianchi Chen; Xiang Yu; Liying Zhang; Liang Zhao
- Reference count: 7
- Introduces transformer-based two-stage method for functional group replacement using SMIRKS representations

## Executive Summary
This work presents a transformer-based approach for automated functional group replacement in chemical compounds. The method uses a two-stage generation process that first predicts which functional group to remove, then predicts what to append, ensuring strict substructure-level modifications. By leveraging SMIRKS-based transformation strings and ChEMBL data, the model generates chemically valid transformations that outperform existing approaches in covering known transformations (up to 79% for certain molecule groups) while exploring novel chemical spaces. The approach demonstrates flexibility for both model-suggested and user-specified replacements.

## Method Summary
The method employs a transformer-based two-stage generation process where the model first predicts functional group removal and then predicts replacement. It uses ChemT5 as the base encoder-decoder model, fine-tuned on 2 million Matched Molecular Pairs (MMPs) extracted from ChEMBL. The model generates SMIRKS transformation strings rather than directly outputting new molecules, enabling interpretable and transferable transformation rules. The approach supports both automated generation and user-specified functional group replacements through conditional decoding.

## Key Results
- Achieves up to 79% coverage of known transformations for certain molecule groups
- Generates chemically valid, diverse, and novel compounds at scale
- Outperforms existing approaches in exploring novel chemical spaces while maintaining chemical validity
- Demonstrates flexibility for both model-suggested and user-specified functional group replacements

## Why This Works (Mechanism)

### Mechanism 1
Sequential two-stage generation (removal → replacement) enforces strict substructure-level modifications better than one-shot molecule translation. By decomposing functional group replacement into two autoregressive steps, the model guarantees that only the functional group level is modified while preserving the core molecular scaffold. The output is a SMIRKS transformation string that encodes both operations atomically.

### Mechanism 2
SMIRKS-based transformation strings as targets enable the model to learn generalized transformation rules rather than memorizing specific molecule pairs. Instead of mapping SMILES→SMILES, the model maps SMILES→SMIRKS, forcing it to learn transferable substitution rules applicable across different molecular contexts through abstract transformation patterns with wildcard atom mappings.

### Mechanism 3
Chemically pretrained encoder-decoder (ChemT5) provides inductive biases for molecular string understanding, improving sample efficiency on transformation learning. ChemT5 is pretrained on chemical text and molecular representations, encoding domain knowledge about SMILES syntax and chemical semantics. Fine-tuning on 2M MMP pairs transfers this knowledge to the specific task of transformation prediction, enabling high validity without training from scratch.

## Foundational Learning

- **SMILES and SMIRKS notation**: The model operates on string representations of molecules (SMILES) and transformations (SMIRKS). Understanding token-level encoding is essential for debugging invalid outputs and interpreting attention patterns.
  - *Quick check question*: Given the SMIRKS string `[*:1]C(=O)O>>[*:1]C(=O)N`, what transformation does this represent? (Answer: Carboxylic acid to amide conversion at a wildcard attachment point.)

- **Matched Molecular Pairs (MMPs)**: Training data is constructed from MMPs—pairs of molecules differing by a localized structural change. Understanding MMP extraction parameters (core ≤50 atoms, R-group ≤13 atoms, R-group ratio <0.33) is critical for data curation and filtering decisions.
  - *Quick check question*: Why might highly promiscuous molecules (many transformations) need to be filtered? (Answer: To prevent them from dominating the training distribution and biasing the model toward specific scaffolds.)

- **Beam search in autoregressive decoding**: Inference uses beam search (size 100–200) to generate multiple candidate transformations. Understanding the trade-off between exploration (larger beams, more novel compounds) and precision (smaller beams, higher known-compound recovery) is essential for application-specific tuning.
  - *Quick check question*: At k=100, Mol2Trans recovers ~73.77% existing compounds. What does the remaining ~26% represent? (Answer: Novel compounds not in the training set—exploration of new chemical space.)

## Architecture Onboarding

- **Component map**: SMILES input → ChemT5 encoder → Autoregressive decoder → SMIRKS transformation string → Apply SMIRKS → Validity check → Product molecule(s)
- **Critical path**: SMILES input → Encoder representation → Decoder generates SMIRKS → Apply SMIRKS transformation → Validity check → Return product molecule(s). For user-specified mode, decoder is forced to begin with the specified functional group pattern.
- **Design tradeoffs**:
  - Mol2Trans vs. Mol2Mol: Mol2Trans (SMIRKS targets) sacrifices slight top-1 "exist" rate (96.92% vs. 99.49%) for better exploration at larger k (73.77% vs. 62.44% at k=100) and explicit transformation interpretability
  - Beam size: Larger beams increase novel compound discovery but reduce precision; smaller beams prioritize known, drug-like space
  - Data filtering: Capping transformations per molecule (≤10) and per pattern (≤10 occurrences) reduces bias but may exclude rare but valuable transformations
- **Failure signatures**:
  - Low validity at high k: Suggests model generates syntactically invalid SMIRKS under beam expansion; investigate tokenization or temperature settings
  - Low coverage on "popular" molecules: May indicate over-regularization from filtering; consider relaxing per-molecule transformation caps
  - Repetitive outputs: Beam search collapse; increase temperature or use nucleus sampling
- **First 3 experiments**:
  1. Baseline validation: Replicate Table 1 results—train Mol2Trans on 2M pairs, evaluate %Valid and %Exist at k∈{1,10,20,50,100} on held-out test set
  2. Ablation on data scale: Train on 500K, 1M, and 2M pairs to measure coverage scaling; assess if performance saturates below 2M
  3. User-specified mode test: For a set of molecules with known functional groups, force decoder to start with each group pattern and measure if ground-truth replacements appear in top-k outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Data representativeness and bias: Training data from ChEMBL favors bioactive, drug-like molecules, potentially limiting applicability to other chemical domains
- Chemical validity vs. synthetic accessibility: High validity only verifies syntactic correctness, not synthetic accessibility or practical utility of generated molecules
- SMIRKS expressiveness constraints: Cannot adequately capture complex transformations involving ring rearrangements, stereochemical changes, or multi-site modifications

## Confidence

**High confidence:**
- The two-stage sequential generation approach successfully produces valid SMIRKS transformations that can be applied to molecules
- ChemT5 pretraining provides effective initialization for the transformation task
- The model achieves higher known-compound recovery than baseline Mol2Mol at all k values

**Medium confidence:**
- The SMIRKS-based representation enables better exploration of novel chemical space compared to direct SMILES translation
- The model can reliably perform user-specified functional group replacements when forced to start with specific patterns
- The filtering strategy appropriately balances bias reduction with coverage

**Low confidence:**
- The model's generated transformations are chemically meaningful beyond syntactic validity
- The claimed "novelty" represents chemically useful or synthetically accessible compounds
- Performance will generalize to non-drug-like molecular scaffolds and transformations

## Next Checks

1. **Independent novelty validation**: Apply the model to a held-out test set of bioactive compounds not in ChEMBL and verify that generated transformations produce novel structures absent from major chemical databases (PubChem, ZINC, etc.). Compare novelty percentages against random sampling baselines.

2. **Property-based evaluation**: For a representative sample of generated transformations, compute key molecular properties (QED, synthetic accessibility score, clogP, MW) and compare distributions to training data. Assess whether novelty corresponds to meaningful chemical diversity or simply non-drug-like outliers.

3. **Complex transformation stress test**: Evaluate model performance on transformations involving: (a) rings of size >6, (b) multiple stereocenters, (c) heteroatom-rich scaffolds, and (d) transformations exceeding the 13-atom R-group limit. Measure validity and coverage degradation as transformation complexity increases.