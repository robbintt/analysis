---
ver: rpa2
title: 'NLD-LLM: A systematic framework for evaluating small language transformer
  models on natural language description'
arxiv_id: '2510.05139'
source_url: https://arxiv.org/abs/2510.05139
tags:
- language
- natural
- code
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLD-LLM, a systematic framework for evaluating
  language models' ability to generate accurate, concise natural language descriptions
  from source code. The framework employs a diverse set of transformer models (Qwen,
  DeepSeek, Phi, LLaMA, Mistral) and a comprehensive prompt engineering strategy to
  ensure fair evaluation.
---

# NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description

## Quick Facts
- arXiv ID: 2510.05139
- Source URL: https://arxiv.org/abs/2510.05139
- Reference count: 24
- Small transformer models can achieve competitive performance with larger models on natural language description generation from code

## Executive Summary
This paper introduces NLD-LLM, a systematic framework for evaluating language models' ability to generate accurate, concise natural language descriptions from source code. The framework employs a diverse set of transformer models (Qwen, DeepSeek, Phi, LLaMA, Mistral) and a comprehensive prompt engineering strategy to ensure fair evaluation. Using semantic and structural metrics, including BLEU, ROUGE-L, METEOR, and BERTScore, the study reveals that smaller models like Qwen can perform competitively with larger models when supported by well-crafted prompts. Qwen achieved the highest BERTScore (0.8853), demonstrating strong semantic fidelity, while LLaMA led in BLEU score (0.0413), indicating better surface-form matching. The results highlight the importance of model selection based on task-specific requirements and resource constraints.

## Method Summary
The NLD-LLM framework evaluates transformer models on natural language description generation from C/C++ source code using a two-step pipeline: initial generation followed by iterative refinement. Five transformer models (Qwen2.5-Coder-1.5B, DeepSeek-Coder-1.3B, Phi-4-mini, LLaMA-3.1-8B, Mistral-7B) are tested using six prompt styles with standardized formatting and task guidance. Inference is performed with temperature=0.7 and top-p=0.9 on NVIDIA H100 GPU via Hugging Face transformers. Performance is measured using five metrics (BLEU, ROUGE-L, METEOR, BERTScore, MAUVE) against a dataset of 150 manually annotated C/C++ code examples with gold-standard descriptions.

## Key Results
- Qwen2.5-Coder-1.5B achieved the highest BERTScore (0.8853), demonstrating strong semantic fidelity despite being the smallest model
- LLaMA-3.1-8B achieved the highest BLEU score (0.0413), indicating better surface-form matching
- DeepSeek-Coder-1.3B consistently performed worst across all metrics, with BLEU score of 0.0053
- Smaller models can perform competitively with larger models when supported by well-crafted prompts

## Why This Works (Mechanism)

### Mechanism 1: Prompt Standardization and Task Guidance
Standardized prompts with explicit task guidance enable smaller models to perform competitively with larger ones on NLD generation. The framework uses six prompt styles combined with guidance principles for task clarity and prompt quality, reducing ambiguity and providing consistent context across model evaluations.

### Mechanism 2: Iterative Refinement Pipeline
A two-step generation-refinement pipeline improves output quality and assesses model adaptability. Initial NLD generation is followed by a refiner step that iteratively improves descriptions, enabling self-correction and enhanced coherence.

### Mechanism 3: Parameter-Efficient Model Selection
Architecture and training quality can compensate for parameter count in specific NLD tasks. Qwen2.5-Coder-1.5B achieved BERTScore 0.8853 and METEOR 0.2999, competitive with Mistral-7B-Instruct, demonstrating that code-specific training and efficient transformer design enable smaller models to match larger general-purpose models on semantic fidelity.

## Foundational Learning

- **Semantic vs. Surface-Level Metrics**: Understanding why BLEU scores are low (0.00-0.05) while BERTScore remains high (0.84-0.89) is essential for correct interpretation. Quick check: Why does BERTScore outperform BLEU for evaluating natural language descriptions of code?

- **Transformer Inference Parameters (Temperature, Top-p)**: All experiments use temperature=0.7 and top-p=0.9. Reproducing results requires understanding how these control output diversity and determinism. Quick check: What happens to output consistency if temperature is reduced to 0.2?

- **Instruction Tuning for Code Tasks**: Models like Qwen2.5-Coder and DeepSeek-Coder are specifically fine-tuned for code understanding. Distinguishing code-tuned from general-purpose models explains performance differences. Quick check: How might a general-purpose LLaMA model compare to Qwen-Coder on the same NLD task?

## Architecture Onboarding

- **Component map**: Code input -> Prompt templates (6 styles) -> LLM inference engine (5 models) -> Two-step pipeline (initial generation → refinement) -> 5 metrics (BLEU, ROUGE-L, METEOR, BERTScore, MAUVE) -> Human-annotated reference dataset

- **Critical path**: Code input → Select prompt style → LLM generates initial NLD → Initial NLD → Refiner prompt → Refined NLD output → Compute metrics against human reference → Aggregate scores

- **Design tradeoffs**: Model size vs. latency (Qwen 1.5B faster vs. Mistral 7B slower with comparable BERTScore), Metric choice (BLEU for exact phrase matching vs. BERTScore for semantic fidelity), Prompt complexity (Few-shot requires curated examples vs. Zero-shot faster but may reduce quality)

- **Failure signatures**: BLEU consistently near 0.00 (expected for NLD), BERTScore below 0.82 (semantic misalignment), DeepSeek pattern (lowest across all metrics), High variance across prompt styles (model is prompt-sensitive)

- **First 3 experiments**: 1) Baseline replication: Run all 5 models on 20 C/C++ samples using "Concise One-Line Description" prompt; 2) Prompt ablation: Test Qwen-1.5B with all 6 prompt styles on identical samples; 3) Refinement delta: Compare initial vs. refined outputs for 30 samples across Qwen and Mistral

## Open Questions the Paper Calls Out

- **Cross-language generalization**: Does the finding that smaller models can match larger ones in semantic fidelity generalize to programming languages other than C and C++? The authors plan to test Python and Java in future work.

- **Memory efficiency integration**: How can memory efficiency and output relevance be quantitatively integrated into the model selection strategy alongside semantic metrics? Future work will focus on incorporating these factors.

- **Frontier model comparison**: How do the evaluated open-source models compare against frontier, closed-source models (e.g., Gemini, GPT-4) in terms of cost-accuracy trade-offs? The evaluation lacks frontier models due to cost and access limitations.

- **Dataset scaling**: Will the competitive performance of smaller models persist when the evaluation dataset is expanded significantly beyond 150 examples? The current conclusion is based on a limited sample size.

## Limitations

- The framework's generalizability is limited by its focus on C/C++ code with only 150 annotated examples, potentially restricting performance conclusions to this language pair.
- The prompt engineering benefits observed for smaller models may not transfer to other code domains (Python, Java) or general-purpose text generation tasks.
- The two-step refinement pipeline's effectiveness across different code complexity levels remains unverified, as the paper does not report per-complexity performance breakdowns.

## Confidence

- **High Confidence**: BERTScore results showing Qwen's semantic fidelity (0.8853) - supported by direct metric comparisons across all models
- **Medium Confidence**: Prompt engineering benefits for smaller models - theoretical mechanism plausible but corpus evidence weak
- **Medium Confidence**: Refinement pipeline improvements - mechanism described but no comparative analysis without refinement
- **Low Confidence**: Cross-language generalization - not tested beyond C/C++

## Next Checks

1. **Cross-language validation**: Apply NLD-LLM framework to Python code samples with 50-100 examples; compare performance patterns to C/C++ results

2. **Prompt sensitivity analysis**: Systematically vary temperature (0.2, 0.5, 0.7) and top-p (0.8, 0.9, 0.95) for Qwen-1.5B; measure BERTScore stability and output diversity

3. **Refinement efficacy measurement**: For 30 diverse code samples, compare initial vs. refined outputs across all five models; quantify semantic drift using BERTScore delta and identify failure cases where refinement degrades quality