---
ver: rpa2
title: 'RESPONSE: Benchmarking the Ability of Language Models to Undertake Commonsense
  Reasoning in Crisis Situation'
arxiv_id: '2503.11348'
source_url: https://arxiv.org/abs/2503.11348
tags:
- human
- answer
- response
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RESPONSE, a human-curated dataset containing
  1,789 annotated instances designed to assess LLMs' commonsense reasoning in disaster
  situations across different time frames. The dataset includes problem descriptions,
  missing resources, time-sensitive solutions, and justifications, with a subset validated
  by environmental engineers.
---

# RESPONSE: Benchmarking the Ability of Language Models to Undertake Commonsense Reasoning in Crisis Situation

## Quick Facts
- arXiv ID: 2503.11348
- Source URL: https://arxiv.org/abs/2503.11348
- Reference count: 18
- Primary result: GPT-4 achieves only 37% correctness for immediate disaster response recommendations versus human responses

## Executive Summary
This paper introduces RESPONSE, a human-curated dataset of 1,789 annotated instances designed to evaluate LLMs' commonsense reasoning in disaster situations across different time frames. The dataset contains problem descriptions, missing resources, time-sensitive solutions, and justifications, with a subset validated by environmental engineers. Through automatic metrics, human evaluation, and LLM-based structured evaluation, the authors demonstrate that current LLMs struggle significantly with immediate disaster response recommendations, achieving only 37% human-evaluated correctness for urgent scenarios despite better performance on longer-term planning.

## Method Summary
The RESPONSE dataset was constructed by sampling 1,789 instances from Incidents1M images across 43 incident types and 49 locations. Each instance includes a problem description, missing resource, and three time-dependent solutions with explanations (immediate, one week, one month). The authors evaluated GPT-4-turbo and Claude-3 Opus using a constrained prompt template (15 words per solution and explanation, temperature=0). Evaluation employed automatic metrics (BLEU, BertScore, BLEURT), human evaluation on 100 sampled pairs per time frame across three criteria (Sufficiency, Usefulness, Correctness), and LLM-based structured evaluation via GPT-4o examining Resource Management, Feasibility, and Individual Impact.

## Key Results
- GPT-4 achieves only 37% correctness for immediate disaster response recommendations versus human responses
- Automatic metrics (BLEU 0.42-0.68) fail to discriminate solution quality and contradict human rankings
- Immediate disaster response reasoning is significantly more challenging for LLMs than longer-term planning scenarios

## Why This Works (Mechanism)
The evaluation framework combines multiple assessment approaches to capture different dimensions of reasoning quality. Human evaluation provides ground truth preferences across sufficiency, usefulness, and correctness dimensions. Automatic metrics offer scalable but ultimately unreliable assessment. The LLM-based structured evaluation attempts to provide consistent, detailed feedback on specific reasoning aspects like resource management and feasibility.

## Foundational Learning
- **Common sense reasoning evaluation**: Needed to establish whether models can handle real-world crisis scenarios beyond pattern matching. Quick check: Verify dataset covers diverse incident types with realistic constraints.
- **Time-dependent reasoning**: Required to test models' ability to adapt solutions across immediate, short-term, and long-term horizons. Quick check: Confirm prompt structure explicitly asks for three distinct time frames.
- **Multi-metric evaluation**: Necessary because single evaluation methods can be misleading. Quick check: Review correlation between automatic and human metrics.

## Architecture Onboarding
- **Component map**: Dataset (RESPONSE) -> LLM Inference (GPT-4, Claude-3) -> Evaluation (Automatic Metrics, Human Evaluation, LLM-based Structured Evaluation)
- **Critical path**: Sample dataset -> Generate LLM responses -> Compute automatic metrics -> Conduct human evaluation on sampled pairs -> Aggregate results
- **Design tradeoffs**: The choice of constrained response length (15 words) balances evaluation tractability against solution completeness, potentially limiting nuanced responses.
- **Failure signatures**: Automatic metrics show compressed ranges and contradict human rankings; immediate timeframe performance significantly worse than longer horizons.
- **First experiments**: 1) Verify dataset structure and content consistency, 2) Test LLM response generation with constraints, 3) Run correlation analysis between automatic metrics and human scores.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset not yet publicly available, preventing independent verification
- Human evaluation relies on small sample (100 instances per time frame) without inter-rater reliability metrics
- GPT-4o-based structured evaluation may introduce circularity by using the same model architecture for both generation and evaluation

## Confidence
- **High Confidence**: Automatic metrics fail to discriminate solution quality and contradict human evaluation rankings
- **Medium Confidence**: Quantitative performance gap between GPT-4 (37% correctness for immediate response) and human responses
- **Medium Confidence**: Claim that immediate disaster response reasoning is more challenging for LLMs than longer-term planning

## Next Checks
1. Confirm the RESPONSE dataset is publicly released with the claimed 1,789 instances and 6,037 question sets, and verify the sampling methodology from Incidents1M includes the specified 43 incident types and 49 locations.
2. Replicate the human evaluation protocol on a subset of 50 instances, measuring inter-rater reliability (Cohen's kappa) for the three criteria and comparing results against the reported preference distributions.
3. Conduct correlation analysis between automatic metrics and human evaluation scores across all 1,789 instances to verify the claimed disconnect and test whether any metric combination provides predictive power.