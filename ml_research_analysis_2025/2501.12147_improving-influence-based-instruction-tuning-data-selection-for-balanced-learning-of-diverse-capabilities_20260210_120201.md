---
ver: rpa2
title: Improving Influence-based Instruction Tuning Data Selection for Balanced Learning
  of Diverse Capabilities
arxiv_id: '2501.12147'
source_url: https://arxiv.org/abs/2501.12147
tags:
- data
- influence
- bids
- training
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving balanced performance
  across diverse tasks when fine-tuning large language models using influence-based
  data selection methods. The authors identify that existing influence-based algorithms
  often suffer from task-level bias in influence scores, leading to imbalanced and
  suboptimal performance across different capabilities.
---

# Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities

## Quick Facts
- **arXiv ID**: 2501.12147
- **Source URL**: https://arxiv.org/abs/2501.12147
- **Reference count**: 40
- **Primary result**: BIDS (Balanced and Influential Data Selection) improves balanced performance across 7 diverse benchmarks using 15% of training data while achieving better or comparable results to full-dataset training.

## Executive Summary
This paper addresses the challenge of achieving balanced performance across diverse tasks when fine-tuning large language models using influence-based data selection methods. The authors identify that existing influence-based algorithms often suffer from task-level bias in influence scores, leading to imbalanced and suboptimal performance across different capabilities. They propose BIDS, which introduces instance-level normalization of influence scores across validation tasks and an iterative selection algorithm that prioritizes training examples improving performance on underrepresented tasks. Experimental results demonstrate that BIDS consistently outperforms both influence-based and non-influence-based selection methods across seven benchmarks spanning five diverse capabilities, with the surprising result that 15% of data selected by BIDS can even outperform full-dataset training while achieving more balanced performance.

## Method Summary
BIDS builds on LESS's influence estimation pipeline, which uses LoRA-based low-dimensional gradient similarity to compute an Attribution Matrix representing how much each training example influences each validation instance. The key innovations are instance-level normalization (column-wise z-score normalization to equalize influence scales across validation instances) and iterative greedy selection (selecting examples that maximize marginal improvement to the most underrepresented validation instance). The method is evaluated on UltraInteract training data with 7 diverse benchmarks, showing improved balanced performance compared to LESS and representation-based baselines.

## Key Results
- BIDS consistently outperforms LESS and representation-based selection across all tested budgets (5%-15%) on 7 diverse benchmarks
- Training on 15% of data selected by BIDS achieves better or comparable performance to full-dataset training
- Macro-average accuracy improves from 51.1% (LESS) to 54.1% (BIDS) at 15% budget
- AID analysis shows BIDS reduces influence disparities across tasks compared to LESS (mean absolute difference decreases from 0.204 to 0.108)

## Why This Works (Mechanism)

### Mechanism 1: Task-Level Influence Bias Discovery
- **Claim**: Influence scores exhibit systematic scale differences across tasks, causing selection algorithms to over-sample data for high-influence tasks regardless of actual performance gains.
- **Mechanism**: The paper's Average Influence Distribution (AID) analysis reveals that MMLU validation instances receive substantially higher average influence scores than BBH instances, despite neither being in-distribution for the training data.
- **Core assumption**: High influence scores should correlate with performance improvement potential, but the paper shows this assumption breaks across tasks (Figure 2: MMLU has highest Task Frequency with Highest Influence but LESS frequently underperforms random baseline on MMLU).
- **Evidence anchors**: [Page 4, Figure 1]: Shows "great disparities in scale for inter-task and intra-task influence" across the training dataset. [Page 4, Section 3]: "This suggests a potential inherent bias in the influence values across different tasks, which could skew the selection algorithm towards certain capabilities."

### Mechanism 2: Instance-Level Normalization for Cross-Task Calibration
- **Claim**: Column-wise z-score normalization of the Attribution Matrix equalizes influence scales across validation instances, enabling fair comparison across tasks.
- **Mechanism**: For each validation instance v_j, normalize: A_norm[i,j] = (A[i,j] - μ_j) / σ_j, where μ_j and σ_j are computed across all training examples. This ensures that intra-column rankings (not raw values) determine selection priority.
- **Core assumption**: Influence score distributions per validation instance are approximately normal; Appendix A.4 (Figure 6) empirically validates this across sampled columns.
- **Evidence anchors**: [Page 5, Section 4]: "This normalization step ensures that the influence scores of different columns are on the same scale." [Page 8, Figure 5]: AID analysis shows progressive reduction in influence disparity from unnormalized to normalized to full BIDS.

### Mechanism 3: Iterative Greedy Selection for Marginal Influence Maximization
- **Claim**: Selecting data iteratively based on marginal improvement to underrepresented tasks outperforms single-pass top-K selection because influence utility is non-additive.
- **Mechanism**: At each iteration, compute utility Δ(i) = max_j{A[i,j] - (1/|T|) Σ_{k∈T} A[k,j]} and select the candidate with highest utility. This explicitly favors examples that improve the most underrepresented validation instance relative to the current selected set.
- **Core assumption**: The first-order linearity assumption in LESS's influence estimation is imperfect—simply accumulating high-influence points doesn't linearly translate to performance gains (Page 8: "simply selecting high-influence points... their effectiveness doesn't linearly add up").
- **Evidence anchors**: [Page 7, Table 3]: Ablation shows -Iter underperforms full BIDS across all budgets, confirming iterative selection's contribution. [Page 5, Figure 3]: Visual comparison of LESS (task-wise max, single-pass) vs. BIDS (iterative) selection pipelines.

## Foundational Learning

### Influence Estimation via Gradient Similarity
- **Why needed here**: BIDS builds directly on LESS's influence estimation, which uses LoRA-based low-dimensional gradient similarity. Understanding that A[i,j] represents how much training example i's gradient aligns with validation instance j's gradient is prerequisite.
- **Quick check question**: Can you explain why cosine similarity of projected gradients approximates data influence, and what the "warmup training" step in LESS accomplishes?

### Multi-Task Instruction Tuning with Mixed Datasets
- **Why needed here**: The core problem (unbalanced performance across diverse capabilities) only emerges when training on heterogeneous task mixtures. Single-task selection doesn't exhibit this bias.
- **Quick check question**: Why might a model trained on coding-heavy data struggle with instruction-following tasks, even if the instruction-following validation set has high influence scores?

### Attribution Matrix Structure and Operations
- **Why needed here**: BIDS's algorithmic innovations (column normalization, iterative selection) operate directly on the |D| × |V| Attribution Matrix. Understanding row vs. column semantics is essential.
- **Quick check question**: In the Attribution Matrix, what does row i represent? What does column j represent? Why is column-wise normalization the correct operation for cross-task calibration?

## Architecture Onboarding

### Component Map
Training Data (D) + Validation Data (V)
           ↓
[LESS Influence Estimation Pipeline]
  - Warmup LoRA training (4 epochs)
  - Gradient computation + projection (dim=8192)
  - Pairwise influence calculation
           ↓
Attribution Matrix A[|D| × |V|]
           ↓
[Instance-Level Normalization] ← BIDS Innovation #1
  - Column-wise z-score: (A[i,j] - μ_j) / σ_j
           ↓
Normalized Attribution Matrix A_norm
           ↓
[Iterative Greedy Selection] ← BIDS Innovation #2
  - Initialize T = ∅
  - While |T| < budget:
    - Compute Δ(i) = max_j{A_norm[i,j] - avg(A_norm[T,j])}
    - Select argmax_i Δ(i), add to T
           ↓
Selected Training Subset T
           ↓
[Fine-tune Target Model on T]

### Critical Path
1. **Influence computation is the bottleneck**: LESS pipeline requires 4-epoch warmup training plus gradient projection for all training and validation examples. This must complete before BIDS selection logic runs.
2. **Normalization is O(|D| × |V|)**: Trivial computation; the paper reports <1 minute on H100 for ~288K × 350 matrix (Appendix A.7).
3. **Iterative selection is O(budget × |D| × |V|)**: Each iteration scans all remaining candidates against all validation instances. For large datasets, this dominates post-influence runtime.

### Design Tradeoffs
- **Z-score vs. min-max normalization**: Paper uses z-score assuming approximate normality (validated in Appendix A.4). Min-max would be more robust to non-normal distributions but sensitive to outliers.
- **Iterative vs. single-pass selection**: Iterative adds O(budget) factor to selection time but enables marginal utility optimization. For very large budgets, consider approximate greedy or batch selection.
- **Validation set composition**: Paper uses equal instances per task (50 each for 7 tasks). Imbalanced validation sets would re-introduce task bias through the selection objective.

### Failure Signatures
1. **Normalization on highly skewed distributions**: If influence scores for a validation instance are heavily skewed, z-score normalization may not properly calibrate. Check distribution histograms before normalizing (as in Appendix A.4).
2. **Iterative selection stalls on dominated tasks**: If one task's validation instances always have higher normalized influence than others, iterative selection may ignore minority tasks. Monitor per-task selection frequencies during iteration.
3. **Influence estimation quality degrades**: LESS relies on LoRA warmup quality. If warmup is insufficient or learning rate is poorly tuned, influence scores become unreliable, and BIDS amplifies this noise.

### First 3 Experiments
1. **Reproduce AID analysis on your dataset**: Before implementing full BIDS, compute the unnormalized Average Influence Distribution for your training data. If task-level disparities are small (unlike Figure 1), normalization may provide limited benefit.
2. **Ablate normalization alone**: Compare -Iter (normalization only, then top-K selection) against unnormalized baseline on your task suite. This isolates normalization's contribution before adding iterative complexity.
3. **Budget sweep with macro-average tracking**: Test BIDS at 5%, 10%, 15% budgets, reporting both macro-average and per-task performance. The paper's key claim is balanced improvement—if certain tasks degrade significantly, investigate validation set composition or influence estimation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the inherent imbalance of utility scores observed in influence-based methods generalize to all non-influence-based data selection algorithms?
- **Basis in paper**: [explicit] The Limitations section states that the Representation-based Data Selection (RDS) baseline showed significant bias towards coding tasks, suggesting that "the imbalance of utility scores... may generally exist for both influence- and non-influence-based data selection approaches."
- **Why unresolved**: The paper focused on optimizing influence-based methods and did not conduct a broader investigation into whether this is a universal property of multi-task data selection.
- **What evidence would resolve it**: A comparative study applying the paper's analysis metrics (e.g., Average Influence Distribution, Task Frequency with Highest Influence) to diverse selection methods like quality scoring or diversity clustering.

### Open Question 2
- **Question**: To what extent does the iterative selection in BIDS mitigate the "first-order linearity assumption" limitation of gradient-based influence functions?
- **Basis in paper**: [inferred] The Analysis section (Page 8) notes that simply selecting high-influence points often fails because effectiveness "doesn’t linearly add up," yet BIDS improves results by balancing the average influence distribution.
- **Why unresolved**: It is unclear if BIDS's iterative balancing explicitly accounts for diminishing returns on high-influence tasks or if the improvement is solely due to better task coverage.
- **What evidence would resolve it**: An ablation study measuring the correlation between the change in normalized AID and the actual performance delta, specifically checking for saturation effects in high-influence tasks.

### Open Question 3
- **Question**: Does the instance-level normalization technique remain robust when validation influence distributions deviate significantly from a standard normal distribution?
- **Basis in paper**: [inferred] Appendix A.4 claims validation distributions generally approximate a standard normal distribution, which justifies the use of normal standardization ($\mu$ and $\sigma$), but the method's sensitivity to this assumption is not tested.
- **Why unresolved**: If the influence scores for a specific task are bimodal or heavily skewed, the standardization method could distort the relative rankings of training examples.
- **What evidence would resolve it**: Experiments applying BIDS to datasets with manipulated influence distributions (e.g., artificially creating bimodal or uniform score distributions) to measure performance degradation compared to the baseline.

## Limitations

- **Cross-dataset generalization**: The paper demonstrates BIDS effectiveness on UltraInteract and seven specific benchmarks, but the mechanism relies on influence score calibration which may behave differently for datasets with different task compositions.
- **Computational overhead**: While the paper reports ~1 minute for post-influence selection, the LESS influence estimation pipeline requires 4-epoch warmup training plus gradient computation for all training-validation pairs, which may be prohibitive for larger models or datasets.
- **Influence estimation reliability**: BIDS amplifies the quality of underlying influence estimates. If LESS's gradient similarity approximation is imperfect, BIDS could systematically over-select or under-select certain types of examples based on estimation artifacts rather than true utility.

## Confidence

- **High confidence**: Task-level influence bias exists and normalization reduces disparities (supported by AID analysis across multiple experiments)
- **Medium confidence**: Iterative selection provides marginal gains over single-pass methods (supported by ablation, but relative improvement is modest)
- **Medium confidence**: BIDS achieves genuinely balanced performance across all tasks (based on macro-averages, though some tasks show larger absolute gains than others)

## Next Checks

1. **Cross-dataset validation**: Apply BIDS to a completely different instruction tuning dataset (e.g., OpenOrca or Databricks-dolly-15k) with a different task distribution. Verify that the task-level bias discovery and normalization mechanisms still provide benefit.

2. **Influence distribution robustness**: Test BIDS with alternative influence estimation methods (e.g., full Hessian approximation or proxy-based methods) to verify that the normalization and iterative selection mechanisms are not artifacts of LESS's specific gradient similarity approach.

3. **Budget sensitivity analysis**: Conduct a finer-grained budget sweep (1%, 3%, 5%, 10%, 15%, 20%) to identify whether BIDS's balanced performance advantage holds across all budget levels or only at specific points, and whether there's a point of diminishing returns.