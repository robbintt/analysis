---
ver: rpa2
title: 'SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic
  EHR Data'
arxiv_id: '2507.07421'
source_url: https://arxiv.org/abs/2507.07421
tags:
- eviction
- notes
- reasoning
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SynthEHR-Eviction, a scalable pipeline combining
  LLMs, human-in-the-loop annotation, and automated prompt optimization to extract
  eviction statuses from clinical notes. Using this pipeline, the authors created
  the largest public eviction-related SDoH dataset to date, comprising 14 fine-grained
  categories.
---

# SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data

## Quick Facts
- **arXiv ID:** 2507.07421
- **Source URL:** https://arxiv.org/abs/2507.07421
- **Reference count:** 40
- **Primary result:** Qwen2.5-7B achieves 88.8% Macro-F1 (eviction), 90.3% (non-eviction SDoH) on human-validated data

## Executive Summary
SynthEHR-Eviction introduces a scalable pipeline combining LLMs, human-in-the-loop annotation, and automated prompt optimization to extract eviction statuses from clinical notes. The system generates synthetic clinical notes via LLM-augmented rewriting, creating the largest public eviction-related SDoH dataset with 14 fine-grained categories. Fine-tuned LLMs trained on this dataset outperform GPT-4o-APO, GPT-4o-mini-APO, and BioBERT on human-validated data. The pipeline reduces annotation effort by over 80% and accelerates dataset creation for cold-start domains.

## Method Summary
The pipeline uses DSPy framework with Chain-of-Thought reasoning for annotation, generating synthetic clinical notes that are rewritten to include specific eviction labels. Models are fine-tuned using Unsloth with LoRA (rank=16, alpha=16, 4-bit quantization) for 2 epochs at lr=2e-4. The optimal training data composition is 70% synthetic + 30% real notes from MIMIC and PMC. Training uses max_seq_length=1024, batch_size=2, and gradient_accumulation=4. The cascaded classification approach first detects eviction presence, then classifies into 7 eviction subtypes or 7 non-eviction SDoH categories.

## Key Results
- Qwen2.5-7B achieves 88.8% Macro-F1 (eviction) and 90.3% Macro-F1 (non-eviction SDoH) on human-validated data
- Outperforms GPT-4o-APO (87.8%, 87.3%), GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%)
- Reduces annotation effort by over 80% while maintaining high performance
- Generalizes to other information extraction tasks beyond eviction detection

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Augmentation for Cold-Start Domains
The pipeline generates synthetic clinical notes via LLM-augmented rewriting to create balanced datasets for rare categories. Since eviction terms appear in fewer than 1 in 1,000 MIMIC records, synthetic generation bypasses manual annotation bottlenecks. This assumes synthetic notes sufficiently capture real clinical text patterns to transfer learned patterns.

### Mechanism 2: Reasoning Distillation for Small Models
Training smaller models (3B parameters) on Chain-of-Thought reasoning traces improves performance by teaching intermediate logical steps rather than surface pattern matching. Llama-3.2-3B improved by +3.8 Macro-F1 when trained with reasoning, compensating for lower model capacity.

### Mechanism 3: Hybrid Composition for Domain Alignment
A training set with 70% synthetic and 30% real data resolves domain shift problems. Pure synthetic data ensures category balance, while real data anchors representations to actual clinical practice. Adding 30% PMC data improved PMC performance by +0.302 Micro-F1.

## Foundational Learning

- **Social Determinants of Health (SDoH) & Z-Codes**: Model outputs must align with medical ontologies (ICD-10-CM Z59 codes). Quick check: Can you explain the difference between "Eviction pending" (legal process started) and "Eviction hypothetical" (threatened but not started)?

- **DSPy & Automated Prompt Optimization (APO)**: The pipeline uses DSPy to programmatically optimize prompts for the Annotator. Quick check: How does the DSPy optimizer use the "trainset" and "devset" to modify the prompt for the Annotator?

- **Temporal Reasoning in Clinical Text**: Most difficult classification errors stem from distinguishing "History" vs. "Current" events. Quick check: Why does the paper suggest temporal ambiguity is a primary source of error in eviction detection?

## Architecture Onboarding

- **Component map:** Source (MIMIC/PMC) -> Augmenter (LLM) -> Annotator (DSPy) -> Fine-Tuning (Unsloth/LoRA)
- **Critical path:** The Augmenter loop (Algorithm 1). If Augmenter produces low-quality synthetic notes that pass human check, the entire training set is poisoned.
- **Design tradeoffs:** 100% Synthetic is fast/cheap but fails on PMC; Hybrid (70/30) is slower but robust. 7B/8B models generalize well naturally; 3B models require reasoning traces (CoT) to perform well.
- **Failure signatures:** Temporal Hallucination (misclassifies "resides now" as "evicted now"), Lexical Bias (over-weights "history"), Faithfulness Gap (correct label, wrong rationale).
- **First 3 experiments:** 1) Ablation on Reasoning: Train Llama-3.2-3B with/without CoT to verify ~3-4% gain. 2) Domain Shift Test: Train 100% Synth vs. 70% Synth/30% PMC and evaluate on PMC. 3) Temporal Error Analysis: Run inference on "Case 1/2" examples and inspect attention/predictions.

## Open Questions the Paper Calls Out

- **Question 1:** Can incorporating explicit temporal tagging or leveraging note timestamps improve disambiguation of eviction event timelines? The paper notes future improvements may require time-aware training frameworks to help models disambiguate event timelines.

- **Question 2:** How can faithfulness of chain-of-thought explanations be ensured to prevent models from being "right for wrong reasons"? The authors note ensuring explanations are correct and useful is important, as cases show correct labels with conceptually misaligned reasoning.

- **Question 3:** Does integration of SynthEHR-Eviction detection into clinical workflows improve downstream health outcomes or housing stability? The paper states future work should assess downstream utility in improving health outcomes and promoting equity.

## Limitations
- Potential domain shift between synthetic clinical notes and real-world EHR data, despite hybrid training approach
- Reasoning distillation may introduce flawed heuristics if traces contain hallucinations or logical fallacies
- Temporal reasoning remains unresolved, with models struggling to distinguish "History" vs. "Current" without explicit temporal markers

## Confidence
- **High Confidence:** Overall pipeline architecture and synthetic dataset generation capability
- **Medium Confidence:** Effectiveness of hybrid training approach in bridging domain shift
- **Medium Confidence:** Generalizability to other information extraction tasks beyond eviction detection

## Next Checks
1. **Reasoning Faithfulness Audit:** Manually validate 50 reasoning traces from training data to quantify hallucination rates and logical consistency
2. **Temporal Reasoning Stress Test:** Create test set with controlled temporal ambiguity to measure model performance on most challenging cases
3. **Domain Shift Quantification:** Systematically vary proportion of real data in training (0%, 30%, 50%, 70%) and measure performance degradation on each test set