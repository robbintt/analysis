---
ver: rpa2
title: 'Pixels to Play: A Foundation Model for 3D Gameplay'
arxiv_id: '2508.14295'
source_url: https://arxiv.org/abs/2508.14295
tags:
- games
- arxiv
- data
- training
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pixels2Play-0.1 (P2P0.1) is a foundation model for playing diverse
  3D video games directly from raw pixels, trained end-to-end with behavior cloning.
  It uses a decoder-only transformer that auto-regresses over discretized keyboard-mouse
  actions, handling large action spaces in real time on a single consumer GPU.
---

# Pixels to Play: A Foundation Model for 3D Gameplay

## Quick Facts
- **arXiv ID:** 2508.14295
- **Source URL:** https://arxiv.org/abs/2508.14295
- **Reference count:** 27
- **Primary result:** Pixels2Play-0.1 achieves novice-level 3D gameplay across Roblox and MS-DOS games using a decoder-only transformer with action imputation

## Executive Summary
Pixels2Play-0.1 (P2P0.1) introduces a foundation model that plays diverse 3D video games directly from raw pixels without per-title engineering. The system uses behavior cloning with a decoder-only transformer architecture that auto-regresses over discretized keyboard-mouse actions, enabling real-time play on a single consumer GPU. By training an inverse-dynamics model to impute actions from abundant unlabeled gameplay videos, the team significantly expands training data and improves validation perplexity by 22% compared to using only labeled examples.

The approach demonstrates competent novice-level performance across multiple game platforms, with text-conditioned control and human-like behavior patterns. This work addresses the challenge of scaling gameplay AI to multiple games without manual engineering per title, leveraging large-scale pretraining and data augmentation through action imputation.

## Method Summary
The core approach combines behavior cloning with action imputation from unlabeled gameplay videos. A decoder-only transformer processes raw pixel inputs and auto-regresses over discretized keyboard-mouse actions. To overcome limited labeled data, an inverse-dynamics model is trained to predict actions from gameplay videos, creating an augmented dataset. The policy is then fine-tuned on this expanded corpus. The model handles large action spaces in real time and supports text-conditioned control, enabling zero-shot adaptation to new games within the training distribution.

## Key Results
- Achieves novice-level performance on both Roblox and MS-DOS game titles
- Validation perplexity improves by 22% when incorporating imputed-label data versus using only a small labeled set
- Runs in real time on a single consumer GPU
- Enables text-conditioned control without per-title engineering

## Why This Works (Mechanism)
The model succeeds by leveraging abundant unlabeled gameplay videos through action imputation, effectively scaling the training dataset beyond what manual labeling would allow. The decoder-only transformer architecture is well-suited for auto-regressive action prediction, while discretization of continuous actions enables tractable modeling. By pretraining on diverse games, the model develops generalizable visual and control representations that transfer across titles.

## Foundational Learning

**Behavior Cloning**
- *Why needed:* Provides direct learning from expert demonstrations without requiring environment interaction
- *Quick check:* Compare performance with and without imputed data to verify data efficiency gains

**Inverse Dynamics Modeling**
- *Why needed:* Enables automatic generation of action labels from unlabeled gameplay videos
- *Quick check:* Measure imputation accuracy on held-out videos with known actions

**Decoder-Only Transformer Architecture**
- *Why needed:* Supports auto-regressive action generation while processing raw pixel inputs
- *Quick check:* Evaluate latency and perplexity as sequence length varies

**Action Discretization**
- *Why needed:* Makes large continuous action spaces tractable for transformer modeling
- *Quick check:* Test different discretization granularities for optimal trade-off between expressiveness and model capacity

## Architecture Onboarding

**Component Map**
Pixel input -> Encoder (optional) -> Transformer decoder -> Action discretization -> Output actions -> Game control

**Critical Path**
Raw pixels → Transformer decoder → Action prediction → Game input

**Design Tradeoffs**
The choice of decoder-only architecture enables simpler training but may limit bidirectional context compared to encoder-decoder models. Action discretization balances expressiveness with model complexity, while the imputation approach trades inference accuracy for dramatically expanded training data.

**Failure Signatures**
Performance degradation likely when game mechanics differ significantly from training distribution, when video quality is poor (affecting imputation accuracy), or when action spaces become too large for effective discretization.

**First Experiments**
1. Measure perplexity on held-out labeled data to establish baseline performance
2. Compare gameplay quality with and without imputed data augmentation
3. Test text-conditioned control by providing different gameplay instructions

## Open Questions the Paper Calls Out
None

## Limitations
- Results are primarily validated on Roblox and MS-DOS games, limiting generalizability to modern 3D environments
- Focus on novice-level performance raises questions about scalability to expert-level play
- Action imputation accuracy may degrade with video compression artifacts or varying game genres

## Confidence
- **High confidence** in technical feasibility of the proposed architecture and training pipeline for tested domains
- **Medium confidence** in reported perplexity improvements from imputed data, pending verification on broader datasets
- **Low confidence** in cross-game generalization and real-world applicability beyond studied game set

## Next Checks
1. Test the model on a held-out set of modern 3D games not seen during training to assess generalization
2. Evaluate robustness by introducing noise or compression artifacts in gameplay videos used for action imputation
3. Measure performance degradation as the number of possible discrete actions increases beyond current benchmarks