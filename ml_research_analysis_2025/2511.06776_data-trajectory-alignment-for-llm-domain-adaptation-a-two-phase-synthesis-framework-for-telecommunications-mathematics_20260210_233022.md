---
ver: rpa2
title: 'Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis
  Framework for Telecommunications Mathematics'
arxiv_id: '2511.06776'
source_url: https://arxiv.org/abs/2511.06776
tags:
- answer
- arxiv
- data
- training
- thinking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of adapting general-purpose\
  \ LLMs to domain-specific tasks in telecommunications, where labeled data is scarce\
  \ and mobile/edge constraints limit expensive inference-time reasoning. The proposed\
  \ Data Trajectory Alignment (DTA) framework treats solution processes\u2014not just\
  \ final answers\u2014as first-class supervision, aligning intermediate steps and\
  \ presentation style with the target student model's inductive biases."
---

# Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis Framework for Telecommunications Mathematics

## Quick Facts
- **arXiv ID:** 2511.06776
- **Source URL:** https://arxiv.org/abs/2511.06776
- **Reference count:** 40
- **Primary result:** DTA-trained models achieve 72.45% pass@1 on TELEMATH, surpassing distilled-only training by +17.65 points and outperforming thinking-enabled baselines by +2.94 points

## Executive Summary
This paper addresses the challenge of adapting general-purpose LLMs to domain-specific telecommunications mathematics tasks where labeled data is scarce and mobile/edge constraints limit expensive inference-time reasoning. The proposed Data Trajectory Alignment (DTA) framework treats solution processes—not just final answers—as first-class supervision, aligning intermediate steps and presentation style with the target student model's inductive biases. The two-phase method first synthesizes diverse candidates using an ensemble of strong teachers, then rewrites and selects high-signal exemplars via agreement checks and reflection-based judging. On the TELEMATH benchmark, DTA-trained models achieve state-of-the-art accuracy without explicit thinking modes: 72.45% pass@1, surpassing distilled-only training by +17.65 points and outperforming a strong thinking-enabled baseline by +2.94 points. Token-shift analyses show gains concentrated on logical-structural discourse markers, indicating improved reasoning scaffolding. Under edge-like inference, DTA reduces energy per output token by ~42% and latency by ~60% versus strong baselines, demonstrating that aligning solution trajectories enables compact, high-yield supervision that is both accurate and efficient for low-resource verticals.

## Method Summary
DTA operates in two phases: Phase I synthesizes diverse candidate solutions using an ensemble of strong teachers (DeepSeek-V3, Qwen3-235B, Kimi-K2) that generate detailed solutions to TELEMATH problems, followed by peer-review filtering via cross-model credibility scoring and MinHash + semantic decontamination to produce ~14K high-quality samples. Phase II extracts the student model's style from its own outputs, summarizes it via GPT-5, and has teachers rewrite solutions using this style guide. Candidates are ranked by a dual reflection scoring system (reversed-IFD + reward judge) with weights (0.5 correctness, 0.2 completeness, 0.2 clarity, 0.1 conciseness), and the top-K solutions are used for supervised fine-tuning on Qwen3-32B with batch=64, lr=1e-6, 3 epochs. The framework specifically targets non-thinking inference deployment to maximize edge efficiency while maintaining accuracy.

## Key Results
- DTA achieves 72.45% pass@1 on TELEMATH, surpassing distilled-only training by +17.65 points and outperforming a thinking-enabled baseline by +2.94 points
- Training loss improvements: DTA-trained models show ~43% lower initial loss and ~33% lower asymptote versus raw SFT, indicating cleaner supervision
- Edge efficiency: Reduces energy per output token by ~42% and latency by ~60% versus strong baselines, demonstrating significant inference cost savings
- Token-shift analysis: Gains concentrated on logical-structural discourse markers rather than domain content tokens, indicating improved reasoning scaffolding

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Debt Reduction
Standard distillation produces rationales reflecting teacher habits (verbosity, step granularity, tone), creating a distributional mismatch between training targets and what the student naturally expresses. This "trajectory debt" increases gradient noise and effective label entropy. DTA rewrites trajectories using a style guide extracted from the student's own outputs, reducing this mismatch before SFT. The core assumption is that the student model has a recoverable, consistent style that can be captured prescriptively, and lower cross-entropy on aligned trajectories generalizes to held-out tasks.

### Mechanism 2: Logical-Structural Token Amplification
By rewarding clarity, step demarcation, and constraint checks during selection, DTA privileges candidates with explicit discourse markers (e.g., "therefore," "evaluated," "derived"). These markers stabilize multi-step derivations and reduce error propagation in unit/constraint-heavy tasks. The assumption is that explicit discourse structure reduces local ambiguity in intermediate steps, and models learn to use these tokens as lightweight control flow for reasoning.

### Mechanism 3: Pass@1-Driven Efficiency Transfer
DTA improves pass@1 (72.45% vs. 54.80% for distilled-only), reducing the need for multi-sample voting. Combined with disabling explicit "thinking" modes, this cuts energy and latency at equivalent or better accuracy. The assumption is that accuracy gains transfer from curated training distribution to test distribution, and the cost of training-time curation is amortized over many inference calls.

## Foundational Learning

- **Knowledge Distillation Basics**
  - Why needed: DTA extends distillation by addressing "trajectory debt"—understanding what distillation does well (quantity, coverage) and where it fails (style mismatch) is prerequisite.
  - Quick check: Given a teacher solution with verbose intermediate steps, would a student with different narration habits learn optimally from it without modification?

- **Inductive Bias and Style Consistency**
  - Why needed: The framework extracts and applies a prescriptive style guide to the student's outputs; understanding how models encode stylistic preferences helps assess alignment feasibility.
  - Quick check: If you prompt a model to imitate its own past outputs, will it reproduce consistent structure, or does style vary unpredictably across domains?

- **Cross-Entropy as Supervision Quality Proxy**
  - Why needed: DTA's effectiveness is partly evidenced by lower training loss (faster descent, lower asymptote), implying cleaner signal. Knowing when loss correlates with generalization is critical.
  - Quick check: Does lower training loss always indicate better generalization, or can it reflect overfitting to a narrow style?

## Architecture Onboarding

- **Component map:** Teacher ensemble (DeepSeek-V3, Qwen3-235B, Kimi-K2) -> candidate generation -> peer-review filtering -> MinHash + semantic decontamination -> student style extraction -> trajectory rewriting -> dual reflection scoring -> top-K selection -> SFT on Qwen3-32B

- **Critical path:**
  1. Style induction quality—if the extracted style guide is weak or contradictory, downstream alignment degrades
  2. Reward/r-IFD weighting—incorrect weights may select verbose or over-concise candidates
  3. Decontamination rigor—residual near-duplicates inflate reported metrics

- **Design tradeoffs:**
  - Thinking mode vs. non-thinking at inference: DTA targets non-thinking deployment for edge efficiency; enabling thinking may narrow or eliminate the advantage
  - Teacher ensemble size: More teachers increase diversity but also filtering cost and potential disagreement
  - Reward weight allocation: Current (0.5, 0.2, 0.2, 0.1) prioritizes correctness; domains requiring deeper derivations may need higher completeness weights

- **Failure signatures:**
  - Training loss plateaus early or oscillates—suggests style guide or reward signals are inconsistent
  - Token-shift analysis shows content-heavy but logic-light gains—indicates reward not emphasizing structural markers
  - Pass@1 improves but cons@16 lags—suggests alignment overfits to specific phrasing, reducing sampling diversity

- **First 3 experiments:**
  1. Ablate style alignment: Train on DTA without style rewriting (raw distilled + selection only) to isolate the contribution of trajectory alignment vs. selection
  2. Reward weight sweep: Vary (w_corr, w_comp, w_clar, w_conc) and measure impact on pass@1, token-shift profile, and output length distribution
  3. Domain transfer test: Apply DTA to a different low-resource domain (e.g., legal reasoning) with same student model to assess whether benefits are telecom-specific or generalizable

## Open Questions the Paper Calls Out
- Can formal verification methods replace or augment the reflection-based LLM judge to eliminate reward hacking and correlated errors in the selection phase?
- Does aligning training data strictly to a single student model's inductive biases lead to over-regularization that harms generalization to out-of-distribution tasks?
- How can inference-time efficiency metrics be integrated directly into the DTA candidate selection criteria to co-optimize for accuracy and edge constraints?
- Does the DTA framework transfer effectively to multimodal telecommunications data involving tables, signal diagrams, and parameter charts?

## Limitations
- Style guide fidelity depends on the assumption that student models have consistent, recoverable styles that can be extracted prescriptively
- Domain generalizability is unproven beyond telecommunications mathematics, with token-shift advantages potentially being domain-specific
- Inference-time tradeoffs rely on disabling thinking modes, which may narrow or eliminate efficiency gains if thinking is enabled

## Confidence
- **High confidence**: The empirical claim that DTA-trained models achieve state-of-the-art accuracy (72.45% pass@1) and outperform distilled-only baselines (+17.65 points) is well-supported by reported experiments and metrics
- **Medium confidence**: The mechanism that trajectory debt reduction improves transfer is plausible but relies on assumptions about style consistency and cross-entropy as a quality proxy, with indirect evidence from training loss curves and word clouds
- **Low confidence**: The assertion that DTA's efficiency gains are directly attributable to higher pass@1 (rather than other factors like model architecture or inference optimization) is not rigorously isolated

## Next Checks
1. Ablate style alignment: Train a baseline using DTA without the style rewriting step (i.e., raw distilled outputs + selection only) to isolate the contribution of trajectory alignment versus selection heuristics
2. Reward weight sweep: Systematically vary the reward weights (w_corr, w_comp, w_clar, w_conc) and measure impact on pass@1, token-shift profile, and output length distribution to identify optimal trade-offs for different task types
3. Domain transfer test: Apply DTA to a different low-resource domain (e.g., legal reasoning or biomedical question-answering) using the same student model to assess whether benefits generalize beyond telecommunications mathematics