---
ver: rpa2
title: 'Via Score to Performance: Efficient Human-Controllable Long Song Generation
  with Bar-Level Symbolic Notation'
arxiv_id: '2508.01394'
source_url: https://arxiv.org/abs/2508.01394
tags:
- generation
- song
- music
- bach
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BACH introduces a bar-level symbolic score generation approach
  for long-form song generation, addressing controllability, generalizability, and
  efficiency limitations in existing audio-based models. The method uses ABC notation
  and a Dual-NTP architecture to separate vocal and accompaniment token streams, combined
  with a bar-stream patching tokenization strategy to preserve rhythmic structure.
---

# Via Score to Performance: Efficient Human-Controllable Long Song Generation with Bar-Level Symbolic Notation

## Quick Facts
- arXiv ID: 2508.01394
- Source URL: https://arxiv.org/abs/2508.01394
- Reference count: 3
- Outperforms commercial systems (Suno, Udio) in content quality, alignment, and vocal agility while generating longer songs with smaller model size and faster speed

## Executive Summary
BACH introduces a bar-level symbolic score generation approach for long-form song generation, addressing controllability, generalizability, and efficiency limitations in existing audio-based models. The method uses ABC notation and a Dual-NTP architecture to separate vocal and accompaniment token streams, combined with a bar-stream patching tokenization strategy to preserve rhythmic structure. Experiments show BACH achieves state-of-the-art performance across automatic metrics, outperforming commercial systems like Suno and Udio in content quality, alignment, and vocal agility, while producing longer songs with smaller model size and faster generation speed. Human evaluations confirm its superiority in musicality, controllability, and perceptual quality.

## Method Summary
BACH employs a three-stage pipeline: (1) a Qwen-based LLM parses user prompts into lyrics and 200 style tags, (2) a 1B-parameter Dual-NTP Transformer generates ABC scores using bar-stream patching (16-character patches) and separate vocal/accompaniment streams, and (3) external renderers (FluidSynth for instruments, VOCALOID for vocals) convert scores to audio. The model uses Chain-of-Score conditioning with a 2:1 ratio of CoS to in-context learning tokens during training, and sampling parameters include top-k=50, top-p=0.93, and temperature=1.

## Key Results
- Achieves state-of-the-art performance on automatic metrics including KL Divergence, FAD, and CLaMP scores
- Outperforms commercial systems (Suno, Udio) in human preference evaluations for musicality and controllability
- Generates 4-minute songs with 1B parameters, demonstrating superior efficiency compared to audio-native models requiring 7B+ parameters

## Why This Works (Mechanism)

### Mechanism 1: Bar-Level Semantic Patching
Grouping musical events into bar-level patches rather than byte-level streams preserves rhythmic structure and improves long-term coherence by aligning tokenization with the natural strong/weak beat structure of music. The bar serves as the fundamental semantic unit for perceiving musical quality.

### Mechanism 2: Track-Decoupled Joint Modeling (Dual-NTP)
Decoupling vocal and accompaniment streams while maintaining a joint probability space prevents "drowning out" while ensuring synchronization. The Dual-NTP architecture emits a tuple (vocal token, accompaniment token) at each time step, optimizing the joint conditional probability.

### Mechanism 3: Symbolic Dimensionality Reduction (Compose-First)
Generating human-readable symbolic scores (ABC notation) instead of raw audio reduces the learning burden, allowing a smaller model to achieve higher controllability and duration. The system decouples "composition" (theory/structure) from "performance" (timbre/audio synthesis).

## Foundational Learning

- **ABC Notation**: Text-based music format used as BACH's input/output representation. Why needed: Unlike MIDI or sheet music, ABC is human-readable and editable. Quick check: Given `|: c2 d2 e2 f2 | g4 a4 |`, can you identify the notes and bar structure?

- **Autoregressive Factorization vs. Joint Probability**: Understanding how BACH predicts the future requires grasping how p(x_t | x_<t) differs from the Dual-NTP approach of p(v_t, a_t | v_<t, a_<t). Quick check: Why might modeling two tokens (vocal + accomp) at time t be harder than one, and how does conditioning on both streams' past help?

- **Domain-Specific Priors in Tokenization**: BACH replaces general-purpose BPE with domain-specific bar-patching. Quick check: Why would a standard language tokenizer potentially split a musical phrase in a way that destroys its semantic meaning to a neural network?

## Architecture Onboarding

- **Component map**: Qwen LLM -> BACH Core (Patch-level Decoder -> Character-level Decoder) -> Renderer (FluidSynth + VOCALOID)
- **Critical path**: The Patch-level Decoder in BACH is the bottleneck for long-form coherence. If this component fails to maintain context, the song structure collapses.
- **Design tradeoffs**: Editability vs. Audio Quality (symbolic files allow editing but restrict audio to synthesizer capabilities) and Model Size vs. Duration (offloading synthesis allows 4-minute songs with 1B parameters).
- **Failure signatures**: Syntax Hallucination (invalid ABC notation), Structural Drift (repeating verses or abrupt transitions), Lyric-Melody Mismatch (phonetic timing not aligning with note durations).
- **First 3 experiments**:
  1. Tokenizer Ablation: Compare BACH's bar-patching vs. standard BPE on rhythmic stability
  2. Stream Decoupling Test: Measure vocal clarity (SIR) between single-token vs. Dual-NTP prediction
  3. Long-Context Stress Test: Generate 4-minute songs with structural constraints to verify Chain-of-Score conditioning

## Open Questions the Paper Calls Out

- **Post-score diffusion refinement**: Would integrating a post-score diffusion refinement step significantly improve fine-grained control over genre and emotion? The authors note that "Genre and emotion control... are weaker" and suggest diffusion could address this limitation.

- **Evaluation metric redesign**: How can automatic evaluation frameworks be redesigned to better capture human subjective preferences? The paper highlights that "existing automatic measures often deviate from human subjective perception."

- **Dedicated score-to-audio model**: Can training a dedicated neural score-to-audio model enhance rendering of subtle note variations and vocal techniques compared to the current FluidSynth and VOCALOID pipeline? The authors propose this to improve audio quality.

## Limitations

- Dataset and architecture opacity: Relies on unpublished proprietary dataset with unspecified model architecture details beyond parameter count
- External renderer dependency: Audio quality contingent on external synthesizers (VOCALOID, FluidSynth) which may lack expressive nuance
- Evaluation scope: Comparisons may not fully isolate symbolic approach impact from architectural innovations

## Confidence

- **High Confidence**: Core architectural claims (Dual-NTP, bar-stream patching) are internally consistent and logically grounded in stated problem
- **Medium Confidence**: Quantitative results likely valid but require independent replication due to dataset opacity
- **Low Confidence**: Robustness in non-standard musical contexts and long-term stability for complex structural prompts not thoroughly validated

## Next Checks

1. **Cross-Dataset Generalization**: Train and evaluate BACH on publicly available symbolic music dataset (e.g., MAESTRO) to verify mechanism generalization

2. **Expressive Rendering Benchmark**: Compare rendered audio quality against audio-native models using objective metrics (MCD, spectral similarity) to quantify controllability vs. expressiveness trade-off

3. **Stress-Test Structural Coherence**: Generate songs with progressively complex constraints (key changes, irregular time signatures) to test Chain-of-Score conditioning and bar-level patching limits