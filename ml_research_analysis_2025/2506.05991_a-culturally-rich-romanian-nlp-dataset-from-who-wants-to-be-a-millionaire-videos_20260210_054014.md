---
ver: rpa2
title: A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?"
  Videos
arxiv_id: '2506.05991'
source_url: https://arxiv.org/abs/2506.05991
tags:
- romanian
- cultural
- performance
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel, culturally-rich multilingual dataset
  derived from Romanian "Who Wants to Be a Millionaire?" videos, annotated for question
  domain, cultural relevance, and difficulty. The dataset was created using OCR, automated
  text extraction, and manual verification, then used to benchmark state-of-the-art
  LLMs including Romanian-adapted models.
---

# A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos

## Quick Facts
- arXiv ID: 2506.05991
- Source URL: https://arxiv.org/abs/2506.05991
- Authors: Alexandru-Gabriel Ganea; Antonia-Adelina Popovici; Adrian-Marius Dumitran
- Reference count: 16
- Primary result: State-of-the-art LLMs show significant performance gaps on Romanian cultural vs. international questions

## Executive Summary
This paper introduces a novel, culturally-rich multilingual dataset derived from Romanian "Who Wants to Be a Millionaire?" videos, annotated for question domain, cultural relevance, and difficulty. The dataset was created using OCR, automated text extraction, and manual verification, then used to benchmark state-of-the-art LLMs including Romanian-adapted models. Results show significant performance disparities: models achieve 80-95% accuracy on international questions but only 50-75% on Romanian-specific cultural questions. Fine-tuning for Romanian improves linguistic adaptation but not cultural knowledge retrieval. Cross-lingual tests reveal consistent performance hierarchies (English > Romanian > French). Translating Romanian questions to English slightly decreased accuracy, underscoring the importance of native-language benchmarks. These findings highlight the need for culturally-grounded datasets to build robust, inclusive NLP systems, especially for educational domains. The dataset is publicly available on Hugging Face.

## Method Summary
The authors extracted 1000 MCQA questions from Romanian WWTBM videos using OCR with Gemini 1.5 Flash, followed by diacritic restoration and duplicate removal. Questions were annotated for cultural relevance (Romanian vs. international), difficulty (based on monetary value), and 12 topic categories using Qwen2.5-72B-Instruct with manual validation. The dataset was evaluated using zero-shot prompting across 12 models including Romanian fine-tuned variants. Evaluation used standardized prompts (max_tokens=1, temperature=0) with deterministic single-letter output.

## Key Results
- LLMs achieve 80-95% accuracy on international questions but only 50-75% on Romanian-specific cultural questions
- Romanian fine-tuning improves linguistic adaptation more than cultural knowledge acquisition
- Translation from Romanian to English decreased accuracy, highlighting native-language benchmark importance
- Performance hierarchy confirmed: English > Romanian > French across cross-lingual tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit significant performance gaps on culturally-specific questions versus internationally generalizable questions, independent of language understanding
- Core assumption: Performance gap reflects training data distribution rather than evaluation artifact
- Evidence: Paired t-test rejected null hypothesis (t(11)=18.82, p<0.001), confirming statistically significant cultural gap; GRILE and MedQARo papers show similar domain-specific challenges
- Break condition: If Romanian-specific questions were systematically harder rather than culturally grounded

### Mechanism 2
- Claim: Romanian fine-tuning improves linguistic adaptation more than cultural knowledge acquisition
- Core assumption: Fine-tuning datasets emphasize linguistic coverage over cultural entity density
- Evidence: RoGemma2-9B and RoMistral-7B improved mainly on international questions, suggesting better linguistic adaptation rather than direct cultural knowledge gains
- Break condition: If fine-tuning corpora were enriched with Romanian cultural knowledge

### Mechanism 3
- Claim: Native-language evaluation preserves culturally-grounded meaning that translation disrupts
- Core assumption: Translation artifacts systematically degrade culturally-grounded queries
- Evidence: Translation did not help and led to slightly worse performance; example: "cai verzi pe pereți" loses idiomatic meaning when translated
- Break condition: If translation quality were extremely high with cultural context preservation

## Foundational Learning

- **Zero-shot prompting**: Why needed - all model evaluations use zero-shot MCQA without examples, testing intrinsic knowledge rather than in-context learning
  - Quick check: Can you explain why zero-shot evaluation isolates pre-trained knowledge from few-shot adaptation?

- **Cultural grounding in NLP**: Why needed - paper distinguishes language competence from cultural knowledge; understanding this distinction is essential for interpreting results
  - Quick check: How would you distinguish a model's linguistic error from a cultural knowledge gap in a QA task?

- **OCR and multimodal extraction pipeline**: Why needed - dataset creation required extracting text from video frames using Gemini 1.5 Flash, followed by diacritic restoration
  - Quick check: What post-processing steps might be necessary when extracting Romanian text from video OCR?

## Architecture Onboarding

- **Component map**: Video → Frame capture (at answer-highlight moments) → Gemini 1.5 Flash OCR → Structured Q&A pairs → Diacritic restoration → Deduplication → Annotation → Evaluation

- **Critical path**: Dataset quality depends on OCR accuracy → diacritic restoration → duplicate removal; annotation reliability depends on LLM-based categorization + manual validation; benchmark validity depends on standardized prompting and deterministic inference

- **Design tradeoffs**: Dataset size (1000 questions) enables controlled analysis but limits statistical robustness; monetary value as difficulty proxy provides natural signal but may not align with general knowledge difficulty; LLM-based annotation is scalable but introduces potential errors

- **Failure signatures**: Models outputting non-letter responses (marked as 'x' placeholder); consistent accuracy drops on specific topics (Literature, Music, General Culture); smaller models (<9B parameters) showing near-random performance on Romanian-specific questions; translation-to-English degrading rather than improving performance

- **First 3 experiments**:
  1. Baseline cultural gap quantification: Run all 12 models on Romanian-specific vs. international subsets, compute paired accuracy difference and statistical significance
  2. Cross-lingual transfer test: Evaluate selected models on original Romanian, French, and English datasets to confirm English > Romanian > French hierarchy
  3. Translation degradation analysis: Translate Romanian questions to English, re-evaluate top models, compare against original-language scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms cause translating Romanian questions to English to decrease model accuracy rather than improve it, despite English being dominant in pre-training data?
- Basis: Authors report "translating Romanian questions to English decreased accuracy, underscoring the value of native-language benchmarks"
- Why unresolved: Paper documents phenomenon but doesn't investigate underlying causes - whether information loss in translation, loss of cultural framing, or failure to transfer Romanian-specific entity knowledge
- What evidence would resolve it: Ablation studies comparing translation quality, entity-preserving vs. literal translation, and controlled experiments isolating linguistic vs. cultural information loss

### Open Question 2
- Question: What training approaches could effectively embed Romanian cultural knowledge, given that Romanian fine-tuning primarily improves linguistic adaptation without proportionate cultural knowledge gains?
- Basis: Paper states fine-tuning yields "mixed results, often improving linguistic adaptation more than cultural knowledge" and calls for developing models "adept across diverse cultural contexts"
- Why unresolved: Current fine-tuning methods may not expose models to sufficient culturally-specific content or may optimize for surface-level fluency over deep cultural grounding
- What evidence would resolve it: Comparing different fine-tuning regimes (cultural corpus augmentation, knowledge-graph integration, targeted cultural pre-training) on Romanian-specific question accuracy

### Open Question 3
- Question: Does the quiz show format's inherent biases limit the dataset's ability to represent broader Romanian cultural knowledge?
- Basis: Authors acknowledge dataset "reflects the knowledge areas, difficulty distribution, and cultural biases of the quiz show format, which may not fully represent broader Romanian culture"
- Why unresolved: Paper does not validate dataset representativeness against external Romanian cultural knowledge sources or compare with alternative collection methods
- What evidence would resolve it: Comparative analysis with Romanian cultural knowledge drawn from diverse sources to identify coverage gaps

## Limitations
- Dataset creation pipeline relies on OCR and automated extraction, which may introduce transcription errors despite manual verification
- Cultural knowledge gap attribution remains correlative rather than causal - performance differences may reflect question complexity rather than cultural knowledge deficits
- Translation artifact ambiguity - performance degradation could result from multiple factors beyond cultural information loss

## Confidence

- **High confidence**: Dataset creation methodology and basic evaluation results are reproducible and internally consistent; performance hierarchy and general pattern of Romanian fine-tuning improving linguistic but not cultural knowledge are well-supported
- **Medium confidence**: Interpretation that performance gaps primarily reflect cultural knowledge deficits is plausible but not definitively proven; mechanism explaining why Romanian fine-tuning doesn't significantly improve cultural knowledge is reasonable but relies on unverified assumptions about fine-tuning corpus composition
- **Low confidence**: Claim that native-language evaluation is essential for culturally-grounded meaning preservation is based on single empirical finding without detailed error analysis or comparison to human-level translation quality

## Next Checks
1. **Annotation reliability assessment**: Conduct inter-annotator agreement study on a subset of questions to quantify reliability of cultural context and topic categorization annotations and identify systematic biases
2. **Controlled complexity matching**: Create matched pairs of Romanian-specific and international questions controlled for syntactic complexity, vocabulary difficulty, and answer specificity; re-run cultural gap analysis on these matched subsets
3. **Translation quality benchmarking**: Compare model performance on Romanian questions translated to English by both machine translation and human translators; analyze error patterns to determine whether degradation specifically affects culturally-grounded queries