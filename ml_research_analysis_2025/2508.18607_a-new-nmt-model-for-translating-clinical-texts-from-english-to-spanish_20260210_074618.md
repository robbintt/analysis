---
ver: rpa2
title: A New NMT Model for Translating Clinical Texts from English to Spanish
arxiv_id: '2508.18607'
source_url: https://arxiv.org/abs/2508.18607
tags:
- translation
- words
- noov
- machine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOOV, a neural machine translation (NMT)
  system designed to translate clinical narratives from English to Spanish, addressing
  challenges such as the lack of parallel-aligned corpora and abundant unknown words.
  The system integrates a bilingual lexicon and a phrase look-up table to handle rare
  words and improve phrase generation.
---

# A New NMT Model for Translating Clinical Texts from English to Spanish

## Quick Facts
- arXiv ID: 2508.18607
- Source URL: https://arxiv.org/abs/2508.18607
- Reference count: 33
- NOOV achieves BLEU scores of 34.71 on EHR data, outperforming baselines by 4.68 points

## Executive Summary
This paper introduces NOOV, a neural machine translation system designed to translate clinical narratives from English to Spanish. The system addresses key challenges in clinical machine translation including rare medical terminology and phrase generation by integrating a bilingual lexicon and phrase look-up table into the decoding process. NOOV combines standard NMT with external knowledge sources to handle unknown words and reduce repetition, showing significant improvements over subword and hybrid NMT baselines on both automatic and human evaluation metrics.

## Method Summary
NOOV is a biLSTM-based encoder-decoder model with attention that integrates external knowledge during decoding. At each step, it combines the standard decoder output with lexicon-derived probabilities using a weighted sum (α=0.2). The system uses Giza++-extracted bilingual lexicons from parallel corpora and phrase pairs from UMLS 2017AB. When the decoder predicts consecutive identical words, it consults the phrase table to break repetition loops. The model is pre-trained on MedlinePlus data then fine-tuned on a custom EHR corpus, with evaluation using BLEU and human assessment of adequacy and fluency.

## Key Results
- On EHR dataset: BLEU score of 34.71 (vs 30.03 and 31.06 for baselines)
- Human evaluators rated NOOV higher in adequacy (3.26 vs 2.78 and 2.97) and fluency (2.84 vs 2.64 and 2.56)
- NOOV particularly effective for longer sentences and medical jargon
- System still struggles with very long sentences and grammar in some cases

## Why This Works (Mechanism)

### Mechanism 1
Integrating bilingual lexicon into decoding improves rare word translation by providing external supervision when decoder representations are insufficient. At each step, the model computes output_i = α·prob_i(V^t) + (1-α)·v(o'_i), where prob_i(V^t) comes from attention-weighted lexicon alignments. The lexicon provides translation probabilities p_j^k for source word w_j^s mapping to target word tok_k^t. This addresses the infrequency problem where seq-to-seq models struggle to learn rare words from limited examples.

### Mechanism 2
Phrase look-up table intervention reduces word repetition and improves multi-word expression translation. When consecutive identical predictions are detected (indicating stuck attention), the system queries the phrase table T using source context w_s^* and previously predicted word w_t^i. It selects the target phrase containing w_t^i whose source phrase appears in the input sentence, preferring longest matches. This breaks the repetition loop and provides grammatically coherent continuations.

### Mechanism 3
Context-aware lexicon extraction improves translation relevance by filtering to sentences sharing vocabulary with current input. Beyond global lexicon from Giza++, the model constructs a local lexicon at decode time by extracting alignments only from sentence pairs that share at least one word with the current source sentence. This addresses polysemy where one source word may map to different targets in different contexts.

## Foundational Learning

- **Attention mechanism in sequence-to-sequence models**: Why needed: NOOV's lexicon integration relies on attention weights att_i^j to determine which source words influence each target word prediction. Quick check: Given attention weights [0.1, 0.7, 0.2] over source words ["patient", "denies", "pain"], which source word most influences the current decoder step?

- **Word alignment and translation probability in statistical MT**: Why needed: The lexicon L contains p(word_target | word_source) probabilities learned via Giza++ using EM algorithm. Understanding that alignment models estimate these probabilities from co-occurrence patterns is essential for diagnosing lexicon quality issues. Quick check: If "polyuria" appears 5 times in parallel corpus, aligned to "poliuria" 4 times and "orina excesiva" 1 time, what are the lexicon probabilities?

- **Out-of-vocabulary (OOV) problem in neural language models**: Why needed: Clinical texts have 25.6% unknown words in the EHR corpus. Understanding why NMT systems traditionally replace rare words with `<unk>` tokens explains why external lexicon guidance is needed. Quick check: Why can't a standard seq2seq model with fixed vocabulary translate a drug name it has never seen during training?

## Architecture Onboarding

- **Component map**: Input Sentence (English) -> [Encoder: biLSTM] -> hidden states -> [Decoder: LSTM + Attention] -> Lexicon path + Standard path -> Combination -> [Repetition Detector] -> Phrase Table Lookup -> Output Word (Spanish)

- **Critical path**: Data preparation: Align parallel corpora using Giza++ to extract lexicon L; extract phrase pairs from UMLS 2017AB. Model training: Train biLSTM encoder + LSTM decoder with attention on combined data. Inference: At each decode step, compute attention, query lexicon, combine with decoder output; check for repetition and consult phrase table if needed.

- **Design tradeoffs**: α parameter (0.2) balances lexicon trust vs decoder trust; global vs local lexicon adds context relevance but computational overhead; UMLS provides high-precision phrases but may miss informal clinical language; pre-training on Medline then fine-tuning on EHR optimizes domain adaptation.

- **Failure signatures**: Very long sentences cause BLEU degradation and incoherent output; grammar errors despite correct vocabulary (missing articles); broken sentence structure from de-identification artifacts; repetition loops persist when phrase table lacks required entries.

- **First 3 experiments**: 1) Ablation study: Run NOOV with lexicon-only, phrase-table-only, and full system on EHR test set, measuring BLEU and unknown-word accuracy. 2) α sensitivity analysis: Sweep α ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5} on development set, plotting BLEU vs α. 3) Length-stratified error analysis: Bin test sentences by length and compute BLEU, manually categorizing errors (vocabulary, grammar, repetition, omission).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does NOOV's advantage over subword baselines change as in-domain parallel training corpus volume increases? The authors hope to create a large parallel-aligned EHR corpus in future work, contrasting with their design goal of requiring little in-domain data.

- **Open Question 2**: Can NOOV's lexicon and phrase look-up integration be adapted to non-recurrent architectures like Transformers to resolve failure in translating very long sentences? The authors propose to explore other models in future work, noting NOOV failed to translate very long sentences due to LSTM limitations.

- **Open Question 3**: Are the reported improvements in adequacy and fluency statistically robust when evaluated by a larger pool of medical professionals? The methodology notes human evaluation was conducted by a single physician, introducing subjectivity and limiting statistical power.

## Limitations

- Performance improvements rely heavily on comparisons with baselines, but absolute performance remains modest (BLEU mid-30s), suggesting significant room for improvement
- Context-aware lexicon mechanism lacks validation through empirical comparison with global lexicon approach
- UMLS-based phrase table may not capture informal clinical language patterns found in EHR narratives
- System struggles with very long sentences (>40 words), producing grammatical errors and missing clauses

## Confidence

**High Confidence**: NOOV outperforms baseline NMT systems (BLEU 34.71 vs 30.03 and 31.06; human adequacy 3.26 vs 2.78 and 2.97)
**Medium Confidence**: Lexicon integration improves rare clinical term translation (mechanistically plausible but direct OOV impact not measured)
**Low Confidence**: Context-aware lexicon extraction significantly improves translation quality (mechanism described but no empirical validation)

## Next Checks

1. **Ablation study to isolate component contributions**: Implement and evaluate three variants of NOOV: lexicon-only without phrase table, phrase table-only without lexicon integration, and full NOOV system. Compare BLEU scores and unknown word accuracy across all variants on the EHR test set to determine which component drives performance improvements.

2. **Context-aware lexicon efficacy test**: Modify inference pipeline to use only global lexicon (without vocabulary overlap filtering) and compare translation quality against full context-aware approach. Measure changes in BLEU score, adequacy ratings, and analyze specific examples where polysemy might be an issue.

3. **Phrase table coverage and intervention analysis**: Analyze EHR test set to determine: (a) what percentage of sentences contain repetition loops that would trigger phrase table lookup, (b) what percentage of these loops are successfully resolved by the phrase table, and (c) what types of medical phrases are most/least covered by UMLS.