---
ver: rpa2
title: 'Replacing Parameters with Preferences: Federated Alignment of Heterogeneous
  Vision-Language Models'
arxiv_id: '2602.00485'
source_url: https://arxiv.org/abs/2602.00485
tags:
- reward
- preference
- routing
- alignment
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning vision-language
  models (VLMs) in federated settings where clients have heterogeneous data distributions
  and model architectures. Traditional federated learning approaches based on parameter
  sharing are inefficient and privacy-sensitive, particularly in domains like healthcare
  and finance.
---

# Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models

## Quick Facts
- arXiv ID: 2602.00485
- Source URL: https://arxiv.org/abs/2602.00485
- Reference count: 19
- Primary result: MoR achieves 94.55% win rate on Medical domain vs 67.27% for individual models in homogeneous federated alignment

## Executive Summary
This paper introduces MoR, a federated alignment framework that shifts from parameter sharing to preference-based alignment for heterogeneous vision-language models. Instead of exchanging model weights, each client trains a local reward model on private preference data and uploads only these models to a central server. A lightweight routing network is collaboratively learned to select and combine these heterogeneous reward signals based on input-response embeddings. The approach addresses privacy concerns and enables alignment of VLMs with different architectures while maintaining computational efficiency and cross-client adaptability.

## Method Summary
MoR operates through a three-stage process: First, each client trains a local reward model using Bradley-Terry pairwise loss on private preference data, capturing client-specific evaluation criteria without exposing raw data. Second, these reward models are uploaded to a central server where a lightweight routing network is federated to compose heterogeneous reward signals, selecting appropriate evaluators per input through softmax-weighted combinations. Third, the central server performs GRPO optimization of the base VLM using routed rewards, with the routing network updated online via Neural Thompson Sampling to handle distribution shifts as the policy evolves. The framework assumes an honest central server and leverages frozen vision towers to stabilize routing signals across heterogeneous architectures.

## Key Results
- Outperforms aggregation baselines (Avg RM, Weighted RM) with 94.55% win rate on Medical domain in homogeneous setting
- Demonstrates superior cross-client adaptability, with 15.35% improvement over individual models in heterogeneous settings
- Router effectively mitigates "bucket effect," maintaining performance even with weak reward models present
- Online router updates via Neural Thompson Sampling provide stability during GRPO training

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Preference from Parameters
Shifting from parameter sharing to preference-based alignment enables scalable, privacy-preserving federated VLM training under client heterogeneity. Each client trains a local reward model on private preference data using Bradley-Terry pairwise loss. Only the trained reward models (not raw data or policy parameters) are uploaded to a central server. A lightweight routing network is then federated to compose these heterogeneous reward signals, selecting the appropriate evaluator per input. Core assumption: Reward models capture client-specific evaluation criteria that are transferable and composable, even when model architectures differ across clients.

### Mechanism 2: Routing-Based Fusion of Heterogeneous Rewards
A learned routing network adaptively aggregates client reward signals, mitigating the "bucket effect" where weak models degrade aggregated performance. The routing network takes input-response pairs and outputs softmax weights over reward models. Mixed reward is computed as a weighted combination. Routing is trained via federated learning on preference tuples, with each client minimizing negative log-likelihood of observed preferences under the mixed reward margin. Core assumption: Input-response embeddings encode sufficient signal for the router to identify which reward model is most reliable for a given context.

### Mechanism 3: Online Router Adaptation via Neural Thompson Sampling
Online router updating during GRPO prevents distribution mismatch between static router training data and evolving policy outputs. The router is framed as a contextual bandit. Neural Thompson Sampling maintains a Gaussian posterior over routing parameters, balancing exploration and exploitation. The router selects a reward model per input-response, receives binary feedback based on GRPO objective improvement, and updates via regularized squared loss. Core assumption: Policy evolution causes gradual, not catastrophic, distribution shift; visual embeddings remain stable due to frozen vision tower.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: Local reward models are trained using pairwise preference learning grounded in this model; understanding the loss formulation is essential for debugging reward quality.
  - Quick check question: Can you explain why log-Ïƒ(reward_margin) is an appropriate loss for preference pairs?

- **Concept: Mixture-of-Experts (MoE) Routing**
  - Why needed here: The routing network design draws from MoE principles; familiarity with softmax gating and expert selection helps understand how heterogeneous reward models are combined.
  - Quick check question: What distinguishes hard vs. soft routing, and why does MoR use soft routing during training but hard selection during online GRPO?

- **Concept: Contextual Bandits and Thompson Sampling**
  - Why needed here: Online router updates use Neural Thompson Sampling; understanding posterior sampling over network parameters is critical for tuning exploration-exploitation.
  - Quick check question: How does Thompson Sampling differ from UCB-based exploration in balancing exploration vs. exploitation?

## Architecture Onboarding

- **Component map**: Local Reward Models (per-client) -> Routing Network (central) -> GRPO Optimizer (central) -> Base VLM (central)

- **Critical path**: 
  1. Each client trains local reward model on private preference data (Bradley-Terry loss)
  2. Upload reward models to server; clients download full set
  3. Federated router training: clients minimize preference NLL under mixed rewards; server aggregates via FedAvg
  4. Upload router to server; clients transmit only non-preference input embeddings
  5. Server runs GRPO with routed rewards; router updated online via Neural Thompson Sampling

- **Design tradeoffs**:
  - Query-level vs. batch-level routing: Query-level is more accurate but slower; batch-level reduces compute at cost of routing precision
  - Hard vs. soft routing: Soft for training stability; hard for online exploration efficiency
  - Freezing vs. updating vision tower: Frozen vision tower stabilizes routing signals but limits visual adaptation

- **Failure signatures**:
  - Router collapses to single reward model (check routing weight entropy)
  - GRPO objective plateaus or oscillates (may indicate exploration noise or stale router)
  - Weak clients drag down Avg RM baseline but MoR should recover (verify routing selects strong models)

- **First 3 experiments**:
  1. Validate local reward model quality: Train and evaluate each client's reward model on held-out preference data; confirm rank correlation with ground-truth preferences
  2. Router overfitting check: Visualize routing weight distributions on validation vs. training inputs; monitor entropy collapse
  3. Ablate online router update: Run GRPO with frozen router vs. online Thompson Sampling; compare final VLM scores and routing stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MoR framework be extended to handle dynamic federated environments where the number of participating clients fluctuates over time?
- Basis in paper: The conclusion states, "Future work may explore extending MoR to more dynamic federated environments, particularly scenarios where the number of participating clients changes over time."
- Why unresolved: The current implementation assumes a fixed client set for training the routing network and aggregating rewards. The routing mechanism (softmax over K clients) and the federated learning process for the router would need adaptive mechanisms to incorporate new clients or handle departures without retraining the entire mixture from scratch.
- What evidence would resolve it: A modified MoR architecture capable of "warm-starting" new reward models into the mixture with minimal disturbance to the existing routing policy, demonstrated via experiments simulating client churn.

### Open Question 2
- Question: How can the framework be modified to ensure robustness and privacy preservation under a malicious or compromised central server?
- Basis in paper: Section 4.1 explicitly states, "Throughout this process, we assume that the central server is honest and non-malicious."
- Why unresolved: The current architecture relies on a trusted central server to aggregate routing parameters and perform the GRPO policy update. A malicious server could manipulate the aggregated router or policy updates to inject backdoors or infer private data properties, breaking the privacy guarantees.
- What evidence would resolve it: The integration of secure aggregation protocols or a move towards a fully decentralized (peer-to-peer) routing mechanism that maintains performance without a central coordinator.

### Open Question 3
- Question: How does the computational overhead and routing precision of MoR scale when the number of federated clients increases significantly (e.g., to hundreds or thousands) compared to the 3-client setup evaluated?
- Basis in paper: Section 5.1.4 limits the experimental setup to "a federated scenario with three clients," while the method claims to be a "scalable solution."
- Why unresolved: While the paper theoretically claims constant complexity for the policy update, the routing network must select from K reward models. With a massive K, the router's classification task becomes significantly harder, and the communication overhead of downloading/uploading numerous reward models might become prohibitive for edge clients.
- What evidence would resolve it: Empirical results from simulations with large client pools (N > 50), analyzing the router's accuracy in selecting the optimal expert and the end-to-end training latency.

## Limitations
- Relies on honest central server assumption without privacy-preserving mechanisms against malicious aggregation
- Router generalization capacity to unseen domains remains uncertain despite cross-client adaptability claims
- Online Thompson Sampling stability under rapid policy shifts not validated through long-horizon training dynamics

## Confidence
- **Medium**: Router generalization capacity and cross-client adaptability claims, as real-world deployment may reveal overfitting limitations
- **Low**: Online router update stability, due to lack of long-horizon training dynamics and rapid policy shift testing
- **Medium**: Heterogeneous architecture transfer robustness, as architectural differences may break reward model composability

## Next Checks
1. **Router generalization stress test**: Evaluate MoR on a held-out client with disjoint domain data (e.g., satellite imagery) to verify routing decisions remain effective when reward models encounter completely unseen content.

2. **Long-horizon online stability**: Run GRPO with online router updates for 50+ rounds while gradually increasing KL penalty; monitor whether routing entropy collapses or oscillates, indicating exploration-exploitation instability.

3. **Architecture transfer robustness**: Replace one reward model with a fundamentally different VLM architecture (e.g., SigLIP-based) and measure degradation in mixed reward quality and routing accuracy compared to homogeneous MoR runs.