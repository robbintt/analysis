---
ver: rpa2
title: Leveraging large language models and traditional machine learning ensembles
  for ADHD detection from narrative transcripts
arxiv_id: '2505.21324'
source_url: https://arxiv.org/abs/2505.21324
tags:
- adhd
- ensemble
- language
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces an ensemble framework combining a large
  language model (LLaMA3), a fine-tuned transformer (RoBERTa), and a traditional SVM
  classifier to detect ADHD from narrative transcripts. The models leverage complementary
  strengths: LLaMA3 captures semantic structure, RoBERTa provides contextual understanding,
  and SVM uses lexical features.'
---

# Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts

## Quick Facts
- arXiv ID: 2505.21324
- Source URL: https://arxiv.org/abs/2505.21324
- Reference count: 33
- Primary result: Ensemble achieved F1=0.71 with recall=0.91 on 441 narrative transcripts

## Executive Summary
This study introduces an ensemble framework combining LLaMA3 (70B), RoBERTa, and SVM to detect ADHD from narrative interview transcripts. The models leverage complementary strengths: LLaMA3 captures semantic structure through prompt-based reasoning, RoBERTa provides contextual understanding via fine-tuning, and SVM uses lexical features. Evaluated on 441 transcripts from the Healthy Brain Network dataset, the ensemble achieved an F1 score of 0.71 with improved recall (0.91), outperforming individual models. This hybrid approach demonstrates enhanced diagnostic accuracy and robustness, highlighting the value of integrating LLMs with traditional machine learning for psychiatric text classification.

## Method Summary
The approach integrates three complementary models: LLaMA3-70B operating in zero-shot mode with DSM-5 criteria prompts, a fine-tuned RoBERTa transformer for contextual understanding, and an SVM classifier using TF-IDF lexical features. The models are aggregated through majority voting (ADHD labeled if ≥2 of 3 models agree). LLaMA3 parses first-token responses to DSM-5 prompts, RoBERTa uses sliding window segments with majority voting for long texts, and SVM combines TF-IDF n-grams with engineered features. The ensemble was evaluated on 441 post-scan interview transcripts from the Healthy Brain Network dataset, achieving F1=0.71 with recall=0.91.

## Key Results
- Ensemble achieved F1=0.71, outperforming individual models (LLaMA3: F1=0.64, RoBERTa: F1=0.65, SVM: F1=0.63)
- Recall improved to 0.91 with precision of 0.61, demonstrating effective false negative reduction
- Majority voting provided robustness while maintaining individual model sensitivities
- The approach showed promise for clinical psychiatric text classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Complementary Inductive Biases Across Model Architectures
Each model captures distinct linguistic signals—LLaMA3 leverages pre-trained semantic knowledge through prompt-based reasoning about DSM-5 criteria; RoBERTa learns contextual patterns from supervised fine-tuning on clinical narratives; SVM extracts explicit lexical markers via TF-IDF n-gram features. These diverse inductive biases mean each model makes different error types, allowing the ensemble to filter idiosyncratic failures when models disagree.

### Mechanism 2: Majority Voting as Noise Filtering
The majority voting (≥2 of 3 models agree) provides robustness by requiring consensus while permitting individual model sensitivity. This filtering strategy is particularly effective when models have asymmetric precision-recall tradeoffs, preventing any single high-recall model from generating excessive false positives while still capturing positives detected by at least two models.

### Mechanism 3: Precision-Recall Asymmetry Exploitation
The ensemble achieves high recall (0.91) by combining high-sensitivity models (LLaMA3, RoBERTa) with a higher-precision model (SVM), improving F1 through balanced error reduction. LLaMA3 and RoBERTa cast wide nets catching true positives; SVM provides regularization. Majority voting requires agreement, preventing any single high-recall model from generating excessive false positives while still capturing positives detected by at least two models.

## Foundational Learning

- **Concept: Majority Voting Ensembles**
  - Why needed here: This is the core aggregation strategy; understanding when and why voting works is essential for reproducing results and diagnosing failures.
  - Quick check question: If two models have 60% accuracy and one has 55%, under what conditions would majority voting perform worse than the best individual model?

- **Concept: TF-IDF and N-gram Feature Extraction**
  - Why needed here: SVM's contribution depends entirely on these lexical features; understanding what they capture helps explain ensemble complementarity.
  - Quick check question: Would TF-IDF unigrams capture "I don't know" repetition patterns better than contextual embeddings? Why or why not?

- **Concept: Prompt Engineering for Classification Tasks**
  - Why needed here: LLaMA3 operates entirely through prompt-based inference without fine-tuning; prompt design is the lever for performance.
  - Quick check question: The paper embeds full DSM-5 criteria in the prompt. What are the tradeoffs of this approach versus a shorter, task-specific prompt?

## Architecture Onboarding

- **Component map:**
  - Transcripts → interviewer/interviewee separation → preprocessing
  - LLaMA3 path: Prompt template + transcript → LLaMA3-70B API → parse first token ("YES"/"NO") + justification text
  - RoBERTa path: Tokenized segments (sliding window, 512 tokens) → fine-tuned RoBERTa → segment-level predictions → majority vote across segments
  - SVM path: TF-IDF vectorization (1-4 grams, top 1000 features) + engineered features (response length, response count, question length) → RBF kernel SVM (C=1024)
  - Aggregation: Three binary predictions → majority voting (≥2 concurrences → ADHD)

- **Critical path:**
  1. Data split (60/20/20 stratified) with balanced class distribution
  2. LLaMA3 prompt iteration on development set (3 cycles of error analysis and revision)
  3. RoBERTa hyperparameter tuning (learning rate: {1e-5, 2e-5, 4e-5}, epochs: {10, 15, 20})
  4. SVM grid search (C: {2–2048}, kernel: {linear, RBF})
  5. Test set evaluation and ensemble aggregation

- **Design tradeoffs:**
  - Majority voting vs. weighted/stacked ensembles: Paper uses simple voting; does not incorporate confidence scores or learned weights
  - Zero-shot LLaMA3 vs. fine-tuning: No LLM fine-tuning attempted; prompt engineering only
  - RoBERTa sliding window vs. truncation: Overlapping 512-token windows preserve full transcript coverage at cost of computational overhead

- **Failure signatures:**
  - LLaMA3: High false positive rate (precision=0.54), over-predicts ADHD due to broad prompt interpretation
  - RoBERTa: Context window truncation artifacts if sliding window implementation is incorrect
  - SVM: Lower recall (0.75) may miss subtle or atypical ADHD language patterns
  - Ensemble: If two models correlate on errors, voting amplifies mistakes rather than correcting them

- **First 3 experiments:**
  1. **Baseline reproduction**: Replicate each individual model's metrics (accuracy, precision, recall, F1) on the provided train/dev/test splits to verify implementation correctness.
  2. **Ablation study**: Run ensemble with each model removed (3 configurations: LLaMA3+RoBERTa, LLaMA3+SVM, RoBERTa+SVM) to quantify each component's contribution.
  3. **Weighted voting exploration**: Replace majority voting with performance-weighted voting (weights from validation F1 scores) to test whether confidence-aware aggregation improves over hard voting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would weighted voting or stacking-based ensemble methods outperform the simple majority voting approach for ADHD classification?
- Basis in paper: "Future work could explore more sophisticated ensemble strategies, such as weighted voting or stacking" and the limitation that "majority voting... does not consider model confidence or weight individual model contributions."
- Why unresolved: The current implementation uses a binary majority rule, treating all models equally regardless of their individual confidence or historical accuracy on different cases.
- What evidence would resolve it: Comparative experiments with stacking meta-classifiers or confidence-weighted voting schemes on the same dataset, showing statistically significant F1 improvements.

### Open Question 2
- Question: How well does the ensemble framework generalize to narrative transcripts from different clinical settings, demographic populations, and age groups?
- Basis in paper: "The dataset used in this study is relatively small and may not capture the full variability present in broader clinical or community populations" and that data "may reflect biases in language use or reporting styles that differ across contexts or demographic groups."
- Why unresolved: The study uses a single source (Healthy Brain Network dataset, ages 5-21) with only 441 transcripts from one specific interview protocol.
- What evidence would resolve it: External validation on independent datasets from different clinical institutions, age ranges (e.g., adults), and narrative elicitation methods.

### Open Question 3
- Question: Can interpretable NLP techniques be integrated with the ensemble to provide clinically meaningful explanations for ADHD predictions?
- Basis in paper: "The interpretability of LLMs such as LLaMA3 remains a challenge, making it difficult to fully understand the decision-making process behind individual predictions" and future work should explore "integrating interpretable NLP techniques."
- Why unresolved: While LLaMA3 generates textual justifications, these are not systematically validated, and the SVM's feature importance is not leveraged in the final ensemble decision.
- What evidence would resolve it: Implementation of explainability methods (e.g., attention visualization, SHAP values for SVM) with clinician evaluation of whether explanations align with known ADHD linguistic markers.

## Limitations
- The ensemble's performance relies on transcripts from a single controlled interview protocol (The Present animated film), limiting generalizability
- LLaMA3's zero-shot approach achieved high recall but low precision (0.54), suggesting potential overfitting to this dataset's stylistic patterns
- The 441 transcript dataset size, while balanced, represents a single clinical population, limiting external validity claims
- The SVM's lexical features may not capture nuanced semantic relationships that larger models exploit

## Confidence
- **Ensemble superiority claims (High):** The F1=0.71 result and recall=0.91 are directly measurable from confusion matrices and statistical comparisons against baselines
- **Complementary model mechanisms (Medium):** While the paper describes different model approaches, the specific error correlation analysis between models is not reported
- **Generalizability claims (Low):** The paper does not validate the approach on external datasets or alternative interview protocols

## Next Checks
1. **Error correlation analysis:** Compute pairwise error matrices between all three models on the test set to empirically verify that errors are uncorrelated
2. **Cross-dataset validation:** Apply the trained ensemble to ADHD transcripts from a different source without retraining
3. **Component ablation under distribution shift:** Systematically remove each model from the ensemble and evaluate performance on transcripts with known challenging characteristics