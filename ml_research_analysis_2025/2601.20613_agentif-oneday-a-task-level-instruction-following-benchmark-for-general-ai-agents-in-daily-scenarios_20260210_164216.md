---
ver: rpa2
title: 'AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI
  Agents in Daily Scenarios'
arxiv_id: '2601.20613'
source_url: https://arxiv.org/abs/2601.20613
tags:
- tasks
- agent
- task
- workflow
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AgentIF-OneDay introduces a task-level instruction-following benchmark
  that evaluates general AI agents across realistic daily-use scenarios. The benchmark
  is structured around three user-centric categories: Open Workflow Execution, Latent
  Instruction Inference, and Iterative Refinement.'
---

# AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios

## Quick Facts
- arXiv ID: 2601.20613
- Source URL: https://arxiv.org/abs/2601.20613
- Authors: Kaiyuan Chen; Qimin Wu; Taiyu Hou; Tianhao Tang; Xueyu Hu; Yuchen Hou; Bikun Li; Chengming Qian; Guoyin Wang; Haolin Chen; Haotong Tian; Haoye Zhang; Haoyu Bian; Hongbing Pan; Hongkang Zhang; Hongyi Zhou; Jiaqi Cai; Jiewu Rao; Jiyuan Ren; Keduan Huang; Lucia Zhu Huang; Mingyu Yuan; Naixu Guo; Qicheng Tang; Qinyan Zhang; Shuai Chen; Siheng Chen; Ting Ting Li; Xiaoxing Guo; Yaocheng Zuo; Yaoqi Guo; Yinan Wang; Yinzhou Yu; Yize Wang; Yuan Jiang; Yuan Tian; Yuanshuo Zhang; Yuxuan Liu; Yvette Yan Zeng; Zenyu Shan; Zihan Yin; Xiaobo Hu; Yang Liu; Yixin Ren; Yuan Gong
- Reference count: 25
- Key outcome: AgentIF-OneDay introduces a task-level instruction-following benchmark that evaluates general AI agents across realistic daily-use scenarios. The benchmark is structured around three user-centric categories: Open Workflow Execution, Latent Instruction Inference, and Iterative Refinement. It comprises 104 tasks covering 767 scoring points, with instance-level rubrics and an evaluation pipeline achieving 80.1% agreement with human judgment using Gemini-3-Pro. Across four leading agents tested, Manus achieved the highest overall score of 0.645, while ChatGPT and Genspark tied in the next tier. Notably, API-based agents performed comparably to RL-based systems, indicating baseline agentic competence has become standardized in modern LLMs. The benchmark also revealed persistent challenges in implicit constraint inference and long-horizon consistency. AgentIF-OneDay provides both a diagnostic tool for current systems and a scalable methodology for generating verifiable agent tasks, offering a high-quality data source for future training.

## Executive Summary
AgentIF-OneDay presents a comprehensive benchmark for evaluating general AI agents on task-level instruction-following across realistic daily scenarios. The benchmark introduces three user-centric categories: Open Workflow Execution, Latent Instruction Inference, and Iterative Refinement, comprising 104 tasks with 767 scoring points. Through instance-level rubric scoring and a refined evaluation pipeline, the benchmark achieves 80.1% agreement between LLM-based verification and human judgment. Testing four leading agents revealed that while API-based agents perform comparably to RL-based systems, persistent challenges remain in implicit constraint inference and long-horizon consistency. The benchmark provides both a diagnostic tool for current systems and a scalable methodology for generating verifiable agent tasks, offering a high-quality data source for future training.

## Method Summary
AgentIF-OneDay evaluates general AI agents on 104 task-level instruction-following scenarios across three categories: Open Workflow Execution (53.8%), Latent Instruction Inference (25.0%), and Iterative Refinement (21.2%). Each task includes attachments, rubrics, and scoring points, with agents producing file-based deliverables (PPT, HTML, reports) evaluated by an LLM-as-judge pipeline using Gemini-3-Pro-preview with binary rubric scoring. The evaluation pipeline includes visual parsing for PPT/HTML using VLMs, web search for factual verification, and normalization of scores per task before averaging. Tasks are synthesized from human seed examples through a five-stage pipeline extracting workflow structure, searching for domain-relevant attachments, generating new queries, synthesizing rubrics, and filtering for verifiability and spatiotemporal independence.

## Key Results
- Manus achieved the highest overall score of 0.645 across four tested agents
- API-based agents (ChatGPT, Genspark) performed comparably to RL-based systems (Manus, Minimax-Agent)
- Latent Instruction Inference category showed the weakest performance across all agents, with agents struggling to maintain structural consistency when inferring implicit rules from attachments
- Binary rubric scoring achieved 80.1% agreement with human judgment, though discrepancies arose on subjective criteria like "conciseness" and "design sense"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-level rubric scoring with binary verification improves evaluation objectivity and LLM-human alignment.
- Mechanism: Each task decomposes into granular rubric items scored as satisfied/not-satisfied, separating bonus items (capability assessment) from penalty items (error detection). Normalized scoring clamps net scores at zero before averaging across tasks.
- Core assumption: Binary scoring reduces LLM judging bias compared to continuous scales.
- Evidence anchors:
  - [abstract] "employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate"
  - [section 3.2] "Binary Scoring, where each rubric item is strictly evaluated as satisfied or not to ensure objectivity and mitigate LLM judging bias"
  - [corpus] Weak corpus support—related benchmarks (IFEval, GAIA) use different evaluation paradigms; no direct comparison of binary vs. continuous scoring.
- Break condition: If rubrics contain subjective criteria (e.g., "conciseness," "design sense"), binary scoring fails to capture nuance, as noted in human-LLM discrepancy analysis.

### Mechanism 2
- Claim: Latent instruction inference requires agents to extract implicit constraints from attachments and generalize them—this remains the weakest capability across tested agents.
- Mechanism: Agents receive task prompts plus attachments (PDFs, images, structured files) containing unstated formatting rules or logic. Success requires: (1) parsing attachment structure, (2) inducing hidden patterns, (3) applying to new content generation.
- Core assumption: Implicit constraint extraction is distinct from explicit instruction following and tests deeper reasoning.
- Evidence anchors:
  - [abstract] "persistent challenges in implicit constraint inference and long-horizon consistency"
  - [section 3.1] "the agent cannot derive the answer directly from the prompt; it must analyze the attached iPhone_Plan.pdf to deduce complex pricing logic"
  - [corpus] Daily-Omni (arXiv:2505.17862) addresses cross-modal temporal alignment, but latent instruction inference from attachments remains underexplored in related work.
- Break condition: If attachments lack structured patterns or contain ambiguous formatting, inference becomes guesswork rather than reasoning.

### Mechanism 3
- Claim: A file-centered automated pipeline enables scalable synthesis of verifiable agent tasks from human seed examples.
- Mechanism: Five-stage pipeline: (1) extract workflow structure from seed task, (2) search for domain-relevant attachments, (3) generate new queries preserving workflow, (4) synthesize rubrics, (5) filter for verifiability and spatiotemporal independence.
- Core assumption: Workflow templates transfer across domains while preserving cognitive demands.
- Evidence anchors:
  - [section 4.2] "This process automatically generates new problems and corresponding evaluation criteria that share the same workflow as the seed tasks but are instantiated in different application scenarios"
  - [section 4.2] Filtering criteria include "Answer Verifiability," "Spatio-Temporal Independence," and "Rubric Alignment"
  - [corpus] No direct corpus evidence for this synthesis approach; appears novel to this benchmark.
- Break condition: If workflow extraction fails to capture implicit dependencies, synthesized tasks may lack logical coherence.

## Foundational Learning

- Concept: **Instruction forgetting in long-context processing**
  - Why needed here: Open Workflow Execution tasks (53.8% of benchmark) require agents to maintain adherence to multi-step workflows without losing intermediate constraints.
  - Quick check question: Can you explain why an agent might correctly execute steps 1-3 of a workflow but fail on step 4 despite having all necessary context?

- Concept: **Multimodal file parsing for structured extraction**
  - Why needed here: Tasks involve 19 file types (PDF, XLSX, SVG, PPT, etc.); evaluation uses VLMs with visual parsing for PPT/HTML to avoid raw-code comprehension issues.
  - Quick check question: How would you extract pricing rules from a PDF comparing carrier plans, and what parsing failures would break downstream reasoning?

- Concept: **State consistency in iterative refinement**
  - Why needed here: Iterative Refinement tasks (21.2%) require maintaining state across multi-turn interactions while applying constraint updates to existing outputs.
  - Quick check question: If a user provides an SVG floor plan and Excel constraints file, what state must persist between initial parsing and final layout optimization?

## Architecture Onboarding

- Component map:
  Task Repository -> Evaluation Pipeline -> Scoring Engine -> Synthesis Module
- Critical path:
  1. Agent receives task prompt + attachments
  2. Agent produces file-based deliverables (PPT, HTML, reports)
  3. Evaluation pipeline parses outputs (visual rendering for HTML/PPT)
  4. Judge model evaluates each rubric item against ground truth
  5. Scores aggregated and normalized per Equation 1-2
- Design tradeoffs:
  - Binary vs. continuous scoring: Binary improves objectivity but loses nuance for subjective criteria
  - LLM vs. human judges: 80.1% agreement enables scalability; discrepancies arise on abstract concepts like "design sense"
  - Automated synthesis vs. human annotation: Reduces cost (3 hours/task for humans) but requires rigorous filtering to maintain quality
- Failure signatures:
  - Latent instruction failures: Agent replicates format but misses coverage, or understands content but loses structural consistency
  - Workflow forgetting: Early steps executed correctly, later steps diverge from specified procedure
  - Attachment mishandling: Strong text-only performance (0.644) but degradation with complex attachments (varies by agent)
- First 3 experiments:
  1. Reproduce the human-LLM agreement study (171 rubrics, 28 problems) using Gemini-3-Pro vs. GPT-5.1 to validate judge model selection.
  2. Ablate the synthesis pipeline by evaluating human-authored vs. synthesized tasks separately to measure quality equivalence.
  3. Test a new agent on Latent Instruction Inference tasks alone to diagnose implicit constraint extraction capabilities before full benchmark runs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can benchmark design extend agent evaluation from daily ("OneDay") to weekly ("OneWeek") time horizons without relying on domain-specific customization that compromises generalizability?
- **Basis in paper:** [explicit] Section 6.2 ("From OneDay to OneWeek") states that extending capabilities to longer horizons is the "next technical frontier" and argues that domain-specific training does not guarantee generalization to other tasks of equivalent duration.
- **Why unresolved:** Current long-horizon benchmarks (e.g., Vending-bench) are domain-specific, and it is unclear how to construct broad-spectrum "OneWeek" tasks that validly assess AGI progress across work, life, and learning without overfitting.
- **What evidence would resolve it:** The creation and validation of a benchmark showing that agent performance on diverse, week-long tasks correlates with general reasoning capabilities rather than specific domain-knowledge retrieval.

### Open Question 2
- **Question:** What specific improvements in model architecture or training are required to resolve the persistent failure of agents in Latent Instruction Inference?
- **Basis in paper:** [explicit] Section 5.3 identifies Latent Instruction Inference as the "weakest capability across agents generally," noting that even top systems struggle to "maintain structural consistency" when inferring implicit rules from attachments.
- **Why unresolved:** Current agents fail to simultaneously replicate format details and content coverage, often missing implicit constraints like citation styles or layout rules found in reference files.
- **What evidence would resolve it:** An agent achieving >0.8 accuracy on the Latent Instruction category by successfully extracting and applying unspoken rules (e.g., visual formatting) from context files to new outputs.

### Open Question 3
- **Question:** How can automated evaluation pipelines (LLM-as-judge) be calibrated to match human judgment on abstract qualitative constraints?
- **Basis in paper:** [explicit] Section 5.3 notes that while Gemini-3-Pro achieved 80.1% consistency, discrepancies arise from "varying interpretations of abstract concepts, including 'conciseness,' 'relative completeness,' and 'design sense.'"
- **Why unresolved:** Binary scoring and verifiable rubrics struggle to capture subjective qualities or "design sense," leading to misalignment between automated scores and human perception of quality.
- **What evidence would resolve it:** Development of a verifier model that correlates with human judgment on abstract rubrics at a rate comparable to inter-annotator agreement (>90%).

## Limitations
- Binary rubric scoring may inadequately capture subjective criteria like "conciseness" and "design sense" where human-LLM discrepancies are highest
- Automated synthesis pipeline lacks corpus validation—no evidence comparing synthesized tasks to human-authored equivalents for quality preservation
- Evaluation framework assumes attachments contain structured patterns amenable to latent instruction inference, but ambiguous or poorly formatted attachments could reduce this to pattern matching rather than reasoning

## Confidence
- **High confidence**: Task-level instruction-following benchmark design and evaluation methodology; 104-task corpus covering realistic daily scenarios; 80.1% human-LLM agreement with Gemini-3-Pro; API-based agents achieving parity with RL-based systems
- **Medium confidence**: Automated synthesis pipeline quality and transferability; latent instruction inference as distinct capability; ranking of agent performance across categories
- **Low confidence**: Long-term generalizability of synthesized tasks; true separation of instruction-following from general reasoning; stability of binary scoring for subjective rubric items

## Next Checks
1. Conduct ablation study comparing human-authored vs. synthesized tasks to measure quality degradation in the synthesis pipeline and validate workflow transferability assumptions
2. Test agent performance on isolated Latent Instruction Inference tasks (without Open Workflow Execution context) to determine if implicit constraint extraction is truly a distinct capability or confounded by general reasoning ability
3. Expand human-LLM agreement study beyond the initial 171 rubrics to include more subjective criteria and assess whether binary scoring systematically underperforms for abstract concepts compared to continuous scales