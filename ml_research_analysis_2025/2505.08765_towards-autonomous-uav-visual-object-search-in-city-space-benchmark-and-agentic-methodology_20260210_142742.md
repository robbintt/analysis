---
ver: rpa2
title: 'Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic
  Methodology'
arxiv_id: '2505.08765'
source_url: https://arxiv.org/abs/2505.08765
tags:
- search
- object
- agent
- semantic
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CityAVOS, the first benchmark dataset for
  autonomous visual object search in urban environments using UAVs. The dataset contains
  2,420 tasks across six object categories with varying difficulty levels.
---

# Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology

## Quick Facts
- arXiv ID: 2505.08765
- Source URL: https://arxiv.org/abs/2505.08765
- Reference count: 40
- Key outcome: PRPSearcher achieves 53.50% success rate and 40.57% SPL on CityAVOS, significantly outperforming baselines but falling short of human performance

## Executive Summary
This paper addresses autonomous visual object search (AVOS) in urban environments using UAVs, introducing CityAVOS - the first benchmark dataset for this task with 2,420 tasks across six object categories and varying difficulty levels. The authors propose PRPSearcher, an MLLM-based agentic method that mimics human three-tier cognition through object-centric semantic maps, cognitive maps based on attraction values, and uncertainty maps. The method incorporates a denoising mechanism and an Inspiration Promote Thought (IPT) prompting mechanism for adaptive exploration-exploitation balancing. Experiments demonstrate PRPSearcher significantly outperforms baselines with +37.69% success rate, +28.96% SPL, -30.69% MSS, and -46.40% NE improvements on average, though still falling short of human performance.

## Method Summary
PRPSearcher implements a three-tier cognition framework for autonomous UAV visual object search. The method uses an object-centric 3D dynamic semantic map where MLLM reasoning pre-filters target-related objects before segmentation to reduce computational overhead. A cognitive map assigns attraction values to semantic elements via MLLM scoring, with a denoising mechanism suppressing already-inspected regions. The IPT prompting mechanism dynamically balances exploration and exploitation by conditionally injecting exploration advice when uncertainty reduction exceeds a threshold. The agent iteratively observes, updates maps, computes attraction and exploration rewards, composes prompts, and selects actions until target detection or timeout.

## Key Results
- PRPSearcher achieves 53.50% success rate and 40.57% SPL on CityAVOS benchmark
- Significant improvements over baselines: +37.69% SR, +28.96% SPL, -30.69% MSS, -46.40% NE on average
- θT=0.1 yields optimal performance, balancing exploration-exploitation trade-off
- Human operators achieve 78.68% SR, highlighting 25% performance gap

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric 3D Dynamic Semantic Map for Efficient Spatial Perception
- Claim: Pre-filtering semantics via MLLM reasoning before segmentation reduces computational overhead and mapping noise in complex urban scenes.
- Mechanism: The MLLM first infers target-related objects from task description (image + text), generating a focused semantic vocabulary. This vocabulary guides Grounded SAM to segment only relevant objects, which are then projected into a 3D grid map via depth and pose. A majority-voting scheme per grid cell resolves label conflicts over time.
- Core assumption: MLLM can reliably identify semantically related objects; irrelevant objects are noise rather than useful context for this task.
- Evidence anchors:
  - [abstract] "object-centric dynamic semantic map enhancing spatial perception"
  - [section 4.2] Equation (1-5) describes the object-centric segmentation and 3D projection pipeline; Table 3 shows object-centric prompting outperforms free-prompt and human-designed baselines.
  - [corpus] Weak direct corpus evidence; related work (UAV-ON, EmbodiedCity) addresses aerial navigation but not this specific object-centric filtering mechanism.
- Break condition: When target descriptions are ambiguous or MLLM fails to identify related objects, segmentation becomes unfocused; when scenes are densely cluttered with target-like distractors, majority voting may not converge correctly.

### Mechanism 2: Attraction-Driven Cognitive Map with Denoising for Target Reasoning
- Claim: Assigning scalar "attraction values" to semantic elements via MLLM enables prioritized search while a denoising mechanism suppresses already-inspected regions.
- Mechanism: The MLLM scores each semantic category's relevance to the target (0-1 attraction value). These values populate a 3D cognitive map aligned with the semantic map. A mirrored binary map tracks which cells have been observed within recognition distance; multiplying the cognitive map by this mask zeros out attraction in already-checked areas. High-attraction regions are clustered via DBSCAN to identify exploitation targets.
- Core assumption: MLLM attraction scores correlate with actual target proximity; semantic similarity implies spatial proximity or contextual relevance.
- Evidence anchors:
  - [section 4.3] Equation (6-8) defines attraction computation and denoising; Figure 4 shows successful case where shop-related semantics guide search to correct area.
  - [section 5.4] Failed case analysis shows sparse semantic cues ("Car, Windows, Building, Wall") lead to unfocused attraction and search failure, indicating mechanism limits.
  - [corpus] No direct corpus validation of this specific attraction-denoising formulation.
- Break condition: When target has few distinctive semantic associations (e.g., generic "black car"), attraction values become diffuse; when non-target objects have high semantic overlap with target, false positives persist.

### Mechanism 3: Inspiration Promote Thought (IPT) Prompting for Adaptive E-E Balance
- Claim: Dynamically injecting exploration advice into the action-planning prompt only when uncertainty reduction exceeds a threshold mimics human "inspiration" and prevents over-exploitation or over-exploration.
- Mechanism: At each step, two advisors run in parallel: (1) Exploitation advisor identifies the centroid of highest-attraction cluster; (2) Exploration advisor computes which action maximizes uncertainty reduction in the 3D uncertainty map. The IPT mechanism embeds exploitation advice as persistent guidance and conditionally adds exploration advice as "inspiration" only when Reward(a*) > θT. The combined prompt is fed to MLLM for final action selection.
- Core assumption: The threshold θT correctly captures when exploration is beneficial; MLLM can integrate competing advice without confusion.
- Evidence anchors:
  - [section 4.4] Equation (12-15) formalizes exploration reward and IPT prompt composition; Table 4 shows θT=0.1 yields optimal SR (53.50%) while θT=0 or θT≥0.5 degrade performance.
  - [section 5.2] Ablation shows PRPSearcher w/o exploration drops SR to 49.19% and w/o exploitation drops to 10.41%, confirming both components are necessary.
  - [corpus] CoordField (arXiv:2505.00091) addresses UAV task allocation in urban settings but uses coordination fields rather than prompt-based E-E balancing; no direct validation of IPT approach.
- Break condition: When θT is set too high, exploration advice never triggers and agent ignores unexplored regions; when too low, constant exploration advice overwhelms exploitation guidance, reducing efficiency.

## Foundational Learning

- Concept: Multi-Modal Large Language Models (MLLMs) for Visual Reasoning
  - Why needed here: PRPSearcher uses MLLMs (GPT-4o, Qwen-vl-max) for three distinct functions: (1) inferring target-related objects, (2) assigning attraction values to semantics, and (3) action planning from combined prompts. Understanding how MLLMs process interleaved image-text inputs and generate structured outputs is essential.
  - Quick check question: Can you explain how an MLLM would process a prompt containing a target image, text description, and a question about related objects?

- Concept: Semantic Mapping and 3D Projection from RGB-D
  - Why needed here: The core perception module requires transforming 2D semantic segmentation masks into a 3D grid map using depth images and camera extrinsics (Equation 3-4). Familiarity with camera intrinsics/extrinsics and voxel grid representation is required.
  - Quick check question: Given a depth pixel (u, v) with depth D, camera intrinsic K, and extrinsic [R|r], can you write the transformation to world coordinates?

- Concept: Exploration-Exploitation Trade-off in Sequential Decision-Making
  - Why needed here: The IPT mechanism explicitly models this trade-off using uncertainty-based exploration rewards and attraction-based exploitation targets. Understanding why pure exploration (FBE baseline: 11.07% SR) and pure exploitation (w/o exploration: 10.41% SR in hard tasks) both fail provides context for why adaptive balancing is necessary.
  - Quick check question: In a search task, what happens if an agent only exploits (follows semantic attraction) without exploring unknown regions?

## Architecture Onboarding

- Component map:
  Perception Layer: RGB-D input → MLLM semantic inference → Grounded SAM segmentation → 3D projection → Dynamic Semantic Map + Uncertainty Map
  Reasoning Layer: Semantic Map → MLLM attraction scoring → Cognitive Map + Denoising via mirrored recognition mask → DBSCAN clustering for exploitation targets
  Planning Layer: Uncertainty Map → Exploration advisor (reward-maximizing action) + Cognitive Map → Exploitation advisor (navigate to highest-attraction cluster) → IPT prompt composer → MLLM action selection → Execute action or Stop

- Critical path:
  1. Task initialization: MLLM infers related objects from target image+text (one-time per task)
  2. Per-step loop: Observe → Update semantic/uncertainty maps → Compute attraction/denoise → Generate exploration+exploitation advice → Compose IPT prompt → MLLM selects action → Check for target detection → Stop or continue

- Design tradeoffs:
  - Grid resolution (Δx, Δy, Δz): Finer grids improve precision but increase memory/computation; coarser grids lose detail. Paper does not specify exact resolution.
  - θT threshold: Lower values increase exploration frequency but risk distraction; optimal found at 0.1 via sweep (Table 4). This is dataset-dependent.
  - MLLM choice: GPT-4o achieves highest SR (53.50%); Qwen-vl-max similar; glm-4v-plus worse SR but better NE (requires closer approach). Trade-off between recognition distance and accuracy.
  - Object-centric vs. open segmentation: Object-centric filtering improves efficiency but risks missing unexpected contextual cues.

- Failure signatures:
  - Low SR with high MSS on hard tasks: Likely attraction values are diffuse (target has generic semantics); check MLLM's related-object inference quality.
  - Agent stuck in local area without finding target: θT may be too high, suppressing exploration; check if exploration advice ever triggers (monitor Nθ in Table 4).
  - Agent stops early at wrong object: Denoising mechanism may be failing; verify mirrored cognitive map correctly marks recognized areas.
  - High navigation error (NE) with successful stops: Recognition distance is small; agent must approach closely. Check MLLM's detection threshold and camera resolution.

- First 3 experiments:
  1. Reproduce baseline comparison: Run Random Exploration, FBE, and STMR on a subset of CityAVOS easy tasks. Verify your implementation matches reported SR (RE: ~10.30%, FBE: ~13.64%, STMR: ~32.68%). This validates your environment setup.
  2. Ablate object-centric filtering: Replace MLLM-guided semantic prompts with free-prompt segmentation. Compare map construction time and SR on medium tasks. Expect ~3% SR drop and higher computational cost per Table 3.
  3. Sweep θT parameter: Run PRPSearcher with θT ∈ {0, 0.05, 0.1, 0.2, 0.5, 1} on a held-out task set. Plot SR vs. θT and count exploration advice frequency (Nθ). Confirm peak near θT=0.1 and understand the failure modes at extremes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can collaborative multi-agent strategies extend PRPSearcher to handle long-horizon, multi-target search tasks?
- **Basis in paper:** [explicit] The conclusion states the intent to "incorporate collaborative human-agent or multi-agent strategies to handle more complex AVOS tasks (e.g. long-horizon multi-target search)."
- **Why unresolved:** The current study limits its scope to single-agent, single-target scenarios and does not test coordination or communication protocols required for multi-agent complexity.
- **What evidence would resolve it:** A modified CityAVOS benchmark with multi-target tasks and results showing PRPSearcher's performance when extended with collaborative exploration algorithms.

### Open Question 2
- **Question:** What specific advancements in semantic reasoning are required to close the 25% performance gap between PRPSearcher and human operators?
- **Basis in paper:** [explicit] The abstract and conclusion note that "the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities."
- **Why unresolved:** While the paper identifies the gap (53.50% SR vs. 78.68% Human SR), it does not isolate which specific reasoning failures (e.g., contextual inference vs. object recognition) contribute most to this deficit.
- **What evidence would resolve it:** A detailed error analysis of failure cases comparing human versus agent reasoning chains, or an ablation study isolating specific reasoning modules.

### Open Question 3
- **Question:** Does the optimal exploration threshold ($\theta_T = 0.1$) for the IPT prompting mechanism generalize to environments with varying urban density?
- **Basis in paper:** [inferred] The ablation study identifies 0.1 as the optimal $\theta_T$ for the dataset, but the method section implies this balances exploration-exploitation, which is highly sensitive to environmental scale and object density.
- **Why unresolved:** The fixed threshold may overfit to the specific spatial distributions in CityAVOS, potentially failing in sparser or denser urban canyons where the cost of exploration differs.
- **What evidence would resolve it:** Experiments evaluating PRPSearcher across different scene types (e.g., dense downtown vs. sparse industrial) without re-tuning $\theta_T$.

## Limitations
- MLLM reliability for semantic inference and attraction scoring across diverse urban scenes remains untested in adverse conditions
- The optimal threshold θT=0.1 may not generalize to environments with different urban densities
- Performance gap of 25% compared to human operators indicates limitations in semantic reasoning and spatial exploration capabilities

## Confidence
- **High confidence**: The benchmark dataset construction methodology and overall performance improvements over baselines are well-supported by experimental results (SR, SPL metrics)
- **Medium confidence**: The three-tier cognition framework is logically sound, but the specific MLLM-based implementations rely on black-box model behavior without extensive ablation
- **Medium confidence**: The claim that PRPSearcher significantly outperforms baselines is supported by Table 2, though human performance data appears incomplete

## Next Validation Checks
1. **MLLM Robustness Test**: Run PRPSearcher on CityAVOS tasks with adversarial target descriptions (ambiguous or multi-target scenarios) to measure degradation in attraction scoring accuracy and semantic inference quality

2. **Cross-Dataset Transfer**: Evaluate the attraction-denoising mechanism on a different urban aerial dataset (e.g., UAV-ON benchmark) to test whether θT=0.1 remains optimal or requires recalibration

3. **Failure Mode Analysis**: Systematically categorize CityAVOS failure cases by root cause (semantic recognition failure, attraction diffusion, premature stopping) and quantify their frequency to validate the claimed mechanism limitations