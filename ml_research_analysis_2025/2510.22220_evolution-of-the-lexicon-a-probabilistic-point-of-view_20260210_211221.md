---
ver: rpa2
title: 'Evolution of the lexicon: a probabilistic point of view'
arxiv_id: '2510.22220'
source_url: https://arxiv.org/abs/2510.22220
tags:
- words
- which
- languages
- distance
- cognate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes probabilistic limits on accuracy in lexicostatistical
  methods for estimating temporal separation between languages. It shows that even
  under ideal conditions, random fluctuations in word replacement and gradual lexical
  modification processes create unavoidable errors in separation time estimates, with
  relative errors ranging from 49% for recent splits to 18% for older ones.
---

# Evolution of the lexicon: a probabilistic point of view

## Quick Facts
- arXiv ID: 2510.22220
- Source URL: https://arxiv.org/abs/2510.22220
- Reference count: 39
- Primary result: Probabilistic analysis shows random fluctuations create unavoidable errors in lexicostatistical dating, with relative errors from 49% (recent splits) to 18% (older ones)

## Executive Summary
This paper analyzes fundamental probabilistic limits on accuracy in lexicostatistical methods for estimating temporal separation between languages. It demonstrates that random fluctuations in word replacement and gradual lexical modification create unavoidable errors in separation time estimates, even under ideal conditions. The study introduces normalized Hamming distance as an alternative approach that doesn't require identifying cognates, and compares three methods: classical cognate counting, blind use of normalized edit distance, and combined use of edit distance limited to cognates.

## Method Summary
The method estimates temporal separation between languages using probabilistic models of word replacement (rate λ ≈ 1.4×10⁻⁴) and gradual lexical modification (rate μ ≈ 1.6×10⁻⁴). It employs Normalized Levenshtein Distance for word comparisons and uses 60 Malagasy dialect datasets with 207-concept Swadesh lists. Three approaches are compared: classical cognate overlap counting, blind normalized edit distance without cognacy identification, and combined edit distance applied only to cognates. Parameters are calibrated using known dialect separations.

## Key Results
- Random fluctuations in word replacement create fundamental error bounds, with relative errors ranging from 49% for recent splits to 18% for older ones
- Gradual lexical modification (rate μ ≈ 1.3×10⁻⁴) is as important as word replacement in reshaping vocabulary over centuries
- Blind edit distance approach is most accurate for separations up to 3 millennia, while combined approach performs better for longer time spans
- The method is validated using 60 Malagasy dialect datasets, confirming parameter estimates and demonstrating practical advantages

## Why This Works (Mechanism)

### Mechanism 1: Fundamental Probabilistic Error Bounds
The number of retained cognates is a binomial random variable, creating a confidence interval for time estimates that is purely mathematical in origin. With standard Swadesh list size (M=207), high variance is unavoidable regardless of data quality.

### Mechanism 2: Dual-Process Signal Integration
Words evolve via binary replacement (rate λ) AND continuous character modification (rate μ). Using Normalized Hamming Distance captures the combined decay rate (λ + μ), tightening error bounds relative to time T.

### Mechanism 3: Error Reduction via Blind Distance Metrics
Classical methods require binary cognate decisions that introduce human error. The blind method uses continuous distances (0.0 to 1.0). While non-cognate pairs add noise, this added variance is less damaging than variance from misidentifying cognates in recent splits.

## Foundational Learning

- **Concept: Binomial Distribution & Variance Scaling**
  - Why needed: To understand why a Swadesh list of 207 words behaves differently than 10²³ atoms
  - Quick check: If you flip a coin 10 times vs. 10,000 times, which experimental probability of heads is likely closer to the theoretical 0.5, and why does this matter for dating languages?

- **Concept: Markov Processes / Steady State Distribution**
  - Why needed: To model how character similarity decays over time to a baseline "random" match probability
  - Quick check: If two cognate words drift apart in spelling, do they eventually become totally different, or do they settle at a baseline level of accidental similarity?

- **Concept: Hamming vs. Levenshtein Distance**
  - Why needed: The paper uses Hamming for math simplicity but recommends Levenshtein for real data (variable length words)
  - Quick check: Why would Hamming distance fail to compare the words "cat" and "cats", and how does Levenshtein fix this?

## Architecture Onboarding

- **Component map:** Swadesh Lists (M=207 concepts) → Pre-processor (String alignment) → Metric Calculator (ω, φ, φ) → Temporal Estimator (Inverts E[φ] = e^{-2(λ+μ)T})

- **Critical path:** Estimation of parameters λ and μ (calibration). Wrong rates bias all time estimates T.

- **Design tradeoffs:**
  - Classical: Best for deep time (>6k years) if cognates are perfect. High human labor/error
  - Blind Distance: Best for recent splits (<3k years). Fully automatable. Lower precision at deep time
  - Combined: Highest precision for mid-range (3k-6k), but requires difficult cognate identification

- **Failure signatures:**
  - High Variance: ω ≈ 0 for recent languages (indicating small sample noise)
  - Signal Saturation: d_H hits baseline (1/N) for very old languages, making T estimation impossible

- **First 3 experiments:**
  1. Implement Eq (16) to plot R_ω vs T and verify 49% error floor at T=300
  2. Take 3 known dialect pairs, compute λ via Eq (68), verify it approximates 1.4×10⁻⁴
  3. Select language pair with known separation time, compute T using both Classical Overlap and Blind Distance to see which lands closer to ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How much does incorrect cognacy attribution increase the variance in the combined edit-distance approach (φ), and at what error rate does this extra variance exceed Var[χ(T)]? The authors acknowledge this remains unquantified despite noting that automatic cognacy strategies likely produce extra-variance larger than Var[χ(T)].

### Open Question 2
Do the estimated rate parameters (λ ≈ 1.4×10⁻⁴, μ ≈ 1.6×10⁻⁴, N ≈ 5.18, L ≈ 7.63) generalize across diverse language families with different phonological and orthographic systems? All estimates derive from 60 Malagasy varieties, while N differs substantially from alphabet size due to character frequency and independence issues.

### Open Question 3
Can the probabilistic framework incorporating both replacement and gradual modification be adapted for animal communication systems where cognacy is undefined? The authors note normalized edit distance "can be applied much more easily in fields other than traditional linguistics, such as animal communication," but do not develop this application.

## Limitations
- Parameter values calibrated only from Malagasy dialects may not generalize to other language families
- Effectiveness of blind edit distance depends on assumption that random matches between non-cognate words contribute predictable noise
- The mathematical framework assumes constant rates and independent character evolution, which may not hold in all languages

## Confidence
- **High Confidence:** Mathematical framework for error bounds and relative performance of different methods
- **Medium Confidence:** Specific parameter values derived from Malagasy data may not generalize universally
- **Medium Confidence:** Claim that gradual lexical modification is "equally important" as word replacement is supported within model but needs cross-linguistic validation

## Next Checks
1. Apply the method to language families with known divergence times (e.g., Romance languages) to test parameter generalizability
2. Conduct sensitivity analysis by varying λ and μ within ±50% to assess impact on temporal estimates
3. Compare results with traditional glottochronological studies on the same language pairs to validate accuracy claims