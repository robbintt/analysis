---
ver: rpa2
title: 'Beyond Modality Collapse: Representations Blending for Multimodal Dataset
  Distillation'
arxiv_id: '2505.14705'
source_url: https://arxiv.org/abs/2505.14705
tags:
- distillation
- dataset
- text
- image
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies and addresses the modality collapse problem
  in multimodal dataset distillation, where cross-modal supervision causes over-concentration
  of representations within each modality and poor alignment across modalities. The
  proposed RepBlend framework mitigates this issue through two key components: representation
  blending, which enhances intra-modal diversity by mixing representations within
  each modality, and symmetric projection trajectory matching, which balances optimization
  dynamics across modalities for better cross-modal alignment.'
---

# Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation

## Quick Facts
- **arXiv ID**: 2505.14705
- **Source URL**: https://arxiv.org/abs/2505.14705
- **Reference count**: 40
- **Primary result**: Proposes RepBlend framework that mitigates modality collapse in multimodal dataset distillation through representation blending and symmetric projection trajectory matching, achieving up to +9.4 IR@10 and +6.3 TR@10 gains on Flickr-30K and MS-COCO under 100-pair settings.

## Executive Summary
This paper identifies and addresses the modality collapse problem in multimodal dataset distillation (MDD), where cross-modal supervision causes over-concentration of representations within each modality and poor alignment across modalities. The proposed RepBlend framework mitigates this issue through two key components: representation blending, which enhances intra-modal diversity by mixing representations within each modality, and symmetric projection trajectory matching, which balances optimization dynamics across modalities for better cross-modal alignment. Extensive experiments on Flickr-30K and MS-COCO show RepBlend consistently outperforms state-of-the-art MDD methods, achieving up to +9.4 IR@10 and +6.3 TR@10 gains under 100-pair settings, while also offering up to 6.7× distillation speedup and improved computational efficiency.

## Method Summary
RepBlend addresses modality collapse in MDD by combining representation blending with symmetric projection trajectory matching. The framework blends representations within each modality using MixUp-style interpolation before projection, enhancing intra-modal diversity while preserving cross-modal alignment. It then matches projection head weights symmetrically for both image and text branches, ensuring balanced optimization dynamics. The method uses frozen pretrained encoders (NFNet/ViT/ResNet for images, BERT for text) with trainable linear projection heads, optimizing a synthetic dataset through weighted binary cross-entropy loss combined with symmetric trajectory matching objectives.

## Key Results
- Achieves up to +9.4 IR@10 and +6.3 TR@10 improvements over state-of-the-art MDD methods under 100-pair settings
- Demonstrates 6.7× distillation speedup through symmetric projection trajectory matching
- Shows consistent performance gains across Flickr-30K and MS-COCO datasets with varying synthetic dataset sizes (100, 200, 500 pairs)
- Validates effectiveness through controlled ablation studies and diagnostic metrics measuring intra-modal similarity and modality gap

## Why This Works (Mechanism)
The method works by addressing the fundamental imbalance in cross-modal optimization. Representation blending introduces controlled perturbations within each modality, preventing over-concentration of features while maintaining cross-modal alignment through the MixUp process. Symmetric projection trajectory matching ensures both image and text branches receive equal optimization pressure, preventing the image branch from plateauing as observed in asymmetric approaches. Together, these mechanisms maintain diverse, well-distributed representations within each modality while preserving the semantic alignment needed for effective cross-modal retrieval.

## Foundational Learning

- **Concept: Multimodal Dataset Distillation (MDD)**
  - **Why needed here:** The core task of the paper. Requires understanding the goal of condensing large-scale image-text datasets into smaller surrogates for efficient cross-modal learning.
  - **Quick check question:** Can you explain the high-level objective function of MDD as defined in Equation 1 and how it differs from unimodal dataset distillation?

- **Concept: Modality Collapse in Contrastive Learning**
  - **Why needed here:** The central problem the paper identifies and solves. One must understand what it means for representations to be "over-concentrated" and how it harms cross-modal retrieval.
  - **Quick check question:** Based on the paper's analysis (e.g., Figure 1), can you describe the two key symptoms of modality collapse and why the non-diagonal similarity scores in Figure 2 (right) are a negative outcome?

- **Concept: Trajectory Matching for Dataset Distillation**
  - **Why needed here:** The foundational technique (from MTT, LoRS) that RepBlend builds upon. It's critical to understand how matching training trajectories transfers knowledge to the synthetic dataset.
  - **Quick check question:** What does the baseline trajectory matching objective (e.g., from LoRS) try to minimize, and why does the paper argue this process is "asymmetric"?

## Architecture Onboarding

- **Component map:**
  Frozen Encoders (NFNet/ViT/ResNet + BERT) -> Representation Blender -> Trainable Projection Heads (f^img_P, f^text_P) -> Trajectory Matcher -> Synthetic Dataset Optimization

- **Critical path:** The distillation loop. For each iteration:
    1. Sample a mini-batch from the synthetic dataset.
    2. Extract image representations using the frozen image encoder.
    3. Apply Representation Blending to both the image representations and the raw text embeddings (Algorithm 2).
    4. Pass the blended representations through the trainable image and text projection heads.
    5. Compute the wBCE loss (Equation 2) using the blended, projected embeddings.
    6. Update the projection heads.
    7. Compute Symmetric Trajectory Matching Loss (Equation 6) by comparing the updated projection head weights to the stored expert trajectories.
    8. Backpropagate to update the synthetic images, text embeddings, and soft labels.

- **Design tradeoffs:**
    *   **Symmetric vs. Asymmetric Matching:** Matching image encoder trajectories (asymmetric, old method) vs. matching image *projection head* trajectories (symmetric, new method). The paper argues the symmetric approach is faster (6.7× speedup) and more balanced despite adding a new component.
    *   **Blending Representations vs. Blending Pixels:** Blending in the representation space is key. The paper shows blending in the representation space is the effective lever for modality diversity.

- **Failure signatures:**
    *   **Modality Collapse:** Diagnosed by computing intra-modal cosine similarity (Sim) and modality gap (Gap) from Equation 4. High Sim and large Gap indicate failure. Visual inspection of embeddings (Figure 1) can also reveal clustering.
    *   **Asymmetric Optimization:** Diagnosed by plotting the trajectory matching loss for image vs. text branches (Figure 4, left). A plateau in the image-side loss indicates failure of the baseline method.
    *   **Semantically Meaningless Blending:** If the λ for blending is too high, or if the blended samples are not from the in-distribution set, the modality gap may increase (Figure 3, yellow bars), indicating failure.

- **First 3 experiments:**
    1.  **Baseline Failure Mode Analysis:** Run the baseline MDD method (e.g., LoRS) and visualize the final embeddings (Figure 1, middle) and plot the intra-modal similarity over iterations (Figure 2, left). This confirms the existence of the problem to be solved.
    2.  **Perturbation Study:** Replicate the controlled experiment in Section 3.2 (Figure 3). Inject Gaussian noise at different levels (λ) and measure the trade-off between intra-modal similarity (Sim) and modality gap (Gap). This validates the core idea that perturbation is needed but random noise is harmful.
    3.  **Ablation of Core Components:** Run the full RepBlend method with and without Representation Blending (RB) and Symmetric Matching (SM). Plot performance (IR@10, TR@10) and diagnostic metrics (Gap, update norms) to isolate the contribution of each proposed mechanism (Figure 5, Figure 4).

## Open Questions the Paper Calls Out
None

## Limitations
- **Architecture Dependence**: Method effectiveness relies heavily on frozen pretrained encoders, with unclear performance on datasets using different architectures or when encoders are trainable
- **Hyperparameter Sensitivity**: Critical parameters like Beta distribution parameters for blending are fixed without comprehensive sensitivity analysis
- **Task Scope Limitation**: Evaluation focuses exclusively on cross-modal retrieval metrics, with unknown impact on other multimodal tasks like VQA or image captioning

## Confidence

- **High Confidence**: The identification of modality collapse as a real problem in MDD, supported by clear diagnostic metrics (Sim/Gap) and visual evidence (Figure 1)
- **Medium Confidence**: The effectiveness of RepBlend components in isolation and combination, based on controlled ablations (Figure 5) and ablation studies
- **Medium Confidence**: The claimed computational efficiency gains, though the 6.7× speedup is derived from trajectory matching analysis rather than direct timing measurements

## Next Checks

1. **Encoder Architecture Transfer**: Evaluate RepBlend performance when replacing frozen NFNet/BERT with other architectures (e.g., ViT, CLIP encoders) to test architecture independence.

2. **Task Generalization Study**: Apply RepBlend-trained distilled datasets to downstream multimodal tasks (image captioning, VQA) to verify benefits extend beyond retrieval.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary Beta distribution parameters (α, β) and trajectory matching settings to establish robustness ranges and identify optimal configurations for different dataset sizes.