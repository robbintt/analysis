---
ver: rpa2
title: When LRP Diverges from Leave-One-Out in Transformers
arxiv_id: '2510.18810'
source_url: https://arxiv.org/abs/2510.18810
tags:
- attention
- layers
- softmax
- bilinear
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work identifies two fundamental flaws in current Layer-Wise\
  \ Relevance Propagation (LRP) variants for Transformers: (1) the bilinear propagation\
  \ rules in AttnLRP violate implementation invariance by assigning different attributions\
  \ to functionally equivalent factorizations, and (2) the softmax propagation rule\
  \ introduces linearization errors that harm alignment with ground-truth Leave-One-Out\
  \ (LOO) scores. Empirically, bypassing softmax propagation\u2014as in CP-LRP\u2014\
  significantly improves LOO alignment (e.g., from r=0.22 to r=0.52 on SST), with\
  \ the largest gains concentrated in middle-to-late Transformer layers."
---

# When LRP Diverges from Leave-One-Out in Transformers

## Quick Facts
- arXiv ID: 2510.18810
- Source URL: https://arxiv.org/abs/2510.18810
- Authors: Weiqiu You; Siqi Zeng; Yao-Hung Hubert Tsai; Makoto Yamada; Han Zhao
- Reference count: 19
- Primary result: LRP variants for Transformers violate implementation invariance and suffer from softmax linearization errors, with CP-LRP significantly improving Leave-One-Out alignment.

## Executive Summary
This work identifies two fundamental flaws in current Layer-Wise Relevance Propagation (LRP) variants for Transformers: (1) the bilinear propagation rules in AttnLRP violate implementation invariance by assigning different attributions to functionally equivalent factorizations, and (2) the softmax propagation rule introduces linearization errors that harm alignment with ground-truth Leave-One-Out (LOO) scores. Empirically, bypassing softmax propagation—as in CP-LRP—significantly improves LOO alignment (e.g., from r=0.22 to r=0.52 on SST), with the largest gains concentrated in middle-to-late Transformer layers. These findings highlight that current LRP methods are not axiomatically sound for Transformers and motivate the development of more faithful approximation methods.

## Method Summary
The paper conducts a theoretical and empirical analysis of LRP variants for Transformers, focusing on implementation invariance violations and linearization errors. It compares AttnLRP and CP-LRP against ground-truth LOO scores across BERT-base models on SST sentiment analysis. The study evaluates how different propagation rules (bilinear factorization, softmax handling) affect attribution fidelity and identifies specific architectural layers where divergence occurs.

## Key Results
- Bilinear propagation rules in AttnLRP violate implementation invariance by assigning different attributions to functionally equivalent factorizations
- Softmax propagation introduces linearization errors that harm alignment with ground-truth LOO scores
- CP-LRP, which bypasses softmax propagation, significantly improves LOO alignment (e.g., from r=0.22 to r=0.52 on SST) with largest gains in middle-to-late Transformer layers

## Why This Works (Mechanism)
The paper identifies that current LRP variants fail because they propagate relevance through attention mechanisms using rules that don't respect fundamental axioms. The bilinear factorization in AttnLRP treats different but mathematically equivalent matrix decompositions as having different importance, violating implementation invariance. Additionally, linearizing softmax operations during propagation introduces approximation errors that compound through the network, causing attributions to diverge from true feature importance measured by LOO.

## Foundational Learning

**Layer-Wise Relevance Propagation (LRP)**: A neural network explanation method that redistributes output relevance backward through layers to identify input features driving predictions. Needed to understand the baseline technique being evaluated. Quick check: Can explain the difference between LRP and gradient-based attribution methods.

**Implementation Invariance**: The principle that functionally equivalent network implementations should receive identical attributions. Needed to identify when LRP rules violate theoretical soundness. Quick check: Can articulate why different matrix factorizations should yield same attributions.

**Leave-One-Out (LOO) Analysis**: A ground-truth feature importance method that measures prediction changes when systematically removing features. Needed to establish the gold standard for evaluating attribution fidelity. Quick check: Understand how masking-based removal can induce distribution shift.

## Architecture Onboarding

**Component Map**: Input -> Embedding Layer -> Transformer Encoder Layers (Multi-Head Attention + Feed-Forward) -> Classification Head -> Output

**Critical Path**: Input features → embedding projection → self-attention mechanism (query/key/value matrices) → layer normalization → feed-forward network → final classification → LRP backward propagation

**Design Tradeoffs**: Accuracy vs. explainability (LRP adds interpretability but may reduce fidelity), computational efficiency (bypassing softmax improves correlation but may increase complexity), theoretical soundness vs. practical implementation

**Failure Signatures**: Low correlation with LOO scores (r < 0.3), attribution instability across equivalent factorizations, layer-specific divergence patterns in middle-to-late transformer blocks

**First Experiments**: 1) Measure LOO alignment across different BERT variants on multiple NLP tasks, 2) Compare CP-LRP vs AttnLRP computational overhead, 3) Conduct ablation studies isolating softmax bypass impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a "block-wise" propagation approach, analogous to BatchNorm canonization in CNNs, be developed to resolve the bilinear factorization sensitivity and softmax linearization errors identified in current LRP variants?
- Basis in paper: [explicit] The Conclusion suggests this as a "promising fix... analogous to merging BatchNorm with preceding layers in CNNs," while the Limitations section states the authors "do not propose a remedy."
- Why unresolved: The paper diagnoses the theoretical flaws and empirical divergence but leaves the development of a theoretically sound, axiomatically grounded alternative for future work.
- What evidence would resolve it: A new LRP variant that propagates relevance through entire attention blocks (treating them as atomic units) and demonstrates high implementation invariance and improved LOO correlation.

### Open Question 2
- Question: Do the violations of implementation invariance and the benefits of bypassing softmax propagation persist in non-encoder architectures, such as decoder-only Large Language Models (LLMs) or Vision Transformers (ViTs)?
- Basis in paper: [explicit] The Limitations section notes the study is restricted to BERT-base and a simple linear attention model, stating that "generalization to other attention-based architectures remains to be explored."
- Why unresolved: The empirical validation focused exclusively on encoder-style text classification models; the structural differences in causal masking (decoders) or patch processing (ViTs) may interact differently with propagation rules.
- What evidence would resolve it: Replication of the paper's factorization and ablation experiments on generative decoder models (e.g., GPT) and vision architectures to verify if CP-LRP still outperforms AttnLRP in middle-to-late layers.

### Open Question 3
- Question: How does the specific choice of feature removal method for the ground-truth LOO reference (masking vs. resampling) impact the measured divergence of LRP attributions?
- Basis in paper: [inferred] The Limitations section acknowledges that the "LOO reference is based on masking-based removal, which may induce distribution shift," suggesting the benchmark itself might be a source of error.
- Why unresolved: The study relies on zero-masking to calculate ground truth importance; if masking pushes data out of the training distribution, the "divergence" of LRP might be an artifact of a flawed baseline rather than solely LRP's failure.
- What evidence would resolve it: A comparative study measuring LRP alignment against multiple LOO definitions (e.g., blurring, noise injection, or resampling) to see if the observed softmax errors are robust to the definition of "removal."

## Limitations
- Empirical evaluation focuses primarily on sentiment analysis tasks (SST) with Transformer-based models, limiting generalizability to other architectures and domains
- Does not address computational overhead of CP-LRP versus AttnLRP in practice, which could limit real-world applicability
- While theoretical violations are sound, practical impact on downstream decision-making remains unexplored

## Confidence
- High confidence in theoretical violations of implementation invariance and existence of linearization errors in current LRP formulations
- Medium confidence in empirical gains from CP-LRP, as results are based on a single dataset and model type
- Low confidence in broader claims about LRP's utility for Transformers without further validation across diverse tasks and architectures

## Next Checks
1. Replicate the LOO alignment experiments on BERT and RoBERTa models across multiple NLP tasks (e.g., NLI, QA) to assess generalizability
2. Measure and compare computational efficiency (runtime, memory) of CP-LRP versus AttnLRP at scale to evaluate practical trade-offs
3. Conduct ablation studies isolating the impact of softmax bypassing versus other CP-LRP modifications to confirm which components drive fidelity improvements