---
ver: rpa2
title: 'DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models'
arxiv_id: '2503.01359'
source_url: https://arxiv.org/abs/2503.01359
tags:
- ders
- upcycling
- experts
- upcycled
- vanilla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DeRS, a novel paradigm for extremely efficient
  upcycled Mixture-of-Experts (MoE) models. DeRS decomposes MoE experts into one expert-shared
  base weight and multiple expert-specific delta weights, then represents these delta
  weights in lightweight forms (sparse or low-rank matrices).
---

# DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models

## Quick Facts
- arXiv ID: 2503.01359
- Source URL: https://arxiv.org/abs/2503.01359
- Reference count: 40
- Key outcome: DeRS achieves up to 2270× parameter reduction in upcycled MoE models while maintaining or improving performance

## Executive Summary
DeRS introduces a novel paradigm for extremely efficient Mixture-of-Experts (MoE) models by decomposing experts into one shared base weight and multiple lightweight expert-specific delta weights. The method applies to two scenarios: DeRS Compression for inference-stage compression of trained MoE models, and DeRS Upcycling for training-stage conversion of dense models to MoE models. By representing delta weights in sparse or low-rank forms, DeRS achieves significant parameter efficiency improvements while maintaining task performance across multi-modal, medical multi-modal, and code generation tasks.

## Method Summary
DeRS decomposes upcycled MoE experts into a shared base weight (W_base) and expert-specific delta weights (Δ_i), where the base weight is initialized from the pre-trained dense model's FFN weights. These delta weights are represented as lightweight sparse matrices (index-value pairs) or low-rank factorizations (A_i × B_i). During inference, expert weights are synthesized on-demand: W_i = W_shared + F(Δ_i). DeRS applies to two scenarios: compression of trained upcycled MoEs using sparsification or quantization, and upcycling of dense models where only lightweight expert parameters need training alongside a shared base FFN.

## Key Results
- DeRS Compression reduces MoE layer parameters by 65% without performance loss using extreme sparsification (0.01% non-zero values)
- DeRS Upcycling achieves up to 2270× reduction in additional parameters compared to vanilla upcycling while maintaining or improving performance
- Performance drops only 1.3-1.6% when the shared base FFN is frozen, validating the efficiency of the lightweight expert parameters

## Why This Works (Mechanism)

### Mechanism 1: Weight Decomposition via Shared Initialization
Since all experts initialize from the same pre-trained FFN weight (W_base), trained expert weights can be expressed as W_i = W_base + Δ_i. The extremely high cosine similarity (>0.999) between experts and the base weight indicates Δ_i represents only minor adjustments. By representing these deltas in lightweight forms (sparse or low-rank), parameter redundancy is eliminated without destroying the expert's learned specialization. This assumes delta weights contain compressible redundancy rather than critical expert-distinguishing information.

### Mechanism 2: Lightweight Delta Representation via Sparsity and Low-Rank
Delta weights are represented as sparse matrices (keeping only p% of values) or low-rank factorizations (A_i × B_i with r ≪ min(d, d_h)). This enables extreme parameter reduction while maintaining performance. The assumption is that delta weights operate in a low-dimensional subspace. Sparse matrices cannot make global adjustments, so low-rank should be used when expert specialization requires distributed modifications.

### Mechanism 3: Shared Base Training with Lightweight Expert Adaptation
Training one shared base FFN (initialized from pre-trained weights) alongside lightweight expert-specific parameters enables more efficient learning than training independent experts. The shared base receives gradient updates from all training samples, enabling robust learning, while expert-specific lightweight parameters only need to capture residual specialization. This requires the shared base to remain trainable during fine-tuning.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding how Top-K routing selects experts is essential since DeRS operates on the MoE layer structure. Quick check: Given a 4-expert MoE layer with Top-2 routing and router scores [0.4, 0.1, 0.3, 0.2], which experts are activated?

- **Upcycling (Dense-to-MoE Conversion)**: DeRS specifically targets upcycled MoE models where experts share identical initialization. Quick check: In vanilla upcycling, how are the N expert weights initialized differently from training an MoE from scratch?

- **Low-Rank Matrix Factorization (LoRA-style)**: DeRS-LM represents delta weights as A × B matrix products. Understanding why low-rank approximations can capture meaningful weight updates is essential. Quick check: If original weight is 4096×4096 and rank r=4, how many parameters does the low-rank representation require?

## Architecture Onboarding

- **Component map**: Dense Model FFN (W_base) → DeRS Upcycling → MoE Layer with shared base FFN and expert-specific lightweight parameters

- **Critical path**:
  1. Load pre-trained dense model
  2. For each target FFN layer: extract W_base, initialize W_shared = W_base
  3. Initialize N expert-specific lightweight parameters (zero-init for values; random/fixed for sparse indices)
  4. During forward pass: synthesize expert weights on-demand via addition
  5. Route inputs via standard Top-K, compute expert outputs with synthesized weights

- **Design tradeoffs**:
  | Choice | Pros | Cons |
  |--------|------|------|
  | DeRS-SM (sparse) | Extreme compression (0.01% non-zero); fastest inference | Cannot make global weight adjustments; requires lower sparsity for performance |
  | DeRS-LM (low-rank) | Global adjustments; stable at very low rank (r=1-4) | More parameters than sparse at equivalent performance |
  | Freeze shared base | Faster training | Significant performance drop (1.3-1.6%) |

- **Failure signatures**:
  - Performance collapse with extreme compression (>0.99 drop rate or <2-bit quantization) on non-fine-tuned base models
  - Degradation when shared base is frozen
  - Worse results with rank >64 (excessive redundancy in expert parameters)
  - DeRS compression inapplicable to MoE models trained from scratch (no shared initialization → no delta extraction)

- **First 3 experiments**:
  1. Upcycle a small dense model (e.g., 350M params) with DeRS-LM (rank=4) and vanilla upcycling. Compare parameter counts and validation loss.
  2. Apply DeRS-Sparsification to a trained upcycled MoE with drop rates [0.5, 0.8, 0.9, 0.99]. Plot performance vs. compression ratio.
  3. Train DeRS-LM with frozen vs. trainable shared base. Measure performance gap to validate the mechanism.

## Open Questions the Paper Calls Out
- Can DeRS be adapted for MoE models trained from scratch rather than upcycled ones?
- How does DeRS performance scale with significantly larger training budgets compared to low-budget settings?
- Can advanced parameter-efficient techniques beyond basic low-rank or sparse matrices further optimize DeRS delta weights?

## Limitations
- DeRS compression is limited to upcycled MoE models and cannot be applied to MoEs trained from scratch
- Claims about optimal sparsity rates and rank values are task-specific and may not generalize
- The paper lacks ablation studies on router architecture choices

## Confidence
- **High Confidence**: Parameter reduction ratios (65% for compression, 2270× for upcycling) are well-supported by experimental results
- **Medium Confidence**: The mechanism (shared initialization → redundant deltas → compressible representations) is theoretically sound but relies on assumptions about weight redundancy
- **Low Confidence**: Optimal sparsity rates and rank values are task-specific and may not generalize to other model families

## Next Checks
1. Apply DeRS compression to MoE models upcycled from different base model families to verify universal >0.999 cosine similarity
2. Systematically vary router architecture (top-1 vs top-2 vs top-3, auxiliary loss weights) to determine impact on delta weight compressibility
3. Test whether DeRS-style decomposition can be applied to MoEs trained from scratch by introducing explicit base weight initialization