---
ver: rpa2
title: 'Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual
  Large Language Models'
arxiv_id: '2505.18673'
source_url: https://arxiv.org/abs/2505.18673
tags:
- uni00000011
- uni00000013
- uni00000014
- uni00000010
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel methodology for efficiently identifying
  cross-lingual weaknesses in multilingual large language models (LLMs) by generating
  bilingual question pairs that expose performance discrepancies between English and
  target languages. The approach uses beam search and LLM-based simulation to iteratively
  perturb English questions and maximize accuracy drops in the target language, producing
  over 6,000 bilingual pairs across 16 languages.
---

# Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models

## Quick Facts
- **arXiv ID**: 2505.18673
- **Source URL**: https://arxiv.org/abs/2505.18673
- **Reference count**: 40
- **Primary result**: Method reveals >50% accuracy drops in target languages across 16 languages using beam search with perturbations

## Executive Summary
This paper introduces a novel methodology for efficiently identifying cross-lingual weaknesses in multilingual large language models (LLMs) by generating bilingual question pairs that expose performance discrepancies between English and target languages. The approach uses beam search and LLM-based simulation to iteratively perturb English questions and maximize accuracy drops in the target language, producing over 6,000 bilingual pairs across 16 languages. Extensive experiments show that the method consistently reveals over 50% accuracy drops in target languages across various models, even state-of-the-art ones. Further analysis demonstrates that linguistically related languages share similar weaknesses and benefit from targeted post-training, highlighting the importance of considering linguistic relationships in developing robust multilingual LLMs.

## Method Summary
The methodology generates bilingual question pairs that expose cross-lingual performance gaps by perturbing English questions while preserving semantic equivalence. The process starts with sampling English questions, translating them to target languages, and then using beam search with LLM-based simulation to iteratively add perturbations that maximize target-language accuracy drops while maintaining English performance. A simulation ensemble of five LLMs scores perturbed pairs using a formula that amplifies high English accuracy while penalizing target-language failure. The method employs semantic verification to ensure perturbations preserve meaning, and uses linguistic affinity analysis to cluster languages by shared weaknesses. The approach successfully identifies vulnerabilities across 16 languages and demonstrates that linguistically related languages benefit more from targeted fine-tuning.

## Key Results
- Method consistently reveals >50% accuracy drops in target languages across 16 languages and multiple models
- Linguistic affinity analysis shows related languages share similar weaknesses and benefit more from targeted fine-tuning
- Generated pairs demonstrate significant performance gaps even in state-of-the-art multilingual models like GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Guided Weakness Exposure
- Claim: Adding semantically irrelevant but contextually plausible text to questions exposes cross-lingual performance gaps that simple translation does not reveal.
- Mechanism: A proxy LLM generates perturbations δq_E influenced by incorrect answer choices. These perturbations increase cognitive load while preserving semantic equivalence. The perturbed questions are concatenated to the original (q_E' = ⊕(q_E, δq_E)) and translated. The optimization objective explicitly minimizes target-language accuracy while preserving English accuracy (Eq. 4).
- Core assumption: Models may rely on shallow heuristics in English but lack robust cross-lingual representations to handle added complexity in lower-resource languages.
- Evidence anchors:
  - [abstract]: "approach leverages beam search and LLM-based simulation to generate bilingual question pairs that expose performance discrepancies"
  - [section 2.2]: "φ : Q × A → Q modifies q_E while preserving its original semantics and embedding patterns influenced by the incorrect answer"
  - [corpus]: Related work "When Abundance Conceals Weakness" documents knowledge conflicts in multilingual models, supporting the premise of uneven cross-lingual representations.
- Break condition: If perturbations degrade English accuracy below threshold (ε), or semantic similarity S(q_E, q_E') < θ, the candidate is rejected.

### Mechanism 2: Simulation-Guided Beam Search
- Claim: Multi-model simulation scores efficiently prioritize perturbations that maximize cross-lingual accuracy gaps without exhaustive search.
- Mechanism: A set of K LLMs M = {M_1, ..., M_K} answers each perturbed bilingual pair. The simulation score V(q_E', q_T') = (β̄_E')^γ - β̄_T' amplifies high English accuracy (γ > 1) while penalizing target-language failure. Beam search retains top-w candidates per iteration, with early stopping when V ≥ θ_inc.
- Core assumption: Weaknesses discovered by simulation models generalize to held-out models not used in simulation.
- Evidence anchors:
  - [section 2.3]: "simulation employs a collection of LLMs...to quantify the cross-lingual performance gap introduced by perturbations"
  - [section 3.2]: When GPT-4o was added to simulation models, its accuracy dropped 58% on generated pairs (Figure 10), demonstrating simulation generalizability.
  - [corpus]: Weak direct evidence; corpus papers focus on evaluation methods rather than search optimization.
- Break condition: If max simulation score never exceeds potential threshold θ_pot, search terminates at initial depth d_1 rather than extending to d_2.

### Mechanism 3: Linguistic Affinity Clustering
- Claim: Cross-lingual weaknesses cluster by language family, enabling efficient transfer of both diagnosis and remediation.
- Mechanism: The Relative Affinity Score D_{x,y} quantifies linguistic proximity based on accuracy patterns. Lower D values indicate shared weaknesses. Embedding analysis via t-SNE (Figure 9) shows Asian and European languages cluster separately, with cosine distances within families significantly smaller than across families.
- Core assumption: Observed performance correlations reflect underlying linguistic/typological relationships rather than training data artifacts.
- Evidence anchors:
  - [section 3.3]: "languages closer in linguistic terms tend to share similar weaknesses"
  - [Table 2]: Fine-tuning on Chinese improves Japanese (+35-40%) and Korean (+37-41%) more than European languages (+13-16%)
  - [corpus]: "Cross-Lingual Generalization and Compression" observes shared neurons across linguistically similar languages, providing independent support.
- Break condition: Does not apply to typologically isolated languages or those with insufficient training data where patterns may be noise.

## Foundational Learning

- Concept: Beam Search with Pruning
  - Why needed here: The method uses beam search (width w=12, depths d_1=4, d_2=6) to explore perturbation space. Understanding how candidates are ranked, retained, or pruned is essential for debugging low conversion rates.
  - Quick check question: Given w=12 and depth d_1=4, what is the maximum number of perturbation paths explored before early stopping may trigger?

- Concept: Cross-Lingual Transfer in Multilingual Models
  - Why needed here: The paper's core claim is that linguistic similarity drives both weakness sharing and fine-tuning benefits. Understanding why mBERT/mGPT exhibit cross-lingual transfer (shared subword vocabularies, aligned embeddings) contextualizes these findings.
  - Quick check question: Why might French-to-Spanish transfer be stronger than French-to-Chinese transfer in a multilingual model?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The paper uses DPO alongside SFT for fine-tuning experiments. DPO avoids explicit reward modeling by optimizing preferences directly—relevant for understanding Table 2's "D Enh." results.
  - Quick check question: How does DPO differ from RLHF in terms of required components?

## Architecture Onboarding

- Component map:
  - Source sampler → English questions from ARC, MMLU, CommonsenseQA, TruthfulQA, SciQ
  - Translator → Google Translate API (perturbations) + GPT-4o (questions)
  - Perturbation generator → GPT-4o-mini (temperature 0.7)
  - Semantic checker → LLM-based verification (threshold θ, θ')
  - Simulation ensemble → K=5 LLMs (Gemma-2-9B/27B, Qwen2.5-72B, GPT-4o-mini, Llama-3.1-8B)
  - Beam search controller → Width 12, depths 4-6, thresholds θ_inc=0.8, θ_pot=0.6
  - Candidate list → Final bilingual pairs (6,713 total across 16 languages)

- Critical path:
  1. Sample English question + translate to target → initial bilingual pair
  2. Generate perturbation δq_E based on incorrect answer
  3. Translate δq_T and concatenate to target question
  4. Semantic check passes → simulation ensemble evaluates both versions
  5. Compute simulation score V → rank and retain top-w
  6. Iterate until V ≥ θ_inc or max depth reached
  7. Add to candidate list if English accuracy preserved + target accuracy drops

- Design tradeoffs:
  - Simulation model diversity vs. cost: More models in M improve generalization but increase API costs. Paper uses 5 cost-effective models.
  - Search depth vs. conversion rate: Deeper search finds more weaknesses (d_2=6) but with diminishing returns. French/Spanish require more iterations due to linguistic proximity to English.
  - Perturbation subtlety vs. effectiveness: Too obvious → semantic check fails; too subtle → no accuracy gap. Temperature 0.7 balances this.

- Failure signatures:
  - Near-zero conversion rate: Check semantic equivalence thresholds; may be too strict for target language
  - English accuracy dropping: Perturbation too aggressive; reduce temperature or add stronger semantic constraint
  - High cost per pair (>$0.10): Likely targeting linguistically close languages; consider lowering thresholds or accepting lower conversion
  - Simulation score plateau: Search stuck; increase beam width or adjust γ parameter

- First 3 experiments:
  1. Reproduce baseline comparison (Table 1): Run NP (no perturbation), DP (direct perturbation), and full method on Chinese. Verify conversion rates ~0, 0.036, 0.431 respectively.
  2. Ablate simulation ensemble: Remove one model from M and measure impact on held-out model accuracy. Expect ~5-10% degradation per removal.
  3. Test linguistic transfer: Fine-tune a small model (Phi-3.5-mini) on 100 Chinese pairs, evaluate on Japanese/Korean vs. French/German. Replicate Table 2's Asian vs. European gap pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the perturbation-based probing framework be adapted to effectively identify cross-lingual weaknesses in short or concise prompts where iterative context addition is not feasible?
- Basis in paper: [explicit] The authors state in the Limitations section that the core approach of iteratively adding perturbations is "less sensitive to identifying those vulnerabilities that manifest in very short, concise prompts."
- Why unresolved: The current method relies on increasing question complexity and cognitive demand via context insertion, which fundamentally alters the nature of a short prompt, potentially missing vulnerabilities unique to brief instructions.
- What evidence would resolve it: The development and validation of an alternative probing strategy (e.g., token substitution or structural reordering) that maintains prompt brevity while successfully revealing performance discrepancies in low-context scenarios.

### Open Question 2
- Question: To what degree do cultural nuances and idiomatic expressions in the generated translations introduce biases that are mistaken for cross-lingual reasoning failures?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that despite LLM-based semantic checks, "subtle nuances arising from cultural context or idiomatic expressions might still introduce minor biases."
- Why unresolved: Current evaluation relies heavily on semantic equivalence metrics which may lack the sensitivity to detect cultural dissonance, making it difficult to distinguish between a model's lack of reasoning capability and a culturally loaded translation artifact.
- What evidence would resolve it: A comparative study using human annotators from the specific target cultures to classify errors into "reasoning failures" vs. "translation/cultural artifacts," followed by an adjustment of the generation pipeline to minimize the latter.

### Open Question 3
- Question: Does the correlation between linguistic similarity and shared cross-lingual weaknesses persist when extending this methodology to extremely low-resource languages with non-Latin scripts?
- Basis in paper: [explicit] The paper notes in the Limitations section that the current language scope is "not fully comprehensive" and calls for extending analysis to "limited resources or significantly different structural characteristics."
- Why unresolved: The current findings are based on a specific set of 16 languages; it is unclear if the "linguistically related languages share similar weaknesses" hypothesis holds for languages that are morphologically rich or typologically distinct from English compared to the current dataset.
- What evidence would resolve it: Expansion of the "Cross-Lingual Pitfalls" dataset to include low-resource languages (e.g., Indigenous or Austronesian languages) and an analysis of the resulting embedding clusters to see if they conform to the current linguistic family groupings.

## Limitations
- The perturbation-based approach is less effective for identifying weaknesses in very short, concise prompts
- Subtle cultural nuances and idiomatic expressions in translations may introduce biases mistaken for reasoning failures
- The current analysis is limited to 16 languages and does not fully address extremely low-resource or typologically distant languages

## Confidence
- **High confidence**: The core methodology works as described (beam search + perturbation + simulation), demonstrated by consistent >50% accuracy drops across 16 languages and multiple models.
- **Medium confidence**: The claim that linguistic similarity drives weakness sharing and fine-tuning benefits, supported by clustering patterns and transfer results but not definitively proven to be causally linguistic rather than data-driven.
- **Low confidence**: The assertion that the method reveals "genuine model weaknesses" rather than artifacts of the perturbation process, as the semantic verification thresholds and their language-specific tuning are not fully detailed.

## Next Checks
1. **Simulation Ensemble Ablation**: Systematically remove each model from the simulation set and measure the degradation in held-out model accuracy. Quantify how ensemble diversity affects generalization and establish minimum viable ensemble size.

2. **Linguistic vs. Data Confounders**: Train matched models with identical architectures but different training corpora (e.g., one with typologically diverse vs. English-centric data). Compare whether linguistic affinity patterns persist when data distribution is controlled.

3. **Perturbation Effect Isolation**: Generate pairs where perturbations are removed post-translation and test whether the accuracy gap persists. This would distinguish whether weaknesses stem from added complexity or from translation artifacts introduced by the perturbation process.