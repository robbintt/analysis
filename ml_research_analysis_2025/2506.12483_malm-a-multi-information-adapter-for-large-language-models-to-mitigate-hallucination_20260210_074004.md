---
ver: rpa2
title: 'MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination'
arxiv_id: '2506.12483'
source_url: https://arxiv.org/abs/2506.12483
tags:
- malm
- hallucination
- knowledge
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of hallucination in large language
  models (LLMs), specifically three types: Input-Conflicting, Context-Conflicting,
  and Fact-Conflicting hallucinations. The proposed Multi-Information Adapter for
  Large Language Models (MALM) employs a multi-layered graph attention network (GAT)
  to model interactions between input queries, contextual information, and external
  factual knowledge.'
---

# MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination

## Quick Facts
- **arXiv ID:** 2506.12483
- **Source URL:** https://arxiv.org/abs/2506.12483
- **Reference count:** 6
- **Key outcome:** Multi-Information Adapter (MALM) achieves 5.10% ROUGE-2 improvement on HaluEval and 132.01% ROUGE-2 improvement on TruthfulQA, with significant gains over RAG baselines.

## Executive Summary
This paper addresses hallucination in large language models (LLMs) through a novel Multi-Information Adapter (MALM) that models interactions between input queries, contextual information, and external factual knowledge using a multi-layer graph attention network. MALM is designed as a plug-in adapter that can be slotted between transformer blocks and the output head of LLMs, establishing three types of connections: input connections (from input to output nodes), context connections (within output nodes), and knowledge connections (from knowledge to output nodes). The method demonstrates significant improvements over baselines on four datasets using seven different base LLMs, with particularly strong performance when combined with retrieval-augmented generation.

## Method Summary
MALM employs a multi-layer graph attention network (GAT) to model interactions between input queries, contextual information, and external factual knowledge. The adapter is inserted after the last transformer block of an LLM and uses directed edges to prevent information contamination while allowing information propagation across different types. The method uses a residual fusion strategy to combine original LLM predictions with graph-refined predictions, with a weight of λ=0.2. Training is performed using AdamW with a learning rate of 5×10^-4 for 2 epochs, with LoRA applied to the base LLM and MALM trained from scratch.

## Key Results
- MALM achieved 5.10% improvement in ROUGE-2 on HaluEval and 132.01% improvement in ROUGE-2 on TruthfulQA compared to LLaMA-2
- When combined with retrieval-augmented generation, MALM outperformed state-of-the-art RAG models by 17.57% to 37.49% in ROUGE-1 and 11.28% to 27.92% in FEQA
- Automated evaluation with GPT-4 showed MALM was preferred in 79.4% of cases, while human evaluation showed preference in 65.6% of cases

## Why This Works (Mechanism)

### Mechanism 1: Cross-Information Graph Attention for Hallucination Type Interdependence
- **Claim:** Jointly modeling the interactions between input, context, and knowledge reduces all three hallucination types simultaneously.
- **Mechanism:** A multi-layer graph attention network constructs three subgraphs (input, partial output, knowledge) with directed edges between them. Input connections (input→output), context connections (masked fully-connected within output), and knowledge connections (knowledge→output) enable information propagation while preventing backward contamination. Multi-head attention learns which tokens from each subgraph should influence current generation.
- **Core assumption:** The three hallucination types are causally interdependent (e.g., better factual knowledge reduces context-conflicting errors), and explicit graph structure captures these relationships better than implicit attention in transformers.
- **Evidence anchors:**
  - [abstract] "MALM mitigates hallucination by establishing input, context, and knowledge connections, allowing information to propagate across different types."
  - [section 1.1, p.3] "In Fig. 1 Scene 2, if the LLM has sufficient knowledge that Peking Duck is from northern China, then the self-contradictory phenomenon would not appear."
  - [corpus] Weak direct evidence—related papers (AutoRAG-LoRA, Cross-Layer Attention Probing) address hallucination but not through graph-based cross-information modeling.
- **Break condition:** When retrieved knowledge is irrelevant, incomplete, or contains noise, knowledge connections propagate incorrect signals rather than corrective ones.

### Mechanism 2: Directed Edge Design Prevents Information Contamination
- **Claim:** Unidirectional edges from input/knowledge to output nodes preserve source integrity while allowing downstream influence.
- **Mechanism:** Input connections use directed edges (A_p,q=1 when p∈T, q∈X<i) rather than bidirectional ones. This prevents hallucinated partial outputs from corrupting the model's understanding of user intent. Similarly, knowledge nodes only influence output nodes, not vice versa, ensuring factual references remain uncontaminated.
- **Core assumption:** Hallucinated tokens in early generation can cascade; preventing backward flow limits error propagation while still allowing corrective signals.
- **Evidence anchors:**
  - [section 3.3, p.8] "The connection from a subsequent token to its preceding token is masked to prevent the former token from obtaining the future information, which violates the sequential nature of text generation."
  - [section 3.3, p.8] "A knowledge connection is also directed from the knowledge nodes to the partial output nodes, to avoid the factual hallucination."
  - [corpus] No comparable directed-edge mechanisms found in neighbor papers.
- **Break condition:** When the input itself is ambiguous or underspecified, directed connections cannot resolve the ambiguity since input tokens cannot receive clarifying context from other sources.

### Mechanism 3: Residual Fusion Balances Original LLM Strengths with Hallucination Mitigation
- **Claim:** Weighted combination (ŷ=Softmax(λy^g+(1-λ)y^o)) preserves LLM fluency while injecting graph-refined predictions.
- **Mechanism:** The adapter outputs refined predictions y^g from graph attention, while the original LLM produces y^o directly from hidden states. With λ=0.2, the system leans heavily on original predictions but allows graph-derived signals to correct hallucination-prone tokens. This avoids over-reliance on graph structure which may over-smooth with too many layers.
- **Core assumption:** Original LLM predictions are generally fluent but occasionally hallucinated; the graph adapter provides targeted corrections without disrupting overall generation quality.
- **Evidence anchors:**
  - [section 3.4, p.9] Equations 7-9 define the weighted residual strategy.
  - [section 4.5, p.18-19] Shows L=0 (no MALM) underperforms L=2, but L=4 degrades due to over-smoothing, validating the need for residual balance.
  - [corpus] AutoRAG-LoRA uses adapters but for knowledge retuning, not residual fusion with graph attention.
- **Break condition:** When the base LLM is highly unreliable (e.g., GPT-2 with 124M parameters), λ=0.2 may weight original predictions too heavily, limiting correction potential.

## Foundational Learning

- **Concept: Graph Attention Networks (GAT)**
  - **Why needed here:** MALM's core relies on multi-head attention over graph-structured token relationships, not sequential attention.
  - **Quick check question:** Can you explain how attention weights α_pq are computed from node features and how they differ from transformer self-attention?

- **Concept: Hallucination Taxonomy (Input/Context/Fact-Conflicting)**
  - **Why needed here:** MALM's design explicitly maps each edge type to a specific hallucination category; understanding this taxonomy is essential for interpreting ablation results.
  - **Quick check question:** Given a generated response "The Eiffel Tower, built in 1700 in London, is 324 meters tall," can you identify which hallucination types are present?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA/Adapters)**
  - **Why needed here:** MALM trains only adapter parameters, freezing the base LLM; understanding LoRA helps replicate the experimental setup correctly.
  - **Quick check question:** How does low-rank adaptation reduce parameter count compared to full fine-tuning, and where are LoRA matrices typically injected in transformer architecture?

## Architecture Onboarding

- **Component map:**
  1. **Foundation Model** (LLaMA-2, Qwen, etc.): Two parallel streams—one processes (T, X<i), the other processes K
  2. **Hidden State Extraction**: From last transformer block, initialize vertex features (T^0, X^0<i, K^0)
  3. **MALM Adapter**: L=2 graph layers, each with 3 subgraphs, H=8 attention heads
  4. **Edge Construction**: Build adjacency matrix A with directed edges per connection type
  5. **Graph Attention Update**: Apply modified GAT (Eq. 3-6) for L layers
  6. **Prediction Fusion**: Linear projection + weighted residual (λ=0.2) with original LLM prediction
  7. **Output**: Softmax over vocabulary for next token

- **Critical path:**
  1. Tokenize (input T, partial output X<i, knowledge K)
  2. Forward pass through foundation model (two streams)
  3. Extract hidden states at last transformer block
  4. Build graph: M+(i-1)+S nodes, populate edges per connection rules
  5. Initialize node features with hidden states
  6. For each of L layers: compute multi-head attention, aggregate neighbor features
  7. Project final output node (x^L_{i-1}) to vocabulary logits
  8. Fuse with original prediction via λ-weighted sum
  9. Sample or greedy decode next token

- **Design tradeoffs:**
  - **Layers (L):** L=2 optimal for MALM-only; L=1 better when using BM25 retriever (longer knowledge documents reduce over-smoothing). Trade-off: more layers increase information integration but risk feature homogenization.
  - **Residual weight (λ):** 0.2 prioritizes original LLM fluency. Lower λ increases graph influence but risks degraded coherence; higher λ may under-correct hallucinations.
  - **Connection directionality:** Directed edges prevent contamination but limit bidirectional reasoning (cannot let output tokens query input for clarification).
  - **Retriever choice:** BM25 best for HaluEval (pre-provided knowledge), DPR best for NQ (dense retrieval matches dataset characteristics). Trade-off: retriever quality directly affects knowledge connection effectiveness.

- **Failure signatures:**
  1. **Numerical hallucinations persist:** Case study (p.23) shows MALM answers "10,000" vs. ground truth "110,925"—graph attention doesn't encode magnitude reasoning.
  2. **Degraded performance with L>3:** Over-smoothing causes node features to converge, reducing discrimination (Table 8, p.18).
  3. **No improvement on closed-source LLMs:** Adapter requires access to hidden states; API-only models cannot use MALM (noted as limitation, p.23).
  4. **Knowledge-unavailable scenarios:** If K is empty or irrelevant, knowledge connections provide no signal, and fact-conflicting hallucinations remain (23.8% reduction vs. 72.7% for input-conflicting, Table 11).

- **First 3 experiments:**
  1. **Reproduce HaluEval QA subset with LLaMA-2-7B:** Fine-tune baseline (LoRA, lr=5×10^-4), then add MALM (L=2, H=8, λ=0.2). Compare ROUGE-1/2/L against Table 2 baselines to validate implementation.
  2. **Ablate connection types:** Run w/o Input, w/o Context, w/o Knowledge variants (Table 7). Confirm input connections have largest impact on Exact Match (-10.2% without), knowledge connections most affect ROUGE-L (-5.9% without).
  3. **Test retriever generalization:** Integrate BM25 retriever with MALM on NQ dataset. Compare against LLaMA-2+BM25 baseline and SELF-RAG (Table 4). Expect ~5-6 point ROUGE-1 improvement if knowledge connections function correctly.

## Open Questions the Paper Calls Out
None

## Limitations
- **Architecture Specificity and Reproducibility:** The paper's implementation details are partially underspecified, particularly regarding LoRA configuration (rank, alpha, target modules) and the exact dimension of the GAT hidden layers relative to the LLM hidden size.
- **Generalization Across Hallucination Types:** MALM shows significantly weaker performance on fact-conflicting hallucinations (23.8% reduction) compared to input-conflicting and context-conflicting hallucinations (72.7% and 63.6% reduction respectively).
- **Knowledge Quality Dependency:** MALM's effectiveness is heavily dependent on the quality and relevance of retrieved knowledge, creating a fundamental limitation where the method cannot overcome the "garbage in, garbage out" problem.

## Confidence

- **High Confidence:** Claims about MALM's effectiveness on input-conflicting and context-conflicting hallucinations are well-supported by quantitative results across multiple datasets and base models (Tables 2, 4, 5, 6).
- **Medium Confidence:** The directed edge design preventing information contamination is theoretically sound and partially supported by ablation results, but lacks direct comparative evidence against bidirectional alternatives.
- **Low Confidence:** Claims about MALM's superiority over state-of-the-art RAG models when combined with retrieval augmentation should be interpreted cautiously due to comparison with proprietary models and evaluation methodology concerns.

## Next Checks

1. **Ablation of Connection Directionality:** Implement a variant of MALM with bidirectional edges for input-knowledge connections and compare against the directed edge design to validate the contamination prevention hypothesis.

2. **Knowledge Quality Sensitivity Analysis:** Systematically degrade the quality of retrieved knowledge (remove correct answers, add noise, truncate documents) and measure MALM's performance degradation curve to quantify dependence on knowledge base quality.

3. **Cross-Model Generalization Test:** Apply MALM to a broader range of base models including smaller models (GPT-2, OPT-125M) and different architectures to assess whether the 0.2 residual weight and 2-layer GAT configuration generalize or require model-specific tuning.