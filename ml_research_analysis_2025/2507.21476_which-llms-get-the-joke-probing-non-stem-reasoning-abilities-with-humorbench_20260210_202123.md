---
ver: rpa2
title: Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench
arxiv_id: '2507.21476'
source_url: https://arxiv.org/abs/2507.21476
tags:
- reasoning
- humor
- arxiv
- humorbench
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HumorBench is a benchmark for evaluating large language models'
  ability to understand and explain sophisticated humor in cartoon captions. The benchmark
  focuses on objective elements of humor comprehension rather than subjective appreciation,
  requiring models to identify connections between concepts, cultural references,
  and wordplay.
---

# Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench

## Quick Facts
- arXiv ID: 2507.21476
- Source URL: https://arxiv.org/abs/2507.21476
- Reference count: 21
- Models trained on STEM reasoning data still perform well on humor comprehension tasks

## Executive Summary
HumorBench is a benchmark designed to evaluate large language models' ability to understand and explain sophisticated humor in cartoon captions. The benchmark focuses on objective elements of humor comprehension rather than subjective appreciation, requiring models to identify connections between concepts, cultural references, and wordplay. It includes approximately 300 cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements.

Benchmarking of current state-of-the-art models revealed three key insights: (1) LLM progress on STEM reasoning benchmarks transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning. On the standard HumorBench benchmark, OpenAI o3 achieved 87.5% accuracy, significantly outperforming other models. However, on the harder subset of examples, no current LLM exceeded 60% accuracy, indicating substantial room for improvement in understanding complex humor.

## Method Summary
The HumorBench benchmark evaluates LLM humor comprehension through two main tasks: explaining humor and identifying essential joke elements. The benchmark includes approximately 300 cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com. Expert annotators created evaluation rubrics identifying essential joke elements for each example. Models are assessed on their ability to provide accurate explanations of humor and correctly identify these predefined joke elements. The evaluation focuses on objective comprehension rather than subjective appreciation of humor, requiring models to demonstrate understanding of wordplay, cultural references, and conceptual connections that make cartoons funny.

## Key Results
- OpenAI o3 achieved 87.5% accuracy on the standard HumorBench benchmark
- No current LLM exceeded 60% accuracy on the harder subset of examples
- Models trained exclusively on STEM reasoning data perform well on humor comprehension tasks

## Why This Works (Mechanism)
Humor understanding requires complex reasoning abilities that combine pattern recognition, cultural knowledge, and logical inference. When models succeed at humor comprehension, they demonstrate the ability to recognize non-literal language, understand multiple layers of meaning, and connect disparate concepts. The transferability from STEM reasoning to humor comprehension suggests that both tasks rely on similar underlying cognitive mechanisms involving abstraction, analogy-making, and contextual reasoning.

## Foundational Learning
- **Humor theory fundamentals** - Understanding incongruity resolution and superiority theories explains why certain patterns in jokes work
  - Why needed: Provides theoretical framework for evaluating what constitutes "good" humor understanding
  - Quick check: Can you distinguish between setup-punchline and wordplay-based humor structures?
- **Cross-domain reasoning** - Ability to transfer logical inference skills from STEM to linguistic domains
  - Why needed: Explains why STEM-trained models perform well on language-based humor tasks
  - Quick check: Does performance on logical reasoning benchmarks correlate with humor comprehension?
- **Cultural literacy assessment** - Knowledge of references, idioms, and social contexts that inform humor
  - Why needed: Many jokes rely on shared cultural understanding that models must recognize
  - Quick check: Can models explain jokes that depend on contemporary cultural references?

## Architecture Onboarding

**Component Map**
Input processing -> Context embedding -> Reasoning engine -> Explanation generation -> Element identification

**Critical Path**
The reasoning engine component is critical, as it must handle multi-step inference to connect visual and textual elements, recognize cultural references, and explain non-literal language.

**Design Tradeoffs**
The benchmark trades breadth (covering many humor types) for depth (detailed analysis of fewer examples), focusing on sophisticated New Yorker-style cartoons rather than broad humor categories. This allows for expert-annotated rubrics but limits generalizability to other humor forms.

**Failure Signatures**
Models struggle with jokes requiring cultural knowledge, fail to recognize subtle wordplay, and often miss multi-step reasoning required to connect visual and textual elements. Performance drops significantly on harder examples requiring deeper cultural understanding or more complex logical inference.

**First Experiments**
1. Test model performance on humor comprehension before and after fine-tuning on HumorBench to distinguish genuine reasoning from pattern matching
2. Evaluate models on humor benchmarks from different cultures/languages to test generalizability
3. Compare performance on humor comprehension versus other non-literal language tasks (metaphors, sarcasm) to isolate specific reasoning capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small dataset of approximately 300 examples limits statistical power
- Focus on English-language New Yorker cartoons may not capture humor diversity across cultures
- Expert-annotated rubrics introduce potential subjectivity in determining essential joke elements
- Distinction between humor comprehension and appreciation remains challenging to operationalize

## Confidence

| Claim | Confidence |
|-------|------------|
| LLM progress on STEM reasoning benchmarks transfers effectively to humor comprehension | Medium |
| Models trained exclusively on STEM reasoning data perform well on HumorBench | High |
| Test-time scaling yields mixed results for humor reasoning | Low |

## Next Checks
1. Cross-cultural validation: Test HumorBench on non-English humor datasets and culturally diverse joke types to assess whether the observed reasoning transfer generalizes beyond Western cartoon humor
2. Mechanistic analysis: Conduct ablation studies comparing models' performance on humor reasoning versus other forms of non-literal language understanding (metaphors, sarcasm, idioms) to isolate whether humor comprehension relies on specific reasoning capabilities or general language understanding
3. Temporal stability assessment: Re-evaluate model performance after fine-tuning on HumorBench data to determine whether observed capabilities represent genuine reasoning ability or pattern matching, and whether models can maintain performance on novel humor types