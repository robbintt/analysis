---
ver: rpa2
title: Understanding the Influence of Synthetic Data for Text Embedders
arxiv_id: '2509.06184'
source_url: https://arxiv.org/abs/2509.06184
tags:
- data
- synthetic
- influence
- training
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Synthetic data has driven recent progress in general-purpose text
  embedders, but publicly available datasets for studying its impact remain scarce.
  This work reproduces and releases the synthetic data from Wang et al.
---

# Understanding the Influence of Synthetic Data for Text Embedders

## Quick Facts
- arXiv ID: 2509.06184
- Source URL: https://arxiv.org/abs/2509.06184
- Reference count: 40
- Key outcome: Synthetic data for text embedders shows sparse, task-specific improvements with cross-task interference; LLaMA-3.1-8B nearly matches 70B quality at 5× lower cost

## Executive Summary
Synthetic data has become a key driver for training general-purpose text embedding models, but publicly available datasets for studying its impact remain scarce. This work reproduces and releases the synthetic data from Wang et al. (2024), generated using LLaMA-3.1-8B and LLaMA-3.1-70B, both more cost-efficient than proprietary models. Our analysis reveals that benefits from synthetic data are sparse and highly localized to individual datasets, with training on examples designed for one task often degrading performance on others. These findings challenge the assumption that more diverse synthetic data always improves generalization and highlight the need for more refined strategies in training robust, general-purpose embedding models.

## Method Summary
The paper trains general-purpose text embedders using contrastive learning on a mixture of public E5 data (~1.5M samples) and synthetic data (~500k samples per category from LLaMA-3.1-8B/70B). Six synthetic data categories are generated using task-specific templates: short-short, short-long, long-long, long-short, bitext, and STS. Base models (Mistral-7B, Qwen2-1.5B) are fine-tuned with LoRA (r=16, α=16) using contrastive loss. The study conducts influence function analysis across 16 model combinations to understand how each synthetic category affects downstream performance on MTEB's 56 datasets across 7 task categories.

## Key Results
- LLaMA-3.1-8B synthetic data performs nearly as well as LLaMA-3.1-70B while costing 5× less
- Benefits from synthetic data are sparse and localized to individual datasets rather than broadly generalizing
- Training on synthetic examples for one task category can degrade performance on others (e.g., short-long improves reranking but harms STS)
- Short-short synthetic data shows no statistically significant improvements on any MTEB category

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data as Task-Specific Contrastive Signal
Synthetic data generated by LLMs provides targeted contrastive training examples that improve specific embedding tasks, but benefits are sparse and localized rather than broadly generalizing. LLMs generate (query, positive, hard negative) triplets following task-specific templates. During contrastive learning, the model learns to pull positive examples closer while pushing hard negatives apart in embedding space. The category structure (short-short, short-long, long-short, long-long) creates inductive biases for different task types.

### Mechanism 2: Cross-Task Interference Through Representation Drift
Training on synthetic data for one task category can degrade performance on others, creating trade-offs. Different synthetic data categories shift the embedding space in incompatible directions. Long-short data optimizes for asymmetric retrieval patterns, while short-long optimizes for different similarity notions. When combined, these gradients can interfere, causing the model to learn representations that sacrifice one task's performance for another.

### Mechanism 3: Cost-Quality Asymptote in Generator Scale
Smaller LLMs (8B parameters) can generate synthetic data nearly as effective as larger models (70B) for embedding training, with 5× cost reduction. The synthetic data quality for contrastive learning plateaus at lower generator capability than expected. The constraint for embedding training is not generator fluency but rather semantic coherence of (query, positive, negative) relationships, which smaller models can capture adequately for the distribution of tasks in MTEB.

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE Loss**
  - Why needed here: The paper trains embedders using contrastive loss on triplets (query, positive, negative). Understanding how this objective shapes the embedding space is essential for interpreting why synthetic data works and why trade-offs emerge.
  - Quick check question: Given a batch of triplets, what does InfoNCE optimize and how does hard negative mining change the gradient signal?

- **Concept: Influence Functions for Data Attribution**
  - Why needed here: Section 3 uses influence functions to estimate how each synthetic data category affects downstream performance. This methodology requires understanding how to attribute model behavior to training subsets.
  - Quick check question: How would you compute the influence of a training subset S on a specific evaluation metric without retraining from scratch?

- **Concept: MTEB Task Taxonomy and Evaluation Protocol**
  - Why needed here: The paper's findings depend on MTEB's 7 task categories (retrieval, classification, clustering, etc.). Each task type has different evaluation protocols that determine what "good embeddings" means.
  - Quick check question: For classification vs. retrieval tasks in MTEB, how are embeddings used differently during evaluation?

## Architecture Onboarding

- **Component map:** LLM Generator (LLaMA-3.1-8B/70B) → Synthetic Data Pipeline (brainstorming → instance generation) → ~500k triplets across 6 categories → Training Data Mixer (public E5 data + synthetic) → Base Embedder (Mistral-7B / Qwen2-1.5B) → LoRA fine-tuning, contrastive loss → MTEB Evaluation (56 datasets, 7 task categories) → Influence Analysis (16 model combinations)

- **Critical path:** The synthetic data generation quality determines downstream gains. Focus first on the prompt templates in Appendix B (Tables 4-7) and the category composition (Table 2: short-long dominates with 146k-154k samples).

- **Design tradeoffs:**
  - More diverse data ≠ better generalization: The paper shows sparse benefits and trade-offs, contradicting the assumption that adding all categories helps
  - Generator scale vs. cost: 8B models are 5× cheaper with minimal quality loss, but the paper notes "more capable models might lead to different conclusions"
  - Batch size and negatives: Training uses batch size 2048 (following Wang et al.), which determines in-batch negative count for contrastive loss

- **Failure signatures:**
  - STS performance degradation when training on short-long data (Figure 3: -1.34 normalized influence)
  - No statistically significant improvement from short-short data on any MTEB category
  - Cluster-specific gains that don't generalize (Figure 4: only 2 of 6 clustering datasets improve significantly)

- **First 3 experiments:**
  1. Reproduce the 16-combination influence analysis: Train Mistral-7B with all subsets of {short-short, short-long, long-short, long-long} to verify trade-offs replicate on your infrastructure
  2. Ablate generator model: Compare LLaMA-3.1-8B vs. 70B vs. GPT-4o-mini on a single category (e.g., short-long) to validate cost-quality tradeoff claims
  3. Test category isolation: Train separate models on individual synthetic categories and evaluate on all MTEB tasks to map the full cross-task influence matrix beyond the 4-category subset studied

## Open Questions the Paper Calls Out

**Interaction Effects**: There might be non-trivial interactions between the synthetic dataset and existing public data that weren't controlled for in the setup, making it impossible to disentangle interaction effects from standalone synthetic data effects.

**Generator Capability Impact**: The paper cannot rule out that synthetic data generated from more capable LLMs might lead to different conclusions, as only LLaMA-3.1-8B and 70B were tested.

**Negative Transfer Mechanisms**: While the paper documents that training on synthetic examples designed for one task can degrade performance on others, it doesn't explain the underlying cause of why certain task pairs exhibit negative transfer.

**Mitigation Strategies**: The conclusion calls for more refined strategies in training robust, general-purpose embedding models, suggesting that uniform mixing of synthetic categories may not be optimal.

## Limitations

- Synthetic data quality and generation procedures are undisclosed, creating uncertainty in reproduction efforts
- Task representation coverage limited to MTEB's 56 datasets across 7 categories, which may not generalize to other embedding scenarios
- Cost-quality generalization may not hold across different synthetic data generation scenarios or task complexities

## Confidence

**High Confidence Claims**:
- The reproduction of synthetic data quality matching prior work is verifiable through MTEB performance metrics
- The observation that training on category-specific synthetic data yields sparse and task-specific improvements can be validated through the influence function analysis
- The cost reduction from using 8B vs 70B generators is a direct calculation from training costs

**Medium Confidence Claims**:
- The mechanism that different synthetic categories shift embedding space in incompatible directions requires more extensive validation across diverse tasks
- The assertion that sparse benefits challenge the assumption that more diverse synthetic data always improves generalization needs broader task coverage to confirm
- The claim that current synthetic data approaches have limitations requires testing with alternative generation strategies

**Low Confidence Claims**:
- Generalizing findings to proprietary generators like GPT-4o beyond the cost-quality comparison made in the paper
- Predicting optimal synthetic data strategies for tasks not represented in MTEB
- Assuming the interference mechanisms observed would persist with larger capacity embedder models or different contrastive learning objectives

## Next Checks

1. Validate Category-Specific Influence Patterns: Reproduce the 16-combination influence analysis across all synthetic data categories to confirm that short-long data improves reranking but degrades sentence similarity, and that short-short provides no statistically significant benefits.

2. Test Generator Model Scaling: Conduct ablation studies comparing LLaMA-3.1-8B, 70B, and GPT-4o-mini on individual synthetic categories (starting with short-long) to independently verify the 5× cost reduction claim and determine if the quality plateau holds across different generator capabilities.

3. Expand Task Coverage: Train separate models on each synthetic category and evaluate on MTEB plus 2-3 additional embedding benchmarks from different domains (e.g., biomedical, legal, multilingual) to test whether the observed trade-offs and sparse benefits generalize beyond the original task set.