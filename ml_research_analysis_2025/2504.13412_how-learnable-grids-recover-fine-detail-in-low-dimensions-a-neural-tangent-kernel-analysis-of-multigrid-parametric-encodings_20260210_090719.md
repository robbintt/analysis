---
ver: rpa2
title: 'How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent
  Kernel Analysis of Multigrid Parametric Encodings'
arxiv_id: '2504.13412'
source_url: https://arxiv.org/abs/2504.13412
tags:
- encoding
- encodings
- grid
- kernel
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes multigrid parametric encodings (MPE) for improving
  neural networks' ability to learn high-frequency details in low-dimensional mappings.
  Using neural tangent kernel (NTK) theory, the authors prove that MPEs raise the
  eigenvalue spectrum of the NTK through their learnable grid structure, not their
  embedding space.
---

# How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings

## Quick Facts
- arXiv ID: 2504.13412
- Source URL: https://arxiv.org/abs/2504.13412
- Reference count: 40
- Primary result: MPEs raise NTK eigenvalue spectrum by 8 orders of magnitude over baseline through learnable grid structure, not embedding space

## Executive Summary
This paper analyzes multigrid parametric encodings (MPE) for improving neural networks' ability to learn high-frequency details in low-dimensional mappings. Using neural tangent kernel (NTK) theory, the authors prove that MPEs raise the eigenvalue spectrum of the NTK through their learnable grid structure, not their embedding space. This is fundamentally different from Fourier feature encodings (FFE), which rely solely on embedding. Experiments on 2D image regression and 3D implicit surface regression show that MPE increases the minimum eigenvalue by 8 orders of magnitude over baseline and 2 over FFE, corresponding to 15 dB (PSNR) / 0.65 (MS-SSIM) improvement over baseline and 12 dB (PSNR) / 0.33 (MS-SSIM) over FFE. The results validate that MPE's superior performance stems from its grid structure, offering a new understanding of how learned grid parameters improve spectral bias mitigation compared to traditional FFE approaches.

## Method Summary
The authors analyze coordinate-based MLPs for 2D image regression and 3D implicit surface regression, comparing baseline networks, Fourier feature encodings (FFE), and multigrid parametric encodings (MPE). MPE uses learnable parameters stored on multigrid cells, interpolated via bilinear/trilinear schemes and concatenated with original coordinates. The theoretical analysis uses NTK theory to prove that MPEs raise the eigenvalue spectrum through their grid structure contribution (K^MPE term), not embedding. Experiments use 100 ImageNet synonym sets for 2D images and Stanford dataset for 3D meshes, with metrics including PSNR, MS-SSIM, and NTK eigenvalue spectrum. Training uses SGD with specified learning rates and batch sizes, with MPE grid parameters initialized N(0, 0.01).

## Key Results
- MPE raises minimum NTK eigenvalue by 8 orders of magnitude over baseline and 2 orders over FFE
- MPE achieves 15 dB PSNR / 0.65 MS-SSIM improvement over baseline, 12 dB / 0.33 over FFE
- MPE spectrum improvement comes from K^MPE contribution, not embedding space (validated by ablation)
- Fine MPE (grid=200) shows significant unused cells at high resolution (white patches in Figure 12)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPEs raise the NTK eigenvalue spectrum through their learnable grid structure, enabling faster convergence on high-frequency details.
- Mechanism: The MPE adds positive semi-definite kernel matrices (K^MPE terms) from each grid layer and parameter to the base NTK. By Weyl's inequality, this sum guarantees λ_i^{MLP} ≤ λ_i^{MLP} + λ_n^{MPE} ≤ λ_i^{MLP+MPE}, raising all eigenvalues.
- Core assumption: The grid parameters produce non-zero minimum eigenvalues in practice (theoretical bound allows zero, but empirical data shows this is far from true).
- Evidence anchors:
  - [abstract] "By finding a lower bound on the smallest eigenvalue of the NTK, we prove that MPEs improve a network's performance through the structure of their grid and not their learnable embedding."
  - [Section 4, Theorem 1] Formal proof using Weyl's inequality showing λ_i^{MLP} ≤ λ_i^{MLP+MPE} for all eigenvalues.
  - [corpus] Weak/absent—no direct corpus support for NTK eigenvalue mechanisms in grid encodings.
- Break condition: If grid parameters remain near initialization (small learning rate or insufficient training), K^MPE contribution stays negligible, and spectrum improvement collapses toward baseline.

### Mechanism 2
- Claim: Higher eigenvalues translate to faster convergence on their corresponding eigenvectors, mitigating spectral bias against high-frequency features.
- Mechanism: Training error dynamics follow |Q(f_θ(X,t) - Y)| = |-e^{-Λt}QY|. Eigenvalues λ with larger magnitude cause exponential decay of error along their eigenvectors faster. High-frequency features map to low eigenvalues in baseline MLPs; MPEs raise these values.
- Core assumption: The finite-width NTK approximates the infinite-width training dynamics sufficiently for spectral analysis to predict convergence rates.
- Evidence anchors:
  - [Section 3.4] Derivation showing e^{-KN TK t} = Q^T e^{-Λt}Q and the relationship between eigenvalue magnitude and convergence speed.
  - [Section 5, Figure 4] Empirical correlation between higher eigenvalue spectra and PSNR/MS-SSIM improvements.
  - [corpus] Weak—neighbor papers discuss learnable components in other domains but not NTK spectral dynamics.
- Break condition: If the NTK changes dramatically during training (breaking lazy training assumptions), the initial spectrum no longer predicts convergence behavior.

### Mechanism 3
- Claim: MPEs and FFEs improve performance through fundamentally different pathways—MPEs via learnable grid parameters, FFEs via embedding space alone.
- Mechanism: FFEs modify only the input to the MLP (γ_F(x)), changing K^{FFE} through embedding dimensions. MPEs add an explicit K^{MPE} term from grid gradients (Equation 12, fourth term). Removing K^{MPE} from analysis drops MPE spectrum to near-baseline levels.
- Core assumption: The ablation (computing NTK without K^{MPE} contribution) accurately isolates the grid's contribution from the embedding effect.
- Evidence anchors:
  - [Section 4] Equation 14 shows FFE lacks the additional gradient term present in Equation 12 for MPE.
  - [Section 4, Figure 2] "Without the contributions of the learnable grid, the MPE has little to no effect on the eigenvalues as compared to baseline."
  - [corpus] No corpus papers directly compare FFE vs. MPE mechanisms.
- Break condition: If grid interpolation gradients become correlated across samples (reducing rank of K^{MPE}), the additive benefit diminishes.

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: The paper's entire theoretical framework uses NTK to explain spectral bias and encoding effectiveness. Without understanding K_NTK = E[⟨∂f/∂θ_i, ∂f/∂θ_j⟩], the proofs and eigenvalue analysis are opaque.
  - Quick check question: Can you explain why the NTK remains constant during training in the infinite-width limit, and why this matters for spectral analysis?

- Concept: **Spectral Bias in Coordinate-Based MLPs**
  - Why needed here: The problem MPEs/FFEs solve is that naive MLPs learn low frequencies orders of magnitude faster than high frequencies. Understanding this motivates why encodings are necessary.
  - Quick check question: Why do low eigenvalues in the NTK correspond to high-frequency features, and how does this create the spectral bias problem?

- Concept: **Bilinear/Trilinear Interpolation on Grids**
  - Why needed here: MPEs compute embeddings by interpolating learnable parameters from grid vertices. The gradient structure of this interpolation (Equation 11) directly determines the K^{MPE} contribution.
  - Quick check question: Given a sample point inside a grid cell, how would you compute the gradient of the interpolated value with respect to each corner's learnable parameter?

## Architecture Onboarding

- Component map:
  - Input coordinates (x ∈ R^d, typically d=1-3) → MPE encoding (γ_M,ϕ) → MLP backbone (2-8 hidden layers) → Output (pixel value / occupancy)
  - MPE encoding contains: L resolution levels × k learnable scalars per grid point, interpolated via bilinear (2D) or trilinear (3D) schemes, then concatenated with original coordinates

- Critical path:
  1. Sample input coordinate x
  2. For each resolution level l: find containing grid cell, compute interpolation weights, retrieve learnable parameters at corners, compute interpolated value per parameter channel
  3. Concatenate all interpolated values (L × k dimensions) with original x
  4. Pass through MLP to produce output
  5. During backprop, gradients flow to grid parameters via interpolation gradients (Equation 11)

- Design tradeoffs:
  - **Grid resolution vs. memory**: Fine grids (e.g., x=200 cells) improve spectrum but increase parameter count linearly. Hash encodings (Müller et al. 2022) reduce memory with spatial hashing collisions.
  - **Number of levels (L) vs. computational cost**: More levels raise eigenvalue spectrum but require more interpolation operations per forward pass.
  - **Parameters per node (k) vs. expressiveness**: Higher k increases embedding dimension and K^{MPE} rank but may waste capacity if grid already captures structure.
  - **FFE frequency (L) vs. aliasing**: High-frequency FFEs improve spectrum but risk aliasing artifacts; MPEs have no such limitation (Section 1).

- Failure signatures:
  - **Spectrum near baseline**: Grid learning rate too low or initialization variance too small (K^{MPE} ≈ 0). Check Figure 2 pattern.
  - **Sparse grid utilization**: Fine grids with uniform sampling may leave cells unused (Figure 12 shows white patches). Consider sparse/hash variants.
  - **NTK instability during training**: If spectrum shifts dramatically from initialization, the lazy training assumption breaks—reduce learning rate or increase network width.
  - **Memory explosion in 3D**: Trilinear interpolation on dense 3D grids scales as O(n³). Use hash encoding or sparse multigrid (Hadadan et al. 2021).

- First 3 experiments:
  1. **Reproduce 2D image regression** (Section 5, Figure 3): Train baseline MLP, low/mid/high FFE, and coarse/fine MPE on a single image. Plot eigenvalue spectra at epoch 150 and 300. Verify MPE spectrum exceeds FFE by 2+ orders of magnitude at minimum eigenvalue. Expected: fine MPE achieves ~41.5 dB PSNR vs. ~36.5 dB for high FFE.
  2. **Ablate K^{MPE} contribution**: Compute NTK with and without the grid gradient term (Equation 12, fourth term). Confirm that without K^{MPE}, the MPE spectrum collapses to near-baseline (Figure 2). This validates the mechanism claim.
  3. **Grid resolution sweep**: Vary grid cell count (x = 50, 100, 150, 200) while holding k and L constant. Plot minimum eigenvalue vs. resolution. Expect monotonic increase, but diminishing returns as resolution exceeds data Nyquist frequency. Check for unused cells (Figure 12 pattern) at fine resolutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of activation function (e.g., sinusoidal, GeLU) impact the Neural Tangent Kernel (NTK) spectrum and spectral bias of Multigrid Parametric Encodings (MPEs) compared to the ReLU networks analyzed?
- Basis in paper: [explicit] The conclusion explicitly states that the analysis "does not account for the influence of different activation functions."
- Why unresolved: The theoretical proof and empirical validation were restricted to networks using ReLU activations.
- What evidence would resolve it: A derivation of the MPE NTK lower bound using non-ReLU activation functions and empirical spectral plots comparing convergence rates across these activations.

### Open Question 2
- Question: Can the interpolation kernel within the MPE (currently bilinear) be modified (e.g., to bicubic or sinc) to analytically improve the NTK eigenvalue spectrum or reduce memory usage without sacrificing detail?
- Basis in paper: [explicit] The conclusion suggests future work could "better optimize them to specific applications by adjusting the interpolation kernel."
- Why unresolved: The study treats the grid interpolation as a fixed bilinear operation and does not vary the kernel function itself in the analysis.
- What evidence would resolve it: A comparative analysis of NTK spectra and PSNR/MS-SSIM metrics for MPEs utilizing various interpolation kernels against the baseline bilinear approach.

### Open Question 3
- Question: Does the massive increase in the minimum eigenvalue (8 orders of magnitude) for MPEs lead to poor generalization or instability in sparse data regimes or extrapolation tasks?
- Basis in paper: [inferred] The paper demonstrates MPEs can learn high-frequency details and "discontinuities" effectively, but the extremely high minimum eigenvalue suggests a capacity to fit noise (overfitting), which is a known trade-off not discussed in the regression-focused experiments.
- Why unresolved: The experiments focus on dense regression (image fitting, implicit surfaces) where the network must fit all data, rather than generalizing from sparse samples.
- What evidence would resolve it: Experiments measuring test error on held-out data when training MPEs on subsampled (sparse) datasets compared to FFEs and baselines.

## Limitations
- Theoretical analysis assumes lazy training (NTK remains approximately constant), but real-world MPE training shows some NTK evolution during early epochs
- 8-order minimum eigenvalue improvement claim relies on a specific initialization scheme that may not generalize to all architectures or tasks
- Corpus analysis reveals no direct neighbors discussing NTK spectral dynamics for grid encodings, limiting external validation

## Confidence
- **High Confidence**: The NTK eigenvalue analysis methodology (Section 3) is mathematically rigorous and well-grounded in established theory
- **Medium Confidence**: The ablation study showing K^{MPE} contribution is critical (Figure 2) is compelling but could benefit from more systematic parameter sweeps
- **Low Confidence**: The claim that MPEs and FFEs work through fundamentally different mechanisms lacks corpus support for comparative analysis

## Next Checks
1. **NTK Evolution Monitoring**: Track eigenvalue spectrum changes throughout training (not just at initialization/end). If eigenvalues shift significantly during early epochs, the lazy training assumption may break, affecting convergence predictions.
2. **Cross-Domain Generalization**: Apply MPE to non-image tasks (e.g., audio waveform regression or 1D signal reconstruction) to test if the spectral improvement generalizes beyond 2D/3D spatial data.
3. **Grid Parameter Analysis**: Visualize and analyze the learned grid parameters across resolution levels and channels. Are parameters sparse or uniformly distributed? Do higher-resolution levels learn distinct patterns compared to lower levels? This would validate whether the grid structure genuinely captures different frequency components.