---
ver: rpa2
title: Multi-task Learning for Identification of Porcelain in Song and Yuan Dynasties
arxiv_id: '2503.14231'
source_url: https://arxiv.org/abs/2503.14231
tags:
- classification
- porcelain
- accuracy
- learning
- ware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of automating the classification
  of Chinese porcelain artifacts from the Song and Yuan dynasties across four attributes:
  dynasty, glaze, ware, and type. We evaluate four CNN architectures (ResNet50, MobileNetV2,
  VGG16, and InceptionV3) with and without transfer learning using a dataset of 5,993
  porcelain images.'
---

# Multi-task Learning for Identification of Porcelain in Song and Yuan Dynasties

## Quick Facts
- arXiv ID: 2503.14231
- Source URL: https://arxiv.org/abs/2503.14231
- Reference count: 40
- Primary result: Transfer learning significantly improves porcelain classification accuracy, with MobileNetV2 achieving 97.3% accuracy for dynasty classification and 86.1% for type classification.

## Executive Summary
This study addresses the challenge of automating the classification of Chinese porcelain artifacts from the Song and Yuan dynasties across four attributes: dynasty, glaze, ware, and type. The authors evaluate four CNN architectures (ResNet50, MobileNetV2, VGG16, and InceptionV3) with and without transfer learning using a dataset of 5,993 porcelain images. Their results demonstrate that transfer learning significantly enhances classification accuracy, with MobileNetV2 and ResNet50 achieving the highest performance across all tasks. The study highlights the effectiveness of transfer learning in cultural heritage applications, particularly when working with limited domain-specific data.

## Method Summary
The authors developed a multi-task learning framework that jointly classifies porcelain artifacts across four attributes: dynasty (2 classes), ware (10 classes), glaze (7 classes), and type (12 classes). They used pre-trained CNNs (ResNet50, MobileNetV2, VGG16, InceptionV3) with ImageNet weights, replacing the final classification layer with four parallel fully connected heads. The model was trained using Adam optimizer (lr=0.001) with a batch size of 32 for 50 epochs, employing random horizontal flips and rotations as data augmentation. The total loss was computed as the sum of four cross-entropy losses.

## Key Results
- Transfer learning significantly improves performance, particularly for complex type classification (73.3% to 86.1% accuracy)
- MobileNetV2 and ResNet50 achieve the highest accuracy across all tasks (MobileNetV2: 97.3% for dynasty, 95.3% for ware, 95.3% for glaze, 86.1% for type)
- VGG16 performs poorly on type classification with balanced accuracy of only 49.4%
- Multi-task learning with shared backbone shows faster convergence and lower training loss compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from ImageNet pre-training substantially improves porcelain classification accuracy, particularly for complex tasks with limited domain data.
- Mechanism: Pre-trained CNNs encode generic visual features (edges, textures, shapes) in early layers that transfer across image domains. These learned representations provide a better initialization point than random weights, reducing the search space for optimal parameters and accelerating convergence.
- Core assumption: Low-level and mid-level visual features learned from natural images (ImageNet) are relevant to porcelain artifact classification.
- Evidence anchors:
  - [abstract]: "Our results demonstrate that transfer learning significantly enhances classification accuracy, particularly for complex tasks like type classification, where models trained from scratch exhibit lower performance."
  - [section 4.3]: "MobileNetV2 demonstrates notable performance gains, with test accuracy improving from 73.3% to 86.1% for the 'Type' task and balanced accuracy rising from 66.9% to 84.8%."
  - [section 4.3]: "models initialized with pre-trained weights exhibit significantly faster convergence and lower overall training loss compared to those trained from scratch"
- Break condition: If target domain images are visually dissimilar from natural images (e.g., spectral imaging, non-photographic data), transfer benefits may diminish.

### Mechanism 2
- Claim: Multi-task learning with shared backbone improves generalization across related porcelain classification tasks by exploiting feature interdependence.
- Mechanism: A shared convolutional backbone extracts common low- and mid-level features useful across all tasks, while task-specific fully connected layers specialize for each output. The joint optimization encourages the shared representation to capture features that are useful for multiple related attributes (dynasty, ware, glaze, type), acting as implicit regularization.
- Core assumption: The four classification tasks share underlying visual features and have meaningful statistical correlations (e.g., certain dynasties produce specific glazes and ware types).
- Evidence anchors:
  - [section 3.3]: "Porcelain features are often interrelated. Certain dynasties are associated with specific styles, glazes, and types of porcelain, meaning that features learned for ware, glaze, and type classification can aid in dynasty recognition."
  - [section 3.3]: "The shared convolutional layers extract common low- and mid-level and mid-level features such as edges, textures, and patterns, which are useful across all tasks."
- Break condition: If tasks are largely independent or conflicting, shared representations may introduce negative transfer, degrading individual task performance.

### Mechanism 3
- Claim: Architecture choice significantly impacts performance on fine-grained porcelain classification, with residual and efficient architectures outperforming older designs.
- Mechanism: ResNet50's residual connections mitigate vanishing gradients in deeper networks. MobileNetV2's inverted residual structure with bottleneck layers provides efficient feature extraction. These architectural features enable better gradient flow and more effective learning of subtle visual distinctions compared to VGG16's uniform deep structure without skip connections.
- Core assumption: Architectural inductive biases (residual connections, efficient bottlenecks) align well with the visual feature hierarchy needed for porcelain classification.
- Evidence anchors:
  - [section 4.2]: "MobileNetV2 and ResNet50 emerge as the most robust models, demonstrating high accuracy and strong generalization across validation and test sets."
  - [section 4.2]: "VGG16, however, performs poorly [on Type classification], with its balanced accuracy plummeting to 49.4% and an F1 score of 0.635"
  - [section 4.1]: "VGG16 exhibits the highest initial loss, approximately 4.0... maintains the highest final loss"
- Break condition: If computational resources are unconstrained and very deep networks can be trained from scratch with sufficient data, architectural advantages may narrow.

## Foundational Learning

- Concept: Transfer Learning with CNNs
  - Why needed here: The entire methodology depends on understanding how pre-trained weights from ImageNet provide feature representations that transfer to domain-specific tasks.
  - Quick check question: Can you explain why features learned on natural images (dogs, cars, etc.) would help classify porcelain artifacts?

- Concept: Multi-Task Learning Loss Aggregation
  - Why needed here: The paper sums cross-entropy losses from four tasks without weighting. Understanding how gradient signals from different tasks interact during backpropagation is essential.
  - Quick check question: What happens to the shared backbone gradients when one task has much higher loss magnitude than others?

- Concept: Class Imbalance and Balanced Accuracy
  - Why needed here: The dataset is highly imbalanced (Song: 5288 vs Yuan: 705; some glaze types have 2668 samples, others 54). The paper uses balanced accuracy to account for this.
  - Quick check question: Why would standard accuracy be misleading when evaluating performance on minority classes?

## Architecture Onboarding

- Component map:
  Input: 224×224 RGB porcelain images → Pre-trained CNN backbone → Four parallel FC heads → Four classification outputs → Sum of four cross-entropy losses → Total loss → Backpropagation

- Critical path:
  1. Load pre-trained weights from ImageNet
  2. Replace final classification layer with four parallel FC heads
  3. Forward pass through shared backbone → four separate outputs
  4. Compute individual cross-entropy losses, sum for total loss
  5. Backpropagate combined gradients through shared backbone

- Design tradeoffs:
  - Unified loss weighting (simple but assumes equal task importance) vs. learned/task-weighted losses
  - Freezing early layers (faster, less overfitting risk) vs. fine-tuning entire network (potentially better domain adaptation)
  - Single-view vs. multi-angle input: paper includes multiple views per artifact but does not explicitly model view relationships

- Failure signatures:
  - VGG16 pattern: High initial loss, slow convergence, poor balanced accuracy on minority classes—suggests architecture mismatch with dataset scale
  - Type classification consistently lowest across all models (86.1% vs. 97%+ for dynasty): indicates high intra-class variability
  - Confusion between visually similar categories (Peng/Ding ware, Green/White glaze, Vase/Plate from top views)

- First 3 experiments:
  1. Replicate MobileNetV2 with transfer learning on single task (dynasty) to establish baseline; compare against from-scratch training to validate transfer benefit magnitude.
  2. Ablate the multi-task structure: train four independent single-task models and compare aggregated performance vs. multi-task to isolate shared representation benefits.
  3. Address class imbalance: implement weighted cross-entropy loss or focal loss for minority classes (e.g., Yuan dynasty, rare glazes) and measure impact on balanced accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does a domain-specific pre-training dataset (focused on ceramics and historical artwork) improve classification performance over general ImageNet pre-training?
- Basis in paper: [explicit] The authors propose developing a "domain-specific pre-training dataset" because ImageNet may not capture the nuanced visual characteristics of porcelain, particularly for the complex "Type" classification task.
- Why unresolved: The current study relied solely on ImageNet weights, which are optimized for general objects rather than the fine-grained textures and glaze details required for cultural heritage analysis.
- What evidence would resolve it: Comparative experiments showing higher balanced accuracy and F1-scores on the "Type" task when using models pre-trained on a specialized cultural heritage dataset versus the current baseline.

### Open Question 2
- Question: Can attention mechanisms or Vision Transformers resolve the misclassification of visually similar categories, such as "Green" glaze vs. "White" glaze, or "Vase" vs. "Plate"?
- Basis in paper: [explicit] The paper suggests integrating attention mechanisms (like Squeeze-and-Excitation) or Vision Transformers to guide models to focus on relevant features, potentially fixing current struggles with subtle hue differences and shape ambiguities.
- Why unresolved: The current CNN architectures (MobileNetV2, ResNet50) struggle with specific confusions (e.g., Peng vs. Ding ware) due to visual similarity and viewing angle variability, resulting in lower balanced accuracy for minority classes.
- What evidence would resolve it: Improved confusion matrices and higher balanced accuracy scores for the "Glaze" and "Type" tasks, demonstrating reduced misclassification rates between visually ambiguous classes.

### Open Question 3
- Question: Does the multi-task learning framework generalize effectively to porcelain artifacts from different cultural regions or collections outside the National Palace Museum in Taipei?
- Basis in paper: [inferred] The "Limitations" section states that generalizability to unseen datasets remains uncertain and that robustness against artifacts from "different collections, regions, or historical contexts is untested."
- Why unresolved: The models were trained and validated on a dataset sourced exclusively from a single museum with consistent imaging specifications, leaving their performance on diverse acquisition environments unknown.
- What evidence would resolve it: Evaluation of the trained models on external datasets from different museums or archaeological digs, maintaining comparable accuracy and robustness without retraining.

## Limitations
- Generalizability remains uncertain as models were trained exclusively on National Palace Museum collection
- Limited domain-specific pre-training may not capture nuanced visual characteristics of porcelain
- Performance on minority classes (rare glazes, Yuan dynasty) remains challenging despite balanced accuracy metrics

## Confidence
High confidence in transfer learning mechanism (multiple evidence anchors, consistent results across models)
Medium confidence in multi-task learning benefits (logical mechanism, but no ablation study presented)
Medium confidence in architecture superiority (VGG16 consistently underperforms, but no direct architecture ablation)
Low confidence in domain-specific limitations (acknowledged but not empirically tested)

## Next Checks
1. Verify class distribution in dataset matches paper's reported numbers (Song: 5288, Yuan: 705)
2. Test MobileNetV2 baseline with ImageNet transfer vs. training from scratch on dynasty task
3. Implement weighted loss for minority classes and measure impact on balanced accuracy