---
ver: rpa2
title: 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive
  Decomposition'
arxiv_id: '2507.18802'
source_url: https://arxiv.org/abs/2507.18802
tags:
- human
- feedback
- dxhf
- text
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DxHF, a user interface designed to improve
  the quality of human feedback for aligning large language models. The key idea is
  to decompose long-form text responses into individual atomic claims and use visual
  encodings like opacity and linking to guide attention during comparison.
---

# DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition

## Quick Facts
- **arXiv ID:** 2507.18802
- **Source URL:** https://arxiv.org/abs/2507.18802
- **Reference count:** 40
- **Key outcome:** Interactive decomposition of text into atomic claims with visual encodings improves human feedback accuracy by 5% in crowdsourced studies.

## Executive Summary
This paper introduces DxHF, a user interface designed to improve the quality of human feedback for aligning large language models. The key idea is to decompose long-form text responses into individual atomic claims and use visual encodings like opacity and linking to guide attention during comparison. A technical simulation study shows that decomposition with ranking and linking improves accuracy, especially for uncertain annotators. A crowdsourcing user study with 160 participants confirms that DxHF increases feedback accuracy by 5% compared to baseline pairwise comparison, though it slightly increases completion time. Accuracy gains are particularly strong for users with lower certainty. An ablation study reveals that both ranking and linking features contribute to DxHF's effectiveness. Overall, DxHF demonstrates that thoughtful user interface design can meaningfully improve the reliability of human feedback in LLM alignment tasks.

## Method Summary
DxHF processes text comparisons through a pipeline of decomposition, ranking, and linking. Sentences are first split using NLTK, then decomposed into atomic claims using GPT-4 with a few-shot prompt. Each claim is scored for relevance to the query using a Cross-Encoder architecture, and opacity encoding visualizes this relevance. Similar claims across responses are linked using cosine similarity of text embeddings, with hover interactions revealing summary keywords. The interface uses an accordion-style UI with fold/unfold toggles for managing claim lists. Technical evaluation uses synthetic annotators modeled with Boltzmann rationality, while user studies compare DxHF against baseline pairwise text comparison using Amazon Mechanical Turk participants.

## Key Results
- DxHF increases feedback accuracy by 5% absolute compared to baseline pairwise text comparison in crowdsourced studies
- Accuracy gains are particularly strong for users with lower certainty (simulated β < 2 conditions)
- Both ranking (opacity encoding) and linking features contribute to effectiveness based on ablation studies
- Average feedback time increases by 18 seconds compared to baseline, trading efficiency for accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long-form text into atomic claims improves judgment accuracy, especially when annotators are uncertain.
- Mechanism: Cognitive load reduction—instead of holding two long paragraphs in working memory and mentally identifying differences, annotators compare discrete, single-information claims one at a time. The decomposition principle (Armstrong, 1975) suggests this supports more thorough consideration under uncertainty.
- Core assumption: Annotators' errors stem primarily from cognitive overload rather than lack of domain knowledge; decomposition does not lose critical information.
- Evidence anchors:
  - [abstract]: "This approach breaks down the text into individual claims instead of directly comparing two long-form text responses... improves feedback accuracy regarding the ground truth, particularly for users with uncertainty."
  - [section 5.3]: Simulation results show decomposition methods outperform baseline when rationality β < 2, supporting the hypothesis that decomposition aids low-certainty conditions.
  - [corpus]: Weak direct corpus evidence—no related papers specifically test decomposition for RLHF interfaces.

### Mechanism 2
- Claim: Relevance-based visual encoding (opacity) guides attention to high-value claims, reducing wasted effort.
- Mechanism: Focus+Context principle—visually de-emphasizing low-relevance claims (lower opacity) while maintaining legibility allows annotators to allocate attention selectively without completely hiding context.
- Core assumption: The Cross-Encoder relevance scores correlate with what annotators *should* prioritize for helpfulness judgments.
- Evidence anchors:
  - [section 4.2]: "We utilize a Cross-Encoder architecture to measure the contextual relevance of each claim: rᵢ = CrossEncoder(Q, cᵢ). The value rᵢ ranges from 0 to 1."
  - [section 7.2]: Ablation study shows DxHF w/o ranking (M=3.81 usefulness) underperforms full DxHF (M=4.31).
  - [corpus]: No corpus papers test opacity-based relevance encoding specifically.

### Mechanism 3
- Claim: Linking semantically similar claims across responses with summary keywords reduces comparison effort.
- Mechanism: Externalized mental alignment—instead of annotators searching for corresponding points across texts, the interface pre-computes semantic similarity (cosine similarity of embeddings) and displays connections with keyword labels on hover.
- Core assumption: Semantic similarity (threshold 0.7) identifies claims that annotators would naturally want to compare; keywords accurately summarize shared meaning.
- Evidence anchors:
  - [section 4.2]: "We connect similar claims from both lists... using the cosine similarity of their textual embeddings... We set threshold th of relevance at 0.3... include links with similarity at 0.7."
  - [section 7.2]: DxHF w/o linking (65.7% accuracy) underperforms full DxHF (66.7%); participants valued links for "topic-specific comparisons."
  - [corpus]: Related work on text alignment visualization (Yousef & Janicke survey cited) supports linking techniques broadly, but no direct RLHF application evidence.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: DxHF targets the preference collection phase of RLHF/DPO pipelines; understanding how pairwise comparisons feed reward models clarifies why feedback quality matters.
  - Quick check question: Can you explain how a single pairwise preference label is used to train a reward model?

- **Atomic Claims / Text Decomposition**
  - Why needed here: The core intervention is decomposing sentences into claims with one piece of information each; understanding fidelity-utility tradeoffs is critical.
  - Quick check question: Given "He wrote and directed two films," what are two valid atomic claims? What information might be lost?

- **Focus+Context Visualization Principle**
  - Why needed here: DxHF's opacity encoding and hover interactions are instantiations of this HCI principle; distinguishing it from simple highlighting matters for design decisions.
  - Quick check question: Why might hiding irrelevant claims entirely be worse than de-emphasizing them with opacity?

## Architecture Onboarding

- **Component map:** NLTK sentence tokenizer -> GPT-4 prompt (Appendix A.1) -> claim lists A, B -> Cross-Encoder(Q, claim) -> relevance scores -> opacity encoding -> text embeddings -> cosine similarity -> linking module -> GPT-4 keyword summarization (Appendix A.2) -> D3.js accordion UI with hover interactions

- **Critical path:**
  1. Sentence tokenization (NLTK)
  2. Per-sentence claim decomposition (GPT-4 call per sentence)
  3. Relevance scoring (Cross-Encoder inference)
  4. Similarity computation and linking
  5. UI rendering with opacity/link overlays
  6. User interaction -> preference selection -> log to feedback dataset

- **Design tradeoffs:**
  - Accuracy vs. speed: DxHF increases average feedback time by 18 seconds (section 6.2); acceptable for high-stakes alignment but not for high-volume collection
  - Fidelity vs. simplification: Decomposition preserves words from original text, but complex sentences may lose nuance
  - Automation vs. bias: LLM-driven decomposition and ranking may introduce systematic biases; full original text remains accessible to mitigate

- **Failure signatures:**
  - Over-decomposition: Claims become fragments lacking subject or context
  - Link sparsity: No links appear when similarity threshold too high
  - Opacity illegibility: Low-opacity text unreadable on certain displays
  - Interaction confusion: Annotators don't discover hover features

- **First 3 experiments:**
  1. **Decomposition fidelity audit**: Manually inspect 50 decomposed sentences for information preservation and grammatical coherence; measure claim-claim and claim-source alignment
  2. **Relevance threshold calibration**: Vary opacity thresholds (e.g., 0.2, 0.3, 0.4) on held-out HH-RLHF pairs; measure correlation between high-relevance claims and ground-truth preference explanations
  3. **Link ablation timing**: Run controlled study with/without links on identical tasks; measure time-to-decision and subjective workload (NASA-TLX) to quantify efficiency gains beyond accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interactive decomposition be adapted to support holistic judgments, such as assessing the coherence, tone, or style of a text, which the current DxHF design explicitly excludes?
- Basis in paper: [explicit] The authors state in the Introduction and Discussion that the current design scope is "not well-suited for holistic judgments that involve assessing the coherence, tone, or style of text as a whole."
- Why unresolved: The atomic claim structure breaks text into small pieces, which inherently loses the "gestalt" or flow of the narrative required for stylistic evaluation.
- What evidence would resolve it: A study evaluating a modified DxHF interface designed for holistic metrics, comparing its performance against the baseline on tasks requiring tone or style assessment.

### Open Question 2
- Question: Can the decomposition principle be effectively applied to alignment dimensions other than helpfulness, specifically "honesty" and "harmlessness" (the HHH framework)?
- Basis in paper: [explicit] The Discussion notes that "future research could also explore decompositions emphasizing different alignment dimensions, such as helpfulness, honesty, and harmlessness (HHH)."
- Why unresolved: The current study focused solely on "helpfulness"; honesty and harmlessness may require different decomposition logic (e.g., verifying facts vs. detecting subtle safety risks).
- What evidence would resolve it: An evaluation of DxHF on datasets curated specifically for truthfulness (honesty) or safety (harmlessness) to see if the accuracy gains hold.

### Open Question 3
- Question: Does the transformation of text into atomic claims fragment the annotator's understanding, leading to a loss of the "overall impression" or context?
- Basis in paper: [explicit] The Discussion acknowledges that "the transformation of the text presentation also increases the risk of fragmenting the annotator's understanding," citing a participant who noted that analysis without reading the whole text can be misleading.
- Why unresolved: While DxHF improves accuracy on specific comparison tasks, it is unclear if the interface encourages a "trees over forest" approach that degrades the annotator's mental model of the full text.
- What evidence would resolve it: Eye-tracking or qualitative studies measuring the retention of overall context and narrative flow for users of DxHF versus standard reading interfaces.

### Open Question 4
- Question: To what extent does the LLM used for decomposition introduce its own biases into the human feedback loop?
- Basis in paper: [explicit] The Discussion raises the concern: "A key concern is whether DxHF might introduce its own bias. If that is the case, it would be attributable to the LLM that is used to pick the highlighted points."
- Why unresolved: The study measured accuracy against ground truth, but did not isolate or measure the specific influence of the LLM's choice of decomposition (what is highlighted vs. what is not) on the human annotator.
- What evidence would resolve it: An ablation study analyzing if different LLMs used for decomposition lead to systematically different preference distributions from human annotators.

## Limitations
- Limited evidence for decomposition mechanism beyond simulation; real-world user performance gains are modest (5% absolute) and may not generalize beyond crowdworkers
- Core assumptions about cognitive load reduction lack direct empirical validation—no cognitive workload measures or eye-tracking were collected
- Relevance scoring and linking rely on black-box LLM outputs without ablation of feature importance or sensitivity analysis
- Simulation uses LLM-as-a-judge which may not capture human judgment patterns, especially in low-certainty conditions

## Confidence

- Mechanism 1 (Decomposition reduces cognitive load): Medium - supported by simulation but limited real-user validation
- Mechanism 2 (Relevance-based opacity): Medium - ablation shows contribution but no calibration study
- Mechanism 3 (Linking improves efficiency): Low-Medium - user feedback positive but no objective efficiency metrics collected

## Next Checks
1. Conduct cognitive workload assessment (NASA-TLX or similar) comparing baseline vs. DxHF interfaces to directly test cognitive load reduction claims
2. Perform feature ablation sensitivity analysis: systematically vary relevance thresholds and similarity cutoffs to identify optimal parameter ranges
3. Replicate with domain experts on alignment-relevant tasks to test generalizability beyond crowdworker preferences