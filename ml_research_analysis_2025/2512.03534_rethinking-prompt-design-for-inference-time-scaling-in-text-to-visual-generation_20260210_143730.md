---
ver: rpa2
title: Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation
arxiv_id: '2512.03534'
source_url: https://arxiv.org/abs/2512.03534
tags:
- prompt
- pris
- prompts
- scaling
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving text-to-visual
  generation by scaling both prompts and visuals at inference time. The core idea
  is to adaptively revise prompts based on recurring failure patterns observed across
  multiple generated samples, rather than keeping prompts fixed as in prior methods.
---

# Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation

## Quick Facts
- arXiv ID: 2512.03534
- Source URL: https://arxiv.org/abs/2512.03534
- Reference count: 40
- Key outcome: Improves text-to-visual generation by 7% on GenAI-Bench and 15% on VBench 2.0 by adaptively revising prompts based on cross-sample failure patterns

## Executive Summary
This paper introduces PRIS, a framework that scales both prompts and visuals at inference time for text-to-visual generation. Unlike prior methods that keep prompts fixed, PRIS adaptively revises prompts based on recurring failure patterns observed across multiple generated samples. The approach uses an Element-level Factual Correction (EFC) verifier that decomposes prompts into semantic elements and evaluates their fulfillment using text-based comparisons to avoid bias in visual question answering. Experiments demonstrate significant gains over best-of-N baselines, with scaling benefits sustained beyond the plateaus observed with fixed prompts.

## Method Summary
PRIS operates through a multi-stage inference loop: initial generation of M samples with the original prompt, element-level verification using EFC to identify which prompt components are fulfilled, selection of top-k samples covering most elements, identification of common failures (elements with <50% success rate), prompt revision to reinforce overlooked elements, and regeneration of additional samples using revised prompts and seeds from top samples. The EFC verifier decomposes prompts into semantic elements, captions each visual, and uses NLI classification against each element rather than direct binary VQA to reduce affirmative bias.

## Key Results
- Achieves 7% improvement on GenAI-Bench for text-to-image generation
- Delivers 15% improvement on VBench 2.0 for text-to-video generation
- Sustains scaling benefits beyond the plateaus observed with fixed-prompt best-of-N methods
- Integrates effectively with visual scaling approaches without reward over-optimization

## Why This Works (Mechanism)

### Mechanism 1: Cross-Sample Failure Aggregation Enables Targeted Prompt Refinement
PRIS identifies systematic weaknesses by aggregating failure patterns across top-performing samples rather than analyzing each sample in isolation. This approach targets recurring failure modes that reflect prompt-level guidance gaps, not random sampling variance. The framework selects top-k samples and identifies elements with <50% success as common failures, then revises prompts to address only these patterns.

### Mechanism 2: Text-Based NLI Verification Reduces Affirmative Bias in Visual Assessment
EFC uses NLI classification between generated captions and prompt elements instead of direct binary VQA, achieving 0.763 accuracy versus 0.700 for decomposed binary VQA. This text-to-text approach mitigates confirmation bias and provides interpretable descriptions, while binary VQA often omits such information. The neutral elements trigger open-ended probing with follow-up NLI rather than yes/no questions.

### Mechanism 3: Seed Reuse with Revised Prompts Preserves Favorable Noise Conditions
PRIS regenerates samples using revised prompts while reusing noise latents from top-k samples, preserving earlier successes more reliably than random initialization. The assumption is that certain noise conditions yield better alignment for specific prompt types, and this quality transfers across semantically related prompt variations.

## Foundational Learning

- **Natural Language Inference (NLI)** — determining whether a premise entails, contradicts, or is neutral to a hypothesis
  - Why needed here: EFC uses NLI to classify each prompt element against generated captions, forming the core verification signal
  - Quick check question: Given caption "a white bird emerges from an orange" and element "the bird is crafted from fresh orange," what NLI label applies?

- **Best-of-N (BoN) inference scaling** — generating N candidates and selecting the highest-scoring
  - Why needed here: PRIS compares against BoN baselines and extends them by adding prompt revision
  - Quick check question: Why does BoN with fixed prompts plateau even as N increases?

- **Diffusion noise seeds** — the initial latent that shapes sampling trajectory
  - Why needed here: PRIS reuses seeds from top-k samples, assuming favorable noise transfers to revised prompts
  - Quick check question: What assumption must hold for seed reuse to improve over random initialization?

## Architecture Onboarding

- **Component map**: Visual Generator (FLUX.1-dev/Wan2.1) -> EFC Verifier (Qwen2.5-VL) -> PRIS Controller -> Optional Reward Model (VideoAlign)
- **Critical path**: 1) Generate M initial samples with original prompt 2) Run EFC to get element-level verification for each sample 3) Select top-k samples covering most elements 4) Identify common failures (<50% success rate in top-k) 5) Revise prompt to reinforce failed elements 6) Regenerate (N−M) samples with revised prompt + top-k seeds 7) Return highest-scoring sample
- **Design tradeoffs**: EFC adds ~3 image generations worth of compute per verification batch; k=⌈N/4⌉ balances exploration with exploitation; one revision provides substantial gains while iterative yields diminishing returns
- **Failure signatures**: Per-sample revision degrades T2V performance by attempting to fix every error; reward over-optimization occurs when visual scaling alone renders prompt text directly on images
- **First 3 experiments**: 1) Reproduce table 1 on GenAI-Bench subset comparing BoN vs. PRIS at fixed NFE=2000 2) Ablate verification method by replacing EFC with decomposed binary VQA or VideoAlign 3) Test iterative scaling by running 2 rounds of PRIS revision to measure compounding gains

## Open Questions the Paper Calls Out

- **Fine-tuning smaller models**: Can smaller language models trained on failure-focused prompt pairs reduce PRIS verification computational overhead?
- **Generalization to other modalities**: Does PRIS framework generalize to text-to-3D or text-to-audio generation domains?
- **Non-semantic quality assessment**: Can EFC verifier be adapted to assess motion smoothness, safety, or bias detection?
- **Optimal failure threshold**: Is the 50% success probability threshold for identifying common failures optimal across varying prompt complexities?

## Limitations
- NLI verification performance may degrade significantly on highly abstract, stylized, or non-photorealistic visuals where caption quality breaks down
- Seed reuse effectiveness may diminish as semantic distance between original and revised prompts increases
- The exact saturation point for PRIS scaling remains unexplored, particularly at very high NFE values

## Confidence

**High Confidence** (Extensive empirical support, clear mechanisms):
- PRIS improves alignment metrics over fixed-prompt baselines on both GenAI-Bench and VBench 2.0
- Common failure aggregation outperforms per-sample revision
- PRIS integrates well with visual scaling methods without reward over-optimization

**Medium Confidence** (Supported by paper evidence but limited external validation):
- Text-based NLI verification reduces affirmative bias compared to binary VQA
- Seed reuse preserves favorable noise conditions across prompt revisions
- Cross-sample failure aggregation identifies systematic weaknesses missed by per-sample analysis

**Low Confidence** (Mechanism claims with minimal empirical or external support):
- Iterative revisions provide cumulative gains (only one iteration tested)
- Scaling benefits extend indefinitely with sufficient NFE
- Performance transfers across diverse model architectures beyond tested ones

## Next Checks

1. **Verification Robustness Test**: Evaluate EFC on abstract, stylized, and low-caption-quality visuals to measure NLI accuracy degradation and identify failure modes where caption generation breaks down.

2. **Semantic Divergence Analysis**: Systematically vary semantic distance between original and revised prompts (measured via embedding similarity) to track seed reuse effectiveness and identify the threshold where transferability breaks.

3. **Scaling Saturation Experiment**: Run PRIS to much higher NFE values (10K-50K) on both T2I and T2V tasks, plotting alignment metrics against NFE to identify the exact point where PRIS plateaus and compare to underlying generator's theoretical limits.