---
ver: rpa2
title: Reinforcement Learning Based Prediction of PID Controller Gains for Quadrotor
  UAVs
arxiv_id: '2502.04552'
source_url: https://arxiv.org/abs/2502.04552
tags:
- controller
- control
- gains
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reinforcement learning-based methodology
  for online fine-tuning of PID controller gains to improve quadrotor UAV trajectory
  tracking performance. The approach employs a Deep Deterministic Policy Gradient
  (DDPG) algorithm to adaptively adjust controller parameters during flight, eliminating
  the need for complete system modeling including drag effects and environmental disturbances
  during training.
---

# Reinforcement Learning Based Prediction of PID Controller Gains for Quadrotor UAVs

## Quick Facts
- arXiv ID: 2502.04552
- Source URL: https://arxiv.org/abs/2502.04552
- Reference count: 21
- RL-based PID gain fine-tuning improves quadrotor tracking RMSE from 12.75×10⁻³ to 11.17×10⁻³ rad in simulation, and from 33.93×10⁻² to 22.55×10⁻² rad in outdoor experiments

## Executive Summary
This paper presents a reinforcement learning methodology for online fine-tuning of PID controller gains to improve quadrotor UAV trajectory tracking performance. The approach employs a Deep Deterministic Policy Gradient (DDPG) algorithm to adaptively adjust controller parameters during flight, eliminating the need for complete system modeling including drag effects and environmental disturbances during training. The method was validated through comprehensive evaluation including simulation, Hardware-In-The-Loop testing, and real-world outdoor experiments using a Pixhawk 2.1 Cube Black flight control unit, demonstrating significant improvements over hand-tuned controllers.

## Method Summary
The methodology implements a hierarchical controller where a feedback linearization inner loop is augmented with RL-adjusted PID gains. A DDPG agent receives a 12-dimensional state vector (positions, Euler angles, and their errors) and outputs normalized weight adjustments for five PID parameters. The agent is trained offline in MATLAB/Simulink using a piecewise reward function based on attitude error norm thresholds, with gain updates constrained to ±40% of initial values. After training convergence (average episode reward exceeding baseline of 117), the actor network weights are extracted and manually reconstructed in Simulink for code generation to the Pixhawk hardware. The system operates at 20 Hz sampling rate and was validated on circular trajectory tracking tasks.

## Key Results
- Simulation: RL-enhanced controller achieved attitude error norm RMSE of 11.17×10⁻³ rad versus 12.75×10⁻³ rad for manual tuning
- HIL Testing: Real-time deployment validated tracking performance with fine-tuned gains tracking reference values
- Outdoor Experiments: RL controller achieved RMSE of 22.55×10⁻² rad compared to 33.93×10⁻² rad for manual tuning, demonstrating effectiveness in real-world conditions

## Why This Works (Mechanism)

### Mechanism 1
The DDPG actor-critic algorithm enables continuous, bounded adjustment of PID gains during flight. An actor network outputs normalized weight adjustments (in [-1, 1]) that scale initial hand-tuned gains via `k_new = k_initial × (1 + a × n_weight)`, where `a=0.4` constrains modifications to ±40% of baseline. The critic network estimates the action-value function to guide policy improvement through gradient-based updates. Core assumption: The manually tuned initial gains provide a stable baseline within the searchable region; extreme deviations would remain unexplored. Evidence: [Section 3, Eq. 15] explicit gain update law; [Section 5.1] heuristic search rate assignment of 0.4 bounds gain modifications. Break condition: If initial gains are severely mis-tuned (>40% error), the constrained search space may exclude optimal solutions.

### Mechanism 2
Piecewise reward shaping based on attitude error norm creates a dense learning signal for fine-grained control improvements. The reward function assigns discrete values (-25 to +10) based on six error thresholds (0.0001 to 0.04 rad), creating steep gradients near low-error regions. This shapes policy toward minimizing steady-state error rather than just stability. Core assumption: The error distribution from manual tuning accurately represents the achievable performance envelope. Evidence: [Section 3, Eq. 16] full piecewise reward specification; [Section 5.1] baseline reward (117) computed from observed step counts per error bin. Break condition: If trajectory characteristics change significantly (e.g., aggressive maneuvers), the threshold calibration may misallocate reward density.

### Mechanism 3
Sim-to-real transfer succeeds despite omitting drag, gyroscopic effects, and environmental disturbances during training. The RL agent learns to adapt gains *online* in response to observed state errors, not by modeling disturbances explicitly. The feedback linearization controller provides a nominal dynamics model; the RL layer compensates residual mismatch. Core assumption: The inner-loop feedback linearization sufficiently cancels dominant nonlinearities, leaving the RL agent to handle residual errors. Evidence: [Section 1] explicitly states training omits drag, gyroscopic effects, and external disturbances; [Section 6] "drag and gyroscopic effects were omitted during training but reintroduced in subsequent tests." Break condition: Under high wind gusts or payload changes beyond training distribution, online adaptation may lag or overshoot.

## Foundational Learning

- **Concept: Actor-Critic Architecture**
  - Why needed: DDPG separates policy (actor) from value estimation (critic); understanding this clarifies why two networks train simultaneously with different objectives
  - Quick check: Can you explain why the critic needs the actor's actions as input, but the actor only needs the state?

- **Concept: Feedback Linearization for Nonlinear Systems**
  - Why needed: The inner-loop controller uses feedback linearization to cancel quadrotor nonlinearities before RL fine-tuning; this is why a simple PD structure suffices for the RL-adjusted layer
  - Quick check: Given `M' = W^{-T}[Bv + Cη̇]`, what happens to control authority when the system approaches singularity in the W matrix?

- **Concept: Off-Policy vs. On-Policy Learning**
  - Why needed: DDPG is off-policy, enabling experience replay; this matters for sample efficiency during the 45-second trajectory episodes
  - Quick check: If you switched to an on-policy algorithm (e.g., PPO), how would training data collection change?

## Architecture Onboarding

- **Component map:**
  Training phase (MATLAB/Simulink): Quadrotor dynamics model → PID controller → DDPG agent (actor: 128×128 NN, critic: similar) → Reward computation → Experience buffer (10⁶ samples)
  Deployment phase (Pixhawk 2.1 Cube Black): Extracted actor weights → Reconstructed NN in Simulink → C++ code generation → PX4 firmware overwrite → Real-time inference at 20 Hz (Ts=0.05s)

- **Critical path:**
  1. Manual PID tuning establishes baseline gains and error distribution
  2. RL training converges when average episode reward exceeds baseline (117)
  3. Actor weights extracted and NN reconstructed (Simulink RL block is not code-gen compatible)
  4. HIL validation before outdoor flight

- **Design tradeoffs:**
  - Network size (128×128): Balances approximation capacity against Pixhawk memory limits; larger networks would exceed Flash/RAM
  - Search rate (a=0.4): Larger values expand exploration but risk instability; smaller values limit improvement potential
  - State space (12D): Includes positions, Euler angles, and errors—excludes velocities to reduce dimensionality, potentially limiting dynamic response prediction

- **Failure signatures:**
  - Gain saturation: If fine-tuned gains hit bounds frequently during flight, the search space is too narrow or initial gains are poor
  - Sudden gain spikes: Observed in outdoor tests; indicates rapid environmental disturbance compensation—may signal need for smoother action filtering
  - RMSE worse than manual: Suggests reward function misaligned with actual performance objectives or overfitting to training trajectory

- **First 3 experiments:**
  1. Reproduce numerical simulation: Train agent on circular trajectory with provided parameters; verify RMSE reduction from 12.75×10⁻³ to ~11.17×10⁻³ rad
  2. Ablate search rate: Test a=0.2, 0.4, 0.6 to characterize stability-performance frontier; monitor gain evolution for saturation patterns
  3. Disturbance injection in simulation: Add wind gusts or sensor noise during sim validation; compare online gain adaptation response against static gains

## Open Questions the Paper Calls Out

### Open Question 1
Does training the DDPG agent with explicit models of wind gusts, ground effects, and sensor noise yield statistically significant improvements in tracking accuracy over the current disturbance-free training approach? Basis: [explicit] The authors state that "incorporating disturbances... directly into the training phase could further improve the agent’s adaptability" and allow for better strategies from the outset. Why unresolved: The current methodology trains the agent in a simplified environment (omitting drag and gyro effects) and relies on online adaptation to handle disturbances. It is unclear if the agent is operating optimally or merely compensating for a "reality gap" that could be closed during training. What evidence would resolve it: A comparative analysis of outdoor flight RMSE between the current agent and an agent trained via domain randomization or high-fidelity simulation including environmental disturbances.

### Open Question 2
Can the integration of RTK GPS or optical flow cameras effectively eliminate the initial position offsets that currently degrade the controller's tracking performance? Basis: [explicit] The discussion identifies "GPS inaccuracies" as a main issue causing "significant offset" and proposes "Integrating more precise positioning systems, such as RTK GPS" as a necessary future step. Why unresolved: The current system struggles with initial bias due to standard GPS limitations. It is unknown if improved sensing is sufficient or if the RL agent's state estimation logic requires fundamental retraining to leverage high-precision data. What evidence would resolve it: Outdoor experimental data using RTK GPS showing a reduction in initial positioning error and a corresponding decrease in the overall trajectory tracking RMSE.

### Open Question 3
Can the proposed single-agent RL architecture scale effectively to multi-agent coordination scenarios without violating the memory constraints of the flight control unit? Basis: [explicit] The conclusion suggests "extending this study to include... multi-agent coordination" to assess the scalability of the approach. Why unresolved: The paper highlights memory limitations (requiring a 128x128 network) for a single agent. Multi-agent coordination introduces complex coupling and communication requirements that may exceed the computational capacity of the Pixhawk hardware used. What evidence would resolve it: Successful deployment of the control strategy on a multi-UAV swarm, demonstrating stable formation flight and inter-agent collision avoidance within similar hardware resource limits.

## Limitations
- Training omits drag, gyroscopic effects, and environmental disturbances, relying entirely on online adaptation
- Validation limited to single circular trajectory type without testing aggressive maneuvers or payload variations
- Search rate parameter (a=0.4) appears arbitrarily chosen without systematic sensitivity analysis
- Piecewise reward function lacks comparison to alternative shaping strategies

## Confidence

- **High confidence**: Simulation results showing 12.75→11.17×10⁻³ rad RMSE improvement are directly reproducible from specified parameters
- **Medium confidence**: HIL results showing gain tracking quality; the methodology is sound but environmental control during testing is unclear
- **Medium confidence**: Outdoor flight results showing 33.93→22.55×10⁻² rad RMSE improvement; real-world validation is valuable but limited sample size and trajectory variety reduce generalizability

## Next Checks

1. **Ablate search rate sensitivity**: Systematically test a=0.2, 0.4, 0.6 to characterize stability-performance tradeoff and identify gain saturation patterns
2. **Cross-trajectory generalization**: Apply trained agent to different trajectory types (square, figure-eight) without retraining to assess transfer capability
3. **Disturbance robustness test**: Inject wind gusts or payload changes in simulation to evaluate online adaptation speed and stability compared to static gains