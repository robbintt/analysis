---
ver: rpa2
title: 'When Less Language is More: Language-Reasoning Disentanglement Makes LLMs
  Better Multilingual Reasoners'
arxiv_id: '2505.15257'
source_url: https://arxiv.org/abs/2505.15257
tags:
- reasoning
- ablation
- language
- performance
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free approach to improve multilingual
  reasoning in large language models (LLMs) by disentangling language-specific representations
  from reasoning representations. The method identifies and ablates language-specific
  components in the model's hidden states during inference, reducing linguistic interference
  and enabling more language-invariant reasoning.
---

# When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners

## Quick Facts
- arXiv ID: 2505.15257
- Source URL: https://arxiv.org/abs/2505.15257
- Authors: Weixiang Zhao; Jiahe Guo; Yang Deng; Tongtong Wu; Wenxuan Zhang; Yulin Hu; Xingyu Sui; Yanyan Zhao; Wanxiang Che; Bing Qin; Tat-Seng Chua; Ting Liu
- Reference count: 40
- Primary result: Training-free method improves multilingual reasoning by 1.4-4.5 percentage points through language-specific component ablation

## Executive Summary
This paper introduces a training-free approach to enhance multilingual reasoning in large language models by disentangling language-specific representations from reasoning representations. The method identifies and ablates language-specific components in hidden states during inference, reducing linguistic interference and enabling more language-invariant reasoning. Experiments across 10 open-weight models (3B-32B parameters) and 11 typologically diverse languages demonstrate consistent performance improvements on three multilingual reasoning benchmarks.

The approach achieves gains comparable to or better than multilingual post-training methods while avoiding the computational costs of fine-tuning. Language-specific ablation shows particularly strong improvements for low-resource languages, with Swahili accuracy more than doubling in some models. Layer-wise analysis reveals that middle layers offer the optimal trade-off between reasoning gains and linguistic fidelity.

## Method Summary
The proposed method identifies language-specific components in LLM hidden states using Jensen-Shannon divergence between language-specific token representations and their averaged cross-lingual counterparts. During inference, these identified components are ablated (set to zero) at different layers to reduce linguistic interference while preserving reasoning capabilities. The approach operates without any model retraining, making it a lightweight alternative to multilingual post-training. The JS divergence threshold for identifying language-specific components is set at 1.0, and ablation is applied selectively to tokens identified as carrying language-specific information.

## Key Results
- Language-specific ablation yields consistent accuracy improvements of 1.4-4.5 percentage points across 10 models on multilingual reasoning benchmarks
- Low-resource languages show dramatic gains, with Swahili accuracy more than doubling in some models
- Middle layers provide optimal trade-off between reasoning improvement and linguistic fidelity preservation
- Performance comparable to or better than multilingual SFT and RL methods without requiring fine-tuning

## Why This Works (Mechanism)
The method works by identifying and removing language-specific components from hidden states during inference, reducing linguistic interference that can degrade cross-lingual reasoning performance. By ablating these components selectively at different layers, the approach enables models to reason more in a language-invariant manner while preserving essential linguistic information needed for task comprehension. The Jensen-Shannon divergence provides a principled way to quantify language-specificity at the token level, allowing targeted interventions that improve reasoning without completely sacrificing linguistic fidelity.

## Foundational Learning

**Jensen-Shannon Divergence**
*Why needed:* Quantifies the similarity between probability distributions to identify language-specific token representations
*Quick check:* JS divergence between two distributions should be 0 when identical and increase as distributions diverge

**Hidden State Ablation**
*Why needed:* Technique for removing specific components from model representations during inference to test their contribution
*Quick check:* Ablating random components should degrade performance, while targeted ablations should show selective effects

**Cross-Lingual Transfer**
*Why needed:* Understanding how models leverage knowledge across languages is central to evaluating multilingual reasoning improvements
*Quick check:* Performance on low-resource languages should improve more than high-resource languages when interference is reduced

**Language Invariance**
*Why needed:* Core concept for reasoning that should operate independently of linguistic variations
*Quick check:* Reasoning accuracy should remain stable or improve when linguistic variability is reduced

## Architecture Onboarding

**Component Map**
Input Text -> Tokenization -> Hidden States -> JS Divergence Analysis -> Language-Specific Component Identification -> Selective Ablation (per layer) -> Reasoning Output

**Critical Path**
Token embeddings → Middle layers (optimal ablation points) → Output layer → Final prediction

**Design Tradeoffs**
- Computational cost vs. performance gain (multiple forward passes required)
- Degree of ablation vs. preservation of linguistic understanding
- Layer selection balancing reasoning improvement against linguistic fidelity

**Failure Signatures**
- Over-ablation leading to complete loss of linguistic coherence
- Under-ablation providing insufficient reasoning improvement
- Ablation at wrong layers degrading both reasoning and linguistic performance

**First Three Experiments**
1. Apply language-specific ablation to a single model on MGSM benchmark across all 11 languages
2. Compare ablation effects at different layers (early, middle, late) on XWinograd
3. Measure computational overhead of ablation approach compared to baseline inference

## Open Questions the Paper Calls Out
None

## Limitations
- Method requires multiple forward passes per input, creating computational overhead
- JS divergence threshold of 1.0 is arbitrarily set without sensitivity analysis
- Effectiveness on open-ended generation tasks and longer reasoning chains not evaluated
- Theoretical mechanism of "linguistic interference" remains speculative

## Confidence

**High Confidence:** Consistent accuracy improvements across multiple models, languages, and reasoning tasks with statistically significant gains (1.4-4.5 percentage points)

**Medium Confidence:** Claims of comparable performance to post-training methods require careful interpretation due to different model sizes and datasets used

**Low Confidence:** Theoretical framing of linguistic interference as primary mechanism remains speculative; generalization to non-tested language families unconfirmed

## Next Checks

1. Evaluate language-reasoning disentanglement on task-specific fine-tuned models to assess maintenance or enhancement of optimized performance

2. Quantify effects on cross-lingual transfer efficiency by measuring performance degradation when training on high-resource and testing on low-resource languages

3. Test method effectiveness on open-ended reasoning generation tasks requiring step-by-step problem solving and creative reasoning beyond extractive formats