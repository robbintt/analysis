---
ver: rpa2
title: 'M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented
  Generation'
arxiv_id: '2512.20136'
source_url: https://arxiv.org/abs/2512.20136
tags:
- multimodal
- knowledge
- audio
- answer
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3KG-RAG, a framework that enhances multimodal
  retrieval-augmented generation by constructing a multi-hop multimodal knowledge
  graph and applying query-aware retrieval with selective pruning. The key innovation
  is a lightweight multi-agent pipeline that builds a context-enriched, multi-hop
  MMKG from raw audio-visual data, enabling modality-specific retrieval rather than
  relying on a shared embedding space.
---

# M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2512.20136
- Source URL: https://arxiv.org/abs/2512.20136
- Reference count: 40
- Introduces M3KG-RAG, a framework that improves multimodal RAG with multi-hop MMKG construction and query-aware selective pruning, achieving up to 14.8% gains in model-as-judge scores.

## Executive Summary
M$^3$KG-RAG addresses the limitations of existing multimodal RAG systems by constructing a context-enriched, multi-hop multimodal knowledge graph (MMKG) from raw audio-visual data. Unlike prior approaches that rely on shared embedding spaces, M3KG-RAG employs modality-specific retrieval through a lightweight multi-agent pipeline. The GRASP mechanism ensures retrieved knowledge is both query-relevant and answer-supporting by grounding entities in the input and pruning non-essential facts. Experiments across Audio QA, Video QA, and Audio-Visual QA benchmarks demonstrate consistent improvements over strong baselines, with gains up to 14.8% in model-as-judge scores and higher pairwise win rates across comprehensiveness, diversity, and empowerment criteria.

## Method Summary
M3KG-RAG constructs a multi-hop multimodal knowledge graph through a seven-agent pipeline (rewriter, extractor, normalizer, searcher, selector, refiner) with self-reflection, building from raw audio-visual data in AudioCaps, ActivityNet, and VALOR. Modality-specific retrieval uses FAISS with InternVL2 (video) and CLAP (audio) encoders, avoiding shared embedding spaces. GRASP grounds entities using GroundingDINO (visual) and TAG (audio), prunes non-essential facts based on presence scores and LLM filtering, then injects remaining triplets into the MLLM. The system achieves retrieval quality through modality-aware selection and selective pruning, with hyperparameters tuned per benchmark.

## Key Results
- M3KG-RAG achieves up to 14.8% improvement in model-as-judge scores over strong baselines.
- Win rates across comprehensiveness, diversity, and empowerment criteria are consistently higher than competing approaches.
- The framework demonstrates robustness across three multimodal QA benchmarks: AudioCaps-QA, VCGPT, and VALOR.
- GRASP's selective pruning maintains retrieval quality while reducing context overload, with performance degradation observed when pruning thresholds are too aggressive (ηav=1.5+).

## Why This Works (Mechanism)
M3KG-RAG works by addressing the fundamental limitation of shared embedding spaces in multimodal retrieval, where modality misalignment causes off-topic neighbors and degrades evidence quality. The multi-hop knowledge graph construction captures rich contextual relationships between entities across modalities, while the GRASP mechanism ensures only query-relevant and answer-supporting facts are retrieved. The modality-specific retrieval avoids the pitfalls of cross-modal miscalibration when encoder training domains don't match query distributions. The lightweight agent pipeline with self-reflection enables adaptive knowledge extraction and entity description generation, while selective pruning maintains computational efficiency without sacrificing answer quality.

## Foundational Learning
- **Multi-hop Knowledge Graphs**: Enable reasoning over multiple connected facts to capture richer context than direct retrieval alone; needed to support complex multimodal queries requiring contextual understanding across audio-visual elements.
- **Modality-specific Retrieval**: Avoids cross-modal miscalibration by encoding audio and visual data separately rather than forcing into a shared embedding space; needed when training domains don't cover query distributions.
- **Entity Grounding**: Links retrieved knowledge to specific visual and audio elements in the input using models like GroundingDINO and TAG; needed to ensure retrieved facts are directly relevant to the query context.
- **Selective Pruning**: Filters retrieved knowledge based on presence scores and LLM evaluation to remove non-essential facts; needed to prevent context overload while maintaining answer-supporting evidence.
- **Multi-agent Pipeline**: Distributes complex knowledge graph construction across specialized agents with self-reflection for quality control; needed to handle the complexity of extracting structured knowledge from raw multimodal data.

## Architecture Onboarding
**Component Map**: Qwen3-8B -> Rewriter -> Extractor -> Normalizer -> Searcher -> Selector -> Refiner -> Inspector -> GRASP -> FAISS Index -> MLLM

**Critical Path**: Raw multimodal input → Multi-agent M3KG construction → Modality-wise retrieval via FAISS → GRASP grounding and pruning → Triplet injection into MLLM → Answer generation

**Design Tradeoffs**: 
- Modality-specific retrieval vs. computational overhead of maintaining separate encoders
- Comprehensive knowledge graph vs. retrieval latency and memory usage
- Selective pruning vs. risk of removing potentially useful context
- Self-reflection iterations vs. construction time and resource requirements

**Failure Signatures**: 
- Off-topic retrieval from shared embedding space indicates modality-wise retrieval is not active
- Over-pruning leaving insufficient context suggests η thresholds are too aggressive
- Hallucinated entity descriptions indicate inspector pass rates are too lenient or LLM callback is unreliable

**First Experiments**:
1. Verify modality-wise retrieval by testing Eq. 3 lifts from audio/visual items rather than text triplets
2. Validate GRASP pruning sensitivity by varying η thresholds around reported values
3. Test inspector self-reflection reliability by implementing the agent and monitoring pass rates

## Open Questions the Paper Calls Out
- **Knowledge Coverage Extension**: How can M3KG's knowledge coverage be extended to better represent long-tail entities and rare relations underrepresented in source multimodal corpora?
- **Cross-modal Miscalibration Mitigation**: How can cross-modal miscalibration be mitigated when multimodal encoder training domains don't match query distributions?
- **Continuous Knowledge Graph Updates**: What mechanisms could enable continuous or dynamic updating of M3KG as new multimodal content becomes available?
- **Scalability Assessment**: How does M3KG-RAG performance scale with larger knowledge graphs and computationally constrained deployment scenarios?

## Limitations
- Knowledge coverage is bounded by the raw multimodal corpora used for construction, potentially underrepresenting long-tail entities and rare relations
- Cross-modal miscalibration can surface off-topic neighbors when encoder training domains don't cover query distributions
- The multi-agent construction and GRASP filtering introduce computational overhead, adding ~63% latency and significantly increasing VRAM requirements

## Confidence
- **High Confidence**: Core methodology of multi-hop MMKG construction and GRASP retrieval mechanism are well-specified
- **Medium Confidence**: Experimental results showing consistent improvements, though implementation details limit full verification
- **Low Confidence**: Complete system reproducibility due to unspecified model weights, unclear crawler implementation, and ambiguous M3KG construction timing

## Next Checks
1. Implement a small-scale test to confirm that Eq. 3 correctly lifts from audio/visual items rather than text triplets, ensuring cross-modal links L are properly utilized.
2. Systematically vary the η thresholds (ηa, ηv, ηav) around reported values to confirm the performance degradation pattern, particularly validating over-pruning at ηav=1.5+.
3. Implement the inspector agent with the provided prompt and test its ability to regenerate entity descriptions scoring below 7, monitoring pass rates and quality of regenerated content.