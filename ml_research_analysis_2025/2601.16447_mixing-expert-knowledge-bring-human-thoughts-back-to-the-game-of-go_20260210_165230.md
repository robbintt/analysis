---
ver: rpa2
title: 'Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go'
arxiv_id: '2601.16447'
source_url: https://arxiv.org/abs/2601.16447
tags:
- reasoning
- move
- general
- game
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LoGos is the first general-purpose large language model to achieve
  professional-level Go proficiency while maintaining strong general reasoning abilities.
  The model employs a novel training paradigm that combines cold-start mixed fine-tuning
  of structured Go expertise and general long Chain-of-Thought reasoning data, followed
  by reinforcement learning with Group Relative Policy Optimization.
---

# Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go

## Quick Facts
- arXiv ID: 2601.16447
- Source URL: https://arxiv.org/abs/2601.16447
- Authors: Yichuan Ma; Linyang Li; Yongkang Chen; Peiji Li; Jiasheng Ye; Qipeng Guo; Dahua Lin; Kai Chen
- Reference count: 40
- Key outcome: First general-purpose LLM to achieve professional-level Go proficiency while maintaining strong general reasoning abilities

## Executive Summary
LoGos is the first general-purpose large language model to achieve professional-level Go proficiency while maintaining strong general reasoning abilities. The model employs a novel training paradigm that combines cold-start mixed fine-tuning of structured Go expertise and general long Chain-of-Thought reasoning data, followed by reinforcement learning with Group Relative Policy Optimization. Using only structured domain-specific data, LoGos achieves 88.6% prediction accuracy on the KataGo-Bench-1K benchmark, surpassing all existing general LLMs and matching human professional players. The model also maintains leading performance on general benchmarks including GPQA-Diamond, MATH, and LiveCodeBench.

## Method Summary
LoGos uses a two-phase training approach: (1) Cold start mixed supervised fine-tuning on 10M Go next-move predictions (annotated by KataGo) combined with general long CoT reasoning data, using heuristic-structured templates; (2) Reinforcement learning with Group Relative Policy Optimization (GRPO) using a tiered reward function based on move ranking and win-rate accuracy. The model employs a heuristic template that decomposes reasoning into player identification, move analysis, summary, and structured output, achieving 88.6% prediction accuracy on KataGo-Bench-1K while maintaining strong performance on general reasoning benchmarks.

## Key Results
- Achieves 88.6% prediction accuracy on KataGo-Bench-1K, surpassing all existing general LLMs and matching human professional players
- Maintains leading performance on general benchmarks including GPQA-Diamond, MATH, and LiveCodeBench
- Demonstrates that general LLMs can acquire expert-level capabilities in specialized domains while preserving general reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
Mixed fine-tuning of structured domain expertise with general long-CoT data enables both knowledge injection and reasoning capability transfer. General CoT data provides reasoning scaffolding; structured domain data provides task-specific knowledge. Together, they initialize the model to reason in-domain using familiar patterns. Core assumption: LLMs can generalize reasoning patterns from one domain (math, code) to structurally different domains (Go) when given appropriate domain primitives. Evidence: Mixed fine-tuning on Go + general CoT data serves as cold start before RL.

### Mechanism 2
GRPO-based reinforcement learning enables self-exploration that discovers stable in-domain reasoning strategies beyond cold-start performance. Cold-start model produces structured outputs; GRPO optimizes policy via group-relative advantages using external rewards (KataGo), allowing model to explore and refine reasoning chains. Core assumption: The reward function provides sufficiently dense, accurate signals; cold-start model is close enough to valid strategies that RL can climb rather than collapse. Evidence: Performance rises from 60–70% to >87% during GRPO training.

### Mechanism 3
Heuristically structured training data determines the ceiling of subsequent self-exploration; naive prediction data yields lower ceilings. Heuristic templates decompose reasoning (player identification → move analysis → summary → structured output), teaching the model how to reason, not just what to predict. Core assumption: The external expert (KataGo) provides reliable annotations; heuristic templates align with natural reasoning patterns LLMs can internalize. Evidence: Naive fitting caps below 50% vs. heuristic rules reaching 88%.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm for self-exploration. Requires understanding of policy gradients, group-based advantage normalization, and KL regularization. Quick check: Can you explain how GRPO's group-relative advantage differs from standard PPO's advantage estimation?

- **Chain-of-Thought (CoT) Reasoning in LLMs**: Mixed cold start relies on transferring CoT patterns from general domains to Go. Requires understanding how CoT elicits reasoning and how to curate CoT data. Quick check: What makes CoT data effective for training vs. prompting, and how might domain mismatch affect transfer?

- **Structured vs. Unstructured Domain Knowledge Encoding**: Paper argues heuristically structured synthetic data outperforms naive prediction. Requires judgment on how to template reasoning for new domains. Quick check: For a new domain (e.g., molecular design), what information would a heuristic template need to include to support reasoning transfer?

## Architecture Onboarding

- **Component map**: KataGo annotations → heuristic template construction → mixed with general CoT datasets → cold-start SFT → GRPO reinforcement learning → evaluation

- **Critical path**: 1) Construct heuristic-structured domain data (quality determines ceiling); 2) Mix with general CoT data at appropriate ratio; 3) Cold-start SFT; 4) GRPO with tiered reward function; 5) Evaluate on domain benchmark and general benchmarks

- **Design tradeoffs**: More domain data epochs → better initial point but potentially lower RL ceiling; denser rewards → higher final performance but more engineering; 2D board rendering → mitigates context curse for long games but changes input format

- **Failure signatures**: RL stalls below beginner level → likely missing or weak cold start; prediction accuracy caps ~50% → likely using naive prediction format instead of heuristic templates; performance degrades on long games → context curse; general benchmark performance drops → excessive domain data or overfitting during RL

- **First 3 experiments**: 1) Ablate cold start: Train GRPO directly on instruct model without mixed fine-tuning; expect ceiling <67%; 2) Ablate heuristic structure: Replace heuristic templates with direct prediction; expect ceiling <50%; 3) Vary reward density: Compare top-1-only vs. tiered rewards; expect tiered to reach higher ceiling

## Open Questions the Paper Calls Out

### Open Question 1
Can the mixed-expert-knowledge training paradigm generalize effectively to specialized domains that lack a scalable expert system (like KataGo for Go) for synthesizing training data? The paper only validates the approach on Go, which uniquely benefits from KataGo's mature annotation capabilities. Other specialized domains may lack equivalent expert systems for generating structured synthetic data at scale.

### Open Question 2
What is the root cause of the "context curse" phenomenon where LLMs struggle to maintain prediction accuracy as game sequences lengthen, and does this represent a fundamental limitation of autoregressive architectures for state-tracking tasks? The 2D rendering solution addresses symptoms but not causes.

### Open Question 3
Why does the gap between move prediction accuracy (96.5%) and explanation correctness (55.6%) persist, and can reinforcement learning reward functions be designed to improve reasoning quality alongside prediction accuracy? The GRPO reward function only optimizes for move correctness and win-rate prediction accuracy, not for reasoning quality or terminology usage.

### Open Question 4
What are the scaling limits of the self-exploration RL phase when applied to domains with larger action spaces or more complex strategic reasoning than Go? The paper demonstrates success in Go but does not investigate whether the self-exploration mechanism scales efficiently to domains requiring even more extensive strategic exploration or real-time reasoning.

## Limitations

- Limited generalization: Success demonstrated only on Go, a domain with KataGo's mature annotation capabilities
- Context curse: LLMs struggle with long sequences despite 2D rendering mitigation
- Reasoning quality gap: High move prediction accuracy (96.5%) doesn't translate to high explanation correctness (55.6%)
- Domain dependency: Requires scalable expert systems for synthesizing training data in new domains

## Confidence

- **High confidence** in empirical results: KataGo-Bench-1K accuracy of 88.6% and controlled ablation studies provide strong evidence
- **Medium confidence** in general reasoning preservation: Performance on general benchmarks suggests preservation but lacks deep analysis
- **Medium confidence** in domain transferability: Success with Go is compelling but mechanism's generalizability to other domains remains untested

## Next Checks

1. **Template Design Sensitivity Analysis**: Systematically vary the heuristic template structure to identify which components are essential versus optional for achieving high prediction accuracy

2. **Cross-Domain Transfer Test**: Apply the identical training pipeline to a structurally different domain like protein folding prediction or legal case analysis to validate whether cross-domain reasoning transfer generalizes

3. **Reward Function Robustness Test**: Replace the piecewise reward function with alternative reward structures to determine whether the specific reward design is critical or if the RL framework is more generally applicable