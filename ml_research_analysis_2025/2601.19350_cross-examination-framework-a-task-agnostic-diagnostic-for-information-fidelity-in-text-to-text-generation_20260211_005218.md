---
ver: rpa2
title: 'Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity
  in Text-to-Text Generation'
arxiv_id: '2601.19350'
source_url: https://arxiv.org/abs/2601.19350
tags:
- questions
- translation
- metrics
- semantic
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Cross-Examination Framework (CEF) is a reference-free, task-agnostic
  approach for evaluating information fidelity in text-to-text generation. It treats
  source and candidate texts as independent knowledge bases, generates verifiable
  questions from each, and performs cross-examination to derive three interpretable
  scores: Coverage, Conformity, and Consistency.'
---

# Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity in Text-to-Text Generation

## Quick Facts
- **arXiv ID:** 2601.19350
- **Source URL:** https://arxiv.org/abs/2601.19350
- **Reference count:** 28
- **Primary result:** CEF is a reference-free, task-agnostic approach for evaluating information fidelity in text-to-text generation, detecting semantic errors missed by traditional metrics.

## Executive Summary
The Cross-Examination Framework (CEF) addresses the critical need for reference-free evaluation of information fidelity in text-to-text generation tasks like machine translation, summarization, and clinical note generation. Unlike traditional metrics that require gold-standard references, CEF treats source and candidate texts as independent knowledge bases, generates verifiable questions from each, and performs cross-examination to derive three interpretable scores: Coverage, Conformity, and Consistency. The framework successfully identifies semantic errors such as omissions and contradictions that metrics like BLEU and BERTScore miss, while demonstrating strong correlation with semantic similarity metrics. A systematic robustness analysis identified DeepSeek-V3 as the most stable judge model, with N=10 questions providing near-optimal reliability and efficiency.

## Method Summary
CEF generates N=10 closed-ended YES-only questions from both source and candidate texts using an LLM judge (DeepSeek-V3), then cross-examines by answering questions with the opposite text as context. The framework computes three orthogonal scores: Coverage (100 - %IDK when source questions query candidate), Consistency (100 - %IDK when candidate questions query source), and Conformity (100 - %NO when source questions query candidate). The YES-only constraint ensures factual grounding and evaluation stability, with 99.4% question label retention upon re-evaluation. The approach requires no reference texts, making it applicable across diverse generation tasks where gold standards are unavailable or impractical.

## Key Results
- CEF achieves strong correlation with semantic similarity metrics while exposing critical factual inconsistencies missed by traditional metrics
- DeepSeek-V3 identified as most stable judge model through ADR/ADS analysis across translation and summarization tasks
- N=10 questions provides near-optimal reliability (only marginal gains beyond this point) while maintaining computational efficiency
- CEF demonstrates superior semantic error detection (78.40% alignment with semantic errors vs 23.32% for non-semantic errors) compared to reference-based metrics

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Question-Answering Cross-Examination
- Claim: If two texts contain the same information, they should answer the same questions
- Mechanism: Generate closed-ended questions from both source (D) and candidate (T), then cross-examine by answering D's questions against T and T's questions against D. Mismatched answers reveal semantic divergences.
- Core assumption: Semantic equivalence can be reliably detected through QA consistency patterns
- Evidence anchors: [abstract] "CEF treats source and candidate texts as independent knowledge bases, generates verifiable questions from each, and performs a cross-examination to derive three interpretable scores"; [section 3] "questions generated from the document QD are presented to the generated candidate T yielding a set of answers... Similarly, ÂQT|D is generated by presenting the questions generated from the generated text to the document"
- Break condition: When questions cannot be reliably generated (low-resource languages show "order-of-magnitude increase in inconsistency" per Appendix A.2) or when document is too short for meaningful question diversity

### Mechanism 2: YES-Only Question Constraint
- Claim: Constraining all generated questions to have verifiable "YES" answers ensures factual grounding and evaluation stability
- Mechanism: Prompt engineering forces the LLM to generate only questions whose answers are confirmable from document content; non-YES answers during cross-examination then signal specific error types
- Core assumption: Models reliably generate grounded factual questions when constrained, and answer types map cleanly to error semantics
- Evidence anchors: [section 4.6] "99.4% of 6,141 'YES' questions retain their label upon re-evaluation, with minimal shifts to 'NO' (27 instances) or 'IDK' (7 instances)"; [section 4.6] Mixed-answer prompts show instability: "only 22.3% of 1,021 'IDK' questions remain unchanged, with 357 shifting to 'YES' and 436 to 'NO'"
- Break condition: When document contains ambiguous or contradictory content; when model lacks sufficient grounding capability for the domain

### Mechanism 3: Tripartite Score Decomposition
- Claim: Three orthogonal scores (Coverage, Conformity, Consistency) capture distinct semantic fidelity dimensions that single metrics conflate
- Mechanism: Coverage = absence of IDK when source questions query candidate (omissions); Conformity = absence of NO when source questions query candidate (contradictions); Consistency = absence of IDK when candidate questions query source (hallucinations)
- Core assumption: Answer type distributions correspond to specific error categories
- Evidence anchors: [section 3] Explicit formulas: "Coverage: 100−%(ÂQD|T == 'IDK')", "Conformity: 100−%(ÂQD|T == 'NO')", "Consistency: 100−%(ÂQT|D == 'IDK')"; [section 4.4] Human validation: "CEF aligns with semantic errors (78.40%) at a rate 3.4 times higher than with non-semantic errors (23.32%)"
- Break condition: When multiple error types produce the same answer pattern; when linguistic complexity causes answer ambiguity

## Foundational Learning

- **Concept: Reference-free evaluation paradigm**
  - Why needed here: CEF explicitly operates without gold-standard references, contrasting with BLEU/ROUGE/BERTScore which were formulated with reference requirements
  - Quick check question: Given only a source document and a candidate translation, what information does CEF require that BLEU does not, and vice versa?

- **Concept: LLM-as-judge stability and selection**
  - Why needed here: Framework reliability is "closely tied to the choice of judge model" (Section 6); poor selection introduces bias and instability
  - Quick check question: If you observe high variance in CEF scores across multiple runs on the same input, what are two possible causes related to the judge model?

- **Concept: Question-answering as semantic probing**
  - Why needed here: The core mechanism transforms document comparison into QA consistency checking; understanding this abstraction is essential for debugging
  - Quick check question: Why would a document with high Coverage but low Consistency indicate a different problem than high Consistency but low Coverage?

## Architecture Onboarding

- **Component map:** Source Document -> Question Generator (N questions) -> Q_source; Candidate Text -> Question Generator (N questions) -> Q_candidate; Cross-Examiner (4 answering passes); Score Aggregator -> {Coverage, Conformity, Consistency, Conciseness}

- **Critical path:** Question generation quality -> Answer grounding accuracy -> Score interpretation. The ablation study (Table 2) shows N<5 causes variance explosion (up to 145.08 for noisy translations).

- **Design tradeoffs:**
  - N=10 vs N=20: Doubling questions yields only marginal stability gains (~1.62-point Coverage variance reduction) at 2× cost
  - YES-only vs mixed prompts: Trades question diversity for reliability (99.4% vs 22.3% stability for IDK questions)
  - Computational cost: CEF requires 6-7× the tokens of GEMBA-MQM (~$0.0049 vs $0.0007 per sample) per Appendix A.4

- **Failure signatures:**
  - High score variance across runs -> Likely N too low or temperature > 0
  - Low Coverage + High Consistency -> Candidate is faithful but omits content
  - High Conformity violations -> Contradictions detected; check for negation errors or entity swaps
  - Unstable question generation -> Low-resource language; consider round-trip mode (Appendix A.2)
  - Many "IDK" responses -> Document may be too short or questions insufficiently grounded

- **First 3 experiments:**
  1. Judge stability validation: Run ADR (Average Disagreement Rate) and ADS (Answer Deviation Score) analysis across 3-5 candidate judge models on a held-out validation set of 50-100 samples; select model with lowest combined scores
  2. Question count ablation: On 20 representative samples, compute score variance across 5 runs each for N ∈ {5, 10, 15}; confirm N=10 provides acceptable stability for your domain
  3. Human alignment sanity check: On 20-30 samples with available human annotations, correlate CEF mismatching questions with annotated error spans; verify alignment exceeds 50% for semantic errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can distilled multilingual models replace large frontier models as CEF judges without significant loss in stability?
- Basis in paper: [explicit] The authors state in the Limitations section that "future work should explore experiments with multilingual small or distilled LLMs as judges to reduce cost and bias."
- Why unresolved: The current study identifies DeepSeek-V3 (a large model) as the most stable judge, but the reliability of smaller, cheaper models remains untested.
- What evidence would resolve it: Benchmarking ADR/ADS scores and semantic alignment of distilled models against the DeepSeek-V3 baseline across translation and summarization tasks.

### Open Question 2
- Question: Does incorporating multi-hop reasoning questions improve CEF's detection of complex factual dependencies?
- Basis in paper: [explicit] The paper suggests that "incorporating multi-hop reasoning questions could enhance the framework's ability to capture complex factual relations."
- Why unresolved: The current methodology relies on simple closed-ended questions, which may miss nuanced relational distortions requiring inference across multiple sentences.
- What evidence would resolve it: An ablation study comparing single-hop vs. multi-hop question generation on benchmarks containing complex logic errors (e.g., FRANK).

### Open Question 3
- Question: Can question generation be stabilized for low-resource languages to avoid the need for fallback round-trip translation?
- Basis in paper: [explicit] The authors note that "improving multilingual question generation will also be critical to expanding CEF's applicability across underrepresented languages."
- Why unresolved: Appendix A.2 shows that direct application in languages like Tigrinya or Dzongkha results in high disagreement rates (failure), forcing the use of a round-trip English pivot.
- What evidence would resolve it: Demonstration of stable ADR scores in direct low-resource evaluation using specialized multilingual prompts or fine-tuning.

## Limitations

- **Low-resource language performance:** Direct application in low-resource languages shows "order-of-magnitude increase in inconsistency," requiring fallback round-trip translation methods that add complexity and cost
- **Computational cost:** CEF requires 6-7× the tokens of traditional metrics (~$0.0049 vs $0.0007 per sample), limiting scalability for production deployment
- **Question diversity constraint:** YES-only constraint, while improving stability, may artificially constrain question diversity and miss nuanced semantic differences that mixed-answer questions could detect

## Confidence

- **High confidence:** Cross-examination mechanism effectiveness for detecting semantic errors in high-resource languages; judge model selection methodology; tripartite score interpretation framework
- **Medium confidence:** YES-only constraint superiority claims; N=10 as optimal question count; robustness across all three task types
- **Low confidence:** Performance guarantees in low-resource languages; generalizability to non-news domains; computational cost efficiency claims for production deployment

## Next Checks

1. **Domain Transfer Validation:** Apply CEF to clinical and technical domains with available human annotations; measure semantic error detection rates and compare against domain-specific metrics
2. **Judge Model Robustness Test:** Conduct ADR/ADS analysis across 5-10 judge models (including open and closed models) on held-out validation sets from each task type; verify DeepSeek-V3 remains optimal
3. **Question Diversity Stress Test:** Compare semantic error detection rates between YES-only and mixed-answer question generation on a balanced dataset of high-quality and error-containing samples; quantify trade-offs between stability and detection sensitivity