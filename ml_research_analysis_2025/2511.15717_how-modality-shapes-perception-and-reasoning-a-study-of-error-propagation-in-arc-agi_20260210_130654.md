---
ver: rpa2
title: 'How Modality Shapes Perception and Reasoning: A Study of Error Propagation
  in ARC-AGI'
arxiv_id: '2511.15717'
source_url: https://arxiv.org/abs/2511.15717
tags:
- correctly
- identified
- blue
- image
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer perception and reasoning
  on ARC-AGI puzzles are shaped by input modality. It hypothesizes that text serializations
  impose 1D bottlenecks while images preserve 2D structure but suffer from patch-based
  aliasing.
---

# How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI

## Quick Facts
- arXiv ID: 2511.15717
- Source URL: https://arxiv.org/abs/2511.15717
- Reference count: 40
- One-line primary result: Multi-modal ARC-AGI input improves reasoning accuracy by ~0.20 median similarity over text-only baselines.

## Executive Summary
This paper investigates how transformer perception and reasoning on ARC-AGI puzzles are shaped by input modality. It hypothesizes that text serializations impose 1D bottlenecks while images preserve 2D structure but suffer from patch-based aliasing. Using a weighted set-disagreement metric and a two-stage reasoning pipeline, the authors compare nine text and image encodings across five challenges. Structured text (JSON/ASCII) excels at precise coordinate identification for sparse features, while image modalities capture 2D shapes but are resolution-sensitive. Combining modalities enables cross-validation that improves execution accuracy—perception scores differ by ~8 points between best and runner-up encodings, and multi-modal inputs raise median execution similarity by ~0.20 over text-only.

## Method Summary
The study uses a two-stage instruction-following pipeline with Gemini 2.5 Pro. Perception: encode 5 ARC grids into 9 modalities (4 text: row-only, col-only, ASCII, JSON; 5 image: 14x14 through 768x768 pixels/cell), query the model for free-form descriptions, then extract and score features against ground truth using a weighted set-disagreement metric. Reasoning: generate transformation instructions from the perceived features and training examples, then execute them to produce output grids scored via cell-exact similarity (0.0–1.0). The methodology isolates perception errors from reasoning errors to trace error propagation.

## Key Results
- Text modalities (JSON/ASCII) provide precise coordinate identification for sparse features; image modalities capture 2D spatial relationships but suffer from resolution-dependent aliasing.
- Structured text encodings achieve ~8-point higher perception accuracy than image modalities on sparse-dot challenges, while images excel at 2D shape detection.
- Combining modalities (row col json image) improves median execution similarity by ~0.20 over text-only baselines through cross-validation error detection.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Validation for Error Detection
- **Claim:** Combining text and image modalities enables the model to cross-examine features, constructing a more accurate internal grid representation that improves execution accuracy by ~0.20 median similarity over text-only baselines.
- **Mechanism:** Text modalities (JSON/ASCII) provide precise coordinate identification on sparse features; image modalities capture 2D spatial relationships via global attention. When presented together, discrepancies between modality-specific perceptions serve as error signals that guide consensus toward more accurate representations, without altering the underlying model.
- **Core assumption:** The model can integrate evidence from multiple input channels to resolve ambiguities and correct prediction errors.
- **Evidence anchors:**
  - [abstract] "aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model"
  - [section 5.2] Figure 2 shows row col json image achieves median similarity ~0.69 vs ~0.51 average for text-only modalities
  - [corpus] "Think Visually, Reason Textually" (arXiv:2511.15703) explores similar vision-language synergy for ARC, but FMR=0.0 limits corroborative weight
- **Break condition:** When combined modalities share the same fundamental limitation (e.g., row-only + col-only are both 1D serializations), cross-validation fails; the paper shows row col performs worse than either format alone.

### Mechanism 2: Modality-Specific Perceptual Bottlenecks
- **Claim:** The choice of representation (text vs. image, serialization direction) systematically shapes which features are reliably perceived, with measurable ~8-point perception accuracy gaps between encodings.
- **Mechanism:** Text serializations flatten 2D grids into 1D token sequences, burdening attention with reconstructing spatial relations—row-wise formats favor horizontal patterns, column-wise favor vertical. Image modalities preserve 2D layout via patches but introduce aliasing when cell resolution misaligns with vision encoder patch boundaries (e.g., single pixels perceived as 2×2 shapes at low resolution).
- **Core assumption:** Transformers have finite attention capacity; information that requires more reconstruction effort will be less reliably perceived.
- **Evidence anchors:**
  - [section 5.1, Table 1] JSON achieves 100% on 13e47133 (sparse dots), row-only 60.61% (misidentifies Q11 vs P11); Image 16×16 achieves 45.73% due to aliasing vs 24×24-1205 at 85%
  - [section 3] "Text serializations flatten inherently 2D structure into 1D token sequences, burdening attention with reconstructing vertical and spatial relations"
  - [corpus] ARC-AGI-2 paper (arXiv:2505.11831, FMR=0.48) discusses representation challenges but does not directly test modality effects
- **Break condition:** When features are well-structured and clearly delimited (e.g., challenge 136b0064), all text modalities achieve 100%, suggesting the bottleneck is task-dependent rather than universal.

### Mechanism 3: Error Propagation from Perception to Reasoning
- **Claim:** Perception errors cascade through the reasoning pipeline—incorrect coordinate identification directly causes wrong transformation rules, which cannot be corrected downstream.
- **Mechanism:** The instruction-first paradigm separates perception (feature extraction) from reasoning (rule generation). When perception is inaccurate (e.g., misreading seed dot coordinates from aliased images), the working hypothesis incorporates wrong spatial information, producing transformation rules that generate systematically incorrect outputs. The paper aligns this with predictive coding principles where early-layer prediction errors propagate upward.
- **Core assumption:** The instruction generation process cannot compensate for missing or corrupted perceptual input; accurate perception is a prerequisite, not a factor that reasoning can overcome.
- **Evidence anchors:**
  - [section 6.1] "When image 16×16 achieved only 45.73% perception accuracy... these errors directly propagated to reasoning failures, yielding a median execution similarity of only 0.42"
  - [section 5.2] Training examples with irregular boundaries (complex divider lines) or color confusion (yellow dots on yellow background) achieve consistently lower scores
  - [corpus] No direct corpus corroboration for predictive coding analogy in ARC context
- **Break condition:** Not explicitly tested; the paper assumes this is a fundamental constraint, but does not rule out mechanisms where reasoning could flag and request re-perception.

## Foundational Learning

- **Concept: Patch-based tokenization in Vision Transformers**
  - **Why needed here:** Understanding that VLMs divide images into fixed-size patches (e.g., 16×16 pixels for Gemini) is essential for interpreting resolution-sensitivity results. When grid cells don't align cleanly with patch boundaries, aliasing occurs.
  - **Quick check question:** If a vision encoder uses 14×14 patches and your grid cells are 14×14 pixels, what happens when a coordinate label straddles two cells?

- **Concept: Instruction-first vs. end-to-end reasoning paradigms**
  - **Why needed here:** The paper's methodology isolates perception from reasoning by using a two-stage pipeline. Understanding this separation is prerequisite for interpreting why perception errors cannot be fixed by better reasoning.
  - **Quick check question:** In an instruction-first system, if the perception stage outputs "seed at Q11" when the true location is P11, can the instruction-generation stage recover?

- **Concept: Transformer attention and positional encoding**
  - **Why needed here:** The 1D processing bias claim rests on transformers using 1D positional encodings for text. Reconstructing vertical relationships from horizontally-serialized text requires attention to operate across distant tokens.
  - **Quick check question:** For a 30×30 grid serialized row-wise as text, how many tokens separate cell A1 from cell A30?

## Architecture Onboarding

- **Component map:** Grid encoding → Perception → Instruction generation → Instruction execution → Output comparison
- **Critical path:** Grid encoding → Perception → Instruction generation → Instruction execution → Output comparison. Perception accuracy directly gates downstream performance; multi-modal combinations improve by enabling cross-validation at the perception stage.
- **Design tradeoffs:**
  - JSON/ASCII: Highest coordinate precision for sparse features, but retains 1D bias; token-inefficient for dense grids.
  - Image modalities: Native 2D representation, but patch-size-dependent aliasing; requires model-specific resolution tuning (~1.5 patches/cell optimal for Gemini).
  - Multi-modal: ~0.20 median similarity gain over text-only, but increases input token count and may not help if modalities share limitations (e.g., row + col).
- **Failure signatures:**
  - Off-by-one coordinate errors in row-only/column-only → 1D processing bias (features perpendicular to serialization direction).
  - Single pixels described as 2×2 rectangles in images → low-resolution aliasing (multiple cells per patch).
  - Correct features but wrong symmetry detection → image modalities under-detecting global patterns that text captures.
  - Hallucinated instructions with row col combination → conflicting 1D biases without genuine complementarity.
- **First 3 experiments:**
  1. **Patch alignment calibration:** For your target VLM, run perception experiments on a 5-task subset across cell resolutions (e.g., 12, 14, 16, 18, 20, 24 pixels/cell). Identify the resolution that minimizes aliasing artifacts (look for: single-pixel hallucinations, coordinate boundary errors).
  2. **Modality ablation on your model:** Replicate Table 1 on your model architecture to determine whether the ~8-point perception gap and text/image complementary strengths hold. If your model has different patch sizes or tokenization, expect different optimal resolutions.
  3. **Cross-modal validation test:** Compare row col json image vs. json-only vs. image-only on execution similarity for a single challenge. Verify that median scores follow the paper's pattern (~0.69 > ~0.51 > ~0.42) before investing in multi-modal infrastructure.

## Open Questions the Paper Calls Out
- **Open Question 1:** Does multi-modal input improve perception accuracy in the same way it improves execution accuracy? The authors state perception experiments tested individual modalities in isolation, leaving this specific interaction unknown. Apply the weighted set-disagreement metric to model outputs when provided with combined modalities to see if feature identification scores exceed those in Table 1.
- **Open Question 2:** How robust are the observed modality biases across diverse ARC challenges? Section 6.4 acknowledges execution experiments focused on a single challenge, limiting generalizability. Run the two-stage reasoning pipeline across a broader distribution of challenges to verify the biases are universal.
- **Open Question 3:** Do transformers internally construct a unified spatial representation from multi-modal inputs? Section 6.4 proposes future work using mechanistic interpretability to trace how each modality is encoded. Analyze attention head activation patterns during the cross-validation process to identify specific layers responsible for aligning JSON coordinates with image visual features.

## Limitations
- The paper's execution experiments focused on a single ARC challenge (13e47133), limiting generalizability of the modality effects across diverse puzzle types.
- Perception experiments tested individual modalities in isolation rather than combined modalities, leaving open whether multi-modal inputs improve perception accuracy or only reasoning accuracy.
- The study does not provide mechanistic proof of how transformers reconcile conflicting information from different modalities, relying on observed performance improvements rather than internal representation analysis.

## Confidence
- **High:** The core finding that text modalities excel at coordinate identification while image modalities capture 2D spatial relationships is well-supported by the structured perception experiments and the ~8-point accuracy gap.
- **Medium:** The claim that cross-modal validation improves execution accuracy by ~0.20 is supported by execution experiments but lacks direct perception accuracy measurement for multi-modal inputs.
- **Low:** The paper's assertion that transformers internally construct a unified spatial representation from multi-modal inputs is based on performance observations rather than mechanistic analysis of internal representations.

## Next Checks
1. **Patch alignment calibration:** For your target VLM, run perception experiments on a 5-task subset across cell resolutions (e.g., 12, 14, 16, 18, 20, 24 pixels/cell). Identify the resolution that minimizes aliasing artifacts (look for: single-pixel hallucinations, coordinate boundary errors).
2. **Modality ablation on your model:** Replicate Table 1 on your model architecture to determine whether the ~8-point perception gap and text/image complementary strengths hold. If your model has different patch sizes or tokenization, expect different optimal resolutions.
3. **Cross-modal validation test:** Compare row col json image vs. json-only vs. image-only on execution similarity for a single challenge. Verify that median scores follow the paper's pattern (~0.69 > ~0.51 > ~0.42) before investing in multi-modal infrastructure.