---
ver: rpa2
title: Disentangled representations of microscopy images
arxiv_id: '2506.20649'
source_url: https://arxiv.org/abs/2506.20649
tags:
- dataset
- learning
- disentangled
- features
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing interpretability
  in deep learning models for microscopy image classification. The authors propose
  a Disentangled Representation Learning (DRL) framework that transfers a disentangled
  representation learned from synthetic data to real microscopy datasets, where factors
  of variation (FoVs) are unknown.
---

# Disentangled representations of microscopy images

## Quick Facts
- arXiv ID: 2506.20649
- Source URL: https://arxiv.org/abs/2506.20649
- Reference count: 40
- This paper addresses the challenge of enhancing interpretability in deep learning models for microscopy image classification.

## Executive Summary
This paper addresses the challenge of enhancing interpretability in deep learning models for microscopy image classification. The authors propose a Disentangled Representation Learning (DRL) framework that transfers a disentangled representation learned from synthetic data to real microscopy datasets, where factors of variation (FoVs) are unknown. The method uses a pretrained ViT16b model with DINO features as input to Ada-GV AE on a synthetic dataset (Texture-dSprite), then transfers and fine-tunes the model on real datasets (Lensless, WHOI15, Vacuoles, Sipakmed). Key results show that using deep features instead of raw images significantly improves classification accuracy (e.g., 94.62% vs 71.93% for Lensless with MLP), while maintaining or improving disentanglement scores. The approach achieves a good trade-off between accuracy and interpretability, with learned factors correlating well with known morphological features like scale, color, and shape. The method also enables anomaly detection in open-set scenarios by revealing differences in latent space between similar classes.

## Method Summary
The authors propose a Disentangled Representation Learning (DRL) framework that transfers a disentangled representation learned from synthetic data to real microscopy datasets. The approach uses a pretrained ViT16b model with DINO features as input to Ada-GV AE on a synthetic dataset (Texture-dSprite), then transfers and fine-tunes the model on real datasets (Lensless, WHOI15, Vacuoles, Sipakmed). The method addresses the challenge of enhancing interpretability in deep learning models for microscopy image classification by maintaining disentanglement scores while improving classification accuracy through the use of deep features instead of raw images.

## Key Results
- Using deep features instead of raw images significantly improves classification accuracy (e.g., 94.62% vs 71.93% for Lensless with MLP)
- The approach maintains or improves disentanglement scores while enhancing classification performance
- Learned factors correlate well with known morphological features like scale, color, and shape

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transferring a representation learned from a synthetic dataset with annotated factors preserves disentanglement on real target datasets better than learning from scratch.
- **Mechanism:** The model initializes its latent space structure on a "Source" dataset (Texture-dSprite) where factors of variation (FoVs) like shape and scale are explicitly controlled. When fine-tuned on real data, this architectural bias constrains the model to map new visual inputs onto these pre-existing semantic axes rather than entangling them randomly.
- **Core assumption:** The morphological factors present in the synthetic source (e.g., scale, texture, orientation) form a sufficient basis to describe the biological features in the real target data.
- **Evidence anchors:**
  - [abstract] "...transfers a disentangled representation learned from synthetic data to real microscopy datasets..."
  - [section II.A] "...transfer a disentangled model – trained with weak supervision on a Source dataset where annotation of FoVs is available – to a Target real dataset..."
  - [corpus] Related work (DRL4Real) supports the general viability of DRL in realistic settings but highlights the difficulty of transfer, making the specific use of synthetic data a key differentiator.
- **Break condition:** If the real target images contain biological factors not present in the synthetic dataset (e.g., specific sub-cellular textures), the model may fail to disentangle them or force them into incorrect latent dimensions.

### Mechanism 2
- **Claim:** Using deep pretrained features (DINO-ViT) as input to the VAE significantly improves classification accuracy and disentanglement robustness compared to raw pixels.
- **Mechanism:** Raw pixels force the VAE to solve a difficult reconstruction task. By projecting images into a high-dimensional feature space ($\Phi$) via a ViT16b model pretrained on ImageNet, the input already encodes semantic structure. This allows the VAE to focus on disentangling high-level concepts rather than low-level pixel noise.
- **Core assumption:** Features learned on natural images (ImageNet) transfer effectively to the microscopy domain.
- **Evidence anchors:**
  - [abstract] "Key results show that using deep features instead of raw images significantly improves classification accuracy (e.g., 94.62% vs 71.93%...)..."
  - [section III.D] "...we observe that the latter [deep features] allows for a more robust transfer... further improving the performances of a disentangled representation..."
  - [corpus] Corpus evidence regarding transfer learning generally supports feature robustness, though specific microscopy transfer success is attributed here to the DINO self-supervised approach.
- **Break condition:** If the specific microscopy modality (e.g., lensless imaging) produces textures fundamentally alien to the ImageNet pretraining, the features might lack the necessary discriminative power.

### Mechanism 3
- **Claim:** Disentangled latent dimensions align with known morphological features, enabling interpretability via correlation.
- **Mechanism:** Because the latent space is constrained to be disentangled, individual dimensions ($z$) are forced to encode single independent factors. The method verifies this by computing the Pearson correlation between specific latent dimensions and handcrafted features (e.g., solidity, scale). A high correlation implies the model has "learned" that morphological concept.
- **Core assumption:** The "Ground Truth" handcrafted features (like solidity) are accurate proxies for the visual concepts we want the model to learn.
- **Evidence anchors:**
  - [section III.D] "Handcrafted and latent scale features... have a high correlation (-0.91)..."
  - [section I] "...we exploit the availability in the literature of several handcrafted features... that we use as a reference..."
  - [corpus] Neighbor paper "Are Representation Disentanglement and Interpretability Linked?" questions this link in recommendation systems; this paper argues it holds for microscopy *if* validated against handcrafted features.
- **Break condition:** If the latent dimension activates on spurious correlations (e.g., background noise) rather than the biological structure, interpretability claims may be misleading.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) & $\beta$-VAE**
  - **Why needed here:** The core architecture (Ada-GV AE / $\beta$-VAE) relies on the VAE framework to compress data into a latent space. Understanding the trade-off between reconstruction loss and the KL-divergence term (controlled by $\beta$) is essential to grasp how the model forces disentanglement.
  - **Quick check question:** How does increasing the weight of the KL-divergence term ($\beta$) typically affect the latent space structure?

- **Concept: Self-Supervised Learning (DINO)**
  - **Why needed here:** The input pipeline depends on ViT-DINO features. You must understand that DINO learns features without labels by discriminating between augmented views, which results in features that are particularly robust for capturing shape and texture.
  - **Quick check question:** Why would features trained on self-supervised ImageNet be preferred over standard supervised features for this specific transfer task?

- **Concept: Transfer Learning & Fine-tuning**
  - **Why needed here:** The methodology is a two-step transfer process (Source $\to$ Target). You need to understand that "fine-tuning" updates the pre-trained weights on the new domain data, and that without this step, the synthetic bias might hurt performance on real data.
  - **Quick check question:** What happens to the "disentanglement" properties if we fine-tune on a target dataset that violates the independence assumptions of the source dataset?

## Architecture Onboarding

- **Component map:**
  1.  **Frozen Backbone:** ViT16b (pretrained with DINO on ImageNet1K). Input: Image ($224 \times 224$). Output: Feature Vector $\Phi$ (768-dim).
  2.  **Source Trainer (Ada-GV AE):** VAE trained on Texture-dSprite (Synthetic). Input: Pairs of $\Phi$ differing by $k=1$ factor.
  3.  **Target Adapter ($\beta$-VAE):** The Source model weights are transferred and fine-tuned unsupervised on real microscopy features $\Phi$.
  4.  **Downstream Heads:** Simple Gradient Boosted Trees (GBT) or MLP for classification; Linear probes for correlation with handcrafted features.

- **Critical path:**
  1.  Generate/Access **Texture-dSprite** (Source).
  2.  Train **Ada-GV AE** on Source features until disentanglement scores (MIG/DCI) stabilize.
  3.  Load Source weights into Target model.
  4.  **Fine-tune** on Target microscopy dataset (unsupervised).
  5.  Evaluate latent dimensions against handcrafted features for interpretability.

- **Design tradeoffs:**
  - **Latent Dimensionality ($d=10$):** Paper fixes this. Increasing $d$ might capture more complex biology but dilute the clean one-to-one mapping required for easy interpretability.
  - **Input Modality ($\Phi$ vs RGB):** Deep features boost accuracy/robustness but lose pixel-level reconstruction capability (the VAE reconstructs features, not pixels).

- **Failure signatures:**
  - **Post-Finetuning Collapse:** Disentanglement scores (e.g., OMES) drop significantly (observed in the RGB baseline, less so in the DINO version).
  - **Latent Inactivity:** Dimensions with standard deviation $< 0.05$ (the paper filters these out).
  - **Feature Misalignment:** Low correlation between the most important latent dimensions and known handcrafted features (as seen partially in the Sipakmed dataset).

- **First 3 experiments:**
  1.  **Sanity Check (Source):** Train Ada-GV AE on Texture-dSprite. Verify modularity using DCI/MIG scores to ensure the model *can* disentangle the known factors.
  2.  **Ablation (Input):** Compare Target performance using Raw Pixels vs. DINO-Features vs. ResNet-Features (Table IV in paper). Confirm that DINO-Features maintain disentanglement post-transfer.
  3.  **Interpretability Validation:** Plot the correlation matrix between the learned latent dimensions and the available handcrafted features (Scale, Solidity, Color) for the Lensless dataset to confirm semantic alignment.

## Open Questions the Paper Calls Out
The paper identifies several future research directions:
- Exploring more complex generative architectures like Diffusion Models instead of VAE-based methods
- Generating domain-specific synthetic source datasets with targeted FoVs
- Developing methods to directly evaluate disentanglement quality on real microscopy datasets lacking FoV annotations
- Understanding the specific mechanisms that enable ViT-DINO features to preserve disentanglement better than raw RGB images during transfer learning

## Limitations
- The interpretability claims depend on the completeness and accuracy of handcrafted features as reference
- Exact architecture of the Ada-GV AE encoder/decoder is not specified, making exact replication challenging
- The method's performance on microscopy datasets with very different characteristics from tested domains is not thoroughly explored

## Confidence
**High Confidence** - The core mechanism of transferring disentangled representations from synthetic to real data is well-supported by the empirical results showing consistent improvements in classification accuracy (e.g., 94.62% vs 71.93% for Lensless). The ablation study comparing DINO features to raw pixels provides strong evidence for the effectiveness of deep features.

**Medium Confidence** - The interpretability claims, while validated through correlation with handcrafted features, depend on the quality and completeness of these handcrafted features. The correlation approach assumes a linear relationship between latent dimensions and morphological features, which may not capture more complex relationships.

**Low Confidence** - The generalization of results to microscopy datasets with very different characteristics from the tested domains (e.g., single-cell vs multi-cell images, brightfield vs fluorescence) is not thoroughly explored. The WHOI15 dataset's under-performance suggests limitations with certain data types.

## Next Checks
1. **Architectural Replication**: Implement the Ada-GV AE with specific encoder/decoder architectures (number of layers, channel sizes, activation functions) and verify that the disentanglement scores (MIG/DCI) on Texture-dSprite match reported values before transfer.

2. **Feature Robustness Test**: Conduct an ablation study using features from different pretrained models (ResNet, supervised ViT) to confirm that DINO features provide unique advantages for both accuracy and disentanglement preservation during transfer.

3. **Interpretability Stress Test**: Systematically vary the completeness of handcrafted features (use subsets of available features) to determine how sensitive the interpretability claims are to missing or imperfect reference features, and whether the model can still produce meaningful latent representations.