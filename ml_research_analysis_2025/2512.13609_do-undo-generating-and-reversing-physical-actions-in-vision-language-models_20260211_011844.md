---
ver: rpa2
title: 'Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models'
arxiv_id: '2512.13609'
source_url: https://arxiv.org/abs/2512.13609
tags:
- image
- action
- reverse
- do-undo
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Do-Undo task and benchmark to address
  a critical gap in vision-language models (VLMs): understanding and generating physically
  plausible scene transformations driven by real-world actions. Unlike prior work
  focused on object-level edits, Do-Undo requires models to simulate the outcome of
  a physical action and then accurately reverse it, reflecting true cause-and-effect
  in the visual world.'
---

# Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models
## Quick Facts
- arXiv ID: 2512.13609
- Source URL: https://arxiv.org/abs/2512.13609
- Reference count: 40
- Introduces Do-Undo task for simulating and reversing physical actions in vision-language models

## Executive Summary
This paper introduces the Do-Undo task and benchmark to address a critical gap in vision-language models (VLMs): understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. The authors curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

## Method Summary
The authors develop a comprehensive approach to the Do-Undo task, starting with a large-scale dataset curation of reversible action sequences from real-world videos. They implement a consistency training objective that enforces bidirectional transformation between original and action-modified scenes. The training pipeline involves joint optimization of action simulation and reversal capabilities, ensuring that models can both generate plausible action outcomes and accurately undo them. This dual objective is designed to strengthen physical reasoning and causality understanding within VLMs.

## Key Results
- Current VLMs perform poorly on physical reversibility, highlighting a fundamental limitation in action grounding
- Consistency training significantly improves performance on both action simulation and reversal tasks
- The curated Do-Undo dataset reveals substantial gaps in physical reasoning capabilities compared to object-level editing tasks

## Why This Works (Mechanism)
The Do-Undo framework works by explicitly training models to learn bidirectional transformations between scenes and their action-modified counterparts. By enforcing consistency through joint optimization of forward and reverse transformations, models develop stronger causal representations of physical interactions. This approach contrasts with unidirectional generative models that lack explicit mechanisms for reversibility.

## Foundational Learning
- **Physical causality**: Understanding how actions affect objects and scenes is fundamental for real-world reasoning
  - Why needed: Real-world AI systems must predict and reason about physical consequences
  - Quick check: Can the model predict what happens when a glass is pushed off a table?
- **Reversibility constraints**: Not all physical transformations can be undone; identifying reversible vs irreversible actions is crucial
  - Why needed: Real-world scenarios involve both reversible and irreversible changes
  - Quick check: Can the model distinguish between pushing a cup (reversible) and breaking it (irreversible)?
- **Multimodal alignment**: Grounding language descriptions to visual transformations requires tight integration of vision and language modalities
  - Why needed: Natural language commands must be accurately translated to visual changes
  - Quick check: Does the model correctly interpret "put the book back on the shelf" from an image?

## Architecture Onboarding
Component map: Input image + action description -> VLM encoder -> Action simulator -> Modified image, then -> Reversal module -> Original image reconstruction
Critical path: VLM feature extraction → Action-conditioned generation → Consistency enforcement → Bidirectional reconstruction
Design tradeoffs: Reversibility constraint vs. generation quality; computational cost of dual transformations vs. single forward pass
Failure signatures: Incorrect object identification, implausible physical interactions, loss of detail during reversal
First experiments:
1. Forward action simulation only (without reversal)
2. Reverse transformation from ground-truth modified images
3. End-to-end Do-Undo with simple action descriptions

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies on curated reversible action sequences, potentially limiting ecological validity
- Focus on short-horizon actions leaves scalability to longer, multi-step reasoning tasks unexplored
- Uncertainty about generalization to novel objects, environments, or action types not seen during training

## Confidence
- High confidence: Identification of reversibility as a missing capability and demonstration of current model limitations
- Medium confidence: Effectiveness of consistency training strategy and dataset quality
- Medium confidence: Specific architectural choices and hyperparameter optimality

## Next Checks
1. Test model performance on irreversible action sequences to assess robustness beyond controlled benchmark setting
2. Evaluate zero-shot generalization to novel object categories and environmental contexts not present in training data
3. Benchmark performance on longer-horizon action sequences requiring multi-step reasoning and planning