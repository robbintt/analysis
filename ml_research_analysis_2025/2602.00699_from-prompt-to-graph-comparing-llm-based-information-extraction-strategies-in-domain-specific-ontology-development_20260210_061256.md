---
ver: rpa2
title: 'From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies
  in Domain-Specific Ontology Development'
arxiv_id: '2602.00699'
source_url: https://arxiv.org/abs/2602.00699
tags:
- extraction
- terms
- relation
- training
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares three LLM-based methods for extracting terms
  and relations to support ontology development in the casting domain. A pre-trained
  model, few-shot in-context learning, and a fine-tuned model were evaluated using
  limited data.
---

# From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development

## Quick Facts
- arXiv ID: 2602.00699
- Source URL: https://arxiv.org/abs/2602.00699
- Reference count: 32
- Key outcome: Fine-tuned LLM achieved 93.8% precision and 87.6% recall in term extraction, and 86.7% precision and 73.4% recall in relation extraction for casting domain ontology construction

## Executive Summary
This study systematically evaluates three LLM-based strategies for extracting terms and relations to support ontology development in the casting manufacturing domain. The research compares a pre-trained model, few-shot in-context learning (ICL), and supervised fine-tuning approaches using a limited dataset of 173 training texts. The fine-tuned model demonstrated superior performance with 93.8% precision and 87.6% recall for term extraction, and 86.7% precision and 73.4% recall for relation extraction. The resulting ontology includes 273 concepts and 620 validated relations, demonstrating the feasibility of using LLMs for semi-automated, domain-specific ontology construction in manufacturing contexts.

## Method Summary
The study implemented three distinct LLM-based extraction strategies on casting domain texts. First, a pre-trained GPT-4.1-mini model was used with basic Chain-of-Thought prompting. Second, an ICL approach retrieved semantically similar examples using UAE-Large-V1 embeddings to construct prompts with k=16 examples. Third, supervised fine-tuning was performed on 173 annotated texts with specific hyperparameters (Epochs=3, Batch size=1, LR Multiplier=2.0 for terms, 1.0 for relations). All methods employed a two-step pipeline: term extraction followed by relation and synonym extraction. The evaluation used 30 hold-out test paragraphs, with terms labeled using distinct symbol pairs (e.g., @@term##) and relations formatted as triples.

## Key Results
- Fine-tuned model achieved highest performance: 93.8% precision and 87.6% recall in term extraction
- Fine-tuned model outperformed others in relation extraction: 86.7% precision and 73.4% recall
- Fine-tuned model demonstrated superior synonym recognition compared to ICL and pre-trained approaches
- Resulting ontology contained 273 concepts and 620 validated relations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity-based example retrieval improves ICL performance for domain-specific extraction
- Mechanism: Uses UAE-Large-V1 text embedding model to calculate cosine similarity between test input and training corpus, retrieving top k=16 most semantically similar examples to populate prompts
- Core assumption: Semantic proximity in vector space correlates with structural similarity in extraction tasks
- Evidence anchors: [section 2.3.1] discusses adapting UAE-Large-V1 for semantic similarity retrieval; mentions neighbors like *Ontology-Guided, Hybrid Prompt Learning*
- Break condition: Fails when training dataset is too small to contain semantically similar neighbors for edge-case inputs

### Mechanism 2
- Claim: Supervised fine-tuning enforces schema consistency and improves recall compared to ICL in specialized domains
- Mechanism: Updates model weights on labeled dataset (173 texts) to internalize specific output format and relation definitions
- Core assumption: Small dataset (approx. 200 samples) is sufficient to shift model priors without catastrophic forgetting
- Evidence anchors: [section 3.2] shows fine-tuning achieved 93.8% precision and 87.6% recall; [abstract] states fine-tuned model outperformed others in synonym recognition
- Break condition: Degrades if training data contains labeling errors or target domain shifts significantly

### Mechanism 3
- Claim: Decomposing ontology construction into sequential sub-tasks reduces complexity
- Mechanism: Separates term extraction from relation extraction, passing identified terms back to LLM for relation identification
- Core assumption: High accuracy in term extraction step is easier to achieve than simultaneous extraction
- Evidence anchors: [section 2.3] describes two-step process used by ICL and fine-tuning methods; *LLM-based Zero-shot Triple Extraction* supports general feasibility
- Break condition: Pipeline fails if term extraction step has low recall

## Foundational Learning

- Concept: **Named Entity Recognition (NER) with Structured Output**
  - Why needed here: Understanding how to format extraction targets using specific delimiters (e.g., @@, ##) to force LLM to output structured data
  - Quick check question: Can you configure an LLM prompt to wrap every "casting process" in XML tags like `<process> ... </process>`?

- Concept: **In-Context Learning (Few-Shot)**
  - Why needed here: Understanding how providing demonstrations (Input/Output pairs) inside prompt influences model's next-token prediction without weight updates
  - Quick check question: If you have 100 training examples, how do you select which 5 to include in a prompt for new input? (Hint: paper uses semantic similarity)

- Concept: **Knowledge Graph Construction (Triples)**
  - Why needed here: Understanding (Subject, Object, Relation) triple format and how it maps to nodes and edges in graph database
  - Quick check question: How would you represent "Aluminum is processed by die casting" as a triple?

## Architecture Onboarding

- Component map: Raw text -> RAG Distillation -> Q&A Pairs -> Human Annotation (Label Studio) -> Retrieval Layer (UAE-Large-V1 -> Cosine Similarity) -> Extraction Layer (LLM via three strategies) -> Storage Layer (Neo4j Graph Database)

- Critical path: Annotation and Distillation phase is the bottleneck, requiring domain expert evaluation to validate RAG-distilled data before training or prompting

- Design tradeoffs:
  - ICL vs. Fine-tuning: ICL is faster to implement but suffers from relation label inconsistency; fine-tuning offers high precision and schema adherence but requires compute investment and curated training data
  - 2-Step Pipeline: Increases latency (two LLM calls per document) but significantly improves accuracy over single-pass extraction

- Failure signatures:
  - Hallucinated Terms: Pre-trained method extracted irrelevant terms (e.g., "rubber") due to lack of domain grounding
  - Label Drift: ICL method generated varying relation names ("processed by" vs "produced by") for same semantic relationship
  - Synonym Blindness: ICL failed to recognize synonyms effectively because they appeared sparsely in retrieved examples

- First 3 experiments:
  1. Baseline Test: Run raw pre-trained model with basic Chain-of-Thought prompt to establish precision floor (expect ~77% precision, low recall)
  2. ICL Optimization: Implement similarity-based retriever, test k=5 vs k=16 to measure impact of context length on extraction accuracy
  3. Fine-tuning Validation: Fine-tune model on 173 annotated samples and compare "relation consistency" and "synonym recognition" metrics against ICL results

## Open Questions the Paper Calls Out

- **Question**: How does comparative performance of pre-trained, ICL, and fine-tuned LLMs change when applied to larger, more diverse industrial corpora?
  - Basis in paper: [explicit] Conclusion states future work should evaluate methods on larger and more diverse corpora
  - Why unresolved: Current study relied on small dataset (173 training texts, 30 testing texts) to simulate limited data scenarios
  - What evidence would resolve it: Replication using significantly larger text corpus (thousands of documents) from varied manufacturing sub-domains

- **Question**: Can specific prompt engineering or constrained decoding strategies effectively mitigate relation label inconsistency observed in ICL method?
  - Basis in paper: [inferred] Results section notes ICL suffered from "Relation label inconsistency" outputting varying names for same relation
  - Why unresolved: Authors concluded ICL learns fewer patterns from labeled data but did not test interventions to enforce label consistency
  - What evidence would resolve it: Follow-up study measuring ICL performance with strictly constrained output schema or post-processing normalization

- **Question**: To what extent does utilizing semantic similarity metrics rather than exact string matching increase effective precision of fine-tuned relation extraction model?
  - Basis in paper: [inferred] Discussion notes many "false positives" were "factually acceptable based on expert opinion," suggesting reported precision might be underestimate
  - Why unresolved: Evaluation relied on exact matching against ground truth triples, penalizing model for semantically correct outputs
  - What evidence would resolve it: Evaluation using semantic similarity scores (e.g., BERTScore) against ground truth compared alongside exact-match precision

## Limitations
- Narrow domain focus limits generalizability to other industrial contexts
- Small training dataset (173 samples) raises concerns about model robustness across diverse inputs
- Manual annotation and distillation process required significant effort that may not scale well

## Confidence

- **High Confidence**: Comparative performance results between three extraction strategies, particularly fine-tuned model's superiority in precision and recall metrics
- **Medium Confidence**: Generalizability of two-step pipeline approach to other domains, given casting-specific nature of data
- **Low Confidence**: Scalability of manual annotation and distillation process for larger ontologies or broader domains

## Next Checks
1. Cross-Domain Transfer Test: Apply fine-tuned casting model to different manufacturing domain (e.g., automotive or aerospace) to evaluate zero-shot performance and identify domain-specific failure patterns
2. Scaling Experiment: Measure annotation effort and extraction accuracy as training dataset scales from 173 to 500+ samples to determine diminishing returns point for fine-tuning
3. Real-Time Performance Analysis: Deploy pipeline on streaming manufacturing data to assess latency and accuracy degradation under production conditions, particularly comparing two-step approach versus potential single-pass alternatives