---
ver: rpa2
title: What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court
  Opinions
arxiv_id: '2511.05320'
source_url: https://arxiv.org/abs/2511.05320
tags:
- factual
- court
- cases
- legal
- decisions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study tackled extracting detailed criminal behavior descriptions
  from Slovak court verdicts, which are more informative than administrative datasets.
  Two main methods were used: advanced regular expressions that detect typographically
  spaced phrases marking the start and end of these descriptions, and large language
  models (LLMs) prompted to extract the same span.'
---

# What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions

## Quick Facts
- arXiv ID: 2511.05320
- Source URL: https://arxiv.org/abs/2511.05320
- Reference count: 40
- Primary result: Automated extraction achieves 97-99.5% coverage and 89.5-92% quality on Slovak criminal verdicts

## Executive Summary
This study addresses the challenge of extracting detailed criminal behavior descriptions from Slovak court verdicts, which contain richer information than administrative datasets. The authors developed and compared multiple methods including advanced regular expressions and large language models (LLMs) to automatically identify the specific text spans containing court-established facts. The research demonstrates that combining advanced regex with LLM prompting achieves 99.5% coverage of relevant verdicts with 92% quality match to human annotations. The approach is particularly valuable for criminology and criminal justice research where administrative data lacks the granularity needed for detailed behavioral analysis.

## Method Summary
The study employed two main extraction approaches: advanced regular expressions and LLM prompting. The regex method was enhanced beyond simple phrase matching to detect typographically spaced phrases marking the beginning and end of factual descriptions in court verdicts. The LLM approach used a carefully crafted prompt instructing the model to identify and extract the specific text span containing court-established facts about criminal behavior. Both methods were evaluated on a test set of 200 Slovak criminal court decisions, with performance measured by coverage (percentage of verdicts containing extractable descriptions) and quality (agreement with human annotations). A combined approach using both methods was also tested to maximize coverage.

## Key Results
- Advanced regex achieved 97% coverage compared to 40.5% for baseline regex
- LLM achieved 98.75% coverage with 91.75% quality match to human annotations
- Combined approach reached 99.5% coverage and 92% quality
- On challenging cases with non-standard formatting, LLM maintained 84% success rate

## Why This Works (Mechanism)
The extraction methods succeed because Slovak court verdicts follow relatively consistent typographic conventions, with court-established facts typically appearing in specific, identifiable text spans. The advanced regex captures these patterns by recognizing typographic spacing cues, while the LLM leverages its understanding of legal document structure to identify the relevant sections. The combination approach provides redundancy, ensuring high coverage even when one method encounters edge cases. The LLM's ability to handle non-standard formatting in challenging cases demonstrates its flexibility compared to rule-based approaches.

## Foundational Learning
- Slovak criminal court verdict structure - Why needed: Understanding the document format is essential for developing effective extraction methods. Quick check: Review sample verdicts to identify consistent structural patterns.
- Regular expression optimization for legal text - Why needed: Standard regex approaches fail on 60% of cases; advanced patterns are required. Quick check: Compare baseline vs. advanced regex performance on a small sample set.
- Prompt engineering for legal document extraction - Why needed: LLM performance depends heavily on prompt specificity and clarity. Quick check: Test multiple prompt variations on a validation set.
- Quality evaluation metrics for text extraction - Why needed: Coverage alone is insufficient; quality assessment ensures extracted content is accurate. Quick check: Implement human evaluation protocol for a sample of extracted spans.
- Cross-jurisdictional document variation - Why needed: Methods may not generalize beyond Slovak court system. Quick check: Compare Slovak verdicts with documents from other jurisdictions.

## Architecture Onboarding

**Component map:** Text preprocessing -> Regex extraction -> LLM extraction -> Quality validation -> Combined output

**Critical path:** Document input → Text normalization → Pattern detection (regex/LLM) → Span extraction → Quality assessment

**Design tradeoffs:** The study chose a few-shot LLM approach over fine-tuning to preserve a clean test set, sacrificing potential accuracy gains for methodological purity. The combined approach adds computational overhead but maximizes coverage.

**Failure signatures:** Baseline regex fails on 59.5% of cases due to lack of typographic cues; LLM fails on non-standard formatting; both methods struggle with unexpected grammatical structures or unusual document layouts.

**First 3 experiments:** 1) Test baseline regex on 50 random verdicts to establish performance floor. 2) Apply advanced regex to same set to measure improvement. 3) Run LLM extraction on cases where regex fails to assess complementary value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the extraction methods be effectively transferred to other legal jurisdictions with different idioms and citation habits?
- Basis in paper: [explicit] The authors state that transferring methods "remains a challenge" and that they "leave systematic evaluation on foreign corpora for future work."
- Why unresolved: The regex inventory is specific to Slovak idioms, and the LLM sample was too narrow to draw general conclusions on cross-jurisdiction generalization.
- What evidence would resolve it: Systematic evaluation of the extraction pipeline on court decisions from additional countries (e.g., Czech Republic, Estonia).

### Open Question 2
- Question: Can prompt refinement address failure modes in "challenging" cases where grammatical structures were unanticipated?
- Basis in paper: [explicit] The authors note that failures in the difficult dataset occurred because formats were not anticipated by the prompt, highlighting "the need to update the prompt, generalizing the description of a factual sentence."
- Why unresolved: The current prompt relies on specific instructions that miss edge cases with non-standard formatting.
- What evidence would resolve it: Evaluation of an updated prompt on the 200-judgment "challenging" dataset to see if the 84% success rate improves.

### Open Question 3
- Question: Would a supervised model trained on the annotated data outperform the current few-shot LLM approach?
- Basis in paper: [explicit] The authors mention that "A small experiment with training a model on the annotated test set could be interesting," though they avoided it to preserve a clean held-out set.
- Why unresolved: The trade-off between the high cost of large-scale annotation and the potential accuracy gains of a fine-tuned model was not tested.
- What evidence would resolve it: A comparison of a fine-tuned model's performance against the few-shot LLM baseline on a held-out test set.

## Limitations
- Results are specific to Slovak criminal court verdicts with consistent typographic conventions
- Limited evaluation scope (200 documents) may not capture full variance in court decision formats
- LLM performance on non-Slovak legal documents remains untested
- High annotation costs prevented training a supervised model that might outperform few-shot approaches

## Confidence
- **High**: Coverage and quality metrics on the tested dataset (97-99.5% coverage, 89.5-92% quality)
- **Medium**: Real-world applicability across different legal systems and languages
- **Medium**: LLM robustness on non-standard formatting cases (84% success rate)

## Next Checks
1. Test the regex and LLM approaches on criminal court opinions from at least two other jurisdictions (e.g., US state courts, German criminal courts) to assess cross-system generalizability
2. Evaluate performance degradation when the typographic conventions used in Slovak verdicts are systematically altered or removed
3. Conduct inter-annotator agreement studies on the LLM-generated extractions to quantify human-level quality benchmarks beyond the single annotator comparison reported