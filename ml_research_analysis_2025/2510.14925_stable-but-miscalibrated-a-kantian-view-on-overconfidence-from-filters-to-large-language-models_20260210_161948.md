---
ver: rpa2
title: 'Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to
  Large Language Models'
arxiv_id: '2510.14925'
source_url: https://arxiv.org/abs/2510.14925
tags:
- sensitivity
- kantian
- internal
- instability
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a control-theoretic approach to AI hallucination,
  reinterpreting Kant's philosophy as a theory of feedback stability. The core method
  introduces H-Risk, a composite index measuring internal instability and conditioning
  in state-space models, and a simple output-level proxy for large language models
  based on confidence fluctuations and overconfident errors.
---

# Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models

## Quick Facts
- arXiv ID: 2510.14925
- Source URL: https://arxiv.org/abs/2510.14925
- Authors: Akira Okutomi
- Reference count: 40
- Primary result: Confidently wrong LLM answers are locally stable representations ("stable miscalibration") that resist correction through output-only heuristics.

## Executive Summary
This paper proposes a control-theoretic framework for understanding LLM hallucination by reinterpreting Kant's philosophy as a theory of feedback stability. The core contribution is H-Risk, a composite index measuring internal instability and conditioning in state-space models, with an output-level proxy for LLMs. Through simulations, the paper demonstrates that higher H-Risk predicts overconfident errors and degraded closed-loop behavior. A key finding is that confidently wrong answers are not unstable—they exhibit "stable miscalibration" where hallucinations are robust but misaligned. The paper introduces a Kantian-inspired policy allowing "cannot judge" responses that reduces policy-aware squared loss in high-stakes domains compared to overconfident baselines.

## Method Summary
The paper defines H-Risk as a composite index aggregating spectral margin, conditioning, temporal sensitivity, and innovation amplification in closed-loop systems. For LLMs, it implements a computationally tractable proxy based on confidence fluctuations and overconfident error rates. Internal stability is probed via layer-wise sensitivity to small input perturbations (σ = 0.01). The LLM experiment uses three 7-8B instruction-tuned models on binary factual questions, comparing overconfident, cautious, and Kantian prompt policies. LTI simulations validate the theoretical framework using synthetic state-space models. The study is reproducible via the `ToppyMicroServices/202510_report_AI` repository with frozen experimental data.

## Key Results
- Higher H-Risk predicts overconfident errors and degraded closed-loop behavior in both LTI systems and LLM proxies
- Confidently wrong answers show no internal instability gap—they are at least as locally stable as correct answers
- Qwen-2.5 achieves low effective sensitivity via high signal-to-noise regime, not small weights
- Kantian-inspired prompts reducing "cannot judge" responses improve policy-aware squared loss in high-stakes domains

## Why This Works (Mechanism)

### Mechanism 1
Higher H-Risk predicts overconfident errors and degraded closed-loop behavior through aggregation of four structural descriptors: spectral margin (mR), conditioning (cR), temporal sensitivity (sR), and innovation amplification (aR). When the closed-loop operator Φ = A − KH approaches instability or has poor conditioning, small perturbations produce large transient responses manifesting as confident errors. Core assumption: epistemic instability in inference can be approximated by linear-Gaussian stability properties around an operating point.

### Mechanism 2
"Stable miscalibration" occurs because confidently wrong answers are locally stable representations rather than transient instability artifacts. Layer-wise sensitivity S_ℓ = ‖Δh_ℓ‖ / ‖ϵ‖ shows overconfidently wrong items have comparable sensitivity to correct items, indicating robust but misaligned attractors. Core assumption: local sensitivity to small input perturbations proxies internal epistemic stability in transformers.

### Mechanism 3
Qwen-2.5's low effective sensitivity stems from a high signal-to-noise, low effective signal temperature regime. Despite larger spectral norms (‖W‖₂), Qwen-2.5 maintains much larger activation norms (‖x‖₂). Under RMSNorm, fixed perturbation ϵ has relative impact ∝ ϵ / RMS(x), which is compressed when ‖x‖₂ is large, creating "inertial" representations resistant to contextual shifts.

## Foundational Learning

- **Closed-loop stability in state-space models (spectral radius ρ(Φ), conditioning, non-normality)**: H-Risk builds on the premise that formally stable systems can still be practically fragile due to poor conditioning; understanding this gap is essential to interpret H-Risk. Quick check: If ρ(Φ) = 0.95 and κ(Φ) = 10⁴, is the system "stable" in practice?

- **Layer-wise local sensitivity (‖Δh‖ / ‖ϵ‖) and its relationship to Jacobian conditioning**: The paper uses this as an internal stability proxy; correctly interpreting "no instability gap" requires understanding what sensitivity does and does not measure. Quick check: Does low local sensitivity guarantee outputs are correct, or only that they are locally robust?

- **Expected Calibration Error (ECE) and Brier score as calibration metrics**: The LLM study uses policy-aware squared loss and Brier/ECE diagnostics; distinguishing between output calibration and internal stability is central to the argument. Quick check: If a model has low ECE but high H-Risk, which metric should you trust for deployment decisions in high-stakes domains?

## Architecture Onboarding

- **Component map**: H-Risk core (Abstract definition → LTI instantiation → LLM proxy) -> LLM proxy pipeline (Domain-wise H_proxy(d) built from confidence fluctuations and overconfident error rates) -> Internal stability probe (Layer-wise sensitivity S_ℓ via small input perturbations) -> Spectral/activation profiling (Measures ‖W‖₂ and ‖x‖₂ across layers)

- **Critical path**: 1. Define H-Risk components for your system (LTI: direct; LLM: Jacobian proxy or output-level approximation). 2. Validate monotonicity: confirm increased instability correlates with miscalibration in held-out data. 3. Deploy domain-wise proxy to identify high-risk regimes; use Kantian-style prompts or critique loops selectively. 4. Monitor internal sensitivity trends over training or prompting changes.

- **Design tradeoffs**: Jacobian-based HRisk_LLM vs output-only proxy (Jacobians are theoretically cleaner but computationally prohibitive; proxies are cheap but coarse). Global damping (Kantian prompt) vs selective intervention (Kantian prompts reduce sensitivity globally, not selectively for wrong answers). Noise scale σ for sensitivity (Small σ preserves linear regime but may miss nonlinear instability; large σ enters nonlinear regime but conflates multiple effects).

- **Failure signatures**: High H-Risk but good surface calibration (Hidden fragility not visible in output metrics; stress-test with distribution shift). Low sensitivity but persistent confident errors (Classic stable miscalibration; output-only heuristics unlikely to help). Sensitivity reductions without accuracy gains (Kantian prompt may "dampen" without correcting attractors; pair with external verification in safety-critical settings).

- **First 3 experiments**: 1. Replicate layer-wise sensitivity analysis on your target model (standard vs Kantian prompt; stratify by correct/wrong) to confirm or refute "no instability gap" pattern. 2. Compute domain-wise H_proxy(d) on your task data; correlate with per-domain ∆SE_policy under different policies to validate risk targeting. 3. Run noise-scale sweep (σ ∈ {0.005, 0.01, 0.025, 0.05}) on your model's sensitivity probe to characterize linear regime boundary.

## Open Questions the Paper Calls Out

- **Can the operator-level HRisk^LLM be efficiently approximated using Jacobian-vector products or layer-wise linearization at scale?**: The paper leaves "full Jacobian-based estimation of HRisk^LLM to future work" due to computational cost. Resolution requires an implementation tractable for large models that demonstrates correlation with proxy metrics.

- **Does the "stable miscalibration" pattern persist in larger checkpoints, different training regimes, and multimodal architectures?**: The study is limited to 7-8B models, leaving generalizability of the high-SNR stability mechanism unknown. Resolution requires layer-wise sensitivity analysis on larger or multimodal models.

- **Can process-level interventions (e.g., multi-step critique-and-revision loops) successfully dislodge stable but misaligned attractors in high-SNR models?**: The empirical study tested only single-shot prompt variants, not the "richer forms of Kantian feedback" suggested for future work. Resolution requires experiments demonstrating iterative self-critique changes internal attractor state where temperature scaling fails.

## Limitations
- LTI simulations may not fully capture the nonlinear dynamics of LLMs
- Internal stability analysis uses small perturbation regime (σ = 0.01) that may miss important nonlinear mechanisms
- LLM study is limited to three 7-8B models on binary factual questions, potentially not extending to multi-class tasks or larger models

## Confidence
- **High confidence**: H-Risk formal definition, LTI simulation results, basic LLM proxy methodology
- **Medium confidence**: Internal stability analysis methodology and results, LLM proxy calibration findings
- **Low confidence**: Generalization of "stable miscalibration" across model scales and task types, effectiveness of Kantian prompts beyond tested domains

## Next Checks
1. **Scale sensitivity validation**: Replicate internal stability analysis across model families spanning 1B to 70B parameters to determine whether stable miscalibration persists at scale.

2. **Task generalization study**: Apply H-Risk analysis to multi-class classification and open-ended generation tasks to compare whether stable miscalibration holds when outputs are not binary factual claims.

3. **Perturbation regime stress test**: Conduct systematic sweep of perturbation scales (σ ∈ {0.001, 0.01, 0.1, 0.5}) and perturbation types (Gaussian, adversarial, input shuffling) to identify boundary between linear and nonlinear instability regimes in LLM representations.