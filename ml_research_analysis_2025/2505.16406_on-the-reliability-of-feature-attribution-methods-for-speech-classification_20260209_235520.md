---
ver: rpa2
title: On the reliability of feature attribution methods for speech classification
arxiv_id: '2505.16406'
source_url: https://arxiv.org/abs/2505.16406
tags:
- attribution
- feature
- input
- methods
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of feature attribution
  methods when applied to speech classification models. The authors focus on evaluating
  whether commonly used attribution techniques consistently identify the same important
  features across multiple model runs initialized with different random seeds.
---

# On the reliability of feature attribution methods for speech classification
## Quick Facts
- arXiv ID: 2505.16406
- Source URL: https://arxiv.org/abs/2505.16406
- Reference count: 0
- Standard feature attribution methods show low reliability on speech models, with ISA scores typically below 0.6

## Executive Summary
This study investigates the reliability of feature attribution methods when applied to speech classification models. The authors evaluate whether commonly used attribution techniques consistently identify the same important features across multiple model runs initialized with different random seeds. They test four attribution methods on wav2vec2 models fine-tuned for three speech classification tasks: gender/speaker identification and intent classification. The primary finding reveals that standard attribution methods generally show low reliability when applied to speech models, with inter-seed agreement scores typically below 0.6, highlighting the need for speech-specific attribution techniques.

## Method Summary
The authors evaluate four attribution methods (Saliency, Integrated Gradients, LIME, and Feature Ablation) on wav2vec2 models fine-tuned for three speech classification tasks. They measure reliability using inter-seed agreement (ISA), which quantifies how often the same input features are identified as important across different model runs initialized with different random seeds. The study compares attribution performance across tasks that rely on different types of features - acoustic properties for gender/speaker identification versus lexical items for intent classification.

## Key Results
- Standard feature attribution methods show low reliability on speech models, with ISA scores typically below 0.6
- Word-aligned perturbation methods (LIME, Feature Ablation) show acceptable reliability for intent classification tasks
- No combination of method and input type achieved consistently reliable results for tasks relying on broader acoustic features
- Aggregating high-resolution attribution scores does not improve reliability

## Why This Works (Mechanism)
The low reliability of standard attribution methods on speech models stems from the inherent complexity and continuous nature of speech signals. Unlike text where individual tokens have clear boundaries and discrete meanings, speech features are distributed across overlapping acoustic properties that vary continuously in time and frequency. When models are trained with different random seeds, they may learn slightly different representations of these continuous features, leading to inconsistent attribution results even when the underlying decision-making process is similar.

## Foundational Learning
- **wav2vec2 model architecture** - Why needed: Forms the base model being evaluated; understanding its self-supervised pre-training and fine-tuning process is crucial. Quick check: Verify the model uses context-aware representations from transformer layers.
- **Feature attribution methods** - Why needed: Different techniques (gradient-based, perturbation-based) have distinct theoretical foundations affecting their reliability. Quick check: Confirm understanding of how each method computes importance scores.
- **Inter-seed agreement (ISA)** - Why needed: The primary metric measuring reliability across different model runs. Quick check: Ensure clarity on how ISA quantifies feature consistency across seeds.
- **Speech feature types** - Why needed: Acoustic vs. lexical features require different attribution approaches. Quick check: Distinguish between continuous acoustic properties and discrete word-level information.

## Architecture Onboarding
Component map: wav2vec2 base model -> Fine-tuning layer -> Attribution method -> Reliability metric
Critical path: Pre-trained wav2vec2 -> Task-specific fine-tuning -> Attribution computation -> ISA calculation
Design tradeoffs: Balance between attribution resolution (frame-level vs. word-level) and reliability; trade-off between computational cost of perturbation methods and gradient-based methods
Failure signatures: Low ISA scores indicate unreliable attributions; method-task mismatch shows up as poor performance on specific feature types
First experiments: 1) Run attribution methods with different random seeds on same model to establish baseline ISA; 2) Compare attribution reliability between acoustic and lexical feature-dependent tasks; 3) Test whether aggregating frame-level scores improves word-level reliability

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on specific set of attribution methods and tasks may not generalize to other speech applications
- Results based on wav2vec2 models fine-tuned on specific datasets, potentially limiting broader applicability
- Inter-seed agreement metric measures consistency but not correctness of attributions

## Confidence
- High confidence: Standard attribution methods show low reliability across different model runs
- Medium confidence: Word-aligned perturbation methods work better for intent classification
- Medium confidence: No reliable methods exist for acoustic-feature-dependent tasks
- Low confidence: Broad generalizability to all speech classification tasks and models

## Next Checks
1. Test additional attribution methods (e.g., SHAP, Occlusion) and model architectures (CNN-based, LSTM-based) on the same tasks to verify if reliability patterns hold across different approaches

2. Apply the methodology to non-English speech datasets and tasks to assess language-dependent effects on attribution reliability

3. Conduct human evaluation studies where speech experts validate whether the attributed features align with known acoustic cues for each task, moving beyond consistency metrics to assess correctness