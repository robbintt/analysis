---
ver: rpa2
title: Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time
  Scaling
arxiv_id: '2509.17905'
source_url: https://arxiv.org/abs/2509.17905
tags:
- uni00000013
- uni0000002c
- reasoning
- uni00000011
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overlooked issue of strategy-selection
  bias in chain-of-thought reasoning during test-time scaling. LLMs exhibit a strong
  preference for certain reasoning strategies (e.g., algebraic solutions) while neglecting
  valid alternatives (e.g., geometric solutions), resulting in insufficient exploration
  of the solution space.
---

# Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling

## Quick Facts
- arXiv ID: 2509.17905
- Source URL: https://arxiv.org/abs/2509.17905
- Reference count: 40
- One-line primary result: TTS-Uniform improves test-time scaling effectiveness by mitigating strategy-selection bias, achieving up to 20% accuracy gains on AIME problems.

## Executive Summary
This paper addresses a critical but overlooked issue in test-time scaling for chain-of-thought reasoning: strategy-selection bias. LLMs naturally favor certain reasoning strategies while neglecting valid alternatives, leading to inefficient exploration of the solution space. The authors propose TTS-Uniform, a framework that identifies potential solution strategies, uniformly allocates sampling budget across them, and filters out unstable strategies based on entropy before aggregation. Experimental results demonstrate significant improvements in scaling effectiveness across multiple LLMs and benchmark datasets, with particularly strong benefits for weaker reasoning models.

## Method Summary
TTS-Uniform mitigates strategy-selection bias by forcing uniform exploration of diverse reasoning approaches. The method first extracts potential strategies (coarse-grained via LLM prompting or fine-grained via reasoning tree construction), then allocates an equal sampling budget to each strategy. It computes the entropy of answer distributions for each strategy as a proxy for complexity/instability, discards the highest-entropy strategies, and aggregates remaining answers via majority voting. The framework is evaluated on mathematical reasoning benchmarks (AQuA, AIME) with multiple sampling budgets and demonstrates improved accuracy over standard self-consistency.

## Key Results
- Up to 20% accuracy improvement on AIME 2025 benchmark
- Consistent improvements across multiple LLMs (GPT-4o-mini, GPT-4.1-mini)
- Particularly strong benefits for weaker reasoning models with higher baseline strategy bias
- Entropy-based filtering effectively identifies and removes unstable strategies

## Why This Works (Mechanism)

### Mechanism 1: Budget Reallocation via Uniform Sampling
The paper argues that standard test-time scaling is inefficient because models over-sample dominant strategies while ignoring valid alternatives. TTS-Uniform identifies potential strategies and explicitly forces equal sampling budget allocation among them, artificially flattening the strategy distribution toward uniformity. This mechanism works when the model can execute strategies when explicitly prompted with them.

### Mechanism 2: Entropy-Based Complexity Filtering
High-complexity reasoning strategies accumulate more errors. Instead of using token length (noisy due to verbosity), TTS-Uniform uses answer entropy as a proxy for strategy complexity/instability. High entropy indicates divergent answers, suggesting the strategy is unstable and should be discarded. This filtering improves aggregation reliability by removing noisy strategies.

### Mechanism 3: Disproportionate Benefit for Weaker Models
Weaker reasoning models exhibit stronger strategy-selection bias, tending to collapse onto dominant patterns from pre-training. Forcing uniform exploration recovers performance lost to this bias. Stronger models naturally explore more broadly, reducing the marginal utility of forced uniformity.

## Foundational Learning

- **Concept: First-Order Stochastic Dominance (FOSD)**
  - Why needed here: The theoretical guarantee relies on FOSD to prove that shifting probability mass toward low-complexity strategies reduces expected error.
  - Quick check question: If distribution X stochastically dominates Y in the first order, does E[X] â‰¥ E[Y] hold for all utility functions? (Yes, specifically non-decreasing ones).

- **Concept: Strategy Equivalence Relations**
  - Why needed here: The method requires defining what constitutes a "distinct" strategy. The paper distinguishes "coarse-grained" (conceptual) vs. "fine-grained" (step-level) equivalence to group reasoning paths.
  - Quick check question: In the "coarse-grained" approach, do two paths using different algebraic manipulations count as the same strategy? (Yes, if they share the same high-level logic).

- **Concept: Answer Entropy as Uncertainty**
  - Why needed here: This is the signal used for the filtering mechanism. Low entropy implies consensus (stability), while high entropy implies chaos (instability).
  - Quick check question: If sampling 64 paths for a strategy results in 64 different answers, is the entropy high or low? (High).

## Architecture Onboarding

- **Component map:** Strategy Extractor -> Uniform Sampler -> Entropy Filter -> Aggregator
- **Critical path:** The Strategy Extraction phase. If the prompt fails to elicit distinct valid strategies, or if reasoning tree construction fails to identify distinct branches, uniform sampling offers no diversity advantage.
- **Design tradeoffs:**
  - Coarse vs. Fine-Grained: Coarse is cheaper (single LLM call) but relies on model's ability to abstract concepts. Fine-grained is compute-intensive but better for ambiguous problems.
  - Filtering Threshold (n): Aggressive filtering removes noise but risks discarding valid "difficult" strategies.
- **Failure signatures:**
  - Collapse to Single Strategy: Extractor returns only 1 strategy, reverting to standard TTS.
  - Over-Filtering: All strategies filtered out due to high entropy on extremely hard problems.
  - Token Length Mismatch: Using token count instead of entropy for filtering.
- **First 3 experiments:**
  1. Baseline Validation: Reproduce gap between "Self-Consistency" and "Uniform-C (Z)" on AIME/GSM8K.
  2. Proxy Validation: Correlate "Minimum Token Requirement" against "Answer Entropy" to validate filtering mechanism.
  3. Scaling Sensitivity: Plot accuracy vs. budget k to confirm TTS-Uniform maintains or improves scaling slopes.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can TTS-Uniform be generalized to non-mathematical reasoning paradigms such as multi-step planning or multimodal inference?
- **Open Question 2:** How can pre-training or alignment procedures be modified to intrinsically mitigate strategy-selection bias, rather than relying on test-time interventions?
- **Open Question 3:** Is answer entropy the most robust proxy for identifying high-complexity strategies compared to other uncertainty metrics or process-based verification?

## Limitations

- Entropy-based complexity filtering lacks comprehensive validation across problem types and domains
- Method assumes strategy diversity directly translates to improved accuracy, which may not hold for problems with single valid solution paths
- Strategy extraction quality significantly impacts performance but lacks systematic evaluation for coverage and distinctness

## Confidence

**High Confidence:** The core observation that LLMs exhibit strategy-selection bias is well-supported by experimental results, particularly the 20% accuracy improvements on AIME 2025.

**Medium Confidence:** The effectiveness of uniform sampling allocation is demonstrated empirically but relies on the assumption that strategy extraction reliably identifies all valid approaches.

**Low Confidence:** The entropy-based complexity filtering mechanism, while theoretically motivated, lacks comprehensive validation across problem types and domains.

## Next Checks

1. Apply TTS-Uniform to reasoning benchmarks outside mathematics (e.g., coding problems, logical reasoning) to verify cross-domain generalization.

2. Systematically evaluate strategy extraction coverage and distinctness across problem difficulty levels to measure correlation with performance gains.

3. Conduct comprehensive ablation study varying the number of filtered strategies (n) across different model capabilities and problem difficulties to establish parameter selection guidelines.