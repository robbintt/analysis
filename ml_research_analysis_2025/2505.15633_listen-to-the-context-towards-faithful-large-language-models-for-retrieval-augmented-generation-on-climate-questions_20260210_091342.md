---
ver: rpa2
title: 'Listen to the Context: Towards Faithful Large Language Models for Retrieval
  Augmented Generation on Climate Questions'
arxiv_id: '2505.15633'
source_url: https://arxiv.org/abs/2505.15633
tags:
- claim
- faithfulness
- climategpt
- context
- faithful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving the faithfulness
  of large language models in retrieval-augmented generation (RAG) for climate-related
  questions. Faithfulness refers to whether generated responses accurately reflect
  the information in retrieved passages, a critical factor for reliable climate information
  access.
---

# Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions

## Quick Facts
- arXiv ID: 2505.15633
- Source URL: https://arxiv.org/abs/2505.15633
- Reference count: 33
- Key outcome: Instruction fine-tuning data curation improves RAG faithfulness from 30% to 57% for climate questions

## Executive Summary
This work addresses the critical challenge of improving faithfulness in large language models for retrieval-augmented generation (RAG) on climate-related questions. The authors find that climate-specialized models like ClimateGPT struggle with faithfulness despite using RAG, often incorporating claims unsupported by retrieved passages. By analyzing instruction fine-tuning data subsets, they identify that excluding unfaithful training examples—particularly from non-expert, open-ended data—significantly improves model faithfulness. Their approach, validated across multiple datasets and metrics, demonstrates that targeted IFT data curation is an effective strategy for enhancing RAG faithfulness in specialized domains.

## Method Summary
The authors develop an automated evaluation pipeline to assess faithfulness by decomposing responses into atomic claims and verifying each against retrieved evidence using GPT-4o. They apply this pipeline to evaluate their climate-specific instruction fine-tuning (IFT) data, identifying subsets with low faithfulness (below 50% claim support). By excluding the lowest-performing subsets—particularly grounded non-expert data—from training, they develop ClimateGPT Faithful+, which improves claim support from 30% to 57%. The model is trained using Megatron-LLM on 4xA100 GPUs for approximately 4 hours.

## Key Results
- ClimateGPT baseline achieves only 30% claim support on their benchmark
- Excluding low-faithfulness training subsets improves claim support to 57% (27 percentage points)
- Faithful+ model outperforms all baseline approaches on climate-specific datasets
- Improvements generalize across multiple evaluation metrics and external datasets

## Why This Works (Mechanism)

### Mechanism 1: Training Data Contamination Removal
- Claim: Excluding unfaithful training examples from IFT data improves model faithfulness at inference time.
- Mechanism: Models learn behavioral patterns from demonstrations. Training on examples where gold responses contain claims unsupported by their provided context teaches the model that context can be ignored—this pattern persists at inference.
- Core assumption: The model's tendency to disregard retrieved context is primarily learned during IFT, not pre-training.
- Evidence anchors:
  - [abstract] "By excluding unfaithful examples from the training data, we develop ClimateGPT Faithful+, which improves claim support from 30% to 57%"
  - [section] Table 3: Removing "Grounded Non-Expert" data (43% claim support) increases model faithfulness from 30% to 57%
  - [corpus] ParamMute (2502.15543) addresses similar faithfulness issues by suppressing knowledge-critical FFNs
- Break condition: If unfaithfulness stems from pre-trained parametric knowledge overwhelming context-attention mechanisms, data filtering alone may be insufficient.

### Mechanism 2: Faithfulness-Factuality Decoupling
- Claim: Faithfulness and factuality are orthogonal evaluation dimensions; a response can be factually correct but unfaithful if it uses knowledge outside the provided context.
- Mechanism: Requiring all claims to be supported by retrieved context creates a verifiable, transparent output—even if the model knows more than the context contains.
- Core assumption: In RAG settings, transparency and verifiability matter more than completeness; users should be able to trace claims to sources.
- Evidence anchors:
  - [abstract] "its effectiveness depends on whether the model's output remains faithful to these passages"
  - [section] Figure 1: ClimateGPT correctly states "scheduled to begin in 2023" but this is marked unfaithful (red) because it's not in the context
  - [corpus] Faithfulness-Aware Uncertainty Quantification (2505.21072) links hallucinations to internal knowledge vs. retrieved context conflicts
- Break condition: If retrieved contexts are incomplete or low-quality, strict faithfulness may produce less helpful responses than allowing selective parametric knowledge use.

### Mechanism 3: Atomic Claim Verification Pipeline
- Claim: Decomposing long-form responses into atomic claims enables granular, automatable faithfulness assessment.
- Mechanism: Three-step pipeline: (1) LLM decomposes response into independent claims, (2) retrieve evidence (RAG passages for faithfulness, KB for factuality), (3) LLM verifies each claim against evidence.
- Core assumption: GPT-4o-based claim decomposition and verification is sufficiently reliable for automated metric construction.
- Evidence anchors:
  - [abstract] "They then develop an automated metric for assessing faithfulness by decomposing responses into claims and verifying whether each claim is supported"
  - [section] Table 1: Claims per response range from 17-25; claim support varies 30-72% across models
  - [corpus] LettuceDetect (2502.17125) offers token-level alternative; achieves 86.7% agreement with human annotations on Climate Policy Radar data
- Break condition: If decomposition is inconsistent or verification is unreliable, the metric won't accurately reflect faithfulness. Paper acknowledges this needs human validation (Limitations section).

## Foundational Learning

### Concept: Faithfulness ≠ Factuality
- Why needed here: Critical to understand that a model can be "right" but still fail if it uses knowledge not in the provided context.
- Quick check question: If context says "GST assesses progress" but model adds "GST began in 2023" (true, not in context)—is this faithful?

### Concept: Grounded vs Ungrounded IFT Examples
- Why needed here: Training data structure determines what behavior model learns; "grounded" examples provide context that responses should derive from.
- Quick check question: In a grounded QA example where annotators only provided sources for "crucial claims," what happens to the remaining claims?

### Concept: Claim-Level vs Response-Level Evaluation
- Why needed here: Long-form responses contain multiple assertions; aggregate scores mask which specific claims are ungrounded.
- Quick check question: A response scores "57% claim support"—what does this tell you about the remaining 43% of claims?

## Architecture Onboarding

### Component Map
IFT Data (categorized by: expert level, open/closed-ended, grounding status) -> Faithfulness Evaluation Pipeline -> Training Data Filtering (exclude low-faithfulness subsets) -> Model Re-training (Megatron-LLM, 4xA100, ~4 hours)

### Critical Path
1. Evaluate existing IFT gold responses for faithfulness against their provided contexts
2. Identify and exclude subsets with <50% claim support (especially non-expert grounded data)
3. Re-train model; validate on held-out test set using multiple metrics

### Design Tradeoffs
- **Faithfulness vs. Helpfulness**: Unfaithful details (e.g., specific dates) may improve user experience but reduce verifiability
- **Data volume vs. Quality**: Removing ~8.5K non-expert grounded examples improved faithfulness nearly 2x
- **Metric strictness**: RAGAs-based verifier is strict (requires explicit support); may undercount supported claims requiring inference

### Failure Signatures
- Claim support ≈ 25-30% with RAG: Model effectively ignoring context (ClimateGPT baseline)
- Large gap between Reference vs. KB support: Retrieval failures, not model failures
- Factuality >> Faithfulness: Parametric knowledge "leaking" into outputs

### First 3 Experiments
1. **Baseline assessment**: Run the claim decomposition + verification pipeline on your current IFT data's gold responses; identify which subsets have <50% claim support
2. **Ablation training**: Train model excluding the lowest-faithfulness subset; measure improvement on held-out test set
3. **Cross-validation**: Evaluate improved model on external dataset (e.g., Climate Policy Radar) using both RAGAs and LettuceDetect metrics to confirm generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unfaithful training examples be salvaged by retrieving supporting passages or generating synthetic context rather than being discarded?
- Basis in paper: [explicit] The Conclusion suggests that rather than removing low-faithfulness data, future work could "enrich them by retrieving supporting passages... [or] synthetic context."
- Why unresolved: The current study only demonstrates that excluding unfaithful subsets improves performance; the utility of repairing these examples remains untested.
- What evidence would resolve it: A comparison of model performance when fine-tuned on "enriched" unfaithful examples versus the exclusion approach used in ClimateGPT Faithful+.

### Open Question 2
- Question: How do general-domain instruction fine-tuning datasets influence the faithfulness of specialized climate models?
- Basis in paper: [explicit] The Limitations section states the authors "did not study the impact of the general domain IFT datasets included in IFT training, such as Open Assistant, Dolly 3 and FLAN v2."
- Why unresolved: The ablation studies focused exclusively on the climate-specific partitions of the training data, leaving the influence of the general domain partition unknown.
- What evidence would resolve it: Ablation experiments comparing models trained with and without general-domain IFT data to isolate their effect on faithfulness metrics.

### Open Question 3
- Question: Does the optimization for strict faithfulness inadvertently degrade the helpfulness or adequacy of responses for end-users?
- Basis in paper: [explicit] The Limitations section notes that "a less faithful output can actually be more helpful or relevant," and the study did not "consider additional quality factors like the helpfulness or adequacy."
- Why unresolved: The current evaluation metrics focus solely on claim support relative to context, ignoring user utility where extrinsic knowledge might be beneficial.
- What evidence would resolve it: Human evaluation scores measuring user preference for helpfulness in the Faithful+ model versus the baseline, specifically for questions where faithfulness constraints are active.

## Limitations

- Automated claim decomposition and verification lacks comprehensive human validation, creating uncertainty about metric accuracy
- Findings may not generalize beyond climate-specific domains and the Megatron-LM architecture used
- Improvement in faithfulness (30% to 57%) still leaves nearly half of claims unsupported, indicating substantial room for improvement

## Confidence

**High Confidence**: The observation that ClimateGPT exhibits low faithfulness (30% claim support) and that this stems from the training data rather than the model architecture itself. The ablation study showing dramatic improvement when removing low-faithfulness subsets (43 percentage points) is methodologically sound.

**Medium Confidence**: The generalizability of the data curation approach across different domains and model architectures. While the mechanism is plausible, the specific thresholds and filtering criteria may need domain-specific calibration.

**Low Confidence**: The completeness of the automated evaluation pipeline. The claim decomposition and verification process may systematically miss certain types of faithfulness violations or produce false positives/negatives that would be caught by human evaluation.

## Next Checks

1. **Human validation study**: Conduct expert human evaluation on a stratified sample of claims (high-confidence supported, low-confidence unsupported, and borderline cases) to assess the accuracy of the automated pipeline and identify systematic errors in decomposition or verification.

2. **Cross-domain replication**: Apply the same data filtering methodology to a different specialized domain (e.g., medical or legal) with its own climate-agnostic RAG model to test whether the faithfulness improvements transfer beyond climate-specific contexts.

3. **Parametric knowledge control experiment**: Implement a controlled experiment where the model is prevented from accessing parametric knowledge for a subset of questions, forcing it to rely solely on retrieved context. Compare faithfulness metrics with and without this constraint to isolate the contribution of parametric knowledge to unfaithfulness.