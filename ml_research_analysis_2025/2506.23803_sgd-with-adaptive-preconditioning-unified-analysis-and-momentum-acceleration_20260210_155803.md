---
ver: rpa2
title: 'SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration'
arxiv_id: '2506.23803'
source_url: https://arxiv.org/abs/2506.23803
tags:
- following
- preconditioning
- uses
- lemma
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a unified theoretical framework for analyzing\
  \ stochastic gradient descent (SGD) with adaptive preconditioning, showing that\
  \ popular algorithms like AdaGrad, Shampoo, and their variants can be analyzed under\
  \ a single convergence proof. The key contribution is developing a unified analysis\
  \ under matrix H\xF6lder smoothness and bounded variance assumptions, which recovers\
  \ state-of-the-art convergence guarantees for multiple existing algorithms and provides\
  \ the first theoretical guarantees for DASGO."
---

# SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration

## Quick Facts
- arXiv ID: 2506.23803
- Source URL: https://arxiv.org/abs/2506.23803
- Reference count: 40
- Primary result: Provides unified convergence analysis for SGD with adaptive preconditioning under matrix Hölder smoothness, recovering and accelerating state-of-the-art rates for AdaGrad, Shampoo, and DASGO

## Executive Summary
This paper develops a unified theoretical framework for analyzing stochastic gradient descent with adaptive preconditioning, showing that popular algorithms like AdaGrad, Shampoo, and their variants can be analyzed under a single convergence proof. The key contribution is establishing that convergence rates for these adaptive methods can be accelerated beyond existing results using Nesterov momentum, without requiring additional restrictive assumptions. This acceleration is particularly effective for algorithms with diagonal preconditioning like AdaGrad and DASGO, providing the first theoretical justification that these methods can simultaneously benefit from both diagonal preconditioning and momentum.

## Method Summary
The method uses an optimization-based preconditioner defined as H_k = argmin_{H∈H∩S++} ⟨H, S_k⟩ + ⟨I, ϕ(H)⟩ where S_k accumulates gradient outer products and ϕ is a potential function. This formulation recovers various adaptive methods by selecting different operator subspaces H. The algorithm computes preconditioner H_k via matrix square root of projected gradient information, then applies preconditioned gradient steps with optional Nesterov momentum. The unified analysis relies on matrix Hölder smoothness assumptions and bounded variance noise, enabling convergence rate bounds that depend on the operator norm of the smoothness parameter rather than worst-case constants.

## Key Results
- Provides first unified convergence analysis for SGD with adaptive preconditioning under matrix Hölder smoothness and bounded variance assumptions
- Recovers state-of-the-art convergence results for AdaGrad, ASGO/One-sided Shampoo, and DASGO through single optimization framework
- Demonstrates Nesterov momentum can accelerate convergence beyond existing results for adaptive methods with diagonal preconditioning
- Establishes first theoretical justification that AdaGrad-type algorithms can simultaneously benefit from both diagonal preconditioning and momentum acceleration
- Provides first theoretical guarantees for DASGO, a recent adaptive method for matrix optimization

## Why This Works (Mechanism)

### Mechanism 1: Unified Preconditioner via Optimization Formulation
A single optimization-based preconditioner definition can recover AdaGrad-Norm, AdaGrad, ASGO/One-sided Shampoo, and DASGO by selecting different operator subspaces H. Defining H_k = η(δI + proj_H(S_k))^{-1/2} with ϕ(h) = δh + η²/h enables direct application of the Follow-the-Leader lemma, yielding Lemma 1's inequality that bounds cumulative preconditioned gradient norms—this single lemma replaces algorithm-specific proofs. Core assumption: Assumption 1 (order-preserving projection onto H, closure under operator functions).

### Mechanism 2: Matrix Hölder Smoothness Captures Anisotropic Problem Structure
Measuring smoothness via weighted norm ‖·‖_L with L ∈ H yields tighter bounds when L has sparse/skewed eigenvalues than isotropic smoothness constants. Assumption 2's matrix Hölder smoothness combined with Lemma 3 bounds ‖∇f(x)‖²_{L^{-1}} by (f(x)−f(x*))^{2ν/(1+ν)}, enabling Lemma 6 to express convergence in terms of ‖L‖_{tr} rather than dimension-dependent worst-case constants—when L is sparse, ‖L‖_tr ≪ d‖L‖_∞. Core assumption: Assumption 2 (convexity + matrix Hölder smoothness with L ∈ H ∩ S++).

### Mechanism 3: Momentum Acceleration via Commutativity (Diagonal Preconditioning)
Nesterov momentum provably accelerates AdaGrad-type methods from O(1/K^{(1+ν)/2}) to O(1/K^{(1+3ν)/2}) when smoothness/noise operators commute with H—automatically satisfied for diagonal preconditioners. Assumption 4 (LH = HL, ΣH = HΣ) enables Lemma 8's key insight that H_k² solves a modified optimization problem, feeding Lemma 9's logarithmic bound on cumulative gradient terms, ultimately yielding Theorem 2's accelerated rate. For diagonal H, commutativity is trivial. Core assumption: Assumption 4 (commutativity of L, Σ with H).

## Foundational Learning

- **Follow-the-Leader / Online Convex Optimization**: The entire unified analysis hinges on the FTL-BTL lemma to bound regret without per-algorithm proofs. Quick check: Why does choosing H_k to minimize cumulative loss Σ_{i=-1}^k l_i(H) yield Lemma 1's inequality?

- **Operator Calculus (Löwner Order, Matrix Functions)**: Assumption 1's closure under ψ(H), Lemma 2's matrix square root, and monotonicity arguments all require operator function machinery. Quick check: Given H = Σ_i λ_i P_i (eigendecomposition), compute ψ(H) per Definition 1.

- **Non-Euclidean Smoothness (Anisotropic/Matrix Norms)**: Standard L-smoothness ignores structure; matrix smoothness with ‖·‖_L captures per-coordinate/row curvature, essential for explaining AdaGrad's diagonal advantage. Quick check: Per Table 1, what norm R(·) corresponds to AdaGrad vs ASGO?

## Architecture Onboarding

- **Component map**: S_k accumulation -> proj_H(S_k) -> H_k computation -> preconditioned gradient step -> (optional momentum update)

- **Critical path**: 1) Accumulate gradient outer products: S_k = Σ_{i=0}^k g_i⟨g_i, ·⟩, 2) Project S_k onto H: proj_H(S_k), 3) Compute preconditioner H_k via matrix square root (Lemma 2), 4) Apply preconditioned gradient step (with optional Nesterov extrapolation)

- **Design tradeoffs**:
  - Diagonal H (AdaGrad/DASGO): O(d) per-step, automatic Assumption 4 satisfaction, acceleration guaranteed
  - Full matrix H (Shampoo/ASGO): Potentially tighter bounds for matrix problems, O(m³) decomposition cost, acceleration not guaranteed
  - δ parameter: Larger δ stabilizes numerics but adds 3√δR·dim(X)/(K+1) term to bound

- **Failure signatures**:
  - Iterate explosion: If R bound (eq. 16) violated, add projection (Algorithms 3/4)
  - Non-diagonal H + momentum fails to accelerate: If L, Σ don't commute with H, Theorem 2 guarantees don't apply
  - Numerical instability: Near-zero S_k eigenvalues require δ > 0 regularization

- **First 3 experiments**:
  1. Verify anisotropic smoothness benefit: Compare AdaGrad vs AdaGrad-Norm on synthetic problem with sparse L (e.g., only 10% of coordinates smooth), measuring ‖l‖_1/‖l‖_∞ gap impact
  2. Validate acceleration for diagonal case: Run Algorithm 1 vs Algorithm 2 (AdaGrad/DASGO) on smooth convex problem (ν=1), confirming O(1/K²) vs O(1/K) gap empirically
  3. Test commutativity break: Apply Algorithm 2 with Shampoo-style (non-diagonal) H on matrix problem where L has non-trivial structure; measure if acceleration degrades vs diagonal DASGO baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can parameter-free gradient methods with diagonal or matrix preconditioning be designed, avoiding the need to tune η ∝ ∥x∗∥? Section 1.4 states: "Designing parameter-free gradient methods with diagonal or matrix preconditioning is an interesting question for future work." Existing parameter-free AdaGrad results only apply to scalar stepsizes, which are rarely used in practice.

### Open Question 2
Can the momentum acceleration results be extended beyond diagonal preconditioners to full matrix preconditioners like ASGO/One-sided Shampoo? Assumption 4 (L, Σ commuting with H) holds automatically for diagonal preconditioners but restricts acceleration results to methods like AdaGrad and DASGO, excluding ASGO with full matrix preconditioning.

### Open Question 3
Can the unified analysis be extended to non-convex objective functions while preserving meaningful convergence guarantees? Appendix A discusses that convex optimization serves as a starting point, and "it is natural to consider the convex setting first before trying to relax it." For non-convex functions, global convergence beyond first-order stationarity cannot be guaranteed without additional assumptions like gradient domination.

### Open Question 4
How does DASGO perform empirically compared to Scion and Adam in practical deep learning scenarios? Section 3.2 states: "It is worth trying to use DASGO in the practical scenarios identified by Pethick et al. (2025) to benefit from using the non-Euclidean norm ∥·‖₂→∞." The paper provides first theoretical guarantees for DASGO but no empirical evaluation of its practical performance.

## Limitations

- The unified analysis relies critically on the Hölder smoothness assumption, which may not hold for many practical deep learning problems with non-smooth or discontinuous gradients
- The commutativity requirement for momentum acceleration limits accelerated results to diagonal preconditioners, excluding full-matrix methods like ASGO
- The theoretical framework requires bounded iterates and bounded variance noise assumptions that may be violated in unbounded domains or heavy-tailed noise scenarios

## Confidence

- **High confidence**: Unified analysis framework (Lemma 1-3) - The Follow-the-Leader based proof structure is rigorous and mathematical derivations are sound
- **Medium confidence**: Momentum acceleration claims - While theory is correct for diagonal preconditioners, empirical validation across diverse problem types is limited
- **Low confidence**: Practical applicability to deep learning - Standard deep learning problems often violate the Hölder smoothness and bounded variance assumptions required for theoretical guarantees

## Next Checks

1. Test momentum acceleration on non-convex problems with varying curvature: Apply Algorithm 2 with AdaGrad to deep neural network training tasks where curvature varies across layers/parameters, measuring if the theoretical acceleration holds empirically despite non-convexity

2. Evaluate adaptive preconditioner performance under heavy-tailed noise: Replace the bounded variance assumption with heavy-tailed noise distributions (e.g., Student's t-distribution) and measure convergence behavior compared to standard Adam/AdamW, which implicitly handle such noise

3. Validate full-matrix preconditioner behavior with momentum: Implement Shampoo/ASGO with Nesterov momentum on matrix factorization problems where the smoothness operator L has non-trivial structure, empirically testing if lack of commutativity significantly degrades acceleration compared to diagonal DASGO