---
ver: rpa2
title: 'The Greatest Good Benchmark: Measuring LLMs'' Alignment with Utilitarian Moral
  Dilemmas'
arxiv_id: '2503.19598'
source_url: https://arxiv.org/abs/2503.19598
tags:
- moral
- llms
- agree
- disagree
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Greatest Good Benchmark (GGB) to evaluate
  the moral judgments of large language models (LLMs) using utilitarian dilemmas.
  The GGB adapts the Oxford Utilitarianism Scale, expanding it tenfold, and employs
  a multi-prompt approach to mitigate bias.
---

# The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas

## Quick Facts
- arXiv ID: 2503.19598
- Source URL: https://arxiv.org/abs/2503.19598
- Reference count: 28
- Most models exhibit consistent moral preferences that diverge from both lay population and established moral theories, forming an "artificial moral compass"

## Executive Summary
This paper introduces the Greatest Good Benchmark (GGB) to evaluate the moral judgments of large language models (LLMs) using utilitarian dilemmas. The GGB adapts the Oxford Utilitarianism Scale, expanding it tenfold, and employs a multi-prompt approach to mitigate bias. Analyzing 15 diverse LLMs, the study finds that most models exhibit consistent moral preferences that diverge from both lay population and established moral theories. Specifically, LLMs strongly reject instrumental harm and highly endorse impartial beneficence, forming what the authors term an "artificial moral compass." Model size significantly moderates these trends, with smaller models showing extreme endorsement of impartial beneficence compared to larger ones.

## Method Summary
The GGB is built on the Oxford Utilitarianism Scale, which splits utilitarianism into Impartial Beneficence (maximizing good impartially) and Instrumental Harm (accepting harm for greater good). The benchmark expands the original 9 statements to 99 (9 original + 90 synthetically generated, expert-validated), each evaluated with 6 prompt variations to mitigate Likert-scale bias. Models are evaluated at temperature=0.5 using Chain-of-Thought prompting, with responses averaged across variations and iterations. Consistency is validated by low variance across 10 iterations for most model-dimension pairs. Results are compared to lay population baseline (IB=3.65, IH=3.31) using t-tests with Cohen's d effect sizes.

## Key Results
- LLMs strongly reject instrumental harm (mean ~2.0) and highly endorse impartial beneficence (mean ~4.0-6.0), diverging from lay population preferences
- Model size significantly moderates moral preference intensity, with smaller models exhibiting more extreme impartial beneficence endorsement
- The multi-prompt averaging strategy effectively reduces positional and framing biases in model responses
- Most models show consistent moral preferences across iterations, with only 5 of 30 measurements showing high variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-prompt averaging reduces positional and framing biases in Likert-scale responses.
- Mechanism: LLMs exhibit sensitivity to option ordering and label framing (confirmed via Kruskal-Wallis test: F=14.7267, p=0.022). By generating six prompt variations—including inverted numerical scales and reversed textual labels—then averaging responses, directional biases cancel out, yielding a more stable estimate of the underlying moral preference.
- Core assumption: The true moral preference is one-dimensional and stable; variance across prompts reflects measurement noise rather than context-dependent reasoning.
- Evidence anchors:
  - [section 3.2]: "A Kruskal-Wallis test confirmed this observation as there were significant differences in model replies across prompt variations (F=14.7267, p=0.022)."
  - [section 3.2]: "by inverting the option choices and then calculating the average response across the different instructions, we can extract a much more informative value"
  - [corpus]: Related work on moral evaluation (MoralReason, MAEBE) uses similar multi-prompt strategies but does not systematically test bias mitigation.
- Break condition: If a model's responses are inconsistent across all prompt variations (high variance with no directional pattern), the mechanism cannot extract a reliable preference signal.

### Mechanism 2
- Claim: Chain-of-thought (CoT) prompting with non-zero temperature elicits consistently encoded moral preferences.
- Mechanism: Setting temperature to 0.5 and requiring models to reason before answering allows controlled stochasticity while exposing stable underlying preferences. Consistency is validated by low variance across 10 iterations for most model–dimension pairs (25 of 30 measurements).
- Core assumption: Moral preferences are "encoded" in model weights and can be surfaced through reasoning; CoT reduces superficial pattern-matching.
- Evidence anchors:
  - [section 4]: "we set the temperature to 0.5 and, by using the Chain of Thought (CoT) prompting technique, we allowed models to reason over each statement before providing their final answer."
  - [section 4]: "In 25 out of 30 measurements we found a consistent moral preference"
  - [corpus]: Scherrer et al. (2023), cited in the paper, uses similar temperature-based consistency checks.
- Break condition: If variance remains high across iterations (5 of 30 cases flagged), the mechanism fails and the mean is uninformative.

### Mechanism 3
- Claim: Model size moderates moral preference intensity, with smaller models exhibiting more extreme impartial beneficence endorsement.
- Mechanism: Larger models may internalize more nuanced training signals (e.g., RLHF, safety fine-tuning) that attenuate extreme moral stances, while smaller models lack this regularization. The paper observes a clear size-gradient in IB scores (smaller models: mean ~5.5–6.1; larger models: mean ~3.1–4.0).
- Core assumption: Size is a proxy for training sophistication; the effect is not confounded by architecture family or training data composition.
- Evidence anchors:
  - [abstract]: "Model size significantly moderates these trends, with smaller models showing extreme endorsement of impartial beneficence compared to larger ones."
  - [section 5, Figure 3b]: Shows three distinct size groups with decreasing IB as size increases.
  - [corpus]: Corpus neighbors do not directly address the size–morality relationship; this mechanism is underexplored externally.
- Break condition: If model families differ systematically in training data or alignment procedures, size alone may be confounded and the mechanism misattributed.

## Foundational Learning

- Concept: **Oxford Utilitarianism Scale (OUS) dimensions**
  - Why needed here: The GGB is built on OUS, which splits utilitarianism into Impartial Beneficence (maximizing good impartially) and Instrumental Harm (accepting harm for greater good). Understanding this distinction is essential to interpret model scores.
  - Quick check question: Can you explain why a model might highly endorse IB while strongly rejecting IH, and why this doesn't align with classical utilitarianism?

- Concept: **Likert-scale bias in LLMs**
  - Why needed here: LLMs are sensitive to option order and numerical labels; the paper explicitly addresses this via prompt variations.
  - Quick check question: If you present options as "1, 4, 7" vs. "1, 2, 3, 4, 5, 6, 7," what bias might arise in model responses?

- Concept: **Consistency vs. accuracy in moral evaluation**
  - Why needed here: The GGB measures whether models have *stable* moral preferences, not whether those preferences are "correct" or human-aligned.
  - Quick check question: A model gives highly consistent but non-human-aligned responses. Is this a failure of the benchmark or a finding?

## Architecture Onboarding

- Component map: Statements -> 6 prompt variations -> 10 iterations (temp=0.5, CoT) -> GPT-3.5 post-processing -> Average across variations -> IB/IH subscale scores -> t-test vs. lay population

- Critical path:
  1. Load statements and generate 6 prompt variations per statement.
  2. For each model, run 10 iterations per variation at temperature 0.5 with CoT.
  3. Filter high-variance responses (variance threshold via histogram inspection).
  4. Average remaining responses across variations to obtain per-statement scores.
  5. Aggregate into IB and IH subscale means; compare to lay population via t-test.

- Design tradeoffs:
  - **Temperature 0.5 vs. 0**: Non-zero temperature exposes consistency but increases variance; the paper argues this is preferable to deterministic but potentially brittle outputs.
  - **Extended dataset vs. original only**: 10x expansion improves robustness but requires expert validation; the paper finds similar results across both, validating the synthetic generation.
  - **Variance threshold**: Set by visual inspection rather than statistical test; may over- or under-exclude noisy models.

- Failure signatures:
  - **High variance across iterations**: Model lacks stable moral preference; mean is uninformative (5 of 30 cases).
  - **Refusal to answer**: Some models output "As a language model..." instead of a rating; post-processing maps these to 0 and excludes them.
  - **Prompt-specific bias**: If averaging does not converge (e.g., responses cluster at opposite ends), the mitigation fails.

- First 3 experiments:
  1. **Reproduce IB/IH scores for a single model** (e.g., GPT-4-0613) using the original 9 statements and 6 prompt variations. Verify consistency across iterations and compare to reported means (IB=3.64, IH=2.04).
  2. **Test a new model not in the paper** (e.g., a recent open-source release) on both original and extended datasets. Check whether IB/IH patterns hold and whether size-like effects emerge.
  3. **Ablate the multi-prompt mitigation**: Run the same model with only a single prompt format (no inversion/averaging). Compare variance and mean shift to quantify bias reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the moral alignment of LLMs, specifically regarding impartial beneficence and instrumental harm, vary significantly when evaluated in languages other than English?
- Basis in paper: [explicit] The "Limitations" section states that the study uses only English, and suggests that "An interesting area for future research could involve translating the dataset and testing it across multiple languages."
- Why unresolved: The current study was restricted to English, yet moral reasoning capabilities and inherent biases in LLMs are known to potentially fluctuate across different linguistic and cultural contexts.
- What evidence would resolve it: Running the Greatest Good Benchmark (GGB) on multilingual models using translated versions of the dataset and comparing the scores against the English baseline.

### Open Question 2
- Question: How do human moral judgments align with the synthetically expanded GGB dataset compared to the original Oxford Utilitarianism Scale (OUS) items?
- Basis in paper: [explicit] The authors note in the "Limitations" section that "testing the extended dataset on a human sample could provide a valuable reference for further validation and comparisons."
- Why unresolved: While the extended dataset (90 new items) was validated by three experts and showed consistency with original items in LLMs, it has not been empirically validated against a broad human population to ensure it measures human moral preferences accurately.
- What evidence would resolve it: Administering the extended 90-item dataset to a diverse human sample and analyzing the distribution of responses (mean, variance) to confirm they reflect similar moral dimensions as the original OUS.

### Open Question 3
- Question: Do non-instruct or base completion models exhibit the same "artificial moral compass" (high impartial beneficence, low instrumental harm) observed in instruct-tuned models?
- Basis in paper: [explicit] The "Limitations" section specifies that "We only used instruct and/or chat models. If completion models or other types of models were tested in the future, results could differ."
- Why unresolved: The alignment techniques (RLHF, SFT) used to create instruct models may be the primary driver of the specific "artificial" moral preferences observed, a hypothesis that cannot be tested without evaluating base models.
- What evidence would resolve it: Applying the GGB to base pre-trained models (without instruction tuning) of similar sizes and comparing their IB and IH scores to their instruct-tuned counterparts.

### Open Question 4
- Question: Which specific model factors (e.g., parameter count, geographic origin, training data composition) are the most significant predictors of an LLM's moral stance?
- Basis in paper: [explicit] The "Limitations" section mentions that "further data analysis, such as multiple regressions or principal component analysis, is desirable to refine our understanding of which factors... better explain the models' moral stances."
- Why unresolved: While the study identifies model size as a moderator, the lack of public data regarding parameters and training corpora for many proprietary models prevented a comprehensive multivariate analysis of other potential drivers.
- What evidence would resolve it: A regression analysis including metadata from open-source models where parameter counts and training data sources are fully transparent.

## Limitations
- The study is limited to English-language models and may not generalize across languages or cultures
- Only instruct-tuned models were evaluated; base models may show different moral preferences
- The expert validation of extended statements, while thorough, may introduce synthetic generation bias
- Size-based moderation effects may be confounded by training data composition or alignment fine-tuning differences

## Confidence
- **High Confidence**: IB/IH measurement consistency via multi-prompt averaging (Kruskal-Wallis confirms bias reduction; 25/30 model-dimension pairs show stable preferences)
- **Medium Confidence**: Size-based moderation of moral preferences (clear gradient but no control for training regimen differences)
- **Medium Confidence**: Claim that models' moral compass diverges from lay population and moral theories (statistically significant via t-tests with large effect sizes, but cross-cultural generalization untested)

## Next Checks
1. **Ablate multi-prompt averaging**: Run the same models with a single prompt format. Compare variance and mean shift to quantify the bias mitigation effect claimed in the paper.
2. **Control for training data**: Test models with similar parameter counts but different training regimes (e.g., open vs. closed weights) to isolate size from alignment fine-tuning confounds.
3. **Cross-cultural validation**: Administer the GGB to a non-Western lay population and re-run t-tests. If divergence patterns persist, the "artificial moral compass" claim strengthens; if not, cultural bias may underlie the effect.