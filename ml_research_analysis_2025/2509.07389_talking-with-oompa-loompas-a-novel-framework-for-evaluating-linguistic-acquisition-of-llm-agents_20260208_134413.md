---
ver: rpa2
title: 'Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition
  of LLM agents'
arxiv_id: '2509.07389'
source_url: https://arxiv.org/abs/2509.07389
tags:
- lira
- sentence
- bani
- moko
- invalid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel framework to evaluate whether LLM
  agents can acquire a constructed language through pattern recognition and interactive
  feedback, analogous to human second-language learning. The experimental setup involves
  an LLM agent conversing with a bot that speaks only Tinkatongue, a synthetic language
  with strict syntactic rules, where the LLM receives feedback on the validity of
  its utterances.
---

# Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents

## Quick Facts
- **arXiv ID**: 2509.07389
- **Source URL**: https://arxiv.org/abs/2509.07389
- **Reference count**: 14
- **Primary result**: LLM agents show varying success acquiring a synthetic language through interactive feedback, with Claude-3.5-haiku achieving 0.34 turn validity rate versus 0.01-0.06 for other models.

## Executive Summary
This study introduces an interactive framework for evaluating LLM agents' ability to acquire a novel constructed language through pattern recognition and feedback, analogous to human second-language learning. The experimental setup involves an LLM conversing with a bot that speaks only Tinkatongue, a synthetic language with strict syntactic rules, where the LLM receives immediate feedback on utterance validity. Across ten trials, Claude-3.5-haiku significantly outperformed GPT-4o-mini and Gemini-2.5-flash in producing valid turns, though none achieved a full conversation within 100 responses. The results demonstrate that models can learn from feedback and employ human-like strategies such as imitation and systematic testing, but struggle with structural constraints like maintaining word adjacency between sentences.

## Method Summary
The framework tests LLM agents' ability to learn Tinkatongue through interactive conversation with a deterministic Oompa Loompa bot. The language consists of 100 valid sentences across 25 predefined conversations, with strict rules: three-word sentences using bisyllabic words, and adjacency constraints requiring shared words between consecutive sentences. Models (GPT-4o-mini, Gemini-2.5-flash, Claude-3.5-haiku) engage in up to 100 turns of conversation, receiving "koro" feedback for valid utterances and "moko lira bani" for invalid ones. Performance is measured through Turn Validity Rate (TVR), Feedback Responsiveness (FR), Adjacency Compliance (AC), and Time to First Valid Turn (TTFK). A secondary language Zingaloom with identical syntax but different vocabulary tests generalization.

## Key Results
- Claude-3.5-haiku achieved TVR of 0.34 versus 0.06 for Gemini-2.5-flash and 0.01 for GPT-4o-mini across 10 trials
- All models demonstrated perfect feedback responsiveness (FR = 1.0) but uniformly low adjacency compliance (AC ≈ 0.08-0.10)
- No model completed a full conversation within 100 turns, with average TTFK varying widely (Claude: 4.9 ± 7.8 turns)
- Qualitative analysis revealed models employed human-like learning strategies including imitation, babbling, and systematic combinatorial testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents can detect syntactic validity patterns from sparse positive/negative feedback signals within the conversational context window.
- Mechanism: The system prompt provides structural priors (bisyllabic words, three-word sentences, adjacency constraints), and the feedback tokens create labeled examples that the model can use for in-context pattern matching against the predefined 100-sentence vocabulary.
- Core assumption: Models possess latent pattern-recognition capabilities sufficient to infer membership in a finite language from partial observations, without weight updates.
- Evidence anchors: [abstract] "fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning" [section 3] "All models demonstrated high Feedback Responsiveness, recovering well from mistakes once valid sentences were identified" [corpus] Weak direct evidence; related work (Trans-EnV) evaluates linguistic robustness but does not address interactive acquisition of novel languages
- Break condition: If the context window overflows before sufficient labeled examples accumulate, or if the language size exceeds what can be sampled meaningfully within the turn limit, pattern inference degrades.

### Mechanism 2
- Claim: Immediate recovery from negative feedback operates through rejection of the prior hypothesis space rather than learned policy optimization.
- Mechanism: When receiving "moko lira bani," models abandon the invalid construction and sample alternative word combinations; FR=1.0 across all models indicates deterministic responsiveness to the error signal.
- Core assumption: The feedback token functions as a hard constraint that reliably shifts generation away from the failed hypothesis.
- Evidence anchors: [section 3] "All models exhibited perfect responsiveness to negative feedback (FR = 1.0, zero variance)" [section 3] "An immediately subsequent valid reply by the LLM agent is recorded as an immediate recovery" [corpus] No directly comparable mechanisms in neighbor papers; "Implicit Numerical Coordination" addresses multi-agent coordination but not feedback-driven language learning
- Break condition: If models begin to overfit to spurious patterns (e.g., associating "moko" with specific lexical items rather than structural invalidity), recovery may become inconsistent.

### Mechanism 3
- Claim: Human-like learning strategies (imitation, babbling, combinatorial testing) emerge from the model's prior training on diverse linguistic behaviors, not from task-specific optimization.
- Mechanism: Without explicit syntactic rules in the reduced system prompt, models default to exploratory generation—repeating observed words (imitation), generating short fragments (babbling), or systematically permuting known vocabulary (combinatorial testing).
- Core assumption: Pretraining exposure to language-learning corpora (e.g., educational materials, child-directed speech transcripts) induces transferable exploratory behaviors.
- Evidence anchors: [section 4] "LLM agents employed approaches similar to human language acquisition, such as babbling and imitation" [Table 2] Gemini-2.5-flash transcript shows progressive word repetition ("soro" → "soro soro" → "soro soro soro") consistent with babbling [corpus] "Capturing Human Cognitive Styles" addresses cognitive state inference but does not establish transfer from pretraining to novel language acquisition
- Break condition: If pretraining lacks sufficient coverage of meta-linguistic or language-learning discourse, exploratory strategies may not emerge; models may instead converge on repetitive failure modes.

## Foundational Learning

- Concept: **Formal Language Membership**
  - Why needed here: Tinkatongue is defined by enumeration (100 valid sentences, 25 valid conversations); understanding that validity is binary and table-lookup based is essential for interpreting results.
  - Quick check question: Can you explain why a sentence with valid words but incorrect word order would still be classified as invalid under this specification?

- Concept: **In-Context Learning**
  - Why needed here: The models receive no gradient updates; adaptation occurs entirely through accumulation of examples and feedback within the context window.
  - Quick check question: If you reset the conversation history between turns, what would you expect to happen to Turn Validity Rate?

- Concept: **Adjacency Constraints**
  - Why needed here: A core evaluation metric (AC) measures whether consecutive sentences share at least one word; this structural requirement proved hardest for models to internalize (AC ≈ 0.08 across all models).
  - Quick check question: Given "banu tira lomo" as the prior turn, which of these is a valid adjacency match: "kina sora lumo" or "lumo banu kina"?

## Architecture Onboarding

- Component map: LLM Agent -> Oompa Loompa Bot -> Tinkatongue/Zingaloom -> Evaluation Loop
- Critical path:
  1. Initialize conversation with Oompa Loompa's first utterance
  2. LLM generates response based on system prompt + history
  3. Validate response against L_sent; emit appropriate feedback
  4. If valid and not terminal, continue; if invalid, reset state
  5. Log metrics; repeat until completion or turn limit

- Design tradeoffs:
  - **Deterministic bot vs. stochastic evaluation**: Determinism aids reproducibility but may not reflect real-world language learning variability
  - **Enum-based validity vs. rule-based grammar**: Enumeration prevents LLMs from deriving generative rules; they must memorize specific sentences
  - **100-turn limit**: Sufficient to assess short-term adaptation but insufficient for sustained conversation acquisition (no model completed a full conversation)

- Failure signatures:
  - Low TVR with high FR: Models can correct after failure but cannot reliably generate valid sentences proactively
  - High variance in TTFK (e.g., Claude: 4.9 ± 7.8): Inconsistent time-to-first-valid suggests unstable hypothesis formation
  - Near-zero AC despite non-zero TVR: Valid sentences are produced without structural conversation awareness

- First 3 experiments:
  1. **Baseline replication**: Run 10 trials per model with default system prompt; verify TVR, FR, AC, TTFK distributions match reported values (Claude ≈ 0.34 TVR, others <0.07)
  2. **Ablate syntactic priors**: Use the reduced system prompt (no explicit rules); expect TVR to approach zero and babbling behavior to increase, as shown in Table 2
  3. **Cross-lexicon generalization**: Test all models on Zingaloom (identical syntax, different vocabulary); if TVR remains stable, confirms adaptation is syntax-driven rather than lexicon-dependent (Table 3 shows consistent performance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does relaxing specific syntactic constraints (e.g., bisyllabic words, fixed sentence length) impact the convergence rate of LLM language acquisition?
- Basis in paper: [Explicit] The authors explicitly state in Section 4 that they plan to conduct "ablation studies over the language parameters."
- Why unresolved: The current study only tests a single, highly artificial language structure, leaving the sensitivity of LLMs to different linguistic complexities unknown.
- Evidence: Performance metrics (TVR, AC) derived from experiments using languages with varied morphological and syntactic rules.

### Open Question 2
- Question: Why does perfect responsiveness to negative feedback fail to translate into sustained conversation-level structural understanding?
- Basis in paper: [Inferred] Results showed models recovered from errors immediately (FR=1.0) but failed to maintain adjacency constraints (Low AC).
- Why unresolved: This gap suggests models treat feedback as a signal to switch local tactics rather than evidence to update a global internal model of the language.
- Evidence: Analysis of internal states or attention patterns during multi-turn interactions to detect if rule representations are being encoded and retained.

### Open Question 3
- Question: Can larger frontier models achieve full conversation completion where smaller variants failed?
- Basis in paper: [Inferred] The study evaluated only efficiency-focused models (GPT-4o-mini, Gemini-2.5-flash, Claude-3.5-haiku), none of which completed the task within 100 turns.
- Why unresolved: It is undetermined if the failure is a fundamental limitation of the in-context learning approach or simply a lack of capacity in the smaller model variants.
- Evidence: Evaluation of state-of-the-art large models (e.g., Claude 3.5 Sonnet, GPT-4) on the identical Tinkatongue benchmark.

## Limitations
- The framework uses enumeration-based validity rather than explicit generative rules, limiting assessment of genuine syntactic learning versus pattern matching
- No model achieved full conversation completion within 100 turns, suggesting fundamental limitations in sustained language acquisition
- Deterministic bot and fixed conversation scripts limit generalizability to open-ended language learning scenarios

## Confidence
- **High Confidence**: Models can recover from negative feedback with perfect responsiveness (FR = 1.0), as this is directly measurable and consistent across trials. The comparative performance ranking (Claude > GPT-4o-mini > Gemini-2.5-flash) is also highly reliable due to the deterministic evaluation framework.
- **Medium Confidence**: The claim that models employ human-like learning strategies (imitation, babbling, combinatorial testing) is supported by qualitative analysis but lacks systematic quantification. These behaviors could arise from pretraining artifacts rather than genuine adaptive learning.
- **Low Confidence**: The assertion that interactive language acquisition is a promising evaluation paradigm for LLM adaptation overstates the case given that no model achieved full conversation completion and adjacency constraints remained poorly learned. The framework demonstrates feasibility but not practical utility.

## Next Checks
1. **Ablate the syntactic priors**: Re-run all trials with the reduced system prompt (no explicit rules) to verify whether TVR drops to near-zero and babbling behavior increases, confirming that explicit priors drive performance.
2. **Cross-lexicon stress test**: Evaluate models on 3-5 additional synthetic languages with identical syntax but novel vocabularies to determine whether performance is syntax-driven or lexicon-dependent.
3. **Adjacency constraint probing**: After each valid turn, require the model to explicitly state which word it intends to reuse in the next sentence. Compare intended vs. actual adjacency compliance to isolate structural vs. stochastic failures.