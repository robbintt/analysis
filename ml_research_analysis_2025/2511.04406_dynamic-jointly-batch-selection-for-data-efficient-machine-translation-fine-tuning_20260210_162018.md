---
ver: rpa2
title: Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning
arxiv_id: '2511.04406'
source_url: https://arxiv.org/abs/2511.04406
tags:
- english
- data
- score
- selection
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a batch selection method for improving machine
  translation fine-tuning efficiency. The approach uses a learnability score that
  combines learner model and pre-trained reference model assessments to prioritize
  data points that are both informative and not yet learned.
---

# Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning

## Quick Facts
- arXiv ID: 2511.04406
- Source URL: https://arxiv.org/abs/2511.04406
- Reference count: 36
- This paper introduces a batch selection method for improving machine translation fine-tuning efficiency, achieving up to 5x data efficiency improvement over iid training while maintaining comparable BLEU and COMET scores.

## Executive Summary
This paper introduces a dynamic jointly batch selection method for improving machine translation fine-tuning efficiency. The approach uses a learnability score that combines assessments from both a learner model and a pre-trained reference model to prioritize data points that are both informative and not yet learned. The method considers interdependencies among data points rather than individual samples, selecting batches based on similarity between source and target embeddings. Experiments on 12 language pairs using mBART models fine-tuned on CCMatrix data demonstrate significant improvements in data efficiency and computational efficiency, with better generalization and more stable loss curves, particularly in early training stages.

## Method Summary
The method employs a two-model framework consisting of a learner model (fine-tuning on target task data) and a reference model (pre-trained on general parallel corpora). A learnability score is computed for each data point using both models' predictions, combining the reference model's embedding similarity and the learner's current knowledge. Batches are selected by computing similarity matrices across sub-batches, ensuring diverse yet informative selections that capture interdependencies. The approach uses cosine similarity between source-target embeddings and incorporates both pre-trained and current model weights to balance exploration and exploitation. A key innovation is the caching mechanism that stores embeddings to reduce computational overhead during batch selection.

## Key Results
- Achieves up to 5x data efficiency improvement over iid training while maintaining comparable BLEU and COMET scores
- Demonstrates 24% computational efficiency gains when caching embeddings
- Shows better generalization with more stable loss curves, particularly in early training stages
- Requires fewer training samples to reach comparable performance levels

## Why This Works (Mechanism)
The method works by leveraging two complementary information sources: the reference model provides a stable baseline of what constitutes "good" translations, while the learner model tracks current knowledge gaps. By combining these through a learnability score, the selection process identifies data points that are both informative (high similarity to reference) and not yet learned (low learner confidence). The batch selection mechanism that considers pairwise similarities within sub-batches ensures diversity while maintaining coherence, preventing the selection of redundant or overly similar examples. This dual-perspective approach allows the model to focus on genuinely challenging examples that will improve generalization rather than simply difficult outliers.

## Foundational Learning
- **Embedding similarity metrics**: Cosine similarity between source and target embeddings is used to measure translation quality and inform batch selection. Why needed: Provides a differentiable measure of translation quality that can be computed efficiently. Quick check: Verify cosine similarity correlates with human translation quality judgments.
- **Learnability scoring**: Combines reference model and learner model assessments to prioritize data points. Why needed: Balances exploration of new knowledge with exploitation of known patterns. Quick check: Test different weighting schemes between reference and learner scores.
- **Batch interdependence**: Considers relationships between data points within batches rather than treating samples independently. Why needed: Prevents selection of redundant examples and ensures batch diversity. Quick check: Compare performance with independent vs. interdependent selection strategies.

## Architecture Onboarding

Component map: Data → Embedding extraction → Learnability scoring (Reference + Learner) → Similarity matrix computation → Batch selection → Model training

Critical path: Embedding extraction and learnability scoring are the computational bottlenecks, as they require processing the entire dataset to compute scores for each data point. The similarity matrix computation within sub-batches is also critical but scales quadratically with sub-batch size.

Design tradeoffs: The method trades computational overhead during batch selection for improved data efficiency and faster convergence. Using a pre-trained reference model provides stable guidance but may introduce bias toward the reference domain. The sub-batch approach balances computational feasibility with capturing interdependencies, though larger sub-batches may yield better results at the cost of memory.

Failure signatures: Early training instability when using lightweight reference models, potential bias toward reference model's translation style, and computational overhead that may negate benefits on small, high-quality datasets.

First experiments: 1) Test learnability score sensitivity to reference model choice (full vs. distilled models), 2) Measure computational overhead of embedding caching vs. on-the-fly computation, 3) Evaluate batch diversity metrics (e.g., pairwise similarity variance) across different selection strategies.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the early-training instability observed when using lightweight reference models (like Distiluse) be mitigated through dynamic weight adjustment between the learner and reference matrices?
- Basis in paper: [explicit] Appendix A notes that smaller models "exhibit slight instability at the beginning of training" and suggests this "may be mitigated by adjusting the weights."
- Why unresolved: The authors mention the possibility but do not implement or test dynamic weighting strategies in the current experiments.
- What evidence would resolve it: Experiments comparing fixed vs. time-dependent weighting schedules for the learnability score when using quantized or smaller reference models.

### Open Question 2
- Question: How does increasing the sub-batch chunk size beyond the memory-constrained limit of 32 samples impact the selection diversity and final translation quality?
- Basis in paper: [explicit] Section 4 states experiments used "sub-batch chunks of 32 samples due to memory limits, though larger sub-batches may improve results further."
- Why unresolved: Hardware constraints limited the experimental scope, leaving the potential benefits of capturing larger batch interdependencies unexplored.
- What evidence would resolve it: Ablation studies on high-memory hardware testing the correlation between chunk size (e.g., 64, 128, 256) and convergence speed or BLEU/COMET scores.

### Open Question 3
- Question: Does the computational overhead of calculating learnability scores negate efficiency benefits in scenarios involving small, meticulously curated datasets?
- Basis in paper: [explicit] Section 6 (Limitations) states the method "may not be optimal in scenarios where a fixed, small, and carefully curated dataset is available" due to utility calculation costs.
- Why unresolved: The study focuses on noisy/large datasets (CCMatrix), lacking analysis on the break-even point where selection cost outweighs data efficiency gains in clean data.
- What evidence would resolve it: Comparative analysis of FLOPS and wall-clock time on high-quality, low-noise datasets relative to the iid baseline.

## Limitations
- Performance on smaller pre-trained models remains untested, limiting generalizability
- No comparison with other batch selection strategies beyond iid training
- Computational overhead of calculating learnability scores across all data points is not fully characterized
- Embedding caching mechanism's memory requirements are not quantified

## Confidence
- High confidence in data efficiency improvements (5x gain claim)
- Medium confidence in computational efficiency gains (24% claim)
- Medium confidence in generalization improvements across all language pairs
- Low confidence in scalability to smaller model architectures

## Next Checks
1. Test the method on smaller pre-trained models (e.g., 200M-500M parameters) to evaluate scalability
2. Compare against alternative batch selection strategies like active learning or curriculum learning approaches
3. Measure the actual computational overhead and memory requirements of the learnability score calculations and embedding caching mechanism