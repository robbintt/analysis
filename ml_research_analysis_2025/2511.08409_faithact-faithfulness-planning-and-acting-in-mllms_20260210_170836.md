---
ver: rpa2
title: 'FaithAct: Faithfulness Planning and Acting in MLLMs'
arxiv_id: '2511.08409'
source_url: https://arxiv.org/abs/2511.08409
tags:
- reasoning
- faithfulness
- step
- faithact
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework for improving perceptual faithfulness\
  \ in multimodal reasoning by large language models (MLLMs). The authors define two\
  \ types of faithfulness\u2014behavioral and perceptual\u2014and focus on the latter,\
  \ ensuring reasoning steps are grounded in visual evidence."
---

# FaithAct: Faithfulness Planning and Acting in MLLMs

## Quick Facts
- arXiv ID: 2511.08409
- Source URL: https://arxiv.org/abs/2511.08409
- Reference count: 40
- Key outcome: Achieves up to 26% improvement in perceptual faithfulness for multimodal reasoning while maintaining task accuracy

## Executive Summary
FaithAct introduces a framework for improving perceptual faithfulness in multimodal reasoning by large language models. The authors distinguish between behavioral faithfulness (following predefined reasoning protocols) and perceptual faithfulness (ensuring reasoning steps are grounded in visual evidence). Their approach focuses on the latter through FAITHEVI, an evaluation pipeline that extracts claimed objects from reasoning steps, verifies their existence via polling and grounding, and computes faithfulness scores. FAITHACT operationalizes this evaluation through a planning-and-acting loop that enforces evidential verification before admitting reasoning steps. Experiments demonstrate significant faithfulness improvements without sacrificing task performance.

## Method Summary
The method introduces FAITHEVI, a pipeline for evaluating faithfulness in multimodal reasoning chains. It extracts claimed objects from reasoning steps, verifies their existence through polling and grounding mechanisms, and computes both step-level and chain-level faithfulness scores. FAITHACT then implements a planning-and-acting loop that enforces evidential verification, requiring each reasoning step to be visually grounded before being admitted to the reasoning chain. This approach systematically reduces hallucination by ensuring all perceptual claims have visual evidence support.

## Key Results
- Up to 26% improvement in perceptual faithfulness metrics on benchmark datasets
- Maintained task accuracy despite increased faithfulness requirements
- Demonstrated reduction in hallucination through qualitative analysis
- Ablation studies confirm the effectiveness of evidential verification mechanism

## Why This Works (Mechanism)
The framework works by creating a feedback loop between reasoning and visual verification. Each reasoning step must be supported by concrete visual evidence before being accepted, preventing the model from generating unsupported perceptual claims. The FAITHEVI evaluation pipeline provides objective metrics for faithfulness by cross-referencing claimed objects with actual visual content, creating a measurable standard for improvement.

## Foundational Learning

**Multimodal reasoning chains**: Sequential reasoning processes that combine visual perception with language understanding. Needed to understand how perceptual claims propagate through reasoning steps. Quick check: Can you trace a reasoning chain from visual input to final conclusion?

**Perceptual faithfulness**: The degree to which reasoning steps are grounded in actual visual evidence rather than hallucination. Critical for ensuring reliability in visual reasoning systems. Quick check: Identify which parts of a reasoning chain are supported by visual evidence versus inference.

**Visual grounding**: The process of associating textual claims with specific visual elements in an image. Essential for verifying perceptual claims. Quick check: Given an image and text claim, can you locate the supporting visual evidence?

**LLM-based object detection**: Using language models to identify and extract objects from visual reasoning steps. Needed for automated faithfulness evaluation. Quick check: Can the LLM accurately extract objects from complex reasoning chains?

## Architecture Onboarding

**Component map**: Visual input -> Object extraction (LLM) -> Polling mechanism -> Grounding verification -> Faithfulness scoring -> Planning loop -> Reasoning output

**Critical path**: The planning-and-acting loop where each proposed reasoning step must pass through the FAITHEVI verification pipeline before being admitted to the chain. This creates the feedback mechanism that enforces faithfulness.

**Design tradeoffs**: Computational overhead versus faithfulness improvement. The method requires significant additional processing for verification, but this cost is justified by the substantial faithfulness gains and maintained accuracy.

**Failure signatures**: 
- Steps that pass verification but contain subtle perceptual errors
- Objects that exist visually but are misidentified by the LLM
- Complex perceptual relationships that the polling mechanism cannot adequately capture

**3 first experiments**:
1. Test single-step reasoning faithfulness on simple visual questions
2. Evaluate chain-level faithfulness on multi-step reasoning problems
3. Compare behavioral versus perceptual faithfulness on the same reasoning chains

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Computational overhead from planning-and-acting loop may limit practical deployment
- Evaluation relies on LLM-based components that may introduce their own biases
- Benchmarking focuses on single-object existence claims, potentially missing complex perceptual errors

## Confidence

**Faithfulness evaluation framework**: Medium - FAITHEVI shows promise but relies on LLM-based components whose reliability varies
**Performance improvements**: High - Task accuracy preservation is well-demonstrated across benchmarks  
**Perceptual faithfulness gains**: Medium - The 26% improvement is methodologically sound but depends on the evaluation framework's validity

## Next Checks

1. Test FAITHACT's robustness on out-of-distribution visual reasoning tasks to assess generalization beyond benchmark scenarios
2. Compare FAITHEVI's perceptual faithfulness scores against human annotator judgments on the same reasoning chains
3. Evaluate the computational overhead and latency impact of the planning-and-acting loop in real-time reasoning scenarios