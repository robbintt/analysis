---
ver: rpa2
title: Realistic pedestrian-driver interaction modelling using multi-agent RL with
  human perceptual-motor constraints
arxiv_id: '2510.27383'
source_url: https://arxiv.org/abs/2510.27383
tags:
- pedestrian
- vehicle
- constraints
- interaction
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a multi-agent reinforcement learning (RL)\
  \ framework that models pedestrian-driver interactions under realistic perceptual\
  \ and motor constraints. Unlike prior approaches that treat road users as purely\
  \ rational agents or black-box predictors, this work explicitly integrates human\
  \ sensory limitations\u2014such as noisy visual perception and gaze-dependent acuity\u2014\
  and biomechanical motor constraints like ballistic walking and acceleration smoothing."
---

# Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints

## Quick Facts
- arXiv ID: 2510.27383
- Source URL: https://arxiv.org/abs/2510.27383
- Authors: Yueyang Wang; Mehmet Dogar; Gustav Markkula
- Reference count: 23
- Introduces multi-agent RL framework modeling pedestrian-driver interactions with explicit human perceptual-motor constraints

## Executive Summary
This study presents a novel multi-agent reinforcement learning framework that models pedestrian-driver interactions by explicitly incorporating human perceptual and motor constraints. Unlike traditional approaches that treat road users as purely rational agents or black-box predictors, this work integrates realistic limitations such as noisy visual perception, gaze-dependent acuity, ballistic walking patterns, and acceleration smoothing. Using real-world data from an unsignalised pedestrian crossing, the model demonstrates that combining both visual and motor constraints produces the most human-like interactions, outperforming simpler models and supervised behavioural cloning approaches.

## Method Summary
The researchers developed a multi-agent reinforcement learning framework where both pedestrian and driver agents interact within a shared environment. Four model variants were trained and evaluated: (1) no constraints, (2) motor constraints only, (3) visual constraints only, and (4) both constraints combined. Visual constraints incorporated noisy perception and gaze-dependent acuity, while motor constraints included ballistic walking and acceleration smoothing. The models were trained using proximal policy optimization and evaluated against real-world crossing data from an unsignalised pedestrian crossing. A supervised behavioural cloning model served as an additional baseline for comparison.

## Key Results
- The combined model with both visual and motor constraints best reproduced human-like interactions
- Models incorporating perceptual-motor constraints significantly outperformed the unconstrained model and behavioural cloning baseline
- The approach successfully captures realistic crossing behaviour and offers a framework for modeling population-level variability

## Why This Works (Mechanism)
The model's effectiveness stems from grounding artificial agents in human physiological and biomechanical realities. By constraining agents with actual human sensory limitations and movement patterns, the resulting interactions naturally emerge as more human-like. The multi-agent framework allows for emergent coordination patterns that mirror real-world pedestrian-driver negotiations, while the RL approach enables adaptive learning of optimal interaction strategies within these constraints.

## Foundational Learning
- **Noisy Visual Perception**: Models human sensory limitations in detecting and tracking objects, essential for creating realistic attention allocation and decision-making patterns
- **Gaze-Dependent Acuity**: Captures how visual precision varies with gaze direction, critical for modeling selective attention during crossing scenarios
- **Ballistic Walking**: Represents the inertia and momentum in human walking patterns, necessary for realistic pedestrian movement trajectories
- **Acceleration Smoothing**: Models the biomechanical constraints on how quickly humans can change speed, preventing unrealistic jerky movements
- **Multi-Agent RL Framework**: Enables emergent coordination between pedestrian and driver agents, fundamental for capturing interactive negotiation behaviours

## Architecture Onboarding

**Component Map**: Perception Module -> Decision Module -> Action Module -> Environment -> Reward Signal -> Value Network

**Critical Path**: Visual input → Noisy perception processing → Decision making → Motor constraint application → Action execution → Environment feedback → Policy update

**Design Tradeoffs**: The explicit modeling of perceptual-motor constraints increases computational complexity but yields more interpretable and generalizable results compared to pure data-driven approaches. The multi-agent setup requires careful reward shaping to ensure cooperative rather than adversarial interactions.

**Failure Signatures**: 
- Over-constrained models may produce overly conservative or unnatural behaviours
- Poor reward shaping can lead to agents gaming the system rather than learning realistic interactions
- Insufficient training data may result in agents not learning to handle edge cases or rare scenarios

**First Experiments**:
1. Compare crossing success rates between constrained and unconstrained models in controlled scenarios
2. Evaluate attention allocation patterns against eye-tracking data from human participants
3. Test model robustness to varying levels of sensory noise and motor constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Validation limited to unsignalised pedestrian crossing dataset, generalizability to other road environments untested
- Perceptual-motor constraints may not capture full complexity of human attention allocation during naturalistic interactions
- Multi-agent RL framework requires substantial computational resources, potentially limiting real-time deployment

## Confidence

High confidence in claims about combined visual and motor constraint model best reproducing human-like interactions.

Medium confidence in claims about modeling population-level variability and relative performance compared to supervised behavioural cloning.

## Next Checks

1. Test model performance across multiple crossing scenarios (signalised, roundabout, mid-block) to assess environmental generalizability

2. Conduct cross-population validation using datasets from different geographical regions and demographic groups

3. Compare against advanced imitation learning methods (e.g., transformer-based behavioral cloning) to establish relative performance more rigorously