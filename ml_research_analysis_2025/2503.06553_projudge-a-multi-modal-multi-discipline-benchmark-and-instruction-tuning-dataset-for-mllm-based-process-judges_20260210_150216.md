---
ver: rpa2
title: 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning
  Dataset for MLLM-based Process Judges'
arxiv_id: '2503.06553'
source_url: https://arxiv.org/abs/2503.06553
tags:
- error
- process
- reasoning
- errors
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProJudgeBench, the first comprehensive benchmark
  designed to evaluate the abilities of multi-modal large language models (MLLMs)
  as process judges for scientific problem-solving. The benchmark includes 2,400 test
  cases with 50,118 step-level annotations spanning four scientific disciplines and
  varying difficulty levels.
---

# ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges

## Quick Facts
- arXiv ID: 2503.06553
- Source URL: https://arxiv.org/abs/2503.06553
- Reference count: 40
- Primary result: Introduces ProJudgeBench with 2,400 test cases spanning 4 disciplines and 50,118 step-level annotations for evaluating MLLM process judgment capabilities

## Executive Summary
This paper introduces ProJudgeBench, the first comprehensive benchmark designed to evaluate the abilities of multi-modal large language models (MLLMs) as process judges for scientific problem-solving. The benchmark includes 2,400 test cases with 50,118 step-level annotations spanning four scientific disciplines and varying difficulty levels. Each step is annotated by human experts for correctness, error type, and explanation, enabling fine-grained evaluation of error detection, classification, and diagnosis capabilities.

To address the performance gap between open-source and proprietary models, the authors propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy. The dataset is constructed using two pathways: controlled error injection and realistic error collection, ensuring both diversity and real-world relevance. The fine-tuning strategy encourages models to explicitly reason through problem-solving steps before assessing solutions, mimicking human expert behavior.

## Method Summary
The method involves constructing ProJudge-173k through two complementary pathways: controlled error injection using GPT-4o to generate synthetic errors in correct solutions, and realistic error collection by annotating actual model-generated solutions. The Dynamic Dual-Phase fine-tuning strategy alternates between direct evaluation (model evaluates student solutions directly) and synthesize-then-evaluate (model generates reference solution first, then evaluates). Models are fine-tuned using LoRA for one epoch on 8Ã—H100 GPUs with specific hyperparameters for different model sizes. Evaluation uses ProJudgeBench to assess step correctness accuracy and error type classification across seven error categories.

## Key Results
- GPT-4o and Gemini-2.0 series achieve highest process evaluation accuracy
- Fine-tuned open-source models (InternVL2.5, Qwen2.5-VL) achieve step correctness accuracy up to 84.5%
- Visual interpretation and calculation errors are most challenging for models
- Models perform better in biology/chemistry than mathematics/physics
- Performance degrades significantly on competition-level vs K12 problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly generating reference solution before evaluation improves accuracy and generalization
- Mechanism: Dynamic Dual-Phase strategy trains models to first synthesize correct solution step-by-step, then evaluate student solution against this reference
- Core assumption: Models that construct correct reasoning paths develop better representations for detecting deviations
- Evidence anchors: DDP improves OlympicArena performance by 3.36-4.79% over standard fine-tuning; abstract describes DDP encouraging explicit reasoning
- Break condition: If model cannot reliably solve correctly itself, synthesized reference may misguide evaluation

### Mechanism 2
- Claim: Training on diverse error types with explicit categorization improves fine-grained classification
- Mechanism: ProJudge-173k provides 7 error-type labels with explanations, enabling models to distinguish error patterns beyond binary correctness
- Core assumption: Error taxonomies transfer across disciplines when sufficiently comprehensive
- Evidence anchors: Fine-tuned InternVL2.5-8B improves error type classification from 6.77% to 45.39% overall; seven error types defined from thorough analysis
- Break condition: Novel error types outside training distribution may be misclassified

### Mechanism 3
- Claim: Combining controlled error injection with realistic error collection balances annotation quality and ecological validity
- Mechanism: Path 1 injects known errors into correct solutions (high accuracy, lower realism); Path 2 collects actual model solutions with annotation (higher realism, depends on annotator quality)
- Core assumption: Synthetic errors from capable models approximate realistic error patterns for transfer
- Evidence anchors: Two complementary paths described with filtering for format consistency, annotation consistency, and error coverage
- Break condition: If synthetic errors systematically differ from naturally-occurring model errors, models may overfit to injection patterns

## Foundational Learning

- Concept: **Process Reward Modeling (PRM) vs. Outcome Reward Modeling (ORM)**
  - Why needed here: ProJudge operates at step-level granularity rather than final-answer evaluation
  - Quick check question: Can you explain why a model might get the right answer with flawed reasoning, and how step-level evaluation catches this?

- Concept: **Instruction Tuning for Classification Tasks**
  - Why needed here: Model must output structured tuples (step, correctness, error_type, explanation) rather than free-form text
  - Quick check question: How would you format training data to teach a model to output Python-list structured predictions?

- Concept: **Cross-Modal Error Detection**
  - Why needed here: Visual interpretation errors require tracing reasoning back to image content, not just text logic
 - Quick check question: If a step claims "the graph shows a peak at x=5" but the image shows x=3, what error type is this and why might models miss it?

## Architecture Onboarding

- Component map: Input processor -> Phase 1 head (Direct Evaluate) -> Phase 2 solver (Generate reference) -> Dynamic router -> Output formatter

- Critical path: 1. Data preparation: Ensure solutions pre-segmented into steps (Qwen2.5-72B-Instruct used for splitting) 2. LoRA fine-tuning: One epoch, bf16 precision, cosine LR scheduler 3. Inference: Use DDP-style prompt that first asks model to solve, then evaluate

- Design tradeoffs: Smaller models (3B-8B) with fine-tuning can match/exceed larger untuned models (72B), but require dataset access; Two-path data construction trades perfect annotation accuracy (Path 1) for realism (Path 2); DDP adds inference compute cost but improves generalization

- Failure signatures: Models struggle most with visual interpretation errors (accuracy as low as 5.31%); Performance degrades significantly on competition-level vs K12 problems; Smaller models fail to evaluate larger models' complex reasoning (MiniCPM-V-2 6 scores 15.69% on GPT-4o solutions)

- First 3 experiments: 1. Baseline: Evaluate model on ProJudgeBench without fine-tuning to establish starting accuracy 2. Fine-tune with standard approach: Train on ProJudge-173k with Direct Evaluate phase only, measure gain on OlympicArena subset 3. Add DDP: Compare standard fine-tuning vs DDP-enhanced, focusing on in-domain vs OOD performance gap

## Open Questions the Paper Calls Out
None

## Limitations
- Models struggle most with visual interpretation errors, with accuracy as low as 5.31% for certain models
- Smaller models cannot effectively judge larger models' complex reasoning (MiniCPM-V-2 6 achieves only 15.69% on GPT-4o solutions)
- Performance gap between proprietary and open-source models remains substantial despite fine-tuning improvements

## Confidence
- Methodological design and benchmark construction: **High**
- Dual-path dataset construction and DDP strategy: **High**
- Unspecified dynamic alternation probability between training phases: **Medium**
- Incomplete LoRA configuration details: **Medium**
- Systematic limitations in visual error detection and cross-model evaluation: **High**

## Next Checks
1. Systematically vary the probability p of alternating between Phase 1 and Phase 2 during training to identify optimal balance between direct evaluation and reference synthesis

2. Test fine-tuned models on error detection tasks outside original four disciplines (e.g., earth science or engineering) to validate taxonomy generalizability

3. Construct targeted test cases emphasizing visual reasoning components to measure specific improvements in visual error detection after fine-tuning