---
ver: rpa2
title: Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning
arxiv_id: '2601.11401'
source_url: https://arxiv.org/abs/2601.11401
tags:
- graph
- value
- learning
- local
- da2c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a diffusion value function (DVF) tailored to
  graph-based multi-agent Markov decision processes (GMDPs). The DVF assigns to each
  agent a value component that decays smoothly with both temporal and spatial distance
  on the influence graph, enabling stable and scalable credit assignment.
---

# Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.11401
- Source URL: https://arxiv.org/abs/2601.11401
- Reference count: 40
- Primary result: Diffusion Advantage Actor-Critic (DA2C) with Diffusion Value Function (DVF) outperforms local and global critic baselines by up to 11% average reward across three graph-based MARL tasks.

## Executive Summary
This paper introduces a diffusion-based value function tailored for graph-based multi-agent reinforcement learning (GMDPs). The method assigns each agent a value component that decays smoothly with both temporal and spatial distance on the influence graph, enabling stable and scalable credit assignment. Implemented in a diffusion advantage actor-critic (DA2C) algorithm, the approach replaces standard global critics with DVF-based components and introduces a learned DropEdge graph neural network (LD-GNN) actor for sparse message-passing policies. Across three domains‚Äîfirefighting, vector graph coloring, and transmit power optimization‚ÄîDA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%. The DVF is shown to be well-defined in infinite-horizon settings, admits a Bellman fixed point, and decomposes the global value via an averaging property.

## Method Summary
The method proposes Diffusion Advantage Actor-Critic (DA2C), which uses a Diffusion Value Function (DVF) critic and a Learned DropEdge GNN (LD-GNN) actor. The DVF assigns each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation, using a diffusion operator Œì = Œ≥AD‚Åª¬π where A is the adjacency matrix and D‚Åª¬π is the inverse degree matrix. The critic is updated via semi-gradient TD learning, while the actor uses policy gradients with diffusion advantages. The LD-GNN actor samples edge activations for sparse communication, with a message-passing bonus that anneals to zero. The approach is evaluated on three GMDP tasks with centralized training and decentralized execution.

## Key Results
- DA2C improves average reward by up to 11% compared to local and global critic baselines
- DVF-based critic remains bounded and stable on large graphs where local values diverge
- LD-GNN actor with sparse communication achieves comparable performance to dense message passing
- Performance gains are most pronounced on sparse Erd≈ës-R√©nyi graphs, with smaller advantages on hub-dominated Barab√°si-Albert graphs
- The method demonstrates strong generalization across graph sizes and topologies

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-Based Credit Assignment
The diffusion operator Œì = Œ≥AD‚Åª¬π weights each agent's value component by rewards from reachable neighbors, where influence decays with both temporal discount Œ≥ and graph distance. This replaces binary inclusion/exclusion in local value functions with smooth decay, yielding stable, agent-specific learning signals. The core assumption is that dynamics exhibit locality‚Äîactions predominantly influence nearby agents within short graph distances. Break condition: in fully-connected graphs, Œì reduces to uniform propagation and DVF converges to global value, eliminating spatial structure advantage.

### Mechanism 2: Contraction Mapping for Stable Bellman Updates
Since ||Œì||‚ÇÅ = max_j Œ£_i Œì_{ij} = Œ≥ < 1, the diffusion Bellman operator T^œÄ is a contraction satisfying ||T^œÄV - T^œÄW||_‚àû ‚â§ Œ≥||V - W||_‚àû. This guarantees unique fixed-point convergence, unlike local values which can diverge on growing graphs. Core assumption: rewards are bounded and Œ≥ ‚àà (0,1). Break condition: if the influence graph's maximum out-degree is unbounded or rewards are unbounded, contraction property may fail.

### Mechanism 3: Value Decomposition via Averaging
Agent-wise DVF components average to recover global value, ensuring alignment with the cooperative objective. By Proposition 3.3, n‚Åª¬πùüô^T V_D(S) = V(S). The diffusion operator satisfies ùüô^T AD‚Åª¬π = ùüô^T (column-stochastic), preserving the reward average across propagation steps. Core assumption: GMDP reward factorization holds: r_t = n‚Åª¬π Œ£_i R_i^t. Break condition: if reward factorization is violated (non-local rewards), averaging property breaks and components may not align with global objective.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message passing**: DVF estimation uses GNNs to compute agent-wise value components from neighborhood information; LD-GNN actor uses sparse message passing for communication decisions. Quick check: Can you explain how a 2-layer GNN's receptive field relates to graph distance, and why this aligns with DVF's spatial decay?

- **Actor-Critic methods and TD learning**: DA2C combines DVF critic with policy gradient; understanding TD error Œ¥_t = Œì(R_t + V_œÜ(S_{t+1})) - V_œÜ(S_t) is essential. Quick check: What is the semi-gradient update for critic parameters, and why do we stop gradients through the bootstrap target?

- **Graph-based MDPs and influence graphs**: The entire framework assumes GMDP structure where transitions and rewards factorize according to the influence graph. Quick check: Given a GMDP with adjacency matrix A, what does edge (i,j) ‚àà E imply about agent i's influence on agent j's rewards?

## Architecture Onboarding

- **Component map**: Environment -> DVF Critic (GNN) -> LD-GNN Actor -> Policy Gradient Update; TD Error -> Critic Update

- **Critical path**: 
  1. Environment provides (S_t, R_t, S_{t+1}) where R_t ‚àà ‚Ñù‚Åø is the vector of local rewards
  2. Critic forward pass: V_D(S) through GNN with m-hop receptive field
  3. Compute Œ¥_t = Œì(R_t + sg[V_D(S_{t+1})]) - V_D(S_t) where Œì requires adjacency matrix and in-degree
  4. Actor samples actions A_t ~ œÄ_Œ∏(¬∑|œÑ_t) and edges C_t via Œª_Œ∏
  5. Update œï ‚Üê œï - c_v ‚àá_œï(n‚Åª¬π||Œ¥_t||¬≤)
  6. Update Œ∏ ‚Üê Œ∏ + c_j ‚àá_Œ∏ J(Œ∏) using diffusion advantage components

- **Design tradeoffs**: Spatial decay via Œì improves scalability but attenuates long-range coordination signals; assumes static, known influence graph‚Äîtime-varying structure requires extensions; GNN depth controls receptive field; BA graphs show smaller DA2C advantage due to hub-mediated rapid propagation reducing locality.

- **Failure signatures**: Dense influence graphs: MAA2C approaches DA2C performance; Divergent local values: if exploding value estimates on large graphs, local critic (NA2C) may be failing while DVF remains bounded; Hub dominance: in BA graphs, gradients may concentrate on hub nodes; Edge sparsity collapse: if Œª_Œ∏ ‚Üí 0 (no communication), verify message-passing bonus annealing schedule.

- **First 3 experiments**:
  1. Baseline comparison on firefighting: Implement IA2C, NA2C, MAA2C, and DA2C with identical actor architectures; report average fire level over 5 seeds and 100 instances. Expect: DA2C < NA2C < MAA2C (lower is better).
  2. Ablation of spatial discount: Vary Œ≥ while keeping message penalty fixed; plot performance vs. Œ≥ to verify that intermediate values balance local/global credit assignment optimally.
  3. OOD generalization test: Train on 5,000-node ER graphs, test on 10,000-node ER and 1,000-node BA graphs. Expect: DA2C maintains advantage across both distributions, with narrower performance gap on BA.

## Open Questions the Paper Calls Out
1. Can the Diffusion Value Function (DVF) be effectively trained in a fully decentralised manner using only local neighbour communication? (Basis: authors state this suggests a path toward decentralised training but leave it to future work)
2. Can DVF-based methods be extended to environments with time-varying or partially observed influence graphs? (Basis: identified as necessary step to increase applicability to realistic large-scale systems)
3. Can the DVF framework be applied to cooperative MARL benchmarks where the influence graph is not explicitly provided? (Basis: extending to standard benchmarks like StarCraft would require learning or approximating the underlying influence structure)

## Limitations
- Spatial attenuation assumption (Œ≥ < 1) may be too restrictive for domains with dense influence graphs where long-range coordination is essential
- Method assumes a static influence graph, limiting applicability to dynamic topologies
- Experimental validation focuses on cooperative tasks with factored rewards; extension to competitive or mixed-motive scenarios remains untested

## Confidence
- **High Confidence**: Diffusion Bellman operator is a contraction (Proposition 3.2), DVF decomposition correctly recovers global value (Proposition 3.3), and empirical performance gains on benchmark tasks are reproducible with provided hyperparameters
- **Medium Confidence**: Scalability advantage over local critics on large graphs is demonstrated, but dependence on graph topology (ER vs. BA) suggests benefits are context-dependent
- **Low Confidence**: Generalization to unseen graph sizes and topologies beyond reported distributions (ER, BA, RG) has not been exhaustively tested

## Next Checks
1. **Density Breakpoint Test**: Systematically vary graph density (p in ER graphs) and measure performance gap between DA2C and NA2C to identify threshold where spatial structure ceases to provide advantage
2. **Time-Varying Topology Experiment**: Introduce dynamic influence graphs (edge addition/removal) and evaluate whether DA2C's critic can adapt without retraining, or if GNN can learn to handle graph changes
3. **Non-Factored Reward Extension**: Modify vector graph coloring task to include non-local rewards (global penalty for monochromatic subgraphs) and verify whether DVF components still align with global objective through averaging