---
ver: rpa2
title: Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance
  Analysis
arxiv_id: '2509.23755'
source_url: https://arxiv.org/abs/2509.23755
tags:
- speech
- textual
- importance
- parameter
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The integration of speech capabilities into large language models\
  \ often degrades their original textual competence, limiting their effectiveness\
  \ in spoken question answering tasks. This study identifies that speech fine-tuning\
  \ disrupts the layer-wise distribution of parameters critical to textual reasoning\u2014\
  a phenomenon termed \"textual importance distribution shift.\" To address this,\
  \ the authors propose two strategies: layer-wise learning rate scheduling, which\
  \ preserves important layers by reducing their update magnitude, and Low-Rank Adaptation\
  \ (LoRA), which constrains updates within a low-rank subspace aligned with the model's\
  \ inherent knowledge structure."
---

# Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis

## Quick Facts
- **arXiv ID**: 2509.23755
- **Source URL**: https://arxiv.org/abs/2509.23755
- **Reference count**: 0
- **Primary result**: Speech fine-tuning disrupts layer-wise parameter distributions critical to textual reasoning, addressed by layer-wise learning rate scheduling and LoRA

## Executive Summary
This study addresses a critical challenge in integrating speech capabilities into large language models: fine-tuning for speech comprehension often degrades the model's original textual competence. Through parameter importance analysis using SHAP scores, the authors identify a "textual importance distribution shift" phenomenon where speech fine-tuning disrupts the layer-wise distribution of parameters essential for textual reasoning. They propose two solutions—layer-wise learning rate scheduling that reduces updates to important layers, and LoRA which constrains updates within a low-rank subspace aligned with the model's knowledge structure. Experimental results on spoken QA benchmarks demonstrate that both methods better maintain textual competence while improving speech performance, with LoRA achieving the highest spoken QA accuracy (42.9% on Web Questions for the 8B model).

## Method Summary
The authors analyze parameter importance using SHAP scores on textual reasoning datasets (MMLU, BBH, GLUE) to identify layers critical for maintaining textual competence. They then apply two strategies during speech fine-tuning: (1) layer-wise learning rate scheduling that applies smaller learning rates to important layers to preserve their textual knowledge, and (2) LoRA which constrains parameter updates to a low-rank subspace, limiting disruption to the model's foundational knowledge structure. The methods are evaluated on spoken QA benchmarks including Web Questions and Spoken SQuAD, measuring both speech comprehension accuracy and preservation of textual reasoning performance.

## Key Results
- Speech fine-tuning causes "textual importance distribution shift," disrupting layer-wise parameter distributions critical for textual reasoning
- LoRA achieves highest spoken QA accuracy (42.9% on Web Questions for 8B model) while maintaining better textual competence than full fine-tuning
- Layer-wise learning rate scheduling better preserves textual accuracy compared to LoRA while still improving speech comprehension

## Why This Works (Mechanism)
The degradation occurs because speech fine-tuning indiscriminately updates all model parameters, including those critical for textual reasoning. The parameter importance analysis reveals that certain layers hold more weight for textual tasks than others, and when these layers are updated during speech adaptation, the model loses its foundational textual competence. By either reducing the learning rate for important layers or constraining updates through LoRA's low-rank decomposition, the model can adapt to speech input while preserving the parameter configurations essential for textual reasoning tasks.

## Foundational Learning

**Parameter Importance Analysis**
- *Why needed*: To identify which layers and parameters are critical for maintaining textual reasoning capabilities during speech adaptation
- *Quick check*: Verify SHAP scores correctly rank layers by their contribution to textual task performance

**Layer-wise Learning Rate Scheduling**
- *Why needed*: To selectively protect important parameters from being overwritten during fine-tuning while allowing less critical parameters to adapt to speech
- *Quick check*: Monitor training curves to ensure important layers maintain stable loss while speech-adapted layers improve

**Low-Rank Adaptation (LoRA)**
- *Why needed*: To constrain parameter updates within a low-rank subspace, limiting disruption to the model's foundational knowledge structure
- *Quick check*: Compare rank values to find optimal balance between adaptation capacity and knowledge preservation

## Architecture Onboarding

**Component Map**
Speech Input -> Speech Encoder -> LoRA/Layer-wise Scheduling -> LLM Backbone -> Textual/QA Output

**Critical Path**
Speech signal → feature extraction → parameter importance-preserved LLM → answer generation

**Design Tradeoffs**
LoRA vs Layer-wise LR: LoRA achieves better speech accuracy but layer-wise scheduling better preserves textual performance; choice depends on application priorities

**Failure Signatures**
Over-aggressive fine-tuning wipes out textual competence; under-adaptation fails to capture speech patterns; improper rank selection in LoRA limits adaptation capacity

**First Experiments**
1. Run parameter importance analysis on MMLU/BBH/GLUE to establish baseline layer criticality rankings
2. Compare full fine-tuning vs layer-wise scheduling vs LoRA on a small spoken QA subset
3. Ablate different rank values in LoRA to find optimal balance between adaptation and preservation

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Analysis relies on specific textual reasoning benchmarks (MMLU, BBH, GLUE) that may not capture all types of textual knowledge
- The trade-off between LoRA (better speech accuracy) and layer-wise scheduling (better text preservation) suggests neither fully solves the degradation problem
- Experiments primarily use 8B parameter models, limiting generalizability to different model scales

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Textual competence degradation occurs during speech fine-tuning | High |
| Parameter importance distribution shift is the mechanism | Medium |
| Layer-wise scheduling and LoRA effectively mitigate degradation | Medium |

## Next Checks
1. Test parameter importance analysis framework on additional textual reasoning benchmarks beyond MMLU, BBH, and GLUE
2. Evaluate both methods on smaller (1-3B) and larger (30B+) parameter models to assess scalability
3. Conduct ablation studies varying layer-wise learning rate ratios and LoRA rank values to optimize hyperparameters