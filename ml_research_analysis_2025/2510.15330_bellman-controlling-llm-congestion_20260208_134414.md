---
ver: rpa2
title: 'BeLLMan: Controlling LLM Congestion'
arxiv_id: '2510.15330'
source_url: https://arxiv.org/abs/2510.15330
tags:
- congestion
- latency
- output
- https
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language model (LLM) applications currently lack visibility
  into the infrastructure underneath and generate tokens autoregressively without
  regard to system load, risking inferencing latency inflation and poor user experience.
  This work proposes beLLMan, a first-party LLM serving system component that enables
  the LLM infrastructure to actively signal the application to adjust output length
  in response to changing system load.
---

# BeLLMan: Controlling LLM Congestion

## Quick Facts
- arXiv ID: 2510.15330
- Source URL: https://arxiv.org/abs/2510.15330
- Reference count: 40
- Key outcome: beLLMan enables LLM infrastructure to signal applications to adjust output length, achieving up to 8× lower end-to-end latency and 25% energy reduction while serving 19% more requests during congestion.

## Executive Summary
Large language model applications currently lack visibility into the infrastructure underneath and generate tokens autoregressively without regard to system load, risking inferencing latency inflation and poor user experience. This work proposes beLLMan, a first-party LLM serving system component that enables the LLM infrastructure to actively signal the application to adjust output length in response to changing system load. On a real testbed with H100 GPUs, beLLMan helps keep inferencing latency under control (up to 8× lower end-to-end latency) and reduces energy consumption by 25% (while serving 19% more requests) during periods of congestion for a summarization workload.

## Method Summary
beLLMan is implemented as a component within the vLLM serving stack that monitors Time Between Tokens (TBT) latency to detect congestion. When TBT exceeds a threshold, the system uses a Longformer-based predictor to estimate the default output length and applies a reduction rate (5-20%) to calculate a target length. The controller then appends a prompt instruction like "summarize in exactly N words" to incoming requests before scheduling them for inference. The approach was tested using Gemma-3 (27B) on 8 H100 GPUs with a summarization workload based on 100 ACM IMC papers.

## Key Results
- Reduced end-to-end latency by up to 8× during congestion periods
- Decreased energy consumption by 25% while increasing request throughput by 19%
- Maintained output quality with similarity scores above 87% using LLM-as-a-judge evaluation

## Why This Works (Mechanism)

### Mechanism 1
Constraining output length via prompt engineering reduces end-to-end latency during system overload. The system monitors Time Between Tokens (TBT), and when TBT rises indicating congestion, the controller appends instructions like "summarize in exactly N words" to queued prompts. This truncates the autoregressive generation loop, reducing total FLOPs and KV cache pressure per request. Core assumption: The LLM has sufficient instruction-following capability to adhere to length constraints without catastrophic quality degradation. Evidence anchors: [abstract] mentions keeping inferencing latency under control (up to 8× lower); [section 3] describes TBT inflation as the trigger and prompt appending as the action. Break condition: If the model ignores the "exactly N words" constraint or if the task requires exhaustive detail, latency gains may negate utility losses.

### Mechanism 2
Predicting unbounded output length allows precise, fine-grained adjustment of token reduction rates. Before the request hits the critical path, a predictor (Longformer encoder + FFN) estimates the default length. The system computes N = L × (1 - r), ensuring the shortened output is proportional to the requested content. Core assumption: Prediction accuracy is sufficient to avoid overshooting or undershooting. Evidence anchors: [section 2.2] details the predictor architecture and calculation of reduction rate r (5-20%); [section 4] discusses "Prediction-less design" as an alternative. Break condition: If predictor MAE is high for a specific domain, N may be set too low, causing the LLM to hallucinate to fill space or cut critical info.

### Mechanism 3
Moving average of Time Between Tokens (TBT) serves as a stable, early signal for congestion onset. Unlike E2E latency (which is delayed) or queue length (which fills up fast), TBT inflation reflects real-time decode step saturation. A 5-second moving average prevents overreaction to transient spikes. Core assumption: TBT inflation correlates directly with system load and inferencing bottlenecks. Evidence anchors: [section 3] explicitly states "TBT latency inflation occurs sooner... and hence could offer early signals"; [section 3] "We pick the median TBT (T₁) during the unbounded run as the threshold." Break condition: If diverse prompt lengths create variable decode times naturally, the moving average might mask genuine congestion or trigger false alarms.

## Foundational Learning

- Concept: **Autoregressive Decoding Bottleneck**
  - Why needed here: Understanding that generation is sequential and memory-bound explains why reducing token count directly lowers latency and energy.
  - Quick check question: Why does generating 10% fewer tokens save more than 10% latency in a memory-bound system? (Hint: KV cache management).

- Concept: **Network Congestion Control Analogues**
  - Why needed here: The paper models beLLMan on TCP/IP congestion control. Understanding concepts like "signals" (packet loss vs. TBT) and "control knobs" (window size vs. word count) is essential for grasping the feedback loop.
  - Quick check question: How does beLLMan's "reduction rate r" mirror TCP's "congestion window"?

- Concept: **LLM-as-a-Judge (Evaluation)**
  - Why needed here: Standard metrics (ROUGE) fail to capture semantic preservation during compression. The authors use an LLM (o3) to score "information loss" to validate the quality tradeoff.
  - Quick check question: Why is lexical overlap insufficient for evaluating the quality of a shortened summary?

## Architecture Onboarding

- Component map: vLLM Scheduler -> beLLMan Controller -> Length Predictor -> Prompt Appender
- Critical path: 1. Monitor TBT -> 2. Detect Threshold Breach -> 3. Run Length Prediction -> 4. Calculate N -> 5. Append Prompt -> 6. Inference
- Design tradeoffs:
  - Latency vs. Quality: Aggressive reduction (high r) saves energy/latency but risks information loss (similarity score drops).
  - Prediction Overhead: Running the predictor adds latency; must be hidden within the queueing time.
- Failure signatures:
  - Oscillation: TBT fluctuates rapidly around the threshold (T₁), causing r to toggle and unstable user experiences.
  - Quality Collapse: Similarity scores drop significantly (e.g., < 70%) because the predictor underestimated L, forcing N too low.
- First 3 experiments:
  1. Threshold Calibration: Run a "normal" workload to establish baseline median (T₁) and 75th percentile (T₂) TBT values. Do not tune controller without this data.
  2. Adherence Testing: Verify if your specific model version follows "exactly N words" instructions using the prompt variations in Fig 2b.
  3. Stress Testing: Apply the Poisson trace (Section 3) to verify if TBT moving average logic prevents false positives during the ramp-up phase.

## Open Questions the Paper Calls Out

### Open Question 1
Can beLLMan effectively generalize to non-summarization workloads like code generation or complex reasoning? Basis in paper: [explicit] The authors note the current study focuses on summarization and explicitly state that coding tasks may be impractical for compression, while speculating that synthesis or conversation might benefit. Why unresolved: The evaluation is limited to document summarization; it is unknown if length reduction degrades the logical integrity or safety of code/reasoning tasks. What evidence would resolve it: Benchmarks measuring functional correctness (e.g., HumanEval) and latency under load when the controller is active.

### Open Question 2
Do advanced control algorithms like Model Predictive Control (MPC) or BBR outperform the simple linear controller? Basis in paper: [explicit] Section 4 suggests an MPC controller might predict load better to avoid oscillations, and a BBR-style controller could optimize the operating point. Why unresolved: The paper implements only a "first-cut" reactive linear controller; sophisticated strategies are proposed but not tested. What evidence would resolve it: Comparative simulations or testbed experiments measuring oscillation frequency and throughput stability using MPC or BBR implementations.

### Open Question 3
Are input token rates or GPU utilization metrics more effective congestion signals than Time Between Tokens (TBT)? Basis in paper: [explicit] Section 4 discusses the trade-offs of different signals, suggesting input tokens per unit time or GPU utilization could offer earlier warnings than TBT. Why unresolved: TBT was selected for the prototype, but the authors acknowledge it may lag behind actual infrastructure load changes. What evidence would resolve it: Profiling the correlation latency between load spikes and signal detection for various metrics under identical trace workloads.

## Limitations

- Limited empirical validation scope: The 8× latency reduction and 25% energy savings claims are based on a single summarization workload and synthetic Poisson trace, with unknown generalizability to other tasks or real-world traffic patterns.
- Infrastructure dependency: Performance metrics are tightly coupled to the specific setup (8 H100 GPUs, Gemma-3 27B), and results may degrade significantly on different hardware or with smaller models.
- Quality degradation risk: While similarity scores >87% are claimed, the LLM-as-a-judge metric lacks extensive validation against human judgment, and aggressive length reduction could compromise output utility for critical tasks.

## Confidence

- **High Confidence:** The fundamental mechanism (appending "in exactly N words" to prompts reduces token count and therefore latency/energy) is well-supported by autoregressive LLM theory and controlled experiments.
- **Medium Confidence:** The specific performance numbers (8× latency, 25% energy) are likely reproducible on the exact same infrastructure and workload, but confidence is lower for claims about other tasks or load patterns.
- **Low Confidence:** The long-term stability under continuous unpredictable load and robustness of quality metrics to adversarial prompts or highly technical content are not sufficiently demonstrated.

## Next Checks

1. **Cross-Domain Prediction Accuracy:** Evaluate the predictor's MAE and resulting output quality on diverse tasks (code generation, dialogue, Q&A) not represented in the ICLR paper training corpus to validate generalizability.

2. **Infrastructure Portability Test:** Reproduce the core experiment on different GPU architectures (A100 or L4) and with smaller models (Gemma-2 9B) to measure if the same reduction rates produce comparable latency and energy savings without significant quality loss.

3. **Robustness to Real-World Traffic:** Replace the synthetic Poisson trace with production LLM API traffic to test if the beLLMan controller avoids oscillation and maintains quality under realistic, bursty, and long-tailed request patterns.