---
ver: rpa2
title: 'Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable
  Latent Planning'
arxiv_id: '2601.09708'
source_url: https://arxiv.org/abs/2601.09708
tags:
- reasoning
- arxiv
- latent
- planning
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fast-ThinkAct introduces efficient vision-language-action reasoning
  through verbalizable latent planning, addressing the inference latency bottleneck
  of existing reasoning VLAs that generate lengthy textual CoT traces. The core method
  compresses linguistic and visual planning into compact continuous latents via preference-guided
  distillation from a textual teacher model, guided by reward preferences and visual
  trajectory alignment.
---

# Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning

## Quick Facts
- **arXiv ID:** 2601.09708
- **Source URL:** https://arxiv.org/abs/2601.09708
- **Reference count:** 40
- **Primary result:** Up to 89.3% inference latency reduction with maintained performance across embodied benchmarks

## Executive Summary
Fast-ThinkAct addresses the inference latency bottleneck of existing reasoning Vision-Language-Action (VLA) models by compressing lengthy textual Chain-of-Thought (CoT) traces into compact continuous latent representations. The method employs preference-guided distillation from a textual teacher model, using reward preferences and visual trajectory alignment to maintain spatial-temporal understanding crucial for embodied control. This enables implicit internal reasoning while achieving up to 89.3% latency reduction compared to state-of-the-art reasoning VLAs, with strong performance on robot manipulation and embodied reasoning tasks.

## Method Summary
Fast-ThinkAct uses a three-stage training pipeline: (1) SFT and CoT-SFT pretraining on embodied datasets, (2) parallel training of a GRPO-trained textual teacher and a latent student using preference-guided distillation and visual trajectory alignment, and (3) action model policy learning conditioned on spatial tokens from the student. The student generates M=6 continuous latent tokens and K=5 spatial tokens, with a verbalizer LLM trained to decode latents into interpretable text. The method leverages Qwen2.5-VL as backbone, Qwen3-0.6B as verbalizer, and DiT-Policy or RDT as action model, achieving significant latency reduction while maintaining task success rates.

## Key Results
- **89.3% inference latency reduction** compared to state-of-the-art reasoning VLAs
- **83.0% success rate** on EgoPlan-Bench2 embodied reasoning tasks
- **77.8% LLM-score** on OpenEQA reasoning benchmarks
- Strong performance across LIBERO, SimplerEnv, and RoboTwin2.0 benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Preference-Guided Latent Distillation
Compresses reasoning into continuous latents using preference pairs (Ï„âº, Ï„â») from GRPO-trained teacher, with DPO-style objective guiding student toward high-quality reasoning patterns. Assumes teacher rewards meaningfully separate reasoning quality.

### Mechanism 2: Verbalizable Latent Reasoning
Trains a verbalizer LLM to decode continuous latents into natural language, grounding latent learning in interpretable textual form. Assumes verbalizer has sufficient capacity to map latents to coherent text.

### Mechanism 3: Visual-Trajectory-Aligned Spatial Tokens
Uses parallel spatial token prediction with L2 alignment loss between teacher and student hidden states, transferring visual planning capability while reducing latency compared to autoregressive waypoint generation.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning in VLMs**
  - Why needed: Method builds on distilling textual CoT into latents
  - Quick check: Can you explain why intermediate reasoning steps improve task success and create latency problems?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: Verbalization loss adapts DPO for latent-to-text preference learning
  - Quick check: What does DPO optimize given a preference pair (Ï„âº, Ï„â»)?

- **Concept: VLA Foundation Models (e.g., OpenVLA, Ï€â‚€)**
  - Why needed: Fast-ThinkAct operates as reasoning enhancement on base VLA policies
  - Quick check: In standard VLAs, what is the direct mapping from inputs to outputs?

## Architecture Onboarding

- **Component map:**
  - Textual Teacher VLM (â„±^T_Î¸) -> Latent Student VLM (â„±_Î¸) -> Verbalizer LLM (ð’±_Ïˆ) -> Action Model (Ï€_Ï†)
  - State Encoder -> Action Model (frozen connection)

- **Critical path:**
  1. SFT (1 epoch) + CoT-SFT (15K iters) on shared checkpoint
  2. Parallel Teacher GRPO + Student latent distillation (4500 iters)
  3. Freeze student, train action model with â„’_IL (20K iters)

- **Design tradeoffs:**
  - M=6 latent tokens optimal but may not generalize across tasks
  - 0.6B verbalizer may under-represent complex reasoning
  - Early-layer KV cache conditioning outperforms late-layer but architecture-specific

- **Failure signatures:**
  - High latency despite latents: Check spatial token generation or action model diffusion
  - Incoherent verbalized reasoning: Verbalizer hallucination acknowledged but not quantified
  - Degraded manipulation performance: Likely â„’_distill misalignment
  - Inconsistent trajectory waypoints: May need autoregressive constraint

- **First 3 experiments:**
  1. Reproduce ablation on â„’_verb and â„’_distill on EgoPlan/RoboVQA/OpenEQA
  2. Latent count sensitivity sweep (M âˆˆ {1, 3, 6, 10, 30}) on LIBERO
  3. Cross-architecture action model transfer (RDT vs DiT-Policy) on RoboTwin2.0

## Open Questions the Paper Calls Out

- **Grounding-aware objectives:** How to integrate hallucination suppression techniques into verbalizer training to ensure faithfulness without affecting latent planning quality?
- **Non-verbalizable reasoning:** Extent to which textual teacher constrains student's ability to learn intuitive spatial reasoning patterns beneficial for manipulation?
- **Fixed latent size:** Whether M=6 imposes upper bound on task complexity compared to variable-length explicit CoT?

## Limitations
- Verbalizer hallucination potential acknowledged but not quantified
- Parallel spatial token prediction may produce temporally incoherent trajectories
- Performance gains benchmarked on specific datasets and architectures only

## Confidence

- **High Confidence:** 89.3% latency reduction, 83.0% success rate on EgoPlan-Bench2
- **Medium Confidence:** Preference-guided latent distillation mechanism, early-layer KV cache advantage
- **Low Confidence:** Verbalizable latents preserving task-relevant information, spatial token coherence

## Next Checks

1. **Verify reward calibration in teacher GRPO:** Train teacher with multiple reward configurations and measure impact on latent distillation performance and success rates.

2. **Test verbalizer hallucination and grounding:** Compare verbalized outputs to human annotations using BLEU/semantic similarity metrics and measure correlation with task success.

3. **Validate spatial token coherence:** Visualize predicted waypoints over time for temporally-dependent tasks and measure trajectory consistency to assess need for autoregressive prediction.