---
ver: rpa2
title: 'WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake
  Detection'
arxiv_id: '2510.05305'
source_url: https://arxiv.org/abs/2510.05305
tags:
- prompt
- speech
- wavelet
- detection
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WaveSP-Net, a parameter-efficient speech deepfake
  detection system that combines learnable wavelet-domain sparse prompt tuning with
  a bidirectional Mamba-based classifier. By leveraging the wavelet transform to structure
  prompt embeddings, the model enhances feature localization and generalization while
  keeping the large pre-trained XLSR backbone frozen.
---

# WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection

## Quick Facts
- arXiv ID: 2510.05305
- Source URL: https://arxiv.org/abs/2510.05305
- Authors: Xi Xuan; Xuechen Liu; Wenxin Zhang; Yi-Cheng Lin; Xiaojian Lin; Tomi Kinnunen
- Reference count: 0
- Primary result: Achieves 10.58% EER on Deepfake-Eval-2024 using only ~1.3% of total parameters

## Executive Summary
WaveSP-Net introduces a parameter-efficient speech deepfake detection system that combines learnable wavelet-domain sparse prompt tuning with a bidirectional Mamba-based classifier. The method enhances feature localization and generalization while keeping the large pre-trained XLSR backbone frozen. By leveraging wavelet transforms to structure prompt embeddings, the approach introduces learnable wavelet filters, sparse coefficient selection, and reconstruction steps to improve robustness and efficiency.

## Method Summary
The WaveSP-Net architecture builds on XLSR-300M features (frozen) and applies learnable wavelet-domain sparse prompt tuning (WSPT) to enhance selected prompt tokens. The system processes raw audio through XLSR to extract 201x1024 features, then concatenates learnable prompt tokens (p=10) to each transformer layer input. Only the last m=4 tokens undergo wavelet processing via learnable analysis filters (F₀, F₁), stochastic sparsification (ρ=0.1), and learnable synthesis filters (H₀, H₁). The enhanced tokens are combined with untransformed tokens and passed through 12 bidirectional Mamba blocks for classification.

## Key Results
- Achieves 10.58% EER on Deepfake-Eval-2024 (DE24) benchmark
- Achieves 0.13% EER on SpoofCeleb benchmark
- Uses only ~1.3% of total parameters compared to full fine-tuning
- Outperforms prior methods including baseline PT-XLSR (20.40% EER) and fixed-filter variants (13.15% EER)

## Why This Works (Mechanism)

### Mechanism 1: Learnable Wavelet Filters
Learnable analysis and synthesis filters (F₀, F₁, H₀, H₁) are co-optimized with the classifier during training, allowing the model to discover discriminative frequency bands specific to synthetic speech rather than relying on preset wavelet bases. This adaptive extraction captures deepfake artifacts in learnable frequency subspaces that differ from natural speech in ways fixed transforms cannot optimally capture.

### Mechanism 2: Sparse Coefficient Selection
After DWT, only a random fraction of coefficients (sparsity ratio ρ=0.1) are selected for updates via stochastic sparsification, reducing redundancy and filtering noise while preserving discriminative features. This acts as implicit regularization that prevents overfitting and strengthens resistance to noise.

### Mechanism 3: Partial Wavelet Processing
Only the last m=4 tokens (of p=10) undergo wavelet-domain enhancement, creating a hybrid representation that balances structured inductive bias with flexibility. This mixed approach allows the model to learn both constrained and unconstrained representations, with partial-WSPT-XLSR outperforming full WSPT-XLSR (10.58% vs 13.15% EER).

## Foundational Learning

- **Self-supervised speech representations (XLSR/Wav2Vec 2.0)**: Why needed - The entire architecture builds on frozen XLSR features; understanding contrastive learning explains transfer to deepfake detection. Quick check - Can you explain why freezing XLSR preserves generalization while updating only prompts adapts to the specific task?

- **Prompt tuning as parameter-efficient fine-tuning**: Why needed - The core innovation modifies prompt tokens, not model weights; understanding prompts as learned "instructions" is essential. Quick check - How do prompt tokens differ from traditional fine-tuning of model weights in terms of parameter count and adaptation scope?

- **Wavelet transform and multi-resolution analysis**: Why needed - The method explicitly relies on DWT's time-frequency localization property. Quick check - Why would wavelet decomposition be more suitable for speech artifact detection than Fourier transforms?

## Architecture Onboarding

- **Component map**: Raw audio -> XLSR (frozen) -> Layer-wise prompt concatenation -> Wavelet processing on selected tokens -> Transformer layers -> Mamba classifier -> Binary decision

- **Critical path**: Raw audio → XLSR (frozen) → Layer-wise prompt concatenation → Wavelet processing on selected tokens → Transformer layers → Mamba classifier → Binary decision

- **Design tradeoffs**:
  - Sparsity ratio (ρ): Lower = more regularization but risk of information loss; optimal found at 0.1
  - Number of wavelet tokens (m): More tokens = more structure but less flexibility; m=4 optimal, m>6 degrades performance
  - Partial vs. full wavelet: Full WSPT underperforms partial (13.15% vs 10.58% EER), suggesting some tokens benefit from remaining unconstrained

- **Failure signatures**:
  - EER >16% with fixed wavelet filters → indicates learnable filters not being optimized properly
  - Minimal improvement over baseline PT-XLSR → check if wavelet module gradients are flowing
  - Large train-val gap → sparsity may be too aggressive (ρ<0.1) or model overfitting to seen attacks

- **First 3 experiments**:
  1. Baseline validation: Replicate PT-XLSR (vanilla prompt tuning) on DE24 to establish reference EER (~20.40% per Table 2)
  2. Ablation-by-component: Test Partial-WSPT-XLSR by removing WDS, LWD, and LWR sequentially to verify each contributes meaningfully (expect 11-14% EER jumps per ablation)
  3. Hyperparameter sweep: Vary sparsity ratio (0.05, 0.1, 0.3, 0.5) and wavelet token count (2, 4, 6) on validation set; confirm optimal configuration matches paper's ρ=0.1, m=4

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to unseen deepfake techniques remains untested - the model may be overfitting to specific artifact patterns in training data
- Computational overhead of learnable wavelet filters hasn't been benchmarked for real-time deployment scenarios
- Sensitivity to sparsity hyperparameters (ρ=0.1, m=4) may require dataset-specific tuning for new deployment scenarios

## Confidence
- **Wavelet-domain sparse prompt tuning effectiveness**: High - Strong empirical support from controlled ablation studies showing consistent performance gains across both benchmarks
- **Learnable wavelet filters versus fixed transforms**: Medium - Table 4 demonstrates improvements, but ablation studies isolating learnability contribution are lacking
- **Sparsity as implicit regularization**: Medium - Performance degradation when removing WDS supports the claim, but lacks direct analysis of overfitting metrics

## Next Checks
1. **Cross-dataset transfer validation**: Evaluate WaveSP-Net on ASVspoof 2019 or 2021 without fine-tuning to assess true generalization capability and measure performance degradation when training on DE24 and testing on independent corpus.

2. **Real-time inference benchmarking**: Profile WaveSP-Net's latency and computational requirements compared to baseline PT-XLSR across varying audio lengths (1-10 seconds), including wavelet processing overhead and bidirectional Mamba inference time.

3. **Hyperparameter sensitivity analysis**: Systematically vary sparsity ratio (0.01, 0.05, 0.1, 0.2, 0.3, 0.5) and wavelet token count (2, 3, 4, 5, 6, 8) across both DE24 and SpoofCeleb benchmarks to map stability landscape and generate contour plots showing EER sensitivity.