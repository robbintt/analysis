---
ver: rpa2
title: Long Context Automated Essay Scoring with Language Models
arxiv_id: '2509.10417'
source_url: https://arxiv.org/abs/2509.10417
tags:
- arxiv
- scoring
- length
- language
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates long-context language models for automated\
  \ essay scoring, addressing the challenge of processing lengthy student essays that\
  \ exceed typical transformer limits. Five models with architectural modifications\u2014\
  XLNet, Longformer, ModernBERT, Mamba, and Llama\u2014were fine-tuned on the ASAP\
  \ 2.0 dataset using both traditional classification and generative scoring approaches."
---

# Long Context Automated Essay Scoring with Language Models

## Quick Facts
- arXiv ID: 2509.10417
- Source URL: https://arxiv.org/abs/2509.10417
- Authors: Christopher Ormerod; Gitit Kehat
- Reference count: 10
- Primary result: Mamba-130m achieves 0.797 QWK, outperforming larger models and human baseline (0.745) while requiring only linear computational scaling

## Executive Summary
This study evaluates long-context language models for automated essay scoring, addressing the challenge of processing lengthy student essays that exceed typical transformer limits. Five models with architectural modifications—XLNet, Longformer, ModernBERT, Mamba, and Llama—were fine-tuned on the ASAP 2.0 dataset using both traditional classification and generative scoring approaches. Mamba-130m, despite its smaller size, achieved the highest overall quadratic weighted kappa (QWK) of 0.797, demonstrating competitive performance with linear complexity models. Longformer reached 0.798 QWK, while traditional models like DeBERTa-Base scored 0.790. All models outperformed human rater agreement (0.745), with Mamba showing particular efficiency advantages for long sequences due to its state-space architecture requiring only linear computational scaling.

## Method Summary
The study fine-tuned five long-context language models on the ASAP 2.0 dataset using two approaches: traditional classification with appended heads and generative scoring with QLoRA adapters. Mamba required special training considerations, with SSM layers frozen to prevent model collapse. All models were evaluated using quadratic weighted kappa (QWK) on a held-out test set, with 10% of training data reserved for development optimization. The study compared performance against human rater agreement (0.745 QWK) while assessing computational efficiency and architectural trade-offs for long sequence processing.

## Key Results
- Mamba-130m achieved highest QWK of 0.797 despite smallest parameter count (130M)
- Longformer reached 0.798 QWK, slightly outperforming Mamba with hybrid attention mechanism
- All models exceeded human rater agreement baseline of 0.745 QWK
- Mamba demonstrated 2-8x inference speedup due to linear complexity scaling
- No single model dominated across all metrics, suggesting ensemble potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba's state-space architecture enables competitive AES performance with linear computational scaling, achieving 0.797 QWK despite having only 130M parameters.
- Mechanism: Replaces transformer attention layers with discretized state-space models defined by h_t = Ah_{t-1} + Bx_t (Equation 3a). Each Mamba layer combines SSM blocks with convolutional layers, linear projections, and activation functions. The linear recurrence relation eliminates quadratic attention computation, scaling as O(n) rather than O(n²).
- Core assumption: Linear complexity translates to practical efficiency gains for long sequences without sacrificing scoring accuracy.
- Evidence anchors:
  - [abstract]: "Mamba-130m, despite its smaller size, achieved the highest overall quadratic weighted kappa (QWK) of 0.797...Mamba showing particular efficiency advantages for long sequences due to its state-space architecture requiring only linear computational scaling."
  - [section 2.6]: "The Mamba blocks can be computed with linear complexity, making them well-suited for long context tasks...optimized implementations may achieve 2-8x speed improvements compared to transformer-based models."
  - [corpus]: Related work validates efficiency claims but Mamba-specific AES validation remains limited in corpus neighbors.
- Break condition: If full model fine-tuning causes model collapse (as noted in Section 3.1), restrict training to embedding layer and L_in/L_out weights only.

### Mechanism 2
- Claim: Longformer's hybrid attention mechanism (local sliding window + selective global attention) achieves the highest QWK (0.798) by preserving rubric-relevant organizational features.
- Mechanism: Local attention operates via sliding window over adjacent tokens, while global attention applies exclusively to special tokens ([CLS], [SEP], [MASK]). This reduces attention computation from O(n²) to O(n×w) where w is window size, enabling 4k context length.
- Core assumption: Organizational scoring rubric elements requiring long-range dependencies can be captured through sparse global token connections.
- Evidence anchors:
  - [abstract]: "This raises serious validity concerns as it undermines the model's ability to fully capture and evaluate organizational elements of the scoring rubric, which requires long contexts to assess."
  - [section 2.2]: "The Longformer model attempts to reconcile the need for local attention with a selective form of global attention...by only using attention selectively, the computational burden is mitigated."
  - [corpus]: Neighbor paper "Empirical Analysis of the Effect of Context in AES" confirms context length significantly impacts transformer-based scoring quality.
- Break condition: If essays exceed 4k tokens, Longformer still requires truncation; consider Mamba or ModernBERT (8k) for longer sequences.

### Mechanism 3
- Claim: Recurrent attention with cached hidden states (XLNet/Transformer-XL) extends effective context to L×D tokens (6,000-12,000) without architectural modifications.
- Mechanism: Attention keys and values incorporate previous segment hidden states via stop-gradient concatenation: h̃^{n-1}_{τ+1} = [SG(h^{n-1}_τ) ∘ h^{n-1}_{τ+1}]. Information flows across segments through cached states, though maximum effective context is bounded by segment length × network depth.
- Core assumption: Graded essays exhibit discourse coherence that propagates information across segment boundaries through recurrent connections.
- Evidence anchors:
  - [section 2.3]: Mathematical formulation (Equations 1a-1e) showing recurrence built into key and value computation; "the output of any token is only a function of at most L×D of the previous tokens."
  - [section 2.3]: "These models have recently been discussed for essays, where the long context was useful in accurately annotating the argumentative components of essays."
  - [corpus]: Ormerod et al. (2023) neighbor reference confirms XLNet effectiveness for essay argumentation annotation tasks.
- Break condition: If essays require dependencies beyond 12,000 tokens (base model) or 24,000 tokens (large model), recurrence depth becomes insufficient.

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: Primary evaluation metric; all performance claims reference QWK scores. Understanding that QWK measures agreement beyond chance (1=perfect, 0=random, -1=inverse) is essential for interpreting model comparison.
  - Quick check question: If Model A scores 0.790 QWK and human raters score 0.745, what does this indicate about Model A's reliability?

- Concept: Transformer Attention Complexity
  - Why needed here: All architectural modifications in this paper address the O(n²) attention scaling problem. Without this foundation, you cannot evaluate why Mamba's O(n) complexity matters or why truncation was historically necessary.
  - Quick check question: Given BERT's 512-token limit, what happens to memory requirements if you double input length to 1,024 tokens?

- Concept: Fine-tuning vs. Frozen Weights
  - Why needed here: Mamba requires partial freezing (SSM layers frozen, only L_in/L_out trained) to prevent model collapse. Understanding which parameters to update is critical for successful deployment.
  - Quick check question: Why might fine-tuning all Mamba parameters cause training instability, while freezing SSM weights succeeds?

## Architecture Onboarding

- Component map:
  Input Processing: Tokenizer-specific subword encoding → Word embeddings + Positional embeddings (RoPE for ModernBERT/Llama) → Context length bound (512-8k depending on model)
  Encoder Variants: (1) Standard transformer attention [DeBERTa], (2) Sliding window + global attention [Longformer], (3) Recurrent attention with segment caching [XLNet], (4) State-space recurrence [Mamba], (5) RoPE-scaled attention [ModernBERT/Llama]
  Task Heads: Classification head (traditional models) or generative output (Llama with QLoRA adapters on L_q, L_k, L_v)
  Output: Score prediction mapped to rubric scale

- Critical path:
  1. Assess essay length distribution against model context limits (Table 1: avg 342-426 words → ~450-550 tokens)
  2. Select architecture based on length requirements: DeBERTa (512) < Longformer (4k) < ModernBERT/XLNet/Mamba (8k effective)
  3. Configure fine-tuning regime: Traditional (classification head) vs. Generative (QLoRA with instruction template)
  4. Optimize QWK on 10% held-out development set with early stopping
  5. Validate against human baseline (0.745 QWK threshold)

- Design tradeoffs:
  - **Accuracy vs. Efficiency**: Longformer (0.798 QWK, 149M params) slightly outperforms Mamba (0.797 QWK, 130M params), but Mamba offers 2-8x inference speedup
  - **Model Size vs. Context Length**: Llama-3.2-8B (8B params, 0.792 QWK) underperforms smaller encoders, suggesting parameter count alone doesn't determine AES quality
  - **Truncation vs. Full Context**: DeBERTa (512 limit) achieves 0.790 QWK but risks validity concerns for organizational rubric elements

- Failure signatures:
  - **Model Collapse (Mamba)**: Full fine-tuning causes training divergence; freeze SSM, conv, and L_gate layers
  - **Memory Overflow**: Batch size reduction from 4→1 may be required for long essays; prioritize gradient accumulation over OOM
  - **Prompt Sensitivity (Generative)**: Rubric phrasing variations significantly impact QWK; use 20 paraphrase variations and optimize on development set

- First 3 experiments:
  1. **Baseline Establishment**: Fine-tune DeBERTa-Base (512 tokens, truncated) on ASAP 2.0 with classification head. Target: ≥0.790 QWK. This validates data pipeline and provides truncation baseline.
  2. **Long Context Ablation**: Fine-tune Longformer (4k) and Mamba-130m (8k effective) on identical data without truncation. Compare QWK scores and training time per epoch. Expect Mamba to match Longformer (±0.001 QWK) with faster wall-clock time.
  3. **Parameter Efficiency Test**: Apply QLoRA to Llama-3.2-8B with 10% development set for rubric optimization. If QWK <0.790, prioritize encoder models over generative approaches for pure scoring tasks.

## Open Questions the Paper Calls Out

- Can ensemble methods combining state-space models (like Mamba) with attention-based transformers yield superior performance compared to single-model approaches?
  - Basis: Discussion section states differences in performance "suggest a potential for ensemble approaches."
  - Why unresolved: Study evaluated each model in isolation to compare architectural efficiencies but did not test combined models.
  - What evidence would resolve it: Experiments blending Mamba and Longformer predictions to see if ensemble QWK exceeds 0.798.

- What specific training dynamics cause model collapse when fully fine-tuning Mamba models for classification, and can this be mitigated?
  - Basis: Methods section notes "Full model training seemed to readily lead to model collapse," forcing authors to freeze SSM weights.
  - Why unresolved: Authors worked around instability by freezing layers rather than investigating root cause.
  - What evidence would resolve it: Ablation study testing various learning rates or regularization techniques for stable, full-parameter fine-tuning of Mamba on AES tasks.

- How does quality of feedback provided by generative models (like Llama) compare to their scoring accuracy when evaluated against human criteria?
  - Basis: Discussion acknowledges generative models "offer the promising capability of providing feedback" despite not outperforming encoders in scoring.
  - Why unresolved: Study measured performance solely via QWK and did not evaluate qualitative utility of generative output.
  - What evidence would resolve it: Dual evaluation measuring both QWK score and validity/explanatory power of generated feedback rubrics.

## Limitations

- Mamba model collapse during full fine-tuning required architectural constraint (freezing SSM layers)
- Limited evaluation of generative model feedback quality beyond scoring metrics
- No ensemble experiments conducted despite performance differences suggesting potential benefits
- Training instability specific to Mamba architecture not fully characterized or resolved

## Confidence

- Mamba achieves 0.797 QWK with linear complexity: High
- Longformer slightly outperforms Mamba (0.798 vs 0.797): High
- All models exceed human baseline of 0.745: High
- Mamba offers 2-8x inference speedup: Medium (based on theoretical scaling claims)
- Full Mamba fine-tuning causes model collapse: High
- Ensemble approaches could improve performance: Low (suggested but not tested)

## Next Checks

1. Replicate Mamba-130m training with frozen SSM layers to verify 0.797 QWK score and confirm model collapse occurs with full fine-tuning
2. Implement Longformer with hybrid attention and test on essays exceeding 512 tokens to validate 0.798 QWK performance
3. Conduct ensemble experiment combining Mamba and Longformer predictions to test whether combined model exceeds single-model maximum of 0.798 QWK