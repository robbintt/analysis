---
ver: rpa2
title: Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste
  Sorting
arxiv_id: '2502.01455'
source_url: https://arxiv.org/abs/2502.01455
tags:
- segmentation
- maps
- objects
- images
- saliency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of weakly supervised video segmentation
  for waste sorting, where manual pixel-level annotations are costly. The method improves
  saliency maps for segmentation by exploiting temporal consistency between consecutive
  video frames.
---

# Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste Sorting

## Quick Facts
- arXiv ID: 2502.01455
- Source URL: https://arxiv.org/abs/2502.01455
- Reference count: 40
- Primary result: 37.84 mIoU on SERUSO dataset for weakly supervised video segmentation in waste sorting

## Executive Summary
This paper addresses the challenge of weakly supervised video segmentation for waste sorting applications, where manual pixel-level annotations are prohibitively expensive. The method improves upon standard Class Activation Maps (CAMs) by enforcing temporal consistency between consecutive video frames and addressing background bias in industrial camera setups. By training an auxiliary classifier on video triplets and incorporating optical flow-based motion compensation, the approach achieves superior segmentation performance while requiring only image-level labels.

## Method Summary
The method trains a ResNet50-based auxiliary classifier on "before"/"after" video frames to generate saliency maps for waste objects. It introduces a three-class problem (Before, After, Background) to mitigate background bias from fixed camera positions. The architecture includes a spatial module that tiles images into patches to enforce spatial consistency through reconstruction loss, and a temporal module that uses optical flow to warp adjacent frame saliency maps to the central frame, creating temporally coherent activation patterns. The total loss combines classification loss, spatial reconstruction loss, and temporal reconstruction loss, with weighting factors increasing during training.

## Key Results
- Achieves 37.84 mIoU on the SERUSO dataset, outperforming baseline methods like GradCAM and PuzzleCAM
- Demonstrates that temporal consistency significantly improves segmentation completeness by forcing network activation on full object extents
- Successfully mitigates background bias through median background subtraction and three-class formulation

## Why This Works (Mechanism)

### Mechanism 1: Temporal Consistency
Enforcing temporal consistency between saliency maps of adjacent frames improves segmentation completeness. A reconstruction loss minimizes the L1 distance between central frame CAM and motion-compensated adjacent frame maps, forcing network to activate on full object extent rather than just discriminative parts. Optical flow must accurately estimate object displacement for correct warping. Fast, non-rigid motion or significant occlusion can cause optical flow estimation to fail.

### Mechanism 2: Background Bias Mitigation
Pre-processing images to separate background elements mitigates classifier bias from fixed camera setups. The method calculates median background image and subtracts it, framing the problem as a 3-class task (Before, After, Background) rather than binary. This prevents classifier from using static background features to predict class, forcing focus on waste objects. The approach assumes background is sufficiently static for pixel-wise median to serve as accurate background model. Highly dynamic environments with rapid lighting changes or varying conveyor belt texture can render median background estimator invalid.

### Mechanism 3: Spatial Consistency
Reconstructing full-image features from tiled patches forces network to learn spatially consistent features. The Spatial Module splits input frame into patches, extracts features for each, and merges them. A reconstruction loss aligns merged features with original full-image features, compelling network to recognize object parts independently and reduce focus on small discriminative regions. Assumes objects of interest are large enough to be meaningfully split into patches without losing critical context. Small objects that disappear or become unidentifiable when image is divided into 2x2 tiling grid represent a break condition.

## Foundational Learning

- **Concept: Class Activation Maps (CAM) & Saliency**
  - Why needed here: Core "weakly supervised" signal; relies on assumption that classifier trained on image-level labels will localize "illegal" objects via high activation values
  - Quick check question: Can you explain why standard CAM might highlight only small part of object (e.g., head of person) rather than whole body?

- **Concept: Optical Flow**
  - Why needed here: Mechanism fundamentally depends on "motion compensation"; must understand how flow vectors map pixels from Frame t-1 to Frame t to understand how "warped" saliency maps are generated
  - Quick check question: If object moves 50 pixels to right between frames, how does optical flow matrix instruct warping function to shift saliency map?

- **Concept: Global Average Pooling (GAP)**
  - Why needed here: Architecture uses GAP to collapse spatial features into class probability; understanding this helps diagnose why spatial/temporal modules are necessary to preserve location information that GAP normally destroys
  - Quick check question: How does adding GAP layer after convolutional block affect spatial resolution of output?

## Architecture Onboarding

- **Component map:** Input (triplet of RGB frames) -> Pre-processing (Median Background Subtraction) -> Backbone (Shared ResNet50 Feature Extractor) -> Head A (Classification with GAP and Multi-label Soft Margin Loss) -> Head B (Spatial: Tiling, Patch Features, Merge, L_spatial Reconstruction) -> Head C (Temporal: Optical Flow Estimation, Warp Mt-1/Mt+1, Fuse via Max, L_temporal Reconstruction)

- **Critical path:** Background subtraction (Step 2) is most fragile dependency; if this step fails, classifier learns background bias, rendering saliency maps useless regardless of sophistication of Spatial/Temporal modules

- **Design tradeoffs:** Accuracy vs. Speed - calculating optical flow for every triplet during training significantly increases computational cost and memory bandwidth compared to static image methods; Generalization - method is tailored for "conveyor belt" physics (linear, consistent motion) and may not generalize to handheld camera jitter without retraining flow/robustness components

- **Failure signatures:** "Shrinking" Masks - if regularization weights (α, β) are too low, CAMs will regress to highlighting only most distinct feature of trash (e.g., logo) rather than whole item; Background Leakage - if median background is imperfect, "Before" class may simply highlight belt seams or stains rather than trash objects

- **First 3 experiments:**
  1. Baseline Bias Check: Train standard ResNet50 classifier on raw "before/after" images (no background removal); verify failure mode using GradCAM (should highlight background)
  2. Ablation (Spatial Only): Run PuzzleCAM module on background-subtracted images to isolate gain from spatial coherence
  3. Full Temporal Integration: Enable triplet sampling and temporal loss (L_temporal) to measure delta provided by optical flow warping on mIoU score

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating adjacent frames during inference phase improve segmentation performance compared to current single-frame approach? The authors state that future work explores including adjacent frames in map computation at inference time as well, noting that saliency maps are currently computed using only single frame during inference. This remains unresolved as current architecture restricts temporal coherence mechanisms solely to training phase. Quantitative results (e.g., mIoU scores) from inference pipeline modified to aggregate saliency maps from adjacent frames would resolve this.

- **Open Question 2:** Can weakly supervised segmentation masks generated by this method serve as effective pseudo-labels for training Fully Supervised (FS) segmentation network? The conclusion proposes that segmentation masks obtained can be used as pseudo-labels to supervise FS segmentation network to improve segmentation performance. This remains unresolved as paper only evaluates raw saliency maps against ground truth, not testing utility of these masks as training data for other models. Performance metrics of standard FS network (e.g., DeepLab) trained on generated masks versus one trained on human-annotated ground truth would resolve this.

- **Open Question 3:** Is this method effective for general industrial quality control tasks involving separation of anomalous elements from heterogeneous stream? The authors suggest dual-camera setup and method can be applied to other industrial processes, e.g., in product quality control processes where anomalous elements need to be removed. This remains unresolved as experiments are restricted to waste sorting domain (SERUSO dataset) and specific PET materials. Successful application and evaluation on non-waste industrial dataset involving distinct legal and illegal objects would resolve this.

## Limitations
- Reliance on accurate optical flow estimation represents significant vulnerability, as any failure in motion compensation directly compromises temporal consistency mechanism
- Assumes relatively static backgrounds and linear object motion typical of conveyor belt systems, limiting applicability to more dynamic environments
- Limited to waste sorting domain without validation on other industrial applications

## Confidence
- Background bias mitigation mechanism: Medium-High (well-supported by ablation experiments showing classifier failure when backgrounds are reintroduced)
- Optical flow component: Low (limited discussion of failure cases and computational overhead not quantified)
- Spatial reconstruction mechanism: Medium (lacks detailed ablation studies isolating individual contribution)

## Next Checks
1. Test method on sequences with non-linear or highly variable motion to assess optical flow robustness
2. Implement ablation study removing temporal module while keeping background subtraction to quantify its isolated contribution
3. Evaluate performance on dataset with varying lighting conditions to test background estimation stability