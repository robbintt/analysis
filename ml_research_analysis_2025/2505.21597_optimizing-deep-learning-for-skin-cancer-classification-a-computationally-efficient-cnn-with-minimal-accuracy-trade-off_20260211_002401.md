---
ver: rpa2
title: 'Optimizing Deep Learning for Skin Cancer Classification: A Computationally
  Efficient CNN with Minimal Accuracy Trade-Off'
arxiv_id: '2505.21597'
source_url: https://arxiv.org/abs/2505.21597
tags:
- skin
- learning
- cancer
- accuracy
- resnet50
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the computational inefficiency of deep learning
  models for skin cancer classification, specifically focusing on the trade-off between
  model complexity and real-world feasibility. The proposed method introduces a custom
  CNN architecture that significantly reduces parameters from 23.9 million (ResNet50)
  to 692,000 (96.7% reduction) while maintaining classification accuracy within 0.022%
  of transfer learning models.
---

# Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off

## Quick Facts
- arXiv ID: 2505.21597
- Source URL: https://arxiv.org/abs/2505.21597
- Reference count: 31
- Primary result: Custom CNN achieves 96.7% fewer parameters (692K vs 23.9M) with <0.022% accuracy loss for skin cancer classification

## Executive Summary
This study addresses the computational inefficiency of deep learning models for skin cancer classification by proposing a custom CNN architecture that dramatically reduces parameters while maintaining classification accuracy. The lightweight model achieves 87.05% accuracy with only 30.04 million FLOPs compared to ResNet50's 4.00 billion FLOPs, representing a 13,216.76% increase in computational efficiency. This optimization enables practical deployment in mobile and edge-based medical diagnostics while reducing energy consumption, memory footprint, and inference time.

## Method Summary
The proposed method introduces a custom shallow CNN architecture replacing ResNet50's 50-layer residual structure with a 3-block convolutional stack (32→64→128 filters) followed by a single dense layer. The model was trained on the HAM10000 dataset with standard preprocessing (224×224 resize, normalization) and augmentation (rotations, flips). Key efficiency gains come from reducing depth while maintaining sufficient feature extraction capacity for skin lesion classification, achieving 96.7% parameter reduction while maintaining accuracy within 0.022% of transfer learning baselines.

## Key Results
- 96.7% reduction in parameters from 23.9M to 692K
- 13,216.76% increase in computational efficiency (30.04M vs 4.00B FLOPs)
- Classification accuracy of 87.05% vs 89.08% for ResNet50 (0.022% deviation)
- Enables practical deployment in mobile and edge-based medical diagnostics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A custom shallow CNN can approximate the classification performance of deep transfer learning models for skin lesion classification while reducing computational cost by orders of magnitude.
- **Mechanism:** The architecture replaces ResNet50's 50-layer residual structure with a 3-block convolutional stack (32→64→128 filters) followed by a single dense layer. This exploits the observation that skin lesion classification in HAM10000 may not require the hierarchical feature depth needed for general ImageNet tasks, allowing task-specific feature learning without pre-trained overhead.
- **Core assumption:** The discriminative features for skin cancer classification (texture, color, boundary patterns) are learnable at shallow depths given appropriate preprocessing and augmentation.
- **Evidence anchors:** [abstract] "proposes a custom CNN model that achieves a 96.7% reduction in parameters... while maintaining a classification accuracy deviation of less than 0.022%"; [section III-B] Normalization and augmentation strengthen model robustness without architectural complexity; [corpus] Related work similarly achieves lightweight medical image classification with reduced parameters.

### Mechanism 2
- **Claim:** FLOP reduction correlates directly with practical deployment benefits (inference speed, energy consumption) more reliably than parameter count alone.
- **Mechanism:** The paper uses layer-wise FLOP calculations to quantify operations per inference. Reducing FLOPs from 4.00B to 30.04M minimizes arithmetic operations per forward pass, directly impacting latency and power draw on edge hardware.
- **Core assumption:** FLOPs are the bottleneck for real-time deployment on target hardware; memory bandwidth and cache efficiency are secondary constraints for this use case.
- **Evidence anchors:** [section III-G] Explicit FLOP formulas provided; [section IV-C] Table IV shows 13,216.76% FLOP increase yields only 0.022% accuracy gain.

### Mechanism 3
- **Claim:** Transfer learning provides diminishing returns when the pre-trained feature space has low transferability to the target domain.
- **Mechanism:** ResNet50 features learned on ImageNet (natural images) may not align optimally with dermoscopic image characteristics. Fine-tuning adapts later layers, but early convolutional filters remain suboptimal, requiring computational overhead for marginal domain adaptation.
- **Core assumption:** The performance ceiling of transfer learning for skin lesion classification is bounded by domain shift, not model capacity.
- **Evidence anchors:** [section III-C] ResNet50 pre-trained on ImageNet, fine-tuned on HAM10000; [section V] "transfer learning models... achieve a peak classification accuracy of 89.08%, they necessitate over 4.00 billion FLOPs"; [corpus] Paper 20232 discusses generalizability challenges in medical DL.

## Foundational Learning

- **Concept: FLOPs vs. Parameter Count**
  - **Why needed here:** The paper emphasizes FLOP reduction as the primary efficiency metric. Understanding that parameters measure memory footprint while FLOPs measure computation is essential for interpreting the trade-off analysis.
  - **Quick check question:** If two models have identical parameter counts but different FLOPs, which is faster on a memory-constrained edge device? (Answer: Cannot determine from parameters alone; FLOPs indicate computational work, but actual speed depends on hardware parallelism and memory bandwidth.)

- **Concept: Transfer Learning vs. Training from Scratch**
  - **Why needed here:** The baseline uses ResNet50 with frozen pre-trained layers, then fine-tunes. Understanding when transfer learning helps versus when it introduces domain mismatch is critical for architecture selection.
  - **Quick check question:** Why might ImageNet-pretrained features be suboptimal for dermoscopic images? (Answer: ImageNet contains natural scenes with color/texture distributions different from clinical dermatology; early filters may detect irrelevant features.)

- **Concept: Class Imbalance in Medical Datasets**
  - **Why needed here:** HAM10000 has seven lesion classes with likely uneven distribution. Accuracy alone can mask poor performance on minority classes (e.g., melanoma).
  - **Quick check question:** If a model achieves 90% accuracy but only 50% recall on melanoma, is it clinically viable? (Answer: Likely no; missing melanoma diagnoses has severe consequences. Precision-recall and F1 per class should be examined.)

## Architecture Onboarding

- **Component map:** Input (224×224×3) → [Conv2D 32, 3×3, ReLU] → MaxPool2D 2×2 → [Conv2D 64, 3×3, ReLU] → MaxPool2D 2×2 → [Conv2D 128, 3×3, ReLU] → MaxPool2D 2×2 → Flatten → Dense(256, ReLU) → Dropout(0.5) → Dense(7, Softmax) → Output

- **Critical path:** The three Conv2D blocks must extract sufficient spatial features before flattening. If the flattened tensor (100,352 elements from 28×28×128) loses spatial relationships critical for lesion discrimination, the dense layer cannot recover them.

- **Design tradeoffs:**
  - **Depth vs. efficiency:** 3 conv blocks minimize FLOPs but may miss hierarchical features captured by ResNet50's 50 layers.
  - **Dense layer size:** 256 units provide representational capacity but dominate parameter count; reducing this further trades accuracy for efficiency.
  - **Dropout rate:** 0.5 mitigates overfitting but may under-regularize if data augmentation is insufficient.

- **Failure signatures:**
  - Validation accuracy plateaus well below training accuracy → overfitting; increase dropout or augmentation.
  - Per-class recall near zero for minority classes → class imbalance; apply weighted loss or oversampling.
  - Inference time scales non-linearly with FLOP reduction → framework overhead or I/O bottleneck; profile with hardware-specific tools.

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train both custom CNN and ResNet50+TL on HAM10000 with identical train/val splits. Verify reported accuracy gap (87.05% vs. 89.08%) and FLOP reduction. Log training time and GPU memory usage.
  2. **Ablate dense layer size:** Replace Dense(256) with Dense(128) and Dense(64). Measure accuracy drop and parameter reduction. Identify the knee point where accuracy degrades sharply.
  3. **Class-wise performance audit:** Compute precision, recall, and F1 for each of the 7 lesion classes. Confirm melanoma recall is clinically acceptable (>80% as a starting threshold). If not, experiment with class-weighted binary cross-entropy or focal loss.

## Open Questions the Paper Calls Out

- **Question:** Can quantization and model pruning techniques be applied to the proposed lightweight CNN to achieve further efficiency gains without dropping below the 87% accuracy threshold required for clinical utility?
- **Question:** Does the lightweight CNN maintain comparable per-class sensitivity for early-stage melanoma and rare lesion types, or does the 2% accuracy gap mask clinically significant drops in minority-class detection?
- **Question:** What is the actual inference latency, memory footprint, and energy consumption of the proposed CNN when deployed on resource-constrained mobile or edge hardware?

## Limitations
- Parameter count discrepancy exists between claimed 692K and calculated values based on Dense(256) layer input size
- Clinical safety depends on per-class recall for melanoma, which is not explicitly reported
- Real-world deployment benefits (latency, energy) remain unverified on target hardware

## Confidence
- **High confidence:** FLOP reduction calculations and computational efficiency claims are methodologically sound and verifiable.
- **Medium confidence:** The accuracy maintenance claim (within 0.022%) is based on a single dataset split and requires reproduction across multiple random seeds.
- **Low confidence:** The clinical viability of the model depends on per-class recall for melanoma, which is not explicitly reported and represents a critical safety consideration.

## Next Checks
1. **Recompute architecture parameters:** Verify the actual parameter count by implementing the model with GlobalAveragePooling2D instead of Flatten to achieve the claimed 692K parameters.
2. **Test clinical safety threshold:** Compute per-class precision, recall, and F1-scores, with special attention to melanoma detection recall. If melanoma recall falls below 80%, the model requires redesign.
3. **Cross-dataset generalization:** Evaluate the custom CNN on independent dermoscopic datasets (e.g., ISIC 2019 challenge) to validate that the architecture's efficiency does not compromise real-world generalization.