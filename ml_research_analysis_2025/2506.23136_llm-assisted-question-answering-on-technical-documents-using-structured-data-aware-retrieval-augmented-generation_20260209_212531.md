---
ver: rpa2
title: LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware
  Retrieval Augmented Generation
arxiv_id: '2506.23136'
source_url: https://arxiv.org/abs/2506.23136
tags:
- document
- pipeline
- context
- data
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a RAG pipeline designed to improve question-answering
  on technical documents containing structured data like tables and images. The key
  innovation is a two-stage retrieval process that combines semantic search with a
  fine-tuned LLM reranker trained via RAFT, allowing the model to better distinguish
  relevant from irrelevant contexts.
---

# LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2506.23136
- Source URL: https://arxiv.org/abs/2506.23136
- Authors: Shadman Sobhan; Mohammad Ariful Haque
- Reference count: 40
- Key outcome: RAG pipeline with two-stage retrieval and table-to-text conversion achieves 94-96% faithfulness and 87-93% answer relevancy on technical documents

## Executive Summary
This paper presents a RAG pipeline specifically designed for question-answering on technical documents containing structured data like tables and images. The system introduces a two-stage retrieval process that combines semantic search with a fine-tuned LLM reranker trained via RAFT, enabling better discrimination between relevant and irrelevant contexts. Tables are converted to descriptive text summaries to preserve structural relationships, while images are processed by VLMs to generate detailed captions. The pipeline supports both searchable and scanned documents, demonstrating significant improvements over standard RAG approaches, particularly for table-based and out-of-context questions.

## Method Summary
The method employs a two-stage retrieval architecture where initial semantic search (BAAI/bge-small-en-v1.5 embeddings) retrieves top 10 chunks, followed by a fine-tuned Gemma-2-9b-it reranker that selects the final top 3 contexts. Tables are detected using YOLO, converted to HTML, then summarized by an LLM using a structured prompt that preserves row-by-row relationships. Images are processed by Llama-3.2-90b-vision to generate descriptive captions. The system was trained on a custom RAFT dataset (1,040 chunks) using LoRA fine-tuning with specific hyperparameters (r=8, alpha=32) over 3 epochs. Evaluation shows the pipeline achieves 94-96% faithfulness and 87-93% answer relevancy across technical and general documents.

## Key Results
- Achieves 94-96% faithfulness and 87-93% answer relevancy scores on technical documents
- Two-stage retrieval improves faithfulness from 0 to 0.5 on table-based questions versus single-stage RAG
- Correctly refuses to answer out-of-context questions (Q5) while single-stage RAG produces hallucinations
- Effective across both technical manuals and general documents, though image interpretation remains limited

## Why This Works (Mechanism)

### Mechanism 1: Table-to-Text Conversion via LLM Summarization
Converting tables to descriptive text preserves structural relationships that vector embeddings lose. Tables are detected by YOLO, converted to HTML, then summarized by Gemma-2-9b-it using a prompt that enforces row-by-row description with explicit column-header-to-cell mappings. The core assumption is that descriptive sentence format maintains cell-header relationships better than raw vector representations. Evidence shows this approach improves faithfulness on table-based questions, though tables with deeply nested headers may produce ambiguous descriptions.

### Mechanism 2: Two-Stage Retrieval with Fine-Tuned LLM Reranker
Adding an LLM-based reranker improves context selection beyond semantic similarity alone, particularly for table-based and out-of-context questions. Stage 1 retrieves K=10 chunks via FAISS vector similarity, while Stage 2 uses a fine-tuned Gemma-2-9b-it reranker to select top N=3 contexts. The reranker assesses "true semantic alignment" between query and context rather than surface-level similarity. Results show significant improvement on table questions and correct refusal on out-of-context queries versus general RAG.

### Mechanism 3: RAFT Training for Context Discrimination
Training the reranker with both oracle (relevant) and distractor (irrelevant) documents teaches explicit context discrimination. The RAFT dataset includes Q + 1 oracle chunk + 2 distractor chunks + CoT-style answer. Gemma-2-9b-it is fine-tuned via LoRA (r=8, alpha=32) and SFT over 3 epochs. The core assumption is that exposure to negative examples during training improves the model's ability to reject plausible-but-irrelevant contexts at inference. Effectiveness depends on distractor quality—random chunks may not teach discrimination against semantically similar but irrelevant contexts.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: Core architecture that reduces hallucination by retrieving relevant contexts before generation
  - Quick check: Given a query about "transformer insulation types," explain why a chunk about "power transformers in general" might have high vector similarity but low relevance

- **Concept: LoRA (Low-Rank Adaptation) / PEFT**
  - Why needed: The reranker is fine-tuned using LoRA; understanding parameter-efficient fine-tuning is necessary to reproduce or modify training
  - Quick check: What are the practical implications of r=8 and lora_alpha=32 for memory usage vs. full fine-tuning?

- **Concept: Vision-Language Models (VLMs) and OCR**
  - Why needed: Image extraction relies on Llama-3.2-90b-vision; scanned documents require Pytesseract OCR. Both have known limitations
  - Quick check: Why might a VLM produce generic descriptions for technical diagrams, and how would this affect downstream retrieval?

## Architecture Onboarding

- **Component map**: Pytesseract (OCR) -> YOLO (table/image detection) -> HTML conversion -> Gemma summarization -> Simple Directory Reader -> BAAI embeddings -> FAISS index -> Gemma reranker -> Gemma generator

- **Critical path**: 1) Table/image extraction accuracy -> quality of descriptions 2) Description quality -> embedding representativeness 3) Stage 1 retrieval recall (must find relevant chunk in top K=10) 4) Reranker discrimination (trained via RAFT) -> final context selection 5) Generator faithfulness to retrieved context

- **Design tradeoffs**: Fixed 512-token chunks vs. semantic/structure-aware chunking (may split table descriptions); K=10 initial retrieval balances recall vs. reranker compute cost; Open-source models (Gemma, Llama via GROQ) chosen for cost; paper explicitly notes VLM limitations for flowcharts

- **Failure signatures**: Low faithfulness on table questions (check if table parsing lost row/column relationships); Hallucination on out-of-context questions (reranker may not be properly fine-tuned); Generic image descriptions (VLM limitation—consider alternative VLMs); Poor performance on scanned documents (OCR errors propagating)

- **First 3 experiments**: 1) Run your own technical document through both single-stage (vector only) and two-stage (with reranker) pipelines; measure faithfulness and answer relevancy using RAGAS or DeepEval 2) Manually verify 5-10 parsed table descriptions against original tables; check for missing rows, misattributed columns, or lost numerical precision 3) Submit 5 questions unrelated to the document; verify the system correctly refuses rather than hallucinates (per Q5 results in paper)

## Open Questions the Paper Calls Out

- **How can retrieval pipelines be adapted to extract and utilize the logical flow of information from flowcharts in technical documents?**
  - Basis: Conclusion states "Currently, information extraction from flowcharts remains an open challenge"
  - Why unresolved: Flowcharts require understanding directional logic and decision nodes which current VLMs failed to capture adequately for retrieval purposes
  - What evidence would resolve it: A specialized processing module capable of converting flowchart topology into a sequential or conditional text format that integrates seamlessly into the RAG retrieval context

- **Can open-source Vision-Language Models (VLMs) be optimized to minimize "unnecessary descriptions" while maximizing specific data extraction for technical image interpretation?**
  - Basis: Discussion notes that VLMs "might describe the images' components with unnecessary descriptions rather than specific details"
  - Why unresolved: The current pipeline uses Llama-3.2-90b-vision-preview, which generates verbose descriptions that may dilute retrieval precision
  - What evidence would resolve it: Comparative evaluation of fine-tuned VLMs versus baseline on technical diagram dataset, measuring precision and recall of specific numerical or label-based facts

- **To what extent do automated LLM-based evaluation metrics (like RAGAS and DeepEval) correlate with human expert judgment in the context of complex technical question-answering?**
  - Basis: Evaluation acknowledges that "quality... depends on personal judgment" and relies on Llama-3.3-70B for scoring without validating against human domain experts
  - Why unresolved: Automated metrics may miss subtle technical inaccuracies or context misinterpretations that human experts would catch
  - What evidence would resolve it: Human-in-the-loop study where technical experts score pipeline's answers, followed by statistical correlation analysis against automated scores

## Limitations

- Table parsing structure loss: Converting complex nested tables to HTML and then text often loses alignment between rows and columns
- VLM performance ceiling: Open-source VLMs add "unnecessary descriptions" to technical diagrams, potentially diluting retrieval quality
- Flowchart interpretation gap: Current VLMs cannot adequately understand directional logic and decision nodes in flowcharts

## Confidence

- **High**: Faithfulness (94-96%) and answer relevancy (87-93%) scores on test set; two-stage retrieval mechanism clearly outperforms single-stage on table-based and out-of-context questions
- **Medium**: RAFT fine-tuning effectiveness—dataset construction method is clear, but distractor quality impact on discrimination ability is not empirically validated
- **Low**: Image interpretation capability—limited quantitative results provided; qualitative limitations acknowledged for flowcharts and complex diagrams

## Next Checks

1. **Document source verification**: Attempt to locate the cited technical manuals and fiction books to reconstruct the exact RAFT dataset composition
2. **Table parsing accuracy audit**: Manually verify 20+ parsed table descriptions against originals, measuring row/column preservation rate and structural fidelity
3. **Out-of-context refusal test**: Systematically submit 15+ questions unrelated to the document corpus, measuring hallucination rate vs. correct refusal