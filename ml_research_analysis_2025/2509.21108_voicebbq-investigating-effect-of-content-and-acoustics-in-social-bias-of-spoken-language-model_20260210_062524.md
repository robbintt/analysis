---
ver: rpa2
title: 'VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of
  Spoken Language Model'
arxiv_id: '2509.21108'
source_url: https://arxiv.org/abs/2509.21108
tags:
- bias
- gender
- speech
- acoustic
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VoiceBBQ, a spoken extension of the BBQ bias
  benchmark, designed to evaluate social bias in Spoken Language Models (SLMs) from
  both content and acoustic aspects. By synthesizing speech with controlled gender
  and accent variations, the study systematically isolates how these two sources contribute
  to bias.
---

# VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model

## Quick Facts
- **arXiv ID:** 2509.21108
- **Source URL:** https://arxiv.org/abs/2509.21108
- **Reference count:** 22
- **Primary result:** VoiceBBQ reveals that SLMs inherit content bias from backbone LLMs while frozen encoders propagate acoustic bias, with architectural choices determining the balance between these two bias sources.

## Executive Summary
This paper introduces VoiceBBQ, a spoken extension of the BBQ bias benchmark, designed to evaluate social bias in Spoken Language Models (SLMs) from both content and acoustic aspects. By synthesizing speech with controlled gender and accent variations, the study systematically isolates how these two sources contribute to bias. Experiments on LLaMA-Omni and Qwen2-Audio reveal that Qwen2-Audio inherits content biases strongly from its backbone LLM and minimizes acoustic bias, while LLaMA-Omni exhibits less content bias inheritance but shows significant bias variation across speaker gender and accent due to its frozen Whisper encoder. The findings demonstrate that SLMs can inherit content-induced bias and be affected by acoustic features, with architectural choices influencing both types of bias differently. VoiceBBQ provides a robust framework for diagnosing and understanding bias in SLMs across both dimensions.

## Method Summary
The study evaluates bias in SLMs by converting the BBQ text benchmark into speech using Kokoro-TTS with 16 speakers (4 per gender×accent combination: American/British × Male/Female), creating 935,872 audio samples. Two SLMs were tested: LLaMA-Omni with frozen Whisper encoder and Qwen2-Audio with end-to-end training. Speech contexts were synthesized by segmenting multi-sentence contexts, generating per-sentence audio, and concatenating results. Generated responses were mapped to A/B/C options via prompt-based classification. Bias scores were computed following BBQ protocol for ambiguous and disambiguated contexts, with Pearson correlations measuring content bias inheritance and McNemar's tests assessing acoustic bias significance.

## Key Results
- Qwen2-Audio exhibits strong correlation (r = 0.844) with its backbone LLM bias patterns, demonstrating content bias inheritance
- LLaMA-Omni shows significant bias variation across speaker gender and accent due to frozen Whisper encoder propagation
- Acoustic bias manifests as statistically significant response differences (McNemar's test) when same content is spoken by different voices
- End-to-end trained Qwen2-Audio substantially dampens acoustic cues while preserving content fidelity

## Why This Works (Mechanism)

### Mechanism 1: Content Bias Inheritance from Backbone LLM
If an SLM is built upon a pre-existing text-only LLM, it likely inherits the social biases present in that backbone, although the strength of this inheritance depends on the fine-tuning data. The pre-trained LLM contains static weights encoding semantic associations (stereotypes). Unless the speech-modality fine-tuning specifically disrupts these weights or introduces counter-balancing data, the SLM will replicate the backbone's tendency to select stereotypical answers in ambiguous contexts. The core assumption is that the speech-adapter training does not fundamentally rewrite the semantic reasoning core of the LLM. Evidence shows Qwen2-Audio exhibits a strong correlation with its backbone Qwen1 (Pearson correlation of r = 0.844). This breaks if the SLM is trained on a fundamentally different instruction dataset (e.g., LLaMA-Omni on InstructS2S-200K), where the correlation with the original backbone may decouple (r=0.301).

### Mechanism 2: Acoustic Bias via Frozen Encoder Propagation
Architectures that utilize a frozen speech encoder and a lightweight adapter are prone to propagating speaker-specific acoustic features (like gender or accent) into the LLM's reasoning process. A frozen encoder (like Whisper-large-v3) outputs high-dimensional representations that preserve paralinguistic features. If the subsequent adapter is simple and does not normalize these features, the LLM receives "extra" information beyond just text, conditioning its probability estimates on the speaker's identity. The core assumption is that the frozen encoder does not strip paralinguistic data, and the LLM is sensitive enough to these variations in the embedding space to alter token generation. Evidence shows LLaMA-Omni exhibited significant differences in responses across speaker characteristics, with speaker characteristics transmitted without substantial transformation. This leakage may be reduced if the adapter is trained with a loss function explicitly penalizing speaker classification (adversarial training), or if the encoder is initialized randomly and trained end-to-end.

### Mechanism 3: Acoustic Normalization via End-to-End Training
End-to-end training of the audio encoder with the LLM creates representations that are more invariant to speaker identity, reducing acoustic bias. When the audio encoder is trained jointly with the LLM, the optimization process learns to pool or attend to features necessary for the linguistic task while ignoring features (like pitch or accent) that do not predict the correct answer. This "washes out" acoustic specificity. The core assumption is that the training objective prioritizes semantic content over speaker fidelity, leading to an embedding space where different speakers map to similar semantic vectors. Evidence shows Qwen2-Audio substantially dampens these cues while preserving content fidelity, with internal representations shaped in a manner that reduces sensitivity to acoustic variations. This mechanism might prevent the model from distinguishing speakers if the downstream task requires speaker identification.

## Foundational Learning

- **Concept: BBQ Evaluation Protocol (Ambiguous vs. Disambiguated)**
  - **Why needed here:** The paper relies on the BBQ metric which distinguishes between contexts that lack information (testing bias by checking if the model guesses a stereotype vs. says "Unknown") and contexts with clear answers (testing if bias overrides facts).
  - **Quick check question:** If a model answers "Unknown" to an ambiguous BBQ question, does it incur a bias score penalty?

- **Concept: Frozen vs. Fine-Tuned Encoders**
  - **Why needed here:** The core finding rests on the difference between keeping the Whisper encoder fixed (LLaMA-Omni) vs. training it (Qwen2-Audio). You must understand that "frozen" means the model retains whatever features it learned in pre-training, regardless of the new task.
  - **Quick check question:** Does a frozen Whisper encoder output the same vector for the same audio file every time, regardless of the LLM it is attached to?

- **Concept: McNemar's Test**
  - **Why needed here:** The authors use this statistical test to prove that the change in model responses (e.g., switching from Male to Female voice) is not random noise but a statistically significant shift in decision making.
  - **Quick check question:** If McNemar's test returns a low p-value (<0.05) when comparing Male vs. Female responses, what does that imply about the model's dependence on gender?

## Architecture Onboarding

- **Component map:** Audio Context + Text Question → Speech Encoder (Whisper) → Adapter/Projector → LLM Backbone → Answer

- **Critical path:**
  1. **Data Validation:** Ensure TTS actually produced distinct genders/accents (verified by 96.5%/94.8% classifier accuracy in the paper).
  2. **Inference:** Run the SLM on the same text content spoken by different voices.
  3. **Analysis:** Calculate Bias Score differences (Δs) across voices; if Δs > 3% or McNemar's test is significant, flag as acoustic bias.

- **Design tradeoffs:**
  - **Modular (Frozen Encoder):** Cheaper to train; retains high-fidelity acoustic info (good for transcription), but leaks speaker bias into reasoning.
  - **Integrated (End-to-End):** Expensive to train; normalizes acoustic features (fairer reasoning), but may lose fine-grained speaker nuance.

- **Failure signatures:**
  - **The "Chameleon" Failure:** The model answers correctly for a Male voice but incorrectly for a Female voice on the exact same logic puzzle.
  - **The "Backbone Mimic":** The SLM bias scores correlate perfectly (r>0.8) with the text-only model, suggesting the audio integration added no new robustness or safety alignment.

- **First 3 experiments:**
  1. **Content Baseline:** Run the standard text-only BBQ on the backbone LLMs (LLaMA-3.1, Qwen1) to establish the "inherent" bias floor.
  2. **Acoustic Ablation:** Run VoiceBBQ on LLaMA-Omni. Specifically compare "American Male" vs "British Female" contexts to see if the variance observed in Table 2 appears.
  3. **Consistency Check:** Filter for "Disambiguated" items (where the answer is factually known) and verify if accuracy drops when the speaker's accent changes. This confirms if bias is overriding fact retrieval.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the observed bias patterns (content inheritance vs. acoustic sensitivity) generalize to other SLM architectures, such as cascaded systems or different end-to-end models? The authors state they "employed only two models for our analysis" and that "Further investigation is needed to generalize our findings to other model architectures." This remains unresolved due to resource constraints and availability of functional open-source inference code.

- **Open Question 2:** What specific architectural modifications or training interventions can effectively mitigate content and acoustic biases in SLMs without degrading performance? The Limitations section notes the study "focused on diagnosing... without proposing concrete methods for mitigating these biases," with plans to "design and evaluate a SLM architecture that actively mitigates the content and acoustic biases."

- **Open Question 3:** What are the underlying sociocultural or data-driven mechanisms that cause specific acoustic features (e.g., male voices) to amplify biases in certain categories like gender identity or SES? The authors acknowledge their analysis "lacks sufficient exploration of the sociocultural mechanisms" and asks "why male voices trigger stronger gender identity biases... remains largely unexplored."

## Limitations
- Findings are based on synthetic TTS speech data, which may not fully capture natural speech variability despite validation with classifier accuracy rates
- Response mapping relies on approximate string matching with configurable thresholds, introducing potential subjectivity
- Study focuses on only two specific SLM architectures, limiting generalizability to other speech processing approaches

## Confidence
- **High Confidence:** The architectural differences between LLaMA-Omni (frozen encoder) and Qwen2-Audio (end-to-end training) and their respective bias patterns are well-supported by experimental results and statistical analysis
- **Medium Confidence:** The inheritance of content bias from backbone LLMs is demonstrated through correlation analysis, but the strength of this inheritance may vary with different training datasets and fine-tuning approaches not explored in this study
- **Medium Confidence:** The acoustic bias findings are robust within the controlled synthetic environment but require validation in naturalistic speech conditions to confirm real-world applicability

## Next Checks
1. **Natural Speech Validation:** Replicate the VoiceBBQ evaluation using naturally recorded speech samples from diverse speakers rather than synthetic TTS to verify if the observed bias patterns persist across real acoustic variations
2. **Cross-Architecture Generalization:** Test additional SLM architectures with different frozen encoder configurations and training approaches to determine if the observed bias patterns (frozen encoder leakage vs. end-to-end normalization) hold across a broader range of implementations
3. **Bias Mitigation Testing:** Apply established bias mitigation techniques (such as adversarial training or data augmentation) to the acoustic conditioning problem and measure whether these interventions can reduce the observed speaker-dependent bias variations in SLMs