---
ver: rpa2
title: 'Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue
  Summarization'
arxiv_id: '2512.14687'
source_url: https://arxiv.org/abs/2512.14687
tags:
- dialogue
- speech
- spoken
- dialogsum
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spoken DialogSum addresses the lack of data linking speech, summaries,
  and paralinguistic cues for emotion-aware or spoken dialogue summarization. The
  authors build a pipeline that transforms scripted dialogues into realistic conversations
  with fillers, disfluencies, and backchannels, then synthesize emotion-rich speech
  using a conditional TTS model.
---

# Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization

## Quick Facts
- arXiv ID: 2512.14687
- Source URL: https://arxiv.org/abs/2512.14687
- Reference count: 0
- 13,460 emotion-diverse dialogues with factual and emotion-focused summaries

## Executive Summary
Spoken DialogSum introduces a novel dataset for emotion-aware spoken dialogue summarization, addressing the lack of aligned speech, text, and paralinguistic data. The authors develop a pipeline that transforms scripted dialogues into realistic conversations with fillers, disfluencies, and backchannels, then synthesize emotion-rich speech using a conditional TTS model. The resulting corpus contains 13,460 emotion-diverse dialogues, each with factual and emotion-focused summaries plus utterance-level speaker attributes. Experimental results demonstrate that an Audio-LLM improves emotion-rich summary ROUGE-L by 28% over a cascaded ASR-LLM system, validating the value of end-to-end speech modeling.

## Method Summary
The dataset generation pipeline transforms DialogSum scripts into realistic conversations using LLMs to add fillers and backchannels based on Switchboard style. GPT-4o-mini annotates each utterance with emotion, pitch, and speaking rate, while Zonos-hybrid TTS synthesizes speech conditioned on these paralinguistic tags and speaker prompts from GigaSpeech. Audio assembly incorporates overlapping speech and backchannels using timing from the CANDOR corpus. The resulting corpus contains 13,460 dialogues with factual and emotion-focused summaries, evaluated using both cascaded ASR-LLM baselines and Audio-LLM architectures.

## Key Results
- Audio-LLM (SALMONN) achieves 28% higher ROUGE-L on emotion-rich summaries compared to cascaded Whisper + LLaMA-2 baseline
- SALMONN-13B shows 6.7% improvement on factual summaries and 5.7% on emotion-rich summaries over the cascaded baseline
- SALMONN-13B reaches 90.2% accuracy on paralinguistic classification (Age/Gender/Emotion) on Spoken DialogSum test set

## Why This Works (Mechanism)
The paper's core innovation lies in conditioning text-to-speech synthesis on paralinguistic features (emotion, pitch, speaking rate), creating audio that preserves emotional nuances typically lost in text-only representations. This synthetic speech, when processed by Audio-LLMs, retains acoustic information about speaker characteristics and emotional tone that cascaded systems discard during ASR transcription. The end-to-end Audio-LLM architecture directly maps audio to summaries, maintaining the rich acoustic signal throughout the processing pipeline.

## Foundational Learning
- **Dialogue Summarization & Paralinguistics**: Essential for understanding the distinction between semantic content and paralinguistic features like emotion and tone that this paper combines. Quick check: Can you name three paralinguistic features present in speech but absent from a text transcript?
- **End-to-End vs. Cascaded Spoken Dialogue Modeling**: Critical for interpreting the experimental comparison between systems that process raw audio directly versus those that discard acoustic information at ASR. Quick check: In a cascaded system, at what point is all acoustic information (like tone of voice) discarded?
- **Audio-Language Model (Audio-LLM) Fusion Architectures**: Necessary to understand how models like SALMONN connect audio encoders to text LLMs via adapter layers. Quick check: What is the role of the "adapter" or "Q-Former" module in an Audio-LLM?

## Architecture Onboarding
- **Component map**: Text Rewrite (LLAMA3.3 70B) -> Paralinguistic Annotation (GPT-4o-mini) -> Conditional TTS (Zonos-hybrid) -> Audio Assembly -> Audio-LLM Evaluation (SALMONN, WavLLM, Qwen-Audio)
- **Critical path**: Audio-LLM (e.g., SALMONN-13B) processing raw audio from Spoken DialogSum for factual and emotion-rich summarization tasks, compared against cascaded Whisper + LLaMA-2 baseline
- **Design tradeoffs**: Synthetic data enables scale (160h) and perfect alignment but risks sim-to-real gaps; LLM-based rewriting is scalable but may propagate biases; conditional TTS preserves emotion but may not capture authentic speech patterns
- **Failure signatures**: Cascaded system outperforming Audio-LLM on emotion-rich task would invalidate end-to-end claims; poor human evaluation scores would indicate flawed synthetic data pipeline; failure to generalize to real datasets would show synthetic data limitations
- **First 3 experiments**: 1) Establish cascaded baseline with Whisper + LLaMA-2 on factual and emotion-rich tasks, 2) Evaluate SALMONN-13B on same tasks using raw audio, 3) (Ablation) Synthesize dataset without paralinguistic conditioning and re-evaluate emotion-rich performance to confirm conditioning's value

## Open Questions the Paper Calls Out
None

## Limitations
- Entirely synthetic dataset introduces potential sim-to-real gaps despite human evaluations suggesting high naturalness
- Reliance on LLM-based rewriting and annotation may propagate model biases rather than capture authentic human distributions
- Conditional TTS conditioning on emotion and acoustic tags may not fully capture the complexity of real emotional speech patterns

## Confidence
- **High Confidence**: Experimental results showing Audio-LLM superiority over cascaded systems on emotion-rich summarization (28% ROUGE-L improvement)
- **Medium Confidence**: Claims about dataset naturalness and emotional consistency based on human evaluations are supported but limited by synthetic nature
- **Medium Confidence**: Assertion that Spoken DialogSum will advance emotion-aware spoken dialogue summarization research depends on broader community adoption

## Next Checks
1. Evaluate Audio-LLMs trained on Spoken DialogSum on established real-world datasets like IEMOCAP or DailyDialog to quantify sim-to-real gap
2. Conduct controlled experiments removing paralinguistic conditioning from TTS synthesis and re-evaluating emotion-rich summarization performance
3. Compare multiple Audio-LLM architectures across diverse summarization tasks and paralinguistic classification to assess critical architectural components