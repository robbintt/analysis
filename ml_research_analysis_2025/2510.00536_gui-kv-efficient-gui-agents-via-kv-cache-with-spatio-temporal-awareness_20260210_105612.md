---
ver: rpa2
title: 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness'
arxiv_id: '2510.00536'
source_url: https://arxiv.org/abs/2510.00536
tags:
- tokens
- gui-kv
- cache
- attention
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency challenge of GUI agents,
  which process long sequences of high-resolution screenshots and face slow inference,
  high costs, and memory bottlenecks. The authors introduce GUI-KV, a plug-and-play
  KV cache compression method that exploits the spatial and temporal redundancy of
  GUI environments.
---

# GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness

## Quick Facts
- **arXiv ID:** 2510.00536
- **Source URL:** https://arxiv.org/abs/2510.00536
- **Reference count:** 40
- **Primary result:** Recovers near-full-cache accuracy with 10-20% KV cache budget on GUI agents

## Executive Summary
GUI agents process long sequences of high-resolution screenshots, leading to slow inference, high costs, and memory bottlenecks. This paper introduces GUI-KV, a plug-and-play KV cache compression method that exploits the spatial and temporal redundancy of GUI environments. By combining spatial saliency guidance (using L2 norms of hidden states) with temporal redundancy scoring (via QR decomposition), GUI-KV achieves near-full-cache accuracy at modest budgets (typically 10-20%). The method outperforms competitive baselines and enables efficient GUI agent performance under tight memory constraints.

## Method Summary
GUI-KV is a KV cache compression method designed specifically for Vision-Language Model (VLM) based GUI agents. It operates in two phases: prefill and decoding. During prefill, it scores visual tokens using both attention scores and L2 norms of hidden states (spatial saliency), then performs QR decomposition on current frame keys to project and filter redundant historical tokens (temporal redundancy). During decoding, it appends new tokens to the pruned cache. The method uses uniform budget allocation across transformer layers, unlike more complex layer-varying schemes.

## Key Results
- Recovers near-full-cache accuracy with 10-20% KV cache budget on six GUI benchmarks
- Achieves 38.9% decoding FLOPs reduction on AgentNetBench with 5 screenshots
- Outperforms PyramidKV and VL-Cache baselines in both accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Spatial Saliency Guidance
- **Claim**: Augmenting attention scores with L2 norms of hidden states improves identification of semantically important visual tokens
- **Mechanism**: L2 norms serve as proxy for "information payload" strength, combining with attention to capture both contextual relevance and absolute content strength
- **Evidence**: L2 norm outperforms Sobel filter, pixel histogram entropy, and center-surround contrast in Table 5

### Mechanism 2: Temporal Redundancy Scoring
- **Claim**: QR decomposition-based projection identifies and prunes visually redundant tokens from historical frames
- **Mechanism**: Project previous frames' keys onto current frame's key subspace; tokens with small residuals are redundant and can be pruned
- **Evidence**: 38.9% FLOPs reduction on AgentNetBench demonstrates effectiveness of this approach

### Mechanism 3: Uniform Budget Allocation
- **Claim**: GUI attention sparsity is uniformly high (>0.99) across all transformer layers
- **Mechanism**: Simple uniform budget allocation avoids over-amplification of tiny numerical differences that layer-varying schemes create in GUI contexts
- **Evidence**: Figure 2 shows uniform allocation (SnapKV) outperforms PyramidKV and VL-Cache

## Foundational Learning

- **Concept: KV Cache Mechanics in Autoregressive Models**
  - **Why needed here**: GUI-KV operates during prefill and decoding phases, selectively pruning cached key-value pairs
  - **Quick check question**: During decoding, when a new token is generated, how are its key-value pairs integrated with the existing (potentially pruned) cache?

- **Concept: Attention Sparsity and Token Importance**
  - **Why needed here**: The entire method hinges on identifying which tokens matter via attention patterns and hidden state norms
  - **Quick check question**: If attention sparsity dropped from 0.99 to 0.90 in your domain, how would this affect the minimum viable cache budget?

- **Concept: Subspace Projection for Redundancy Detection**
  - **Why needed here**: Temporal redundancy scoring uses QR decomposition to find orthonormal bases and compute projection residuals
  - **Quick check question**: If a previous frame token's key vector lies entirely within the current frame's key subspace, what would its residual norm ρ equal, and what does this imply for retention decisions?

## Architecture Onboarding

- **Component map**: Prefill Phase: Screenshot → Vision Encoder → Visual Tokens → For each layer/head: compute attention scores, extract hidden states, compute L2 norms (spatial), QR decomposition on current frame keys, project past keys for temporal scores, combine signals → Top-k selection → Pruned KV cache. Decoding Phase: Generated token → Append to pruned cache → Continue generation with reduced memory footprint

- **Critical path**:
  1. **Saliency weight α calibration** (Equation 5): Too low → spatial guidance ignored; too high → attention signal drowned out. Paper uses α=2.
  2. **QR rank r selection** (Equation 6): Controls subspace dimensionality. Paper finds r=32 optimal (Table 4); too small → underspecifies current frame; too large → computational overhead.
  3. **Uniform budget γ**: Paper uses uniform γ across all layers. Critical to validate this assumption for your domain before deployment.

- **Design tradeoffs**:
  - **Spatial-only vs. Temporal-only**: Spatial excels with fewer screenshots (≤7), temporal with more (≥7). Combine both for robustness.
  - **Rank r vs. accuracy**: Table 4 shows r=32 best but r=8-128 all viable. Higher r increases QR cost but may better capture complex GUIs.
  - **Budget γ vs. performance**: 10-20% budget typically recovers near-full-cache accuracy for UI-TARS-1.5-7B; OpenCUA-7B requires more.

- **Failure signatures**:
  - **Sudden accuracy drop at specific budget**: Budget too low for your model's redundancy characteristics.
  - **Non-monotonic performance across budgets**: Unstable token selection. May need to adjust observation window ω or temperature τ.
  - **Temporal component hurts performance**: High visual change rate in your domain. Try spatial-only variant or increase rank r.

- **First 3 experiments**:
  1. **Baseline budget sweep**: Run GUI-KV at budgets [1%, 5%, 10%, 20%, 40%, 80%] on your target benchmark. Plot accuracy vs. budget to find saturation point.
  2. **Ablation: Spatial vs. Temporal**: With fixed budget (γ=20%), test spatial-only, temporal-only, and combined variants across screenshot counts [3, 5, 7, 10].
  3. **Cross-benchmark generalization**: Test whether uniform budget allocation holds for your specific GUI domain.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific architectural or training factors enable UI-TARS-1.5-7B to maintain accuracy under aggressive KV eviction, whereas OpenCUA-7B exhibits significant sensitivity?
- **Open Question 2**: Does the performance drop beyond 7 screenshots imply that GUI agents require dynamic context horizon management rather than fixed-length history compression?
- **Open Question 3**: Can the spatial saliency guidance be enhanced by learning the trade-off parameter α or using trainable attention projections instead of fixed L2 norms?

## Limitations

- **Sparsity Assumption**: The uniform budget allocation strategy relies on the claim that GUI attention sparsity is uniformly high (>0.99) across all transformer layers, which may not generalize to all GUI environments
- **Domain Generalization**: The method shows strong performance on web and mobile UI screenshots but hasn't been validated on radically different GUI domains like 3D interfaces, AR/VR environments, or specialized industrial control panels
- **Implementation Complexity**: The temporal redundancy scoring using QR decomposition is computationally non-trivial and requires careful implementation to avoid numerical instability

## Confidence

- **High Confidence**: Spatial saliency guidance effectiveness, temporal redundancy scoring for static backgrounds, overall accuracy recovery
- **Medium Confidence**: Uniform budget allocation superiority, parameter calibration values
- **Low Confidence**: Subspace projection residual interpretation, cross-model generalization

## Next Checks

- **Validation Check 1**: Verify the uniform high sparsity (>0.99) assumption holds for your specific GUI domain by running attention analysis on representative screenshots
- **Validation Check 2**: Test GUI-KV on GUI environments outside standard web/mobile benchmarks, specifically on high visual dynamism, specialized industrial interfaces, and 3D/AR interfaces
- **Validation Check 3**: Conduct systematic ablation studies on key parameters (α, r, τ, ω) for your specific model and domain to identify the most impactful parameters for your use case