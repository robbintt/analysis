---
ver: rpa2
title: Memory OS of AI Agent
arxiv_id: '2506.06326'
source_url: https://arxiv.org/abs/2506.06326
tags:
- memory
- user
- memoryos
- retrieval
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of limited long-term memory and
  contextual coherence in AI agents powered by large language models. The proposed
  MemoryOS framework introduces a hierarchical memory management system inspired by
  operating system principles, organizing memory into short-term, mid-term, and long-term
  storage units with dynamic updating, semantic retrieval, and persona-driven generation.
---

# Memory OS of AI Agent

## Quick Facts
- arXiv ID: 2506.06326
- Source URL: https://arxiv.org/abs/2506.06326
- Reference count: 9
- Key outcome: MemoryOS achieves average improvements of 49.11% in F1 and 46.18% in BLEU-1 over baselines like MemGPT and A-Mem on LoCoMo benchmark.

## Executive Summary
MemoryOS addresses the challenge of limited long-term memory and contextual coherence in AI agents powered by large language models. The framework introduces a hierarchical memory management system inspired by operating system principles, organizing memory into short-term, mid-term, and long-term storage units with dynamic updating, semantic retrieval, and persona-driven generation. Experiments on the LoCoMo benchmark demonstrate MemoryOS achieves significant improvements in F1 (49.11%) and BLEU-1 (46.18%) metrics compared to existing memory management systems.

## Method Summary
MemoryOS implements a three-tier hierarchical memory system: STM (short-term memory) uses a fixed-length FIFO queue for immediate context, MTM (mid-term memory) employs segmented paging with topic clustering and heat-based eviction, and LPM (long-term memory) stores structured user and agent profiles. The system migrates data between tiers based on engagement metrics, using a composite "Heat" score combining frequency, interaction length, and recency. MemoryOS uses a two-stage retrieval process combining semantic and keyword-based search to fetch relevant context from all three tiers for personalized response generation.

## Key Results
- MemoryOS achieves average improvements of 49.11% in F1 and 46.18% in BLEU-1 over baselines like MemGPT and A-Mem on LoCoMo benchmark
- MemoryOS reduces LLM calls per response to ~4.9 compared to A-Mem's 13 calls while maintaining superior performance
- The system demonstrates effective memory migration between STM → MTM → LPM tiers with optimal threshold values (θ=0.6 for segmentation, τ=5 for LPM transfer)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Memory Migration (STM → MTM → LPM)
- **Claim:** Organizing memory into a three-tier hierarchy with distinct migration protocols maintains coherence better than flat or dual-tier systems.
- **Mechanism:** STM uses FIFO queue, migrates to MTM when full; MTM uses segmented paging by topic with heat-based eviction; high-engagement data updates LPM.
- **Core assumption:** Conversation history follows power-law distribution where recent context and high-value semantic clusters are sufficient for generation.
- **Evidence anchors:** Abstract describes the three-tier architecture with dynamic updates; Section 3.1 details the three levels of storage units; neighbor paper supports hierarchical structures.

### Mechanism 2: Heat-Based Retention and Eviction
- **Claim:** Prioritizing memory segments using composite "Heat" score (frequency + interaction volume + recency) effectively mimics human retention by discarding low-value context.
- **Mechanism:** Heat score calculated as H = α·N_visit + β·L_interaction + γ·R_recency; lowest heat segments evicted from MTM; segments exceeding threshold τ promoted to LPM.
- **Core assumption:** Access frequency and recency are valid proxies for semantic importance and user preference.
- **Evidence anchors:** Section 3.3 describes heat-based eviction mechanism; Section 4.2 shows MemoryBank's poor performance with Ebbinghaus decay; neighbor paper warns hand-designed heuristics may lack optimality.

### Mechanism 3: Persona-Driven Response Integration
- **Claim:** Decoupling static user profiles from dynamic interaction history and fusing them during retrieval ensures personalized, consistent responses.
- **Mechanism:** LPM stores static profile data and dynamic User Traits (90 dimensions); system retrieves context from all three tiers and merges into single prompt.
- **Core assumption:** LLM can successfully synthesize conflicting or overlapping information from three distinct memory sources without hallucination.
- **Evidence anchors:** Abstract mentions user/agent profile incorporation; Section 3.5 describes final prompt construction; neighbor paper highlights challenges in memory-context integration.

## Foundational Learning

- **Concept:** Operating System Memory Management (Paging vs. Segmentation)
  - **Why needed here:** MemoryOS relies on OS metaphors (FIFO queues, pages, segments) to structure LLM context windows.
  - **Quick check question:** How does "segmentation" (grouping by topic) differ from "paging" (fixed-size blocks) in this architecture?

- **Concept:** Semantic Retrieval (Vector Search vs. Keyword Search)
  - **Why needed here:** MTM retrieval uses hybrid F_score combining cosine similarity and Jaccard similarity.
  - **Quick check question:** Why might pure vector similarity fail where hybrid approach (vectors + keywords) succeeds in memory retrieval?

- **Concept:** Persona Consistency in LLMs
  - **Why needed here:** LPM module attempts to maintain consistent "self" for agent and "profile" for user.
  - **Quick check question:** What is the risk of hard-coding persona traits vs. extracting them dynamically from dialogue?

## Architecture Onboarding

- **Component map:** STM (dialogue_pages + dialogue_chains) -> MTM (Segments containing Pages) -> LPM (User Profile + Agent Profile)
- **Critical path:** New utterance enters STM → STM Overflow triggers migration to MTM → MTM Update calculates Heat → Query triggers retrieval from all three tiers → LLM synthesizes combined prompt
- **Design tradeoffs:** Latency vs. Coherence (4.9 LLM calls vs. 13 for A-Mem); Recall vs. Noise (increasing k improves F1 but introduces noise)
- **Failure signatures:** Topic Bleeding (θ too low), Stale Persona (τ too high), Chain Breaking (meta_chain generation fails)
- **First 3 experiments:** 1) Hyperparameter Sensitivity (vary k on your data domain), 2) Heat Threshold (τ) Analysis, 3) Ablation on Segmented Paging

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does varying weighting coefficients (α, β, γ) in heat score calculation impact retention of critical information versus recent noise?
- **Basis in paper:** Equation 4 defines Heat score with three coefficients, but implementation details state these are "equality set to 1" without sensitivity analysis.
- **Why unresolved:** Unclear if retrieval frequency, interaction length, and recency are truly of equal importance.
- **Evidence to resolve it:** Ablation study varying weights and evaluating impact on LoCoMo's "Temporal" question category.

### Open Question 2
- **Question:** To what extent does pre-defined 90-dimension schema for User Traits limit system's ability to model users with niche or domain-specific preferences?
- **Basis in paper:** Section 3.3 describes constructing User Traits with "90 dimensions across three categories," implying fixed ontology.
- **Why unresolved:** Fixed schema may fail to capture unique user characteristics outside predefined categories.
- **Evidence to resolve it:** Comparative analysis in specialized domain measuring semantic drift between extracted traits and ground-truth profiles.

### Open Question 3
- **Question:** Can adaptive retrieval mechanism for number of dialogue pages (k) outperform static thresholding currently used?
- **Basis in paper:** Section 4.4 notes increasing retrieved pages improves performance up to point where excessive content introduces noise.
- **Why unresolved:** Paper selects fixed k=10 as compromise, but optimal k likely varies per query.
- **Evidence to resolve it:** Implementation of dynamic k-selector showing improved F1 scores without increased latency.

## Limitations
- MemoryOS requires extensive hyperparameter tuning sensitive to conversation patterns and user engagement levels
- Significant computational overhead from multiple model invocations per response
- Potential for context fragmentation when conversations span multiple domains

## Confidence
- **High confidence** in hierarchical architecture design and tiered memory management approach
- **Medium confidence** in specific hyperparameter choices (may require adjustment for different domains)
- **Low confidence** in long-term robustness over extended conversations beyond 300 turns

## Next Checks
1. **Domain transfer validation** - Test MemoryOS on different domains (technical support, creative writing, healthcare) to assess hyperparameter generalization
2. **Long-duration stability test** - Evaluate performance over extended conversations (10,000+ turns) to identify degradation in persona consistency
3. **User perception study** - Conduct human evaluations comparing MemoryOS responses against baselines to verify perceived improvements in conversational coherence