---
ver: rpa2
title: 'Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial
  Attack'
arxiv_id: '2505.21967'
source_url: https://arxiv.org/abs/2505.21967
tags:
- arxiv
- safety
- adversarial
- harmful
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates vulnerabilities in Large Vision-Language
  Models (LVLMs) to adversarial attacks. The authors conduct a systematic representational
  analysis to understand why conventional adversarial attacks can bypass safety mechanisms
  in LVLMs.
---

# Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack

## Quick Facts
- arXiv ID: 2505.21967
- Source URL: https://arxiv.org/abs/2505.21967
- Reference count: 23
- Large Vision-Language Models remain vulnerable to adversarial attacks despite safety mechanisms

## Executive Summary
This paper investigates vulnerabilities in Large Vision-Language Models (LVLMs) to adversarial attacks through a systematic representational analysis. The authors develop a two-stage evaluation framework to assess both attack success rates and the severity of harmful responses. Their findings reveal that LVLMs without optical character recognition (OCR) capabilities can still extract semantic cues from adversarially perturbed images through visual encoder alignment, while OCR-capable models remain susceptible due to weak cross-modal alignment. The proposed evaluation framework provides granular safety assessment by distinguishing between instruction non-compliance, refusals, and successful attacks.

## Method Summary
The authors propose a two-stage evaluation framework for assessing adversarial attacks on LVLMs. The first stage classifies model responses into three categories: instruction non-compliance (responses unrelated to the task), refusal (explicit safety rejections), and successful attack (harmful responses to adversarial inputs). The second stage quantifies response severity using a 5-point Likert scale while categorizing refusals into direct, soft, or partially helpful types. The framework combines automated classification with human evaluation to assess both attack effectiveness and safety behavior. Representational analysis examines how adversarial perturbations affect visual and textual representations across different model architectures.

## Key Results
- LVLMs without OCR can still extract semantic cues from adversarially perturbed images through visual encoder alignment
- OCR-capable models remain vulnerable due to weak cross-modal alignment between visual and language representations
- The two-stage evaluation framework enables granular assessment by distinguishing between safety-preserving behaviors and genuine attack successes

## Why This Works (Mechanism)
The paper demonstrates that adversarial attacks bypass safety mechanisms in LVLMs through multiple pathways. Without OCR, models capture semantic information from visual patterns even when text recognition is disabled, suggesting that visual encoders align with textual representations in ways that preserve attack-relevant information. For OCR-capable models, the cross-modal alignment between visual features and language embeddings appears insufficient to filter harmful content, allowing attacks to succeed even when text recognition should theoretically enable safety filtering. The two-stage evaluation framework works by systematically separating safety-preserving behaviors (refusals, non-compliance) from actual harmful responses, providing clearer measurement of attack success.

## Foundational Learning
- Adversarial attack effectiveness on LVLMs: Understanding how attacks can manipulate model behavior despite safety training
  - Why needed: Establishes the practical security implications and motivates the need for robust evaluation frameworks
  - Quick check: Measure attack success rates across different model architectures and safety training regimes

- Representational analysis techniques: Methods for examining how models encode and process adversarial perturbations
  - Why needed: Reveals which features models extract from adversarial inputs regardless of safety mechanisms
  - Quick check: Compare representations of clean vs. adversarial images across model components

- Cross-modal alignment in vision-language models: How visual and textual representations interact and influence each other
  - Why needed: Explains why certain model architectures remain vulnerable despite having safety features like OCR
  - Quick check: Measure alignment quality between visual and language embeddings before and after safety training

## Architecture Onboarding
- Component map: Vision encoder -> Cross-modal attention layers -> Language decoder -> Safety classifier
- Critical path: Input image -> Visual feature extraction -> Adversarial perturbation analysis -> Response generation
- Design tradeoffs: Safety mechanisms (OCR, alignment) vs. attack vulnerability, evaluation granularity vs. computational cost
- Failure signatures: Semantic cue extraction without OCR, weak cross-modal filtering, misclassification of safety responses
- First experiments: 1) Test attack success rates on models with disabled vs. enabled OCR, 2) Measure cross-modal alignment quality using standard metrics, 3) Conduct ablation studies removing visual features systematically

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Representational analysis lacks detailed ablation studies identifying specific visual features enabling attack success
- 5-point Likert scale for severity quantification may introduce subjectivity across annotators
- Claims about weak cross-modal alignment lack systematic investigation of alignment mechanisms

## Confidence
- Core findings about attack effectiveness: High
- Specific mechanisms explaining vulnerabilities: Medium
- Evaluation framework utility: Medium

## Next Checks
1. Conduct ablation studies systematically removing visual features to identify which components enable semantic cue extraction from adversarial perturbations
2. Measure inter-annotator agreement for the 5-point Likert scale severity ratings and refusal categorizations across multiple annotator pools
3. Perform controlled experiments comparing cross-modal alignment quality between models with and without OCR capabilities using standardized alignment metrics