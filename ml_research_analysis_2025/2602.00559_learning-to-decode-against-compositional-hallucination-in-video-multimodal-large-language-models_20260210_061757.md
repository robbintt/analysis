---
ver: rpa2
title: Learning to Decode Against Compositional Hallucination in Video Multimodal
  Large Language Models
arxiv_id: '2602.00559'
source_url: https://arxiv.org/abs/2602.00559
tags:
- video
- hallucination
- wang
- compositional
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of compositional hallucinations
  in video multimodal large language models (VLLMs), where incorrect reasoning over
  multiple interacting spatial and temporal factors leads to unreliable video understanding.
  The authors introduce OmniVCHall, a comprehensive benchmark designed to systematically
  evaluate both isolated and compositional hallucinations across eight fine-grained
  types, including a novel camera-based category, with 823 videos and 9,027 visual
  question answering (VQA) samples.
---

# Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2602.00559
- **Source URL**: https://arxiv.org/abs/2602.00559
- **Reference count**: 40
- **Primary result**: Introduces TriCD, achieving over 10% average accuracy improvement on compositional hallucination tasks in video VQA

## Executive Summary
This paper addresses compositional hallucinations in video multimodal large language models (VLLMs), where incorrect reasoning over multiple interacting spatial and temporal factors leads to unreliable video understanding. The authors introduce OmniVCHall, a comprehensive benchmark designed to systematically evaluate both isolated and compositional hallucinations across eight fine-grained types, including a novel camera-based category, with 823 videos and 9,027 visual question answering (VQA) samples. To mitigate these hallucinations, they propose TriCD, a triple-pathway contrastive decoding framework that combines an adaptive perturbation controller with saliency-guided enhancement, optimized via reinforcement learning. Experiments show that TriCD consistently improves performance, achieving an average accuracy gain of over 10% on two representative VLLM backbones, with the largest improvements seen in compositional hallucination tasks.

## Method Summary
The authors develop TriCD (Triple-pathway Contrastive Decoding), a framework that calibrates VLLM predictions through three parallel pathways: the original video processing, a negative path with adaptively perturbed video, and a positive path with saliency-enhanced video. The adaptive perturbation controller dynamically selects context-aware negative operations (blur, noise, reverse, shuffle, sample, grayscale, horizontal/vertical mirror) using reinforcement learning to construct negative video variants. The saliency-guided enhancement module fuses spatial saliency from DINOv3 attention with temporal saliency from optical flow to reinforce grounded visual evidence. The final logits combine these pathways using weighted residuals to suppress hallucination-prone outputs while preserving accurate reasoning.

## Key Results
- TriCD achieves over 10% average accuracy improvement on two representative VLLM backbones for compositional hallucination tasks
- The framework shows largest improvements on compositional hallucination tasks (C_YNQA, C_MCQA) compared to isolated hallucinations
- Adaptive perturbation controller significantly outperforms random perturbation baselines, with TriCD with RP achieving only 0.63 accuracy versus backbone performance
- Saliency-guided enhancement provides complementary benefits, with performance degradation observed when motion cues are removed from C_MCQA tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Triple-pathway contrastive decoding calibrates predictions by establishing grounded upper and hallucination-prone lower bounds around the original output.
- **Mechanism**: The final logit combines three streams: original (q^o_t), negative from perturbed video (q^n_t), and positive from saliency-enhanced video (q^p_t). Formula: q_t = q^o_t + α₁(q^p_t - q^o_t) + α₂(q^o_t - q^n_t). The negative residual suppresses hallucination pathways amplified by corrupted visual input; the positive residual reinforces grounded visual evidence.
- **Core assumption**: Hallucination patterns are amplified under perturbation and can be subtracted out; conversely, saliency-weighted tokens represent trustworthy grounding.
- **Evidence anchors**:
  - [abstract] "a contrastive decoding framework with a triple-pathway calibration mechanism"
  - [Section 3.3] "By treating the original prediction as a baseline and applying directed residual corrections, the model effectively widens the margin between grounded visual evidence and potential hallucination pathways"
  - [corpus] SEASON and related work confirm contrastive decoding mitigates temporal hallucinations via similar residual logit strategies
- **Break condition**: If perturbations do not reliably amplify hallucination patterns, or saliency maps misidentify irrelevant regions, residual corrections introduce noise rather than signal.

### Mechanism 2
- **Claim**: Adaptive perturbation controller selects context-aware negative operations more effectively than static perturbations.
- **Mechanism**: Cross-attention conditions learnable query tokens on video-query hidden states, then attends to tool description embeddings to score 8 perturbation tools (blur, noise, reverse, shuffle, sample, grayscale, horizontal/vertical mirror). Tools within cumulative probability threshold γ are selected and composed to construct V⁻.
- **Core assumption**: Different video-query contexts require different perturbation types to expose relevant hallucination vulnerabilities.
- **Evidence anchors**:
  - [abstract] "An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants"
  - [Section 3.2.1] "tools are ranked in descending order, and all tools within a cumulative probability threshold γ are selected"
  - [corpus] CounterVid confirms random perturbations fail to address root causes; adaptive selection is underexplored
- **Break condition**: If tool embeddings poorly capture functional semantics, or if selected perturbations over-corrupt useful signal, negative logit provides misleading guidance.

### Mechanism 3
- **Claim**: Fusing spatial (DINOv3) and temporal (optical flow) saliency provides more comprehensive visual grounding than either alone.
- **Mechanism**: Spatial saliency from CLS-to-patch attention captures foreground objects; temporal saliency from Farneback optical flow with dual-stage filtering isolates intentional motion. Learnable gate β conditions on hidden states to fuse: w_sal = β·s_spa + (1-β)·s_mot. Vision tokens reweighted: X'_v = w_sal ⊙ X_v.
- **Core assumption**: Compositional hallucinations require grounding across both static object presence and dynamic temporal transitions.
- **Evidence anchors**:
  - [abstract] "saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences"
  - [Section 3.2.2] "This mechanism establishes a grounded 'upper bound' by ensuring the model remains anchored to both critical objects and their dynamic interactions"
  - [corpus] Limited direct corpus evidence on DINOv3+optical flow fusion; most prior work uses single saliency source
- **Break condition**: If DINOv3 attention fails to localize relevant objects or optical flow captures background noise despite filtering, fusion weights misguide enhancement.

## Foundational Learning

- **Contrastive Decoding (CD) Fundamentals**
  - **Why needed here**: TriCD extends CD by introducing adaptive negative and positive pathways rather than static perturbation.
  - **Quick check question**: Can you explain why subtracting logits from a perturbed input suppresses hallucination pathways?

- **Policy Gradient (REINFORCE)**
  - **Why needed here**: Tool selection in APC is discrete and non-differentiable; RL enables learning without backprop through VLLM.
  - **Quick check question**: Why does REINFORCE require a baseline to reduce variance, and how does the moving average serve this role?

- **Spatial-Temporal Saliency Maps**
  - **Why needed here**: SGE must extract meaningful attention weights and motion signals to identify where to reinforce.
  - **Quick check question**: How does CLS-to-patch attention in Vision Transformers indicate foreground importance, and why must optical flow be filtered to isolate intentional motion?

## Architecture Onboarding

- **Component map**: Input: Video V, Query T -> Original Pass -> Vision/Text Tokens -> LLM Decoder -> q^o_t; APC (Negative) -> Perturbation Selection -> V⁻ -> LLM Decoder -> q^n_t; SGE (Positive) -> DINOv3 + Farneback -> X'_v -> LLM Decoder -> q^p_t -> Logit Fusion: q_t = q^o_t + α₁(q^p_t - q^o_t) + α₂(q^o_t - q^n_t)

- **Critical path**: APC tool selection -> perturbation application -> negative logit extraction (dominant contributor to ablation gains per Section 4.4). SGE fusion provides complementary grounding.

- **Design tradeoffs**:
  - γ threshold: Higher includes more tools (richer negative) but risks over-corruption
  - α₁ vs α₂ balance: α₁=0.8, α₂=0.4 optimal; over-weighting suppression harms fluency
  - Parallel vs sequential branching: Parallel maintains efficiency (9.12s vs 8.05s for MotionCD) but shares hidden states

- **Failure signatures**:
  - Random perturbation baseline (TriCD w/ RP) collapses to 0.63 accuracy, worse than backbone (Section 4.4)
  - Removing motion cues degrades C_MCQA specifically (0.64→0.62)
  - High α₂ without proper negatives amplifies noise

- **First 3 experiments**:
  1. Ablate APC alone vs SGE alone on compositional tasks (C_YNQA, C_MCQA) to confirm APC dominance
  2. Sweep γ ∈ {0.2, 0.4, 0.6} to find threshold sensitivity for different video types
  3. Replace DINOv3 with CLIP attention or remove temporal filtering to validate dual-saliency necessity

## Open Questions the Paper Calls Out
None

## Limitations
- The adaptive perturbation controller may not generalize to videos outside the benchmark's domains, potentially requiring domain-specific retraining for specialized content like medical imaging
- The 10% average accuracy improvement measured against controlled compositional errors may not translate directly to naturally occurring hallucinations in deployed systems
- The REINFORCE-based optimization requires careful reward shaping and hyperparameter tuning, with sensitivity to reward scaling potentially affecting reproducibility

## Confidence
- **High confidence**: The benchmark construction methodology (OmniVCHall) and its categorization of eight hallucination types are well-documented and reproducible. The claim that compositional hallucinations are more challenging than isolated ones is supported by consistent performance gaps across experiments.
- **Medium confidence**: The core mechanism of triple-pathway contrastive decoding is supported by ablation studies showing APC and SGE contributions. However, the specific optimal hyperparameters (γ=0.4, α₁=0.8, α₂=0.4) may require domain-specific tuning, and the claim of 10% average improvement should be interpreted as benchmark-specific rather than universal.
- **Low confidence**: The claim that DINOv3 attention combined with optical flow provides superior grounding compared to single-source saliency methods lacks extensive empirical validation against other saliency approaches.

## Next Checks
1. **Cross-domain generalization test**: Evaluate TriCD on video datasets outside OmniVCHall's domains (e.g., medical procedures, industrial processes, or wildlife documentation) to verify whether the learned perturbation controller transfers or requires domain-specific retraining.

2. **Ablation of perturbation selection strategy**: Replace the RL-learned APC with alternative selection methods (random selection with weighting, heuristic-based selection, or gradient-based soft selection) to quantify the marginal benefit of the current adaptive approach versus simpler alternatives.

3. **Real-world hallucination detection**: Apply TriCD to a corpus of VLLM outputs on unconstrained video content and measure actual hallucination reduction versus the benchmark-controlled environment, establishing whether the 10% improvement in controlled settings translates to practical deployment scenarios.