---
ver: rpa2
title: 'Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding'
arxiv_id: '2512.12822'
source_url: https://arxiv.org/abs/2512.12822
tags:
- spatial
- point
- object
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lemon, a unified transformer architecture
  for 3D multimodal understanding that processes point cloud patches and language
  tokens as a single sequence, eliminating the need for separate 3D encoders. The
  model employs a dynamic patchification scheme with hierarchical spatial partitioning
  and a three-stage training curriculum from object recognition to scene-level spatial
  reasoning.
---

# Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding

## Quick Facts
- arXiv ID: 2512.12822
- Source URL: https://arxiv.org/abs/2512.12822
- Reference count: 40
- Primary result: State-of-the-art 3D multimodal understanding via unified transformer architecture

## Executive Summary
Lemon introduces a unified transformer architecture that processes point cloud patches and language tokens as a single sequence, eliminating the need for separate 3D encoders. The model employs hierarchical spatial partitioning with Z→Y→X ordering and a three-stage curriculum training approach. Lemon achieves state-of-the-art performance across 3D understanding tasks, including 57.22% accuracy on embodied object QA and 74.32% on scene spatial awareness QA, while demonstrating favorable scaling properties compared to existing 3D LMMs.

## Method Summary
Lemon uses Qwen2.5-7B-Instruct as a backbone transformer that processes point cloud patches and language tokens jointly. Point clouds are dynamically partitioned using hierarchical Z→Y→X spatial splitting with adaptive split counts, converted to fixed-size patches via FPS sampling, and mapped to language embedding space through a linear projector. The model follows a three-stage curriculum: Stage 1 learns object recognition from 1.87M samples, Stage 2 bridges geometry to language via captioning/grounding (140K samples), and Stage 3 develops scene-level spatial reasoning (142K samples). Training uses modified LLaMA-Factory with batch size 512, learning rate 1e-5, and cosine schedule over 78 hours on 8×H100 GPUs.

## Key Results
- Achieves 57.22% accuracy on 3D MM-Vet embodied object QA benchmark
- Scores 74.32% on Scene Spatial Awareness QA task
- Outperforms ShapeLLM by +5.9 on 3D MM-Vet and +5.4 on Scene Spatial QA under identical conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unified token processing eliminates representational bottleneck from frozen 3D encoders
- **Mechanism:** Point cloud patches mapped to language embedding space via learnable linear projector, concatenated with text tokens into single sequence processed by one transformer
- **Core assumption:** Language model's representational capacity can learn task-relevant 3D features directly when trained at scale
- **Evidence anchors:** [abstract] "processes point cloud patches and language tokens as a single sequence, eliminating the need for separate 3D encoders"; [section 3.3, Table 4] Lemon outperforms ShapeLLM by +5.9 on 3D MM-Vet and +5.4 on Scene Spatial QA under identical conditions
- **Break condition:** If point cloud data scale remains severely limited (<100K diverse samples), unified approach may underfit compared to encoders pretrained on synthetic 3D datasets

### Mechanism 2
- **Claim:** Hierarchical spatial partitioning preserves geometric relationships in flattened token sequences
- **Mechanism:** Recursive Z→Y→X axis splitting with adaptive split counts, separator tokens (`<layer_sep>`, `<row_sep>`) encode spatial hierarchy
- **Core assumption:** Z→Y→X ordering aligns with gravitational and semantic hierarchies in indoor scenes (floor→table→object)
- **Evidence anchors:** [section 2.1] "The number of splits for each axis is determined adaptively based on point distribution"; [Appendix D.4, Table 7] Z→Y→X ordering achieves 53.45% on Scene Spatial QA vs 35.20% for FPS-based discontinuous sampling
- **Break condition:** If scenes have irregular geometries violating gravitational assumptions (e.g., rotated architectural spaces), fixed axis ordering may misrepresent spatial relationships

### Mechanism 3
- **Claim:** Three-stage progressive curriculum stabilizes training and builds transferable spatial representations
- **Mechanism:** Stage 1 learns 3D token semantics through object recognition (1.87M samples), Stage 2 bridges geometry to language via captioning/grounding (140K), Stage 3 develops complex spatial reasoning (142K)
- **Core assumption:** Foundational object-level representations must precede scene-level reasoning
- **Evidence anchors:** [abstract] "three-stage training curriculum from object recognition to scene-level spatial reasoning"; [section 3.3, Figure 5b] Complete curriculum outperforms mixed Stage 1&2 training and training without Stage 1 initialization on both captioning and scene QA
- **Break condition:** If stage boundaries are poorly calibrated, later stages may fail to converge or catastrophically forget earlier capabilities

## Foundational Learning

- **Concept:** Point cloud patchification
  - **Why needed here:** Understanding how irregular 3D data converts to regular token sequences is essential for debugging tokenization issues and interpreting attention patterns
  - **Quick check question:** Given a point cloud with 10,000 points and M=512, K=5, how many maximum patches would be generated?

- **Concept:** Autoregressive attention over structured sequences
  - **Why needed here:** The model processes spatial separator tokens alongside content tokens; understanding how attention flows across hierarchical boundaries helps diagnose spatial reasoning failures
  - **Quick check question:** What happens to attention patterns when `<layer_sep>` tokens are removed—would nearby patches in Z-space attend to each other differently?

- **Concept:** Curriculum learning and catastrophic forgetting
  - **Why needed here:** The three-stage training requires preserving earlier capabilities while adding new skills; understanding the tradeoff prevents regressions during Stage 3
  - **Quick check question:** If Stage 3 training uses only scene-level QA data without mixing Stage 2 data, what capability degradation would you expect?

## Architecture Onboarding

- **Component map:** Raw point cloud → FPS sampling → Dynamic patchification → Linear projection → Token concatenation with text → Transformer forward pass → Autoregressive generation

- **Critical path:** Point cloud → FPS → Patchification → Linear projection → Token concatenation with text → Transformer forward pass → Autoregressive generation

- **Design tradeoffs:**
  - Encoder-free vs encoder-based: Ablation shows adding PointNet++ degrades performance (Figure 5c), but this may not hold for domains with extremely limited 3D training data
  - Patch size (M=512): Larger patches capture more context but increase sequence length; smaller patches lose local geometry
  - Separator tokens: Add sequence overhead (+2 token types per structural boundary) but critical for spatial reasoning (Table 8 shows 13.22 point drop without them)

- **Failure signatures:**
  - Spatial hallucination: Model generates plausible but incorrect spatial claims—check if separator tokens are properly inserted
  - Object recognition degradation after Stage 3: Indicates insufficient Stage 2 data mixture in Stage 3 training
  - Sparse point cloud failures: Model trained on dense synthetic data may fail on real-world scans with occlusions (Table 11 shows ~3% drop at 50% sampling)

- **First 3 experiments:**
  1. **Validate patchification:** Run a single point cloud through patchification and verify separator token placement matches expected Z→Y→X structure (concrete example in Appendix D.1)
  2. **Ablate separator tokens:** Train a simplified model without `<layer_sep>` and `<row_sep>` on scene QA to quantify their contribution (expect ~10+ point drop per Table 8)
  3. **Test scaling behavior:** Train Stage 1 only with 0.5M vs 1.0M vs 1.87M samples, evaluate on captioning to verify power-law scaling (Figure 5a pattern)

## Open Questions the Paper Calls Out

- **Open Question 1:** How can fine-grained 3D grounding capabilities be developed within a unified transformer architecture to enable precise object localization beyond qualitative scene understanding?
  - **Basis in paper:** [explicit] The conclusion explicitly identifies "developing fine-grained 3D grounding capabilities" as a future direction
  - **Why unresolved:** Lemon achieves strong spatial QA performance but current evaluations focus on scene-level reasoning rather than precise coordinate-level localization required for manipulation tasks
  - **What evidence would resolve it:** Benchmarks requiring bounding box or segmentation mask prediction in 3D space, with metrics comparable to 2D referring expression comprehension

- **Open Question 2:** What cross-modal alignment techniques would most effectively scale unified 3D-language models when trained on diverse, large-scale 3D datasets?
  - **Basis in paper:** [explicit] The conclusion lists "exploring cross-modal alignment techniques" as a future direction; the scaling analysis notes "introducing more diverse and richer 3D-language paired datasets could achieve further performance gains"
  - **Why unresolved:** Current scaling experiments use only Stage 1 classification data; the unified architecture's scaling behavior with heterogeneous scene-level data remains uncharacterized
  - **What evidence would resolve it:** Systematic evaluation of scaling laws across varying dataset diversity, measuring whether power-law behavior holds for complex spatial reasoning tasks

- **Open Question 3:** Can discretization artifacts from fixed-size patch tokenization be mitigated while preserving the computational efficiency of the unified architecture?
  - **Basis in paper:** [inferred] The limitations section states "point cloud patch tokenization may also introduce discretization artifacts that affect fine-grained spatial reasoning"
  - **Why unresolved:** The patchification enforces uniform M=512 points per patch through FPS and replication, which may lose fine geometric detail at patch boundaries
  - **What evidence would resolve it:** Ablation studies comparing patchification against variable-size patch schemes on tasks requiring sub-centimeter spatial precision

## Limitations

- Spatial ordering scheme lacks validation on non-gravitational scenes and rotated architectural spaces
- Curriculum boundaries between stages remain unvalidated and may be brittle to task distribution shifts
- Encoder-free design may struggle with extremely sparse or noisy real-world point clouds where geometric priors could help

## Confidence

- **Unified token processing mechanism:** Medium — Ablation against ShapeLLM is strong, but no validation on truly limited 3D data scenarios
- **Hierarchical spatial partitioning:** Medium — Ordering effects shown, but no validation of alternative hierarchies or non-vertical scenes
- **Three-stage curriculum:** Medium — Stage benefits demonstrated, but boundaries and generalization unproven
- **Scaling properties:** High — Power-law trends align with established LLM scaling observations

## Next Checks

1. **Validate spatial ordering robustness:** Test Lemon on rotated architectural scenes (90°/180° rotated rooms) to verify Z→Y→X hierarchy generalizes beyond standard gravitational assumptions. Compare performance against random axis ordering and FPS-only baselines.

2. **Evaluate curriculum boundary sensitivity:** Systematically vary the data ratio mixture between Stage 1 and Stage 2 during Stage 3 training. Measure retention of object recognition accuracy while optimizing scene-level spatial reasoning to identify optimal knowledge transfer points.

3. **Test sparse point cloud generalization:** Evaluate model performance on real-world LiDAR scans with controlled progressive downsampling (100%→50%→25% points). Compare degradation patterns against synthetic training data to quantify real-world robustness gaps.