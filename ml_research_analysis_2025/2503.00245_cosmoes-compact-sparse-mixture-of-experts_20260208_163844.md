---
ver: rpa2
title: 'CoSMoEs: Compact Sparse Mixture of Experts'
arxiv_id: '2503.00245'
source_url: https://arxiv.org/abs/2503.00245
tags:
- expert
- experts
- dense
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoSMoEs (Compact Sparse Mixture of Experts),
  a framework enabling MoE architectures for on-device inference by addressing three
  key challenges: quality, memory, and latency. The authors demonstrate that MoE models
  outperform FLOP-aligned dense models by at least 2.35% in language modeling tasks,
  introducing weight-decomposed experts for additional performance gains of up to
  1.1%.'
---

# CoSMoEs: Compact Sparse Mixture of Experts

## Quick Facts
- **arXiv ID**: 2503.00245
- **Source URL**: https://arxiv.org/abs/2503.00245
- **Reference count**: 10
- **One-line result**: Sparse MoE outperforms FLOP-aligned dense models by 2.35% on language modeling while enabling on-device deployment

## Executive Summary
This paper introduces CoSMoEs (Compact Sparse Mixture of Experts), a framework addressing three key challenges for deploying MoE architectures on edge devices: quality, memory, and latency. The authors demonstrate that MoE models outperform FLOP-aligned dense models by at least 2.35% in language modeling tasks, and introduce weight-decomposed experts for additional performance gains of up to 1.1%. To tackle memory and latency constraints, they propose a novel "block-wise expert selection" (BlES) loss that reduces expert offloading by 6x and improves inference latency by 50% compared to standard offloaded MoEs. The framework enables high-quality, privacy-preserving foundational models for edge devices while maintaining competitive performance against established open-source models.

## Method Summary
CoSMoEs addresses on-device MoE deployment through three mechanisms: (1) Token-choice routing with top-2 expert selection from 8 total experts, (2) weight-decomposed experts using low-rank factorization to reduce parameters while maintaining quality, and (3) block-wise expert selection (BlES) loss that penalizes expert switches between consecutive tokens during training. The framework targets two scales - "Phone-sized" (~1.4B active parameters) and "Wearable-sized" (~200M active parameters) - trained on the FineWeb Education dataset for 310k steps. The BlES loss reduces inference-time expert offloading frequency by encouraging block-structured expert selection, improving both memory efficiency and generation speed.

## Key Results
- MoE architectures outperform FLOP-aligned dense models by at least 2.35% on language modeling tasks
- Weight-decomposed experts provide up to 1.1% additional quality gains
- BlES reduces expert offloading by 6x and improves inference latency by 50% compared to standard offloaded MoEs

## Why This Works (Mechanism)

### Mechanism 1: Sparse MoE Quality Advantage at On-Device Scale
Sparse MoE architectures improve language modeling over FLOP-aligned dense models at sub-3B scales when confounding factors are controlled. Token-choice routing selects k experts per token from a larger pool, increasing representational capacity without proportional FLOP increase. The routing gradient flows through the softmax selector, allowing the model to learn input-dependent expert specialization. Core assumption: Expert specialization emerges from gradient-driven routing; capacity benefits translate to downstream task performance. Evidence: Phone-sized MoE improves average LM performance from 48.78% (dense FA) to 51.13%; Wearable-sized from 39.90% to 43.22%.

### Mechanism 2: Weight-Decomposed Experts for Parameter Efficiency
Replacing full-rank expert matrices with low-rank factorizations (n×m → n×r × r×m) can reduce total parameters while preserving or improving quality. Each expert's feed-forward weights are decomposed into two smaller matrices. The factorization acts as an implicit regularizer, potentially encouraging expert specialization to low-dimensional subspaces. Core assumption: Individual experts do not require full-rank representations for their specialized functions. Evidence: WD-MoE achieves 52.22% average (Phone) and 43.24% (Wearable), outperforming standard MoE by 1.1% and 0.02% respectively.

### Mechanism 3: Block-wise Expert Selection (BlES) Loss for Offload Reduction
A training-time loss penalizing expert switches between consecutive tokens can substantially reduce inference-time expert offloading frequency. The loss combines: (1) a hard term counting discrete expert replacements between adjacent tokens, and (2) a soft term measuring probability distribution shifts. Minimizing their product encourages block-structured expert selection—maintaining the same experts across contiguous token spans. Core assumption: Token sequences exhibit local semantic coherence such that the same experts remain relevant across short spans. Evidence: BlES reduces ExRep from 43.82% to 6.55% (6x improvement); generation speed improves from 15.02 to 23.10 tok/s (50%).

## Foundational Learning

- **Concept**: Sparse MoE routing (top-k token choice)
  - Why needed here: Understanding how tokens are routed to subsets of experts is prerequisite to interpreting BlES loss and offloading dynamics
  - Quick check question: Given 8 total experts and k=2, how many experts are active per token, and what determines which ones?

- **Concept**: FLOP-alignment vs parameter-alignment
  - Why needed here: The paper's "fair comparison" methodology depends on distinguishing active (compute-determining) from total (memory-determining) parameter counts
  - Quick check question: An MoE with 1.4B active and 3.75B total parameters should be compared against which dense baseline for quality? For memory?

- **Concept**: Expert offloading mechanics
  - Why needed here: Latency gains from BlES only manifest when experts exceed device memory and must be swapped in/out during inference
  - Quick check question: If all experts fit in GPU memory, does BlES provide latency benefits? Why or why not?

## Architecture Onboarding

- **Component map**: Router (softmax over expert logits → top-k selection) → Experts (8 feed-forward networks, standard or weight-decomposed) → BlES Loss (sequence-level penalty on expert replacements, training only) → Offloading Controller (runtime logic swapping inactive experts to CPU)

- **Critical path**: Router decisions → expert activations → (inference) offloading triggers → latency. BlES modifies training to shape routing distributions toward block structure

- **Design tradeoffs**:
  - More active experts → higher quality, slower inference, same memory (without offloading)
  - More total experts → higher quality, same inference speed (without offloading), higher memory
  - BlES strength → lower latency, potential quality regression, risk of expert imbalance
  - Weight decomposition → lower memory, similar quality, additional hyperparameter (rank r)

- **Failure signatures**:
  - Expert collapse: One expert selected >80% of the time (check per-layer ∆Uniform in Figure 6)
  - Excessive switching: ExRep >40% indicates BlES not effective or not applied
  - Memory overflow: Total parameters exceed device capacity without offloading
  - Quality cliff: Sudden drops may indicate over-aggressive BlES or insufficient rank in WD

- **First 3 experiments**:
  1. Replicate FLOP-aligned comparison (Dense 1.5B vs MoE 1.37B active) on a subset of LM eval benchmarks to validate the 2.35% gain claim under your compute budget
  2. Ablate BlES loss weight (try 0.0, 0.01, 0.1, 1.0) measuring ExRep, tok/s, and avg LM score to find the quality-latency Pareto frontier
  3. Profile offloading overhead on target hardware: measure per-token latency with and without BlES for a 128-token generation to confirm the 50% speedup generalizes beyond the reported server environment

## Open Questions the Paper Calls Out

**Open Question 1**: What is the mechanistic explanation for the layer-wise shift in expert divergence observed when using Block-wise Expert Selection (BlES)? The authors observe that BlES causes "larger expert divergence... in lower layers" compared to standard MoEs, stating, "While we don’t have a clear understanding of the reasoning and impact of these differences."

**Open Question 2**: Can the "minor performance regression" induced by the Block-wise Expert Selection loss be recovered without sacrificing latency gains? In Section 3.4.1, the authors note that the BlES model shows a "slight performance drop compared to the standard MoE setup" across various benchmarks (e.g., average score drop from 51.13% to 50.70% on Phone-sized models).

**Open Question 3**: Does restricting expert switching frequency via BlES impair the model's ability to handle long-range dependencies? The authors exclude long-context evaluations (SQuAD, DROP) due to a sequence length restriction of 2048 (Section 3.3). Since BlES explicitly penalizes switching experts between consecutive tokens, it may negatively impact tasks requiring distinct experts for distant context.

## Limitations

- Model scale constraints: Demonstrated advantages only at sub-3B parameter scales; quality gains may not persist at larger scales
- Hardware dependency: 50% latency improvement assumes specific offloading conditions; no benefit on devices with sufficient memory
- Evaluation scope: Quality improvements measured only on 9 language modeling benchmarks; downstream task performance and long-context reasoning untested

## Confidence

**High Confidence (4/5)**:
- MoE architectures outperform FLOP-aligned dense models by 2.35% on language modeling tasks
- Weight-decomposed experts provide up to 1.1% additional quality gains
- BlES reduces expert offloading by 6x and improves inference latency by 50%

**Medium Confidence (3/5)**:
- The three proposed mechanisms (quality, memory, latency) work synergistically to enable on-device MoE deployment

**Low Confidence (2/5)**:
- BlES loss weight optimization and rank selection for weight decomposition are optimal for all on-device scenarios

## Next Checks

1. **Scale Sensitivity Analysis**: Replicate the FLOP-aligned comparison (Dense 1.5B vs MoE 1.37B active) at 3-5 different parameter scales (e.g., 500M, 1B, 2B, 3B active parameters) to determine whether quality gains scale linearly or plateau

2. **Hardware-Agnostic Latency Benchmarking**: Measure tokens/second on three hardware configurations: (a) all experts fit in memory, (b) partial offloading required, (c) aggressive offloading with BlES to quantify the actual memory threshold where BlES becomes beneficial

3. **Expert Collapse Stress Test**: Train MoE models with BlES loss weights spanning 3 orders of magnitude (0.001 to 1.0) and monitor per-layer expert selection distributions to identify the critical α value where expert diversity collapses versus when latency gains plateau