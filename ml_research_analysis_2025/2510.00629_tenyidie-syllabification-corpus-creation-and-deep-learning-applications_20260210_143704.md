---
ver: rpa2
title: Tenyidie Syllabification corpus creation and deep learning applications
arxiv_id: '2510.00629'
source_url: https://arxiv.org/abs/2510.00629
tags:
- tenyidie
- syllabification
- blstm
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work creates a syllabification corpus for the Tenyidie language
  and applies deep learning models to perform syllabification. The corpus consists
  of 10,120 manually annotated words, with 80% used for training, 10% for validation,
  and 10% for testing.
---

# Tenyidie Syllabification corpus creation and deep learning applications

## Quick Facts
- arXiv ID: 2510.00629
- Source URL: https://arxiv.org/abs/2510.00629
- Reference count: 16
- Key outcome: BLSTM model achieves 99.21% word-level accuracy on Tenyidie syllabification task

## Executive Summary
This work creates the first annotated syllabification corpus for the Tenyidie language (10,120 manually annotated words) and evaluates four deep learning architectures for the task. The authors frame syllabification as a sequence labeling problem where each character is labeled as either the start (S) or continuation (C) of a syllable. Among the tested architectures—LSTM, BLSTM, BLSTM+CRF, and Encoder-decoder with attention—the BLSTM model achieves the highest accuracy of 99.21% on the test set, outperforming other models by 2-4 percentage points.

## Method Summary
The authors create a syllabification corpus by manually annotating 10,120 Tenyidie words with syllable boundaries. They convert this into a sequence labeling task by tagging each character as either the start (S) or continuation (C) of a syllable. Four deep learning architectures are implemented and evaluated: LSTM, BLSTM, BLSTM+CRF, and Encoder-decoder with attention. The BLSTM model uses bidirectional processing to capture both prefix and suffix context, while the encoder-decoder model attempts to generate syllable sequences directly. The corpus is split 80/10/10 for training, validation, and testing respectively.

## Key Results
- BLSTM model achieves highest accuracy of 99.21% on test set
- Encoder-decoder with attention underperforms at 94.27%, attributed to limited training data
- BLSTM+CRF (99.01%) slightly underperforms plain BLSTM (99.21%)
- LSTM achieves 97.04% accuracy, showing bidirectional context provides 2.17% improvement

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Context Capture for Boundary Detection
BLSTM achieves superior syllabification accuracy by processing character sequences in both directions simultaneously. The forward pass captures prefix-dependent patterns (e.g., recognizing "ke-" as a nominalizer prefix), while the backward pass identifies suffix markers (e.g., "-u" definite marker). This bidirectional processing allows the model to resolve ambiguities that unidirectional LSTM cannot—explaining the 2.17% accuracy gap (97.04% → 99.21%).

### Mechanism 2: Sequence Labeling Formulation Reduces Complexity
Framing syllabification as per-character binary classification (S=start, C=continue) enables efficient learning with limited data. Rather than generating syllable sequences directly, the model assigns S/C labels to each character position. For "tenyidie" → "t/S e/C n/S y/C i/C d/S i/C e/C", the model learns local decision boundaries rather than full sequence generation. This reduces output space complexity from O(2^n) possible syllabifications to O(2^n) independent binary decisions with contextual conditioning.

### Mechanism 3: Encoder-Decoder Underperformance on Limited Data
The encoder-decoder with attention model underperforms (94.27%) because sequence-to-sequence generation requires more training examples than sequence labeling. Encoder-decoder models must learn both encoding and generation mappings. With only 8,096 training examples, the model cannot adequately explore the output space. In contrast, BLSTM's per-character classification has constrained output vocabulary (S/C), enabling effective learning from limited data.

## Foundational Learning

- **Concept: Sequence Labeling vs. Sequence-to-Sequence**
  - Why needed here: Understanding why BLSTM (labeling) outperforms encoder-decoder (seq2seq) by 4.94% requires distinguishing tasks that preserve sequence length from those that transform it.
  - Quick check question: Given that syllabification doesn't change character count (input: "tenyidie" [8 chars] → output: "t e n y i d i e" with boundary labels [8 labels]), why would encoder-decoder be architecturally mismatched?

- **Concept: Bidirectional Recurrence**
  - Why needed here: The 2.17% accuracy improvement from LSTM→BLSTM demonstrates that suffix information (visible only in backward pass) resolves ambiguities unresolvable from prefixes alone.
  - Quick check question: In the error case "shesou" (should be she+so+u, predicted as she+sou), what backward context would help identify the final "u" as a separate syllable?

- **Concept: Conditional Random Fields (CRF) for Structured Output**
  - Why needed here: BLSTM+CRF (99.01%) slightly underperforms plain BLSTM (99.21%), suggesting CRF's global normalization may not suit this task's local decision structure.
  - Quick check question: CRF layers enforce label transition constraints (e.g., S→C is valid, C→S is valid, but invalid transitions are penalized). Why might this constraint hurt rather than help for Tenyidie syllabification?

## Architecture Onboarding

- **Component map:**
  ```
  Input Layer: Character sequence (one-hot or embedded)
      ↓
  Embedding Layer: 128-dim character embeddings (shared across positions)
      ↓
  Recurrent Layer: BLSTM (512 units per direction, 793,475 trainable params)
      ↓
  Output Layer: TimeDistributed Dense → Softmax (2 classes: S, C)
      ↓
  Loss: Categorical cross-entropy (per-character)
  ```

- **Critical path:**
  1. Data preparation: Convert syllabified words → S/C label sequences
  2. Vocabulary: 25 Tenyidie characters (excluding Q, X; adding Ü) + special tokens
  3. Model training: 40 epochs, batch_size=128, Adam optimizer, lr=0.001
  4. Inference: Per-character S/C prediction → reconstruct syllable boundaries

- **Design tradeoffs:**
  - **128-dim embeddings** vs. higher dimensions: Paper uses 128; Assumption: sufficient for 25-character vocabulary, but no ablation reported
  - **Word-level accuracy metric** vs. character-level: Paper reports word-level (entire word must be correct); Assumption: this is stricter and more appropriate for downstream NLP tasks
  - **No CRF layer** (BLSTM wins over BLSTM+CRF): Suggests local decisions are nearly independent; global normalization adds complexity without benefit

- **Failure signatures:**
  - **Marker confusion**: Table 4 shows systematic errors on "-u" (definite marker) vs. "ü" vowel—suggests visual/phonological similarity causes misclassification
  - **Single vowel syllables**: Paper notes common annotation errors on isolated vowels (beginning/middle/end of words)
  - **Consonant cluster boundaries**: "thepfhetheü" error suggests CCV vs. CCCV parsing ambiguity

- **First 3 experiments:**
  1. **Baseline replication**: Train BLSTM on provided split (80/10/10), verify 99.21% test accuracy. If significantly different, check data preprocessing and label encoding.
  2. **Ablation—embedding dimension**: Test 64, 128, 256 dims. Hypothesis: 25-character vocabulary may saturate at lower dims; watch for overfitting at 256.
  3. **Error pattern analysis**: Extract all test errors, categorize by syllable type (CV, CCV, CVV, etc.) and position (beginning/middle/end). Target: identify if specific patterns (e.g., V-only syllables) need architectural fixes (e.g., attention mechanism) or data augmentation.

## Open Questions the Paper Calls Out

- **Question**: Can the Encoder-Decoder architecture outperform the current BLSTM baseline if trained on a significantly larger annotated corpus?
  - Basis in paper: [explicit] The conclusion states, "It is also intended to annotate a larger dataset and see the performance of the Encoder-decoder model since these models perform well on larger datasets."
  - Why unresolved: The current experiment used only 10,120 words, which the authors suggest is insufficient for the Encoder-Decoder model to learn effectively compared to the BLSTM.
  - What evidence would resolve it: Benchmarking the Encoder-Decoder model against the BLSTM using a corpus with substantially more than 10,120 words.

- **Question**: To what extent does automatic syllabification improve performance in downstream NLP tasks for Tenyidie, such as Machine Translation or Part-of-Speech tagging?
  - Basis in paper: [explicit] The conclusion notes, "In the future, we will use the syllabifier in various syllable-based NLP tasks, such as... morphological analysis, part-of-speech tagging, machine translation, etc."
  - Why unresolved: The current work focuses strictly on the corpus creation and the isolated syllabification task, without integrating the model into downstream applications.
  - What evidence would resolve it: Comparative metrics (e.g., BLEU scores for MT, accuracy for POS tagging) for systems trained with and without the syllabification component.

- **Question**: How robust is the reported 99.21% accuracy across different data splits given the exclusion of cross-validation?
  - Basis in paper: [inferred] The authors note they "tested on 10% of our dataset due to the limited size of our annotated dataset for performing other evaluation techniques such as cross-validation."
  - Why unresolved: A single 80:10:10 split may not capture the variance in model performance, particularly with a relatively small dataset of ~10k words.
  - What evidence would resolve it: Results from k-fold cross-validation demonstrating consistent accuracy across different subsets of the data.

## Limitations

- Small dataset size (10,120 words) relative to model complexity (793,475 parameters) raises overfitting concerns
- Encoder-decoder underperformance claim not empirically validated through controlled experiments with varied dataset sizes
- S/C labeling scheme assumes syllable boundaries are locally determinable, which may not hold for morphologically complex words
- Lack of error analysis on how specific phonological patterns affect model performance

## Confidence

- **High confidence**: BLSTM achieving 99.21% accuracy on the test set - directly measurable and reproducible
- **Medium confidence**: Bidirectional processing as key differentiator between LSTM (97.04%) and BLSTM (99.21%) - supported by results but specific error patterns not fully characterized
- **Low confidence**: Encoder-decoder underperformance due solely to dataset size limitations - requires systematic experimentation across multiple dataset sizes to validate

## Next Checks

1. **Controlled dataset size experiment**: Systematically vary training set sizes (10%, 25%, 50%, 75%, 100%) for all four model architectures to quantify how sample complexity affects each model type's performance, particularly testing the encoder-decoder sample complexity hypothesis.

2. **Cross-linguistic generalization test**: Apply the best-performing BLSTM model to syllabification tasks in related languages (Manipuri, Ao, Angami) to assess whether the learned patterns are language-specific or capture generalizable syllable boundary heuristics.

3. **Error pattern characterization**: Perform detailed error analysis on the 8 test errors, categorizing them by syllable type (CV, CCV, CVV, etc.), position in word (beginning/middle/end), and phonological features to identify systematic weaknesses requiring architectural modifications or data augmentation strategies.