---
ver: rpa2
title: Answer Matching Outperforms Multiple Choice for Language Model Evaluation
arxiv_id: '2507.02856'
source_url: https://arxiv.org/abs/2507.02856
tags:
- answer
- question
- matching
- choice
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a critical flaw in current language model
  evaluation: multiple choice questions allow models to bypass generative reasoning
  by exploiting choice-only shortcuts, which undermines the assessment of their true
  generative capabilities. To address this, the authors propose "answer matching,"
  a generative evaluation method where models generate free-form answers to questions
  and then use another modern language model to determine if the response semantically
  matches a reference answer.'
---

# Answer Matching Outperforms Multiple Choice for Language Model Evaluation

## Quick Facts
- **arXiv ID**: 2507.02856
- **Source URL**: https://arxiv.org/abs/2507.02856
- **Reference count**: 40
- **Primary result**: Answer matching with reference answers achieves near-perfect human alignment while revealing discriminative shortcuts in MCQ benchmarks

## Executive Summary
This paper identifies a critical flaw in current language model evaluation: multiple choice questions allow models to bypass generative reasoning by exploiting choice-only shortcuts, undermining assessment of true generative capabilities. The authors propose "answer matching," where models generate free-form answers and a modern language model determines semantic equivalence with reference answers. Through human grading on MMLU-Pro and GPQA-Diamond, answer matching achieves near-perfect alignment with human evaluators—comparable to inter-annotator agreement—while both standard multiple choice evaluation and LLM-as-a-judge without references show significantly lower alignment. The approach is cost-effective, changes model rankings, and reveals more headroom for improvement than MCQ benchmarks suggest.

## Method Summary
The method filters MCQ datasets for questions answerable without choices, generates free-form responses from candidate models, and uses another language model as a semantic matcher against reference answers. Questions are rated for specificity (threshold ≥8/10) using an LLM, responses are generated at temperature 0.3-0.6 with max 16,384 tokens, and matching is performed with reference answers available to the matcher. Scott's π is used to measure alignment with human ground truth grading. The approach contrasts with LLM-as-judge by providing reference answers to the matcher, reducing the task from "solve this" to "are these equivalent?"

## Key Results
- Choice-only classifiers achieve 83% accuracy on TruthfulQA-v2 and 51% on MMMU-Pro, demonstrating discriminative shortcuts
- Qwen3-4B achieves Scott's π = 0.97 on MATH and π = 0.87-0.90 on GPQA-Diamond and MMLU-Pro, comparable to human inter-annotator agreement
- MCQ accuracy (77% on MATH) significantly exceeds generative accuracy (52% on MATH), revealing discriminative shortcuts
- Model rankings change substantially: R1-Distill-Llama-70B drops 30 points on GPQA-Diamond; GPT-4o-Mini and Claude-3.5-Haiku climb in generative rankings

## Why This Works (Mechanism)

### Mechanism 1: Discriminative Shortcuts Undermine MCQ Validity
- Claim: Multiple-choice benchmarks measure discriminative ability, not generative capability
- Mechanism: Models exploit statistical separability between correct/incorrect choices without reading the question. A finetuned classifier achieved 83% on TruthfulQA-v2 and 51% on MMMU-Pro using only choices.
- Core assumption: Models learn surface patterns in distractors rather than solving the underlying question
- Evidence anchors:
  - [Section 2]: "finetune a language model (Qwen3-4B) to predict the correct answer a given only the choices {a} ∪ {wi} without the question Q... strikingly high accuracies can be achieved"
  - [Figure 3]: Shows shortcut accuracy above chance across 9 benchmarks
  - [Corpus]: Related work (Balepur et al., 2024) confirms choice-only prompting achieves non-trivial accuracy on HellaSwag

### Mechanism 2: Answer Matching Leverages Semantic Equivalence Detection
- Claim: Modern LLMs achieve near-human agreement when matching responses to reference answers
- Mechanism: Matching only requires detecting paraphrase/functional equivalence (easier than verifying correctness de novo). The matcher has access to the ground truth, reducing the task from "solve this" to "are these equivalent?"
- Core assumption: Recent small models have sufficient language understanding for equivalence detection
- Evidence anchors:
  - [Section 3.2]: "Qwen3-4B (in no-thinking mode), achieves near-perfect alignment with the ground-truth (π = 0.97)" on MATH
  - [Figure 5]: Qwen3-4B achieves Scott's π = 0.87-0.90 on GPQA-Diamond and MMLU-Pro, within human inter-annotator range
  - [Corpus]: Parallel work (Krumdick et al., 2025) finds small models with reference outperform large models without reference

### Mechanism 3: Free-Form Evaluation Changes Model Rankings
- Claim: Evaluation method choice causally affects leaderboard conclusions
- Mechanism: MCQ overestimates performance via shortcuts; generative evaluation reveals true capability gaps. Chat-optimized models improve ranking; distillation-trained models decline.
- Core assumption: Generative performance reflects real-world utility better than discriminative selection
- Evidence anchors:
  - [Section 4, Figure 6]: R1-Distill-Llama-70B drops 30 points on GPQA-Diamond; GPT-4o-Mini and Claude-3.5-Haiku climb in generative rankings
  - [Section 4]: "benchmarks that appear saturated... begin to reveal substantial headroom when switched to generative evaluation"

## Foundational Learning

- **Discriminative vs. Generative Evaluation**
  - Why needed here: The paper's core argument hinges on distinguishing tasks where models select among options (discriminative) from tasks requiring open-ended generation
  - Quick check question: If you give a model only answer choices without the question and it scores above random, what does this prove?

- **Semantic/Functional Equivalence**
  - Why needed here: Answer matching requires the matcher to judge whether two responses are equivalent, not just identical strings
  - Quick check question: Would "The capital is Paris" match a reference answer "Paris" if the question asks for France's capital?

- **Inter-Annotator Agreement (Scott's π)**
  - Why needed here: The paper uses Scott's π to measure how well automatic graders align with human judgment, accounting for chance agreement
  - Quick check question: Why is raw percent agreement inadequate when model accuracy is skewed (e.g., 80% correct)?

## Architecture Onboarding

- **Component map**: Question → Candidate Model → Free-form response → Matcher Model + Reference answer → Match decision (0/1)

- **Critical path**:
  1. Filter questions for specificity and unique answers (paper filtered ~50% of MMLU-Pro)
  2. Generate free-form responses at temperature 0.3-0.6
  3. Run matching with recent model (Qwen3-4B, Llama-4-Scout, or DeepSeek-V3)
  4. Compute Scott's π against human annotations for validation

- **Design tradeoffs**:
  - **Matcher size**: Smaller models (1.7B) work if recent; older models (Llama-2-7B) fail (π = 0.44)
  - **Cost**: Free-form + matching can be cheaper than MCQ because models generate fewer tokens when not evaluating each choice
  - **Robustness**: Rankings stable across matchers (DeepSeek-V3 vs Llama-4-Scout show no significant differences)

- **Failure signatures**:
  - Questions requiring choices for disambiguation (e.g., "Which of the following...")
  - Multiple semantically distinct correct answers not in reference
  - Adversarial responses designed to game the matcher (not tested in paper)

- **First 3 experiments**:
  1. **Baseline validation**: Replicate the MATH experiment (Figure 4)—compare MCQ, MC-Verify, and answer matching alignment with rule-based ground truth using Qwen3-4B as matcher
  2. **Domain transfer**: Apply answer matching to your target domain; filter questions for specificity via LLM rating ≥8 (per paper's protocol); manually validate 100 samples
  3. **Matcher ablation**: Compare 3 matcher sizes (1.7B, 4B, 70B+) on your filtered set—confirm Scott's π plateaus at small recent models per paper findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are language model answer matchers against adversarial gaming strategies or optimization pressure from candidate models?
- Basis in paper: [explicit] Section 6 states, "In this work, we did not study how robust are language models as answer matchers to gaming (for example, candidate outputting vague or multiple answers...) or optimization pressure."
- Why unresolved: The authors note that real-world evaluations will be optimized for, and models might output vague answers to trick matchers, but they did not evaluate the method's vulnerability to such adversarial manipulations.
- What evidence would resolve it: Testing answer matchers against candidate models specifically optimized or prompted to exploit matcher biases or to generate adversarial outputs that receive false positives.

### Open Question 2
- Question: Can answer matching be effectively applied to generative tasks with multiple correct answers or subjective metrics?
- Basis in paper: [explicit] Section 6 notes, "Answer matching can not always be used... results do not apply to questions with multiple correct answers... evaluation of many generative tasks can not be simply formulated with answer matching, e.g. translation, summarization..."
- Why unresolved: The study filtered for questions with unique answers to ensure ground truth validity, leaving the efficacy of this method for open-ended tasks like translation, summarization, or theorem proving untested.
- What evidence would resolve it: Developing and validating answer matching frameworks that utilize rubrics or lists of valid references for subjective benchmarks, and measuring alignment with human graders.

### Open Question 3
- Question: Does the relative superiority of answer matching hold for tasks where semantic matching is computationally harder than verification?
- Basis in paper: [explicit] Section 6 discusses "On the hardness of matching" and notes that for tasks like graph generation, matching (solving graph isomorphism) can be NP-Hard while verifying graph properties is simpler.
- Why unresolved: While answer matching aligns better with humans on natural language benchmarks, the theoretical hardness suggests it may fail or become inefficient for specific structural or logic-heavy domains compared to direct verification.
- What evidence would resolve it: Comparing the alignment and computational cost of answer matchers versus direct verifiers on benchmarks requiring complex structural outputs, such as graph generation or formal logic proofs.

## Limitations
- Human annotation datasets are relatively small (493 MMLU-Pro, 126 GPQA-Diamond), limiting generalizability
- Only ~50% of MMLU-Pro questions passed the specificity filter, suggesting many domains may not be suitable for free-form evaluation
- The approach requires questions with unique, specific answers—filtering out many real-world scenarios

## Confidence
- **High Confidence**: Choice-only classifiers achieving non-trivial accuracy across benchmarks is well-supported and reproducible
- **Medium Confidence**: Answer matching provides more valid evaluations than MCQ, supported by human grading alignment data but limited to three datasets
- **Medium Confidence**: Cost-effectiveness claim depends on specific implementation details that may vary across applications

## Next Checks
1. **Cross-Domain Validation**: Apply answer matching to a dataset from a different domain (e.g., medical or legal reasoning) with minimal filtering. Measure both human alignment and the percentage of questions that pass the specificity filter.
2. **Controlled Mechanism Test**: Design an experiment comparing LLM-as-Judge with and without reference answers on identical responses. Measure not just agreement with humans but also inter-judge consistency.
3. **Adversarial Robustness Test**: Generate responses specifically designed to exploit the matcher (e.g., highly plausible but incorrect answers that match the reference semantically). Measure how well different matcher models resist these attacks.