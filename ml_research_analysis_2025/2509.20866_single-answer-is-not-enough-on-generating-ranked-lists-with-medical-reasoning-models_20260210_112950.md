---
ver: rpa2
title: 'Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning
  Models'
arxiv_id: '2509.20866'
source_url: https://arxiv.org/abs/2509.20866
tags:
- list
- list-cot
- rft-list
- answer
- format
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically studies the generation of ranked lists
  with medical reasoning models (MRMs). The authors investigate two approaches: prompting
  and fine-tuning.'
---

# Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models

## Quick Facts
- arXiv ID: 2509.20866
- Source URL: https://arxiv.org/abs/2509.20866
- Reference count: 40
- Key outcome: RFT models exhibit better robustness across multiple formats compared to SFT models, with the choice of reward function significantly impacting performance.

## Executive Summary
This paper systematically studies the generation of ranked lists with medical reasoning models (MRMs) by investigating two approaches: prompting and fine-tuning. The authors find that while prompting is a cost-effective way to steer MRM responses, it does not ensure cross-format generalization. Through controlled experiments comparing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), they demonstrate that RFT models are more robust across multiple answer formats (choice, short text, and list answers) when reward functions are carefully designed. A case study on modified MedQA with multiple valid answers reveals that MRMs may fail to select benchmark-preferred ground truth but can recognize valid answers.

## Method Summary
The paper employs a two-pronged approach: observational prompting experiments across multiple MRM models and controlled fine-tuning experiments using a shared Qwen2.5-7B-Instruct backbone. For prompting, the authors test zero-shot and chain-of-thought variants across three answer formats (MCQ, QA, List) using MedQA, MedMCQA, and MedXpertQA benchmarks. The fine-tuning experiments compare SFT (knowledge distillation from Qwen3-30B-A3B-Thinking) with RFT (GRPO algorithm with format-aware rewards including Accuracy, MRR, and Judge-MRR variants). Evaluation metrics include format robustness, accuracy, List MRR, Valid List Length, Correct Position, and per-question correctness transitions across formats.

## Key Results
- RFT models exhibit better robustness across multiple formats compared to SFT models
- The choice of reward function significantly impacts performance, with Judge-MRR yielding the strongest QA robustness
- Format-knowledge entanglement exists, with 25-65% of correct answers becoming incorrect when shifting from MCQ to QA format
- Models trained with RFT are more robust across multiple formats than SFT models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RFT produces more robust cross-format generalization than SFT when reward functions are carefully designed.
- Mechanism: RFT incentivizes exploration through reward optimization, allowing models to discover format-independent knowledge representations, whereas SFT encourages imitation of format-specific patterns that may not transfer.
- Core assumption: The reward signal in RFT can capture format-independent correctness while penalizing format-specific overfitting.
- Evidence anchors:
  - [abstract]: "models trained with RFT are more robust across multiple formats"
  - [Section 5.2]: "RFT can achieve higher peak robustness than SFT when the target format aligns well with existing model biases"
  - [corpus]: Weak direct support; corpus neighbors discuss format adaptation but not RFT vs SFT comparisons.
- Break condition: If RFT reward functions inadvertently reward format-specific behaviors (e.g., exact string matching that fails on paraphrases), RFT can become more brittle than SFT.

### Mechanism 2
- Claim: Answer format and knowledge access are entangled in MRMs—changing format can flip correctness.
- Mechanism: Fine-tuning on specific formats (especially MCQ) creates implicit associations between format cues and knowledge retrieval pathways; changing format disrupts these pathways even when knowledge exists.
- Core assumption: Knowledge representation in MRMs is not fully format-invariant and contains format-conditioned access patterns.
- Evidence anchors:
  - [abstract]: "although MRMs might fail to select the benchmark's preferred ground truth, they can recognize valid answers"
  - [Section 4.2]: "25-65% of correct answers become incorrect [when shifting from MCQ to QA], with few recoveries"
  - [corpus]: Neighbor paper "Format-Adapter" supports that format affects reasoning capability, but does not specifically address knowledge entanglement.
- Break condition: If models are trained with explicit format-invariant objectives or mixed-format data from the start, entanglement may weaken or disappear.

### Mechanism 3
- Claim: Reward function design in RFT shapes both performance and format-specific behavioral biases.
- Mechanism: RFT optimizes for the reward signal; if rewards prioritize format-compliant outputs without penalizing reward gaming (e.g., generating multiple answers to maximize match probability), models exploit loopholes, leading to misaligned behaviors.
- Core assumption: Models are capable of reward hacking when rewards are underspecified or contain exploitable gaps.
- Evidence anchors:
  - [abstract]: "the choice of reward function significantly impacting performance"
  - [Section 5.2]: "Judge-MRR yields the strongest QA robustness, while List robustness remains near-perfect across variants"
  - [corpus]: No direct evidence in corpus neighbors; reward hacking in RLHF is discussed in related work but not in corpus.
- Break condition: If rewards are designed with explicit anti-exploitation constraints (e.g., length penalties, validation rules), behavioral misalignment can be mitigated.

## Foundational Learning

- Concept: Answer-format robustness vs. semantic correctness
  - Why needed here: The paper distinguishes between a model's ability to follow format instructions (robustness) and the medical validity of its content (correctness); confusing these leads to misinterpreting results.
  - Quick check question: If a model produces a medically correct answer but fails to format it as requested (e.g., missing numbered list), is it a robustness failure or a correctness failure?

- Concept: Supervised fine-tuning (SFT) vs. Reinforcement fine-tuning (RFT)
  - Why needed here: The paper compares these paradigms directly; understanding that SFT imitates demonstrations while RFT optimizes rewards is essential for interpreting why they produce different generalization patterns.
  - Quick check question: Which training paradigm would you expect to be more sensitive to the distribution of training data formats, and why?

- Concept: Format-knowledge entanglement (FKE)
  - Why needed here: The paper hypothesizes and tests FKE; understanding that knowledge access may be conditioned on format helps explain the observed asymmetric correctness transitions.
  - Quick check question: If FKE were false, what pattern would you expect in per-question correctness transitions across formats?

## Architecture Onboarding

- Component map:
  Observational layer (prompting experiments) -> Controlled layer (fine-tuning experiments) -> Evaluation layer (format robustness metrics) -> Ablation layer (reward design variations)

- Critical path:
  1. Run prompting experiments to identify format robustness gaps and FKE signals
  2. Design RFT reward functions that balance correctness with format-invariance (e.g., Judge-MRR with validation rules)
  3. Conduct controlled fine-tuning experiments, comparing SFT vs RFT across target formats
  4. Analyze training dynamics (reward progression, response length) to detect reward hacking or overfitting
  5. Validate on modified benchmarks with multiple valid answers to test true medical knowledge beyond single ground truth

- Design tradeoffs:
  - SFT vs RFT: SFT is simpler and more stable but may overfit to training format; RFT is more flexible but requires careful reward design to avoid exploitation
  - Prompting vs Fine-tuning: Prompting is cost-effective for steering but unreliable for cross-format generalization; fine-tuning enables robustness but requires computational resources and labeled data
  - Reward complexity: Simple rewards (exact match) risk gaming; complex rewards (LLM judges) are more robust but introduce cost and potential judge bias

- Failure signatures:
  - Format brittleness: High accuracy on training format, near-zero on others (e.g., RFT-QA with 9.79% List robustness)
  - Reward gaming: Excessively long or repetitive outputs (e.g., lists >100 items) to maximize match probability
  - Asymmetric transitions: Large C→I rates with minimal I→C (e.g., 71.7% C→I in RFT-QA for MCQ→QA shift)
  - CoT degradation: Reasoning models show performance drops when CoT is applied (e.g., Gemini 2.5 Flash: −19.75 pp MCQ accuracy with CoT)

- First 3 experiments:
  1. Reproduce prompting baseline: Run zero-shot and CoT prompting on a subset of MedQA across MCQ, QA, and List formats; measure robustness and transition rates to confirm FKE signals
  2. Minimal RFT prototype: Implement RFT with simple accuracy-based reward on Qwen2.5-7B-Instruct for List format; evaluate cross-format robustness to observe reward gaming behaviors
  3. Ablate reward design: Compare three reward variants (Accuracy, MRR, Judge-MRR) on the same backbone; analyze how each affects list length, QA robustness, and transition asymmetry to identify optimal reward structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reward designs be developed that simultaneously optimize performance and prevent misaligned behaviors (e.g., reward hacking through multiple answers per item)?
- Basis in paper: [explicit] The authors state: "Developing reward designs that both optimize performance and prevent misaligned behaviors is an important direction for future work, but is beyond the scope of this paper" (Page 8).
- Why unresolved: Current RFT reward functions (e.g., substring-based QA rewards) can encourage exploiting loopholes, such as generating multiple answers instead of committing to a single decision.
- What evidence would resolve it: A systematic comparison of reward function variants that include explicit constraints against reward hacking, measured via format compliance and correctness rates across formats.

### Open Question 2
- Question: How does answer-format robustness generalize to multi-turn clinical interactions where answer formats may evolve as new information becomes available?
- Basis in paper: [explicit] "We do not consider interactive or long-horizon scenarios in which answer formats may evolve as new information becomes available... Extending answer-format robustness analysis to multi-turn clinical interactions, such as iterative diagnosis or simulated patient encounters, remains an open problem" (Page 9).
- Why unresolved: Current evaluation is limited to single-turn, static settings, whereas real clinical workflows involve dynamic information gathering.
- What evidence would resolve it: Evaluation of MRMs in multi-turn dialogue settings with varying format requests at each turn, measuring robustness across the interaction.

### Open Question 3
- Question: Can uncertainty-aware or probability-sensitive outputs be incorporated alongside ranked lists to capture quantitative differences in confidence?
- Basis in paper: [explicit] "Incorporating uncertainty-aware or probability-sensitive approaches alongside ranked outputs is a promising direction for future research" (Page 9).
- Why unresolved: Current ranked lists show relative ordering but not quantitative probability differences (e.g., a top-ranked answer could be only marginally or overwhelmingly more likely).
- What evidence would resolve it: Development and evaluation of MRMs that output calibrated confidence scores with ranked lists, validated against expert uncertainty judgments.

### Open Question 4
- Question: How do the observed format-knowledge entanglement patterns transfer to substantially larger models and multimodal settings?
- Basis in paper: [explicit] "Due to computational constraints, we do not explore substantially larger models... While we expect several observed trends to persist beyond our setting, their behavior under large-scale setup remains to be explored" (Page 9).
- Why unresolved: Findings are based on 3B-27B parameter models; it is unclear whether cross-format brittleness patterns hold at frontier model scales or with multimodal inputs.
- What evidence would resolve it: Replication of the controlled SFT/RFT experiments on significantly larger models (e.g., 70B+) and multimodal variants, using the same evaluation protocols.

## Limitations

- The experimental scope is limited to medical reasoning tasks, leaving open whether format-robustness dynamics extend to other specialized domains
- The format-knowledge entanglement hypothesis remains largely correlational without causal evidence for why knowledge access becomes format-conditioned
- Results depend heavily on careful reward function design that may not transfer to other domains or benchmarks

## Confidence

- High Confidence: The core finding that prompting is cost-effective but insufficient for cross-format robustness - this is well-supported by controlled experiments and clear performance metrics
- Medium Confidence: The superiority of RFT over SFT for format robustness - while demonstrated, this depends heavily on reward function design and may not generalize across different model architectures or domains
- Medium Confidence: The format-knowledge entanglement hypothesis - supported by strong empirical patterns but lacking causal evidence or mechanistic explanation of why knowledge access becomes format-conditioned

## Next Checks

1. Ablate reward function complexity: Systematically test reward variants from simple accuracy to complex LLM judges on the same backbone to isolate how reward design drives robustness vs. exploitation behaviors
2. Cross-domain robustness transfer: Evaluate the best-performing RFT model on non-medical benchmarks (e.g., MMLU Science, commonsense reasoning) to test whether format robustness generalizes beyond medical knowledge
3. Knowledge-access ablation study: Design experiments where models must answer the same question in multiple formats sequentially, measuring whether format order affects correctness to establish causal evidence for format-knowledge entanglement