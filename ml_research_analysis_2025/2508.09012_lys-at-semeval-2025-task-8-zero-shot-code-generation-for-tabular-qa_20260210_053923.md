---
ver: rpa2
title: 'LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA'
arxiv_id: '2508.09012'
source_url: https://arxiv.org/abs/2508.09012
tags:
- code
- tabular
- data
- question
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot code generation approach for Tabular
  Question Answering (Tabular QA) in SemEval 2025 Task 8. The system leverages a Large
  Language Model (LLM) to dynamically generate executable Python code for extracting
  information from tabular data based on natural language questions.
---

# LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA

## Quick Facts
- **arXiv ID**: 2508.09012
- **Source URL**: https://arxiv.org/abs/2508.09012
- **Reference count**: 14
- **Primary result**: Zero-shot code generation system achieving 65 and 68 points accuracy in Subtasks 1 and 2 respectively

## Executive Summary
This paper presents a zero-shot code generation approach for Tabular Question Answering in SemEval 2025 Task 8. The system leverages a Large Language Model to dynamically generate executable Python code for extracting information from tabular data based on natural language questions. The modular pipeline consists of a column selector, answer generator, and iterative code fixer. The approach achieves competitive performance, ranking 32nd and 31st among 49 participants, demonstrating strong code generation capabilities while revealing challenges with complex data structures.

## Method Summary
The system employs a three-stage pipeline for zero-shot tabular QA. First, a column selector identifies relevant table attributes using an LLM with prompt-based context. Second, the answer generator produces executable Python code to extract information based on the question and selected columns. Third, an iterative code fixer handles runtime errors through repeated execution attempts with modified code. The entire process operates without training on task-specific data, relying solely on the LLM's pre-trained capabilities and carefully crafted prompts.

## Key Results
- Achieved 65 points accuracy in Subtask 1 during test phase
- Achieved 68 points accuracy in Subtask 2 during test phase
- Ranked 32nd in Subtask 1 and 31st in Subtask 2 among 49 participants

## Why This Works (Mechanism)
The system works by leveraging the LLM's natural language understanding capabilities to bridge the gap between human questions and programmatic data extraction. The LLM interprets the question context, maps it to relevant table columns, and generates syntactically correct Python code that can execute against the tabular data. The iterative code fixer mechanism provides resilience against runtime errors by attempting multiple execution paths, while the modular design allows each component to focus on its specific task without requiring end-to-end training.

## Foundational Learning
1. **Zero-shot learning** - Why needed: Enables system to handle unseen table schemas without task-specific training; Quick check: Verify no fine-tuning data was used for specific table formats
2. **Code generation from natural language** - Why needed: Translates human questions into executable Python code; Quick check: Test with diverse question types and table structures
3. **Iterative error correction** - Why needed: Handles runtime exceptions in generated code; Quick check: Measure success rate of code fixer across different error types
4. **Column selection for tabular data** - Why needed: Identifies relevant attributes before code generation; Quick check: Evaluate precision of column selection across question types
5. **LLM prompting for structured tasks** - Why needed: Guides model to produce appropriate code structure; Quick check: Test different prompt formulations for code quality
6. **Tabular data processing** - Why needed: Understands table structure and data types; Quick check: Validate handling of various data formats (strings, numbers, dates)

## Architecture Onboarding
**Component Map**: Question -> Column Selector -> Answer Generator -> Iterative Code Fixer -> Final Answer

**Critical Path**: Natural language question flows through column selector to identify relevant table attributes, then to answer generator which produces Python code, followed by iterative code fixer that executes and refines the code until successful extraction or timeout.

**Design Tradeoffs**: Zero-shot approach sacrifices adaptation capability for broad applicability; modular pipeline enables focused error handling but adds execution overhead; LLM-based components provide flexibility but may produce inconsistent outputs.

**Failure Signatures**: Runtime errors during code execution, incorrect column selection leading to irrelevant code, generated code that fails to handle complex data types (lists/dictionaries), infinite loops in iterative fixer due to unresolvable errors.

**Three First Experiments**:
1. Test column selection accuracy with increasingly complex table schemas
2. Measure code fixer success rate across different error types
3. Evaluate performance degradation when handling nested data structures

## Open Questions the Paper Calls Out
None

## Limitations
- Struggles with complex nested data structures like lists and dictionaries
- Zero-shot approach limits adaptation to domain-specific terminology and specialized table schemas
- Iterative code fixer lacks transparency regarding success rates and computational overhead

## Confidence
**High Confidence**: The system successfully generates and executes Python code for tabular QA, achieving verifiable accuracy scores (65 and 68 points) and ranking among 49 participants.

**Medium Confidence**: Claims of "strong performance in code generation and execution" are qualified by admitted struggles with complex data types, suggesting the performance is good but not optimal.

**Low Confidence**: The paper lacks specific metrics on struggles with lists and dictionaries, providing no detailed error analysis or frequency data for failures with these complex data structures.

## Next Checks
1. Conduct detailed breakdown of runtime errors encountered during execution, categorizing by type and frequency to identify most common failure modes in code fixer mechanism.

2. Create controlled benchmark with tables containing various levels of nested lists and dictionaries, systematically measuring success rate of code generation and execution across different nesting depths.

3. Test system on tables with non-standard schemas including unconventional column names, mixed data types, and implicit relationships to quantify impact of schema complexity on zero-shot performance.