---
ver: rpa2
title: 'Yet another algorithmic bias: A Discursive Analysis of Large Language Models
  Reinforcing Dominant Discourses on Gender and Race'
arxiv_id: '2508.10304'
source_url: https://arxiv.org/abs/2508.10304
tags:
- language
- discursive
- women
- black
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender and racial biases in Large Language
  Models (LLMs) through a qualitative discursive analysis of short stories generated
  about Black and white women. Unlike automated bias detection methods, this research
  examines how LLMs reproduce stereotypes by analyzing narrative structures, metaphors,
  and recurring themes.
---

# Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race

## Quick Facts
- arXiv ID: 2508.10304
- Source URL: https://arxiv.org/abs/2508.10304
- Reference count: 8
- Primary result: Qualitative discursive analysis reveals LLMs reproduce racial and gender stereotypes through narrative structures, with Black women portrayed via ancestry/resilience themes and white women through self-discovery narratives

## Executive Summary
This study investigates how Large Language Models reproduce gender and racial biases through qualitative discourse analysis of short stories about Black and white women. Unlike automated bias detection methods, the research examines narrative structures, metaphors, and recurring themes to understand how models perpetuate stereotypical representations. The analysis reveals that Black women are primarily portrayed through ancestry, resilience, and resistance narratives, while white women appear in self-discovery and belonging stories. When prompted to correct these biases, models offered superficial revisions that preserved problematic underlying meanings. The findings demonstrate that LLMs reflect and reinforce historically dominant discourses rather than critically engaging with ideological implications of their outputs.

## Method Summary
The study collected 82 short stories from 7 different language models (LLaMa3-8b-8192, LLaMa3-70B-8192, Sabiá-2, Sabiá-3, GPT-4, GPT-4o, GPT-4o with Canva) using public chat interfaces. Stories were generated via prompts asking for narratives about Black and white women in both Portuguese and English. The researchers applied a five-layer qualitative discourse analysis: corpus familiarization, extraction of Discursive Sequences (DSs), comparative analysis across models and languages, synthesis of Reference Discursive Sequences (DSRs), and interpretation of discursive resonances. This interdisciplinary approach was validated by team members with backgrounds in computer science, humanities, and social sciences.

## Key Results
- Black women were portrayed primarily through ancestry, resilience, and resistance themes, while white women appeared in self-discovery and belonging narratives
- When asked to identify and correct biases, models flagged superficial textual elements while preserving underlying narrative structures
- Geographic and cultural essentialization emerged, with Black women often linked to African settings and white women to universal or Western contexts
- The study demonstrates that current LLMs lack the semantic understanding needed to recognize and address ideological biases in their outputs

## Why This Works (Mechanism)

### Mechanism 1: Statistical Pattern Inheritance from Training Corpora
LLMs reproduce stereotypical narratives because their outputs reflect statistically dominant patterns in training data rather than critical understanding of meaning. Models generate text through probabilistic token associations optimized for surface-level coherence, reproducing frequent co-occurrences like "Black woman" + "strength" + "resistance" without understanding their ideological implications.

### Mechanism 2: Discursive Formation Constraint on Narrative Possibility
Models are constrained by dominant Western discursive formations that position Black and white women in oppositional, racially-coded narrative structures. The "discursive formation" defines what can be said and who is authorized to speak, crystallizing in training data to limit narrative possibilities for different demographic groups.

### Mechanism 3: Semantic-Statistical Gap in Bias Correction
Models fail to correct biases meaningfully because they lack discursive understanding—they treat bias as a surface-level lexical problem rather than an ideological one. When prompted to "remove bias," models identify superficial markers while preserving underlying meaning structures, flagging contextually neutral terms while missing racialized narrative constraints.

## Foundational Learning

- **Discursive Formation (Pêcheux)**: Language operates within systems of rules that determine what can be said and who can say it—explains why statistical patterns reproduce ideology. *Quick check*: Can you explain why the same word ("strength") carries different ideological weight when applied to Black vs. white characters?

- **Representational Memory**: Explains how bodies and identities become "crystallized" in discourse through repetitive historical representation, constraining what narratives are possible. *Quick check*: How does the "ancestry" theme function differently for Black women (historical trauma/resistance) vs. white women (genetic curiosity)?

- **Statistical vs. Discursive Coherence**: Distinguishes surface-level plausibility from semantic depth—critical for understanding why "fluent" outputs can still be harmful. *Quick check*: If a model generates a grammatically correct story about a Black woman overcoming racism, what makes it potentially biased despite being "coherent"?

## Architecture Onboarding

- **Component map**: Training Corpora → [Statistical Learning] → Token Probability Distributions → [Discursive Formations Embedded] → Prompt → [Narrative Retrieval] → Output → Correction Prompt → [Surface-level Modification] → Paraphrased Output
- **Critical path**: Training data collection → Discursive pattern absorption → Generation conditioned on dominant formations → Surface-level correction attempts
- **Design tradeoffs**: Quantitative bias metrics (scalable) vs. qualitative discourse analysis (captures nuance but labor-intensive); Automated bias detection (misses ideological depth) vs. human interpretive analysis (context-sensitive but not replicable at scale); Surface-level debiasing (achievable) vs. structural narrative change (requires semantic understanding models lack)
- **Failure signatures**: Models flag contextually neutral terms as biased while missing ideological patterns; Correction prompts produce paraphrasing rather than narrative restructuring; Consistent thematic divergence across models trained on similar corpora; Geographic/cultural essentialization
- **First 3 experiments**: 1) Baseline audit: Generate 50 stories each for "Black woman" and "white woman" prompts across multiple models; manually code for ancestry/resistance vs. self-discovery themes. 2) Correction failure analysis: Take 10 biased outputs, prompt models to identify and correct bias. Compare what models flag vs. what discourse analysis identifies. 3) Counter-narrative injection: Fine-tune a small model on deliberately diverse narratives about Black women. Test whether this shifts statistical associations at inference time.

## Open Questions the Paper Calls Out

- **How do LLMs represent identities beyond the binary of Black and white women, particularly regarding other underrepresented social categories?** The current study limited its scope strictly to intersecting gender and race biases affecting Black and white women.

- **How do subtle textual choices in LLM outputs, such as character names, reflect underlying racial and gender biases?** The study focused on narrative structures and themes, with only preliminary observations regarding naming conventions.

- **How do discursive biases manifest in LLMs across different languages and cultural contexts?** The study primarily analyzed Portuguese and English, noting differences but not fully exploring cultural frameworks of other languages.

- **Can the qualitative discursive framework developed in this study be operationalized into automated tools to improve technical bias mitigation?** There is a gap between qualitative identification of bias and technical implementation of mitigation strategies in model training.

## Limitations
- The study relies on qualitative interpretation which may introduce researcher subjectivity despite inter-rater validation
- Model versions and generation parameters are not fully controlled, introducing variability in outputs
- Only 82 stories across 7 models, limiting statistical generalizability of thematic patterns
- The study focuses specifically on short story generation; findings may not generalize to other text generation tasks

## Confidence
- **High**: The core finding that Black women are predominantly portrayed through ancestry/resistance themes while white women appear in self-discovery narratives is well-supported by systematic qualitative analysis
- **Medium**: The claim that models fail to meaningfully correct biases because they lack discursive understanding is compelling but depends on subjective interpretation of "superficial" corrections
- **Low**: The assertion that current quantitative bias metrics are insufficient is stated but not empirically tested against alternative measurement approaches

## Next Checks
1. **Replication with controlled parameters**: Generate stories using API access with fixed temperature and seed values across all models to ensure comparability and rule out generation randomness as a factor
2. **Quantitative validation of qualitative themes**: Code a subset of stories using both qualitative discourse analysis and quantitative theme frequency analysis to test whether manual coding reveals patterns that automated metrics miss
3. **Cross-task bias consistency**: Test whether the same thematic patterns emerge in other generation tasks (e.g., news articles, product reviews) to determine if the bias is specific to creative writing or reflects broader discursive formations in model training data