---
ver: rpa2
title: 'Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for
  MLLM Alignment'
arxiv_id: '2510.05283'
source_url: https://arxiv.org/abs/2510.05283
tags:
- reward
- reasoning
- harmo
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces HARMO, a hybrid reward modeling framework
  for aligning multimodal large language models (MLLMs). It combines rule-based rewards
  for verifiable tasks with learned reward models for subjective quality, and adds
  multi-aspect rewards to enforce instruction adherence and prevent reward hacking
  via brevity.
---

# Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment

## Quick Facts
- arXiv ID: 2510.05283
- Source URL: https://arxiv.org/abs/2510.05283
- Authors: Radha Gulhane; Sathish Reddy Indurthi
- Reference count: 22
- Primary result: 9.5% overall average improvement, 16% math-specific gain over baseline

## Executive Summary
This paper introduces HARMO, a hybrid reward modeling framework for aligning multimodal large language models (MLLMs). It combines rule-based rewards for verifiable tasks with learned reward models for subjective quality, and adds multi-aspect rewards to enforce instruction adherence and prevent reward hacking via brevity. The framework is validated on mathematical and multimodal reasoning benchmarks, achieving an overall average improvement of 9.5% over the baseline and a 16% gain on math-specific tasks. The method improves both reasoning accuracy and output quality while reducing dependency on extensive human data annotation.

## Method Summary
HARMO implements a hybrid reward framework that routes tasks to either rule-based verifiers (for math, logic, multiple-choice) or learned reward models (for open-ended quality assessment). The reward combines three components: hybrid rewards (R_hybrid), length penalty (R_λ), and format adherence (R_fmt). GRPO optimization is used with modified mean-only advantage normalization to reduce difficulty-dependent bias. The framework was evaluated on the VLAA-Thinking dataset using Qwen2.5-VL-3B/7B-Instruct models with both full RM and embedding-based reward models.

## Key Results
- 9.5% overall average improvement across all reasoning benchmarks
- 16% improvement specifically on mathematical reasoning tasks
- Length penalty successfully stabilizes output quality and prevents brevity bias
- Multi-aspect rewards (format adherence + length penalty) contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
Hybrid reward combining rule-based verification with learned preference models provides more reliable training signal than monolithic rewards. For verifiable tasks, rule-based verifiers produce high-confidence binary signals, while learned RMs score subjective quality for open-ended tasks. This division prevents the RM from rewarding plausible-but-incorrect outputs.

### Mechanism 2
Dynamic length penalty counteracts reward hacking via brevity, stabilizing output quality. For each generation group, minimum length among correct responses establishes a threshold, and incorrect responses shorter than this are penalized. This targets the failure mode where RL policies learn to produce short, incomplete answers.

### Mechanism 3
Mean-only advantage normalization reduces difficulty-dependent bias compared to mean+std normalization. Standard GRPO normalization can overweight high-variance prompts based on reward variance. Using only mean-centering creates uniformly scaled advantages across difficulty levels.

## Foundational Learning

- **Reward Hacking**: Why needed here: Core motivation for multi-aspect rewards; models exploit single-metric optimization at expense of actual quality. Quick check: Can you explain why an RL policy might learn to output "I don't know" to maximize accuracy reward on difficult questions?

- **GRPO (Group-Relative Policy Optimization)**: Why needed here: HARMO builds on GRPO's critic-free framework, adding hybrid rewards instead of learned RM dependency. Quick check: How does GRPO estimate advantage without a learned value function?

- **Calibration in Reward Models**: Why needed here: Paper claims monolithic RMs "lack confidence calibration across domain-specific tasks." Quick check: Why might a reward model assign high scores to fluent but factually incorrect responses?

## Architecture Onboarding

- Component map: Input (prompt + image) → Policy π_θ → G sampled responses → HARMO Reward Computation → Advantage estimation → PPO-style clipped objective + KL penalty

- Critical path: (1) Correct task routing for R_hybrid, (2) Valid λ_min computation when ≥1 correct response exists, (3) Stable advantage estimation across heterogeneous prompts

- Design tradeoffs: Embedding-based RM (22M params) vs. full RM (7B params): former is cheaper but less nuanced; paper shows full RM + rules performs best. Penalty cap P_max controls maximum length penalty. Group size G affects advantage estimation variance.

- Failure signatures: Response length crashes during training → length penalty not triggering. Math accuracy degrades → rule-based verifier misclassifying or RM over-weighted on open-ended tasks. High variance in rewards across runs → advantage normalization unstable.

- First 3 experiments: (1) Ablate each reward component: Train with H-only, H+F, H+F+λ to reproduce Table 3 progression. (2) Stress-test task routing: Construct adversarial dataset with ambiguous verifiable/open-ended labels. (3) Baseline comparison: Train standard GRPO with monolithic RM on same data.

## Open Questions the Paper Calls Out

- Would a learned, dynamic weighting of the hybrid and multi-aspect reward components yield better performance than the fixed summation used in the current HARMO formulation? The Conclusion states HARMO provides a foundation for future research into "dynamic reward weighting."

- Does the 16% performance gain on mathematical reasoning tasks scale effectively to significantly larger models (e.g., 70B+ parameters), or does the signal diminish? Experiments are restricted to Qwen2.5-VL 3B and 7B families.

- How robust is the dynamic length penalty against the generation of low-quality verbose outputs in domains where longer responses are not strictly penalized by the accuracy reward? The mechanism might inadvertently encourage longer incorrect responses or fail to curb verbosity in open-ended tasks.

## Limitations
- Hybrid routing effectiveness depends critically on accurate classification of tasks as verifiable versus open-ended, but limited detail is provided on how this classification is performed.
- Length penalty mechanism assumes at least one correct response exists in each generation group to establish λ_min; fallback strategies for all-incorrect scenarios are not specified.
- Embedding-based reward model effectiveness versus full RM is only partially evaluated, with limited analysis of signal quality for subjective quality assessment.

## Confidence
- High Confidence: Overall performance improvements (9.5% average, 16% math-specific) are well-supported by experimental results in Table 2 and 3.
- Medium Confidence: Mean-only advantage normalization reducing difficulty-dependent bias is supported by citations but lacks direct empirical comparison within the paper.
- Low Confidence: Effectiveness of embedding-based reward model versus full RM is only partially evaluated, with insufficient analysis of signal quality for all task types.

## Next Checks
1. Task Classification Robustness: Construct a test set with ambiguous or borderline cases between verifiable and open-ended tasks to measure classification accuracy and evaluate impact of incorrect routing.

2. Length Penalty Edge Cases: Systematically test scenarios where all sampled responses are incorrect to verify that the training loop handles undefined λ_min appropriately.

3. Advantage Normalization Comparison: Run controlled experiments comparing mean-only versus mean+std advantage normalization on the same training data to directly measure the claimed reduction in difficulty-dependent bias.