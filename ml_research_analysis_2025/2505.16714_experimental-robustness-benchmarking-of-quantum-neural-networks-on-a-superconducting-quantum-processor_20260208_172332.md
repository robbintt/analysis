---
ver: rpa2
title: Experimental robustness benchmarking of quantum neural networks on a superconducting
  quantum processor
arxiv_id: '2505.16714'
source_url: https://arxiv.org/abs/2505.16714
tags:
- quantum
- robustness
- adversarial
- input
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first experimental benchmarking of robustness
  for 20-qubit quantum neural network (QNN) classifiers executed on a superconducting
  quantum processor. The study introduces an efficient adversarial attack algorithm,
  masked adversarial attack (Mask-FGSM), tailored for quantum hardware to probe QNN
  robustness.
---

# Experimental robustness benchmarking of quantum neural networks on a superconducting quantum processor

## Quick Facts
- **arXiv ID:** 2505.16714
- **Source URL:** https://arxiv.org/abs/2505.16714
- **Reference count:** 0
- **Primary result:** First experimental benchmarking of 20-qubit QNN robustness on superconducting hardware, with adversarial attacks validated against fidelity-based bounds.

## Executive Summary
This work presents the first experimental benchmarking of robustness for 20-qubit quantum neural network classifiers executed on a superconducting quantum processor. The study introduces an efficient adversarial attack algorithm, masked adversarial attack (Mask-FGSM), tailored for quantum hardware to probe QNN robustness. Experimental results show that the extracted robustness upper bounds deviate by only 3×10⁻³ from theoretical lower bounds, confirming the tightness of fidelity-based robustness bounds and the effectiveness of the attack strategy. Adversarial training significantly enhances QNN robustness by regularizing input gradients, reducing sensitivity to targeted perturbations. Additionally, QNNs exhibit superior adversarial robustness compared to classical neural networks, attributed to inherent quantum noise-induced gradient attenuation. This work establishes a scalable framework for diagnosing QML security and reliability in realistic hardware settings.

## Method Summary
The study evaluates robustness of 20-qubit QNN classifiers on a superconducting processor using a masked adversarial attack (Mask-FGSM) that exploits gradient sparsity to reduce computational cost. The QNN architecture employs a hardware-efficient ansatz with SU(2) and CZ gates across 40 layers (181 parameters), trained via Adam optimizer with mini-batch SGD. Input data includes EMNIST letter images (downsampled to 15×15) and linear cluster excitation identification states. Robustness is quantified using fidelity-based bounds (R_LB) compared against attack-derived upper bounds (R_UB), with error mitigation via Iterative Bayesian Unfolding for readout correction.

## Key Results
- First experimental validation of QNN robustness on superconducting hardware with 20-qubit classifiers
- Masked adversarial attack (Mask-FGSM) achieves 3×10⁻³ deviation between empirical upper and theoretical lower robustness bounds
- QNNs demonstrate superior adversarial robustness compared to classical networks due to quantum noise-induced gradient attenuation
- Adversarial training enhances robustness by 40% through input gradient regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localized (Masked) adversarial attacks enable efficient robustness probing on noisy hardware by exploiting input-gradient sparsity.
- **Mechanism:** The Masked Fast Gradient Sign Method (Mask-FGSM) constructs a binary mask $M$ by identifying the top $r$ fraction of input features with the largest gradient magnitudes. By perturbing only these coordinates, the attack reduces the number of circuit evaluations (gradient estimations) by a factor of $1/r$ and filters out "noisy" low-magnitude gradients that are dominated by measurement noise.
- **Core assumption:** The input-gradient vectors of the QNN are sparse, and a small subset of features captures the majority of the gradient mass ($G_r/G \geq 0.9$).
- **Evidence anchors:**
  - [Page 3]: "The binary mask is derived by analyzing the magnitude of input gradient components... minimizing the computational cost."
  - [Page 6]: "This attack strategy computes gradients solely for mask-selected coordinates, reducing the overall gradient estimation... by approximately a factor of $1/r$."
  - [Corpus]: "Adversarial Threats in QML" highlights the general necessity of efficient attacks, though specific masking mechanisms are unique to this paper.
- **Break condition:** If the gradient distribution becomes dense (no dominant features) or noise levels swamp the top gradient signals, the mask will fail to identify vulnerable subspaces, rendering the attack ineffective.

### Mechanism 2
- **Claim:** Adversarial training enhances QNN robustness by regularizing the alignment between input gradients and perturbation vectors.
- **Mechanism:** Adversarial training incorporates perturbed samples into the loss function. A first-order Taylor expansion shows this introduces a regularization term $R = \frac{1}{2} \nabla_x L \cdot \delta$. Minimizing this term forces the model to "rotate" its input gradients away from the perturbation direction, reducing the sensitivity $S = \Delta L / \hat{\epsilon}$.
- **Core assumption:** The perturbation $\delta$ is small enough for the first-order approximation to hold.
- **Evidence anchors:**
  - [Page 5]: "Adversarial training reduces sensitivity to targeted perturbations by regularizing input gradients."
  - [Page 8]: "Adversarial training introduces a regularization term R... encouraging turning the direction of the input gradient away from the target perturbation."
- **Break condition:** If the perturbation size $\epsilon$ is too large, higher-order terms dominate, and the gradient regularization no longer guarantees robustness.

### Mechanism 3
- **Claim:** Inherent quantum noise in NISQ devices confers superior adversarial robustness compared to noiseless classical models via gradient attenuation.
- **Mechanism:** Hardware noise (relaxation/dephasing) is modeled as a depolarizing channel with contraction factor $\xi$. This contracts the Bloch vector and scales the sensitivity $S$ by a factor $C(\xi, l, z_0) < 1$. This "gradient masking" effect dampens the model's response to small adversarial perturbations while (ideally) preserving the signal for larger legitimate features.
- **Core assumption:** The noise level is high enough to mask perturbations but low enough to maintain classification accuracy (a "low-pass filter" regime).
- **Evidence anchors:**
  - [Abstract]: "QNNs exhibit superior adversarial robustness compared to classical neural networks, attributed to inherent quantum noise."
  - [Page 6]: "We attribute this experimentally observed robustness enhancement to noise-induced gradient attenuation... effectively masking the fine-grained perturbations."
  - [Page 11]: "Noise reduces sensitivity and thereby increases the robustness score."
- **Break condition:** If error mitigation techniques completely remove this noise, or if noise levels increase to the point of destroying the classification signal (accuracy drops), this robustness advantage disappears.

## Foundational Learning

- **Concept:** **Variational Quantum Circuits (VQC) & Parameter Shift Rule**
  - **Why needed here:** The QNN architecture is a VQC. Understanding how gradients are computed on hardware (Parameter Shift Rule) is essential to grasp why attacks are expensive and why the "Masked" approach is necessary.
  - **Quick check question:** Can you explain why calculating the gradient for a single parameter on a QNN requires at least two circuit evaluations?

- **Concept:** **Fidelity-based Robustness Bounds**
  - **Why needed here:** The paper validates its attack by comparing the empirical upper bound ($R_{UB}$) derived from attacks against a theoretical lower bound ($R_{LB}$) based on quantum state fidelity.
  - **Quick check question:** Does a smaller gap between the lower and upper robustness bounds indicate a tighter (better) estimation of the model's true robustness?

- **Concept:** **FGSM (Fast Gradient Sign Method)**
  - **Why needed here:** Mask-FGSM is a modification of the classical FGSM. Understanding the baseline attack (perturbing in the direction of the gradient sign) is required to understand the innovation of the masking technique.
  - **Quick check question:** In standard FGSM, how does the sign of the gradient determine the direction of the adversarial perturbation?

## Architecture Onboarding

- **Component map:**
  - 72-qubit superconducting processor (20-qubit 1D array selected) -> Hardware-efficient ansatz (40 layers, 181 parameters) -> Interleaved data encoding + variational blocks -> Pauli-Z measurement -> Classical Adam optimizer + MB-SGD

- **Critical path:**
  1. **Data Encoding:** Downsample images ($15 \times 15$) or prepare quantum states (LCEI).
  2. **Circuit Execution:** Run state preparation + variational layers.
  3. **Measurement:** Measure $\langle \sigma_z \rangle$ to derive prediction probability $p$.
  4. **Attack/Train:** If attacking, compute gradients via Parameter Shift Rule $\to$ Construct Mask $\to$ Apply Mask-FGSM.

- **Design tradeoffs:**
  - **Mask Sparsity ($r$):** A lower $r$ (e.g., 0.15) reduces cost but risks missing vulnerable features. The paper empirically selects $r$ where $G_r/G \approx 0.9$.
  - **Noise vs. Robustness:** Intrinsic noise aids robustness (Mechanism 3) but degrades clean accuracy. The system must operate in a regime where noise acts as a regularizer, not a destructor.

- **Failure signatures:**
  - **Ineffective Attack:** Gap between $R_{UB}$ and $R_{LB}$ widens significantly (indicates Mask-FGSM is missing the optimal perturbation direction).
  - **Training Collapse:** Adversarial training fails to converge if the perturbation strength $\epsilon$ is too high relative to the data scale.

- **First 3 experiments:**
  1. **Verify Gradient Sparsity:** Run gradient estimation on a batch of samples and plot the cumulative $L_1$ mass ($G_r/G$) to confirm the "elbow" point justifies the chosen mask ratio (e.g., 0.15).
  2. **Validate Robustness Bounds:** Execute Mask-FGSM on 10 random samples, extract the empirical Upper Bound ($R_{UB}$), and compare it against the theoretical Lower Bound ($R_{LB}$) to ensure the gap is $\approx 3 \times 10^{-3}$.
  3. **Ablation on Noise:** Compare the robustness score of the QNN on the hardware against a noiseless classical simulator (or FNN) to isolate the "noise-induced robustness" effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the robustness evaluation framework be effectively extended to unsupervised quantum machine learning models, such as variational quantum autoencoders?
- Basis in paper: [explicit] The conclusion states, "Future research should extend the robustness evaluation framework to a broader class of QMLs, including unsupervised models such as variational quantum autoencoders."
- Why unresolved: The current experimental study is restricted to supervised binary classification tasks (EMNIST and LCEI).
- Evidence: Successful application of the Mask-FGSM attack and robustness metrics on unsupervised quantum circuits on hardware.

### Open Question 2
- Question: Do quantum analogs of advanced classical defense strategies, such as defensive distillation, provide superior or more universal protection than the adversarial training demonstrated here?
- Basis in paper: [explicit] The authors note that while classical methods like defensive distillation and gradient regularization exist, "their quantum analogs remain largely unexplored."
- Why unresolved: The paper demonstrates that adversarial training acts primarily as a "patching mechanism" rather than providing universal protection against high-dimensional adversarial examples.
- Evidence: Development and experimental benchmarking of quantum defensive distillation protocols on superconducting processors.

### Open Question 3
- Question: Does the superior adversarial robustness of QNNs over classical neural networks persist in fault-tolerant quantum computing regimes where hardware noise is significantly reduced?
- Basis in paper: [inferred] The paper attributes the QNN's robustness advantage to "inherent quantum noise-induced gradient attenuation" but explicitly calls for "extending the robustness estimation to large-scale fault-tolerant quantum computing."
- Why unresolved: If noise acts as a natural gradient mask that aids robustness, reducing noise for fault tolerance might reduce this inherent advantage.
- Evidence: Comparative benchmarking of robustness scores between noisy intermediate-scale quantum (NISQ) devices and error-mitigated or fault-tolerant simulations.

## Limitations
- **Hardware-specific noise:** The observed QNN robustness advantage depends on specific noise characteristics (T₁, T₂) of the 72-qubit processor, which may not generalize to different quantum hardware.
- **Limited dataset scale:** Robustness validation performed on only 100 EMNIST samples and 50 LCEI states, potentially insufficient for statistical significance across broader distributions.
- **Gradient sparsity assumption:** Mask-FGSM effectiveness relies on consistent input-gradient sparsity across samples, which may degrade with more complex or diverse datasets.

## Confidence

- **High Confidence:** The theoretical derivation of fidelity-based robustness bounds and the Mask-FGSM algorithm's computational efficiency (gradient evaluation reduction by factor $1/r$) are mathematically sound and experimentally validated.
- **Medium Confidence:** The attribution of QNN robustness advantage to quantum noise-induced gradient attenuation is plausible but hardware-specific. Different NISQ devices with varying noise profiles may not exhibit the same robustness enhancement.
- **Medium Confidence:** The 3×10⁻³ bound gap indicates tight bounds, but this measurement is based on a limited sample size. Statistical significance across broader datasets requires verification.

## Next Checks

1. **Gradient Sparsity Verification:** Execute gradient estimation on 50 random samples and plot the cumulative $L_1$ mass ($G_r/G$) distribution to confirm the "elbow" point consistently justifies the chosen mask ratio $r \approx 0.15$ across diverse inputs.

2. **Cross-Processor Validation:** Compare robustness scores between the original 72-qubit processor and a different superconducting quantum processor (or simulator with controlled noise) to isolate hardware-specific effects from fundamental QNN properties.

3. **Statistical Bound Tightness:** Increase sample size to 500 EMNIST and 200 LCEI states, recalculate $R_{UB}$ and $R_{LB}$, and perform statistical hypothesis testing to verify the 3×10⁻³ gap is significant (p < 0.05) and not due to sampling variance.