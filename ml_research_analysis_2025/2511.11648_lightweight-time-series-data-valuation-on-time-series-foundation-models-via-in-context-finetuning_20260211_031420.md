---
ver: rpa2
title: Lightweight Time Series Data Valuation on Time Series Foundation Models via
  In-Context Finetuning
arxiv_id: '2511.11648'
source_url: https://arxiv.org/abs/2511.11648
tags:
- data
- time
- series
- valuation
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient time series data
  valuation for large-scale time series foundation models (TSFMs). Traditional data
  valuation methods are computationally prohibitive for TSFMs due to their reliance
  on expensive Hessian calculations or exponential subset sampling.
---

# Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning

## Quick Facts
- arXiv ID: 2511.11648
- Source URL: https://arxiv.org/abs/2511.11648
- Reference count: 40
- Primary result: LTSV approximates influence functions through single-step in-context finetuning, achieving efficient data valuation that scales linearly with model size and generalizes across downstream architectures

## Executive Summary
This paper addresses the computational challenge of data valuation for time series foundation models (TSFMs), which typically requires expensive Hessian calculations. The proposed LTSV method approximates influence functions through lightweight in-context finetuning, measuring the change in context loss after a single gradient update. By incorporating temporal block aggregation and hierarchical scoring, LTSV captures both temporal dependencies and sample-level contributions. Experiments across five datasets and three TSFM architectures demonstrate that LTSV identifies high-quality samples that improve model performance, with top 50% selections achieving comparable results to full-data fine-tuning while being computationally efficient.

## Method Summary
LTSV computes data valuations by partitioning target data into overlapping temporal blocks, performing one-step in-context finetuning on each block, and measuring the resulting change in context loss. The method uses a five-fold strategy where four folds serve as context and one as target. Block-level influence scores are aggregated hierarchically to produce sample-level valuations. The approach approximates classical influence functions without requiring Hessian inversion, making it scalable to large TSFMs. Hyperparameters include learning rate of 1e-5, Adam optimizer, and block length of 100 by default. The method demonstrates transferability of valuations across diverse downstream architectures.

## Key Results
- LTSV achieves comparable or better performance than full-data fine-tuning when using the top 50% of high-valued samples
- The method scales linearly with model size versus cubic scaling of traditional influence functions
- Valuations computed on TSFMs generalize effectively to diverse downstream models including DLinear, PatchTST, and PAttn
- LTSV shows robustness to temporal block length variations, with modest performance impact across L ∈ {50, 75, 100, 125}

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single gradient update on a target sample, measured by its effect on context loss, approximates the classical influence function without requiring Hessian matrix inversion.
- Mechanism: Through first-order Taylor expansion, the paper shows that influence is proportional to the change in context loss after one gradient step, replacing H⁻¹∇θL computation with an observable loss difference.
- Core assumption: The learning rate η is sufficiently small for the first-order approximation to hold within a local optimization neighborhood.
- Evidence anchors: Theoretical derivation in Theorem 2; limited direct support from related TFMAdapter work.
- Break condition: Large learning rates or distant model convergence invalidate the approximation.

### Mechanism 2
- Claim: Hierarchical aggregation from overlapping temporal blocks preserves time-series dependencies that point-wise valuation would miss.
- Mechanism: Time series are divided into overlapping blocks of length L (default 100). Block-level influence scores are computed, averaged across blocks containing each point, then aggregated to sample-level scores.
- Core assumption: Temporal dependencies operate within the block length L; overlapping coverage ensures evaluation in multiple temporal contexts.
- Evidence anchors: Equations formalizing hierarchical aggregation; ablation study shows robustness across block lengths.
- Break condition: Block length too short (misses long-range dependencies) or too long (dilutes local signal).

### Mechanism 3
- Claim: Data valuations computed on a foundation model transfer to diverse downstream architectures without recomputation.
- Mechanism: Pre-trained TSFMs learn universal temporal representations; samples that improve TSFM performance also benefit downstream models because they share underlying quality characteristics.
- Core assumption: The foundation model's learned inductive biases align with what benefits downstream models—high-quality time series is quality-agnostic across architectures.
- Evidence anchors: Transferability demonstrated across Time-MoE, DLinear, PatchTST, and PAttn; supported by TFMAdapter observations about TSFM generalization.
- Break condition: Fundamentally mismatched inductive biases between TSFM and downstream model.

## Foundational Learning

- Concept: **Influence Functions**
  - Why needed here: The theoretical foundation for data valuation—quantifies how a training sample affects model predictions on validation data.
  - Quick check question: Can you explain why computing H⁻¹ (Hessian inverse) becomes prohibitive for models with millions of parameters?

- Concept: **In-Context Finetuning**
  - Why needed here: The paper's key insight substitutes a single gradient step for full influence computation; understanding why this approximation works is central.
  - Quick check question: How does one-step gradient update differ from full fine-tuning, and why is the approximation sufficient here?

- Concept: **Temporal Dependencies in Time Series**
  - Why needed here: Unlike i.i.d. data, time series samples exhibit autocorrelation; the block aggregation mechanism explicitly addresses this.
  - Quick check question: Why would naive point-wise valuation fail to capture temporal structure?

## Architecture Onboarding

- Component map:
  - Data Partitioner -> Block Segmenter -> In-Context Finetuning Engine -> Context Loss Calculator -> Hierarchical Aggregator -> Data Selector

- Critical path:
  1. Partition target data using 5-fold strategy (one fold target, four context)
  2. For each block: finetune → compute context loss change → store block score
  3. Aggregate to sample-level valuations
  4. Rank and select high-value samples
  5. Train downstream model on selected subset

- Design tradeoffs:
  - Block length L: Shorter = granular but higher compute; longer = more context but diluted signal. Paper finds L=75-100 optimal.
  - Selection ratio: 50% matches full-data performance; 20% exposes quality differences more sharply.
  - Foundation model size: Larger TSFMs (Time-MoE 2.4B) may yield more transferable valuations but slower; MOMENT (40M) also works.

- Failure signatures:
  - Top-k underperforms random: Block length may mismatch data's temporal scale; run ablation on L.
  - Valuations don't transfer: Verify TSFM and downstream model share task type (both forecasting); cross-task transfer untested.
  - Computation slow: Confirm single-step finetuning (not multi-step); check Hessian isn't accidentally computed.

- First 3 experiments:
  1. Reproduce Table 1 on Illness dataset: verify Top-50% > Bottom-50% and ≈ full-data baseline.
  2. Transfer test: Compute valuations on Time-MoE, train DLinear on top-20%/bottom-20%/random; expect Table 2 pattern.
  3. Efficiency validation: Compare LTSV vs. classical influence runtime across model sizes (Linear→LSTM→PatchTST→MOMENT); expect linear vs. cubic scaling (Figure 2).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations about the relationship between temporal block length and intrinsic temporal dependencies, the robustness of single-step approximations under varying learning rates and non-convex loss landscapes, and the generalizability of valuations to tasks beyond forecasting such as classification, anomaly detection, or imputation.

## Limitations
- Approximation quality depends on sufficiently small learning rates and proximity to model convergence
- Temporal block aggregation may miss long-range temporal patterns if block length is suboptimal
- Transferability demonstrations are limited to specific model pairs without theoretical guarantees for arbitrary downstream architectures

## Confidence
- **High confidence**: Computational efficiency claims and core methodology are well-supported by mathematical framework and empirical timing results
- **Medium confidence**: Transferability across architectures is demonstrated but not theoretically proven; results may not generalize beyond tested model pairs
- **Medium confidence**: Block aggregation effectiveness is validated through ablation but depends on appropriate block length selection

## Next Checks
1. Test influence approximation sensitivity by varying learning rates (1e-4 to 1e-3) and measuring valuation quality degradation
2. Evaluate long-range temporal dependencies by comparing L=50, 100, 200 block lengths on datasets with known multi-scale patterns
3. Validate transfer robustness by computing valuations on TSFM, then testing on three additional downstream architectures not used in original experiments