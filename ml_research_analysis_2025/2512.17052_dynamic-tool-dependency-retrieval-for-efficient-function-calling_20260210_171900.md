---
ver: rpa2
title: Dynamic Tool Dependency Retrieval for Efficient Function Calling
arxiv_id: '2512.17052'
source_url: https://arxiv.org/abs/2512.17052
tags:
- function
- tool
- retrieval
- tools
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic Tool Dependency Retrieval (DTDR) addresses the problem
  of efficient function calling in on-device agents by improving tool retrieval with
  query- and history-aware dynamic dependency modeling. Unlike static retrievers that
  rely only on query or tool descriptions, DTDR conditions on both the user query
  and the evolving execution context, enabling adaptive retrieval of relevant tools
  as plans unfold.
---

# Dynamic Tool Dependency Retrieval for Efficient Function Calling

## Quick Facts
- arXiv ID: 2512.17052
- Source URL: https://arxiv.org/abs/2512.17052
- Reference count: 40
- Primary result: DTDR improves function calling success rates by 23%–104% vs. static retrievers while reducing prompt length up to 72%

## Executive Summary
Dynamic Tool Dependency Retrieval (DTDR) addresses the challenge of efficient function calling in on-device agents by conditioning tool retrieval on both user queries and execution history. Unlike static retrievers that rely solely on query or tool descriptions, DTDR models the evolving execution context to adaptively retrieve relevant tools as plans unfold. The approach significantly outperforms state-of-the-art static retrievers across multiple datasets and model scales, while maintaining efficiency suitable for on-device deployment through lightweight architectures and prompt-length reduction techniques.

## Method Summary
DTDR introduces two variants: DTDR-C (clustering-based) and DTDR-L (learned linear classifier), both conditioning retrieval on query and tool-call history. DTDR-C clusters demonstration queries via K-Means, builds per-cluster Markov dependency graphs, and retrieves tools by traversing these graphs based on execution history. DTDR-L trains a linear classifier to predict next-tool probabilities from concatenated query and history embeddings. Both variants use Weighted Hard Masking to prune prompts to high-probability tools, reducing prompt length while maintaining or improving accuracy. The approach is evaluated across four datasets (TinyAgent, TaskBench DailyLife, HuggingFace, Multimedia) with multiple LLM backbones.

## Key Results
- DTDR improves function calling success rates by 23%–104% compared to state-of-the-art static retrievers
- Weighted Hard Masking reduces prompt length by up to 72% while maintaining or improving accuracy
- DTDR-L achieves highest overall performance, particularly with sufficient demonstrations (≥1k)
- History length of 3 is optimal; gains taper beyond this point
- Smaller models benefit most from Hard Masking, while larger models perform better with Soft or Raw Demonstrations

## Why This Works (Mechanism)

### Mechanism 1
Conditioning retrieval on both query and full tool-call history yields more relevant tool subsets than static, single-signal approaches. The retriever computes next-tool probabilities using the concatenation of query embedding and trajectory of previously invoked tools, either via K-Means clustering mapping queries to task-specific dependency graphs (DTDR-C) or learned linear classifier on frozen embeddings (DTDR-L). This dual conditioning filters tools that are semantically relevant to intent and structurally plausible given partial plans.

### Mechanism 2
Task-specific dependency graphs (DTDR-C) or learned tool embeddings (DTDR-L) compress multi-step dependencies into lightweight retrievers suitable for on-device constraints. DTDR-C clusters demonstration queries, then builds Markov-style dependency graphs per cluster mapping history sequences to next-tool distributions. DTDR-L trains a single linear layer to predict next-tool logits from (query + history) embeddings, effectively learning tool representations that align with task context. The underlying dependency structure is low-order (history length l=3 suffices).

### Mechanism 3
Weighted Hard Masking improves function selection by restricting LLM's action space to high-probability tools while communicating confidence scores. The retriever outputs probability distribution over tools; Hard Masking prunes prompt to only include tools above threshold α, and weighted variant exposes probabilities in prompt. This reduces prompt length and focuses LLM on smaller, scored candidate set. The retriever's probability estimates must be calibrated enough that higher scores indicate genuinely more likely tools.

## Foundational Learning

- Concept: Markov-style sequence dependencies
  - Why needed here: DTDR-C explicitly constructs P(f'|f_l) where f_l is length-l history; understanding this helps interpret why low-order histories suffice and when they won't
  - Quick check question: Can you explain why a first-order Markov model might bias toward repeatedly calling the same function, and how conditioning on full trajectory mitigates this?

- Concept: Clustering for task-specific priors
  - Why needed here: DTDR-C uses K-Means to partition demonstration queries, then builds per-cluster dependency graphs; this enables query-aware but lightweight retrieval without per-query retrieval from full demonstration set
  - Quick check question: If new query falls near cluster boundary, how might retrieval quality degrade, and what could you do to make assignment more robust?

- Concept: Hard vs. Soft masking in constrained action spaces
  - Why needed here: Choice between pruning tools (Hard) vs. hinting preferences (Soft) trades off between simplifying decision problem and preserving flexibility, especially important for smaller models
  - Quick check question: When would Soft Masking be preferable to Hard Masking, and how does model scale affect this choice?

## Architecture Onboarding

- Component map: Embedding model θ (frozen sentence encoder) → DTDR-C: K-Means clusterer C → cluster-specific dependency graphs G_k → history-based traversal to retrieve F_t OR DTDR-L: Linear classifier ϕ → next-tool logits → thresholding to retrieve F_t → Prompt encoder: Weighted Hard/Soft Masking → pruned or annotated tool list fed to LLM π → LLM agent π → samples next function f_t from masked set

- Critical path: 1) Receive (q, f₀:t₋₁) 2) Embed query and (for DTDR-L) concatenated history 3) Retrieve F_t via DTDR-C (cluster lookup + graph traversal) or DTDR-L (linear layer + threshold) 4) Encode F_t into prompt via Weighted Hard Masking 5) LLM samples f_t; loop until "end-of-plan"

- Design tradeoffs: DTDR-C vs. DTDR-L: DTDR-C is unsupervised, interpretable (explicit graphs), and robust to low data; DTDR-L is supervised, higher-performing with sufficient demonstrations (≥1k), but prone to overfitting if data is scarce. Cluster count K: ~1/10 of demonstrations is optimal; too few clusters yields query-agnostic retrieval; too many (nearest-neighbor) overfits. History length l: l=3 is sweet spot; longer histories add cost without gains.

- Failure signatures: Repeated tool calls (e.g., "get email address" called excessively) → history not being used; check that f₀:t₋₁ is correctly passed to ϕ. "end-of-plan" never predicted → BM-25/QTS retrievers cannot retrieve it since it has no description; ensure dependency-aware retriever is active. Prompt length exceeds budget → Hard Masking not applied; verify threshold α and masking logic.

- First 3 experiments: 1) Baseline retrieval comparison on TinyAgent with Qwen 3 0.6B: run Random, BM-25, QTS, DR, LR, DTDR-C, DTDR-L; report MRR and Function Selection Accuracy to confirm 23–104% gains. 2) Ablation on history length (l = 0, 1, 2, 3, 4) for DTDR-C and DTDR-L on TinyAgent; plot FSA vs. l to validate l=3 saturation. 3) Prompt encoding sweep (No ICL, Raw Demonstrations, Hard Masking, Soft Masking, Weighted variants) across Qwen 3 0.6B/4B/14B; confirm that smaller models benefit most from Hard Masking and larger models from Soft or Raw Demonstrations.

## Open Questions the Paper Calls Out

- How can the DTDR framework be modified to remain robust when demonstration data contains imperfect, sub-optimal, or incorrect tool trajectories? The conclusion identifies "handling imperfect demonstrations" as primary future direction. Current DTDR-L and DTDR-C variants learn dependency probabilities and classifier weights directly from provided demonstration data D, assuming these trajectories represent valid or optimal execution paths.

- What mechanisms are required to enable DTDR to adapt to dynamically evolving tool sets (additions/removals) in real-time without full retraining? The authors list "adapting to evolving tool sets" as specific area for future work. DTDR-L uses linear classifier with output dimension fixed by number of tools |F|, and DTDR-C builds static dependency graphs; adding or removing tools currently necessitates re-indexing or retraining architecture.

- Can the Dynamic Tool Dependency Retrieval approach be generalized to multimodal agentic tasks, such as robotics, where tool inputs and outputs are not purely textual? The paper proposes "extending to multimodal tool-based tasks (e.g. robotics)" in final section. Current implementation relies on frozen text-embedding model (θ) to encode query and tool history; handling visual or auditory state contexts requires fundamentally different embedding strategy.

## Limitations

- Dataset bias and real-world generalization: Evaluation relies on synthetic or narrowly scoped agent datasets that may not reflect open-ended, multi-modal user requests or tools with ambiguous descriptions
- Dependence on demonstration quality: DTDR-C's performance hinges on K-Means clustering producing semantically coherent groups; DTDR-L requires sufficient demonstrations to avoid overfitting to frequent tools
- Masking threshold calibration: Weighted Hard Masking assumes retriever probabilities are well-calibrated, but paper does not report calibration metrics or validate that high-probability tools are indeed correct

## Confidence

- High confidence: Claims about prompt length reduction (up to 72%) and relative gains over static retrievers (23–104%) are directly supported by experimental tables and ablation studies
- Medium confidence: Claims about on-device suitability and lightweight architecture are inferred from parameter counts and clustering-based retrieval, but not benchmarked against explicit memory or latency constraints
- Low confidence: Claims about generalization to new tools or multi-domain scenarios are extrapolations; paper does not test cross-dataset transfer or novel tool introduction

## Next Checks

1. Calibration study: Measure Expected Calibration Error (ECE) of DTDR-C and DTDR-L retrievers; test whether Weighted Hard Masking's performance degrades when retriever confidences are artificially miscalibrated (e.g., via temperature scaling)

2. Cross-dataset generalization: Train DTDR-L on TinyAgent, evaluate on TaskBench DailyLife and Multimedia. Quantify drop in FSA and MRR to assess robustness to domain shift and tool set expansion

3. Prompt efficiency vs. accuracy tradeoff: Sweep masking threshold α across multiple LLM scales; plot FSA vs. prompt length reduction to identify Pareto frontier and determine whether aggressive pruning (e.g., top-1 tool only) still maintains acceptable accuracy