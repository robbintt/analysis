---
ver: rpa2
title: A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose
  to Democratic Societies
arxiv_id: '2505.00036'
source_url: https://arxiv.org/abs/2505.00036
tags:
- should
- persuasion
- your
- they
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper assesses the political persuasion risks posed by Large
  Language Model (LLM) chatbots to democratic societies by evaluating both exposure
  and persuasion effectiveness. The authors conducted two survey experiments (N =
  10,417) comparing AI chatbots to human persuasion across three political domains
  (immigration, transgender rights, minimum wage).
---

# A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies

## Quick Facts
- arXiv ID: 2505.00036
- Source URL: https://arxiv.org/abs/2505.00036
- Reference count: 40
- Primary result: LLM chatbots match human persuasion effectiveness in forced-exposure settings but face significant real-world scalability constraints

## Executive Summary
This paper evaluates the political persuasion risks posed by LLM chatbots through two large-scale survey experiments (N=10,417). The study compares AI chatbots to human video persuasion across three political domains, finding comparable per-person effectiveness under forced exposure. However, when accounting for real-world exposure costs and scalability, LLM-based persuasion costs $48-$74 per persuaded voter versus $100 for traditional methods. Current LLM persuasion is constrained by limited audience pools and low voluntary engagement rates, limiting their threat to democratic processes despite comparable effectiveness in controlled settings.

## Method Summary
The study conducted two survey experiments comparing LLM chatbots (Claude 3.5/3.7 via AWS Bedrock API) against human video persuasion. Participants were recruited via Prolific panel and Meta ads, randomized to conditions, and exposed to treatments measuring immediate and 5-week attitude change. The experiments implemented Zaller's Receive-Accept Model framework, distinguishing between exposure costs and persuasion effectiveness. Treatment effects were analyzed using linear regression with pre-treatment covariates, while cost-effectiveness was estimated by combining survey effects with real-world engagement data from Meta advertising campaigns.

## Key Results
- LLM chatbots achieved similar persuasion effects (~13 percentage points) as human video persuasion in forced-exposure experiments
- Real-world LLM persuasion costs $48-$74 per persuaded voter compared to $100 for traditional methods when accounting for exposure costs
- Current LLM persuasion faces significant scalability constraints: 59% zero-engagement conversations in Meta ads and limited audience pools (~110K US users on Prolific)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world persuasion impact is bounded by a two-step process: exposure (receive) and persuasion conditional on exposure (accept).
- Mechanism: Laboratory experiments measure only the "accept" step by forcing exposure, which systematically overestimates real-world effectiveness. The paper explicitly adopts Zaller's (1992) framework where voters must first receive a message before accepting it.
- Core assumption: Assumption: Exposure costs in the wild meaningfully differ between LLM-based and traditional campaign methods.
- Evidence anchors:
  - [abstract] "taking into account both the 'receive' and 'accept' steps in the persuasion process (Zaller 1992)"
  - [section] "political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure"
  - [corpus] Related work on persuasion effectiveness (FMR=0.62 meta-analysis) similarly notes inconsistent findings when comparing lab to real-world settings.
- Break condition: If exposure costs converge (e.g., chatbots become embedded in high-traffic platforms), this bottleneck disappears.

### Mechanism 2
- Claim: Extended conversational interactions enable persuasion through personalization, active listening, and dynamic argument adaptation.
- Mechanism: The chatbot prompts explicitly implement persuasion techniques from canvassing research: 8th-grade reading level, back-and-forth engagement, vulnerability-building, compassion signaling, and counter-argument anticipation. Median conversations involved 4-5 turns.
- Core assumption: Assumption: Conversational depth causally contributes to persuasion; single-message studies may underestimate effects.
- Evidence anchors:
  - [section] "persuasion research often finds that extended conversations are particularly persuasive"
  - [section] "LLMs engaging in conversation might be substantially more persuasive than single-message interactions as they can promote more active engagement, dynamically respond to voter concerns"
  - [corpus] "Persuade Me if You Can" framework (FMR=0.53) emphasizes conversational evaluation but lacks direct human benchmarks.
- Break condition: If shorter interactions prove equally persuasive, the conversational investment is unnecessary overhead.

### Mechanism 3
- Claim: Per-person persuasion effectiveness of LLMs matches human campaign ads, but scalability is constrained by audience pool limits and low voluntary engagement rates.
- Mechanism: Forced-exposure experiments show ~13 percentage point immediate effects for both AI and human conditions. However, real-world Meta ads produced 59% zero-engagement conversations, and Prolific's US pool is ~110K users.
- Core assumption: Assumption: Current engagement patterns persist; users don't develop stronger chatbot relationships over time.
- Evidence anchors:
  - [abstract] "LLM-based persuasion costs between $48-$74 per persuaded voter compared to $100 for traditional campaign methods"
  - [section] "it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion"
  - [corpus] Weak corpus signal—related papers focus on per-message effectiveness, not exposure scalability.
- Break condition: If LLMs achieve platform-level integration (e.g., default assistants), exposure costs drop toward zero.

## Foundational Learning

- Concept: Zaller's Receive-Accept Model (1992)
  - Why needed here: This is the paper's core theoretical framework; understanding why exposure and acceptance are separate steps is essential for interpreting the cost-simulation results.
  - Quick check question: If an LLM is highly persuasive in a lab setting but no one voluntarily talks to it, what is the real-world effect?

- Concept: External Validity of Survey Experiments
  - Why needed here: The paper estimates real-world human persuasion at 1% of survey-experiment effects based on expert elicitation; understanding this adjustment is critical for the cost model.
  - Quick check question: Why might forced video-watching in a survey overstate the effect of a YouTube ad people can skip?

- Concept: Perceived vs. Actual Persuasiveness
  - Why needed here: The paper critiques industry evaluations that use perceived persuasiveness ratings rather than measured attitude change.
  - Quick check question: If raters judge a message "highly persuasive," does that predict actual opinion change? What does O'Keefe (2018) say?

## Architecture Onboarding

- Component map:
  - Chatbot backend (Claude 3.5/3.7 via AWS Bedrock API) -> Frontend chat interface (JavaScript in Qualtrics via SMARTRIQS) -> Recruitment channels (Prolific panel, Meta ads) -> Measurement (pre/post/follow-up attitude surveys) -> Cost-tracking (ad spend, API inference, participant payments)

- Critical path:
  1. Recruit participants → 2. Randomize to condition → 3. Deliver treatment (video or chat) → 4. Measure immediate attitudes → 5. Follow-up at 5 weeks → 6. Estimate treatment effects with covariate adjustment

- Design tradeoffs:
  - Forced exposure (clean causal identification) vs. voluntary engagement (external validity)
  - Single-topic depth vs. multi-topic generalization
  - Expert-elicited deflators vs. direct real-world measurement

- Failure signatures:
  - High attrition between treatment and follow-up (actual: ~35% in Study 1)
  - Zero-turn conversations in voluntary settings (59% in Meta unpaid ads)
  - No persistent effects at 5-week follow-up
  - Covariate imbalance across conditions (checked and not found)

- First 3 experiments:
  1. Replicate Study 1 with a platform-integrated chatbot (e.g., within a social app) to test whether organic exposure increases engagement rates.
  2. Test whether shorter single-message LLM outputs achieve similar effects as multi-turn conversations to identify the marginal value of conversation depth.
  3. Run a field experiment delivering LLM persuasion via text message to a voter file, measuring both engagement costs and actual voting behavior (not just attitudes).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does repeated exposure and long-term relationship-building with chatbots affect persuasive effectiveness compared to single-conversation interactions?
- Basis in paper: [explicit] "We only studied the persuasive impact of a single chatbot conversation. Chatbots will very likely become more persuasive over time as users come to trust them and form parasocial relationships with them. However, the costs of such long-term use would also be significant."
- Why unresolved: The study design limited interactions to single conversations; longitudinal persuasive effects from sustained chatbot relationships remain untested.
- What evidence would resolve it: A field experiment tracking attitude change across multiple chatbot interactions over weeks or months, comparing against single-interaction effects.

### Open Question 2
- Question: What is the real-world treatment effect of LLM persuasion compared to forced-exposure survey experimental estimates?
- Basis in paper: [explicit] "We assume... the real-world efficacy of our chatbot would be 100% that of our survey experiment. If the subjects in our survey experiment are more persuadable or more willing to engage with the chatbot than the typical American (which we find to be the case when comparing the transcripts of chats collected via Prolific compared to Meta), we could be overstating the efficacy of chatbot-based persuasion. Future research should consider better real-world estimates of these treatment effects."
- Why unresolved: Forced-exposure survey settings may overstate real-world effectiveness; Prolific participants appear more engaged than typical Meta-recruited users.
- What evidence would resolve it: Field experiments measuring actual attitude change from organic LLM persuasion encounters outside survey contexts.

### Open Question 3
- Question: Are non-chatbot forms of AI persuasion (e.g., social media content flooding) more cost-effective or scalable than chatbot-based persuasion?
- Basis in paper: [explicit] "We only study the persuasive impact of chatbots... However, other forms of AI persuasion, such as flooding social media, may prove to be more cost effective."
- Why unresolved: The study design focused exclusively on chatbot interactions; alternative AI persuasion modalities were not evaluated.
- What evidence would resolve it: Comparative experiments testing cost-per-persuaded-voter across different AI persuasion methods (chatbots, social media posts, generated articles, etc.).

### Open Question 4
- Question: How generalizable are these persuasion findings across a broader range of political issues?
- Basis in paper: [explicit] "We only studied three political issues. It is possible that on other issues or with different prompts, the AI may perform quite differently."
- Why unresolved: Only immigration, transgender rights, and minimum wage were tested; persuasion dynamics may vary on issues with different salience, complexity, or partisan alignment.
- What evidence would resolve it: Replication across additional political domains spanning different issue types and partisan valences.

## Limitations

- The cost-effectiveness analysis relies on expert-elicited deflators rather than empirical real-world measurement of traditional persuasion effectiveness
- The study assumes current low engagement patterns with chatbots will persist, potentially underestimating future adoption
- The analysis excludes second-order effects like network amplification or feedback loops that could dramatically alter persuasion economics

## Confidence

- High Confidence: Per-person persuasion effectiveness comparisons between LLM and human methods under forced exposure (based on randomized controlled trials)
- Medium Confidence: Cost-per-persuaded-voter estimates for current LLM persuasion (dependent on uncertain exposure cost assumptions)
- Low Confidence: Scalability projections and long-term threat assessments (heavily dependent on speculative assumptions about future engagement patterns)

## Next Checks

1. **Empirical Deflator Validation**: Conduct a field experiment measuring actual real-world persuasion effects of traditional campaign methods (not expert-elicited estimates) to calibrate the 1% deflator used in cost calculations.

2. **Platform Integration Test**: Deploy the same LLM persuasion protocol within a popular messaging platform or social media interface to measure whether organic exposure rates differ substantially from the survey-based forced exposure.

3. **Longitudinal Engagement Study**: Track the same participants over multiple persuasion attempts to determine whether LLM persuasion effectiveness and engagement rates change with repeated exposure, testing the assumption that current low engagement is a persistent constraint.