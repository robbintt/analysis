---
ver: rpa2
title: "MARch\xE9: Fast Masked Autoregressive Image Generation with Cache-Aware Attention"
arxiv_id: '2506.12035'
source_url: https://arxiv.org/abs/2506.12035
tags:
- tokens
- attention
- generation
- march
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MARch\xE9, a training-free inference framework\
  \ that accelerates masked autoregressive (MAR) image generation by reducing redundant\
  \ attention computation. MARch\xE9 partitions tokens into active and cached sets,\
  \ reusing previously computed key/value projections for cached tokens and only recomputing\
  \ representations for contextually relevant active tokens."
---

# MARché: Fast Masked Autoregressive Image Generation with Cache-Aware Attention

## Quick Facts
- arXiv ID: 2506.12035
- Source URL: https://arxiv.org/abs/2506.12035
- Reference count: 40
- 1.7× speedup over baseline MAR with negligible FID/IS degradation

## Executive Summary
MARché introduces a training-free inference acceleration framework for masked autoregressive (MAR) image generation by exploiting temporal stability in token representations. The method partitions tokens into active (generating, caching, refreshing) and cached sets, reusing key/value projections for cached tokens while selectively recomputing only contextually relevant active tokens. This achieves up to 1.7× speedup without modifying the underlying model architecture, maintaining generation quality as measured by FID and IS metrics.

## Method Summary
MARché accelerates MAR inference through cache-aware attention that partitions tokens into active and cached sets. Active tokens (generating, caching, and refreshing tokens identified via attention scores) compute fresh Q/K/V representations, while cached tokens reuse stored KV projections. The framework uses online softmax to merge attention results from active and cached partitions without concatenation overhead. Layers 1-2 use full attention with score aggregation to identify top-K refreshing tokens, while layers 3+ employ cache-aware attention. A full KV cache refresh occurs every 3 steps to prevent drift, with the active batch size fixed at 64 tokens.

## Key Results
- 1.7× speedup over baseline MAR with minimal FID degradation (2.56→2.61 on MAR-L)
- Consistent latency improvements across MAR-B (1.6×), MAR-L (1.7×), and MAR-H (1.5×) models
- Quality maintained: FID changes within 0.05-0.32 points, IS within 0.01-0.08 points across scales
- Active set size of 64 provides optimal balance between speed and quality

## Why This Works (Mechanism)

### Mechanism 1: Temporal Locality of Token Representations
- Claim: Most token KV projections remain semantically stable across decoding steps, enabling safe reuse without recomputation.
- Core assumption: Token representations that were contextually sufficient in step t-1 remain sufficient for step t unless they receive high attention from newly generated tokens.
- Evidence anchors: Cosine similarity analysis shows values exceeding 0.95 for most tokens between consecutive steps.

### Mechanism 2: Attention-Guided Refresh Selection
- Claim: Tokens receiving high attention scores from generating tokens are contextually relevant and require KV refresh to maintain generation quality.
- Core assumption: Attention weights correlate with semantic dependency—tokens that the model "looks at" during generation need updated representations.
- Evidence anchors: FID degrades from 2.56 to 3.80 when selecting low-attention tokens instead of high-attention tokens.

### Mechanism 3: Online Softmax for Partitioned Attention
- Claim: Computing attention over active and cached sets separately then merging via online softmax yields mathematically identical results to full concatenation.
- Core assumption: Floating-point precision is sufficient to maintain numerical equivalence in practice.
- Evidence anchors: Appendix A provides formal equivalence proof showing z_i = Attn(q_i, K, V).

## Foundational Learning

- **Masked Autoregressive (MAR) Generation**
  - Why needed: MAR generates tokens in a fixed order using bidirectional attention, unlike causal autoregressive models. This bidirectional context is what makes caching possible but also necessitates refresh.
  - Quick check: How does MAR differ from standard autoregressive decoding with causal attention?

- **Key-Value Caching in Transformers**
  - Why needed: Standard LLM KV-cache assumes tokens only attend to past tokens. MAR's bidirectional attention means cached KV pairs can become stale as context changes—this is the core problem MARché addresses.
  - Quick check: Why can't standard LLM KV-caching be directly applied to MAR models?

- **Online Softmax / FlashAttention**
  - Why needed: The cache-aware attention must merge results from two partitions without materializing the full attention matrix. Online softmax algorithms enable this incremental computation.
  - Quick check: What numerical problem does the shared max term (m_i) solve in online softmax?

## Architecture Onboarding

- **Component map**: Token Categorizer -> KV Cache Store -> Attention Score Aggregator -> Cache-Aware Attention Kernel -> Refresh Scheduler

- **Critical path**: 1) Start of step t → Identify generating tokens G(t) 2) Layer 1-2: Full attention, collect scores from generating tokens 3) Aggregate scores → Select top-K refreshing tokens R(t) 4) Build active set A(t) = G(t) ∪ R(t) ∪ N(t) 5) Layers 3+: Cache-aware attention for active tokens, reuse cached KV for others 6) Update cache with newly computed (K, V) for A(t) 7) Every 3 steps: Full refresh all cached entries

- **Design tradeoffs**: Active set size (default 64): Larger → better FID, slower inference; Refresh layer selection: Layer 2 offers speed/quality balance; Full refresh frequency: Every 3 steps balances drift correction vs. overhead

- **Failure signatures**: FID spike >0.5: Likely refresh selection broken or active set too small; No speedup gain: Cache not being used; Visual artifacts accumulating: Full refresh not triggering; NaN/Inf in attention: Online softmax max computation issue

- **First 3 experiments**: 1) Cosine similarity baseline: Log KV similarity between steps without caching 2) Active set size sweep: Run inference with active set sizes [32, 64, 128] 3) Refresh layer ablation: Compare Layer 1 vs Layer 2 vs Layer 3 for token selection

## Open Questions the Paper Calls Out

- **Can MARché extend to multi-modal generation tasks?** The authors conclude the core ideas "may extend to... multi-modal generation."
- **Does the KV stability assumption hold for discrete token models?** The method relies on MAR's specific continuous token space.
- **Can speculative decoding be combined with MARché?** Related work notes speculative decoding is "orthogonal" to the masked setting.
- **Would a learned selection policy outperform the attention-score heuristic?** The method uses a fixed, training-free heuristic to identify refreshing tokens.

## Limitations

- **Numerical stability concerns**: No empirical validation provided for numerical drift over long sequences in partitioned attention computation.
- **Active set size tuning**: Fixed at 64 without sensitivity analysis across different MAR model scales or image resolutions.
- **Refresh frequency assumption**: 3-step interval presented as optimal without rigorous theoretical justification.

## Confidence

- **High Confidence**: Core mechanism of partitioning tokens and selective refresh is well-supported with consistent latency improvements across all three MAR model scales.
- **Medium Confidence**: Equivalence proof for online softmax is mathematically sound but practical numerical stability over long sequences hasn't been empirically validated.
- **Low Confidence**: Exact implementation details of custom cache-aware attention kernel and attention score aggregation method for refreshing token selection are not fully specified.

## Next Checks

1. **Numerical Stability Benchmark**: Run MARché inference for 1000+ steps on a fixed prompt, tracking FID, IS, and attention score distributions across steps. Compare against baseline MAR to quantify any quality degradation from accumulated numerical drift.

2. **Active Set Size Sensitivity Analysis**: Systematically vary active set size from 32 to 128 for MAR-B, MAR-L, and MAR-H models, measuring the full Pareto frontier of FID vs. latency. Identify whether the 64-token default is universally optimal.

3. **Refresh Frequency Stress Test**: Evaluate MARché with refresh intervals of 1, 2, 3, 5, and 10 steps on complex prompts requiring diverse visual elements. Measure FID degradation over time, latency overhead, and visual quality consistency.