---
ver: rpa2
title: Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored
  Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel
arxiv_id: '2504.14907'
source_url: https://arxiv.org/abs/2504.14907
tags:
- data
- loss
- ship
- motion
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of sea state estimation in
  autonomous vessels, where traditional methods struggle with data imbalance and feature
  redundancy in ship motion data. The proposed Temporal-Graph Contrastive Clustering
  Sea State Estimator (TGC-SSE) combines three key components: a time dimension factorization
  module to reduce data redundancy, a dynamic graph-like learning module to capture
  complex variable interactions, and a contrastive clustering loss function to manage
  class imbalance.'
---

# Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel

## Quick Facts
- **arXiv ID:** 2504.14907
- **Source URL:** https://arxiv.org/abs/2504.14907
- **Reference count:** 40
- **Primary result:** TGC-SSE achieves highest accuracy in 9/14 datasets with 20.79% improvement over second-best method for sea state estimation.

## Executive Summary
This paper addresses the challenge of sea state estimation in autonomous vessels, where traditional methods struggle with data imbalance and feature redundancy in ship motion data. The proposed Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE) combines three key components: a time dimension factorization module to reduce data redundancy, a dynamic graph-like learning module to capture complex variable interactions, and a contrastive clustering loss function to manage class imbalance. The model was evaluated on 14 public datasets, achieving the highest accuracy in 9 datasets with a 20.79% improvement over the second-best method. In sea state estimation, TGC-SSE outperformed five benchmark methods and seven deep learning models. The ablation study confirmed the effectiveness of each module in enhancing overall model performance, demonstrating that TGC-SSE significantly improves the accuracy of sea state estimation while exhibiting strong generalization capabilities.

## Method Summary
TGC-SSE processes multivariate ship motion data through a three-stage architecture. First, the Time Dimension Factorization (TDF) module slices the time series into interleaved subsequences to reduce redundancy and computational load. Second, the Dynamic Graph-like Learning (DGL) module treats sensor channels as nodes in a graph, learning interaction strengths between variables. Third, the model combines Cross-Entropy loss with a Contrastive Clustering Loss (CCL) that balances classification accuracy with cluster uniformity across classes. The architecture was evaluated using simulated ship motion data from the Marine System Simulator toolbox, with performance measured using macro-averaged precision, recall, and F1-score.

## Key Results
- TGC-SSE achieved the highest accuracy in 9 out of 14 benchmark datasets, with a 20.79% improvement over the second-best method.
- In sea state estimation, TGC-SSE outperformed five benchmark methods and seven deep learning models.
- Ablation study confirmed each module's effectiveness, with TDF reducing redundancy, DGL capturing variable interactions, and CCL managing class imbalance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Slicing the time dimension into interleaved subsequences reduces information redundancy and computational load.
- **Mechanism:** The Time Dimension Factorization (TDF) module segments the input time series $X$ into $s$ subsequences (e.g., slicing by stride). Each slice is processed by a dedicated Multilayer Perceptron (MLP) to extract local temporal dynamics before being merged back. This acts as a form of structured downsampling.
- **Core assumption:** Adjacent time steps in ship motion data possess high similarity (redundancy), meaning the sequence can be sparsely sampled without losing critical state information.
- **Evidence anchors:**
  - [Abstract]: "time dimension factorization module to reduce data redundancy"
  - [Section III.C]: "A critical examination of the temporal windows revealed inherent temporal redundancy... segments the time series data into s subsequences."
  - [Corpus]: Weak support. Neighbors focus on LSTM or physics-based models; none validate the "slicing" mechanism specifically.
- **Break condition:** If the sea state changes occur on a timescale shorter than the slicing stride, transient events may be missed.

### Mechanism 2
- **Claim:** Treating sensor channels as nodes in a dynamic graph captures inter-variable dependencies (e.g., heave vs. pitch) better than fixed grid convolutions.
- **Mechanism:** The Dynamic Graph-like Learning (DGL) module constructs edge features $e_{i,j}$ for every pair of channel features. It learns an "interaction strength" (weight) for each edge and propagates information based on this weight. This allows the model to emphasize strongly correlated degrees of freedom dynamically.
- **Core assumption:** The relationship between ship motion variables (surge, sway, yaw, etc.) is not constant and carries distinct signatures for different sea states.
- **Evidence anchors:**
  - [Abstract]: "dynamic graph-like learning module to capture complex variable interactions"
  - [Section III.D]: "constructs a graph structure to model the dynamic interconnections between channels."
  - [Corpus]: Weak support. Neighbors discuss "system identification" and "transfer functions," which align conceptually with variable interaction, but do not employ graph-learning architectures.
- **Break condition:** If sensor noise is high, the learned interaction strengths may correlate noise rather than signal, degrading feature quality.

### Mechanism 3
- **Claim:** Combining contrastive loss with cluster distribution enforcement forces the model to separate minority classes effectively.
- **Mechanism:** The Contrastive Clustering Loss (CCL) minimizes distance between same-class samples (positive pairs) and maximizes distance between different-class samples (negative pairs), weighted by class frequency. Crucially, it adds a $L_{cluster}$ term to encourage a balanced distribution of samples across clusters, preventing the model from mapping all inputs to the majority class.
- **Core assumption:** Feature embeddings for minority classes (rare sea states) exist but are collapsed into majority clusters by standard Cross-Entropy loss.
- **Evidence anchors:**
  - [Abstract]: "contrastive clustering loss function to effectively manage class imbalance"
  - [Section III.E]: "This loss function comprises four elements... encourages a uniform distribution of samples across clusters."
  - [Corpus]: No support. Neighbors do not discuss contrastive clustering for maritime data.
- **Break condition:** If the feature distributions of different sea states overlap significantly in the latent space, the contrastive margin may be impossible to enforce without overfitting.

## Foundational Learning

- **Concept:** **Time Dimension Factorization (Patching/Slicing)**
  - **Why needed here:** To understand that the model does not look at the raw stream continuously but at "chunks" to speed up processing and reduce noise.
  - **Quick check question:** If the sampling rate doubles, should the slice size $s$ change to capture the same physical duration?

- **Concept:** **Graph Neural Networks (GNN) / Dynamic Graphs**
  - **Why needed here:** To differentiate between treating data as a fixed sequence vs. a set of interacting entities (sensors).
  - **Quick check question:** In this architecture, do the "edges" represent physical connections or learned correlation strengths?

- **Concept:** **Contrastive Learning (Positive vs. Negative Pairs)**
  - **Why needed here:** To grasp how the model learns class separation without relying solely on soft-max probabilities, which bias toward majority classes.
  - **Quick check question:** How does the model define a "positive pair" in the context of ship motion windows?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Multivariate Ship Motion Data ($N \times F \times T$).
  2. **TDF Module:** Slices $T$ into $s$ chunks → MLP per chunk → Merge.
  3. **Auxiliary Extraction:** 1D-CNN runs in parallel on raw data; features are concatenated with TDF output.
  4. **DGL Module:** Nodes = Channels; learns Edge Weights → Dynamic Aggregation.
  5. **Head:** Classifier with Contrastive Clustering Loss (CCL).

- **Critical path:** The concatenation of TDF (temporal features) and DGL (correlation features). If the TDF output is too sparse, the DGL has insufficient signal to build edge weights.

- **Design tradeoffs:**
  - **Slice size ($s$):** High $s$ (small chunks) reduces redundancy but risks losing long-term trend context. The paper identifies $s=8$ as optimal.
  - **Loss weights ($\alpha$ vs $\beta$):** Balancing Cross-Entropy vs. Contrastive Loss. The paper finds 1:1 is best; deviating skews the model toward either raw accuracy or cluster balance.

- **Failure signatures:**
  - **Majority collapse:** High overall accuracy but near-zero recall for rare sea states (indicating CCL is failing or $\beta$ is too low).
  - **Training instability:** Fluctuating loss if the contrastive temperature $\tau$ or margin is inappropriate for the embedding density.

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run TGC-SSE without the DGL module on a small subset to confirm TDF features alone are insufficient for high F1-score.
  2. **Sensitivity Probe:** Vary the slice factor $s$ (e.g., 4 vs. 8 vs. 16) to observe the "redundancy vs. context" tradeoff on validation loss.
  3. **Latent Space Visualization:** Train with only Cross-Entropy vs. Combined Loss and plot t-SNE of the embeddings to verify that the Combined Loss creates tighter, separated clusters for minority classes.

## Open Questions the Paper Calls Out

- **Question:** Can the TGC-SSE architecture be effectively adapted for regression tasks to estimate continuous sea state parameters?
  - **Basis in paper:** [explicit] The conclusion states future work involves "extending the model’s functionality to include regression tasks, thereby expanding its applicability in marine condition estimation."
  - **Why unresolved:** The current model design, specifically the contrastive clustering loss and softmax classification layers, is optimized for discrete class labels (Sea States 0-6) rather than continuous variables.
  - **What evidence would resolve it:** Successful training and evaluation of a modified TGC-SSE using regression loss functions (e.g., MSE) to predict precise wave heights or periods.

- **Question:** Does integrating frequency domain features with the existing temporal modules improve the robustness of sea state estimation?
  - **Basis in paper:** [explicit] The conclusion identifies the need to study "the feasibility of extracting both time and frequency domain characteristics" to estimate conditions more accurately.
  - **Why unresolved:** While the model uses Time Dimension Factorization, it currently does not explicitly extract frequency features, which were shown to be useful in related work like SpectralSeaNet.
  - **What evidence would resolve it:** Ablation studies showing performance gains when a spectral feature extraction branch is concatenated with the temporal and graph-like modules.

- **Question:** Can adaptive hyperparameter tuning methods replace the manual sensitivity analysis required for parameters like slice size?
  - **Basis in paper:** [explicit] The discussion notes that performance is sensitive to hyperparameters and suggests "further stud[ying] adaptive methods... to enhance the model’s performance in various datasets."
  - **Why unresolved:** The current paper relies on manual grid search to determine optimal static values (e.g., slice=8) for specific datasets, which is computationally inefficient.
  - **What evidence would resolve it:** Implementation of a meta-learning or auto-tuning framework that dynamically adjusts slice size and learning rate while maintaining high accuracy.

## Limitations
- Core architectural dimensions (MLP hidden sizes, CNN kernel sizes, embedding dimensions) are unspecified, preventing exact replication.
- The temperature parameter τ in the contrastive loss is not reported, yet it critically affects gradient stability.
- Number of DGL iterations n is mentioned in the algorithm but omitted from the main hyperparameter table.

## Confidence
- **High** confidence in the conceptual novelty of combining TDF + DGL + CCL for SSE.
- **Medium** confidence in the quantitative results (9/14 datasets best, 20.79% improvement) due to potential unreported architectural tuning.
- **Low** confidence in the exact implementation details necessary for direct reproduction.

## Next Checks
1. **Sanity Check:** Run TGC-SSE without DGL on a small subset to confirm TDF features alone are insufficient for high F1-score.
2. **Sensitivity Probe:** Vary slice factor s (e.g., 4 vs. 8 vs. 16) to observe the "redundancy vs. context" tradeoff on validation loss.
3. **Latent Space Visualization:** Train with only Cross-Entropy vs. Combined Loss and plot t-SNE of embeddings to verify that Combined Loss creates tighter, separated clusters for minority classes.