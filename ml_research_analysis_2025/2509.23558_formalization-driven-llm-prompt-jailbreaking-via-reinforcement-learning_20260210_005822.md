---
ver: rpa2
title: Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning
arxiv_id: '2509.23558'
source_url: https://arxiv.org/abs/2509.23558
tags:
- attack
- jailbreak
- llms
- malicious
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PASS, a novel jailbreaking attack framework
  that uses reinforcement learning and formalization of prompts to bypass LLM alignment
  defenses. PASS decomposes malicious queries into atomic formalization steps, then
  uses an RL agent to dynamically combine them into stealthy, diverse attack prompts.
---

# Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.23558
- Source URL: https://arxiv.org/abs/2509.23558
- Reference count: 27
- Key outcome: PASS achieves 99.03% attack success rate on Deepseek-V3, outperforming baselines like ArtPrompt, FlipAttack, PAIR, DRA, and SATA

## Executive Summary
This paper introduces PASS, a novel jailbreaking attack framework that leverages reinforcement learning and prompt formalization to bypass LLM alignment defenses. The approach decomposes malicious queries into atomic formalization steps and uses an RL agent to dynamically combine them into stealthy, diverse attack prompts. A GraphRAG module stores and retrieves successful attack patterns for continuous learning. Experiments on AdvBench and JailbreakBench datasets demonstrate significantly higher success rates compared to existing methods, with PASS achieving 99.03% success on Deepseek-V3. The paper argues that PASS exploits fundamental limitations in current alignment mechanisms, particularly their inability to detect novel, disguised inputs that evade embedding space boundaries.

## Method Summary
PASS employs a two-phase approach: first, it formalizes malicious queries into atomic components representing different attack strategies; second, it uses a reinforcement learning agent to dynamically combine these components into effective jailbreaking prompts. The framework incorporates a GraphRAG module that stores successful attack patterns and retrieves relevant strategies based on current attack contexts. This allows PASS to learn from past successes and adapt to different LLMs and defense mechanisms. The RL agent is trained to optimize for attack success while maintaining stealthiness, avoiding obvious trigger words or patterns that would be easily detected by alignment filters.

## Key Results
- PASS achieves 99.03% attack success rate on Deepseek-V3, significantly outperforming baseline methods
- The framework demonstrates strong performance across multiple LLMs including Qwen2.5, GPT-4o, and Claude-3.5-Sonnet
- GraphRAG module enables continuous learning and adaptation to new defense mechanisms

## Why This Works (Mechanism)
PASS works by exploiting the gap between how alignment defenses are trained and how real-world attacks can be constructed. Traditional defenses rely on detecting known malicious patterns or identifying inputs that fall outside learned embedding space boundaries. PASS circumvents these approaches by using formalization to break down attacks into components that individually appear benign, then uses RL to find combinations that achieve the desired malicious outcome while remaining stealthy. The GraphRAG component allows the system to learn from successful attacks and adapt to new defensive strategies, creating a dynamic adversarial relationship where PASS continuously evolves to bypass emerging protections.

## Foundational Learning
- Reinforcement Learning for dynamic prompt generation: Needed to adaptively combine atomic formalization steps; Quick check: Verify RL reward function captures both attack success and stealth requirements
- GraphRAG for pattern storage and retrieval: Needed to enable continuous learning from successful attacks; Quick check: Confirm GraphRAG can effectively retrieve relevant patterns for novel attack contexts
- Formalization of malicious queries: Needed to decompose complex attacks into manageable atomic components; Quick check: Validate that atomic components remain undetected by alignment mechanisms

## Architecture Onboarding

Component Map: Malicious Query -> Formalization Engine -> RL Agent -> GraphRAG Module -> Attack Prompt -> LLM

Critical Path: The RL agent is the critical component, as it dynamically combines formalized atomic steps into effective attack prompts. Its performance directly determines the success rate of the entire system.

Design Tradeoffs: The framework balances attack effectiveness against stealthiness through the RL reward function. Higher success rates may require less stealthy approaches, while maximum stealth may reduce overall effectiveness. The GraphRAG module adds computational overhead but enables continuous learning and adaptation.

Failure Signatures: Common failure modes include:
- RL agent getting stuck in local optima, producing repetitive or ineffective attack patterns
- GraphRAG module failing to retrieve relevant patterns for novel attack contexts
- Formalization engine producing components that are still detectable by advanced alignment mechanisms

First Experiments:
1. Test PASS against a single LLM with known alignment defenses to establish baseline performance
2. Evaluate the contribution of the GraphRAG module by comparing performance with and without it
3. Assess the impact of different RL reward function weights on the balance between attack success and stealthiness

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different alignment mechanisms is not thoroughly addressed
- Evaluation scope is limited to specific datasets and models
- Computational overhead of the RL training process with GraphRAG modules is not discussed

## Confidence

High confidence:
- Technical description of PASS framework architecture is well-documented
- Superiority over baseline methods is supported by experimental design

Medium confidence:
- Attack success rates are reproducible given experimental setup
- Performance comparisons to baselines are valid within tested conditions

Low confidence:
- Claims about fundamental limitations of current alignment mechanisms are speculative
- Theoretical implications for LLM security are not fully substantiated

## Next Checks
1. Test PASS against a broader range of alignment defenses beyond embedding space boundary detection
2. Evaluate transferability of learned attack patterns across different LLM architectures
3. Conduct ablation studies to isolate contributions of individual components (RL agent, GraphRAG, formalization)