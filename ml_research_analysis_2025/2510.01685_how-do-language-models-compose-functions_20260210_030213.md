---
ver: rpa2
title: How Do Language Models Compose Functions?
arxiv_id: '2510.01685'
source_url: https://arxiv.org/abs/2510.01685
tags:
- rank
- reciprocal
- layer
- tasks
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) solve
  compositional tasks by computing intermediate steps or directly mapping inputs to
  outputs. It focuses on two-hop factual recall tasks expressed as g(f(x)), where
  f and g are functions like word-to-antonym or person-to-university.
---

# How Do Language Models Compose Functions?

## Quick Facts
- **arXiv ID**: 2510.01685
- **Source URL**: https://arxiv.org/abs/2510.01685
- **Reference count**: 40
- **One-line result**: Language models solve compositional tasks using both compositional and direct mechanisms, with mechanism choice predicted by embedding space geometry.

## Executive Summary
This paper investigates whether large language models (LLMs) solve compositional tasks by computing intermediate steps or by directly mapping inputs to outputs. Focusing on two-hop factual recall tasks (g(f(x))), the authors first confirm a "compositionality gap"—models often fail to solve the full composition even when they can solve each hop separately. Using logit lens analysis on residual stream activations, they identify two processing mechanisms: compositional (with intermediate representations) and direct (without detectable intermediates). The choice of mechanism correlates with embedding space geometry—tasks with linear mappings from x to g(f(x)) are more likely to use direct processing. This reveals that models use both compositional and idiomatic mechanisms, with the mechanism choice related to task representation in embedding space.

## Method Summary
The study analyzes how LLMs solve two-hop compositional tasks (g(f(x))) using three analyses: (1) measure compositionality gap, (2) classify processing as compositional vs direct using logit lens on residual streams, and (3) correlate mechanism choice with embedding space linearity. The authors use 17 tasks including antonym-translation, factual recall from WikiData/IMDb, arithmetic, and string manipulation. They employ 10-shot in-context learning with greedy decoding on multiple LLMs (Llama 3, OLMo 2, DeepSeek, GPT families). Logit lens projects intermediate residual streams through the unembedding layer to decode which variables (x, f(x), g(f(x))) are represented at which layers. Linear regression fits input-to-output mappings in embedding space to measure linearity.

## Key Results
- Models exhibit a "compositionality gap": they often fail to solve g(f(x)) even when they can solve f(x) and g(f(x)) separately.
- Two processing mechanisms identified: compositional (with intermediate representations of f(x) in residual stream) and direct (without detectable intermediates).
- Strong inverse correlation (r² = 0.53) between embedding space linearity and compositionality of processing—linear input-to-output mappings favor direct processing.
- Mechanism choice is consistent across different models and task types, suggesting it's task-dependent rather than model-dependent.

## Why This Works (Mechanism)

### Mechanism 1: Compositional Processing with Intermediate Representations
- Claim: LLMs can solve g(f(x)) by explicitly computing f(x) as an intermediate representation in the residual stream.
- Mechanism: The model passes through a "crossover" point where f(x) is represented in vocabulary space before g(f(x)) emerges in later layers. Logit lens analysis shows reciprocal rank peaks for f(x) occurring between peaks for x and g(f(x)).
- Core assumption: The detected intermediate representations are causally involved in computing the final output, not merely epiphenomenal.
- Evidence anchors:
  - [abstract] "one which solves tasks compositionally, computing f(x) along the way to computing g(f(x))"
  - [section 4.2] "we see a very clear peak signal for the intermediate variable f(x), as expected, between those for x and g(f(x))"
  - [corpus] Related work (Yang et al., 2025) finds compositional signatures consistently across their tasks; corpus evidence for causality is limited.
- Break condition: Tasks with low embedding space linearity but high intermediate variable presence (e.g., antonym-spanish) should show stronger causal effects from patching intermediate representations.

### Mechanism 2: Direct Processing Without Intermediate Representations
- Claim: LLMs can solve g(f(x)) by directly mapping x to g(f(x)) without detectable intermediate representations of f(x).
- Mechanism: The model exploits statistical regularities or learned shortcuts that bypass explicit intermediate computation. No vocabulary-space signature of f(x) appears in the residual stream during processing.
- Core assumption: The absence of logit lens signatures genuinely reflects the computational path, not just limitations in detection methods.
- Evidence anchors:
  - [abstract] "one which solves them directly, without any detectable signature of the intermediate variable f(x)"
  - [section 4.2] "for the movie-director-birthyear task, there is no decodable signal for f(x), the movie's director, before the model produces their birth year"
  - [corpus] Scaling work (arxiv 2507.07207) suggests compositional generalization improves with scale, but doesn't address direct mechanisms directly.
- Break condition: If patching interventions on supposedly "direct" examples reveal hidden causal intermediates, the mechanism classification is incomplete.

### Mechanism 3: Embedding Space Linearity as Mechanism Selector
- Claim: The geometry of embedding space predicts which processing mechanism the model will use—linear input-to-output mappings favor direct processing.
- Mechanism: When a linear transformation well-approximates the mapping from x embeddings to g(f(x)) unembeddings, the model exploits this shortcut. When no such linear mapping exists, compositional processing is more likely required.
- Core assumption: Pre-training determines which relations are captured linearly, and the model opportunistically uses whichever mechanism is available.
- Evidence anchors:
  - [abstract] "the idiomatic mechanism being dominant in cases where there exists a linear mapping from x to g(f(x)) in the embedding spaces"
  - [section 5.2] "strong inverse correlation (r² = 0.53) between the linearity of the representation and the compositionality of the processing"
  - [corpus] Hernandez et al. (2024, cited in paper) shows some subject→object relations are represented by single linear transformations.
- Break condition: If linearity is a consequence rather than cause of mechanism choice (e.g., direct processing creates linear structure), the direction of causality would reverse.

## Foundational Learning

- **Concept: Residual stream analysis**
  - Why needed here: All mechanism detection relies on projecting residual stream activations to vocabulary space.
  - Quick check question: Can you explain why the residual stream's additive structure enables logit lens decoding at any layer?

- **Concept: Compositionality gap**
  - Why needed here: The central empirical finding is that hop-level competence doesn't imply compositional competence.
  - Quick check question: If a model answers "Paris" for "France's capital" and "France" for "Eiffel Tower location," why might it still fail at "Eiffel Tower location's capital"?

- **Concept: Embedding space geometry**
  - Why needed here: Linearity of input-to-output mappings predicts mechanism choice.
  - Quick check question: How would you test whether a relation (e.g., country→capital) is linearly encoded in embedding space?

## Architecture Onboarding

- **Component map**: Input embedding layer → Transformer layers (residual streams) → Unembedding layer (logits)
- **Critical path**:
  1. Tokenize input x and in-context examples
  2. Process through layers; at each layer, residual stream contains accumulated computations
  3. Logit lens reveals which variables (x, f(x), g(f(x))) are represented at which layers
  4. Mechanism determined by presence/absence of intermediate variable signatures
  5. Linearity test: fit linear map from x embeddings to g(f(x)) unembeddings
- **Design tradeoffs**:
  - Compositional: More interpretable, potentially better generalization, but slower and requires hop competence
  - Direct: Faster, exploits shortcuts, but may fail on out-of-distribution examples
  - Logit lens vs. Patchscope: Logit lens is simpler; Patchscope may better align with model computation but requires more setup
- **Failure signatures**:
  - Compositionality gap: Model solves both hops independently but fails on composition
  - Incorrect intermediate representations: Logit lens shows wrong f(x) candidate
  - Mechanism mismatch: Attempting patching interventions on "direct" examples shows no causal effect
- **First 3 experiments**:
  1. Replicate the linearity-compositionality correlation on a new task family (e.g., person→occupation→industry) to test generalization of the finding.
  2. Run activation patching experiments (per Appendix G) comparing causal effects between compositional and direct examples to establish causality claims.
  3. Test whether fine-tuning on tasks with low embedding space linearity shifts models toward compositional processing, probing the causal direction of the linearity relationship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of a compositional processing mechanism (computing intermediate steps) versus a direct mechanism causally improve a model's ability to generalize compositionally?
- Basis in paper: [Explicit] The authors state in the Discussion: "Intuitively, we would expect there to be a relationship between the use of the mechanism and the ability to generalize... However, we do not test this intuition directly in this paper."
- Why unresolved: The study focused on accuracy and processing signatures for existing tasks but did not measure out-of-distribution generalization or perform causal interventions to link mechanism choice to generalization performance.
- What evidence would resolve it: Experiments that intervene on intermediate variables (e.g., ablating $f(x)$ representations) to test if "direct" processing leads to systematic generalization failures compared to "compositional" processing.

### Open Question 2
- Question: Do pre-training data statistics, such as the frequency of specific relations, causally determine whether a model adopts a direct or compositional processing mechanism?
- Basis in paper: [Explicit] The Discussion hypothesizes that "the primitives are those things which are well represented as a result of (pre-)training, and that compositional mechanisms are invoked to handle those things which are not sufficiently well represented."
- Why unresolved: The paper establishes a correlation between embedding space linearity and mechanism choice but does not verify if this linearity (and thus the mechanism) is caused by pre-training data exposure.
- What evidence would resolve it: Controlled training experiments where models are trained on datasets with manipulated relation frequencies to observe if high frequency induces "direct" processing and embedding linearity.

### Open Question 3
- Question: How do models with different inductive biases (e.g., reasoning models or those generating Chain-of-Thought) implement compositional functions compared to the standard autoregressive models analyzed?
- Basis in paper: [Explicit] The Limitations section states: "It is also necessary to understand how other models (e.g. larger models, reasoning models... ) implement compositional functions. Our findings reflect the tasks we happen to test..."
- Why unresolved: The mechanistic analysis (logit lens) was conducted primarily on Llama 3 (3B), while tests on reasoning models were limited to measuring the behavioral "compositionality gap" rather than internal mechanisms.
- What evidence would resolve it: Applying logit lens or patchscope analysis to the hidden states of reasoning models during their generation of intermediate reasoning tokens.

## Limitations

- The causal relationship between embedding space linearity and mechanism choice remains unproven—it's unclear whether linear structure enables direct processing or results from it.
- The logit lens analysis may miss intermediate representations that exist in non-vocabulary subspaces of the residual stream.
- The study focuses on two-hop tasks; whether findings generalize to deeper compositions is untested.

## Confidence

**High confidence**: The empirical finding of a compositionality gap across multiple LLMs and task types is robust. The correlation between embedding space linearity and processing mechanism choice (r² = 0.53) is statistically significant and reproducible.

**Medium confidence**: The characterization of "compositional" vs "direct" processing based on logit lens signatures is well-supported for the specific tasks and models studied, but may not generalize to all compositional reasoning. The claim that embedding space linearity predicts mechanism choice is supported by correlation but lacks causal evidence.

**Low confidence**: The assertion that the absence of logit lens signatures definitively proves direct processing is the weakest claim, as it relies on negative evidence and may miss non-vocabulary representations.

## Next Checks

1. **Causality validation**: Perform activation patching experiments comparing compositional and direct examples to establish whether intermediate representations are causally necessary for compositional processing, as suggested in Appendix G.

2. **Geometry-causality test**: Use fine-tuning to manipulate embedding space linearity (e.g., by training on tasks with varying linearity properties) and measure whether this shifts mechanism choice, establishing the causal direction of the linearity relationship.

3. **Cross-task generalization**: Apply the analysis pipeline to a new task family with different semantic properties (e.g., person→occupation→industry or country→currency→exchange_rate) to test whether the linearity-compositionality correlation generalizes beyond the original 17 tasks.