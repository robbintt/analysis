---
ver: rpa2
title: A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering
arxiv_id: '2508.02841'
source_url: https://arxiv.org/abs/2508.02841
tags:
- arxiv
- reasoning
- preprint
- system
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a multi-agent system (MAS) for radiology
  visual question answering (RVQA) that decomposes complex multimodal reasoning into
  three specialized agents: context understanding, multimodal reasoning, and answer
  validation. Unlike black-box MLLM approaches, the MAS retrieves relevant examples,
  predicts task type, generates image-grounded answers, and validates predictions
  to reduce hallucinations and improve interpretability.'
---

# A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering

## Quick Facts
- arXiv ID: 2508.02841
- Source URL: https://arxiv.org/abs/2508.02841
- Reference count: 40
- Primary result: MAS achieves 63.66% accuracy on ReXVQA-Hard, outperforming MedGemma (44.03%) by nearly 20 points

## Executive Summary
This paper introduces a multi-agent system (MAS) for radiology visual question answering that decomposes complex multimodal reasoning into three specialized agents: context understanding, multimodal reasoning, and answer validation. Unlike black-box MLLM approaches, the MAS retrieves relevant examples, predicts task type, generates image-grounded answers, and validates predictions to reduce hallucinations and improve interpretability. Evaluated on a challenging ReXVQA subset constructed via model disagreement filtering, the MAS achieves 63.66% accuracy, outperforming the best baseline (MedGemma at 44.03%) by nearly 20 percentage points. It also produces superior explanations, with the highest BLEU (0.1230) and ROUGE-L (0.3692) scores. An ablation study confirms each agent's contribution, and a case study illustrates how agent collaboration improves diagnostic reasoning.

## Method Summary
The MAS employs a three-agent pipeline: (1) Context Understanding Agent retrieves top-10 relevant QA examples from ReXVQA-RAG using FAISS and reranks them with MMed-Llama-3-8B, then predicts task category via weighted voting over top-5 examples; (2) Multimodal Reasoning Agent uses MedGEMMA to generate answers and explanations from X-ray images and retrieved context; (3) Answer Validation Agent estimates confidence using MMed-Llama-3-8B and revises answers if confidence falls below 0.7. The ReXVQA-Hard test set is constructed by filtering ReXVQA-Pool through five MLLMs and selecting examples incorrectly answered by at least three models.

## Key Results
- MAS achieves 63.66% accuracy on ReXVQA-Hard, outperforming MedGemma (44.03%) by nearly 20 percentage points
- MAS produces superior explanations with highest BLEU (0.1230) and ROUGE-L (0.3692) scores
- Ablation study confirms each agent's contribution, with ~9% accuracy drop when removing validation

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Guided Context Grounding
The Context Understanding Agent (CUA) uses embedding-based retrieval (FAISS) followed by an LLM reranking step to fetch top-k semantically similar QA examples. This external memory acts as a semantic prior, likely priming the downstream reasoner with correct answer patterns and reducing the probability of generating factually untethered responses.

### Mechanism 2: Multimodal Specialization via Functional Decomposition
The Multimodal Reasoning Agent (MRA) uses a specialized medical MLLM (MedGEMMA) to fuse visual (X-ray) and textual (retrieved context) inputs. By isolating this step, the system leverages a model optimized for vision-language alignment, rather than forcing a generalist agent to handle both retrieval assessment and visual diagnosis simultaneously.

### Mechanism 3: Confidence-Based Error Correction
The Answer Validation Agent (AVA) estimates a confidence score for the MRA's output. If confidence < 0.7, the system re-prompts an LLM with the question and retrieved context to generate a corrected answer. This acts as a "reviewer" mechanism, filtering out low-certainty hallucinations.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The CUA relies entirely on RAG principles to transform a zero-shot problem into a few-shot one by injecting external knowledge
  - Quick check question: How does the system handle a query where the nearest neighbors in the retrieval set have conflicting answers?

- **Concept: Multimodal Alignment (Vision-Language)**
  - Why needed here: The MRA must map pixels in a chest X-ray to semantic concepts (e.g., "consolidation") mentioned in the retrieved text
  - Quick check question: What is the risk of "visual hallucination" where the MRA describes a finding present in the retrieved text but not in the actual image?

- **Concept: Chain-of-Thought / Self-Reflection**
  - Why needed here: The AVA effectively implements a reflection step, asking "Am I sure about this?" before finalizing the output
  - Quick check question: Does the validation agent have access to the raw image, or does it rely solely on the textual reasoning traces?

## Architecture Onboarding

- **Component map:** CUA (Context) -> MRA (Reasoner) -> AVA (Validator)
- **Critical path:** The retrieval quality of the CUA dictates the upper bound of the MRA's performance; if the CUA retrieves noise, the MRA will likely hallucinate
- **Design tradeoffs:** Sequential agent calls significantly increase inference time compared to a single-pass MLLM; fixed 0.7 confidence threshold is rigid compared to dynamic or entropy-based thresholds
- **Failure signatures:** CUA predicts wrong task category, MRA's visual encoder misses subtle features, AVA over/under-corrects based on miscalibrated confidence
- **First 3 experiments:**
  1. Ablate AVA to measure raw performance delta (Table 3 suggests ~9% drop)
  2. Inject adversarial examples into ReXVQA-RAG to test MRA's robustness to conflicting context
  3. Sweep AVA confidence threshold (0.5 to 0.9) to verify if fixed 0.7 is optimal for "hard" vs. "easy" subsets

## Open Questions the Paper Calls Out
- The paper identifies the fixed confidence threshold (0.7) as a limitation that may hinder generalization and explicitly proposes exploring dynamic thresholding in future work
- The authors note that while accuracy improves, the clinical utility depends on real-time responsiveness, which may be compromised by the multi-step orchestration and multiple LLM calls

## Limitations
- Reliance on fixed 0.7 confidence threshold may not generalize across domains or difficulty levels
- Performance gains come from filtered "hard" subset, raising questions about real-world applicability on unfiltered clinical data
- Sequential three-agent pipeline significantly increases inference time compared to single-pass MLLM baselines

## Confidence
- High Confidence: Architectural description and evaluation methodology are well-specified with clear ablation studies
- Medium Confidence: Retrieval-guided context grounding reduces hallucinations, but depends on embedding similarity correlating with reasoning similarity
- Medium Confidence: Multimodal specialization is reasonable given MedGEMMA use, but visual acuity for subtle pathologies is not directly validated
- Medium Confidence: Confidence-based error correction is plausible, but fixed threshold approach may be suboptimal

## Next Checks
1. Replace fixed 0.7 threshold with data-driven calibration (e.g., based on validation set performance or entropy measures) to verify if adaptive thresholds improve generalization across difficulty levels
2. Test MAS on unfiltered RVQA dataset or different medical imaging domain (e.g., dermatology) to assess whether retrieval-and-validation approach maintains advantage without task-specific fine-tuning
3. Conduct detailed error analysis categorizing mistakes by agent (CUA retrieval failures, MRA visual misses, AVA over/under-correction) to identify whether pipeline's sequential nature creates error amplification or if certain agents are consistently weaker