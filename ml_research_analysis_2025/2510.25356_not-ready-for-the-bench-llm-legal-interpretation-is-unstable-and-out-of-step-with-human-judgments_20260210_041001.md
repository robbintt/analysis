---
ver: rpa2
title: 'Not ready for the bench: LLM legal interpretation is unstable and out of step
  with human judgments'
arxiv_id: '2510.25356'
source_url: https://arxiv.org/abs/2510.25356
tags:
- judgment
- question
- human
- legal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models (LLMs) can\
  \ reliably determine the \u2018ordinary meaning\u2019 of legal terms as part of\
  \ legal interpretation. Using 138 insurance contract scenarios, the authors query\
  \ 15 models across 9 systematically varied question formats."
---

# Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments

## Quick Facts
- arXiv ID: 2510.25386
- Source URL: https://arxiv.org/abs/2510.25386
- Reference count: 29
- Models show high sensitivity to prompt variation and only achieve moderate correlation with human judgments

## Executive Summary
This study evaluates whether large language models can reliably determine the "ordinary meaning" of legal terms in insurance contracts. Using 138 scenarios and 9 systematically varied question formats, the authors test 15 models across different sizes and training types. Results show models are highly inconsistent, with judgments varying widely across both model type and question phrasing. Only the largest instruction-tuned models achieve moderate correlation with human judgment, but this correlation is variant-dependent and often unreliable. The study concludes that current LLMs are unstable and poorly aligned with human judgment, making them unsuitable for use in legal interpretation without further development and validation.

## Method Summary
The study uses 138 insurance contract scenarios from Waldon et al. (2023), each containing a term, definition, and loss event. Nine systematic prompt variants are constructed (Yes/No, No/Yes, Negation, Agreement, AgreementWithNegation, Disagreement, DisagreementWithNegation, Options, OptionsFlipped). Fifteen models across five families (Llama, GPT, OLMo, Mistral, Gemma) are evaluated in base and instruction-tuned variants. Model judgments are operationalized as first-token probabilities for yes/no tokens, converted to a Δ score (p(COVERED) - p(NOT COVERED)). Robustness is measured by consistency across variants, and correlation is measured by R² between model Δ scores and human %COVERED judgments from 1,338 participants.

## Key Results
- Models are highly inconsistent: 2,061 of 2,070 item-model responses show variation across question variants
- Only instruction-tuned models with 70+ billion parameters achieve R² > 0.5 correlation with human judgment
- Four question variants (Disagreement, AgrWithNeg, DisagrWithNeg, Options) produce ~67% of minority judgments
- Token probabilities are biased or misaligned with human interpretation distributions

## Why This Works (Mechanism)

### Mechanism 1: Prompt Sensitivity Drives Judgment Instability
LLM interpretive judgments are highly sensitive to minor variations in question phrasing, making them unreliable for legal interpretation. Models respond to pragmatic and syntactic features of the prompt rather than the underlying legal question, treating semantically equivalent variants as meaningfully different tasks.

### Mechanism 2: Lack of Metalinguistic Grounding in Human Usage
LLMs do not synthesize human "ordinary meaning" judgments from training data in a way that correlates reliably with actual human interpretations. Models may imitate metalinguistic text (dictionaries, forums) rather than reflect aggregate human usage patterns.

### Mechanism 3: Instruction Tuning Expands Probability Range but Introduces Unpredictable Bias
Instruction tuning enables wider probability distributions but does not consistently improve alignment with human judgment. The direction and magnitude of bias changes vary unpredictably across models and prompt types.

## Foundational Learning

- **Token probability as judgment proxy**: Models operationalize judgment as p(COVERED) - p(NOT COVERED) using first-token probabilities. Quick check: If a model assigns p(yes)=0.6, p(no)=0.3, and p(other)=0.1, what is Δ for a Yes/No prompt where "yes" means COVERED?

- **Prompt robustness vs. semantic equivalence**: The study systematically varies prompts while preserving semantic content. Quick check: If a model answers "covered" for 5 of 9 question variants on the same scenario, is it robust? What threshold would you consider acceptable for legal use?

- **Correlation (R²) as alignment metric**: The paper uses R² between model probability differences and human %COVERED judgments. Quick check: The best model achieves R²=0.60 with 83% accuracy on predicting human majority judgment. Is a 1-in-6 error rate acceptable for a tool meant to inform judicial decisions?

## Architecture Onboarding

- **Component map**: Input layer (138 scenarios) -> Prompt template (9 variants) -> Model layer (15 models) -> Output layer (first-token probability extraction) -> Evaluation layer (robustness and correlation)

- **Critical path**: 1) Load scenario from Waldon et al. (2023) dataset, 2) Format with one of 9 question variant templates, 3) Extract first-token probabilities for yes/no variants, 4) Compute Δ = p(COVERED) - p(NOT COVERED), 5) Compare across variants (robustness) and against human %COVERED (correlation)

- **Design tradeoffs**: First-token probability vs. full decoded text (faster but may miss nuance), 9 controlled variants vs. LLM-generated paraphrases (more interpretable but may not cover all natural phrasings), binary judgment vs. ternary (human data includes "can't decide" but models may not reliably produce it)

- **Failure signatures**: "Stopped clock" behavior (>90% same judgment regardless of content), negative correlation (model judgments anti-correlated with human judgments), minority judgment spikes (4 variants produce ~67% of minority judgments)

- **First 3 experiments**: 1) Replicate Yes/No baseline for your target model on 10 scenarios; compute categorical agreement with Table 4 patterns, 2) Test robustness: Run all 9 variants on same 10 scenarios; count unanimous vs. split judgments, 3) Spot-check correlation: For 5 scenarios with human consensus data, plot model Δ against human %COVERED; compute R²

## Open Questions the Paper Calls Out

1. Would "reasoning" models or chain-of-thought prompting improve stability and alignment with human judgment in legal interpretation tasks? The authors excluded reasoning-enhanced models and prompting strategies that have shown promise in other domains.

2. To what extent does data contamination affect the moderate correlation observed in the largest models? No contamination checks were performed, and the best-performing models had training cutoffs after the data's 2023 release.

3. How representative are insurance contract scenarios of legal interpretation demands in broader legal practice? The 138 scenarios cover only insurance contracts; statutory interpretation, criminal law, and other domains remain untested.

## Limitations

- The binary nature of the judgment task may oversimplify human interpretive judgments, with ~9% of human responses being "can't decide"
- Reliance on first-token probabilities assumes token-level decisions capture model intent, but instruction-tuned models sometimes produce multi-token justifications
- The study uses a specific domain (insurance contracts) and question format, which may not generalize to other legal domains or more complex interpretive tasks

## Confidence

- **High Confidence**: Models show systematic sensitivity to prompt variation (different answers across 9 variants for same scenario)
- **Medium Confidence**: Instruction-tuned models with >70B parameters show moderate correlation (R² > 0.5) with human judgments
- **Low Confidence**: Models fail to capture "ordinary meaning" from training data - correlation exists but doesn't definitively prove models aren't capturing some aspects of ordinary meaning

## Next Checks

1. Replicate the 9-variant robustness test on a random sample of 20 scenarios across 3 different model families (e.g., GPT-4, Llama-3-70B, OLMo-7B). Calculate the percentage of unanimous judgments versus the paper's finding that only 9 of 2,070 item-model combinations showed full unanimity.

2. Apply the same methodology to a different legal domain (e.g., contract interpretation in commercial law or statutory interpretation) using 20-30 scenarios from a comparable dataset. Run 3 models that showed highest correlation in insurance contracts and compute R² against human judgments.

3. Focus specifically on the 4 negation variants that produced 67% of minority judgments. For 15 scenarios, manually inspect decoded outputs (temperature=0) to verify whether models correctly interpret negated questions and calculate the error rate.