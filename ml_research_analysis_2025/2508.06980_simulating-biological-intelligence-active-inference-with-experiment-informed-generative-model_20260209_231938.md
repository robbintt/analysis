---
ver: rpa2
title: 'Simulating Biological Intelligence: Active Inference with Experiment-Informed
  Generative Model'
arxiv_id: '2508.06980'
source_url: https://arxiv.org/abs/2508.06980
tags:
- active
- inference
- agents
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a biologically inspired active inference framework
  for modeling decision-making in embodied agents, using experiment-informed generative
  models based on partially observable Markov decision processes (POMDPs). The authors
  simulate decision-making processes in a Pong-like game environment, mirroring the
  DishBrain experiments that demonstrated learning in biological neuronal networks.
---

# Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model

## Quick Facts
- arXiv ID: 2508.06980
- Source URL: https://arxiv.org/abs/2508.06980
- Reference count: 18
- Primary result: Memory-based active inference (CFL) with horizon T≥4 significantly outperforms biological neuronal network controls in Pong-like decision-making tasks

## Executive Summary
This work presents a biologically inspired active inference framework for modeling decision-making in embodied agents using experiment-informed generative models based on partially observable Markov decision processes (POMDPs). The authors simulate decision-making processes in a Pong-like game environment, mirroring DishBrain experiments that demonstrated learning in biological neuronal networks. They compare three active inference algorithms—classical one-step planning, dynamic programming with expected free energy, and counterfactual learning with varying memory horizons. Results show that counterfactual learning agents with memory horizons of 4 or higher significantly outperform both biological controls and other AI agents across key metrics including average rally length, percentage of long rallies, and percentage of aces.

## Method Summary
The study implements three active inference algorithms in a 2432-state Pong environment where ball position and paddle position are fully observable. AIF-1 computes expected free energy for one-step policies and selects actions via softmax. DPEFE uses recursive backward computation of expected free energy over planning horizons. CFL learns a state-action mapping via updates: CL ← CL + t⟨(1 - 2Γ(t))⟨u⊗s⟩⟩, sampling actions from P(u|s) = σ(ln(CL · s)), with Γ_prior=0.55 decreasing toward zero as goals are achieved. All agents use informed transition dynamics B derived from environment physics, and performance is evaluated over 100 random seeds across 70 episodes per 20-minute trial.

## Key Results
- CFL agents with memory horizons T≥4 significantly outperform both biological neuronal cultures and other AI agents across all game metrics
- Memory-based decision-making proves more effective than planning-based approaches in this environment
- Analysis reveals decreasing risk and entropy in decision-making parameters, indicating increasing confidence and goal-directed learning
- Performance scales with memory horizon up to T=4, plateauing thereafter

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Learning with Risk-Weighted State-Action Mapping
Agents learn by updating a state-action mapping (CL) using a time-varying risk parameter (Γ), rather than computing expected free energy at each step. The CL mapping is updated via: CL ← CL + t⟨(1 - 2Γ(t))⟨u⊗s⟩⟩, where actions are sampled from P(u|s) = σ(ln(CL · s)). As Γ decreases (risk reduces), the agent shifts from exploration to exploitation. The memory horizon T determines how many past state-action pairs inform the current update.

### Mechanism 2: Memory Horizon Enables Trajectory Consolidation
Performance scales with memory horizon because longer horizons allow agents to consolidate multi-step state-action sequences into coherent policies. CFL-T agents store the last T state-action pairs. At each step, they compute the average ⟨u⊗s⟩ over this window before updating CL. Longer windows smooth noise and capture longer-range dependencies in the Pong environment.

### Mechanism 3: Entropy Minimization Reflects Confidence Accumulation
Normalized total entropy (NTE) of learned parameters decreases as agents accumulate confident beliefs about state-action relationships. NTE is defined as TE(Z) = Σ H(X) over distributions in parameter Z, then normalized. Decreasing NTE in CL indicates the probability distributions are becoming more peaked (less uncertain) about which action to take in each state—evidence of goal-directed learning.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: The entire generative model is built on POMDP structure (states S, observations O, actions U, transitions B, likelihood A, prior preferences C). Understanding tuple notation and categorical distributions is essential for implementing the algorithms. Quick check: Can you explain why the likelihood mapping A is set to identity in this paper, and what that implies about observability?

- **Active Inference & Expected Free Energy (EFE)**: AIF-1 and DPEFE algorithms directly minimize EFE (Equation 5). The CFL method can be derived from the same free energy minimization principle (per Isomura & Friston 2020). Quick check: What are the two terms in EFE (Equation 5), and which one encodes goal-directed vs. uncertainty-seeking behavior?

- **Kronecker Product & Softmax Action Selection**: CL update uses u⊗s (Kronecker product) to form state-action pairs. Action sampling uses softmax over ln(CL · s). Quick check: Why use ln(CL · s) as logits rather than CL · s directly?

## Architecture Onboarding

- **Component map**: Environment (Pong simulator) → observations (ball-x, ball-y, paddle-y) → Generative Model (POMDP: 2432 states, 3 actions) → state inference (identity A → fully observable) → Decision Module: AIF-1 computes EFE for 1-step policies → softmax select OR DPEFE uses recursive EFE backward from T → action distribution OR CFL updates CL via memory buffer (size T) → sample from CL → action → Environment (execute action, get next observation)

- **Critical path**: 1) Initialize POMDP with informed B (transition dynamics) from environment physics 2) For CFL: Initialize Γ=0.55, CL uniform, create memory buffer of size T 3) Each timestep: observe state → sample action from CL → execute → store (s,a) in buffer → update CL if buffer full → update Γ if goal reached 4) Log NTE of parameters for explainability

- **Design tradeoffs**: Memory vs. computation: CFL trades memory (buffer size T) for avoiding expensive planning; DPEFE trades computation (recursive EFE) for memory efficiency. Assumption: CFL requires environment to have learnable state-action regularities; fails in highly stochastic environments where past trajectories mislead. Biological plausibility: CFL-4+ outperforms but "memory horizon of 4 may not be biologically plausible" (Section 3.1)—trade-off between engineering performance and biological modeling fidelity

- **Failure signatures**: AIF-1 shows no improvement over time (Figure 3, bottom row): one-step planning insufficient for Pong dynamics. DPEFE with T>10 shows performance decline: "over-planning" accumulates estimation errors. CFL with T=1-2 matches biological cultures but doesn't exceed them: insufficient memory for trajectory prediction. Γ stuck >0.5: agent never transitions from exploration to exploitation (check goal-state detection)

- **First 3 experiments**: 1) Reproduce CFL-4 vs. biological baseline: Run CFL-T for T∈{1,2,3,4,8,16} with 100 random seeds, compare rally length and ace % against published MCC/HCC values. Verify T≥4 threshold. 2) Ablate B matrix informedness: Replace informed transition dynamics B with uniform priors; test if CFL still learns (tests dependency on environment knowledge). 3) Non-stationary environment test: After 10 minutes of training, invert ball velocity mapping; measure adaptation speed of CFL vs. DPEFE (tests robustness of learned CL vs. online planning)

## Open Questions the Paper Calls Out

### Open Question 1
Does the learning observed in biological neuronal networks (such as DishBrain) rely on sophisticated memory mechanisms similar to the CFL agents, or is it driven primarily by rapid adaptive restructuring? The Discussion explicitly calls for "determining whether sophisticated memory, as modelled, is essential for biological learning, or if rapid adaptive restructuring alone underpins the observed behaviours." While the simulations show memory horizons ≥4 yield superior performance, the authors note that such long-term memory capabilities have not yet been demonstrated in biological cultures like DishBrain.

### Open Question 2
Can the generative model's structure (e.g., state space dimensions) be learned autonomously by the agent rather than being hard-coded based on experimental parameters? The Methods section notes that "Algorithms for self-learning the structure of generative models are a promising direction to pursue," acknowledging the current reliance on manual definition. This study utilized a fixed state space of 2432 states derived directly from the physical setup, leaving the automation of model discovery as an unaddressed challenge.

### Open Question 3
How does the assumption of an identity likelihood mapping (full observability) impact the robustness of these active inference agents when applied to environments with significant sensory noise? The Methods section states the authors assume the likelihood mapping A is an "identity distribution" and observations are "fully observable," acknowledging the problem is treated as "control-oriented" rather than a perception problem. Biological systems must operate under partial observability and noise; the reliance on perfect state information may limit the framework's applicability to more realistic, uncertain environments.

## Limitations
- The paper assumes identity likelihood mapping (A = I) without validating whether this simplification adequately captures the DishBrain experimental setup
- Memory horizon T=4+ outperforms biological controls, yet the authors acknowledge this may lack biological plausibility
- The relationship between decreasing risk parameter Γ and goal-directed learning requires further validation
- Transition dynamics B are "informed" but implementation details are unspecified

## Confidence
- **High confidence**: CFL algorithm outperforms AIF-1 and DPEFE in Pong environment; memory horizon positively correlates with performance up to T=4; entropy reduction in CL parameters correlates with behavioral improvement
- **Medium confidence**: The claim that CFL-4 matches or exceeds biological neuronal network performance (HCC/MCC cultures); the specific T=4 threshold for biological plausibility; the mechanism by which Γ dynamics enable learning
- **Low confidence**: Generalization of results to other environments; biological interpretability of T=4+ performance; sensitivity to POMDP parameterization

## Next Checks
1. **Biological plausibility validation**: Compare CFL-T performance across T∈{1,2,3,4,8,16} against published DishBrain metrics with statistical significance testing
2. **Robustness to environmental knowledge**: Replace informed transition dynamics B with uniform priors and measure performance degradation in CFL vs. DPEFE
3. **Adaptation dynamics test**: Introduce non-stationarity (e.g., invert ball velocity after 10 minutes) and measure learning/recovery rates across algorithms