---
ver: rpa2
title: 'GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals'
arxiv_id: '2503.08015'
source_url: https://arxiv.org/abs/2503.08015
tags:
- data
- performance
- dataset
- loss
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPT-PPG, a GPT-based foundation model for
  photoplethysmography (PPG) signals. The authors adapt the standard GPT architecture
  to handle continuous PPG data by using a logit-Laplace distribution loss and a linear
  layer for signal modeling.
---

# GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals

## Quick Facts
- arXiv ID: 2503.08015
- Source URL: https://arxiv.org/abs/2503.08015
- Authors: Zhaoliang Chen, Cheng Ding, Saurabh Kataria, Runze Yan, Minxiao Wang, Randall Lee, Xiao Hu
- Reference count: 0
- Primary result: GPT-based foundation model for PPG signals achieving state-of-the-art AF detection (F1 up to 0.847) and competitive performance on HR, RR, and BP estimation

## Executive Summary
This paper introduces GPT-PPG, a foundation model that adapts the GPT architecture to continuous photoplethysmography (PPG) signals. The authors address key challenges in applying transformer models to physiological signals by introducing a logit-Laplace distribution loss to prevent model collapse on normalized data, and a mixed-objective fine-tuning framework that combines task-specific and signal modeling losses. Pre-trained on over 200 million 30-second PPG samples, the model demonstrates strong performance across multiple downstream tasks including atrial fibrillation detection, heart rate estimation, respiration rate estimation, and blood pressure estimation, while also showing impressive generative capabilities for signal denoising without additional fine-tuning.

## Method Summary
GPT-PPG adapts the standard GPT architecture for continuous PPG signals by using 1-second patches (40 samples at 40Hz) as tokens. The model is pre-trained with next-patch prediction using a logit-Laplace distribution loss that prevents collapse to mean prediction. Fine-tuning employs a mixed-objective framework combining task-specific loss and signal modeling loss with annealed weight. The architecture includes linear patch embedding, rotary positional embeddings, causal attention, and task-specific prediction heads. Extensions include bidirectional feature extraction for improved HR estimation and likelihood-informed predictions for parameter-efficient fine-tuning.

## Key Results
- Achieves state-of-the-art AF detection with F1 score up to 0.847
- Heart rate estimation with MAE down to 4.98 bpm
- Respiration rate estimation with MAE down to 0.93 breaths/min
- Blood pressure estimation with MAE down to 6.78 mmHg
- Demonstrates strong generative capabilities for signal denoising without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The logit-Laplace distribution loss prevents model collapse that occurs with MSE loss on normalized PPG signals.
- **Mechanism:** Min-max normalization constrains PPG to [0,1], but MSE loss has support over the entire real line, creating distribution mismatch. The logit-Laplace distribution restricts support to (0,1) and its NLL minimization is equivalent to L1 distance, preventing the "predict the mean ~0.5" collapse mode.
- **Core assumption:** Good-quality normalized PPG signals are approximately symmetric about their mean, making mean-prediction an easy local minimum for MSE.
- **Evidence anchors:**
  - [section 2.2.2]: "our initial experiments with MSE led to model collapse, where the model predicts the mean value of the normalized signals regardless of the input sequence, which is a non-informative constant around 0.5"
  - [section 2.2.2]: "minimizing the negative log likelihood of a Laplace distribution is equivalent to minimizing L1 distance loss"
  - [corpus]: Weak direct validation; SIGMA-PPG notes that "Standard masked modeling often yields trivial solutions" but in a different architectural context.

### Mechanism 2
- **Claim:** Mixed-objective fine-tuning (task loss + signal modeling loss) improves downstream performance by aligning the model to the downstream data distribution.
- **Mechanism:** The signal modeling loss (logit-Laplace NLL) forces the GPT backbone to learn the distribution of downstream PPG signals, which may differ from pre-training data. This improves patch-level representations before task-specific prediction.
- **Core assumption:** Downstream datasets have different PPG distributions than pre-training data, and representation quality depends on distributional alignment.
- **Evidence anchors:**
  - [section 2.3]: "Empirically, we found that when performing full-parameter fine-tuning, using the combined loss function is significantly better than using objective loss alone"
  - [figure 2 + section 3.2]: HR estimation datasets (WESAD, DaLiA, IEEE) show higher signal modeling loss and worse performance when GPT is frozen, indicating distribution mismatch
  - [corpus]: No direct validation in neighbor papers for this specific mixed-objective approach.

### Mechanism 3
- **Claim:** The fallback method enables parameter-efficient fine-tuning by using likelihood as a confidence signal.
- **Mechanism:** GPT's autoregressive training yields exact sequence likelihood p(x). When L(x) is high, rely on model prediction P(x); when low, fall back to a learned baseline y_fallback. This requires only one extra parameter for regression tasks.
- **Core assumption:** Low likelihood indicates out-of-distribution inputs where the model's predictions are unreliable.
- **Evidence anchors:**
  - [section 2.4.2]: "A unique advantage of GPT models is that we naturally obtain a sequence level likelihood. Such information is not easily available for models that aim to perform mask reconstruction like BERT"
  - [table 6]: Fallback method shows 13-20% improvement on HR datasets when GPT is frozen, but marginal or negative improvement on well-aligned datasets
  - [corpus]: No direct validation; neighboring papers do not address likelihood-informed prediction.

## Foundational Learning

- **Concept:** Autoregressive next-token prediction with causal attention
  - **Why needed here:** GPT-PPG predicts each 1-second patch based on preceding patches; understanding this sequential dependency is critical for debugging reconstruction and generation behavior.
  - **Quick check question:** Can you explain why the model predicts patch i using patches 1 through i-1, but not patches i+1 through 30?

- **Concept:** Distribution matching via negative log-likelihood
  - **Why needed here:** The logit-Laplace loss is derived from maximum likelihood estimation; understanding this connects the loss function to probabilistic modeling of continuous bounded signals.
  - **Quick check question:** Why does minimizing NLL of a Laplace distribution collapse to L1 loss, and why does the sigmoid transformation restrict support to (0,1)?

- **Concept:** Foundation model transfer and distribution shift
  - **Why needed here:** The paper explicitly shows performance degrades when downstream data differs from pre-training distribution (HR datasets vs. AF/BP datasets), and proposes personalization as mitigation.
  - **Quick check question:** What does a high signal modeling loss on a downstream dataset indicate, and how would you address it?

## Architecture Onboarding

- **Component map:** PPG signal → resample to 40Hz → min-max normalize to [0,1] → reshape to [30 patches × 40 samples] → linear patch embedding → learnable start token h_s → GPT decoder (RMSNorm, RoPE, causal attention) → signal modeling head (μ, b for logit-Laplace) or task prediction head (attention pooling + Gated MLP)

- **Critical path:** Pre-training: 200M+ samples, minimize logit-Laplace NLL across all 30 patches → Fine-tuning: Mixed loss L = L_task + λ × L_signal, anneal λ → 0 over training

- **Design tradeoffs:**
  - Model scale: 19M → 85M shows largest gains; 345M/1B improve further but with steep computational cost
  - Unidirectional vs. bidirectional extraction: Bidirectional (masking + next-patch prediction) improves HR estimation by 8-28% but increases compute
  - Full fine-tuning vs. frozen backbone: Frozen + fallback enables efficient deployment but underperforms on distribution-shifted data

- **Failure signatures:**
  - Model outputs constant ~0.5: Using MSE loss instead of logit-Laplace; switch to distribution-based loss
  - Frozen model performs poorly on new dataset: High distribution shift; use test-time personalization (5-10% unlabeled target data with signal modeling loss only)
  - Cross-dataset transfer fails (e.g., UCSF → Stanford): GPT-PPG has limited OOD generalization; requires fine-tuning on target domain

- **First 3 experiments:**
  1. Reproduce pre-training loss curve: Train GPT-19M on a 1M-sample subset with logit-Laplace loss; verify NLL decreases and predictions are not collapsed to mean
  2. Ablate mixed-objective weight λ: Fine-tune on Stanford AF with fixed λ ∈ {0, 0.5, 1.0} vs. annealed λ; compare F1 scores to confirm λ-annealing benefit
  3. Validate fallback mechanism: Freeze GPT-85M, train only prediction head with and without fallback on WESAD HR estimation; expect ~13% MAE reduction with fallback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pre-training on diverse, multi-source datasets impact the generalization capabilities of GPT-PPG compared to single-source pre-training?
- Basis in paper: [explicit] The authors state in Section 4.3 that "Exploring the impact of more diverse pre-training data on model generalization is a crucial next step," noting the current model was trained only on the UCSF dataset.
- Why unresolved: The study is limited by its reliance on a single, albeit large, institutional dataset, which restricts the model's robustness to varying signal distributions found in other clinical or wearable environments.
- What evidence would resolve it: Ablation studies showing downstream performance on out-of-distribution datasets after pre-training on a combined corpus of PPG data from multiple distinct sources (e.g., different hospitals and wearable devices).

### Open Question 2
- Question: Can model distillation effectively compress GPT-PPG to sizes suitable for edge devices without significant performance degradation?
- Basis in paper: [explicit] Section 4.3 lists model size as a limitation, stating, "Future work should explore the effect of model distillation to smaller sizes that can be deployed on edge devices."
- Why unresolved: While the paper explores scaling laws (19M to 1B), it does not investigate compression techniques required to run these foundation models directly on resource-constrained wearables.
- What evidence would resolve it: Benchmarks comparing the inference latency and task accuracy of a distilled "student" model against the larger "teacher" models when deployed on standard wearable hardware.

### Open Question 3
- Question: How does GPT-PPG compare against encoder-only architectures (e.g., BERT-based models) when pre-trained on the exact same dataset?
- Basis in paper: [explicit] The authors note in Section 4.3 that "a direct comparison between GPT-PPG and other architectures, such as encoder-only models, pre-trained on the same dataset was not performed."
- Why unresolved: It remains unclear if the generative, causal masking approach used here is fundamentally superior or inferior to bidirectional masked reconstruction for extracting physiological features.
- What evidence would resolve it: A controlled experiment training a BERT-style baseline on the UCSF data and comparing fine-tuning performance on the same downstream tasks (AF detection, HR estimation).

## Limitations
- Performance highly dependent on data distribution alignment between pre-training and downstream tasks
- Single institutional source (UCSF) for pre-training limits generalizability
- Several architectural hyperparameters remain underspecified, making exact reproduction challenging
- Fallback mechanism provides limited benefit when full-parameter fine-tuning is feasible

## Confidence
- **High confidence:** GPT-PPG achieves state-of-the-art performance on AF detection and competitive results on other tasks when fine-tuned; the logit-Laplace loss effectively prevents model collapse compared to MSE; the mixed-objective fine-tuning framework improves downstream performance
- **Medium confidence:** The generative capabilities (denoising, signal reconstruction) without task-specific fine-tuning; the bidirectional extension provides consistent improvements; the model demonstrates strong zero-shot adaptation through likelihood-based confidence
- **Low confidence:** Claims about generalizability across institutions without fine-tuning; the scalability benefits of larger models (345M/1B) beyond computational cost; the effectiveness of test-time personalization with only 5-10% unlabeled data

## Next Checks
1. **Distribution shift sensitivity test:** Systematically evaluate GPT-PPG performance across multiple institutional PPG datasets with varying demographic and device characteristics to quantify generalization limits and identify failure patterns

2. **Hyperparameter sensitivity analysis:** Conduct ablation studies on critical architectural choices including boundary transformation parameters, attention pooling initialization, and λ annealing schedules to establish robustness to implementation variations

3. **Computational efficiency benchmark:** Compare the practical deployment costs of full fine-tuning versus frozen+fallback approaches across different model scales on representative edge devices to validate the claimed parameter-efficient benefits