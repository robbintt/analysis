---
ver: rpa2
title: 'AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker
  Understanding'
arxiv_id: '2512.16250'
source_url: https://arxiv.org/abs/2512.16250
tags:
- speaker
- time
- arxiv
- token
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AMUSE, a benchmark for evaluating agentic
  multi-speaker understanding in multimodal models. It defines six tasks requiring
  reasoning over audio-visual dialogue, such as speaker association and temporal grounding,
  and evaluates models across zero-shot, guided, and agentic modes.
---

# AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding

## Quick Facts
- arXiv ID: 2512.16250
- Source URL: https://arxiv.org/abs/2512.16250
- Reference count: 26
- Multi-speaker audio-visual understanding remains challenging for current models, with significant gaps in agentic reasoning

## Executive Summary
AMUSE introduces a benchmark for evaluating agentic multi-speaker understanding in multimodal models. It defines six tasks requiring reasoning over audio-visual dialogue, such as speaker association and temporal grounding, and evaluates models across zero-shot, guided, and agentic modes. Current models show weak multi-speaker reasoning, especially in autonomous settings. To address this, the authors propose RAFT, a data-efficient alignment framework integrating reward optimization with selective reasoning adaptation. RAFT improves accuracy by up to 39.52% on AMUSE tasks, demonstrating its effectiveness in enhancing agentic reasoning. The work highlights the need for specialized alignment strategies for complex multi-speaker audio-visual understanding.

## Method Summary
AMUSE defines six tasks for multi-speaker audio-visual understanding and evaluates models across three modes: zero-shot, guided, and agentic. RAFT, the proposed alignment framework, combines reward optimization with selective reasoning adaptation. It uses reflective reward optimization (RRO) with intrinsic multimodal self-evaluation as a reward signal, and selective reasoning adaptation (SRA) to restrict updates to cross-modal reasoning layers. Temporal coherence regularization is added to prevent modality drift. The framework is trained on a curated dataset from AVA, VoxCeleb2, FriendsMMC, AMI, and YouTube, and evaluated on tasks including speaker identification, temporal grounding, and active speaker detection.

## Key Results
- RAFT improves accuracy by up to 39.52% on AMUSE tasks
- Selective reasoning adaptation (SRA) achieves parameter-efficient alignment, with 0.5% trainable parameters achieving comparable performance to 5% LoRA
- Models show significant performance drops in agentic mode, highlighting the need for improved autonomous reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Reflective Reward Optimization (RRO)
RRO improves multi-speaker grounding by using intrinsic perceptual consistency as a reward signal rather than external critics. The model samples K candidate responses, computes rewards based on cross-modal perceptual alignment (sync, face, speech, diarization), and applies softmax-weighted gradient updates that favor candidates with higher perceptual grounding scores. The core assumption is that self-evaluation scores correlate with actual grounding correctness. Evidence includes the abstract statement "RAFT...integrates reward optimization with intrinsic multimodal self-evaluation as reward" and section 4.3's description of the reward function. A break condition is if perceptual rewards become decoupled from task correctness.

### Mechanism 2: Selective Reasoning Adaptation (SRA)
SRA achieves parameter-efficient alignment by restricting updates to cross-modal reasoning layers only. Gradients are masked so only θ_cross (explicit cross-modal reasoning blocks) receive updates; θ_base (general perception) remains frozen. This focuses capacity on audio-visual-text reasoning pathways. The core assumption is that cross-modal reasoning layers are the bottleneck for agentic multi-speaker tasks. Evidence includes section 4's statement "restrict updates to cross-modal inference paths" and appendix C.2's description of gradient masking. A break condition is if multi-speaker errors stem from base perception failures.

### Mechanism 3: Temporal Coherence Regularization
Temporal coherence regularization prevents modality drift by enforcing synchronized embeddings across audio, visual, and text streams over time. A loss term penalizes divergence between f_a(t), f_v(t), f_t(t), f_r(t) at each timestep, plus their temporal derivatives. This stabilizes speaker identity tracking and reduces abrupt cross-modal mismatches. The core assumption is that embedding proximity correlates with correct temporal grounding. Evidence includes section 4.3's equation 4.5 describing the temporal loss and appendix C.3's addition of the temporal derivative term. A break condition is if modalities are genuinely misaligned in the input.

## Foundational Learning

- **Concept: Speaker Diarization and Active Speaker Detection**
  - Why needed here: AMUSE tasks require attributing speech to visible faces under overlap; RAFT's perceptual reward uses diarization outputs as ground truth.
  - Quick check question: Given an audio clip with overlapping speech from 3 speakers, can you explain how diarization segments would be generated and matched to face tracks?

- **Concept: Policy Gradient Methods and Reward-Weighted Regression**
  - Why needed here: RRO uses softmax-weighted log-probability updates, a variant of REINFORCE-style optimization, to align model outputs with perceptual rewards.
  - Quick check question: How does reward-weighted regression differ from PPO in its treatment of the value function and advantage estimation?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA, Adapters)**
  - Why needed here: SRA builds on adapter-style updates but restricts them to specific cross-modal layers; understanding LoRA helps contextualize the efficiency gains.
  - Quick check question: If LoRA adds low-rank updates to all linear layers, what changes when SRA restricts updates to a subset of layers?

## Architecture Onboarding

- **Component map**: Input (audio, video, text) -> Perception tools (Whisper, Pyannote, InsightFace, SyncNet) -> Metadata extraction -> Core MLLM with SRA adapters in cross-modal reasoning blocks -> RRO module (samples K responses, computes perceptual rewards, returns weighted gradients) -> Temporal coherence regularizer (operates on embeddings f_a, f_v, f_t, f_r) -> Output (structured response {plan, act, reflect})

- **Critical path**: 1) Dataset curation from AVA, VoxCeleb2, FriendsMMC, AMI, YouTube (rule-based pipeline, §B.1) 2) Training loop: compute L_align -> sample K candidates -> compute RRO weights -> apply L_temp -> update only θ_cross via SRA 3) Evaluation: Zero-shot (raw input), Guided (precomputed cues), Agentic (autonomous tool invocation)

- **Design tradeoffs**: Softmax vs linear weighting in RRO (softmax more stable but may over-share; β ∈ [0.3, 1.0] recommended); SRA adapter size (0.5% trainable parameters balances efficiency and performance); Temporal regularizer weight (α) (too high constrains flexibility; too low allows drift)

- **Failure signatures**: Speaker identity confusion in SRID/AVSA under high overlap (accuracy drops >15% at overlap >0.3); Temporal grounding drift in long clips (>40s: avg score drops to 49.7 vs 56.8 for <20s clips); Tool invocation errors in agentic mode (NSP shows lowest tool decision correctness at 69.4%); Modality ablation (removing audio or video causes -12.7 to -16.3 point drops)

- **First 3 experiments**: 1) Validate SRA parameter budget: Train Qwen2.5-Omni-7B with SRA-0.2%, 0.5%, 1.0% on AMUSE STG task; plot tIoU vs trainable parameter fraction 2) Ablate RRO temperature (β): Sweep β ∈ [0.1, 2.0] on AVDS task; measure BLEU/CIDEr and stability 3) Stress test under overlap: Stratify AVSA evaluation by speaker overlap ratio; compare Zero-shot vs Agentic vs RAFT

## Open Questions the Paper Calls Out

- Can the RAFT alignment framework be generalized to broader vision-language and embodied reasoning tasks beyond audio-visual dialogue? The current study validates RAFT only on the specific multi-speaker audio-visual tasks defined in AMUSE. Evidence would be successful application to established embodied AI or static image reasoning benchmarks.

- How can models reduce their dependence on explicit scaffolding to bridge the performance gap between guided and fully agentic evaluation modes? The paper finds that models "over-rely on guided prompts" and exhibit a steep performance drop when moving from guided to autonomous settings. Evidence would be achieving performance parity between guided and agentic modes without manual prompt engineering.

- Can intrinsic multimodal representations replace the reliance on external perception tools (e.g., Whisper, Pyannote) without degrading agentic accuracy? The agentic mode depends on calling external tools for grounding and rewards. Evidence would be high accuracy on the Speaker Temporal Grounding task in a "tool-free" setting where the model relies solely on native audio-visual encoders.

## Limitations
- Dataset availability and reproducibility: Exact download links, train/val/test splits, and sample-level annotations not provided
- Underspecified reward aggregation formula in RRO and definition of "cross-modal reasoning layers" for SRA
- Limited ablation of RRO effectiveness across different model sizes and tasks

## Confidence
- **High Confidence**: The core mechanism of RAFT (combining reward optimization with selective reasoning adaptation) is well-described and empirically validated across multiple tasks and models.
- **Medium Confidence**: The effectiveness of the temporal coherence regularizer is supported by ablation studies, but the specific aggregation of perceptual rewards in RRO is less detailed.
- **Low Confidence**: The claim that SRA achieves parameter efficiency without sacrificing performance is based on a single comparison; more comprehensive ablation is needed.

## Next Checks
1. Validate SRA parameter budget: Train Qwen2.5-Omni-7B with SRA adapters at 0.2%, 0.5%, and 1.0% trainable parameters on the AMUSE STG task. Plot tIoU vs. trainable parameter fraction to confirm the efficiency curve.

2. Ablate RRO temperature (β): Sweep β from 0.1 to 2.0 on the AVDS task. Measure BLEU/CIDEr scores and inter-seed variance to verify that the recommended range [0.3, 1.0] balances performance and stability.

3. Stress test under overlap: Stratify AVSA evaluation by speaker overlap ratio (0-0.1, 0.1-0.2, 0.2-0.3, >0.3). Compare Zero-shot, Guided, Agentic, and RAFT modes to quantify performance degradation and identify regimes where the framework struggles most.