---
ver: rpa2
title: Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?
arxiv_id: '2507.04391'
source_url: https://arxiv.org/abs/2507.04391
tags:
- reasoning
- tasks
- data
- training
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether learning mathematical problem-solving\
  \ (MPS) generalizes to broader reasoning tasks in large language models. It compares\
  \ five training strategies\u2014continual pretraining on math text, SFT on STEM\
  \ data, SFT on MPS with short/long CoT, and rule-based RL on MPS queries\u2014across\
  \ 5 MPS and 8 general reasoning benchmarks."
---

# Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?

## Quick Facts
- arXiv ID: 2507.04391
- Source URL: https://arxiv.org/abs/2507.04391
- Authors: Ruochen Zhou; Minrui Xu; Shiqi Chen; Junteng Liu; Yunqi Li; Xinxin Lin; Zhengyu Chen; Junxian He
- Reference count: 28
- One-line primary result: Long CoT training on MPS data generalizes better to general reasoning tasks than short CoT or conventional SFT.

## Executive Summary
This paper investigates whether learning mathematical problem-solving (MPS) generalizes to broader reasoning tasks in large language models. It compares five training strategies—continual pretraining on math text, SFT on STEM data, SFT on MPS with short/long CoT, and rule-based RL on MPS queries—across 5 MPS and 8 general reasoning benchmarks. Continual pretraining on math text and SFT with long CoT both improve general reasoning, with long CoT notably increasing response length and activating "long reasoning mode." Conventional SFT on short CoT hurts generalization, and non-MPS reasoning datasets show limited gains. Rule-based RL also generalizes well. Results suggest longer reasoning chains with self-reflection are promising for broad reasoning gains, while short-chain MPS fine-tuning is not.

## Method Summary
The paper evaluates five training strategies on Mistral-7B: (1) Continual pretraining on math text (OpenWebMath) followed by general SFT, (2) SFT on STEM data (WebInstruct) followed by general SFT, (3) SFT on MPS data with short CoT mixed with general SFT, (4) SFT on MPS data with long CoT on a strong instruct model, and (5) rule-based RL on MPS queries. Training mixes specialized data with general SFT to avoid catastrophic forgetting. Evaluation spans 5 MPS benchmarks (GSM8K, MATH) and 8 general reasoning benchmarks (GPQA, LogiQA, ZebraLogic, ARC-c, etc.), with average response length as a secondary metric.

## Key Results
- Training with long CoT responses for MPS samples significantly enhances generalization by extending reasoning processes into other domains.
- Models trained on diverse data (continual pretraining and fine-tuning on STEM data) generally perform better on a larger number of tasks.
- Rule-based reinforcement learning on MPS queries generalizes better than SFT on short CoT, which actually degrades performance on general reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1: Long CoT Activation Mode
- **Claim:** SFT on long, self-reflective reasoning chains for MPS tasks activates a transferable "long reasoning mode" in the model.
- **Mechanism:** Long-chain training (~1000 examples) with self-verification steps increases average response length by 3–8× on non-MPS tasks (e.g., LogiQA, GPQA), suggesting the model learns a domain-agnostic extended-reasoning behavior rather than task-specific patterns.
- **Core assumption:** The extended reasoning mode operates as a generalized cognitive strategy, not just surface-form lengthening.
- **Evidence anchors:**
  - [abstract] "Notably, training with long CoT responses for MPS samples ... significantly enhancing generalization by extending the model's reasoning processes into other domains."
  - [Section 3.3, Table 2] LIMO and s1.1 show consistent gains on general reasoning benchmarks with response lengths increasing from ~400 to ~2000–4000 tokens.
  - [corpus] Neighbor paper *Decomposing Elements of Problem Solving: What "Math" Does RL Teach?* notes that accuracy metrics alone are insufficient to assess reasoning capabilities; this aligns with the need to analyze reasoning depth beyond surface accuracy.
- **Break condition:** If long-CoT training instead teaches domain-specific templates that do not transfer, generalization gains would collapse on tasks with differing reasoning structures.

### Mechanism 2: Distributional Coverage in Training Data
- **Claim:** Training data with broader distributional overlap with target tasks supports better generalization.
- **Mechanism:** Continual pretraining on OpenWebMath and SFT on WebInstruct (STEM QA) cover more of the query embedding space than narrow MPS SFT datasets (e.g., MetaMath). Greater coverage reduces distribution shift at test time.
- **Core assumption:** Embedding-space overlap correlates with functional generalization.
- **Evidence anchors:**
  - [Section 3.2, Figure 3] PCA visualization shows OpenWebMath and WebInstruct embeddings overlap more with general reasoning benchmarks than MetaMath does.
  - [Section 3.2] "Models trained on diverse data (continual pretraining and fine-tuning on STEM data) generally perform better on a larger number of tasks."
  - [corpus] No direct corpus evidence confirms embedding-coverage→generalization; signal is weak.
- **Break condition:** If embedding overlap does not causally predict reasoning transfer, then coverage-based interventions would not reliably improve generalization.

### Mechanism 3: RL-Induced Reasoning Emergence
- **Claim:** Rule-based RL on MPS queries (with format constraints and correctness rewards) elicits emergent long reasoning behaviors that generalize.
- **Mechanism:** RL optimizes for verifiable correctness (e.g., \boxed{} format + ground-truth match) rather than imitating fixed patterns, allowing the model to discover effective multi-step strategies that transfer across domains.
- **Core assumption:** The reward signal generalizes; correctness-based shaping does not overfit to math-specific heuristics.
- **Evidence anchors:**
  - [Section 3.4, Table 3] SimpleRL and SimpleRL-Zero outperform base models on both MPS and general reasoning benchmarks (e.g., LogiQA, ARC-c).
  - [Section 2] "Recent works have shown that RL-based methods can facilitate the emergence of long reasoning chains and yield substantial improvements in mathematical problem-solving."
  - [corpus] *Advancing Reasoning in Large Language Models: Promising Methods and Approaches* notes RL as a promising direction but does not provide direct evidence for cross-domain transfer from math to general reasoning.
- **Break condition:** If RL simply amplifies math-specific shortcuts without improving core reasoning, generalization gains would not hold on out-of-distribution tasks.

## Foundational Learning
- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** The paper contrasts short vs. long CoT training; understanding CoT as a step-by-step decomposition method is essential.
  - **Quick check question:** Can you explain how CoT prompting differs from direct answer generation?
- **Concept: Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL)**
  - **Why needed here:** The paper compares SFT and rule-based RL; understanding their optimization objectives clarifies why RL may induce more transferable behaviors.
  - **Quick check question:** What is the primary objective function difference between SFT and RL in LLM post-training?
- **Concept: Distribution Shift and Embedding-Space Analysis**
  - **Why needed here:** The paper uses PCA on embeddings to argue training-test overlap matters; grasping this helps interpret generalization claims.
  - **Quick check question:** Why might low overlap between training and test distributions impair generalization?

## Architecture Onboarding
- **Component map:** Five training pipelines: (1) CPT on math text → general SFT, (2) STEM SFT → general SFT, (3) short-CoT MPS SFT mixed with general SFT, (4) long-CoT MPS SFT on strong instruct model, (5) rule-based RL on MPS queries. Evaluation spans MPS, math-related, and 8 general reasoning benchmarks.
- **Critical path:** For generalization-focused deployments, prioritize long-CoT SFT or rule-based RL on MPS queries over short-CoT SFT. Long-CoT requires fewer examples (~800–1000) but needs a strong instruct base model.
- **Design tradeoffs:** Short-CoT SFT maximizes MPS benchmark scores but harms generalization; long-CoT/RL improve generalization at the cost of longer inference and higher compute. CPT on math text improves generalization moderately but is compute-intensive.
- **Failure signatures:** Short-CoT SFT models show performance drops on 7/8 general reasoning tasks; overfitting to MPS format (e.g., GSM8K MQA decline). Non-MPS reasoning datasets (Magicoder, Magpie, OpenOrca) yield inconsistent or narrow gains.
- **First 3 experiments:**
  1. **Ablate CoT length:** Train separate SFT models on the same MPS data but with short vs. long CoT; compare generalization on LogiQA and GPQA to isolate length effects.
  2. **Probe embedding coverage:** Compute PCA or centroid distances between your training data embeddings and target benchmark embeddings; correlate with performance shifts.
  3. **Pilot RL with simple rule rewards:** Apply rule-based RL (e.g., format + correctness) on a small MPS dataset; evaluate whether response length and general reasoning scores increase compared to SFT baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training tasks, beyond mathematical problem-solving (MPS), can induce robust cross-domain generalization without relying on long reasoning chains?
- Basis in paper: [explicit] The Discussion states that "new training goals may be required to achieve generalized reasoning" after finding that existing non-MPS datasets failed to generalize well.
- Why unresolved: The preliminary study (Section 4) tested three non-MPS datasets (Magicoder, Magpie, OpenOrca) which showed limited or narrow improvements, failing to demonstrate broad generalization.
- What evidence would resolve it: Identifying and validating a novel training task (e.g., code input-output prediction as hinted in the text) that yields consistent performance gains across the 8 general reasoning benchmarks without necessitating extensive long-CoT data.

### Open Question 2
- Question: What are the precise mechanistic causes for the "long reasoning mode" activation and its transfer effects from MPS to non-MPS domains?
- Basis in paper: [explicit] Section 3.3 notes that training on long CoT "significantly increases the response length across other domains" and that this activation "appears to enable better generalization," but the exact cause is not fully isolated.
- Why unresolved: The paper observes a correlation between increased response length/self-reflection and improved accuracy (e.g., in LogiQA), but leaves the underlying representational changes or attention mechanisms as an open area.
- What evidence would resolve it: Mechanistic interpretability studies comparing attention heads or circuit activation in long-CoT models versus short-CoT models when processing non-mathematical queries.

### Open Question 3
- Question: Why does rule-based reinforcement learning on MPS queries generalize more effectively to non-MPS tasks than supervised fine-tuning on short chains?
- Basis in paper: [explicit] The Discussion explicitly encourages future research to "explore how different training approaches such as reinforcement learning... generalize better than SFT."
- Why unresolved: While empirical results show SimpleRL outperforming short-CoT SFT, the paper does not definitively explain whether the generalization stems from the rule-based rewards, the optimization landscape of RL, or the avoidance of "memorization" associated with SFT.
- What evidence would resolve it: Ablation studies comparing rule-based RL against SFT on identical data distributions to isolate the impact of the optimization algorithm versus the data content on general reasoning benchmarks.

## Limitations
- The claim that long CoT training induces a "long reasoning mode" is compelling but not fully proven—no direct ablation shows whether gains stem from response length or other long CoT features.
- The distributional coverage mechanism is weakly supported; while PCA overlap is shown, the causal link between embedding-space overlap and generalization is asserted rather than demonstrated.
- RL's role in emergent reasoning is plausible given its optimization for correctness, but the exact reward shaping details are sparse, making it hard to assess whether the observed generalization is robust or task-specific.

## Confidence
- **High**: The empirical finding that short-CoT SFT on MPS harms general reasoning generalization, and that long-CoT SFT and rule-based RL improve it, is well-supported by the reported benchmark results.
- **Medium**: The interpretation that long-CoT training activates a "long reasoning mode" is reasonable but not definitively proven; alternative explanations (e.g., increased training diversity) are not ruled out.
- **Low**: The claim that embedding-space overlap predicts generalization is weakly supported; no ablation or causal analysis is provided.

## Next Checks
1. **Ablate CoT length:** Train separate SFT models on the same MPS data but with short vs. long CoT; compare generalization on LogiQA and GPQA to isolate length effects.
2. **Probe embedding coverage:** Compute PCA or centroid distances between your training data embeddings and target benchmark embeddings; correlate with performance shifts.
3. **Pilot RL with simple rule rewards:** Apply rule-based RL (e.g., format + correctness) on a small MPS dataset; evaluate whether response length and general reasoning scores increase compared to SFT baseline.