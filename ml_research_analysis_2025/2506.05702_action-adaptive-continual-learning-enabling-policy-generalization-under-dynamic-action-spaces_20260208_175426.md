---
ver: rpa2
title: 'Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic
  Action Spaces'
arxiv_id: '2506.05702'
source_url: https://arxiv.org/abs/2506.05702
tags:
- action
- actions
- learning
- task
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual Learning with Dynamic Capabilities
  (CL-DC), a problem where agents must learn with dynamically changing action spaces,
  addressing a gap in existing continual learning frameworks. The authors propose
  Action-Adaptive Continual Learning (AACL), which decouples policies from specific
  action spaces by building an action representation space through self-supervised
  learning.
---

# Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces

## Quick Facts
- **arXiv ID:** 2506.05702
- **Source URL:** https://arxiv.org/abs/2506.05702
- **Reference count:** 40
- **One-line result:** AACL achieves 0.90-0.91 continual return vs 0.72-0.87 for baselines across dynamic action space scenarios

## Executive Summary
This paper addresses Continual Learning with Dynamic Capabilities (CL-DC), where agents must learn sequential tasks with dynamically changing discrete action spaces while maintaining environment dynamics. The authors propose Action-Adaptive Continual Learning (AACL), which decouples policies from specific actions by learning an action representation space through self-supervised inverse dynamics modeling. The framework uses asymmetric regularization—applying Elastic Weight Consolidation (EWC) only to the decoder—to preserve previously learned mappings while allowing the encoder to adapt to new capabilities. Experiments across MiniGrid, Bigfish, and Atlantis environments demonstrate AACL's superior performance in handling action space expansion, contraction, and combined scenarios.

## Method Summary
AACL learns a latent action representation space via self-supervised learning on state transitions, decoupling the policy from raw actions. The system uses an encoder-decoder architecture where the encoder maps state pairs to embeddings and the decoder maps embeddings to action probabilities. The policy outputs embeddings rather than discrete actions. When action spaces change, the decoder output layer is resized and fine-tuned with EWC regularization (applied only to decoder weights) to preserve knowledge of previous tasks. The encoder remains unconstrained to maximize plasticity. The framework balances stability and adaptability through asymmetric stability-plasticity, allowing the representation space to evolve while protecting critical decoder weights.

## Key Results
- AACL achieves 0.90-0.91 average continual return versus 0.72-0.87 for other methods across three environments
- Superior forward transfer performance (0.55-0.60) enabling better zero-shot generalization
- Effective handling of expansion, contraction, and combined action space change scenarios
- Ablation studies confirm asymmetric EWC (decoder only) and random exploration are critical components

## Why This Works (Mechanism)

### Mechanism 1: Latent Action Space Decoupling
Decoupling the policy from raw discrete actions via learned latent representation enables generalization across physical capability changes. The encoder-decoder SSL mapping creates an action embedding space where the policy learns to output embeddings rather than specific actions. This assumes action semantics remain consistent across tasks while physical capabilities change.

### Mechanism 2: Asymmetric Stability-Plasticity
Applying EWC solely to the decoder preserves previously learned mappings while allowing the encoder to adapt to new capabilities. This asymmetric approach protects the decoder's long-term memory while maximizing the encoder's plasticity for representation space refinement.

### Mechanism 3: Structural Network Evolution
Dynamically resizing the decoder output layer handles varying action set sizes without full retraining. New neurons are added for expansion and outputs are masked for contraction, enabling the agent to adapt to changing physical capabilities.

## Foundational Learning

- **Self-Supervised Learning (Inverse Dynamics Models)**: Learn what actions "mean" in terms of state changes without relying on sparse rewards to create the action embedding space. *Quick check:* Can you predict the action taken, given the current state and next state?
- **Elastic Weight Consolidation (EWC)**: Essential for mitigating catastrophic forgetting in the decoder when fine-tuning on new action spaces. *Quick check:* How does the Fisher Information Matrix estimate the importance of a specific weight?
- **Discrete Action Masking**: Handles "contraction" scenarios where the agent loses capabilities by preventing selection of invalid actions. *Quick check:* How do you ensure gradients do not flow back through masked output nodes?

## Architecture Onboarding

- **Component map:** State $S$ -> Encoder $f_\phi$ (maps $(S,S')$ to embedding $E$) -> Policy $\tilde{\pi}_\theta$ (maps $S \to E$) -> Decoder $g_\delta$ (maps $E \to$ Action Probability)
- **Critical path:**
  1. Exploration: Collect transitions $(s, a, s')$
  2. SSL Update: Train Encoder/Decoder to reconstruct $a$ from $(s, s')$
  3. RL Update: Train Policy to maximize reward by outputting embeddings that Decoder maps to successful actions
- **Design tradeoffs:**
  - Encoder Plasticity: Avoid regularizing encoder to allow representation space to accommodate new actions, trading stability for adaptability
  - Exploration Policy: Random exploration for SSL data collection (previous task policy performed worse due to bias)
- **Failure signatures:**
  - Performance Drop on Contraction: Insufficient decoder regularization disrupts latent distribution for remaining actions
  - Zero-Shot Failure: Low forward transfer indicates failed alignment of similar actions across tasks (check t-SNE)
- **First 3 experiments:**
  1. Ablation (AACL-O): Run without EWC regularization on decoder to confirm it drives memory retention
  2. Contraction Scenario: Test agent on sequence where actions are removed (7→3 actions) to verify masking mechanism
  3. Representation Visualization: Use t-SNE on latent space $E$ to verify semantically similar actions cluster together

## Open Questions the Paper Calls Out

- **Dynamic Continuous Action Spaces:** How to extend AACL to handle dynamic continuous action spaces where dimensionality and ranges change, potentially via hierarchical representations or Gaussian decoders.
- **Partial/Complete Action Space Shifts:** How to automatically detect specific changes in partial or complete action space shift scenarios, requiring better separation in representation space or novelty detection.
- **Optimal Exploration Policies:** What are the most effective exploration policies for the action representation learning stage when transitioning to new action spaces, balancing coverage with relevant data collection.

## Limitations
- Architectural ambiguity in encoder input specification (action-based vs. state-transition-based)
- Reliance on external IMPALA hyperparameters without explicit specification
- Limited generalizability demonstrated across only three environments
- Assumes action semantics remain invariant across tasks without empirical verification

## Confidence
- **High:** Core mechanism of decoupling policies from discrete actions via SSL-trained representations
- **Medium:** Asymmetric EWC application (decoder only) with heuristic regularization coefficient
- **Low:** Assumption of invariant action semantics across tasks

## Next Checks
1. Implement and test both interpretations of encoder input (action-based vs. state-transition-based) to align with paper results
2. Conduct grid search over EWC regularization coefficient λ to identify optimal values and verify robustness
3. Design experiment where action semantics are intentionally altered to measure performance degradation and validate invariant task logic assumption