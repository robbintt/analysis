---
ver: rpa2
title: Theoretical Performance Guarantees for Partial Domain Adaptation via Partial
  Optimal Transport
arxiv_id: '2506.02712'
source_url: https://arxiv.org/abs/2506.02712
tags:
- partial
- domain
- source
- adaptation
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops theoretical guarantees for partial domain adaptation
  using partial optimal transport. The authors derive two generalization bounds that
  depend on the partial Wasserstein distance between source and target feature distributions,
  and introduce explicit weighting schemes for source samples based on the optimal
  coupling matrix.
---

# Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport

## Quick Facts
- arXiv ID: 2506.02712
- Source URL: https://arxiv.org/abs/2506.02712
- Reference count: 40
- Key outcome: Introduces WARMPOT algorithm with theoretical generalization bounds for partial domain adaptation using partial optimal transport

## Executive Summary
This paper develops theoretical guarantees for partial domain adaptation using partial optimal transport. The authors derive two generalization bounds that depend on the partial Wasserstein distance between source and target feature distributions, and introduce explicit weighting schemes for source samples based on the optimal coupling matrix. These weights are theoretically motivated and aim to reduce negative transfer by down-weighting outlier classes. The work proposes WARMPOT, an algorithm that minimizes a combined objective of weighted source loss and partial Wasserstein distance. Experiments on Office-Home and ImageNet→Caltech datasets show WARMPOT achieves competitive performance with state-of-the-art methods, and that the proposed weights outperform heuristic alternatives, including improving the performance of the SOTA ARPM algorithm when integrated.

## Method Summary
WARMPOT addresses partial domain adaptation by minimizing a combined objective of weighted source classification loss and partial Wasserstein distance between source and target feature distributions. The algorithm computes sample-level weights from the optimal coupling matrix of a partial optimal transport problem, where the partial Wasserstein distance transports only a fraction of mass between domains. Source samples are inflated by $1/\beta$ where $\beta$ indicates the expected proportion of non-outlier source samples. The method uses a ResNet-50 backbone with a 256-dim hidden layer classifier, trained with SGD and entropic regularized partial OT solver (POT library, ε=7.0). Key hyperparameters are α=0.8 (target mass fraction), β=0.35 (source outlier fraction), η₁=0.125, η₂=1.75.

## Key Results
- WARMPOT achieves competitive performance with state-of-the-art methods on Office-Home and ImageNet→Caltech datasets
- The proposed sample-level weights outperform heuristic alternatives including uniform, entropy-based, and adversarial weighting schemes
- Integration of WARMPOT weights into the SOTA ARPM algorithm improves its performance
- Outlier classes receive only 6.04% of total weight despite comprising 58.41% of source samples in experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial Wasserstein distance enables principled alignment by allowing selective mass transport between source and target distributions.
- Mechanism: The partial Wasserstein distance $PW_\alpha$ transports only a fraction $\alpha$ of mass from source to target, with constraints $\Pi 1_n \leq \frac{1}{\beta}P_s$ and $\Pi^T 1_m \leq Q_t$. Source samples are inflated by $1/\beta$ where $\beta < 1$ indicates the expected proportion of non-outlier source samples. This mathematical structure naturally ignores outlier classes during alignment.
- Core assumption: The fraction $\beta$ approximates the true proportion of shared classes between domains; $\alpha$ approximates the clean target fraction.
- Evidence anchors:
  - [section 3.2, Definition 3.1]: Formal definition of partial Wasserstein with mass constraints
  - [section 4]: "The parameters $\alpha$ and $\beta$ control the alignment between the source and target distributions"
  - [corpus]: Related work "Bi-level Unbalanced Optimal Transport for PDA" similarly uses unbalanced transport, but WARMPOT provides explicit bounds rather than heuristic alignment
- Break condition: If $\alpha$ or $\beta$ are severely misspecified relative to true outlier proportions, the bound becomes loose and alignment may fail.

### Mechanism 2
- Claim: Sample-level weights derived from the optimal coupling matrix systematically downweight outlier source samples.
- Mechanism: Given optimal coupling matrix $\hat{\Pi}^*$ from $PW_\alpha(\frac{1}{\beta}P_z^f, Q_t^w)$, weights are computed as $\hat{p}_i = (\hat{\Pi}^* 1_{n_t})_i$. Samples that receive high coupling mass to target features receive high training weights. Outlier class samples, which cannot align well with target features, naturally accumulate small $\hat{p}_i$ values.
- Core assumption: The feature extractor $f$ produces representations where shared-class samples are closer in Wasserstein distance than outlier samples.
- Evidence anchors:
  - [section 3.2, Eq. 11-12]: Constructive weight definitions from coupling matrix
  - [section 5.5, Figure 1]: Empirical validation—outlier classes receive only 6.04% of total weight despite comprising 58.41% of source samples
  - [corpus]: Corpus papers use heuristic class-level weights; this work provides instance-level weights with explicit theoretical derivation
- Break condition: If the feature extractor collapses representations or produces poor geometry, coupling weights may be uninformative.

### Mechanism 3
- Claim: Joint distribution alignment via partial Wasserstein bounds both covariate and label shift.
- Mechanism: Theorem 3.3 extends the bound to joint distributions $(f(x), y)$ for source and $(f(\tilde{x}), w(\tilde{x}))$ for target, using cost function $c((x,y), (\tilde{x}, \tilde{y})) = \zeta\gamma\|f(x) - f(\tilde{x})\| + \ell(y, \tilde{y})$. This captures both feature misalignment and label prediction errors in a single transport problem.
- Core assumption: The loss function $\ell$ is a metric and $\zeta$-Lipschitz; classifier $g$ is $\gamma$-Lipschitz.
- Evidence anchors:
  - [section 3.2, Theorem 3.3]: Full joint distribution bound with explicit cost structure
  - [section 4, Eq. 19]: WARMPOT objective combines both feature ($\eta_1$) and label ($\eta_2$) distances
  - [corpus]: Weak direct corpus evidence on joint distribution PDA; this is a distinguishing feature
- Break condition: If predicted target labels $w(\tilde{x})$ are highly inaccurate early in training, joint alignment may propagate errors.

## Foundational Learning

- Concept: **Wasserstein Distance and Optimal Transport**
  - Why needed here: The entire theoretical framework builds on partial Wasserstein distance as the domain alignment measure; understanding transport plans and coupling matrices is essential to interpret the weight derivation.
  - Quick check question: Given two discrete distributions with uniform weights and cost matrix $C_{ij}$, can you write the linear program for optimal transport?

- Concept: **Generalization Bounds and PAC-Bayes**
  - Why needed here: Corollary 3.5-3.6 derive high-probability bounds on target risk; understanding how empirical bounds connect to population risk via KL divergence is required to see why the algorithm is theoretically motivated.
  - Quick check question: What role does the KL divergence term $D_{KL}(P_{W|Z,T} \| Q_W)$ play in the bound?

- Concept: **Partial Domain Adaptation Setting**
  - Why needed here: Distinguishes PDA from standard domain adaptation—target label space $\tilde{\mathcal{Y}} \subset \mathcal{Y}$ creates outlier classes that cause negative transfer if not handled.
  - Quick check question: Why does uniform weighting of source samples fail in PDA but work in standard DA?

## Architecture Onboarding

- Component map:
  Feature extractor $f$ (ResNet-50 backbone) -> Classifier $g$ (256-dim hidden layer) -> Partial OT solver -> Weight computation -> Loss aggregation

- Critical path:
  1. Forward pass through $f$ to get embeddings
  2. Compute cost matrix $C_{ij} = \eta_1\|f(x_i) - f(\tilde{x}_j)\| + \eta_2\ell(y_i, w(\tilde{x}_j))$
  3. Solve partial OT to get $\hat{\Pi}^*$ and extract weights $\hat{p}_i$
  4. Backward pass through weighted source loss + alignment term

- Design tradeoffs:
  - $\alpha$ (target mass fraction): Lower $\alpha$ ignores more target outliers but discards data; sensitivity analysis shows $\alpha > 0.1$ degrades performance on ImageNet→Caltech
  - $\beta$ (source outlier fraction): Controls source inflation $1/\beta$; variations within reasonable range have <2% accuracy impact
  - Mini-batch vs full-dataset weights: WARMPOT computes weights per batch; ARPM/BA3US compute every $n=500$ iterations on full data—trade-off between accuracy and computational cost

- Failure signatures:
  - High weight variance across iterations: Indicates unstable coupling; check $\epsilon$ regularization
  - All weights concentrated on few samples: $\beta$ may be too small or feature extractor collapsed
  - Alignment term dominates loss: $\eta_1, \eta_2$ may need scaling relative to cross-entropy

- First 3 experiments:
  1. Ablation on weighting schemes (Table 1): Replace WARMPOT weights with uniform (MPOT), entropy-based (BA3US), or adversarial (ARPM) weights to isolate contribution
  2. Weight distribution visualization (Figure 1): Plot histograms of $\hat{p}_i$ for shared vs outlier class samples to verify downweighting
  3. SOTA integration test (ARPM+our-weights): Replace ARPM's heuristic weights with WARMPOT weights to test portability across architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can uncertainty reduction and robustness terms (as in ARPM) be explicitly incorporated into the partial optimal transport theoretical framework to yield tighter bounds or more powerful PDA algorithms?
- Basis in paper: [explicit] Conclusion states: "The SOTA algorithm ARPM includes additional loss terms that aim to reduce prediction uncertainty and improve robustness. Such quantities are not explicitly present in our generalization bounds... A promising direction for future research is to explore these aspects within our theoretical framework, potentially enabling more powerful algorithms."
- Why unresolved: The current bounds only include weighted source loss and partial Wasserstein distance terms; uncertainty/robustness mechanisms from ARPM (Gu et al., 2024, Thm. 1) are not theoretically motivated within this framework.
- What evidence would resolve it: Derivation of new generalization bounds incorporating uncertainty/robustness terms, followed by an algorithm achieving improved performance over ARPM+our-weights.

### Open Question 2
- Question: Can computationally efficient approximations to exact bound minimization be improved to yield further performance gains?
- Basis in paper: [explicit] Conclusion states: "An exact minimization of our bounds is prohibitively expensive from a computational standpoint, and hence, WARMPOT relies on some approximations. Additional performance gains may be obtained by optimizing this implementation."
- Why unresolved: WARMPOT uses mini-batch partial transport with entropic regularization and fixed hyperparameters; the gap between empirical performance and theoretical bound tightness remains unquantified.
- What evidence would resolve it: Systematic comparison of different approximation strategies (e.g., varying batch sizes, regularization schedules) against bound values, demonstrating reduced approximation gap.

### Open Question 3
- Question: How can the alignment parameters α and β be automatically selected based on dataset characteristics rather than cross-validation?
- Basis in paper: [inferred] Sensitivity analysis (Appendix G) shows performance varies with αmax and β, with significant drops when αmax > 0.1 due to source outliers. The paper manually tunes these via parameter search.
- Why unresolved: The theoretical bounds provide no guidance on optimal α, β selection; the relationship between outlier proportions and optimal parameter values remains heuristic.
- What evidence would resolve it: Derivation of theoretical conditions relating α, β to source/target outlier proportions, or an adaptive scheme that estimates these parameters from data properties (e.g., class prediction entropy, transport plan marginals).

## Limitations
- Parameter sensitivity: Performance significantly degrades when αmax > 0.1, requiring careful tuning of alignment parameters
- Computational cost: Exact bound minimization is prohibitively expensive, requiring approximation via mini-batch OT
- Theoretical gap: Current bounds do not incorporate uncertainty and robustness mechanisms that improve empirical performance

## Confidence
- Theoretical framework (Section 3): High - formal generalization bounds derived with clear mathematical proofs
- WARMPOT algorithm (Section 4): High - well-specified implementation details and hyperparameter choices
- Experimental results (Section 5): Medium - competitive performance shown but limited ablation studies on hyperparameters
- Weight effectiveness (Section 5.5): High - clear evidence that proposed weights outperform alternatives

## Next Checks
1. Verify weight distribution histograms show outlier classes receiving ~6% of total weight as claimed
2. Implement ARPM+our-weights integration and confirm performance improvement over baseline ARPM
3. Conduct sensitivity analysis on α and β parameters to reproduce the performance degradation when αmax > 0.1