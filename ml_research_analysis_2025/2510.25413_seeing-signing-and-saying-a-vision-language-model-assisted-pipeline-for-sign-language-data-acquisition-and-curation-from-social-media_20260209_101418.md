---
ver: rpa2
title: 'Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for
  Sign Language Data Acquisition and Curation from Social Media'
arxiv_id: '2510.25413'
source_url: https://arxiv.org/abs/2510.25413
tags:
- sign
- language
- data
- dataset
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first automated framework for sign language
  dataset acquisition using Vision-Language Models (VLMs). The approach leverages
  VLMs to filter, annotate, and validate sign language videos from social media platforms
  like TikTok.
---

# Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media

## Quick Facts
- **arXiv ID:** 2510.25413
- **Source URL:** https://arxiv.org/abs/2510.25413
- **Reference count:** 19
- **Primary result:** Automated VLM-based pipeline creates TikTok-SL-8 dataset (49 hours, 8 sign languages) with 0.75-0.82 accuracy matching human annotators

## Executive Summary
This paper introduces the first automated framework for sign language dataset acquisition using Vision-Language Models (VLMs). The approach leverages VLMs to filter, annotate, and validate sign language videos from social media platforms like TikTok. The pipeline includes three stages: FaceDetector to ensure visibility, SignActivityDetector to verify signing activity, and TextExtractor to extract on-screen text. A model-as-a-judge (Phi-4-multimodal) validates alignment between the extracted text and the signing. The resulting TikTok-SL-8 dataset contains approximately 49 hours of video across 8 sign languages, curated without manual annotation. The framework achieves an accuracy of 0.75 on German Sign Language (DGS) and 0.82 on American Sign Language (ASL), closely matching human annotator performance.

## Method Summary
The pipeline uses Qwen2.5-VL-7B to process videos at 224x224 resolution in three roles: FaceDetector, SignActivityDetector, and TextExtractor. Videos are first collected via TikTok hashtags and user lists. The FaceDetector verifies clear face visibility, SignActivityDetector confirms signing activity, and TextExtractor extracts embedded on-screen text. A separate Phi-4-multimodal-instruct model acts as a judge to verify alignment between text and signing content. The resulting TikTok-SL-8 dataset contains approximately 49 hours of video across 8 sign languages. Two baseline SLT models (SEM-SLT and Signformer) are trained on the curated data to demonstrate the dataset's utility.

## Key Results
- TikTok-SL-8 dataset created with ~49 hours of video across 8 sign languages
- VLM-based pipeline achieves 0.75 accuracy on DGS and 0.82 on ASL, matching human annotators
- Pretrained SEM-SLT shows better robustness to noisy data than Signformer
- Caption agreement BLEURT scores of 0.53 (DGS) and 0.71 (ASL) suggest moderate-to-high alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLM-based filtering reduces non-signing content through cascaded detection stages.
- Mechanism: Qwen2.5-VL analyzes frames for face visibility (FaceDetector) and signing activity (SignActivityDetector) using prompted classification, rejecting videos that fail either check. The cascading design progressively narrows the candidate pool before expensive annotation.
- Core assumption: VLMs trained on general video can distinguish signing from other hand movements via prompt engineering without sign-specific fine-tuning.
- Evidence anchors: [abstract] "VLM-based pipeline includes a face visibility detection, a sign activity recognition"; [section 3.2] "VLM FaceDetector analyzes the video frames to determine if a person... face is clearly visible... VLM SignActivityDetector assesses whether the person... is primarily using sign language"

### Mechanism 2
- Claim: In-video OCR combined with semantic verification produces weakly-aligned text annotations.
- Mechanism: TextExtractor uses Qwen2.5-VL's OCR capabilities to extract embedded text, then Phi-4-Multimodal evaluates whether extracted text semantically matches the signing content (model-as-a-judge), filtering mismatched pairs.
- Core assumption: On-screen text in TikTok signing videos frequently represents the creator's intended translation, not just metadata.
- Evidence anchors: [abstract] "text extraction from video content, and a judgment step to validate alignment between video and text"; [section 3.3] "creators often embed text directly within the video itself... not always a faithful translation"

### Mechanism 3
- Claim: Pretrained SLT models tolerate noise better than training-from-scratch approaches.
- Mechanism: SEM-SLT's pretrained embedding space provides robust semantic grounding that absorbs caption misalignment, while Signformer's tabula-rasa learning overfits to noisy labels.
- Core assumption: Weakly-supervised pretraining transfers to social media domains despite distribution shift.
- Evidence anchors: [abstract] "pretraining-based SEM-SLT showing better robustness to noisy data than Signformer"; [section 4.4] "SEM-SLT, a model incorporating pre-training, consistently outperforms Signformer, even when trained on noisy data"

## Foundational Learning

- **Concept: Model-as-a-Judge Pattern**
  - Why needed here: Using a different VLM (Phi-4) for validation than annotation (Qwen2.5) mitigates self-preference bias, where models prefer their own outputs.
  - Quick check question: Can you explain why using the same model for generation and evaluation would inflate quality scores?

- **Concept: Weak Supervision for SLT**
  - Why needed here: The pipeline produces noisy labels; understanding how models handle label noise determines which architectures are viable.
  - Quick check question: What is the difference between weak supervision and semi-supervised learning in the context of sign language data?

- **Concept: Multimodal Alignment Verification**
  - Why needed here: The core innovation is verifying sign-text correspondence; misalignment directly impacts downstream SLT performance.
  - Quick check question: Why might BLEURT be preferred over BLEU for measuring semantic alignment between signs and text?

## Architecture Onboarding

- **Component map:** Raw Videos → [FaceDetector (Qwen2.5-VL)] → [SignActivityDetector (Qwen2.5-VL)] → [TextExtractor (Qwen2.5-VL OCR)] → [Judge (Phi-4-Multimodal)] → TikTok-SL-8 Dataset → [SLT Models: SEM-SLT / Signformer]

- **Critical path:** The Judge stage is the quality gate—errors here propagate as misaligned training pairs. Monitor precision/recall tradeoffs at this stage closely.

- **Design tradeoffs:**
  - Resolution fixed at 224px balances throughput vs. fine-grained sign recognition (fingerspelling may suffer)
  - Different models for annotation vs. judgment trades computational cost for bias mitigation
  - Hashtag + user-based retrieval trades coverage for precision in initial collection

- **Failure signatures:**
  - Low yield (<10% pass rate): Prompts may be too strict; relax SignActivityDetector thresholds
  - High false positive rate (manual validation): Judge prompt may need refinement; check for prompt leakage
  - Poor SLT performance despite high dataset quality: Feature extractor (EF-Net-B0 vs. S3D) may be mismatched to social media video characteristics

- **First 3 experiments:**
  1. **Prompt sensitivity analysis:** Vary FaceDetector and SignActivityDetector prompts on 100 manually-labeled videos to measure precision/recall tradeoffs
  2. **Judge threshold calibration:** Sweep agreement thresholds on the Phi-4 judge output (if probability/confidence available) to optimize F1 against human annotations
  3. **Cross-platform validation:** Apply the pipeline to YouTube-SL-25 videos (already curated) to verify generalization beyond TikTok's distribution

## Open Questions the Paper Calls Out
- **Question:** To what extent can advanced prompting strategies, such as Chain-of-Thought (CoT), improve the reliability and accuracy of the VLM-based curation pipeline compared to the standard prompting methods employed?
  - Basis: Authors note in Limitations that their approach "primarily relied on prompt engineering, which may not yield optimal results" and suggest that "More sophisticated prompting strategies, such as Chain-of-Thought prompting... could potentially enhance our framework."
  - Why unresolved: Current study utilized standard prompt templates; potential gains through reasoning-based prompting remain unquantified.

- **Question:** How does the pipeline’s performance degrade when applied to longer video formats given the context length limitations of current Vision-Language Models?
  - Basis: Paper notes in Limitations that "significant challenges remain—particularly in handling longer videos due to context length limitations."
  - Why unresolved: TikTok-SL-8 consists of short-form content; authors acknowledge this constraint but do not evaluate scalability on longer formats where context window truncation might lose critical semantic information.

## Limitations
- VLM generalization across sign languages without sign-specific fine-tuning remains unproven, given the short training time (5 minutes) for FaceDetector and SignActivityDetector
- 224px resolution may limit detection of fine-grained signs like fingerspelling, potentially introducing systematic bias
- Claims about VLM annotation accuracy matching human performance rely on limited manual validation without quantitative intermediate stage analysis

## Confidence
- **High Confidence:** Dataset creation methodology and baseline SLT evaluation results are well-documented and reproducible
- **Medium Confidence:** VLM annotation accuracy (0.75-0.82) closely matching human performance, as this relies on limited manual validation
- **Low Confidence:** Claims about VLM generalization across sign languages without sign-specific fine-tuning, given the short training time (5 minutes) for FaceDetector and SignActivityDetector

## Next Checks
1. **Intermediate Stage Analysis:** Manually annotate 100 randomly sampled pipeline outputs to measure precision/recall at each VLM stage, particularly for SignActivityDetector and TextExtractor
2. **Resolution Sensitivity Test:** Re-run the pipeline on a subset of videos at 448px resolution to quantify the impact of resolution on annotation accuracy and downstream SLT performance
3. **Cross-Lingual Robustness:** Apply the complete pipeline to a non-curated dataset (e.g., YouTube-SL-25) for languages not in TikTok-SL-8 to test VLM generalization claims