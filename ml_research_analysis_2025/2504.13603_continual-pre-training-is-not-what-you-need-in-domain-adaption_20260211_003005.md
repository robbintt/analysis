---
ver: rpa2
title: Continual Pre-Training is (not) What You Need in Domain Adaption
arxiv_id: '2504.13603'
source_url: https://arxiv.org/abs/2504.13603
tags:
- legal
- tasks
- reasoning
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether domain-adaptive continual pre-training
  (DACP) improves legal reasoning in large language models, particularly within the
  Taiwanese legal system. DACP involves continuously pre-training general-purpose
  models on extensive domain-specific data to better align with in-domain distributions.
---

# Continual Pre-Training is (not) What You Need in Domain Adaption

## Quick Facts
- arXiv ID: 2504.13603
- Source URL: https://arxiv.org/abs/2504.13603
- Reference count: 9
- Domain-adaptive continual pre-training (DACP) did not uniformly improve legal reasoning performance across all task types in Taiwanese legal system

## Executive Summary
This paper investigates whether domain-adaptive continual pre-training improves legal reasoning capabilities in large language models. The authors trained LLAWA using DACP on Taiwanese and German legal data and compared it with LoRA-based fine-tuned models (BLLAWA, BLAWSTRAL). They evaluated performance across four legal reasoning tasks: multiple-choice questions, legal argumentation, and essay responses. Results showed DACP did not consistently improve performance - BLLAWA underperformed compared to its base model on multiple-choice tasks, and preference optimization techniques failed to boost results. BLAWSTRAL performed best on argumentation tasks. The study concludes that DACP benefits are not uniform across task types, suggesting hybrid approaches combining DACP with task-specific fine-tuning for balanced enhancement.

## Method Summary
The study employed three models: LLAWA (Llama-3-TAIDE with DACP + full instruction tuning + preference alignment), BLLAWA (Meta-Llama-3-8B-Instruct with LoRA fine-tuning), and BLAWSTRAL (Mistral-Nemo-Instruct-2407 with LoRA fine-tuning). DACP involved pre-training on ~107B tokens of legal data (Taiwan Law 56B, German Law 41B, self-curated 10B). LLAWA underwent three stages: DACP, full-parameter instruction tuning, and preference optimization (DPO/ORPO). BLLAWA and BLAWSTRAL used parameter-efficient LoRA with rank=64 and alpha=128. Evaluation used four legal reasoning tasks with accuracy metrics for multiple-choice and GPT-4o scoring for essays.

## Key Results
- BLLAWA (LoRA fine-tuned) underperformed its base model on multiple-choice tasks (36.56% vs 38.82%)
- BLAWSTRAL (LoRA fine-tuned) achieved the highest accuracy on legal argumentation tasks (66.67%)
- Preference optimization techniques (DPO/ORPO) degraded performance rather than improving it
- DACP benefits were task-dependent rather than uniform across all legal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Domain-Specific Token Exposure
- Claim: DACP shifts model priors toward in-domain legal language distributions, improving specialized knowledge but potentially degrading general instruction-following.
- Mechanism: By continuing pre-training on ~107B tokens of Taiwanese/German legal documents, the model recalibrates token probability distributions toward legal syntax and terminology. This improves factual legal knowledge but may overwrite general reasoning patterns.
- Core assumption: The model's capacity is finite; new domain knowledge competes with existing competencies (catastrophic forgetting).
- Evidence anchors:
  - [abstract] "while DACP enhances domain-specific knowledge, it does not uniformly improve performance across all legal tasks"
  - [section 4.1] "META-LLAMA-3-8B-INSTRUCT outperforms BLLAWA on Tasks A and B despite BLLAWA being fine-tuned for these specific tasks"
  - [corpus] Related work (Chen et al., 2020; Cheng et al., 2023) documents similar trade-offs between domain adaptation and prompting performance
- Break condition: If domain-specific and general corpora are mixed with optimal ratios (unexplored in this paper), or if regularization techniques preserve general capabilities.

### Mechanism 2: Parameter-Efficient LoRA Preserves Base Competencies Better Than Full Fine-Tuning
- Claim: LoRA-based adaptation achieves competitive legal reasoning performance while better preserving base model reasoning patterns.
- Mechanism: LoRA adds low-rank adapter matrices (rank=64) to linear layers without modifying base weights, allowing domain injection without full distribution shift. BLAWSTRAL (LoRA) outperformed BLLAWA (also LoRA) and LLAWA variants on argumentation.
- Core assumption: Low-rank updates can capture domain-specific patterns without disrupting general reasoning representations.
- Evidence anchors:
  - [abstract] "BLAWSTRAL performed best on argumentation tasks"
  - [section 3.3.2] "We add adapter weights to all linear layers. We use a rank of 64, an alpha of 128"
  - [corpus] Limited direct evidence; corpus papers don't specifically compare LoRA vs. full fine-tuning for legal domain
- Break condition: If adapter rank is too low to capture legal reasoning complexity, or if base model lacks sufficient foundational reasoning.

### Mechanism 3: Preference Optimization Fails with Noisy or Misaligned Preference Signals
- Claim: DPO and ORPO degraded performance, suggesting ground-truth-as-preferred is suboptimal for this training paradigm.
- Mechanism: Designating gold answers as "preferred" and model outputs as "rejected" may create misleading preference pairs if model outputs are semantically valid but stylistically different, causing the model to overfit to surface features.
- Core assumption: The preference signal properly captures what makes legal reasoning "better."
- Evidence anchors:
  - [section 4.1] "Contrary to expectations, these techniques did not yield performance improvements... The designation of the ground truth as the preferred answer and the model output as the rejected answer may not be optimal"
  - [section 4.1] "Limited variability in the training data may have led to rapid overfitting"
  - [corpus] No corpus papers specifically address preference optimization failures in legal domains
- Break condition: If preference pairs are constructed from human expert rankings rather than binary gold/model comparisons.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed here: DACP introduces new domain knowledge that can overwrite pre-existing general competencies; understanding this trade-off is essential for interpreting results.
  - Quick check question: Can you explain why a model fine-tuned on legal MCQs might perform worse than its base model on those same MCQs?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: BLLAWA and BLAWSTRAL use LoRA; understanding how adapter matrices work explains why this approach may preserve base capabilities better.
  - Quick check question: What is the difference between updating all model weights vs. training low-rank adapter matrices on top of frozen weights?

- **Concept: Preference Optimization (DPO/ORPO)**
  - Why needed here: The paper experiments with DPO and ORPO and finds they degrade performance; understanding these methods helps diagnose why.
  - Quick check question: How does DPO differ from traditional RLHF, and what could go wrong if your "preferred" and "rejected" pairs are poorly constructed?

## Architecture Onboarding

- **Component map:**
  Base Model → DACP (107B legal tokens) → Full Instruction Tuning → [Optional] Preference Alignment
                                    ↘
                                      LoRA Fine-Tuning (alternative path)

- **Critical path:** DACP requires significant compute (16× H100, ~8 days); LoRA path is resource-light (1× A100). For rapid iteration, start with LoRA.

- **Design tradeoffs:**
  | Path | Pros | Cons |
  |------|------|------|
  | DACP + Full FT | Maximum domain knowledge injection | Expensive; risks generalization loss |
  | LoRA only | Cheap; preserves base competencies | May underutilize domain data |
  | + Preference opt | Theoretically improves output quality | Failed in this study; needs better signal design |

- **Failure signatures:**
  - Fine-tuned model underperforms base model on target task → Likely catastrophic forgetting or overfitting
  - DPO/ORPO decreases accuracy → Preference pairs may be misaligned or data too homogeneous
  - Argumentation works but MCQs fail → Task-specific capability gap; model may lack test-taking skills

- **First 3 experiments:**
  1. **Reproduce LoRA baseline:** Fine-tune base model on legal instruction data with rank=64, alpha=128; evaluate on Tasks A-C to establish benchmark.
  2. **Ablate preference signal construction:** Compare ground-truth-as-preferred vs. human-annotated preference pairs on a held-out legal reasoning subset.
  3. **Hybrid approach test:** Combine DACP (reduced epochs) with LoRA fine-tuning; measure whether domain knowledge injection + parameter-efficient adaptation preserves general reasoning better than full DACP.

## Open Questions the Paper Calls Out

- **What is the optimal balance and volume of training data required to retain general reasoning skills while maximizing domain-specific legal performance?**
  - Basis in paper: [explicit] Page 5 notes that the performance drop in BLLAWA relative to its base model "raises an important research question about the optimal balance and amount of training data."
  - Why unresolved: The study found that fine-tuning improved in-domain generation but seemingly at the cost of "essential test-taking skills," yet the specific data ratios to mitigate this were not determined.
  - What evidence would resolve it: A series of ablation studies varying the ratio of general instruction data to legal task data to identify a point where catastrophic forgetting is minimized without diluting domain gains.

- **Why did preference optimization techniques like DPO and ORPO fail to improve model performance in this specific legal adaptation context?**
  - Basis in paper: [explicit] Page 5 states that contrary to expectations, DPO and ORPO "did not yield performance improvements" and explicitly proposes investigating this failure as a "promising avenue for future research."
  - Why unresolved: The authors hypothesize causes such as overfitting due to limited data variability or suboptimal hyperparameters, but could not conduct a comprehensive exploration due to constraints.
  - What evidence would resolve it: Experiments isolating the failure modes, specifically testing different hyperparameter schedules and data augmentation strategies to prevent rapid overfitting on legal preferences.

- **Can hybrid approaches combining DACP with task-specific fine-tuning or meta-learning provide a more balanced enhancement than DACP alone?**
  - Basis in paper: [explicit] Page 8 concludes that "Future research should explore hybrid approaches that combine DACP with other methods... to achieve a more balanced enhancement."
  - Why unresolved: The results showed DACP benefits are not uniform and can diminish generalization; the authors did not test integrated hybrid pipelines in this work.
  - What evidence would resolve it: A comparative evaluation of a DACP-only model against a model trained using a hybrid DACP-plus-meta-learning curriculum on both legal reasoning and generalization benchmarks.

## Limitations
- DACP pipeline requires substantial computational resources (16×H100 for ~8 days), limiting reproducibility
- Legal instruction dataset specifics remain underspecified beyond "law-related tasks"
- Preference optimization failure may stem from data quality issues rather than fundamental methodological limitations
- Task D evaluation using GPT-4o introduces a black-box scoring mechanism whose reliability is not independently validated
- Study focuses exclusively on Taiwanese legal domain, limiting generalizability

## Confidence
- **High confidence**: LoRA-based fine-tuning (BLAWSTRAL) outperforming DACP-continuously pre-trained model on argumentation tasks
- **Medium confidence**: DACP benefits are task-dependent rather than uniform across all legal reasoning tasks
- **Medium confidence**: Preference optimization techniques failed due to suboptimal preference signal construction
- **Low confidence**: Generalizability of findings beyond Taiwanese legal domain

## Next Checks
1. **Cross-system validation**: Replicate BLAWSTRAL-style LoRA fine-tuning on German legal instruction data and evaluate on equivalent German legal reasoning tasks
2. **Preference signal ablation**: Systematically vary preference pair construction methods while holding all other variables constant to isolate the effect of preference signal quality
3. **Hybrid training regime**: Test whether reduced-epoch DACP followed by LoRA fine-tuning preserves more general reasoning capabilities than full DACP while maintaining domain knowledge gains