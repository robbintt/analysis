---
ver: rpa2
title: 'UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action'
arxiv_id: '2510.17790'
source_url: https://arxiv.org/abs/2510.17790
tags:
- tool
- action
- agents
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UltraCUA, a foundation model for computer-use
  agents that overcomes the limitations of GUI-only agents through hybrid action,
  seamlessly integrating primitive GUI operations with high-level tool execution.
  The key contributions include an automated pipeline for synthesizing tool capabilities
  from documentation and code, a dual-pipeline synthetic data engine generating 17,000+
  verifiable tasks, comprehensive hybrid action trajectory collection, and a two-stage
  training methodology combining supervised fine-tuning with online reinforcement
  learning.
---

# UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action

## Quick Facts
- arXiv ID: 2510.17790
- Source URL: https://arxiv.org/abs/2510.17790
- Authors: Yuhao Yang; Zhen Yang; Zi-Yi Dou; Anh Nguyen; Keen You; Omar Attia; Andrew Szot; Michael Feng; Ram Ramrakhya; Alexander Toshev; Chao Huang; Yinfei Yang; Zhe Gan
- Reference count: 28
- One-line primary result: 22% relative improvement over base models on OSWorld with 11% faster execution

## Executive Summary
This paper introduces UltraCUA, a foundation model for computer-use agents that overcomes GUI-only limitations through hybrid action, seamlessly integrating primitive GUI operations with high-level tool execution. The key contributions include an automated pipeline for synthesizing tool capabilities from documentation and code, a dual-pipeline synthetic data engine generating 17,000+ verifiable tasks, comprehensive hybrid action trajectory collection, and a two-stage training methodology combining supervised fine-tuning with online reinforcement learning. Experiments show UltraCUA achieves 22% relative improvement over base models on OSWorld, with 11% faster execution, and demonstrates strong cross-platform generalization with 21.7% success rate on WindowsAgentArena. The hybrid action paradigm reduces error propagation and improves execution efficiency, establishing a scalable paradigm for robust computer-use automation.

## Method Summary
UltraCUA employs a two-stage training approach: first, supervised fine-tuning on 26.8K successful trajectories generated by a multi-agent rollout system (OpenAI o3 planner + GTA1-7B grounder) executing 17K+ synthetic tasks; second, online reinforcement learning using a GRPO variant to optimize tool selection strategy. The system extracts 881 programmatic tools from documentation and code across 10 domains, generates tasks through evaluator-first (Verifier -> Task) and instruction-first (Context -> Task) pipelines, and employs a dual-pipeline synthetic data engine to ensure verifiable training signals. The architecture splits planning (Planner) from execution (Grounder/GTA1-7B) with working memory via <memory></memory> tags, training on UI-TARS-1.5-7B and OpenCUA-32B base models.

## Key Results
- 22% relative improvement over base models on OSWorld benchmark
- 11% faster execution times through reduced error propagation
- 21.7% success rate on WindowsAgentArena demonstrating cross-platform generalization
- Tool-related failures drop 46% after reinforcement learning optimization

## Why This Works (Mechanism)

### Mechanism 1: Action Space Compaction via Hybrid Primitives
Integrating high-level programmatic tools with low-level GUI actions reduces failure rates by shortening execution chains. GUI-only agents rely on long sequences of brittle actions where step N depends on the exact state produced by step N-1. A single grounding error propagates, causing cascading failures. By mapping complex multi-step operations to single programmatic calls, the agent reduces the horizon and potential points of failure. Core assumption: programmatic tools are reliable and their execution environment is deterministic.

### Mechanism 2: Verifiable Scaffolding via Evaluator-First Synthesis
Generating tasks from pre-existing verification functions (evaluators) guarantees reliable training signals compared to generating tasks and verifying post-hoc. By reversing the pipeline—collecting atomic evaluators first and then prompting an LLM to generate instructions that satisfy these conditions—the system ensures every training sample has a deterministic ground truth. This allows for precise reward assignment in RL. Core assumption: atomic evaluators cover the state space sufficiently.

### Mechanism 3: Reinforcement Learning for Mode Selection Strategy
While Supervised Fine-Tuning teaches action mechanics, online RL is essential for optimizing the strategic selection between GUI and tool modalities. RL enables the model to distinguish between scenarios where a tool is efficient versus where a GUI action is necessary, reducing harmful or unnecessary tool calls. Core assumption: the reward function aligns with user preferences for efficiency and correctness.

## Foundational Learning

- **Concept: ReAct / Thought-Action-Observation Loop**
  - Why needed here: The system uses a Planner agent (OpenAI o3) in a ReAct framework to decompose tasks before execution. Understanding this loop is necessary to grasp how the system decides what to do before the Grounder decides where to click.
  - Quick check question: Can you trace how an "Observation" from a tool execution updates the "Thought" process for the next step?

- **Concept: Sparse vs. Dense Rewards**
  - Why needed here: The RL stage uses a binary environment reward. This sparsity makes credit assignment difficult; understanding this explains why the authors add a dense "tool-use bonus" and filter tasks for difficulty.
  - Quick check question: Why might a pure success/failure signal be insufficient for training a model to prefer tools over GUI clicks?

- **Concept: Visual Grounding (Screen-to-Action)**
  - Why needed here: The architecture splits planning from execution. The Grounder must map high-level semantic instructions to precise pixel coordinates or UI elements.
  - Quick check question: How does the system handle a situation where the visual grounding model fails to locate an element that the planner expects to exist?

## Architecture Onboarding

- **Component map:** Tool Collector -> Synthetic Engine -> Rollout System -> Training Pipeline
- **Critical path:** Tools → Evaluator Definition → Synthetic Tasks → Trajectory Collection → SFT Model → RL Model
- **Design tradeoffs:**
  - Generality vs. Efficiency: Relying heavily on extracted tools risks brittleness if the app updates; falling back to GUI is slower but more robust.
  - SFT Imitation vs. RL Exploration: SFT provides stability and basic competency; RL is required for optimizing the "switching logic" between modes but risks catastrophic forgetting.
  - Task Difficulty: RL training selects tasks with success rates in [0.4, 0.8]. Too easy = no learning; too hard = no gradient signal.
- **Failure signatures:**
  - Cascading Errors: Long chains of GUI clicks where one offset error ruins the sequence.
  - Tool Hallucination: The SFT model attempts to call tools that do not exist or uses incorrect syntax.
  - Context Loss: The agent forgets the initial goal or intermediate variables when switching between tool and GUI modes.
- **First 3 experiments:**
  1. Hybrid Ablation: Run the ablation study comparing UI-TARS-1.5-7B (GUI-Only) vs. UltraCUA-7B (Hybrid) to isolate the performance delta contributed specifically by tool integration.
  2. Memory Stress Test: Disable the <memory> tags in the prompt and run a multi-step workflow to observe context drift.
  3. Cross-Domain Tool Check: Introduce a new, unseen application with documentation to the Tool Collector and verify if the pipeline can dynamically generate and use a new tool for a simple task.

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimum model scale and capability required for effective hybrid action decision-making, and do scaling laws predict when models become proficient at strategic tool-versus-GUI selection? Basis: Figure 4 shows tool usage strongly correlates with model capability—the multi-agent framework uses 60-80 tool calls vs. UltraCUA-7B's 0-20, suggesting a capability threshold exists for leveraging hybrid action effectively. Unresolved because the paper demonstrates the correlation but does not characterize the threshold or predict behavior at intermediate scales.

### Open Question 2
What mechanisms enable or limit cross-platform transfer of hybrid action strategies, and why does Ubuntu→Windows transfer achieve only 21.7% despite tool-level compatibility? Basis: Table 4 shows 21.7% success on WindowsAgentArena without Windows training, surpassing Windows-trained baselines, but the absolute performance remains limited and the transfer mechanisms are not analyzed. Unresolved because the paper demonstrates transfer occurs but does not identify whether limitations stem from platform-specific tool gaps, UI grounding differences, or fundamental strategy mismatches.

### Open Question 3
How can agents better generalize to out-of-distribution tools at inference time, beyond the modest +1.9% improvement observed? Basis: Table 7 shows models adapt to unseen tools with only +1.9% relative success rate improvement, and increased step counts suggest exploration overhead, indicating limited zero-shot tool generalization. Unresolved because the paper acknowledges context-length limits exclude some tools from training, but does not propose mechanisms for rapid tool understanding or meta-learning for tool adaptation.

## Limitations

- Hybrid action effectiveness is demonstrated only on specific tool sets and operating systems with no evidence for scenarios where programmatic APIs are unavailable or poorly documented.
- Synthetic data generation pipeline relies heavily on the quality and coverage of extracted evaluators, which may not generalize to novel application states or edge cases.
- Online RL training requires careful reward shaping and task filtering to avoid mode collapse toward GUI-only solutions, but sensitivity to hyperparameters is not fully explored.

## Confidence

- **High Confidence:** The 22% relative improvement on OSWorld and 11% faster execution times are directly measurable from reported metrics and ablation studies.
- **Medium Confidence:** The cross-platform generalization claim (21.7% success on WindowsAgentArena) is supported by single-point measurements but lacks statistical significance testing.
- **Low Confidence:** The assertion that RL is essential for "strategic decision-making" about tool versus GUI usage is inferred from pre- and post-RL comparisons without ablations isolating the RL contribution.

## Next Checks

1. **API Availability Stress Test:** Evaluate UltraCUA on applications without documented APIs or with frequently changing interfaces to determine the fallback effectiveness of pure GUI actions and the model's ability to gracefully degrade.

2. **Evaluator Coverage Analysis:** Systematically measure what fraction of real-world application states can be verified by the atomic evaluators extracted during training, identifying potential blind spots in the synthetic data generation pipeline.

3. **Long-Horizon Dependency Preservation:** Test the <memory> mechanism's effectiveness by designing multi-step workflows requiring precise state tracking across tool and GUI mode switches, measuring context retention accuracy and error propagation compared to pure GUI baselines.