---
ver: rpa2
title: Supervised Guidance Training for Infinite-Dimensional Diffusion Models
arxiv_id: '2601.20756'
source_url: https://arxiv.org/abs/2601.20756
tags:
- diffusion
- conditional
- guidance
- score
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of conditioning infinite-dimensional
  diffusion models to sample from posterior distributions in function spaces, particularly
  for Bayesian inverse problems. The authors develop a principled framework using
  an infinite-dimensional extension of Doob's h-transform, proving that the conditional
  score decomposes into an unconditional score and a tractable guidance term under
  two general settings.
---

# Supervised Guidance Training for Infinite-Dimensional Diffusion Models

## Quick Facts
- arXiv ID: 2601.20756
- Source URL: https://arxiv.org/abs/2601.20756
- Authors: Elizabeth L. Baker; Alexander Denker; Jes Frellsen
- Reference count: 40
- Key outcome: Introduces Supervised Guidance Training (SGT) for conditioning infinite-dimensional diffusion models, achieving superior accuracy on Bayesian inverse problems with smaller networks and shorter training schedules compared to heuristic baselines

## Executive Summary
This paper addresses the challenge of sampling from posterior distributions in function spaces using infinite-dimensional diffusion models. The authors develop a principled framework that decomposes the conditional score into an unconditional score and a tractable guidance term using an infinite-dimensional extension of Doob's h-transform. They introduce Supervised Guidance Training (SGT), a simulation-free score matching objective that efficiently learns the guidance term without requiring ground truth data or trajectory simulation. The method is evaluated on sparse observations, heat equation, and shape inpainting tasks, demonstrating superior accuracy compared to heuristic baselines like FunDPS while using smaller networks and shorter training schedules.

## Method Summary
The method leverages an infinite-dimensional extension of Doob's h-transform to decompose the conditional score into an unconditional score (pre-trained and frozen) and a tractable guidance term. SGT uses a simulation-free denoising score matching objective to learn the guidance term without simulating SDE trajectories during training. The guidance network output is preconditioned by the covariance operator C^k (k∈{0,1/2,1}) to encode regularity assumptions. The framework works under two settings: when the prior lies in the Cameron-Martin space of C, or when it's absolutely continuous with respect to a Gaussian measure. The combined score (unconditional + guidance) drives a reverse SDE to generate posterior samples.

## Key Results
- SGT achieves superior accuracy on sparse observations task compared to heuristic baselines (FunDPS) with smaller networks (4-layer FNO vs 8-layer) and shorter training schedules (40k vs 100k+ steps)
- Performance is competitive with specifically trained conditional models while requiring less computational resources
- SGT demonstrates resolution invariance - maintains performance when sampling at higher resolutions than training
- The preconditioner choice k significantly impacts performance, with k=1 giving best RMSE on sparse observations and k=0 competitive on heat equation

## Why This Works (Mechanism)

### Mechanism 1: Score Decomposition via Doob's h-transform
The conditional score for posterior sampling can be decomposed into an unconditional score plus a guidance term under Cameron-Martin support or absolute continuity conditions. This allows training only the guidance network rather than a full conditional model. The h-transform derivation fails if the prior is mutually singular with respect to the Gaussian reference measure.

### Mechanism 2: Simulation-Free Denoising Score Matching
The guidance term is learned via a denoising score matching objective that avoids expensive REINFORCE-style gradient estimation. The loss minimizes to the true guidance term when bounded, providing efficient training without trajectory sampling. The approach breaks down if the likelihood potential is non-differentiable or unbounded.

### Mechanism 3: Preconditioning by Covariance Operator
Preconditioning the guidance network output by C^k encodes regularity assumptions and improves sample quality. Higher k enforces smoother guidance by placing it in the Cameron-Martin space. The approach requires C to be known and factorizable; poor matching between C and data distribution can cause oversmoothing or undersmoothing.

## Foundational Learning

- **Score-Based Diffusion in Hilbert Spaces**: Finite-dimensional densities don't exist in infinite dimensions; scores must be defined via conditional expectations. Quick check: Explain why `s(t,x) = -α_t(x - e^{-t/2}E[X_0|X_t=x])` replaces `∇_x log p_t(x)`.

- **Cameron-Martin Space**: Determines which directions allow measure changes; essential for Setting 1 and preconditioner choice. Quick check: Given covariance C with eigenvalues λ_k = 1/k^ν, which functions are in its Cameron-Martin space?

- **Tweedie's Formula (Infinite-Dimensional)**: Connects score to posterior mean; enables likelihood-informed network parameterization. Quick check: How does Proposition 4.1 justify replacing E[X_0|X_t] with the Tweedie estimate in the guidance approximation?

## Architecture Onboarding

- **Component map**: Unconditional score s_θ(t,x) [pre-trained, frozen] → Forward process: x_t = e^{-t/2}x_0 + √(1-e^{-t}) C^{1/2} ε → Guidance network u_ϕ(t, x_t, y) [trained] → Combined: s_θ + C^k u_ϕ → Reverse SDE → Posterior samples at t=0

- **Critical path**: The stopgrad on s_θ (Algorithm 1, line 7) is essential—do not backprop through the unconditional score during guidance training.

- **Design tradeoffs**:
  - k=1: Maximum smoothness, best for sparse observations; requires C factorizable
  - k=1/2: Balanced; output naturally in Cameron-Martin space
  - k=0: Most flexible, slower convergence; network learns smoothing
  - Likelihood-informed bias: Adds ∇_Φ term explicitly—faster convergence when Φ is cheap, but requires Φ differentiable

- **Failure signatures**:
  - Posterior samples diverge from observations → guidance network undertrained OR learning rate too high
  - Samples are oversmoothed → k too high OR C eigenvalues decay too fast
  - Training loss explodes → B unbounded; check if K contains support of π_y
  - FunDPS diverges but SGT works → γ tuning failed (heuristic methods are unstable)

- **First 3 experiments**:
  1. Sparse observations (1D): Replicate Table 1 with k∈{0,1/2,1}. Use FNO with 4 layers, 16 modes, 64 channels. Train guidance network for 40k steps—should match or beat FunDPS within 10-15% of conditional diffusion RMSE.
  2. Ablate preconditioner: On heat equation task, compare all three k values. Expect k=0 and k=1 to be close (Table 2 shows negligible difference), confirming operator smoothing dominates for ill-posed forward operators.
  3. Resolution invariance test: Train at N=64 grid points, sample at N=128 and N=256. SGT should maintain performance; finite-dimensional baselines degrade. This validates the infinite-dimensional formulation.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes the Cameron-Martin support condition or absolute continuity with respect to Gaussian reference measures; fails for singular priors
- Requires the covariance operator C to be factorizable and the preconditioner C^k to be well-defined
- Assumes Φ is differentiable; non-differentiable likelihoods break the Tweedie-based guidance approximation

## Confidence
- **High**: Score decomposition theorem, simulation-free training objective, and preconditioning mechanism
- **Medium**: Preconditioner choice (k values), likelihood-informed bias term, and resolution invariance claims
- **Low**: Runtime efficiency comparisons (FunDPS training instability makes fair comparison difficult)

## Next Checks
1. **Singular prior test**: Apply SGT to an inverse problem with a prior singular with respect to Gaussian (e.g., Besov space prior) and measure performance degradation.
2. **Non-differentiable likelihood**: Replace smooth Φ with a non-smooth loss (e.g., ℓ¹ norm) and evaluate whether SGT still converges or requires the full SOC loss.
3. **Operator spectrum sensitivity**: Systematically vary the eigenvalue decay rate of C and measure impact on k=1 preconditioning effectiveness across different inverse problems.