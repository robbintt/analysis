---
ver: rpa2
title: 'PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning'
arxiv_id: '2509.19894'
source_url: https://arxiv.org/abs/2509.19894
tags:
- promptcot
- prompt
- arxiv
- prompts
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PromptCoT 2.0, a scalable framework for\
  \ synthesizing reasoning problems that addresses the data scarcity bottleneck in\
  \ large language model training. The method employs an expectation-maximization\
  \ loop where rationales\u2014intermediate reasoning structures\u2014are iteratively\
  \ refined to guide prompt construction, enabling generation of problems that are\
  \ both harder and more diverse than existing corpora."
---

# PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning

## Quick Facts
- **arXiv ID:** 2509.19894
- **Source URL:** https://arxiv.org/abs/2509.19894
- **Reference count:** 34
- **Primary result:** Synthesizes harder, more diverse reasoning problems than prior approaches, achieving new state-of-the-art results in both self-play (+4.4/4.8/5.3 on AIME 24/25/HMMT 25, +6.1/+5.0 on LiveCodeBench v5/v6, +35 Elo on Codeforces) and supervised fine-tuning (73.1/65.6/53.4 accuracy on AIME 24/25/LiveCodeBench v5).

## Executive Summary
PromptCoT 2.0 introduces a scalable framework for synthesizing reasoning problems to address data scarcity in large language model training. The method uses an expectation-maximization loop where rationales—intermediate reasoning structures—are iteratively refined to guide prompt construction, enabling generation of problems that are both harder and more diverse than existing corpora. Evaluated under two regimes: (1) self-play, where Qwen3-30B-A3B-Thinking-2507 trained on PromptCoT 2.0 prompts achieves new state-of-the-art results at 30B scale; (2) supervised fine-tuning, where Qwen2.5-7B-Instruct trained solely on synthetic prompts outperforms models trained on human or hybrid data.

## Method Summary
PromptCoT 2.0 employs an expectation-maximization loop where rationales guide prompt construction for generating reasoning problems. The framework starts with a cold-start corpus of 9,221 Codeforces and 6,365 AoPS problems, which are annotated with concepts and rationales by four teacher models. The EM loop alternates between an E-step that trains a rationale model via reinforcement learning (rewarding log-likelihood of problem generation) and an M-step that trains a prompt model using the refined rationales. For self-play, the framework uses direct preference optimization with verifiable feedback (unit tests for code, majority-vote for math), filtering problems solved multiple times. For supervised fine-tuning, it distills traces from a strong teacher model to train smaller instruction models.

## Key Results
- **Self-play:** Qwen3-30B-A3B-Thinking-2507 trained on PromptCoT 2.0 prompts achieves new state-of-the-art results at 30B scale (+4.4/4.8/5.3 on AIME 24/25/HMMT 25, +6.1/+5.0 on LiveCodeBench v5/v6, +35 Elo on Codeforces).
- **Supervised fine-tuning:** Qwen2.5-7B-Instruct trained solely on synthetic prompts outperforms models trained on human or hybrid data (73.1/65.6/53.4 accuracy on AIME 24/25/LiveCodeBench v5).
- **Problem quality:** Analyses confirm PromptCoT 2.0 produces problems with distinct distributional properties and significantly greater difficulty than prior approaches.

## Why This Works (Mechanism)
PromptCoT 2.0 works by iteratively refining rationales through an expectation-maximization loop, which guides the generation of harder and more diverse reasoning problems. The EM framework allows the model to learn from its own synthesized data, creating a self-improving cycle where each iteration produces more challenging problems. The use of verifiable feedback (unit tests for code, majority-vote for math) ensures that only solvable problems are retained, while the iterative refinement process pushes the difficulty beyond what human-curated datasets typically offer.

## Foundational Learning
- **Expectation-Maximization (EM) Loop:** Alternates between refining rationales (E-step) and generating prompts (M-step). Needed for iterative improvement of problem difficulty and diversity. Quick check: Monitor NLL on held-out (concept, problem) pairs.
- **Reinforcement Learning with Verifiable Feedback:** Uses rewards based on problem generation likelihood and correctness verification. Needed to ensure generated problems are solvable and meaningful. Quick check: Fraction of correct rollouts in self-play.
- **Direct Preference Optimization (DPO):** Trains models to prefer correct solutions over incorrect ones. Needed for self-play training with verifiable feedback. Quick check: Elo gain on Codeforces benchmark.

## Architecture Onboarding

**Component Map:** Concept extraction -> Rationale generation -> Prompt synthesis -> Problem generation -> Verification -> EM refinement

**Critical Path:** The EM loop (E-step: rationale refinement → M-step: prompt generation) is the core innovation that enables scalable synthesis of harder problems.

**Design Tradeoffs:** 
- Uses proprietary GPT-OSS-120B for teacher traces and verification, but claims open substitutes work.
- Balances problem difficulty via pass@1 thresholds (<25%) and diversity via KL divergence from human data.
- Filters self-play problems solved ≥4/8 times to avoid collapse.

**Failure Signatures:** 
- Generated problems too easy or incoherent (check pass@1 on held-out set).
- Self-play collapses (monitor fraction of correct rollouts; if >50%, increase difficulty or temperature).

**First Experiments:**
1. **Cold-start training:** Train q_φ and p_θ on 9,221 Codeforces + 6,365 AoPS problems with concept/rationale annotations.
2. **EM iteration test:** Run 3-5 EM iterations, monitor NLL on held-out pairs to assess convergence.
3. **Self-play sanity check:** Generate 100 problems, verify via unit tests/majority-vote, check Elo gain on Codeforces.

## Open Questions the Paper Calls Out
None

## Limitations
- **Proprietary teacher models:** GPT-OSS-120B used for distillation and verification is inaccessible, limiting exact replication.
- **EM convergence:** Number of iterations and stopping criteria are unspecified, requiring empirical tuning.
- **Programming verification:** Unit-test generation prompts are referenced but not provided, potentially affecting code problem quality.

## Confidence
**High Confidence**
- EM framework and data pipeline are clearly described and reproducible with open substitutes.
- Benchmark results are verifiable if synthetic prompts are generated.

**Medium Confidence**
- Performance gains depend on exact problem filtering and temperature settings, which are not fully specified.
- SFT results hinge on GPT-OSS-120B traces; open substitutes may yield lower but still competitive results.

**Low Confidence**
- Claims about problem diversity and difficulty are supported by distributional analyses, but methodology for quantifying "hardness" is not fully transparent.
- Ablation showing superiority over human-curated data may not hold if verification protocols differ.

## Next Checks
1. **Reproduce synthetic problem pass@1 scores** on a held-out set using Qwen2.5-72B; verify that difficulty (pass@1 <25%) and diversity (KL divergence from human data) match reported trends.
2. **Run self-play with open substitutes** (e.g., DeepSeek-R1 for math verification) and compare Codeforces Elo gains to the claimed +35; document sensitivity to temperature and filtering thresholds.
3. **Train SFT models** on synthetic prompts distilled from an open teacher (e.g., Qwen2.5-72B) and evaluate on AIME 24/25; assess gap versus GPT-OSS-120B-based results.