---
ver: rpa2
title: 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local
  and Web Searches'
arxiv_id: '2508.08088'
source_url: https://arxiv.org/abs/2508.08088
tags:
- search
- agent
- deep
- local
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HierSearch is a hierarchical deep search framework that integrates
  local and Web searches using reinforcement learning. It trains separate agents for
  local and Web domains and a planner agent to coordinate them.
---

# HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches

## Quick Facts
- **arXiv ID**: 2508.08088
- **Source URL**: https://arxiv.org/abs/2508.08088
- **Reference count**: 31
- **Primary result**: HierSearch achieves up to 62.83% EM and 72.81% F1 on enterprise QA benchmarks while reducing web search calls

## Executive Summary
HierSearch introduces a hierarchical deep search framework that integrates local enterprise knowledge bases with web search capabilities for enterprise question answering. The system employs a two-stage reinforcement learning approach where specialized local and web agents are first trained separately, then coordinated by a planner agent. A knowledge refiner filters out irrelevant or hallucinated evidence from lower-level agents. Experiments demonstrate superior performance across six benchmarks compared to flat RL and multi-source retrieval baselines, while requiring fewer web search calls.

## Method Summary
HierSearch employs a hierarchical deep search architecture with three levels of agents: a local agent for querying enterprise knowledge bases (text chunks and knowledge graphs), a web agent for external search, and a planner agent that coordinates between them. The system uses Qwen2.5-7B-Instruct as the backbone model and trains through a two-stage reinforcement learning process using the GRPO algorithm within the VERL framework. In stage one, local and web agents are trained separately on mixed QA data; in stage two, the planner agent learns to call these low-level agents as tools. A knowledge refiner filters irrelevant evidence through embedding-based similarity scoring to the current thinking step and agent conclusions.

## Key Results
- Achieves 62.83% EM and 72.81% F1 on finance and medical domain benchmarks
- Outperforms flat RL and multi-source retrieval baselines across six test datasets
- Reduces web search calls compared to flat RL while maintaining superior performance
- Demonstrates effectiveness on MuSiQue, OmniEval, BioASQ, NQ, HotpotQA, and PubMedQA benchmarks

## Why This Works (Mechanism)
The hierarchical approach works by decomposing the complex multi-source search problem into specialized sub-tasks handled by dedicated agents. The local agent efficiently retrieves from structured enterprise knowledge, while the web agent handles external information needs. The planner agent coordinates between these specialized tools, making strategic decisions about when to use each. The knowledge refiner addresses hallucination and irrelevance issues by filtering evidence based on semantic similarity to current reasoning steps, preventing error propagation up the hierarchy.

## Foundational Learning

**GRPO reinforcement learning**: Why needed: Optimizes agent behavior through reward signals rather than supervised learning. Quick check: Verify reward structure correctly differentiates between format compliance, F1 scores, and exploration.

**Multi-source retrieval**: Why needed: Enterprise QA requires both internal knowledge bases and external web information. Quick check: Confirm separate local (knowledge graph + chunks) and web (search engine + pages) sources are properly integrated.

**Knowledge graph construction**: Why needed: Structured enterprise data requires specialized retrieval beyond text chunks. Quick check: Validate HippoRAG pipeline correctly extracts entities, relations, and builds embeddings.

## Architecture Onboarding

**Component map**: Qwen2.5-7B-Instruct -> Local Agent -> Web Agent -> Planner Agent -> Knowledge Refiner -> Answer Generator

**Critical path**: User query → Planner agent selects tool → Local/Web agent executes → Knowledge refiner filters evidence → Planner updates reasoning → Final answer generation

**Design tradeoffs**: Hierarchical vs flat RL: HierSearch trades increased model complexity for better specialization and reduced hallucination. Knowledge refiner vs raw evidence: Adds filtering overhead but improves answer quality.

**Failure signatures**: 
- Local agent over-reliant on web tools despite available local evidence
- Planner agent copying hallucinated intermediate answers verbatim
- Knowledge refiner allowing irrelevant evidence through due to poor similarity thresholds

**First experiments**:
1. Verify local agent can retrieve from both text chunks and knowledge graph independently
2. Test web agent's ability to search and browse URLs successfully
3. Confirm planner agent can call both low-level agents and receive filtered evidence from knowledge refiner

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge refiner hyperparameters α and β are unspecified, potentially affecting hallucination filtering performance
- Missing GRPO hyperparameters beyond learning rate create uncertainty in exact reproduction
- Corpus sampling strategy lacks specificity on negative sampling ratios
- No cross-domain validation beyond the six tested benchmarks

## Confidence

**High confidence**: The hierarchical architecture design and reported performance improvements are well-documented with specific EM/F1 scores. The two-stage training procedure is clearly specified.

**Medium confidence**: The GRPO training implementation is described but missing critical hyperparameters. The knowledge refiner mechanism is explained but lacks specific threshold values.

**Low confidence**: The exact implementation of planner agent decision-making and prompt engineering for tool invocation is not fully detailed.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Test the knowledge refiner with different α and β values (e.g., 25%, 50%, 75%) to determine optimal filtering thresholds and assess performance stability.

2. **Cross-domain generalization**: Evaluate HierSearch on an additional domain (e.g., legal or scientific research) not covered in the original six benchmarks to test domain transferability.

3. **Ablation study on agent hierarchy**: Compare full HierSearch against (a) local-only agent, (b) web-only agent, and (c) flat RL baseline with both tools available to quantify the contribution of hierarchical coordination.