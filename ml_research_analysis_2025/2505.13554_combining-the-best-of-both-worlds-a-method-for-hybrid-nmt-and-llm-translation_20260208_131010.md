---
ver: rpa2
title: 'Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation'
arxiv_id: '2505.13554'
source_url: https://arxiv.org/abs/2505.13554
tags:
- translation
- test
- sets
- when
- bleurt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of using large
  language models (LLMs) for translation by proposing a hybrid approach that combines
  neural machine translation (NMT) and LLMs. The core method uses a decision maker
  that leverages source sentence features to determine when to use LLM translation
  versus NMT, minimizing LLM usage while ensuring better translation quality.
---

# Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation

## Quick Facts
- arXiv ID: 2505.13554
- Source URL: https://arxiv.org/abs/2505.13554
- Reference count: 16
- One-line primary result: Hybrid approach combines NMT and LLMs using a decision maker that minimizes LLM usage while ensuring better translation quality

## Executive Summary
This paper addresses the high computational cost of using large language models (LLMs) for translation by proposing a hybrid approach that combines neural machine translation (NMT) and LLMs. The core method uses a decision maker that leverages source sentence features to determine when to use LLM translation versus NMT, minimizing LLM usage while ensuring better translation quality. Experiments on multilingual test sets show that the proposed method achieves optimal translation performance with minimal LLM usage, outperforming baseline approaches that rely on quality estimation models.

## Method Summary
The paper proposes three hybrid translation approaches that route source sentences to either NMT or LLM based on different decision-making strategies. The first approach (PPLT) uses sentence perplexity computed by a small language model to identify complex sentences for LLM routing. The second approach (JDM) employs a binary classifier trained on translation quality differences to predict when LLM will outperform NMT. A baseline QET approach uses quality estimation models for routing. All methods target approximately 25% LLM usage while maximizing translation quality. The system was evaluated on Chinese-English, German-English, and Japanese-English language pairs using WMT22 News, Flores, Literary, and Tech test sets.

## Key Results
- JDM achieves 78.81 average DA with 29.52% LLM usage, outperforming QET baseline (77.58 DA, 30.55% usage)
- PPLT achieves 79.24 DA on News test set with 38.19% LLM usage
- LLM usage varies by domain: 80.40% on Literary test set vs 7.00% on Tech test set
- Complementary strengths: LLM outperforms NMT by 3.80 DA on hard sentences vs only 1.41 on simple sentences

## Why This Works (Mechanism)

### Mechanism 1
Sentence-level perplexity computed by a small language model acts as a proxy for translation difficulty, allowing the system to route complex inputs to the LLM. A small LM is trained on the same monolingual data used for NMT training. For each source sentence, the model computes perplexity (PPL). If PPL exceeds a predefined threshold, the sentence is flagged as atypical relative to the training distribution and routed to the LLM, which has broader coverage from pretraining. Core assumption: High-perplexity sentences correlate with cases where NMT underperforms and LLM provides superior translation. Evidence: Table 2 shows PPLT achieves 79.24 DA on News (vs. 78.99 NMT baseline) with only 38.19% LLM usage. Break condition: If the small LM's training distribution diverges significantly from production input (domain shift), perplexity may become unreliable as a routing signal.

### Mechanism 2
A binary classifier trained on labeled translation-quality differences can predict when LLM will outperform NMT without requiring reference translations at inference time. Using bilingual data, the system obtains both NMT and LLM translations, scores them with a reference-based metric (wmt22-comet-da), and labels samples as positive when (1) NMT quality is low (QNMT < T1) AND (2) LLM quality exceeds NMT by a margin (QLLM âˆ’ QNMT > T2). An XLM-RoBERTa-base classifier is fine-tuned on these labels to predict routing decisions from source text alone. Core assumption: The quality patterns learned from training data generalize to unseen test sentences. Evidence: Table 2 shows JDM achieves 78.81 average DA with 29.52% LLM usage, outperforming QET (77.58 DA, 30.55% usage). Break condition: If NMT and LLM models are significantly updated (beyond fine-tuning), the classifier may require retraining to capture new quality patterns.

### Mechanism 3
The effectiveness of hybrid routing depends critically on complementary strengths between NMT and LLM across domains and sentence types. NMT excels on technical/in-domain content where it was extensively trained; LLM excels on informal expressions, idioms, and internet memes due to broader pretraining data. The routing system exploits this complementarity by allocating sentences to the stronger model per context. Core assumption: NMT and LLM have non-overlapping failure modes that can be predicted from source features. Evidence: Table 1 shows LLM outperforms NMT by 3.80 DA on hard sentences vs. only 1.41 on simple sentences. Appendix A/Table 4: LLM usage is 68% for informal expressions/memes but only 5% for context-dependent sentences. Table 2: JDM uses 80.40% LLM on Literary test set but only 7.00% on Tech test set. Break condition: If both models have similar performance profiles (no complementarity), hybrid routing provides minimal benefit.

## Foundational Learning

- **Perplexity as Distributional Distance**
  - Why needed here: Understanding how perplexity quantifies how "surprised" a language model is by input text relative to its training distribution
  - Quick check question: If a source sentence uses domain-specific jargon unseen during LM training, would you expect high or low perplexity?

- **Quality Estimation (QE) vs. Reference-Based Evaluation**
  - Why needed here: The paper contrasts QE-based routing (QET, which requires no reference) with their source-only approach; understanding this distinction is critical
  - Quick check question: Why can QET make suboptimal routing decisions even when the QE model is accurate?

- **Threshold Calibration via Quantile Selection**
  - Why needed here: All routing methods require threshold setting; the paper uses statistical quantiles (e.g., 25th percentile) to control LLM invocation rates
  - Quick check question: If you want to reduce LLM usage from 25% to 10%, how would you adjust the PPL threshold?

## Architecture Onboarding

- **Component map**:
  NMT Model (Deep Transformer-Big) -> Decision Maker (PPLT/JDM) -> LLM (Llama-3.1-8B-Instruct) or NMT Output

- **Critical path**:
  1. Source sentence received
  2. Decision phase: Either compute PPL (PPLT) or run classifier (JDM)
  3. Routing: Send to NMT or LLM based on decision
  4. Return translation (single model output, no ensembling)

- **Design tradeoffs**:
  - **PPLT vs. JDM**: PPLT is simpler (single threshold on PPL) but less adaptive; JDM captures more nuanced patterns but requires classifier training data
  - **LLM usage cap**: Lower thresholds reduce cost but may miss LLM-advantage cases; paper targets ~25% as balance point
  - **Domain sensitivity**: PPLT performs better on in-domain technical content (LM trained on similar data); JDM generalizes better across domains

- **Failure signatures**:
  - High LLM usage on simple sentences: Indicates threshold too low or classifier over-predicting positive class
  - Quality degradation vs. NMT-only: Suggests routing is sending sentences to wrong model; check if LLM actually outperforms NMT on flagged sentences
  - Domain shift collapse: If production input differs significantly from training data, both PPL and classifier signals may become unreliable

- **First 3 experiments**:
  1. **Threshold sweep**: Run PPLT with thresholds corresponding to 10%, 25%, 40% LLM usage on held-out validation data; plot translation quality (DA score) vs. LLM usage to find optimal operating point for your cost/quality constraints
  2. **Per-domain routing analysis**: Separate test data by domain (technical, literary, news); measure LLM usage percentage per domain for JDM to verify adaptive behavior (expected: high LLM usage for literary, low for technical)
  3. **Ablation of classifier training data composition**: Train JDM with different positive/negative sample ratios (paper uses 10K:30K); measure impact on both LLM usage rate and translation quality to validate sample selection strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a hybrid translation system be adapted to provide performance gains when the NMT and LLM models exhibit statistically equivalent quality rather than complementary strengths?
- Basis in paper: The Limitations section states: "We find that the final integration performance depends on the complementarity between NMT and LLM... If the NMT performs equally as LLM, we see no improvement after integration."
- Why unresolved: The current Joint Decision-making (JDM) method relies on identifying scenarios where one model outperforms the other; it lacks a mechanism to leverage diversity or ensembling when models are equally accurate
- What evidence would resolve it: A modified routing strategy or fusion mechanism that achieves higher BLEU/COMET scores than individual baselines on test sets where NMT and LLM raw scores are statistically tied

### Open Question 2
- Question: Can the decision-maker be improved to successfully route or translate "Category 3" sentences that require context beyond the sentence level?
- Basis in paper: Appendix A/Table 4 shows that for Category 3 sentences (context required), LLM usage is only 5%, implying the decider correctly avoids the LLM, but the paper acknowledges LLMs also fail here without context, leaving this "hard" category unresolved
- Why unresolved: The current system optimizes for sentence-level translation quality and explicitly flags context-dependent sentences as a failure mode for both NMT and LLM approaches
- What evidence would resolve it: Integration of a document-level context window or retrieval mechanism that improves translation quality specifically for the "context required" subset of the test data

### Open Question 3
- Question: What mechanisms can bridge the performance gap between the JDM decider and the theoretical "Oracle" upper bound?
- Basis in paper: Tables 2 and 3 consistently show a performance gap between the proposed JDM method and the "Oracle" (e.g., in Literary Zh2En, JDM scores 65.70 vs. Oracle 68.41), indicating the decider frequently makes sub-optimal choices
- Why unresolved: The decider relies on limited features (primarily source complexity/domain) which do not fully capture the semantic nuances that determine which model will produce the superior translation
- What evidence would resolve it: The inclusion of additional predictive features (e.g., source uncertainty, syntactic complexity) or a confidence-based cascade that reduces the decision error rate to approach Oracle performance

### Open Question 4
- Question: Is it possible to implement a dynamic threshold adjustment policy that adapts to real-time computational budgets without manual re-calculation?
- Basis in paper: The paper notes in Section 3.2 and Appendix F that thresholds are determined statistically offline to control LLM usage to ~25%, but implies dynamic adjustment is a separate, unexplored capability
- Why unresolved: The current approach uses static thresholds (T1, T2) derived from a fixed dataset, which may not generalize well to fluctuating inference loads or varying cost constraints in a production environment
- What evidence would resolve it: A reinforcement learning agent or adaptive controller that maintains translation quality while dynamically scaling LLM usage based on a defined cost/latency budget

## Limitations

- **Model Architecture Transparency**: The paper specifies "Deep Transformer-Big" for NMT without providing exact hyperparameters (layer depth, hidden size, attention heads)
- **Test Set Availability**: The Literary and Tech test sets are not yet open-sourced, though Appendix B provides category definitions through examples
- **Statistical Calibration Dependence**: Both PPLT and JDM approaches rely on statistical thresholds (perplexity percentiles, quality score quantiles) with limited exploration of sensitivity to these choices

## Confidence

- **High Confidence**: PPLT achieves 79.24 DA on News test set with 38.19% LLM usage; JDM achieves 78.81 average DA across domains with 29.52% LLM usage; domain-specific routing patterns are well-supported by data; complementary NMT/LLM strengths across sentence types are demonstrated
- **Medium Confidence**: The binary classifier (JDM) generalizes from training data to unseen test cases; threshold calibration using statistical quantiles provides optimal cost/quality tradeoff; perplexity as a proxy for translation difficulty reliably identifies LLM-advantage cases
- **Low Confidence**: The exact impact of positive/negative sample ratio (10K:30K) on JDM performance; generalizability of results to language pairs beyond the three tested; performance when production input domain significantly differs from training data

## Next Checks

1. **Threshold Sensitivity Analysis**: Run PPLT with thresholds corresponding to 10%, 25%, 40%, and 50% LLM usage on a held-out validation set. Plot translation quality (DA score) against LLM usage percentage to identify the optimal operating point and verify the paper's ~25% target is justified.

2. **Domain Transfer Validation**: Test JDM performance when routing decisions are made on a domain not represented in the training data (e.g., legal or medical text). Measure both LLM usage percentage and translation quality to assess domain generalization capability.

3. **Sample Ratio Ablation**: Train JDM with varying positive/negative sample ratios (e.g., 5K:35K, 15K:25K, 20K:20K) while keeping total sample count constant. Evaluate impact on both LLM usage rate and translation quality to validate the effectiveness of the paper's 10K:30K selection strategy.