---
ver: rpa2
title: 'CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework
  for Document-level Sentiment Analysis'
arxiv_id: '2507.14022'
source_url: https://arxiv.org/abs/2507.14022
tags:
- table
- classification
- document
- sentiment
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes the Cognitive Pairwise Comparison Classification
  Model Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
  based on expert knowledge judgment, is used to calculate the weights of evaluation
  criteria, including accuracy, precision, recall, F1-score, specificity, Matthews
  Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and efficiency.
---

# CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis

## Quick Facts
- **arXiv ID:** 2507.14022
- **Source URL:** https://arxiv.org/abs/2507.14022
- **Reference count:** 40
- **Key outcome:** CPC-CMS framework selects ALBERT as best model for document-level sentiment analysis across three social media datasets when evaluation excludes time, but no single model dominates when time is included.

## Executive Summary
This study introduces the Cognitive Pairwise Comparison Classification Model Selection (CPC-CMS) framework for document-level sentiment analysis. The framework uses expert knowledge to calculate weights for eight evaluation criteria including accuracy, F1-score, MCC, and efficiency. Seven classification models (Naive Bayes, LSVC, Random Forest, Logistic Regression, XGBoost, LSTM, ALBERT) are evaluated on three social media datasets. The weighted decision matrix approach enables objective model selection by balancing multiple performance metrics and computational constraints. Results demonstrate ALBERT's superiority in semantic understanding while highlighting XGBoost as a more efficient alternative when time constraints are considered.

## Method Summary
The CPC-CMS framework combines Multi-Criteria Decision Making with document-level sentiment analysis. Experts construct a Pairwise Opposite Matrix (POM) to assign comparative weights between evaluation criteria. These weights are processed via Row Average plus normal Utility (RAU) operator to generate normalized weights. Seven classification models are trained on three social media datasets with 80/10/10 train/validation/test splits. Each model is evaluated on eight metrics: accuracy, precision, recall, F1-score, specificity, MCC, Kappa, and efficiency (time). A weighted decision matrix aggregates these scores using the expert-derived weights to select the optimal model for the given classification problem.

## Key Results
- ALBERT achieves highest accuracy (0.821) and F1-score (0.821) across all three datasets when time is excluded
- When efficiency is included, no single model consistently outperforms others across all datasets
- XGBoost emerges as the most efficient model with strong performance trade-offs
- The framework successfully demonstrates model selection across diverse social media sentiment contexts

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Weighting of Evaluation Criteria
Expert judgment is systematically encoded into weights through Pairwise Opposite Matrix (POM) where experts compare criteria importance. The RAU operator processes these comparisons to generate normalized weights, ensuring transparent decision logic. The Accordance Index (AI â‰¤ 0.1) flags inconsistent human judgment. If AI exceeds 0.1, the expert input process must restart.

### Mechanism 2: Normalization of Heterogeneous Metrics
The Weighted Decision Matrix aggregates classification metrics (ratios) with resource constraints (time) by normalizing all metrics to [0,1] scale. Efficiency (time) uses reverse Min-Max normalization to map runtime where higher values indicate better performance. This enables objective ranking across diverse model architectures.

### Mechanism 3: Contextual Feature Representation
ALBERT uses Multi-Head Self-Attention to capture document-level semantics through bidirectional context, detecting long-range dependencies missed by bag-of-words approaches. While computationally expensive, this mechanism provides superior sentiment understanding for complex documents with sufficient context interdependence.

## Foundational Learning

- **Concept: Multi-Criteria Decision Making (MCDM)**
  - Why needed: The problem requires selecting the "best overall" model given eight competing constraints, not just highest accuracy
  - Quick check: If Model A has 90% accuracy in 1s, and Model B has 91% accuracy in 100s, which is "better"? MCDM provides math-based answer

- **Concept: Matthews Correlation Coefficient (MCC)**
  - Why needed: The framework prioritizes MCC (weight 0.237) over Accuracy (0.079) because MCC is more robust to class imbalance
  - Quick check: Why would Accuracy be misleading for a dataset where 95% of samples are "Positive"?

- **Concept: Overfitting in Sequence Models**
  - Why needed: The framework explicitly uses Early Stopping for LSTM and ALBERT to prevent memorization
  - Quick check: Why does ALBERT risk memorizing training data on medium-sized datasets more than Naive Bayes?

## Architecture Onboarding

- **Component map:** Preprocessing (cleaning, tokenization) -> Feature Extraction (TF-IDF vs. embeddings) -> Model Zoo (7 baselines) -> Evaluation Engine (8 metrics) -> CPC-Selector (POM weights)
- **Critical path:** The CPC Weighting process is critical - if POM inputs are inconsistent (AI > 0.1), final ranking is invalid regardless of model performance
- **Design tradeoffs:** ALBERT vs. XGBoost: ALBERT offers maximum semantic understanding but minimum efficiency; XGBoost offers high performance with drastically lower time consumption
- **Failure signatures:** Inconsistent POM (AI > 0.1), overfitting (validation accuracy diverges from training accuracy)
- **First 3 experiments:**
  1. Sanity Check: Run 7 models on small subset to verify Naive Bayes is fast but less accurate, ALBERT is slow but accurate
  2. Weight Sensitivity Analysis: Modify POM to heavily weight "Efficiency" vs. "MCC" - does XGBoost become top choice?
  3. Cross-Dataset Validation: Train top performer (ALBERT) and efficiency winner (XGBoost) on different datasets to check trade-off consistency

## Open Questions the Paper Calls Out

- Can the CPC-CMS framework be effectively generalized to classification applications in domains other than document-level sentiment analysis?
- How does the CPC-CMS framework perform when applied to sentence-level or aspect-level sentiment analysis?
- To what extent does the variance in expert judgment regarding criteria weights affect the final model selection?

## Limitations
- Framework relies heavily on expert judgment encoded through POM tables, introducing subjectivity
- Computational efficiency of ALBERT may not be feasible for resource-constrained environments
- Framework's performance across diverse domains beyond social media remains untested

## Confidence
- **High Confidence:** Methodology for constructing weighted decision matrix and normalizing heterogeneous metrics
- **Medium Confidence:** ALBERT's superiority in semantic understanding and efficiency trade-offs
- **Low Confidence:** Reliability of expert judgment in POM process and potential biases in expert inputs

## Next Checks
1. Apply CPC-CMS to non-social media datasets (medical or legal text) to test generalizability
2. Conduct sensitivity analysis by varying POM inputs to measure impact on model rankings
3. Evaluate framework performance on low-resource hardware to validate ALBERT efficiency trade-offs