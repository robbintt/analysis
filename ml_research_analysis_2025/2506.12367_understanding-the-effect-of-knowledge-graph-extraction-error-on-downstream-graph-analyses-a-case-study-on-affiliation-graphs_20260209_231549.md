---
ver: rpa2
title: 'Understanding the Effect of Knowledge Graph Extraction Error on Downstream
  Graph Analyses: A Case Study on Affiliation Graphs'
arxiv_id: '2506.12367'
source_url: https://arxiv.org/abs/2506.12367
tags:
- extraction
- error
- networks
- errors
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how errors in knowledge graph (KG) extraction
  from text affect downstream graph analyses. The authors conduct both micro-level
  evaluations (precision, recall, F1) and macro-level analyses (centrality, clustering,
  degree distribution) using real-world affiliation graphs extracted from historical
  social register books.
---

# Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs

## Quick Facts
- arXiv ID: 2506.12367
- Source URL: https://arxiv.org/abs/2506.12367
- Authors: Erica Cai; Brendan O'Connor
- Reference count: 22
- Key outcome: KG extraction errors systematically bias downstream graph metrics, with F1 > 0.92 maintaining relative MAE < 0.08 for most metrics

## Executive Summary
This paper investigates how errors in knowledge graph extraction from text propagate to affect downstream graph analyses. Using real-world affiliation graphs extracted from historical social register books, the authors find that as extraction performance declines, graph metrics show consistent directional biases rather than random noise. Most metrics exhibit predictable over- or under-estimation patterns, with extraction F1 above 0.92 maintaining relative mean absolute errors below 8% for most metrics. Manual error analysis reveals that extraction errors frequently add nodes through misspellings or entity misidentification, leading to inflated graph sizes.

## Method Summary
The authors extract ⟨Person, member, Club⟩ tuples from historical social register books using a pipeline combining OCR (DocumentAI, Textract) with LLM extraction (Gemini, Claude). They conduct micro-level evaluations using precision, recall, and F1 at the tuple level, then perform macro-level analyses on the resulting graphs measuring centrality, clustering, and degree distribution. Three real-world datasets are analyzed with ground truth knowledge graphs available for comparison. The study also compares real extraction errors against simulated error models to understand bias patterns.

## Key Results
- Extraction errors systematically inflate node counts more than edge counts, reducing graph density
- Most graph metrics show consistent directional biases (over/under-estimation) as extraction F1 decreases
- F1 threshold of 0.92 maintains relative MAE below 0.08 for most graph metrics
- Simple error models fail to replicate real bias patterns, overestimating bias by 4× or more
- Manual error analysis shows 67-100% of false positives create new nodes through misspellings or entity conflation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KG extraction errors from text systematically inflate node counts more than edge counts, producing graphs with artificially lower density.
- Mechanism: Misspellings create new "ghost" nodes (e.g., "BELLASIS" vs. "BELLASISS"), entity conflation merges distinct nodes, and misidentified nearby text introduces spurious entities. These errors add nodes without proportionally adding edges, reducing edge-to-node ratios.
- Core assumption: The observed error patterns in affiliation graphs generalize to other bipartite KG extraction tasks.
- Evidence anchors:
  - [abstract] "Manual error analysis reveals that extraction errors frequently add nodes through misspellings or entity misidentification, leading to inflated graph sizes."
  - [Section 3, Table 2] Error categorization shows 67-100% of false positives in Denver/Tennessee/Rhodesia create new nodes.
  - [corpus] LightKGG (arXiv:2510.23341) confirms LLM-based KG extraction remains error-prone; AffilKG dataset paper (arXiv:2505.10798) provides the ground truth used here.
- Break condition: If extraction methods achieve near-perfect entity linking or use constrained vocabularies, node inflation may diminish.

### Mechanism 2
- Claim: Downstream graph metrics exhibit consistent directional bias (not random noise) as extraction F1 decreases.
- Mechanism: Systematic error types (spurious nodes, redirected edges) create predictable structural distortions—density and clustering are consistently underestimated; connected components and communities are overestimated. Bias direction is reproducible across datasets.
- Core assumption: Bias patterns observed in social register affiliation graphs transfer to other domains with similar error profiles.
- Evidence anchors:
  - [abstract] "many metrics exhibit increasingly pronounced biases, with each metric tending toward a consistent direction of either over- or under-estimation"
  - [Section 5, Tables 5-7] Relative bias values show consistent signs (positive/negative) across F1 ranges; MAE closely tracks bias magnitude.
  - [corpus] Weak external validation—no corpus papers directly replicate this bias-direction finding; this is a novel contribution.
- Break condition: If errors become truly random (not systematic), bias direction would vary; if multiple error types cancel out, net bias could approach zero despite low F1.

### Mechanism 3
- Claim: An extraction F1 threshold of ~0.92 yields downstream metric errors below 8% relative MAE for most metrics.
- Mechanism: At high F1, the absolute count of false positive edges and spurious nodes is low enough that structural properties (density, clustering, path lengths) remain within narrow tolerance of ground truth.
- Core assumption: The F1→bias relationship is approximately monotonic; threshold generalizes to other bipartite graph types.
- Evidence anchors:
  - [abstract] "when F1 exceeds 0.92, relative mean absolute errors stay below 0.08 for most metrics"
  - [Section 5, Tables 5-7] F1 ∈ [0.92, 1.00) shows rel. MAE ≤ 0.08 for density, clustering, degree statistics; higher for component counts.
  - [corpus] No corpus papers validate this specific threshold; future replication needed.
- Break condition: If error types shift (e.g., more false negatives, fewer spurious nodes), the F1→downstream-error mapping may change.

## Foundational Learning

- Concept: **Bipartite/Affiliation Graphs**
  - Why needed here: The entire analysis operates on ⟨Person, member, Club⟩ tuples forming two-mode networks; metrics like projection density and bipartite clustering assume this structure.
  - Quick check question: Can you explain why projecting a bipartite graph onto one node type creates edges between nodes that share neighbors?

- Concept: **Precision, Recall, F1 for Edge Tuples**
  - Why needed here: Micro-level extraction evaluation uses tuple-level matching; F1 serves as the independent variable for predicting downstream bias.
  - Quick check question: If precision is 0.7 and recall is 0.9, what is F1? (Answer: 2×0.7×0.9 / (0.7+0.9) ≈ 0.788)

- Concept: **Graph Metrics: Density, Clustering, Connected Components**
  - Why needed here: These are the downstream metrics measured for bias; understanding their definitions is prerequisite to interpreting error propagation.
  - Quick check question: Does adding isolated nodes (with no edges) increase or decrease graph density?

## Architecture Onboarding

- Component map:
  OCR Layer -> LLM Extraction Layer -> Entity Normalization -> Graph Construction -> Evaluation Module

- Critical path: OCR quality → LLM extraction accuracy → node inflation → downstream metric bias. OCR errors compound through the pipeline.

- Design tradeoffs:
  - Simple error models (random edge perturbation) are easy to implement but fail to replicate real bias patterns.
  - Node-level error models (misspelling simulation, spurious node addition) better match bias direction but overestimate magnitude by 4×+.
  - Using F1 as sole performance indicator collapses precision/recall tradeoffs; low-precision and low-recall systems may have similar F1 but different bias profiles.

- Failure signatures:
  - Extracted graph has significantly more nodes than ground truth (check |V_pred| / |V_true|).
  - Bipartite density is substantially lower than ground truth despite edge count being similar.
  - Connected component count is inflated; largest component proportion is reduced.

- First 3 experiments:
  1. **Baseline extraction**: Run 2-3 OCR+LLM combinations on a single book; measure precision, recall, F1 and compare node/edge counts to ground truth.
  2. **Metric sensitivity**: For each extracted graph, compute density, clustering coefficient, and connected component count; calculate relative bias vs. ground truth.
  3. **Error model comparison**: Apply random edge error and node-disaggregation error models to ground truth graph at matched precision/recall; compare bias direction and magnitude to real extraction errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mixed-error models that combine multiple interacting error types (misspellings, entity conflation, spurious nodes) accurately replicate the bias magnitude and direction observed in real KG extraction?
- Basis in paper: [explicit] "Future work could develop mixed-error models that reflect the full diversity and interactions of real world error sources observed in KG extraction from text."
- Why unresolved: Current models simulate only single error types; even tailored models overestimate bias by at least 4x.
- What evidence would resolve it: Develop mixed-error simulation models and demonstrate that resulting bias patterns match real extraction errors within acceptable tolerance.

### Open Question 2
- Question: Does the F1 ≥ 0.92 threshold for maintaining relative MAE < 0.08 generalize across diverse text domains, languages, and graph types beyond affiliation networks?
- Basis in paper: [explicit] "Future work could refine this with more granular performance metrics and by validating these findings on new datasets as more ground truth KGs become available."
- Why unresolved: Study only examined affiliation graphs from English-language historical social registers.
- What evidence would resolve it: Replicate analysis on diverse KGs (e.g., scientific, legal, biomedical) and test whether the same F1 threshold holds.

### Open Question 3
- Question: What extraction performance metrics beyond F1 (e.g., per-entity-type precision, node-level accuracy) better predict downstream metric biases?
- Basis in paper: [inferred] The authors find F1 useful but note that error types (spelling vs. entity confusion) have different structural impacts, suggesting aggregate F1 may obscure predictive signal.
- Why unresolved: Precision and recall aggregate across all errors, but error type distributions affect bias differently.
- What evidence would resolve it: Correlate fine-grained error-type-specific metrics with downstream bias across extraction methods to identify better predictors.

## Limitations

- Analysis restricted to bipartite affiliation graphs from social registers, limiting generalizability to other KG domains
- Small sample size (3 books) limits statistical power for drawing universal conclusions
- Specific extraction pipeline (OCR + LLM) may not represent all KG extraction approaches

## Confidence

- **High confidence**: The directional bias patterns observed (node inflation, density underestimation) are robust across all three datasets studied
- **Medium confidence**: The F1 > 0.92 threshold for <8% relative MAE applies specifically to the studied graph metrics and may not generalize to other metrics or graph types
- **Low confidence**: Claims about broader generalizability to other KG extraction tasks beyond affiliation graphs

## Next Checks

1. **Cross-domain replication**: Apply the same analysis framework to KG extraction from scientific papers or news articles to test if the F1 > 0.92 threshold holds for different graph structures and error profiles

2. **Error model refinement**: Develop and validate more sophisticated error models that capture systematic biases observed in real extraction errors, particularly focusing on entity conflation and context-dependent disambiguation failures

3. **Precision-recall tradeoff analysis**: Conduct experiments isolating precision from recall effects on downstream metrics to determine if F1 alone adequately captures the relationship between extraction quality and analysis reliability