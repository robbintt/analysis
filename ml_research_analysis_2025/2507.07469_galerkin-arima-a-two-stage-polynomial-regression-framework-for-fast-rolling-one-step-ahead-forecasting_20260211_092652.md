---
ver: rpa2
title: 'Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling
  One-Step-Ahead Forecasting'
arxiv_id: '2507.07469'
source_url: https://arxiv.org/abs/2507.07469
tags:
- arima
- galerkin-arima
- basis
- series
- galerkin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Galerkin-ARIMA replaces the linear autoregressive component of
  ARIMA with a flexible spline-based function approximated via Galerkin projection,
  retaining the moving-average structure. The model is estimated in closed form through
  two-stage ordinary least squares, avoiding iterative maximum-likelihood optimization.
---

# Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting

## Quick Facts
- **arXiv ID**: 2507.07469
- **Source URL**: https://arxiv.org/abs/2507.07469
- **Reference count**: 6
- **Primary result**: Galerkin-ARIMA matches classical ARIMA's forecasting accuracy while achieving 20× faster computation via closed-form two-stage OLS estimation.

## Executive Summary
Galerkin-ARIMA introduces a two-stage polynomial regression framework that replaces ARIMA's linear autoregressive component with a flexible spline-based function approximated via Galerkin projection, retaining the moving-average structure. The method achieves closed-form estimation through ordinary least squares, avoiding iterative maximum-likelihood optimization required by classical ARIMA. Theoretical analysis establishes asymptotic unbiasedness and consistency under smoothness and basis-growth conditions, with computational complexity reduced to O(pNK) + O(N(K+q)²). Extensive simulations on four synthetic processes demonstrate that Galerkin-ARIMA matches classical ARIMA's forecasting accuracy while achieving orders-of-magnitude faster computation.

## Method Summary
Galerkin-ARIMA operates by representing the AR component as m_t = Φ(x_t)^T β where Φ(x_t) is a vector of K basis functions evaluated at lag vector x_t. The method uses Galerkin orthogonality to transform the approximation problem into a linear-in-parameters regression. Estimation proceeds in two stages: Stage 1 regresses the differenced series onto basis functions to obtain initial residuals, and Stage 2 constructs a combined design matrix with lagged residuals to jointly estimate both AR and MA components via OLS. This approach avoids the iterative MLE optimization required by classical ARIMA while retaining provable asymptotic properties.

## Key Results
- Galerkin-ARIMA achieves MAE ≈ 0.43–0.94 and RMSE ≈ 0.62–1.08, matching classical ARIMA's accuracy across all four synthetic processes
- Computational complexity reduces to O(pNK) + O(N(K+q)²), yielding 20× speedup (per-fit time dropping from ~0.01s to ~0.0005s)
- Method successfully captures both linear and nonlinear temporal dynamics while maintaining closed-form estimation
- Accuracy plateaus quickly with basis size K while computation grows with K², demonstrating efficient approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the linear autoregressive component with a flexible spline-based function approximated via Galerkin projection enables capture of nonlinear temporal dependencies while retaining closed-form estimation.
- Mechanism: The method represents the AR component as m_t = Φ(x_t)^T β where Φ(x_t) ∈ R^K is a vector of K basis functions evaluated at the lag vector x_t. Galerkin orthogonality requires the approximation residual to be orthogonal to each basis function, yielding a linear system Gθ = f solvable via OLS. This transforms the potentially nonlinear lag-response relationship into a linear-in-parameters regression problem.
- Core assumption: The underlying lag-response function f: R^p → R is sufficiently smooth (belongs to a Hölder class with smoothness index r) and can be well-approximated by the chosen basis on a compact domain.
- Evidence anchors: [abstract] "By replacing the fixed linear autoregressive component with a spline-based basis expansion, Galerkin-ARIMA flexibly approximates the underlying relationship among past values via ordinary least squares"; [Section 3.1] "the approximation error can be made arbitrarily small as K grows... inf sup |f(x) - Φ(x)^T β| = O(K^{-r/p})"; [corpus] Related work on ARIMA-polynomial hybridization (arXiv:2505.06874) similarly finds basis expansions improve forecasting over pure linear ARIMA.
- Break condition: If the true dynamics are discontinuous, have sharp transitions, or exhibit chaos, the smoothness assumption fails and bias may not decrease with increasing K.

### Mechanism 2
- Claim: Two-stage ordinary least squares estimation avoids iterative maximum-likelihood optimization while recovering both AR and MA components with provable asymptotic properties.
- Mechanism: Stage 1 regresses y^(d)_t onto Φ(x_t) to obtain initial residuals ê^(0)_t. Stage 2 constructs a combined design matrix Ψ with rows [Φ(x_t)^T | ê^(0)_{t-1:t-q}] and solves the augmented OLS problem ˆγ = (Ψ^T Ψ)^{-1} Ψ^T Y. The lagged residuals proxy for the unobserved MA innovations, enabling joint estimation.
- Core assumption: The initial Stage 1 residuals provide sufficiently good proxies for the true innovations ε_t; Gaussian innovations with mean 0 and variance σ².
- Evidence anchors: [abstract] "We derive closed-form solutions for both the AR and MA components using two-stage Galerkin projections"; [Section 3.1] "Estimating the coefficients in this Galerkin framework can be done in closed form using least squares, thanks to the linearity in the parameters β"; [corpus] Limited direct corpus evidence for two-stage OLS for ARMA; most related work uses iterative MLE or neural approaches.
- Break condition: If Stage 1 residuals are poor proxies (e.g., severe underfitting with too few basis functions), Stage 2 MA coefficient estimates become biased.

### Mechanism 3
- Claim: Computational complexity reduction from O(I·(p+q)N) to O(pNK) + O(N(K+q)²) enables orders-of-magnitude faster rolling forecasts for moderate basis sizes.
- Mechanism: Classical ARIMA requires I iterations of likelihood evaluation (each O((p+q)N)), where I depends on optimizer convergence. Galerkin-ARIMA replaces this with two matrix operations: basis evaluation (O(pNK)) and OLS solves (O(N(K+q)²)). For K << N and moderate K relative to (p+q)², the ratio O_ARIMA/O_GAL ≈ I(p+q)/(K+q)² favors the closed-form approach.
- Core assumption: The basis dimension K remains moderate (paper uses K ≈ 2p+1 for polynomial basis with linear and quadratic terms); the number of optimizer iterations I in MLE is non-trivial (typically I > 10).
- Evidence anchors: [abstract] "computational complexity reduced to O(pNK) + O(N(K+q)²), substantially lower than classical ARIMA for moderate basis sizes"; [Section 4.3] "average per-fit time dropping from ~0.01 s to ~0.0005 s" (20× speedup observed in simulations); [corpus] Corpus papers on ARIMA hybridization do not report comparable closed-form speedups; most use iterative or neural methods with higher overhead.
- Break condition: If K grows large (e.g., K ∼ N^{0.5} or higher), OLS matrix inversion costs dominate and speedup diminishes or reverses.

## Foundational Learning

- Concept: **Galerkin Projection**
  - Why needed here: Core mathematical machinery for approximating unknown functions by projecting onto finite-dimensional basis function spaces.
  - Quick check question: Given basis functions ϕ₁(x) = 1, ϕ₂(x) = x, and inner product ⟨f,g⟩ = ∫f(x)g(x)dx, write the Galerkin orthogonality conditions for approximating f(x) = e^x.

- Concept: **Bias-Variance Tradeoff in Nonparametric Regression**
  - Why needed here: Determines how to select basis size K—too small yields approximation bias, too large yields estimation variance.
  - Quick check question: If MSE = Bias² + Variance, and Bias = O(K^{-r/p}) while Variance = O(K/N), what growth rate of K minimizes asymptotic MSE?

- Concept: **ARIMA Model Structure and MLE**
  - Why needed here: Provides the baseline being extended; understanding why ARIMA requires iterative optimization clarifies the advantage of closed-form alternatives.
  - Quick check question: In an ARMA(1,1) model y_t = ψy_{t-1} + θε_{t-1} + ε_t, why does the likelihood depend on all past innovations, preventing closed-form MLE?

## Architecture Onboarding

- Component map:
  Raw series y_t → Differencing (d times) → y^(d)_t → Lag vector x_t = [y^(d)_{t-1}, ..., y^(d)_{t-p}] → Basis evaluation Φ(x_t) ∈ R^K → Stage 1 OLS: ˆβ = (Φ^T Φ)^{-1} Φ^T Y → Initial residuals ê^(0)_t → Build lagged residual vectors ê^(0)_{t-1:t-q} → Stage 2 OLS on Ψ = [Φ | lagged residuals] → ˆγ = (ˆβ, ˆθ) → Forecast: ŷ^(d)_{t+1} = Φ(x_{t+1})^T ˆβ + Σ ˆθ_j ê_{t+1-j}

- Critical path: Basis function evaluation → Stage 1 OLS → Residual construction → Stage 2 OLS. The choice of basis (type and K) directly controls both approximation quality and computational cost.

- Design tradeoffs:
  - Larger K: Lower approximation bias, higher variance, slower computation
  - Smaller K: Faster computation, risk of underfitting nonlinear dynamics
  - Basis type (polynomial vs. B-spline): Polynomial is simpler but may extrapolate poorly; B-splines offer local control but require knot placement
  - Orders (p, q): Must span true dynamics; paper shows (p, q) = (1, 5) or (5, 1) recover ARMA(2,1) well, but pure MA or pure AR underperform

- Failure signatures:
  - Forecasts lag behind true values by ~1 step: Typical when p or q too small to capture feedback dynamics
  - Forecasts flatten/over-smooth oscillations: Basis insufficient to capture periodicity; occurs when p < seasonal period
  - Trend systematically misestimated: No AR component to model drift; pure MA (p=0) fails on trending series
  - Numerical instability in OLS: K too large relative to N, causing (Ψ^T Ψ) near-singular

- First 3 experiments:
  1. **Basis size sweep**: On the Noisy_ARMA data, fit Galerkin-ARIMA(2, 1) with K ∈ {3, 5, 10, 20, 50} polynomial basis functions. Record MAE, RMSE, and per-fit time. Verify the paper's finding that accuracy plateaus quickly while computation grows with K².
  2. **Order misspecification test**: On Seasonal sine data (period=20), compare (p, q) ∈ {(1, 0), (5, 0), (0, 5), (5, 5)} with K fixed. Confirm that p ≥ 5 is needed to capture seasonality and that pure MA (p=0) fails regardless of q.
  3. **Rolling window scaling**: Generate series of lengths N ∈ {100, 500, 1000, 5000} from the Nonlinear recursion process. Fit both classical ARIMA(5, 1) via MLE and Galerkin-ARIMA(5, 1) with K=11. Plot total fitting time vs. N for each method to verify the paper's claimed orders-of-magnitude speedup scales with series length.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited to synthetic data; performance on real-world datasets with complex dynamics remains untested
- The two-stage OLS approach depends critically on Stage 1 residuals being good proxies for true innovations, which may fail with severe underfitting
- Computational advantage diminishes when basis size K becomes large relative to series length N, limiting scalability for very complex dynamics

## Confidence

- **High confidence**: The theoretical framework (Galerkin orthogonality → closed-form OLS) is mathematically sound and the two-stage estimation algorithm is correctly specified. The polynomial basis construction and ARMA/ARIMA model definitions are standard.
- **Medium confidence**: The empirical simulation results (MAE, RMSE, timing) appear internally consistent and the reported speedups align with the stated O-complexity advantage. However, exact replication depends on precise basis indexing and out-of-sample residual handling, which are not fully detailed.
- **Low confidence**: Direct comparisons to neural or kernel methods are absent; it's unclear whether Galerkin-ARIMA would retain its efficiency and accuracy advantage against modern alternatives in more complex settings.

## Next Checks

1. **Real-world robustness**: Apply Galerkin-ARIMA to a benchmark dataset (e.g., M3 competition monthly series). Compare forecasting accuracy and speed against both classical ARIMA and at least one neural method (e.g., N-BEATS). Track accuracy degradation/gain as K increases.

2. **Approximation error analysis**: For the Noisy_ARMA data, compute the true lag-response function f(lags) from known coefficients and compare it to the fitted basis expansion Φ(x_t)^T β̂. Plot approximation error decay as K grows and verify the O(K^{-r/p}) rate empirically.

3. **Scalability stress test**: On series lengths N ∈ {10³, 10⁴, 10⁵}, measure both per-fit and total rolling time for classical ARIMA vs. Galerkin-ARIMA across (p,q) ∈ {(1,1), (5,5), (10,10)}. Confirm that the O-complexity advantage scales as claimed and identify at what N or K the speedup plateaus or reverses.