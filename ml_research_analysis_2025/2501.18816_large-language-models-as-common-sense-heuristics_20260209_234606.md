---
ver: rpa2
title: Large Language Models as Common-Sense Heuristics
arxiv_id: '2501.18816'
source_url: https://arxiv.org/abs/2501.18816
tags:
- walk
- action
- actions
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method that uses large language models
  (LLMs) to guide local search for planning tasks in a virtual household environment.
  The approach leverages the LLM's world knowledge by using its output as a heuristic
  for Hill-Climbing Search, enhanced with solution estimates to guide the search.
---

# Large Language Models as Common-Sense Heuristics

## Quick Facts
- arXiv ID: 2501.18816
- Source URL: https://arxiv.org/abs/2501.18816
- Reference count: 20
- The paper introduces a novel method that uses large language models (LLMs) to guide local search for planning tasks in a virtual household environment, achieving a 22 percentage point improvement in task success rate over similar systems.

## Executive Summary
This paper presents a method that leverages large language models as common-sense heuristics for planning tasks in a virtual household environment. The approach uses the LLM's world knowledge to guide Hill-Climbing Search, enhanced with solution estimates, to achieve consistently executable plans. By operating directly in the VirtualHome language, the method eliminates the need for intermediate translation steps and demonstrates significant improvements in task success rates over similar systems.

## Method Summary
The method uses an LLM (specifically gpt-4o-mini-2024-07-18) as a heuristic to guide Hill-Climbing Search in a virtual household environment called VirtualHome. The LLM partitions applicable actions into sublists of 100, selects candidates from each sublist, and then chooses the final action from these candidates. The system can use either "Low-Level Guides" (VirtualHome language) or "High-Level Guides" to steer the search. The approach operates directly in the VirtualHome language, eliminating the need for an intermediate translation step. Temperature is set to 0.2, with a maximum solution length of 20 and a maximum of 10 repeated queries.

## Key Results
- Achieves 22 percentage point improvement in task success rate over similar systems
- Generates consistently executable plans without intermediate translation steps
- Successfully handles 10 household tasks in the VirtualHome environment

## Why This Works (Mechanism)
The method works by leveraging the LLM's extensive world knowledge as a common-sense heuristic. By using the LLM to evaluate and select actions during the search process, the system can make more informed decisions than traditional planning algorithms. The partitioning of actions into manageable sublists allows the LLM to focus on smaller decision spaces while still maintaining global context through the guide system.

## Foundational Learning
- **VirtualHome Environment**: A simulated household environment with 450 objects and ~14k relationships where planning tasks are executed. (Why needed: Provides the realistic simulation context for testing the planning approach. Quick check: Verify the environment loads correctly with all objects and relationships.)
- **STRIPS-like Planning Domain**: A formal representation of actions and their preconditions/effects used for planning. (Why needed: Defines the formal action space the LLM must navigate. Quick check: Confirm all action schemas match the expected format.)
- **Hill-Climbing Search**: A local search algorithm that iteratively moves toward better solutions. (Why needed: Provides the basic search framework that the LLM guides. Quick check: Verify the search loop correctly evaluates goal conditions.)
- **Action Partitioning Strategy**: The method of dividing applicable actions into sublists of 100 for LLM evaluation. (Why needed: Makes LLM reasoning tractable while maintaining coverage. Quick check: Confirm action lists are correctly split and processed.)
- **Error Correction Module**: Logic to handle cases where LLM returns mismatched action indices. (Why needed: Ensures robustness against LLM output inconsistencies. Quick check: Test the module by providing deliberately mismatched responses.)

## Architecture Onboarding

**Component Map:**
VirtualHome Environment -> Action Space Generator -> LLM Query Interface -> Error Correction -> State Transition -> Goal Checker -> Search Loop

**Critical Path:**
1. Environment initialization with specific task setup
2. Action space generation from current state
3. LLM-guided action selection through partitioning
4. State transition and goal evaluation
5. Loop until success, failure, or limit

**Design Tradeoffs:**
- **Direct Language Operation**: Eliminates translation overhead but requires precise language specification
- **Action Partitioning**: Makes LLM reasoning tractable but may miss optimal actions across partitions
- **Hill-Climbing**: Simple and efficient but vulnerable to local maxima
- **Temperature 0.2**: Balances exploration with consistent decision-making

**Failure Signatures:**
- LLM selects semantically correct but formally invalid actions (e.g., DROP vs PUTIN)
- Mismatched action indices that error correction fails to resolve
- Search gets stuck in local maxima due to irreversible actions
- Goal conditions not satisfied despite seemingly correct action sequences

**Exactly 3 First Experiments:**
1. Run a single simple task (e.g., "microwave salmon") through the complete pipeline to verify all components interact correctly
2. Test the error correction module by providing deliberately mismatched LLM responses to ensure it recovers appropriately
3. Compare the LLM-guided search against random action selection on a simple task to validate the heuristic's value

## Open Questions the Paper Calls Out
**Open Question 1**: Can techniques like Retrieval-Augmented Generation (RAG) successfully pass formal environment descriptions to the LLM to resolve discrepancies between its internal world model and the actual simulation dynamics? The paper suggests this could address the current reliance on the LLM's parametrised knowledge, which often mismatches specific environment constraints.

**Open Question 2**: Does incorporating Chain of Thought (CoT) decomposition during the search process improve performance on tasks requiring complex logical reasoning? The authors note that the current single-action querying approach hinders "big-picture reasoning" for logic-heavy tasks.

**Open Question 3**: Would replacing Hill-Climbing with a beam search algorithm effectively mitigate the risk of the agent getting stuck in local maxima or taking irreversible actions? The paper states that local search issues "could be mitigated with an algorithm like beam search."

## Limitations
- Relies on LLM's parametrised knowledge which may mismatch specific environment constraints
- Hill-Climbing approach lacks completeness guarantees and cannot backtrack from irreversible dead ends
- Performance sensitive to prompt quality and the LLM's ability to distinguish between semantically similar actions in the STRIPS domain

## Confidence
- 22 percentage point improvement over baselines: Medium confidence
- Consistently executable plans: Medium confidence
- LLM as common-sense heuristic effectiveness: Medium confidence

## Next Checks
1. Verify Initial State Reconstruction: Compare your reconstructed environment graphs for each task against the authors' released data to ensure object configurations match exactly.
2. Test Error Correction Module: Implement a diagnostic mode that logs all instances where the LLM returns mismatched indices, and verify the error correction logic successfully recovers valid actions.
3. Baseline Implementation Validation: Implement and run the LTL2DAgger baseline from the authors' evaluation to confirm your environment setup produces comparable baseline results before testing the LLM-guided approach.