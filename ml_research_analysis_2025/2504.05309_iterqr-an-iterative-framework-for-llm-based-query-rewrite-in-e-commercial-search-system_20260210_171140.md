---
ver: rpa2
title: 'IterQR: An Iterative Framework for LLM-based Query Rewrite in e-Commercial
  Search System'
arxiv_id: '2504.05309'
source_url: https://arxiv.org/abs/2504.05309
tags:
- query
- rewrites
- rewrite
- user
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IterQR addresses the challenge of improving e-commerce search by
  rewriting user queries to be more accurate and relevant. It introduces an iterative
  framework that leverages Large Language Models (LLMs) to generate, evaluate, and
  refine query rewrites.
---

# IterQR: An Iterative Framework for LLM-based Query Rewrite in e-Commercial Search System

## Quick Facts
- arXiv ID: 2504.05309
- Source URL: https://arxiv.org/abs/2504.05309
- Reference count: 20
- Primary result: Iterative LLM-based framework improves e-commerce search metrics by 0.21-0.34% in conversion rates

## Executive Summary
IterQR introduces an iterative framework for improving e-commerce search through LLM-based query rewriting. The system addresses the challenge of transforming ambiguous user queries into precise search terms by leveraging Large Language Models with domain-specific knowledge. Through a three-stage process of generation, online signal collection, and multi-task post-training, IterQR achieves significant improvements in both online conversion metrics and offline precision. The framework specifically targets the long-tail query problem by grounding LLM outputs in real user behavior data.

## Method Summary
IterQR operates through a three-stage iterative framework. First, it generates candidate rewrites using a Chain-of-Thoughts (CoT) approach enhanced with Retrieval-Augmented Generation (RAG) to incorporate domain-specific restaurant and cuisine data. Second, it collects online signals by attributing user clicks and purchases to specific rewrites, using these interactions as training labels. Third, it performs multi-task post-training on the LLM using three objectives: rewrite generation, quality classification, and relevance classification. The system uses Qwen2.5 as the base model and iterates this process three times, collecting positive rewrites from user interactions to refine the model progressively.

## Key Results
- Online performance: Order Volume (+0.27%), PV CXR (+0.34%), QV CXR (+0.27%), UV CXR (+0.21%)
- Offline precision: Precision@1/5/10 improvements over baselines
- Outperforms both keyword and semantic rewrite methods in efficiency metrics
- Ablation study shows multi-task training significantly improves performance

## Why This Works (Mechanism)

### Mechanism 1: Iterative Feedback Loop via Online Signal Collection
The framework bridges static LLM knowledge and dynamic e-commerce inventory by using real user interactions as ground truth filters. Generated rewrites are deployed online, and successful ones (leading to clicks/purchases) are flagged as positive signals. These successful rewrites become labels for the next round of LLM post-training, theoretically aligning the model closer to actual user intent over time.

### Mechanism 2: Domain Grounding via RAG and CoT
Performance gains derive from grounding the LLM in specific domain context (RAG) and forcing explicit reasoning (CoT) before generation. Instead of direct rewriting, the system retrieves associated restaurant/cuisine titles and forces a step-by-step internal monologue—identifying typos, intent, and meaning—before outputting the rewrite.

### Mechanism 3: Multi-Task Alignment for Quality and Relevance
Post-training the LLM on multiple objectives (Generation, Quality, Relevance) simultaneously constrains the model to produce rewrites that are not only syntactically correct but also commercially viable. Explicitly teaching the model to score relevance and quality acts as a regularizer, preventing drifting rewrites that match text but not user intent.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: IterQR relies on injecting dynamic, location-specific restaurant data into the prompt. Without RAG, the LLM would rely solely on pre-trained knowledge, insufficient for real-time inventory or local niche dishes.
  - Quick check: Can you explain how to format a retrieved database record into a prompt template so the LLM treats it as context rather than user input?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed: Used in Stage 1 to decompose the rewriting task (Typo Check → Intent Check → Rewrite). Critical for handling tail queries which are often ambiguous.
  - Quick check: How would you design a prompt to force a model to output intermediate reasoning steps before the final answer?

- **Concept: Supervised Fine-Tuning (SFT) / Post-Training**
  - Why needed: The core engine of the iterative loop is updating the LLM weights using the "positive rewrites" collected online. You need to understand how to structure a dataset (Instruction, Input, Output) for this fine-tuning.
  - Quick check: What is the risk of "catastrophic forgetting" if we fine-tune a general LLM exclusively on food delivery queries?

## Architecture Onboarding

- **Component map:** User Query → Retrieval Augmenter (fetches top-clicked restaurants/cuisines) → LLM Generator (Query + Context + CoT Prompt → Outputs Rewrites) → Search Engine (retrieval + rewrite channel) → Signal Collector (logs clicks attributed to rewrite channel) → Trainer (SFT pipeline using collected positive rewrites)

- **Critical path:** The "Online Signal Collection" (Stage 2). This is the unique value prop. If you cannot accurately attribute a user's click to the rewrite specifically (vs. original query or embedding retrieval), the feedback loop is broken.

- **Design tradeoffs:**
  - Latency vs. Quality: Using CoT and RAG increases prompt length and inference time. The paper uses a 7B parameter model (implied by Qwen2.5 usage), suggesting a tradeoff favoring speed over massive model intelligence.
  - Exploration vs. Exploitation: The system generates multiple rewrites (exploration) but relies on post-training to exploit successful patterns.

- **Failure signatures:**
  - Empty Results: Rewrites are too specific or hallucinated, matching no documents in the index.
  - Semantic Drift: The rewrite changes the intent (e.g., "Spicy Chicken" → "Chicken"), increasing clicks but lowering user satisfaction/conversion.
  - Feedback Loop Collapse: If "Level-1" signals are too rare, the training dataset grows too slowly to improve the model.

- **First 3 experiments:**
  1. Offline Precision Benchmark: Run the generator on a held-out set of tail queries. Measure if generated rewrites match a "ground truth" set of successful rewrites.
  2. Attribution Sanity Check: Before turning on the training loop, verify the "Level-1" signal logging. Ensure that when a user clicks a result only found by the rewrite, that specific rewrite is logged in the database.
  3. A/B Test on Rewrite Channel: Deploy the initial rewrites online but do not use them for training yet. Compare Order Volume and Conversion Rate against the baseline to ensure rewrites are not harming user experience.

## Open Questions the Paper Calls Out

- **Question 1:** How can the overfitting of the Relevance objective be mitigated during the iterative post-training process?
  - Basis: Section 5.2.2 states "We conjecture that Relevance is over fitted during multiple training," noting iterative training on Relevance did not improve the metric.
  - Evidence: A modification to the training schedule (e.g., dynamic loss weighting) that results in stable or improved Relevance scores across more than three iterations.

- **Question 2:** Does the reliance on online click signals for defining "positive" rewrites introduce a popularity bias that neglects valid rewrites for long-tail queries?
  - Basis: Section 4.2 defines "good" rewrites exclusively based on user interactions (clicks/purchases).
  - Evidence: An analysis of false negative rates among unclicked rewrites, or a comparison of semantic similarity scores versus click-based labels for low-frequency queries.

- **Question 3:** To what extent does the "Rewrite Quality" classification task conflict with the "Relevance" objective?
  - Basis: Table 3 indicates the model without iterative Relevance task achieves higher Relevance score compared to full model, suggesting a trade-off.
  - Evidence: A Pareto frontier analysis of the multi-task loss weights to determine if Relevance and Quality objectives can be optimized simultaneously without degradation.

## Limitations
- Unknown training hyperparameters (learning rate, batch size, epochs, LoRA vs full fine-tuning)
- Exact Qwen2.5 model variant/size used remains unspecified
- Reliance on another LLM (GPT-4o) for labeling quality and relevance introduces potential inconsistency
- Feedback loop stability over long-term deployment is unclear

## Confidence

- **High confidence:** Online metric improvements (order volume +0.27%, conversion rates +0.21-0.34%) are well-documented and specific
- **Medium confidence:** Multi-task post-training framework is clearly described, though impact of each objective is partially demonstrated through ablation
- **Medium confidence:** CoT+RAG generation approach is technically sound, but effectiveness depends heavily on quality of retrieval corpus
- **Low confidence:** Long-term stability of iterative framework is unclear, particularly regarding potential feedback loop collapse or semantic drift

## Next Checks

1. **Signal attribution validation:** Implement a controlled A/B test to verify that clicks can be reliably attributed to the rewrite channel versus the original query, using synthetic queries with known outcomes.

2. **Feedback loop stability test:** Run the iterative framework for 5+ cycles on a small dataset, monitoring not just performance metrics but also the diversity and novelty of generated rewrites to detect early signs of collapse.

3. **Semantic drift measurement:** Implement a human evaluation pipeline to check whether rewrites that increase clicks actually improve user satisfaction, measuring intent preservation across the rewrite chain.