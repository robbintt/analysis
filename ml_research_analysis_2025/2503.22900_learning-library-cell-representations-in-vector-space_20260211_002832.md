---
ver: rpa2
title: Learning Library Cell Representations in Vector Space
arxiv_id: '2503.22900'
source_url: https://arxiv.org/abs/2503.22900
tags:
- cell
- functional
- representations
- lib2vec
- cells
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lib2Vec is a self-supervised framework for learning meaningful
  vector representations of library cells that captures functional and electrical
  relationships. It generates training data from Liberty files and evaluates representations
  through automated regularity tests.
---

# Learning Library Cell Representations in Vector Space

## Quick Facts
- arXiv ID: 2503.22900
- Source URL: https://arxiv.org/abs/2503.22900
- Reference count: 20
- Primary result: Self-supervised framework learns vector representations capturing functional and electrical relationships between library cells

## Executive Summary
Lib2Vec introduces a self-supervised framework for learning meaningful vector representations of library cells from Liberty files. The framework uses an attention-based architecture to accommodate varying pin counts and produces property-specific embeddings for electrical characteristics. Through automated regularity tests, Lib2Vec achieves high accuracy in identifying functionally and electrically similar cells, with performance significantly exceeding random guessing. The learned representations reveal intuitive cell relationships and enhance downstream circuit learning applications, particularly when labeled data is scarce.

## Method Summary
Lib2Vec learns cell representations through self-supervised masked prediction tasks using Liberty file data. The framework generates four types of datasets: functional output prediction, electrical output prediction, and their difference prediction variants. Two separate attention-based models handle functional and electrical properties, with property-specific embeddings created by concatenating base embeddings with learnable property tokens. The models are trained on sampled input conditions from Liberty files and evaluated through automated regularity tests measuring functional and electrical similarity.

## Key Results
- Achieves up to 89% top-3 accuracy in electrical similarity tests
- Near-perfect accuracy in functional similarity tests
- Significantly outperforms random guessing baselines
- Improves downstream circuit learning applications, achieving 80% accuracy in logic output prediction with only 4 training samples

## Why This Works (Mechanism)

### Mechanism 1
Constraining model capacity (single-head attention, two-layer FC) forces semantic knowledge into embeddings rather than attention weights, improving transferability. Limited parameter count prevents the model from memorizing cell relationships in weights; instead, cell embeddings must encode the semantics to minimize loss.

### Mechanism 2
Difference prediction tasks capture relational structure that absolute-value prediction alone misses. By training on "how does cell A differ from cell B under identical inputs," the model learns comparative semantics, making the embedding space respect relative relationships.

### Mechanism 3
Property-specific embeddings via concatenation of base embedding + property token enable fine-grained electrical characterization without separate models. A base electrical embedding captures shared cell characteristics; concatenating a learnable property token and passing through Property-FCL produces specialized representations.

## Foundational Learning

- **Concept**: Self-supervised masked prediction (NLP-inspired)
  - Why needed here: Adapts "predict missing from context" to cells, treating (input conditions, output responses) as context for learning semantics
  - Quick check question: Can you explain why predicting masked words in NLP is analogous to predicting a cell's output given its inputs?

- **Concept**: Attention over variable-length sequences
  - Why needed here: Library cells have different pin counts; attention naturally handles variable input/output configurations without padding
  - Quick check question: Why does attention handle variable-length inputs better than fixed-size MLPs?

- **Concept**: Liberty file format and NLDM lookup tables
  - Why needed here: Training data is extracted from Liberty files; understanding lookup tables is required to interpret the sampling strategy
  - Quick check question: What are the two axes of a typical NLDM delay lookup table, and how does Lib2Vec sample from them?

## Architecture Onboarding

- **Component map**: Liberty parser → input condition sampling → training dataset generation → model training → regularity test validation
- **Critical path**: Liberty file parsing → input condition sampling → training dataset generation → model training (functional: ~20 min, electrical: ~4 hrs on A100) → regularity test validation
- **Design tradeoffs**: Smaller embeddings (32-dim) maximize top-1 electrical accuracy (52%); larger embeddings (64-dim) improve top-10 inverting functionality (61%). Constrained attention (single-head) sacrifices training loss for representation quality and transferability.
- **Failure signatures**: Training destabilization or degraded regularity test accuracy → check if difference prediction datasets are included. Poor transfer to downstream tasks → verify embeddings, not attention weights, are being extracted and used.
- **First 3 experiments**: 1) Run all three test sets with random embeddings to establish baseline. 2) Train Lib2Vec with and without difference prediction tasks; compare regularity test accuracy. 3) Integrate pre-trained Lib2Vec embeddings into a GNN for logic prediction with 4–135 labeled samples; compare against random initialization.

## Open Questions the Paper Calls Out

### Open Question 1
How can the functional similarity evaluation be effectively extended to individual output pins of multi-output cells? The current methodology simplifies evaluation by focusing primarily on single-output cells.

### Open Question 2
How can Lib2Vec be integrated to establish a groundwork for broader circuit foundation models? While the paper demonstrates embeddings for library cells, it does not demonstrate integration into large-scale, multi-task circuit foundation models.

### Open Question 3
What are the limits and mechanisms of Lib2Vec's ability to enable few-shot learning in downstream applications? The paper shows improved accuracy with limited data but the specific mechanisms driving this transfer efficiency are not fully characterized.

## Limitations
- Hyperparameter sensitivity not reported, making performance assessment difficult
- Transfer learning generalization limited to specific downstream tasks
- Electrical property coverage limited to 6 properties, may not scale to real-world libraries

## Confidence

- **High confidence**: Regularity test methodology and results are well-specified and reproducible
- **Medium confidence**: Architectural claims supported by ablation but lack direct comparative analysis
- **Low confidence**: Mechanism explaining why difference prediction tasks improve representation quality is theoretically grounded but lacks empirical validation

## Next Checks

1. **Hyperparameter robustness test**: Systematically vary learning rate, batch size, and embedding dimensions to identify performance sensitivity and optimal configurations.

2. **Downstream task generalization**: Apply Lib2Vec embeddings to three additional circuit learning tasks and compare transfer learning benefits against random initialization.

3. **Property-specific scalability**: Extend the electrical model to handle 10+ additional Liberty properties and evaluate whether concatenation-based embeddings maintain accuracy.