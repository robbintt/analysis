---
ver: rpa2
title: 'Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach
  for Precise Wall Segmentation'
arxiv_id: '2512.02413'
source_url: https://arxiv.org/abs/2512.02413
tags:
- loss
- mitunet
- segmentation
- tversky
- floor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of precise wall segmentation
  in floor plan images for automated 3D reconstruction. Existing methods often fail
  to accurately detect thin walls and maintain geometric precision.
---

# Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation

## Quick Facts
- arXiv ID: 2512.02413
- Source URL: https://arxiv.org/abs/2512.02413
- Reference count: 28
- Achieves 87.84% mIoU on CubiCasa5k with precision >95% using hybrid Mix-Transformer+U-Net architecture

## Executive Summary
This paper addresses the challenge of precise wall segmentation in floor plan images for automated 3D reconstruction. Existing methods often fail to accurately detect thin walls and maintain geometric precision. The authors propose MitUNet, a hybrid architecture combining a Mix-Transformer encoder with a U-Net decoder enhanced by spatial and channel attention. This design captures global context while preserving fine structural details, optimized with Tversky loss for better precision-recall balance. Experiments show MitUNet outperforms standard models in boundary accuracy and structural fidelity while using less memory than comparable architectures.

## Method Summary
The authors develop MitUNet, a hybrid neural network that combines a Mix-Transformer (MiT-b4) encoder from SegFormer with a U-Net decoder enhanced by spatial and channel squeeze-excitation (scSE) blocks. The model uses asymmetric Tversky loss to balance precision and recall, particularly important for thin wall structures. Input images are resized to 512×512, and the model produces binary wall segmentation masks suitable for vectorization. The architecture is trained with annotation refinement that dilates door/window masks by ~30px and applies morphological closing to ensure clean wall boundaries.

## Key Results
- MitUNet achieves 87.84% mIoU on CubiCasa5k dataset, outperforming standard architectures
- Precision exceeds 95% with Tversky loss (α=0.6, β=0.4), compared to 92.54% with Dice loss
- Memory efficient at 1751 MiB, less than comparable architectures
- Successfully handles thin wall structures with sharp, vectorization-ready boundaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hybrid Transformer-CNN architecture enables simultaneous capture of global layout context and local thin-structure boundaries.
- **Mechanism:** The hierarchical Mix-Transformer encoder (MiT-b4) produces multi-scale features at resolutions {1/4, 1/8, 1/16, 1/32} through overlapping patch merging, which preserves local continuity for tracing continuous wall geometries. These features feed into a U-Net decoder with skip connections that progressively recover spatial details lost during downsampling.
- **Core assumption:** Thin wall structures require both global understanding of room topology and high-resolution local features for precise boundary delineation.
- **Evidence anchors:**
  - [abstract]: "hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks"
  - [Section 4.1]: "produces multi-scale features at resolutions of {1/4,1/8,1/16,1/32} relative to the input image... overlapping patch merging mechanism, which preserves local continuity"
  - [corpus]: FloorSAM similarly addresses geometric detail loss in floor plan reconstruction using attention-guided approaches.

### Mechanism 2
- **Claim:** The asymmetric Tversky loss function provides explicit control over the precision-recall trade-off, yielding sharper, vectorization-ready boundaries.
- **Mechanism:** Unlike symmetric losses, Tversky loss introduces separate penalty weights α for false positives and β for false negatives. Setting α=0.6, β=0.4 penalizes false positives more heavily, forcing the model toward conservative predictions that suppress boundary noise and wall dilation artifacts common with Dice loss.
- **Core assumption:** For downstream vectorization, false positives are more harmful than false negatives, as post-processing can interpolate gaps but cannot easily remove noise.
- **Evidence anchors:**
  - [abstract]: "optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery"
  - [Table 2]: Tversky (0.6/0.4) achieves 94.84% precision vs. Dice at 92.54%; Tversky (0.9/0.1) achieves 97.97% precision but drops recall to 82.57%
  - [Section 5.3, Figure 3]: "Dice Loss results in dilated wall thickness... Tversky yields the sharpest, thinnest boundaries"
  - [corpus]: No direct corpus evidence on Tversky loss for floor plans; related work focuses on architectural innovations.

### Mechanism 3
- **Claim:** Spatial and channel squeeze-excitation (scSE) blocks in the decoder adaptively recalibrate feature maps to emphasize wall-relevant pixels and semantic channels.
- **Mechanism:** The scSE module applies parallel spatial squeeze (highlighting pixels relevant to wall locations) and channel squeeze (emphasizing feature maps carrying structural vs. texture information). This ensures global context from the Transformer is translated into pixel-accurate local boundaries.
- **Core assumption:** Not all encoder features are equally relevant; attention mechanisms can suppress background clutter while amplifying structural signals.
- **Evidence anchors:**
  - [Section 4.1]: "scSE module adaptively recalibrates the feature maps: Spatial Squeeze—Highlights pixels relevant to wall locations, suppressing background noise. Channel Squeeze—Emphasizes feature maps that carry the most relevant semantic information"
  - [Table 1]: U-Net (scSE) with ResNet50 achieves 86.25% mIoU vs. standard architectures without attention showing lower performance
  - [corpus]: CFA U-Net demonstrates similar attention-fusion benefits in seismic horizon segmentation.

## Foundational Learning

- **Concept: Transformer attention and hierarchical feature extraction**
  - **Why needed here:** The Mix-Transformer encoder replaces CNN backbones to capture long-range dependencies. Understanding multi-head self-attention and patch merging is essential for debugging encoder outputs.
  - **Quick check question:** Can you explain how overlapping patch merging differs from standard ViT patch partitioning, and why it matters for continuous structures?

- **Concept: Semantic segmentation loss functions (Dice, Focal, Lovasz, Tversky)**
  - **Why needed here:** The paper's core contribution involves loss function selection. You must understand why symmetric losses cause boundary dilation and how asymmetric penalties shift the precision-recall operating point.
  - **Quick check question:** Given α=0.7, β=0.3 in Tversky loss, will the model produce thicker or thinner walls compared to α=0.5, β=0.5? Why?

- **Concept: Skip connections and decoder design in U-Net**
  - **Why needed here:** The hybrid architecture relies on skip connections to recover spatial resolution. Understanding feature fusion at multiple scales is critical for modifying the decoder.
  - **Quick check question:** What information is lost when downsampling to 1/32 resolution, and how does the skip connection from 1/4 resolution recover it?

## Architecture Onboarding

- **Component map:** Input (512×512) → MiT-b4 Encoder → Multi-scale features {1/4, 1/8, 1/16, 1/32} → U-Net Decoder with scSE ← Skip connections → Output mask (512×512 binary wall segmentation)

- **Critical path:**
  1. Load MiT-b4 with ImageNet weights
  2. Replace SegFormer's MLP decoder with U-Net decoder (4 stages)
  3. Insert scSE blocks after each decoder convolution
  4. Implement annotation refinement: dilate door/window masks by ~30px, subtract from wall mask, apply morphological closing (5×5 kernel)
  5. Train with Tversky loss, batch size 4, Adam optimizer, ReduceLROnPlateau scheduler

- **Design tradeoffs:**
  - MiT-b4 vs. MiT-b5: b4 chosen for memory efficiency (1751 MiB) vs. b5 would require more VRAM but may capture finer textures
  - Precision vs. Recall: Tversky α=0.6/β=0.4 prioritizes precision (94.84%) for vectorization; α=0.7/β=0.3 yields higher precision (95.37%) but lower recall (90.80%)
  - Resolution 512×512: Balances thin structure visibility with batch size constraints on 16GB VRAM

- **Failure signatures:**
  - Dilated/thickened walls → Dice loss being used; switch to Tversky with α>β
  - Missing thin walls → α too high (>0.8); reduce α or increase β
  - Noise around boundaries → Focal loss being used; switch to Tversky
  - Discontinuous walls → Check annotation refinement; ensure door/window subtraction is properly dilated
  - Staircase artifacts in vectorization → Model trained with symmetric loss; retrain with Tversky

- **First 3 experiments:**
  1. **Baseline validation:** Replicate MitUNet with Tversky (0.6/0.4) on CubiCasa5k; verify mIoU ~76% before fine-tuning. Check that VRAM stays under 2GB.
  2. **Loss function ablation:** Train identical architecture with Dice, Lovasz, Focal, and Tversky (0.6/0.4); compare boundary sharpness visually and precision/recall metrics. Expect Tversky to show highest precision.
  3. **Domain adaptation test:** Pre-train on CubiCasa5k, fine-tune on target dataset with reduced LR (1e-5); measure improvement from scratch vs. pre-trained initialization. Paper reports mIoU improvement from 87.84% to 88.64% with pre-training.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary regional datasets for validation, limiting reproducibility
- Tversky loss configuration (α=0.6, β=0.4) is empirically chosen without sensitivity analysis across the full parameter space
- Mix-Transformer encoder increases computational complexity compared to pure CNN approaches

## Confidence
- **High Confidence:** The hybrid architecture combining Mix-Transformer encoder with U-Net decoder is technically sound and well-supported by the multi-scale feature extraction mechanism and skip connections.
- **Medium Confidence:** The Tversky loss function's effectiveness is demonstrated empirically, but the optimal parameter selection lacks rigorous ablation studies across different floor plan characteristics.
- **Medium Confidence:** The scSE attention mechanism's contribution is validated through comparative experiments, though the specific architectural choices could benefit from more extensive exploration.

## Next Checks
1. **Loss Function Sensitivity Analysis:** Systematically vary α and β parameters in Tversky loss (e.g., 0.3/0.7, 0.7/0.3, 0.8/0.2) and evaluate impact on boundary precision, recall, and downstream vectorization quality across multiple floor plan datasets.
2. **Cross-Dataset Generalization Test:** Train MitUNet on CubiCasa5k and evaluate on at least two additional diverse floor plan datasets (different countries, architectural styles, drawing conventions) to assess true generalization capability.
3. **Attention Mechanism Ablation:** Remove scSE blocks and compare performance against full MitUNet to quantify the exact contribution of spatial and channel attention to wall segmentation accuracy and noise suppression.