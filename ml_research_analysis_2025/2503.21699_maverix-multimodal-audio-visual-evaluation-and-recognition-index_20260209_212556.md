---
ver: rpa2
title: 'MAVERIX: Multimodal Audio-Visual Evaluation and Recognition IndeX'
arxiv_id: '2503.21699'
source_url: https://arxiv.org/abs/2503.21699
tags:
- video
- audio
- multimodal
- understanding
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MA VERIX introduces a challenging benchmark for evaluating multimodal
  video-language understanding, featuring 2,556 questions from 700 videos that require
  tight integration of audiovisual information. The benchmark includes both eight-option
  multiple-choice and open-ended questions designed to assess causal reasoning, social
  interaction, and temporal understanding.
---

# MAVERIX: Multimodal Audio-Visual Evaluation and Recognition IndeX

## Quick Facts
- arXiv ID: 2503.21699
- Source URL: https://arxiv.org/abs/2503.21699
- Reference count: 25
- Introduces challenging benchmark requiring tight integration of audiovisual information, showing significant gap between state-of-the-art models (64% accuracy) and human experts (92.8%)

## Executive Summary
MAVERIX is a multimodal benchmark designed to evaluate video-language understanding by requiring tight integration of audiovisual information. The benchmark features 2,556 questions from 700 videos that span eight agentic categories including causal reasoning, social interaction, and temporal understanding. Through an iterative ablation methodology, questions are refined to eliminate unimodal shortcuts, forcing models to perform genuine cross-modal reasoning rather than relying on single-modality priors.

Experiments with state-of-the-art models like Qwen 2.5 Omni and Gemini 2.5 Flash-Lite reveal performance around 64% accuracy, significantly below human expert performance of 92.8%. The benchmark demonstrates that current models struggle particularly with long-form videos and socially grounded scenarios, highlighting the need for improved audiovisual integration in future multimodal systems.

## Method Summary
MAVERIX introduces a benchmark for multimodal video-language understanding through 2,556 questions derived from 700 videos (10s-63min duration). The dataset undergoes iterative refinement using GPT-4o-mini and Gemini 2.0-FL to eliminate unimodal shortcuts through ablation testing. Questions are evaluated in both multiple-choice (8 options) and open-ended formats, with open-ended responses scored by GPT-4o on five dimensions. The benchmark includes both localized segments and full-length videos (MAVERIX-Long) to test temporal reasoning capabilities.

## Key Results
- State-of-the-art models achieve approximately 64% accuracy on MCQ questions, significantly below human baseline of 92.8%
- Models struggle particularly with long-form videos (10-65 minutes), showing degraded performance on social and causal reasoning tasks
- Audio-capable models like Qwen 2.5 Omni show inconsistent benefits from native audio integration, with some models regressing when audio is added
- Performance drops more significantly on long videos for socially grounded tasks requiring fine-grained asynchronous cue integration

## Why This Works (Mechanism)

### Mechanism 1: Unimodal Shortcut Filtering
If questions are iteratively refined by testing whether they can be solved using only text, video, or subtitles in isolation, the resulting dataset forces models to perform joint audio-visual reasoning rather than relying on single-modality priors. An automated "adversarial" review loop uses GPT-4o-mini and Gemini 2.0-FL to attempt solving questions under ablated conditions. If a model succeeds without full modalities, the question is flagged and revised to increase modality interdependence.

### Mechanism 2: Temporal Horizon Expansion
Extending video context from localized clips to full-length videos degrades model performance specifically on social and causal reasoning tasks, indicating that current architectures struggle to integrate sparse, asynchronous cues over long durations. This challenges the model's attention mechanism to link cause (early video) and effect (later video) without losing context.

### Mechanism 3: Native Audio Integration vs. Text Proxies
Models ingesting raw audio waveforms are hypothesized to outperform those relying on ASR transcripts on non-speech tasks, although current results show inconsistent fusion benefits. Direct audio encoding allows the model to capture paralinguistic features and non-vocal sounds that are lost in text transcription, but if the multimodal fusion layer is poorly calibrated, adding audio can introduce noise or "modality competition."

## Foundational Learning

- **Cross-Modal Disambiguation**: Why needed here: Many benchmark questions fail if the model relies on just vision or just audio. Engineers must understand that valid reasoning requires correlating specific visual frames with specific audio timestamps to reject plausible but incorrect distractors. Quick check: Can your model determine if a sound is coming from an on-screen object or is background noise without relying on the video title or transcript?

- **The "Negative" Fusion Constraint**: Why needed here: Standard training maximizes likelihood of positive pairs. MAVERIX introduces "distractors" that are semantically close but factually wrong. Models must learn to suppress "plausible" outputs that don't match the precise audio-visual evidence. Quick check: If you show the model a video of silence but play a soundtrack of a crash, does it report a crash, or does it correctly identify the mismatch?

- **Long-Context Temporal Decay**: Why needed here: The performance gap between localized and full-length videos reveals that models lose track of causal chains over time. Quick check: Does your model's accuracy on "Why did X happen?" questions degrade linearly or exponentially as the distance between the cause-event and the question-prompt increases?

## Architecture Onboarding

- **Component map**: Raw A/V -> Encoder -> Projection -> Joint Attention -> Output
- **Critical path**: The bottleneck is the Fusion Layer. The paper notes VITA 1.5 regressed when audio was added, while Qwen 2.5 Omni improved. This suggests the alignment training is more critical than the encoder existence.
- **Design tradeoffs**: Native Audio vs. ASR (native captures music/tone but requires complex alignment training), Context Window vs. Granularity (sampling more frames helps visual reasoning but exhausts context windows needed for long videos)
- **Failure signatures**: Visually Induced Hallucination (model claims audio event occurred because it sees visual cue), Unimodal Bias (answering correctly using video-only but failing when audio is added), Temporal Drift (model defaults to generic descriptions rather than specific details in long videos)
- **First 3 experiments**: 1) Ablation Study (run benchmark in Video-Only, Audio-Only, and A+V modes), 2) Audio-Type Segmentation (evaluate performance specifically on "Music" and "Natural Sound" categories vs. "Speech"), 3) Temporal Localization Test (compare accuracy on "Localized" vs. "Full Length" videos)

## Open Questions the Paper Calls Out

### Open Question 1
What architectural or training advancements are necessary to close the substantial gap between current multimodal models (~64% accuracy) and human-level performance (92.8%) in socially grounded, agentic scenarios? The authors conclude that "a sizable gap to human performance remains, especially for socially grounded or dynamic scenarios," and hope the benchmark guides progress toward "socially intelligent models."

### Open Question 2
How can models be designed to prevent performance regression when integrating raw audio with video, ensuring that auditory signals are effectively fused rather than degrading visual reasoning capabilities? The paper observes that models like VITA 1.5 "results in regression when subtitles and audio are added," indicating limitations in audio-video fusion.

### Open Question 3
What computational mechanisms are required to enable the integration of subtle, asynchronous audiovisual cues in long-form videos (10-65 minutes) where relevant information is sparsely distributed? The authors note that for long videos, "social relationship, emotion, and situational understanding often rely on fine-grained and sometimes asynchronous cues... Performance drops more."

## Limitations

- Benchmark validity depends on ablation models (GPT-4o-mini and Gemini 2.0-FL) being sufficiently capable to reliably detect unimodal shortcuts
- Human expert baseline of 92.8% represents a single group's performance without established inter-rater reliability
- Reliance on YouTube-sourced videos raises questions about representativeness across domains and potential copyright restrictions

## Confidence

- **High Confidence**: The methodology for creating cross-modal dependencies through iterative ablation refinement is well-specified and theoretically sound. The observed performance gap between state-of-the-art models (~64%) and human experts (92.8%) is clearly documented and significant.
- **Medium Confidence**: The claim that native audio integration should outperform text proxies for non-speech tasks is partially supported but inconsistent in current results. The specific mechanisms by which models fail on long-form content are hypothesized but not definitively proven.
- **Low Confidence**: The benchmark's generalizability beyond the current video corpus is uncertain, as is the stability of performance metrics across different human evaluation groups or model versions.

## Next Checks

1. **Ablation Model Capability Validation**: Test GPT-4o-mini and Gemini 2.0-FL on a separate cross-modal dataset to establish their baseline performance before using them as ablation tools. Verify they can detect true unimodal shortcuts rather than false positives/negatives.

2. **Long-Form Temporal Decay Analysis**: Conduct controlled experiments varying video length systematically (e.g., 30s, 2min, 10min intervals) to determine whether performance degradation follows a linear, exponential, or threshold pattern, and whether this correlates with specific architectural limitations.

3. **Audio-Type Performance Breakdown**: Run the benchmark with models using only native audio (no text proxy) and evaluate performance specifically on music, environmental sounds, and speech categories. Compare against models using only Whisper-v3 to quantify the exact contribution of native audio encoding versus text-based approaches.