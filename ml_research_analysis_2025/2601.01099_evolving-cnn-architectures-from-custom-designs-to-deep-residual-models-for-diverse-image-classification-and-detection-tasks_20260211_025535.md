---
ver: rpa2
title: 'Evolving CNN Architectures: From Custom Designs to Deep Residual Models for
  Diverse Image Classification and Detection Tasks'
arxiv_id: '2601.01099'
source_url: https://arxiv.org/abs/2601.01099
tags:
- pretrained
- learning
- transfer
- classification
- custom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates custom CNN architectures against
  pretrained and transfer learning models across five diverse image datasets, including
  binary classification, fine-grained multiclass recognition, and object detection
  tasks. A custom CNN with residual connections is progressively evolved, demonstrating
  that deeper architectures with bottleneck residual blocks significantly improve
  fine-grained multiclass performance, while lightweight pretrained and transfer learning
  models excel in simpler binary classification tasks.
---

# Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks

## Quick Facts
- arXiv ID: 2601.01099
- Source URL: https://arxiv.org/abs/2601.01099
- Reference count: 7
- This paper systematically evaluates custom CNN architectures against pretrained and transfer learning models across five diverse image datasets, showing that model selection should align with task complexity and resource constraints.

## Executive Summary
This paper presents a systematic evaluation of evolving CNN architectures across diverse image classification and detection tasks. The authors develop custom architectures from simple baseline models to deep residual networks with bottleneck blocks, comparing their performance against pretrained and transfer learning approaches. Their results demonstrate that deeper architectures with bottleneck residual blocks significantly improve fine-grained multiclass performance, while lightweight pretrained models excel in simpler binary classification tasks. For object detection, full fine-tuning of pretrained models is essential for reliable localization.

## Method Summary
The study evaluates multiple CNN architectures across five image datasets: binary classification (Road Damage, FootpathVision), fine-grained multiclass recognition (MangoImageBD, PaddyVarietyBD), and object detection (Auto-Rickshaw). Architectures evolve from a simple baseline (7×7 initial conv + residual blocks) to enhanced models with bottleneck residual blocks for multiclass tasks. Transfer learning uses frozen MobileNetV2/EfficientNetB0 backbones, while custom detectors (MiniYOLO) and state-of-the-art frameworks (Faster R-CNN, YOLO) are assessed for detection. Training uses cross-entropy loss for classification and composite loss for detection, with evaluation metrics including accuracy, precision, recall, F1-score, and parameter efficiency.

## Key Results
- Deeper architectures with bottleneck residual blocks significantly improve fine-grained multiclass performance (MangoImageBD, PaddyVarietyBD)
- Lightweight pretrained and transfer learning models excel in simpler binary classification tasks (Road Damage, FootpathVision)
- For object detection, full fine-tuning of pretrained models is essential for reliable localization; frozen backbones fail completely
- Custom CNN with residual connections evolved systematically from baseline to achieve state-of-the-art performance on complex tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bottleneck residual blocks enable deeper feature hierarchies necessary for fine-grained multiclass discrimination.
- Mechanism: 1×1 convolutions reduce channel dimensions before 3×3 spatial extraction, then restore dimensions via another 1×1 convolution. This preserves representational capacity while reducing parameters, allowing more layers to be stacked without gradient degradation. Shortcut connections preserve gradient flow during backpropagation.
- Core assumption: Fine-grained visual distinctions require progressively more abstract feature representations that only deeper networks can provide.
- Evidence anchors:
  - [abstract] "demonstrating that deeper architectures with bottleneck residual blocks significantly improve fine-grained multiclass performance"
  - [section 3.4] "Each bottleneck residual block consists of three convolutional layers: a 1×1 convolution for dimensionality reduction, a 3×3 convolution for spatial feature extraction, and another 1×1 convolution to restore the channel dimension."
  - [corpus] Neighbor papers on residual learning for classification tasks show consistent patterns, though limited direct evidence on bottleneck mechanisms specifically.
- Break condition: If dataset lacks subtle inter-class variations (e.g., coarse binary distinctions), bottleneck depth provides diminishing returns.

### Mechanism 2
- Claim: Transfer learning with frozen backbones is effective for binary classification but inadequate for fine-grained multiclass and spatial localization tasks.
- Mechanism: Pretrained ImageNet features encode general visual patterns (edges, textures, shapes). For binary tasks with coarse distinctions (damaged vs. undamaged roads), these frozen features are sufficient. For fine-grained tasks requiring domain-specific features (paddy grain varieties), frozen representations cannot adapt to subtle class boundaries. For detection, frozen backbones fail to capture task-specific spatial structures needed for bounding box regression.
- Core assumption: Task complexity determines the minimum required feature adaptation depth.
- Evidence anchors:
  - [abstract] "lightweight pretrained and transfer learning models excel in simpler binary classification tasks"
  - [section 5.2.4, Table 9] "EfficientNetB0 (Transfer Learning) showed a notable drop in recall, indicating limited adaptability when the backbone is frozen."
  - [section 5.2.5, Table 10] "transfer learning variant of Faster R-CNN failed to detect objects altogether, yielding zero accuracy and F1-score. This is due to a zero IoU score during evaluation."
  - [corpus] Weak direct evidence; neighbor papers focus on transfer learning benefits without systematic comparison of frozen vs. fine-tuned regimes.
- Break condition: If pretrained domain closely matches target domain and classes are visually similar to ImageNet categories, frozen features may suffice even for multiclass tasks.

### Mechanism 3
- Claim: Early-layer architecture choices (kernel size, depth) affect low-level feature capture and class imbalance handling.
- Mechanism: Stacked 3×3 convolutions provide multiple nonlinearities for complex local patterns but may overfit minority classes. Single 7×7 convolution offers broader receptive field early, potentially capturing more global context that aids generalization on imbalanced datasets. Simplified classification heads (removing intermediate dense layers) reduce overfitting to majority classes.
- Core assumption: Class imbalance problems stem partly from overparameterized early layers memorizing majority class patterns.
- Evidence anchors:
  - [section 2.1] "The use of multiple nonlinearities between layers also allows the model to learn more complex local patterns."
  - [section 5.1.1, Table 1] Evolved Baseline with 7×7 initial conv achieved 0.78 accuracy vs. Custom CNN's 0.72 on Road Damage, with better minority class recall.
  - [corpus] Insufficient evidence; neighbor papers do not systematically analyze early-layer kernel size effects.
- Break condition: If dataset has balanced classes and high-resolution texture details, stacked 3×3 convolutions may outperform larger single kernels.

## Foundational Learning

- Concept: **Residual connections and gradient flow**
  - Why needed here: The paper progressively evolves architectures from standard convolutions to residual blocks to bottleneck residuals. Understanding why residual connections enable deeper networks is essential for interpreting performance gains on multiclass tasks.
  - Quick check question: Can you explain why adding the input to a block's output (F(x) + x) helps train deeper networks compared to just learning F(x)?

- Concept: **Transfer learning vs. full fine-tuning**
  - Why needed here: The paper's central finding is that optimal training strategy depends on task complexity. Understanding when frozen features suffice versus when full adaptation is needed is critical for model selection.
  - Quick check question: Given a dataset with 35 visually similar paddy varieties, would you expect transfer learning (frozen backbone) to match full fine-tuning performance? Why or why not?

- Concept: **Feature hierarchy in CNNs**
  - Why needed here: The paper shows binary tasks work with shallow/transfer features while fine-grained tasks need deeper hierarchies. Understanding which layers capture which abstraction levels guides architecture selection.
  - Quick check question: In a CNN, do early layers or later layers encode class-specific semantic information? How does this relate to whether you should freeze or fine-tune pretrained backbones?

## Architecture Onboarding

- Component map:
  Input → Initial Feature Extractor (3×3 stack OR 7×7 conv) → Stage 1-4 (Residual Blocks with depthwise separable OR standard OR bottleneck convs) → Global Average Pooling → Classification Head (with/without intermediate Dense) → Output

  Detection variant: Compact backbone → Adaptive avg pooling → Detection head (class logits + bbox)

- Critical path: Start with Evolved Baseline (7×7 initial conv + standard residual blocks + simplified head) for binary tasks. Add bottleneck blocks (Section 3.4 configuration: 3-4-6-3 blocks across stages) only for fine-grained multiclass tasks. Use pretrained EfficientNetB0 for balanced efficiency/performance when training resources are limited.

- Design tradeoffs:
  | Choice | Pro | Con | Best for |
  |--------|-----|-----|----------|
  | Stacked 3×3 initial | Rich local patterns | May overfit minority class | Balanced datasets |
  | Single 7×7 initial | Broader context, better generalization | Less early nonlinearity | Imbalanced binary |
  | Depthwise separable | Parameter efficiency | Reduced capacity | Resource-constrained |
  | Bottleneck residual | High capacity, gradient flow | 90MB+ parameters | Fine-grained multiclass |
  | Transfer (frozen) | Fast, few parameters | Limited adaptation | Binary, coarse distinctions |
  | Full fine-tuning | Maximum adaptation | Slow, more parameters | Detection, fine-grained |

- Failure signatures:
  - Near-zero recall on minority class → Model overfitting to majority; try simplified classification head or initial 7×7 conv
  - Transfer learning fails on detection (zero IoU) → Frozen backbone cannot capture spatial structures; must use full fine-tuning
  - Bottleneck model underperforms binary task → Overparameterization; use Evolved Baseline instead
  - High training time (>100s/epoch) with modest gains → Model too deep for task complexity; reduce to pretrained lightweight model

- First 3 experiments:
  1. Establish baseline: Train Evolved Baseline (Section 3.3) on your binary classification task. Log accuracy, F1, and per-class recall to identify imbalance issues.
  2. Transfer learning comparison: Train MobileNetV2 with frozen backbone + custom head (Section 4.1) on same task. Compare F1-score and training time per epoch to baseline.
  3. Depth scaling test: If multiclass task, compare baseline without bottleneck vs. deeper model with bottleneck blocks (Section 3.4). Measure accuracy gain vs. parameter count increase to justify added complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated neural architecture search (NAS) or hybrid CNN-transformer architectures optimize the trade-off between the high computational cost of custom bottleneck models and the performance limitations of lightweight transfer learning?
- Basis: [explicit] The Conclusion explicitly states plans to "investigate hybrid CNN-transformer architectures, attention mechanisms, and automated neural architecture search (NAS) strategies" to enhance adaptability.
- Why unresolved: The current study manually tuned architectures, finding that high performance in fine-grained tasks required computationally expensive custom models (~90MB parameters), whereas efficient models underperformed.
- What evidence would resolve it: Comparative benchmarks showing NAS-discovered or transformer-hybrid models achieving comparable accuracy to the custom CNN on MangoImageBD with significantly fewer parameters or lower latency.

### Open Question 2
- Question: Are the reported localization metrics (e.g., IoU 0.7366) statistically robust given the evaluation was conducted on only 8 images?
- Basis: [inferred] Section 6.1 states the detector was "evaluated on only 8 images," which introduces high variance and uncertainty despite the strong numerical results reported in Section 5.2.5.
- Why unresolved: A sample size of 8 is insufficient to generalize performance across "complex traffic scenes" or establish reliable baseline metrics for the Auto-RickshawImageBD dataset.
- What evidence would resolve it: Re-evaluation of the MiniYOLO and Faster R-CNN models on a standard-sized hold-out test set (e.g., n > 100) to validate if the high precision and recall hold statistically.

### Open Question 3
- Question: Why does the transfer learning strategy (frozen backbone) fail completely for object detection (0.0 accuracy) while succeeding in classification?
- Basis: [inferred] Section 5.2.5 shows Faster R-CNN transfer learning yielded 0.0 accuracy/IoU, whereas classification transfer learning (e.g., MobileNetV2) achieved >0.89 F1-score.
- Why unresolved: The paper notes the failure is due to the frozen backbone's inability to capture spatial patterns, but does not explain why spatial localization requires full fine-tuning when feature extraction for classification does not.
- What evidence would resolve it: An ablation study varying the number of unfrozen layers in the detection backbone to identify the specific feature hierarchy required for localization versus classification.

## Limitations
- Critical hyperparameters (learning rates, batch sizes, weight decay) are omitted, limiting exact reproduction capability
- Detection head architecture details for MiniYOLO are ambiguous without explicit grid or anchor configurations
- Class imbalance mitigation strategies are not specified, though the paper notes these as critical failure modes

## Confidence
- **High Confidence**: Binary classification results with pretrained models (well-established transfer learning patterns)
- **Medium Confidence**: Fine-grained multiclass performance claims (limited direct evidence on bottleneck mechanisms)
- **Low Confidence**: Detection transfer learning failures (only one dataset, no ablation studies on backbone freezing)

## Next Checks
1. **Hyperparameter sensitivity**: Systematically vary learning rates and batch sizes on binary classification tasks to identify optimal configurations
2. **Frozen vs. fine-tuned detection**: Conduct controlled experiments freezing different percentages of backbone layers to quantify adaptation requirements
3. **Kernel size ablation**: Compare 3×3 stacked vs. 7×7 initial convolutions on imbalanced datasets to validate generalization claims