---
ver: rpa2
title: 'PACIFIC: a framework for generating benchmarks to check Precise Automatically
  Checked Instruction Following In Code'
arxiv_id: '2512.10713'
source_url: https://arxiv.org/abs/2512.10713
tags:
- instructions
- instruction
- output
- pacific
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PACIFIC is a framework for automatically generating benchmarks
  to evaluate instruction-following and code dry-running capabilities in large language
  models. It uses modular instruction building blocks to construct complex multi-step
  tasks, with deterministic evaluation via output comparison against reference implementations.
---

# PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code

## Quick Facts
- arXiv ID: 2512.10713
- Source URL: https://arxiv.org/abs/2512.10713
- Reference count: 23
- PACIFIC is a framework for automatically generating benchmarks to evaluate instruction-following and code dry-running capabilities in large language models.

## Executive Summary
PACIFIC is a framework for automatically generating benchmarks to evaluate instruction-following and code dry-running capabilities in large language models. It uses modular instruction building blocks to construct complex multi-step tasks, with deterministic evaluation via output comparison against reference implementations. The framework allows control over difficulty through instruction count and output length parameters. Experimental results show that even state-of-the-art LLMs struggle with compositional instruction execution, with performance degrading significantly as task complexity increases. The framework provides a scalable, contamination-resistant methodology for assessing core competencies in code-related tasks that are not adequately captured by existing benchmarks.

## Method Summary
PACIFIC generates benchmarks by assembling pipelines of 22 modular instruction building blocks (each with natural language descriptions and code implementations in Python, Java, and C++). The system uses type consistency constraints and length-aware selection to chain instructions into multi-step tasks. Evaluation is deterministic, comparing LLM outputs (parsed from `[ANSWER]` tags) against reference implementation results. Difficulty is controlled through two parameters: instruction count (I) and target output length (L). The framework supports seed-based resampling to generate variants while preserving difficulty profiles.

## Key Results
- Performance degrades significantly as instruction count and output length increase, even for state-of-the-art LLMs
- Deterministic evaluation via reference implementations enables contamination-resistant benchmarking
- PACIFIC captures fundamental code reasoning capabilities not addressed by existing benchmarks
- Models struggle with compositional instruction execution despite strong individual performance on simpler tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deterministic evaluation is achievable without LLM-as-a-judge by comparing model outputs against pre-computed reference implementations.
- **Mechanism:** The framework includes a repository of instructions with corresponding code implementations (Python, Java, C++). During benchmark generation, the system executes these reference implementations to determine the "ground truth" expected output. Evaluation simply compares the LLM's parsed output string against this ground truth.
- **Core assumption:** The reference implementations are correct and the output format parsing rules (regex-based extraction of `[ANSWER]` tags) capture the model's intent accurately.
- **Evidence anchors:**
  - [abstract]: "deterministic evaluation via output comparison against reference implementations"
  - [Section 3.3.1]: "PACIFIC employs a fully rule-based evaluation pipeline... validating each instruction by comparing the output against the expected result."
  - [corpus]: Related work like *ATLAS* supports the viability of automated verified code synthesis, though specific dry-running validation is unique to this paper.
- **Break condition:** If instructions have ambiguous specifications that lead to multiple valid interpretations (not present in current set), reference implementations may not cover all valid model outputs.

### Mechanism 2
- **Claim:** Increasing "instruction count" and "target output length" functions as an effective difficulty regulator, causing performance degradation even in SOTA models.
- **Mechanism:** Complexity is managed via two parameters: $I$ (instruction count) forces longer reasoning chains (compositional depth), while $L$ (target length) forces more extensive data transformations per step. The generator dynamically selects instructions that satisfy length constraints.
- **Core assumption:** Difficulty is linearly or exponentially related to the number of steps and data volume, and models fail due to compounding error rates in sequential logic.
- **Evidence anchors:**
  - [Section 3.3.2]: "Increasing the number of instructions requires the model to perform longer reasoning chains... larger output lengths introduce additional difficulty."
  - [Section 5.1.1]: "As the number of instructions increases, performance consistently declines... clear negative correlation."
  - [corpus]: *InfoSynth* notes similar challenges in benchmark synthesis complexity, supporting the difficulty scaling premise.
- **Break condition:** If models develop robust chain-of-thought state tracking, the linear degradation curve may flatten.

### Mechanism 3
- **Claim:** Contamination resistance is achieved by decoupling the semantic logic (the instruction graph) from the specific surface form (input values and random seed).
- **Mechanism:** The framework uses a constraint-based sampler (type consistency + length adjustment) rather than a static dataset. By changing the random seed, the system generates a completely new set of inputs and instruction permutations that maintain the same difficulty profile but differ in content.
- **Core assumption:** Memorization relies on specific input-output pairs or prompt templates rather than the underlying abstract computational graph.
- **Evidence anchors:**
  - [Section 3.3.3]: "Seed-based resampling... preserving target difficulty while altering content."
  - [Section 3.3.3]: "These variants maintain the underlying computational graph but change surface form... reducing memorization."
  - [corpus]: Limited specific corpus evidence for this specific contamination mechanism in code benchmarks.
- **Break condition:** If models learn the *logic* of the 22 specific instruction types (e.g., "shift_back") perfectly, they may generalize across seeds, rendering contamination resistance less effective against capability measurement (though still valid for data hygiene).

## Foundational Learning

- **Concept:** **Code Dry-Running (Mental Simulation)**
  - **Why needed here:** PACIFIC explicitly isolates the model's ability to predict code execution state without actually running the code.
  - **Quick check question:** Can you mentally execute the function `shift_back` on the string "IBM" and predict the result is "HAL"?

- **Concept:** **Type Consistency (Composition)**
  - **Why needed here:** The benchmark generator relies on matching output types (number/string) to input types of subsequent instructions to form valid pipelines.
  - **Quick check question:** If Instruction A outputs a `string` and Instruction B requires a `number` input, can they be chained?

- **Concept:** **Deterministic Parsing Strategies**
  - **Why needed here:** The evaluation engine extracts answers using strict tag patterns (e.g., `[ANSWER][i]`). Understanding this is key to debugging low scores.
  - **Quick check question:** If a model outputs "The answer is 42" inside `[ANSWER][1]The answer is 42[\ANSWER]`, will the parser accept it, or does it require the literal "42"?

## Architecture Onboarding

- **Component map:** Instruction Pool -> Benchmark Generator -> Inference Interface -> Evaluation Engine

- **Critical path:** Defining/verifying the instruction pool -> Setting generation params (Seed, I, L) -> Generating sample inputs -> LLM Inference -> Parsing [ANSWER] tags -> Comparison against Reference Implementation.

- **Design tradeoffs:**
  - **Prompt vs. Chat Mode:** Prompt mode is cheaper/faster but context window fills faster; Chat mode allows step-by-step interaction but increases token cost (Section 4).
  - **Code vs. NL Instructions:** Code snippets test syntax familiarity; NL instructions test semantic understanding of logic.

- **Failure signatures:**
  - **Formatting Errors:** High "missing answers" rate (e.g., `gpt-oss-120b` in Section 5.1.3). The model hallucinates outside the [ANSWER] tags.
  - **Reasoning Drift:** Low "Instruction Level Accuracy" on high L values. The model fails to maintain state over long string transformations.
  - **Type Mismatch:** Model attempts to apply a string operation to a number, usually resulting in an early stop or default error message.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run the simplest benchmark (I=3, L=1) to verify the environment and parsing logic are working; expect high accuracy (>80%).
  2. **Stress Test:** Run the max difficulty (I=15, L=10) on your target model to identify the "skill gap" floor; expect near 0% accuracy.
  3. **Ablation on Format:** Compare performance when instructions are given as Python code vs. Natural Language to determine if the model relies on syntax parsing or semantic reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What additional factors beyond instruction count and output length contribute to PACIFIC benchmark difficulty, and are there interaction effects between these two controlled parameters?
- **Basis in paper:** [explicit] Section 7 states: "The difficulty of a PACIFIC benchmark may be affected by additional factors beyond the number of instructions and output size. Moreover, there may be combined effects of the two factors that we control."
- **Why unresolved:** The framework currently operationalizes difficulty through only two parameters, but the authors acknowledge this may be incomplete.
- **What evidence would resolve it:** Systematic ablation studies varying instruction types, sequencing patterns, and their interactions to identify additional difficulty predictors.

### Open Question 2
- **Question:** Does Chat mode elicit different LLM performance compared to Prompt mode for sequential instruction following?
- **Basis in paper:** [explicit] Section 7 notes: "Chat mode in a PACIFIC benchmark may behave differently than the single prompt mode. We have anecdotally checked this and saw no difference, but a thorough investigation is needed."
- **Why unresolved:** Only anecdotal comparison was performed; the experiment used Prompt mode exclusively for cost reasons.
- **What evidence would resolve it:** Controlled comparison across both modes with identical instruction sequences and statistical analysis of performance differences.

### Open Question 3
- **Question:** How can the contamination resistance property of PACIFIC benchmarks be empirically validated?
- **Basis in paper:** [explicit] Section 7 states: "Contamination resistance, a core design objective of PACIFIC, has not yet been empirically validated across all scenarios."
- **Why unresolved:** While the framework design theoretically supports contamination resistance through seed-based resampling, no systematic evaluation has confirmed this in practice.
- **What evidence would resolve it:** Evaluation of models before and after exposure to PACIFIC variants, measuring whether regenerated benchmarks maintain predictive validity.

### Open Question 4
- **Question:** How do specific seed instructions or initial algorithms influence the generated benchmark difficulty?
- **Basis in paper:** [explicit] Section 7 notes: "The benchmarks generated by PACIFIC for the experimental evaluation may be influenced by the seed instructions or by the initial algorithms to evaluate."
- **Why unresolved:** The authors observed no such influence but did not systematically investigate it.
- **What evidence would resolve it:** Controlled experiments varying seed instruction pools while holding other parameters constant to measure difficulty variance.

## Limitations

- The framework's contamination resistance mechanism lacks direct experimental validation against benchmark-specific memorization effects
- Only one C++ code example is provided for the 22 instruction types, limiting insight into edge case handling
- The length-adjustment algorithm for instruction selection is described but not fully specified
- Performance degradation curves may flatten if models develop robust chain-of-thought state tracking

## Confidence

**High Confidence** (4 claims):
- The deterministic evaluation methodology using reference implementations and regex parsing is clearly specified and directly implementable
- Performance degradation with increased instruction count and output length is consistently observed across multiple model sizes
- The framework provides a viable methodology for assessing compositional instruction execution capabilities
- The distinction between prompt-level and instruction-level accuracy meaningfully captures different failure modes

**Medium Confidence** (2 claims):
- The contamination resistance mechanism effectively prevents memorization while maintaining difficulty profiles
- The specific instruction implementations across all three languages (Python, Java, C++) are sufficiently specified for practical reproduction

**Low Confidence** (1 claim):
- The length-adjustment algorithm for instruction selection produces the intended difficulty scaling without bias

## Next Checks

1. **Implementation Completeness Verification**: Implement all 22 instruction types and verify that the reference implementations produce deterministic outputs across the full input space (numbers 1-100, strings of length 1-10), checking for edge cases not covered by the single C++ example.

2. **Contamination Resistance Test**: Train or fine-tune a model on the 22 instruction types in isolation (without PACIFIC's compositional structure), then evaluate on PACIFIC benchmarks with different random seeds to measure if performance exceeds baseline expectations, confirming the framework's resistance to instruction-level memorization.

3. **Length Scaling Validation**: Systematically vary the target output length parameter while holding instruction count constant, measuring the precise relationship between L and performance to confirm the claimed linear or exponential difficulty scaling.