---
ver: rpa2
title: 'Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in
  Multimodal LLMs'
arxiv_id: '2507.06999'
source_url: https://arxiv.org/abs/2507.06999
tags:
- reasoning
- answer
- arxiv
- training
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal reasoning
  in large language models (LLMs), specifically focusing on tasks that require both
  visual and textual understanding, such as mathematical problem-solving. Existing
  approaches often rely on extensive data annotation or complex reward systems, increasing
  training costs and limiting scalability.
---

# Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs

## Quick Facts
- arXiv ID: 2507.06999
- Source URL: https://arxiv.org/abs/2507.06999
- Authors: Yahan Yu; Yuyang Dong; Masafumi Oyamada
- Reference count: 40
- Qwen2.5-vl-7B achieves 60.6% accuracy on GEOQA-8K with D2I, improving 14.0% over base model

## Executive Summary
This paper introduces the Deliberate-to-Intuitive (D2I) framework to enhance multimodal reasoning in large language models without requiring extensive annotations or complex reward systems. The key innovation is training with structured deliberation strategies (LOC, JUS, PAR) using rule-based format rewards, then shifting to intuitive reasoning at inference by removing these constraints. Experiments on Qwen2.5-vl-7B demonstrate consistent improvements across both in-domain and out-of-domain benchmarks, with particularly strong gains on mathematical reasoning tasks. The approach shows that format-only rewards can effectively steer multimodal alignment while reducing training costs.

## Method Summary
D2I employs deliberate reasoning strategies during training—identifying crucial image regions (LOC), explaining visual clues (JUS), and parsing image structure (PAR)—using rule-based format rewards. During inference, the model shifts to intuitive reasoning, removing structured constraints and allowing flexible, implicit responses. The framework builds on GRPO (Group Relative Policy Optimization) with dual rewards: format (tag structure compliance) and answer (final answer correctness). The model learns to ground reasoning in visual semantics to satisfy format constraints and maximize rewards.

## Key Results
- On GEOQA-8K dataset, D2I achieves 60.6% accuracy, representing a 14.0% improvement over the base model and a 7.5% improvement over GRPO
- D2I consistently outperforms strong baselines across in-domain and out-of-domain benchmarks
- Different strategies show complementary strengths: LOC excels on spatial tasks, JUS on interpretability-focused benchmarks, and PAR on complex-layout benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Training-Inference Decoupling
The model internalizes reasoning patterns during constrained training that persist when constraints are removed at inference. This allows the model to apply acquired skills without being disrupted by potentially low-quality intermediate outputs like inaccurate bounding boxes.

### Mechanism 2: Format-Only Reward Sufficiency
Rule-based format rewards alone can enhance multimodal reasoning without annotated reasoning chains or complex reward models. Format compliance correlates with desirable reasoning behaviors, steering optimization without human-labeled reasoning traces.

### Mechanism 3: Strategy-Specific Strengths
Different deliberate strategies yield complementary strengths across task types. LOC emphasizes spatial grounding, JUS emphasizes natural-language visual explanation, and PAR emphasizes global structural parsing. No single strategy dominates universally.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: D2I builds on GRPO for reinforcement learning, sampling multiple responses and computing groupwise advantages
  - Quick check question: Can you explain how GRPO computes advantages differently from PPO?

- **Multimodal Alignment**
  - Why needed here: The paper targets modality alignment—ensuring visual regions correctly map to reasoning steps
  - Quick check question: What failure modes occur when visual-textual alignment is poor in MLLMs?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Deliberate reasoning extends CoT by adding structured visual grounding steps; intuitive reasoning relaxes back toward standard CoT at inference
  - Quick check question: How does test-time CoT scaling differ from training-time reasoning depth?

## Architecture Onboarding

- **Component map**: Base model (Qwen2.5-VL-7B-Instruct) -> GRPO training with format+answer rewards -> three strategy prompts (LOC/JUS/PAR) -> Inference with intuitive prompt
- **Critical path**: 1) Format prompt selection determines which tags model must generate, 2) GRPO training loop (150 steps, batch 128, LR 1e-6) teaches model to satisfy format + answer rewards, 3) Inference with intuitive prompt generates answers without tag constraints, 4) Evaluation on in-domain and out-of-domain benchmarks
- **Design tradeoffs**: D2I vs. D2D - D2I excels on complex math reasoning where intermediate outputs may be noisy; D2D performs better on easier general tasks where structured outputs add value
- **Failure signatures**: Model generates syntactically correct tags but semantically irrelevant content (format gaming); performance drops on general benchmarks when using D2I
- **First 3 experiments**: 1) Reproduce GRPO baseline on GEOQA-8K with Qwen2.5-VL-7B; confirm 53.1% intuitive accuracy as reference, 2) Implement single strategy (start with JUS); train and evaluate D2I vs. D2D, 3) Benchmark on one math OOD (MathVista-mini) and one general OOD (MMVet); verify D2I advantage on math, D2D potential advantage on general tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the D2I framework be effectively adapted for temporal reasoning domains, such as instructional videos or procedural planning?
- Basis in paper: [explicit] The Conclusion states, "In future work, we plan to explore further applications to other multimodal domains such as science diagrams, instructional videos, or procedural planning."
- Why unresolved: The current study is restricted to static images, utilizing spatial strategies that do not directly translate to temporal sequences
- What evidence would resolve it: Successful application of time-aware deliberate strategies on video understanding benchmarks (e.g., VideoQA) demonstrating improved temporal reasoning

### Open Question 2
- Question: Can the complementary strengths of the LOC, JUS, and PAR strategies be unified or dynamically selected to maximize performance across diverse tasks?
- Basis in paper: [inferred] Section 6.1 notes that "no single strategy consistently dominating the others" (LOC aids spatial tasks, JUS aids semantic tasks), yet they are trained and evaluated in isolation
- Why unresolved: The paper does not explore whether a single model can learn to switch between or combine these strategies based on input type
- What evidence would resolve it: A unified model that dynamically generates location boxes or justifications as needed, outperforming single-strategy baselines on a mixed-suite benchmark

### Open Question 3
- Question: Is it possible to design a mechanism that adaptively switches between "Deliberate" (D2D) and "Intuitive" (D2I) reasoning at inference time based on task complexity?
- Basis in paper: [inferred] Section 6.2 reports that D2I underperforms D2D on general benchmarks because structured outputs "positively contribute" to simpler tasks
- Why unresolved: The framework currently commits to a fixed inference style (Intuitive), potentially sacrificing accuracy on easier queries where explicit grounding is beneficial
- What evidence would resolve it: A "router" model that achieves higher average accuracy than fixed D2I by routing simple visual questions to D2D and complex reasoning to D2I

## Limitations
- The framework assumes the model internalizes alignment patterns rather than simply learning to generate compliant tags
- Out-of-domain results show smaller margins on some benchmarks compared to strong in-domain gains
- The paper doesn't explore whether more sophisticated format rewards or dynamic strategy switching could yield further improvements

## Confidence
- **High Confidence**: In-domain performance improvements on GEOQA-8K (60.6% accuracy for D2I vs. 53.1% for GRPO)
- **Medium Confidence**: Out-of-domain transfer effectiveness across diverse benchmarks (results positive but with variable margins)
- **Medium Confidence**: Claim that zero-annotation training is sufficient for multimodal reasoning enhancement

## Next Checks
1. **Strategy Interference Test**: Train a combined LOC+JUS+PAR model to determine whether strategies complement or interfere with each other, and whether dynamic strategy selection based on input complexity could improve results.

2. **Format Gaming Detection**: Implement automated analysis of tag quality—measure semantic coherence of generated tags versus syntactic compliance—to verify the model isn't simply gaming format rewards while producing empty visual grounding.

3. **Zero-Shot Generalization Stress Test**: Evaluate D2I on truly out-of-distribution reasoning tasks (e.g., novel visual domains, cross-modal transfer tasks) to determine whether improvements generalize beyond the tested benchmark families.