---
ver: rpa2
title: 'Less LLM, More Documents: Searching for Improved RAG'
arxiv_id: '2510.02657'
source_url: https://arxiv.org/abs/2510.02657
tags:
- corpus
- scaling
- https
- retrieval
- larger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how retrieval corpus size affects Retrieval-Augmented\
  \ Generation (RAG) performance, and whether scaling the corpus can compensate for\
  \ smaller language models. Using a controlled setup with Qwen3 models (0.6B\u2013\
  14B parameters) and a web corpus (ClueWeb22-A), the authors systematically vary\
  \ corpus scale and evaluate on NQ, TriviaQA, and WebQ."
---

# Less LLM, More Documents: Searching for Improved RAG

## Quick Facts
- arXiv ID: 2510.02657
- Source URL: https://arxiv.org/abs/2510.02657
- Authors: Jingjie Ning; Yibo Kong; Yunfan Long; Jamie Callan
- Reference count: 40
- One-line primary result: Corpus scaling consistently improves RAG performance and can often offset the need for larger language models

## Executive Summary
This paper investigates how retrieval corpus size affects Retrieval-Augmented Generation (RAG) performance and whether scaling the corpus can compensate for smaller language models. Using a controlled setup with Qwen3 models (0.6B-14B parameters) and a web corpus (ClueWeb22-A), the authors systematically vary corpus scale and evaluate on NQ, TriviaQA, and WebQ. The key finding is that enlarging the retrieval corpus consistently improves RAG performance through increased coverage of answer-bearing passages, while utilization efficiency remains stable across corpus scales.

The analysis reveals that mid-sized models (4-8B parameters) benefit the most from corpus scaling, with 2× corpus increases often matching the performance gains of doubling model size. Tiny models (0.6B) require much larger corpus increases (6-10×) to compensate for the next tier, while very large models (14B) see diminishing returns from corpus scaling. Performance gains plateau after roughly 5× corpus increase, indicating practical limits to corpus scaling benefits.

## Method Summary
The authors conduct controlled experiments using the ClueWeb22-A web corpus, randomly sharded into 12 balanced shards (~22M documents each). They evaluate five Qwen3 model sizes (0.6B, 1.7B, 4B, 8B, 14B) across three datasets (NQ, TriviaQA, WebQ) while systematically varying the number of corpus shards used for retrieval. The retrieval pipeline uses MiniCPM-Embedding-Light for encoding, top-10 document retrieval, and top-8 chunks passed to generators. The study isolates corpus scale effects while holding retrieval parameters and evidence budget constant, allowing clean measurement of coverage versus utilization effects.

## Key Results
- Enlarging the retrieval corpus consistently improves RAG performance across all model sizes
- A 1.7B model with 4× larger corpus outperforms a 4B model, and a 4B model with 2× larger corpus consistently outperforms an 8B model
- Mid-sized models (4-8B parameters) benefit most from corpus scaling, while tiny models (0.6B) require much larger increases and very large models (14B) see diminishing returns
- Improvements primarily come from increased coverage of answer-bearing passages rather than changes in utilization efficiency
- Performance gains plateau after roughly 5× corpus increase, indicating diminishing returns at larger scales

## Why This Works (Mechanism)

### Mechanism 1: Answer-Bearing Evidence Coverage
Corpus scaling improves RAG by increasing the probability that retrieved passages contain the gold answer, not by improving how models use context. Larger corpora contain more diverse documents; with fixed top-k retrieval, a bigger pool raises the hit rate for answer-bearing passages. Coverage grows monotonically with corpus scale.

### Mechanism 2: Context Utilization Stability
LLMs' ability to extract answers from retrieved context stays roughly constant across corpus scales. Utilization Ratio (CB@n / Coverage@n) measures how often models succeed when gold answers are present. This ratio remains flat as n increases, indicating scaling improves availability, not extraction capability.

### Mechanism 3: Size-Dependent Compensation Efficiency
Mid-sized models (≈4-14B) gain most from corpus scaling; tiny models need steep corpus expansions; large models show diminishing marginal gains. Tiny models lack reasoning capacity to exploit evidence even when present; large models have more parametric knowledge, reducing retrieval-driven gains. Mid-sized models hit a sweet spot.

## Foundational Learning

- **RAG Pipeline with Fixed Evidence Budget**
  - Why needed here: The paper isolates corpus scale while holding retrieval (top-k=10) and evidence budget (top-m=8 chunks) constant. Understanding this constraint is essential to interpret why coverage, not context length, drives gains.
  - Quick check: If you increase corpus from 1 to 12 shards but keep top-k=10, what changes and what stays fixed?

- **Coverage Rate vs. Utilization Rate**
  - Why needed here: The paper decomposes RAG gains into (1) finding answer-bearing passages and (2) extracting answers from them. This distinction explains why corpus scaling works without model changes.
  - Quick check: Coverage@6=55% and CB@6=22%. What is the Utilization Ratio? If you double corpus and Coverage rises to 65%, what would you expect CB to become if utilization stays stable?

- **Diminishing Returns in Scaling**
  - Why needed here: The paper shows marginal gains decay sharply after ~5× corpus increase. Understanding this helps set practical scaling limits.
  - Quick check: Why might the first shard provide a 16-20% CB jump while the 10th shard adds <1%? What does Figure 7's confidence interval pattern suggest?

## Architecture Onboarding

- **Component map:**
  Query → Encode → ANN Search (active shards) → Retrieve top-10 → Chunk → Rerank → Select top-8 → Generator → Answer. Corpus scaling adds shards to ANN Search step only.

- **Critical path:**
  Query → Encode → ANN Search (active shards) → Retrieve top-10 → Chunk → Rerank → Select top-8 → Generator → Answer. Corpus scaling adds shards to ANN Search step only.

- **Design tradeoffs:**
  - Corpus size vs. retrieval latency/storage: More shards increase coverage but require more index storage and multi-shard search
  - Corpus size vs. model size: n⋆ analysis shows 2× corpus can substitute for 2× model size in mid-range (4B→8B), but tiny models need 6-10× corpus
  - Quality vs. quantity: Reversed shard order lowers absolute performance but n⋆ patterns remain stable, suggesting quantity can partially compensate for quality

- **Failure signatures:**
  - Tiny model + small corpus: Low absolute EM/F1; steep n⋆ requirements (6-10×) to catch next tier
  - Large model + large corpus: Diminishing returns after n≈5; 95% CI includes zero in paired bootstrap differences
  - Corpus quality collapse: Forward→Reverse comparison shows ~2-3 point F1 drop; coverage curves shift down but shape preserved

- **First 3 experiments:**
  1. **Baseline calibration**: Run all model sizes with n=1 shard; record EM/F1 to establish compensation targets
  2. **Coverage-utilization decomposition**: For n∈{1,3,6,12}, compute Gold Answer Coverage Rate and Utilization Ratio separately
  3. **Compensation threshold sweep**: Binary search for n⋆ where smaller model matches next-tier model's n=1 baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Domain generalization uncertainty: Results may not transfer to specialized domains where answer-bearing documents are sparser
- Model architecture dependencies: Results are specific to Qwen3 family and particular retrieval pipeline configuration
- Quality vs. quantity tradeoffs: The paper doesn't explore whether strategically selecting higher-quality shards could achieve better performance than random scaling

## Confidence

**High Confidence**: The primary finding that corpus scaling improves RAG performance through increased answer-bearing coverage (not utilization efficiency) is well-supported by the data.

**Medium Confidence**: The size-dependent compensation efficiency (n* values for different model tiers) is reasonably supported but may vary with different model families or retrieval configurations.

**Low Confidence**: The mechanism that larger models have more parametric knowledge reducing retrieval value is hypothesized but not directly tested.

## Next Checks

1. **Domain Transfer Validation**: Replicate the scaling experiments on a specialized corpus (e.g., PubMed for biomedical QA) to test whether the n* scaling relationships hold when answer-bearing documents are sparser.

2. **Retrieval Quality Degradation Test**: Systematically evaluate retrieval precision@10 across different corpus scales to verify the assumption that embedding quality and ANN accuracy remain stable.

3. **Strategic Corpus Selection**: Instead of random shard scaling, implement a quality-aware corpus selection method and compare the n* scaling relationships to the random scaling baseline.