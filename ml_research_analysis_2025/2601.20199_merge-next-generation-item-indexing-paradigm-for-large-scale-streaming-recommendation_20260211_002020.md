---
ver: rpa2
title: 'MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation'
arxiv_id: '2601.20199'
source_url: https://arxiv.org/abs/2601.20199
tags:
- cluster
- item
- clusters
- merge
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERGE introduces a dynamic item indexing paradigm for large-scale
  streaming recommendation that adaptively generates clusters from scratch, monitors
  cluster occupancy in real time, and builds hierarchical index structures via fine-to-coarse
  merging. Unlike traditional Vector Quantization approaches, MERGE achieves significantly
  higher assignment accuracy (cosine similarity improving from 0.6 to 0.9), more uniform
  cluster occupancy, and better cluster separation (inter-cluster cosine similarity
  reducing from 0.6 to near zero).
---

# MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation

## Quick Facts
- arXiv ID: 2601.20199
- Source URL: https://arxiv.org/abs/2601.20199
- Reference count: 40
- Next-generation dynamic item indexing paradigm for streaming recommendation

## Executive Summary
MERGE introduces a dynamic item indexing paradigm for large-scale streaming recommendation that adaptively generates clusters from scratch, monitors cluster occupancy in real time, and builds hierarchical index structures via fine-to-coarse merging. Unlike traditional Vector Quantization approaches, MERGE achieves significantly higher assignment accuracy (cosine similarity improving from 0.6 to 0.9), more uniform cluster occupancy, and better cluster separation (inter-cluster cosine similarity reducing from 0.6 to near zero). Online A/B tests demonstrate substantial gains in user engagement metrics, including a +0.0081% increase in Average Active Days and +0.0546% in Average Active Hours, while also improving exposure balance for low-VV and recently published content.

## Method Summary
MERGE processes a stream of 64-dimensional collaborative item embeddings grouped by 100 prior tags. The method operates in two phases: fine-grained codebook construction where items are threshold-matched to existing clusters (τ=0.88) or grouped via Union-Find to form new clusters (τ'=0.83), with cluster centers updated via EMA (γ=0.9993) and occupancy monitoring to reset stale clusters; followed by fine-to-coarse hierarchical merging using affinity scores penalized by cluster size (λ=0.01) with silhouette-based pruning. The system monitors cluster occupancy through EMA counts and resets clusters failing to stabilize within M=80 steps, building a hierarchical index structure that improves both accuracy and uniformity over traditional Vector Quantization methods.

## Key Results
- Assignment accuracy improves from 0.6 to 0.9 cosine similarity
- Cluster-to-cluster similarity reduces from 0.6 to near zero
- Online A/B tests show +0.0081% increase in Average Active Days and +0.0546% in Average Active Hours

## Why This Works (Mechanism)

### Mechanism 1: Threshold-Gated Assignment for Semantic Fidelity
- **Claim:** Requiring a minimum similarity threshold before assigning items to existing clusters improves semantic alignment and cluster separation.
- **Mechanism:** Unlike traditional VQ which assigns items to the closest codebook vector regardless of distance, MERGE rejects assignments if the best similarity s_i* < τ. Unmatched items are grouped via Union-Find to form new, distinct clusters. This forces the codebook to expand when data distribution shifts, rather than distorting existing cluster centers.
- **Core assumption:** Assumes item embeddings are of sufficient quality that "unmatched" items represent meaningful new semantic directions rather than noise.
- **Evidence anchors:** Abstract states MERGE "adaptively generates clusters from scratch... leading to poor assignment accuracy [in VQ]." Section 3.2.1 describes threshold matching and Union-Find processing of unmatched items.

### Mechanism 2: EMA-Based Occupancy Control for Uniformity
- **Claim:** Dynamically resetting clusters that fail to accumulate sufficient items prevents "rich-get-richer" dominance and improves visibility of long-tail content.
- **Mechanism:** The system tracks EMA count N_k for each cluster. Clusters are classified as "underfilled," "growing," or "stable." If a cluster remains underfilled or fails to stabilize within M steps, it is reset (embeddings zeroed). This frees up capacity for new or rare patterns.
- **Core assumption:** Assumes cluster utility is positively correlated with steady accumulation of items; valid but niche clusters are assumed to pass the "growing" phase eventually.
- **Evidence anchors:** Abstract mentions "dynamically monitors cluster occupancy... [achieving] more uniform cluster occupancy." Section 3.2.2 describes maintaining balanced distribution through threshold adjustment.

### Mechanism 3: Fine-to-Coarse Agglomeration for Hierarchical Stability
- **Claim:** Building a hierarchy by merging fine-grained clusters (bottom-up) rather than splitting a coarse space (top-down) reduces error propagation and improves cluster separation.
- **Mechanism:** MERGE constructs a coarse codebook P by merging elements of the fine codebook Q. It uses an affinity score that penalizes large clusters (λ · min(N_x, N_y)) to encourage balanced merging, followed by silhouette-based pruning to remove poor connections.
- **Core assumption:** Assumes fine-grained clusters generated in the first stage are accurate enough to serve as atomic units for the hierarchy.
- **Evidence anchors:** Abstract states MERGE "builds hierarchical index structures via fine-to-coarse merging." Section 3.2.3 explains how MERGE mitigates error propagation inherent in residual-based approaches.

## Foundational Learning

- **Concept: Vector Quantization (VQ) & Codebooks**
  - **Why needed here:** Must understand the baseline paradigm (mapping continuous vectors to discrete codes) to grasp why forcing "nearest neighbor" assignments causes the accuracy and uniformity issues MERGE solves.
  - **Quick check question:** In standard VQ, if a new item is semantically distinct from all existing codebook entries but equidistant to two poor matches, where is it assigned?

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** MERGE uses EMA to update cluster centers (codewords) without storing the full history of assigned items. This provides stability against distribution shifts.
  - **Quick check question:** How does a high decay factor (e.g., 0.99) affect the speed at which a cluster center adapts to a sudden shift in incoming item embeddings?

- **Concept: Union-Find (Disjoint Set Union)**
  - **Why needed here:** This algorithm is the engine for MERGE's "Cluster Extension." It efficiently groups unmatched items into new clusters based on pairwise similarity thresholds.
  - **Quick check question:** Why is Union-Find preferred over K-Means for grouping the "failed" batch B^- in a streaming context? (Hint: Look at computational complexity and fixed K).

## Architecture Onboarding

- **Component map:** Stream of 64-dim collaborative embeddings + Tags -> Similarity Calculation -> Threshold Matching -> [Branch 1: EMA Update] / [Branch 2: Union-Find Cluster Creation] -> Occupancy Monitor (Resets stale clusters) -> Hierarchical Codebooks {P, Q} -> Real-time Item Indexing
- **Critical path:** The Matching Decision (Step 2 in Section 3.2.1). The similarity threshold τ is the single most sensitive parameter; it dictates the trade-off between cluster fragmentation (too high) and semantic collapse (too low).
- **Design tradeoffs:**
  - **Recall vs. Precision:** MERGE optimizes for accuracy/separation at the potential cost of recall if the "Item Recycling" logic is too aggressive.
  - **Complexity vs. Stability:** The paper explicitly notes MERGE is more complex to implement than VQ. The hierarchy construction (Pruning/Reconnection) adds significant overhead compared to flat VQ.
- **Failure signatures:**
  - **Codebook Explosion:** Valid clusters are not merged fast enough, or new clusters are created too rapidly, exhausting memory.
  - **Thrashing:** Clusters toggle between "growing" and "reset" states, resulting in unstable IDs for items.
  - **Semantic Drift:** EMA updates shift a cluster center too far, causing previously assigned items to fall below the similarity threshold.
- **First 3 experiments:**
  1. **Threshold Sensitivity:** Vary τ on a static validation set to plot the curve of Assignment Accuracy (I2C CosSim) vs. Codebook Size (active cluster count).
  2. **Uniformity Stress Test:** Inject a burst of "Head" items (high popularity) followed by a burst of "Tail" items (novel embeddings). Verify if Tail items trigger Cluster Extension rather than being absorbed by Head clusters.
  3. **Hierarchy Stability:** Track the stability of the coarse codebook P over time. Does the silhouette coefficient remain stable, or does the hierarchy degrade as new items arrive?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MERGE effectively integrate multi-modal embeddings as a replacement for collaborative embeddings while maintaining the same level of indexing quality?
- Basis in paper: [explicit] Section E states "exploring the use of multi-modal embeddings as a replacement for collaborative embeddings to achieve richer item representations."
- Why unresolved: MERGE currently relies on 64-dimensional collaborative embeddings from a retriever model; multi-modal embeddings may have different distributional properties that affect threshold-based matching and Union-Find cluster formation.
- What evidence would resolve it: A comparative study evaluating MERGE's accuracy, uniformity, and separation metrics when using multi-modal embeddings versus collaborative embeddings on the same streaming dataset.

### Open Question 2
- Question: How can the construction of the hierarchical index structure be simplified and accelerated without sacrificing the quality gains over VQ-based approaches?
- Basis in paper: [explicit] The authors acknowledge in Section E that "the overall method is relatively complex, making its implementation substantially more challenging than VQ-based approaches" and list investigating more efficient approaches as future work.
- Why unresolved: MERGE involves multiple iterative steps (matching, Union-Find, occupancy monitoring, fine-to-coarse merging with pruning and reconnection) that increase computational and engineering complexity compared to single-pass VQ methods.
- What evidence would resolve it: Benchmarking results demonstrating reduced training time, lower memory footprint, or fewer hyperparameters while maintaining comparable improvements in I2C CosSim, cluster uniformity, and C2C separation.

### Open Question 3
- Question: Can MERGE successfully extend to generative retrieval scenarios, and what adaptations would be required for token sequence generation?
- Basis in paper: [explicit] The authors note "the current application of our method is limited to traditional retrieval scenarios" and list leveraging MERGE to support generative retrieval as a future direction. The paper also mentions that generative models "often adopt a larger index length (e.g., L=3 or 4)."
- Why unresolved: MERGE's two-layer hierarchy (P, Q) may not provide sufficient token sequence length for generative models, and the dynamic codebook evolution mechanism may conflict with the need for stable token mappings during generation.
- What evidence would resolve it: Experiments applying MERGE to a generative recommendation framework (e.g., TIGER-style sequence generation) with metrics on generation quality, token uniqueness, and downstream recommendation performance.

### Open Question 4
- Question: How robust is MERGE's performance when tag-based batch formation is unavailable or when tag quality degrades?
- Basis in paper: [inferred] Section 3.2.1 states that tag-based batch formation "has been observed to significantly accelerate convergence and improve the stability of cluster formation." The paper also notes that "forcing these diverse embeddings into clusters prematurely may introduce noise and instability."
- Why unresolved: The method's dependency on "100 categories" of prior tags from multimodal information is presented as a critical preprocessing step, but no ablation study examines MERGE's behavior without this auxiliary information.
- What evidence would resolve it: Ablation experiments comparing MERGE with and without tag-based batching, measuring convergence speed, final cluster quality metrics, and online business metric impacts.

## Limitations

- Embedding quality dependency: MERGE's threshold-based mechanisms assume high-quality, discriminative embeddings that may not generalize across different embedding architectures.
- Parameter sensitivity: Performance appears highly sensitive to multiple thresholds and hyperparameters without systematic sensitivity analysis across different data distributions.
- Computational overhead: The fine-to-coarse merging hierarchy introduces significant complexity that may outweigh accuracy gains for industrial-scale deployment.
- Long-tail content dynamics: The occupancy monitoring mechanism may not handle extremely rare content patterns that appear sporadically over long time horizons.

## Confidence

- Assignment Accuracy Improvement (I2C CosSim: 0.6→0.9): **Medium Confidence** - The mechanism is sound, but dramatic improvement depends heavily on embedding quality and threshold tuning not demonstrated across multiple datasets.
- Cluster Uniformity Improvement: **High Confidence** - EMA-based occupancy monitoring provides clear, implementable mechanism for preventing cluster dominance, though effectiveness depends on appropriate threshold calibration.
- Hierarchical Structure Benefits: **Low Confidence** - While fine-to-coarse approach theoretically reduces error propagation, practical benefits over simpler hierarchical methods are not empirically demonstrated.
- A/B Test Results: **Medium Confidence** - Reported improvements in engagement metrics are statistically measurable but may not be practically significant for all recommendation scenarios.

## Next Checks

1. **Embedding Sensitivity Test:** Run MERGE with embeddings of varying quality (different dimensionality, noise levels) to quantify how performance degrades as embedding discriminability decreases. This validates the core assumption about embedding quality dependency.

2. **Parameter Robustness Analysis:** Systematically vary MERGE's key thresholds (τ, τ', ε₁, ε₂) across multiple orders of magnitude to identify regimes where performance collapses. This reveals whether MERGE requires careful manual tuning or has robust default settings.

3. **Overhead vs. Accuracy Trade-off:** Measure MERGE's end-to-end latency and memory consumption compared to baseline Vector Quantization across different codebook sizes and streaming rates. Quantify whether accuracy gains justify additional computational resources required.