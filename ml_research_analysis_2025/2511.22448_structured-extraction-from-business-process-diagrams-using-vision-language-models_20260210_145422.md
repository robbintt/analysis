---
ver: rpa2
title: Structured Extraction from Business Process Diagrams Using Vision-Language
  Models
arxiv_id: '2511.22448'
source_url: https://arxiv.org/abs/2511.22448
tags:
- bpmn
- process
- arxiv
- type
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting structured BPMN
  (Business Process Model and Notation) diagrams from images when original source
  files are unavailable. The authors propose a vision-language model (VLM) pipeline
  that uses carefully designed prompts to guide VLMs in identifying and labeling BPMN
  elements directly from diagram images.
---

# Structured Extraction from Business Process Diagrams Using Vision-Language Models

## Quick Facts
- arXiv ID: 2511.22448
- Source URL: https://arxiv.org/abs/2511.22448
- Reference count: 40
- Key outcome: Vision-language models can extract structured BPMN diagrams from images with F1 scores above 0.70 for name extraction when using DFS+BFS traversal prompting and OCR enrichment for mid-tier models.

## Executive Summary
This paper addresses the challenge of extracting structured BPMN (Business Process Model and Notation) diagrams from images when original source files are unavailable. The authors propose a vision-language model (VLM) pipeline that uses carefully designed prompts to guide VLMs in identifying and labeling BPMN elements directly from diagram images. They enhance this approach with optional OCR-based text enrichment to recover textual labels that may be missed by the VLM. The methodology was evaluated on a dataset of 202 BPMN diagram pairs, with multiple VLMs tested in both vision-only and OCR-enhanced configurations. Top-performing models like GPT-4.1 and Mistral-Small-3.1 achieved F1 scores above 0.70 for name extraction, while mid-tier models showed significant improvements with OCR enrichment. The study also included extensive statistical analyses and prompt ablation experiments, demonstrating that structured traversal strategies like DFS+BFS outperformed other prompting approaches.

## Method Summary
The methodology extracts structured BPMN elements from PNG images using zero-shot VLM inference with schema-constrained prompts. The pipeline takes diagram images as input, optionally runs OCR to extract text tokens, and uses VLMs to generate JSON-formatted BPMN element lists. OCR enrichment fills missing names in the VLM output via string matching. The approach was evaluated on 202 English BPMN diagram pairs from the bpmn-for-research repository, split 50:25:25 into train/dev/test sets. Four evaluation regimes (name-only, name+type, relations+type, type-only) with strict and relaxed matching were used to compute precision, recall, and F1 scores against gold-standard XML-derived CSVs.

## Key Results
- Top VLMs (GPT-4.1, Mistral-Small-3.1) achieved F1 > 0.70 for name extraction using DFS+BFS traversal prompting
- Mid-tier models showed significant F1 improvements with OCR enrichment, while top-tier models sometimes degraded
- DFS+BFS prompting strategy outperformed baseline, CoT, and self-consistency approaches for entity and relation extraction
- Error rates were highest for sequence/message flows and gateways, indicating challenges with relational/directional semantics

## Why This Works (Mechanism)
The approach leverages VLMs' multimodal capabilities to jointly process visual and textual information from BPMN diagrams. By using structured traversal prompts like DFS+BFS, the model systematically explores diagram elements rather than relying on random sampling. OCR enrichment provides complementary text signals that VLMs may miss due to low contrast or small font sizes. The schema-constrained prompting ensures consistent JSON output format, while post-processing enrichment fills gaps in entity naming. The combination of visual pattern recognition and text extraction enables comprehensive element identification across different BPMN element types.

## Foundational Learning

- Concept: BPMN 2.0 Notation
  - Why needed here: The entire extraction task assumes knowledge of BPMN elements (tasks, events, gateways, flows, pools, lanes) and their visual symbols; without this, prompt design and evaluation are meaningless.
  - Quick check question: "Can you sketch the visual symbol for an exclusive gateway and explain how it differs from an inclusive gateway?"

- Concept: Vision-Language Model (VLM) Inference
  - Why needed here: The pipeline relies on VLMs to jointly process image and text; understanding zero-shot prompting, context windows, and vision-only vs. OCR-enhanced modes is essential.
  - Quick check question: "Explain how a VLM might fail to detect a label that OCR can recover, and one reason OCR might introduce noise."

- Concept: OCR Limitations
  - Why needed here: OCR is used as a secondary signal; understanding its failure modes (low-contrast text, overlapping elements, layout misinterpretation) is critical for debugging enrichment.
  - Quick check question: "List two scenarios where OCR would likely fail to extract correct text from a BPMN diagram."

## Architecture Onboarding

- Component map:
  Input Layer (BPMN image I, optional OCR text T) -> Core Inference (VLM f_θ with prompt P) -> Raw response R -> JSON parsing J(R) -> Enrichment E' (using T) -> Final JSON output -> Evaluation against gold-standard CSVs

- Critical path:
  1. Image encoding (base64) and optional OCR extraction
  2. Prompt construction (zero-shot, schema-constrained, traversal-optional)
  3. VLM inference → raw text output
  4. JSON parsing; if parsing fails, save raw text for manual inspection
  5. If OCR enabled, enrichment fills missing names via token matching
  6. JSON validation/normalization → final structured output
  7. Evaluate against gold-standard XML-derived CSVs

- Design tradeoffs:
  - VLM-only vs. OCR-enhanced: VLM-only is simpler but may miss low-contrast text; OCR adds noise risk, especially for lower-tier VLMs
  - Prompt complexity: Baseline is stable but modest; DFS+BFS improves coverage but requires longer prompts and may stress smaller models
  - Strict vs. relaxed evaluation: Strict measures exact type/label matching; relaxed allows partial type matches (e.g., "exclusivegateway" ≈ "gateway")

- Failure signatures:
  - High error rates on sequence/message flows and gateways (Table 5) → models struggle with relational/directional semantics
  - Low-tier VLMs show no benefit or degradation with OCR → noisy OCR tokens overwhelm weak multimodal integration
  - JSON parsing failures (e.g., LLaMA-3.2-11B outputs .txt) → model fails to follow schema constraints

- First 3 experiments:
  1. Baseline vs. DFS+BFS Prompt: Run GPT-4.1 and a mid-tier model (e.g., Qwen-7B) with both prompts on a 10-diagram subset; compare F1 for entities and relations
  2. OCR Enrichment Ablation: For Gemma-4B and Qwen-7B, run VLM-only, VLM+Tesseract, and VLM+Pix2Struct on 20 diagrams; measure change in name-only and name+type F1
  3. Element-wise Error Analysis: For the best-performing model (e.g., GPT-4.1), compute error rates per element type on the full test set; identify top 3 error-prone types and manually inspect 5 failure cases for each

## Open Questions the Paper Calls Out

- To what extent does fine-tuning on larger, more diverse BPMN datasets improve cross-diagram generalization compared to zero-shot prompting?
  - Basis in paper: [explicit] The authors explicitly state plans to "investigate fine-tuning on larger and more diverse BPMN datasets, with a focus on cross-diagram generalization" in the Conclusion.
  - Why unresolved: The current study relied on a relatively small dataset (202 diagrams) derived from a single source (Camunda training), limiting the analysis of generalization to radically different visual styles or domain-specific notations.
  - What evidence would resolve it: A comparative study evaluating zero-shot vs. fine-tuned performance on a hold-out set of hand-drawn or industry-specific BPMN variants.

- Can advanced prompt engineering and multi-step reasoning techniques ensure the consistent conversion of extracted elements into executable XML?
  - Basis in paper: [explicit] The Conclusion notes a future aim to "explore advanced prompt engineering and multi-step reasoning techniques... to ensure consistent conversion of BPMN images into executable XML."
  - Why unresolved: The current pipeline extracts a JSON element list but does not fully reconstruct the hierarchical, executable XML structure required for direct process mining integration.
  - What evidence would resolve it: A pipeline that outputs valid BPMN 2.0 XML files and passes a structural validation test against the original source XML.

- How can OCR-based enrichment be adapted to avoid degrading performance in top-tier VLMs that already possess strong visual grounding?
  - Basis in paper: [inferred] The results (Table 1 and Figure 3) show that top models like GPT-4.1 and Mistral-Small-3.1 suffer performance drops (negative Cohen's d) when enriched with OCR, suggesting the noise outweighs the utility for capable models.
  - Why unresolved: The paper concludes OCR helps mid-tier models but provides no mechanism to selectively disable or filter OCR for high-capability models to prevent the observed hallucination of spurious tokens.
  - What evidence would resolve it: An adaptive enrichment mechanism that only supplements VLM output when the model's internal confidence is low, preserving the accuracy of top-tier models.

## Limitations
- The exact prompt text and JSON schema are not provided in the paper, requiring inference from the repository and introducing potential discrepancies
- Evaluation details such as distance metrics and thresholds for OCR enrichment matching are unspecified, making strict replication difficult
- The study focuses on English BPMN diagrams from a specific repository, limiting generalizability to other diagram styles, languages, or diagram types

## Confidence
- High confidence: The overall methodology of using VLMs for zero-shot BPMN extraction, the dataset construction from bpmn-for-research, and the finding that GPT-4.1 and Mistral-Small-3.1 achieve F1 > 0.70 for name extraction are well-supported
- Medium confidence: The claim that OCR enrichment consistently benefits low-tier models and harms high-tier models is supported by the ablation study, but the specific conditions and failure modes are not fully detailed
- Low confidence: The assertion that DFS+BFS traversal is the best prompt strategy lacks a comprehensive comparison with all other prompting methods across all models

## Next Checks
1. Reconstruct the baseline prompt from the repository and run it on a small subset of diagrams to verify JSON schema compliance and output consistency
2. Implement OCR enrichment with a chosen OCR tool (e.g., Pix2Struct) and evaluate its impact on a mid-tier model (e.g., Qwen-7B) across 20 diagrams, measuring changes in F1 scores for name-only and name+type regimes
3. Perform per-element error analysis on the best-performing model (e.g., GPT-4.1) to identify the top three most error-prone BPMN element types and manually inspect five failure cases for each to diagnose root causes