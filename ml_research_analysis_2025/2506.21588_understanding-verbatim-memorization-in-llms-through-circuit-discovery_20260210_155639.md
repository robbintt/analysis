---
ver: rpa2
title: Understanding Verbatim Memorization in LLMs Through Circuit Discovery
arxiv_id: '2506.21588'
source_url: https://arxiv.org/abs/2506.21588
tags:
- memorization
- circuit
- circuits
- logit
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models (LMs) decide to memorize
  and continue memorized content by discovering minimal computational circuits responsible
  for these behaviors. The authors construct contrastive datasets that isolate points
  where models diverge from memorized continuations, enabling identification of circuits
  for both memorization initiation and continuation maintenance.
---

# Understanding Verbatim Memorization in LLMs Through Circuit Discovery

## Quick Facts
- **arXiv ID**: 2506.21588
- **Source URL**: https://arxiv.org/abs/2506.21588
- **Reference count**: 20
- **Key outcome**: Compact circuits (as small as 14 edges) discovered that faithfully reproduce memorization behaviors, with initiation circuits maintaining memorization but maintenance circuits unable to trigger it

## Executive Summary
This paper investigates how language models decide to memorize and continue memorized content by discovering minimal computational circuits responsible for these behaviors. The authors construct contrastive datasets that isolate points where models diverge from memorized continuations, enabling identification of circuits for both memorization initiation and continuation maintenance. Using Edge Attribution Patching with Integrated Gradients, they find compact circuits (as small as 14 edges, 0.04% of model) that faithfully reproduce memorization behaviors. Key results show that circuits enabling memorization initiation can also maintain it, while circuits for continuation maintenance cannot trigger memorization. Cross-domain experiments reveal that memorization prevention mechanisms generalize across different text domains (GitHub, Enron Emails, Common Crawl), whereas memorization induction is more context-dependent.

## Method Summary
The authors use Edge Attribution Patching with Integrated Gradients (EAP-IG) to discover minimal computational circuits in GPT-Neo-125m responsible for verbatim memorization. They construct contrastive datasets isolating memorization decision points and branch comparison behaviors, then use binary search to find the smallest edge sets achieving â‰¥85% faithfulness. The method involves creating clean/corrupted sample pairs from Wikipedia text, computing edge importance scores via gradients, and validating circuits through cross-task and cross-domain testing.

## Key Results
- Circuits as small as 14 edges (0.04% of model) can faithfully reproduce memorization behaviors
- Memorization initiation circuits can maintain memorization, but maintenance circuits cannot trigger initiation
- Prevention mechanisms generalize across text domains (GitHub, Enron, Common Crawl), while induction is context-dependent
- Circuits discovered via contrastive datasets fail with zero-ablation or mean-value replacements

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Circuit Functionality (Initiation vs. Maintenance)
The paper identifies two distinct computational subgraphs: a "Memorization Decision" circuit that handles both detection of memorizable content and control of its usage, and a "Branch Comparison" circuit that only handles continuation once memorization is established. The initiation circuit contains necessary computational paths to sustain the decision, while the maintenance circuit lacks control points for the initial trigger. This is evidenced by cross-task experiments where the Memorization Decision circuit (141 edges) perfectly blocked memorization in a maintenance task (faithfulness 1.00), but the minimal maintenance circuit (14 edges) failed to induce memorization in a decision task.

### Mechanism 2: Generalization of Prevention vs. Induction Mechanisms
The researchers found that "noising" approaches (patching activations to disrupt memorization) transferred effectively across different text domains, while "denoising" approaches (patching to induce memorization) were less effective in cross-domain settings. This suggests that blocking retrieval may be more general-purpose (promoting alternative tokens), while triggering it is more context-dependent.

### Mechanism 3: Dependency on Contrastive Counterfactuals for Circuit Discovery
The identified circuits are highly sensitive to the specific contrastive datasets used for discovery and perform poorly when tested with alternative ablation methods like zero-ablation or mean-value replacements. This indicates the circuits capture specific pathways of memorization computation rather than the entire robust mechanism.

## Foundational Learning

- **k-extractability / Memorization Score**: Defines memorized sequences as those the model reproduces verbatim when prompted with k prior tokens from training data. Why needed: The methodology depends on identifying "memorized" sequences. Quick check: Given a 50-token prefix from training with 100% accuracy on next 50 tokens, what is the memorization score?

- **Transformer Circuit**: A minimal computational subgraph within a model that performs a specific function. Why needed: This is the core unit of analysis. Quick check: What is the primary criterion for evaluating if a discovered subgraph is a valid circuit?

- **Edge Attribution Patching with Integrated Gradients (EAP-IG)**: Uses gradients to approximate the importance of each edge in the computational graph for a given contrastive task. Why needed: This efficient method is used instead of slow activation patching. Quick check: What is the main advantage of EAP-IG over direct activation patching?

## Architecture Onboarding

- **Component map**: Full Model (GPT-Neo-125M) -> Contrastive Datasets (Memorization Decision, Branch Comparison) -> Circuit Discovery Engine (EAP-IG via AutoCircuit) -> Discovered Circuits (Memorization Decision ~141 edges, Branch Comparison ~14 edges) -> Evaluation Framework (faithfulness metrics)

- **Critical path**: 1) Data Curation: Identify memorized Wikipedia samples, 2) Divergence Point Identification: Find "Potentially Memorized" points, 3) Contrastive Pair Construction: Build decision and comparison datasets, 4) Circuit Discovery: Run EAP-IG to assign edge importance scores, 5) Circuit Extraction: Binary search for smallest faithful circuit, 6) Verification: Cross-task, cross-corpus, and alternative patching tests

- **Design tradeoffs**: EAP-IG chosen for efficiency but is an approximation; contrastive dataset specificity vs. robustness (circuits work well with specific counterfactuals but poorly with generic ablations); faithfulness threshold (binary search finds minimal circuit for given level)

- **Failure signatures**: Overly minimal circuits may be incomplete; metric blind spots (using multiple metrics like logit_diff, accuracy_mem, logprob_pred); out-of-distribution patching causes circuits to fail

- **First 3 experiments**: 1) Reproduce minimal circuit discovery on provided Wikipedia datasets, 2) Run cross-task verification testing Memorization Decision circuit on Branch Comparison dataset, 3) Test cross-corpus prevention applying Branch Comparison circuit to different Pile subsets

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the extremely compact circuits (14 edges) represent the complete memorization mechanism or merely a critical but incomplete component? The paper raises this in the conclusion, noting the circuits are highly faithful on specific tasks but show limited generalizability.

- **Open Question 2**: How do memorization mechanisms scale with model size and differ across architectures? The study is limited to GPT-Neo-125m, and future work should investigate scaling effects.

- **Open Question 3**: Are the identified circuits robust when verified using direct activation patching rather than gradient-based approximations? The paper notes EAP-IG is an approximation and proposes validating with direct activation patching.

## Limitations

- Circuit discovery is highly sensitive to specific contrastive datasets, with performance collapsing under zero-ablation or mean-value replacements
- The study is limited to a single small model (GPT-Neo-125m), limiting generalizability to larger architectures
- Cross-domain tests are restricted to only three text domains from The Pile, which may not represent diverse memorization contexts

## Confidence

**High Confidence**: The discovery methodology successfully identifies compact circuits, and the asymmetric generalization pattern is reproducible within tested domains.

**Medium Confidence**: The functional interpretation of circuit asymmetry could be influenced by size differences rather than pure specialization; circuit sensitivity to counterfactuals is well-demonstrated but may reflect incomplete discovery.

**Low Confidence**: Broader implications for memorization across diverse domains remain uncertain due to limited domain testing.

## Next Checks

1. Apply direct activation patching (not just EAP-IG) to verify discovered circuits remain minimal and faithful
2. Construct artificial size-matched pairs of initiation and maintenance circuits to isolate whether functional asymmetry is due to size or specialization
3. Test discovered circuits on memorization tasks from domains not in The Pile (e.g., biomedical text, legal documents) to validate generalization claims