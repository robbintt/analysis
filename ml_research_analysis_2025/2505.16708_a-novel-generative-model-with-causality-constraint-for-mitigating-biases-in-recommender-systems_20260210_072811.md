---
ver: rpa2
title: A Novel Generative Model with Causality Constraint for Mitigating Biases in
  Recommender Systems
arxiv_id: '2505.16708'
source_url: https://arxiv.org/abs/2505.16708
tags:
- latent
- lcdr
- recommendation
- causal
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses latent confounding bias in recommender systems,
  which can obscure true causal relationships between user feedback and item exposure,
  degrading recommendation performance. The proposed LCDR framework uses an identifiable
  Variational Autoencoder (iVAE) to learn latent causal representations, which then
  constrain the latent representations learned by a standard VAE through a unified
  loss function.
---

# A Novel Generative Model with Causality Constraint for Mitigating Biases in Recommender Systems

## Quick Facts
- **arXiv ID:** 2505.16708
- **Source URL:** https://arxiv.org/abs/2505.16708
- **Reference count:** 40
- **Primary result:** Proposes LCDR framework using iVAE to learn latent causal representations that mitigate latent confounding bias in recommender systems

## Executive Summary
This paper addresses the critical problem of latent confounding bias in recommender systems, where unobserved confounders create spurious correlations between user feedback and item exposure. The proposed LCDR framework leverages an identifiable Variational Autoencoder (iVAE) to learn causal representations, which are then used to constrain a standard VAE through an alignment loss. This approach enables recovery of latent confounders even from weak or noisy proxy variables, leading to more accurate and unbiased recommendations. Extensive experiments on three real-world datasets demonstrate that LCDR consistently outperforms state-of-the-art methods, achieving 19.4% higher NDCG@5 and 8.6% higher RECALL@5 on average compared to the best baseline.

## Method Summary
LCDR operates through a two-stage process: first, an iVAE learns identifiable latent causal representations Z from exposure data A and proxy variables W by conditioning both the prior and posterior on W, breaking the identifiability barrier of standard VAEs. Second, a standard VAE (LCVAE) learns representations Z_lc from A alone but is constrained through an L2 alignment loss ∥Z_lc - Z∥² that transfers the causal structure from iVAE. The unified loss function combines reconstruction fidelity with causal alignment, allowing the model to recover latent confounders even with noisy proxies. Finally, backdoor-style adjustment over learned Z_lc enables unbiased estimation of potential outcomes, mitigating the confounding bias in recommendations.

## Key Results
- LCDR achieves 19.4% higher NDCG@5 and 8.6% higher RECALL@5 on average compared to the best baseline method
- Performance peaks at intermediate alignment strength (λ = 0.1-0.9), declining at extremes - suggesting balance between constraint and reconstruction
- The method demonstrates robustness to noisy proxy variables while maintaining superior performance
- Ablation studies confirm the importance of the causal alignment constraint for debiasing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** iVAE recovers identifiable latent causal representations when proxy variables exhibit sufficient variability across different conditioning contexts.
- **Mechanism:** By conditioning both the prior p(Z|W) and approximate posterior q(Z|A,W) on proxy variables W and exposure A, the iVAE breaks the identifiability barrier in standard VAEs. This auxiliary conditioning enables recovery of latent structure up to a linear transformation under specific conditions on W's variability.
- **Core assumption:** Assumption 1 holds - the conditional distribution p(Z|W) follows an exponential family with sufficient statistics that are differentiable and linearly uncorrelated on measure-positive subsets, and there exist n_k + 1 distinct W points yielding an invertible matrix L.
- **Evidence anchors:** [abstract] "LCDR leverages an identifiable Variational Autoencoder (iVAE) as a causal constraint"; [section 4.2, Eq. 1-5] Derivation of iVAE objective with conditioning on W.
- **Break condition:** If proxy variables W lack sufficient variability (e.g., constant or near-constant across samples), matrix L becomes rank-deficient, and identifiability fails.

### Mechanism 2
- **Claim:** The L2 alignment loss ∥Z_lc − Z∥² transfers the causal structure recovered by iVAE to the standard VAE, enabling robust confounder recovery even with noisy proxies.
- **Mechanism:** The unified loss (Eq. 6) adds a regularization term III that penalizes deviation between LCVAE's Z_lc and iVAE's Z. This indirect constraint allows LCVAE to inherit iVAE's identifiability properties while maintaining VAE's reconstruction fidelity.
- **Core assumption:** The iVAE-learned Z approximates the true latent confounder distribution sufficiently well that aligning Z_lc toward Z preserves causal structure rather than amplifying noise.
- **Evidence anchors:** [section 4.3, Eq. 6] Explicit formulation of ℒ_LCVAE with KL divergence and L2 constraint; [section 5.3, Figure 4] Hyperparameter analysis shows performance peaks at intermediate λ (0.1-0.9).
- **Break condition:** If λ is too high, Z_lc collapses toward Z and loses task-relevant information; if too low, the causal constraint becomes ineffective and confounding bias persists.

### Mechanism 3
- **Claim:** Backdoor-style adjustment over learned Z_lc enables unbiased estimation of potential outcomes p(r^a_ui).
- **Mechanism:** Given p(Z_lc|A,W) and p(r_ui|A,W), the framework applies Eq. 13-16 to marginalize over latent confounders. The integration removes spurious correlation between exposure A and outcome R induced by Z_lc, yielding causal effect estimates.
- **Core assumption:** The conditional distribution p(Z_lc|A,W) is uniquely characterized by the factor model, and the causal graph in Figure 1 accurately represents the data-generating process.
- **Evidence anchors:** [section 4.4, Eq. 13-16] Formal derivation of potential outcome identification via integration over Z_lc; [section 3.2, Figure 1] DAG showing W → Z → A and Z → R structure.
- **Break condition:** If unobserved confounders exist outside Z_lc (i.e., the causal graph is misspecified), residual bias remains.

## Foundational Learning

### Concept: Variational Inference and ELBO
- **Why needed here:** Both iVAE and LCVAE are trained by maximizing Evidence Lower Bound; understanding the trade-off between reconstruction (term I) and regularization (term II/III) is essential for debugging loss dynamics.
- **Quick check question:** Can you explain why maximizing ELBO is equivalent to minimizing KL divergence between approximate and true posterior?

### Concept: Identifiability in Latent Variable Models
- **Why needed here:** The paper's core theoretical contribution hinges on iVAE achieving identifiability under specific conditions; practitioners must recognize when these conditions fail.
- **Quick check question:** Why does a standard VAE fail to achieve identifiability, and how does conditioning on auxiliary variables (W) resolve this?

### Concept: Causal Graphical Models and Backdoor Adjustment
- **Why needed here:** The debiasing mechanism relies on Pearl's backdoor criterion; incorrect graph specification invalidates causal claims.
- **Quick check question:** In Figure 1, why must we condition on Z_lc rather than W to block the backdoor path from A to R?

## Architecture Onboarding

### Component map:
- iVAE encoder: (A, W) → (μ_i, log_var_i) → Z via reparameterization
- LCVAE encoder: A → (μ, log_var) → Z_lc via reparameterization
- Alignment module: Computes ∥Z_lc − Z∥² as causal constraint
- Recommendation head: Concatenates Z_lc with user/item embeddings (MF: P_u, Q_i) → rating prediction
- Loss aggregator: ℒ_total = ℒ_LCVAE + ℒ_MF (Eq. 18)

### Critical path:
1. Forward pass through iVAE and LCVAE in parallel (lines 4-7, Algorithm 1)
2. Compute alignment loss (line 8)
3. Backpropagate combined loss to update both encoders (line 9)
4. After convergence, extract Z_lc for recommendation model training (lines 12-16)

### Design tradeoffs:
- **λ tuning:** High λ prioritizes causal alignment but risks over-regularization; dataset-specific tuning required (0.1 for Yahoo!R3, 0.9 for Coat/KuaiRand per Section 5.1)
- **Proxy variable selection:** Noisy proxies degrade iVAE identifiability; the method claims robustness to "weak or noisy" proxies but quality still matters
- **Backbone choice:** Paper uses MF for simplicity; richer models (e.g., LightGCN) may require architecture modifications

### Failure signatures:
- NDCG/Recall plateauing near baseline: Check if λ ≈ 0 (constraint disabled) or if proxy variables W are constant/near-constant
- Training instability: Verify gradient magnitudes from alignment loss aren't dominating reconstruction loss
- Identifiability violation: If matrix L (Eq. 25) is near-singular, increase proxy diversity or reduce latent dimension

### First 3 experiments:
1. **Ablation on λ:** Sweep λ ∈ {0.0, 0.1, 0.5, 0.9, 1.0} on a held-out validation set; plot NDCG@5 vs. λ to replicate Figure 4's U-shaped curve
2. **Proxy quality stress test:** Artificially corrupt W by adding Gaussian noise (σ ∈ {0.1, 0.5, 1.0}) and measure performance degradation to validate robustness claims
3. **Identifiability check:** Compute the condition number of matrix L (Eq. 25) for your dataset; if κ(L) > 10^6, proxy variability is insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the LCDR framework be effectively extended to mitigate selection bias in addition to latent confounding bias?
- **Basis in paper:** [explicit] The authors state in the Conclusion that they "plan to explore the potential of utilizing the LCDR model to address other challenges in recommender systems, such as selection bias."
- **Why unresolved:** The current methodology, theoretical proofs, and experiments are tailored exclusively to addressing latent confounding bias.
- **What evidence would resolve it:** A modified LCDR formulation targeting selection bias, validated on datasets specifically characterized by Missing Not At Random (MNAR) data patterns.

### Open Question 2
- **Question:** Does LCDR retain its debiasing advantages when integrated with complex, non-linear recommendation backbones like Graph Neural Networks?
- **Basis in paper:** [inferred] The methodology adopts Matrix Factorisation (MF) for "comparative purposes" and simplicity (Section 4.5), leaving the interaction between the proposed constraints and deep architectures untested.
- **Why unresolved:** It is unclear if the alignment loss interferes with the optimization landscapes or representation capacities of high-capacity models like DeepFM or LightGCN.
- **What evidence would resolve it:** Experiments replacing the MF backbone with deep learning models, comparing the constrained vs. unconstrained performance on the same benchmarks.

### Open Question 3
- **Question:** How does LCDR perform in the complete absence of proxy variables compared to scenarios with merely noisy proxies?
- **Basis in paper:** [inferred] The Limitations section claims LCDR "retains functionality even without these variables," but all empirical evaluations utilized available (though noisy) proxy variables.
- **Why unresolved:** The robustness claim is supported theoretically but lacks empirical verification regarding the magnitude of performance drop when proxies are totally absent.
- **What evidence would resolve it:** Ablation studies on the datasets where the proxy variable inputs $W$ are completely removed from the iVAE training process.

## Limitations

- The method's performance critically depends on the identifiability conditions for iVAE being satisfied, which requires specific properties of proxy variables that are not explicitly verified on real-world datasets
- All experiments use Matrix Factorization as the recommendation backbone, leaving uncertainty about how the alignment loss interacts with more complex architectures like Graph Neural Networks
- The causal graph structure is assumed to be correctly specified, but in practice this is difficult to validate and misspecification could leave residual bias

## Confidence

- **High confidence:** The experimental results showing superior NDCG@5 and RECALL@5 compared to baselines on three datasets are well-documented and reproducible given the described methodology.
- **Medium confidence:** The theoretical identifiability claims under Assumption 1 are sound mathematically, but their practical applicability depends on proxy variable properties that require empirical validation.
- **Medium confidence:** The mechanism by which the L2 alignment constraint transfers causal structure from iVAE to LCVAE is plausible given the loss formulation, though the exact sensitivity to hyperparameter λ across different datasets suggests some brittleness.

## Next Checks

1. **Condition number verification:** Compute and report the condition number of matrix L (Eq. 25) for each dataset to empirically verify that the identifiability conditions are met in practice.
2. **Proxy quality ablation:** Systematically vary the quality and informativeness of proxy variables W (e.g., by adding increasing amounts of noise or removing features) to quantify the method's sensitivity to proxy quality.
3. **Baseline expansion:** Implement and compare against additional strong baselines from causal recommendation literature (e.g., methods from arXiv:2510.12325) to more comprehensively situate LCDR's performance.