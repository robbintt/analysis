---
ver: rpa2
title: Low-N Protein Activity Optimization with FolDE
arxiv_id: '2510.24053'
source_url: https://arxiv.org/abs/2510.24053
tags:
- mutants
- protein
- folde
- activity
- mutant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FolDE is an active learning method for protein activity optimization
  that addresses the challenge of limited experimental resources by iteratively predicting
  and testing mutants. It introduces naturalness-based warm-starting, which pretrains
  activity prediction models on protein language model outputs before fine-tuning
  on measured data, preserving model performance across rounds.
---

# Low-N Protein Activity Optimization with FolDE

## Quick Facts
- arXiv ID: 2510.24053
- Source URL: https://arxiv.org/abs/2510.24053
- Reference count: 18
- Primary result: 23% more top 10% mutants and 55% more likely to find top 1% mutants compared to state-of-the-art baselines

## Executive Summary
FolDE is an active learning method for protein activity optimization that addresses the challenge of limited experimental resources by iteratively predicting and testing mutants. It introduces naturalness-based warm-starting, which pretrains activity prediction models on protein language model outputs before fine-tuning on measured data, preserving model performance across rounds. The method also employs a constant-liar batch selector to promote diversity in selected mutants. In benchmarks across 20 protein targets, FolDE discovers 23% more top 10% mutants and is 55% more likely to find top 1% mutants compared to state-of-the-art baselines. The complete workflow is available as open-source software.

## Method Summary
FolDE implements an active learning workflow for protein activity optimization with limited experimental resources (16 mutants per round, 3 rounds total). The method uses ESMC-300M embeddings (960-dim) and an ensemble of 5 MLPs trained with Bradley-Terry ranking loss. Round 1 selects mutants based on zero-shot naturalness scores from protein language models. Rounds 2+ use a two-phase training approach: warm-starting on all single mutants' naturalness scores (50 epochs) followed by fine-tuning on measured activities (200 epochs). A constant-liar batch selector promotes diversity by assuming pessimistic outcomes for already-selected mutants, updating the covariance structure to downweight similar candidates. The workflow is available as open-source software.

## Key Results
- 23% more top 10% mutants discovered compared to state-of-the-art baselines
- 55% more likely to find at least one top 1% mutant within 3 rounds
- Round-2 model collapse prevented: Spearman correlation maintained at ~0.48 vs dropping to ~0.04 without warm-start

## Why This Works (Mechanism)

### Mechanism 1: Naturalness Warm-Starting Preserves Cross-Round Model Quality
Pretraining activity prediction models on PLM naturalness scores before fine-tuning on biased round-1 measurements prevents predictive accuracy collapse in subsequent rounds. By training on all single mutants rather than just the high-naturalness mutants selected in round-1, the procedure exposes the model to the full spectrum of mutant quality.

### Mechanism 2: Constant-Liar Batch Selection Promotes Diversity via Covariance Propagation
Pessimistic "lie" assumptions about already-selected mutants propagate through ensemble covariance to downweight similar candidates, improving batch diversity. After selecting top-scoring mutant, the algorithm assumes it performs at minimum observed activity, updating the Gaussian Process posterior via covariance structure.

### Mechanism 3: Ranking Loss Enables Scale-Invariant Two-Phase Training
Bradley-Terry ranking loss outperforms regression by learning pairwise preferences, enabling naturalness pretraining followed by activity fine-tuning. Scale invariance permits warm-start on naturalness (log-likelihood units) then fine-tune on activity (arbitrary units).

## Foundational Learning

- **Active Learning / Bayesian Optimization**: Understanding how prediction uncertainty guides experimental selection to debug poor campaign performance. Quick check: Can you explain why UCB (upper confidence bound) didn't improve batch diversity in this paper's benchmarks?
- **Exploration-Exploitation Tradeoff in Sequential Decision Making**: Understanding how round-1 exploitation can harm round-2 model quality. Quick check: If your round-2 predictions are no better than random despite good round-1 performance, which side of the tradeoff was over-weighted?
- **Protein Language Model Embeddings and Naturalness**: Understanding what PLMs encode (evolutionary constraints, not activity directly) to predict when warm-start will transfer vs. fail. Quick check: Why might naturalness correlate poorly with activity for a de novo designed enzyme?

## Architecture Onboarding

- **Component map**: ESMC-300M -> MLP Ensemble (5 models, 960→100→50→1) -> Bradley-Terry ranking loss -> Constant-liar batch selector
- **Critical path**: 1) Generate candidate mutants, 2) Embed with ESMC-300M, 3) Round 1: Rank by naturalness, 4) Rounds 2+: Warm-start → fine-tune → predict with uncertainty → constant-liar batch selection
- **Design tradeoffs**: ESMC-300M vs ESM2-15b (similar performance, use smaller model unless GPU headroom); α=6 vs lower (lower α improves diversity but slightly reduces top-10% hit count); 5 ensemble members (more = better uncertainty but linear compute cost)
- **Failure signatures**: Round-2 Spearman ρ drops below 0.2 (warm-start not applied or weak naturalness correlation); all batch mutations target same 2-3 loci (α too high or constrained candidate pool); zero top-1% mutants after 3 rounds (PLM prior misaligned)
- **First 3 experiments**: 1) Validate naturalness correlation for your target (Spearman ρ>0.3), 2) Ablate warm-start on training split (confirm round-2 collapse prevention), 3) Tune α on multi-mutation proxy (test α∈{1,3,6,100} on similar ProteinGym dataset)

## Open Questions the Paper Calls Out

- **Open Question 1**: Would more theoretically motivated batch selection methods like parallel knowledge gradient outperform the constant-liar heuristic for selecting diverse mutant batches? The authors did not compare against provably optimal batch acquisition methods.
- **Open Question 2**: Does FolDE maintain its performance advantage in campaigns exploring sequence spaces with three or more cumulative mutations from wild-type? The multi-mutation benchmark includes only double mutants, yet real campaigns routinely explore deeper sequence spaces.
- **Open Question 3**: Can naturalness-based warm-starting improve optimization efficiency for other scientific foundation models beyond protein language models? The approach was only demonstrated on protein targets.

## Limitations

- The constant-liar diversity mechanism shows improved locus exploration but doesn't translate to better top-line optimization metrics in benchmarks
- Naturalness-activity correlation assumption is reasonable but not universally validated across all protein classes
- The core warm-starting claim lacks external validation beyond the paper's own benchmarks

## Confidence

**High confidence**: Round-2 model collapse without warm-start (Spearman ρ dropping from ~0.48 to ~0.04) is clearly demonstrated with direct evidence from ablation.

**Medium confidence**: Naturalness-activity correlation assumption is reasonable but not universally validated. The 23% more top 10% mutants claim is benchmarked but diversity benefit from constant-liar is not clearly translated to optimization gains.

**Low confidence**: Claim that FolDE "addresses the challenge of limited experimental resources" more effectively than alternatives is partially supported - while diversity improves, actual optimization performance gains are modest.

## Next Checks

1. **Validate naturalness correlation for your target**: Compute Spearman correlation between PLM naturalness scores and known activity measurements for your protein class. FolDE requires ρ>0.3 for reliable warm-starting.

2. **Ablate warm-start on held-out data**: Hold out 20% of mutants from your target protein, run 3-round simulations with and without warm-start, confirm that round-2 predictions remain correlated (ρ>0.3) only with warm-start applied.

3. **Benchmark constant-liar diversity**: For your protein optimization campaign, test α∈{1,3,6,100} on similar ProteinGym datasets. Measure unique loci explored per batch and compare to top-line optimization metrics to validate the diversity-exploitation tradeoff.