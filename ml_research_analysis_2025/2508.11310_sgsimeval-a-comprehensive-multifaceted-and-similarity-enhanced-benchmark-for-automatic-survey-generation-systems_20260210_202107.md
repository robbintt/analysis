---
ver: rpa2
title: 'SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark
  for Automatic Survey Generation Systems'
arxiv_id: '2508.11310'
source_url: https://arxiv.org/abs/2508.11310
tags:
- content
- systems
- survey
- evaluation
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SGSimEval, a comprehensive benchmark for
  evaluating automatic survey generation systems that integrates outline quality,
  content adequacy, and reference appropriateness assessments. The proposed similarity-enhanced
  framework combines LLM-based scoring with quantitative metrics, introducing human
  preference metrics that emphasize both inherent quality and similarity to human-authored
  surveys.
---

# SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems

## Quick Facts
- arXiv ID: 2508.11310
- Source URL: https://arxiv.org/abs/2508.11310
- Reference count: 32
- Introduces SGSimEval benchmark that reveals CS-specific ASG systems outperform general-domain approaches, with most ASG systems exceeding human performance in outline generation but humans maintaining substantial lead in reference quality

## Executive Summary
This paper introduces SGSimEval, a comprehensive benchmark for evaluating automatic survey generation systems that integrates outline quality, content adequacy, and reference appropriateness assessments. The proposed similarity-enhanced framework combines LLM-based scoring with quantitative metrics, introducing human preference metrics that emphasize both inherent quality and similarity to human-authored surveys. Extensive experiments on 80 highly-cited surveys across five representative ASG systems reveal that computer science-specialized systems consistently outperform general-domain approaches, with most ASG systems exceeding human performance in outline generation. However, significant challenges remain in reference curation, where human-generated references maintain substantially higher quality. The evaluation metrics demonstrate strong consistency with human assessments, providing a more nuanced and reliable evaluation framework for automated survey generation.

## Method Summary
The method constructs a dataset of 80 highly-cited arXiv survey papers, parses them using MinerU, and decomposes each into outline, content, and reference components. Contextual embeddings are generated using Text-Embedding-V3 and stored in ChromaDB. Five representative ASG systems are evaluated using a multi-dimensional framework: outline assessed via hierarchy scores and LLM coherence; content evaluated through faithfulness (citation support) and five-dimension LLM quality metrics; references scored by supportiveness and LLM quality. The similarity-enhanced evaluation blends intrinsic quality scores with human reference alignment using cosine similarity weights, offering Vanilla, Human-as-Perfect, and Balanced variants. All LLM judgments use Qwen-Plus-2025-04-28 with specific prompt engineering.

## Key Results
- CS-specific ASG systems (AutoSurvey, SurveyForge) consistently outperform general-domain systems across all dimensions
- Most ASG systems exceed human performance in outline generation, but humans maintain substantial lead in reference quality
- Similarity-enhanced metrics demonstrate strong consistency with human assessments while providing more nuanced evaluation
- Reference supportiveness scores reveal significant gaps: SurveyForge (81.34%) vs. SurveyX (39.04%) under vanilla evaluation

## Why This Works (Mechanism)

### Mechanism 1
Similarity-weighted evaluation better aligns with human preferences than vanilla LLM scoring alone. The framework computes cosine similarity between ASG-generated and human-authored embeddings, then uses this as a weighting factor to blend intrinsic quality scores with human reference alignment. Human-as-Perfect weighting assumes Q₀=5 for human content; Balanced weighting incorporates actual human quality scores. Core assumption: High-quality academic surveys share structural and semantic patterns; similarity to human-authored work indicates quality. Evidence anchors: [abstract] "introduces human preference metrics that emphasize both inherent quality and similarity to humans"; [section 3.3] "Qk′_i = σk_i · Q0 + (1 − σk_i) · Qk_i" and "Qk′′_i = σk_i · Qk_human,i + (1 − σk_i) · Qk_i". Break condition: If human-authored surveys in a domain are low-quality or idiosyncratic, treating them as reference standards may penalize novel but valid ASG outputs.

### Mechanism 2
Decomposing evaluation into outline, content, and references reveals dimension-specific system strengths and weaknesses. Each survey component undergoes specialized assessment—outline via depth-weighted hierarchy scoring; content via faithfulness (citation support) and 5-dimension LLM quality assessment; references via supportiveness scoring. This separation prevents aggregate scores from masking critical failures. Core assumption: Survey quality is multi-dimensional and requires component-specific evaluation criteria. Evidence anchors: [abstract] "integrating assessments of the outline, content, and references"; [section 5.1] "CS-specific systems consistently outperform general-domain systems across all dimensions" while "humans maintain a substantial lead in reference quality". Break condition: If component boundaries are ambiguous (e.g., outline deeply intertwined with content logic), decomposition may double-count or miss cross-cutting quality factors.

### Mechanism 3
Domain-specialized ASG systems outperform general-domain systems due to targeted retrieval databases. CS-specific systems (AutoSurvey, SurveyForge) leverage domain-specific databases for retrieval, yielding higher reference relevance and content adequacy. General-domain systems rely on broader web search, diluting reference quality. Core assumption: Reference quality is tightly coupled to retrieval corpus domain specificity. Evidence anchors: [section 5.1] "systems reliant on general online search tend to exhibit weaker reference quality"; [table 2] Reference supportiveness scores: SurveyForge (81.34%) vs. SurveyX (39.04%) under vanilla evaluation. Break condition: If a specialized database is incomplete or biased, domain-specific systems may propagate domain gaps rather than overcome them.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: All evaluated ASG systems use RAG to ground survey content in retrieved literature; evaluation metrics assess whether retrieved references actually support claims
  - Quick check question: Can you explain how a retrieved document becomes a cited reference in an LLM-generated survey?

- Concept: LLM-as-a-Judge paradigm
  - Why needed here: SGSimEval relies on LLMs to score hierarchy coherence, content quality dimensions, and reference relevance; understanding biases and limitations of this approach is critical
  - Quick check question: What are two failure modes when using an LLM to evaluate another LLM's output?

- Concept: Embedding-based semantic similarity
  - Why needed here: The similarity-enhanced framework depends on vector embeddings to compute alignment between generated and human-authored surveys
  - Quick check question: Why might cosine similarity between embeddings fail to capture structural differences in document organization?

## Architecture Onboarding

- Component map: Data Collection → Topic Mining (LLM-based topic extraction) → Decomposition (rule-based outline/content/reference splitting) → Embedding Generation (Text-Embedding-V3, stored in ChromaDB) → Evaluation (Vanilla / Balanced / Human-as-Perfect weighting)

- Critical path:
  1. Parse 80 human-authored surveys (MinerU PDF extraction → rule-based component extraction)
  2. Generate contextual embeddings for outline paths, content sections, reference entries
  3. For each ASG output: compute intrinsic quality scores (LLM + quantitative metrics) → compute similarity weights → blend into final scores

- Design tradeoffs:
  - Human-as-Perfect vs. Balanced: HP rewards human-alignment but may penalize novel high-quality outputs; Balanced is more nuanced but requires human content quality scoring
  - Nk selection: Small N (5 for outline/content) prioritizes precision; large N (20 for references) improves recall but adds noise
  - Single LLM judge (Qwen-Plus) vs. multi-model ensemble: consistency vs. potential systematic bias

- Failure signatures:
  - Reference supportiveness scores below 50% indicate retrieval pipeline issues (wrong corpus or poor query formulation)
  - Content faithfulness below 70% suggests hallucination or weak citation grounding
  - Large divergence between Vanilla and HP scores may indicate high ASG creativity but low human-alignment

- First 3 experiments:
  1. Reproduce table 2 results on 10 survey topics to validate pipeline correctness (compare Vanilla, Balanced, HP scores)
  2. Ablate similarity weighting: set σk_i = 0 to measure pure intrinsic quality vs. similarity-weighted scores
  3. Test cross-domain transfer: evaluate CS-specialized systems on non-CS topics to confirm domain-specific performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SGSimEval framework's LLM-based evaluation correlate with direct human expert assessments?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "future work must include direct comparisons with human expert scores to robustly validate the reliability of our LLM evaluator."
- Why unresolved: Current validation relies on LLM-based pairwise comparisons (using Qwen models) rather than ground-truth human annotation, leaving potential circular reasoning and stylistic biases unverified
- What evidence would resolve it: A study measuring the correlation coefficient between SGSimEval scores and human expert rankings on the same set of generated surveys

### Open Question 2
- Question: Does the reliance on semantic similarity weighting penalize novel survey structures that diverge from existing human patterns?
- Basis in paper: [inferred] The paper acknowledges the "Balanced Similarity Weighting" approach is designed to ensure high-quality work is "not unfairly penalized for deviating from the human reference," but does not quantify this trade-off
- Why unresolved: The current framework inherently rewards alignment with the status quo of human-authored surveys, making it difficult to distinguish between low-quality structural incoherence and high-quality structural innovation
- What evidence would resolve it: Evaluation of ASG systems specifically prompted to generate unconventional survey structures, comparing their "Balanced" scores against independent assessments of logical coherence

### Open Question 3
- Question: Can the evaluation framework generalize effectively to domains outside of Computer Science and arXiv-based literature?
- Basis in paper: [inferred] The dataset is curated exclusively from 80 "highly-cited survey papers from arXiv" and evaluates systems restricted to or optimized for the Computer Science domain
- Why unresolved: Structural norms and citation practices in scientific surveys may differ significantly from those in the Humanities or Social Sciences, potentially biasing the "Human-as-Perfect" similarity metrics
- What evidence would resolve it: Applying SGSimEval to a multi-domain dataset and analyzing the variance in "Human-as-Perfect" weighting factors across different fields

## Limitations
- Evaluation framework's reliance on LLM-based judgments introduces potential systematic biases, though partially mitigated by similarity-enhanced weighting approach
- Human-as-perfect assumption may not hold across all domains, particularly where human-authored surveys exhibit significant quality variance
- Decomposition rules for parsing survey components lack detailed specification for edge cases in survey structure

## Confidence
- **High Confidence**: Component-wise evaluation approach and observed domain-specific performance gaps between CS-specialized and general ASG systems
- **Medium Confidence**: Similarity-enhanced weighting effectiveness, given limited independent validation of this specific mechanism
- **Medium Confidence**: Reference quality assessment, due to potential variability in citation support evaluation across domains

## Next Checks
1. Replicate the ablation study showing the impact of similarity weighting by setting σ=0 and comparing to the full similarity-enhanced scores
2. Test cross-domain generalizability by evaluating CS-specialized systems on non-CS survey topics
3. Conduct human evaluation study to validate whether the similarity-enhanced scores better align with expert assessments than vanilla LLM scores alone