---
ver: rpa2
title: Concealed Adversarial attacks on neural networks for sequential data
arxiv_id: '2502.20948'
source_url: https://arxiv.org/abs/2502.20948
tags:
- attack
- adversarial
- attacks
- data
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel concealed adversarial attack method
  for time series data, addressing the challenge of creating realistic perturbations
  that are hard to detect by both humans and machine learning models. The proposed
  approach simultaneously optimizes a target model's loss and a discriminator's loss,
  with the discriminator trained to detect adversarial examples.
---

# Concealed Adversarial attacks on neural networks for sequential data

## Quick Facts
- arXiv ID: 2502.20948
- Source URL: https://arxiv.org/abs/2502.20948
- Reference count: 40
- Key outcome: Novel method for creating realistic, hard-to-detect adversarial perturbations on time series data that outperforms existing approaches

## Executive Summary
This paper addresses the challenge of creating concealed adversarial attacks for time series data, which must balance effectiveness against the target model with realism to avoid detection by both humans and other ML models. The proposed approach introduces a discriminator-based framework that is trained to detect adversarial examples, with the attack optimization process simultaneously targeting both the model's loss and the discriminator's loss. A key innovation is the progressive training procedure for the discriminator, which handles attacks of varying strengths throughout the attack generation process.

Experiments across six UCR time series datasets and four architectures (ResCNN, RNNAttention, S4, PatchTST) demonstrate the superiority of this approach in balancing attack efficiency and concealability. The method significantly outperforms vanilla attacks and the SGM baseline, achieving success rates often exceeding 0.8 across various model-dataset combinations. The work highlights the growing challenge of designing robust time series models against realistic and effective attacks.

## Method Summary
The proposed method introduces a novel adversarial attack framework for time series data that incorporates a discriminator trained to detect adversarial examples. The attack optimization process jointly minimizes the target model's loss and maximizes the discriminator's loss, creating perturbations that are both effective and concealed. A progressive training procedure for the discriminator is employed, where the discriminator is exposed to attacks of increasing strength during the attack generation process. This approach addresses the unique challenges of time series data, where perturbations must remain realistic and hard to detect while maintaining attack effectiveness. The method is evaluated across multiple UCR time series datasets and architectures, demonstrating superior performance in balancing attack efficiency and concealability compared to baseline approaches.

## Key Results
- The proposed concealed attack method significantly outperforms vanilla attacks and the SGM baseline across six UCR datasets and four architectures
- Success rates frequently exceed 0.8, demonstrating high attack effectiveness while maintaining concealability
- The discriminator-based approach successfully creates perturbations that are hard to detect by both humans and ML models
- The progressive discriminator training procedure proves effective in handling attacks of varying strengths

## Why This Works (Mechanism)
The method works by creating a adversarial game between the attack generator and a discriminator trained to detect adversarial examples. By jointly optimizing for both the target model's loss and the discriminator's loss, the attack learns to create perturbations that are both effective against the target and difficult to distinguish from normal data. The progressive training of the discriminator, where it is exposed to increasingly strong attacks during the generation process, allows it to effectively detect a wide range of attack strengths. This dual-objective optimization creates a natural tension that pushes the attack toward solutions that are both successful and concealed.

## Foundational Learning
- **Time series data characteristics**: Time series have temporal dependencies and local patterns that make them particularly sensitive to perturbation methods - understanding these properties is crucial for designing effective attacks.
- **Adversarial attack transferability**: Attacks that work well on one model may not transfer to others, requiring careful consideration of attack design for different architectures.
- **Discriminator-based attack detection**: Using a separate model to detect adversarial examples provides a different perspective than relying solely on perturbation magnitude metrics.
- **Progressive training methodology**: Gradually increasing difficulty during training (here, attack strength) can lead to more robust models or attacks.
- **Concealability metrics**: Beyond attack success rate, measuring how detectable an attack is requires specialized evaluation metrics that capture human perception and model detection capabilities.

## Architecture Onboarding

**Component map**: Attack generator -> Target model -> Discriminator -> Attack generator feedback loop

**Critical path**: 
1. Initialize attack generator with clean input
2. Generate adversarial perturbation by optimizing both target model loss and discriminator loss
3. Discriminator evaluates whether example is adversarial
4. Update attack generator based on combined loss
5. Repeat until convergence or attack budget exhausted

**Design tradeoffs**: 
- Balance between attack strength and concealability requires careful weighting of the two loss components
- Progressive discriminator training adds computational overhead but improves detection capability
- Joint optimization of both losses can lead to unstable training requiring careful hyperparameter tuning

**Failure signatures**:
- If discriminator is too weak, attacks become detectable by simple metrics
- If target model loss dominates, attacks become less concealed
- If discriminator is too strong, attacks may fail to converge or become ineffective

**3 first experiments**:
1. Compare attack success rates on a simple dataset (e.g., ECG5000) across different architectures
2. Evaluate detectability of attacks using both human evaluation and automated metrics
3. Test the impact of discriminator strength on the balance between attack effectiveness and concealability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on UCR time series datasets, which may not represent all real-world sequential data applications
- Effectiveness across different data modalities and larger, more complex datasets remains unexplored
- Long-term effectiveness against evolving defensive strategies is yet to be established

## Confidence
High: The method significantly outperforms existing approaches on tested UCR datasets and architectures
Medium: Claims regarding general superiority across all potential time series applications and real-world scenarios
Medium: Claims about the long-term effectiveness of the progressive discriminator training against evolving defenses

## Next Checks
1. Evaluate the method's performance on larger, more diverse time series datasets from various domains (e.g., healthcare, finance, IoT sensor data) to assess its generalizability
2. Conduct extensive experiments to determine the method's effectiveness against state-of-the-art defensive mechanisms, including adversarial training and input preprocessing techniques
3. Investigate the computational efficiency and scalability of the proposed approach for real-time applications and large-scale time series data processing