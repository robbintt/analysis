---
ver: rpa2
title: Agentic generative AI for media content discovery at the national football
  league
arxiv_id: '2510.07297'
source_url: https://arxiv.org/abs/2510.07297
tags:
- media
- content
- language
- agentic
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes an agentic generative AI system developed for
  the NFL to enable natural language search of historical game footage. The system
  translates user queries into structured database API calls using LLM-based entity
  extraction, schema selection, and API formulation, with semantic caching to improve
  accuracy and latency.
---

# Agentic generative AI for media content discovery at the national football league

## Quick Facts
- arXiv ID: 2510.07297
- Source URL: https://arxiv.org/abs/2510.07297
- Reference count: 28
- The paper describes an agentic generative AI system developed for the NFL to enable natural language search of historical game footage.

## Executive Summary
The paper presents an agentic generative AI system that enables natural language search of historical NFL game footage by translating user queries into structured database API calls. The system achieves over 95% accuracy and reduces search time from 10 minutes to 30 seconds through LLM-based entity extraction, schema selection, and API formulation with semantic caching. Deployed across NFL media teams, it enables faster retrieval of relevant video content and allows users to focus more on creative production.

## Method Summary
The system implements a LangGraph-orchestrated agentic workflow that processes natural language queries through five stages: intent classification via Claude 3 Haiku, entity/action/condition extraction, schema routing with semantic caching, API formulation with few-shot examples and step-by-step reasoning, and execution with up to three auto-correction retries. Queries are semantically cached using placeholder substitution for player/team names, enabling reuse of expensive reasoning when structurally similar queries recur. The workflow maintains conversational context across turns and integrates with NFL's Next Gen Stats data in OpenSearch and Media Asset Management system for video retrieval.

## Key Results
- Achieved over 95% accuracy on a test set of 100 queries
- Reduced average search time from 10 minutes to 30 seconds
- Successfully deployed across NFL media teams for faster video content retrieval

## Why This Works (Mechanism)

### Mechanism 1: Agentic Query Decomposition with Schema-Grounded API Formulation
- Claim: Decomposing natural language queries into structured primitives and grounding LLM generation in explicit schema definitions enables reliable translation to executable database API calls.
- Mechanism: A low-latency model classifies intent; an extraction LLM parses entities/teams, actions, and conditions; a router LLM selects relevant NGS schemas; an API-formulation LLM receives schema context with field definitions and few-shot examples, then constructs OpenSearch API calls step-by-step with auto-correction on syntax failures (up to 3 retries).
- Core assumption: Natural language queries can be deterministically mapped to database primitives when schemas provide sufficient field-level semantics and few-shot demonstrations.
- Evidence anchors: Abstract states the workflow "breaks it into elements, and translates them into the underlying database query language." Section 4.1 details extraction of "Patrick Mahomes" (Entity), "touchdown throw" (Action), ">10 yards" (Condition); describes router selecting schemas and passing only relevant context; API-formulation LLM "reasons step-by-step" with schema reference and few-shots.

### Mechanism 2: Semantic Caching with Placeholder Substitution
- Claim: Caching prior queries and API structures—with player/team names replaced by placeholders—enables reuse of expensive reasoning when semantically similar queries recur, improving latency and reducing error surface.
- Mechanism: Store prior queries with placeholders (e.g., "[PLAYER]", "[TEAM]"); on new query, perform vector similarity search; if high similarity score, retrieve cached API call template, substitute fresh IDs, and execute directly—bypassing entity extraction, schema selection, and API formulation.
- Core assumption: Query patterns are sufficiently repetitive across users and time that cache hits occur frequently enough to justify cache maintenance overhead.
- Evidence anchors: Abstract states "Accuracy and latency are further improved through carefully designed semantic caching." Section 4.1 explains "When a new query gets submitted, the system first applies a vector search to find similar queries in the past. If a high score is achieved... the cached API calls with placeholders will be replaced with the fresh IDs and executed directly."

### Mechanism 3: Conversational Context Inheritance
- Claim: Maintaining conversational context across turns allows follow-up queries to implicitly inherit prior entities and constraints, reducing user cognitive load and enabling iterative refinement.
- Mechanism: After an initial query resolves entities, subsequent queries in the same session preserve resolved IDs; LLM rephrases follow-ups to incorporate inherited context (e.g., "What about all the throws that were intercepted?" → "Find all the plays where Patrick Mahomes throws are intercepted").
- Core assumption: Users predominantly engage in multi-turn research flows where context inheritance reduces friction without introducing confusion.
- Evidence anchors: Section 4.1 states "Follow-up conversations inherit prior context... The LLM implicitly keeps 'Patrick Mahomes' as the entity, and will rephrase the user's prompt."

## Foundational Learning

- Concept: Text-to-SQL / Text-to-API translation
  - Why needed here: Core capability enabling natural language queries to be converted into structured OpenSearch API calls against NGS schemas.
  - Quick check question: Given the schema field "passYards (range float, 0-inf)" and "touchdown (singular integer, 0-1)", how would you map "touchdown passes over 10 yards" to API filters?

- Concept: Semantic similarity and vector search
  - Why needed here: Underpins semantic caching—determining whether a new query is structurally similar to a cached query template.
  - Quick check question: If two queries have different player names but identical action/condition structure, what embedding strategy would maximize cache hit rate while preserving correctness?

- Concept: Agentic workflow orchestration (LangGraph)
  - Why needed here: Coordinates multi-step reasoning (intent classification → extraction → routing → formulation → execution → summarization) with state management and error handling.
  - Quick check question: If the API formulation step fails with a syntax error after 3 retries, what should the workflow state contain to enable graceful user re-prompting?

## Architecture Onboarding

- Component map:
  Frontend -> Intent Classification -> Entity Extraction -> Semantic Cache Check -> Schema Routing -> API Formulation -> OpenSearch Execution -> Result Summarization -> MAM Link Retrieval -> Response

- Critical path:
  Query → Intent classification (guardrail) → Entity extraction → Semantic cache check → [if miss] Schema routing → API formulation → OpenSearch execution (with auto-correction) → Result summarization → MAM link retrieval → Response with embedded media links

- Design tradeoffs:
  - Context length vs. routing accuracy: Passing only relevant schemas reduces token cost but requires accurate schema selection; incorrect routing propagates to formulation.
  - Latency vs. accuracy: Semantic caching improves latency but assumes structural similarity implies semantic equivalence; rare edge cases may return incorrect results.
  - Retry budget vs. user experience: 3 auto-correction attempts balance robustness against user frustration; exhausted retries force rephrasing.

- Failure signatures:
  - API syntax errors after 3 retries → user prompted to rephrase
  - Ambiguous player names (duplicates) → disambiguation prompt shown
  - Schema selection errors → API formulation uses wrong fields, returns zero/incorrect results
  - Semantic cache false positive → structurally similar but semantically different query returns incorrect cached results

- First 3 experiments:
  1. Measure semantic cache hit rate and accuracy on a held-out production query sample; quantify latency improvement vs. error rate tradeoff.
  2. A/B test full-schema vs. routed-schema context on API formulation accuracy for medium/complex queries (50%/20% of test set per Table 3).
  3. Profile latency distribution across workflow stages (classification, extraction, routing, formulation, execution) to identify optimization targets; validate reported 30-second average against baseline 10-minute manual search.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system's latency and cost-efficiency scale when transitioning from a limited group of internal media editors to a fan-facing version serving hundreds of millions of users?
- Basis in paper: The conclusion states, "In the future, there are opportunities to... create fan-facing version of the solution, providing richer contents and offerings to hundreds of millions of avid football fans worldwide."
- Why unresolved: The current deployment involves 250 onboarded users. The infrastructure implications (rate limiting, concurrency, inference costs) of scaling to millions of concurrent requests are not discussed.
- What evidence would resolve it: Load testing results or cost modeling simulations demonstrating system behavior under high-concurrency traffic typical of consumer-facing applications.

### Open Question 2
- Question: To what extent does semantic caching introduce error when handling queries with high semantic similarity but different logical constraints?
- Basis in paper: The paper claims semantic caching improves accuracy and latency. However, relying on vector similarity to bypass expensive reformulation risks missing subtle logical negations or boundary conditions if the cache retrieval is purely similarity-based.
- Why unresolved: The paper does not provide an ablation study on the failure rates of the semantic cache or how "high score" thresholds are tuned to prevent false positive cache hits.
- What evidence would resolve it: Evaluation of retrieval accuracy specifically on the semantic caching module using a dataset of adversarial "near-miss" queries.

### Open Question 3
- Question: How can the agentic workflow be adapted to process natural language queries that rely on visual features absent from the structured tracking data?
- Basis in paper: The methodology relies entirely on translating text to the Next Gen Stats (NGS) database API. NGS data is sensor-based; it likely does not contain rich visual descriptors (e.g., "crowd noise," "weather conditions," "specific camera angles") that media editors might search for.
- Why unresolved: The current architecture assumes a direct mapping from language to structured API fields. It does not address the "semantic gap" where user intent exists only in the video pixels, not the metadata.
- What evidence would resolve it: A technical extension demonstrating the integration of multimodal embeddings (video+text) into the agentic workflow to handle non-sensor-based search criteria.

## Limitations

- The semantic caching mechanism's effectiveness depends on unstated implementation details including embedding model, similarity threshold, and caching policy.
- The paper lacks per-stage accuracy breakdowns and statistical validation (confidence intervals, per-stage breakdowns, or A/B test results).
- Schema selection mechanism does not address cross-schema query patterns or how the system handles queries spanning multiple domains.

## Confidence

- **High confidence**: The agentic workflow architecture (intent classification → extraction → routing → formulation → execution) is well-specified and technically sound. The OpenSearch integration with schema-grounded API formulation is clearly described.
- **Medium confidence**: The reported >95% accuracy and 30-second latency improvement are plausible given the architecture, but lack statistical validation (confidence intervals, per-stage breakdowns, or A/B test results). The semantic caching mechanism's effectiveness depends on unstated implementation details.
- **Low confidence**: The conversational context inheritance mechanism is minimally specified, with no details on context window limits, entity resolution conflicts, or user experience for disambiguation scenarios.

## Next Checks

1. **Semantic cache validation**: Measure cache hit rate, false positive rate, and accuracy degradation on a held-out production query sample. Compare latency and accuracy against baseline (no caching) to quantify the claimed improvements.

2. **Per-stage accuracy analysis**: Instrument the workflow to log confidence scores, error rates, and user correction patterns for each LLM stage. Identify whether accuracy bottlenecks are in entity extraction, schema selection, or API formulation.

3. **Cross-schema query testing**: Create a test suite of complex queries spanning multiple schemas (e.g., "Find all games where a quarterback threw for 300+ yards and the defense recorded 3+ sacks"). Measure schema selection accuracy and API formulation success rate for these edge cases.