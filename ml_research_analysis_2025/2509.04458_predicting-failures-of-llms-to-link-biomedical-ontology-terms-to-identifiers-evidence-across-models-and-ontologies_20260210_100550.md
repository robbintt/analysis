---
ver: rpa2
title: Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers
  Evidence Across Models and Ontologies
arxiv_id: '2509.04458'
source_url: https://arxiv.org/abs/2509.04458
tags:
- terms
- ontology
- term
- biomedical
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We analyzed why large language models (LLMs) often fail to correctly
  link biomedical ontology terms to their unique identifiers. Using GPT-4o and LLaMa
  3.1 405B, we evaluated 18,988 Human Phenotype Ontology (HPO) and 4,023 Gene Ontology
  (GO-CC) terms across nine features capturing term familiarity, identifier exposure,
  and ontology structure.
---

# Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies

## Quick Facts
- arXiv ID: 2509.04458
- Source URL: https://arxiv.org/abs/2509.04458
- Reference count: 33
- Key outcome: Usage frequency in PubMed Central and curated annotations—not term morphology or ontology structure—are the strongest predictors of linking success for biomedical ontology terms.

## Executive Summary
This study investigates why large language models (LLMs) fail to correctly link biomedical ontology terms to their unique identifiers. Using GPT-4o and LLaMa 3.1 405B, researchers evaluated 18,988 Human Phenotype Ontology (HPO) and 4,023 Gene Ontology (GO-CC) terms across nine features capturing term familiarity, identifier exposure, and ontology structure. The analysis revealed that ontology terms with no usage ("ontology deserts") show near-zero linking accuracy, while usage frequency in PubMed Central and curated annotations are the strongest predictors of success. A novel "leading zero" formatting effect in HPO identifiers was also discovered, improving model performance.

## Method Summary
The researchers evaluated LLM performance in linking biomedical ontology terms to their identifiers using two models: GPT-4o and LLaMa 3.1 405B. They analyzed 18,988 HPO terms and 4,023 GO-CC terms across nine features including term frequency in PubMed Central, curated annotations, term length, morphological complexity, and ontology depth. Both univariate and multivariate analyses were conducted to identify predictors of linking success. The study specifically examined "ontology deserts"—terms with zero usage—and their impact on model accuracy.

## Key Results
- Usage frequency in PubMed Central and curated annotations are the strongest predictors of linking success, not term morphology or ontology structure
- Over 40% of HPO and 54% of GO-CC terms are unused ("ontology deserts") with near-zero linking accuracy
- A "leading zero" formatting effect in HPO identifiers significantly improves model performance

## Why This Works (Mechanism)
The study reveals that LLM failures in linking biomedical ontology terms to identifiers are systematic rather than random. Models struggle primarily because they lack exposure to identifiers during training, not because of inherent model limitations. The "ontology deserts" phenomenon demonstrates that terms never appearing in training data cannot be correctly linked, regardless of model capability. The leading zero effect suggests that specific formatting conventions can create predictable patterns that models learn to recognize, providing opportunities for optimization.

## Foundational Learning
1. **Ontology desert concept** - why needed: Understanding why models fail on terms with zero usage; quick check: Identify terms with zero frequency in training data
2. **PubMed Central frequency analysis** - why needed: Proxy for training data exposure; quick check: Calculate term frequency in biomedical literature corpus
3. **Identifier formatting effects** - why needed: Reveals how presentation impacts model performance; quick check: Test different identifier formatting variations
4. **Univariate vs multivariate analysis** - why needed: Distinguish individual vs combined feature effects; quick check: Compare single-feature correlations with combined model predictions
5. **Ontology structure metrics** - why needed: Evaluate if hierarchical position affects linking; quick check: Calculate depth and sibling relationships for each term
6. **Morphological complexity measures** - why needed: Assess if term structure impacts model understanding; quick check: Analyze term length and linguistic features

## Architecture Onboarding

**Component Map:**
HPO/GO-CC Terms -> Feature Extraction (9 features) -> LLM Prompt -> Linking Output -> Accuracy Analysis

**Critical Path:**
Term selection → Feature computation → Model prompting → Response parsing → Accuracy calculation

**Design Tradeoffs:**
- Focused on two ontologies vs broader coverage
- Two models vs model diversity
- Frequency proxies vs direct training data analysis
- 9 features vs potentially more comprehensive feature sets

**Failure Signatures:**
- Zero accuracy for ontology desert terms
- Performance improvement with leading zero formatting
- Feature importance shifts between univariate and multivariate analysis

**First Experiments:**
1. Test leading zero effect by removing zeros from HPO identifiers in test prompts
2. Compare performance on used vs unused terms within the same ontology
3. Evaluate different feature combinations to optimize prediction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to two LLM models (GPT-4o and LLaMa 3.1 405B), limiting generalizability
- Focused exclusively on two biomedical ontologies (HPO and GO-CC)
- Relies on frequency proxies rather than direct training data analysis
- Nine features may not capture all relevant factors influencing linking performance

## Confidence

**High confidence:** Ontology desert terms show near-zero linking accuracy; leading zero formatting effect is consistently replicated

**Medium confidence:** Usage frequency drives linking success more than morphology or structure; consistent reporting would improve performance

## Next Checks
1. Test the same analysis framework across additional LLM architectures (Claude, Gemini, Mistral) to assess model-specific versus general patterns
2. Expand ontology coverage to include chemical and disease ontologies to determine generalizability beyond HPO and GO-CC
3. Conduct ablation studies removing leading zeros from HPO identifiers to quantify practical impact on clinical applications