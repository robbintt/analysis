---
ver: rpa2
title: Analyzing public sentiment to gauge key stock events and determine volatility
  in conjunction with time and options premiums
arxiv_id: '2502.05403'
source_url: https://arxiv.org/abs/2502.05403
tags:
- data
- stock
- sentiment
- market
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a sentiment-driven algorithm to forecast
  stock earnings and volatility by integrating social media sentiment analysis with
  financial data. It employs transformer-based models (RoBERTa for Reddit, FinBERT
  for financial news) to extract sentiment, merges this with stock metrics from AlphaQuery,
  and applies ensemble ML models (LightGBM achieving 70.1% accuracy) to predict price
  movements.
---

# Analyzing public sentiment to gauge key stock events and determine volatility in conjunction with time and options premiums

## Quick Facts
- arXiv ID: 2502.05403
- Source URL: https://arxiv.org/abs/2502.05403
- Reference count: 30
- Primary result: 70.1% accuracy using LightGBM to predict stock price movement around earnings events

## Executive Summary
This paper introduces a sentiment-driven algorithm to forecast stock earnings and volatility by integrating social media sentiment analysis with financial data. It employs transformer-based models (RoBERTa for Reddit, FinBERT for financial news) to extract sentiment, merges this with stock metrics from AlphaQuery, and applies ensemble ML models (LightGBM achieving 70.1% accuracy) to predict price movements. The approach addresses limitations of traditional long-term forecasts by focusing on short-term, event-driven volatility. Key challenges included data scraping obstacles, temporal misalignment, and bias in single-source sentiment data. Future work aims to expand data sources (e.g., Twitter), refine credibility scoring, and incorporate options data for improved accuracy.

## Method Summary
The method combines transformer-based sentiment analysis with gradient boosting classification. RoBERTa extracts sentiment from Reddit discussions while FinBERT analyzes financial news. These sentiment scores are merged with stock metrics (prices, volume, earnings proximity) and scaled before being fed into a LightGBM classifier. SMOTE addresses class imbalance, and PCA reduces dimensionality. The approach focuses on short-term volatility prediction around earnings events rather than long-term trends.

## Key Results
- LightGBM achieved 70.1% accuracy, outperforming Naive Bayes (54%), Random Forest (64%), and Gradient Boosting (62.6%)
- Sentiment features were less dominant than price/volume features but provided valuable supplementary information during high-volatility periods
- Model showed better precision and fewer false negatives in predicting the "Increase" class compared to other models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific transformer models improve sentiment signal extraction from heterogeneous text sources.
- Mechanism: RoBERTa captures informal, context-heavy Reddit discussions while FinBERT leverages financial domain pre-training for Yahoo Finance news. This specialization allows each model to handle vocabulary, sarcasm, and domain jargon that general-purpose sentiment classifiers miss.
- Core assumption: Sentiment from social media and financial news contains predictive signal about short-term price movements around earnings events.
- Evidence anchors: [abstract] "employs transformer-based models (RoBERTa for Reddit, FinBERT for financial news) to extract sentiment"; [section 4.1] "RoBERTa was employed for informal and context-heavy discussions on Reddit, while FinBERT, specialized in financial text, analyzed Yahoo Finance news summaries"
- Break condition: If sentiment distributions between Reddit and news sources become too weakly correlated with actual price movements (e.g., during low-volatility periods), model degrades to noise.

### Mechanism 2
- Claim: Feature-level fusion of sentiment scores with stock metrics captures event-driven volatility better than either modality alone.
- Mechanism: Sentiment polarity scores, weighted by engagement (upvotes), are merged with open/close prices, volume, and time-based features (earnings proximity, day of week). SMOTE addresses class imbalance. PCA reduces dimensionality while preserving signal.
- Core assumption: Market reaction to earnings depends on sentiment divergence between retail (Reddit) and institutional (news) sources.
- Evidence anchors: [abstract] "merges this with stock metrics from AlphaQuery"; [section 3.2] "Sentiment Features: Sentiment polarity scores, weighted sentiment (using upvotes), and aggregated sentiment trends were calculated"
- Break condition: Temporal misalignment between sentiment timestamps and market hours causes label contamination; paper explicitly notes this challenge.

### Mechanism 3
- Claim: Ensemble tree-based models outperform neural sequence models for short-horizon classification with structured+unstructured features.
- Mechanism: LightGBM (70.1% accuracy) handles heterogeneous features efficiently and provides feature importance. LSTM was planned but not fully implemented, suggesting gradient boosting was sufficient for the binary increase/decrease task.
- Core assumption: Short-term volatility around earnings events is more predictable than long-term trends.
- Evidence anchors: [abstract] "LightGBM achieving 70.1% accuracy"; [section 4.3] "LightGBM outperformed other models with an accuracy of 70.1%... significantly fewer false negatives in predicting the 'Increase' class"
- Break condition: If feature importance shifts toward sentiment during high-volatility events but the model was trained on low-volatility periods, generalization fails. Paper notes sentiment features were "less dominant" than price/volume but "provided valuable supplementary information during high-volatility periods."

## Foundational Learning

- Concept: **Transformer-based sentiment analysis (BERT variants)**
  - Why needed here: RoBERTa and FinBERT are core feature extractors. Understanding attention mechanisms, pre-training vs. fine-tuning, and domain adaptation (FinBERT's financial corpus pre-training) is essential for debugging sentiment outputs.
  - Quick check question: Can you explain why FinBERT outperforms standard BERT on financial news text?

- Concept: **Gradient boosting with imbalanced classification (SMOTE + LightGBM)**
  - Why needed here: The target variable (Increase vs. Decrease) was imbalanced. SMOTE generates synthetic minority samples; LightGBM's leaf-wise growth handles large datasets efficiently.
  - Quick check question: Why might SMOTE cause overfitting if applied before train-test split rather than only on training data?

- Concept: **Temporal data alignment and leakage prevention**
  - Why needed here: Paper explicitly notes challenges merging datasets with different granularities (daily news vs. hourly Reddit comments). Maintaining temporal integrity in train-test splits prevents look-ahead bias.
  - Quick check question: If you align sentiment data by company name without timestamp matching, what type of data leakage risk does this introduce?

## Architecture Onboarding

- Component map: Selenium + BeautifulSoup scrapers → raw CSVs (Reddit comments, Yahoo Finance headlines, stock prices from AlphaQuery) → Text cleaning, tokenization, StandardScaler for numerical features → RoBERTa (Reddit) + FinBERT (Yahoo Finance) → sentiment scores + labels → Sentiment aggregation, time features, PCA (optional), SMOTE → LightGBM classifier → Accuracy, precision, recall, F1, confusion matrix

- Critical path: 1. Scrape and store raw text + stock data 2. Generate sentiment scores using correct model per source 3. Merge on company + date (temporal alignment is fragile point) 4. Engineer features including earnings proximity 5. Train LightGBM with SMOTE-balanced training set 6. Evaluate on held-out test set (30%)

- Design tradeoffs:
  - Transformer vs. Naive Bayes baseline: Paper shows 64% vs. 54% accuracy—10-point gain but higher compute cost
  - Company-level alignment vs. timestamp-precise alignment: Company-level merging sacrifices temporal precision but handles missing timestamps; introduces noise
  - LightGBM vs. LSTM: Trees faster to train and interpret; LSTM would capture sequential patterns but wasn't fully tested

- Failure signatures:
  - Accuracy drops near 50% (random baseline): Check sentiment extraction pipeline for errors or data leakage
  - High false negatives on "Increase" class: Feature importance may underweight sentiment during volatile periods
  - Scraping failures: Anti-bot measures (CAPTCHAs, rate limits) block data collection—paper documents this explicitly
  - Negative scaled values for stock prices: This is expected behavior with StandardScaler on highly correlated magnitudes; not a bug

- First 3 experiments:
  1. Reproduce baseline comparison: Run Naive Bayes, Random Forest, Gradient Boosting, and LightGBM on the same train-test split. Verify ~54%, ~64%, ~62.6%, and ~70.1% accuracy respectively. This validates the data pipeline.
  2. Ablate sentiment features: Train LightGBM on stock features only (no sentiment), then add sentiment features. Measure delta in accuracy and F1. This quantifies sentiment contribution.
  3. Temporal robustness test: Train on Q1-Q3 earnings events, test on Q4. Check if accuracy degrades during different market regimes (e.g., high volatility vs. low volatility periods). Paper claims short-term event focus but doesn't test regime shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does incorporating options premiums, volume, and decay rates improve the model's ability to identify mispriced earnings events compared to using sentiment and historical price data alone?
- Basis in paper: [explicit] The authors state in Section 7 that "options premiums data could also be a key component" for determining market direction and identifying inaccurately priced options.
- Why unresolved: The current iteration of the model relies on sentiment and stock metrics but excludes options data, which the authors identify as a limitation.
- What evidence would resolve it: A comparative ablation study showing prediction accuracy and false-positive rates before and after the integration of options chain data.

### Open Question 2
- Question: Can a dynamic credibility score based on user history and engagement successfully mitigate the noise and bias inherent in Reddit sentiment analysis?
- Basis in paper: [explicit] Section 7 proposes "quantifying users' posts on Reddit and assigning them a credibility score" to weigh contributions and filter out uncredible sources.
- Why unresolved: The current model treats all scraped comments with equal weight, suffering from "source bias" and the inability to verify user expertise.
- What evidence would resolve it: Backtesting results comparing the Sharpe ratio of trades generated by the credibility-weighted model against the current unweighted baseline.

### Open Question 3
- Question: Do Long Short-Term Memory (LSTM) networks outperform the LightGBM ensemble in capturing the temporal dependencies of sentiment during the "earnings run-up" period?
- Basis in paper: [inferred] Section 4.1 notes that LSTM networks were "planned" to capture temporal dependencies but were "not fully implemented," representing a methodological gap.
- Why unresolved: The final results utilize ensemble tree methods, leaving the potential benefits of modeling temporal sequences in sentiment data unexplored.
- What evidence would resolve it: Performance metrics (e.g., F1-score, Mean Squared Error) comparing the proposed LSTM architecture against the current LightGBM benchmark on the same dataset.

## Limitations

- The approach relies on three fragile assumptions: sentiment contains predictive signal, temporal alignment doesn't introduce label contamination, and transformer models extract meaningful signal
- Without independent replication on the exact ticker list and date range, the 70.1% accuracy claim remains unverified
- The planned LSTM model was not implemented, leaving the gradient boosting vs. neural sequence model comparison incomplete

## Confidence

- **High Confidence**: The general framework of combining transformer sentiment analysis with gradient boosting is technically sound and well-documented in the literature
- **Medium Confidence**: The 70.1% accuracy result is plausible given related work, but requires independent validation on the specific dataset and split methodology used
- **Low Confidence**: Claims about sentiment divergence between retail and institutional sources driving volatility are not empirically tested in this paper

## Next Checks

1. Reproduce baseline comparison: Run Naive Bayes, Random Forest, Gradient Boosting, and LightGBM on the same train-test split. Verify ~54%, ~64%, ~62.6%, and ~70.1% accuracy respectively. This validates the data pipeline.

2. Ablate sentiment features: Train LightGBM on stock features only (no sentiment), then add sentiment features. Measure delta in accuracy and F1. This quantifies sentiment contribution.

3. Temporal robustness test: Train on Q1-Q3 earnings events, test on Q4. Check if accuracy degrades during different market regimes (e.g., high volatility vs. low volatility periods). Paper claims short-term event focus but doesn't test regime shift.