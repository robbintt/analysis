---
ver: rpa2
title: Token Caching for Diffusion Transformer Acceleration
arxiv_id: '2409.18523'
source_url: https://arxiv.org/abs/2409.18523
tags:
- cache
- caching
- diffusion
- token
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck of diffusion
  transformers (DiT), which suffer from high inference costs due to the quadratic
  complexity of attention mechanisms and multi-step denoising processes. To tackle
  this, the authors propose TokenCache, a novel acceleration method that caches and
  reuses intermediate token features to reduce redundant computations.
---

# Token Caching for Diffusion Transformer Acceleration

## Quick Facts
- arXiv ID: 2409.18523
- Source URL: https://arxiv.org/abs/2409.18523
- Authors: Jinming Lou; Wenyang Luo; Yufan Liu; Bing Li; Xinmiao Ding; Weiming Hu; Yuming Li; Chenguang Ma
- Reference count: 40
- Primary result: TokenCache achieves up to 2.4× speedup on DiT models while maintaining or improving generation quality

## Executive Summary
This paper addresses the computational bottleneck of diffusion transformers (DiT), which suffer from high inference costs due to the quadratic complexity of attention mechanisms and multi-step denoising processes. To tackle this, the authors propose TokenCache, a novel acceleration method that caches and reuses intermediate token features to reduce redundant computations. The key innovation is a Cache Predictor that dynamically determines which tokens to cache, which blocks to target, and at which time steps to apply caching, enabling fine-grained, adaptive acceleration. Experiments across multiple models (DiT, PixArt-α, Open-Sora-Plan) demonstrate that TokenCache achieves significant speedups while maintaining or even improving generation quality, outperforming existing cache-based acceleration methods.

## Method Summary
TokenCache introduces a hierarchical approach to accelerate DiT inference: token pruning based on importance scores, block-level cache ratio allocation, and temporal scheduling to balance speed and quality. The method trains a lightweight Cache Predictor (a shared DiT block initialized from the first block) that outputs per-token importance scores. These scores determine which tokens to cache at which blocks and timesteps. The predictor is trained using a dual-objective loss that minimizes local error while maximizing the number of cached tokens. After predictor training, the final K blocks are fine-tuned with LoRA adapters. During inference, TokenCache applies selective token caching based on the predictor's decisions, with periodic I-steps that force full computation to reset accumulated errors.

## Key Results
- Achieves up to 2.4× speedup on DiT models while maintaining or improving FID scores
- Outperforms existing cache-based acceleration methods like BWCache and FastCache
- Effective across different tasks: ImageNet class-conditional generation, MSCOCO text-to-image, and Mixkit text-to-video
- Robust performance across different sampling steps and resolutions

## Why This Works (Mechanism)

### Mechanism 1: Token Update Redundancy Across Timesteps
Token updates in DiT blocks exhibit high similarity across consecutive denoising steps, creating computationally redundant operations that can be cached and reused. The diffusion denoising process operates on latent representations that evolve gradually, with correlated token updates across adjacent timesteps, particularly for tokens representing stable image regions.

### Mechanism 2: Importance-Weighted Cache Prediction
A learned Cache Predictor can assign per-token importance scores that predict which token updates can be safely reused without degrading output quality. The Cache Predictor outputs scalar scores trained via a dual-objective loss that balances local error minimization with maximizing cached tokens.

### Mechanism 3: Hierarchical Error Accumulation Control
Multi-level scheduling prevents cached-token errors from accumulating beyond recoverable thresholds. The approach implements a three-tier hierarchy: I-steps force full computation to reset error accumulation, block-level ratios allocate more caching to less important blocks, and token-level pruning selects specific tokens within blocks.

## Foundational Learning

- Concept: **Diffusion Transformer (DiT) Architecture**
  - Why needed here: TokenCache operates specifically on DiT's homogeneous transformer blocks, not U-Net structures
  - Quick check question: Can you explain why DiT uses a stack of identical transformer blocks rather than encoder-decoder U-Net, and how this affects where caching can be applied?

- Concept: **DDIM and DPM-Solver Sampling**
  - Why needed here: The paper's timestep scheduling depends on understanding how diffusion samplers discretize the denoising process
  - Quick check question: What is the difference between stochastic and deterministic diffusion samplers, and why might deterministic samplers be more amenable to caching strategies?

- Concept: **Gradient-Based Cache Optimization with LoRA**
  - Why needed here: The Cache Predictor is trained end-to-end while keeping the main DiT frozen, with LoRA adapters on final blocks for refinement
  - Quick check question: How does LoRA enable fine-tuning the final K blocks without modifying the frozen DiT backbone, and why is this preferable to full fine-tuning?

## Architecture Onboarding

- Component map:
  Cache Predictor -> Token Filtering Module -> Block Ratio Allocator -> Timestep Scheduler -> LoRA Refinement Blocks

- Critical path:
  1. Initialize Cache Predictor from first DiT block weights
  2. Train Cache Predictor using paired samples with cache loss + cost loss
  3. Fine-tune LoRA adapters on final K blocks
  4. Pre-compute I-step schedule via batch inference on representative samples
  5. During inference: run Cache Predictor → quantize decisions → apply selective token caching per block

- Design tradeoffs:
  - Cache ratio (r=0.9 default): Higher ratio = more speedup but risk of quality degradation
  - Number of LoRA blocks (K=4): More blocks = better quality recovery but lower speedup
  - I-step frequency: More frequent I-steps = better error correction but lower overall acceleration

- Failure signatures:
  - Semantic drift: Output diverges from prompt → I-step intervals too sparse
  - Texture artifacts: Blurry regions → cache ratio too high for specific blocks
  - Training instability: Cache Predictor outputs uniform scores → increase λ or check learning rate

- First 3 experiments:
  1. Visualize token update heatmaps across timesteps to confirm redundancy exists
  2. Train Cache Predictor with and without LoRA refinement to quantify LoRA contribution
  3. Test cache ratio sweep (r ∈ {0.6, 0.7, 0.8, 0.9}) to identify performance knee point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Cache Predictor be adapted into a training-free framework to eliminate the setup cost of LoRA fine-tuning?
- Basis in paper: The method requires training the Cache Predictor and fine-tuning the last K blocks, whereas methods like FORA or DeepCache are often training-free
- What evidence would resolve it: A variant that utilizes fixed rules or zero-shot metrics to predict cache decisions with comparable performance

### Open Question 2
- Question: How does the token-level caching mechanism interact with orthogonal acceleration techniques like quantization or structural pruning?
- Basis in paper: The paper compares against other caching methods but does not evaluate composability with compression techniques
- What evidence would resolve it: Experiments applying TokenCache to a quantized DiT model to measure cumulative speedup and quality degradation

### Open Question 3
- Question: Is there a theoretical upper bound on the global cache ratio r that is universally applicable, or must it always be manually tuned per model?
- Basis in paper: The ablation study shows quality drops sharply at certain ratios, and different models require different hyperparameters
- What evidence would resolve it: An automated search or theoretical derivation for r and β that maintains consistent performance across different DiT variants

## Limitations
- Requires training a Cache Predictor and fine-tuning final blocks, adding setup complexity
- Global cache ratio r and I-step frequency require manual tuning per model
- Performance may degrade if training and inference distributions differ significantly
- Memory overhead from storing cached tokens, though generally outweighed by speedup benefits

## Confidence

| Claim | Confidence |
|-------|------------|
| TokenCache achieves significant speedup (2.4×) | High |
| Quality is maintained or improved across tasks | High |
| Cache Predictor effectively identifies redundant computations | Medium |
| Hierarchical scheduling prevents error accumulation | Medium |
| Method generalizes across different DiT variants | Low |

## Next Checks
1. Verify token update redundancy exists in your target DiT variant by visualizing token heatmaps across timesteps
2. Implement a minimal Cache Predictor and test on a small validation set to confirm training stability
3. Measure actual speedup vs theoretical speedup by profiling attention computation reduction