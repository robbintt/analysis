---
ver: rpa2
title: Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery
  Stages
arxiv_id: '2505.04150'
source_url: https://arxiv.org/abs/2505.04150
tags:
- learning
- class
- similarity
- proportion
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OSLSP addresses two limitations in Learning from Label Proportions
  (LLP) for weakly supervised whole-slide image analysis: (1) inability to update
  feature extractors for specialized domains like muscle tissue, and (2) treating
  ordinal class relationships as nominal. The method introduces a similarity proportion
  loss computed between two bags of instances, using cosine similarity of feature
  vectors and a ground truth similarity distribution derived from binomial theorem.'
---

# Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages

## Quick Facts
- arXiv ID: 2505.04150
- Source URL: https://arxiv.org/abs/2505.04150
- Reference count: 24
- Primary result: 46.005% accuracy on ordinal muscle recovery classification vs 44.442% for fine-tuned DINO

## Executive Summary
This paper addresses two key limitations in Learning from Label Proportions (LLP) for weakly supervised whole-slide image analysis of skeletal muscle tissue: (1) inability to update feature extractors for specialized domains, and (2) treating ordinal class relationships as nominal. The authors introduce a similarity proportion loss computed between two bags of instances using cosine similarity of feature vectors and a ground truth similarity distribution derived from binomial theorem. This enables feature extractor updates while incorporating ordinal information about muscle recovery stages.

## Method Summary
The approach builds on LLP framework where WSIs are labeled with proportion of each recovery stage rather than individual cell labels. The key innovation is the similarity proportion loss that captures ordinal relationships between recovery stages by comparing instance-level feature similarity distributions across bag pairs. The method uses ViT-B/8 backbone (DINO pretrained) with last block unfrozen, combined with a 3-layer MLP classifier. Two KL divergence losses are combined: L_SimProp for ordinal similarity and L_prop for class proportions. Gaussian expansion with σ=0.1 enables differentiable histogram computation of similarity distributions.

## Key Results
- OSLSP achieves 46.005% accuracy on skeletal muscle regeneration classification
- Outperforms fine-tuned DINO (44.442% accuracy) and pre-trained DINO (20.967% accuracy)
- Achieves RMSE of 2.152 on ordinal scale vs 2.431 (fine-tuned DINO) and 1.760 (pre-trained DINO)
- Shows improved performance in capturing ordered progression of muscle recovery stages

## Why This Works (Mechanism)
The method leverages ordinal class relationships by computing similarity distributions between instance pairs from different bags, where the ground truth similarity is derived from the ordinal structure (sim(k,k') = 1 - |k'-k|/(K-1)). By comparing predicted vs ground truth similarity distributions via KL divergence, the model learns to preserve ordinal relationships while updating feature extractors. The differentiable histogram with Gaussian expansion allows gradient flow through the similarity proportion computation.

## Foundational Learning
- Learning from Label Proportions (LLP): Weakly supervised learning where only bag-level class proportions are known - needed for scenarios where individual instance labels are unavailable; quick check: verify bag-level proportion computation
- Ordinal classification: Classes have inherent order but not necessarily equal intervals - needed for muscle recovery stages progression; quick check: validate ordinal similarity matrix
- Differentiable histogram computation: Using Gaussian expansion instead of indicator functions to enable gradient flow - needed for similarity proportion loss; quick check: verify gradient propagation through histogram
- Cosine similarity for feature comparison: Measures angular distance between feature vectors - needed for instance-level similarity computation; quick check: confirm cosine similarity range [-1,1]
- KL divergence for distribution comparison: Measures information loss between predicted and ground truth distributions - needed for both loss terms; quick check: ensure distributions sum to 1

## Architecture Onboarding

Component Map: WSI patches -> Cellpose segmentation -> 64×64 cell crops -> ViT-B/8 feature extraction -> 3-layer MLP -> Combined loss (L_SimProp + L_prop)

Critical Path: The similarity proportion loss computation is the critical path - it requires instance-level feature extraction from two different bags, cosine similarity computation between all instance pairs, Gaussian expansion to create differentiable histograms, and KL divergence calculation against ground truth similarity distribution.

Design Tradeoffs: Unfreezing only the last ViT block balances feature extractor adaptation with preventing overfitting on limited data. The combined loss approach trades computational complexity for better ordinal structure preservation.

Failure Signatures: Poor ordinal structure capture manifests as high confusion between adjacent classes (Intact MF vs Ghost fiber). Non-differentiable histogram computation leads to zero gradients in similarity proportion loss.

First Experiments:
1. Verify gradient flow through L_SimProp by checking that cosine similarity and Gaussian expansion operations are differentiable
2. Validate bag-level proportion computation by checking that p_d values match expected biological progression
3. Test instance-level feature extraction by confirming cosine similarity captures meaningful semantic relationships between similar recovery stages

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (31 WSIs) without cross-validation
- No ablation studies isolating impact of ordinal constraints versus LLP framework
- Gaussian expansion hyperparameters (σ, bin count) not experimentally validated

## Confidence

High confidence: The ordinal class structure and biological motivation for muscle recovery stages

Medium confidence: The mathematical formulation of similarity proportion loss and its differentiability

Low confidence: The optimal hyperparameters (learning rate, bag size, σ, bin count) and their sensitivity analysis

## Next Checks

1. Perform ablation studies: train with only L_prop (standard LLP) versus only L_SimProp versus combined loss to quantify ordinal information contribution

2. Conduct sensitivity analysis on Gaussian kernel width σ and bin count b in the differentiable histogram to verify robustness of results

3. Test transferability to other ordinal classification tasks (e.g., disease staging, cell cycle phases) to evaluate generalizability beyond muscle tissue