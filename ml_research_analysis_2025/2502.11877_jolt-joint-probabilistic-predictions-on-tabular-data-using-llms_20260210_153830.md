---
ver: rpa2
title: 'JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs'
arxiv_id: '2502.11877'
source_url: https://arxiv.org/abs/2502.11877
tags:
- jolt
- data
- missing
- tabular
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JoLT introduces a simple method for probabilistic predictions on
  tabular data using LLMs. The approach uses in-context learning to define joint distributions
  over tabular data conditioned on user-specified side information, supporting heterogeneous
  data types and automatically handling missing data without requiring preprocessing
  or training.
---

# JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs

## Quick Facts
- arXiv ID: 2502.11877
- Source URL: https://arxiv.org/abs/2502.11877
- Reference count: 40
- Primary result: JoLT achieves up to 0.83 AUC on classification tasks and lower NLL than baselines, with 1.14 MAE for imputation vs 3.52 MAE for mean imputation

## Executive Summary
JoLT introduces a simple method for probabilistic predictions on tabular data using LLMs via in-context learning. The approach defines joint distributions over tabular data conditioned on user-specified side information, supporting heterogeneous data types and automatically handling missing data without requiring preprocessing or training. JoLT outperforms competitive methods on low-shot single and multi-target tabular classification and regression tasks, achieving strong performance on both classification (up to 0.83 AUC) and regression tasks while effectively imputing missing data using textual side information.

## Method Summary
JoLT uses in-context learning with LLMs to perform joint probabilistic predictions on tabular data. The method serializes training rows and test features into a prompt format using special tokens to delimit examples, features, and values. It supports two prediction modes: rejection sampling with format validation for regression tasks, and full distribution prediction via LLM logits for classification. The joint negative log-likelihood (NLL) is computed autoregressively using the product rule. Missing data is handled by simply omitting cells from the prompt. The approach requires no preprocessing, training, or fine-tuning, and works with heterogeneous data types (numerical, categorical, text).

## Key Results
- Achieves up to 0.83 AUC on classification tasks across multiple datasets
- Lower NLL than baseline methods on multi-target regression tasks
- Effective missing data imputation with 1.14 MAE vs 3.52 MAE for mean imputation
- Outperforms competitive methods on low-shot single and multi-target tabular classification and regression

## Why This Works (Mechanism)
JoLT leverages the pattern recognition capabilities of large language models to learn joint distributions over tabular data through in-context learning. By providing the LLM with serialized examples of input-output pairs along with descriptive side information (column headers), the model can infer the underlying relationships between features and targets. The autoregressive computation of joint probabilities ensures proper probabilistic calibration, while the rejection sampling approach handles the generation of valid numerical outputs. The method's ability to process heterogeneous data types and handle missing values without explicit preprocessing makes it particularly flexible for real-world tabular datasets.

## Foundational Learning
- **In-context learning**: LLMs' ability to learn tasks from few examples provided in prompts without parameter updates. Why needed: Enables zero-training approach for tabular prediction. Quick check: Verify LLM generates reasonable outputs when given example prompts.
- **Joint probability distributions**: Modeling the probability of multiple variables occurring together. Why needed: Enables multi-target predictions and uncertainty quantification. Quick check: Confirm autoregressive decomposition correctly computes joint probabilities.
- **Rejection sampling**: Generating samples and discarding those that don't meet validity criteria. Why needed: Ensures numerical outputs are valid and within expected ranges. Quick check: Measure acceptance rate of generated samples.
- **Serialization format**: Converting structured data into text format using delimiters. Why needed: Enables tabular data to be processed by text-based LLMs. Quick check: Verify parsing of serialized examples from LLM outputs.
- **Autoregressive decomposition**: Breaking down joint probability into product of conditional probabilities. Why needed: Enables tractable computation of multi-target joint probabilities. Quick check: Validate NLL computation across different target orderings.
- **Context windows**: Maximum input length LLMs can process. Why needed: Limits number of in-context examples that can be provided. Quick check: Measure performance degradation with reduced shots.

## Architecture Onboarding

Component Map: Training examples -> Prompt serialization -> LLM inference -> Output parsing -> Joint NLL computation

Critical Path: Data preparation → Prompt construction → LLM inference → Result validation → Metric computation

Design Tradeoffs:
- **LLM size vs. context length**: Larger models offer better performance but smaller context windows limit in-context examples
- **Rejection sampling vs. direct generation**: Rejection sampling ensures validity but may be computationally expensive
- **Joint vs. independent predictions**: Joint modeling captures dependencies but requires autoregressive computation
- **Serialization verbosity vs. context efficiency**: More descriptive prompts improve performance but consume context

Failure Signatures:
- Invalid numerical formats in regression outputs → Increase rejection sampling iterations
- Unknown categories in classification → Verify category coverage in training examples
- Context length exceeded → Reduce number of in-context examples
- Poor performance without side information → Ensure descriptive column headers are included

First Experiments:
1. Test basic functionality: Run JoLT on a small synthetic dataset with known ground truth to verify prompt serialization and output parsing
2. Validate rejection sampling: Generate regression samples and measure acceptance rate and output validity
3. Compare with baselines: Evaluate JoLT against mean imputation on a simple missing data task

## Open Questions the Paper Calls Out
- **LLM bias propagation**: Do inherent biases in pre-trained LLMs propagate to JoLT predictions, and can standard de-biasing techniques mitigate these effects? The authors identify this as an open area of research due to the black-box nature of the method.
- **Many-shot scaling**: How does JoLT performance scale when increasing the number of in-context examples from low-shot regimes to hundreds or thousands of examples? The paper restricts experiments to low-shot settings due to context size limits.
- **Autoregressive ordering**: To what extent does the autoregressive ordering of multiple target variables violate the Kolmogorov exchangeability condition and affect prediction accuracy? The authors identify this order dependency as a limitation but do not analyze its impact.

## Limitations
- Underspecified sampling hyperparameters (temperature, number of samples, rejection criteria) beyond top-p=0.9
- Numerical NLL computation precision lacks specific bin width settings per dataset
- Prefix text used for different datasets not fully specified, though sample prompts are provided
- Performance may degrade with very large context windows or complex dependencies
- Autoregressive structure may violate Kolmogorov exchangeability condition for multi-target tasks

## Confidence
- **High confidence**: Core methodological approach (in-context learning, autoregressive NLL computation) is clearly specified and reproducible
- **Medium confidence**: Classification performance metrics (AUC) can likely be reproduced with reasonable fidelity given clear algorithm specification
- **Medium confidence**: Regression and imputation results are reproducible but may show variation due to underspecified sampling hyperparameters and numerical precision settings

## Next Checks
1. Validate numerical output formatting by testing with a small dataset to ensure rejection sampling correctly handles invalid numerical formats and unknown categories
2. Benchmark performance degradation when reducing context length to identify minimum effective training example count for maintaining performance
3. Compare JoLT performance with and without text side information on a held-out dataset to quantify impact of column headers and descriptive prefixes