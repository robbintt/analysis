---
ver: rpa2
title: 'SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic
  Search'
arxiv_id: '2507.15245'
source_url: https://arxiv.org/abs/2507.15245
tags:
- query
- retrieval
- search
- academic
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SPAR, a multi-agent framework for academic
  paper retrieval that enhances search effectiveness through RefChain-based query
  decomposition and query evolution. The system features specialized agents for query
  understanding, multi-source retrieval, iterative query refinement, relevance judgment,
  and result reranking.
---

# SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search

## Quick Facts
- arXiv ID: 2507.15245
- Source URL: https://arxiv.org/abs/2507.15245
- Reference count: 40
- SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline.

## Executive Summary
This paper introduces SPAR, a multi-agent framework for academic paper retrieval that enhances search effectiveness through RefChain-based query decomposition and query evolution. The system features specialized agents for query understanding, multi-source retrieval, iterative query refinement, relevance judgment, and result reranking. To enable systematic evaluation, the authors construct SPARBench, a high-quality benchmark with expert-annotated relevance labels spanning computer science and biomedicine. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. The results highlight SPAR's superior ability to balance precision and recall while providing a scalable, interpretable, and high-performing foundation for scholarly retrieval research.

## Method Summary
SPAR is a multi-agent framework that decomposes the academic search task into specialized components: Query Understanding, Retrieval, Judgement, Query Evolver, and Reranker. The system performs intent classification, domain identification, and temporal constraint parsing to refine queries, then conducts multi-source retrieval with RefChain expansion (one layer deep) for recall enhancement. Retrieved documents are scored by a relevance judge, and the system iteratively evolves queries based on document content focusing on methods, applications, and limitations. Final results are reranked based on authority, timeliness, and relevance. The framework is evaluated on AutoScholar and SPARBench, a new benchmark with expert-annotated relevance labels for computer science and biomedical queries.

## Key Results
- SPAR achieves up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline
- RefChain increases recall from 0.58 to 0.77 on AutoScholar but reduces precision from 0.29 to 0.19
- Query Evolution improves F1 from 0.33 to 0.34 on AutoScholar and 0.24 to 0.26 on SPARBench with precision gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage query interpretation improves retrieval precision by explicitly disambiguating user intent and domain context.
- Mechanism: The Query Understanding Agent performs intent classification (survey vs. methods vs. recent advances), domain identification, and temporal constraint parsing. Based on this analysis, it applies intent-aware query refinement—generating multiple semantically enriched queries. This structured decomposition addresses the "anomalous state of knowledge" problem where user queries are incomplete articulations of complex information needs.
- Core assumption: User queries in academic search are often underspecified; LLMs can reliably infer latent intent and domain context from brief queries.
- Evidence anchors:
  - [abstract] "SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution"
  - [section] Page 3, Section 3.1: "Given an academic search query q, the agent first performs intent classification... It simultaneously conducts domain identification... and detects any temporal constraints... These annotations help the system tailor downstream retrieval operations."
- Break condition: If intent classification accuracy drops, refinement produces off-target expanded queries, reducing precision gains.

### Mechanism 2
- Claim: RefChain (reference chain) expansion improves recall by traversing citation networks to discover topically relevant papers beyond direct keyword matches.
- Mechanism: After initial retrieval and relevance filtering, the system extracts reference lists from high-relevance papers. These cited papers are scored by the same Judgement Agent, and high-scoring papers are merged into the results. Expansion is limited to one layer deep to balance recall gains against precision loss and computational cost.
- Core assumption: Citation relationships indicate topical relevance; papers cited by relevant papers are themselves likely relevant (transitivity of relevance).
- Evidence anchors:
  - [abstract] "RefChain-based query decomposition and query evolution to enable more flexible and effective search"
  - [section] Page 4, Section 3.2: "SPAR enhances knowledge expansion through RefChain. For each paper ri ∈ R, the Retrieval Agent extracts its list of references... Then these referred papers are scored using the same Judgement Agent. High-relevance papers are merged with the Related Pool."
  - [section] Page 8, Table 5: RefChain increases recall from 0.58 to 0.77 (raw) on AutoScholar and 0.26 to 0.31 on SPARBench, but precision drops (0.29→0.19 on AutoScholar).
- Break condition: If citation networks are sparse or noisy, precision drops sharply; in domains with poor metadata, RefChain yields minimal gain.

### Mechanism 3
- Claim: Query evolution based on retrieved document content diversifies search directions and improves F1 by generating semantically focused follow-up queries.
- Mechanism: The Query Evolver Agent takes top-retrieved papers and generates three new queries per paper focusing on methodological alternatives, applications, and limitations—conditioned on retrieval history and document metadata. A random subset is added to the query list for subsequent iterations, enabling progressive exploration of the literature space while avoiding redundancy via query deduplication and keyword overlap suppression.
- Core assumption: High-relevance documents contain semantic cues (methods, applications, limitations) that can be extracted to guide further exploration; LLMs can reliably generate diverse, non-redundant follow-up queries.
- Evidence anchors:
  - [abstract] "query evolution to enable more flexible and effective search"
  - [section] Page 4, Section 3.2: "The Query Evolver Agent then generates three new queries for pi ∈ P, focusing on its methodological insights, applications, and limitations. These queries are conditioned on the retrieval history trajectory..."
  - [section] Page 8, Table 3: Query Evolution improves F1 from 0.33 to 0.34 on AutoScholar and 0.24 to 0.26 on SPARBench, with precision gains (+0.02 on both).
- Break condition: If evolved queries drift from original intent, precision suffers; if document content is sparse, evolved queries may be generic, reducing diversification benefit.

## Foundational Learning
- **Reference Chain (RefChain)** – The recursive, citation-driven process of following references from one paper to another to expand retrieval scope beyond direct query matches.
  - Why needed here: Core to SPAR's recall enhancement mechanism; distinguishes it from keyword-based search.
  - Quick check question: Given a paper on "transformer attention mechanisms," what would its RefChain expansion retrieve? (Papers it cites, e.g., foundational attention papers, LSTM works.)

- **Multi-Source Retrieval** – Querying multiple academic databases (Google Scholar, Semantic Scholar, OpenAlex, PubMed, arXiv) with source-adaptive strategies (keyword extraction vs. full query string).
  - Why needed here: Different sources have domain strengths (PubMed for biomedicine, arXiv for CS preprints); source selection is part of query interpretation.
  - Quick check question: For a query about "CRISPR gene editing ethics," which sources should be prioritized? (PubMed for biomedical, OpenAlex for interdisciplinary.)

- **Reranking Signals** – Post-retrieval reordering based on publication authority (venue prestige, author h-index), timeliness (recency, temporal constraints), and relevance scores.
  - Why needed here: Final output quality depends on surfacing authoritative, recent, relevant papers; reranking improves user-facing ranking.
  - Quick check question: Two papers match a query equally on relevance—one from NeurIPS 2024, one from an unknown workshop 2019. Which should rank higher and why? (NeurIPS 2024: higher authority + recency.)

## Architecture Onboarding
- **Component map**: User query → Query Understanding (refined queries Q) → Retrieval Agent (multi-source fetch + RefChain) → Judgement Agent (filter to R, expand to P) → Query Evolver (new queries → append to Q) → loop until P size/depth limit → Reranker → final ranked list.
- **Critical path**: Query Understanding Agent: Intent classification → domain identification → temporal parsing → multi-query refinement → source selection; Retrieval Agent: Source-adaptive querying → result aggregation → RefChain extraction (one layer); Judgement Agent: Relevance scoring → threshold filtering → Related Pool (R) and Paper Cache (P) population; Query Evolver Agent: Document-guided query generation (methods/applications/limitations) → deduplication → iterative loop control; Reranker: Authority/timeliness/relevance weighted reordering of top-K results.
- **Design tradeoffs**: RefChain depth: Single layer limits recall but controls noise/computation (vs. PaSa's RL-trained depth); Prompt complexity: Brief prompts outperform complex prompts for relevance judgment (Qwen3-32B brief: F1=0.38 vs. complex: 0.08 on AutoScholar), but complex prompts may be needed for nuanced queries; Precision vs. recall: RefChain boosts recall but drops precision; Query Evolution and QInterp boost precision but may reduce recall.
- **Failure signatures**: RefChain in low-citation domains (new fields, sparse metadata): minimal recall gain, unchanged precision; Intent misclassification: Query refinement produces off-target expanded queries → low precision; Reranker over-weighting recency: Foundational older papers ranked too low; LLM relevance judge inconsistency: Different prompts yield divergent scores (brief vs. complex).
- **First 3 experiments**: 1) Ablate RefChain: Run SPAR with and without RefChain on SPARBench; measure recall/precision/F1 to quantify recall gain vs. precision cost; 2) Vary judgement prompt: Compare brief vs. complex relevance prompts across models (Qwen3-32B, LLaMA3.3-70B) on AutoScholar subset; select best prompt-model combo; 3) Tune reranking weights: Adjust authority/timeliness/relevance weights; measure Recall@5 on AutoScholar top-10 to find optimal balance (baseline: +27.6% gain with current weights).

## Open Questions the Paper Calls Out
None

## Limitations
- Domain Generalization: Effectiveness in highly specialized domains (e.g., pure mathematics, niche engineering fields) remains untested; RefChain may underperform in fields with sparse citation networks.
- LLM Dependency Risks: Performance heavily depends on underlying LLM capabilities; results may not transfer to other LLM families or smaller models.
- Scalability Concerns: Iterative query evolution and multi-source retrieval may face practical limitations in high-query-volume production environments.

## Confidence
- **High Confidence**: Query understanding improves precision through structured intent decomposition; RefChain expansion effectively increases recall at the cost of precision; The overall multi-agent framework architecture is sound and interpretable.
- **Medium Confidence**: Query evolution provides meaningful diversification benefits; The specific weighting scheme in the reranker optimally balances authority, timeliness, and relevance.
- **Low Confidence**: Generalizability to domains outside computer science and biomedicine; Performance consistency across different LLM model families and sizes.

## Next Checks
1. **Cross-Domain Validation**: Evaluate SPAR on academic queries from mathematics, social sciences, and humanities domains to assess generalizability. Measure whether RefChain and query evolution mechanisms maintain effectiveness in fields with different citation practices and terminology.
2. **Ablation of LLM Components**: Systematically replace the LLM-based Query Understanding and Judgement Agents with rule-based or keyword-matching alternatives. Quantify the performance contribution of LLM components versus architectural design.
3. **Real-World Usage Study**: Deploy SPAR in a live academic search environment with actual user queries over a 4-week period. Collect click-through rates, session abandonment rates, and user satisfaction scores to validate that benchmark performance translates to practical utility.