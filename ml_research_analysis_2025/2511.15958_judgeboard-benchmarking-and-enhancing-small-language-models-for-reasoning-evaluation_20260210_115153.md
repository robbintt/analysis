---
ver: rpa2
title: 'JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning
  Evaluation'
arxiv_id: '2511.15958'
source_url: https://arxiv.org/abs/2511.15958
tags:
- reasoner
- qwen3
- profile
- logical
- gemma3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JudgeBoard introduces a novel evaluation pipeline that directly
  queries language models to assess the correctness of candidate answers without requiring
  extra answer comparisons. This approach treats models as direct evaluators, enabling
  scalable and flexible assessment across mathematical and science reasoning domains.
---

# JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation

## Quick Facts
- **arXiv ID**: 2511.15958
- **Source URL**: https://arxiv.org/abs/2511.15958
- **Reference count**: 10
- **Primary result**: Multi-Agent Judging (MAJ) framework substantially improves SLM judgment accuracy through profile-induced reasoning diversity and collaborative debate.

## Executive Summary
JudgeBoard introduces a novel evaluation pipeline that directly queries language models to assess the correctness of candidate answers without requiring extra answer comparisons. This approach treats models as direct evaluators, enabling scalable and flexible assessment across mathematical and science reasoning domains. The study constructs task-specific leaderboards using both accuracy and Elo-style metrics, revealing a significant performance gap between small language models (SLMs) and large language models (LLMs) in judgment tasks. To address SLM limitations, the Multi-Agent Judging (MAJ) framework leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results show that MAJ substantially improves SLM reliability and consistency, with smaller-sized Qwen3 models (4B, 8B, and 14B versions) performing comparably or even better than their larger-sized counterparts across all tasks and datasets.

## Method Summary
The JudgeBoard framework directly queries judge models to evaluate answer correctness without comparison-based metrics. For each question-answer pair, judges receive the question, candidate answer, and a structured prompt to determine correctness. The system constructs leaderboards using both accuracy metrics (overall, Student-Wrong, Student-Right) and Elo ratings that account for question difficulty. The Multi-Agent Judging (MAJ) enhancement assigns distinct reasoning profiles (Deductive, Logical, Robust Reasoner) to identical SLM backbones, then runs structured debate and voting to aggregate judgments. This profile-induced diversity aims to cancel individual biases while amplifying shared correct conclusions.

## Key Results
- MAJ substantially improves SLM judgment accuracy across all tested datasets and model families
- Smaller Qwen3 models (4B, 8B, 14B) achieve comparable or better performance than their 30B counterpart
- Significant performance gap exists between SLMs and LLMs in direct judgment tasks
- Profile effectiveness varies by model family, with some models showing inconsistent MAJ gains

## Why This Works (Mechanism)

### Mechanism 1: Profile-Induced Reasoning Diversity Reduces Systematic Errors
Assigning distinct reasoning profiles to identical SLM backbones creates complementary evaluation perspectives that, when aggregated through debate and voting, outperform single-agent judgment. Profile prompts steer the same model toward different reasoning strategies, enabling individual biases to cancel while shared correct conclusions amplify. Evidence shows profile quality varies significantly, with Robust Reasoner yielding most consistent Elo gains across categories while Logical Reasoner is most inconsistent.

### Mechanism 2: Elo-Based Relative Scoring Captures Difficulty-Weighted Judgment Quality
Pairwise Elo ratings reveal judgment capabilities that raw accuracy obscures by rewarding correct judgments on hard questions where others fail. This implicitly weights by task difficulty, differentiating models with similar overall accuracy but different consistency patterns. The approach is particularly valuable when question samples cover sufficient difficulty variance, though it can be destabilized by imbalanced splits with few correct student answers.

### Mechanism 3: Direct Correctness Query Eliminates Comparator Dependency
Asking models to judge correctness directly rather than comparing candidate pairs enables scalable single-pass evaluation without requiring entailment metrics or multiple answer versions. Judges receive question + candidate answer + structured prompt and output binary judgment with reasoning. This eliminates dependency on comparison anchors and predefined metrics that may not capture nuanced reasoning errors.

## Foundational Learning

- **Model-as-Judge Paradigm**: Understanding this role shift is prerequisite to interpreting results. Quick check: Can you explain why judging correctness is a different capability than generating correct answers?
- **Elo Rating System**: Leaderboards use Elo alongside accuracy; understanding expected-score calculation and K-factor updates is necessary to interpret Table 1. Quick check: Given two judges with ratings 1050 and 1000, what is the expected score for the higher-rated judge using the logistic formula?
- **Multi-Agent Debate Protocols**: MAJ relies on structured debate followed by voting; understanding turn-taking, revision phases, and aggregation is required to replicate or extend the framework. Quick check: In a 3-agent MAJ system, what happens if the post-debate votes are tied?

## Architecture Onboarding

- **Component map**: Student Model → Judge Models → Profiler → Debate Orchestrator → Aggregator → Elo Engine → Leaderboard Builder
- **Critical path**: Student answer generation → Judge evaluation (with or without debate) → Pairwise comparison against gold labels → Elo update → Leaderboard construction
- **Design tradeoffs**: More debate rounds → potentially better convergence but higher latency and cost; more profiles → more diversity but risk of low-quality profiles adding noise; larger judge committee → better robustness but diminishing returns
- **Failure signatures**: High overall accuracy but low Student-Wrong accuracy indicates sycophantic judges; Elo score highly sensitive to small sample suggests checking question count and balance; debate causing accuracy drop suggests profiles may induce adversarial rather than complementary reasoning
- **First 3 experiments**:
  1. Reproduce single-judge leaderboard on one dataset: Run Qwen3-4B, Qwen3-14B, Gemma3-12B on ARC-Challenge subset; verify overall accuracy, SW accuracy, and Elo rankings match Table 3 within sampling variance
  2. Ablate MAJ components: Run MAJ with (a) no profiles, (b) profiles but no debate, (c) full MAJ; isolate contribution of diversity vs. debate vs. aggregation
  3. Stress-test on imbalanced data: Construct subset with 90% wrong student answers; observe whether Elo and accuracy metrics remain stable or whether SW accuracy dominates rankings

## Open Questions the Paper Calls Out
None

## Limitations
- Profile effectiveness varies significantly across model families, suggesting the reasoning profiles may not be universally robust and could be sensitive to model architecture or pretraining data
- Elo-based scoring relies on balanced difficulty distributions and sufficient sample sizes; datasets with few correct student answers may yield unstable rankings
- The study does not explicitly model or report the latency and cost of multi-agent debate, making it difficult to assess practical trade-offs between accuracy gains and computational overhead

## Confidence

- **High**: Direct correctness query approach reduces comparator dependency and enables scalable single-pass evaluation
- **Medium**: Profile-induced reasoning diversity improves SLM judgment accuracy through debate and voting
- **Low**: Elo-based relative scoring consistently captures meaningful differences in judgment quality beyond raw accuracy

## Next Checks

1. Conduct a cross-model ablation study comparing MAJ performance across diverse SLM families to identify which models benefit most from profile-based debate and whether profiles need customization per model family
2. Test Elo stability on artificially imbalanced datasets to quantify its sensitivity to question-answer pair distributions and compare it against alternative ranking methods
3. Measure and report end-to-end latency and cost for MAJ inference on a representative dataset, then compare against single-judge baselines to determine if accuracy gains justify computational overhead in practical settings