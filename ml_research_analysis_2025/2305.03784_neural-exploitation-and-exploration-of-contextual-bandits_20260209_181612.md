---
ver: rpa2
title: Neural Exploitation and Exploration of Contextual Bandits
arxiv_id: '2305.03784'
source_url: https://arxiv.org/abs/2305.03784
tags:
- neural
- exploration
- reward
- ee-net
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the exploration-exploitation dilemma in contextual
  multi-armed bandits by introducing EE-Net, a novel neural-based approach. Instead
  of relying on statistical confidence bounds like existing methods, EE-Net employs
  two neural networks: one for exploitation (learning the reward function) and another
  for exploration (learning potential gains compared to the current reward estimate).'
---

# Neural Exploitation and Exploration of Contextual Bandits

## Quick Facts
- arXiv ID: 2305.03784
- Source URL: https://arxiv.org/abs/2305.03784
- Reference count: 40
- One-line primary result: Introduces EE-Net, a neural approach for contextual bandits using bidirectional exploration via two networks, achieving $\tilde{O}(\sqrt{T})$ regret with faster inference than NeuralUCB.

## Executive Summary
This paper addresses the exploration-exploitation dilemma in contextual multi-armed bandits by introducing EE-Net, a novel neural-based approach. Instead of relying on statistical confidence bounds like existing methods, EE-Net employs two neural networks: one for exploitation (learning the reward function) and another for exploration (learning potential gains compared to the current reward estimate). The exploration network takes gradients of the exploitation network as input, enabling adaptive upward and downward exploration based on whether the model underestimates or overestimates rewards. The authors provide an instance-dependent regret bound of $\tilde{O}(\sqrt{T})$ and demonstrate superior performance compared to linear and neural bandit baselines on real-world datasets. The method also offers computational advantages, being faster at inference time while maintaining competitive regret bounds.

## Method Summary
EE-Net separates the bandit problem into two neural networks: $f_1$ for exploitation (estimating rewards) and $f_2$ for exploration (estimating potential gains). The key innovation is using the gradient of $f_1$ as input to $f_2$, allowing the exploration network to learn from the exploitation network's internal state. The potential gain is defined as the signed residual $r - f_1(x)$, enabling bidirectional exploration. Gradients are reduced to a manageable dimensionality (10D) using Locally Linear Embedding (LLE). The final arm selection is based on $f_1(x_i) + f_2(\phi(x_i))$, where $\phi(x)$ is the reduced gradient representation. The method is trained with SGD updates on both networks every round, with $f_1$ minimizing reward error and $f_2$ minimizing residual error.

## Key Results
- EE-Net achieves $\tilde{O}(\sqrt{T})$ instance-dependent regret bound while avoiding the $O(p^2)$ space complexity of NeuralUCB.
- Outperforms NeuralUCB, Neural-Epsilon, and NeuralTS on MNIST, Movielens, and Yelp datasets in terms of cumulative regret.
- Demonstrates faster inference time compared to NeuralUCB due to avoiding matrix inversions.
- Signed potential gain labels ($y = r - f_1$) outperform absolute value or ReLU variants, validating the bidirectional exploration mechanism.

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Bidirectional Exploration
- **Claim:** EE-Net resolves the exploration-exploitation dilemma more effectively than UCB or Thompson Sampling by distinguishing between "upward" exploration (when rewards are underestimated) and "downward" exploration (when rewards are overestimated).
- **Mechanism:** The method trains a second neural network ($f_2$) to predict the *potential gain*, defined as the residual between the observed reward and the exploitation network's prediction ($y = r - f_1(x)$). Unlike UCB, which only adds a positive confidence bound, $f_2$ outputs signed values. If $f_1$ underestimates, $f_2$ outputs a positive value (upward); if $f_1$ overestimates, $f_2$ outputs a negative value (downward).
- **Core assumption:** The residual $r - f_1(x)$ is a learnable signal that correlates with the model's uncertainty, rather than pure noise.
- **Evidence anchors:**
  - [abstract] "...adaptive upward and downward exploration based on whether the model underestimates or overestimates rewards."
  - [section 4] Definition 4.1 defines "Potential Gain" and explains the labeling mechanism.
  - [corpus] Corpus evidence regarding this specific bidirectional mechanism is weak; neighbors focus on standard UCB/TS extensions.
- **Break condition:** If the exploitation network $f_1$ is a perfect predictor (residual is 0), or if the noise dominates the signal, $f_2$ will output noise, degrading performance.

### Mechanism 2: Gradient-Based Input Representation
- **Claim:** The exploration network ($f_2$) can make better exploration decisions by observing the internal state of the exploitation network ($f_1$) rather than just the raw context.
- **Mechanism:** The input to $f_2$ is the gradient of $f_1$ with respect to its parameters ($\nabla_{\theta_1} f_1$). This gradient incorporates information about the arm's context and the discriminative ability of $f_1$. $f_2$ effectively learns to map the "sensitivity" of the current model to specific inputs to predict potential gains.
- **Core assumption:** The gradient $\nabla_{\theta_1} f_1$ contains sufficient information to predict the potential gain, and a reduced-dimensionality representation (via LLE) preserves this information.
- **Evidence anchors:**
  - [page 6] "The input of the exploration network $f_2$ is the gradient of $f_1$ to incorporate the information of arm contexts and the discriminative ability of $f_1$."
  - [corpus] No direct validation in corpus; neighbors focus on external context or temporal features, not internal gradient states.
- **Break condition:** If the exploitation network is very deep/wide, the raw gradient dimension is massive. Without effective dimensionality reduction (e.g., LLE), $f_2$ becomes computationally infeasible or suffers from the curse of dimensionality.

### Mechanism 3: Instance-Dependent Regret Bounds
- **Claim:** Under over-parameterized conditions, separating exploration and exploitation into two networks preserves the $\tilde{O}(\sqrt{T})$ regret bound while simplifying the complexity term compared to NeuralUCB.
- **Mechanism:** The proof treats the exploration network as an online regression problem predicting residuals. By avoiding the maintenance of a full gradient outer-product matrix (required by NeuralUCB/NeuralTS), the method reduces space complexity from $O(p^2)$ to $O(p)$ while maintaining statistical efficiency.
- **Core assumption:** The network is sufficiently wide ($m \ge \Omega(poly(T, L, R, n...))$) to satisfy over-parameterization assumptions (NTK regime).
- **Evidence anchors:**
  - [abstract] "...provides an instance-dependent regret bound of $\tilde{O}(\sqrt{T})$..."
  - [theorem 1] States the bound relies on the complexity term $\Psi(\theta^2_0, R)$ which is the smallest squared regression error.
- **Break condition:** If the network is under-parameterized (cannot fit the reward function), the theoretical guarantees dissolve.

## Foundational Learning

- **Concept: Contextual Multi-Armed Bandits (CMAB)**
  - **Why needed here:** This is the base problem class. You must understand the trade-off between pulling the best-known arm (exploitation) and trying new arms to gain information (exploration).
  - **Quick check question:** How does a contextual bandit differ from a standard A/B test?

- **Concept: Upper Confidence Bound (UCB) vs. Thompson Sampling (TS)**
  - **Why needed here:** The paper positions itself as a replacement for these standard strategies. UCB relies on "optimism in the face of uncertainty" (always adding a positive bonus), while TS samples from a posterior. EE-Net claims these fail to handle "downward" exploration properly.
  - **Quick check question:** Why is "optimism" (UCB) insufficient when the model currently overestimates the reward of a suboptimal arm?

- **Concept: Neural Tangent Kernel (NTK) & Over-parameterization**
  - **Why needed here:** The theoretical guarantees rely on the NTK regime, where infinitely wide neural networks behave like kernel regression. This explains why the authors can claim a regret bound similar to linear bandits despite using deep networks.
  - **Quick check question:** In the NTK regime, how does the behavior of gradient descent change compared to standard deep learning?

## Architecture Onboarding

- **Component map:**
1. **Exploitation Network ($f_1$):** Standard fully-connected network; Input: Context $x$; Output: Estimated Reward $\hat{r}$.
2. **Gradient Extractor:** Computes $\nabla_{\theta_1} f_1(x)$.
3. **Dimensionality Reduction:** Applies Locally Linear Embedding (LLE) to reduce gradient dimensions (e.g., to 10D vector $\phi(x)$).
4. **Exploration Network ($f_2$):** Smaller network; Input: Reduced gradient $\phi(x)$; Output: Potential Gain (signed scalar).
5. **Aggregator:** Selects arm $\hat{i} = \text{argmax}_i (f_1(x_i) + f_2(\phi(x_i)))$.

- **Critical path:**
 The inference speed relies on $f_2$ being faster to evaluate than computing a matrix inverse (required by NeuralUCB). The critical path during training is synchronizing the updates: $f_1$ updates based on reward error, $f_2$ updates based on residual error.

- **Design tradeoffs:**
 - **Label Selection for $f_2$:** The paper tests $y = r - f_1$ (signed), $|r - f_1|$ (absolute), and ReLU$(r - f_1)$. Signed performed best (Figure 3).
 - **Gradient Dimension:** Raw gradients are huge. LLE is used to compress them. Higher dimensions (e.g., 200) offer slightly better performance but cost more compute (Table 3).
 - **Space Complexity:** EE-Net avoids the $O(p^2)$ memory of NeuralUCB, trading slightly higher training compute (two networks) for faster inference and lower memory.

- **Failure signatures:**
 - **Exploding Exploration:** If $f_2$ overestimates potential gains, it will dominate $f_1$, leading to random selection.
 - **LLE Staleness:** If the LLE embedding is not updated or computed correctly as $f_1$ evolves, $f_2$ receives garbage input.
 - **Stagnation:** If $f_1$ learns very quickly, residuals approach zero, and $f_2$ may provide no signal, causing the algorithm to default to pure exploitation prematurely.

- **First 3 experiments:**
1. **Baseline Verification:** Replicate the MNIST experiment (Figure 2a) comparing EE-Net vs. NeuralUCB and Neural-Epsilon to verify the convergence speed advantage.
2. **Ablation on $f_2$ Input:** Compare using the raw gradient $\nabla f_1$ vs. the LLE-reduced gradient $\phi(x)$ to measure the performance drop (if any) vs. speed gain.
3. **Label Ablation:** Run the UCI/Movielens tasks using the three label variants ($y_1, y_2, y_3$) to confirm that preserving the sign (upward/downward) is statistically significant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the initialization and training strategy of the exploration network $f_2$ be optimized across different learning phases to minimize early-stage suboptimality?
- Basis in paper: [explicit] Appendix A.5 states that "optimizing the initialization and training strategy of $f_2$ across different phases remains an interesting direction for future exploration" to address the issue of inaccurate initial potential gain estimates.
- Why unresolved: The current method relies on random initialization, which leads to suboptimal actions during the initial rounds before $f_2$ accumulates enough data to refine its estimates.
- What evidence would resolve it: A modified training schedule or initialization procedure (e.g., meta-learning or pre-training) that demonstrably lowers the cumulative regret in the first $k$ rounds compared to the random baseline.

### Open Question 2
- Question: What is the theoretical impact of dimensionality reduction techniques (like LLE) on the regret bound, and does projecting the gradient input $\nabla_{\theta_1} f_1$ lose critical information required for effective exploration?
- Basis in paper: [inferred] Remark 4.1 notes that the gradient input might have high dimensions and proposes using dimensionality reduction (specifically LLE), but the theoretical analysis in Section 5 assumes the network operates on the inputs without explicitly bounding the error introduced by such compression.
- Why unresolved: While the paper empirically tests different dimensions (Table 3), it does not provide theoretical guarantees that the compressed gradient preserves the necessary properties to maintain the $\tilde{O}(\sqrt{T})$ regret bound.
- What evidence would resolve it: A theoretical extension of Theorem 1 that includes a term for compression error or an empirical study showing the regret bound holds (or degrades gracefully) as the dimensionality of the exploration input decreases.

### Open Question 3
- Question: Can the EE-Net regret bounds be extended to non-fully-connected architectures, such as Transformers or Convolutional Neural Networks, which are mentioned as potential structures for $f_1$?
- Basis in paper: [inferred] Remark 4.1 mentions that "structures of the networks $f_1$ and $f_2$ can vary... for instance, in vision tasks, $f_1$ can be implemented as transformers," whereas the theoretical guarantees in Section 5 strictly assume the fully-connected network structure defined in Equation (5.1).
- Why unresolved: The proof relies on the convergence properties of over-parameterized fully-connected networks and the Neural Tangent Kernel (NTK), which may not directly apply to attention mechanisms or convolutional layers without modification.
- What evidence would resolve it: A derivation of the regret bound specifically for Transformer or CNN-based exploitation networks, or empirical proof that the existing theoretical framework applies to these architectures.

## Limitations
- The exact implementation of LLE for gradient dimensionality reduction is not fully specified, creating uncertainty about runtime and stability.
- Theoretical guarantees rely on over-parameterization assumptions (NTK regime) that may not hold in all practical scenarios.
- Performance claims depend heavily on unspecified implementation details like learning rates and LLE fitting strategy.

## Confidence

- **High Confidence:** The bidirectional exploration mechanism and computational advantage claims are well-supported by the methodology and theoretical analysis.
- **Medium Confidence:** The instance-dependent regret bound relies on over-parameterization assumptions that are reasonable but may not hold in all practical scenarios.
- **Low Confidence:** The claim of superior performance on real-world datasets depends heavily on exact implementation details that are not fully specified.

## Next Checks

1. **LLE Implementation Impact:** Run MNIST experiments comparing EE-Net with three gradient input strategies: (a) raw gradients, (b) fixed random projection to 10D, and (c) LLE-reduced gradients (re-fitted each round) to quantify the performance vs. speed tradeoff.
2. **Signed vs. Absolute Residuals:** Conduct ablation studies on the Movielens/Yelp datasets using all three label variants ($y_1 = r - f_1$, $y_2 = |r - f_1|$, $y_3 = \text{ReLU}(r - f_1)$) to statistically validate that preserving the sign is crucial for the bidirectional exploration mechanism.
3. **Network Width Sensitivity:** Test the algorithm with varying network widths (e.g., 50, 100, 200) on MNIST to verify that the theoretical guarantees and performance advantages hold under different over-parameterization levels.