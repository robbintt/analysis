---
ver: rpa2
title: 'LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data'
arxiv_id: '2510.24434'
source_url: https://arxiv.org/abs/2510.24434
tags:
- luxembourgish
- instruction
- data
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LuxIT is a monolingual instruction tuning dataset for Luxembourgish
  developed to address the scarcity of high-quality training data for low-resource
  languages. The dataset was synthetically generated from native Luxembourgish texts
  using DeepSeek-R1-0528, chosen for its proficiency in the language.
---

# LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data

## Quick Facts
- arXiv ID: 2510.24434
- Source URL: https://arxiv.org/abs/2510.24434
- Reference count: 0
- LuxIT is a monolingual instruction tuning dataset for Luxembourgish with 59,242 high-quality samples, developed using synthetic generation and LLM-as-a-judge quality assurance.

## Executive Summary
LuxIT addresses the scarcity of high-quality instruction tuning data for low-resource languages by creating a monolingual dataset for Luxembourgish. The dataset was synthetically generated from native Luxembourgish texts using DeepSeek-R1-0528 and filtered through an LLM-as-a-judge approach, retaining 89.8% of initial samples. Small-scale LLMs were fine-tuned on LuxIT and evaluated on Luxembourgish language proficiency exams, showing mixed improvements across different CEFR levels. The work provides both a valuable resource for Luxembourgish NLP and a replicable methodology for instruction tuning in low-resource language contexts.

## Method Summary
The LuxIT dataset was created through a five-step pipeline: (1) extraction of Luxembourgish Wikipedia and RTL news articles, (2) application of heuristic filters including minimum character count and removal of stubs/lists, (3) generation of three instruction-answer pairs per seed using DeepSeek-R1-0528 with embedded context, (4) quality filtering using GPT-5-mini as an LLM-as-a-judge on four criteria (linguistic quality, factual accuracy, instruction adherence, helpfulness), and (5) post-filtering to produce the final dataset. Fine-tuning was performed using Unsloth with LoRA on Tesla V100 32GB GPUs, with specific hyperparameters per model. Evaluation was conducted on Luxembourgish language proficiency exams (INLL) across CEFR levels A1-C2.

## Key Results
- Generated 66,005 instruction-answer pairs synthetically from native Luxembourgish texts
- Filtered to 59,242 high-quality samples (89.8% retention rate) using LLM-as-a-judge quality assurance
- Mixed performance improvements observed when fine-tuning small LLMs on LuxIT, with gains on some CEFR levels but regressions on others
- Demonstrated methodology for creating instruction tuning datasets for low-resource languages

## Why This Works (Mechanism)
The approach leverages synthetic data generation using a model proficient in Luxembourgish (DeepSeek-R1-0528) combined with LLM-as-a-judge quality filtering to create high-quality instruction tuning data where none previously existed. By embedding context in instructions and using multiple quality criteria, the method ensures generated pairs are both linguistically accurate and instruction-adherent. The use of native Luxembourgish texts as seeds ensures domain relevance and cultural authenticity, while the filtering pipeline maintains high standards despite the synthetic generation process.

## Foundational Learning
- **Synthetic data generation for low-resource languages**: Why needed - addresses scarcity of native instruction data; Quick check - verify generated pairs match native speaker quality
- **LLM-as-a-judge quality filtering**: Why needed - scalable quality assurance for large synthetic datasets; Quick check - compare judge scores with human evaluation on sample subset
- **Instruction embedding with context**: Why needed - improves instruction relevance and response quality; Quick check - validate context is properly incorporated in generated responses
- **LoRA fine-tuning methodology**: Why needed - efficient parameter-efficient adaptation of small models; Quick check - monitor training loss and validation metrics per epoch
- **CEFR level benchmarking**: Why needed - standardized evaluation across proficiency levels; Quick check - ensure exam questions are properly parsed and scored
- **Monolingual instruction tuning**: Why needed - prevents language mixing in low-resource contexts; Quick check - verify all data and instructions remain in Luxembourgish

## Architecture Onboarding

**Component Map**
Raw Luxembourgish Texts -> Heuristic Filtering -> DeepSeek-R1-0528 Generation -> GPT-5-mini Quality Filtering -> Post-Filtering -> Fine-tuning Pipeline -> CEFR Benchmark Evaluation

**Critical Path**
Seed text extraction → generation with DeepSeek-R1-0528 → quality filtering with GPT-5-mini → dataset compilation → LoRA fine-tuning → benchmark evaluation

**Design Tradeoffs**
- Synthetic generation vs. human annotation: chose synthetic for scalability despite potential quality variance
- GPT-5-mini vs. human judges: chose automated filtering for efficiency despite occasional under-detection of linguistic issues
- LoRA vs. full fine-tuning: chose parameter-efficient adaptation for computational efficiency
- Single-language focus vs. multilingual: chose monolingual to avoid language mixing in low-resource context

**Failure Signatures**
- Invalid model outputs on exams indicates dataset may be too small or not diverse enough for consistent gains
- GPT-5-mini under-detecting linguistic issues suggests potential for false positives in quality filtering
- Mixed benchmark results (improvements on some levels, regressions on others) indicates potential overfitting to specific instruction types or exam formats

**First Experiments**
1. Generate 100 samples with DeepSeek-R1-0528 and manually validate quality before full pipeline execution
2. Run GPT-5-mini quality filtering on a small batch and compare with human evaluation to calibrate thresholds
3. Fine-tune a small model on 1,000 samples and evaluate on a single CEFR level to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Mixed benchmark results with improvements on some CEFR levels but regressions on others suggest dataset may not provide consistent benefits across all proficiency levels
- Reliance on future LLM-as-a-judge model (GPT-5-mini) creates reproducibility challenges and potential quality assurance gaps
- RTL news data access is not publicly available, limiting full reproducibility of the seed data collection process
- Small model size (1B parameters) may be insufficient to demonstrate consistent gains from instruction tuning

## Confidence

| Claim | Confidence |
|-------|------------|
| Dataset construction methodology is clearly specified and replicable | High |
| Reported benchmark improvements are consistent across all CEFR levels | Medium |
| Exact RTL data access and filtering heuristics are fully specified | Low |
| LLM-as-a-judge quality filtering reliably detects all linguistic issues | Low |

## Next Checks
1. Re-run the LLM-as-a-judge quality filtering with an ensemble of available models (e.g., GPT-4o, Claude-3.5) to assess consistency and robustness of the retained samples
2. Perform per-level CEFR accuracy breakdown and statistical significance testing to verify which model improvements are consistent versus noise
3. Test fine-tuning on a balanced subset of instruction types (e.g., summarization, Q&A, classification) to identify which task formats benefit most from LuxIT and whether gains generalize beyond language exams