---
ver: rpa2
title: A Review of the Long Horizon Forecasting Problem in Time Series Analysis
arxiv_id: '2506.12809'
source_url: https://arxiv.org/abs/2506.12809
tags:
- time
- series
- forecasting
- data
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive review of long horizon forecasting
  (LHF) in time series analysis, covering 35 years of research and focusing on deep
  learning approaches over the past 7 years. It examines how various neural architectures
  incorporate time series decomposition techniques, preprocessing methods, and windowing
  schemes to improve LHF performance.
---

# A Review of the Long Horizon Forecasting Problem in Time Series Analysis

## Quick Facts
- **arXiv ID**: 2506.12809
- **Source URL**: https://arxiv.org/abs/2506.12809
- **Reference count**: 12
- **One-line primary result**: Comprehensive 35-year review of long horizon forecasting with focus on deep learning approaches, identifying error propagation as primary challenge and frequency-domain techniques as key advancement

## Executive Summary
This paper presents a systematic review of long horizon forecasting (LHF) in time series analysis spanning 35 years, with particular emphasis on deep learning approaches from the past seven years. The authors examine how neural architectures incorporate time series decomposition, preprocessing methods, and windowing schemes to address the fundamental challenge of error propagation in multi-step forecasting. The review categorizes models into MLPs, RNNs, self-attention models, and pyramidal self-attention models, providing detailed analysis of their mechanisms and performance characteristics. Through ablation experiments on the ETTm2 dataset, the study demonstrates that frequency domain transformations enable more parameters without overfitting, while also revealing that simpler models like N-Linear can be competitive in univariate settings.

## Method Summary
The paper conducts a comprehensive literature review of 35 years of long horizon forecasting research, with detailed analysis of 12 deep learning models including NBEATS, NHITS, TiDE, N-Linear, D-Linear, FiLM, xLSTM, Informer, Autoformer, FEDformer, PatchTST, Pyraformer, and Triformer. The authors implement these models on the ETTm2 dataset using specific train/validation/test splits (16/4/4 months) and evaluate performance across four horizons (96, 192, 336, 720 time steps). Training employs early stopping based on MAE optimization, with models using various normalization schemes. The authors make their trained models and code publicly available through a GitHub repository. Ablation experiments systematically examine the impact of frequency domain transformations, decomposition techniques, and architectural choices on forecasting performance.

## Key Results
- Error propagation increases proportionally with horizon length, except for xLSTM and Triformer models which show better performance on longer horizons
- Frequency domain transformations enable more parameters without overfitting, with models like FEDformer, Autoformer, NBEATS, and FiLM using frequency domain effectively
- While self-attention models perform well, simpler models like N-Linear can be competitive in univariate settings, and the choice of layers and multivariate forecasting type remains an open question

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Regularization
If a model operates on frequency-domain transformations (e.g., Fourier/Wavelet), it may support a higher parameter count without overfitting compared to time-domain-only approaches, particularly on limited datasets. By projecting temporal data into the frequency domain, the model isolates dominant periodic signals and filters high-frequency noise. This effectively acts as a hard-coded inductive bias, reducing the hypothesis space and allowing the optimizer to allocate capacity to learnable parameters without fitting to random noise. The underlying assumption is that the time series contains periodic structures (seasonality) that are more efficiently represented in the frequency domain than in the raw time domain.

### Mechanism 2: Hierarchical Context Aggregation (Patching)
If an architecture groups time steps into "patches" or hierarchical nodes, it reduces the effective sequence length and computational complexity while improving the capture of macroscopic trends. Instead of attending to every individual time step (which is computationally expensive and noisy), models like PatchTST or Pyraformer aggregate adjacent steps into patches. This shifts the attention mechanism from point-wise correlations to sub-series level semantics, mimicking the human act of looking at "shapes" in a chart rather than individual dots. The semantic meaning of the time series is assumed to be contained in local contiguous segments (shapes/trends) rather than specific isolated time points.

### Mechanism 3: Normalization-Induced Stationarity
If a model subtracts the last value of the input sequence (or a moving average) before processing, it stabilizes performance against distribution shifts common in long horizons. By subtracting the last value and adding it back post-prediction, the model learns to forecast the residual change (difference) rather than the absolute value. This transforms a non-stationary target into a stationary one, which is easier for MLPs and Transformers to learn. The series is assumed to follow a continuity where future values are highly correlated with the last observed value, and the trend is the primary source of distribution shift.

## Foundational Learning

- **Concept: Iterative vs. Direct Multi-Step (DMS) Forecasting**
  - Why needed here: The paper identifies LHF as an "error propagation problem." Understanding that Iterative (IMS) models accumulate errors at each step while Direct (DMS) models predict the whole horizon at once is crucial for selecting architectures.
  - Quick check question: Does the model predict $y_{t+1}$ and feed it as input for $y_{t+2}$ (IMS), or does it output the vector $[y_{t+1}, ..., y_{t+H}]$ simultaneously (DMS)?

- **Concept: Decomposition (Trend vs. Seasonality)**
  - Why needed here: Many SOTA models rely on explicitly separating the time series into trend (long-term direction) and seasonality (repeating cycles) before modeling.
  - Quick check question: Can you explain how a moving average kernel extracts a trend component and leaves a seasonal residual?

- **Concept: Attention Complexity ($O(L^2)$)**
  - Why needed here: The review highlights "computational complexity and memory" as a bottleneck. Transformers struggle with long sequences because self-attention scales quadratically.
  - Quick check question: Why does doubling the input sequence length typically quadruple the memory usage in a standard Transformer?

## Architecture Onboarding

- **Component map:** Input Layer (Windowing) -> Preprocessing (Normalization/Decomposition) -> Feature Encoder (MLP/RNN/Attention) -> Projector (Linear layer to horizon size)

- **Critical path:** The choice of Preprocessing determines the Model Complexity tolerance. If you skip decomposition, you often need simpler models or frequency enhancements to handle noise. If you use heavy decomposition, standard Transformers perform better.

- **Design tradeoffs:**
  - **MLPs (N-Linear/TiDE):** Fastest training, competitive on univariate data, but may struggle with complex cross-variable interactions in multivariate LHF.
  - **Transformers (Informer/PatchTST):** Excellent at capturing long-term dependencies, but computationally expensive and sensitive to hyperparameters.
  - **RNNs (xLSTM):** Resists error propagation on long horizons, potentially bridging the gap between MLP speed and Transformer capacity.

- **Failure signatures:**
  - **The "Yellow Heatmap":** MSE heatmaps turning yellow at the tail end of the horizon indicate the model is drifting.
  - **Distribution Shift Collapse:** The model simply repeats the last input value for all future time steps.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Implement N-Linear (subtract last value -> MLP -> add back) on the ETTm2 dataset. If this fails, data preprocessing is likely incorrect.
  2. **Horizon Sweep:** Train a selected model and evaluate MSE at $H=\{96, 192, 336, 720\}$. Plot the error curve. If it curves upward exponentially, the model suffers from error propagation.
  3. **Ablation on Decomposition:** Run the model with and without input decomposition. Verify if "frequency domain" or "decomposition" actually improves metrics on the specific dataset.

## Open Questions the Paper Calls Out

1. **Model Layers vs. Multivariate Forecasting Type**: How does the choice of model layers interact with the multivariate forecasting type (single-input vs. multiple-input) to influence performance? The authors observe conflicting results where NHITS performs better in single-input settings while N-Linear excels in multiple-input settings, but the underlying cause for this divergence is not established.

2. **RNN Hybrid Performance**: How do older RNN hybrids (e.g., Expectation-biased LSTMs, DSTP-RNN) perform when evaluated against standard long horizon forecasting datasets? There is a lack of comparative data between these historical RNN approaches and current state-of-the-art models on uniform benchmarks like ETTm2.

3. **Patching in MLP Architectures**: Can patching techniques, currently effective in transformers, be successfully implemented in Multi-Layer Perceptron (MLP) architectures? While patching improves performance in self-attention models by increasing input history, it is unknown if this mechanism provides similar benefits to MLP-based architectures.

## Limitations
- Exclusive focus on ETTm2 dataset without extensive cross-dataset validation
- Absence of computational cost comparisons crucial for practical deployment decisions
- Limited ablation studies across diverse dataset characteristics to validate generalizability claims

## Confidence
- **High confidence**: Architectural taxonomy and experimental findings on ETTm2 dataset are well-supported
- **Medium confidence**: Claims about generalizability across datasets are primarily based on single dataset analysis
- **Low confidence**: Comparative claims between self-attention and simpler models lack comprehensive validation across diverse conditions

## Next Checks
1. **Cross-dataset generalization**: Replicate the key experiments (N-Linear vs. Transformer comparison) on at least two additional datasets with different characteristics to validate the claimed performance patterns hold beyond ETTm2.

2. **Computation-accuracy tradeoff analysis**: Measure and compare the computational resources (training time, memory usage, inference latency) for the top-performing models (xLSTM, Triformer, Autoformer) to quantify whether their performance gains justify their resource requirements.

3. **Preprocessing sensitivity study**: Systematically vary the decomposition parameters and normalization approaches across all models to determine whether the claimed benefits of frequency-domain transformations are consistent across different preprocessing configurations.