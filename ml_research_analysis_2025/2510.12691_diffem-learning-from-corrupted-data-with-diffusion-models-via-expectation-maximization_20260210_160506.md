---
ver: rpa2
title: 'DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation
  Maximization'
arxiv_id: '2510.12691'
source_url: https://arxiv.org/abs/2510.12691
tags:
- diffusion
- diffem
- conditional
- distribution
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffEM introduces a new EM-based method for training diffusion
  models from corrupted data by directly modeling the posterior distribution using
  a conditional diffusion model. Unlike prior approaches that approximate posterior
  sampling with unconditional diffusion models, DiffEM avoids ad hoc approximations
  by learning the posterior via conditional score matching.
---

# DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization

## Quick Facts
- arXiv ID: 2510.12691
- Source URL: https://arxiv.org/abs/2510.12691
- Authors: Danial Hosseintabar; Fan Chen; Giannis Daras; Antonio Torralba; Constantinos Daskalakis
- Reference count: 40
- Primary result: DiffEM outperforms prior methods on posterior sampling and unconditional generation from corrupted data

## Executive Summary
DiffEM introduces a novel EM-based framework for training diffusion models from corrupted observations by directly modeling the posterior distribution using a conditional diffusion model. Unlike prior approaches that approximate posterior sampling with unconditional diffusion models, DiffEM avoids ad hoc approximations by learning the posterior via conditional score matching. The method alternates between E-steps (generating reconstructions) and M-steps (training the conditional model), with monotonic improvement guarantees under appropriate conditions. Experiments on synthetic manifold learning, CIFAR-10, and CelebA datasets demonstrate that DiffEM consistently outperforms prior methods like Ambient-Diffusion and EM-MMPS in both posterior sampling and unconditional generation tasks.

## Method Summary
DiffEM addresses the challenge of learning diffusion models from corrupted data by implementing an Expectation-Maximization framework that directly models the posterior distribution $p(X|Y)$. The method alternates between E-steps, where reconstructions are generated from the current conditional model $q_\theta(X|Y)$, and M-steps, where the conditional model is trained via score matching to maximize the expected log-likelihood. Unlike previous approaches that approximate posterior sampling with unconditional diffusion models, DiffEM learns the posterior directly, avoiding ad hoc approximations. The framework includes theoretical guarantees for monotonic improvement and demonstrates practical effectiveness across various corruption types including masking, blurring, and JPEG compression.

## Key Results
- Consistently outperforms Ambient-Diffusion and EM-MMPS on posterior sampling tasks across CIFAR-10 and CelebA datasets
- Achieves superior unconditional generation quality while being computationally more efficient than moment-matching based methods
- Demonstrates robustness to various corruption types including random masking (Ï=0.75), Gaussian blur, and JPEG compression
- Shows monotonic improvement in training objective across EM iterations under appropriate conditions

## Why This Works (Mechanism)
DiffEM works by directly learning the posterior distribution $p(X|Y)$ rather than approximating it with an unconditional model. The EM framework ensures monotonic improvement in the log-likelihood objective by alternating between generating samples from the current model (E-step) and training the model to maximize expected log-likelihood (M-step). The conditional diffusion model architecture allows efficient posterior sampling through standard diffusion sampling techniques while maintaining the ability to generate unconditional samples by ignoring the conditioning information.

## Foundational Learning

**Conditional Diffusion Models**
*Why needed*: Required to model the posterior distribution $p(X|Y)$ given corrupted observations
*Quick check*: Can generate high-quality samples conditioned on corrupted inputs

**Score Matching**
*Why needed*: Enables training of the conditional model without requiring explicit likelihood computation
*Quick check*: Loss function (Eq. 10) properly estimates gradients of log-density

**Expectation-Maximization Framework**
*Why needed*: Provides theoretical guarantees for monotonic improvement in log-likelihood
*Quick check*: Algorithm alternates correctly between E-step and M-step

**Posterior Sampling**
*Why needed*: Core task is generating clean data from corrupted observations
*Quick check*: Samples from $q_\theta(X|Y)$ should reconstruct original data well

## Architecture Onboarding

**Component Map**
Data Loader -> Corruption Generator -> Conditional U-Net -> DDPM Sampler -> Score Matching Loss -> Optimizer

**Critical Path**
1. Load corrupted data Y
2. Generate reconstructions via DDPM sampling
3. Train conditional model using score matching loss
4. Update parameters via gradient descent

**Design Tradeoffs**
- Computational efficiency vs. sample quality (more EM iterations improve quality but increase cost)
- Model capacity vs. overfitting risk (deeper U-Net may capture more complex posteriors but requires more data)
- Noise schedule parameters affect sampling stability and speed

**Failure Signatures**
- MAD (Model Autophagy Disorder): Performance degradation after too many EM iterations with severe corruption
- Convergence failure: Poor reconstruction quality if initialization is far from true posterior
- Misspecification issues: Performance drops when corruption model assumptions are violated

**First Experiments**
1. Test posterior sampling quality on synthetic 2D manifold data with varying corruption levels
2. Evaluate unconditional generation on CIFAR-10 with random masking corruption
3. Compare convergence speed with EM-MMPS baseline on CelebA dataset

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational cost scales with number of EM iterations and dataset size due to repeated sampling
- Performance can degrade after sufficient iterations with severe corruption (MAD effect)
- Requires known corruption likelihood function Q(Y|X) rather than handling completely unknown corruptions

## Confidence
**High** confidence in core methodology and experimental results based on detailed specification
**Medium** confidence in computational efficiency claims due to limited high-resolution testing
**Medium** confidence in robustness claims across all corruption types based on limited corruption variations tested

## Next Checks
1. Test DiffEM with varying corruption types beyond those explored (e.g., structured vs. random corruptions) to better understand failure modes
2. Compare computational efficiency in terms of samples/second between DiffEM and moment-matching based methods
3. Evaluate the method's performance when the corruption likelihood Q(Y|X) is misspecified or unknown