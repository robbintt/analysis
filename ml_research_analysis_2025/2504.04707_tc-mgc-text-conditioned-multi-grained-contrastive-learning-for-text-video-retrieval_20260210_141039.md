---
ver: rpa2
title: 'TC-MGC: Text-Conditioned Multi-Grained Contrastive Learning for Text-Video
  Retrieval'
arxiv_id: '2504.04707'
source_url: https://arxiv.org/abs/2504.04707
tags:
- similarity
- video
- retrieval
- multi-grained
- tc-mgc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of text-video retrieval by proposing
  a Text-Conditioned Multi-Grained Contrastive Learning framework (TC-MGC). The core
  idea is to generate text-conditioned visual representations at multiple granularities
  and then apply similarity reorganization and decorrelation regularization to enhance
  cross-modal alignment.
---

# TC-MGC: Text-Conditioned Multi-Grained Contrastive Learning for Text-Video Retrieval

## Quick Facts
- **arXiv ID**: 2504.04707
- **Source URL**: https://arxiv.org/abs/2504.04707
- **Reference count**: 40
- **Primary result**: Achieves +2.8%, +2.2%, and +1.5% relative improvements in text-to-video retrieval R@1 on MSR-VTT, DiDeMo, and VATEX benchmarks respectively

## Executive Summary
This paper addresses the problem of text-video retrieval by proposing a Text-Conditioned Multi-Grained Contrastive Learning framework (TC-MGC). The core idea is to generate text-conditioned visual representations at multiple granularities and then apply similarity reorganization and decorrelation regularization to enhance cross-modal alignment. The method combines language-video attention with multi-grained contrastive learning, producing significant improvements over existing methods across three major benchmarks.

## Method Summary
TC-MGC generates text-conditioned visual representations through a cross-attention mechanism that filters text-irrelevant visual information. The framework computes four types of similarity scores (video-sentence, video-word, sentence-frame, frame-word) and applies a Similarity Reorganization module to selectively preserve high-magnitude similarities while removing or fusing low-magnitude ones. An auxiliary Similarity Decorrelation Regularization loss ensures balanced learning across different granularities by minimizing the variance of similarity scores for positive pairs.

## Key Results
- **R@1 improvements**: +2.8%, +2.2%, and +1.5% relative gains on MSR-VTT, DiDeMo, and VATEX respectively
- **State-of-the-art performance**: Outperforms existing methods on all three benchmarks with CLIP-ViT-B/32
- **Parameter efficiency**: Similarity Reorganization module adds only 0.4M parameters while delivering significant performance gains

## Why This Works (Mechanism)

### Mechanism 1: Text-Conditioned Visual Representation via Language-Video Attention
A cross-attention block uses sentence/word embeddings as queries over frame embeddings (keys/values), producing semantically relevant aggregated video and frame representations. This filters out text-irrelevant visual information that could mislead the model. The core assumption is that text-agnostic video features encode misleading information not described in the text.

### Mechanism 2: Similarity Reorganization (SR) for Noise Filtering
The SR module identifies attentive similarities and reorganizes cross-modal similarity vectors and matrices by preserving top-k similarities and removing/fusing low-magnitude ones. This reduces noisy feature interactions in the final aggregation. The core assumption is that low-magnitude similarities correspond to unimportant feature interactions that introduce noise.

### Mechanism 3: Similarity Decorrelation Regularization (SDR) for Balanced Learning
An auxiliary loss term computes the variance of multi-grained similarity scores for positive pairs, encouraging balanced scores across granularities. This prevents one granularity from dominating the aggregation. The core assumption is that performance is limited if one similarity score dominates, causing under-utilization of other granularities.

## Foundational Learning

- **Concept: Multi-Grained Contrastive Learning**
  - **Why needed here**: Understanding why combining coarse-grained (global) and fine-grained (local) contrasts captures both contextual and detailed semantic alignment is essential to grasp the model's motivation.
  - **Quick check question**: Can you explain why a model that only compares a full video to a full sentence might fail to retrieve a video where only a few frames match the query?

- **Concept: Cross-Attention in Transformers**
  - **Why needed here**: The core Language-Video Attention block uses scaled dot-product cross-attention. Understanding queries, keys, values, and attention weights is critical for comprehending how text-conditioned representations are generated.
  - **Quick check question**: In the language-video attention block, what serves as the `Query` and what serves as the `Key`/`Value` when generating a sentence-conditioned video representation?

- **Concept: Regularization in Loss Functions**
  - **Why needed here**: The SDR loss is an auxiliary regularization term added to the primary InfoNCE loss. Understanding how regularization modifies the optimization objective is key to understanding the model's training dynamics.
  - **Quick check question**: The SDR loss minimizes variance. What problem in the multi-grained similarity scores is this designed to solve?

## Architecture Onboarding

- **Component map**: Raw text & video -> CLIP Encoders -> Temporal Encoder (video) -> Language-Video Attention -> Multi-Grained Contrast -> SR/Bi-SR -> ISA/Bi-ISA -> LSA -> Final Score -> Loss
- **Critical path**: The data flows through CLIP encoders, temporal encoder, language-video attention block, similarity computation, reorganization, aggregation, and finally to the loss function.
- **Design tradeoffs**: Performance vs. speed (9x slower inference), simplicity vs. adaptability (static keep rate), capacity vs. overfitting (residual connections).
- **Failure signatures**: No improvement over baseline (poor keep rate tuning), high variance in similarity scores (SDR not effective), model favors one granularity (LSA instability).
- **First 3 experiments**:
  1. Validate SR Module: Compare TC-MGC with SR disabled vs. enabled on MSR-VTT.
  2. Ablate SDR Loss: Train with and without SDR loss, plot similarity score variance over training epochs.
  3. Compare Aggregation Methods: Replace LSA with simple average or softmax-weighted aggregation.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can a dynamic strategy be implemented within the Similarity Reorganization (SR) module to adaptively control the keep rate based on input data characteristics?
- **Open Question 2**: Does the integration of phrase-level textual and clip-level visual representations into the TC-MGC framework significantly improve retrieval recall?
- **Open Question 3**: Can the text-conditioned video representations generated by the Language-Video Attention block improve the performance of hierarchical cross-modal interaction models?
- **Open Question 4**: How can the Similarity Decorrelation Regularization (SDR) loss be stabilized to prevent performance degradation when aggregating specific combinations of multi-grained similarities?

## Limitations
- The model is approximately 9x slower than baseline CLIP models due to intensive attention computations
- The effectiveness of Similarity Reorganization depends heavily on the keep rate hyperparameter r, requiring dataset-specific tuning
- The assumption that low-magnitude similarity scores represent noise is not empirically validated

## Confidence

- **High Confidence**: Multi-grained contrastive learning improves retrieval performance (supported by ablation studies across all three benchmarks)
- **Medium Confidence**: Text-conditioned representations reduce semantic gaps (plausible but relies on attention mechanism correctness)
- **Medium Confidence**: Similarity Reorganization module effectiveness (empirically demonstrated but keep rate choice appears arbitrary)

## Next Checks

1. **Ablation of the keep rate r**: Systematically vary the keep rate r from 0.05 to 0.5 and measure the tradeoff between retrieval performance (R@1) and computational efficiency (inference time) to identify the optimal operating point.

2. **Attention mechanism analysis**: Visualize the attention weights produced by the Language-Video Attention block to verify that the model is attending to semantically relevant frames and words, and assess whether the attention patterns are consistent with human intuition.

3. **SDR loss sensitivity analysis**: Conduct experiments with varying SDR weights (Î± = 0, 0.1, 0.5, 1.0) and measure both the variance of similarity scores and retrieval performance to determine whether variance minimization directly correlates with improved retrieval.