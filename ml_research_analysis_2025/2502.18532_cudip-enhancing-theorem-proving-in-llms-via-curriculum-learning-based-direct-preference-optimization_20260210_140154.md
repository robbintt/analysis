---
ver: rpa2
title: 'CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct
  Preference Optimization'
arxiv_id: '2502.18532'
source_url: https://arxiv.org/abs/2502.18532
tags:
- theorem
- data
- preference
- proof
- proving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CuDIP, a novel method for enhancing automated
  theorem proving in Large Language Models (LLMs) using curriculum learning-based
  Direct Preference Optimization (DPO). The key challenge addressed is the limited
  alignment between theorem proving processes and human preferences in existing supervised
  fine-tuning methods.
---

# CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization

## Quick Facts
- **arXiv ID:** 2502.18532
- **Source URL:** https://arxiv.org/abs/2502.18532
- **Reference count:** 40
- **Primary result:** Achieves 38.5% pass@1 on MiniF2F-valid, a 7.4% improvement over baselines.

## Executive Summary
This paper introduces CuDIP, a novel method for enhancing automated theorem proving in Large Language Models (LLMs) using curriculum learning-based Direct Preference Optimization (DPO). The key challenge addressed is the limited alignment between theorem proving processes and human preferences in existing supervised fine-tuning methods. To overcome the lack of high-quality preference data for theorem proving, CuDIP introduces a method for constructing preference data using LLMs and existing theorem proving data, reducing reliance on human annotations while enhancing diversity. The method integrates this preference data construction with curriculum learning, iteratively fine-tuning the theorem proving model through DPO. Experiments on MiniF2F and ProofNet datasets demonstrate the effectiveness of CuDIP, achieving a maximum improvement of 7.4% on the MiniF2F-valid benchmark compared to baseline methods. The results show that CuDIP significantly enhances LLMs' reasoning capabilities in theorem proving tasks.

## Method Summary
CuDIP is a three-stage pipeline: (1) Preparation - construct curriculum data by difficulty (distance to proof completion), train base prover via SFT; (2) Preference Data Generation - sample tactics using the current prover, fine-grained scoring via MCTS, train a score generator to predict scores, construct preference pairs where score difference exceeds a threshold; (3) Curriculum-based DPO Iteration - apply DPO to the prover using preference pairs, iterating through curriculum levels from easy to hard. The method uses existing Lean 4 theorem proving data and constructs preference pairs without human annotation, reducing cost while enhancing diversity.

## Key Results
- Achieves 38.5% pass@1 on MiniF2F-valid, a 7.4% improvement over baseline methods
- Maximum improvement of 7.4% on MiniF2F-valid benchmark compared to baselines
- Outperforms existing methods on ProofNet dataset (25.0% pass@1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automated preference data construction via fine-grained scoring (FGPS) provides a scalable training signal for theorem proving without human annotation.
- **Mechanism:** For a proof state $s$, candidate tactics $t$ are sampled. A score is computed as `nsuccess/nattempt` via short-horizon MCTS. A "Score Generator" model is trained via SFT to predict these scores, scaling the labeling process. Tactic pairs with a score difference exceeding a threshold (e.g., >0.5) form DPO preference pairs $(s, t_w, t_l)$.
- **Core assumption:** The probability of downstream proof success within a limited search depth (Assumption: 10 steps) is a valid proxy for human preference regarding tactic quality.
- **Evidence anchors:** [abstract] Mentions utilizing LLMs and existing data to enhance preference diversity. [section] Section 4.3 (FGPS) details the scoring equation `Score(t|s) = nsuccess/nattempt` and the score generator's role. [corpus] Corpus evidence is weak for this specific ATP application; neighbors focus on other domains.
- **Break condition:** The MCTS search depth (limited to 10 in appendix) is insufficient to distinguish useful tactics from dead ends, resulting in noisy preference labels.

### Mechanism 2
- **Claim:** Curriculum-based DPO iteration improves sample efficiency and final performance by organizing training from simple to complex.
- **Mechanism:** Proof data is partitioned by "difficulty," defined as the distance (number of tactics) from the current state to the proof goal. The prover model undergoes iterative DPO fine-tuning: $P_0 \xrightarrow{DPO} P_1$ (easy data) $\dots \xrightarrow{DPO} P_n$ (hard data).
- **Core assumption:** An LLM must first master "easy" subgoals (close to the "no goals" state) before it can effectively learn to solve complex initial states.
- **Evidence anchors:** [abstract] Highlights "Curriculum Learning-based DPO Iterative Theorem Proving." [section] Section 4.2 (Definitions 1 & 2) defines Distance/Difficulty. Section 4.4 details the iteration. [corpus] "2D-Curri-DPO" confirms combining curriculum learning with DPO is an effective strategy.
- **Break condition:** The "distance to goal" metric fails to capture logical complexity, causing the curriculum to be poorly ordered.

### Mechanism 3
- **Claim:** Direct Preference Optimization (DPO) effectively aligns an LLM's tactic generation policy with proof success, improving upon supervised fine-tuning (SFT) baselines.
- **Mechanism:** The model is optimized using the DPO loss on the constructed preference pairs $(s, t_w, t_l)$. This directly increases the likelihood of high-scoring tactics ($t_w$) and decreases that of low-scoring tactics ($t_l$) relative to a reference model, bypassing the need for a separate reward model.
- **Core assumption:** The Bradley-Terry preference model underlying DPO is suitable for the structured, formal reasoning domain of theorem proving.
- **Evidence anchors:** [abstract] States this is the first work to apply DPO to formal ATP. [section] Section 4.4 details applying the DPO loss to the preference triplets. [corpus] "MDPO" and related DPO variants show strong results in mathematical reasoning.
- **Break condition:** DPO's mode-seeking behavior reduces the diversity of generated tactics, hindering the exploration required for tree search.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** CuDIP builds its entire training loop on DPO as a replacement for RLHF to align the prover model.
  - **Quick check question:** How does DPO eliminate the need for a reward model compared to PPO? (Answer: It analytically derives the optimal policy under the Bradley-Terry model, directly optimizing on preference pairs).

- **Concept: Curriculum Learning**
  - **Why needed here:** CuDIP's core contribution is structuring the DPO iterations based on a difficulty curriculum.
  - **Quick check question:** Why might training on examples ordered by difficulty (easy to hard) improve convergence? (Answer: It allows the model to learn basic patterns first, providing a stable foundation for complex tasks).

- **Concept: Interactive Theorem Proving (ITP)**
  - **Why needed here:** The prover operates in Lean, a formal system where every tactic is verified.
  - **Quick check question:** What is a "proof state" in Lean? (Answer: The current set of goals and hypotheses that must be resolved to complete the proof).

## Architecture Onboarding

- **Component map:**
  Prover Model ($P$) -> Score Generator ($G$) -> Data Pipeline

- **Critical path:**
  1. SFT base Prover on all data $\to P_0$.
  2. For each curriculum level $n$:
     - Sample tactics using $P_{n-1}$.
     - Score subset via MCTS; train Score Generator $G_n$.
     - Use $G_n$ to score remaining tactics.
     - Construct preference pairs where $\Delta Score > Th$.
     - Apply DPO to $P_{n-1}$ using pairs $\to P_n$.

- **Design tradeoffs:**
  - **Accuracy vs. Cost in Scoring:** The paper trades off scoring accuracy by training a Score Generator ($G$) instead of running expensive MCTS on every single candidate tactic. This is necessary for scalability but introduces approximation error.
  - **Curriculum Granularity:** The definition of "difficulty" is simple (distance to goal) and computationally cheap but may not reflect semantic complexity.

- **Failure signatures:**
  - **Stagnating pass@1:** If the Score Generator is inaccurate, DPO pairs are noisy, and performance plateaus early (Figure 3 shows continued gains, suggesting success here).
  - **Mode Collapse:** The model repeatedly generates the same few valid tactics for simple subgoals but fails on novel, hard states.

- **First 3 experiments:**
  1. **Baseline Verification:** Replicate the "Mathstral + SFT" baseline to ensure the foundation is correct.
  2. **Scoring Ablation:** Compare the performance of a DPO model trained with FGPS (using the Score Generator) vs. a model trained with binary success/fail labels to validate the "fine-grained" claim.
  3. **Curriculum Ablation:** Run the CuDIP pipeline with shuffled data (no curriculum) vs. ordered data to isolate the impact of the curriculum strategy.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not specify how the difficulty of problems is determined, which is crucial for curriculum learning effectiveness.
- The exact implementation details of the MCTS scoring algorithm and its integration with the Lean theorem prover are not provided, making exact replication difficult.
- The preference data construction relies heavily on synthetic data generated by the score generator, which may not fully capture human preferences.

## Confidence
- **High confidence:** The overall framework of combining curriculum learning with DPO for theorem proving is sound and the reported performance improvements are statistically significant.
- **Medium confidence:** The effectiveness of the fine-grained preference scoring (FGPS) method, as the correlation between MCTS scores and actual proof success is not thoroughly validated.
- **Medium confidence:** The claim that CuDIP is the first to apply DPO to formal theorem proving, as the paper does not provide a comprehensive survey of all related work in this specific domain.

## Next Checks
1. **MCTS Scoring Validation:** Conduct ablation studies comparing CuDIP's performance when using MCTS scores vs. simpler heuristic scores (e.g., tactic frequency or depth) to quantify the contribution of fine-grained scoring.
2. **Curriculum Ablation:** Run the full CuDIP pipeline with shuffled data (no curriculum) vs. ordered data to isolate the impact of the curriculum strategy and validate its necessity.
3. **Human Preference Alignment:** Design a small-scale human evaluation study where theorem proving experts rate the quality of tactics generated by CuDIP vs. baselines, to assess if the automated preference data aligns with human judgment.