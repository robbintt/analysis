---
ver: rpa2
title: 'InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object
  Interactions'
arxiv_id: '2502.20390'
source_url: https://arxiv.org/abs/2502.20390
tags:
- policy
- interaction
- reference
- contact
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InterMimic tackles the challenge of learning realistic physics-based
  human-object interactions from imperfect MoCap data. It uses a teacher-student distillation
  framework where subject-specific teacher policies first correct MoCap artifacts
  and handle retargeting, then a student policy is trained to generalize across diverse
  skills.
---

# InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions

## Quick Facts
- **arXiv ID**: 2502.20390
- **Source URL**: https://arxiv.org/abs/2502.20390
- **Reference count**: 40
- **Primary result**: InterMimic outperforms PPO and DAgger baselines in success rate and imitation duration across multiple MoCap datasets, and generalizes zero-shot to kinematic generators and humanoid robots.

## Executive Summary
InterMimic addresses the challenge of learning realistic physics-based human-object interactions from noisy motion capture data. It uses a teacher-student distillation framework where subject-specific teacher policies first correct MoCap artifacts and handle retargeting within a physics simulator, then a single student policy is trained to generalize across diverse skills. The student benefits from both teacher rollouts and RL fine-tuning, achieving higher success rates and longer imitation durations than pure imitation or RL baselines. The method also generalizes to kinematic generators for text-to-HOI and future interaction prediction, and adapts to humanoid robots without external retargeting.

## Method Summary
InterMimic uses a two-stage teacher-student distillation framework to learn physics-based human-object interaction policies from noisy MoCap data. First, subject-specific teacher policies are trained with PPO to refine MoCap data within a physics simulator, correcting artifacts like floating contacts. These refined teacher rollouts are then distilled into a single student policy (a Transformer) via a combination of reference distillation and DAgger-style policy distillation with PPO fine-tuning. The reward function separates embodiment-aware kinematic tracking from embodiment-agnostic dynamics tracking to enable retargeting across different body shapes. Contact-guided RL is used to recover plausible hand interactions missing from the original MoCap data.

## Key Results
- InterMimic outperforms PPO and DAgger baselines on OMOMO dataset, achieving higher success rates and longer imitation durations
- Student policy generalizes zero-shot to kinematic generators (HOI-Diff, InterDiff) for text-to-HOI and future interaction prediction
- Method adapts to humanoid robots without external retargeting, demonstrating embodiment generalization

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Teacher-Student Distillation for Handling Imperfect MoCap Data
The two-stage curriculum (teacher-first, then student) improves training stability by using subject-specific teachers to first refine noisy MoCap data within a physics simulator, correcting artifacts like contact floating. These cleaner teacher rollouts are then used to supervise a general student policy via distillation, resulting in better generalization than training directly on raw data.

### Mechanism 2: Embodiment-Agnostic Reward for Dynamics-Preserving Retargeting
The reward function separates kinematic tracking (embodiment-aware) from dynamics tracking (embodiment-agnostic), enforcing that interaction dynamics like object motion and contact forces remain consistent across different body shapes while allowing joint trajectories to vary.

### Mechanism 3: Contact-Guided RL for Recovering Missing Details (Hands)
The method infers contact intentions from kinematic cues like object acceleration and uses contact-promoting rewards to enable RL exploration that discovers plausible hand poses satisfying these dynamics, effectively filling in missing details from the original MoCap data.

## Foundational Learning

- **Concept**: Teacher-Student Distillation in RL
  - Why needed: The core framework relies on teachers providing refined demonstrations to train a more general student policy
  - Quick check: What are the two main roles the teacher policies play? (Answer: 1. Providing refined reference trajectories for the student to imitate. 2. Providing direct action supervision for policy distillation.)

- **Concept**: Reinforcement Learning (RL) with PPO and Imitation Rewards
  - Why needed: Policies are trained with RL, where reward functions guide agents to learn physically plausible interactions
  - Quick check: What is the key difference between embodiment-aware and embodiment-agnostic rewards? (Answer: The former tracks specific kinematics, while the latter tracks invariant dynamics like object motion and contact.)

- **Concept**: Motion Retargeting
  - Why needed: The method aims to handle data from different human subjects and humanoid robots, requiring motion mapping across different skeletons
  - Quick check: How does InterMimic integrate retargeting into its learning process? (Answer: By using a reward structure that enforces consistent interaction dynamics while allowing kinematic trajectories to vary.)

## Architecture Onboarding

- **Component map**: Raw MoCap Data -> Reference Data Processor -> Teacher Policy Training (RL with PSI, IET) -> Teacher Rollouts -> Student Policy Training (Distillation + RL fine-tuning) -> Final Student Policy

- **Critical path**: The flow from noisy MoCap data through teacher refinement to student distillation is the core of the method, with the physics simulator serving as the artifact correction mechanism.

- **Design tradeoffs**:
  - Teacher Complexity vs. Student Generality: More specialized teachers may provide cleaner data but increase distillation overhead
  - Simulator Fidelity vs. Realism: Box/cylinder approximations speed simulation but may miss fine-grained contact geometry
  - RL Exploration vs. Imitation Constraint: Balance between DAgger schedule (teacher-following) and PPO updates (exploration) is critical

- **Failure signatures**:
  - Unrecoverable Artifacts: Severe MoCap errors may cause teacher failures, leading to data discard from student training
  - Averaging Behavior in Student: Ineffective RL fine-tuning may cause the student to average different teacher actions
  - Unnatural Object Support: Policy may produce penetrations instead of relying on friction, indicating reward formulation flaws

- **First 3 experiments**:
  1. Validate Teacher Policy Refinement: Train teacher on flawed MoCap clip and compare output against raw data to verify artifact correction
  2. Ablate Distillation Components: Train student with only reference distillation, only policy distillation, and full approach to understand component contributions
  3. Test Generalization to Unseen Objects: Deploy student on novel objects with different geometries to evaluate zero-shot generalization

## Open Questions the Paper Calls Out
None

## Limitations
- The extent to which the physics simulator corrects versus introduces MoCap artifacts is not quantified
- The assumption of dynamics invariance across embodiments is plausible but not empirically validated across diverse body shapes
- The method's extensive compute requirements (8192 environments, multiple training phases) make efficiency difficult to assess

## Confidence

- **High Confidence**: The two-stage distillation framework is clearly described and teachers do refine data as claimed; success rates and tracking errors are specific and measurable
- **Medium Confidence**: Generalization to kinematic generators is demonstrated but evaluation is qualitative without direct quantitative baseline comparison
- **Medium Confidence**: Zero-shot robot adaptation is supported by single experiment, which is promising but not comprehensive

## Next Checks

1. **Artifact Correction Validation**: Systematically inject known contact artifacts into MoCap data and measure reduction after teacher refinement
2. **Embodiment Generalization Test**: Train student on diverse body shapes and evaluate task performance without retraining
3. **Robot Transfer Robustness**: Deploy policy on physical/simulated humanoid robot and measure performance degradation compared to simulation, focusing on interaction forces and object stability