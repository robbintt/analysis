---
ver: rpa2
title: Progressive Cross Attention Network for Flood Segmentation using Multispectral
  Satellite Imagery
arxiv_id: '2501.11923'
source_url: https://arxiv.org/abs/2501.11923
tags:
- attention
- segmentation
- features
- flood
- cross
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ProCANet, a deep learning model for flood
  segmentation using multispectral satellite imagery. The model leverages progressive
  self- and cross-attention mechanisms to optimally combine multispectral features
  for improved flood detection.
---

# Progressive Cross Attention Network for Flood Segmentation using Multispectral Satellite Imagery

## Quick Facts
- arXiv ID: 2501.11923
- Source URL: https://arxiv.org/abs/2501.11923
- Reference count: 21
- Primary result: IoU of 0.815 on Sen1Floods11 flood segmentation dataset

## Executive Summary
This paper introduces ProCANet, a deep learning model for flood segmentation using multispectral satellite imagery. The model leverages progressive self- and cross-attention mechanisms to optimally combine multispectral features for improved flood detection. Evaluated on Sen1Floods11 and custom Citarum River basin datasets, ProCANet achieved superior performance with an Intersection over Union (IoU) score of 0.815, outperforming state-of-the-art segmentation models. Ablation studies confirmed the importance of the attention mechanism and the effective use of RGB and NIR bands.

## Method Summary
ProCANet employs a dual-encoder U-Net architecture with progressive cross-attention blocks. The model processes both RGB+NIR imagery through one encoder and NIR-only imagery through a second encoder. At each pooling scale, self-attention masks amplify modality-specific features while cross-attention captures inter-modal dependencies. Features are fused via element-wise addition after attention application. The network uses 128×128 non-overlapping patches, filters patches with >50% nodata pixels, and is trained with Dice+BCE loss using Adam optimizer with cosine annealing warm restarts over 25 epochs.

## Key Results
- Achieved IoU of 0.815 on Sen1Floods11 dataset, outperforming state-of-the-art segmentation models
- Dual-NIR pathway improved performance from IoU 0.483 (RGB only) to 0.815 (+68.5% relative improvement)
- Progressive attention mechanism provided +1.1% absolute IoU improvement over baseline without attention

## Why This Works (Mechanism)

### Mechanism 1: Progressive Self-Attention for Modality-Specific Feature Filtering
Applying self-attention masks progressively at each pooling scale amplifies relevant modality-specific features while suppressing noise, improving flood boundary discrimination. Intermediate features at scale i pass through 3×3 convolution → sigmoid activation → attention map (values [0,1]) → element-wise multiplication with original features. This gating allows the network to learn which features to retain. Relevant flood features within each modality can be identified independently before cross-modality fusion.

### Mechanism 2: Cross-Attention for Inter-Modal Dependency Modeling
Cross-attention between RGB+NIR and NIR-only modalities captures complementary information that naive concatenation misses, producing more accurate water/non-water boundaries. Self-attended features from one modality generate a cross-attention mask via conv+sigmoid that modulates the OTHER modality. Fused features are combined via element-wise addition: X̃R+N = X̃R ⊕ X̃N. Different spectral bands provide complementary signals; selective fusion outperforms direct concatenation.

### Mechanism 3: Dual NIR Utilization Pathway
Providing NIR both as part of RGB+NIR input AND as a standalone encoder input improves water delineation by allowing independent feature extraction at multiple spectral resolutions. Encoder 1 processes 4-channel input (R+G+B+NIR); Encoder 2 processes NIR only. Both streams undergo progressive attention fusion before decoding. NIR contains water-specific signals (moisture, boundaries) that benefit from a dedicated processing path rather than being merged early with RGB.

## Foundational Learning

- Concept: **Self-Attention Gating in CNNs**
  - Why needed here: Understanding how learnable masks modulate feature importance is critical for debugging attention collapse.
  - Quick check question: What happens to gradient flow if sigmoid outputs cluster near 0 or 1?

- Concept: **Multispectral Remote Sensing Basics**
  - Why needed here: NIR absorption by water vs. reflection by vegetation explains why this band aids flood delineation.
  - Quick check question: Why does standing water appear dark in NIR imagery compared to surrounding vegetation?

- Concept: **UNet Skip Connections**
  - Why needed here: ProCANet builds on UNet; understanding how skip connections preserve spatial detail at multiple scales is essential.
  - Quick check question: What spatial information would be lost without skip connections in an encoder-decoder?

## Architecture Onboarding

- Component map:
```
[RGB+NIR (4ch)] → Encoder1 ─┐
                             ├→ Progressive Cross-Attention Blocks (at each scale i) → Decoder → Flood Mask
[NIR (1ch)] → Encoder2 ─────┘
```

- Critical path:
  1. Verify dual encoder outputs at each scale before attention blocks.
  2. Confirm sigmoid outputs are in [0,1]; check for collapse to extremes.
  3. Ensure cross-attention uses Modality A to generate masks for Modality B (not self-modulation).
  4. Validate fusion is element-wise addition, not concatenation.

- Design tradeoffs:
  - Patch size 128×128: balances local detail vs. GPU memory (iteratively tuned).
  - Two encoders: doubles parameters but enables modality-specific feature extraction.
  - Attention at every pooling layer: higher compute cost for multi-scale refinement.

- Failure signatures:
  - Validation IoU < 0.5: check band ordering, normalization, or label quality.
  - Attention masks ~uniform (0.45–0.55): sigmoid not learning; inspect learning rate and loss.
  - Poor generalization to new sensor (e.g., PlanetScope): resolution mismatch or overfitting to Sen1Floods11.

- First 3 experiments:
  1. Establish baseline with RGB-only single encoder (Table IV, IoU 0.483).
  2. Ablate attention: dual encoder with direct addition, no attention (Table III, IoU 0.804).
  3. Validate dual NIR pathway: compare RGB+NIR (single encoder) vs RGB+NIR + NIR (dual encoder) (Table IV, IoU 0.804 vs 0.815).

## Open Questions the Paper Calls Out

- Question: How does the integration of additional data modalities, such as Synthetic Aperture Radar (SAR) or Digital Elevation Models (DEM), affect ProCANet's segmentation performance?
  - Basis in paper: The conclusion states that "utilizing other modalities... could be explored in future work," specifically noting the current reliance on multispectral imagery.
  - Why unresolved: The current study restricted inputs to optical RGB and NIR bands to test the cross-attention mechanism, leaving the integration of active sensing or topographic data unexplored.
  - Evidence: Comparative performance metrics (IoU, F1) from experiments adding SAR or DEM input encoders to the ProCANet architecture.

- Question: Can alternative fusion strategies outperform the element-wise addition used in the final cross-attended fusion stage?
  - Basis in paper: The conclusion suggests that "employing other fusion technique[s]" is a potential avenue for future research.
  - Why unresolved: The current architecture relies on a specific element-wise addition ($\oplus$) to combine cross-attended features, but it is unknown if concatenation or transformer-based attention blocks would yield better feature synthesis.
  - Evidence: Ablation studies comparing the current fusion method against alternative fusion layers (e.g., concatenation with convolutions) on the Sen1Floods11 dataset.

- Question: To what extent does domain shift versus label noise contribute to the performance drop observed in the Citarum River generalization test?
  - Basis in paper: The Citarum generalization test used "modified NDWI (pseudo ground truth)" rather than manual labels, and the reported IoU dropped significantly (from 0.815 to 0.659).
  - Why unresolved: It is unclear if the lower IoU results from the model failing to generalize to the 5m resolution PlanetScope imagery or from errors inherent in the NDWI-based pseudo-labels.
  - Evidence: Evaluation of the pre-trained model on a high-resolution dataset with rigorously validated, manually annotated ground truth labels.

## Limitations
- Ablation study limited to only two conditions (with vs. without attention), making it difficult to isolate individual contributions of self-attention versus cross-attention.
- Custom Citarum River basin dataset is mentioned but not characterized in terms of size, distribution, or annotation quality, limiting generalizability claims.
- Claims about superiority over state-of-the-art are based on comparison to only three baselines without broader benchmarking against recent attention-based segmentation methods.

## Confidence

**High Confidence**: The core architecture implementation (dual encoders, progressive attention blocks, element-wise addition fusion) is technically coherent and reproducible based on the detailed specification. The reported IoU of 0.815 on Sen1Floods11 is internally consistent with the methodology described.

**Medium Confidence**: The mechanism explanations for why attention improves flood segmentation are plausible but rely heavily on architectural intuition rather than extensive empirical validation. The dramatic improvement from dual-NIR processing (+68.5% relative IoU) is based on a single comparison without isolating the contribution of the second encoder pathway.

**Low Confidence**: Claims about generalizability to other geographic regions and sensor types are not well-supported given the limited evaluation on a single dataset (Sen1Floods11) and an uncharacterized custom dataset.

## Next Checks

1. **Ablation of Dual-NIR Pathway**: Train and evaluate three variants: (a) RGB-only single encoder (baseline), (b) RGB+NIR single encoder, and (c) RGB+NIR + NIR dual encoder to isolate the specific contribution of the second NIR pathway beyond simple multimodal fusion.

2. **Cross-Attention Isolation**: Modify the architecture to use only cross-attention (remove self-attention) and compare against the full progressive attention model to quantify the relative importance of inter-modal dependency modeling versus modality-specific feature filtering.

3. **Geographic Generalization Test**: Evaluate the trained ProCANet model on a held-out subset of Sen1Floods11 from different geographic regions (e.g., Asia vs. Europe) to assess whether the progressive attention mechanism generalizes across diverse flood characteristics and landscapes.