---
ver: rpa2
title: Learning to Reason Efficiently with Discounted Reinforcement Learning
arxiv_id: '2510.23486'
source_url: https://arxiv.org/abs/2510.23486
tags:
- policy
- reasoning
- arxiv
- blackwell
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of efficient reasoning in large
  language models, which often produce overly long chains of thought that increase
  computational cost and latency. The authors propose a principled approach using
  discounted reinforcement learning to encourage concise yet accurate reasoning.
---

# Learning to Reason Efficiently with Discounted Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.23486
- Source URL: https://arxiv.org/abs/2510.23486
- Reference count: 18
- Primary result: Discounted RL achieves similar accuracy with significantly shorter reasoning traces compared to undiscounted approaches

## Executive Summary
This work addresses the challenge of overly long reasoning chains in large language models, which increase computational cost without necessarily improving accuracy. The authors propose using discounted reinforcement learning to encourage both correctness and conciseness in reasoning responses. By applying a discount factor close to 1 to correctness rewards while leaving formatting rewards undiscounted, they create an objective that maximizes accuracy while minimizing expected response length.

The approach is validated across multiple mathematical reasoning benchmarks (GSM8K, MATH, AMC 2023, AIME 2025, MINERVA, OLYMPIAD) using various model sizes. The method consistently matches the accuracy of traditional undiscounted approaches while significantly reducing mean response length, demonstrating that shorter reasoning traces can be achieved without sacrificing performance.

## Method Summary
The authors propose a discounted reinforcement learning approach for training language models to reason efficiently. The key insight is to apply a discount factor γ to the environment reward (correctness) while leaving intrinsic rewards (like formatting) undiscounted. This creates an objective that simultaneously maximizes accuracy and minimizes expected response length. They use GRPO (Group Relative Policy Optimization) as the training algorithm and demonstrate the approach across multiple mathematical reasoning benchmarks with various model sizes.

## Key Results
- GSM8K: 91.07% accuracy with 170.08 tokens vs 91.06% accuracy with 217.60 tokens for undiscounted baseline
- Across all tested benchmarks, discounted RL consistently matches undiscounted accuracy while reducing response length
- Generalizes across different model sizes (Qwen2.5 7B/14B, Llama 3 8B, Phi-4)
- Blackwell optimal policies exist within restricted policy classes for deterministic MDPs with binary rewards

## Why This Works (Mechanism)
The mechanism works by creating a trade-off between accuracy and response length through discounted rewards. By applying a discount factor close to 1 to correctness rewards, the model learns to value shorter responses that still achieve high accuracy. The undiscounted intrinsic rewards ensure proper formatting and structural constraints are maintained. This approach leverages the theoretical foundation that Blackwell optimal policies can achieve both maximum accuracy and minimum expected response length in certain MDP structures.

## Foundational Learning
- **Reinforcement Learning**: Needed to understand how models learn through reward maximization. Quick check: Verify the agent updates its policy based on received rewards.
- **Discount Factors**: Essential for understanding how future rewards are weighted. Quick check: Confirm γ close to 1 means near-term rewards are prioritized but distant rewards still matter.
- **Policy Optimization**: Critical for grasping how model behavior is shaped. Quick check: Ensure the policy gradient updates are correctly implemented.
- **Markov Decision Processes**: Fundamental for modeling the reasoning process as sequential decision-making. Quick check: Verify state transitions capture the reasoning steps accurately.
- **GRPO Algorithm**: Specific optimization method used for training. Quick check: Confirm group relative advantages are computed correctly.

## Architecture Onboarding
- **Component Map**: Environment -> Reward Computation -> Policy Network -> Action Sampling -> Next State
- **Critical Path**: State encoding → Policy network → Action sampling → Reward calculation → Policy update
- **Design Tradeoffs**: Shorter responses vs. potential loss of reasoning depth; theoretical guarantees vs. practical applicability
- **Failure Signatures**: Accuracy degradation with aggressive discounting; inability to capture complex reasoning patterns; sensitivity to discount factor selection
- **First Experiments**: 1) Validate discount factor sensitivity on a small benchmark 2) Compare against length-based penalties 3) Test on deterministic vs. stochastic environments

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes deterministic MDPs with binary terminal rewards, which oversimplifies real reasoning scenarios
- Empirical results limited to Qwen2.5 model family, raising questions about generalizability
- Focus on response length reduction may compromise reasoning depth in domains requiring detailed explanations
- Discount factor tuning appears ad hoc rather than theoretically derived

## Confidence
- **High confidence**: Core empirical finding that discounted RL achieves similar accuracy with shorter responses across tested benchmarks
- **Medium confidence**: Theoretical framework's practical relevance given idealized assumptions
- **Low confidence**: Generalizability to other model families and real-world deployment scenarios

## Next Checks
1. Test the approach on non-Qwen2.5 model families (e.g., OpenAI models, Claude, or open-source alternatives like Mistral) to verify generalizability across different base architectures and training methodologies.

2. Evaluate performance on tasks where longer reasoning is known to be beneficial (e.g., complex mathematical proofs, multi-step scientific reasoning) to determine if the length reduction comes at the cost of reasoning depth in appropriate contexts.

3. Implement the method in a real-world deployment setting with delayed or sparse rewards (rather than immediate correctness feedback) to test robustness when ground truth labels are not readily available during training.