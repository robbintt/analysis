---
ver: rpa2
title: 'SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise
  Use'
arxiv_id: '2505.17332'
source_url: https://arxiv.org/abs/2505.17332
tags:
- your
- language
- words
- have
- swear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SweEval, a novel benchmark to evaluate how
  Large Language Models (LLMs) handle swearing and inappropriate language in real-world
  enterprise scenarios. The benchmark includes 2,725 prompts per language across eight
  languages, using multilingual and transliterated swear words in formal and informal
  contexts with both positive and negative tones.
---

# SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use

## Quick Facts
- **arXiv ID**: 2505.17332
- **Source URL**: https://arxiv.org/abs/2505.17332
- **Reference count**: 17
- **Primary result**: LLMs struggle with swear words in transliterated Indic languages, showing higher harmful rates compared to English across 13 models and 2,725 prompts per language

## Executive Summary
This paper introduces SweEval, a novel benchmark designed to evaluate how Large Language Models handle swearing and inappropriate language in real-world enterprise scenarios. The benchmark tests models across eight languages with 2,725 prompts each, incorporating multilingual and transliterated swear words in various formal and informal contexts with different emotional tones. The study reveals that current LLMs, despite their size (7B to 141B parameters), demonstrate significant weaknesses in recognizing and appropriately handling profanity, particularly in low-resource and transliterated Indic languages where harmful response rates are notably higher than for English.

## Method Summary
The authors developed SweEval by creating a comprehensive dataset of 2,725 prompts per language across eight languages, carefully designed to include swear words in both their native scripts and transliterated forms. The prompts were crafted to represent various contexts including formal business communications, casual conversations, and customer service interactions, with both positive and negative emotional tones. The benchmark was then used to evaluate 13 different LLM models ranging from 7B to 141B parameters, measuring their ability to either avoid or appropriately handle profanity while maintaining contextual understanding. The evaluation focused on measuring harmful response rates across different language pairs and script variations.

## Key Results
- LLMs show significantly higher harmful response rates when processing swear words in transliterated Indic languages compared to native scripts
- Models consistently fail to recognize cultural and contextual nuances in multilingual settings, leading to inappropriate responses
- There is a clear performance gap between English and low-resource language handling of profanity, with English showing substantially better safety alignment
- Current safety alignment mechanisms struggle with the complexity of multilingual profanity detection and contextual understanding

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive approach to testing LLM safety across multiple dimensions. By incorporating both native script and transliterated swear words, the benchmark exposes models' weaknesses in cross-script comprehension. The varied contexts (formal, informal, business, casual) reveal how models handle profanity differently based on situational cues. The multilingual design specifically highlights disparities in safety alignment between high-resource languages like English and low-resource languages, exposing fundamental gaps in current LLM training approaches for multilingual safety.

## Foundational Learning
**Multilingual Profanity Detection**: Understanding how swear words function across different languages and scripts - needed to create comprehensive safety benchmarks; quick check: compare native vs transliterated profanity recognition rates
**Cultural Context Processing**: How models interpret context and tone in different cultural settings - needed to evaluate contextual understanding; quick check: analyze model responses across formal vs informal scenarios
**Transliteration Impact**: The effect of converting words between scripts on model comprehension - needed to assess cross-script understanding; quick check: measure performance gap between native and transliterated inputs
**Safety Alignment Mechanisms**: How models are trained to avoid or handle inappropriate content - needed to identify alignment weaknesses; quick check: compare harmful response rates across model sizes
**Low-Resource Language Processing**: Challenges in handling languages with limited training data - needed to understand performance disparities; quick check: correlate training data availability with safety performance

## Architecture Onboarding
**Component Map**: Prompt Generator -> Model Inference Engine -> Safety Classifier -> Response Evaluator -> Benchmark Score Calculator
**Critical Path**: The benchmark workflow follows: prompt generation → model response generation → safety classification → scoring → analysis
**Design Tradeoffs**: Balance between comprehensive profanity coverage and practical evaluation feasibility; between strict safety alignment and natural language generation
**Failure Signatures**: Higher harmful rates in transliterated scripts, inability to recognize cultural context, inconsistent handling across languages, failure to maintain safety in informal contexts
**Three First Experiments**:
1. Test model performance on native script vs transliterated swear words in the same language
2. Compare harmful response rates across different emotional tones (positive vs negative) using identical profanity
3. Evaluate cross-lingual transfer by testing if model safety alignment in one language affects performance in another

## Open Questions the Paper Calls Out
- How can safety alignment be improved for low-resource languages without compromising performance in high-resource languages?
- What are the optimal training strategies for multilingual profanity detection that account for cultural and contextual variations?
- How do different model architectures (7B vs 141B parameters) impact multilingual safety performance, and what architectural changes could help?
- Can transfer learning from high-resource to low-resource languages effectively improve safety alignment in underrepresented languages?

## Limitations
- Benchmark focus on explicit profanity may miss broader safety concerns and implicitly harmful content
- Predefined profanity categories may not capture emerging slang or culturally nuanced harmful expressions
- Limited coverage of complex enterprise safety scenarios beyond simple profanity detection
- Potential bias toward more common swear words while missing context-specific harmful expressions
- Assumption: The benchmark may not fully capture enterprise-specific safety requirements beyond general profanity detection

## Confidence
- **High Confidence**: LLMs struggle with transliterated Indic languages and show higher harmful rates compared to English
- **Medium Confidence**: Models fail to recognize cultural and contextual nuances in multilingual settings
- **Medium Confidence**: Need for better data curation and training to improve safety alignment across languages
- Unknown: Long-term effectiveness of current safety alignment approaches across diverse enterprise use cases

## Next Checks
1. Conduct cross-cultural validation studies with domain experts from each supported language to verify benchmark coverage beyond explicit profanity
2. Perform longitudinal studies to assess how well models retain multilingual safety alignment over time and with fine-tuning on enterprise data
3. Expand benchmark to include implicit harmful content detection scenarios and measure performance on nuanced safety challenges in enterprise contexts
4. Test whether safety alignment improvements in high-resource languages transfer effectively to low-resource languages through transfer learning approaches