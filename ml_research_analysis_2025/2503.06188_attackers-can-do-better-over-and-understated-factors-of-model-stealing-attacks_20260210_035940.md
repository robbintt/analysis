---
ver: rpa2
title: 'Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks'
arxiv_id: '2503.06188'
source_url: https://arxiv.org/abs/2503.06188
tags:
- data
- target
- substitute
- attack
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that target model performance is the key limiting
  factor for stealing success, not target architecture complexity. Attacks succeed
  better on high-performing targets because substitutes learn correct predictions
  far more easily than mistakes.
---

# Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks

## Quick Facts
- arXiv ID: 2503.06188
- Source URL: https://arxiv.org/abs/2503.06188
- Reference count: 40
- Primary result: Target model performance, not architecture complexity, is the key limiting factor for model stealing attack success.

## Executive Summary
This paper challenges conventional assumptions about model stealing attacks by demonstrating that substitute model fidelity is primarily bottlenecked by the target model's own accuracy, not its architectural complexity. Through extensive experiments on CIFAR-10 image classification, the authors show that attackers can achieve high-fidelity model theft with less effort than previously assumed. The work identifies key factors including data complexity matching, transfer learning alignment, and active learning optimization that significantly impact attack success. These findings suggest that attackers can achieve effective model stealing with query budgets smaller than the target's training set size, and that current proactive defenses are insufficient against label-only attacks.

## Method Summary
The study performs an ablation analysis of model stealing attacks using CIFAR-10 dataset with three target models (SimpleNet, ResNet-34 from scratch, ResNet-34 pretrained) and substitutes (SimpleNet, ResNet-18, ResNet-34). Attackers use three data sources: original CIFAR-10 subsets, problem-domain CINIC-10, and artificially generated images via Stable Diffusion 2.1. The paper tests query budgets from 1k to 45k samples using random sampling, active learning (modified DFAL + k-center), and adversarial augmentation (DeepFool). The methodology systematically evaluates how target performance, architecture complexity, transfer learning strategy, and data quality affect attack fidelity and efficiency.

## Key Results
- Target model accuracy is the primary bottleneck for substitute fidelityâ€”high-performing targets are easier to steal because substitutes learn correct predictions far more easily than mistakes
- Substitute architecture complexity should match data complexity rather than target model complexity
- Transfer learning strategy alignment matters: substitutes trained from scratch learn better from scratch-trained targets
- Data-free attacks are feasible with query budgets smaller than the target training set, and active learning can significantly boost their efficiency
- Attackers with more complex data underestimate performance, while simpler data leads to overestimation

## Why This Works (Mechanism)

### Mechanism 1: High-fidelity theft is easier from accurate models due to correct prediction replication.
- **Claim:** Attack fidelity is bottlenecked by the target model's own accuracy because substitutes learn correct predictions far more easily than mistakes.
- **Mechanism:** A substitute model is trained to approximate the target's input-output mapping. This mapping is cleaner and more consistent for samples the target classifies correctly. Conversely, the target's errors are likely noisy, inconsistent, or based on spurious correlations, making them significantly harder for the substitute to learn and replicate.
- **Core assumption:** The decision boundaries learned by a high-performing model are more structured and thus more learnable for a substitute than the inconsistent behavior associated with a model's errors.
- **Evidence anchors:**
  - [Abstract] "Attacks succeed better on high-performing targets because substitutes learn correct predictions far more easily than mistakes."
  - [Section IV.B] "The results presented in Table IV clearly demonstrate that substitute models learn correct predictions significantly better than incorrect predictions. Therefore, if the target model makes fewer mistakes, its behaviour is easier to copy..."
  - [Corpus] This specific mechanism regarding target accuracy as a limiting factor is not directly addressed in the provided corpus neighbor abstracts.

### Mechanism 2: Substitute architecture complexity should match data complexity, not target complexity.
- **Claim:** The optimal complexity for a substitute model is determined by the complexity of the data used for training the substitute, not by the architecture complexity of the target model.
- **Mechanism:** The substitute model must extract the relevant features and decision boundaries from its training dataset (attacker's data) to mimic the target. A more complex dataset (e.g., more samples, higher variance) requires a model with more capacity (parameters, layers) to capture its nuances. The target's size is irrelevant if the substitute is working from a different, potentially smaller or less complex, data distribution.
- **Core assumption:** The "task complexity" can be adequately approximated by the properties of the attacker's available dataset (quantity and quality).
- **Evidence anchors:**
  - [Abstract] "Architecture choice should match data complexity rather than target model complexity."
  - [Section IV.C] "The complexity of the substitute architecture should correspond to the complexity of the attacker's data rather than be compared to the complexity of the target model."
  - [Corpus] This insight is consistent with the general model stealing literature, though the specific finding that data complexity overrides target complexity is a unique contribution of this paper not highlighted in the corpus.

### Mechanism 3: Transfer learning strategy alignment improves attack efficiency.
- **Claim:** Using a similar training strategy (from scratch vs. transfer learning) for the substitute as was used for the target improves the fidelity of the stolen model.
- **Mechanism:** Models trained from scratch and models initialized with pre-trained weights (transfer learning) likely converge to different internal representations and decision boundaries for the same task. A substitute trained from scratch is better positioned to replicate the representations of a target trained from scratch, as they learn features directly from the provided data.
- **Core assumption:** The internal representations (feature spaces) of models trained with similar strategies are more similar, and this similarity is crucial for high-fidelity replication when only input-output labels are available.
- **Evidence anchors:**
  - [Abstract] "Transfer learning impacts attacks: substitutes trained from scratch learn better from scratch-trained targets."
  - [Section IV.D] "Training target models from scratch does not make them less prone to be stolen. In fact, if a substitute model is trained from scratch, it reaches higher scores when attacking the target model trained from scratch..."
  - [Corpus] This specific finding on transfer learning alignment is not mentioned in the provided corpus neighbor abstracts.

## Foundational Learning

- **Concept:** **Model Stealing (Extraction) Attacks**
  - **Why needed here:** This is the core subject of the paper. Understanding the black-box scenario (querying an API to get labels) and the goal (creating a functional substitute) is essential for interpreting all results.
  - **Quick check question:** In a black-box model stealing attack, what information does the attacker typically not have access to?

- **Concept:** **Transfer Learning**
  - **Why needed here:** The paper explicitly analyzes how the use of transfer learning for both the target and substitute models affects attack success. Distinguishing between "from scratch" and "transfer learning" training is critical for following the discussion.
  - **Quick check question:** What is the primary difference between training a model "from scratch" versus training it with "transfer learning"?

- **Concept:** **Fidelity vs. Accuracy**
  - **Why needed here:** The paper uses these as key metrics. Fidelity measures how well the substitute mimics the target, while accuracy measures performance on the ground truth. The central finding that target accuracy is a bottleneck for fidelity depends on understanding this distinction.
  - **Quick check question:** If a target model misclassifies a cat as a dog, and a substitute model also classifies that image as a dog, what does this contribute to the substitute's *fidelity* score?

## Architecture Onboarding

- **Component map:**
  Target Model -> Query Engine -> Attacker's Data Source -> Substitute Model Training Pipeline

- **Critical path:**
  1. **Data Acquisition:** Select a dataset. Focus on data complexity over volume if possible.
  2. **Query Optimization (Optional):** If query budget is limited, use strategies like active learning to select the most informative samples.
  3. **Model Selection:** Choose a substitute architecture whose complexity matches the complexity of your data. Initialize it in a way that aligns with the suspected training strategy of the target.
  4. **Training:** Train the substitute on the target-labeled data.

- **Design tradeoffs:**
  - **Data Complexity vs. Query Budget:** More complex data may yield better substitutes but requires a model with sufficient capacity to learn it.
  - **Query Budget vs. Optimization Overhead:** Active learning improves efficiency but adds computational overhead to select queries. It's most beneficial for mid-range budgets.

- **Failure signatures:**
  - **Low Fidelity on a High-Accuracy Target:** This contradicts the paper's primary finding. Failure mode: The substitute architecture is too simple for the data.
  - **High Performance on Validation Set, Low Performance on Test Set:** Failure mode: The attacker's data is simpler than the real-world data distribution, leading to overestimated performance.

- **First 3 experiments:**
  1. Establish a baseline by training a substitute model on a subset of the original data to confirm the target accuracy bottleneck.
  2. Test data complexity by training substitutes of varying complexity on a dataset more complex than the target's training data to see if a more complex architecture is now required.
  3. Evaluate transfer learning alignment by comparing the performance of "from scratch" vs. pre-trained substitutes against a "from scratch" trained target.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the correlation between attacker's data complexity and performance over-/under-estimation apply to Non-Problem Domain (NPD) attacks?
- Basis in paper: [explicit] "However, as we only consider data with the same context as the original data, this statement needs further investigation for non-problem domain attacks." (Section IV-E)
- Why unresolved: The paper limited its analysis of data complexity effects to original and problem-domain data, leaving the behavior of NPD data unconfirmed.
- What evidence would resolve it: Experimental results repeating the complexity analysis using datasets distinct from the target model's problem domain.

### Open Question 2
- Question: How can generative models be optimized to produce artificial data with sufficient complexity to improve data-free attack fidelity?
- Basis in paper: [explicit] "As optimising artificial data quality was not the prime goal of this work, we expect to reach higher scores with further research." (Section V)
- Why unresolved: The authors utilized a standard Stable Diffusion approach to generate artificial data, noting it was often simpler than the original data, leading to performance overestimation on validation sets.
- What evidence would resolve it: Development of specialized data generation strategies that maximize data complexity to better match or exceed the target model's training data distribution.

### Open Question 3
- Question: Can effective proactive defenses be developed against label-only model stealing attacks without degrading the target model's utility?
- Basis in paper: [explicit] "None of the current proactive defenses can defend against attacks on image classifiers that use only labels." (Section VI-B)
- Why unresolved: The paper demonstrates that existing defenses like input perturbation or output rounding either fail to stop the attack or significantly harm the model's accuracy on legitimate queries.
- What evidence would resolve it: A new defense mechanism that successfully reduces the fidelity of the attacks described in the paper while maintaining the target model's classification accuracy within an acceptable margin.

## Limitations

- The study uses only three architectures (SimpleNet, ResNet-18, ResNet-34) and three data sources, which may not generalize to all scenarios.
- The modified DFAL algorithm description is incomplete, making exact replication difficult.
- The Stable Diffusion approach relies on specific prompt engineering and class distributions that may not transfer to other domains or tasks beyond image classification.

## Confidence

- **High confidence**: Target model accuracy as the primary bottleneck for attack fidelity, demonstrated across multiple experiments and metrics (fidelity, joint accuracy).
- **Medium confidence**: Architecture complexity matching data complexity rather than target complexity, though limited to specific model families and datasets.
- **Medium confidence**: Transfer learning alignment benefits, based on controlled experiments but with only two training strategies tested.
- **Medium confidence**: Active learning efficiency gains, though optimal query budget ranges are not precisely defined.

## Next Checks

1. **Architecture-capacity mapping**: Systematically vary substitute model depth/width across multiple datasets to establish quantitative relationships between data complexity metrics and required model capacity.

2. **AL algorithm transparency**: Implement and test the exact active learning strategy (DFAL + k-center modification) described in the paper to verify claimed efficiency gains and investigate the 1k query failure mode.

3. **Cross-domain data-free attacks**: Apply the Stable Diffusion approach to non-image domains (text, tabular data) to test generalizability of data-free attack methodology.