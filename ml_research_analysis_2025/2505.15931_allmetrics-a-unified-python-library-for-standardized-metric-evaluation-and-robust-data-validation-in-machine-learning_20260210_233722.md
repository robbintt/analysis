---
ver: rpa2
title: 'AllMetrics: A Unified Python Library for Standardized Metric Evaluation and
  Robust Data Validation in Machine Learning'
arxiv_id: '2505.15931'
source_url: https://arxiv.org/abs/2505.15931
tags:
- allmetrics
- metric
- evaluation
- tasks
- library
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AllMetrics is a unified Python library that addresses the fragmentation
  and inconsistency of machine learning metric evaluation by standardizing implementations
  across regression, classification, clustering, segmentation, and image-to-image
  tasks. The library combines a modular API with robust data validation protocols
  to ensure reproducibility and reliability in model evaluation.
---

# AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning

## Quick Facts
- arXiv ID: 2505.15931
- Source URL: https://arxiv.org/abs/2505.15931
- Reference count: 12
- Key outcome: Unified Python library that standardizes metric evaluation across ML tasks with robust data validation protocols.

## Executive Summary
AllMetrics addresses the fragmentation and inconsistency in machine learning metric evaluation by providing standardized implementations across regression, classification, clustering, segmentation, and image-to-image tasks. The library combines a modular API with robust data validation protocols to ensure reproducibility and reliability in model evaluation. By resolving implementation differences (ID) and reporting differences (RD), AllMetrics enables consistent metric computation and configurable output aggregation. Extensive benchmarking against major libraries shows strong alignment in results, with accuracy up to seven decimal places in regression tasks and near-identical values in segmentation and classification tasks.

## Method Summary
The library implements consistent algorithmic approaches for each metric across five ML task domains, replacing fragmented library-specific conventions with single verified implementations. It uses task-specific validator classes that check for edge cases like empty masks, class imbalance, and dimensional mismatches before computation. The benchmarking approach compares AllMetrics against Scikit-Learn, PyTorch, TensorFlow, and domain-specific libraries using default parameters on public datasets including Weather, Heart Disease, Wine-Quality, and lung CT tumor segmentations. Models include decision trees, linear regression, k-means, 3D Attention U-Net, and Pix2Pix, evaluated on Windows 11 with Intel i7-11370H and 16GB RAM.

## Key Results
- Benchmarking shows accuracy up to seven decimal places in regression tasks compared to Scikit-Learn
- Segmentation metrics (Dice, IoU, Hausdorff) align nearly identically with MONAI and MedPy implementations
- Classification metrics match PyCM outputs with class-wise precision/recall/F1 values
- Validation framework successfully detects edge cases including empty masks and severe class imbalances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standardized metric implementations reduce cross-library evaluation discrepancies by unifying computational methodologies across ML tasks.
- Mechanism: The library implements consistent algorithmic approaches for each metric (e.g., Euclidean distance for Hausdorff Distance, configurable window sizes for SSIM), replacing fragmented library-specific conventions with a single verified implementation.
- Core assumption: That implementation differences (ID), not user error or data quality, are the primary source of metric evaluation inconsistency across frameworks.
- Evidence anchors:
  - [abstract] "By resolving implementation differences (ID) and reporting differences (RD), AllMetrics enables consistent metric computation"
  - [section 3.2.1] "AllMetrics successfully resolves these inconsistencies through standardized implementations that reconcile ID"
  - [corpus] Weak direct evidence; related library papers (GAICo, torchmil) similarly identify standardization gaps but do not validate AllMetrics specifically.

### Mechanism 2
- Claim: Configurable aggregation parameters prevent misinterpretation by making reporting explicit and user-controlled.
- Mechanism: For multi-class tasks, AllMetrics defaults to class-wise output but exposes `average` parameter options (macro, micro, weighted, None), allowing users to explicitly select aggregation strategies rather than receiving implicit library-specific defaults.
- Core assumption: That users will correctly select and document their chosen aggregation strategy when comparing results.
- Evidence anchors:
  - [abstract] "The library implements class-specific reporting for multi-class tasks through configurable parameters to cover all use cases"
  - [section 4] "AllMetrics introduces an explicit average parameter that provides controlled aggregation options"
  - [corpus] No corpus papers validate this specific parameterization approach.

### Mechanism 3
- Claim: Proactive data validation prevents silent metric computation errors from invalid or degenerate inputs.
- Mechanism: Task-specific validator classes check for empty masks, class imbalance, missing values, and dimensional mismatches before computation, raising warnings or errors rather than returning undefined/incorrect values.
- Core assumption: That catching edge cases before computation materially improves downstream decision-making and that users will act on warnings.
- Evidence anchors:
  - [abstract] "The library's validation framework proactively detects input anomalies, ensuring trustworthy evaluations"
  - [section 3.2.1] "For segmentation tasks, it successfully detects and warns about empty masks... for classification tasks, it flags severe class imbalances"
  - [corpus] No corpus papers evaluate validation frameworks for metric libraries.

## Foundational Learning

- Concept: Implementation Differences (ID) vs. Reporting Differences (RD)
  - Why needed here: The paper's central thesis is that these two distinct problem types cause evaluation inconsistency; understanding the distinction is prerequisite to using the library correctly.
  - Quick check question: If two libraries return different precision values for identical data, is this necessarily an ID issue?

- Concept: Metric Aggregation Strategies (micro/macro/weighted/class-wise)
  - Why needed here: AllMetrics exposes these as configurable parameters; users must understand when each is appropriate for their task.
  - Quick check question: For an imbalanced 3-class problem where class 2 has 5% of samples, which averaging strategy would most heavily weight class 2's performance?

- Concept: Edge Case Handling in Metric Computation
  - Why needed here: The validation framework's value proposition depends on recognizing which input conditions produce degenerate or undefined metric values (e.g., R² with zero variance, Hausdorff Distance with empty masks).
  - Quick check question: What should IoU return when the ground truth mask is entirely zero?

## Architecture Onboarding

- Component map:
  - Task modules: `regression`, `classification`, `clustering`, `segmentation`, `image_to_image` — each implements metrics as independent functions
  - Validator classes: Task-specific (e.g., `RegressionValidator`, `SegmentationValidator`) with shared core validation functions
  - Discovery interface: `list_of_metrics()` and `get_metric_details()` for programmatic documentation access
  - Unified validation entry point: `validate_all()` method executing all relevant checks

- Critical path:
  1. Install via `pip install allmetrics`
  2. Import task-specific module (e.g., `from allmetrics.segmentation import iou_score`)
  3. Prepare ground truth and prediction arrays
  4. (Optional but recommended) Run `validate_all()` on inputs
  5. Call metric function with desired parameters
  6. Interpret class-wise or aggregated results per configuration

- Design tradeoffs:
  - Per-class default output improves granularity but requires explicit aggregation for single-number summaries
  - Validation adds computational overhead; configurable check activation allows throttling for large datasets
  - User-adjustable parameters (e.g., SSIM `window_size`) increase flexibility but introduce additional configuration surface for errors

- Failure signatures:
  - Empty mask warnings during segmentation: check for preprocessing errors that zeroed ground truth regions
  - Class imbalance flags in classification: consider rebalancing or using weighted averaging
  - Dimension mismatch errors: verify that prediction and ground truth shapes align (including channel dimensions)
  - Divergent results vs. other libraries: confirm matching parameters (e.g., SSIM window size, R² adjusted vs. standard)

- First 3 experiments:
  1. Reproduce a binary classification result from the paper (Heart Disease dataset), comparing AllMetrics class-wise output against Scikit-Learn macro-average to internalize RD
  2. Run segmentation metrics on a synthetic mask with an empty region to observe validation behavior and warning messages
  3. Compute SSIM on a sample image pair using different `window_size` values to quantify sensitivity to this parameter

## Open Questions the Paper Calls Out
None

## Limitations
- Evidence anchors are mostly from the paper itself rather than independent validation
- Benchmarking limited to default parameter comparisons, missing some real-world usage patterns
- No systematic analysis of where implementation differences actually arise across libraries
- Long-term maintenance and adoption depends on broader community engagement

## Confidence
- High confidence in the library's basic functionality and API design
- Medium confidence in its claims about resolving metric inconsistency
- Medium confidence in the validation framework's effectiveness without independent assessment
- Low confidence in long-term maintenance without broader community engagement

## Next Checks
1. Run classification metrics on a multi-class imbalanced dataset comparing AllMetrics class-wise output against multiple libraries with documented aggregation strategies
2. Test segmentation validation on edge cases (empty masks, single-pixel masks) to verify warning messages and error handling
3. Compute SSIM on a sample image pair using different window sizes to quantify parameter sensitivity and compare against PyTorch/TensorFlow baselines