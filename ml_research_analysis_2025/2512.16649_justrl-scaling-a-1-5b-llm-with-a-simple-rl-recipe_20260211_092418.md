---
ver: rpa2
title: 'JustRL: Scaling a 1.5B LLM with a Simple RL Recipe'
arxiv_id: '2512.16649'
source_url: https://arxiv.org/abs/2512.16649
tags:
- training
- simple
- performance
- recipe
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses whether complex RL training techniques are\
  \ necessary for small 1.5B reasoning models by proposing JustRL, a minimal single-stage\
  \ RL approach with fixed hyperparameters. The method achieves state-of-the-art performance\
  \ on two 1.5B models, reaching 54.9% and 64.3% average accuracy across nine mathematical\
  \ benchmarks while using 2\xD7 less compute than sophisticated approaches."
---

# JustRL: Scaling a 1.5B LLM with a Simple RL Recipe

## Quick Facts
- arXiv ID: 2512.16649
- Source URL: https://arxiv.org/abs/2512.16649
- Reference count: 5
- Primary result: 54.9% and 64.3% average accuracy on 9 math benchmarks with 1.5B models using 2× less compute than complex approaches

## Executive Summary
JustRL demonstrates that a minimal, single-stage RL approach with fixed hyperparameters can achieve state-of-the-art performance on 1.5B reasoning models without the complex interventions typically required in RL training. The method achieves 54.9% and 64.3% average accuracy across nine mathematical benchmarks while using half the compute of sophisticated approaches. Critically, the training exhibits stable, monotonic improvement over 4,000+ steps without the collapses or plateaus that motivate complex techniques. The study reveals that "standard tricks" like explicit length penalties and robust verifiers may actually degrade performance by collapsing exploration, suggesting that complexity may sometimes address symptoms rather than fundamental RL challenges.

## Method Summary
JustRL employs a single-stage GRPO approach using the veRL framework with binary rewards from a DAPO rule-based verifier. The method uses fixed hyperparameters including batch_size=256, lr=1e-6, rollout_n=8, and asymmetric clipping [0.8,1.28] without KL loss or entropy regularization. Training runs for 4,000+ steps on DAPO-Math-17k dataset with models initialized from DeepSeek-R1-Distill-Qwen-1.5B or OpenMath-Nemotron-1.5B. The approach achieves stable training with healthy entropy (1.0-1.6) and monotonic reward improvement, contrasting with the instability that motivates complex multi-stage training and dynamic scheduling in other work.

## Key Results
- Achieves 54.9% average accuracy on DeepSeek-R1-Distill-Qwen-1.5B across nine benchmarks
- Achieves 64.3% average accuracy on OpenMath-Nemotron-1.5B across nine benchmarks
- Uses 2× less compute than sophisticated RL approaches while maintaining state-of-the-art performance
- Training remains stable for 4,000+ steps with monotonic improvement and no collapses

## Why This Works (Mechanism)
The success of JustRL appears to stem from avoiding the exploration collapse that occurs with standard RLHF techniques. By maintaining higher entropy (1.0-1.6) and avoiding explicit length penalties and robust verifiers, the method preserves learning signal nuance and prevents the model from getting stuck in local optima. The simple, fixed hyperparameter configuration provides stability without the need for complex interventions that may introduce conflicting optimization pressures.

## Foundational Learning
- **GRPO (Group Relative Policy Optimization)**: Policy gradient method that uses group-based advantage estimation; needed for stable RL training without value networks; quick check: monitor reward curve for monotonic improvement
- **veRL framework**: Implementation framework for RL training; needed to provide standardized RL components and training loop; quick check: verify framework installation and basic functionality
- **Rule-based verifier**: Automated reward system that extracts and checks answers in \boxed{} format; needed for scalable binary rewards without human annotation; quick check: test verifier on sample responses to ensure correct extraction
- **Asymmetric clipping**: Gradient clipping with different ranges for positive and negative updates; needed to stabilize training while preserving learning signals; quick check: monitor gradient norms and training stability
- **Entropy regularization**: Technique to maintain exploration by penalizing low entropy; needed to prevent premature convergence; quick check: track entropy values throughout training (should stay 1.0-1.6)

## Architecture Onboarding

**Component Map**: DAPO-Math-17k dataset -> veRL framework -> GRPO training -> DAPO rule-based verifier -> binary rewards -> model weights update

**Critical Path**: Data loading and preprocessing -> GRPO rollout generation -> Reward computation via verifier -> Policy gradient update -> Model checkpointing

**Design Tradeoffs**: Simplicity vs. performance ceiling - simple single-stage approach trades potential peak performance for stability and reduced compute; Fixed hyperparameters vs. adaptive tuning - avoids complexity but may miss optimal configurations for different settings

**Failure Signatures**: Entropy collapse to 0.5-0.6 (vs. healthy 1.0-1.6) indicates exploration failure; Reward plateaus at 50% (vs. 55% baseline) suggest learning signal degradation; Response length instability indicates training instability

**First Experiments**:
1. Train for 100 steps and verify entropy remains in healthy range (1.0-1.6)
2. Test DAPO verifier on 10 sample responses to confirm correct \boxed{} extraction
3. Run single batch with asymmetric clipping to verify implementation

## Open Questions the Paper Calls Out
- Which specific components of JustRL's minimal recipe are critical to its success—the hyperparameters, the training dataset (DAPO-Math-17k), the simple rule-based verifier, or their interaction? The paper demonstrates that simple RL works well but cannot isolate why, as no component-wise ablations were performed.
- Does the JustRL approach generalize to other domains (coding, general QA), larger model scales, or noisier reward signals? Experiments are limited to 1.5B models on mathematical reasoning with verifiable rewards, leaving generalization uncertain.
- Under what specific conditions (compute constraints, reward noise, performance ceilings) do complex interventions like multi-stage training or dynamic scheduling provide genuine benefits over simple baselines? The paper shows complexity is unnecessary in one setting but does not map the boundary conditions where it becomes necessary.
- Why do "standard tricks" like explicit length penalties and robust verifiers degrade performance by collapsing exploration in JustRL's setting, when they reportedly help in other work? The paper hypothesizes about conflicting pressures but does not test the underlying mechanisms.

## Limitations
- Limited to 1.5B parameter models, leaving uncertainty about scalability to larger architectures
- Results confined to mathematical reasoning tasks with verifiable rewards, limiting domain generality claims
- Component-wise ablations were not performed, preventing isolation of which recipe elements are truly critical
- Asymmetric clipping implementation details remain unspecified, potentially affecting reproducibility

## Confidence
High confidence in the core finding that simple RL achieves state-of-the-art performance on 1.5B models. Medium confidence in generalizability to larger scales and other domains. Low confidence in understanding which specific recipe components are essential versus incidental.

## Next Checks
1. Scale the JustRL approach to 7B and 13B models to test whether the simple recipe maintains stability and performance advantages at larger scales
2. Systematically test the ablation findings by adding individual "standard tricks" (length penalties, KL loss, entropy regularization) separately rather than in combinations to isolate specific failure modes
3. Compare JustRL against recent complex approaches on additional reasoning tasks beyond mathematics to evaluate domain generality