---
ver: rpa2
title: Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese
  Speech Annotation
arxiv_id: '2506.07646'
source_url: https://arxiv.org/abs/2506.07646
tags:
- labels
- speech
- annotation
- accent
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically annotating
  phonemic and prosodic labels for Japanese speech, which is essential for building
  high-quality text-to-speech (TTS) datasets but difficult due to polyphones and context-dependent
  pitch accents. The proposed method fine-tunes a pre-trained ASR model (Whisper)
  to simultaneously generate phrase-level graphemes and annotation labels from audio-transcript
  pairs.
---

# Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation

## Quick Facts
- **arXiv ID:** 2506.07646
- **Source URL:** https://arxiv.org/abs/2506.07646
- **Authors:** Rui Hu; Xiaolong Lin; Jiawang Liu; Shixi Huang; Zhenpeng Zhan
- **Reference count:** 0
- **Primary result:** Fine-tuned Whisper with transcript prompts and dictionary correction achieves 0.57% CER for phonemic labeling and 87.32% accent phrase boundary accuracy in Japanese TTS annotation.

## Executive Summary
This paper addresses the challenge of automatically annotating phonemic and prosodic labels for Japanese speech, essential for building high-quality text-to-speech (TTS) datasets but difficult due to polyphones and context-dependent pitch accents. The proposed method fine-tunes a pre-trained ASR model (Whisper) to simultaneously generate phrase-level graphemes and annotation labels from audio-transcript pairs. To improve phonemic labeling, a decoding strategy using dictionary prior knowledge is employed to correct Kanji pronunciations based on the predicted graphemes. Objective evaluations show that the method achieves 0.57% CER for phonemic labeling and 87.32% accuracy for accent phrase boundary prediction, outperforming text-only and audio-only baselines. Subjective MOS tests indicate that TTS models trained with the generated labels achieve naturalness scores comparable to those trained with manual annotations. The approach enables effective construction of Japanese TTS datasets without expensive manual labeling.

## Method Summary
The method fine-tunes Whisper-large-v3-turbo to generate phrase-level graphemes and Japanese TTS labels from audio-transcript pairs. During fine-tuning, the decoder is prompted with the ground truth transcript tokenized between special tokens (`<startofprev>` and `<startoftranscript>`), allowing cross-attention to both acoustic representations and semantic information. The target output combines graphemes and TTS labels (phonemic Katakana with prosodic markers) separated by a delimiter. For inference, dictionary-enhanced decoding corrects Kanji pronunciations by selecting the dictionary candidate with minimum edit distance to the predicted pronunciation, adjusting accent types based on mora count changes.

## Key Results
- Achieves 0.57% CER for phonemic labeling and 87.32% accent phrase boundary accuracy on private dataset
- Outperforms text-only and audio-only baselines significantly in objective metrics
- TTS models trained with generated labels achieve MOS scores comparable to those trained with manual annotations
- Dictionary correction reduces phonemic labeling errors from 0.80% to 0.57% CER

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning Whisper on ground truth transcripts during fine-tuning reduces phonetic confusion errors compared to audio-only conditioning.
- Mechanism: The transcript is tokenized and placed between `<startofprev>` and `<startoftranscript>` tokens as a decoder prompt, allowing cross-attention to both acoustic representations and semantic information from the transcript when generating labels.
- Core assumption: The transcript provides reliable semantic grounding that constrains pronunciation disambiguation for polyphonic Kanji characters.
- Evidence anchors:
  - [abstract] "Our approach involves fine-tuning a large-scale pre-trained ASR model, conditioned on ground truth transcripts, to simultaneously output phrase-level graphemes and annotation labels."
  - [section 2.2] "The decoder can attend to both the acoustic information from input speech and the semantic information of the transcript when generating labels."
  - [corpus] Related work on grapheme-coherent annotation (arXiv:2506.04527) confirms grapheme conditioning improves phonemic label coherence, supporting this mechanism.
- Break condition: If transcripts contain errors or are misaligned with audio, the prompt may introduce noise rather than signal.

### Mechanism 2
- Claim: Dictionary-enhanced decoding corrects residual phonemic labeling errors for Kanji characters.
- Mechanism: After decoding each accent phrase (marked by "#"), predicted graphemes are segmented via MeCab/UniDic, candidate pronunciations are retrieved, and the candidate with minimum edit distance to the model's predicted pronunciation is selected. Accent type is then adjusted based on mora count changes.
- Core assumption: The grapheme sequence predicted by the model is sufficiently accurate (CER 0.16% reported) for dictionary lookup to find valid pronunciation candidates.
- Evidence anchors:
  - [abstract] "To further correct errors in phonemic labeling, we employ a decoding strategy that utilizes dictionary prior knowledge."
  - [section 2.3] "We select the one with the shortest edit distance to the pronunciation predicted by the annotation model as the corrected pronunciation."
  - [corpus] Weak direct corpus evidence for dictionary-based correction in ASR-to-TTS pipelines; mechanism is primarily validated by this paper's results.
- Break condition: If grapheme predictions diverge significantly from valid dictionary entries (e.g., out-of-vocabulary proper nouns), correction may fail or introduce errors.

### Mechanism 3
- Claim: Joint output of phrase-level graphemes and TTS labels improves annotation quality compared to outputting TTS labels alone.
- Mechanism: Requiring the model to sequentially produce graphemes followed by TTS labels (delimited by "|") forces explicit content understanding before annotation, reducing tendencies toward phonetically similar confusions.
- Core assumption: The sequential generation process provides implicit self-correction signals.
- Evidence anchors:
  - [section 2.2] "We found that the model has shown some improvements but errors still persist... Our proposed annotation model, however, takes both audio and its transcript as input."
  - [Table 2] Annt-v2-priv (joint output, audio-only) achieves 0.80% CER vs. Annt-v1-priv (labels only) at 1.13% CER.
  - [corpus] Related work (arXiv:2506.04527) on implicit/explicit grapheme conditioning provides converging evidence.
- Break condition: If grapheme generation is inaccurate (as in audio-only conditioning), it may provide misleading guidance for subsequent label decoding.

## Foundational Learning

- **Whisper architecture (encoder-decoder Transformer for ASR)**
  - Why needed here: The method fine-tunes Whisper-large-v3-turbo; understanding its log-Mel encoder input, cross-attention decoder, and prompt tokens (`<startofprev>`, `<startoftranscript>`) is prerequisite to modifying the training pipeline.
  - Quick check question: Can you explain how Whisper's decoder receives encoder outputs and how prompt tokens influence generation?

- **Japanese pitch accent and mora structure**
  - Why needed here: TTS labels encode accent nucleus position, pitch rising/falling ("[", "]"), and accent phrase boundaries ("#"); incorrect accent type calculation during dictionary correction directly affects synthesis quality.
  - Quick check question: What is the difference between mora and syllable in Japanese, and how does accent type 0 differ from type n?

- **Edit distance (Levenshtein distance) for pronunciation matching**
  - Why needed here: The decoding strategy selects dictionary pronunciations by minimum edit distance to model predictions; implementation requires converting Katakana sequences to comparable representations.
  - Quick check question: Given predicted "シアワセニ" and candidates "シヤワセニ" and "シアワセヌ", which has lower edit distance?

## Architecture Onboarding

- **Component map:**
  - Log-Mel spectrogram (encoder) + tokenized transcript prompt (decoder input) -> Whisper encoder (acoustic hidden representations) -> Whisper decoder (cross-attention to encoder outputs, auto-regressive generation) -> Post-processing (MeCab+UniDic segmentation, edit-distance pronunciation selection, accent type adjustment) -> Output (phrase-level graphemes + TTS labels)

- **Critical path:**
  1. Prepare audio-transcript pairs with manual TTS labels (JSUT with GPT-4 alignment, or custom corpus)
  2. Tokenize transcripts, insert as decoder prompts between special tokens
  3. Fine-tune Whisper with modified vocabulary (Unicode symbols for "↑", "↓", "①", etc., replacing "[", "]", "#", " ", "|")
  4. During inference, decode with beam search; after each "#", apply dictionary correction
  5. Evaluate: CER for phonemes, accuracy/F1 for prosody, MOS for TTS naturalness

- **Design tradeoffs:**
  - **Symbol vocabulary:** Original symbols ("[", "]") caused hallucinations; Unicode alternatives not in Whisper's vocabulary are safer but require tokenizer extension
  - **Accent type adjustment (Equation 1):** Simplified heuristic may misplace accent nucleus when mora count changes; authors note "restoration is not perfect but impact is minimal"
  - **Training data size:** Models trained on private 121.82-hour corpus (Annt-*-priv) significantly outperform public JSUT-only models (Annt-*-pub)

- **Failure signatures:**
  - **Phonetic confusion:** Model outputs "クタモ" instead of "クテモ" (similar sounds) → indicates insufficient transcript conditioning
  - **Grapheme normalization drift:** Model converts "1週" to "一週" → inconsistent with training labels but semantically correct
  - **Hallucination with standard symbols:** Using "#" or "[" directly triggers spurious outputs → switch to Unicode tokens

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune Whisper (audio-only) to output TTS labels; measure CER on held-out set to confirm degradation (expect ~1.13% CER per paper)
  2. **Prompt ablation:** Add transcript prompt but withhold dictionary correction; verify CER drops to ~0.63% (Annt-v3-priv level)
  3. **Full pipeline validation:** Enable dictionary-enhanced decoding; confirm final CER ~0.57% with acceptable prosody F1 (may see slight degradation as noted)

## Open Questions the Paper Calls Out

- **Can the phrase-level graphemes and TTS labels generated by this method be effectively utilized to construct high-quality polyphone disambiguation datasets?**
  - Basis in paper: [explicit] The Conclusion states the model "holds potential for constructing polyphone disambiguation datasets... in future work."
  - Why unresolved: The paper demonstrates the model's annotation capability but does not validate the downstream utility of the generated data specifically for training polyphone disambiguation modules.
  - What evidence would resolve it: A comparative study evaluating the accuracy of a disambiguation model trained on auto-generated labels versus one trained on human-verified labels.

- **Is it feasible to train text-based accent sandhi estimation models using the prosodic labels produced by this system?**
  - Basis in paper: [explicit] The Conclusion identifies "training text-based accent sandhi estimation models" as a potential future application of the proposed method.
  - Why unresolved: The current work focuses on extracting labels from audio-transcript pairs, whereas accent sandhi estimation typically requires predicting these labels from text alone, a capability not yet tested.
  - What evidence would resolve it: Experiments showing that a text-based estimator trained on the model's outputs generalizes well to new text inputs without requiring audio.

- **How does the dictionary-enhanced decoding strategy perform on out-of-vocabulary (OOV) words or neologisms not present in the UniDic dictionary?**
  - Basis in paper: [inferred] Section 2.3 describes a decoding strategy that relies on MeCab and UniDic to "look up each word's candidate pronunciations," implying a dependency that may fail for OOV terms.
  - Why unresolved: The paper does not analyze failure cases where the dictionary lacks an entry for the predicted graphemes, leaving the fallback mechanism undefined.
  - What evidence would resolve it: An ablation study or error analysis specifically targeting words absent from the dictionary to see if the model reverts to the raw Whisper prediction or hallucinates.

## Limitations

- **Dataset dependency:** The best performance (0.57% CER) was achieved on a private 121.82-hour dataset, while publicly available models show significantly lower performance (1.31% CER).
- **Dictionary coverage limitations:** The correction mechanism depends on UniDic coverage, which may fail for out-of-vocabulary words, proper nouns, or dialectal pronunciations.
- **Limited subjective validation:** MOS testing involved only 20 native Japanese speakers evaluating 20 sentences, providing limited scope for generalizability assessment.

## Confidence

- **High Confidence:** The core claim that conditioning Whisper on ground truth transcripts during fine-tuning improves phonetic labeling accuracy (mechanism 1) is well-supported by both ablation results and related work on grapheme conditioning.
- **Medium Confidence:** The effectiveness of dictionary-enhanced decoding for correcting Kanji pronunciation errors (mechanism 2) is demonstrated but relies on the assumption that grapheme predictions are sufficiently accurate for dictionary lookup.
- **Low Confidence:** The claim that the approach achieves "comparable" TTS naturalness to manual annotations (MOS ~4.1) is based on limited subjective testing without comparison to other automatic annotation methods.

## Next Checks

1. **Dataset Size Sensitivity Analysis:** Reproduce the approach on progressively smaller subsets of the private dataset (or JSUT) to determine the minimum dataset size required for acceptable performance.
2. **Error Analysis on Dictionary Correction:** Conduct a detailed error analysis on the dictionary correction step, particularly for out-of-vocabulary words, proper nouns, and dialectal variations.
3. **Cross-Dataset Generalization Test:** Evaluate the fine-tuned models on a held-out dataset from a different domain (e.g., different speakers, recording conditions, or text genres) to assess robustness and generalization beyond the training distribution.