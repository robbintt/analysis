---
ver: rpa2
title: Evaluating the Generalization Capabilities of Large Language Models on Code
  Reasoning
arxiv_id: '2504.05518'
source_url: https://arxiv.org/abs/2504.05518
tags:
- program
- code
- programs
- reasoning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how large language models (LLMs) generalize
  to different program distributions in code reasoning tasks. The authors create three
  datasets: LLM-List (list-processing functions generated by LLMs), DSL-List (functions
  sampled from a domain-specific language), and LeetCode (human-written competitive
  programming solutions).'
---

# Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning

## Quick Facts
- **arXiv ID**: 2504.05518
- **Source URL**: https://arxiv.org/abs/2504.05518
- **Reference count**: 40
- **Primary result**: Latest reasoning models (QwQ, DeepSeek-R1, o3-mini) achieve near-perfect performance across all program types and distributions, demonstrating strong generalization abilities

## Executive Summary
This paper evaluates how large language models generalize to different program distributions in code reasoning tasks. The authors create three datasets: LLM-List (list-processing functions generated by LLMs), DSL-List (functions sampled from a domain-specific language), and LeetCode (human-written competitive programming solutions). They introduce mutation operators to create out-of-distribution variants and measure model performance on original vs. mutated programs.

The study finds that earlier models rely heavily on pattern matching, showing high correctness on in-distribution problems but significant drops on mutated or out-of-distribution programs. In contrast, the latest reasoning models achieve near-perfect performance across all program types and distributions, demonstrating strong generalization abilities. Traditional models show decreasing performance with program complexity, while reasoning models maintain accuracy regardless of complexity.

## Method Summary
The authors evaluate LLM code reasoning generalization through execution prediction (predict output given program+input) and execution choice (select between original/mutated programs, then predict output). They create three datasets: LLM-List (112 list functions from GPT-4o), DSL-List (100 programs sampled from custom DSL), and LeetCode (184 before-cutoff + 190 after-cutoff contest solutions). Each has original and mutated versions. The DSL sampler uses CFG probabilities to generate programs, mutation operators apply syntactic changes (arithmetic, relational, logical, keyword, literal), and mutant selection uses MostSimilarCoverage. Models are evaluated with pass@1 metrics across 10 different models.

## Key Results
- Earlier models exhibit high pattern matching on in-distribution LLM-List programs but struggle with out-of-distribution DSL-List programs and mutated versions
- Latest reasoning models (QwQ, DeepSeek-R1, o3-mini) demonstrate near-perfect performance across all program types and distributions
- Mutation reversion rates reveal that earlier models incorrectly predict mutant outputs for original programs, while reasoning models maintain consistency

## Why This Works (Mechanism)

### Mechanism 1: Distributional Divergence Detection via Mutation
Applying semantics-altering mutation operators creates out-of-distribution test cases that differentiate memorization from genuine reasoning. Mutation operators introduce localized syntactic perturbations that preserve structural complexity while changing program behavior. If a model relies on memorized training patterns, it will correctly predict outputs for original programs but revert to original outputs when reasoning about mutated programs—manifesting as high mutation reversion (MR).

### Mechanism 2: DSL Sampling for OOD Generation
Sampling programs from a domain-specific language with a combinatorially large program space generates structurally valid but training-unseen programs that directly test generalization. A DSL defines primitives and type constraints compiled into a context-free grammar. Sampling from the CFG with controlled depth/complexity generates programs unlikely to exist in training data. The combinatorial explosion of possible programs makes memorization infeasible, forcing models to perform genuine code reasoning.

### Mechanism 3: Introspective Preference via Execution Choice
Models can introspect about their confidence in reasoning about different program versions, and preference for original programs over mutants signals awareness of distributional familiarity. In the execution choice task, models are presented with both original and mutated programs and asked to choose which they are more confident reasoning about, then predict output. Consistent preference for original programs indicates the model recognizes in-distribution patterns.

## Foundational Learning

- **Concept: Mutation Testing** - Why needed: Understanding mutation operators (changing arithmetic operators, relational comparisons) is essential for interpreting why they create OOD programs while preserving complexity. Quick check: Given the mutation `a < b` → `a <= b`, does this always change program output? What if the values of `a` and `b` are never equal in test inputs?

- **Concept: Distribution Shift and OOD Detection** - Why needed: The core thesis depends on distinguishing in-distribution programs (likely memorized) from OOD programs (requiring generalization). Quick check: If a model scores 99% on original LLM-List programs but 40% on mutated versions, what does this suggest about its reasoning capabilities?

- **Concept: Pass@k Metric** - Why needed: The paper uses pass@1 to measure correctness probability from limited samples. Quick check: If a model produces 3 correct outputs in 5 generations for a problem, what is its estimated pass@1?

## Architecture Onboarding

- **Component map**: Dataset Pipeline (DSL sampler → Transpiler → Python; LLM-List; LeetCode scraper → Filters) → Mutation Engine (Apply operators → Filter for executability + different output → Select by coverage similarity) → Evaluation Harness (Execution Prediction; Execution Choice) → Model Interface (vLLM for open models; API for closed models)

- **Critical path**: Generate/collect original datasets → Apply mutation pipeline to create paired problems → Run execution prediction for each model → Run execution choice experiments with order swapping → Aggregate metrics

- **Design tradeoffs**: Mutation selection (coverage-similarity selection preserves complexity vs. random selection), Prompt design (zero-shot for reasoning models vs. one-shot CoT for traditional models), Complexity proxy (Lines of code vs. AST depth or cyclomatic complexity)

- **Failure signatures**: High OR on original programs (Model incorrectly reverting to mutant outputs), Preference always ~50% on LLM-List (Model cannot distinguish in-distribution programs), Near-random correctness on DSL-List across all models (DSL programs may be too complex or contain errors)

- **First 3 experiments**: Baseline execution prediction on LLM-List with GPT-4o-mini (expect high OC ~99%, moderate MC ~68%, low OR ~0.3%, moderate MR ~28%), DSL-List sanity check with DC-7B and o3-mini (expect low OC ~25% for DC-7B confirming OOD difficulty; near-perfect for o3-mini), Position bias control for execution choice (run with original always first, then swapped)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the latest reasoning models' near-perfect generalization capabilities extend to more complex program structures beyond list-processing and competitive programming solutions? The study is limited to specific program classes; it remains unclear whether results generalize to systems code, concurrent programs, or enterprise-scale codebases.

- **Open Question 2**: Can the mutation-based evaluation framework effectively measure generalization for code reasoning tasks beyond execution prediction, such as specification reasoning or intermediate state prediction? The authors state their approach is directly applicable to other settings but only evaluate execution prediction.

- **Open Question 3**: How does the gap between before-cutoff and after-cutoff performance evolve for reasoning models trained on increasingly diverse code corpora, and at what point does this evaluation technique become ineffective? As models train on broader code distributions, the distinction between in-distribution and out-of-distribution based on temporal cutoffs may become meaningless.

## Limitations

- Limited sample size for DSL and LeetCode datasets (100 DSL programs, 184 before-cutoff LeetCode problems) may not fully represent the diversity of programming scenarios
- Unknown training data overlap prevents definitive establishment of true in-distribution vs. OOD boundary
- Controlled evaluation environment using simplified list-processing problems without external dependencies may not extrapolate to real-world programming scenarios

## Confidence

- **High confidence**: The methodology for creating OOD test cases through DSL sampling and mutation is technically sound and provides clear differentiation between memorization and reasoning behaviors
- **Medium confidence**: The claim that reasoning models achieve "near-perfect" generalization requires cautious interpretation given limited dataset sizes and controlled problem scope
- **Low confidence**: The introspective preference mechanism's interpretation as genuine self-assessment is speculative and could have alternative explanations

## Next Checks

1. **Dataset scaling validation**: Replicate the study with 1000+ DSL programs and 1000+ LeetCode solutions, maintaining the same mutation and evaluation pipeline, to verify that performance trends persist with increased statistical power

2. **Cross-task generalization test**: Apply the same mutation and DSL sampling methodology to text-based reasoning tasks (e.g., mathematical word problems) to determine whether observed model behaviors generalize beyond code reasoning

3. **Training data proximity analysis**: Conduct approximate nearest neighbor searches in token space between DSL programs and model training corpora to empirically estimate the likelihood of direct memorization versus genuine generalization