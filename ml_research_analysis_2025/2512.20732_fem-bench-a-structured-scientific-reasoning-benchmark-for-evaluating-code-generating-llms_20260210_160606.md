---
ver: rpa2
title: 'FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating
  LLMs'
arxiv_id: '2512.20732'
source_url: https://arxiv.org/abs/2512.20732
tags:
- local
- stiffness
- fem-bench
- element
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FEM-Bench, a benchmark designed to evaluate\
  \ large language models\u2019 ability to generate correct finite element method\
  \ and computational mechanics code. FEM-Bench 2025 includes 33 introductory but\
  \ nontrivial tasks covering 1D and 2D FEM, and 3D matrix structural analysis, each\
  \ with reference implementations, verification inputs, and unit tests."
---

# FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs

## Quick Facts
- arXiv ID: 2512.20732
- Source URL: https://arxiv.org/abs/2512.20732
- Reference count: 12
- Primary result: Introduces FEM-Bench 2025 benchmark with 33 introductory FEM tasks for evaluating LLMs' code generation capabilities in scientific computing

## Executive Summary
This paper introduces FEM-Bench, a benchmark designed to evaluate large language models' ability to generate correct finite element method and computational mechanics code. FEM-Bench 2025 includes 33 introductory but nontrivial tasks covering 1D and 2D FEM, and 3D matrix structural analysis, each with reference implementations, verification inputs, and unit tests. Tasks are grouped by complexity and the availability of helper functions, enabling fine-grained analysis of model performance. Evaluation focuses on both code correctness and the ability to generate unit tests that detect known failure cases. Results show that even the strongest models achieve only partial success, with the best performing model solving 30/33 tasks at least once, and 26/33 tasks consistently across five attempts. The best model at unit test writing achieved a 73.8% average joint success rate. Performance deteriorates significantly on tasks requiring geometric nonlinear reasoning or geometric stiffness computation, highlighting gaps in both domain knowledge and compositional reasoning. FEM-Bench establishes a structured foundation for evaluating scientific code generation, with future iterations planned to include more advanced tasks and track ongoing model progress.

## Method Summary
FEM-Bench 2025 consists of 33 structured tasks covering finite element method and computational mechanics concepts. Each task includes a detailed specification, reference implementation in Python, verification inputs, and comprehensive unit tests. Tasks are organized into three complexity levels and categorized by whether they require helper functions. The evaluation methodology involves five independent attempts per model for each task, assessing both code correctness and unit test generation capabilities. The benchmark specifically evaluates geometric nonlinear reasoning and geometric stiffness computation, which are identified as particularly challenging areas for current LLMs. Performance is measured using success rates across multiple attempts, with particular attention to consistency and the ability to detect known failure modes through unit testing.

## Key Results
- Best-performing model solved 30/33 tasks at least once, with 26/33 tasks solved consistently across five attempts
- Top model achieved 73.8% average joint success rate for code generation plus effective unit test creation
- Performance significantly degraded on tasks requiring geometric nonlinear reasoning and geometric stiffness computation
- Models showed limited compositional reasoning ability when combining multiple FEM concepts in single tasks

## Why This Works (Mechanism)
FEM-Bench works by providing a structured evaluation framework that isolates specific scientific computing capabilities while maintaining realistic complexity levels. The benchmark's design forces models to demonstrate both domain-specific knowledge of finite element methods and general programming competency through the combination of reference implementations, verification inputs, and unit tests.

## Foundational Learning
- **Finite Element Method Fundamentals**: Understanding of FEM discretization, shape functions, and stiffness matrix assembly - needed for all tasks, quick check through basic beam element implementation
- **Matrix Structural Analysis**: Knowledge of global stiffness matrix construction and load vector assembly - required for structural analysis tasks, quick check through simple truss problems
- **Geometric Nonlinearity**: Understanding of geometric stiffness and its role in large displacement analysis - critical for advanced tasks, quick check through geometric stiffness matrix computation
- **Unit Test Design**: Ability to create tests that catch common implementation errors - essential for evaluation, quick check through test coverage of boundary conditions
- **Python Scientific Computing**: Proficiency with NumPy and linear algebra operations - foundational for all code generation, quick check through basic matrix operations
- **Code Composition**: Skill in combining multiple programming concepts into coherent solutions - required for complex tasks, quick check through multi-step problem solving

## Architecture Onboarding
Component Map: Task Specification -> Code Generation -> Reference Implementation -> Unit Tests -> Evaluation
Critical Path: Problem understanding → Algorithmic design → Code implementation → Testing → Validation
Design Tradeoffs: Granular task breakdown vs. realistic complexity; helper functions vs. complete solutions; multiple attempts vs. single-shot evaluation
Failure Signatures: Syntax errors, incorrect mathematical formulations, missing boundary conditions, inadequate unit test coverage
First Experiments:
1. Basic 1D FEM element implementation to establish baseline capability
2. Unit test generation for simple numerical integration task
3. Geometric stiffness computation for nonlinear analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope limited to 33 introductory tasks that don't capture full real-world complexity
- Evaluation doesn't assess runtime efficiency, numerical stability, or scalability of generated code
- Five-attempt protocol may overestimate model reliability compared to typical single-shot user experience
- Helper function reliance introduces confounding factors when comparing model performance across task categories

## Confidence
- High: Core finding that LLMs struggle with geometric nonlinear reasoning and geometric stiffness computation
- Medium: Relative performance rankings between models due to potential inflation from five-attempt protocol
- Medium: Unit test generation results due to subjective evaluation criteria for "joint success"

## Next Checks
1. Expand benchmark to include tasks requiring parallel computation and mesh refinement for computationally intensive FEM problems
2. Conduct user study comparing human experts' performance on FEM-Bench tasks against evaluated LLMs, especially for geometric nonlinear reasoning
3. Evaluate generated code's numerical stability and convergence properties on larger, more complex problem instances beyond provided test cases