---
ver: rpa2
title: 'SafePassage: High-Fidelity Information Extraction with Black Box LLMs'
arxiv_id: '2510.00276'
source_url: https://arxiv.org/abs/2510.00276
tags:
- information
- entity
- context
- llms
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of hallucinations in black-box
  LLM information extraction by introducing a "safe passage" concept and a three-step
  SAFEPASSAGE pipeline: (1) structured generation of entities and context, (2) string-based
  alignment to verify grounding in the document, and (3) scoring to ensure the context
  supports the extracted information. The pipeline reduces hallucinations by up to
  85% on legal information extraction tasks with minimal false positives.'
---

# SafePassage: High-Fidelity Information Extraction with Black Box LLMs

## Quick Facts
- arXiv ID: 2510.00276
- Source URL: https://arxiv.org/abs/2510.00276
- Reference count: 14
- This paper introduces a three-step pipeline that reduces LLM hallucination rates by up to 85% on legal information extraction tasks with minimal false positives.

## Executive Summary
This paper addresses the critical problem of hallucinations in black-box LLM information extraction by introducing a "safe passage" concept and a three-step SAFEPASSAGE pipeline. The approach requires LLMs to generate structured entities with supporting context, uses string-based alignment to verify grounding in source documents, and employs scoring to ensure context supports extracted information. Surprisingly, a small transformer encoder fine-tuned on just 1-2 hours of human annotations outperforms an LLM scorer at detecting unsafe passages, running at roughly 2,000 times lower cost and with substantially lower latency.

## Method Summary
The SAFEPASSAGE pipeline operates in three stages: (1) structured generation where LLMs output entities with associated document context via constrained decoding, (2) character-level Smith-Waterman alignment to verify the context exists in the source document, and (3) scoring via a fine-tuned NLI model to confirm the context supports the extracted information. The best performer uses DeBERTaV3 fine-tuned on LLM-generated pseudolabels followed by 500 human labels, achieving 85% hallucination detection with minimal false positives at dramatically lower cost than LLM scorers.

## Key Results
- Reduces hallucinations by up to 85% on legal information extraction tasks
- Small transformer encoder outperforms LLM scorer at 2,000× lower cost
- Alignment threshold τ=0.6 effectively handles LLM transcription errors while rejecting hallucinations
- NLI scores enable reliable offline evaluation of LLM extraction quality

## Why This Works (Mechanism)

### Mechanism 1: Context Extraction as Explicit Verification Artifact
The pipeline requires LLMs to output both structured entities and their supporting context, creating a verifiable audit trail. This transforms grounding verification from an implicit trust problem into an explicit string-matching problem, allowing detection of hallucinations that might otherwise pass as valid extractions.

### Mechanism 2: Character-Level Fuzzy Alignment for Grounding Verification
Character-level Smith-Waterman alignment handles LLM transcription errors (OCR correction, capitalization normalization, word insertion/elision) while maintaining a threshold (τ=0.6) that balances recall against precision. This allows legitimate transcription variations to pass while rejecting hallucinated contexts.

### Mechanism 3: Task-Specific NLI Fine-Tuning Outperforms LLM Scorers
A small transformer encoder fine-tuned on 500 human labels + LLM-generated silver labels outperforms general LLM scorers by learning domain-specific entailment patterns. The NLI framing (premise=context, hypothesis=entity) captures the "support" relationship more effectively than generic scoring approaches.

## Foundational Learning

### Natural Language Inference (NLI) as Entailment Classification
**Why needed**: The scoring mechanism recasts "does this context support this extraction?" as an NLI problem. Understanding NLI outputs (entails/neutral/contradicts) is essential for interpreting SafePassage scores.
**Quick check**: Given premise "The hearing date(s) of january 17, 2012" and hypothesis "Hearing Date: 2012-01-17", what NLI relationship should a well-calibrated model predict?

### Smith–Waterman Local Sequence Alignment
**Why needed**: The alignment step uses character-level Smith-Waterman to handle LLM transcription errors. Understanding alignment scores (matches/mismatches/gaps → score) helps set appropriate thresholds.
**Quick check**: If an LLM-generated context "date of hearing January 17, 2012" aligns to document text "date(s) of hearing january 17, 2012" with 31 matches out of 35 alignment length, what is the alignment score? Should it pass a τ=0.6 threshold?

### Silver Label Pre-Training for Low-Resource Fine-Tuning
**Why needed**: LLM-generated pseudolabels improve NLI scorer quality at all human annotation levels. Understanding this transfer learning pattern (large noisy data → small clean data) is critical for cost-efficient scorer training.
**Quick check**: Why might LLM-generated silver labels help even though they're noisy? What failure mode would you expect if you skipped silver label pre-training and only used 100 human labels?

## Architecture Onboarding

### Component Map
Document Input → [LLM Extractor] → Structured Entities + Contexts → [String Aligner] → Aligned Spans + Scores → [NLI/LLM Scorer] → Final SafePassage Scores → [Filter/Flag] → Verified Extractions OR Flagged for Review

### Critical Path
1. **Schema Design**: Define Pydantic models with `context` field for each entity type. Incorrect schema design cascades failures.
2. **Alignment Threshold Tuning**: τ=0.6 works for legal documents with OCR errors; validate on your domain.
3. **Scorer Selection**: Start with pre-trained DeBERTaV3 NLI; if F1 < 0.8 on validation, collect 200-500 human labels and fine-tune with silver label pre-training.

### Design Tradeoffs
- **LLM Scorer vs. NLI Scorer**: LLM scorers generalize to new entity types without retraining but cost ~2,000× more per prediction. NLI scorers require task-specific labels but are cost-efficient at scale.
- **Precision vs. Recall**: High-recall scorers catch more hallucinations but may flag safe passages; high-precision scorers have fewer false flags but may miss hallucinations.
- **Exact Match vs. Fuzzy Alignment**: Exact match is faster but rejects 5-10% of valid contexts with transcription errors; fuzzy alignment adds latency but improves recall.

### Failure Signatures
| Symptom | Likely Cause | Diagnostic |
|---------|--------------|------------|
| >15% of extractions flagged | Alignment threshold too high or LLM poor at copying contexts | Check alignment score distribution; verify LLM instruction-following |
| Hallucinations passing through | NLI scorer underfitting or threshold too low | Check scorer calibration; verify human annotation quality |
| High latency | Using LLM scorer at scale | Switch to fine-tuned NLI encoder |
| Entity extraction schema violations | LLM not following structured output format | Verify prompt includes schema examples; try different LLM |

### First 3 Experiments
1. **Baseline Alignment Analysis**: Run the LLM extractor on 50 documents, compute alignment scores for all contexts, and plot the distribution. Confirm scores separate into clear "grounded" (s > 0.7) and "ungrounded" (s < 0.4) clusters before setting τ.
2. **Scorer Ablation**: Compare pre-trained DeBERTaV3 NLI vs. LLM scorer (GPT-4.1) on 100 human-labeled entity-context pairs. If NLI F1 < 0.75, proceed to fine-tuning.
3. **Silver Label Impact**: Fine-tune two NLI models—one with silver label pre-training, one without—using 200 human labels. Compare F1 scores to quantify the value of LLM-generated pseudolabels for your domain.

## Open Questions the Paper Calls Out

### Open Question 1
**Can SAFEPASSAGE scores be effectively utilized as a feedback mechanism in a multi-turn setting to allow the LLM to self-correct rejected extractions?**
The current pipeline functions prophylactically (flagging) or therapeutically (suppressing), but it does not currently loop the rejection signal back to the generator for a retry. An implementation where low-scoring extractions are fed back to the LLM with prompts to re-extract, resulting in higher recall without a loss in precision, would resolve this.

### Open Question 2
**Is it possible to develop a task-agnostic scorer that matches the performance of the task-specific fine-tuned NLI model?**
The conclusion notes "In future work we aim to improve more task-agnostic scorers," distinguishing this from the current best-performing task-specific approach. A truly task-agnostic scorer would eliminate the 1-2 hours of task-specific human annotation required for the current approach.

### Open Question 3
**Does the efficiency of the fine-tuned NLI scorer generalize to domains outside of legal text, such as medical or financial documents?**
The paper evaluates exclusively on the ASYLEX dataset (legal domain) and notes that "hallucinations are a particularly large concern in certain critical domains, like legal and medical tasks." It is untested whether the NLI scorer or alignment threshold (τ=0.6) transfers effectively to other high-stakes domains without re-annotation.

## Limitations
- Domain-specific optimization: The τ=0.6 alignment threshold was optimized for legal documents with OCR errors and may not generalize to other domains
- Fundamental LLM grounding limitations: The pipeline cannot overcome cases where LLMs fail to reliably copy supporting evidence (~5-10% of cases)
- Single domain evaluation: The 85% hallucination reduction claim is based solely on the ASYLEX legal dataset

## Confidence

**High Confidence**: The three-step pipeline architecture is well-specified and the 2,000× cost-efficiency comparison between NLI and LLM scorers is directly measurable. The Smith-Waterman alignment mechanism is standard bioinformatics practice, and the ~95% grounding rate for o4-mini provides empirical support.

**Medium Confidence**: The claim that a small transformer encoder outperforms an LLM scorer at hallucination detection is supported by Table 1, but generalisability to other document types and entity schemas requires validation. The assertion that SAFEPASSAGE scores enable reliable offline evaluation is demonstrated but not extensively tested across diverse extraction scenarios.

**Low Confidence**: The paper's claim that the approach reduces hallucinations by "up to 85%" is based on a single legal domain dataset. The identified break conditions (LLM poor at copying contexts, hallucinated contexts achieving high alignment scores) are theoretically sound but not empirically validated as failure points in practice.

## Next Checks

1. **Domain Transfer Test**: Apply SAFEPASSAGE to a non-legal domain (e.g., medical records or financial documents) and measure hallucination reduction compared to baseline LLM extraction. Compare alignment score distributions and NLI scorer performance across domains to identify domain-specific tuning requirements.

2. **Failure Mode Analysis**: Systematically generate test cases where the alignment step should fail (hallucinated contexts that achieve high alignment scores by copying unrelated document spans) and measure false negative rates. Similarly, test cases where the NLI scorer should fail (contexts that exist but don't support extractions) to quantify real-world failure rates.

3. **Cost-Benefit Scaling Study**: Measure the relationship between human annotation hours invested and hallucination detection performance across 100, 500, and 1,000 label regimes. Determine the marginal benefit of additional labels and identify the point of diminishing returns for different extraction task complexities.