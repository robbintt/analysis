---
ver: rpa2
title: 'Tokenized SAEs: Disentangling SAE Reconstructions'
arxiv_id: '2502.17332'
source_url: https://arxiv.org/abs/2502.17332
tags:
- features
- saes
- figure
- unigram
- tokenized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that many features learned by sparse auto-encoders
  (SAEs) in language models predominantly reconstruct common input n-grams rather
  than capturing interesting model behavior, due to class imbalance in training data.
  The authors propose "Tokenized SAEs" to address this issue by introducing a per-token
  lookup table that handles token reconstruction separately from feature reconstruction.
---

# Tokenized SAEs: Disentangling SAE Reconstructions

## Quick Facts
- arXiv ID: 2502.17332
- Source URL: https://arxiv.org/abs/2502.17332
- Authors: Thomas Dooms; Daniel Wilhelm
- Reference count: 27
- Key outcome: Tokenized SAEs improve reconstruction quality in sparse regimes while reducing feature count by ~25% through separate token reconstruction

## Executive Summary
This paper addresses a fundamental limitation in sparse auto-encoders (SAEs) where learned features tend to reconstruct common n-grams rather than capturing meaningful model behavior. The authors identify that this occurs due to class imbalance in training data, causing SAEs to spend capacity memorizing frequent tokens. Their proposed solution, Tokenized SAEs, introduces a per-token lookup table that handles token reconstruction separately from feature reconstruction. This architectural modification allows SAEs to learn more semantically meaningful features while maintaining or improving reconstruction quality across multiple models including GPT-2 and Pythia.

## Method Summary
Tokenized SAEs modify the standard SAE architecture by adding a per-token lookup table that specifically handles token reconstruction. This separation allows the feature dictionary to focus on learning semantically meaningful patterns rather than memorizing common n-grams. The lookup table is designed to be computationally efficient, adding minimal overhead while significantly improving the model's ability to learn complex features. The approach is validated across multiple model sizes and shows consistent improvements in both reconstruction metrics and feature interpretability.

## Key Results
- Achieves comparable or better reconstruction metrics (MSE, cross-entropy) compared to standard SAEs
- Reduces feature count by approximately 25% while maintaining performance
- Features are faster to train and demonstrate improved interpretability
- Lookup table adds minimal computational overhead while enabling more complex feature learning

## Why This Works (Mechanism)
Standard SAEs struggle with class imbalance because common tokens dominate the training data, causing the model to allocate significant capacity to memorizing these frequent patterns. By separating token reconstruction into a dedicated lookup table, Tokenized SAEs free up the feature dictionary to focus on learning semantically meaningful patterns that are less frequent but more informative about model behavior.

## Foundational Learning

1. **Sparse Auto-Encoders (SAEs)**
   - Why needed: Core architecture for feature learning in language models
   - Quick check: Understand how SAEs compress activations and reconstruct inputs

2. **Class Imbalance in Language Models**
   - Why needed: Explains why standard SAEs learn n-gram features
   - Quick check: Recognize frequency distribution of tokens in training data

3. **Feature Dictionary vs Token Reconstruction**
   - Why needed: Core insight behind Tokenized SAE design
   - Quick check: Distinguish between memorizing tokens and learning semantic features

## Architecture Onboarding

**Component Map:**
Input -> Token Lookup Table -> Feature Dictionary -> Decoder -> Reconstruction

**Critical Path:**
Token reconstruction handled by lookup table, feature reconstruction handled by dictionary, combined in decoder

**Design Tradeoffs:**
- Added lookup table complexity vs improved feature quality
- Minimal computational overhead vs significant interpretability gains
- Reduced feature count vs maintained reconstruction performance

**Failure Signatures:**
- Lookup table becomes too large (inefficient)
- Feature dictionary still learns n-grams (insufficient separation)
- Reconstruction quality degrades (improper balance)

**First Experiments:**
1. Compare reconstruction MSE between standard SAE and Tokenized SAE on held-out data
2. Analyze feature interpretability through qualitative examples
3. Measure feature count reduction while maintaining reconstruction quality

## Open Questions the Paper Calls Out
None

## Limitations
- Limited quantitative validation that features correspond to interesting model behaviors beyond lookup table
- Unclear long-term implications for feature sparsity and model compression benefits
- Qualitative interpretability claims lack comprehensive quantitative evidence

## Confidence

- **High confidence**: Empirical observation of n-gram correlation in SAE features is well-established
- **Medium confidence**: ~25% feature count reduction claim demonstrated across tested models
- **Medium confidence**: Interpretability improvements supported by qualitative examples but lack quantitative validation

## Next Checks
1. Conduct systematic ablation studies measuring interpretability gains from the lookup table specifically
2. Evaluate feature transferability and generalization across different models and tasks
3. Measure impact on downstream applications like model editing and interpretability tools