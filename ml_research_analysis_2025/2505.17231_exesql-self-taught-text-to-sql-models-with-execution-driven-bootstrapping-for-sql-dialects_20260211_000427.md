---
ver: rpa2
title: 'ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping
  for SQL Dialects'
arxiv_id: '2505.17231'
source_url: https://arxiv.org/abs/2505.17231
tags:
- arxiv
- data
- table
- zhang
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating SQL queries across
  multiple dialects (PostgreSQL, MySQL, Oracle) by introducing an execution-driven
  bootstrapping framework. The core method uses iterative query generation, execution-based
  rejection sampling, and preference optimization to adapt a model to new SQL dialects
  without requiring manual annotation.
---

# ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects

## Quick Facts
- arXiv ID: 2505.17231
- Source URL: https://arxiv.org/abs/2505.17231
- Reference count: 40
- Primary result: Achieves 15.2%, 10.38%, and 4.49% improvements over GPT-4o on PostgreSQL, MySQL, and Oracle text-to-SQL benchmarks respectively

## Executive Summary
This paper addresses the challenge of generating SQL queries across multiple dialects (PostgreSQL, MySQL, Oracle) by introducing an execution-driven bootstrapping framework. The core method uses iterative query generation, execution-based rejection sampling, and preference optimization to adapt a model to new SQL dialects without requiring manual annotation. Experiments show that the approach significantly improves text-to-SQL performance, achieving average improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and Oracle, respectively, across datasets of varying difficulty.

## Method Summary
The method employs a three-stage pipeline: (1) Translation bootstrapping using GPT-4o to convert SQLite SQL to target dialects with iterative error feedback, (2) Iterative supervised fine-tuning (SFT) with execution-based rejection sampling (best-of-N selection), and (3) Direct Preference Optimization (DPO) training using pairs of valid and invalid SQL queries. The framework generates dialect-specific training data by translating existing SQLite datasets, executing generated SQL to verify correctness, and using execution failures as negative examples for preference learning. The base model is Deepseek-Coder-7B, fine-tuned with full-parameter updates.

## Key Results
- Average improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and Oracle respectively
- Execution-driven bootstrapping significantly improves text-to-SQL performance across Spider, WikiSQL, BIRD, and Dr.Spider benchmarks
- Ablation studies confirm the contribution of each component: translation bootstrapping, rejection sampling, and preference optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Execution feedback enables iterative SQL refinement that static translation cannot achieve.
- Mechanism: When LLM-translated SQL fails execution, the error message is fed back as context for the next translation attempt, allowing the model to correct syntax errors, type mismatches, and dialect-specific constraint violations.
- Core assumption: The target database environment accurately reflects production constraints; error messages contain actionable diagnostic information.
- Evidence anchors: The paper states that "data generated purely through static prompting—without validating SQLs via execution—tends to be noisy and unreliable."

### Mechanism 2
- Claim: Best-of-N rejection sampling extracts high-quality training data from a model that already produces some correct outputs.
- Mechanism: For each question, generate N SQL candidates, execute all, retain successful queries for SFT and failed queries for negative examples.
- Core assumption: The base model's correctness rate > 0; execution environment is deterministic and fast enough for N-fold sampling.
- Evidence anchors: Results show the bootstrapped model already produces many correct samples, with larger N further improving correctness.

### Mechanism 3
- Claim: DPO on execution-based preference pairs teaches the model to avoid syntactically valid but semantically incorrect SQL.
- Mechanism: Construct pairs (S_pos from D_Valid, S_neg from D_Neg) for the same question; DPO loss encourages P(S_pos|Q) > P(S_neg|Q).
- Core assumption: Negative examples are meaningfully informative; preference signal generalizes across questions.
- Evidence anchors: By leveraging execution failures as negative examples and correct executions as positive examples, the model learns to generate more reliable and executable SQL queries.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO to align the model toward executable SQL without training a separate reward model.
  - Quick check question: Given a preference pair (y_w, y_l), does DPO maximize P(y_w|y_w,y_l) directly, or does it optimize a KL-constrained objective?

- Concept: **Execution-Based Verification**
  - Why needed here: The entire framework hinges on treating the database as a ground-truth verifier.
  - Quick check question: A SQL query executes without errors but returns an empty result set. Is this a positive or negative example for DPO?

- Concept: **Rejection Sampling**
  - Why needed here: The iterative data generation loop uses rejection sampling to filter candidates.
  - Quick check question: If your base model has 30% accuracy, what N is needed to achieve 95% probability of finding at least one correct SQL per question?

## Architecture Onboarding

- Component map: SQLite Dataset → LLM Translator + Execution Env → Bootstrap Dataset D_Trans → Base Model M_θ → Best-of-N Sampler → Execution Filter → D_Valid, D_Neg → SFT on D_Valid → Iterative Model Update → DPO on (D_Valid, D_Neg) pairs → Final ExeSQL Model

- Critical path:
  1. Bootstrap dataset creation (translation + execution verification) is the bottleneck for cold-start
  2. Execution filtering must use the same database environment as evaluation; schema mismatches cause false negatives
  3. DPO training requires balanced positive/negative pairs; severely imbalanced datasets degrade preference learning

- Design tradeoffs:
  - Execution overhead vs. data quality: BIRD (large-scale) takes 22× longer to execute than Spider (small-scale)
  - Mixed-dialect vs. single-dialect training: Mixed training improves average performance but slightly reduces peak performance per dialect
  - LLM API cost vs. accuracy: Using SQLGlot pre-filtering reduces API calls by ~35% but misses semantic errors

- Failure signatures:
  - High false-negative rate: Many SQLs marked as "failed" actually have correct semantics but hit database-specific quirks
  - DPO loss not decreasing: May indicate preference pairs are too similar or negatives are trivial syntax errors
  - Overfitting to Spider/WikiSQL: Performance drops on Dr.Spider (OOD benchmark) if model memorizes question templates

- First 3 experiments:
  1. Validate bootstrap pipeline: Translate 100 SQLite→PostgreSQL queries, execute, manually inspect failures
  2. Ablate rejection sampling N: Compare N=2,4,8 on a held-out set
  3. Sanity-check DPO: Train on 500 preference pairs, evaluate on 100 held-out questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does execution success adequately capture semantic correctness, or does it conflate syntactically valid queries with semantically correct ones?
- Basis in paper: The paper defines the reward function R(S) ∈ {0, 1} based solely on whether "S executes successfully," but acknowledges that "direct translation does not guarantee correctness due to differences in SQL syntax and execution semantics across dialects."
- Why unresolved: Binary execution verification cannot distinguish between queries that run correctly but return wrong results versus those returning correct results.
- What evidence would resolve it: An analysis comparing execution-based rewards against result-matching rewards on a held-out test set with known ground-truth outputs.

### Open Question 2
- Question: How does the method perform on more diverse SQL dialects beyond PostgreSQL, MySQL, and Oracle?
- Basis in paper: The Limitations section states: "In future research, we plan to explore more dialects and more complex database conditions, aiming to enhance the coverage and robustness of our multi-dialect text-to-SQL framework."
- Why unresolved: The current evaluation covers only three mainstream dialects with relatively simple environments, leaving coverage of advanced dialect-specific features untested.
- What evidence would resolve it: Evaluation on additional dialects such as BigQuery, DuckDB, or T-SQL with benchmarks containing complex dialect-specific operations.

### Open Question 3
- Question: Can more sophisticated data mixing strategies mitigate the "forgetting" phenomenon observed in mixed-dialect training?
- Basis in paper: Section A.6 observes: "Models tend to overfit to the specific syntax they are trained on... To further improve cross-dialect generalization, more sophisticated mixing strategies such as in-batch mixing, data replay, or even model merging may be necessary."
- Why unresolved: Naive mixed training improves average performance but slightly reduces peak performance on individual dialects.
- What evidence would resolve it: Comparative experiments with in-batch mixing, data replay, and model merging approaches measured on per-dialect and cross-dialect generalization metrics.

## Limitations
- Dataset Coverage: Relies on translating from SQLite datasets, which may not capture all dialect-specific features and edge cases
- Execution Environment Fidelity: Success in the target database environment doesn't guarantee correctness - queries may execute without errors but return incorrect results
- Computational Overhead: Best-of-N rejection sampling requires N executions per question, scaling linearly with N

## Confidence
- High Confidence: The core claim that execution feedback enables iterative SQL refinement is well-supported by ablation studies
- Medium Confidence: The preference optimization claims are supported but the contribution is incremental
- Low Confidence: Claims about generalization to unseen databases or queries are not extensively validated

## Next Checks
1. Error Message Analysis: Manually categorize execution failures from the bootstrap pipeline to determine what fraction are actionable vs. uninformative
2. Cross-Dialect Robustness Test: Train a mixed PostgreSQL+MySQL model and evaluate on Oracle to quantify performance degradation
3. Semantic vs. Syntactic Error Separation: Create a test set of queries that execute successfully but return incorrect results, then measure whether ExeSQL can detect and correct these semantic errors through its refinement loop