---
ver: rpa2
title: 'DISCO: Diversifying Sample Condensation for Efficient Model Evaluation'
arxiv_id: '2510.07959'
source_url: https://arxiv.org/abs/2510.07959
tags:
- performance
- evaluation
- disco
- samples
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of evaluating
  modern machine learning models, which can require thousands of GPU hours for benchmarks
  like LMMs-Eval and HELM. The authors propose DISCO (Diversifying Sample Condensation),
  a method that selects evaluation samples maximizing model disagreement rather than
  sample diversity.
---

# DISCO: Diversifying Sample Condensation for Efficient Model Evaluation

## Quick Facts
- arXiv ID: 2510.07959
- Source URL: https://arxiv.org/abs/2510.07959
- Reference count: 32
- This paper proposes DISCO, a method that reduces model evaluation costs by 99.3% with only 1.07 percentage points of error.

## Executive Summary
This paper addresses the high computational cost of evaluating modern machine learning models, which can require thousands of GPU hours for benchmarks like LMMs-Eval and HELM. The authors propose DISCO (Diversifying Sample Condensation), a method that selects evaluation samples maximizing model disagreement rather than sample diversity. DISCO uses predictive diversity scoring (PDS) to identify samples where models produce varied outputs, then estimates model performance directly from model signatures (concatenated outputs on selected samples) using simple predictors like random forests or kNN. Theoretical analysis shows that inter-model disagreement is information-theoretically optimal for performance estimation. Experiments on MMLU, HellaSwag, Winogrande, and ARC datasets show DISCO achieves state-of-the-art efficiency-precision trade-offs.

## Method Summary
DISCO reduces model evaluation costs by selecting a small subset of samples that maximize inter-model disagreement, then predicting full benchmark performance from target model outputs on these samples. The method works by: (1) computing Predictive Diversity Scores (PDS) for each sample based on maximum class probabilities across source models, (2) selecting top-k samples with highest PDS, (3) encoding target model outputs on these samples as high-dimensional signatures, (4) applying PCA dimensionality reduction, and (5) training simple predictors (Random Forest or kNN) to map signatures to benchmark performance. The approach is theoretically justified by showing that maximizing Jensen-Shannon Divergence between model predictions is optimal for model differentiation.

## Key Results
- DISCO reduces evaluation costs by 99.3% (from 14,042 to 100 samples) on MMLU with only 1.07 percentage points of error
- Achieves state-of-the-art efficiency-precision trade-off: 0.987 rank correlation and 2.5% MAE on MMLU
- PDS-based sample selection outperforms random selection, task-stratified selection, and max-entropy methods
- Generalizes to vision tasks like ImageNet compression with similar performance gains

## Why This Works (Mechanism)

### Mechanism 1: Predictive Diversity Scoring (PDS) for Sample Selection
Samples that produce diverse model responses carry more information about model performance than samples that produce uniform responses. PDS measures per-sample disagreement across source models using the formula PDS = (1/C) Σ_c max_m f^m_c(x_i), which captures the maximum probability assigned to each class across all models. Higher PDS indicates samples where models disagree more. Samples are ranked by PDS and top-k are selected greedily. The core assumption is that models that perform similarly will have similar outputs on informative samples; samples that distinguish models are those where models produce varied outputs.

### Mechanism 2: Model Signature Representation for Direct Prediction
Raw concatenated model outputs on selected samples provide richer signal for performance prediction than scalar accuracy summaries or latent parameter estimates. Instead of computing accuracy on anchor points (losing distributional information), DISCO concatenates full probability distributions over classes for each selected sample, creating a high-dimensional "fingerprint." Simple predictors (RF, kNN) then map signatures directly to benchmark performance. The core assumption is that models with similar output patterns (distributions over classes) on the selected subset will have similar full benchmark performance.

### Mechanism 3: Information-Theoretic Optimality of Disagreement
Selecting samples with maximum inter-model Jensen-Shannon Divergence is theoretically optimal for model differentiation under stated assumptions. The proof establishes that mutual information between model index and predictions equals JSD across model predictions. Since the goal is to predict S(m) (e.g., accuracy), samples maximizing JSD provide maximum information about which model produced the outputs. PDS serves as a computable proxy with provable bounds to JSD. The core assumptions are that S(m) is injective (different models have different true performances); predictions are conditionally deterministic given the model index; and uniform prior over models.

## Foundational Learning

### Concept: Jensen-Shannon Divergence (JSD)
- **Why needed here**: Core theoretical justification for disagreement-based selection; JSD measures how different M probability distributions are from their ensemble mean. High JSD indicates samples where models produce meaningfully different predictions.
- **Quick check question**: Given predictions from 3 models on a binary classification sample—Model A: [0.9, 0.1], Model B: [0.5, 0.5], Model C: [0.1, 0.9]—would JSD be high or low? What about if all three predicted [0.5, 0.5]?

### Concept: Model Signature / Behavioral Embedding
- **Why needed here**: Understanding how to represent a model's behavior as a fixed-length vector. The signature captures the model's "fingerprint" on a reference dataset, enabling similarity-based performance prediction.
- **Quick check question**: If two models have cosine similarity 0.95 in their signatures on DISCO-selected samples, what does this imply about their relative benchmark performance?

### Concept: Dimensionality Reduction for Metamodel Generalization
- **Why needed here**: Signatures can be very high-dimensional (e.g., 3100 dims for 100 samples × 31 outputs). Table 2d shows PCA to 256 dims improves rank correlation from .918 to .987 by reducing overfitting.
- **Quick check question**: Why might using the full 3100-dimensional signature overfit when you only have 382 source models for training?

## Architecture Onboarding

### Component Map:
1. Source Model Pool -> PDS Computation Module -> Sample Selector -> Signature Encoder -> Performance Predictor

### Critical Path:
1. Offline Phase: Run all source models on full benchmark → obtain predictions and ground-truth performances → compute PDS per sample → select top-k samples → fit PCA and train predictor on source signatures
2. Online Phase: For new target model → run inference only on k selected samples → encode signature → predict performance via trained predictor

### Design Tradeoffs:
- Number of samples (k): Figure 5 shows MAE decreases with k; kNN outperforms at extreme compression (k=10), RF better at k≥100
- Number of source models: Table 2c shows .969 with 100 models → .987 with 382 models; diminishing returns
- Stratification vs. pure PDS: Table 2b shows stratification by task (.978) underperforms pure PDS selection (.987) on MMLU
- Dimensionality reduction: PCA-256 optimal; UMAP underperforms; no reduction causes severe overfitting

### Failure Signatures:
1. Temporal distribution shift: New architectures/training methods not in source pool → degraded accuracy
2. Insufficient source model diversity: <100 source models → poor generalization to target models
3. Missing PCA step: Direct use of high-dimensional signatures → overfitting to source models

### First 3 Experiments:
1. Reproduce MMLU compression (k=100): Use provided code, train on pre-Jan 2024 models, test on post-Jan 2024 models; target ~1.07% MAE, .987 rank correlation
2. Ablate PDS vs. Random selection: Compare (Random samples + RF predictor) vs. (PDS samples + RF predictor) on held-out models to isolate PDS contribution
3. Stress test temporal robustness: Train on models from Q1-Q3 2023, evaluate on Q4 2023 models; quantify degradation rate to inform retraining cadence

## Open Questions the Paper Calls Out
1. How can the selection mechanism be adapted to remain robust under significant distribution shifts, such as the emergence of new model architectures not represented in the source pool?
2. Can the "model signature" approach be effectively translated to generative tasks where model outputs are free-form text rather than bounded probability vectors?
3. Does the Predictive Diversity Score (PDS) remain an optimal selection criterion when the pool of source models is small or highly correlated?

## Limitations
- Performance degrades when target models use fundamentally different architectures or training paradigms than source models
- Requires a diverse pool of 382+ source models for optimal performance
- Initial computational cost of running all source models on full benchmark can be prohibitive for very large benchmarks

## Confidence
**High Confidence**:
- DISCO achieves superior efficiency-precision trade-offs compared to existing methods
- PDS-based sample selection outperforms random or task-stratified selection
- Model signature representation with PCA outperforms scalar accuracy or latent parameter approaches

**Medium Confidence**:
- Theoretical optimality of disagreement-based selection (relies on assumptions that may not hold in practice)
- Generalization to unseen architectures (supported by experiments but temporal robustness unclear)

**Low Confidence**:
- Long-term viability as ML architectures continue evolving
- Performance on extremely small sample budgets (k=10) where kNN outperforms RF

## Next Checks
1. Temporal Robustness Test: Train DISCO on models from Q1-Q3 2023, evaluate on Q4 2023 models, and measure degradation rate. Compare against baselines to quantify how quickly the source model pool becomes stale.
2. Architecture Gap Analysis: Systematically test DISCO's performance when source models are from one architecture family and target models are from different families. Measure the drop in accuracy to establish boundaries of applicability.
3. Minimal Source Model Pool: Conduct an ablation study varying the number of source models from 50 to 500 to identify the minimum viable pool size for maintaining >95% of peak performance.