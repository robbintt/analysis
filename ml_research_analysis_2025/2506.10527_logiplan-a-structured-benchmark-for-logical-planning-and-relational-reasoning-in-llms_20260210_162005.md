---
ver: rpa2
title: 'LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning
  in LLMs'
arxiv_id: '2506.10527'
source_url: https://arxiv.org/abs/2506.10527
tags:
- task
- reasoning
- relations
- number
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LOGI PLAN is a new benchmark for evaluating LLMs on logical planning
  and relational reasoning. It tests models' abilities to generate consistent relational
  graphs, detect inconsistencies like cycles, and verify relationships in structured
  data.
---

# LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs

## Quick Facts
- arXiv ID: 2506.10527
- Source URL: https://arxiv.org/abs/2506.10527
- Authors: Yanan Cai; Ahmed Salem; Besmira Nushi; Mark Russinovich
- Reference count: 40
- Primary result: LogiPlan evaluates LLMs on logical planning and relational reasoning, showing reasoning models significantly outperform instruction-tuned models, though all struggle with larger, more complex graphs.

## Executive Summary
LogiPlan is a new benchmark designed to evaluate large language models (LLMs) on logical planning and relational reasoning. The benchmark tests models' abilities to generate consistent relational graphs, detect inconsistencies like cycles, and verify relationships in structured data. Tasks are dynamically generated with adjustable complexity. Across nine state-of-the-art models, reasoning models like O1 and O3-mini significantly outperformed others in plan generation and consistency detection, though all models struggled with larger, more complex graphs. Instruction-tuned models often failed to maintain consistency or avoid duplicates as task difficulty increased. Self-correction improved accuracy for most models, especially reasoning ones, highlighting the value of iterative refinement. Overall, the results show a clear gap between model types and emphasize the challenges LLMs face in scalable logical reasoning.

## Method Summary
LogiPlan systematically evaluates LLMs on logical planning and relational reasoning through dynamically generated tasks. The benchmark assesses models' ability to construct consistent relational graphs, detect inconsistencies such as cycles, and verify relationships within structured data. Tasks are designed with adjustable complexity to test performance across different difficulty levels. The evaluation framework includes plan generation, consistency detection, and self-correction mechanisms, with nine state-of-the-art models tested under controlled conditions. Results are measured using accuracy metrics for plan generation, consistency detection, and duplicate avoidance across varying graph sizes.

## Key Results
- Reasoning models (O1, O3-mini) significantly outperformed instruction-tuned models in plan generation and consistency detection
- All models struggled with larger, more complex graphs, showing performance degradation as task difficulty increased
- Self-correction mechanisms improved accuracy for most models, particularly reasoning models, demonstrating the value of iterative refinement

## Why This Works (Mechanism)
LogiPlan works by providing a controlled environment where LLMs must demonstrate structured logical reasoning through graph-based tasks. The benchmark isolates key reasoning capabilities - consistency maintenance, cycle detection, and relationship verification - that are essential for logical planning. By generating tasks dynamically with adjustable complexity, the benchmark can systematically evaluate model performance across different difficulty levels. The inclusion of self-correction mechanisms allows assessment of models' ability to refine their reasoning through iteration. The clear performance gaps between reasoning and instruction-tuned models suggest that explicit reasoning training provides measurable advantages in structured logical tasks.

## Foundational Learning

**Logical Planning**: Why needed: Core capability for structured reasoning and goal-oriented problem solving. Quick check: Can the model generate valid step-by-step plans from initial conditions to goals?

**Relational Graph Construction**: Why needed: Fundamental for representing and reasoning about structured relationships. Quick check: Does the model create consistent, non-contradictory relationship networks?

**Cycle Detection**: Why needed: Essential for identifying logical inconsistencies and infinite loops. Quick check: Can the model identify circular dependencies in relationship chains?

**Consistency Verification**: Why needed: Critical for maintaining logical coherence across complex structures. Quick check: Does the model detect contradictions between established relationships?

**Self-Correction Mechanisms**: Why needed: Enables iterative refinement and error recovery in reasoning processes. Quick check: Does iterative prompting improve output accuracy and consistency?

## Architecture Onboarding

**Component Map**: Task Generator -> Model Interface -> Plan Generator -> Consistency Checker -> Self-Correction Module -> Evaluation Metrics

**Critical Path**: Dynamic task generation → Plan generation → Consistency verification → Self-correction (optional) → Performance evaluation

**Design Tradeoffs**: Flexibility vs. complexity (adjustable task difficulty increases evaluation richness but computational cost), accuracy vs. efficiency (self-correction improves accuracy but increases inference time)

**Failure Signatures**: Duplicate relationships in generated plans, undetected cycles, logical inconsistencies between relationship assertions, performance degradation with graph size increase

**First Experiments**:
1. Test baseline performance on simple relational graphs with 5-10 nodes
2. Evaluate consistency detection on graphs with known cycles of varying lengths
3. Compare single-pass vs. multi-pass self-correction on medium-complexity tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark uses controlled, synthetically generated data rather than real-world relational structures, limiting practical applicability
- Self-correction improvements tested with single iterative approach without exploring alternative refinement strategies
- No investigation of transfer learning capabilities - whether models can generalize to unseen relational structures or domains

## Confidence

**High confidence**: Performance differences between reasoning and instruction-tuned models, effectiveness of self-correction for most models

**Medium confidence**: Difficulty scaling to larger graphs, specific failure modes in duplicate avoidance and consistency maintenance

**Low confidence**: Generalizability to real-world relational reasoning tasks, long-term retention of learned reasoning capabilities

## Next Checks

1. Test model performance on LogiPlan tasks using real-world knowledge graphs from domains like biomedical ontologies or social networks to assess practical applicability

2. Evaluate transfer learning by training models on subset of LogiPlan tasks and testing on structurally different but conceptually related reasoning problems

3. Compare LogiPlan results with human performance on identical tasks to establish baseline expectations and identify whether current model limitations represent fundamental barriers or solvable engineering challenges