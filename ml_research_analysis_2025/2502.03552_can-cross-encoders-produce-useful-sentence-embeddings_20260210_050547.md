---
ver: rpa2
title: Can Cross Encoders Produce Useful Sentence Embeddings?
arxiv_id: '2502.03552'
source_url: https://arxiv.org/abs/2502.03552
tags:
- retrieval
- layer
- used
- sentence
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The prevailing approach uses cross encoders for reranking and dual
  encoders for initial retrieval, as cross encoders are too slow for initial search.
  The paper shows that embeddings from earlier layers of cross encoders can be used
  for search, often outperforming dual encoder embeddings.
---

# Can Cross Encoders Produce Useful Sentence Embeddings?

## Quick Facts
- **arXiv ID**: 2502.03552
- **Source URL**: https://arxiv.org/abs/2502.03552
- **Reference count**: 9
- **Primary result**: Embeddings from earlier layers of cross encoders can be used for search, often outperforming dual encoder embeddings, enabling faster models with minimal accuracy loss.

## Executive Summary
Cross encoders (CEs) are highly accurate for sentence pair tasks but too slow for initial retrieval due to O(n) pair computations. This paper shows that embeddings from earlier layers of CEs can be extracted and used for information retrieval, often outperforming traditional dual encoder (DE) embeddings. By transferring CE early-layer weights to a shallow DE (2 layers), the approach achieves 5.15x speedup while maintaining similar accuracy. This enables training fast DEs without complex knowledge distillation procedures for creating hard negatives, simplifying the pipeline while preserving performance.

## Method Summary
The approach involves extracting embeddings from earlier layers of cross encoders by pairing a sentence with itself and mean-pooling tokens. For knowledge infusion, weights from the embedding layer and encoder layer 0 of a CE are copied to a 2-layer DE, with layer 1 randomly initialized. The DE is then trained using only BM-25 hard negatives with standard contrastive loss. Layer-wise analysis shows CE early layers contain stronger retrieval signals than DE early layers, despite both sharing the same pretrained base model. This weight transfer eliminates the need for complex distillation pipelines while achieving similar accuracy to deeper DEs.

## Key Results
- CE early-layer embeddings outperform DE early-layer embeddings by 29.5% on average across 10 datasets for ms-marco model pairs
- DE-2 CE achieves 5.15x speedup compared to 12-layer baseline DE with <1% accuracy drop on 6/12 datasets after reranking
- CE-infused DEs match baseline accuracy without complex knowledge distillation procedures for creating hard negatives
- The 2-layer DE with CE-infused weights clearly outperforms the same architecture with random initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Earlier layers of cross encoders contain usable sentence representations for retrieval tasks.
- **Mechanism:** Cross encoders are fine-tuned from pretrained models that already encode sentence semantics. During CE fine-tuning, early layers preserve generalizable sentence representations while later layers specialize toward computing pair-wise differences for classification. When extracting embeddings from early layers (by pairing a sentence with itself and mean-pooling tokens), these representations remain effective for similarity search.
- **Core assumption:** The pretrained base model contains transferable sentence-level semantics that survive CE fine-tuning in early layers.
- **Evidence anchors:** [abstract] "embeddings from earlier layers of CEs can in fact be used within an information retrieval pipeline"; [section 1] "hidden states of earlier layers of the CE contain a good signal for sentence representations for IR, but later layers do not, presumably because later hidden states reflect the computation of differences between the pair of sentences"
- **Break condition:** If later layers are required for the target task (e.g., complex reasoning), or if the CE was trained with objectives that modify early representations, this mechanism degrades.

### Mechanism 2
- **Claim:** Cross encoder training concentrates IR-relevant information in earlier layers more effectively than dual encoder training.
- **Mechanism:** The pair-wise classification objective in CE training may force the model to extract semantic features early, as it must ultimately compare two sentences. In contrast, DE training with contrastive loss distributes representation learning across all layers to optimize embedding space geometry. This results in CE early layers outperforming DE early layers for retrieval, despite identical pretrained initialization.
- **Core assumption:** Training objective differences (classification vs. contrastive) cause distinct layer-wise feature distributions.
- **Evidence anchors:** [section 1] "hidden states of the earlier layers of the DE are significantly worse than those of the CE for the same dataset, despite both sharing the same base pretrained model"; [section 4.3] "hidden states from the DE's embedding layer were substantially worse than that of the CE by an average of 29.5% across the 10 datasets for the ms-marco model pairs (paired two-tailed t test was significant, p < .01)"
- **Break condition:** If DE architectures use different pretraining or training regimens that explicitly target early-layer quality, this gap may narrow or reverse.

### Mechanism 3
- **Claim:** Transferring CE early-layer weights to a shallow DE provides a stronger initialization than random weights, reducing training complexity.
- **Mechanism:** The CE's early layers encode IR-relevant features learned through its supervised pair-wise training. By copying these weights (embedding layer + encoder layer 0) to a 2-layer DE and training only with BM-25 hard negatives (avoiding complex distillation pipelines), the DE inherits useful representations. Random initialization requires learning these features from scratch with less signal.
- **Core assumption:** Early-layer CE features are architecture-agnostic and transfer effectively to the DE's independent encoding paradigm.
- **Evidence anchors:** [section 3.2] "Weights from the embedding layer in BERT, which converts each input token to an input-vector embedding, and layer 0 from the BERT encoder are both copied into a DE model from the CE. Encoding layer 1 was initialized to a set of random weights"; [section 4.4] "DE-2 CE is clearly better than DE-2 Rand... The difference between DE-2 CE after reranking... was at least within 1% of the 12 layered baseline DE with reranking on 6/12 datasets"
- **Break condition:** If the target domain differs significantly from CE training data, or if the DE task requires representations the CE never learned, weight transfer provides minimal benefit.

## Foundational Learning

- **Concept: Cross Encoder vs. Dual Encoder Architectures**
  - **Why needed here:** The paper's contribution hinges on understanding why CEs are accurate but slow (O(n) pair computations at inference) while DEs are fast (precomputed embeddings, O(1) similarity) but harder to train.
  - **Quick check question:** Given a corpus of 1M documents and a query, why can't a CE be used for initial retrieval without prohibitive cost?

- **Concept: Layer-wise Representations in Transformers**
  - **Why needed here:** The core finding depends on recognizing that transformer layers encode different levels of abstraction—early layers capture general semantics, later layers specialize to task-specific computations.
  - **Quick check question:** If CE later layers "reflect computation of differences between sentence pairs," what would you expect if you extracted embeddings from layer 11 of a 12-layer CE?

- **Concept: Knowledge Distillation vs. Knowledge Infusion**
  - **Why needed here:** The paper distinguishes its approach (direct weight copying, "infusion") from traditional distillation (using teacher outputs as soft labels or for hard negative mining). Understanding this distinction clarifies why training simplifies.
  - **Quick check question:** Traditional DE distillation often requires CE-generated hard negatives. What does this paper use instead, and why does it work?

## Architecture Onboarding

- **Component map:**
Cross Encoder (source, 12 layers)
├── Embedding layer ──────────────┐
├── Encoder Layer 0 ─────────────┼──► Copy to ──► Dual Encoder (target, 2 layers)
├── Encoder Layers 1-11 (unused) │                  ├── Embedding layer (from CE)
└── Classification head (unused) │                  ├── Encoder Layer 0 (from CE)
                                   │                  ├── Encoder Layer 1 (random init)
                                   └── Pooling → Embedding output
                                                              │
                                   Training: Standard DE loss + BM-25 negatives only

- **Critical path:**
1. Select CE-DE pair sharing the same pretrained base (e.g., MiniLM-L-12)
2. Verify CE early layers outperform DE early layers on validation data (layer-wise analysis)
3. Copy embedding layer + encoder layer 0 weights from CE to new 2-layer DE
4. Initialize encoder layer 1 randomly
5. Train DE with contrastive loss using only BM-25 hard negatives
6. Evaluate retrieval metrics (Hits@10, MRR@10) with optional CE reranking

- **Design tradeoffs:**
- **Layer count:** 2 layers → 5.15x speedup, ~1% accuracy drop on 6/12 datasets; 12 layers → baseline accuracy, no speedup
- **Training complexity:** BM-25 negatives only vs. CE-distilled hard negatives (simpler but potentially less robust on out-of-domain data)
- **Model size:** 15M parameters (DE-2-CE) vs. 33M (baseline DE)

- **Failure signatures:**
- CE early layers underperform DE early layers on target task → transfer provides no benefit; revert to standard DE
- DE-2-CE significantly underperforms DE-2-Rand → weight transfer harmful; check for architecture mismatch or layer selection error
- Large accuracy gaps after reranking → 2-layer model too shallow for task; consider 4-6 layer variants
- Speedup < 3x → bottleneck elsewhere (I/O, tokenization); profile inference pipeline

- **First 3 experiments:**
1. **Layer-wise extraction benchmark:** For your CE-DE pair, extract and evaluate embeddings from each layer (0-11) on 2-3 validation datasets. Confirm CE early layers ≥80% of final DE performance before proceeding.
2. **Ablation: DE-2-Rand vs. DE-2-CE:** Train both with identical hyperparameters. The delta measures infusion benefit (paper shows clear improvement).
3. **Inference latency profile:** Measure end-to-end retrieval time for DE-12 (baseline) vs. DE-2-CE on a realistic corpus (100K+ documents). Confirm ≥4x speedup; if not, identify bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do early layers of CEs contain stronger retrieval signals than early DE layers when both share the same pretrained base model?
- **Basis in paper:** [explicit] The paper calls this a "curious finding" and states "It appears that CEs extract information relevant for information retrieval in earlier layers than DEs" without providing a theoretical explanation.
- **Why unresolved:** The paper empirically demonstrates the phenomenon (CE early layers outperform DE early layers by ~29% on Hits@10) but offers only speculation that "later hidden states reflect the computation of differences between the pair of sentences."
- **What evidence would resolve it:** Layer-wise probing experiments analyzing what linguistic and semantic properties are captured at each layer, or attention pattern analysis comparing early CE vs. DE representations.

### Open Question 2
- **Question:** What is the optimal architecture and layer transfer strategy for CE-to-DE knowledge infusion?
- **Basis in paper:** [explicit] The authors acknowledge "There is a huge range of architectures for infusing a DE with CE knowledge. We explored a small fraction of that space, and not exhaustively."
- **Why unresolved:** The paper only tested copying embedding + layer 0 from CE to a 2-layer DE, mentioning they "tried copying 2 encoding layers or no encoding layers at all" but did not systematically explore the design space.
- **What evidence would resolve it:** Ablation studies varying the number of transferred layers, the total DE depth, and different fusion strategies across multiple model families.

### Open Question 3
- **Question:** Do CE-derived embeddings transfer effectively to languages beyond English?
- **Basis in paper:** [explicit] "Finally, we note that all of our work was done on English text, and we did not study how it applies in other languages."
- **Why unresolved:** Multilingual models may exhibit different layer-wise behavior due to language-specific fine-tuning or architectural adaptations.
- **What evidence would resolve it:** Experiments on the same CE/DE pairs fine-tuned for multilingual tasks, evaluated on non-English IR benchmarks.

## Limitations

- **Layer transferability generalization:** The approach is demonstrated only for MiniLM-L-12 CE and 2-layer DE, with unclear generalization to other base models or DE depths.
- **Training signal sufficiency:** Using only BM-25 hard negatives may not provide sufficient signal for all domains, potentially sacrificing robustness for training simplicity.
- **Inference pipeline bottlenecks:** The 5.15x speedup assumes ideal conditions; real-world bottlenecks from tokenization, I/O, or batch processing may diminish theoretical advantages.

## Confidence

**High Confidence:** The core finding that CE early-layer embeddings outperform DE early-layer embeddings for retrieval is well-supported by layer-wise analysis across 10 datasets. The 29.5% average improvement and statistical significance (p < .01) provide strong empirical evidence.

**Medium Confidence:** The claim that CE early-layer weights can be successfully transferred to DE with minimal accuracy loss is supported but requires more extensive validation. The 6/12 datasets showing <1% accuracy drop after reranking is promising but leaves questions about the other datasets.

**Low Confidence:** The assertion that this approach eliminates the need for complex knowledge distillation pipelines is premature. The paper shows success with simple BM-25 negatives, but long-term performance across diverse domains and tasks remains unproven.

## Next Checks

1. **Cross-Model Architecture Transfer:** Apply the same weight infusion approach using different base models (BERT, RoBERTa, other MiniLM variants) and varying DE layer counts (4, 6, 8 layers). Measure whether the early-layer advantage and transfer benefits persist across architectures, or if they are specific to the MiniLM-L-12 combination.

2. **Domain Generalization Stress Test:** Evaluate DE-2 CE performance on datasets significantly different from MS MARCO (e.g., scientific literature, legal documents, social media). Compare against DE models trained with CE-generated hard negatives to quantify the robustness tradeoff between training simplicity and retrieval quality.

3. **End-to-End Production Profiling:** Implement a realistic retrieval pipeline with the DE-2 CE model on a large corpus (100K+ documents) using production hardware and batch sizes. Profile end-to-end latency (including tokenization, indexing, search) to verify the 5x speedup holds under realistic deployment conditions, and identify potential bottlenecks that could negate theoretical advantages.