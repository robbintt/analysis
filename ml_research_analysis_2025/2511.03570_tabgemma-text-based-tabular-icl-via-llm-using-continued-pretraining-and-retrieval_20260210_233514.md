---
ver: rpa2
title: 'TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval'
arxiv_id: '2511.03570'
source_url: https://arxiv.org/abs/2511.03570
tags:
- tabular
- regression
- classification
- tasks
- carte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TabGemma, a schema-agnostic in-context learner
  for tabular prediction that treats rows as sequences and handles mixed text, numeric,
  and categorical fields. The method tackles two key challenges: unstable numeric
  tokenization and limited context size.'
---

# TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval
## Quick Facts
- arXiv ID: 2511.03570
- Source URL: https://arxiv.org/abs/2511.03570
- Reference count: 40
- Key outcome: TabGemma establishes new SOTA on classification across low- and high-data regimes and improves monotonically with more context rows on semantically rich tabular benchmarks.

## Executive Summary
TabGemma is a schema-agnostic in-context learner for tabular prediction that treats rows as sequences and handles mixed text, numeric, and categorical fields. It addresses two key challenges: unstable numeric tokenization and limited context size. The method canonicalizes numbers via signed scientific notation and continues pretraining a 12B Gemma 3 model with a target imputation objective using a large-scale real-world dataset. For inference, it retrieves informative exemplars using compact n-gram-based retrieval to fit within a 128k-token window.

On semantically rich benchmarks, TabGemma establishes a new state of the art on classification across low- and high-data regimes and improves monotonically with more context rows. For regression, it is competitive at small sample sizes but trails conventional approaches as data grows. The results show that LLMs can be effective tabular in-context learners on highly semantic tasks when paired with dedicated numeric handling and context retrieval.

## Method Summary
TabGemma treats tabular data as text sequences, converting each row into a tokenized format where numerical values are represented in signed scientific notation to address tokenization instability. The model continues pretraining on a large-scale real-world tabular dataset using a target imputation objective, learning to predict missing values given context. During inference, n-gram-based retrieval selects informative exemplars to maximize context window utilization within a 128k-token limit. The approach is schema-agnostic, requiring no explicit feature mapping or structured understanding, and relies entirely on semantic patterns within the data.

## Key Results
- Establishes new SOTA on classification across low- and high-data regimes on semantically rich benchmarks
- Improves monotonically with more context rows, showing better performance with increased exemplar diversity
- Competitive regression performance at small sample sizes but trails conventional methods as dataset size increases

## Why This Works (Mechanism)
TabGemma works by transforming tabular data into a text format that LLMs can process while addressing the core challenges of numerical instability and context limitations. By canonicalizing numbers into signed scientific notation, it creates stable tokenization that preserves numerical information while avoiding the variability of standard decimal representations. The continued pretraining on real-world tabular data teaches the model to understand semantic patterns within cell values rather than just surface-level correlations. The n-gram-based retrieval system intelligently selects exemplars that maximize information density within the context window, allowing the model to learn from diverse examples without exceeding token limits. This combination of numerical stability, semantic understanding, and efficient context utilization enables effective in-context learning on tasks where the meaning within cells matters more than the structured relationships between features.

## Foundational Learning
- **Signed Scientific Notation**: Numerical values are converted to a canonical form (e.g., 1234.56 → +1.23456e3) to eliminate tokenization instability and ensure consistent representation across different numerical formats.
- **Target Imputation Objective**: The model learns to predict missing values given context during continued pretraining, teaching it to understand semantic relationships within tabular data rather than just memorizing patterns.
- **N-gram-based Retrieval**: Uses compact text-based matching to select informative exemplars that fit within the 128k-token context window while maximizing semantic diversity and relevance.
- **Schema-agnostic Processing**: Treats all tabular data as text sequences without requiring explicit feature mapping or structured understanding, making it flexible but potentially vulnerable to ordering sensitivity.
- **In-context Learning**: Leverages exemplars within the context window to perform predictions without parameter updates, relying on the model's ability to generalize from semantic patterns in the examples.

## Architecture Onboarding
**Component Map**: Tabular data -> Scientific Notation Conversion -> Text Sequence Formation -> N-gram Retrieval -> Context Window Assembly -> Continued Pretrained Gemma Model -> Prediction Output

**Critical Path**: Data canonicalization → Exemplar retrieval → Context assembly → Prediction generation

**Design Tradeoffs**: 
- Schema-agnostic flexibility vs. loss of structured feature understanding
- Numerical stability via canonicalization vs. potential information loss in standard tokenization
- Retrieval-based exemplar selection vs. comprehensive but token-limited context coverage

**Failure Signatures**: 
- Regression performance degradation with increasing dataset size
- Sensitivity to column ordering due to lack of structured feature understanding
- Potential numerical bias from canonicalization approach

**First 3 Experiments**:
1. Compare canonicalized vs standard numerical tokenization on purely numerical regression tasks
2. Systematically permute column orders to measure impact on classification accuracy
3. Characterize regression performance scaling across dataset sizes from 10 to 10,000 samples

## Open Questions the Paper Calls Out
None

## Limitations
- Regression performance degrades as dataset size increases, trailing conventional methods
- Schema-agnostic design trades away structured feature understanding for flexibility
- Numerical canonicalization represents a departure from standard LLM tokenization that may introduce biases

## Confidence
- Classification results: High confidence—multiple benchmarks, consistent outperformance across data regimes
- Regression results: Medium confidence—performance degrades with scale in ways that warrant further investigation
- Schema-agnostic claim: Medium confidence—demonstrated but at the cost of structured feature understanding
- Numerical handling efficacy: Medium confidence—innovative but introduces non-standard tokenization that needs validation

## Next Checks
1. **Regression scaling analysis**: Conduct controlled experiments across dataset sizes from 10 to 10,000 samples to characterize the exact point and nature of performance degradation relative to baseline methods.

2. **Numerical information retention test**: Design experiments comparing TabGemma's canonicalized number representations against standard tokenization on purely numerical tasks to quantify any information loss or bias introduction.

3. **Ordering sensitivity validation**: Systematically permute feature column orders in tabular data to measure impact on prediction accuracy, testing the claim of true schema-agnosticism under varying input structures.