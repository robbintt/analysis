---
ver: rpa2
title: 'InstructAudio: Unified speech and music generation with natural language instruction'
arxiv_id: '2511.18487'
source_url: https://arxiv.org/abs/2511.18487
tags:
- speech
- control
- music
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InstructAudio, the first unified framework
  for instruction-controlled speech and music generation. The core innovation is a
  multimodal diffusion transformer (MM-DiT) architecture that uses natural language
  descriptions to control acoustic attributes across both tasks.
---

# InstructAudio: Unified speech and music generation with natural language instruction

## Quick Facts
- arXiv ID: 2511.18487
- Source URL: https://arxiv.org/abs/2511.18487
- Reference count: 0
- Unified instruction-controlled speech and music generation via natural language descriptions

## Executive Summary
This paper introduces InstructAudio, the first unified framework for instruction-controlled speech and music generation. The core innovation is a multimodal diffusion transformer (MM-DiT) architecture that uses natural language descriptions to control acoustic attributes across both tasks. By standardizing input as instruction-phoneme pairs and leveraging joint and single diffusion transformer layers, the model achieves unified text-based control over timbre, emotion, style, genre, instrumentation, and more. Trained on 50K hours of speech and 20K hours of music, InstructAudio achieves state-of-the-art performance in instruction-based TTS while maintaining competitive TTM results.

## Method Summary
InstructAudio employs a multimodal diffusion transformer architecture that unifies speech and music generation through natural language instruction control. The model uses 14 joint and 6 single diffusion transformer layers based on Stable Diffusion 3, with 1.34B parameters. Input is standardized as instruction-phoneme pairs, where instructions describe desired acoustic attributes and phonemes encode the text/lyrics. The architecture incorporates a frozen Qwen2.5-7B instruct encoder and Zipformer phoneme encoder, with continuous latents generated via a frozen Mel-VAE codec at 43Hz. Training uses conditional flow matching on 70K hours of audio (50K speech, 20K music) with 32× A800 80GB GPUs, achieving unified control over speaker characteristics, emotions, genres, and musical attributes without requiring reference audio.

## Key Results
- Achieves state-of-the-art instruction-based TTS performance (best WER, speaker similarity, emotion similarity, and classification control accuracy)
- Maintains competitive text-to-music performance (best SongEval metrics including Coherence, Musicality, Memorability, Clarity, Naturalness)
- Successfully supports dialogue generation and eliminates reference audio requirements

## Why This Works (Mechanism)
The unification succeeds through standardized instruction-phoneme input representation that creates a shared latent space for both speech and music. The MM-DiT architecture's joint layers learn cross-modal alignment between acoustic patterns and language descriptions, while single layers specialize for each domain. The conditional flow matching objective preserves the structure of both speech and music distributions during generation. By using continuous latents from Mel-VAE instead of raw waveforms, the model captures high-level acoustic patterns while maintaining efficient generation. The frozen instruct encoder provides rich semantic representations that map naturally to acoustic attributes across domains.

## Foundational Learning
- **Multimodal Diffusion Transformers**: Neural architectures that combine diffusion models with transformer layers for cross-modal generation. Needed to handle the complex relationships between language instructions and audio patterns. Quick check: Verify joint attention mechanisms properly align text and audio features.
- **Conditional Flow Matching**: Training objective that conditions generation on input features while preserving target distribution structure. Needed to maintain realistic speech and music distributions during instruction-based control. Quick check: Monitor reconstruction loss on held-out data.
- **Continuous Latents**: Compressed audio representations (43Hz in this case) that preserve essential acoustic information while enabling efficient generation. Needed to reduce computational complexity while maintaining audio quality. Quick check: Compare generated audio quality at different latent resolutions.
- **Instruction-Encoding**: Process of converting natural language descriptions into structured representations that control generation attributes. Needed to enable text-based control over complex acoustic parameters. Quick check: Test instruction parsing accuracy across different attribute types.
- **Phoneme Encoding**: Conversion of text/lyrics into phonetic representations for generation. Needed to provide precise pronunciation control while maintaining language flexibility. Quick check: Verify phoneme-to-audio alignment accuracy.

## Architecture Onboarding

**Component Map**: Natural Language Instruction → Qwen2.5-7B Encoder → MM-DiT (14 Joint + 6 Single Layers) → Mel-VAE Decoder → Audio Output

**Critical Path**: The critical path runs from instruction-phoneme input through the MM-DiT layers to the Mel-VAE decoder. The joint layers (14) handle cross-modal alignment between instructions and audio patterns, while single layers (6) specialize for speech/music-specific generation. The frozen instruct encoder and Mel-VAE ensure stable semantic and acoustic representations throughout training.

**Design Tradeoffs**: The unified approach trades some task-specific optimization for cross-modal flexibility. Using continuous latents reduces computational cost but may limit fine-grained waveform details. The 1.34B parameter size balances performance with efficiency, though larger models might achieve better quality. Text-only control eliminates reference audio complexity but may introduce ambiguity in one-to-many mappings.

**Failure Signatures**: Poor classification control accuracy indicates instruction-description misalignment. Lower MOS scores compared to reference-audio baselines suggest text-only control limitations. Degraded performance on rare instruction combinations reveals insufficient training coverage. Music generation artifacts on extended durations indicate the short-clip optimization constraint.

**3 First Experiments**:
1. Train a reduced-scale model (256M parameters) on a 10K-hour subset to verify MM-DiT architecture functionality before full-scale training.
2. Test cross-lingual instruction control by generating audio from mixed Chinese-English instructions to validate unified representation consistency.
3. Evaluate generation quality on audio segments longer than 20 seconds to establish performance degradation patterns for long-form content.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can multimodal control mechanisms effectively mitigate the quality degradation caused by text-only one-to-many mapping ambiguity? (Section 4 identifies this as future work; the current text-only approach inherently loses acoustic detail compared to reference audio.)
- **Open Question 2**: Can the framework scale to support coherent long-form music generation while maintaining unified alignment with speech? (Section 3.4 notes the model constrains music to 5-20s to match speech durations, and Section 4 lists "support longer music generation" as future work.)
- **Open Question 3**: Can sound effect generation be integrated into the unified framework without disrupting the established cross-modal alignment? (Section 4 identifies "incorporate sound effect generation" as a specific goal for future work.)

## Limitations
- Critical dependency on Mel-VAE codec details not fully specified, making exact reproduction challenging
- Model optimized for short audio clips (2-20s), limiting applicability to longer-form content generation
- 1.34B parameter size may limit performance on more complex audio generation tasks compared to larger models

## Confidence
- **High confidence**: Unified MM-DiT architecture design and cross-modal generation capability
- **Medium confidence**: State-of-the-art benchmark performance claims (limited sample sizes, specific evaluation frameworks)
- **Medium confidence**: Elimination of reference audio requirement (potential quality trade-offs warrant investigation)

## Next Checks
1. Reproduce the unified generation capability by training a smaller-scale version (e.g., 256M parameters) on a subset of the data to verify core MM-DiT architecture functionality
2. Evaluate cross-lingual instruction control by testing the model with mixed Chinese-English instructions and assessing unified representation semantic consistency
3. Test generalization to longer audio segments by extending input durations beyond 20 seconds and measuring performance degradation patterns