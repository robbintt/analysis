---
ver: rpa2
title: Composable Building Blocks for Controllable and Transparent Interactive AI
  Systems
arxiv_id: '2506.02262'
source_url: https://arxiv.org/abs/2506.02262
tags:
- building
- blocks
- systems
- interactive
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to enhance the transparency
  and controllability of interactive AI systems by representing them as sequences
  of structural building blocks (e.g., AI models, control mechanisms) and explaining
  them through visual building blocks (e.g., XAI techniques). The method extends beyond
  individual model explanations to address system-level workflows, enabling both humans
  and automated agents like LLMs to understand and audit complex architectures.
---

# Composable Building Blocks for Controllable and Transparent Interactive AI Systems

## Quick Facts
- arXiv ID: 2506.02262
- Source URL: https://arxiv.org/abs/2506.02262
- Reference count: 33
- One-line primary result: Novel framework representing interactive AI systems as sequences of structural building blocks with corresponding visual building blocks for enhanced transparency and controllability

## Executive Summary
This paper addresses the challenge of making complex interactive AI systems transparent and controllable by introducing a composable building block approach. Rather than explaining individual AI models in isolation, the authors propose representing entire system architectures as sequences of structural building blocks (AI models, control mechanisms, data transformations) that can be explained through corresponding visual building blocks using XAI techniques. The framework enables both human users and automated agents like LLMs to understand, audit, and control complex AI workflows through a shared knowledge base. A 5-layer architecture is presented, extending beyond traditional model explanations to encompass entire system pipelines and workflows.

## Method Summary
The authors propose a 5-layer architecture that organizes interactive AI systems into structural building blocks defining the system pipeline, an API layer exposing functionality to automated agents, and visual building blocks providing explanations and control interfaces. The approach leverages existing XAI techniques (LIME, SHAP, What-If tools) but extends them to explain system-level workflows rather than just individual model predictions. The method includes control mechanisms such as filters and guards that can be explained and audited. A prototype heart disease prediction ensemble demonstrates the approach, showing how structural blocks like model ensembles and control mechanisms can be explained through visual interfaces. The framework aims to create a shared knowledge base where both humans and automated agents can understand and interact with AI systems in a unified way.

## Key Results
- Introduces a 5-layer architecture for representing interactive AI systems as composable building blocks
- Demonstrates how XAI techniques can be extended beyond individual models to explain entire system workflows
- Shows prototype implementation with heart disease prediction ensemble using LIME, SHAP, and What-If tools
- Proposes shared knowledge base concept enabling both human and LLM understanding of AI systems
- Addresses system-level transparency gaps in current XAI approaches

## Why This Works (Mechanism)
The approach works by decomposing complex AI systems into manageable structural components that map to corresponding visual explanations. This decomposition enables scalable transparency by allowing each component to be understood individually while maintaining awareness of the overall system workflow. The API layer serves as an interface for automated agents to query and understand system structure, while visual building blocks provide human-interpretable explanations. Control mechanisms like filters and guards add governance layers that can be audited alongside the AI models themselves. The shared knowledge base concept ensures consistency between human and machine interpretations of system behavior.

## Foundational Learning

**Structural Building Blocks** - Why needed: To decompose complex AI systems into manageable, explainable components
Quick check: Can identify all components in a multi-model AI pipeline and their relationships

**Visual Building Blocks** - Why needed: To provide human-interpretable explanations of structural components using XAI techniques
Quick check: Can map XAI outputs (LIME, SHAP) to specific structural components and explain their system-level impact

**Control Mechanisms** - Why needed: To add governance layers that can be audited alongside AI models
Quick check: Can identify and explain how filters, guards, and other control mechanisms affect system behavior

**API for Automated Agents** - Why needed: To enable LLMs and other agents to understand and interact with system structure
Quick check: Can query the API to extract complete system architecture and component relationships

**Shared Knowledge Base** - Why needed: To ensure consistency between human and machine interpretations of system behavior
Quick check: Can verify that human explanations and automated agent understanding align for the same system components

## Architecture Onboarding

Component map: Structural Building Blocks -> API Layer -> Visual Building Blocks -> Control Mechanisms -> User Interface

Critical path: System workflow decomposition → Structural block identification → Visual block mapping → Control mechanism integration → API exposure

Design tradeoffs: Granularity vs. complexity (finer decomposition improves explainability but increases system overhead), standardization vs. flexibility (rigid structures enable better automation but reduce adaptability), transparency vs. performance (additional explanation layers may impact system efficiency)

Failure signatures: Incomplete structural decomposition leading to opaque system portions, misalignment between structural and visual blocks causing explanation gaps, API limitations preventing automated agent understanding, control mechanism conflicts creating inconsistent behavior

First experiments:
1. Implement the 5-layer architecture with a simple two-model ensemble system
2. Test API queries with basic LLM agents to verify structural understanding
3. Conduct preliminary user study comparing comprehension between traditional documentation and building block approach

## Open Questions the Paper Calls Out
None

## Limitations
- No empirical validation through user studies or formal evaluation of comprehension improvement
- Scalability to large production systems with dozens or hundreds of components untested
- Integration challenges between XAI techniques and complex control mechanisms not fully addressed
- Practical implementation details for API standardization and automated agent interaction unclear

## Confidence

High confidence in: Identifying the genuine need for system-level transparency beyond individual model explanations in interactive AI systems

Medium confidence in: The logical framework of the 5-layer architecture as a potential solution for system transparency, though practical efficacy remains unproven

Low confidence in: Specific implementation details for visual building block integration and the claim that the approach enables effective shared understanding between humans and automated agents

## Next Checks

1. Conduct user studies comparing comprehension and control effectiveness between systems using the proposed building block approach versus traditional system documentation or existing XAI tools

2. Implement the framework with a larger, more complex AI system (e.g., multi-modal medical diagnosis system) to evaluate scalability and identify integration challenges

3. Develop and test standardized APIs for automated agent interaction, measuring whether LLMs can actually extract meaningful understanding and provide useful control suggestions through the proposed visual building block interfaces