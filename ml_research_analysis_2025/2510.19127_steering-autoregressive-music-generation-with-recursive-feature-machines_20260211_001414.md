---
ver: rpa2
title: Steering Autoregressive Music Generation with Recursive Feature Machines
arxiv_id: '2510.19127'
source_url: https://arxiv.org/abs/2510.19127
tags:
- steering
- control
- music
- generation
- probes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of controllable music generation
  using autoregressive models, specifically focusing on fine-grained control over
  musical attributes like notes and chords. The proposed method, MusicRFM, adapts
  Recursive Feature Machines (RFMs) to enable steering of frozen, pre-trained music
  models by directly manipulating internal activations.
---

# Steering Autoregressive Music Generation with Recursive Feature Machines

## Quick Facts
- arXiv ID: 2510.19127
- Source URL: https://arxiv.org/abs/2510.19127
- Reference count: 20
- Primary result: MusicRFM achieves note classification accuracy of 0.82 with CLAP score within 0.02 of baseline, demonstrating fine-grained control over music generation

## Executive Summary
This paper addresses the challenge of controllable music generation using autoregressive models, specifically focusing on fine-grained control over musical attributes like notes and chords. The proposed method, MusicRFM, adapts Recursive Feature Machines (RFMs) to enable steering of frozen, pre-trained music models by directly manipulating internal activations. RFMs identify interpretable "concept directions" within the model's activation space, corresponding to specific musical attributes.

The core idea is to train lightweight RFM probes to discover these concept directions in the model's hidden states, then inject them back during inference to guide the generation process. The method introduces layer-based control through top-K selection and exponential weighting, time-based control using dynamic schedules, and multi-direction control for simultaneous enforcement of multiple attributes.

## Method Summary
MusicRFM adapts Recursive Feature Machines to identify concept directions in MusicGen's activation space that correspond to musical attributes. The method trains lightweight kernel ridge regression probes on model activations labeled for target concepts (notes, chords, tempo), then computes the Average Gradient Outer Product (AGOP) matrix to extract orthogonal eigenvectors representing these concepts. During inference, these eigenvectors are injected into the residual stream through forward hooks with layer-wise weighting and stochastic gating. The approach enables fine-grained control without requiring model retraining or per-step optimization.

## Key Results
- Classification accuracy for target note increases from 0.23 (baseline) to 0.82 (steered)
- Text prompt adherence (CLAP score) remains within approximately 0.02 of unsteered baseline
- Layer pruning and exponential weighting reduce distributional drift (FD) from 0.599 to 0.225 while maintaining 88% accuracy
- Multi-direction steering achieves 0.92 accuracy for paired notes+chords but causes CLAP to drop to 0.13

## Why This Works (Mechanism)

### Mechanism 1: RFM-Derived Concept Directions via AGOP
RFM probes identify orthogonal, eigenvalue-ranked directions in activation space that correspond to musical attributes. Train lightweight kernel ridge regression probes on model activations labeled for target concepts. Compute the Average Gradient Outer Product (AGOP) matrix M = (1/n)Σgᵢgᵢᵀ from per-sample gradients. Eigendecomposition yields orthonormal directions q_j where eigenvalues λ_j measure model sensitivity to each direction. Iterate: train predictor → compute AGOP → recondition features with T^(t) = Q^(t)(Λ^(t))^α(Q^(t))^⊤.

### Mechanism 2: Residual Stream Injection via Forward Hooks
Injecting RFM-derived directions into the residual stream biases generation toward target concepts without per-step optimization. At inference, register forward hooks on selected layers S. For each timestep t and layer ℓ, modify hidden state: h'_{t,ℓ} = h_{t,ℓ} + η_ℓ(t)q_{ℓ,j*}. The steering vector q is broadcast to shape (1,1,d_ℓ) and added to all token positions.

### Mechanism 3: Layer Pruning and Stochastic Temporal Gating
Selective layer weighting and probabilistic injection reduce artifacts while preserving controllability. (1) Layer pruning: Rank layers by probe validation AUC, apply top-K selection or exponential weighting w_ℓ = w₀·ŝ_ℓ^(1/κ) where κ∈(0,1) concentrates weight on high-scoring layers. (2) Stochastic gating: At each step, apply steering with probability p (Bernoulli): h'_{t,ℓ} = h_{t,ℓ} + b_t·η_ℓ(t)·q_ℓ where b_t ~ Bernoulli(p).

## Foundational Learning

- **Concept: Average Gradient Outer Product (AGOP) and PSD Matrices**
  - Why needed here: RFM's core operation is computing M = (1/n)Σ∇_x f(x_i)∇_x f(x_i)^⊤. Understanding that this yields a positive semi-definite matrix with orthonormal eigenvectors is essential for interpreting concept directions.
  - Quick check question: Given per-sample gradients g₁=[1,0], g₂=[0,1], what is the AGOP matrix M and its eigenvalues?

- **Concept: Residual Stream Architecture in Transformers**
  - Why needed here: MusicRFM injects steering vectors directly into the residual stream h_{t,ℓ}. Understanding how residual connections accumulate information across layers explains why additive perturbations propagate to the output.
  - Quick check question: In a 48-layer transformer, if you add vector v to the residual stream at layer 10, which subsequent computations does it directly affect?

- **Concept: Autoregressive Generation with Neural Audio Codecs**
  - Why needed here: MusicGen operates on EnCodec tokens (discrete audio representations) autoregressively. Steering affects token predictions, which decode to audio. Understanding this pipeline clarifies where interventions can occur.
  - Quick check question: In an autoregressive model generating audio tokens, what happens if you modify the hidden state at timestep t=100 but not at earlier timesteps?

## Architecture Onboarding

- **Component map:**
  Audio Input → EnCodec (frozen) → MusicGen Transformer (48 layers, frozen) → Extract activations per layer → RFM Probe Training on SYNTHEORY → AGOP eigenvectors q_j per concept → At inference: Forward hooks inject q_j with η(t) weighting → Modified hidden states → Token predictions → Decode to audio

- **Critical path:**
  1. Probe training quality: Mean-pooled activations from MusicGen → RFM probes must achieve AUC > 0.7 on validation to yield useful steering directions. Last-token pooling fails (Table 1: 0.776 avg vs. 0.942 mean-pooled).
  2. Layer selection: Use validation AUC to rank layers; exponential weighting with κ=0.95 or top-K with K=12-16 balances control vs. quality.
  3. Coefficient tuning: η₀=0.15-0.45 provides meaningful control without excessive distributional drift (FD/MMD). η₀>0.6 risks artifacts.

- **Design tradeoffs:**
  - Control strength vs. prompt adherence: Higher η₀ increases probe accuracy (notes: 0.23→0.82) but reduces CLAP (0.32→0.30) and increases FD.
  - Layer coverage vs. artifact risk: Uniform 48-layer steering causes FD=0.599; top-K=12 achieves FD=0.225 with 88% accuracy.
  - Temporal precision vs. pooling: Mean pooling discards temporal ordering, limiting performance on progressions/scales (Table 1: 0.943-0.905 vs. 0.985 for tempo).
  - Multi-direction vs. distributional fidelity: Pairwise steering (notes+chords) achieves higher probe accuracy (0.92 at η₀=0.6) but FD doubles and CLAP drops to 0.13.

- **Failure signatures:**
  - Probe accuracy flat despite increasing η₀: Concept not linearly encoded; try different layers or temporal pooling strategies.
  - CLAP drops >0.1 with η₀<0.3: Steering vector may be misaligned; retrain probe with gradient centering (Eq. 6).
  - Audible artifacts (clicking, distortion): Over-steering; reduce p (stochastic gating) or apply more aggressive layer pruning.
  - Temporal controls don't follow schedule: Mean pooling dilutes timestep-specific information; consider attention pooling or sequence-level RFMs (Section 7).

- **First 3 experiments:**
  1. Replicate single-direction note steering: Train RFM probes on SYNTHEORY notes category using mean-pooled activations from layers 1-48. Extract top eigenvector, inject at η₀=0.3 with p=0.3 and exponential layer weighting. Measure probe accuracy and CLAP on 100 generations. Target: accuracy >0.5, CLAP within 0.03 of baseline.
  2. Ablate layer selection strategy: Compare uniform (K=48), top-K (K=8, 12, 16), and exponential weighting (κ=0.92, 0.95, 0.98) on the notes task. Plot FD/MMD/CLAP/Accuracy tradeoff curves. Identify Pareto-optimal configuration.
  3. Test temporal crossfade: Implement two-note crossfade (n₁→n₂) using complementary decay/rise schedules over 1500 steps. Use η₀=0.45 for both directions. Log per-timestep probe softmax scores for both notes. Verify that n₁ probability decays as n₂ rises, with crossover at ~750 steps. Measure FD, MMD, CLAP on 50 crossfaded generations.

## Open Questions the Paper Calls Out

- **Can temporally aware pooling strategies improve RFM performance on sequential musical attributes like chord progressions?**
  - Basis in paper: [explicit] The authors state in Section 7 that their probes rely on mean-pooled features which discard temporal ordering, limiting performance on concepts with strong sequential dependencies like scales and progressions.
  - Why unresolved: The current implementation treats audio segments as static vectors, failing to capture the time-evolving dynamics essential for sequential music theory concepts.
  - What evidence would resolve it: A comparative study evaluating classification accuracy and steering control between the current mean-pooling approach and proposed alternatives (e.g., attention pooling, recurrent aggregation) on progression datasets.

- **Do lower-ranked eigenvectors in the RFM AGOP matrix retain useful information for controlling fine-grained musical features?**
  - Basis in paper: [explicit] Section 7 notes that the study only utilizes the top eigenvector and acknowledges that extending to multiple components could capture richer subspaces, though variance analyses have not yet been performed.
  - Why unresolved: It is currently unknown if the variance captured by lower eigenvalues represents noise or interpretable sub-concepts that could enhance multi-faceted control.
  - What evidence would resolve it: Variance analysis of lower-order components and steering experiments using multiple eigenvectors to see if they enable more nuanced control without introducing artifacts.

- **Can activation steering effectively control perceptual or production-level attributes, such as timbre or articulation style?**
  - Basis in paper: [explicit] In Section 7, the authors note that experiments are currently limited to symbolic, music-theoretic concepts (notes, chords, tempo) and suggest extending RFMs to perceptual qualities as a future direction.
  - Why unresolved: The current reliance on the synthetic SYNTHEORY dataset restricts analysis to symbolic logic, leaving the method's applicability to subjective, timbral qualities unproven.
  - What evidence would resolve it: Training RFM probes on datasets labeled with timbre or instrument identity (e.g., MusicBench) and measuring the correlation between steering vector injection and perceptual changes in generated audio.

## Limitations

- The reliance on mean-pooling of hidden states limits performance on temporally structured concepts where sequential information is critical but discarded.
- Multi-direction steering (e.g., notes+chords) causes significant interference, with CLAP scores dropping to 0.13 and FD doubling.
- The method requires access to the frozen model's internal activations and gradients, limiting applicability to black-box APIs.

## Confidence

- **High Confidence:** The core mechanism of RFM-derived concept directions and their injection via forward hooks is well-specified and validated through multiple experiments.
- **Medium Confidence:** The layer pruning and stochastic gating techniques show empirical improvements, but the theoretical justification for specific hyperparameters is limited.
- **Low Confidence:** The temporal crossfade demonstrations are promising but rely on assumptions about linear interpolation in activation space that aren't validated.

## Next Checks

1. **Temporal Concept Validation:** Test RFM probes on temporal concepts (scales, progressions) using attention pooling instead of mean pooling. Compare probe accuracy and CLAP scores to identify whether the pooling strategy or concept representation is the limiting factor.

2. **Multi-Direction Interference Analysis:** For paired steering (notes+chords), measure the dot product between steering vectors and analyze whether orthogonality preservation breaks down under multi-direction injection. Test whether re-orthogonalization improves performance.

3. **Layer Contribution Mapping:** Systematically ablate individual layers (rather than top-K selection) to identify which specific layers contribute most to each concept. This would validate whether the layer pruning strategy is optimal or if certain layers are critical for multiple concepts.