---
ver: rpa2
title: 'Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time
  Scaling'
arxiv_id: '2512.19905'
source_url: https://arxiv.org/abs/2512.19905
tags:
- reward
- error
- preprint
- arxiv
- result
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a minimal analytically tractable model for
  inference-time scaling in large language models, using Bayesian linear regression
  with reward-weighted sampling. The key idea is to model the LLM-as-a-judge scenario
  where inference-time samples are generated and selected based on a reward function
  that may differ from the ground-truth teacher.
---

# Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling

## Quick Facts
- **arXiv ID**: 2512.19905
- **Source URL**: https://arxiv.org/abs/2512.19905
- **Reference count**: 40
- **Primary result**: Theoretical model reveals when inference-time scaling helps vs. hurts, with optimal sample count and temperature parameters validated on GSM8K

## Executive Summary
This paper presents a minimal analytically tractable model for inference-time scaling in LLM-as-a-judge scenarios using Bayesian linear regression with reward-weighted sampling. The model captures the trade-off between sample count and reward alignment, showing that generalization error can decrease monotonically with sample count when rewards are well-aligned, but may increase beyond an optimal point when rewards are poorly aligned. Experiments with Llama-3-8B and Mistral-7B on GSM8K validate these theoretical predictions, demonstrating the existence of optimal sample count and temperature values in realistic settings.

## Method Summary
The method models LLM-as-a-judge inference-time scaling using Bayesian linear regression where k samples are generated from the posterior predictive and selected based on a reward function. The reward model may differ from the ground-truth teacher, creating a misalignment that affects generalization error. Theoretical analysis in the high-dimensional regime uses deterministic equivalents to derive closed-form expressions for posterior predictive mean and variance. The selection mechanism uses softmax over rewards at a temperature T, with theoretical predictions validated through experiments on GSM8K using Llama-3-8B for generation and Mistral-7B for judging.

## Key Results
- Generalization error decreases monotonically with sample count k when reward model is sufficiently aligned with teacher
- Poorly aligned rewards create a finite optimal k beyond which more sampling increases error
- Optimal temperature T exists for fixed sample count that minimizes generalization error
- Best-of-k limit with perfect reward achieves Θ(1/k²) error scaling
- Inference-time scaling can outperform additional training data under flat prior, ample data, and low task difficulty conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing inference samples (k) only reduces generalization error if reward model is sufficiently aligned with ground truth; otherwise, error may increase beyond optimal k*.
- **Mechanism**: Softmax over rewards selects among k samples. When reward weight (w_R) deviates significantly from teacher (w_T), increasing k increases probability of selecting sample that maximizes "wrong" reward signal (reward hacking), causing generalization error δ to be non-monotonic in k.
- **Core assumption**: System operates in high-dimensional regime where deterministic equivalents for posterior predictive are valid.
- **Evidence anchors**: [abstract]: "Substantial reward misspecification induces a finite optimal k beyond which more sampling can increase the generalization error." [section 3.4]: "When w_R is sufficiently far from w_T... increase in k increases the error."

### Mechanism 2
- **Claim**: Optimal temperature (T) exists for reward-weighted sampling process that minimizes generalization error.
- **Mechanism**: Temperature controls "sharpness" of selection. High T approximates uniform sampling (ignoring judge), while low T enforces greedy selection (over-trusting potentially flawed judge). Optimum balances utility of judge against imperfections.
- **Core assumption**: Number of samples k is fixed and finite.
- **Evidence anchors**: [abstract]: "For fixed k, there exists an optimal temperature for the reward sampling process." [section 3.5]: "At very low temperatures, selection becomes too aggressive and can over-amplify any mismatch between w_R and w_T."

### Mechanism 3
- **Claim**: Best-of-k limit with well-aligned reward achieves Θ(1/k²) generalization error decay, potentially outperforming data scaling.
- **Mechanism**: Low-temperature limit (T → 0) selection follows extreme value theory (Weibull distribution). Error depends on second-order statistics of tails of sample distribution, scaling quadratically rather than linearly.
- **Core assumption**: Flat prior (γ² ≫ σ²), ample data (n ≫ d), and low task difficulty.
- **Evidence anchors**: [abstract]: "In the 'best-of-k' limit... generalization error decays as Θ(1/k²)." [section 3.6]: Uses extreme value theory on non-central chi-squared variables to derive tail behavior.

## Foundational Learning

- **Concept**: Deterministic Equivalents (Random Matrix Theory)
  - **Why needed here**: To derive closed-form expressions for posterior predictive mean and variance in high dimensions (d, n → ∞) where standard probability calculations are intractable.
  - **Quick check question**: Can you explain why the empirical covariance Σ̂ can be replaced by the population covariance Σ in the proportional limit?

- **Concept**: Extreme Value Theory (Weibull Distribution)
  - **Why needed here**: To model behavior of minimum error (best-of-k) as k → ∞.
  - **Quick check question**: Why does distribution of the *minimum* of bounded chi-squared variables belong to the Weibull domain of attraction rather than Gumbel?

- **Concept**: Teacher-Student Setup
  - **Why needed here**: To mathematically define "generalization error" by comparing learned weights against ground truth "teacher" model.
  - **Quick check question**: In this paper, does the "Student" refer to the Bayesian regressor or the Judge LLM? (Answer: The regressor).

## Architecture Onboarding

- **Component map**: Input x -> Posterior Sampling -> Reward Calculation -> Softmax Selection -> Output y_out
- **Critical path**: Input x → Posterior Sampling → Reward Calculation → Softmax Selection → Output y_out
- **Design tradeoffs**:
  - **Alignment vs. Sample Count**: Misaligned judge forces capping k to finite optimum; well-aligned judge allows aggressive scaling of k
  - **Temperature vs. Robustness**: Lower T requires higher confidence in judge; higher T acts as safety buffer against judge errors
- **Failure signatures**:
  - **U-shaped Error Curve**: Test error decreases with k but then starts rising again. *Diagnosis*: Reward model is misspecified (w_R ≉ w_T). *Fix*: Decrease k or increase T.
  - **Flat Error Curve**: Increasing k yields no gain. *Diagnosis*: Temperature T is too high, washing out reward signal.
- **First 3 experiments**:
  1. **Alignment Ablation**: Fix T and k. Vary distance ||w_R - w_T|| and plot generalization error to verify transition from monotonic to non-monotonic regions (Fig 2).
  2. **Temperature Sweep**: Fix k and slightly misaligned reward. Sweep T to find theoretical minimum error predicted by Remark 4 (Fig 5).
  3. **Scaling Law Verification**: Set T=0 with perfect reward. Log error vs. k on log-log plot to confirm Θ(1/k²) slope (Fig 6a).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does trade-off between training and inference-time compute, identified in linear model, empirically manifest in realistic Large Language Models (LLMs)?
- **Basis in paper**: [Explicit] The authors state, "We have also discussed trade-off between training and inference time scaling, analyzing it carefully in large language models is an important open question."
- **Why unresolved**: Theoretical results (Remark 6) are derived specifically for Bayesian linear regression in proportional limit (d, n → ∞) and may not hold for non-convex, non-linear dynamics of transformer training.
- **What evidence would resolve it**: Empirical scaling laws plotting generalization error against both training data volume (n) and inference samples (k) for standard LLM architectures to see if crossover regimes predicted by theory exist.

### Open Question 2
- **Question**: Can an auxiliary classifier trained to identify systematic model weaknesses successfully perform classifier-guided reward shaping to improve inference-time selection?
- **Basis in paper**: [Explicit] The paper suggests leveraging property that optimal reward differs from teacher, noting that "A comprehensive study of these directions is left to future work."
- **Why unresolved**: While linear theory provides closed-form adjustment for w_R, it's unclear how to map this "shift" to high-dimensional LLMs where teacher/reward model is opaque neural network.
- **What evidence would resolve it**: Experiments where separate model is trained to predict base LLM's specific failure modes, with its output used to modulate judge's reward signal, showing lower generalization error than judge alone.

### Open Question 3
- **Question**: How do non-i.i.d. sampling strategies, such as beam search or self-consistency, alter existence or location of optimal temperature (T) and sample count (k)?
- **Basis in paper**: [Explicit] Appendix F discusses extending model to "Going beyond independent inference time sampling," noting that "Both ideas introduce dependence between candidates... While we do not pursue these directions here."
- **Why unresolved**: Paper's core theoretical results (Results 1-3) rely on assumption that inference samples y_i are drawn independently from posterior predictive distribution.
- **What evidence would resolve it**: Theoretical extensions deriving deterministic equivalents for order statistics of correlated samples, or empirical studies showing if optimal k and T shift significantly when beam search replaces standard sampling.

## Limitations
- Parameterization in real LLM experiments introduces uncertainty as quadratic reward model may not perfectly capture judge LLM's actual scoring behavior
- Task difficulty assumptions in scaling law analysis may not generalize beyond synthetic regime
- Medium confidence in selection temperature mechanism due to limited external validation

## Confidence
- **High confidence**: Mechanism 1 (alignment vs. sample count relationship) and Mechanism 3 (Best-of-k scaling law)
- **Medium confidence**: Mechanism 2 (optimal temperature existence) due to limited external validation
- **Medium confidence**: Extension from Bayesian linear regression to real LLM-as-a-judge scenarios given approximations required

## Next Checks
1. **Cross-task generalization test**: Apply theoretical framework to non-mathematical reasoning tasks (e.g., commonsense reasoning, summarization quality assessment) to verify alignment-dependent sample scaling and temperature optimization generalize beyond GSM8K

2. **Judge model alignment ablation**: Systematically vary alignment between judge and teacher reward models in LLM experiments by using judges with different instruction-following tendencies or reward specifications, and measure how this affects optimal k* and temperature T*

3. **Alternative selection mechanisms**: Replace softmax selection with alternative strategies (e.g., top-k deterministic selection, Thompson sampling) to test whether temperature-dependent behavior is specific to softmax or represents broader principle of inference-time scaling