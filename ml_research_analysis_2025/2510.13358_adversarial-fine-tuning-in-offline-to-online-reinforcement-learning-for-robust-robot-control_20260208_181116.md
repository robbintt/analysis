---
ver: rpa2
title: Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust
  Robot Control
arxiv_id: '2510.13358'
source_url: https://arxiv.org/abs/2510.13358
tags:
- training
- offline
- adversarial
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adversarial fine-tuning to improve robustness
  of offline-trained policies against action-space perturbations, such as actuator
  faults in robot control. The method first pretrains a policy offline with conservative
  offline RL, then performs online fine-tuning with adversarial perturbations injected
  into executed actions.
---

# Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control

## Quick Facts
- arXiv ID: 2510.13358
- Source URL: https://arxiv.org/abs/2510.13358
- Reference count: 40
- One-line primary result: Adversarial fine-tuning improves robustness of offline-trained policies against action-space perturbations in legged robot control tasks.

## Executive Summary
This paper introduces a method to improve the robustness of offline-trained policies by performing online fine-tuning with adversarial perturbations injected into executed actions. The approach first pretrains a policy offline using conservative offline RL (TD3+BC), then fine-tunes it online while applying action-space perturbations designed to induce compensatory behavior. A curriculum mechanism adjusts the perturbation probability during training to balance robustness gains with stability. Experiments on Hopper-v2, HalfCheetah-v2, and Ant-v2 demonstrate that this method consistently outperforms both offline-only and fully online baselines in terms of robustness to perturbations while converging faster than training from scratch.

## Method Summary
The method consists of two phases: offline pretraining followed by online adversarial fine-tuning. First, a policy is pretrained with TD3+BC on clean D4RL datasets for 5M steps to establish a strong initialization. Next, adversarial perturbations are pre-computed using differential evolution against the pretrained policy to maximize action-space damage. During online fine-tuning (1M-3M steps), these perturbations are injected into executed actions with probability q, inducing the policy to learn compensatory behaviors. A performance-aware curriculum dynamically adjusts q based on exponential moving average of policy performance, increasing it when performance improves and decreasing it when performance drops to maintain the robustness-stability trade-off.

## Key Results
- Adversarial fine-tuning consistently improves robustness compared to offline-only and fully online baselines across Hopper-v2, HalfCheetah-v2, and Ant-v2
- The method converges faster than training from scratch while maintaining nominal performance
- Matching fine-tuning and evaluation conditions yields the strongest robustness
- Adaptive curriculum outperforms linear curriculum by better preserving nominal performance while achieving comparable adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1: Compensatory Action Learning via Adversarial Perturbation Injection
Injecting adversarial perturbations into executed actions during fine-tuning induces the policy to learn compensatory behaviors that maintain consistent action distributions under perturbation. The policy outputs actions that, when perturbed, result in executed actions matching the distribution it would produce under normal conditions.

### Mechanism 2: Performance-Aware Curriculum for Robustness-Stability Trade-off
An adaptive curriculum that adjusts perturbation probability q based on exponential moving average (EMA) of policy performance prevents overfitting to adversarial conditions while maintaining robustness gains. The update rule Î”q = Î·(ð‘…Ì„â‚™ - ð‘…Ì„â‚™â‚‹â‚) with ð‘…Ì„â‚™ = Î²ð‘…â‚™ + (1-Î²)ð‘…Ì„â‚™â‚‹â‚ increases q when performance improves and decreases q when performance drops.

### Mechanism 3: Sample Efficiency via Offline Initialization with Online Adaptation
Offline pretraining on clean data provides a strong initialization that reduces online fine-tuning samples needed to achieve robustness, compared to training from scratch. TD3+BC pretraining constrains the policy to match dataset actions, producing a policy that already performs well under normal conditions.

## Foundational Learning

- **Concept: Conservative Offline RL (specifically TD3+BC)**
  - Why needed here: The method uses TD3+BC for offline pretraining; understanding the behavior cloning penalty ||Ï€(s) - a||Â² and why it's removed during fine-tuning is essential.
  - Quick check question: Can you explain why constraining the policy to dataset actions helps offline training but hurts adaptation to perturbations?

- **Concept: Adversarial Perturbation Generation via Differential Evolution**
  - Why needed here: Perturbations are pre-computed using DE before fine-tuning; this avoids expensive inner-loop optimization during training but requires understanding how DE maximizes action-space damage.
  - Quick check question: Why pre-generate perturbations rather than compute them on-the-fly during each environment step?

- **Concept: Actor-Critic Fine-Tuning Dynamics**
  - Why needed here: The transition from TD3+BC to standard TD3 during fine-tuning changes the loss landscape; understanding target policy smoothing and twin critics helps diagnose instability.
  - Quick check question: What happens to Q-value estimates when perturbed transitions first enter the replay buffer?

## Architecture Onboarding

- **Component map:**
  D4RL Dataset â†’ TD3+BC Pretraining, 5M steps â†’ Ï€_off, Q_off
    â†“
  DE-based Perturbation Generator â†’ {Î´_a} (precomputed)
    â†“
  Replay Buffer: offline samples + online perturbed transitions
    â†“
  TD3 Fine-tuning with perturbation injection at probability q
    â†“
  Periodic Evaluation â†’ EMA Performance Signal â†’ Curriculum q update

- **Critical path:**
  1. Pretrain with TD3+BC on clean D4RL data (5M steps) â€” this determines initial policy quality
  2. Generate adversarial perturbations using DE against Ï€_off â€” perturbation quality directly affects robustness
  3. Fine-tune with perturbation injection, starting with r_off = 0.1 offline data ratio in buffer â€” too little causes instability, too much slows adaptation
  4. If using adaptive curriculum, set Î² = 0.9 and tune Î· per environment â€” critical for balancing robustness/nominal performance

- **Design tradeoffs:**
  - **Fixed q vs. Curriculum**: Fixed q=0.5 is simpler but may overfit; curriculum adds hyperparameters (Î², Î·, q_max) but better preserves nominal performance
  - **Perturbation magnitude Îµ**: Larger Îµ improves robustness to severe faults but may make training unstable; paper uses Îµ âˆˆ {0.3, 0.5} depending on environment
  - **Offline data ratio r_off**: Higher ratios stabilize early fine-tuning but slow convergence to perturbation-adapted behaviors
  - **Precomputation vs. on-the-fly perturbations**: Precomputed DE perturbations are computationally efficient but static; on-the-fly could adapt to policy changes but requires costly optimization

- **Failure signatures:**
  - **Nominal performance collapse**: Normal-condition rewards drop significantly during fine-tuning â†’ indicates q increased too fast or Îµ too large; check if adaptive curriculum is reducing q when performance drops
  - **Adversarial robustness plateaus at low levels**: Policy never recovers from initial perturbation exposure â†’ may indicate poor offline pretraining quality or perturbations too strong relative to action bounds
  - **High variance across runs**: Standard deviation >30% of mean â†’ check random seeds, ensure replay buffer initialization is consistent, verify perturbation generation is deterministic given Ï€_off
  - **Early training instability**: First 100K steps show wild Q-value oscillations â†’ increase r_off or reduce initial q_init

- **First 3 experiments:**
  1. **Sanity check**: Pretrain TD3+BC on Hopper-v2 with medium-replay dataset, fine-tune with fixed q=0.5 and random perturbations (Îµ=0.3), evaluate under normal/random/adversarial conditions. Compare to offline-only baseline to confirm perturbation injection works.
  2. **Curriculum comparison**: On HalfCheetah-v2, compare fixed q=0.5 vs. linear curriculum (q_max=1.0) vs. adaptive curriculum (Î²=0.9, Î·=0.7) over 3M steps. Plot q trajectories and normal/adversarial performance to verify adaptive curriculum maintains nominal performance while achieving robustness.
  3. **Ablation on matching conditions**: Fine-tune three policies on Ant-v2 under normal/random/adversarial conditions separately. Evaluate each under all three conditions to confirm the paper's claim that "matching fine-tuning and evaluation conditions yields the strongest robustness."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What theoretical convergence guarantees exist for the performance-aware curriculum-based adaptation strategy?
- **Basis in paper:** [explicit] The Conclusion explicitly states: "Future work should investigate theoretical guarantees for curriculum-based adaptation."
- **Why unresolved:** The paper empirically validates that the adaptive curriculum works but provides no formal proof that the dynamic adjustment of perturbation probability (q) ensures stable convergence or prevents pathological oscillation.
- **What evidence would resolve it:** A theoretical analysis or proof bounding the sub-optimality gap or convergence rate of the policy update under the dynamic q schedule.

### Open Question 2
- **Question:** How does the proposed method perform when state-space and action-space perturbations occur simultaneously in complex environments?
- **Basis in paper:** [explicit] The Conclusion identifies the need to "explore the interplay between state-space and action-space perturbations in more complex environments."
- **Why unresolved:** The current study isolates action-space perturbations (actuator faults) in standard benchmarks, leaving the compounded effect of sensory noise and actuator failure unverified.
- **What evidence would resolve it:** Experiments in environments featuring both observation noise and action perturbations, or deployment on physical robotic hardware.

### Open Question 3
- **Question:** Does pre-computing adversarial perturbations based on the initial offline policy limit robustness compared to dynamically generating them?
- **Basis in paper:** [inferred] Section 4.2 notes perturbations are pre-generated using the offline policy to avoid "costly inner-loop minimization," implying a trade-off between computational efficiency and the relevance of perturbations as the policy evolves.
- **Why unresolved:** As the policy diverges from the initial Ï€_off during fine-tuning, the pre-computed attacks may fail to approximate the worst-case action perturbations for the current policy.
- **What evidence would resolve it:** A comparison of robustness between policies trained with static pre-computed perturbations versus those trained with perturbations generated dynamically online.

### Open Question 4
- **Question:** Is the framework dependent on high-quality "clean" offline data, or can it succeed with sub-optimal demonstrations?
- **Basis in paper:** [inferred] The Abstract specifies the framework "trains policies on clean data," and experiments utilize expert datasets (D4RL), leaving the method's sensitivity to dataset quality unaddressed.
- **Why unresolved:** Real-world offline datasets often contain noisy or sub-optimal trajectories; if the curriculum relies on a strong initial policy to bootstrap robustness, it may fail on lower-quality data.
- **What evidence would resolve it:** Evaluation using D4RL datasets of varying quality (e.g., "medium" or "random") to assess the necessity of expert data for successful adversarial fine-tuning.

## Limitations

- **Major uncertainties include** the lack of specification for differential evolution hyperparameters (population size, generations, mutation rates) which could significantly impact perturbation quality and robustness outcomes.
- **The method's dependence on high-quality offline data and its performance degradation when perturbation distributions mismatch deployment conditions represent practical limitations** not fully explored in the current evaluation.
- **Confidence is Medium** for curriculum-based claims, as while the adaptive curriculum shows better nominal performance preservation, the parameter sensitivity (particularly Î·) and its interaction with different environment dynamics requires further investigation.

## Confidence

- **Confidence is High** for claims regarding the basic adversarial fine-tuning mechanism and its ability to improve robustness compared to offline-only training, supported by consistent performance improvements across three environments.
- **Confidence is Medium** for curriculum-based claims, as while the adaptive curriculum shows better nominal performance preservation, the parameter sensitivity (particularly Î·) and its interaction with different environment dynamics requires further investigation.
- **Confidence is Medium** for sample efficiency claims, as convergence speed comparisons are shown but the computational overhead of perturbation generation and curriculum management is not fully characterized.

## Next Checks

1. **Perturbation generation ablation**: Compare pre-computed DE perturbations against on-the-fly optimization or random perturbations to isolate the contribution of adversarial vs. random robustness.
2. **Curriculum parameter sensitivity**: Systematically vary Î² and Î· across environments to characterize the stability-robustness trade-off and identify optimal settings.
3. **Distributional shift evaluation**: Test performance when deployment perturbations follow a different distribution than training perturbations to assess real-world robustness.