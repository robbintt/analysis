---
ver: rpa2
title: 'Toward Responsible ASR for African American English Speakers: A Scoping Review
  of Bias and Equity in Speech Technology'
arxiv_id: '2508.18288'
source_url: https://arxiv.org/abs/2508.18288
tags:
- speech
- language
- data
- design
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review examines fairness, bias, and equity in ASR
  systems for African American English (AAE) speakers, analyzing 44 interdisciplinary
  papers across ML/NLP, HCI, and sociolinguistics. While technical bias mitigation
  and inclusive data practices are growing, the review finds a critical gap in governance-centered
  approaches that foreground community agency and linguistic justice.
---

# Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology

## Quick Facts
- **arXiv ID:** 2508.18288
- **Source URL:** https://arxiv.org/abs/2508.18288
- **Reference count:** 28
- **Primary result:** Scoping review of 44 papers finds ASR systems systematically misrecognize AAE, calling for governance-centered frameworks that embed community co-governance across the ASR development lifecycle.

## Executive Summary
This scoping review examines fairness, bias, and equity in automatic speech recognition (ASR) systems for African American English (AAE) speakers. Analyzing 44 interdisciplinary papers across machine learning/NLP, human-computer interaction, and sociolinguistics, the review reveals that while technical bias mitigation and inclusive data practices are growing, a critical gap remains in governance-centered approaches that foreground community agency and linguistic justice. Most studies remain narrowly technical, lacking participatory accountability or epistemic justice. The authors propose a governance-centered ASR life-cycle framework embedding community co-governance, cultural stewardship, and reflexive oversight across all stages of ASR development, moving beyond inclusion toward structural accountability.

## Method Summary
The review employed PRISMA-ScR methodology with 6 thematic keyword clusters expanded to 1,872 terms via ResearchRabbit. From 72 initially identified papers, 44 peer-reviewed papers were selected across ACM Digital Library (19), ACLWeb (18), IEEE Xplore (4), arXiv (1), PNAS (1), Frontiers (1), and ProQuest (1). Papers were independently coded using Atlas.ti with iterative codebook development and group consensus, achieving mean pairwise Cohen's Îº = 0.82 across 5 coders (15 coder pairs). The analysis mapped interdisciplinary discourse on ASR fairness and identified gaps in governance-centered approaches.

## Key Results
- ASR systems systematically misrecognize AAE features (phonological, morphosyntactic) due to training on SAE-dominant datasets, resulting in elevated WER for AAE speakers.
- Technical misrecognition acts as socio-technical harm, triggering psychological burden, distrust, and self-silencing among users who must code-switch to be understood.
- Most ASR bias research remains narrowly technical, lacking participatory accountability or epistemic justice; governance-centered frameworks embedding community co-governance are critically needed.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ASR models trained on SAE systematically misrecognize AAE features.
- **Mechanism:** Acoustic and language models penalize AAE phonological and morphosyntactic variations (e.g., consonant cluster reduction, habitual "be") underrepresented in training corpora, causing elevated WER.
- **Core assumption:** Dialectal variation is systematic and predictable, not random noise; standard benchmarks fail to capture this specificity.
- **Evidence anchors:**
  - [abstract] Notes AAE patterns are "frequently misrecognized, misinterpreted, or erased by ASR systems predominantly trained on standardized American English (SAE) datasets."
  - [section] "ML/NLP: Quantifying Disparities..." cites Koenecke et al. (2020) documenting racial disparities in WER.
  - [corpus] 'A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems...' confirms significant accuracy degradation for African American speakers compared to Caucasian speakers.
- **Break condition:** Models adapted using dialect-specific constraints or diverse corpora (e.g., CORAAL) that validate AAE's internal consistency.

### Mechanism 2
- **Claim:** Technical misrecognition acts as socio-technical harm, triggering psychological burden and distrust.
- **Mechanism:** Recognition failures function as microaggressions or identity invalidations; users forced to code-switch to be understood face enforced linguistic hierarchy, reducing trust and excluding them from high-stakes domains.
- **Core assumption:** User trust and retention correlate directly with system's ability to validate linguistic identity, not just functional task completion.
- **Evidence anchors:**
  - [abstract] Notes failures "reflect and reinforce broader societal inequities around whose voices are recognized."
  - [section] "HCI: Trust, Experience, and Interaction Harms" details how ASR failures prompt "code-switching, self-silencing, and distrust."
  - [corpus] 'Finding A Voice: Exploring the Potential of African American Dialect...' suggests linguistic similarity fosters trust and engagement.
- **Break condition:** System feedback issues affirmations or shifts burden of adaptation from user to system.

### Mechanism 3
- **Claim:** Governance-centered life-cycles address linguistic erasure by redistributing epistemic authority.
- **Mechanism:** Embedding community co-governance and participatory checkpoints at every stage (problem definition to post-deployment) prevents data extraction without accountability and ensures evolving linguistic norms are continuously updated.
- **Core assumption:** Community advisory boards or data trusts have capacity and structural power to veto or redirect technical decisions, not merely consult.
- **Evidence anchors:**
  - [abstract] Proposes "governance-centered ASR life-cycle framework, embedding community co-governance... across all stages."
  - [section] "A Framework for Governance-Centered ASR" argues technical fixes are "insufficient" without attending to "political, social, and epistemic power structures."
  - [corpus] Corpus signals focus primarily on technical bias detection rather than governance structures, highlighting the novelty gap this mechanism attempts to fill.
- **Break condition:** Governance checkpoints become "checkbox exercises" without veto power or funding continuity, allowing structural bias to persist.

## Foundational Learning

- **Concept:** Sociolinguistic Legitimacy vs. Deficit Framing
  - **Why needed here:** To understand AAE as rule-governed variety, not "broken English," and that ASR errors are model failures, not speaker errors.
  - **Quick check question:** Can you explain why "habitual be" (e.g., "she be working") poses a distinct modeling challenge compared to auxiliary verb omission?

- **Concept:** Epistemic Justice in Data Curation
  - **Why needed here:** To move beyond "adding more data" to understanding *who* labels data and *whose* interpretation of meaning is encoded as ground truth.
  - **Quick check question:** How might a non-AAE speaking annotator mislabel the intent of an AAE phrase compared to an AAE-speaking annotator?

- **Concept:** Participatory Co-governance
  - **Why needed here:** To differentiate between "user testing" (checking if tool works) and "governance" (giving community authority to define what tool *should* do and when it should be decommissioned).
  - **Quick check question:** In the proposed life-cycle, what specific decision-making power would a Community Advisory Board hold during the "Problem Definition" phase?

## Architecture Onboarding

- **Component map:** Community-led Data Collection -> Dialect-Aware Modeling -> Participatory Checkpoints -> Redress Mechanisms -> Equity Audits
- **Critical path:** The connection between Data Sourcing and Evaluation. If data is owned/curated by community, evaluation metrics must be defined by that same community to ensure validity. If this link is broken (e.g., community data used for standard SAE benchmarks), the system reverts to extraction.
- **Design tradeoffs:**
  - Static Accuracy vs. Linguistic Fluidity: Highly tuned models may achieve high accuracy today but fail to capture rapid lexical innovation of AAE (slang) without continuous retraining loops.
  - Efficiency vs. Participation: Integrating "race-primed" annotators and community review is slower and costlier than automated scraping and standard annotation.
- **Failure signatures:**
  - Consultation Theater: Gathering community feedback that is then ignored in model training due to "technical constraints."
  - Standardization Drift: System initially tuned for AAE slowly converges back to SAE norms because loss function penalizes non-standard features during updates.
  - Contextual Misinterpretation: Model transcribes words correctly but misinterprets intent (e.g., misclassifying AAE discourse markers as "toxicity").
- **First 3 experiments:**
  1. **Benchmarking Gap Analysis:** Run existing commercial ASR models against CORAAL dataset to quantify WER disparity between AAE and SAE speakers.
  2. **Annotator Alignment Study:** Compare toxicity labels assigned to AAE tweets by "race-primed" annotators vs. standard annotators to measure "interpretive bias" delta.
  3. **Redress Mechanism Prototype:** Deploy low-stakes voice assistant feature that explicitly acknowledges misrecognition ("I'm sorry, I didn't catch that because of how I process that dialect, not because you misspoke") and measure user trust recovery.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the proposed governance-centered ASR life-cycle be empirically validated and refined through collaboration with AAE speech communities?
- **Basis in paper:** [explicit] The Conclusion states, "Future work should also empirically and collaboratively validate this framework with diverse speech communities by co-developing benchmarks, refining models to community priorities, and evaluating social and technical impacts."
- **Why unresolved:** The framework is a conceptual contribution derived from a scoping review; it has not yet been tested in real-world development pipelines.
- **What evidence would resolve it:** Case studies or pilot programs demonstrating the framework's application in ASR projects serving AAE speakers, showing measurable improvements in community trust and linguistic equity.

### Open Question 2
- **Question:** What institutional mechanisms can operationalize community co-governance and data sovereignty beyond short-term consultation?
- **Basis in paper:** [explicit] The Discussion urges future scholarship "to move from inclusive design to inclusive governance," noting that current participatory methods are often "pilot-scale" consultation rather than sustained co-governance with decision-making power.
- **Why unresolved:** Existing incentive structures prioritize technical novelty over participatory integrity, and there is a lack of models for transferring power to community advisory boards or data trusts.
- **What evidence would resolve it:** Documented governance models where community bodies hold legal or administrative authority over data usage, model updates, or deployment criteria.

### Open Question 3
- **Question:** How can ASR evaluation methodologies be expanded to capture non-technical harms such as linguistic microaggressions and epistemic erasure?
- **Basis in paper:** [inferred] The paper critiques dominance of Word Error Rate (WER), noting technical metrics fail to capture "affective burden," "trust erosion," and "lived experience" of misrecognition. Authors call for augmenting WER with qualitative evaluations.
- **Why unresolved:** Quantitative metrics dominate ML/NLP pipelines, and there is no standardized, scalable method for measuring subjective experiences like microaggressions in model evaluation.
- **What evidence would resolve it:** Development and adoption of standardized qualitative metrics or mixed-method auditing protocols that correlate system performance with user-reported psychological impact.

## Limitations
- The governance-centered framework remains largely theoretical, with limited empirical studies testing participatory co-governance models in ASR development.
- The review may underrepresent non-academic perspectives, particularly community-led ASR initiatives operating outside traditional peer-reviewed channels.
- Claims about psychological harms from technical misrecognition rely on indirect evidence rather than controlled experiments measuring behavioral outcomes.

## Confidence
- **High Confidence:** Documented technical disparities in ASR performance for AAE speakers (elevated WER) are well-established through multiple empirical studies.
- **Medium Confidence:** Mechanism linking technical misrecognition to psychological harms and trust erosion is plausible but relies on indirect evidence.
- **Low Confidence:** Governance-centered framework's effectiveness remains speculative, as no studies in corpus demonstrate practical implementation or measurable impact.

## Next Checks
1. **Governance Framework Pilot:** Implement proposed community co-governance checkpoints in existing ASR development pipeline and measure changes in model performance, community satisfaction, and linguistic representation over 12 months.
2. **Longitudinal Trust Study:** Track AAE speakers' usage patterns and trust ratings for ASR systems with redress mechanisms versus those without, controlling for dialect density and task complexity.
3. **Governance vs. Technical Impact Comparison:** Compare outcomes (accuracy, user retention, community trust) between ASR systems using only technical bias mitigation versus those combining technical fixes with participatory governance structures.