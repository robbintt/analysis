---
ver: rpa2
title: Text embedding models can be great data engineers
arxiv_id: '2505.14802'
source_url: https://arxiv.org/abs/2505.14802
tags:
- data
- adept
- embeddings
- text
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADEPT, a framework that uses text embeddings
  to replace conventional data engineering steps in time series classification. Instead
  of preprocessing, feature engineering, and cleaning, ADEPT serializes raw time series
  into text, embeds them with a pretrained LLM-based model, and applies a Variational
  Information Bottleneck (VIB) to filter noise and enhance information gain.
---

# Text embedding models can be great data engineers

## Quick Facts
- arXiv ID: 2505.14802
- Source URL: https://arxiv.org/abs/2505.14802
- Authors: Iman Kazemian; Paritosh Ramanan; Murat Yildirim
- Reference count: 40
- Replaces traditional data engineering with text embeddings for time series classification

## Executive Summary
ADEPT is a framework that leverages text embeddings from Large Language Models to bypass conventional data engineering steps in time series classification. By serializing raw time series data into text and embedding it with a frozen LLM, ADEPT achieves state-of-the-art accuracy across diverse domains without manual feature engineering, cleaning, or preprocessing. A Variational Information Bottleneck (VIB) filters noise from embeddings, significantly boosting performance. The approach demonstrates that text embeddings can effectively replace complex, domain-specific pipelines for scalable predictive analytics.

## Method Summary
ADEPT serializes raw time series (timestamps + values) into text chunks, embeds them using a frozen LLM, and applies a VIB to compress embeddings into noise-resistant latent codes. A Transformer-based classifier with multi-head attention captures intra- and inter-view dependencies across time windows, followed by classification. The framework eliminates traditional data preprocessing and feature engineering steps, relying instead on the semantic compression learned by LLMs and VIB filtering to maintain predictive power.

## Key Results
- Achieves 97.83% accuracy on astrophysical light curve classification
- Demonstrates 73.68% accuracy on EEG-based healthcare data
- Shows 88.49% accuracy on Bitcoin price trend forecasting
- Achieves 74.35% accuracy on industrial IoT failure mode detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text embeddings of serialized raw time series data can functionally replace manual feature engineering pipelines by capturing equivalent informational entropy.
- **Mechanism:** Serializes numerical time series into text strings, processes with frozen LLM embedding model. Hypothesis: semantic compression learned by LLMs on web-scale text generalizes to "textually dense" numerical representations, capturing spatiotemporal correlations without domain-specific preprocessing.
- **Core assumption:** The entropy inherent in a raw text representation (RFR) of time series is semantically equivalent or superior to that of hand-crafted numerical feature vectors.
- **Evidence anchors:**
  - [abstract] "entropy of embeddings... intuitively viewed as equivalent (or in many cases superior) to that of numerically dense vector representations"
  - [Page 2] "ADEPT framework exploits text embeddings... to effectively leapfrog data cleaning, feature engineering and extraction steps"
  - [corpus] General evidence exists for LLMs handling non-text modalities (e.g., "Joint Embeddings Go Temporal"), but specific evidence for *time-series-to-text* replacing engineering pipelines is isolated to this paper's benchmarks.
- **Break condition:** If serialization process destroys critical floating-point precision or local continuity that standard numerical normalization would preserve.

### Mechanism 2
- **Claim:** A Variational Information Bottleneck (VIB) acts as a domain-agnostic "filter" to remove the noise inherent in generic text embeddings, improving classification performance.
- **Mechanism:** Raw text embeddings are high-dimensional and redundant. VIB compresses these embeddings into lower-dimensional latent code ($z$) by optimizing trade-off between prediction accuracy (cross-entropy) and information capacity (KL divergence). Forces model to retain only features relevant to class label.
- **Core assumption:** The performance gap between raw embeddings and engineered features is primarily due to "noise variance" and redundancy rather than fundamental lack of signal in text embedding.
- **Evidence anchors:**
  - [abstract] "constructs a variational information bottleneck criteria to mitigate entropy variance"
  - [Page 5] "VIB criteria enables ADEPT to apply a filtering mechanism... boosting classification accuracy"
  - [Page 8, Table 1c] Shows ADEPT v1.0 (no VIB) failing at 45.4% while v2.0 (with VIB) achieves 88.49% on Bitcoin data.
- **Break condition:** If regularization weight $\beta$ is set too high, causing bottleneck to discard signal along with noise (information loss), or if bottleneck dimension $d$ is insufficient to separate classes.

### Mechanism 3
- **Claim:** A Transformer-based classifier can capture intra- and inter-view dependencies from distilled latent codes to handle multi-view time series events.
- **Mechanism:** Architecture assembles latent codes from multiple time windows ("views") and chunks into sequence. Transformer Autoencoder pre-trains to reconstruct embeddings (capturing structure), followed by classification fine-tuning phase. Attention mechanisms aggregate information across time chunks and sensor views.
- **Core assumption:** Temporal order and inter-view relationships are sufficiently preserved in sequence of chunked embeddings for attention mechanism to exploit.
- **Evidence anchors:**
  - [Page 5] "adopts a powerful multi-head attention (MHA) based Transformer to capture both intra- and inter-view dependencies"
  - [Page 6] "pretrain $T_\psi$ as a joint autoencoder... capturing common structure across views"
- **Break condition:** If chunking strategy (parameter $M$) fragments critical temporal patterns (e.g., splitting relevant waveform across two chunks) such that local dependencies are lost.

## Foundational Learning

- **Concept:** **Variational Autoencoders (VAEs) & Information Bottlenecks**
  - **Why needed here:** Understanding how reparameterization trick ($z = \mu + \sigma \odot \epsilon$) and KL divergence loss allow model to learn compressed, noise-resistant representation space.
  - **Quick check question:** Can you explain how increasing the $\beta$ parameter in VIB loss affects the latent space geometry and model robustness?

- **Concept:** **Text Embeddings for Non-Text Data**
  - **Why needed here:** Core innovation relies on mapping numerical time series to text tokens. Understanding tokenization limits (e.g., handling of floats vs. integers) is critical.
  - **Quick check question:** How does sequence length constraint of embedding model (e.g., 8k context window) dictate chunking strategy $M$ for long time series?

- **Concept:** **Transformer Self-Attention**
  - **Why needed here:** Final classifier uses attention to fuse information. Understanding how queries, keys, and values interact helps diagnose why model might miss long-range dependencies.
  - **Quick check question:** Why does architecture pre-train as autoencoder before fine-tuning for classification?

## Architecture Onboarding

- **Component map:** Raw Time Series (Timestamps + Values) -> RFR Decomposition: Serialize into $M$ text chunks -> Embedder: Frozen LLM (e.g., OpenAI small / Nomic) -> VIB Encoder: Learnable compression to latent $z$ -> Transformer: Multi-head attention over sequence of $z$ -> Head: Softmax classifier.

- **Critical path:** The serialization format and the VIB regularization ($\beta$). If text format confuses embedder, or if VIB over-compresses, pipeline fails.

- **Design tradeoffs:**
  - **V1 vs. V2:** V1 (Embedding -> Classifier) is faster but noisier. V2 (Embedding -> VIB -> Classifier) adds training complexity but significantly boosts accuracy (e.g., 45% -> 88% in finance).
  - **Cloud vs. Local:** Use cloud APIs (OpenAI) for maximum embedding quality if data privacy permits; use local models (Nomic) for proprietary data (HRI dataset) at cost of potential performance drops.

- **Failure signatures:**
  - **Low Accuracy / High Variance:** VIB may be disabled or $\beta$ too low (treating noise as signal).
  - **Convergence to Mean:** VIB $\beta$ too high or bottleneck dimension $d$ too small.
  - **Data Corruption Errors:** Improper serialization of missing values (NaNs) into text string.

- **First 3 experiments:**
  1. **Serialization Validation:** Serialize small batch of time series (including missing values) and verify text representation preserves visual patterns (e.g., via reconstruction or manual inspection).
  2. **Ablation Study (V1 vs V2):** Run pipeline with and without VIB component on validation set to quantify "noise reduction" contribution.
  3. **Hyperparameter Sweep ($\beta$ & $d$):** Sweep VIB weight $\beta$ (e.g., $10^{-5}$ to $10^{-2}$) and bottleneck dimension to find "information sweet spot" for specific dataset size.

## Open Questions the Paper Calls Out
None

## Limitations

- **Serialization Sensitivity:** Framework's performance highly dependent on text serialization format. Unclear how sensitive model is to variations in formatting (e.g., delimiters, precision of floating-point numbers, handling of missing values).
- **Generalization Across Domains:** Strong results across diverse domains, but benchmarks are limited in scale and diversity. Framework's ability to generalize to other time series tasks, especially those with complex multivariate dependencies or extremely long sequences, remains open question.
- **Computational Cost:** While ADEPT replaces manual feature engineering, it introduces computational cost of text embedding generation. For large-scale datasets, time and resources required to embed millions of time series could be prohibitive, especially when using cloud-based LLM APIs.

## Confidence

- **High Confidence:** Core mechanism of using text embeddings to replace feature engineering is supported by strong empirical results across multiple domains. Ablation studies clearly demonstrate value of VIB component in filtering noise and improving accuracy.
- **Medium Confidence:** Claim that text embeddings capture "equivalent or superior" entropy compared to hand-crafted features is plausible but relies on specific serialization strategy used. Framework's sensitivity to this strategy is potential weakness.
- **Low Confidence:** Framework's ability to handle extremely long time series or those with complex multivariate dependencies is not well-validated. Computational cost implications for large-scale deployments are also not fully addressed.

## Next Checks

1. **Serialization Ablation:** Systematically test different text serialization formats (e.g., CSV, JSON, space-delimited) and evaluate their impact on embedding quality and classification accuracy.
2. **Long Sequence Handling:** Evaluate ADEPT's performance on time series with lengths exceeding text embedding model's context window. Test different chunking strategies and their impact on preserving long-range dependencies and classification accuracy.
3. **Computational Cost Analysis:** Benchmark total computational cost (embedding time, model training time) of ADEPT against traditional feature engineering pipelines on large-scale datasets.