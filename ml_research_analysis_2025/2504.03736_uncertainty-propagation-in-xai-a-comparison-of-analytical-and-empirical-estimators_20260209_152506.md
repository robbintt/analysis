---
ver: rpa2
title: 'Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators'
arxiv_id: '2504.03736'
source_url: https://arxiv.org/abs/2504.03736
tags:
- uncertainty
- explanations
- methods
- explanation
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares two fundamental approaches
  to quantifying uncertainty in XAI: empirical estimation via Monte Carlo sampling
  and analytical estimation via first-order uncertainty propagation. The authors introduce
  a unified framework for measuring uncertainty propagation from input data and model
  parameters to explanations, using a general explanation function that captures sensitivity
  to perturbations.'
---

# Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators

## Quick Facts
- arXiv ID: 2504.03736
- Source URL: https://arxiv.org/abs/2504.03736
- Reference count: 40
- Primary result: XAI methods often fail to reliably propagate uncertainties from inputs and model parameters to explanations, with gradient-based methods showing suppressed or saturated uncertainty due to ReLU activations.

## Executive Summary
This paper introduces a unified framework for quantifying uncertainty propagation in Explainable AI by comparing analytical (first-order uncertainty propagation) and empirical (Monte Carlo sampling) estimators. The authors systematically evaluate five standard XAI methods across classification and regression tasks, computing mean uncertainty in explanations (MUE) as a scalar metric. Their experiments reveal three distinct scenarios for uncertainty propagation: alignment between analytical and empirical estimates, near-zero empirical uncertainty for gradient-based methods on tabular data, and uncertainty plateaus for gradient-based methods on image data due to ReLU activation structure.

## Method Summary
The framework measures uncertainty propagation from input data and model parameters to explanations using a general explanation function that captures sensitivity to perturbations. The analytical estimator computes linearized covariance via the explanation Jacobian using finite differences, while the empirical estimator generates Monte Carlo samples of perturbed inputs/weights and estimates covariance from sampled explanations. The comparison uses MUE (trace of explanation covariance matrix normalized by explanation norm) to quantify and compare uncertainty across methods and perturbation regimes.

## Key Results
- Gradient-based XAI methods (Saliency, GuidedBackprop) show near-zero empirical uncertainty for small perturbations on tabular data due to constant gradients within ReLU regions
- Uncertainty plateaus below certain perturbation scales for gradient-based methods on image data when using ReLU activations, regardless of input variance
- Occlusion and other perturbation-based methods propagate uncertainty more reliably, with aligned analytical and empirical estimates across all variance regimes
- Analytical and empirical MUE curves diverge significantly for gradient-based methods, indicating unreliable uncertainty propagation in these approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-order uncertainty propagation provides tractable analytical estimates of explanation variance when perturbations are small and approximately Gaussian.
- Mechanism: The explanation function e(x, f) is locally linearized via its Jacobian J_e. For input perturbations Δx ~ N(0, σ²I), the covariance propagates as Var(e) ≈ σ² · J_e,x · J_e,x^T (Equation 8). This transforms input uncertainty through the explainer's local sensitivity matrix.
- Core assumption: The linearization holds within the perturbation radius; higher-order terms are negligible.
- Evidence anchors:
  - [abstract] "We illustrate the approach using a first-order uncertainty propagation as the analytical estimator."
  - [section 3.2] "By linearizing the perturbed explainer around x we arrive at: e(ẽx, f) ≈ e(x, f) + J_e,x · Δx."
  - [corpus] Limited direct corpus support for first-order XAI uncertainty; neighbor papers focus on perturbation calibration rather than analytical propagation.
- Break condition: When explanation nonlinearity is strong (e.g., ReLU-induced saturation on images), empirical variance plateaus and diverges from linear predictions (Case 3).

### Mechanism 2
- Claim: Gradient-based XAI methods with ReLU networks exhibit suppressed or saturated uncertainty propagation due to piecewise-linear activation structure.
- Mechanism: ReLU activations create constant-gradient regions. For small perturbations, the gradient (and thus explanation) remains unchanged, yielding near-zero empirical variance (Case 2). For images, the saturation effect keeps variance elevated regardless of σ² (Case 3).
- Core assumption: The network architecture uses ReLU or similar piecewise-linear activations; the perturbation magnitude stays within ReLU linear regions or crosses many regions inconsistently.
- Evidence anchors:
  - [section 5.2] "This occurs for Saliency and GuidedBackProp when applied to the tabular dataset... the gradient is constant for small perturbations leading to constant explanations."
  - [section 5.3] "By removing the ReLU activation in the CNN, this plateau in the gradient-based methods turns either into Case 1 or Case 2."
  - [corpus] No corpus papers directly address ReLU-specific uncertainty saturation in XAI.
- Break condition: Removing ReLU or using smooth activations aligns analytical and empirical estimates for gradient methods.

### Mechanism 3
- Claim: Perturbation-based XAI methods (e.g., Occlusion) propagate uncertainty more reliably than gradient-based methods across variance regimes.
- Mechanism: Occlusion directly measures output sensitivity to input masking, bypassing gradient computation. Its explanation variance scales proportionally with σ² and matches first-order predictions, because the method's sensitivity is not mediated by potentially-vanishing or saturated gradients.
- Core assumption: The perturbation method's internal parameters (patch size, stride) are held fixed; uncertainty sources are limited to input or model weights.
- Evidence anchors:
  - [section 5.1] "We also observe this behavior for perturbations in input variables x, however, only for explanations computed with Occlusion (on both tasks)."
  - [figure 2] Shows aligned analytical and empirical MUE curves for Occlusion across both MNIST and Auto MPG.
  - [corpus] "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration" links perturbation reliability to model calibration but does not compare to analytical propagation.
- Break condition: If occlusion patch size becomes very large relative to input, explanations may degrade independently of uncertainty propagation.

## Foundational Learning

### Concept: Jacobian-based sensitivity analysis
- Why needed here: The entire analytical framework depends on computing J_e,x and J_e,ω to propagate covariance through linear approximations.
- Quick check question: Given a scalar function f(x) = x³, what is the first-order variance estimate at x=1 if Var(x)=0.01? (Answer: (3x²)²·0.01 = 0.09)

### Concept: Covariance propagation under affine transformations
- Why needed here: The formula Var(Ax + b) = A·Var(x)·A^T is applied to propagate input/model uncertainty to explanation covariance.
- Quick check question: If y = 2x₁ + 3x₂ and Cov(x) = I₂, what is Var(y)? (Answer: 2² + 3² = 13)

### Concept: ReLU gradient properties
- Why needed here: Understanding why gradient-based methods show zero or saturated uncertainty requires knowing that ReLU has zero gradient for negative inputs and constant gradient (1) for positive inputs.
- Quick check question: Why might small input perturbations not change the gradient of a ReLU network? (Answer: Within each linear region, gradients are constant; perturbations too small to cross region boundaries leave gradients unchanged.)

## Architecture Onboarding

### Component map:
Explanation function e_θ(x, f) → Jacobian blocks (J_e,x, J_e,ω) → Analytical estimator (Σ_lin) + Empirical estimator (Σ_MC) → MUE metric

### Critical path:
1. Select XAI method and compute base explanation e(x, f)
2. Compute Jacobian via finite differences (perturb each input dimension/weight, recompute explanation)
3. For each σ² in regime: compute Σ_lin (analytical) and Σ_MC (N Monte Carlo samples)
4. Compute MUE for both; compare curves to diagnose propagation behavior

### Design tradeoffs:
- **Finite differences vs. automatic differentiation**: Paper uses finite differences (simpler, model-agnostic) but notes autodiff could improve accuracy (Footnote 6)
- **Computational cost**: Jacobian computation scales with input/weight dimension; analytical approach amortizes cost across all σ² values, while MC requires fresh samples per σ²
- **Perturbation scope**: Study limits weight perturbations to final dense layer; full-network perturbations would increase computational burden and potentially introduce higher-order effects

### Failure signatures:
- **Zero Jacobian**: J_e,x ≈ 0 yields Σ_lin ≈ 0 while Σ_MC may be non-zero (gradient methods on tabular ReLU networks)
- **Plateaued empirical variance**: Σ_MC stays flat as σ² → 0 while Σ_lin decreases (gradient methods on image CNNs with ReLU)
- **Distribution non-Gaussianity**: If perturbed explanations are non-Gaussian, first-order approximation may misestimate true variance (not extensively tested in paper)

### First 3 experiments:
1. **Baseline alignment check**: On a simple linear model with synthetic data, verify that analytical and empirical MUE curves match across σ² regimes for all tested XAI methods. This establishes that discrepancies are due to model/explainer nonlinearity, not implementation errors.
2. **Activation ablation**: Replicate the MNIST experiment with and without ReLU (using sigmoid or no activation) to confirm that plateau behavior (Case 3) disappears, as reported in Section 5.3.
3. **Weight vs. input perturbation comparison**: For a single XAI method (e.g., Saliency), compute MUE curves for input perturbations and final-layer weight perturbations on the same model. Verify that weight perturbations consistently show Case 1 alignment (per Figure 3) while input perturbations may diverge (per Figure 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-Gaussian perturbation models (adversarial noise, heavy-tailed distributions, heteroscedastic noise) affect uncertainty propagation in XAI methods compared to the Gaussian assumption?
- Basis in paper: [explicit] "Future research could extend our framework to non-Gaussian perturbations such as adversarial noise, multimodal distributions or dataset-specific perturbation models and provide a more comprehensive view of explanation robustness."
- Why unresolved: The paper relies on Gaussian perturbations for mathematical tractability with first-order approximations, but real-world noise often exhibits different characteristics.
- What evidence would resolve it: Comparative experiments computing MUE under varied noise distributions across XAI methods, demonstrating whether analytical estimates remain valid under non-Gaussian assumptions.

### Open Question 2
- Question: What are appropriate tolerable uncertainty thresholds for deploying uncertainty-aware XAI in different high-stakes application domains?
- Basis in paper: [explicit] "Future work could explore how different applications tolerate uncertainty in explanations and determine appropriate thresholds for integrating UXAI into decision-making processes."
- Why unresolved: The framework quantifies MUE but provides no guidance on what MUE values render explanations unreliable for practical decision-making.
- What evidence would resolve it: Domain-specific user studies or benchmarks establishing acceptable MUE ranges for healthcare, finance, and autonomous systems applications.

### Open Question 3
- Question: How does uncertainty propagation change when accounting for intrinsic randomness in stochastic XAI methods (e.g., LIME/KernelSHAP sampling)?
- Basis in paper: [explicit] "Extending our analysis to account for uncertainties inherent in explanation methods — such as the choice of hyperparameters, sampling strategies or surrogate model approximations — would provide a more comprehensive picture of uncertainty in XAI."
- Why unresolved: The paper deliberately excludes explanation method parameters θ, leaving uncertainty from stochastic sampling processes uncharacterized.
- What evidence would resolve it: Experiments measuring combined uncertainty from input, model weights, and explanation parameters, quantifying how much additional variance stochastic sampling introduces.

### Open Question 4
- Question: Can activation functions be designed or selected to improve gradient-based XAI methods' uncertainty propagation while preserving model performance?
- Basis in paper: [inferred] The plateau effect (Case 3) and near-zero uncertainty (Case 2) in gradient-based methods are linked to ReLU activations, which "limits the sensitivity of explanations to input perturbations."
- Why unresolved: The paper documents the phenomenon but does not investigate whether alternative activations could enable reliable uncertainty propagation.
- What evidence would resolve it: Systematic comparison of MUE behavior across models with different activations (LeakyReLU, ELU, GELU) while controlling for task performance.

## Limitations

- The study uses only two simple model architectures (CNN and MLP) and five standard XAI methods, limiting generalizability to more complex models and diverse explanation approaches.
- The first-order analytical framework assumes locally linear behavior and Gaussian perturbations, which may not hold for highly nonlinear explanations or non-Gaussian uncertainty sources.
- The ReLU-specific saturation effects observed may not extend to other activation functions or deeper architectures beyond the tested CNN with single ReLU layer.

## Confidence

- **High confidence**: The comparison framework between analytical and empirical uncertainty estimators is sound and reproducible. The identification of three distinct propagation scenarios is well-supported by empirical evidence across both datasets and multiple XAI methods.
- **Medium confidence**: The mechanistic explanations for why gradient-based methods show suppressed/saturated uncertainty (ReLU piecewise-linearity) are plausible but not extensively validated across architectures or activation functions.
- **Low confidence**: The implications for real-world XAI applications assume that the studied uncertainty patterns generalize to more complex models, larger datasets, and diverse domains beyond vision and simple tabular regression.

## Next Checks

1. Test the three-case uncertainty propagation framework on deeper CNNs and transformer-based models to assess scalability and architecture sensitivity.
2. Evaluate alternative activation functions (sigmoid, tanh, ELU) to determine if uncertainty saturation is specific to ReLU or represents a broader architectural phenomenon.
3. Implement automatic differentiation-based Jacobian computation to compare against finite differences and quantify numerical stability across high-dimensional inputs.