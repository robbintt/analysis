---
ver: rpa2
title: 'Explainable Artificial Intelligence techniques for interpretation of food
  datasets: a review'
arxiv_id: '2504.10527'
source_url: https://arxiv.org/abs/2504.10527
tags:
- food
- data
- issn
- techniques
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews eXplainable Artificial Intelligence (XAI) techniques
  applied to food quality datasets, addressing the lack of transparency in AI models
  used in food engineering. The authors present a taxonomy classifying research by
  data types (tabular, pictorial, spectral, time series) and explanation methods (numerical,
  rule-based, visual, mixed).
---

# Explainable Artificial Intelligence techniques for interpretation of food datasets: a review

## Quick Facts
- arXiv ID: 2504.10527
- Source URL: https://arxiv.org/abs/2504.10527
- Reference count: 40
- Primary result: Systematic review and taxonomy of XAI methods applied to food quality datasets, highlighting dominance of visual explanations for images and gaps in spectral/time-series analysis.

## Executive Summary
This survey provides a comprehensive review of eXplainable Artificial Intelligence (XAI) techniques applied to food quality datasets, addressing the critical need for transparency in AI models used in food engineering. The authors present a taxonomy classifying research by data types (tabular, pictorial, spectral, time series) and explanation methods (numerical, rule-based, visual, mixed). Analyzing over 100 papers, they identify trends such as the dominance of visual explanations for pictorial data and the need for better XAI methods for spectral and time series data. The study highlights challenges like interpretability and suggests future directions, including rule-based and concept-based explanations. This work bridges the gap between XAI and food quality, providing insights to improve model transparency and trust.

## Method Summary
The authors conducted a systematic literature review using Google Scholar and Scopus databases, searching for papers combining "XAI" with "food quality" keywords over the past decade. They used snowballing references from foundational XAI papers (LIME, SHAP, CAM, PDP, LRP) and excluded papers not explicitly describing a specific XAI technique in food quality contexts. The retained papers were mapped to a taxonomy classifying them by data type (tabular, pictorial, spectral, time series) and explanation type (numerical, visual, rule-based, mixed).

## Key Results
- XAI techniques such as SHAP and Grad-CAM can pinpoint which spectral wavelengths or image regions contribute most to a prediction.
- Most applications of XAI in food safety focus on providing visual explanations due to the frequent use of pictorial data.
- Spectral data and time series analysis remain under-explored areas for XAI in food quality tasks.

## Why This Works (Mechanism)

### Mechanism 1: Post-Hoc Feature Attribution via Gradients and Surrogates
XAI techniques operate by approximating or extracting the contribution of specific input features (pixels, wavelengths, variables) to a model's output, provided the model is differentiable or locally interrogable. Techniques like Grad-CAM utilize gradient flows into the final convolutional layer to highlight spatial regions influencing a prediction. Conversely, model-agnostic methods like LIME and SHAP generate local surrogate models (e.g., linear regressors) to perturb inputs and observe output shifts, assigning importance weights to features. This mechanism fails if features are highly collinear (making attribution ambiguous) or if the model relies on spurious correlations rather than the target object.

### Mechanism 2: Data-Modality to Explanation-Format Mapping
Effective interpretation requires matching the explanation format to the data modality to leverage human cognitive strengths. The system routes inputs based on data type: Pictorial data (images) is routed to visual explainers (heatmaps via Grad-CAM) to intuitively highlight defects; Tabular data is routed to numerical explainers (SHAP values) to quantify factor influence. This alignment ensures the "explanation token" is consumable by the user. The mechanism breaks when data is high-dimensional but non-visual (e.g., complex spectral data) and forced into a visual format that lacks spatial semantic meaning.

### Mechanism 3: Local Justification for Trust Calibration
XAI facilitates the adoption of complex AI in critical sectors (like food safety) by validating individual predictions ("local" explanations) rather than requiring full transparency of the global model logic. Instead of explaining the entire mathematical function of a Deep Neural Network, the system provides justification for a specific instance (e.g., "This specific apple is rotten because *these* pixels are dark"). This allows inspectors to verify critical cases without needing to understand the model's global weights. If the local explanation is consistent for a wrong prediction (e.g., highlighting a shadow as a defect), trust is eroded faster than if no explanation were provided.

## Foundational Learning

**Concept: The Interpretability-Accuracy Trade-off**
Why needed: The paper posits that highly expressive models (DL, Transformers) offer the accuracy needed for complex food data but sacrifice inherent interpretability. Understanding this trade-off is the prerequisite for selecting an XAI method.
Quick check: If you need 99% accuracy on hyperspectral image classification, can you use a Linear Regression model? (No, you likely need a CNN, which necessitates a post-hoc XAI method like Grad-CAM).

**Concept: Model-Agnostic vs. Model-Specific XAI**
Why needed: Different AI architectures require different probing techniques. You cannot apply Grad-CAM to a Random Forest, nor is SHAP always the most efficient choice for a CNN if speed is critical.
Quick check: You are using a Support Vector Machine (SVM) on tabular soil data. Can you use Grad-CAM? (No, Grad-CAM is specific to CNNs/Deep Learning; you should use LIME or SHAP).

**Concept: Food Quality Taxonomy**
Why needed: The "mechanism" of the solution depends on the goal. Detecting a contaminant (Safety) requires different validation than predicting crop yield (Traceability).
Quick check: A model detects mercury in fish. Is this an Authenticity task or a Safety task? (Safety).

## Architecture Onboarding

**Component map:** Food Dataset (Pictorial, Spectral, Tabular, Time Series) -> AI Model (CNN for images, RF/XGBoost for tabular) -> XAI Layer (Grad-CAM/LIME -> Heatmap; SHAP/Permutation Importance -> Feature Ranking; Decision Trees/Fuzzy Logic -> IF-THEN logic) -> Explanation (Visual/Numerical/Rule) + Prediction.

**Critical path:** The alignment of **Data Type** -> **AI Model** -> **XAI Technique**. For example, Pictorial Data -> CNN -> Grad-CAM (Visual Explanation) is the dominant path identified in the review.

**Design tradeoffs:**
- *Expressivity vs. Interpretability:* Using a Transformer (High Accuracy) forces reliance on complex post-hoc approximations; using Linear Regression (High Interpretability) may miss complex non-linear patterns in spectral data.
- *Local vs. Global:* Local explanations (LIME) are precise for single instances but expensive to aggregate for system-wide behavior. Global explanations are cheaper but less precise for anomaly detection.

**Failure signatures:**
- Noisy Heatmaps: Grad-CAM highlights background or watermarks instead of the food defect (indicates model overfitting to dataset artifacts).
- Complex Rule Explosion: Rule-based explanations generate thousands of IF-THEN statements, becoming incomprehensible to engineers.
- Contradiction: LIME and SHAP provide conflicting feature importance for the same prediction.

**First 3 experiments:**
1. **Visual Validation (Pictorial):** Train a MobileNetV2 classifier on a fruit defect dataset (e.g., PlantVillage). Apply Grad-CAM to verify if the "attention" aligns with the physical lesions or if it focuses on leaf edges/background.
2. **Feature Importance (Tabular):** Train a Random Forest on wine quality tabular data. Use SHAP summary plots to identify the top 3 chemical features driving quality scores.
3. **Method Comparison (Spectral/Time Series):** Train a 1D-CNN on spectral data for freshness detection. Compare LIME vs. SHAP explanations to see which method better identifies the specific wavelengths associated with spoilage.

## Open Questions the Paper Calls Out

**Open Question 1:** How can XAI techniques be adapted to effectively interpret spectral and time-series data in food quality tasks? The review notes spectral data "has yet to be adequately addressed" and few XAI methods exist for time series analysis in this field.

**Open Question 2:** What hierarchical explainability approaches are required to interpret models utilizing multi-modal data fusion? The authors suggest "hierarchical-based explainability approaches" are needed to manage the complexity of integrating chemical, physical, and sensory data.

**Open Question 3:** Can rule-based and concept-based explanations be leveraged to uncover causal relationships in food properties? The text states rule-based techniques offer potential for causal insight but are significantly underutilized in current research.

## Limitations
- The survey's comprehensiveness is limited by the specific keyword search strategy and database selection, potentially missing relevant papers using alternative terminology.
- The taxonomic classification relies on author interpretation of paper content, introducing subjectivity in categorizing data types and explanation methods.
- The analysis focuses on categorizing existing work rather than providing quantitative measures of XAI method effectiveness across different food data modalities.

## Confidence
- **High Confidence:** The mechanism of post-hoc feature attribution (Mechanism 1) is well-supported by the corpus and standard XAI literature.
- **Medium Confidence:** The data-modality to explanation-format mapping (Mechanism 2) is logically sound but relies on an assumption about domain expert cognitive preferences that is not directly tested in the paper.
- **Low Confidence:** The specific claims about the dominance of visual explanations for pictorial data and the identified gaps for spectral/time series data are based on the surveyed literature count, which may be biased by the search methodology.

## Next Checks
1. **Reproduction Check:** Replicate the database search using the same keywords and time restrictions to verify the initial paper set and assess the sensitivity of the results to search variations.
2. **Classification Consistency:** Independently review a random sample of the classified papers to assess the inter-rater reliability of the data type and explanation method categorization, especially for the "mixed" category.
3. **Method Effectiveness Test:** Conduct a small empirical study comparing the performance and interpretability of a Grad-CAM explanation versus a SHAP explanation on a common food quality dataset (e.g., fruit defect classification) to test the assumed superiority of visual explanations for images.