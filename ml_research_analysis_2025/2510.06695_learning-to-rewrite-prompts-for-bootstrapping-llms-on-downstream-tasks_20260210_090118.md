---
ver: rpa2
title: Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks
arxiv_id: '2510.06695'
source_url: https://arxiv.org/abs/2510.06695
tags:
- llms
- tasks
- rewriting
- input
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for improving Large Language
  Model (LLM) performance on downstream tasks by rewriting the input component of
  prompts rather than the instruction component. The approach addresses the limitation
  that existing prompt engineering methods primarily focus on optimizing instructions,
  which is less effective for tasks like machine translation where the input component
  plays a critical role.
---

# Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks

## Quick Facts
- arXiv ID: 2510.06695
- Source URL: https://arxiv.org/abs/2510.06695
- Reference count: 10
- Primary result: Rewriting input components (not instructions) of prompts significantly improves LLM performance on downstream tasks like machine translation and summarization.

## Executive Summary
This paper introduces ROI (Rewrite Input), a novel method for improving LLM performance on downstream tasks by rewriting the input component of prompts rather than the instruction component. The approach addresses the limitation that existing prompt engineering methods primarily focus on optimizing instructions, which is less effective for tasks like machine translation where the input component plays a critical role. The method employs a small-parameter rewriting model trained using a back-translation-based strategy, where the original training data is rewritten and used to train the model to learn the LLM's preferences for input data. A filtering mechanism using similarity metrics (BLEU, Edit Rate, and ROUGE-L) is applied to remove hallucinations and maintain data quality. Experimental results on multiple machine translation datasets (IT, Medical, Koran, and Law domains) show significant improvements in BLEU scores and Edit Rates compared to using original inputs. The method also demonstrates effectiveness on summarization tasks (XSum) and natural language understanding tasks (GLUE benchmark), with improvements in RougeL, accuracy, and F1 scores across different LLMs.

## Method Summary
The ROI method employs a back-translation-based strategy to train a small rewriting model. First, an LLM generates synthetic training data by back-translating the target outputs to create "preferred" input formats. A small language model (e.g., mBART, mT5) is then trained on pairs of original inputs and these LLM-generated synthetic inputs. During inference, the rewriting model processes raw inputs and passes them to the LLM. A filtering mechanism compares rewritten inputs against originals using BLEU, Edit Rate, and ROUGE-L metrics, discarding rewrites that deviate too far from the original to prevent hallucinations. The method uses hyperparameters including learning rate 2e-5, batch size 4, dropout 0.3, temperature 0.1, and beam size 4.

## Key Results
- Significant BLEU score improvements across multiple translation datasets (IT, Medical, Koran, Law domains) compared to using original inputs
- Improved Edit Rates and RougeL scores on summarization (XSum) and NLU tasks (GLUE benchmark)
- Effectiveness across different LLMs with varying parameter sizes (7B-13B models)
- Filtering mechanism successfully removes hallucinated rewrites while preserving performance gains

## Why This Works (Mechanism)

### Mechanism 1: Input Preference Alignment via Style Transfer
LLMs may exhibit higher performance on inputs that match their internal representation preferences, even when semantic content is identical. The system posits that LLMs possess an inherent "preference" for specific input styles. By rewriting user inputs to match this preferred distribution (rather than optimizing the instruction), the model processes the task more efficiently. Core assumption: The LLM generates higher-quality outputs when the input style closely matches the data distribution it was trained on or finds "accessible," and this style can be approximated by a rewriter model. Evidence: Section 3.1 states "We hypothesize that LLMs have their own preferences regarding the data they process, which may diverge from conventional human expression patterns."

### Mechanism 2: Knowledge Distillation via Back-Translation
A small language model (SLM) can learn to rewrite inputs effectively by imitating the reverse-generation process of the target LLM. The method constructs a synthetic training set where the "target" for the rewriter is generated by the LLM itself (via back-translation of the ground-truth output). The rewriter learns to map raw inputs to this LLM-generated "ideal" input format, effectively distilling the LLM's input preferences into a smaller, efficient model. Core assumption: The input generated by the LLM during back-translation represents an "optimal" or "preferred" input form that yields better performance than the original raw data. Evidence: Section 3.1 states "We draw inspiration from back translation... utilize LLMs to write back the training set output as input... This data is used to train the rewriting model."

### Mechanism 3: Consistency Filtering via Similarity Thresholds
Rewriting effectiveness is non-universal; performance gains are realized only when semantic drift is explicitly constrained. The pipeline includes a filter that compares the rewritten input against the original using metrics like BLEU and Edit Distance. Low-similarity rewrites (indicating potential hallucinations or excessive alteration) are discarded in favor of the original input. Core assumption: High-quality rewrites must preserve semantic fidelity; divergence from the original text usually indicates noise rather than useful reformulation. Evidence: Section 3.2 states "When the similarity... is low, it might be because LLMs have outputted hallucinations... we replace them with the original text."

## Foundational Learning

**Concept**: Back-Translation (in NMT)
- Why needed here: The core training signal for the rewriter relies on reversing the standard translation direction (Target → Source) using the LLM to generate synthetic training pairs.
- Quick check question: How does generating synthetic input from an output help train a model to rewrite *new*, unseen inputs?

**Concept**: Instruction vs. Input Components in Prompting
- Why needed here: The paper's central thesis relies on distinguishing the task definition (Instruction) from the data to be processed (Input), arguing that optimization efforts have been disproportionately focused on the former.
- Quick check question: In a summarization task, which part of the prompt does this method propose to rewrite?

**Concept**: Hallucination in Generation
- Why needed here: The filtering mechanism is explicitly designed to mitigate hallucinations introduced by the rewriting model, which is critical for maintaining data integrity.
- Quick check question: If a rewriting model changes "The bank is closed" to "The financial institution is near a river," which component of the ROI pipeline would catch this error?

## Architecture Onboarding

**Component map**: Data Constructor (LLM back-translation) → Rewriter (SLM training) → Filter (similarity check) → Inference Pipeline (rewritten input → LLM)

**Critical path**: The quality of the Data Constructor determines the ceiling of the rewriter's utility. If the Target LLM generates poor synthetic inputs during back-translation, the Rewriter learns to degrade performance. The Filter is the primary safety rail.

**Design tradeoffs**:
- Rewriter Size: Larger rewriters (mBART-large) capture more nuance but increase latency; smaller models (Tiny-mBART) may fail to learn complex rewriting patterns (See Table 4)
- Filtering Threshold: High thresholds are conservative (safety) but may reject helpful rewrites; low thresholds allow more noise

**Failure signatures**:
- Semantic Drift: Output meaning changes drastically. (Check: Is the filter threshold too low?)
- Revert Loop: System constantly falls back to original input. (Check: Is the rewriter too aggressive or filter too strict?)
- Domain Mismatch: Performance drops on specialized domains (e.g., Koran/Law) seen in Table 2/4

**First 3 experiments**:
1. Back-Translation Quality Audit: Before training the rewriter, manually inspect (Original, Synthetic) pairs to verify the Target LLM produces "cleaner" or "preferred" inputs rather than hallucinations
2. Threshold Sweep: Run the filtering module on a validation set with varying BLEU/ROUGE thresholds (e.g., 0.3, 0.5, 0.8 as in Figure 3) to find the "retain vs. rewrite" sweet spot
3. Ablation on Rewriter Capacity: Train both a large (mBART) and small (mT5/Tiny) rewriter to measure the performance gap against inference cost, referencing results in Table 4

## Open Questions the Paper Calls Out

**Open Question 1**: Does the ROI method retain its effectiveness when applied to proprietary, large-scale LLMs (e.g., GPT-3.5/4) compared to the open-source 7B-13B models tested?
- Basis: The authors explicitly state in the Limitations section: "we have not yet conducted experiments on larger-scale LLMs like GPT-3.5"
- Why unresolved: It is unclear if the rewriting strategy provides marginal gains for models that may already possess superior input understanding or if the method is only beneficial for smaller, fine-tuned models like Alpaca
- What evidence would resolve it: Benchmark results on translation and NLU tasks using the ROI framework with GPT-3.5 or GPT-4 as the target model

**Open Question 2**: Can the rewriting strategy be successfully adapted for tasks involving complex reasoning, planning, or multi-turn dialogue?
- Basis: The authors state: "Our method is not applicable to tasks involving reasoning, planning... [and] is primarily limited to single-turn question-answering language tasks"
- Why unresolved: The current methodology focuses on tasks where instructions are fixed and input is paramount. It is unknown if rewriting inputs disrupts the logical dependencies required for reasoning or the context history in multi-turn interactions
- What evidence would resolve it: A modified ROI framework demonstrating improved performance on reasoning benchmarks (e.g., GSM8K) or multi-turn dialogue datasets

**Open Question 3**: Can small-parameter rewriting models be optimized to perform effectively on NLU tasks, removing the need to use the LLM itself for rewriting?
- Basis: The methodology section notes that "using smaller models for rewriting yields unsatisfactory results" on NLU tasks, forcing a fallback to the heavier LLM, but does not investigate why small models fail here
- Why unresolved: Determining if this failure is due to model capacity or training data construction is necessary to reduce the computational overhead of the framework for NLU tasks
- What evidence would resolve it: An ablation study identifying the failure modes of small rewriting models on sentiment analysis, followed by a training method that enables a small model to match LLM-based rewriting quality

## Limitations
- Limited to single-turn question-answering language tasks and not applicable to reasoning, planning, or multi-turn dialogue
- Performance gains vary significantly across domains, with specialized domains (Koran, Law) showing smaller improvements
- Computational overhead of the rewriting pipeline, particularly when small models cannot effectively rewrite NLU tasks

## Confidence
**High Confidence** in: The core observation that input rewriting can improve LLM performance compared to using raw inputs, and that filtering mechanisms effectively remove hallucinated rewrites. The experimental results across multiple translation datasets provide robust evidence for this claim.

**Medium Confidence** in: The scalability of the approach to non-translation tasks and the generalizability across different LLM architectures. While results are shown for summarization and NLU, the methodology's effectiveness for these tasks appears less thoroughly validated than for translation.

**Low Confidence** in: The optimal configuration of filtering thresholds and the precise impact of rewriter model size on different domains. The paper tests thresholds but doesn't provide clear guidance on selecting optimal values for new tasks or datasets.

## Next Checks
**Validation Check 1: Back-Translation Quality Audit**: Before training the rewriter, conduct a systematic analysis of the LLM-generated synthetic inputs across different domains. Manually inspect 100+ (original, synthetic) pairs for each dataset to quantify hallucination rates and stylistic preferences. Correlate these findings with subsequent rewriter performance to establish whether back-translation quality is the limiting factor.

**Validation Check 2: Domain Adaptation Protocol**: Test whether domain-specific rewriting models significantly outperform general-purpose models. Train separate rewriters for IT, Medical, Koran, and Law domains using their respective back-translated data, then compare performance against a single multi-domain model. This will clarify whether the domain generalization gap is addressable through specialized training.

**Validation Check 3: Cross-Task Applicability Study**: Apply the rewriting methodology to at least two additional task types beyond those demonstrated (e.g., code generation and question answering). For each new task, document the prompt structure, measure performance gains, and analyze whether the input rewriting mechanism requires task-specific adaptations beyond simple translation-style prompts.