---
ver: rpa2
title: 'OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System'
arxiv_id: '2601.08288'
source_url: https://arxiv.org/abs/2601.08288
tags:
- generation
- humor
- stand-up
- system
- comedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OpenMic is a multi-agent system for generating Chinese stand-up\
  \ comedy from a user topic. It addresses the difficulty of producing humorous, culturally\
  \ grounded, and stage-ready performances by decomposing the pipeline into specialized\
  \ agents\u2014AudienceAnalyzer, ComedyDirector, JokeWriter, PerformanceCoach, and\
  \ QualityController\u2014working in iterative refinement loops."
---

# OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System

## Quick Facts
- **arXiv ID**: 2601.08288
- **Source URL**: https://arxiv.org/abs/2601.08288
- **Reference count**: 6
- **Primary result**: Multi-agent system with RAG and low-temperature generation produces Chinese stand-up scripts with high humor, coherence, and reactivity scores

## Executive Summary
OpenMic is a multi-agent system that generates Chinese stand-up comedy from user topics by decomposing the task into specialized roles: AudienceAnalyzer, ComedyDirector, JokeWriter, PerformanceCoach, and QualityController. The system uses retrieval-augmented generation (RAG) with a triadic inner-conversation to ground humor in retrieved materials, and a fine-tuned JokeWriter operating at low temperature (0.1) to maintain coherence. Experimental results show high performance across multiple dimensions including persona fidelity (82.5), humor (95.5), reactivity (88.0), and coherence (97.5), with multi-round refinement improving quality within three iterations.

## Method Summary
The system implements a 5-agent AutoGen pipeline where each agent has a specific role in the comedy generation process. A shared blackboard persists state across agents while a "Secret Blackboard" isolates noisy RAG artifacts. The JokeWriter is fine-tuned with QLoRA (r=16, α=32) on completion-only loss using Chinese crosstalk-to-talkshow converted scripts. RAG employs a triadic inner-conversation (retrieval → scoring → refinement) to filter and distill comedic materials before injection. The pipeline iterates through multi-round refinement with QualityController providing targeted feedback, converging within three rounds. The system outputs structured performance scripts annotated with timing and delivery cues, which can be rendered into narrated comedy videos.

## Key Results
- Low sampling temperature (0.1) with RAG grounding yields highest scores: persona fidelity (82.5), humor (95.5), reactivity (88.0), coherence (97.5)
- Multi-round refinement improves quality, converging within three iterations
- End-to-end pipeline demonstrates both script and video generation capability
- System successfully generates 3-5 minute Chinese stand-up performances with structured annotations

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent role decomposition enables controllable optimization of competing objectives in humor generation. Five specialized agents operate on a shared blackboard with typed outputs, each optimizing a narrow sub-objective rather than a single model attempting all simultaneously. This prevents information loss in long context and allows localized failure repair.

### Mechanism 2
RAG with triadic inner-conversation and "Secret Blackboard" improves comedic signal-to-noise ratio by filtering and distilling retrieved materials before injection. Raw retrieval candidates are scored by a dedicated LLM agent for comedic potential, then refined into abstract building blocks, isolating noisy intermediate artifacts from downstream agents.

### Mechanism 3
Low sampling temperature (0.1) with RAG grounding yields higher coherence and reactivity by constraining model drift. The fine-tuned JokeWriter operates at low temperature, reducing stochastic exploration and keeping generation tightly coupled to retrieved materials, trading creative variance for faithfulness to comedic building blocks.

## Foundational Learning

- **Incongruity–Resolution Theory**: Humor is framed as controlled expectation violation with resolvable reinterpretation; understanding this clarifies why multi-step reasoning and timing matter. Quick check: Can you sketch a "backdoor-style" joke structure where the bridge entity is a phonetic pun versus a "frontdoor-style" multi-hop conceptual link?

- **Blackboard Pattern in Multi-Agent Systems**: OpenMic uses a shared blackboard for persistent state across agents; knowing this pattern helps debug why an agent's output may not affect others. Quick check: If AudienceAnalyzer updates the taboo list mid-iteration, which agents must re-read the blackboard before acting?

- **QLoRA (Quantized Low-Rank Adaptation)**: The JokeWriter is specialized via QLoRA; understanding rank, alpha, and completion-only loss clarifies what the fine-tuning can and cannot change. Quick check: If training loss drops but humor scores don't improve, what hypothesis would you test about the training data or loss masking?

## Architecture Onboarding

- **Component map**: Topic → AudienceAnalyzer → ComedyDirector → RAG retrieval/scoring/refinement → JokeWriter → PerformanceCoach → QualityController → (loop if REVISION) → final script → optional video synthesis
- **Critical path**: User topic flows through AudienceAnalyzer (persona/tabu), ComedyDirector (structure), RAG triad (materials), JokeWriter (draft), PerformanceCoach (annotations), QualityController (feedback), with potential iteration
- **Design tradeoffs**: Low temperature increases coherence but may reduce novelty for under-retrieved topics; multi-round iteration improves quality but adds latency; fine-tuning improves setup–punchline structure but reduces flexibility to out-of-domain topics
- **Failure signatures**: High temperature + weak retrieval → generic tropes, low reactivity; QualityController overly strict → excessive iterations; Secret Blackboard not properly isolated → context window overflow
- **First 3 experiments**: 1) Replicate temperature sweep (0.1, 0.3, 0.5, 0.7, 0.9) on held-out topics; 2) Ablate Secret Blackboard by passing raw retrieval candidates directly to JokeWriter; 3) Test convergence ceiling with max iterations set to 5

## Open Questions the Paper Calls Out

### Open Question 1
Does the LLM-as-Judge evaluation framework correlate with human audience ratings for Chinese stand-up comedy? The paper uses a "senior executive producer persona" powered by Grok-4 for multi-dimensional scoring but provides no human evaluation baseline or correlation study.

### Open Question 2
Does the multi-agent RAG approach successfully bridge the "empathy-novelty gap" identified in prior comedy generation research? While OpenMic reports high humor scores, no ablation specifically measures empathy versus novelty trade-offs, and no human study validates emotional resonance.

### Open Question 3
What is the optimal sampling temperature for balancing retrieval grounding against creative divergence in comedy generation? The paper concludes low temperature with RAG is optimal but this represents a trade-off, not a resolution—different comedic goals may require different temperature settings.

### Open Question 4
Can the multi-agent framework and RAG architecture generalize to stand-up comedy in other languages and cultural contexts? The system is explicitly designed for Chinese comedic conventions; no experiments test the architecture on English, Japanese, or other comedy traditions.

## Limitations
- Corpus dependency on CFunSet and converted crosstalk scripts raises concerns about topical diversity and novelty generation for under-represented themes
- Evaluation relies on LLM-as-Judge scores without human validation or correlation studies
- System is explicitly designed for Chinese stand-up and may not generalize to other languages or cultural contexts
- RAG filtering effectiveness lacks quantitative ablation evidence demonstrating the filtering step's contribution

## Confidence

- **Multi-agent decomposition benefits**: Medium confidence (theoretical arguments but lacks direct empirical comparison to single-agent baselines)
- **RAG with triadic filtering**: Low confidence (mechanism described but no quantitative evidence of filtering effectiveness)
- **Low temperature superiority**: High confidence (directly supported by experimental results)
- **Multi-round refinement convergence**: Medium confidence (reports convergence within 3 rounds but doesn't explore task-dependency or failure modes)

## Next Checks

1. **Human evaluation validation**: Conduct blind human evaluations comparing OpenMic outputs against single-agent generation at equivalent temperatures to determine whether improvements come from multi-agent structure versus RAG grounding and temperature control.

2. **Ablation of RAG filtering**: Remove the LLM scoring and refinement steps from the RAG pipeline, feeding raw retrieval candidates directly to JokeWriter, to measure changes in context window usage, generation quality, and topic coverage.

3. **Topic diversity stress test**: Systematically test the system across topics spanning different retrieval corpus densities (high, medium, low) to quantify how temperature, RAG filtering, and multi-round refinement interact when retrieval returns sparse or irrelevant results.