---
ver: rpa2
title: Beyond Models! Explainable Data Valuation and Metric Adaption for Recommendation
arxiv_id: '2502.08685'
source_url: https://arxiv.org/abs/2502.08685
tags:
- data
- recommendation
- value
- performance
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data quality issues in recommender systems
  by proposing an explainable and versatile framework for data valuation and metric
  adaptation. The framework, called DVR, employs a data valuator that calculates Shapley
  values to assess data quality and a metric adapter based on reinforcement learning
  to handle various evaluation metrics, including non-differentiable ones.
---

# Beyond Models! Explainable Data Valuation and Metric Adaption for Recommendation

## Quick Facts
- arXiv ID: 2502.08685
- Source URL: https://arxiv.org/abs/2502.08685
- Reference count: 35
- Primary result: Proposed DVR framework improves recommendation performance across multiple metrics (accuracy, diversity, fairness) with up to 34.7% NDCG gains over state-of-the-art methods

## Executive Summary
This paper introduces DVR, an explainable data valuation and metric adaptation framework for recommender systems. The framework employs a game-theoretic approach using Shapley values to assess data quality and a reinforcement learning-based metric adapter to optimize both differentiable and non-differentiable evaluation metrics. Extensive experiments on four real-world datasets demonstrate significant improvements in recommendation performance across accuracy, diversity, and fairness metrics compared to state-of-the-art methods.

## Method Summary
DVR uses bilevel optimization where the inner loop trains a recommender model (BPRMF, NeuMF, MGCF, or LightGCN) on selected samples via BPR loss, while the outer loop updates a data valuator using REINFORCE with evaluation metrics as rewards plus MSE loss predicting batch BPR loss. The data valuator employs a HarsanyiNet-style architecture with stacked Harsanyi blocks to efficiently compute Shapley values for sample quality assessment. These Shapley values are normalized and used as Bernoulli sampling probabilities for sample selection, enabling optimization of both differentiable and non-differentiable metrics.

## Key Results
- DVR achieves up to 34.7% improvements in NDCG@20 compared to state-of-the-art methods
- The framework demonstrates robustness in enhancing accuracy, diversity, and fairness metrics simultaneously
- Results are validated across four real-world datasets (Beauty, CD, LastFM, Gowalla) with various recommendation backbones

## Why This Works (Mechanism)

### Mechanism 1: Shapley Value-Based Data Quality Attribution
Training samples that contribute more to model performance are higher quality; Shapley values provide an interpretable quantification of this contribution. Each training triplet is treated as a "player" in a cooperative game where model performance is the outcome. The Shapley value computes the average marginal contribution across all possible sample coalitions within a batch. A HarsanyiNet-style architecture with stacked Harsanyi blocks captures AND relationships between neurons, enabling efficient Shapley computation in a single forward pass without exponential retraining.

### Mechanism 2: Reinforcement Learning for Metric Adaptation
Treating evaluation metrics as RL rewards enables optimization of both differentiable and non-differentiable metrics without requiring gradient flow through the metric itself. A Bernoulli sampling policy parameterized by normalized Shapley values selects samples for training. The REINFORCE policy gradient uses the evaluation metric as the reward signal to update the data valuator. This decouples metric optimization from backpropagation—only the sampling policy needs gradients, not the metric.

### Mechanism 3: Bilevel Optimization with Sample Selection
Joint optimization of data valuator and recommendation model via bilevel structure improves data utilization efficiency. Inner optimization trains the recommender on selected samples using BPR loss. Outer optimization updates the data valuator using both MSE loss (predicting batch loss from Shapley values) and the evaluation metric reward. Iterative alternation between inner and outer steps allows the valuator to adapt to model state changes.

## Foundational Learning

- **Shapley Values and Cooperative Game Theory**
  - Why needed here: Core to understanding how DVR attributes contribution to individual samples without black-box heuristics
  - Quick check question: Can you explain why Shapley values require considering all possible coalitions, and how Harsanyi interaction approximates this efficiently?

- **REINFORCE / Policy Gradient Methods**
  - Why needed here: Enables optimization of non-differentiable metrics by treating metric scores as rewards for a sampling policy
  - Quick check question: Why does REINFORCE only require the gradient of the log-probability of actions, not the gradient of the reward itself?

- **Bayesian Personalized Ranking (BPR) Loss**
  - Why needed here: The inner optimization uses BPR to train the base recommender; understanding triplet construction and negative sampling is essential
  - Quick check question: How does BPR differ from pointwise loss functions, and why is negative sampling critical to its formulation?

## Architecture Onboarding

- **Component map**:
  Input: User-item interaction data → batch sampling → triplets (user, positive item, negative item)
  Sample Encoder: Concatenated embeddings → MLP → sample representation
  Batch Valuator (HarsanyiNet): L stacked Harsanyi blocks with AND relationships → neuron activations → Shapley value computation
  Prediction Head: Weighted sum of neuron activations → predicted batch loss (MSE supervision)
  Metric Adapter: Normalized Shapley values → Bernoulli sampling policy → REINFORCE update using evaluation metric as reward
  Base Recommender: Any BPR-compatible model (MF, NeuMF, MGCF, LightGCN) trained on selected samples

- **Critical path**:
  1. Batch sampled from training data → encoded to representations
  2. Batch Valuator predicts Shapley values and batch loss simultaneously
  3. Normalized Shapley values parameterize sampling policy
  4. Selected samples train recommender (inner loop, BPR loss)
  5. Evaluation metric computed on validation set → reward signal
  6. REINFORCE gradient updates valuator (outer loop)
  7. Iterate until convergence

- **Design tradeoffs**:
  - Batch-level Shapley approximation vs. dataset-level: faster but may miss cross-batch interactions
  - REINFORCE vs. actor-critic: simpler implementation but higher variance gradients
  - Joint training vs. pre-trained valuator: end-to-end but requires careful balancing of inner/outer learning rates

- **Failure signatures**:
  - Shapley values collapse to uniform (valuator not learning) → check MSE loss convergence
  - Metric performance degrades while loss improves → reward signal may be misaligned with actual goal
  - High variance in sample selection across epochs → reduce learning rate or add baseline to REINFORCE

- **First 3 experiments**:
  1. **Ablation on batch size**: Test whether smaller batches (fewer coalition approximations) degrade Shapley quality; monitor cosine similarity between Shapley values and negative BPR loss
  2. **Metric-specific optimization**: Run DVR-Recall, DVR-NDCG, DVR-CC separately on a single dataset; verify that targeted metric improves without catastrophic drops in others
  3. **Backbone compatibility**: Apply DVR to both simple (MF) and complex (LightGCN) models; confirm consistent improvements to rule out architecture-specific overfitting

## Open Questions the Paper Calls Out

- **Question 1**: What is the computational overhead and convergence latency of the DVR framework compared to standard training procedures?
- **Question 2**: How sensitive is the Shapley value approximation to the architectural depth and configuration of the Harsanyi blocks?
- **Question 3**: How robust is the REINFORCE-based metric adapter against high variance in gradient estimation for sparse metrics?

## Limitations
- Scalability concerns: Batch-level Shapley computation may not generalize well to large-scale datasets
- Theoretical gaps: Limited theoretical guarantees for Shapley value attribution in batch settings
- Computational cost: No analysis of memory requirements or training time overhead for HarsanyiNet architecture

## Confidence
- **Major claims**: Medium confidence
- **Experimental results**: Medium confidence
- **Theoretical foundations**: Low confidence
- **Scalability claims**: Low confidence

## Next Checks
1. **Scale validation**: Test DVR on larger-scale datasets (e.g., MovieLens-25M) to assess whether batch-level Shapley approximations maintain quality at scale
2. **Reward stability**: Evaluate policy gradient variance across different metric rewards (accuracy vs. fairness) and implement variance reduction techniques to ensure stable training
3. **Ablation on HarsanyiNet depth**: Systematically vary the number of Harsanyi blocks (L) and neurons per block to quantify the trade-off between Shapley approximation accuracy and computational cost