---
ver: rpa2
title: 'MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training'
arxiv_id: '2510.12831'
source_url: https://arxiv.org/abs/2510.12831
tags:
- question
- country
- table
- population
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MTSQL-R1 addresses the limitations of short-horizon multi-turn
  Text-to-SQL by introducing an agentic training framework that enables long-horizon
  reasoning. The method casts the task as a Markov Decision Process where an agent
  iteratively proposes, executes, verifies, and refines SQL queries using database
  feedback and persistent dialogue memory.
---

# MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training

## Quick Facts
- arXiv ID: 2510.12831
- Source URL: https://arxiv.org/abs/2510.12831
- Reference count: 40
- Primary result: Qwen3-1.7B/4B achieve state-of-the-art results on CoSQL/SParC via agentic training

## Executive Summary
MTSQL-R1 addresses the limitations of short-horizon multi-turn Text-to-SQL by introducing an agentic training framework that enables long-horizon reasoning. The method casts the task as a Markov Decision Process where an agent iteratively proposes, executes, verifies, and refines SQL queries using database feedback and persistent dialogue memory. Training combines self-taught warm-start supervised fine-tuning with end-to-end reinforcement learning using multi-level rewards. Experiments on CoSQL and SParC show consistent improvements over strong baselines, with Qwen3-1.7B/4B achieving state-of-the-art results. The approach demonstrates superior execution accuracy and exact match rates, particularly on multi-turn dialogues and complex queries, while maintaining better generalization across domains.

## Method Summary
The method treats multi-turn Text-to-SQL as a Markov Decision Process with iterative propose→execute→verify→refine cycles. An agent interacts with a database for execution feedback and persistent dialogue memory for coherence verification. Training follows a two-phase approach: self-taught warm-start supervised fine-tuning collects rollouts, filters correct trajectories, and applies difficulty-aware sampling, followed by curriculum reinforcement learning with multi-level rewards (outcome and process). The framework uses Qwen3-1.7B/4B backbones with max 4000-token prompts and 8000-token responses.

## Key Results
- Qwen3-1.7B/4B achieve state-of-the-art results on CoSQL and SParC benchmarks
- Long-horizon reasoning significantly improves execution accuracy (79.9 vs 74.6 without EXECUTE feedback)
- Persistent dialogue memory and database execution feedback provide complementary benefits
- The framework maintains better generalization across domains compared to short-horizon approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Environment-driven feedback via database execution exposes SQL errors that would otherwise go undetected in short-horizon generation.
- **Mechanism:** The agent executes candidate SQL (EXECUTE action), observes results/errors (observation), then verifies correctness (E-VERIFY). Failures trigger SELF-CORRECT, creating a propose→execute→verify→refine loop until checks pass.
- **Core assumption:** Models can learn to interpret execution feedback (error messages, null results) and map them to SQL corrections when provided training signals.
- **Evidence anchors:** [abstract]: "performing an iterative propose→execute→verify→refine cycle until all checks pass"; [section 3.1]: "Observation. Determined by the preceding action (e.g., EXECUTE yields results/errors)"

### Mechanism 2
- **Claim:** Persistent dialogue memory enables cross-turn coherence verification by maintaining constraint/entity history for explicit self-checks.
- **Mechanism:** Long-Memory M_t-1 stores prior questions, SQL, and tool-parsed constraints. M-VERIFY action compares candidate SQL against M_t-1 to detect constraint drops, over-carries, and scope mismatches.
- **Core assumption:** Models can be trained to recognize when current SQL contradicts or ignores previously established constraints via explicit memory retrieval.
- **Evidence anchors:** [abstract]: "agent interacts with...a persistent dialogue memory for coherence verification"; [section 3.1]: "M-VERIFY: check ŷk against M_t-1 for cross-turn coherence (constraints/entities)"

### Mechanism 3
- **Claim:** Self-taught warm-start SFT with difficulty-aware reject sampling expands trajectory coverage while curriculum RL with multi-level rewards stabilizes learning on hard examples.
- **Mechanism:** Algorithm 1 collects rollouts, filters correct trajectories, samples longer trajectories for hard queries (clustering by embedding), and iteratively fine-tunes. RL phase uses curriculum (easy→hard bins) plus dense process rewards (clause F1, verification correctness) alongside sparse outcome rewards.
- **Core assumption:** Self-generated correct trajectories reflect natural error patterns better than gold-SQL-synthesized trajectories; process rewards provide denser gradients than outcome-only signals.
- **Evidence anchors:** [section 3.2.2]: "we perform difficulty-aware reject sampling...for hard items, we retain longer trajectories"; [table 2]: Ablation shows RL (Outcome + Process) outperforms RL (Outcome) alone (74.6 vs 74.0 avg EX)

## Foundational Learning

- **Concept: Markov Decision Process (MDP) for LLM agents**
  - Why needed here: The paper casts Text-to-SQL as sequential decision-making with states (dialogue history + schema + observations), actions (PROPOSE/EXECUTE/VERIFY/CORRECT/FINALIZE), and rewards. Understanding MDPs is essential to grasp why iterative verification is formalized this way.
  - Quick check question: Given state s_k = (H_t-1, schema, current question, memory, partial SQL), what action should the policy take if the last observation was an SQL execution error?

- **Concept: GRPO (Group Relative Policy Optimization) with loss masking**
  - Why needed here: The RL phase uses GRPO with token-level masking to train only on reasoning tokens, not environment outputs. This prevents the model from memorizing execution results rather than learning verification behavior.
  - Quick check question: Why would loss masking on execution observation tokens (obs_1:k-1) improve learning compared to training on all tokens?

- **Concept: Curriculum learning for sparse-reward RL**
  - Why needed here: The paper partitions training data by difficulty (success count over 20 rollouts) and trains easy→hard. This addresses credit assignment challenges where hard queries yield sparse rewards.
  - Quick check question: If a model achieves 18/20 success on a query, should it be placed in an earlier or later curriculum bin than a query with 5/20 success?

## Architecture Onboarding

- **Component map:**
  - Policy LLM (Qwen3-1.7B/4B backbone) -> Database Tool (exec_sql) -> Memory Tool (memory_retrieve) -> Reward Functions (EX/EM, clause F1, verification correctness) -> Training Pipeline (self-taught SFT -> GRPO RL)

- **Critical path:**
  1. Initialize with base LLM
  2. Run Algorithm 1 (self-taught SFT): collect rollouts → filter valid → difficulty-aware sampling → SFT → repeat N rounds
  3. Partition remaining data into curriculum bins by success count
  4. Run curriculum GRPO training: for each bin, sample trajectories → compute multi-level rewards → GRPO update with loss masking
  5. Evaluate on CoSQL/SParC (in-domain and cross-domain)

- **Design tradeoffs:**
  - **Max Interactions (4) vs. accuracy vs. latency**: More interactions allow deeper refinement but increase token length (avg 2379 tokens vs. 47 for short-horizon) and latency (16.6s vs. 0.8s)
  - **Process rewards vs. complexity**: Dense rewards improve hard-case learning but require careful design (e.g., clause F1 tables, verification correctness matrices)
  - **Self-taught vs. gold-trajectory SFT**: Self-taught reflects natural errors but risks stalling on hard queries; gold-trajectory provides coverage but may not match realistic error distributions

- **Failure signatures:**
  - **Aggregation Drift persists**: Table 7 and Fig. 7 show this error type is not reduced, indicating extra-hard aggregation queries remain unresolved
  - **Coverage gaps in SFT**: If Round 3 coverage (7,555/9,337 for CoSQL) leaves hard queries unsolved, RL may struggle with sparse rewards
  - **Token truncation**: 8000-token cap causes 6 failures; long dialogues may not complete refinement cycles
  - **Small model function-calling**: Qwen3-1.7B base scores drop from 54.3 to 20.2 without training (cannot follow long-horizon tool instructions)

- **First 3 experiments:**
  1. **Ablate EXECUTE tool**: Disable database execution, measure impact on execution accuracy. Expect EX drop (per Table 4: 79.9→74.6 for Qwen3-4B).
  2. **Vary curriculum bin size**: Test 1000 vs. 2000 vs. 4000 sample bins to find optimal difficulty granularity for stable reward progression.
  3. **Analyze trajectory length vs. difficulty correlation**: Verify Fig. 4b pattern holds—hard queries should trigger more interactions. If not, process rewards may not be guiding refinement correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-turn Text-to-SQL systems be improved to handle Aggregation Drift errors, where aggregation/ranking/window context from prior turns is lost or mutated?
- **Basis in paper:** [explicit] Page 9: "Aggregation Drift changes little, since aggregation drift-related SQL are mostly extra hard, suggesting a hard open problem on extra-hard queries and a direction for future work."
- **Why unresolved:** The current M-VERIFY action focuses on constraint and entity coherence but does not explicitly track aggregation context (GROUP BY, HAVING, ORDER BY, window functions) across dialogue turns.
- **What evidence would resolve it:** Demonstrated reduction in Aggregation Drift error rate (currently unchanged per Fig. 7) through extended verification actions or specialized memory representations for aggregation context.

### Open Question 2
- **Question:** Can long-horizon agentic training for Text-to-SQL be extended to extra-hard queries (complex nesting, multiple set operations) where current methods still show significant performance gaps?
- **Basis in paper:** [explicit] Page 10: "residual errors remain, notably Aggregation Drift... and some extra-hard cases are still unresolved." Page 7-8 shows lower accuracy on "Extra Hard" difficulty bucket.
- **Why unresolved:** Extra-hard queries may require more than 4 agent-environment interactions (current max) or denser process rewards to guide exploration through longer reasoning chains.
- **What evidence would resolve it:** Improved accuracy on the "Extra Hard" subset of SParC/CoSQL benchmarks, or analysis showing whether increased interaction limits or alternative reward designs help.

### Open Question 3
- **Question:** How does the MTSQL-R1 framework scale to larger model sizes (7B–70B+), and does the relative benefit of long-horizon reasoning diminish as base model capability increases?
- **Basis in paper:** [inferred] Experiments only report results on 1.7B and 4B Qwen3 backbones; Table 2 shows untrained long-horizon reasoning degrades 1.7B performance substantially but not 14B, suggesting model size interacts with training requirements.
- **Why unresolved:** The authors do not report results with larger fine-tuned backbones, leaving unclear whether the SFT+RL training pipeline transfers or whether stronger models may benefit less from explicit verification loops.
- **What evidence would resolve it:** Controlled experiments applying the same training pipeline to 7B, 14B, and larger variants, reporting EX/EM on identical benchmarks.

### Open Question 4
- **Question:** Can the latency overhead of long-horizon reasoning (16–28s per query vs. <1s for short-horizon SFT) be reduced while maintaining accuracy gains?
- **Basis in paper:** [explicit] Page 15: "In this work we focus on accuracy gains from long-horizon reasoning; optimizing latency/throughput is left for future work." Table 8 shows 10–30× latency increase.
- **Why unresolved:** Multiple tool calls (EXECUTE, M-VERIFY) and iterative refinement increase inference time, which may limit deployment in interactive settings.
- **What evidence would resolve it:** Techniques such as early termination, parallel verification, or distilled short-horizon models trained to mimic long-horizon behavior, with accuracy/latency trade-off curves.

## Limitations

- **Aggregation Drift persistence**: Extra-hard aggregation queries remain unresolved despite iterative verification
- **Latency overhead**: Long-horizon reasoning increases inference time by 10-30× compared to short-horizon approaches
- **Small model sensitivity**: Base 1.7B models show significant performance degradation without training (54.3→20.2), indicating scalability concerns

## Confidence

- **High Confidence**: Environment-driven verification improves execution accuracy (79.9→74.6 EX drop when removing execution feedback)
- **Medium Confidence**: Memory-guided coherence verification contribution is less directly measured (M-VERIFY + E-VERIFY shows 75.3→78.4 improvement, but effects not isolated)
- **Low Confidence**: Scalability claims for small models are partially undermined by base model weakness and lack of larger model testing

## Next Checks

1. **Isolate verification mechanisms**: Run ablations that separately disable E-VERIFY and M-VERIFY to quantify each component's individual contribution to EX/EM gains
2. **Test larger model scaling**: Evaluate MTSQL-R1 training on 7B+ models to determine whether the framework's benefits scale with model capacity or are specific to smaller architectures
3. **Analyze error persistence**: Conduct detailed error analysis on aggregation drift failures (Fig. 7) to determine if additional verification mechanisms or reward shaping could address this persistent failure mode