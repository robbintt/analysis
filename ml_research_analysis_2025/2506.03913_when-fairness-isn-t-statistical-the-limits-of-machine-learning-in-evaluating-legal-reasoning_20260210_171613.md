---
ver: rpa2
title: 'When Fairness Isn''t Statistical: The Limits of Machine Learning in Evaluating
  Legal Reasoning'
arxiv_id: '2506.03913'
source_url: https://arxiv.org/abs/2506.03913
tags:
- legal
- fairness
- decisions
- methods
- refugee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether machine learning can assess fairness\
  \ in refugee adjudication using a dataset of 59,000+ Canadian refugee decisions.\
  \ Three methods\u2014feature-based analysis, semantic clustering, and predictive\
  \ modeling\u2014are applied to detect disparities."
---

# When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning

## Quick Facts
- **arXiv ID**: 2506.03913
- **Source URL**: https://arxiv.org/abs/2506.03913
- **Reference count**: 5
- **Primary result**: ML methods detect disparities but cannot distinguish justified judicial discretion from bias in refugee adjudication

## Executive Summary
This study evaluates whether machine learning can assess fairness in refugee adjudication using 59,000+ Canadian refugee decisions. Three methods—feature-based analysis, semantic clustering, and predictive modeling—are applied to detect disparities. Results show high predictive accuracy (up to 93.8%) but reliance on procedural features (e.g., hearing date) rather than legally salient ones (e.g., credibility, legal grounds). Disparities are detected across time, geography, and demographics, but clustering and modeling fail to capture legal reasoning or distinguish justified discretion from bias. Fairness metrics based on outcomes alone are insufficient; the study concludes that ML methods lack the causal insight and interpretability needed to evaluate procedural fairness in legally discretionary contexts.

## Method Summary
The study applies three computational methods to the ASYLEX corpus of Canadian refugee decisions (1996-2022): feature-based statistical analysis extracts structured variables and tests for disparities; semantic clustering uses text embeddings to group similar decisions; predictive modeling trains classifiers on features and full text. All methods focus on detecting outcome disparities rather than evaluating legal reasoning quality. The analysis tests equality of opportunity fairness metrics and examines which features drive predictions.

## Key Results
- Predictive models achieve 93.8% accuracy but rely on procedural features like hearing dates rather than legal grounds
- Judge-level grant rates range from 0% to 100%, with significant geographic and temporal disparities detected
- Semantic clustering correlates with outcomes but fails to capture substantive legal reasoning patterns
- All three methods detect disparities but cannot distinguish justified judicial discretion from bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Feature-based statistical analysis can detect measurable disparities across predefined variables but cannot establish causality or assess whether disparities are legally problematic.
- **Mechanism**: Extracts categorical features (judge ID, city, year, demographics) from annotated legal texts; computes grant rate differences across groups using chi-square tests and descriptive statistics to surface outcome variability.
- **Core assumption**: Observable statistical variation in outcomes signals a fairness-relevant pattern worth investigating.
- **Evidence anchors**:
  - [abstract]: "Disparities are detected across time, geography, and demographics"
  - [section 5, Feature-Based Statistical Analysis]: "Judge-Level Disparities Grant rates among judges range from 0% to 100%... This variation aligns with previous literature findings"
  - [corpus]: "Facets of Disparate Impact" paper discusses legally consistent bias metrics—weak direct support for this specific mechanism
- **Break condition**: When no annotated structured features exist, or when feature extraction quality is poor/noisy.

### Mechanism 2
- **Claim**: Semantic clustering via text embeddings captures linguistic similarity that partially correlates with outcomes and judge behavior but fails to align with substantive legal reasoning.
- **Mechanism**: Embeds full decision texts using OpenAI's text-embedding-3-small; applies K-means clustering to group semantically similar cases; analyzes cluster composition against outcomes, judge identity, and legal grounds.
- **Core assumption**: Assumption: Linguistic similarity in legal documents implies comparable case content or reasoning patterns.
- **Evidence anchors**:
  - [abstract]: "semantic clustering fails to capture substantive legal reasoning"
  - [section 5, Semantic Clustering]: "CLUSTER 1 contains just 7.2% positive outcomes, while CLUSTER 2 includes 40.6%... This suggests that linguistic patterns learned by the model correlate with decision leniency"
  - [corpus]: No strong corpus support for embedding-based legal clustering limitations
- **Break condition**: When legal reasoning is concentrated in final paragraphs only (surface-level patterns dominate), or when embedding model choice biases cluster formation.

### Mechanism 3
- **Claim**: Predictive models achieve high accuracy (up to 93.8%) by leveraging procedural and contextual features rather than legally salient justifications, making them unreliable for fairness evaluation.
- **Mechanism**: Trains Random Forest classifiers on structured feature subsets and neural networks on full text; measures feature importance via Gini impurity reduction; evaluates group-wise recall for fairness metrics.
- **Core assumption**: Assumption: High predictive accuracy indicates the model has learned meaningful patterns about the decision-making process.
- **Evidence anchors**:
  - [abstract]: "predictive modeling often depends on contextual and procedural features rather than legal features"
  - [section 5, Predictive Modeling]: "Contextual and procedural features dominate, while legally salient features contribute relatively little... The hearing date ranks as the top feature"
  - [corpus]: "Unravelling the (In)compatibility of Statistical-Parity and Equalized-Odds" discusses fairness metric limitations—partial relevance
- **Break condition**: When legally salient features are confounded with procedural artifacts, or when ground truth about decision correctness is unavailable.

## Foundational Learning

- **Concept: Distributive vs. Procedural Fairness**
  - Why needed here: The paper critiques ML methods for focusing on outcome parity (distributive) while legal fairness requires evaluating reasoning processes (procedural).
  - Quick check question: If two similar cases receive different outcomes, does that automatically indicate unfairness under legal standards?

- **Concept: Equality of Opportunity in ML Fairness**
  - Why needed here: The study uses EO (equal true positive rates across groups) as its fairness metric; understanding what this measures—and what it misses—is critical.
  - Quick check question: Why might equal recall across demographic groups fail to capture whether decisions were legally justified?

- **Concept: Feature Importance vs. Causal Importance**
  - Why needed here: High feature importance (hearing date, judge ID) reflects correlation, not causation; the paper emphasizes this distinction as a core limitation.
  - Quick check question: If "hearing date" has the highest feature importance in predicting outcomes, does that mean it causes the outcome?

## Architecture Onboarding

- **Component map**: Data layer (ASYLEX corpus) → Method 1 (Feature-based analysis) → Method 2 (Semantic clustering) → Method 3 (Predictive modeling) → Evaluation layer (fairness metrics)
- **Critical path**: Feature extraction quality → Method selection based on hypothesis vs. exploration → Cross-method comparison to identify divergent signals → Interpretability assessment for legal reasoning access
- **Design tradeoffs**:
  - Interpretability vs. discovery: Method 1 is interpretable but constrained; Method 2 enables discovery but lacks legal grounding
  - Accuracy vs. legal meaningfulness: Method 3 achieves 93.8% accuracy but on procedurally driven features
  - Outcome focus vs. reasoning focus: All three methods center on outcomes; none captures deliberative justification
- **Failure signatures**:
  - Methods contradict each other on the same dimension (e.g., geographic significance detected by Method 1 but not by Methods 2-3)
  - High accuracy driven by legally irrelevant features (temporal artifacts, judge ID)
  - Clusters align with surface language patterns (credibility boilerplate) rather than legal grounds distinctions
  - Fairness metrics show group disparities without ability to attribute cause
- **First 3 experiments**:
  1. **Replicate feature-based disparity detection**: Run chi-square tests on grant rates across judges, cities, and years in ASYLEX; verify the 0-100% judge range and 37.6% temporal swing
  2. **Test embedding cluster alignment**: Apply text-embedding-3-small + K-means on a held-out subset; measure cluster-outcome correlation and check if legal grounds are distributed non-uniformly across clusters
  3. **Feature ablation study**: Train Random Forest with incremental feature subsets (Part 1-6 as defined); verify that "Explanations" subset underperforms vs. "External" features in the CONTROLLED SUBSET

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can machine learning frameworks be redesigned to evaluate procedural fairness based on legal reasoning rather than outcome parity?
- Basis in paper: [explicit] The authors "advocate for a shift away from outcome-based fairness metrics toward approaches that incorporate procedural fairness in the sense of legal reasoning."
- Why unresolved: Current models rely on procedural artifacts (e.g., hearing dates) instead of substantive legal justifications, failing to capture normative reasoning.
- What evidence would resolve it: A model that successfully distinguishes justified discretion from bias using argumentation structure rather than case metadata.

### Open Question 2
- Question: Can causal inference methods be effectively applied to unstructured legal text to distinguish systemic bias from legitimate judicial discretion?
- Basis in paper: [inferred] The Discussion notes that applying causal modeling to text is "challenging" yet necessary to avoid misinterpreting legitimate variation as unfairness.
- Why unresolved: Current correlational methods cannot determine if features like judge identity cause unfairness or merely act as proxies for unobserved case complexity.
- What evidence would resolve it: A framework that maps linguistic features to causal variables, enabling counterfactual evaluation of legal decisions.

### Open Question 3
- Question: Do text-level annotations of legal justifications improve the alignment of computational fairness evaluations with legal standards?
- Basis in paper: [explicit] The authors suggest "future datasets could include text-level annotations of justifications, enabling fairness evaluations that align with legal standards."
- Why unresolved: Current corpora lack granular annotations, preventing models from learning the specific reasoning patterns that legally define a fair decision.
- What evidence would resolve it: Comparative studies showing that models trained on justification-annotated data improve detection of legally relevant disparities over current baselines.

## Limitations

- The study cannot distinguish between legally justified discretion and problematic bias due to fundamental limitations in causal inference from observational legal data
- Semantic clustering results depend heavily on embedding model choice and may reflect surface language patterns rather than substantive legal distinctions
- Predictive modeling achieves high accuracy (93.8%) but on procedurally-driven features, raising questions about what "accuracy" actually measures

## Confidence

- **High confidence**: Feature-based statistical analysis results showing geographic, temporal, and judge-level disparities (replicated from established literature)
- **Medium confidence**: Predictive modeling accuracy figures and feature importance rankings, though interpretation remains uncertain
- **Medium confidence**: Semantic clustering alignment with outcomes, but legal interpretability remains limited
- **Low confidence**: Any claims about whether detected disparities represent actual unfairness rather than justified discretion

## Next Checks

1. **Ground truth validation**: Compare model predictions against a small sample of manually coded decisions where legal experts have assessed reasoning quality
2. **Temporal artifact analysis**: Re-run predictive modeling excluding year/month features to isolate whether accuracy depends on temporal patterns vs. substantive case features
3. **Legal ground alignment**: Test whether K-means clusters align with substantive legal grounds (e.g., refugee status criteria) rather than procedural features