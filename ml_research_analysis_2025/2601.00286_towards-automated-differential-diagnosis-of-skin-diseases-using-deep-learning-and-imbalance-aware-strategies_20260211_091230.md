---
ver: rpa2
title: Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning
  and Imbalance-Aware Strategies
arxiv_id: '2601.00286'
source_url: https://arxiv.org/abs/2601.00286
tags:
- skin
- learning
- classification
- transformer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a deep learning model for automated classification
  of skin diseases, addressing the challenge of limited dermatological expertise and
  dataset class imbalance. The approach leveraged a Swin Transformer architecture
  pretrained on ImageNet-1K, enhanced with BatchFormer for improved representation
  of minority classes, Focal Loss to focus learning on hard examples, and ReduceLROnPlateau
  for adaptive learning rate scheduling.
---

# Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies

## Quick Facts
- arXiv ID: 2601.00286
- Source URL: https://arxiv.org/abs/2601.00286
- Reference count: 21
- Primary result: 87.71% classification accuracy on ISIC2019 skin lesion dataset

## Executive Summary
This study addresses the challenge of automated skin disease classification by developing a deep learning framework that tackles both the complexity of dermatological diagnosis and the severe class imbalance in skin lesion datasets. The proposed approach combines a Swin Transformer architecture with BatchFormer for cross-sample learning, Focal Loss for hard example emphasis, and selective augmentation strategies. The framework achieves 87.71% accuracy on the ISIC2019 dataset, outperforming traditional CNN models and demonstrating strong potential for clinical diagnostic support.

## Method Summary
The method employs a Swin Transformer backbone pretrained on ImageNet-1K, enhanced with BatchFormer for improved minority class recognition through cross-batch attention, Focal Loss to prioritize hard misclassified examples, and ReduceLROnPlateau for adaptive learning rate scheduling. The model was trained on ISIC2019 with stratified 70/15/15 splits, using selective Elastic Deformation augmentation for classes with fewer than 2,000 samples. The framework processes 224×224 dermoscopic images through four hierarchical transformer stages with shifted window attention, enabling efficient multi-scale feature extraction.

## Key Results
- Achieved 87.71% classification accuracy on ISIC2019, outperforming traditional CNN approaches
- Demonstrated superior minority class recognition through BatchFormer and Focal Loss combination
- Validated that selective augmentation outperforms general-purpose methods like AutoAugment for Swin Transformer architecture

## Why This Works (Mechanism)

### Mechanism 1: Swin Transformer's Shifted Window Attention
The Swin Transformer's shifted window attention enables efficient multi-scale feature extraction suitable for skin lesion classification where both fine textures and global shape patterns matter. Window-based Multi-head Self-Attention (W-MSA) restricts computation to non-overlapping windows for efficiency, while Shifted Window-based Multi-head Self-Attention (SW-MSA) enables cross-window information exchange by shifting partitions. Patch merging progressively reduces spatial resolution while increasing feature depth across four stages. This hierarchical design captures both local lesion textures and broader shape asymmetries critical for dermoscopic diagnosis.

### Mechanism 2: BatchFormer's Cross-Sample Knowledge Transfer
BatchFormer improves minority class recognition by enabling cross-sample gradient propagation within mini-batches, acting as implicit data augmentation for underrepresented categories. A Transformer encoder operates across the batch dimension, encoding inter-sample similarities. Gradients for each sample are influenced by others in the batch, allowing majority class features to inform minority class representations. This mechanism facilitates knowledge transfer from head classes to tail classes, effectively improving minority class recognition through virtual data augmentation.

### Mechanism 3: Focal Loss for Hard Example Emphasis
Focal Loss reshapes the loss landscape to prioritize hard, misclassified examples, which disproportionately represent minority classes in imbalanced datasets. The modulating factor (1−pt)^γ reduces loss contribution when the model predicts the true class with high confidence. For low-confidence or misclassified examples, loss remains high, forcing gradient updates toward difficult cases. This dynamic scaling mechanism ensures the model focuses learning on informative examples rather than being dominated by easy majority class predictions.

## Foundational Learning

- **Concept: Self-Attention vs. Convolution Inductive Biases**
  - Why needed here: The paper chooses Swin Transformer over pure CNNs, claiming superior long-range dependency modeling. Understanding why shifted windows approximate convolution's locality while enabling global context clarifies this architectural decision.
  - Quick check question: Given a 224×224 dermoscopic image, how does window size (e.g., 7×7) in W-MSA affect the receptive field compared to a ResNet-50 convolution at the same stage?

- **Concept: Long-Tail Distribution and Class Imbalance**
  - Why needed here: The ISIC2019 dataset shows severe imbalance (NV: 12,000+ samples; DF: <300). The paper combines BatchFormer, Focal Loss, and selective augmentation to address this.
  - Quick check question: If a classifier achieves 90% accuracy by always predicting the majority class "NV," what metric would reveal this failure mode?

- **Concept: Transfer Learning from Natural to Medical Images**
  - Why needed here: ImageNet-1K pretraining is used despite domain mismatch. The paper acknowledges this limitation but shows strong results.
  - Quick check question: What features learned from ImageNet (e.g., edge detectors, texture patterns) might transfer to dermoscopic images, and what features likely require fine-tuning?

## Architecture Onboarding

- **Component map:** Input Image (224×224×3) → Swin Transformer Backbone (ImageNet-1K pretrained, 4 stages) → BatchFormer Module (cross-batch attention) → Classification Head (8 classes) → Focal Loss + ReduceLROnPlateau scheduler

- **Critical path:**
  1. Load ISIC2019 with stratified 70/15/15 split—preserve class imbalance ratios
  2. Apply Elastic Deformation selectively to classes with <2,000 samples
  3. Initialize Swin Transformer with ImageNet-1K weights
  4. Insert BatchFormer between backbone and classification head
  5. Train with Focal Loss (tune α and γ on validation set)
  6. Monitor validation loss; trigger ReduceLROnPlateau on plateau

- **Design tradeoffs:**
  - SAM preprocessing reduced accuracy across all tested models (ViT: 65.6%→62.0%; Swin: 76.6%→70.8%)—general-purpose segmentation discards diagnostically relevant features like color variegation
  - Elastic Deformation outperformed AutoAugment for Swin Transformer (87.71% vs. 87.34%), but the margin is small; consider both in ablation
  - BatchFormer adds computational overhead during training but minimal inference cost

- **Failure signatures:**
  - Accuracy drops when SAM preprocessing is applied: check for lost color/texture features
  - Minority class recall remains low despite Focal Loss: increase γ or apply stronger selective augmentation
  - Training diverges: ReduceLROnPlateau may be reducing LR too aggressively; add minimum LR floor

- **First 3 experiments:**
  1. **Baseline replication**: Train Swin Transformer (ImageNet pretrained) with standard cross-entropy on ISIC2019 stratified split. Target: ~76-77% accuracy per Table 3.
  2. **Ablation by component**: Add BatchFormer, then Focal Loss, then ReduceLROnPlateau incrementally. Measure validation accuracy and per-class recall after each addition.
  3. **Augmentation comparison**: Compare Elastic Deformation vs. AutoAugment vs. no augmentation on minority class F1-scores. Apply augmentation only to classes with <2,000 samples as specified.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would pretraining on dermatology-specific datasets yield superior performance compared to ImageNet-1K pretraining for the Swin Transformer backbone?
  - Basis in paper: The conclusion states the model was initialized with pretrained weights from general-purpose datasets such as ImageNet, which may not be optimally suited for the specific characteristics of dermoscopic images, and suggests fine-tuning using domain-specific dermatology datasets.
  - Why unresolved: The authors used only ImageNet-1K pretraining; no domain-specific pretraining experiments were conducted.
  - What evidence would resolve it: A controlled experiment comparing Swin Transformer models pretrained on dermatology image datasets (e.g., HAM10000, SD-198) versus ImageNet-1K, evaluated on the same ISIC2019 test set.

- **Open Question 2**: Would dynamic class-weighted loss functions or adaptive sampling techniques outperform the combined Focal Loss and BatchFormer approach for minority class recognition?
  - Basis in paper: The conclusion recommends integrating more robust imbalance-aware learning strategies such as dynamic class-weighted loss functions or adaptive sampling techniques to improve classification for underrepresented lesion types.
  - Why unresolved: The current framework uses only Focal Loss and BatchFormer; alternative imbalance strategies were not compared.
  - What evidence would resolve it: Ablation studies comparing per-class recall and F1-scores between the current approach and methods like class-balanced sampling, MWNL loss, or dynamically weighted losses on the same imbalanced dataset.

## Limitations

- The study relies on ImageNet-1K pretraining despite known domain differences between natural and medical images, acknowledged as a limitation
- BatchFormer mechanism lacks direct validation from the corpus, though theoretically promising for cross-sample knowledge transfer
- Selective augmentation threshold of 2,000 samples appears arbitrary without sensitivity analysis
- No per-class precision-recall curves or confusion matrices reported, limiting understanding of specific lesion pair challenges

## Confidence

- **High confidence**: Swin Transformer architecture provides superior feature extraction compared to traditional CNNs for skin lesion classification
- **Medium confidence**: BatchFormer and Focal Loss combination effectively addresses class imbalance
- **Medium confidence**: ImageNet-1K pretraining provides strong initialization despite domain mismatch

## Next Checks

1. **Cross-dataset generalization**: Evaluate the trained model on independent datasets (e.g., HAM10000 or Derm7pt) to assess real-world applicability beyond ISIC2019.

2. **Per-class error analysis**: Generate confusion matrices and per-class precision-recall curves to identify specific lesion pairs where the model struggles most.

3. **BatchFormer ablation study**: Remove BatchFormer while keeping other components constant to quantify its isolated contribution to minority class recognition.