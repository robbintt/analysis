---
ver: rpa2
title: 'Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label
  Learning'
arxiv_id: '2509.17971'
source_url: https://arxiv.org/abs/2509.17971
tags:
- mixup
- learning
- complementary
- labels
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of complementary-label learning
  (CLL), where models are trained with labels indicating classes an instance does
  not belong to, rather than standard ordinary labels. The authors identify that the
  widely-used Mixup data augmentation technique is ineffective for CLL due to noise
  introduced when mixing complementary labels that may contain the true class.
---

# Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning

## Quick Facts
- **arXiv ID**: 2509.17971
- **Source URL**: https://arxiv.org/abs/2509.17971
- **Reference count**: 40
- **Primary result**: ICM improves CLL accuracy by 30% on MNIST and 10% on CIFAR datasets

## Executive Summary
This paper addresses the challenge of complementary-label learning (CLL), where models are trained with labels indicating classes an instance does not belong to, rather than standard ordinary labels. The authors identify that the widely-used Mixup data augmentation technique is ineffective for CLL due to noise introduced when mixing complementary labels that may contain the true class. To address this, they propose Intra-Cluster Mixup (ICM), which clusters examples first and then applies Mixup only within each cluster to reduce noise while preserving the benefits of label sharing. ICM consistently improves CLL performance across various state-of-the-art algorithms and settings, achieving significant accuracy increases of 30% on MNIST and 10% on CIFAR datasets compared to baselines. The method is validated on both balanced and imbalanced CLL scenarios using synthetic and real-world labeled datasets.

## Method Summary
The paper proposes Intra-Cluster Mixup (ICM) to improve complementary-label learning by addressing the noise issues that arise when applying standard Mixup to complementary labels. ICM first clusters examples using an unsupervised clustering algorithm, then applies Mixup only within each cluster. This approach reduces noise because complementary labels from examples in the same cluster are more likely to share the same true class. The method works by: (1) clustering the training data using k-means, (2) generating synthetic examples through Mixup interpolation within each cluster, and (3) training CLL models on this augmented dataset. The key insight is that intra-cluster mixing reduces the probability of introducing the true class label into synthetic examples while maintaining the benefits of label sharing and data augmentation.

## Key Results
- ICM improves CLL accuracy by 30% on MNIST and 10% on CIFAR datasets compared to baselines
- ICM consistently improves performance across multiple state-of-the-art CLL algorithms
- The method shows effectiveness on both balanced and imbalanced CLL scenarios
- ICM outperforms standard Mixup and other augmentation techniques in CLL settings

## Why This Works (Mechanism)
ICM reduces noise in complementary-label learning by leveraging the cluster structure of the data. In standard Mixup, when complementary labels are mixed, there's a high probability that the resulting synthetic label contains the true class, which introduces harmful noise. By restricting Mixup to examples within the same cluster, ICM increases the likelihood that the true class is excluded from both complementary labels being mixed. This preserves the benefits of data augmentation (label sharing, feature interpolation) while minimizing the detrimental noise that standard Mixup introduces in CLL settings.

## Foundational Learning
- **Complementary-label learning (CLL)**: A learning paradigm where each training example is associated with a label indicating classes it does not belong to, rather than its true class. CLL is needed when obtaining true labels is expensive or impractical, but negative class information is available.
- **Mixup data augmentation**: A technique that creates synthetic training examples by linearly interpolating between pairs of examples and their labels. Quick check: Verify that Mixup preserves the convex combination property of labels.
- **k-means clustering**: An unsupervised algorithm that partitions data into k clusters by minimizing within-cluster variance. Quick check: Confirm that clustering captures meaningful semantic groupings relevant to the classification task.
- **Label noise in CLL**: The problem where synthetic examples created through Mixup may inadvertently include the true class in their complementary labels, degrading model performance.
- **Cluster-based data augmentation**: A strategy that applies augmentation techniques within clusters rather than across the entire dataset, reducing noise while preserving benefits.

## Architecture Onboarding

**Component Map**: Raw data -> k-means clustering -> Intra-cluster Mixup -> CLL algorithm -> Trained model

**Critical Path**: Clustering → Mixup generation → Model training. The clustering step is critical because it determines which examples can be safely mixed without introducing noise.

**Design Tradeoffs**: ICM trades computational overhead (clustering step) for improved CLL performance. The choice of k (number of clusters) affects the noise reduction versus data diversity balance. Too few clusters increases noise risk; too many reduces the benefits of data augmentation.

**Failure Signatures**: Performance degradation occurs when: (1) clusters are poorly formed and don't align with true class boundaries, (2) the number of clusters is too small, leading to high noise ratios, or (3) the number of clusters is too large, limiting data diversity.

**First Experiments**:
1. Compare ICM performance with varying numbers of clusters (k=5, 10, 20) on MNIST to find optimal k
2. Ablation study: ICM with vs. without clustering to quantify clustering's contribution
3. Test ICM on a simple CLL baseline (e.g., forward loss correction) before applying to complex algorithms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Intra-Cluster Mixup (ICM) framework be effectively extended to multi-complementary-label learning settings where instances carry more than one complementary label?
- Basis in paper: [explicit] The "Limitation and Future Works" section states, "We have not yet evaluated our approach in scenarios where instances carry multiple complementary labels. Investigating the benefits of ICM in a multi-complementary-label learning setting remains an important direction for future work."
- Why unresolved: The current formulation assumes a single complementary label per instance, and it is unclear how the label-mixing coefficients in ICM would interact with multiple simultaneous constraints or soft labels.
- What evidence would resolve it: Empirical results on datasets with multiple complementary labels per instance, showing that ICM maintains low noise ratios and improves accuracy over multi-complementary-label baselines.

### Open Question 2
- Question: How can the ICM technique be adapted to prevent performance degradation in low-capacity models such as linear classifiers and MLPs on complex datasets?
- Basis in paper: [explicit] The "Limitation and Future Works" section notes that on FMNIST, "our augmentation technique yields reduced performance [on linear models and MLPs]... This decline stems from the limited capacity of these models, which struggle to accommodate the added complexity."
- Why unresolved: The current method introduces feature overlap that requires sufficient model capacity to disentangle; the authors acknowledge this limitation but do not propose a solution for shallower architectures.
- What evidence would resolve it: A modified ICM strategy (e.g., adaptive mixing intensity or regularization) that results in accuracy gains for linear and MLP models comparable to those observed in ResNet18.

### Open Question 3
- Question: Does the noise reduction provided by ICM enable the successful application of complementary-label learning to large-scale datasets with significantly more than 20 classes?
- Basis in paper: [inferred] The paper states in Section 4.1 that they do not benchmark on TinyImageNet or ImageNet because "state-of-the-art CLL algorithms struggle to produce meaningful classifiers for 100 classes." Given ICM's significant boost on smaller datasets, its utility in bridging this gap remains untested.
- Why unresolved: The experimental scope is strictly limited to datasets with 10 or 20 classes, so the scalability of the clustering and mixing efficiency to high-dimensional class spaces is unknown.
- What evidence would resolve it: Benchmarking ICM on datasets like CIFAR100 or TinyImageNet to verify if it achieves non-trivial classifier performance where standard CLL baselines fail.

### Open Question 4
- Question: Why does combining ICM with Cutout data augmentation degrade performance, and can this incompatibility be resolved?
- Basis in paper: [inferred] Section 4.2 observes that "Cutout appears to hurt performance when used together with ICM," hypothesizing it causes "overly distorted synthetic samples," but offers no definitive explanation or fix.
- Why unresolved: The interaction between the spatial removal of features (Cutout) and the cluster-based interpolation (ICM) introduces an unstudied failure mode in the augmentation pipeline.
- What evidence would resolve it: An ablation study analyzing the feature integrity of ICM-synthesized samples when Cutout is applied, potentially leading to a spatially-aware ICM variant that avoids the issue.

## Limitations
- Limited theoretical justification for why intra-cluster mixup specifically reduces noise compared to standard mixup in CLL settings
- Clustering approach introduces additional hyperparameters (number of clusters, clustering algorithm choice) that affect performance
- Limited discussion of computational overhead introduced by clustering step
- Evaluation focused primarily on image datasets (MNIST, CIFAR); applicability to other data types remains unclear

## Confidence
- **High confidence** in the core observation that standard Mixup is ineffective for CLL due to noise issues
- **Medium confidence** in the proposed ICM solution effectiveness, given strong empirical results but limited theoretical grounding
- **Medium confidence** in generalization across different CLL algorithms, though results are consistent across tested methods

## Next Checks
1. Conduct ablation studies to quantify the specific contribution of clustering versus other components of ICM
2. Test ICM performance on non-image datasets (text, tabular, or time-series data) to assess generalizability
3. Perform runtime analysis comparing computational costs of ICM versus baseline CLL methods