---
ver: rpa2
title: 'LangMark: A Multilingual Dataset for Automatic Post-Editing'
arxiv_id: '2511.17153'
source_url: https://arxiv.org/abs/2511.17153
tags:
- translation
- post-editing
- dataset
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LangMark is a multilingual human-annotated dataset for automatic
  post-editing (APE) of neural machine translation (NMT) outputs, comprising over
  200,000 triplets across seven languages (English to Brazilian Portuguese, French,
  German, Italian, Japanese, Russian, and Spanish). The dataset includes source segments,
  NMT outputs from a proprietary system, and human post-edited translations by expert
  linguists.
---

# LangMark: A Multilingual Dataset for Automatic Post-Editing

## Quick Facts
- arXiv ID: 2511.17153
- Source URL: https://arxiv.org/abs/2511.17153
- Reference count: 13
- Primary result: LangMark dataset enables effective few-shot APE using LLMs, with high-precision models outperforming high-recall ones on human edit prediction.

## Executive Summary
LangMark is a human-annotated multilingual dataset for automatic post-editing (APE) of neural machine translation (NMT) outputs, containing over 200,000 triplets across seven languages (EN→DE/ES/FR/IT/JP/BR/RU). Each triplet consists of a source segment, NMT output from a proprietary system, and a human post-edited translation. The dataset is designed to train and evaluate APE models that correct NMT errors using source and MT output. Experiments show that large language models (LLMs) with few-shot prompting, particularly GPT-4o, can effectively improve upon both commercial and proprietary NMT systems, highlighting the challenge of determining when edits are necessary.

## Method Summary
The LangMark dataset comprises 206,983 triplets (source, NMT output, human post-edit) across seven language pairs in the marketing domain. Data is split 90/10 per language, with 90% forming the retrieval pool and 10% as test data. Few-shot prompting is used with retrieval-based embedding: source segments are embedded using OpenAI text-embedding-3-small, and the top-20 most similar (source, human-post-edit) pairs are retrieved per test item. LLMs (e.g., GPT-4o, Qwen2.5-72B) are prompted with a 20-shot template to generate post-edits. Performance is measured using CHRF (primary), TER (↓), and BLEU, with additional analysis on edit classification precision/recall.

## Key Results
- GPT-4o achieves the highest CHRF scores among tested models, demonstrating effective few-shot APE.
- High-precision, low-recall models (e.g., GPT-4o) outperform high-recall, low-precision models (e.g., Claude 3.5-Sonnet) on human edit prediction.
- Few-shot prompting with retrieval is critical; zero-shot prompting does not improve over NMT baseline.
- The dataset reveals that determining when edits are necessary is a core challenge in APE.

## Why This Works (Mechanism)
APE performance is driven by the quality of few-shot examples retrieved based on source similarity. High-precision models succeed by avoiding unnecessary edits, aligning better with human post-editing behavior. The proprietary NMT baseline ensures consistent, realistic errors for APE models to correct.

## Foundational Learning
- **Retrieval-based few-shot prompting**: Needed to provide context for APE models; quick check: verify retrieval quality by inspecting top-20 examples for a test source.
- **Edit classification precision/recall**: Needed to evaluate model behavior on when to edit; quick check: compute precision/recall on edit decisions vs human post-edit.
- **CHRF vs BLEU**: CHRF better captures character-level accuracy for languages with rich morphology; quick check: compare CHRF and BLEU scores on a sample.
- **Proprietary NMT baseline**: Needed to ensure consistent, realistic errors; quick check: inspect NMT outputs for error patterns.
- **Marketing domain specificity**: Limits generalizability; quick check: test model on a different domain.
- **Embedding-based similarity**: Enables effective retrieval of relevant examples; quick check: visualize embedding distances for a sample.

## Architecture Onboarding
- **Component map**: Source + NMT output -> Embedding retrieval -> Few-shot prompt template -> LLM -> Post-edit output -> CHRF/TER/BLEU evaluation
- **Critical path**: Retrieval of similar (source, post-edit) pairs -> Prompt construction -> LLM generation -> Evaluation against human post-edit
- **Design tradeoffs**: Few-shot vs. zero-shot prompting (few-shot required for performance), high-precision vs. high-recall models (precision preferred for human alignment)
- **Failure signatures**: Zero-shot prompting fails to improve over NMT; high-recall models over-edit and score worse; retrieval quality directly impacts performance
- **First experiment**: 1) Run retrieval for a test source and inspect top-20 examples for relevance; 2) Evaluate GPT-4o with 20-shot prompt on a sample; 3) Compute precision/recall on edit decisions vs human post-edit

## Open Questions the Paper Calls Out
None

## Limitations
- The proprietary NMT system used to generate baseline outputs is not available, though pre-generated outputs are fixed in the dataset.
- Exact LLM API parameters (temperature, max_tokens, etc.) for each model are unspecified, potentially affecting reproducibility of scores.
- The dataset is marketing-domain specific, limiting generalizability to other domains.

## Confidence
- High confidence in dataset construction and evaluation framework
- Medium confidence in comparative model performance findings
- Medium confidence in generalizability of findings

## Next Checks
1. Verify the impact of exact LLM API parameters (temperature, max_tokens, etc.) on APE performance using the same few-shot prompting approach.
2. Test model performance on LangMark across different domains to assess generalizability beyond marketing content.
3. Compare retrieval-based few-shot prompting against alternative methods (e.g., in-context learning without retrieval, fine-tuning) to validate the necessity of the retrieval component.