---
ver: rpa2
title: 'Leveraging Domain Knowledge at Inference Time for LLM Translation: Retrieval
  versus Generation'
arxiv_id: '2503.05010'
source_url: https://arxiv.org/abs/2503.05010
tags:
- terminology
- demonstrations
- translation
- source
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares retrieval and generation methods for domain
  adaptation of large language models (LLMs) in machine translation. It examines how
  demonstrations (example translations) and terminology (bilingual dictionaries) can
  be sourced externally (from a bitext datastore) or internally (generated by an LLM)
  to improve translation quality in law, medical, and Koran domains.
---

# Leveraging Domain Knowledge at Inference Time for LLM Translation: Retrieval versus Generation

## Quick Facts
- arXiv ID: 2503.05010
- Source URL: https://arxiv.org/abs/2503.05010
- Reference count: 20
- Key outcome: Retrieval-based demonstrations outperform generation and terminology for domain adaptation, with demonstrations working primarily through style alignment rather than terminology assistance

## Executive Summary
This paper compares retrieval and generation methods for domain adaptation of large language models (LLMs) in machine translation. The study examines how demonstrations (example translations) and terminology (bilingual dictionaries) can be sourced externally from a bitext datastore or internally generated by an LLM to improve translation quality in law, medical, and Koran domains. The key findings show that demonstrations consistently outperform terminology, and retrieval consistently outperforms generation. However, generating demonstrations with weaker models can close the performance gap with larger models' zero-shot translation.

## Method Summary
The paper compares four knowledge acquisition strategies for domain adaptation: retrieved demonstrations (BM25 over training datastores), retrieved terminology (LLM-extracted silver dictionaries), generated demonstrations (LLM with static exemplars), and generated terminology (LLM with static exemplars). Experiments use German-English translation across three domains with ~2000 test entries per domain. Quality is measured with COMET22 neural metric, and retrieval uses k=3 exemplars. Generated knowledge uses Figure 8/10 prompts with 2-shot domain-specific static exemplars, while translation uses Figure 5/6 prompts.

## Key Results
- Retrieval consistently outperforms generation across all domains and model sizes
- Demonstrations outperform terminology by 1.0-2.0 COMET points on average
- Generated demonstrations with weaker models can close the performance gap with larger models' zero-shot results
- Style templates (demonstrations with terms masked) account for ~65% of gains from retrieved demonstrations

## Why This Works (Mechanism)

### Mechanism 1: Style Alignment Over Terminology
- Claim: Demonstrations improve domain-adapted LLM translation primarily by conveying target-side writing style rather than domain-specific terminology
- Core assumption: The multi-domain benchmark tests adaptation to specific document styles rather than broad domain knowledge
- Evidence: Style templates account for ~65% of gains from retrieved demonstrations
- Break condition: If a domain requires substantial novel terminology not in LLM pre-training data

### Mechanism 2: Synthetic Demonstration Generation via Parametric Memory
- Claim: Weaker LLMs can generate their own effective domain-specific demonstrations by retrieving relevant knowledge from their parametric memory
- Core assumption: LLM's parametric memory contains sufficient domain knowledge to generate plausible synthetic examples
- Evidence: Gemma-2 27B IT with generated demonstrations improves by +2.3 COMET points over zero-shot
- Break condition: For domains with very little pre-training data, generated demonstrations may be low-quality

### Mechanism 3: Inferential Knowledge Elicitation vs. External Retrieval
- Claim: Retrieving knowledge from external datastore is consistently more effective than generating from LLM's parametric memory
- Core assumption: High-quality, domain-specific external datastores are available
- Evidence: Retrieved demonstrations consistently outperform generated demonstrations across all domains
- Break condition: If external datastore is noisy or not well-aligned with target domain

## Foundational Learning
- **In-Context Learning (ICL)**: Why needed: Entire paper built on ICL where model learns from examples in prompt without weight updates. Quick check: Can you explain how adding 3 German-English sentence pairs to a prompt changes how an LLM translates a new German sentence?
- **Domain Adaptation**: Why needed: Core problem being solved. Quick check: How does adapting an LLM for "medical translation" at inference time differ from training a model on a medical corpus?
- **Parametric vs. Non-Parametric Knowledge**: Why needed: Central comparison between using LLM's own memory versus external database. Quick check: When an LLM "generates" a demonstration, where is that information coming from?

## Architecture Onboarding

### Component Map
Knowledge Source -> Elicitation Module -> Knowledge Representation -> Main MT Prompt -> MT Engine

### Critical Path
1. Input: Source sentence
2. Elicitation (Parallel Paths):
   - Retrieval: Query Datastore with source sentence -> Retrieve top-k similar source-target pairs or lookup exact matches
   - Generation: Prompt an LLM with the source sentence -> Get synthetic demonstrations or terminology
3. Prompt Construction: Inject elicited knowledge into main MT prompt template
4. Translation: Send final prompt to MT LLM to get target translation

### Design Tradeoffs
- Retrieval vs. Generation: Retrieval more accurate but requires costly datastore; generation resource-light but lower quality
- Demonstrations vs. Terminology: Demonstrations more effective for style but longer; terminology more concise but less effective
- Static vs. Instance-Specific: Static domain-specific demonstrations as effective as instance-specific for strong LLMs

### Failure Signatures
- No improvement with terminology (large LLMs): LLM already knows the terms
- Degraded performance with generated demonstrations: Hallucinated poor-quality examples
- Inconsistency in term translation: One source term maps to multiple valid target terms

### First 3 Experiments
1. Establish zero-shot baseline: Translate entire test set with only zero-shot prompt, measure with COMET
2. Compare retrieved demonstrations vs. terminology: Add k=3 retrieved demonstrations vs. terminology to subset, compare COMET to baseline
3. Ablate on demonstration generation: Compare generating demos without ICL, with generic ICL, and with domain-specific ICL, measure impact on translation quality

## Open Questions the Paper Calls Out
- How can the field construct a machine translation benchmark that challenges parametric knowledge of modern LLMs rather than testing writing style adaptation?
- Does superiority of demonstrations over terminology persist with high-quality, human-curated dictionaries vs. LLM-derived silver terminology?
- Do findings regarding style dominance generalize to morphologically rich languages or low-resource language pairs?

## Limitations
- Study restricted to German-English language pair, limiting generalizability to other language families
- Use of LLM-derived silver terminology may underestimate value of high-quality human-curated dictionaries
- Style vs. terminology decomposition relies on indirect ablation study rather than direct measurement

## Confidence
- **High Confidence**: Retrieval consistently outperforms generation; demonstrations outperform terminology
- **Medium Confidence**: Demonstrations work primarily through style alignment; generated demonstrations close gap with larger models
- **Medium Confidence**: Findings generalize to other domains and language pairs

## Next Checks
1. Design experiment to independently measure contribution of style and terminology by using pure style demonstrations and pure terminology lists
2. Test retrieval vs. generation hierarchy in domain with highly specialized terminology absent from LLM pre-training data
3. Systematically vary quality and coverage of external datastore to determine threshold where generation becomes superior to retrieval