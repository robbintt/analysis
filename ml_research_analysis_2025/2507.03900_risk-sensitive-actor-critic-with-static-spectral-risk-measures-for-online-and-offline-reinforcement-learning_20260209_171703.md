---
ver: rpa2
title: Risk-sensitive Actor-Critic with Static Spectral Risk Measures for Online and
  Offline Reinforcement Learning
arxiv_id: '2507.03900'
source_url: https://arxiv.org/abs/2507.03900
tags:
- risk
- policy
- learning
- offline
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for optimizing static Spectral
  Risk Measures (SRM) in both online and offline reinforcement learning settings.
  The authors address the limitation of iterative risk measures in DRL, which can
  lead to suboptimal policies, by directly optimizing static SRMs using a bi-level
  optimization approach.
---

# Risk-sensitive Actor-Critic with Static Spectral Risk Measures for Online and Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.03900
- Source URL: https://arxiv.org/abs/2507.03900
- Authors: Mehrdad Moghimi; Hyejin Ku
- Reference count: 19
- Primary result: Framework for optimizing static Spectral Risk Measures (SRM) in both online and offline RL, outperforming iterative risk-sensitive methods.

## Executive Summary
This paper addresses the limitations of iterative risk measures in deep reinforcement learning by introducing a framework for optimizing static Spectral Risk Measures (SRM) using a bi-level optimization approach. The method employs an actor-critic architecture with a distributional critic that models return quantiles, enabling the learning of risk-sensitive policies tailored to different risk preferences. The authors provide theoretical convergence guarantees for finite state-action settings and demonstrate extensive empirical improvements across diverse domains including finance, healthcare, and robotics.

## Method Summary
The framework reformulates SRM optimization as a bi-level problem using the supremum representation, separating risk-function adaptation from policy improvement. The extended state space (x, s, c) tracks accumulated rewards and discount factors, allowing consistent application of static risk preferences across time-steps. A distributional critic with quantile regression estimates return distributions, while the risk function h is updated periodically using quantile weights derived from the risk spectrum φ. The approach supports both stochastic (AC-SRM) and deterministic (TD3-SRM) policies with appropriate modifications for online and offline settings.

## Key Results
- TD3-SRM and AC-SRM consistently outperform existing risk-sensitive algorithms in both expected return and worst-case performance across diverse domains.
- Static SRMs (TD3-SRM) produce more risk-averse policies than iterative methods (TD3-iCVaR) while achieving higher expected returns in the Mean-Reverting Trading environment.
- The method successfully learns risk-sensitive policies in offline settings using behavior cloning regularization, achieving better CVaR and expected returns than risk-neutral baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing static SRM optimization via bi-level optimization with supremum representation enables learning policies that optimize trajectory-level risk rather than per-step risk.
- Mechanism: The framework reformulates $J(\pi) = \text{SRM}_\phi(G^\pi)$ as $\max_{h \in H} \max_{\pi \in \Pi} J(\pi, h)$ using the supremum representation of SRMs (Equation 3). The outer loop updates $h$ via the closed-form solution $h_{k+1} = \tilde{h}_{\phi, G^{\pi_k}}$ (Equation 4), while the inner loop optimizes the policy for fixed $h$. This separates risk-function adaptation from policy improvement.
- Core assumption: The distributional critic provides sufficiently accurate estimates of return quantiles to compute $h$ correctly; the risk spectrum $\phi$ accurately reflects true risk preferences.
- Evidence anchors:
  - [abstract]: "Our method involves an actor-critic framework with a distributional critic, enabling the learning of risk-sensitive policies tailored to different risk preferences."
  - [section 4]: "We leverage the supremum representation of the SRM (Equation (3)) to reformulate this objective as a bi-level optimization problem."
  - [corpus]: Related work "Beyond CVaR" (Moghimi & Ku, 2025) similarly uses closed-form solutions for static SRM in distributional RL, supporting the approach but indicating it remains an active research area.
- Break condition: If quantile estimates become highly inaccurate (e.g., due to insufficient samples or distributional shift in offline settings), the computed $h$ may misalign with the intended risk spectrum, leading to policies that optimize the wrong objective.

### Mechanism 2
- Claim: Extending the state space with accumulated rewards and discount factors enables risk-adjusted Q-values that remain consistent with static risk preferences across time-steps.
- Mechanism: The extended state $\bar{x} = (x, s, c)$ tracks cumulative discounted reward $S_{t+1} = S_t + C_t R_t$ and discount factor $C_{t+1} = \gamma C_t$. The Q-value function (Equation 7) uses $h(s + c G^\pi(\bar{x}, a))/c$, which adjusts future returns to the initial time-step. This allows the risk function $h$—defined on the full return distribution—to apply correctly at intermediate states.
- Core assumption: The MDP has finite horizon or the episode terminates, ensuring $s$ and $c$ remain bounded; the policy is Markovian in the extended state space.
- Evidence anchors:
  - [section 4]: "Equation (8) also demonstrates how the extended state variables $s$ and $c$ allow the function $h$ to apply the same risk preference at different time-steps."
  - [section 4]: Proposition 4.1 provides the piecewise linear approximation of $h$ using quantile weights $w_i = \hat{\tau}_i(\phi(\tau_{i-1}) - \phi(\tau_i))$.
  - [corpus]: Related work on CVaR optimization (Chow et al., 2015, cited in paper) also uses state augmentation, suggesting this is a standard approach for static risk measures.
- Break condition: For very long horizons, numerical precision in tracking $s$ and $c$ may degrade; if the episode never terminates, the extended state becomes unbounded.

### Mechanism 3
- Claim: Using a distributional critic with quantile regression enables differentiable risk-sensitive policy gradients while preserving theoretical convergence guarantees.
- Mechanism: The critic models return distributions $\eta_q(x,a)$ using $N$ quantiles via QR-DQN (Equation 2). For stochastic policies, the gradient (Equation 9) uses the advantage function $A^\pi_h$ derived from risk-adjusted Q-values. For deterministic policies, the gradient (Equation 11) includes $\phi(F_{G^\pi}(s + cG^\pi))$ that weights gradient updates by risk preference at the relevant quantile. Theorem 4.2 proves convergence under Natural Policy Gradient with Robbins-Monro learning rates.
- Core assumption: Finite state-action spaces for theoretical guarantees; bounded rewards $[R_{\min}, R_{\max}]$ with $R_{\min} \geq 0$; Lipschitz continuity of $h$ via $\phi(0)$.
- Evidence anchors:
  - [abstract]: "Theoretical guarantees of convergence are provided for the finite state-action setting."
  - [section 4.1]: Theorem 4.2 establishes convergence to optimal policy in the tabular setting; Theorem 4.3 proves monotonic improvement for parameterized policies.
  - [corpus]: "On the Global Convergence of Risk-Averse Natural Policy Gradient Methods" (corpus) similarly proves convergence for risk-averse NPG, suggesting convergence analysis for risk-sensitive methods is an established but non-trivial area.
- Break condition: With function approximation and continuous action spaces, convergence guarantees do not hold; overestimation bias in critic quantiles can destabilize training (addressed partly by twin critics).

## Foundational Learning

- Concept: **Spectral Risk Measures (SRM)**
  - Why needed here: SRMs generalize risk measures like CVaR by assigning different weights $\phi(u)$ to different quantiles of the return distribution. Understanding that $\text{SRM}_\phi(Z) = \int_0^1 F_Z^{-1}(u)\phi(u)du$ is essential to grasp why the framework needs the full return distribution, not just expected values.
  - Quick check question: Given a risk spectrum $\phi(u) = 2(1-u)$ (linearly decreasing weights), would this SRM place more weight on good outcomes or bad outcomes?

- Concept: **Distributional Bellman Operator**
  - Why needed here: The distributional critic is updated via $(\mathcal{T}^\pi \eta)(x,a) = \mathbb{E}_\pi[(\hat{R}, \gamma)_\# \eta(X', A')]$ which pushes forward the return distribution through the transition dynamics. Without understanding this, the quantile regression loss (Equation 2) and target computation will be opaque.
  - Quick check question: Why does the distributional Bellman operator require sampling target actions $a' \sim \pi(\cdot|x')$ rather than using the expected action?

- Concept: **Time Inconsistency in Risk-Sensitive RL**
  - Why needed here: The paper's central motivation is that iterative (per-step) application of risk measures leads to policies that optimize the wrong objective. Understanding that optimizing $\text{CVaR}_{0.1}$ at each state $\neq$ optimizing $\text{CVaR}_{0.1}$ of the trajectory return explains why the extended state space and bi-level optimization are necessary.
  - Quick check question: If you apply CVaR at every time-step with $\alpha=0.1$, would you expect the resulting policy to be more conservative or less conservative than optimizing static CVaR on the full return?

## Architecture Onboarding

- Component map: Environment/Dataset D → [Extended State Encoder (s, c)] → Actor π_θ → [Distributional Critic G_θ1, G_θ2] → Quantile estimates q_i(x,a) → [Risk Function h] → Risk-adjusted Q-values Q^π_h(x,a) → [Policy Gradient] → Actor update

- Critical path:
  1. **Initialize**: Distributional critics with $N=50$ quantiles, actor, target networks
  2. **Collect data** (online) or **sample batch** (offline)
  3. **Update critics**: Minimize quantile regression Huber loss with twin-target selection
  4. **Compute advantage**: $A(\bar{x}, a) = Q_1(\bar{x}, a) - \mathbb{E}_{\tilde{a}}[Q_1(\bar{x}, \tilde{a})]$
  5. **Update actor**: Use Equation 9 (stochastic) or Equation 11/12 (deterministic)
  6. **Periodically update $h$**: Using initial state return distribution estimates

- Design tradeoffs:
  - **Stochastic vs Deterministic policies**: Stochastic (AC-SRM) for exploration in uncertain environments; deterministic (TD3-SRM) for stability and better offline performance where exploration is impossible
  - **Online vs Offline**: Offline requires policy constraints (KL divergence via Equation 10 or BC regularization via Equation 12) to prevent distributional shift
  - **Quantile count $N$**: More quantiles = finer distribution approximation but higher compute; $N=50$ used in experiments

- Failure signatures:
  - **Overly conservative policies**: If $h$ is updated too frequently or quantile estimates are noisy, policies may avoid all risk, yielding low returns
  - **Critic overestimation**: In offline settings, actions outside the dataset may have inflated quantile estimates; twin critics and target networks mitigate but don't eliminate this
  - **Risk spectrum mismatch**: Choosing $\phi$ that doesn't match true preferences (e.g., CVaR with wrong $\alpha$) produces policies optimized for the wrong objective

- First 3 experiments:
  1. **Validate on Mean-Reverting Trading (online)**: Compare TD3-CVaR and AC-CVaR against DSAC-iCVaR with $\alpha=0.2$; expect similar CVaR$_{0.2}$ but higher expected return for static methods (Figure 4a)
  2. **Test offline learning with Expert-Replay dataset**: Train TD3BC-CVaR and OAC-CVaR on the trading dataset; verify that risk-sensitive policies can be learned without environment interaction (Figure 4b)
  3. **Ablate iterative vs static risk**: Compare TD3-Exp against TD3-iExp across multiple $\alpha$ values; confirm that iterative measures produce overly conservative policies with lower expected returns (Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can return estimation be effectively decoupled from entropy estimation in soft actor-critic methods to improve risk-sensitive performance without sacrificing exploration?
- Basis in paper: [explicit] The authors state in the Discussion that applying risk measures to soft critics naively includes entropy, which is incorrect, and suggest "decoupling return estimation from entropy" as a "compelling direction for future work."
- Why unresolved: Current implementations apply the risk measure to the combined value (return + entropy), degrading performance. An architectural modification to separate these components for spectral risk measures has not yet been developed.
- What evidence would resolve it: A modified soft actor-critic algorithm that isolates the risk adjustment to the return component and demonstrates improved risk-sensitive metrics over the baseline.

### Open Question 2
- Question: To what extent does the inherent randomness of stochastic policies degrade the effectiveness of risk-averse policies compared to deterministic policies in static SRM optimization?
- Basis in paper: [inferred] The Discussion notes an observation that "added randomness can hinder the effectiveness of risk-sensitive policies" and that deterministic policies (TD3-SRM) often outperformed stochastic ones, yet the authors provide stochastic variants (AC-SRM) as a valid contribution.
- Why unresolved: While the empirical trend is noted, the theoretical boundaries or specific environment conditions under which stochastic policies fail or succeed in a static SRM context are not fully characterized.
- What evidence would resolve it: A theoretical analysis or extensive ablation study identifying the interaction between policy variance, risk spectrum sharpness, and environment stochasticity.

### Open Question 3
- Question: Do the convergence guarantees for static SRM optimization extend to the continuous state-action settings utilized in the empirical evaluations?
- Basis in paper: [inferred] The paper provides convergence proofs for the "finite state-action setting" (Theorems 4.2, 4.3) but empirically tests on continuous control tasks like MuJoCo and financial environments without theoretical guarantees for these settings.
- Why unresolved: The theoretical proofs rely on tabular representations (softmax policies over finite actions) which do not account for the function approximation errors present in the deep learning implementations used in practice.
- What evidence would resolve it: A convergence proof or counter-example demonstrating the stability of the bi-level optimization scheme under standard deep function approximation assumptions (e.g., Lipschitz continuity, approximation error bounds).

## Limitations
- Theoretical convergence guarantees are limited to finite state-action spaces, with no formal guarantees provided for the function approximation case used in experiments.
- The effectiveness of the proposed methods depends heavily on accurate quantile estimation from the distributional critic, which may be unreliable in offline settings with limited or biased data.
- The risk spectrum φ must be carefully chosen to match true risk preferences, as incorrect specification results in optimizing the wrong objective.

## Confidence

- **High Confidence**: The mechanism of using bi-level optimization with supremum representation for static SRM is sound and theoretically justified for tabular cases. The extended state space approach for time-consistent risk adjustment is well-established in the literature.
- **Medium Confidence**: Empirical performance claims are supported by extensive experiments across diverse domains, but the relative improvements may depend on specific hyperparameter choices and implementation details not fully specified in the paper.
- **Low Confidence**: The paper's claims about avoiding iterative risk measure pitfalls are conceptually correct, but the practical significance depends on how often real-world problems actually suffer from this issue versus benefiting from iterative risk sensitivity.

## Next Checks

1. **Convergence validation**: Run the algorithms with increasing state-action space sizes to empirically assess when theoretical convergence guarantees break down, and measure the gap between optimal and learned policies in function approximation settings.
2. **Sensitivity analysis**: Systematically vary the risk spectrum φ parameters (e.g., α for CVaR) across a wider range to quantify how performance degrades when risk preferences are misspecified.
3. **Offline data quality impact**: Create synthetic offline datasets with varying levels of coverage and noise, then measure how quantile estimation errors propagate to risk function h updates and ultimately affect policy performance.