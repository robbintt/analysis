---
ver: rpa2
title: Generative Modeling through Spectral Analysis of Koopman Operator
arxiv_id: '2512.18837'
source_url: https://arxiv.org/abs/2512.18837
tags:
- koopman
- kswgd
- spectral
- generative
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Koopman Spectral Wasserstein Gradient Descent
  (KSWGD), a particle-based generative modeling framework that learns the Langevin
  generator via Koopman theory and integrates it with Wasserstein gradient descent.
  The key insight is that the spectral structure required for accelerated Wasserstein
  gradient descent can be directly estimated from trajectory data via Koopman operator
  approximation, eliminating the need for explicit knowledge of the target potential
  or neural network training.
---

# Generative Modeling through Spectral Analysis of Koopman Operator

## Quick Facts
- arXiv ID: 2512.18837
- Source URL: https://arxiv.org/abs/2512.18837
- Authors: Yuanchao Xu; Fengyi Li; Masahiro Fujisawa; Youssef Marzouk; Isao Ishikawa
- Reference count: 40
- Key outcome: Koopman Spectral Wasserstein Gradient Descent (KSWGD) achieves faster convergence than existing particle-based generative methods by directly estimating spectral preconditioners from trajectory data

## Executive Summary
KSWGD is a particle-based generative modeling framework that learns the Langevin generator via Koopman operator theory and integrates it with Wasserstein gradient descent. The method estimates the spectral structure required for accelerated Wasserstein gradient descent directly from trajectory data, eliminating the need for explicit knowledge of the target potential or neural network training. By maintaining an approximately constant dissipation rate through spectral truncation, KSWGD overcomes the vanishing-gradient phenomenon that hinders existing kernel-based particle methods while achieving linear convergence.

## Method Summary
KSWGD constructs a data-driven spectral preconditioner by approximating the Koopman operator from trajectory data using EDMD or kernel-EDMD, then using its eigenpairs to create a truncated inverse operator. The particle update velocity depends on the gradient of the estimated eigenfunctions rather than the gradient of a potential or score. The framework accommodates both time-series data (via direct trajectory pairs) and static data (via synthetic Langevin pairs generated through KDE score estimation). Once the spectral basis is estimated, the generative sampling process is deterministic and "training-free" in the sense of deep learning.

## Key Results
- KSWGD consistently achieves faster convergence than existing methods across diverse settings including compact manifold sampling, metastable multi-well systems, image generation, and high-dimensional SPDEs
- The approach maintains an approximately constant dissipation rate, establishing linear convergence and overcoming the vanishing-gradient phenomenon typical of kernel-based methods
- Experiments demonstrate that KSWGD achieves high sample quality while being training-free once the Koopman basis is estimated

## Why This Works (Mechanism)

### Mechanism 1: Data-Driven Spectral Preconditioning
The method approximates the ideal preconditioner for Wasserstein gradient descent (the inverse Langevin generator) without explicit knowledge of the target potential function by exploiting the identity $A = -L$, where $A$ is the Koopman infinitesimal generator and $L$ is the Langevin generator. The framework estimates the spectral components of $A$ from trajectory data using EDMD, then constructs a truncated inverse operator to define the particle update rule.

### Mechanism 2: Constant Dissipation Rate via Truncation
KSWGD prevents the "vanishing-gradient" phenomenon typical of kernel-based particle methods by truncating the spectral expansion to rank $r$, restricting dynamics to a subspace where eigenvalues are lower-bounded. This maintains an approximately constant dissipation rate $\alpha \approx 1 - \epsilon_r$, establishing linear convergence.

### Mechanism 3: Gradient-Free Particle Evolution
The generative sampling process is deterministic and "training-free" once the spectral basis is established, with particle update velocity depending on the gradient of the estimated eigenfunctions $\nabla \hat{\phi}_k$ rather than the gradient of a potential or score.

## Foundational Learning

- **Koopman Operator Theory**: Converts nonlinear state dynamics into linear operator dynamics by acting on "observables" (functions of state) rather than the state itself. Why needed: This is the mathematical engine that enables the spectral preconditioning approach.
- **Wasserstein Gradient Flows**: Frames generative modeling as moving a distribution of particles along a gradient flow in probability space. Why needed: Understanding this geometry is essential for grasping how preconditioners accelerate convergence.
- **Extended Dynamic Mode Decomposition (EDMD)**: Practical algorithm used to estimate the Koopman eigenvalues and eigenfunctions from finite data matrices. Why needed: This is the core computational method for obtaining the spectral components from trajectory data.

## Architecture Onboarding

- **Component map**: Data Ingest -> Spectral Estimator -> Particle Store -> Update Engine
- **Critical path**: The estimation of the Koopman matrix and its subsequent eigendecomposition. If the dictionary size $n$ is too small or ill-conditioned, the spectral approximation fails.
- **Design tradeoffs**:
  - Truncation Rank ($r$): High $r$ reduces bias but increases computational cost and risks fitting noise; low $r$ ensures stability but may miss finer distributional modes
  - Step Size ($h$): Controls convergence speed vs. stability; theoretical bounds require $h < 1/\alpha$ but must be tuned
- **Failure signatures**:
  - Manifold Drift: Particles deviating from underlying manifold geometry
  - Mode Collapse: Particles converging to basin minima rather than full distribution
  - Numerical Divergence: Exploding particle norms if estimated eigenvalues are near zero
- **First 3 experiments**:
  1. 1D/2D Gaussian Mixture: Verify particles migrate from localized init to multiple modes using synthetic dynamics
  2. Quadruple Well Potential: Test "vanishing gradient" fix by comparing movement rates against SVGD or DMPS
  3. Torus Manifold Sampling: Validate Riemannian geometry handling by ensuring particles stay on manifold surface

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the KSWGD framework be extended to conditional sampling problems using non-zero killing rates in the Feynman-Kac formulation?
- **Basis in paper**: The conclusion explicitly states this as an interesting direction for future work.
- **Why unresolved**: Current formulation assumes zero potential ($U=0$) to target unconditional sampling; implementing conditional sampling requires integrating importance weights (killing rates) into the Wasserstein gradient flow.

### Open Question 2
- **Question**: Can the approach for static data be refined to close the performance gap with diffusion models?
- **Basis in paper**: Appendix A.4 notes a "notable performance gap" compared to Latent Diffusion Models on CelebA-HQ.
- **Why unresolved**: The method relies on constructing synthetic dynamics from static data (using KDE), which appears less effective than specialized architectures used in state-of-the-art image generation.

### Open Question 3
- **Question**: To what extent does the choice of dictionary (basis functions) affect the stability of the convergence guarantees?
- **Basis in paper**: Theoretical convergence relies on Assumption 4.6, but the paper experiments with diverse dictionaries without analyzing how choice impacts error bounds.
- **Why unresolved**: While convergence rate depends on eigenvalue estimation quality, the sensitivity to richness or type of Koopman dictionary remains unquantified.

## Limitations

- The method relies on critical assumptions about trajectory data sampling the invariant measure and dictionary functions capturing relevant slow modes
- High-dimensional settings face exponential growth in dictionary size, potentially limiting practical applicability
- The truncation rank introduces a fundamental bias-variance tradeoff that is not fully characterized

## Confidence

- **High Confidence**: Theoretical framework connecting Koopman operators to Langevin generators and linear convergence proof structure
- **Medium Confidence**: Practical efficacy across diverse experimental domains, though sample complexity requirements need more systematic evaluation
- **Low Confidence**: Claim of being "training-free" - while no neural network training is required, parameter tuning (kernel bandwidth, truncation rank, step size) is still required

## Next Checks

1. **Dictionary Sensitivity Analysis**: Systematically evaluate KSWGD performance across varying dictionary sizes and basis function types on the Quadruple Well benchmark, quantifying the tradeoff between approximation error and computational cost.

2. **High-Dimensional Scaling Study**: Test KSWGD on progressively higher-dimensional synthetic distributions (e.g., Gaussian mixtures in 10D, 20D, 50D) to empirically validate claims about curse-of-dimensionality mitigation.

3. **Comparison to Adaptive Step Size Methods**: Benchmark KSWGD against modern adaptive step size particle methods (e.g., SVGD with adaptive kernels) on the same problems to isolate the contribution of spectral preconditioning versus step size optimization.