---
ver: rpa2
title: 'LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring'
arxiv_id: '2512.24235'
source_url: https://arxiv.org/abs/2512.24235
tags:
- arabic
- essays
- laila
- essay
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAILA, the first large-scale Arabic Automated
  Essay Scoring (AES) dataset with both holistic and trait-specific annotations across
  seven writing dimensions. It contains 7,859 essays from 4,372 students across 8
  prompts, collected under exam-like conditions in 24 Qatari high schools.
---

# LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring

## Quick Facts
- **arXiv ID**: 2512.24235
- **Source URL**: https://arxiv.org/abs/2512.24235
- **Reference count**: 40
- **Primary result**: First large-scale Arabic AES dataset with 7,859 essays across 8 prompts, achieving state-of-the-art performance with AraBERT (QWK: 0.74) in prompt-specific setup

## Executive Summary
This paper introduces LAILA, the first large-scale Arabic Automated Essay Scoring dataset with both holistic and trait-specific annotations across seven writing dimensions. The dataset contains 7,859 essays from 4,372 students across 8 prompts, collected under exam-like conditions in 24 Qatari high schools. Human annotators used the CAST rubric to score essays, achieving substantial inter-annotator agreement. Benchmarking experiments show that AraBERT achieves state-of-the-art performance in the prompt-specific setup (average QWK: 0.74), while feature-based models remain competitive in the cross-prompt setup, highlighting the need for future research on generalizable AES models. LAILA fills a critical gap in Arabic AES resources and provides strong baselines for future research.

## Method Summary
LAILA is a multi-task dataset for Arabic AES containing 7,859 essays across 8 prompts with 7 trait-specific scores plus holistic scores. The dataset supports both prompt-specific (training with target prompt data) and cross-prompt (generalizing to unseen prompts) evaluation setups. Human annotators used the CAST rubric to score essays, achieving substantial inter-annotator agreement. Benchmarking experiments compare feature-based models (LR, RF, XGB, NN with 816 handcrafted features), encoder-based models (AraBERT, AraT5, ProTACT, MOOSE), and LLMs (ALLaM, Command-R7B-Arabic, Fanar) in both zero-shot and few-shot settings. Hyperparameters were tuned via Optuna TPESampler (20 trials, 5 startup, seed=11) with 5-fold cross-validation for prompt-specific and 8-fold leave-one-prompt-out for cross-prompt evaluation.

## Key Results
- AraBERT achieves state-of-the-art performance in prompt-specific setup with average QWK: 0.74
- Feature-based models (XGB) remain remarkably competitive in cross-prompt setup with QWK: 0.597
- AraBERT performance drops significantly in cross-prompt setup (QWK: 0.549), while feature-based models maintain robustness
- Few-shot LLM prompting provides consistent but sub-optimal performance, scoring 16.5 points below AraBERT in prompt-specific setup

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned Arabic-specific encoder models (AraBERT) achieve state-of-the-art performance in prompt-specific AES. Pre-trained language models capture rich contextual and semantic representations that, when fine-tuned with prompt-specific essays, align closely with human scoring patterns. The core assumption is that sufficient labeled essays from the target prompt exist for fine-tuning. Evidence shows AraBERT achieving QWK: 0.74 in prompt-specific setup, but performance degrades to QWK: 0.549 in cross-prompt when training data is scarce.

### Mechanism 2
Handcrafted linguistic features generalize better across prompts than encoder-based models in cross-prompt AES. Feature-based models capture explicit, interpretable linguistic properties (surface, readability, lexical, semantic, syntactic) that are less sensitive to prompt-specific content patterns. The core assumption is that features are designed to capture writing quality independent of topic. Evidence shows feature-based models remain competitive in cross-prompt setup, while encoder-based models like AraBERT drop significantly in performance.

### Mechanism 3
Few-shot LLM prompting provides consistent but sub-optimal AES performance across both evaluation setups. LLMs infer scoring criteria from in-context examples without parameter updates, capturing general evaluation principles but lacking prompt-specific optimization. The core assumption is that the LLM has sufficient Arabic language understanding and the rubric can be implicitly learned from examples. Evidence shows few-shot LLMs score 16.5 points below AraBERT in prompt-specific setup, indicating a performance ceiling without fine-tuning.

## Foundational Learning

- **Concept**: Quadratic Weighted Kappa (QWK)
  - **Why needed here**: Primary evaluation metric measuring agreement between model and human scores, accounting for chance agreement and ordinal nature of scores
  - **Quick check question**: Why is QWK preferred over simple accuracy or correlation for AES evaluation?

- **Concept**: Trait-based vs. Holistic Scoring
  - **Why needed here**: LAILA provides both granular trait scores (7 dimensions) and holistic scores, enabling multi-task learning and fine-grained feedback
  - **Quick check question**: What are the tradeoffs between predicting individual traits vs. a single holistic score?

- **Concept**: Prompt-specific vs. Cross-prompt Evaluation
  - **Why needed here**: Two fundamentally different real-world scenariosâ€”training with target prompt data (prompt-specific) vs. generalizing to unseen prompts (cross-prompt)
  - **Quick check question**: Why does cross-prompt performance typically lag behind prompt-specific performance?

## Architecture Onboarding

- **Component map**: Essay text -> AraBERT tokenizer -> Encoder (AraBERT) -> Max-pooled features -> Concatenate with handcrafted features (816) -> 8 parallel regression heads (7 traits + holistic) -> Sigmoid output rescaled to trait-specific ranges

- **Critical path**: 1) Text preprocessing with AraBERT tokenizer, 2) Feature extraction (surface, readability, lexical, semantic, syntactic), 3) Feature selection based on Pearson/Spearman correlation thresholds, 4) Encoder fine-tuning with separate learning rates for encoder and regression heads, 5) Multi-task loss computation across all traits, 6) QWK-based evaluation on development set for hyperparameter selection

- **Design tradeoffs**: Prompt-specific setup achieves highest accuracy (0.74 QWK) but requires labeled data from target prompt; cross-prompt setup is more realistic but has lower ceiling (0.597 QWK); feature-based models surprisingly robust; LLM zero-shot/few-shot enables fast deployment but underperforms fine-tuned models by 15-20 points

- **Failure signatures**: REL trait underperformance with narrow 0-2 scale limiting granularity; cross-prompt encoder collapse with AraBERT dropping from 0.740 to 0.549; LLM context overflow with few-shot examples truncated when exceeding 4096 token limit

- **First 3 experiments**: 1) Establish prompt-specific baseline by fine-tuning AraBERT with handcrafted features on single prompt using 5-fold cross-validation targeting 0.72+ average QWK, 2) Evaluate cross-prompt generalization by training on 5 prompts, validating on 1, testing on 1 (leave-one-prompt-out) and comparing AraBERT vs. XGB feature-based approach, 3) Analyze trait-specific difficulty by identifying which traits (e.g., Grammar, Mechanics) show lower inter-annotator agreement and model performance

## Open Questions the Paper Calls Out

### Open Question 1
How can Arabic AES models be improved to achieve cross-prompt generalization comparable to or exceeding feature-based approaches? The conclusion states "the need to prioritize future research on cross-prompt AES" while noting that feature-based models like XGB remain "remarkably competitive." This remains unresolved as AraBERT's performance drops significantly from QWK 0.74 to 0.549 in cross-prompt settings.

### Open Question 2
Can fine-tuned LLMs bridge the performance gap with encoder-based models in Arabic AES? The authors note that few-shot LLMs "still lagged behind, with the best variant, Fanar (5), scoring 16.5 points below AraBERT" in prompt-specific settings. This remains unresolved as LLMs were only evaluated in zero-shot and few-shot settings without exploring fine-tuning.

### Open Question 3
How do Arabic AES models trained on LAILA generalize to other Arabic-speaking populations with diverse educational systems and dialects? The limitations section states the dataset was "collected from students in a single country, Qatar" which may limit generalizability to other Arabic-speaking populations. This remains unresolved as no external validation was conducted on essays from other Arab countries.

### Open Question 4
What architectures or training methods are needed for Arabic AES models to perform robustly on long-form essays (300+ words) and diverse writing genres? The limitations note the dataset "lacks extended writing samples" (average 171 words) and "includes only explanatory and persuasive writing prompts." This remains unresolved as current models were only validated on short, argumentative essays.

## Limitations
- Limited generalizability across diverse Arabic dialects and educational contexts due to Qatari-specific dataset collection
- Inter-annotator agreement constraints and their impact on model training reliability not fully characterized
- LLM prompting strategy ambiguity with limited detail on few-shot example selection and formatting

## Confidence
- **High Confidence**: Prompt-specific AES performance (AraBERT achieving QWK 0.74) and cross-prompt feature-based model competitiveness
- **Medium Confidence**: Cross-prompt encoder performance degradation and LLM consistent-but-suboptimal performance
- **Low Confidence**: Generalization claims beyond the Qatari context and the full explanation for REL trait underperformance across all models

## Next Checks
1. Reproduce cross-prompt setup with MOOSE adaptation using specified AraBERT versions and evaluate on same leave-one-prompt-out splits to verify reported QWK of 0.549 for AraBERT
2. Analyze inter-annotator disagreement patterns across traits and prompts to understand impact on model training and identify potential annotation guidelines improvements
3. Test LLM prompting variations by systematically varying the number of few-shot examples (1, 3, 5, 10) and their selection criteria to establish relationship between prompt design and performance