---
ver: rpa2
title: 'Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning
  and Bounded Rationality'
arxiv_id: '2502.18529'
source_url: https://arxiv.org/abs/2502.18529
tags:
- regret
- error
- have
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies heterogeneous decision making between automated
  vehicles (AVs) and human-driven vehicles (HVs) in mixed traffic environments. The
  authors formulate the problem as a stochastic game where HVs make decisions with
  bounded rationality while AVs employ uncertainty-aware planning based on predicted
  HV actions.
---

# Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality

## Quick Facts
- **arXiv ID:** 2502.18529
- **Source URL:** https://arxiv.org/abs/2502.18529
- **Reference count:** 40
- **Key outcome:** This paper studies heterogeneous decision making between automated vehicles (AVs) and human-driven vehicles (HVs) in mixed traffic environments, formulating the problem as a stochastic game with uncertainty-aware AV planning and bounded rationality HV behavior.

## Executive Summary
This paper addresses the challenge of decision-making in mixed traffic where automated vehicles (AVs) must interact with human-driven vehicles (HVs) exhibiting bounded rationality. The authors formulate the problem as a stochastic game where AVs employ uncertainty-aware planning based on predicted HV actions, while HVs make sub-optimal decisions due to cognitive limitations. The core contribution is a regret analysis that decomposes system performance into AV planning regret and HV bounded rationality regret, revealing key insights about optimal planning horizon selection and the relative importance of different error sources.

## Method Summary
The method involves AVs using L-step lookahead planning with function approximation error, while HVs make decisions with bounded rationality errors. The AV predicts HV actions with uncertainty, then optimizes its own trajectory over L steps using the predicted HV actions. The regret analysis provides upper bounds that decompose into AV and HV components, showing how prediction errors accumulate during lookahead and how bounded rationality creates a persistent performance floor. The authors derive theoretical regret bounds and validate them empirically, demonstrating that prediction error has significantly larger impact on system performance than function approximation error.

## Key Results
- Increasing planning horizon initially helps AV performance but eventually degrades it due to Goodhart's Law effects from compounding prediction errors
- HV's bounded rationality creates a persistent regret contribution that does not decay with interaction time, unlike AV's regret which decreases with T
- Prediction error has 500x larger impact on regret than function approximation error, suggesting prediction model improvement should be prioritized
- Optimal planning horizon balances function approximation error decay against prediction error accumulation, typically around L=3-5

## Why This Works (Mechanism)

### Mechanism 1: Goodhart's Law in AV Lookahead Planning
- **Claim:** Increasing planning horizon L initially reduces AV regret but eventually increases it due to prediction error compounding.
- **Mechanism:** The regret bound contains two competing terms: γ^L·μ_{v,0} (function approximation error, decays with longer L) and Σ_{l=1}^L γ^l·√(EA(l)) (prediction error accumulation, grows with L). The balance point determines optimal horizon.
- **Core assumption:** AV's prediction error ε_A(t) ~ N(μ_A, σ_A²I) remains bounded but accumulates as A^{l-1}B_H ε_A during state rollouts.
- **Evidence anchors:** Performance gap curves show U-shaped relationship with planning horizon across all settings.
- **Break condition:** When prediction error variance σ_A² exceeds a threshold relative to function approximation error μ_{v,0}, longer horizons always harm performance.

### Mechanism 2: Bounded Rationality Induces Static Regret Floor
- **Claim:** HV's bounded rationality creates a persistent regret contribution that does not decay with interaction time T.
- **Mechanism:** HV regret is bounded by s_max·M·σ_H² + (s_max + λ_H)||μ_H||², independent of T. Unlike AV's γ^L/T term which decays, HV errors compound additively.
- **Core assumption:** HV selects sub-optimal actions u_H(t) = u*_H(t) + ε_H(t) where ε_H(t) ~ N(μ_H, Σ_H) captures both systematic bias (μ_H) and noise (Σ_H).
- **Evidence anchors:** Explicit regret bound independent of interaction count T.
- **Break condition:** If HV's planning horizon N approaches L and μ_H → 0 (fully rational), HV regret contribution vanishes.

### Mechanism 3: Prediction Error Dominates Function Approximation Error
- **Claim:** System performance is substantially more sensitive to prediction error (μ_A) than function approximation error (μ_{v,0}).
- **Mechanism:** System regret decomposes into Ψ_A (prediction), Ψ_v (function approximation), and Ψ_H (bounded rationality) terms. The empirical scaling shows 30% regret reduction from halving μ_A vs. 0.06% from halving μ_{v,0}.
- **Core assumption:** Discount factor γ < 1 causes function approximation errors to be discounted by γ^L, while prediction errors accumulate without such damping in the diffusion model.
- **Evidence anchors:** Visual comparison shows μ_A changes affect regret by ~2000 units while μ_{v,0} changes affect by ~0.04 units.
- **Break condition:** When γ → 1 or L is very short, function approximation error can dominate.

## Foundational Learning

- **Concept: Stochastic Games (SG)**
  - **Why needed here:** The AV-HV interaction is formalized as tuple M = (X, U_A, U_H, P, r_A, r_H, γ). Understanding joint action spaces and transition probabilities is prerequisite for following Section 3's formulation.
  - **Quick check question:** Can you explain why a stochastic game differs from a standard MDP when modeling two agents with asymmetric information?

- **Concept: Regret Analysis with Adaptive Policies**
  - **Why needed here:** The paper uses regret R_A(T|u_H) = E[1/T Σ (V*(x|u_H(t)) - V_{π_t}(x))] which differs from standard regret by conditioning on opponent actions and using time-varying policies π_t.
  - **Quick check question:** Why does adaptive regret (tracking changing π_t) require different analytical techniques than fixed-policy regret?

- **Concept: Wasserstein Distance for Distribution Comparison**
  - **Why needed here:** The non-linear case uses Wasserstein distance W(P̂, P) to bound the impact of prediction errors on value function differences, not KL divergence which can be unbounded.
  - **Quick check question:** Given two Gaussians N(μ₁, σ₁²I) and N(μ₂, σ₂²I), can you derive the Wasserstein distance formula used in Equation (12)?

## Architecture Onboarding

- **Component map:**
  - Observe state x(t) → HV Prediction → û_H sequence
  - AV Planning → solve argmax over L-step trajectory → action u_A(t)
  - Execute u_A(t), observe true u_H(t) and next state x(t+1)
  - Update prediction error statistics (μ_A, σ_A) if time-varying
  - Update value function approximation Q̂_t

- **Critical path:**
  1. Observe state x(t) → HV Prediction → û_H sequence
  2. AV Planning → solve argmax over L-step trajectory → action u_A(t)
  3. Execute u_A(t), observe true u_H(t) and next state x(t+1)
  4. Update prediction error statistics (μ_A, σ_A) if time-varying
  5. Update value function approximation Q̂_t

- **Design tradeoffs:**
  - **L (planning horizon):** Larger L reduces function approximation dependency (γ^L factor) but increases prediction error accumulation. Paper suggests L=3-5 as typical sweet spot.
  - **Discount factor γ:** Higher γ amplifies prediction error impact but enables longer-term reasoning. Paper uses γ=0.85 as default.
  - **Prediction model complexity vs. accuracy:** Time-varying μ_A(t) captures learning but requires maintaining full covariance over accumulated errors.

- **Failure signatures:**
  - **Exploding regret with long horizons:** If L > 6-8 and prediction error is non-zero, regret increases sharply → indicates prediction model is unreliable for long rollouts
  - **Flat regret curve over T interactions:** If HV regret dominates, system cannot improve regardless of AV learning → indicates need to address HV modeling
  - **Regret divergence between γ settings:** Large gap between γ=0.5 and γ=0.9 curves → prediction error is the bottleneck, not value approximation

- **First 3 experiments:**
  1. **Horizon sweep:** Run AV with L ∈ {1, 2, 3, 5, 8, 10} on fixed HV policy, plot performance gap. Identify the L where regret is minimized.
  2. **Error source isolation:** Fix all parameters, vary μ_A ∈ {0.1, 0.2, 0.4, 0.5} while keeping μ_{v,0}=10. Then fix μ_A and vary μ_{v,0} ∈ {1, 10, 50, 100}. Quantify regret sensitivity ratio.
  3. **Regret decomposition validation:** Compute R_A(T), R_H(T), and R_{A-H}(T) separately. Verify that R_{A-H}(T) ≤ Σ_l terms + Ξ_H + γ^L/T·Ψ_v from Corollary 5.1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the bounded rationality parameters (μH, σH) of individual human drivers be accurately estimated online from interaction data, and how quickly must this estimation converge to improve overall system performance?
- **Basis in paper:** The authors state "it remains open to develop accurate models for HV's decision making" and assume bounded rationality parameters are "unknown a prior."
- **Why unresolved:** The paper analyzes regret bounds assuming fixed parameters but does not address the practical challenge of learning these parameters during deployment.
- **What evidence would resolve it:** An algorithm that provably estimates human bounded rationality parameters with sample complexity bounds, validated in simulation or real-world driving scenarios.

### Open Question 2
- **Question:** How does the regret analysis extend to scenarios with multiple AVs and HVs, where inter-agent coupling creates non-linear effects on prediction error propagation?
- **Basis in paper:** Remark 5.2 mentions "our analysis approach is feasible to extend beyond one AV and one HV setting" but only provides "preliminary steps" in Appendix G.
- **Why unresolved:** The extension in Appendix G assumes simple scaling by agent counts, but real multi-agent interactions likely exhibit complex coupling effects not captured by linear scaling.
- **What evidence would resolve it:** Formal regret bounds for N_AV > 1 and N_HV > 1 scenarios with empirical validation showing non-linear scaling effects.

### Open Question 3
- **Question:** Can the Goodhart's Law effect in AV planning be mitigated through adaptive planning horizon selection based on real-time uncertainty estimates?
- **Basis in paper:** The paper identifies Goodhart's Law where "increasing planning horizon initially helps but eventually degrades performance" and suggests "adjusting the look-ahead length (e.g., through grid search) is essential."
- **Why unresolved:** Grid search is impractical for real-time systems; no principled method for dynamically adjusting L based on current prediction uncertainty is provided.
- **What evidence would resolve it:** An adaptive algorithm that adjusts L online with theoretical guarantees and empirical demonstration of improved regret over fixed-L strategies.

## Limitations

- The theoretical regret bounds assume Gaussian prediction errors with known parameters, but real-world traffic prediction errors may have heavier tails or time-varying statistics
- The analysis focuses on linear dynamics and quadratic rewards, which may not fully capture the nonlinear and multimodal nature of real traffic scenarios
- The Goodhart's Law effect for planning horizon is demonstrated theoretically but the optimal horizon (L=3-5) is based on synthetic parameters rather than empirically validated in real traffic data

## Confidence

- **High Confidence:** The regret decomposition showing prediction error dominates function approximation error (verified through Corollary 5.1 and empirical scaling analysis)
- **Medium Confidence:** The Goodhart's Law mechanism for planning horizon selection (theoretically sound but parameter-dependent optimal point)
- **Medium Confidence:** The bounded rationality regret floor being independent of interaction time T (theoretically derived but assumes stationary HV behavior)

## Next Checks

1. **Error Distribution Validation:** Test whether replacing Gaussian prediction errors with t-distribution or other heavy-tailed distributions changes the regret bounds and optimal planning horizon significantly
2. **Nonlinear Dynamics Extension:** Apply the regret analysis framework to a nonlinear traffic model (e.g., intelligent driver model) and verify if prediction error still dominates function approximation error
3. **HV Policy Variation:** Test the regret bounds when HV policies are not stationary (learning over time) to see if the bounded rationality regret floor assumption holds under policy adaptation