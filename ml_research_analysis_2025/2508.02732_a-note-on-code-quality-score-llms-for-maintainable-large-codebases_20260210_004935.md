---
ver: rpa2
title: 'A Note on Code Quality Score: LLMs for Maintainable Large Codebases'
arxiv_id: '2508.02732'
source_url: https://arxiv.org/abs/2508.02732
tags:
- code
- issue
- review
- issues
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Code Quality Score (CQS) system, an LLM-powered
  system designed to automatically evaluate code quality and generate code reviews
  at industrial scale. The CQS system is powered by two fine-tuned Llama3 models,
  trained using supervised fine-tuning (SFT) and Direct Preference Optimization (DPO),
  to detect common code quality issues and provide critiques for LLM-generated code
  reviews.
---

# A Note on Code Quality Score: LLMs for Maintainable Large Codebases

## Quick Facts
- arXiv ID: 2508.02732
- Source URL: https://arxiv.org/abs/2508.02732
- Authors: Sherman Wong, Jalaj Bhandari, Leo Zhou Fan Yang, Xylan Xu, Yi Zhuang, Cem Cayiroglu, Payal Bhuptani, Sheela Yadawad, Hung Duong
- Reference count: 25
- Primary result: Multi-agent LLM system achieves 78.2% precision for automated code reviews at Meta scale

## Executive Summary
This paper introduces the Code Quality Score (CQS) system, an LLM-powered system designed to automatically evaluate code quality and generate code reviews at industrial scale. The CQS system is powered by two fine-tuned Llama3 models, trained using supervised fine-tuning (SFT) and Direct Preference Optimization (DPO), to detect common code quality issues and provide critiques for LLM-generated code reviews. To maintain a good user experience, the system includes hand-crafted rules to filter out incorrect responses and hallucinations. Offline evaluations show that the CQS system achieves an impressive precision rate for identifying valid issues, and it has been rolled out to over 5000 engineers at Meta, achieving a week-over-week user helpfulness rate of approximately 60%.

## Method Summary
The CQS system uses a two-stage fine-tuning approach with Llama 3.1-70b as the base model. First, an "Issue Collector" model is trained via supervised fine-tuning on 5k curated (diff, review) pairs, then further refined with Direct Preference Optimization using 8.4k preference pairs generated by an LLM-judge. A separate "Issue Validator" (Judge) model is trained on 1.5k (review, critique) pairs. During inference, the Collector proposes issues, the Validator scores them 0-10, and rule-based filters remove low-scoring or hallucinated outputs. The system explicitly prioritizes precision over recall, achieving 78.2% precision at 1.2% recall.

## Key Results
- The CQS system achieves 78.20% precision for valid issue identification
- Week-over-week user helpfulness rate of approximately 60% across 5000+ Meta engineers
- DPO fine-tuning improves recall from 7.62% (SFT only) to 9.25% for the Collector model
- Precision increases from ~13% (Collector only) to 78.20% when using the Validator and rules

## Why This Works (Mechanism)

### Mechanism 1: Specialized Critique-Based Alignment (DPO)
If an LLM is first fine-tuned via SFT and then aligned using Direct Preference Optimization (DPO) with an LLM-judge, it appears to improve recall and generalization for code issue detection compared to SFT alone. SFT forces the model to mimic high-quality human reviews, while DPO sharpens this by optimizing a preference objective: pushing up valid issues and pushing down invalid ones, learning the boundary of developer acceptance without training a separate reward model. The core assumption is that the LLM-judge serves as a scalable, accurate proxy for human developer preferences. Evidence shows the SFT+DPO model achieves 9.25% recall versus 7.62% for SFT-only. Performance degrades if the LLM-judge is miscalibrated, causing the model to optimize for "judge-approved" but human-irrelevant comments.

### Mechanism 2: Multi-Agent Precision Filtering
Decomposing the review process into separate "Issue Collection" and "Issue Validation" agents significantly boosts precision, trading a small loss in recall for high user trust. A generative "Collector" agent maximizes coverage of potential issues, while a discriminative "Validator" agent scores and filters these issues, preventing the Collector's hallucinations from reaching the user. The core assumption is that validation is easier than generation; a model can be trained to reliably critique an existing suggestion better than generate it from scratch. Evidence shows precision jumping from ~13% (Collector only) to 78.20% (with Validator and rules), while recall drops to 1.20%. If the Validator is too aggressive, the system suffers from "empty review" syndrome where valid but subtle issues are filtered out.

### Mechanism 3: Hybrid Rule-Neural Guardrails
Layering hand-crafted deterministic rules on top of LLM outputs is required to catch systematic hallucinations that persist even after fine-tuning. Neural models struggle with strict syntactic consistency (e.g., exact line numbers, specific language syntax constraints), so rule-based filters act as a final sanity check, enforcing hard constraints that cannot be reliably learned from limited SFT data. The core assumption is that the cost of false positives (annoying developers) is significantly higher than the cost of false negatives (missed issues). Evidence mentions layering the system with "hand-crafted rules to filter out incorrect responses/hallucinations" and details "hard coded filters specific to each issue tag" during post-processing. Maintenance burden increases as the codebase evolves; static rules may block valid reviews for new frameworks or languages not anticipated during rule authoring.

## Foundational Learning

- **Code Diffs as Context**: The system inputs code changes (diffs), not whole files. You must understand unified diff syntax (+/- lines) to debug why the model misses context or hallucinates line numbers. Quick check: Can you distinguish between context lines (starting with space) and added lines (starting with +) in a standard diff output?

- **Direct Preference Optimization (DPO)**: The paper relies on DPO for "Phase 2" training. Unlike standard RLHF which trains a separate reward model, DPO optimizes the policy directly on preference pairs. Quick check: In the context of this paper, what entity acts as the "human" labeler to generate the preference pairs for the DPO algorithm?

- **LLM-as-a-Judge**: The system uses a fine-tuned LLM to grade the output of another LLM. This "Judge" model determines what constitutes a "winner" vs "loser" response during training and filters output during inference. Quick check: Why might a generic base model (like standard Llama 3.1) fail as a Judge compared to the fine-tuned Judge described in the paper?

## Architecture Onboarding

- **Component map:** Input (Code Diff + Metadata) -> Issue Collector (Llama 3.1-70b SFT+DPO) -> Issue Validator (Llama 3.1-70b SFT as Judge) -> Post-Processor (Rule-based engine + Threshold filters) -> Output (Filtered code review comments)

- **Critical path:** The interaction between the Collector and Validator. If the Collector fails to propose a valid issue, the Validator never sees it (low recall floor). If the Validator is too lenient, low-quality reviews leak through (low precision ceiling).

- **Design tradeoffs:** The system explicitly optimizes for Precision over Recall (78.2% precision / 1.2% recall), preferring to stay silent rather than give a possibly wrong suggestion, prioritizing "user trust" over "total coverage."

- **Failure signatures:**
  - High Hallucination: Model suggests an issue at a line number that doesn't exist or references code deleted in the diff
  - Judge Misalignment: Validator gives a high score (8-10) to a comment that is syntactically correct but logically irrelevant to the business logic
  - Over-filtering: Valid security or performance issues are dropped by the post-processing rules due to strict keyword matching

- **First 3 experiments:**
  1. Threshold Tuning: Vary the Validator's score cutoff (e.g., 5 vs. 7) to map the trade-off curve between developer helpfulness ratings and volume of comments shown
  2. Ablation on SFT Data Quality: Retrain the Collector using only "high quality" human critiques vs. raw human comments to quantify the impact of data cleanliness
  3. Judge vs. Rules Audit: Route a sample of traffic to bypass the LLM-Judge and rely only on rules (and vice-versa) to isolate which component is responsible for the majority of valid issue rejections

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on a specialized LLM-judge for both training and inference, with critical configuration details undisclosed
- Extremely low recall (1.20%) raises questions about missed issues that developers would value
- Hand-crafted rules are not specified in detail, making generalizability and maintenance burden difficult to assess

## Confidence

**High Confidence Claims:**
- The multi-agent architecture (Collector + Validator) effectively improves precision over single-agent approaches
- DPO fine-tuning provides measurable recall improvements over SFT alone
- The system achieves substantial scale deployment with positive user reception

**Medium Confidence Claims:**
- The specific mechanisms by which the LLM-judge creates preference pairs
- The exact composition and impact of hand-crafted filtering rules
- The generalizability of the approach beyond Meta's internal codebase and languages

**Low Confidence Claims:**
- Long-term maintenance requirements for rule-based filters
- Performance on codebases significantly different from Meta's internal systems
- Impact of model drift over time as code patterns evolve

## Next Checks

1. **Judge Calibration Study:** Conduct a controlled experiment where human developers rate the same set of issues scored by the LLM-judge to quantify alignment gaps and identify systematic biases in the validation process.

2. **Recall Threshold Analysis:** Systematically vary the Validator's score threshold and measure the resulting trade-off between user helpfulness ratings and issue coverage to identify optimal operating points for different use cases.

3. **Rule Maintenance Simulation:** Implement a synthetic evolution of the codebase with new patterns and frameworks to stress-test the rule-based filters, measuring false negative rates and maintenance burden over time.