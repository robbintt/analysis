---
ver: rpa2
title: 'scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model
  Knowledge'
arxiv_id: '2503.04357'
source_url: https://arxiv.org/abs/2503.04357
tags:
- dataset
- distillation
- scrna-seq
- synthetic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses challenges in single-cell RNA sequencing (scRNA-seq)
  data analysis, including high dimensionality, sparsity, batch effects, and class
  imbalance. To overcome these issues, the authors propose a novel dataset distillation
  framework called scDD (latent codes-based scRNA-seq dataset distillation) that transfers
  foundation model knowledge and original dataset information into a compact latent
  space, generating synthetic datasets.
---

# scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model Knowledge

## Quick Facts
- **arXiv ID:** 2503.04357
- **Source URL:** https://arxiv.org/abs/2503.04357
- **Reference count:** 40
- **Primary result:** Achieves 7.61% absolute and 15.70% relative improvement over previous state-of-the-art methods for scRNA-seq dataset distillation

## Executive Summary
scDD introduces a novel dataset distillation framework for single-cell RNA sequencing (scRNA-seq) data that addresses challenges of high dimensionality, sparsity, batch effects, and class imbalance. The method transfers foundation model knowledge and original dataset information into a compact latent space, generating synthetic datasets that preserve biological characteristics and inter-class discriminability. By using a single-step conditional diffusion generator (SCDG) and optimizing latent codes rather than direct gene expression values, scDD significantly improves distillation performance across various scRNA-seq data analysis tasks while maintaining privacy-preserving properties suitable for cross-organization data sharing.

## Method Summary
scDD operates in two stages: first training a single-step conditional diffusion generator (SCDG) using a pretrained foundation model (SCimilarity), then performing distillation by optimizing latent codes in the foundation model's latent space. The method encodes original scRNA-seq data into latent codes Z, generates synthetic data through SCDG conditioned on class labels, and updates only the latent codes (not foundation model weights) using either gradient or distribution matching losses. The single-step diffusion approach avoids gradient decay while preserving the non-negativity and sparsity characteristics inherent to scRNA-seq data. The framework generates compact synthetic datasets (1-10 samples per class) that maintain classification performance across cell type annotation, disease status, development stage, and anatomical entity prediction tasks.

## Key Results
- Achieves 7.61% absolute and 15.70% relative improvement over previous state-of-the-art methods on average
- Maintains 85% of original performance at 0.94‰ compression (SPC=1) and 91% at 9.4‰ compression (SPC=10)
- SCDG improves average performance by 1.07% over standard decoder across six benchmark datasets
- Synthetic datasets demonstrate better generalization and inter-class discriminability compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
Latent-space optimization preserves scRNA-seq data characteristics better than direct gene expression updates. Instead of updating sparse, high-dimensional gene expression values directly (which disrupts non-negativity and sparsity patterns), the framework updates compact latent codes Z via a pretrained encoder. A generator (SCDG) then decodes these optimized codes back to synthetic gene expression data. This indirect optimization constrains outputs to remain within the learned manifold of valid scRNA-seq data. Core assumption: The foundation model's encoder-decoder captures sufficient biological structure that optimizing in its latent space preserves essential characteristics. Break condition: If the foundation model's latent space poorly represents rare cell types, distillation will fail for underrepresented classes regardless of optimization quality.

### Mechanism 2
Single-step diffusion back-propagation avoids gradient decay that degrades multi-step diffusion distillation. Traditional diffusion models chain T forward steps then T reverse steps, causing gradients to vanish through long computational graphs. SCDG performs iterative single-step cycles: sample t~U(1,T), add noise to get Zt, then denoise back to Z0 in one step. The denoising network Uθ directly predicts Z0 (not noise ε), enabling single-step gradient flow during distillation. Core assumption: Predicting Z0 directly is functionally equivalent to noise prediction (proven via linear transformation) but provides more stable gradients for distillation. Break condition: If t sampling is biased toward low noise levels, synthetic data diversity may collapse.

### Mechanism 3
Conditional generation during distillation maintains inter-class discriminability under category imbalance. The generator receives condition codes τθ(c) (e.g., cell-type labels) alongside latent codes. This ensures synthetic samples retain class-specific features even when distillation gradients from dominant classes would otherwise overwhelm rare class information. Cross-entropy loss on the task head provides explicit discriminative supervision. Core assumption: Condition embedding network τθ captures sufficient class-specific variance that synthetic samples remain distinct across categories. Break condition: If rare classes have insufficient training data to learn meaningful condition embeddings, synthetic samples will blur toward dominant classes.

## Foundational Learning

- **Concept: Diffusion Models (DDPM/LDM)**
  - **Why needed here:** SCDG modifies the standard diffusion training objective; understanding forward/reverse processes is prerequisite to grasping why single-step prediction differs from noise prediction.
  - **Quick check question:** Given a noisy latent Zt = √α̅t·Z0 + √(1-α̅t)·ε, can you derive what Uθ should predict to recover Z0 in one step?

- **Concept: scRNA-seq Data Characteristics**
  - **Why needed here:** The entire motivation stems from properties specific to scRNA-seq: ~90% sparsity, ~20k dimensions, non-negative counts, batch effects. Understanding why image-based distillation fails here is critical.
  - **Quick check question:** Why would updating gene expression values directly violate biological constraints that image pixel updates don't face?

- **Concept: Dataset Distillation (Gradient/Distribution Matching)**
  - **Why needed here:** scDD uses both gradient matching (LDC) and distribution matching (LDM) losses. Understanding what each objective optimizes helps diagnose which matching strategy suits specific tasks.
  - **Quick check question:** Gradient matching aligns ∂L/∂θ between expert and student models—what information does distribution matching capture that gradient matching might miss?

## Architecture Onboarding

- **Component map:** Original Dataset O → Encoder E → Latent Codes Z (trainable) → SCDG Generator G = D∘Uθ → Condition τθ(c) → Synthetic Dataset S → Foundation Model F + Task Head θ → Distillation Loss (LDC or LDM) → ∂L/∂Z → Update Z (not θ!)

- **Critical path:** The distillation loop updates only Z (latent codes), never foundation model weights. Freezing F is essential—Table VII shows 3110× loss increase when unfrozen.

- **Design tradeoffs:**
  - **Gradient vs. Distribution Matching:** Gradient matching outperforms on average (Table III) but requires careful task head sizing—smaller heads work better (Figure 4).
  - **SPC (samples per class):** SPC=1 gives ~85% of original performance at 0.94‰ compression; SPC=10 reaches ~91% (Table VI). Choose based on storage vs. accuracy constraints.
  - **SCDG vs. Standard Decoder:** SCDG adds +1.07% average improvement over plain decoder (Table V) but requires diffusion training overhead.

- **Failure signatures:**
  - Synthetic data clusters collapse in UMAP visualization → condition embedding τθ may be undertrained
  - Loss plateaus early with frozen foundation model → reduce task head size or increase SPC
  - Cross-architecture generalization drops sharply → latent codes may be overfitting to distillation model architecture

- **First 3 experiments:**
  1. **Sanity check:** Run distillation with SPC=5 on a small dataset (e.g., SMC-Blood) with frozen foundation model and single-layer task head. Verify synthetic data achieves >50% of original accuracy.
  2. **Ablation:** Compare SCDG vs. standard SCimilarity decoder as generator. Expect +1-2% improvement on cell-type annotation; if negative, check condition embedding quality.
  3. **Cross-architecture test:** Train synthetic data using SCimilarity, evaluate on CellTypist and scDeepInsight. If performance variance >15%, latent codes may lack architectural independence.

## Open Questions the Paper Calls Out

### Open Question 1
Does the latent codes-based distillation effectively preserve privacy against membership inference attacks? The abstract states the goal of delivering a "desensitized synthetic dataset" to enable "cross-organization privacy data resource sharing," but the paper provides no experiments or metrics verifying that the synthetic data prevents reconstruction of original patient-level records. What evidence would resolve it: Quantitative analysis using membership inference attacks to test if the synthetic dataset leaks information about specific cells in the original training set.

### Open Question 2
Does the disruption of data sparsity in the synthetic dataset hinder downstream biological analyses that rely on count statistics? Section V.C.3 acknowledges that the synthetic dataset "disrupted the sparsity of the original gene expressions" and filled them with "newly added non-zero values," but while classification performance is maintained, standard scRNA-seq workflows (e.g., differential expression, trajectory inference) often assume specific count distributions (e.g., zero-inflation) that the synthetic data may violate. What evidence would resolve it: Benchmarking the synthetic dataset on statistical biological discovery tasks (e.g., identifying differentially expressed genes) rather than just machine learning classification tasks.

### Open Question 3
Is the scDD framework robust to the choice of foundation model, or is it overfitted to the SCimilarity architecture? The method relies heavily on the SCimilarity encoder-decoder structure for mapping to the latent space. The paper does not test the distillation pipeline using alternative foundation models (e.g., scGPT or scBERT), leaving the dependency on SCimilarity's specific latent space unexplored. What evidence would resolve it: Experiments substituting the SCimilarity backbone with other pre-trained models to observe if the distillation quality remains stable.

## Limitations
- Relies heavily on SCimilarity foundation model without testing robustness to alternative architectures
- Lacks explicit training hyperparameters (learning rates, distillation iterations, batch sizes) and SCDG U-Net architectural details
- No statistical significance testing on the reported 15.70% relative improvement metric
- Assumes foundation model's latent space adequately captures biological variance for all rare cell types

## Confidence
- **High confidence:** The core mechanism of latent-space optimization preserving scRNA-seq characteristics over direct gene expression updates
- **Medium confidence:** The 7.61% absolute improvement metric, as it depends on unreported hyperparameter choices
- **Low confidence:** Generalization claims to unseen model architectures, as only two foundation models were evaluated

## Next Checks
1. Perform paired t-tests on classification accuracy across all datasets to establish statistical significance of improvements over LapDDPM
2. Evaluate synthetic data quality specifically on the rarest cell types (≤10 samples per class) to verify conditional generation maintains discriminability under extreme imbalance
3. Generate synthetic data using SCimilarity, then train and evaluate classifiers on two additional foundation models (e.g., scGPT, scBERT) not used during distillation to quantify cross-architecture generalization limits