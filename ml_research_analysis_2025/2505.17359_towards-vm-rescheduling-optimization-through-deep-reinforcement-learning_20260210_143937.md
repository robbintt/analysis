---
ver: rpa2
title: Towards VM Rescheduling Optimization Through Deep Reinforcement Learning
arxiv_id: '2505.17359'
source_url: https://arxiv.org/abs/2505.17359
tags:
- vmr2l
- each
- different
- rescheduling
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VMR2L addresses VM rescheduling (VMR) in large data centers, where
  VM migration is needed to optimize resource utilization. Existing methods like MIP
  solvers and heuristics either fail to meet strict latency requirements or provide
  suboptimal solutions.
---

# Towards VM Rescheduling Optimization Through Deep Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.17359
- **Source URL:** https://arxiv.org/abs/2505.17359
- **Reference count:** 40
- **Primary result:** VMR2L achieves Fragment Rate (FR) within 2.86% of optimal MIP solution at second-level inference time

## Executive Summary
VMR2L addresses virtual machine rescheduling in large data centers by leveraging deep reinforcement learning to optimize resource utilization under strict latency constraints. Traditional methods like MIP solvers provide optimal solutions but are too slow, while heuristics lack optimality. VMR2L introduces a two-stage RL framework with sparse attention and risk-seeking evaluation, achieving near-optimal performance (2.86% behind MIP) with inference times under 2 seconds. The system generalizes well across different constraints, objectives, and workload conditions, scaling to thousands of VMs and PMs while requiring minimal training data due to the deterministic nature of VMR.

## Method Summary
VMR2L uses a two-stage PPO-based RL approach where a VM actor first selects a VM to migrate, followed by a PM actor selecting a destination from valid PMs. The method employs sparse local attention to capture intra-PM fragmentation dynamics, and risk-seeking evaluation samples multiple trajectories at inference to deploy the highest-reward solution. Trained on deterministic VM-PM mappings from open datasets, the system achieves second-level inference times while maintaining FR within 2.86% of optimal MIP solutions. The approach scales to large clusters and generalizes across different MNL values and workload conditions without retraining.

## Key Results
- Achieves Fragment Rate within 2.86% of optimal MIP solution (0.2941 vs 0.2859 at MNL=50)
- Inference time under 2 seconds per VM-PM mapping, meeting strict latency requirements
- Generalizes to different MNL values, workload intensities, and constraints without retraining
- Data-efficient: requires only initial VM-PM mappings for training due to deterministic VMR dynamics

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Action Decomposition
Sequential VM-then-PM selection reduces exploration complexity while enforcing hard constraints by construction. Instead of sampling from O((M×N)^MNL) joint actions, Stage 1 selects a VM to migrate; Stage 2 selects a destination from only valid PMs (masked by resource/affinity constraints). This eliminates illegal actions without penalty-based shaping. The approach assumes constraint validity can be determined after VM selection through pairwise VM-PM checks.

### Mechanism 2: Sparse Local Attention for Tree-Level Relational Features
The model applies three-stage attention per block: sparse local attention within each PM tree (PM root + hosted VM leaves), self-attention across all PMs and across all VMs, and VM-PM cross-attention. This enables each VM to reason about co-located VMs' sizes to predict whether migration creates or eliminates fragments. The assumption is that fragmentation dynamics are primarily intra-PM, making local tree-level reasoning sufficient for effective rescheduling decisions.

### Mechanism 3: Risk-Seeking Evaluation with Trajectory Sampling
At inference, VMR2L samples K trajectories from the learned policy and uses the deterministic simulator to evaluate each trajectory's final FR, deploying the best. This exploits VMR's deterministic dynamics by sampling multiple action sequences rather than relying on single-argmax decisions. The approach assumes perfect world model access with no stochastic workload changes within an episode, enabling reliable trajectory evaluation.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: The underlying RL algorithm providing stable policy gradient updates with clipped objectives to prevent large policy shifts during training. Quick check: Can you explain why PPO's clipping mechanism helps when reward signals are dense but action spaces are large?

- **Attention Mechanisms (Self-Attention and Cross-Attention)**: Transformer-style attention processes variable-sized VM and PM sets while capturing relational dependencies. Self-attention computes interactions within a set, while cross-attention computes interactions between different sets. Quick check: How does self-attention differ from cross-attention, and where is each applied in the VMR2L architecture?

- **Mixed Integer Programming (MIP) Formulation**: VMR is originally formulated as MIP with objective to minimize X-core fragments under capacity, NUMA, and MNL constraints. Understanding this formulation clarifies what the RL agent approximates. Quick check: What does the Migration Number Limit (MNL) constrain, and why does increasing MNL exponentially increase MIP solve time?

## Architecture Onboarding

- **Component map**: Simulator -> VM Actor (Stage 1) -> PM Actor (Stage 2) -> Sparse Attention Blocks -> Risk-Seeking Evaluator
- **Critical path**: Load initial VM-PM mapping → Pass through shared embedding + sparse attention → VM actor selects VM → Compute valid PM mask → PM actor selects destination → Simulator applies action and computes reward → Repeat for MNL steps → Train via PPO → At deployment: sample K trajectories, simulate, deploy best
- **Design tradeoffs**: Two-stage simplifies exploration but may miss globally optimal joint placements; sparse attention reduces compute but may miss long-range dependencies; higher K improves solution quality but increases latency
- **Failure signatures**: Training non-convergence suggests incorrect constraint masking or sparse rewards; good training FR but poor deployment FR indicates distribution shift; slow inference (>5s) suggests attention cost growth beyond training distribution
- **First 3 experiments**: 1) Reproduce baseline comparison on Medium dataset vs HA, POP, MIP with MNL=50; 2) Ablate sparse attention to confirm FR gap reduction; 3) Test generalization to held-out workload from Medium-trained model

## Open Questions the Paper Calls Out
- Can allowing the agent to swap multiple VMs simultaneously identify feasible migration paths more efficiently than the current sequential approach?
- Does incorporating predictive features, such as estimated VM remaining runtime and future VM demands, enhance rescheduling performance?
- Can aligning the training objective with inference behavior via risk-seeking training improve the optimality of the learned policy?

## Limitations
- Two-stage decomposition may prune valid solutions when constraints require joint multi-VM feasibility (e.g., clique anti-affinity)
- Sparse attention may miss cluster-wide optimization opportunities requiring coordinated multi-PM swaps
- Risk-seeking evaluation effectiveness degrades if real-time workload changes occur mid-trajectory

## Confidence
- **High confidence**: FR within 2.86% of optimal MIP with second-level latency is well-supported by experimental results
- **Medium confidence**: Generalization claims supported by ablation studies but limited to workload intensity variations
- **Medium confidence**: Data efficiency claim is theoretically sound but depends on unspecified hyperparameters

## Next Checks
1. **Constraint Coverage Validation**: Test VMR2L on workloads requiring joint multi-VM feasibility constraints to verify the two-stage decomposition doesn't prune valid solutions
2. **Cross-Dataset Generalization**: Evaluate VMR2L trained on Medium dataset on completely different data center topologies to test true generalization capability
3. **Determinism Stress Test**: Introduce controlled stochastic workload changes during inference to quantify the impact on risk-seeking evaluation's effectiveness when environment assumptions are violated