---
ver: rpa2
title: A probabilistic framework for dynamic quantization
arxiv_id: '2505.10689'
source_url: https://arxiv.org/abs/2505.10689
tags:
- quantization
- neural
- dynamic
- performance
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic framework for dynamic quantization
  of neural networks that enables input-adaptive rescaling of quantization parameters
  while maintaining low memory overhead. The method works by estimating the dynamic
  range of pre-activations using a lightweight surrogate model based on statistical
  assumptions about neural network weights, allowing adaptive quantization on a per-input
  basis.
---

# A probabilistic framework for dynamic quantization

## Quick Facts
- **arXiv ID:** 2505.10689
- **Source URL:** https://arxiv.org/abs/2505.10689
- **Reference count:** 40
- **Primary result:** Input-adaptive quantization framework that achieves dynamic quantization performance with static quantization memory overhead

## Executive Summary
This paper presents a probabilistic framework for dynamic quantization that estimates quantization parameters per input without buffering full-precision activations. The method uses a lightweight surrogate model that treats pre-activations as random variables, computing their expected value and variance from weight statistics and input moments. This approach combines the computational efficiency of static quantization with the accuracy benefits of dynamic quantization, achieving performance close to dynamic quantization while maintaining memory requirements similar to static quantization.

## Method Summary
The framework works by first extracting weight statistics (mean and variance) for each layer during offline calibration. At runtime, before executing each layer, a lightweight estimator computes the expected value and variance of pre-activations using input statistics and cached weight statistics. These statistical moments are converted to quantization ranges via calibrated probability intervals, with sampling stride optimization to reduce computational overhead. The method supports both per-tensor and per-channel quantization and is designed for resource-constrained embedded devices.

## Key Results
- Achieves only 0.88% mAP50-95 degradation compared to dynamic quantization on average
- Up to 12.6% improvement over static quantization in per-channel settings
- Maintains constant memory overhead unlike dynamic quantization which requires O(batch×channels×H×W) working memory
- Validated across five computer vision tasks: object detection, segmentation, pose estimation, oriented bounding box prediction, and image classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Input-adaptive quantization parameters can be estimated before layer execution to avoid buffering full-precision outputs
- **Mechanism:** Uses a surrogate model treating pre-activations as random variables, computing expected value and variance using weight and input statistics
- **Core assumption:** Neural network weights are independent and identically distributed (i.i.d.) according to a Normal distribution
- **Evidence anchors:** Section 4.1 derives equations for E[y] and Var[y] based on i.i.d. Normal weight assumption
- **Break condition:** If weights are highly sparse, quantized, or exhibit strong correlations, variance estimation will be inaccurate

### Mechanism 2
- **Claim:** Statistical moments can be converted to quantization ranges via calibrated probability intervals
- **Mechanism:** Computes prediction interval I(α, β) = [μ_y - ασ_y, μ_y + βσ_y] using hyperparameters tuned during calibration
- **Core assumption:** Activation distribution is sufficiently Gaussian-like for standard deviation intervals to provide meaningful coverage
- **Evidence anchors:** Section 4.1 defines probability interval computation for layer pre-activations
- **Break condition:** Highly skewed or multimodal activation distributions may not be optimally captured

### Mechanism 3
- **Claim:** Computational cost of estimation can be decoupled from output spatial size via sub-sampling
- **Mechanism:** Uses sampling stride γ to compute statistics only on subset of output patches, reducing complexity quadratically
- **Core assumption:** Local spatial statistics are representative of global distribution
- **Evidence anchors:** Section 4.2 introduces sampling stride parameter and shows latency dropping quadratically
- **Break condition:** Localized high-magnitude features between sampling strides may be clipped

## Foundational Learning

- **Concept:** **Uniform Affine Quantization**
  - **Why needed here:** The method specifically optimizes scale (s) and zero-point (z) calculation for this scheme
  - **Quick check question:** How does an error in estimating scale (s) affect signal-to-quantization-noise ratio if actual values exceed predicted range?

- **Concept:** **Weight Statistics Propagation**
  - **Why needed here:** Core surrogate model relies on propagating input statistics through layers using weight statistics
  - **Quick check question:** If you double the variance of weights in a linear layer, what happens to predicted variance of pre-activations?

- **Concept:** **Static vs. Dynamic Memory Trade-offs**
  - **Why needed here:** Primary selling point is reducing working memory required by dynamic quantization
  - **Quick check question:** Why does standard dynamic quantization require O(batch×channels×H×W) working memory while this method maintains constant overhead?

## Architecture Onboarding

- **Component map:** Offline Calibration -> Runtime Estimator -> Parameter Resolver -> Quantized Operator
- **Critical path:** Accuracy of Runtime Estimator - if input variance calculated incorrectly, derived scale causes overflow or under-utilization
- **Design tradeoffs:**
  - Sampling Stride (γ): Increasing lowers latency but risks missing outlier activations
  - Calibration Size: Performance saturates quickly (16-32 images), suggesting stable statistical moments
  - Per-Tensor vs. Per-Channel: Per-channel more robust to method's approximations
- **Failure signatures:**
  - Sudden accuracy drop on specific inputs: Likely "heavy-tailed" inputs exceeding 3σ or 4σ interval
  - High latency: Estimation overhead too high; fix by increasing stride γ
  - No improvement over static: Calibration set not representative or weight distributions far from Normal
- **First 3 experiments:**
  1. Plot histograms of trained weights against Gaussian PDF to validate i.i.d. assumption
  2. Implement variance estimation logic and measure latency vs. γ ∈ {1, 4, 8, 16}
  3. Run inference on adversarial inputs and compare estimated vs. actual dynamic range to measure interval predictor "miss rate"

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Core assumption of i.i.d. Normal weights is acknowledged as a "surrogate model" that "does not provably hold" for trained networks
- Lack of specification for how α and β hyperparameters are tuned (global vs. per-layer, optimization vs. percentile matching)
- Fixed-point arithmetic handling for statistical estimation phase not specified, critical for embedded hardware reproduction
- Calibration using only 16 images may not capture full distribution of activation ranges across diverse inputs

## Confidence

- **High Confidence:** Memory overhead claims and latency reduction through sampling stride are directly supported by proposed architecture
- **Medium Confidence:** Performance degradation metrics depend on validity of statistical assumptions across all five CV tasks
- **Low Confidence:** Claims about working "without any prior knowledge of input" overstate case since method still requires weight statistics and calibrated parameters

## Next Checks

1. Generate histograms of actual trained weights from YOLO11 and ResNet50 across multiple layers and overlay with Gaussian PDFs to quantify deviation from i.i.d. Normal assumption

2. Implement variance estimation using same fixed-point arithmetic strategy as target hardware and verify numerical stability across edge cases

3. For representative validation images, compare predicted 3σ intervals against actual observed min/max of pre-activations to calculate empirical coverage rate and identify systematic patterns