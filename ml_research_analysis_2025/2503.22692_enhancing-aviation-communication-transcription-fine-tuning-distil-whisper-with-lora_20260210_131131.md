---
ver: rpa2
title: 'Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper
  with LoRA'
arxiv_id: '2503.22692'
source_url: https://arxiv.org/abs/2503.22692
tags:
- rank
- learning
- lora
- alpha
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA

## Quick Facts
- arXiv ID: 2503.22692
- Source URL: https://arxiv.org/abs/2503.22692
- Authors: Shokoufeh Mirzaei; Jesse Arzate; Yukti Vijay
- Reference count: 23
- Key outcome: None

## Executive Summary
This paper presents a parameter-efficient fine-tuning approach for improving automatic speech recognition (ASR) of aviation communication using Distil-Whisper with Low-Rank Adaptation (LoRA). The method achieves state-of-the-art performance on the ATC0 corpus, reducing word error rate (WER) to 3.86% through systematic hyperparameter optimization across two-stage 5-fold cross-validation. The approach leverages LoRA's efficiency to adapt the model while preserving the base knowledge, making it particularly suitable for resource-constrained aviation environments.

## Method Summary
The study employs a two-stage 5-fold cross-validation approach to fine-tune Distil-Whisper with LoRA for aviation communication transcription. Stage 1 optimizes Distil-Whisper hyperparameters (learning rate, batch size, epochs) with fixed LoRA parameters. Stage 2 tunes LoRA-specific hyperparameters (Alpha and Rank) using the best configuration from Stage 1. The ATC0 corpus (LDC94S14A) provides ~70 hours of US airport communications, preprocessed to 29,091 aligned audio-text pairs with standardized formatting and numeric conversion.

## Key Results
- Achieved average WER of 3.86% across five folds on ATC0 corpus
- Best configuration: learning rate 5e-4, batch size 6, epochs 5, Alpha 256, Rank 256 or 512
- Alpha=512 configurations showed significant degradation in accuracy
- Distil-Whisper with LoRA reduced training parameters while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adaptation Reduces Trainable Parameters While Preserving Base Knowledge
- Claim: LoRA enables efficient domain adaptation by freezing original weights and injecting low-rank matrices
- Mechanism: Weight updates are decomposed into two smaller matrices (A × B) rather than modifying full-rank weights directly
- Core assumption: The necessary adaptations for aviation ASR can be captured in low-dimensional subspaces
- Evidence anchors: [abstract] "Parameter-Efficient Fine-tuning method called Low-Rank Adaptation to fine-tune a more computationally efficient version of Whisper"
- Break condition: If domain shift requires fundamentally different acoustic feature representations, low-rank adaptation may underfit

### Mechanism 2: Distil-Whisper Maintains Accuracy with Reduced Computational Load
- Claim: The distilled variant provides comparable transcription quality to full Whisper while enabling faster training
- Mechanism: Knowledge distillation transfers capabilities from teacher (Whisper) to smaller student model
- Core assumption: The distillation process preserves representations necessary for specialized domains like ATC
- Evidence anchors: [abstract] "distil-Whisper... 5.8 times faster than Whisper, with 49% fewer parameters, while providing comparable WER"
- Break condition: If downstream tasks require full model's capacity, distillation may lose critical representations

### Mechanism 3: Hyperparameter Interactions Govern Convergence and Final WER
- Claim: Learning rate, batch size, epochs, and LoRA Alpha/Rank interact non-linearly
- Mechanism: Higher learning rates accelerate convergence; smaller batches provide more frequent gradient updates
- Core assumption: Observed hyperparameter sensitivities generalize to other ATC datasets
- Evidence anchors: [abstract] "achieving an impressive average word error rate of 3.86% across five folds"
- Break condition: Different noise profiles, accents, or vocabularies may shift optimal hyperparameters

## Foundational Learning

- Concept: Word Error Rate (WER)
  - Why needed here: Primary metric for ASR evaluation; quantifies substitution, deletion, and insertion errors
  - Quick check question: If a reference has 10 words and the output has 2 substitutions + 1 deletion, what is the WER?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: Enables domain adaptation without full-weight updates; critical for resource-constrained environments
  - Quick check question: Why does freezing base model weights reduce catastrophic forgetting risk?

- Concept: K-Fold Cross-Validation
  - Why needed here: Provides robust performance estimates by averaging across multiple train/test splits
  - Quick check question: How does 5-fold CV differ from a single hold-out split in terms of variance of the estimate?

## Architecture Onboarding

- Component map:
  - Input pipeline: Audio segmentation (5-30s), resampling (8kHz → 16kHz), text normalization, numeric-to-word conversion
  - Model: Distil-Whisper encoder-decoder with LoRA adapters injected into transformer layers
  - Training: Cross-entropy loss, Adam-style optimizer, grid search over LR/batch/epochs/Alpha/Rank
  - Evaluation: JiWER for aligned WER calculation

- Critical path:
  1. Curate and segment ATC0 corpus → 29,091 aligned audio-text pairs
  2. Initialize distil-Whisper with LoRA (Alpha=256, Rank=256 or 512)
  3. Run 5-fold CV with LR=5e-4, batch=6, epochs=5
  4. Select best fold; report mean WER ± variance

- Design tradeoffs:
  - Smaller batch sizes improve WER but increase training time
  - Higher Alpha (>256) degrades accuracy despite faster convergence
  - More epochs reduce WER but with diminishing returns and higher compute cost

- Failure signatures:
  - WER >10% with low LR (1e-5): model under-fitting, insufficient gradient signal
  - WER spikes at Alpha=512: LoRA updates overpowering base weights, potential overfitting
  - High variance across folds: dataset heterogeneity or insufficient data per accent/airport

- First 3 experiments:
  1. Replicate baseline: LR=5e-4, batch=6, epochs=5, Alpha=256, Rank=256 on ATC0; confirm mean WER ≈3.86%
  2. Ablate Alpha: Run Alpha={128, 256, 384, 512} with fixed Rank=256; plot WER vs. Alpha to validate sensitivity
  3. Out-of-distribution test: Evaluate best checkpoint on different ATC corpus (e.g., European-accent data) to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Access and Generalization: Core validation relies on restricted ATC0 corpus; performance may not transfer to European ATC or other aviation sub-domains
- Hyperparameter Reporting Inconsistency: Abstract states Alpha=64, Rank=32 for initial LoRA but body reports Alpha=256, Rank=256/512 in Stage 1
- Missing Training Details: Critical training configuration underspecified including optimizer variant, weight decay, and early stopping criteria

## Confidence
- High Confidence: LoRA mechanism for parameter-efficient adaptation is well-established; reported WER (3.86%) is plausible given controlled experimental setup
- Medium Confidence: Distil-Whisper base performance and distillation benefits supported by external literature but domain-specific gains not independently verified
- Low Confidence: Claims of optimal hyperparameter configurations based on single dataset sweep lack broader validation

## Next Checks
1. Out-of-Domain Transfer Test: Evaluate best LoRA checkpoint on different ATC corpus (e.g., European-accent or en-route communications) to quantify domain generalization
2. Ablation of LoRA vs Full Fine-Tuning: Train full fine-tuned Distil-Whisper baseline on ATC0 to isolate actual performance contribution of LoRA
3. Hyperparameter Sensitivity Across Splits: Perform finer-grained hyperparameter sweep on multiple random 80-20 splits of ATC0 to assess stability and variance