---
ver: rpa2
title: Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics
  Bias
arxiv_id: '2502.10577'
source_url: https://arxiv.org/abs/2502.10577
tags:
- responses
- human
- llms
- nouns
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates masculine generic bias in large language
  models (LLMs) by analyzing how often they use masculine forms when responding to
  gender-neutral instructions in French. The authors created a human noun database
  and filtered generic instructions from human-written and AI-generated datasets.
---

# Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias

## Quick Facts
- arXiv ID: 2502.10577
- Source URL: https://arxiv.org/abs/2502.10577
- Reference count: 25
- Primary result: Masculine generic bias in LLMs with 39.5% overall MG usage rising to 73.1% for responses containing human nouns

## Executive Summary
This paper investigates masculine generic (MG) bias in large language models (LLMs) by analyzing how often they use masculine forms when responding to gender-neutral instructions in French. The authors created a comprehensive human noun database and filtered generic instructions from human-written and AI-generated datasets. They then collected responses from six LLMs and found that approximately 39.5% of all responses contained masculine generics, rising to 73.1% for responses containing human nouns. GPT-4o mini showed the highest bias rate at 42.03%, while Llama 3 8B was the least biased at 36.61%. The study also found that LLMs rarely use gender-fair language markers, with neutral words appearing in only 10.7% of responses.

## Method Summary
The authors developed a human noun database and instruction filtering pipeline to identify gender-neutral prompts in French. They used an ensemble HScorer model (LR, XGBoost, CamemBERT) to classify nouns as human/non-human with full agreement voting. After filtering instructions to remove specific masculine contexts, they queried six LLMs via OpenRouter with temperature=1 and validated human nouns in responses using GPT-4o mini with in-context learning. MG detection was performed using a dictionary of 5,140 male-only human nouns, and M Scores were computed as the ratio of masculine generics to total human nouns.

## Key Results
- 39.5% of LLM responses contain masculine generics overall, rising to 73.1% in responses containing human nouns
- GPT-4o mini shows highest bias rate at 42.03%, while Llama 3 8B is least biased at 36.61%
- LLMs rarely use gender-fair language markers, with neutral words appearing in only 10.7% of responses
- 'Profession' and 'doer' nouns are most frequently used as masculine generics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MG bias in LLM outputs reflects statistical patterns in training corpus composition.
- Mechanism: LLMs trained on large-scale text corpora internalize the frequency distribution of masculine generic usage, which then surfaces during generation when human nouns are required in gender-ambiguous contexts.
- Core assumption: Training data contains statistically more MG forms than gender-fair alternatives, which the paper infers but does not directly measure.
- Evidence anchors:
  - [abstract] "Approximately 39.5% of LLM responses contain masculine generics overall, rising to 73.1% in responses containing human nouns."
  - [section 1] "These models inherently reflect the biases to be found in the training data, including those related to gender representation."
  - [corpus] Related work (Zmigrod et al., 2019) suggests counterfactual data augmentation can mitigate such biases, implying training data composition is causal.
- Break condition: If training data were balanced or systematically debiased (via filtering, rewriting, or augmentation as suggested in Section 5), MG rates should decrease.

### Mechanism 2
- Claim: Contextual priming from instruction tokens influences MG probability in autoregressive generation.
- Mechanism: When instructions contain gender cues (including MG nouns), autoregressive models condition subsequent token predictions on this context, propagating the grammatical gender pattern through the response.
- Core assumption: The autoregressive decoding process maintains gender consistency within a generated sequence.
- Evidence anchors:
  - [section 3.2.1] "Since LLMs are autoregressive models, if a MG is used in an instruction, the LLM receiving that instruction has very high chance of making it a part of its answer."
  - [section 3.2.1] The authors explicitly filtered instructions containing MG nouns to avoid this priming effect.
  - [corpus] Weak direct evidence in corpus; mechanism inferred from model architecture properties.
- Break condition: If instructions are systematically filtered for neutral phrasing (as done in this paper), contextual priming is reduced but not eliminated (residual bias from training remains).

### Mechanism 3
- Claim: Semantic category of human nouns (profession, doer, etc.) correlates with MG usage frequency.
- Mechanism: Professional and agentive nouns have stronger historical associations with masculine forms in French corpus data; models amplify this pattern when selecting nouns for generic reference.
- Core assumption: Noun class annotations (NHUMA-based) capture semantically meaningful categories that correlate with gender bias.
- Evidence anchors:
  - [section 4] "'Profession' and 'doer' nouns were the most frequently used as masculine generics."
  - [section 4] Proprietary models (gemini, claude-3-haiku, gpt4o_mini) showed higher use of "profession" class nouns (114-120 unique nouns) compared to local models.
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: Explicit prompting or fine-tuning with occupation-neutral alternatives (e.g., collective nouns) should reduce this category-level bias.

## Foundational Learning

- **Masculine Generics (MG)**: Grammatical feature in gender-marked languages where masculine gender forms serve as purportedly "neutral" defaults for mixed-gender or unknown-gender referents. Psycholinguistic research (Gygax et al., 2008) shows MG are not cognitively neutral.
  - Why needed here: The entire paper's dependent variable is MG usage rate; understanding what MGs are is prerequisite to interpreting results.
  - Quick check question: In French, would "Les médecins" (doctors) trigger male-biased mental representations according to the cited psycholinguistics literature?

- **M Score**: Ratio of MG terms to total human nouns in a text (calculated only when HN count > 0). Can be computed as overall (dataset-wide) or mean (per-text average).
  - Why needed here: This is the paper's primary bias metric; misunderstanding it leads to misinterpreting Figure 3 results.
  - Quick check question: A text with 3 MG nouns and 2 neutral human nouns would have what M Score?

- **Gender-Fair Language Alternatives**: Linguistic strategies including neutral nouns (e.g., "personne"), inclusive pairs (e.g., "il ou elle"), feminine endings (e.g., "auteur·ice"), and neopronouns (e.g., "iel").
  - Why needed here: The paper measures how rarely LLMs use these alternatives (only 10.7% of responses contained neutral terms).
  - Quick check question: Which gender-fair marker type showed zero occurrences across all tested LLMs?

## Architecture Onboarding

- **Component map**: Raw instruction → spaCy NER/filtering → HScorer ensemble → GPT-4o validation → MG detection → M Score computation

- **Critical path**: Raw instruction → NER/filtering → HN detection → GPT-4o validation → MG classification → M Score computation. The GPT-4o validation step is the bottleneck for scalability (API costs, rate limits).

- **Design tradeoffs**:
  - Ensemble HScorer prioritizes precision over recall (only words classified as HN by all 3 models are retained), potentially missing valid human nouns.
  - Filtering removes specific contexts but cannot eliminate all ambiguous cases; polysemic nouns remain challenging.
  - Temperature=1 reflects "real usage" but introduces variability in MG measurements across runs.

- **Failure signatures**:
  - Low agreement between models in HScorer would indicate domain shift in vocabulary.
  - High false positive rate in HN validation would inflate MG scores.
  - Polysemic nouns (e.g., "facteur" = mailman OR factor) require context disambiguation; the LLM validation step may still miss non-human senses.

- **First 3 experiments**:
  1. **Baseline replication**: Run the 10,000 filtered instructions through a new LLM and compute MG rate to establish comparability with paper's 39.5% average.
  2. **Prompting intervention**: Add explicit instructions to use gender-fair language (e.g., "Use neutral terms like 'personne' when gender is unknown") and measure M Score reduction.
  3. **Category-specific analysis**: Isolate "profession" class nouns and test whether targeted replacement with collective nouns (e.g., "personnel soignant" instead of "infirmiers") reduces overall MG rate.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on inference about training data composition rather than direct measurement
- While instruction filtering is comprehensive, it cannot eliminate all contextual priming effects
- The semantic category analysis lacks direct corpus evidence, relying instead on observed patterns in LLM outputs

## Confidence
- **Masculine generic bias prevalence (39.5% overall, 73.1% with human nouns)**: High confidence
- **GPT-4o mini showing highest bias (42.03%)**: Medium confidence
- **LLMs rarely use gender-fair language (10.7% neutral words)**: High confidence
- **Training data composition as primary cause**: Low confidence
- **Semantic category correlation with bias**: Medium confidence

## Next Checks
1. **Direct training data analysis**: Analyze the actual training corpora of tested LLMs (or similar models) to measure masculine generic frequency directly, validating or refuting the inferred training data mechanism.

2. **Cross-linguistic replication**: Apply the same methodology to Spanish or Italian to test whether masculine generic bias patterns hold across gender-marked languages with different grammatical structures.

3. **Prompt engineering intervention**: Test whether explicit instructions to use gender-fair language (e.g., "Use inclusive language: personne, ou, ice") significantly reduces M Scores, distinguishing between training-based bias and prompt-conditional generation patterns.