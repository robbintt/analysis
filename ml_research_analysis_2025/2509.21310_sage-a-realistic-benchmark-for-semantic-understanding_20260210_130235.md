---
ver: rpa2
title: 'SAGE: A Realistic Benchmark for Semantic Understanding'
arxiv_id: '2509.21310'
source_url: https://arxiv.org/abs/2509.21310
tags:
- similarity
- dataset
- semantic
- datasets
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SAGE, a comprehensive benchmark designed
  to evaluate semantic understanding capabilities of both embedding models and classical
  similarity metrics. SAGE assesses five key dimensions: Human Preference Alignment,
  Transformation Robustness, Information Sensitivity, Clustering Performance, and
  Retrieval Robustness, using over 30 datasets and adversarial conditions.'
---

# SAGE: A Realistic Benchmark for Semantic Understanding

## Quick Facts
- arXiv ID: 2509.21310
- Source URL: https://arxiv.org/abs/2509.21310
- Reference count: 40
- No single approach excels across all dimensions of semantic understanding

## Executive Summary
SAGE is a comprehensive benchmark that evaluates semantic understanding capabilities across five key dimensions: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Using over 30 datasets and adversarial conditions, the benchmark reveals significant performance trade-offs between embedding models and classical similarity metrics. While state-of-the-art embedding models excel at human preference alignment and clustering, they show extreme brittleness under perturbations and underperform on information sensitivity tasks compared to classical metrics.

## Method Summary
SAGE evaluates five embedding models and four classical metrics across five dimensions using standardized perturbations and datasets. The benchmark applies six transformations (three superficial, three semantic) and two perturbation types (needle insertion, span removal) to generate adversarial test conditions. Each dimension uses specific metrics: Pearson correlation for human preference, ordinal ranking for robustness, MAE vs theoretical decay for sensitivity, V-measure for clustering, and harmonic mean of NDCG@10 retention for retrieval. The final SAGE score is an unweighted average of all five dimensions.

## Key Results
- Embedding models excel at clustering (text-embedding-3-large: 0.443) and human preference alignment (0.682) but show extreme brittleness (0.011 robustness)
- Classical metrics outperform embeddings on information sensitivity (Jaccard: 0.905 vs embedding max: 0.794)
- text-embedding-3-small achieves highest clustering performance (0.483) but demonstrates extreme brittleness with lowest robustness score (0.011)
- No single approach excels across all dimensions of semantic understanding

## Why This Works (Mechanism)

### Mechanism 1
Multi-dimensional stress-testing reveals capability trade-offs invisible to single-task benchmarks. SAGE evaluates five semantically distinct tasks and computes unweighted averages. When a model optimizes for one task (e.g., clustering), it may catastrophically fail on another (e.g., robustness), exposing internal objective conflicts. Real-world semantic understanding requires simultaneously satisfying multiple constraints; weakness in any dimension can cause deployment failure.

### Mechanism 2
Ordinal similarity ranking under perturbation isolates robustness from semantic understanding. Apply three perturbation types (superficial, semantic, summary) and verify a robust metric maintains: similarity(original→superficial) > similarity(original→summary) > similarity(original→semantic). Violations indicate the metric conflates surface patterns with meaning. Surface noise should preserve similarity more than summarization, which should preserve more than semantic negation/factual change.

### Mechanism 3
Information sensitivity is better captured by character/token overlap than learned embeddings. Insert irrelevant content or remove spans at varying proportions and measure whether similarity degrades monotonically. Classical metrics (Jaccard) directly measure token overlap change; embeddings compress information nonlinearly, causing calibration errors. Semantic degradation should produce predictable, approximately linear similarity decreases following theoretical_curve = 1 - p/(1+p).

## Foundational Learning

- **Perturbation taxonomy (superficial vs. semantic)**: Why needed here - Transformation robustness depends on distinguishing meaning-preserving noise from meaning-altering changes. Quick check - Does "every 10th character deleted" preserve semantic intent? What about "sentence shuffling"?

- **Harmonic mean for retention aggregation**: Why needed here - Retrieval robustness aggregates NDCG@10 retention ratios across 18 perturbations. Harmonic mean penalizes any single catastrophic failure more than arithmetic mean, matching production requirements. Quick check - If a model retains 0.9 on 17 perturbations but 0.1 on one, what's the harmonic vs. arithmetic mean?

- **V-measure for clustering evaluation**: Why needed here - Clustering performance uses V-measure (harmonic mean of homogeneity and completeness), which requires understanding entropy-based cluster quality, not just accuracy. Quick check - If all documents from one class split across two clusters, which component of V-measure drops - homogeneity or completeness?

## Architecture Onboarding

- **Component map**: Human Preference Alignment -> OpenAI feedback dataset -> correlation + pairwise accuracy; Transformation Robustness -> 3 document corpora × 6 perturbations -> ordinal ranking check; Information Sensitivity -> 6 datasets × insertion/removal perturbations -> MAE vs. theoretical degradation; Clustering Performance -> 11 MTEB datasets -> agglomerative clustering + V-measure; Retrieval Robustness -> BEIR datasets × 18 perturbations -> NDCG@10 retention harmonic mean

- **Critical path**: 1) Generate embeddings/similarity scores for all datasets; 2) Apply perturbations systematically (reproducible random seeds); 3) Compute task-specific metrics per dimension; 4) Normalize to [0,1] scale; 5) Aggregate via unweighted average for overall SAGE score

- **Design tradeoffs**: Classical metrics - High sensitivity, low semantic clustering (Jaccard: 0.905 sensitivity, 0.191 clustering); Embeddings - High preference alignment, potential brittleness (text-embedding-3-large: 0.682 preference, 0.243 robustness); Dimensionality - Higher dimensions (3072 vs 1536) improve headroom but double storage/compute

- **Failure signatures**: High clustering + low robustness → text-embedding-3-small pattern (0.483 / 0.011); High sensitivity + low clustering → classical metric pattern; Robustness < 0.1 indicates character-level noise breaks ordinal ranking

- **First 3 experiments**: 1) Run text-embedding-3-large and Jaccard on a single dataset across all five dimensions; verify your pipeline reproduces Table 1 scores within ±0.02; 2) Isolate transformation robustness: apply only the six perturbations to 100 documents, manually inspect whether ordinal ranking holds for each; 3) Stress-test retrieval: take one BEIR dataset, apply only needle insertion at 50% size, measure NDCG@10 retention ratio before/after

## Open Questions the Paper Calls Out

### Open Question 1
What architectural or training modifications could enable embedding models to achieve both high clustering performance and high transformation robustness, rather than exhibiting the extreme trade-off observed in text-embedding-3-small (0.483 clustering vs. 0.011 robustness)? Authors state "no single approach excels across all dimensions" and note the critical trade-off but do not investigate its root causes or propose solutions.

### Open Question 2
Why do classical metrics (Jaccard: 0.905) substantially outperform state-of-the-art embedding models (best: 0.794) on information sensitivity tasks, and can this gap be closed through modified training objectives? Authors highlight this performance gap but do not investigate whether it stems from training objectives, representation limitations, or fundamental differences in how embeddings encode semantic degradation.

### Open Question 3
Do the six synthetic perturbations used in SAGE accurately represent the distribution and severity of naturally occurring text corruptions? The paper applies synthetically designed perturbations without validating ecological validity against corpora with authentic, naturally occurring text corruption.

### Open Question 4
Which specific "defensive architectures, ensemble methods, and appropriate safeguards" would most effectively mitigate the observed brittleness of embedding models in high-noise production environments? Authors conclude practitioners should deploy accordingly but provide no empirical investigation of which strategies work best.

## Limitations
- Evaluation of transformation robustness depends critically on correctly classifying perturbations as superficial versus semantic, with potential edge case misclassification
- The claim that embedding models are "brittle" at the character level assumes that robustness < 0.1 is universally problematic without deployment context validation
- Information sensitivity evaluation relies on a theoretical decay curve that may not capture nonlinear perceptual effects in human semantic judgment

## Confidence

- **High confidence**: Multi-dimensional benchmark design exposing trade-offs between embedding models and classical metrics; statistical aggregation methodology using unweighted averages and harmonic means
- **Medium confidence**: The five-task decomposition of semantic understanding as comprehensive; the perturbation taxonomy's ability to isolate robustness from semantic understanding
- **Low confidence**: The universal applicability of the < 0.1 robustness threshold; the theoretical decay curve's validity for perceptual similarity

## Next Checks

1. **Transformation Robustness Edge Case Audit**: Manually review 50 randomly sampled perturbation instances where the robustness score dropped below 0.1 to verify correct classification between superficial and semantic alterations

2. **Perceptual Validation of Sensitivity Curve**: Conduct a human evaluation study (n=100) comparing perceived similarity degradation against the theoretical (1-p)/(1+p) curve for needle insertion and span removal tasks

3. **Deployment Context Calibration**: Test the same embedding models on a production semantic search task under realistic noise conditions to determine if the observed brittleness translates to measurable performance degradation in actual use