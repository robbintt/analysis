---
ver: rpa2
title: Whose Facts Win? LLM Source Preferences under Knowledge Conflicts
arxiv_id: '2601.03746'
source_url: https://arxiv.org/abs/2601.03746
tags:
- source
- credibility
- preferences
- media
- government
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show consistent preferences for institutional
  sources over individuals when resolving knowledge conflicts, following a credibility
  hierarchy of government newspaper person social media. However, these preferences
  can be reversed by repeating information from less credible sources, demonstrating
  a vulnerability to disinformation.
---

# Whose Facts Win? LLM Source Preferences under Knowledge Conflicts

## Quick Facts
- arXiv ID: 2601.03746
- Source URL: https://arxiv.org/abs/2601.03746
- Reference count: 40
- Large language models show consistent institutional source preferences (government > newspaper > person > social media) that can be reversed by repetition, with a fine-tuning method achieving up to 99.8% reduction in repetition bias.

## Executive Summary
Large language models exhibit stable preferences for institutional sources over individuals when resolving knowledge conflicts, following a credibility hierarchy of government > newspaper > person > social media. However, these preferences can be easily reversed through repetition of information from less credible sources, revealing a vulnerability to disinformation. The authors demonstrate that a fine-tuning method using paired contrastive loss preserves original source preferences under repetition while reducing repetition bias by up to 99.8%, outperforming simple credibility prompting approaches.

## Method Summary
The study uses synthetic data from the NeoQA dataset, generating 7,440 conflict pairs through counterfactual perturbations of fictional entities. Four synthetic source types (government, newspaper, person, social media) are created with specific templates. The evaluation measures source preference using a metric that compares model probabilities between attributed and unattributed contexts. For mitigation, a teacher-student distillation approach with KL divergence loss is applied to GEMMA-3-4B using LoRA fine-tuning on paired (repeated, non-repeated) contexts.

## Key Results
- 11 of 13 tested models prefer institutional sources over individual sources with high inter-model agreement (Kendall's W of 0.74)
- Repetition setting reverses source preferences in all models except LLAMA-3.1-70B, with average dSP gap of 30.04
- Fine-tuning method reduces repetition bias by up to 99.8% while preserving at least 88.8% of original credibility-based preferences
- Mitigation outperforms simple credibility prompting, which only partially reduces repetition effects

## Why This Works (Mechanism)

### Mechanism 1: Source Credibility Hierarchy from Training Distribution
LLMs encode implicit credibility rankings that emerge from statistical patterns in pre-training corpora. Models learn that institutional sources co-occur more frequently with accurate information than individual or social media sources. During conflict resolution, token probability shifts favor information attributed to higher-ranked sources. This hierarchy would weaken if training data contained adversarial examples where social media sources were more accurate than institutional ones.

### Mechanism 2: Repetition Bias as Token-Level Frequency Heuristic
Repetition overrides learned credibility preferences through a frequency-based heuristic independent of source quality. When identical tokens appear multiple times in context, models assign higher probability to associated answers, operating as a shortcut heuristic that bypasses source evaluation. If attention mechanisms were explicitly trained to downweight duplicate passages, this repetition effect would diminish.

### Mechanism 3: Knowledge Distillation with Paired Contrastive Loss
Fine-tuning with paired (repeated, non-repeated) contexts and KL divergence loss teaches models to maintain consistent preferences. The student model is penalized when its output distribution differs from the teacher on repeated inputs, using a dual loss term that simultaneously preserves original source preferences and enforces repetition-invariance. If the teacher model already exhibits strong repetition bias, distillation would amplify rather than correct it.

## Foundational Learning

- **Concept: Inter-context knowledge conflicts** - Why needed: The entire experimental framework depends on presenting models with contradictory information from different sources and measuring which "wins". Quick check: Can you explain the difference between parametric vs. contextual knowledge conflicts?

- **Concept: Source credibility dimensions (presumed vs. reputed)** - Why needed: The paper draws on credibility theory to design experiments testing both general assumptions (government > social media) and feature-based judgments (high vs. low circulation). Quick check: How would you design an experiment to distinguish between presumed and reputed credibility effects?

- **Concept: KL divergence for distribution matching** - Why needed: The mitigation method uses KL divergence to enforce that the fine-tuned model produces similar output distributions to the original model. Quick check: Why is KL divergence asymmetric, and why does that matter for knowledge distillation?

## Architecture Onboarding

- **Component map:** NeoQA entity generator -> Counterfactual perturbation module -> Source attribution layer -> Conflict pair constructor -> Model evaluation harness (probability extraction) -> dSP metric calculator -> Fine-tuning pipeline (paired loader + dual KL loss)

- **Critical path:** 1. Synthetic data generation (entities, sources, conflicts) 2. Probability extraction from token logits (A vs. B) 3. dSP computation comparing attributed vs. unattributed conditions 4. Paired fine-tuning with (C′U, C′R) batches

- **Design tradeoffs:** Synthetic setting enables tight control but may not transfer to real RAG noise; forced-choice evaluation isolates preferences but excludes "abstain" option; LoRA fine-tuning preserves base model but limits adaptation capacity

- **Failure signatures:** Models with extreme position bias may show unreliable dSP (check baseline preferences); source template contamination occurs if location overlap between train/test; Lambda=0.75 not extensively tuned; may need adjustment per model family

- **First 3 experiments:** 1. Replicate government vs. social media dSP result on your target model to establish baseline hierarchy 2. Test repetition bias with your model's probability extraction—confirm flip occurs at 2+ repetitions 3. Run fine-tuning pipeline on small model (≤4B params) and verify dSP-gap reduction on held-out test set before scaling

## Open Questions the Paper Calls Out

- **Open Question 1:** Do LLM source credibility hierarchies remain consistent across different languages and cultural contexts? The authors note they only experiment with English language data using U.S. preferences and acknowledge that other languages may evoke different credibility behavior. This could be resolved by applying the framework to multilingual datasets with source types tailored to non-Western cultural norms.

- **Open Question 2:** How do source preferences interact with message style and topic complexity in real-world scenarios? The authors use tabular data to isolate source effects but leave investigation of how other aspects of content interact with source credibility to future work. This could be addressed by testing the framework on unstructured, natural text data where style is not controlled.

- **Open Question 3:** Does the repetition bias and source preference hierarchy persist in open-ended generation or chain-of-thought reasoning settings? The methodology relies exclusively on forced-choice multiple-choice QA without step-by-step reasoning or generative answers. This could be investigated by replicating the repetition experiments using generative QA and chain-of-thought prompting.

## Limitations

- Findings rely on synthetic data that may not capture real-world retrieval noise or the messy diversity of authentic documents
- Source attribution templates are artificial constructs that differ from actual news or government sites in style and tone
- Study focuses on inter-context conflicts while models in production often face parametric or RAG-context conflicts
- Mitigation method requires labeled attribution pairs and has only been tested on GEMMA-3-4B, leaving scaling and robustness questions open

## Confidence

**High Confidence**: Models exhibit stable credibility hierarchy (government > newspaper > person > social media) in unattributed conflicts; repetition reliably flips preferences across most models; KL-divergence fine-tuning effectively reduces repetition bias.

**Medium Confidence**: The mechanism that repetition bias stems from frequency-based heuristic is plausible but not definitively isolated; 88.8% preservation of original credibility preferences post-mitigation is based on limited hyperparameters.

**Low Confidence**: That learned hierarchy mirrors real-world credibility distributions; that mitigation would generalize to other model families or noisy RAG inputs; that observed preference stability translates to improved downstream performance.

## Next Checks

1. **Replication with Real-World Documents**: Replace synthetic sources with authentic news articles, government reports, and social media posts to test whether credibility hierarchy and repetition effects persist in natural text.

2. **Generalization Across Architectures**: Apply paired contrastive fine-tuning method to diverse model families (e.g., Mistral, Mixtral) and parameter sizes to verify robustness and scalability.

3. **Downstream Impact Assessment**: Measure whether reduced repetition bias in source preference translates to better accuracy or reliability in realistic retrieval-augmented task (e.g., multi-document QA or fact-checking).