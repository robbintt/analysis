---
ver: rpa2
title: 'NNTile: a machine learning framework capable of training extremely large GPT
  language models on a single node'
arxiv_id: '2504.13236'
source_url: https://arxiv.org/abs/2504.13236
tags:
- training
- nntile
- layer
- gpus
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NNTile enables training GPT2 models up to 49.9 billion parameters\
  \ on a single node with 8 A100 GPUs by using task-based parallelism and automatic\
  \ CPU-GPU offloading. This is more than 4\xD7 larger than PyTorch FSDP, which is\
  \ limited to 10.6 billion parameters under the same hardware and precision."
---

# NNTile: a machine learning framework capable of training extremely large GPT language models on a single node

## Quick Facts
- arXiv ID: 2504.13236
- Source URL: https://arxiv.org/abs/2504.13236
- Authors: Aleksandr Mikhalev; Aleksandr Katrutsa; Konstantin Sozykin; Ivan Oseledets
- Reference count: 22
- Primary result: Trains GPT2 models up to 49.9B parameters on a single node with 8 A100 GPUs, exceeding PyTorch FSDP's 10.6B parameter limit through task-based parallelism and automatic CPU-GPU offloading

## Executive Summary
NNTile is a machine learning framework that enables training extremely large GPT language models on a single node by leveraging task-based parallelism and automatic CPU-GPU offloading. The framework uses StarPU runtime to dynamically schedule computational tasks across heterogeneous resources, allowing models to exceed GPU VRAM limits by transparently offloading data to CPU RAM. By decomposing neural network operations into tile-based computations, NNTile achieves more than 4× larger model capacity compared to PyTorch FSDP while maintaining comparable training speeds for smaller models.

## Method Summary
NNTile implements a task-based parallel framework using StarPU runtime to manage heterogeneous computation across CPUs and GPUs. The approach decomposes tensors into tiles and operations into fine-grained tasks, building a DAG that the scheduler dynamically maps to available resources. A memory manager handles automatic data movement between CPU RAM and GPU VRAM, treating CPU memory as an overflow pool. The framework reimplements key neural network operations (embedding, attention, normalization, SoftMax, loss, optimizers) in tile-based form, enabling fine-grained parallelism necessary for efficient resource utilization across heterogeneous hardware.

## Key Results
- Trains GPT2 models up to 49.9 billion parameters on 8×A100 80GB GPUs, compared to PyTorch FSDP's 10.6 billion parameter limit
- Achieves training speeds comparable to PyTorch FSDP for smaller models (4-layer architecture)
- Demonstrates automatic CPU offloading capability, using up to 900GB CPU RAM to extend model size beyond GPU VRAM capacity (640GB)
- Shows performance degradation for deeper networks (8 layers) due to suboptimal greedy scheduling decisions

## Why This Works (Mechanism)

### Mechanism 1: Task-Based Parallelism via Runtime Scheduling
Decoupling the computational graph from hardware execution allows dynamic mapping of operations onto heterogeneous resources, potentially maximizing resource utilization where static methods fail. Operations are decomposed into fine-grained "tasks" operating on data chunks ("tiles"), with StarPU building a DAG and scheduling these tasks dynamically based on data availability and device readiness.

### Mechanism 2: Transparent CPU Offloading via Unified Memory Management
Treating CPU RAM as a primary memory pool for large models allows exceeding GPU VRAM limits, provided data movement is asynchronous and hidden by computation. The memory manager retains ownership of data tiles, asynchronously prefetching them to GPUs only when needed and evicting them to CPU RAM immediately after use.

### Mechanism 3: Tile-Based Linear Algebra
Decomposing tensors into blocks (tiles) enables fine-grained parallelism necessary for the scheduler to load-balance across diverse compute units. Neural network layers are reimplemented to operate on tiles rather than monolithic tensors, allowing the scheduler to place small chunks of large matrix multiplications on different devices simultaneously.

## Foundational Learning

- **Concept: StarPU Runtime & DAG Scheduling**
  - Why needed here: NNTile is essentially a wrapper around StarPU; understanding how StarPU submits tasks and executes them is required to debug performance stalls
  - Quick check question: If a task depends on a tile currently evicted to CPU RAM, which component is responsible for triggering the data transfer before execution?

- **Concept: Arithmetic Intensity**
  - Why needed here: The paper notes that neural networks have lower arithmetic intensity than standard linear algebra, determining whether the scheduler should favor high-throughput GPUs or latency-hiding strategies
  - Quick check question: Why does low arithmetic intensity make a system more sensitive to memory bandwidth and data transfer overhead?

- **Concept: Tensor Parallelism vs. Data Parallelism**
  - Why needed here: NNTile splits data across embedding dimensions to fit tiles into limited memory, rather than just replicating or sharding whole parameters
  - Quick check question: In NNTile, if a large weight matrix is split into tiles, how does the scheduler handle the gradient accumulation for a specific tile that might reside on different devices?

## Architecture Onboarding

- **Component map:** NNTile (Python/C++ layer defining layers/optimizers) -> StarPU (Core engine managing Task Graph, Memory Manager, and Scheduler) -> Custom CUDA/CPU kernels for tile operations

- **Critical path:** 1) User defines model in NNTile 2) NNTile breaks tensors into Tiles and submits Tasks (Sequential Task Flow) 3) StarPU builds DAG of dependencies 4) Scheduler picks ready tasks and assigns them to workers 5) Memory Manager ensures required tiles are prefetched 6) Kernel executes, data marked valid/invalid, dependent tasks unlocked

- **Design tradeoffs:** Scheduler Selection (greedy heuristics are suboptimal for deep networks), Tile Size (smaller tiles increase parallelism but decrease arithmetic intensity), CPU vs GPU (CPUs used primarily for memory overflow and auxiliary tasks)

- **Failure signatures:** OOM (combined Working Set exceeds CPU + GPU RAM), Slow Convergence/Stalling (greedy scheduler making poor decisions for deep dependency chains), Reduction Bottlenecks (gradient reduction via STARPU_REDUX caused scaling issues)

- **First 3 experiments:** 1) Baseline Profiling: Run single layer with varying tile sizes to visualize trade-off between kernel performance and scheduling overhead 2) Memory Stress Test: Train small model with artificially limited GPU memory to force CPU offloading 3) Scheduler Comparison: Reproduce 4-layer vs 8-layer experiment using dmdasd policy to observe performance degradation

## Open Questions the Paper Calls Out

- **Can advanced scheduling policies mitigate performance degradation in deeper networks?**
  - Basis in paper: Section 4 states that 8-layer model suffers because dmdasd makes "far from optimal decisions" as network depth increases
  - Why unresolved: Paper identifies greedy scheduler as bottleneck but doesn't implement or test alternative scheduling strategies
  - What evidence would resolve it: Benchmarks showing stable or improved throughput for >8 layer models with non-greedy schedulers

- **Can standard deep transformer architectures be efficiently trained within NNTile?**
  - Basis in paper: Section 3 notes that "training standard pretrained GPT2 models did not scale" and gradient reduction made things worse
  - Why unresolved: Authors circumvented this by using custom models with few blocks but large embedding sizes
  - What evidence would resolve it: Successful training runs of standard deep architectures (e.g., GPT-2 Large) without custom shallow configurations

- **Does task-based efficiency hold when scaling to distributed heterogeneous clusters?**
  - Basis in paper: Abstract defines scope as "heterogeneous clusters" but all experiments limited to single node
  - Why unresolved: Inter-node communication overhead versus intra-node CPU-GPU offloading remains unquantified
  - What evidence would resolve it: Multi-node experiments demonstrating NNTile maintains memory advantage and competitive throughput

## Limitations

- Performance degradation occurs for deeper networks (8+ layers) due to suboptimal greedy scheduling decisions
- Evaluation scope limited to single model architecture without exploring generalization to other neural network types
- Hardware dependency on specific configuration (8×A100 80GB) without exploring scalability to different setups

## Confidence

**High Confidence Claims:**
- NNTile can train models larger than PyTorch FSDP on same hardware (49.9B vs 10.6B parameters)
- Framework successfully implements automatic CPU-GPU offloading for memory overflow
- Task-based parallelism with StarPU enables dynamic scheduling across heterogeneous resources

**Medium Confidence Claims:**
- Training speeds are comparable to PyTorch FSDP for smaller models
- Performance degradation occurs for deeper networks due to scheduler decisions
- Tile-based decomposition enables observed scaling benefits

**Low Confidence Claims:**
- Specific performance characteristics across different model depths and sizes
- Generalizability of approach to other model architectures
- Scalability to different hardware configurations

## Next Checks

1. **Component Isolation Test**: Reproduce performance of individual components (e.g., single matrix multiplication with different tile sizes) to quantify overhead of task-based scheduling versus traditional approaches, separating scheduler overhead from memory management benefits

2. **Scheduler Alternative Evaluation**: Implement or test alternative scheduling policies beyond greedy dmdasd approach to determine if performance degradation for deeper networks is fundamentally a scheduling problem or indicative of deeper architectural limitations

3. **Cross-Architecture Generalization**: Evaluate NNTile on alternative transformer architectures (e.g., BERT, ViT) or non-transformer models to validate whether claimed benefits extend beyond specific GPT2 configuration tested, providing evidence for framework's broader applicability