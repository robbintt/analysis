---
ver: rpa2
title: 'SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities'
arxiv_id: '2504.04596'
source_url: https://arxiv.org/abs/2504.04596
tags:
- financial
- question
- other
- secque
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces SECQUE, a benchmark designed to evaluate
  large language models on real-world financial analysis tasks using SEC filings.
  The benchmark includes 565 expert-written questions across four key categories:
  comparison analysis, ratio calculation, risk assessment, and financial insight generation.'
---

# SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities

## Quick Facts
- arXiv ID: 2504.04596
- Source URL: https://arxiv.org/abs/2504.04596
- Reference count: 40
- Models tested: 7 large language models on financial analysis tasks using SEC filings

## Executive Summary
SECQUE is a benchmark dataset of 565 expert-written questions designed to evaluate large language models on real-world financial analysis tasks using SEC filings. The benchmark covers four key categories: comparison analysis, ratio calculation, risk assessment, and financial insight generation. To address the challenge of evaluating open-ended financial responses, the authors developed SECQUE-Judge, an automated evaluation mechanism using multiple LLM-based judges that shows strong alignment with human evaluations. Testing revealed that while leading models like GPT-4o demonstrate promising capabilities, significant challenges remain particularly in complex reasoning tasks and analyst insights generation.

## Method Summary
SECQUE consists of 565 financial analysis questions across four categories, each paired with context from SEC filings (10-K/10-Q documents). The evaluation uses SECQUE-Judge, which invokes a Single-judge LLM (GPT-4o) five times per answer with temperature=0.3, sums the scores, and applies thresholds (UT=6, LT=4) to produce final categorical scores. The benchmark tests models using four context variants (HTML/Markdown tables, with/without headers) and evaluates performance using Strict Accuracy (only score=2 counts) and Normalized Accuracy (scores 0-2 weighted). The dataset and evaluation framework are publicly available for reproducibility.

## Key Results
- GPT-4o achieved the highest performance across all question categories, but even leading models struggled with Analyst Insights questions
- HTML table representation with headers consistently outperformed Markdown and no-header variants across all models
- The baseline prompt significantly outperformed the financial-specific prompt, with accuracy drops of 9-26% across models
- Smaller models like Phi-4 were particularly sensitive to context representation, gaining boosts from token reduction in Markdown format

## Why This Works (Mechanism)

### Mechanism 1
Multi-judge threshold aggregation improves alignment with human evaluation by reducing stochastic variance in LLM judgments. Five independent Single-judge invocations produce scores that are summed and mapped via thresholds, where a single score of 2 or 0 among five judges can determine the final categorical score.

### Mechanism 2
Context representation format creates measurable performance differences due to structural clarity and token efficiency. HTML tables provide explicit structure via tags with higher token counts (5.4K mean), while Markdown is more concise (2.9K mean) but less expressive, affecting smaller models' interpretation capabilities.

### Mechanism 3
Question category determines difficulty based on required capabilities, with Analyst Insights being hardest (requiring synthesis of all capabilities plus domain knowledge) and Risk Factors being easiest (requiring basic text analysis). This hierarchy reflects genuine task complexity rather than benchmark artifacts.

## Foundational Learning

- **SEC Filing Structure (10-K vs. 10-Q)**
  - Why needed here: All benchmark questions derive from these documents; understanding sections (Risk Factors, Item 15, Financial Statements) is prerequisite to evaluating model responses.
  - Quick check question: Can you identify where Operating Income appears in a 10-K and distinguish it from Net Income?

- **LLM-as-Judge Paradigm**
  - Why needed here: The entire evaluation system depends on using LLMs to score open-ended responses; understanding scoring calibration is essential for interpreting results.
  - Quick check question: Why would aggregated judge scores outperform single evaluations for consistency?

- **Financial Ratio Computation**
  - Why needed here: 188/565 questions (33%) require ratio calculation; must understand formulas (e.g., Interest Coverage = EBIT/Interest Expense) to validate model outputs.
  - Quick check question: Given EBIT of $32,972M and Interest Expense of $257M, what is the Interest Coverage Ratio?

## Architecture Onboarding

- **Component map:** SECQUE dataset (565 questions × 4 context variants = 2,260 instances) → SECQUE-Judge (5× Single-judge calls → Sum aggregation → Threshold mapping UT=6, LT=4) → Scoring outputs (Strict Accuracy vs. Normalized Accuracy)

- **Critical path:** Load question + context variant → Generate model response → Invoke Single-judge 5× with GPT-4o → Sum scores → Apply thresholds → Final categorical score → Aggregate to Strict/Normalized accuracy per question type

- **Design tradeoffs:** HTML (more tokens, clearer structure) vs. Markdown (fewer tokens, less explicit); Baseline prompt (highest performance) vs. Financial prompt (lower token usage, worse performance); Strict accuracy (penalizes partial correctness) vs. Normalized (gives partial credit)

- **Failure signatures:** Context exceeding model window (Phi-4: 16K limit → unanswered questions); Judge disagreement (4/124 cases had full 0-2 score range disagreement); Prompt sensitivity (Financial prompt caused 9-26% accuracy drop across models)

- **First 3 experiments:** 1) Establish baseline with GPT-4o using HTML+headers, baseline prompt, temperature=0.3 across all 4 question categories. 2) Ablate context representation: run same model on Markdown vs. HTML, measure Strict Accuracy delta per category. 3) Validate judge alignment: sample 30 questions, compare SECQUE-Judge scores against human expert scores, compute F1(2) and confusion matrix.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can evaluation frameworks effectively accommodate the financial domain's inherent ambiguity where multiple valid calculation methods or interpretations may exist for a single query? The current SECQUE-Judge relies on single ground truth answers, risking penalizing valid alternative analytical approaches used by financial professionals.

- **Open Question 2:** Does high performance on SEC filing analysis (10-K/10-Q) correlate with competence in analyzing other financial document types, such as earnings call transcripts or macro-economic reports? The benchmark is restricted to specific SEC filings, leaving models' ability to generalize to broader financial corpus unknown.

- **Open Question 3:** What underlying mechanisms cause specialized "financial" prompts to degrade model performance compared to generic baseline prompts in financial reasoning tasks? The paper documents the performance drop but does not analyze why domain-specific instructions hindered the models.

- **Open Question 4:** How does model performance on SECQUE degrade when shifting from the provided "oracle" context to a realistic Retrieval-Augmented Generation (RAG) setup? The methodology provides exact text chunks needed to answer questions, isolating reasoning capability from retrieval capability despite real-world tasks requiring information extraction from lengthy documents.

## Limitations
- Limited validation sample size (30 questions across three human evaluators) may not capture full spectrum of evaluation challenges
- Threshold-based aggregation approach lacks theoretical grounding for optimal threshold selection
- Focus on SEC filings may not generalize to other financial document types or real-world analyst workflows

## Confidence
- **High Confidence:** Benchmark construction methodology, question categorization framework, and basic performance rankings across models
- **Medium Confidence:** SECQUE-Judge evaluation mechanism's superiority over alternative methods, pending larger-scale human validation
- **Low Confidence:** Specific performance differences between HTML and Markdown representations, potentially confounded by token count differences

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary UT and LT thresholds and measure impact on human-judge alignment metrics to determine if current parameters are optimal
2. **Cross-Domain Generalization:** Apply SECQUE-Judge to evaluate responses on non-SEC financial documents to test whether evaluation mechanism maintains alignment without SEC-specific tuning
3. **Prompt Sensitivity Validation:** Conduct ablation studies on Single-judge prompt structure itself to identify whether performance differences stem from prompt sensitivity rather than model capabilities