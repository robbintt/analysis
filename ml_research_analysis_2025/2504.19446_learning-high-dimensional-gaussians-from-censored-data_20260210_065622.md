---
ver: rpa2
title: Learning High-dimensional Gaussians from Censored Data
arxiv_id: '2504.19446'
source_url: https://arxiv.org/abs/2504.19446
tags:
- algorithm
- data
- samples
- missingness
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses distribution learning from high-dimensional
  Gaussian data with missing values under the missing-not-at-random (MNAR) assumption.
  It proposes algorithms for two settings: (1) self-censoring, where missingness depends
  on the value of each coordinate individually, and (2) linear thresholding, where
  missingness depends on the sample as a whole via linear inequalities.'
---

# Learning High-dimensional Gaussians from Censored Data

## Quick Facts
- **arXiv ID:** 2504.19446
- **Source URL:** https://arxiv.org/abs/2504.19446
- **Reference count:** 40
- **Primary result:** Polynomial-time algorithms for learning high-dimensional Gaussians from censored data under MNAR assumptions

## Executive Summary
This paper addresses the challenging problem of learning Gaussian distributions when data is missing not at random (MNAR), where the missingness mechanism depends on the underlying values. The authors develop algorithms for two distinct censoring scenarios: self-censoring (where each coordinate's missingness depends on its own value) and linear thresholding (where missingness depends on linear combinations of the full sample). By leveraging truncation techniques and projected stochastic gradient descent, the algorithms can recover the true mean and covariance parameters efficiently under certain assumptions about the missingness mechanism.

## Method Summary
The approach combines two main techniques: (1) truncation methods that handle censored data by leveraging known bounds on the censoring mechanism, and (2) projected stochastic gradient descent to optimize the parameters while respecting the constraints imposed by the censoring. For self-censoring, the algorithm uses a recursive approach that alternates between estimating the mean and covariance, with each step involving truncated expectations. For linear thresholding, the method reformulates the problem as estimating parameters under linear inequality constraints and uses gradient descent with projection onto the feasible set. Both methods carefully account for the bias introduced by the censoring mechanism and correct for it through iterative refinement.

## Key Results
- Achieves epsilon accuracy in total variation distance for self-censoring with sample complexity tilde O(d²(λ_max/λ_min)²/(αε²))
- For linear thresholding, achieves epsilon accuracy in Mahalanobis norm with sample complexity poly(d, 1/α, 1/β, 1/γ, λ_max/λ_min, 1/ε, log(1/δ))
- Both algorithms run in polynomial time under the stated assumptions
- Recovery is possible even when censoring rates approach 50% (α, β, γ > 0)

## Why This Works (Mechanism)
The algorithms work by exploiting the structure of the censoring mechanism to create unbiased estimators despite the missing data. For self-censoring, the key insight is that when missingness depends only on individual coordinates, the censored samples still contain sufficient information about the correlation structure. The truncation approach effectively "reweights" the observed data to correct for selection bias. For linear thresholding, the mechanism is more complex, but the authors show that by carefully parameterizing the problem and using constrained optimization, they can still recover the underlying distribution parameters. The projected gradient descent ensures that the estimates remain in the feasible region defined by the censoring constraints.

## Foundational Learning

**Gaussian distribution theory** - why needed: Understanding multivariate Gaussian properties is essential for analyzing the statistical guarantees of the recovery algorithms
Quick check: Verify the relationship between mean/covariance and total variation/Mahalanobis distance

**Stochastic optimization** - why needed: Projected SGD is the core computational tool for parameter estimation under constraints
Quick check: Confirm convergence rates for projected SGD with bounded gradients

**Censored/truncated data analysis** - why needed: The missingness mechanisms require specialized techniques beyond standard EM algorithms
Quick check: Validate that truncation corrections properly unbias the estimators

**High-dimensional probability** - why needed: Concentration bounds for eigenvalue ratios and operator norms are crucial for sample complexity analysis
Quick check: Verify the concentration inequalities used for λ_max/λ_min ratios

## Architecture Onboarding

**Component map:** Data preprocessing -> Truncation correction -> Parameter estimation (mean/cov) -> Iterative refinement -> Convergence check

**Critical path:** The estimation algorithm iteratively alternates between mean and covariance estimation (for self-censoring) or parameter optimization under constraints (for linear thresholding), with each iteration requiring truncation-corrected gradient computations

**Design tradeoffs:** The algorithms trade off between computational efficiency (polynomial time) and statistical accuracy (epsilon precision), with sample complexity depending on eigenvalue ratios that can be large in high dimensions

**Failure signatures:** Poor performance when α, β, or γ approach zero (censorship becomes too severe), when λ_max/λ_min is very large (ill-conditioned covariance), or when the missingness mechanism assumptions are violated

**3 first experiments:** 1) Validate recovery on synthetic data with known parameters and varying censoring rates, 2) Test sensitivity to eigenvalue ratio by generating data with increasingly ill-conditioned covariances, 3) Compare performance under true MNAR mechanism vs. misspecified mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Requires strong assumptions on missingness mechanisms (probabilities bounded away from 0 and 1)
- Sample complexity depends on potentially large eigenvalue ratios λ_max/λ_min
- Designed for exact recovery rather than robust estimation under model misspecification
- Practical performance may degrade in very high-dimensional settings

## Confidence

**High confidence** in theoretical framework and proof techniques for idealized settings
**Medium confidence** in practical applicability given strong assumptions required
**Medium confidence** in sample complexity bounds due to dependence on potentially large eigenvalue ratios

## Next Checks

1. Empirical evaluation of algorithm performance on synthetic data with varying missingness patterns and covariance structures to validate theoretical predictions
2. Sensitivity analysis of sample complexity bounds with respect to the eigenvalue ratio λ_max/λ_min and missingness parameters α, β, γ
3. Extension of analysis to cases where missingness mechanism parameters are estimated from data rather than assumed known