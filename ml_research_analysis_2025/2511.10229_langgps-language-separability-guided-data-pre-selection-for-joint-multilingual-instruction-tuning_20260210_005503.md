---
ver: rpa2
title: 'LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual
  Instruction Tuning'
arxiv_id: '2511.10229'
source_url: https://arxiv.org/abs/2511.10229
tags:
- multilingual
- data
- training
- samples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of selecting effective multilingual\
  \ training data for instruction tuning of large language models. The authors propose\
  \ LangGPS, a two-stage pre-selection framework guided by language separability,\
  \ which quantifies how well samples in different languages are distinguishable in\
  \ the model\u2019s representation space."
---

# LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning

## Quick Facts
- arXiv ID: 2511.10229
- Source URL: https://arxiv.org/abs/2511.10229
- Authors: Yangfan Ye; Xiaocheng Feng; Xiachong Feng; Lei Huang; Weitao Ma; Qichen Hong; Yunfei Lu; Duyu Tang; Dandan Tu; Bing Qin
- Reference count: 40
- Primary result: A two-stage pre-selection framework that uses language separability to improve multilingual instruction tuning, achieving up to 6.37% relative gains on understanding tasks.

## Executive Summary
This paper addresses the challenge of selecting effective multilingual training data for instruction tuning of large language models. The authors propose LangGPS, a two-stage pre-selection framework guided by language separability, which quantifies how well samples in different languages are distinguishable in the model's representation space. The method first filters training data based on separability scores and then refines the subset using existing selection methods. Experiments across six benchmarks and 22 languages show that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability, especially for understanding tasks and low-resource languages.

## Method Summary
LangGPS is a two-stage pre-selection framework for multilingual instruction tuning. First, it computes language separability scores for each sample using the silhouette score based on last-token hidden states from the base model. Samples are then ranked by separability within each language, and the top ρ% (default 20%) are selected per language. Second, an existing data selection method (KMC, DSIR, LESS, or Random) is applied to this pre-selected subset to obtain the final training set. The approach is evaluated on LLaMA-3.1-8B and Qwen2.5-7B models using 97,696 instruction pairs from the Aya dataset across 31 languages, with evaluation on six benchmarks in 22 languages.

## Key Results
- LangGPS improves existing selection methods, achieving up to 6.37% relative gain on LLaMA-3.1-8B for understanding tasks (MMMLU, XNLI, XStoryCloze)
- High-separability samples significantly outperform low-separability and random selection under data constraints (<2000 samples)
- Low-separability samples function as bridges for cross-lingual alignment, showing minimal degradation in X→En translation
- Interleaving samples with diverse separability levels (Balanced curriculum) yields stable gains in multilingual curriculum learning

## Why This Works (Mechanism)

### Mechanism 1: Language Separability Filters for Representation Quality
- Claim: Selecting samples that cluster distinctly by language in representation space improves multilingual learning efficiency, particularly under data-constrained regimes.
- Mechanism: The silhouette score quantifies intra-language compactness versus inter-language separation for each sample. High-scoring samples have well-structured, language-specific representations that accelerate the model's ability to form and maintain distinct linguistic boundaries during fine-tuning.
- Core assumption: Models learn language-specific features more efficiently when training data exhibits clear representational boundaries rather than entangled cross-lingual representations.
- Evidence anchors:
  - [abstract] "highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation"
  - [Section 3] Formal definition using silhouette score: s(p_i^l) = (b(p_i^l) - a(p_i^l)) / max{a(p_i^l), b(p_i^l)}
  - [Figure 1a] Under limited training data (<2000 samples), high-separability samples significantly outperform low-separability and random selection
  - [corpus] Weak direct corroboration; related work on demonstration selection (arXiv:2506.06033) suggests LLMs benefit from principled data filtering, but does not address linguistic structure.
- Break condition: If the base model already has strong multilingual representations (e.g., Qwen2.5-7B), separability-based filtering yields diminishing returns compared to weaker multilingual models (e.g., LLaMA-3.1-8B).

### Mechanism 2: Pre-Selection as a Lightweight Compatibility Layer
- Claim: Applying separability filtering before existing data selection methods improves their effectiveness in multilingual contexts.
- Mechanism: LangGPS acts as a pre-filter (Stage 1) that removes linguistically entangled samples, allowing downstream selection methods (Stage 2: diversity-based, gradient-based, etc.) to operate on a cleaner candidate pool. This two-stage design preserves compatibility with arbitrary selectors.
- Core assumption: Existing selection methods designed for monolingual data do not account for linguistic structure; adding a separability filter corrects this blind spot without requiring method redesign.
- Evidence anchors:
  - [abstract] "LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods"
  - [Table 1] LangGPS+LESS improves Δ from +4.90% to +6.37% on LLaMA-3.1-8B; LangGPS+DSIR improves Qwen2.5-7B from +2.61% to +4.62%
  - [Section 4.1] "ρ = 20" pre-selection ratio used in main experiments
  - [corpus] No direct corpus evidence on two-stage pre-selection for multilingual tuning; this appears novel to this work.
- Break condition: If pre-selection ratio ρ is too low (e.g., 10%), diversity suffers due to over-similarity among highly separable samples; if ρ approaches 100%, the effect converges to vanilla baseline.

### Mechanism 3: Low-Separability Samples Enable Cross-Lingual Bridging
- Claim: Low-separability samples, despite slower initial adaptation, contribute to cross-lingual alignment due to their entangled representations.
- Mechanism: Samples with low separability scores occupy overlapping regions in representation space across languages. When used in curriculum learning or translation tasks, these samples reduce degradation in X→En translation and provide milder performance drops during training instabilities.
- Core assumption: Cross-lingual transfer benefits from training on samples that share representational features across languages, not just language-specific samples.
- Evidence anchors:
  - [abstract] "low-separability samples tend to function as bridges for cross-lingual alignment"
  - [Section 5.3] On FLORES+ translation, low-separability samples show least degradation in X→En direction and milder decline in En→X during the 200-2000 sample dip
  - [Figure 4] Low-separability samples cause severe cold-start at 50 samples but stabilize differently across translation directions
  - [corpus] CM-Align (arXiv:2509.08541) addresses multilingual alignment via consistency, but does not frame it as a sample-level property; no direct corroboration.
- Break condition: Low-separability samples alone are insufficient for cold-start; high-separability samples are required initially to establish language boundaries.

## Foundational Learning

- Concept: **Silhouette Score (Clustering Validation)**
  - Why needed here: The core metric for quantifying language separability; understanding its components (intra-cluster compactness a, inter-cluster separation b) is essential for interpreting why certain samples score higher.
  - Quick check question: Given a sample with silhouette score 0.8, would you expect it to be close to or far from samples of other languages in representation space?

- Concept: **Cross-Lingual Transfer via Representation Alignment**
  - Why needed here: Explains why low-separability samples function as bridges; samples with entangled representations across languages facilitate knowledge sharing.
  - Quick check question: If a model's representations for English and Spanish are perfectly overlapped in embedding space, would you expect high or low cross-lingual transfer on aligned tasks?

- Concept: **Curriculum Learning in Multilingual Settings**
  - Why needed here: Section 5.4 shows that separability can guide training order; the "Balanced" strategy (interleaving separability levels) outperforms pure ascending/descending order.
  - Quick check question: Why might training on only high-separability samples first lead to suboptimal multilingual performance compared to interleaved training?

## Architecture Onboarding

- Component map:
  ```
  Full Training Corpus (D)
         ↓
  [Stage 1: Separability Scoring]
    - Extract last-token hidden states via forward pass
    - Compute silhouette scores per sample within language clusters
    - Select top ρ% per language (default ρ=20)
         ↓
  Pre-Selected Subset
         ↓
  [Stage 2: Existing Selection Method]
    - Apply KMC, DSIR, LESS, Random, etc.
         ↓
  Final Training Set
  ```

- Critical path:
  1. Representation extraction (2 hours on single A100 for ~98K samples; this is the bottleneck)
  2. Silhouette scoring (O(|D|²) but negligible in practice at 1 minute)
  3. Downstream selector application (varies; LESS is most expensive at 8 hours total)

- Design tradeoffs:
  - **Pre-selection ratio ρ**: Lower ρ increases efficiency (smaller candidate pool for expensive selectors like LESS) but risks diversity loss. Paper finds ρ=20-70% works well.
  - **Model-specific vs. universal**: Separability depends on the base model's representations; LangGPS requires re-scoring for different models (stated limitation in Appendix D).
  - **High vs. low separability emphasis**: High for understanding tasks/low-resource languages; low for cross-lingual alignment tasks.

- Failure signatures:
  - Performance drops on generation tasks relative to understanding tasks (Table 1 shows smaller gains on XLSum/MKQA)
  - Over-similarity in selected data (Figure 1b shows high-separability samples have higher pairwise similarity)
  - Severe cold-start when training only on low-separability samples (<100 samples)

- First 3 experiments:
  1. **Replicate Figure 1a**: Train LLaMA-3.1-8B on high-separability vs. low-separability vs. random samples (500, 1000, 2000, 5000) on MMMLU to validate separability signal on your model and dataset.
  2. **Ablate pre-selection ratio ρ**: Test ρ ∈ {10, 20, 40, 70, 100} on your target benchmark to find optimal tradeoff between diversity and efficiency for your specific downstream selector.
  3. **Curriculum strategy comparison**: Implement Ascending, Descending, and Balanced curriculum strategies (Section 5.4) to determine if interleaving separability levels yields gains over pure selection on your task type (understanding vs. generation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language separability metrics calculated on a smaller proxy model effectively guide data selection for a larger target model?
- Basis in paper: [explicit] Appendix D notes that LangGPS requires model-specific data selection because separability reflects the model's own capability.
- Why unresolved: The authors acknowledge that separability varies by model, but the potential for cross-model transfer to reduce computational overhead was not investigated.
- What evidence would resolve it: Experiments showing that data ranked by a small model (e.g., 1B) improves performance when used to train a much larger model (e.g., 70B).

### Open Question 2
- Question: Does prioritizing low-separability samples specifically enhance cross-lingual alignment or translation quality in dedicated translation tasks?
- Basis in paper: [inferred] Section 5.3 hypothesizes low-separability samples act as bridges, but Appendix D clarifies the training data was general instruction data, not translation data.
- Why unresolved: The specific benefit of low-separability samples for translation remains theoretical because the experimental setup did not use a translation-specific corpus.
- What evidence would resolve it: An ablation study on a machine translation dataset (e.g., WMT) comparing models trained exclusively on low-separability versus high-separability data.

### Open Question 3
- Question: Can a dynamic curriculum strategy that adapts separability levels based on training loss outperform the static "Balanced" interleaving approach?
- Basis in paper: [inferred] Section 5.4 shows static ordering (Ascending/Descending) fails while Balanced interleaving succeeds, suggesting the need for a more nuanced schedule.
- Why unresolved: The paper only tests fixed strategies; it is unknown if the model requires different separability profiles at different stages of convergence.
- What evidence would resolve it: Implementation of a loss-aware scheduler that increases or decreases the ratio of high-separability samples dynamically during training.

## Limitations

- **Model-specific separability scores**: The separability metric is computed from the base model's representations, meaning LangGPS requires re-scoring for each target model (explicitly acknowledged as a limitation). This restricts practical transferability across model families and prevents use of universal pre-selection.
- **Computational overhead of representation extraction**: The first stage (encoding all samples and computing silhouette scores) takes approximately 2 hours on a single A100 for the full 97K sample corpus. This is the primary bottleneck and may be prohibitive for larger datasets or frequent re-training cycles.
- **Limited diversity in highly separable samples**: Figure 1b shows that high-separability samples have higher pairwise similarity within languages, potentially reducing linguistic diversity in the final training set. The paper does not extensively explore the tradeoff between separability and diversity.

## Confidence

- **High confidence**: The core mechanism that high-separability samples improve understanding task performance and low-separability samples serve as cross-lingual bridges is well-supported by ablation studies and curriculum learning experiments. The two-stage design is clearly demonstrated to be compatible with existing selection methods.
- **Medium confidence**: The claim that LangGPS is a "lightweight compatibility layer" is supported by consistent improvements across multiple downstream selectors (KMC, DSIR, LESS), but the magnitude of improvement varies significantly by selector and model, suggesting the compatibility benefit is context-dependent.
- **Low confidence**: The assertion that separability-based pre-selection will generalize to arbitrary multilingual instruction datasets beyond Aya is not directly tested. The paper evaluates on one dataset and six benchmarks, leaving open questions about performance on different data distributions or instruction formats.

## Next Checks

1. **Model Transferability Test**: Apply LangGPS pre-selection computed on LLaMA-3.1-8B representations to Qwen2.5-7B fine-tuning without re-scoring. Measure performance degradation to quantify the cost of model-specific separability and validate the stated limitation.

2. **Dataset Generalization Test**: Apply LangGPS to a different multilingual instruction dataset (e.g., MT-Bench or an instruction-tuned subset of Wikipedia) and evaluate whether the same separability-performance relationship holds. This would validate whether separability is a universal property or Aya-specific.

3. **Diversity-Accuracy Tradeoff Analysis**: Systematically vary the pre-selection ratio ρ (e.g., 10%, 20%, 40%, 70%, 100%) and measure both performance and linguistic diversity (e.g., using n-gram overlap or sentence embedding variance). Plot the Pareto frontier to determine if the performance gains justify the diversity loss at different ρ values.