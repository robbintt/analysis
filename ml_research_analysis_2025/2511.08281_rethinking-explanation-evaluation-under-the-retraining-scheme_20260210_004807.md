---
ver: rpa2
title: Rethinking Explanation Evaluation under the Retraining Scheme
arxiv_id: '2511.08281'
source_url: https://arxiv.org/abs/2511.08281
tags:
- evaluation
- features
- retraining
- explanation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies a distortion issue in the standard retraining-based
  evaluation scheme (ROAR), where residual information from negatively attributed
  features undermines assessment reliability. The authors analyze the root cause and
  propose two core remedies: (1) lowest-first occlusion (KEAR) to avoid retaining
  negatively attributed features, and (2) fine-tuning variants (KAFT and KAFT-C) to
  improve computational efficiency.'
---

# Rethinking Explanation Evaluation under the Retraining Scheme

## Quick Facts
- arXiv ID: 2511.08281
- Source URL: https://arxiv.org/abs/2511.08281
- Reference count: 40
- This work identifies a distortion issue in standard retraining-based evaluation (ROAR) and proposes two core remedies: lowest-first occlusion (KEAR) and fine-tuning variants (KAFT and KAFT-C) that achieve up to 100× speedup while delivering more reliable evaluations.

## Executive Summary
This paper identifies a fundamental distortion issue in standard retraining-based evaluation schemes for feature attribution methods. The "Sign issue" occurs when negatively attributed features—which may contain task-relevant information shared across classes—are retained during manipulation, leading to inflated accuracy and underestimation of explanation quality. The authors propose two solutions: KEAR (lowest-first occlusion) reframes the evaluation to focus on identifying influential features, while KAFT-C (classification-head-only fine-tuning) achieves 100× speedup while maintaining evaluation validity. Experiments across small- and large-scale datasets demonstrate that KAFT-C provides reliable, efficient evaluations while revealing limitations in attribution methods like IG on transformer architectures.

## Method Summary
The authors develop three evaluation schemes: KEAR (lowest-first occlusion), KAFT (fine-tuning), and KAFT-C (classification-head-only fine-tuning). For KAFT-C, they keep the lowest-attributed k% of features, replace the rest with baseline values, and fine-tune only the final classification layer for 10 epochs using 10% of the manipulated training data. This approach is compared against ROAR (highest-first occlusion) and |RAFT-C| (magnitude-based occlusion). The evaluation metric is accuracy degradation at 90% feature removal, with ΔAcc computed as the area between KAFT-C and |RAFT-C| curves. Experiments are conducted on MNIST, CIFAR10, STL10 (small-scale) and ImageNet with ResNet50/SwinT (large-scale).

## Key Results
- KAFT-C achieves up to 100× speedup over full retraining while maintaining evaluation validity
- On ImageNet-ResNet50, baseline accuracy drops from 76.13% to 12.98% with 90% feature removal
- SIG achieves the highest drop (35.61%) on ImageNet-ResNet50, demonstrating superior feature identification
- IG exhibits performance collapse on ImageNet-SwinT, highlighting limitations on transformer architectures
- KEAR consistently produces rankings matching theoretical expectations, unlike inconsistent ROAR results

## Why This Works (Mechanism)

### Mechanism 1: The Sign Issue as Root Cause of Evaluation Distortion
Highest-first occlusion (ROAR) allows negatively attributed features to survive manipulation. These features can contain task-relevant information shared across classes that becomes "secondary evidence." After primary evidence removal, these secondary features increase in utility for the target class due to distribution shift, inflating retraining accuracy and causing underestimation of explanation quality. Theorem 1 proves that mutual information between secondary evidence and label increases after manipulation: Ĩ(S₂;y) > I(S₂;y) ≫ 0.

### Mechanism 2: Lowest-First Occlusion (KEAR) Reframes Evaluation Objective
KEAR asks "Does the explainer identify influential features?" rather than "Does the explainer identify ALL task-relevant features?" By keeping top-ranked features, effective explainers preserve information needed for prediction, resulting in higher retraining accuracy. This approach aligns with theoretical expectations and better discriminates explainer quality by focusing on feature preservation rather than removal.

### Mechanism 3: Classification-Head-Only Fine-Tuning (KAFT-C) Preserves Evaluation Validity at 100× Speedup
Fine-tuning only the classification head on manipulated data, while freezing feature extractors, maintains evaluation discriminability while reducing computational cost by ~100×. The classification head adapts to altered context without reorganizing all learned features, preserving the evaluation's focus on model behavior rather than data utility.

## Foundational Learning

- **Mutual Information in Feature Attribution**: Understanding how feature subsets relate to prediction utility is essential for grasping why secondary evidence "becomes" primary after manipulation. Quick check: Can you explain why removing features for class y* increases the utility of those same features for predicting class y?

- **Gradient-Based Attribution Methods (IG, SG, G⊙I)**: The evaluation schemes are tested on gradient-based explainers with distinct theoretical foundations. IG uses baseline-integration, SG uses smoothing, and their relative performance validates evaluation schemes. Quick check: Why does IG theoretically outperform SG, and how does the baseline choice affect attribution?

- **Softmax Normalization and Class Competition**: Theorem 2 shows softmax can assign negative attributions to features that positively activate a class logit because their contribution is weak relative to other classes. Quick check: A feature increases logit o_y by +0.5 but increases all other logits by +2.0 on average. Will its attribution to f_y be positive or negative, and why?

## Architecture Onboarding

- **Component map**: [Explainer] → Attribution scores → [Manipulation Module] → Occlusion (Lowest-first/Highest-first) → [Model Update] → Fine-tune/Retrain → [Evaluation Metric] → Accuracy on test set

- **Critical path**:
  1. Generate attributions for entire dataset using chosen explainer
  2. For KAFT-C: Sort attributions ascending, keep bottom k% features, replace rest with baseline
  3. Fine-tune only final classification layer for 10 epochs on 10% of manipulated training data
  4. Evaluate on original test set; compare to random baseline
  5. For comprehensive evaluation: repeat with |RAFT-C| and compute ΔAcc

- **Design tradeoffs**:
  - Occlusion priority: Lowest-first (KEAR) evaluates feature preservation / Highest-first (ROAR) evaluates feature removal but vulnerable to sign issue
  - Model update scope: Full retrain = most thorough but costly / Classification-head-only = fast but may miss representation-level adaptations
  - Baseline selection: Must match explainer baseline for fair evaluation; mismatched baselines introduce bias

- **Failure signatures**:
  - ROAR shows all explainers performing no better than random → sign issue distortion likely present
  - KAFT-C accuracy near random baseline → explainer fails to identify influential features OR manipulation ratio too aggressive
  - Large discrepancy between KAFT-C and |RAFT-C| on same explainer → explainer correctly identifies relevance but misassigns signs
  - IG performs near random on transformer (SwinT) → gradient saturation plateau or cancellation effects

- **First 3 experiments**:
  1. **Baseline validation**: Run KAFT-C vs. random on a small dataset (MNIST/CIFAR10) with IG; expect IG curve significantly above random
  2. **Sign issue demonstration**: Compare ROAR vs. KEAR vs. |ROAR| on same explainer/dataset; if ROAR >> KEAR accuracy, sign issue is confirmed
  3. **Efficiency-quality tradeoff**: On ImageNet-ResNet50, compare full KEAR vs. KAFT vs. KAFT-C; report accuracy at 90% removal and wall-clock time

## Open Questions the Paper Calls Out

### Open Question 1
How can the integration path in attribution methods be modified to resolve the performance collapse of Integrated Gradients on vision transformers? The conclusion states the need for "investigation into path selection and allocation of feature interactions" to address IG's failure on SwinT. The authors identify saturation and cancellation effects as likely causes but do not validate a specific corrective path mechanism. A modified IG variant demonstrating recovered performance on SwinT benchmarks comparable to SIG or ResNet baselines would resolve this.

### Open Question 2
Can a unified evaluation framework combining highest-first and lowest-first removal priorities reliably detect misassignment of attribution signs? Appendix A.6 proposes "future work... developing a more structured explanation evaluation framework that aligns results... with diverse manipulation priorities." The paper validates individual schemes but does not define or test a combined metric for diagnosing specific attribution errors like sign inversion. A formalized metric utilizing the accuracy gap between ROAR and KEAR that correlates with induced sign flips in synthetic data would resolve this.

### Open Question 3
Does the "Weak Positive Contributor" phenomenon distort evaluations in architectures that do not rely on softmax normalization? The theoretical proof of the Sign issue (Definition 2) relies on the interaction between feature contribution and softmax normalization. The paper limits empirical validation to image classifiers using softmax, leaving the generalizability to other output layers untested. Empirical validation of the KEAR/ROAR accuracy gap on models using alternative loss functions or output normalizations would resolve this.

## Limitations

- **Sign issue universality**: While demonstrated on gradient-based explainers, the extent to which it affects attribution methods beyond those tested remains unclear
- **Computational tradeoff validity**: The claimed 100× speedup depends on specific hardware configurations and implementation details that aren't fully specified
- **Large-scale transformer evaluation**: The dramatic performance collapse of IG on SwinT suggests fundamental limitations, but the paper doesn't fully explore whether this stems from transformer-specific properties or implementation choices

## Confidence

- **High confidence**: The mathematical proof of the sign issue (Theorem 1), the effectiveness of lowest-first occlusion (KEAR) as a conceptual reframing, and the basic computational efficiency gains of KAFT-C
- **Medium confidence**: The claim that KAFT-C achieves "up to 100× speedup while delivering more reliable evaluations" (depends on unspecified implementation details), the relative ranking of explainers being consistent across datasets
- **Low confidence**: The claim that the sign issue represents a fundamental flaw in all retraining-based evaluation schemes (may be explainer-specific), the completeness of transformer evaluation (only one architecture tested)

## Next Checks

1. **Sign issue breadth test**: Apply KAFT-C evaluation to non-gradient-based attribution methods (e.g., SHAP, LIME) to determine whether the sign issue affects all explanation families or is specific to gradient-based approaches

2. **Implementation validation**: Replicate the KAFT-C vs. full retraining timing comparison on standardized hardware using open-source attribution libraries to verify the claimed computational efficiency gains

3. **Transformer generalization test**: Evaluate additional transformer architectures (e.g., ViT, DeiT) and attribution methods to determine whether IG's performance collapse on SwinT represents a broader pattern or a model-specific artifact