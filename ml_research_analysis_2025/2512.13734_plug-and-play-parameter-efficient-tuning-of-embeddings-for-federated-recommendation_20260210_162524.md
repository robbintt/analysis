---
ver: rpa2
title: Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation
arxiv_id: '2512.13734'
source_url: https://arxiv.org/abs/2512.13734
tags:
- embeddings
- embedding
- item
- full
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a communication-efficient federated recommendation
  framework using parameter-efficient fine-tuning (PEFT) for item embeddings. The
  proposed approach freezes pre-trained full item embeddings on client devices and
  only updates lightweight compressed embeddings during federated training, significantly
  reducing communication overhead while maintaining or improving recommendation performance.
---

# Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation

## Quick Facts
- **arXiv ID:** 2512.13734
- **Source URL:** https://arxiv.org/abs/2512.13734
- **Reference count:** 11
- **Primary result:** PEFT-based compressed embeddings consistently outperform full embeddings and compressed-only embeddings in federated recommendation across multiple datasets and models.

## Executive Summary
This paper introduces a communication-efficient federated recommendation framework using parameter-efficient fine-tuning (PEFT) for item embeddings. The proposed approach freezes pre-trained full item embeddings on client devices and only updates lightweight compressed embeddings during federated training, significantly reducing communication overhead while maintaining or improving recommendation performance. Three compressed embedding strategies are explored: LoRA, hash-based methods (with and without SENet), and Residual Quantized Variational Autoencoders (RQ-VAE). Extensive experiments across multiple FR models and datasets demonstrate that PEFT-based embeddings consistently outperform full embeddings and compressed embeddings alone in most scenarios. The framework achieves a favorable trade-off between communication efficiency and recommendation accuracy, with RQ-VAE showing particularly strong and stable performance. The method also exhibits robustness under differential privacy constraints.

## Method Summary
The framework pre-trains full item embeddings using item attributes via an AutoEncoder on the server, then distributes these embeddings to clients for local recommendation tasks. Instead of updating full embeddings during federated training, clients only optimize lightweight compressed representations (LoRA matrices, hash tables, or RQ-VAE codebooks) that are aggregated by the server. The final item embedding is the sum of frozen full embedding and compressed embedding. Three PEFT strategies are implemented: LoRA uses low-rank matrices to approximate embedding updates; hash-based methods map items to shared vectors with optional SENet attention; RQ-VAE uses multi-level vector quantization with frozen semantic codes. The approach significantly reduces communication overhead while maintaining or improving recommendation performance.

## Key Results
- RQ-VAE consistently outperforms LoRA and hash-based methods across all datasets and models
- PEFT-based embeddings (P-LoRA, P-RQ-VAE) achieve 58-86% communication reduction while matching or exceeding full embedding performance
- Hash-based methods with SENet show improved performance for MLP architectures but degrade for pure embedding models
- The framework demonstrates robustness under differential privacy constraints, though RQ-VAE shows slightly higher sensitivity to noise injection

## Why This Works (Mechanism)

### Mechanism 1: Residual Quantized Codebooks as Compressed Embeddings
The server pre-trains full item embeddings using item attributes via an AutoEncoder. These embeddings are distributed once and frozen on clients. During federated training, only compressed embeddings (e.g., RQ-VAE codebooks, LoRA matrices, or hash tables) are optimized locally and aggregated by the server. The final embedding is the sum of frozen full embedding and compressed embedding. Core assumption: Pre-trained embeddings capture stable semantic content that need not be updated collaboratively; collaborative user-item interaction signals can be encoded in the low-dimensional residual/compressed space.

### Mechanism 2: RQ-VAE Multi-Level Quantization Preserves Semantic Hierarchy
RQ-VAE encodes items via residual quantization across `l` levels. Each level quantizes the residual from the previous level using a shared codebook. Semantic codes (indices) are pre-trained on server and frozen; only codebooks are fine-tuned during FR. Final compressed embedding is the sum of codebook vectors at each level. Core assumption: Semantic hierarchy can be captured via residual decomposition; codebook quality from pre-training determines representation fidelity.

### Mechanism 3: Hash-Based Collision Mitigation via SENet Reweighting
Items are mapped to multiple vectors in a shared hash table via universal hash functions. Embedding is constructed as weighted sum of hash vectors, where weights are learned via SENet squeeze-excitation. This mitigates random collision effects by learning which hash entries matter most. Core assumption: Hash collisions can be compensated by learned attention weights; MLP architectures benefit from dynamic reweighting more than pure embedding-based models.

## Foundational Learning

- **Concept: Federated Recommendation (FR)**
  - **Why needed here:** The entire framework operates in a distributed setting where users keep raw data locally and only share model parameters with a central server. Understanding FR basics (client sampling, local training rounds, server aggregation) is prerequisite.
  - **Quick check question:** Can you explain the difference between local training on a client and global aggregation on the server in a federated recommendation setting?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The core innovation is applying PEFT techniques (LoRA, hash, RQ-VAE) to item embeddings rather than model weights. Understanding low-rank adaptation, freezing vs. fine-tuning, and the goal of reducing trainable parameters is essential.
  - **Quick check question:** What does "freezing" a parameter mean during training, and why does it reduce communication overhead in federated learning?

- **Concept: Vector Quantization and Codebooks**
  - **Why needed here:** RQ-VAE relies on multi-level vector quantization with shared codebooks. You need to understand how discrete codes index into continuous codebook vectors and how residual quantization works.
  - **Quick check question:** If you have a 3-level RQ-VAE with codebook size 256, how many unique item representations are theoretically possible? (Answer: 256³ = 16,777,216, though semantic code assignments from pre-training constrain actual diversity.)

## Architecture Onboarding

- **Component map:**
  Server: Pre-training module (AutoEncoder for full embeddings) -> RQ-VAE pre-training (Encoder, Decoder, l codebooks) -> Compressed embedding initializer (LoRA A,B / Hash table H / RQ-VAE codebooks) -> Federated aggregator (averages uploaded compressed parameters)
  Client: Frozen full embeddings E (downloaded once after warm-up) -> Frozen semantic codes (RQ-VAE only) or hash functions (Hash only) -> Trainable compressed embeddings (A,B for LoRA; H for Hash; codebooks for RQ-VAE) -> Local recommendation model W_g (if applicable)

- **Critical path:**
  1. Pre-train full embeddings on server using item attributes with AutoEncoder (L_AE reconstruction loss)
  2. Optional warm-up: Train full embeddings in FR for <20 rounds to stabilize early optimization
  3. Initialize and distribute compressed embeddings (LoRA: zero-initialized B; Hash: random H; RQ-VAE: pre-trained semantic codes + freshly initialized codebooks)
  4. Freeze full embeddings on clients; only optimize compressed parameters locally
  5. Upload and aggregate only compressed parameters each round
  6. Inference: Final embedding = full embedding + compressed embedding

- **Design tradeoffs:**
  - LoRA: Direct learnable embeddings, no collisions, but communication O(k_L · (n + k)) scales with item count. Best for smaller catalogs or when representation fidelity is critical.
  - RQ-VAE: Most compact communication O(d_R · l), large representation space, but depends heavily on pre-training quality. Best for large catalogs with rich item attributes.
  - Hash: Smallest communication O(d_H), collision-resistant via hash functions, but underperforms in many settings. Best with MLP models when combined with SENet.
  - SENet attention: Adds ~O(h²) computation, improves Hash performance for MLP models, degrades performance for pure embedding models.

- **Failure signatures:**
  - C-Hash (compressed only) performs poorly: Random hashing without full embedding fallback lacks semantic grounding
  - RQ-VAE performance drops with too many levels (l=6): Over-quantization introduces redundancy, weakens item relevance
  - SENet hurts performance on FedMF/FedPerGNN: Attention weights add noise in models without MLP to leverage dynamic reweighting
  - LoRA with k_L > 4 shows no improvement or degradation: Diminishing returns; overly large latent dimension may overfit local data
  - RQ-VAE under CDP (Central Differential Privacy) degrades significantly on ML1M: Quantized representations may be more sensitive to noise injection than full embeddings

- **First 3 experiments:**
  1. Baseline comparison: Replicate Table 2 results on a single dataset (e.g., ML1M) with one FR backbone (e.g., FedNCF). Compare Full, C-LoRA, C-RQ-VAE, P-LoRA, P-RQ-VAE. Verify that PEFT embeddings (P-*) outperform compressed-only (C-*) and match/exceed Full.
  2. Communication profiling: Measure actual communication cost per client for each strategy (Figure 3). Confirm LoRA/RQ-VAE achieve target reduction ratios while maintaining accuracy.
  3. Hyperparameter sensitivity: Replicate RQ-VAE sensitivity analysis (Figure 4) by varying codebook size d_R ∈ {32, 64, 128, 256, 512} and levels l ∈ {2, 3, 4, 5, 6}. Identify optimal configuration for your dataset.

## Open Questions the Paper Calls Out

- **Question:** Can hybrid approaches combining multiple PEFT strategies optimize the trade-offs between recommendation performance, communication efficiency, and client storage cost?
  - **Basis in paper:** [explicit] The conclusion identifies a future direction in exploring "trade-offs between performance, communication efficiency, and client storage cost, potentially through hybrid approaches."
  - **Why unresolved:** Current compressed-only methods save storage but lose accuracy, while PEFT methods improve accuracy but differ in efficiency; no single method dominates.
  - **What evidence would resolve it:** A study evaluating a combined framework (e.g., LoRA with Hashing) against single-strategy baselines on storage and bandwidth metrics.

- **Question:** Would initializing the RQ-VAE codebooks using the pre-trained server-side weights (warm-start) improve convergence compared to the random initialization used in the current framework?
  - **Basis in paper:** [inferred] The methodology states codebooks are initialized randomly to match full embedding distributions, explicitly discarding the pre-trained codebook weights from the server.
  - **Why unresolved:** The paper assumes random initialization is necessary for FR settings but does not empirically test if retaining the pre-trained structural knowledge in the codebooks offers a better starting point.
  - **What evidence would resolve it:** Ablation experiments comparing federated training convergence speed and final accuracy using random vs. pre-trained codebook initializations.

- **Question:** How can the framework automatically determine the optimal compressed embedding strategy (LoRA, Hash, RQ-VAE) for a given dataset and model architecture?
  - **Basis in paper:** [explicit] The conclusion notes that "No single strategy consistently outperforms others across all datasets and FR models."
  - **Why unresolved:** Strategy effectiveness varies significantly by context (e.g., Hash performs poorly on FedMF but well on FedNCF), leaving users without a clear heuristic for selection.
  - **What evidence would resolve it:** Development of a meta-learner or an analytical framework that predicts the best PEFT strategy based on dataset characteristics (sparsity, size) and model type.

## Limitations

- The framework assumes pre-trained full embeddings are sufficient for downstream recommendation tasks, but this may not hold if item catalogs change rapidly or if user preferences shift significantly.
- The effectiveness of RQ-VAE depends heavily on the quality of semantic code pre-training, which is not directly evaluated.
- Hash-based methods show mixed results across different model architectures, suggesting that their benefits may be context-dependent.
- The paper does not explore the impact of differential privacy on compressed embeddings versus full embeddings in depth.

## Confidence

- **High Confidence:** The communication efficiency gains of PEFT-based embeddings over full embeddings are well-supported by empirical results and the mathematical formulation of reduced parameter updates.
- **Medium Confidence:** The superiority of RQ-VAE over other compressed embedding strategies is demonstrated, but the results are dataset-dependent and the impact of pre-training quality is not fully explored.
- **Medium Confidence:** The claim that SENet improves hash-based methods for MLP models is supported, but the underlying mechanism for why it hurts pure embedding models is not thoroughly explained.

## Next Checks

1. **Ablation Study on Pre-training Quality:** Evaluate RQ-VAE performance using full embeddings pre-trained with varying reconstruction losses or different AutoEncoder architectures to isolate the impact of pre-training quality on downstream FR performance.

2. **Robustness to Item Catalog Changes:** Simulate a scenario where new items are added to the catalog mid-training. Measure the performance degradation of PEFT strategies versus full embeddings to assess their adaptability.

3. **Privacy Sensitivity Analysis:** Compare the impact of central and local differential privacy on compressed embeddings (LoRA, Hash, RQ-VAE) versus full embeddings. Quantify the trade-off between privacy guarantees and recommendation accuracy for each approach.