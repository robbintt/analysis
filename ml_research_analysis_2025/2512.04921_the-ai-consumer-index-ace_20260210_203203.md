---
ver: rpa2
title: The AI Consumer Index (ACE)
arxiv_id: '2512.04921'
source_url: https://arxiv.org/abs/2512.04921
tags:
- each
- criteria
- high
- gemini
- whether
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AI Consumer Index (ACE) introduces a novel benchmark for assessing
  frontier AI models' ability to perform everyday consumer tasks across shopping,
  food, gaming, and DIY domains. ACE employs a rigorous evaluation methodology that
  includes hurdle criteria to gatekeep core task completion and a three-step grounding
  process to check whether claims are supported by web sources.
---

# The AI Consumer Index (ACE)

## Quick Facts
- arXiv ID: 2512.04921
- Source URL: https://arxiv.org/abs/2512.04921
- Reference count: 20
- Primary result: Benchmark for assessing frontier AI models' ability to perform everyday consumer tasks across shopping, food, gaming, and DIY domains

## Executive Summary
The AI Consumer Index (ACE) introduces a comprehensive benchmark for evaluating how well frontier AI models can handle real-world consumer tasks. The benchmark tests models across four key domains - shopping, food, gaming, and DIY - using a rigorous methodology that includes hurdle criteria to ensure task completion and a three-step grounding process to verify claims against web sources. When evaluated on 400 hidden test cases, even the top-performing model (GPT-5 with Thinking = High) achieved only 56.1%, highlighting the substantial gap between current AI capabilities and consumer needs.

The evaluation reveals significant performance variation across domains, with models performing best in food-related tasks (70.1%) and worst in shopping (under 50%). A critical finding is models' proneness to hallucination, particularly for prices and links, with grounding failures having a more substantial negative impact on scores than other types of errors. The benchmark demonstrates that while models can often complete tasks in principle, their tendency to provide unsupported information significantly undermines their practical utility for consumers.

## Method Summary
ACE employs a sophisticated evaluation methodology that tests frontier AI models across four consumer domains using 400 hidden test cases. The approach uses hurdle criteria to gatekeep core task completion and a three-step grounding process to verify whether model claims are supported by web sources. Models are evaluated on their ability to complete tasks like finding products, booking reservations, or following game instructions, with performance measured through a combination of task completion rates and the accuracy of information provided. The benchmark includes both open-ended tasks and structured evaluations to capture the full range of consumer AI interactions.

## Key Results
- GPT-5 (Thinking = High) achieved the highest overall score of 56.1%, followed closely by o3 Pro (Thinking = On) at 55.2% and GPT-5.1 (Thinking = High) at 55.1%
- Best model scored under 50% in Shopping domain but reached 70.1% in Food domain
- Grounding failures had more substantial negative impact on scores than other types of errors
- Models showed significant proneness to hallucinating prices and links across all domains

## Why This Works (Mechanism)
ACE works by creating a realistic testing environment that mirrors actual consumer AI use cases, rather than abstract reasoning or coding tasks. The hurdle criteria ensure models must complete core task elements before being evaluated on quality, preventing inflated scores from partial responses. The three-step grounding process provides a systematic way to catch hallucinations by verifying claims against actual web sources, which is crucial since unsupported information is particularly problematic in consumer contexts where accuracy directly impacts user decisions and actions.

## Foundational Learning

**Hurdle Criteria** - Binary gates that determine if a task is considered complete; needed to prevent partial responses from receiving credit and to ensure meaningful evaluation of task completion.

**Three-Step Grounding Process** - Systematic verification of model claims against web sources through: 1) claim extraction, 2) source verification, and 3) confidence scoring; needed to objectively measure hallucination rates and their impact on consumer utility.

**Domain-Specific Task Design** - Tasks tailored to real consumer needs in shopping, food, gaming, and DIY; needed to ensure benchmark relevance to actual market applications rather than academic exercises.

**Hidden Test Cases** - Concealed evaluation prompts that prevent model training on test data; needed to ensure fair comparison between models and prevent performance inflation through memorization.

**Thinking Setting Variations** - Different model configurations (on, high, low) that control reasoning depth; needed to understand how reasoning effort impacts consumer task performance.

**Quick Check**: Verify that each task has clear completion criteria and that grounding checks use multiple independent sources to avoid false positives.

## Architecture Onboarding

**Component Map**: Test Case Generator -> Model Interface -> Response Processor -> Hurdle Evaluator -> Grounding Verifier -> Score Calculator -> Results Dashboard

**Critical Path**: Test Case Selection -> Model Query -> Response Collection -> Hurdle Assessment -> Grounding Verification -> Scoring

**Design Tradeoffs**: 
- Balance between task complexity and completion feasibility
- Tradeoff between grounding strictness and reasonable variation tolerance
- Choice between automated vs. human evaluation for ambiguous cases

**Failure Signatures**:
- Models completing hurdles but failing grounding indicate hallucination issues
- Consistent domain-specific failures suggest knowledge gaps
- Time-dependent score variations reveal temporal sensitivity

**3 First Experiments**:
1. Compare performance of the same model across different Thinking settings to isolate reasoning depth effects
2. Test grounding tolerance by introducing slight variations in correct responses to measure false negative rates
3. Evaluate inter-domain correlation to identify transferable vs. domain-specific capabilities

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

- Limited test case diversity with only 400 cases across four domains may not capture full consumer task spectrum
- Temporal variation in model evaluation timing introduces potential environmental differences affecting comparisons
- Grounding criteria may be overly strict, potentially penalizing models for reasonable variations in correct information

## Confidence

**High confidence**: The benchmark's methodology and evaluation framework are sound and well-documented, providing a reliable foundation for comparing model performance across consumer tasks.

**Medium confidence**: The relative ranking of models within the benchmark is likely accurate, though absolute performance scores may be influenced by the limitations mentioned above.

**Medium confidence**: The findings regarding domain-specific performance variations and the importance of grounding are supported by the data, though the exact magnitude of these effects may be subject to some uncertainty.

## Next Checks

1. Expand the test case corpus to include a more diverse and larger set of consumer tasks across additional domains to validate whether current findings hold across broader use cases.

2. Conduct a systematic analysis of grounding failures to distinguish between true hallucinations and cases where models provide correct but differently-phrased information, refining the grounding criteria accordingly.

3. Perform a controlled evaluation where all models are tested simultaneously under identical conditions with standardized settings to eliminate temporal and configuration biases in the comparison.