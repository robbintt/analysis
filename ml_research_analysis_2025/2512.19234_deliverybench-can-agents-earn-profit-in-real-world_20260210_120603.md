---
ver: rpa2
title: 'DeliveryBench: Can Agents Earn Profit in Real World?'
arxiv_id: '2512.19234'
source_url: https://arxiv.org/abs/2512.19234
tags:
- agent
- agents
- delivery
- orders
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeliveryBench, a city-scale embodied benchmark
  for evaluating VLM-based agents in long-horizon food delivery tasks. The benchmark
  simulates realistic delivery scenarios with complex constraints such as delivery
  deadlines, resource management (stamina, battery), transportation costs, and physical
  dynamics affecting food quality.
---

# DeliveryBench: Can Agents Earn Profit in Real World?

## Quick Facts
- arXiv ID: 2512.19234
- Source URL: https://arxiv.org/abs/2512.19234
- Reference count: 40
- Agents earn only $20-30/hour versus humans' $55+/hour in simulated food delivery

## Executive Summary
This paper introduces DeliveryBench, a city-scale embodied benchmark for evaluating VLM-based agents in long-horizon food delivery tasks. The benchmark simulates realistic delivery scenarios with complex constraints such as delivery deadlines, resource management (stamina, battery), transportation costs, and physical dynamics affecting food quality. Experiments with nine procedurally generated cities and seven VLM models show that current agents significantly underperform human couriers, earning only $20-30/hour versus humans' $55+/hour. Agents struggle with temporal planning, resource management, and handling implicit environmental constraints. Multi-agent settings reveal coordination challenges and distinct behavioral patterns across models. Context engineering and supervised fine-tuning provide some performance improvements, but the substantial gap to human performance remains.

## Method Summary
DeliveryBench is a city-scale embodied benchmark that evaluates VLM-based agents in long-horizon food delivery tasks. The benchmark features 9 procedurally generated cities (small: 11-15 roads, medium: 16-25, large: 26-30) with 10 active orders and 40% special requirements. Agents must maximize net profit over 2 virtual hours while managing 6 constraint types (spatial, temporal, resource, physical, economic, social). The planning pipeline π(Ot, Mt, Pt, Ft-1) → (at, Pt+1) uses VLMs at temperature=0 with 512-token max length and 300 API call budget. Context engineering techniques (ACE, Dynamic Cheatsheet) and supervised fine-tuning on human trajectories are explored as performance enhancements.

## Key Results
- Current VLM agents earn only $20-30/hour versus humans' $55+/hour in simulated food delivery
- Agents struggle with temporal planning, resource management, and handling implicit environmental constraints
- Context engineering and supervised fine-tuning provide performance improvements, but substantial gap to human performance remains
- Multi-agent settings reveal coordination challenges and distinct behavioral patterns across VLM models

## Why This Works (Mechanism)
The benchmark works by simulating realistic delivery scenarios with complex constraints that challenge current VLM capabilities. The 6 constraint types (spatial, temporal, resource, physical, economic, social) create a rich environment where agents must balance multiple competing objectives simultaneously. The procedurally generated cities ensure diversity in road layouts and building distributions, preventing overfitting to specific environments. The 300 API call budget and temperature=0 setting create a controlled evaluation framework that isolates the agent's planning capabilities from sampling artifacts.

## Foundational Learning
- **Embodied simulation**: Why needed - Provides realistic physical interactions and constraints; Quick check - Verify temperature dynamics affect food quality scores
- **Procedural city generation**: Why needed - Ensures diverse testing environments; Quick check - Confirm road count distribution matches specifications
- **Context engineering**: Why needed - Improves agent decision-making through better memory and planning; Quick check - Compare performance with/without ACE on simple city
- **Multi-constraint optimization**: Why needed - Reflects real-world delivery complexity; Quick check - Verify agents balance stamina, battery, and delivery deadlines

## Architecture Onboarding
- **Component map**: SimWorld environment -> VLM planning pipeline -> Agent actions -> Constraint checking -> Profit calculation
- **Critical path**: Observation encoding -> VLM inference -> Action selection -> Environment update -> Reward calculation
- **Design tradeoffs**: 300 API call budget limits exploration vs. computational cost; temperature=0 ensures deterministic behavior vs. natural language flexibility
- **Failure signatures**: Resource depletion (stamina/battery) causing interruptions; sequential vs parallel delivery indicated by time efficiency <1.0
- **3 first experiments**:
  1. Single-agent evaluation across 3-4 representative cities measuring hourly profit
  2. ACE context engineering implementation and validation on simple city
  3. Resource management failure mode testing for prevention ratio and violation rate metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Exact implementation details of SimWorld environment and procedural city generation are not fully specified
- Context engineering techniques (ACE and Dynamic Cheatsheet) lack detailed specifications for memory update rules and warm-up procedures
- SFT training dataset generation process is not fully specified, particularly regarding the sampling strategy for human trajectory pairs

## Confidence
- **High Confidence**: Benchmark design and fundamental finding that VLM agents underperform humans by factor of 2-3 in hourly profit
- **Medium Confidence**: Specific performance numbers for individual VLM models and effectiveness of context engineering techniques
- **Medium Confidence**: Coordination analysis in multi-agent settings and behavioral pattern classifications

## Next Checks
1. Reproduce the single-agent profit gap by running the benchmark with at least one VLM model (e.g., GPT-4V) across 3-4 representative cities, measuring hourly profit against the reported human baseline of $55/hour
2. Implement and validate the ACE context engineering technique by comparing performance with and without ACE on a simple city (11-15 roads) to verify the reported improvements
3. Test the resource management failure modes by intentionally creating scenarios where agents must balance stamina, battery, and delivery deadlines to confirm the prevention ratio and violation rate metrics accurately capture the reported struggles