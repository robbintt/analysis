---
ver: rpa2
title: 'Align to Structure: Aligning Large Language Models with Structural Information'
arxiv_id: '2504.03622'
source_url: https://arxiv.org/abs/2504.03622
tags:
- discourse
- text
- alignment
- reward
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Structural Alignment, a novel alignment
  framework that integrates linguistically grounded discourse structures into large
  language models to enhance long-form text generation. The approach employs two complementary
  reward models within a Proximal Policy Optimization framework: one scoring surface-level
  text structures for readability and another analyzing hierarchical discourse motifs
  to reinforce coherence.'
---

# Align to Structure: Aligning Large Language Models with Structural Information

## Quick Facts
- arXiv ID: 2504.03622
- Source URL: https://arxiv.org/abs/2504.03622
- Reference count: 10
- Models with structural alignment outperform standard RLHF baselines, generating more coherent and human-like essays and summaries

## Executive Summary
This paper introduces Structural Alignment, a novel alignment framework that integrates linguistically grounded discourse structures into large language models to enhance long-form text generation. The approach employs two complementary reward models within a Proximal Policy Optimization framework: one scoring surface-level text structures for readability and another analyzing hierarchical discourse motifs to reinforce coherence. A dense reward scheme is proposed to improve training stability by providing fine-grained, token-level rewards based on discourse distinctiveness. Experiments show that structurally aligned models outperform standard RLHF baselines, generating more coherent and human-like essays and summaries, with graph-level alignment yielding the strongest improvements in tasks like long-document summarization.

## Method Summary
The method uses PPO with two reward models: a surface-level LLM judge (72B) scoring flow, organization, and balance, and a graph-level scorer using RST parsing to extract discourse motifs and classify them via Longformer for "human-like" probability. Dense rewards (½ × num_tokens) are assigned to tokens contributing to human-distinctive motifs (MF-IDF threshold ≥1 std). The policy model is QWEN2-1.5B-INSTRUCT, trained on 26K essay prompts from PERSUADE corpus, CMV subreddit, and proficiency exams. The framework segments long documents at paragraph level for RST parsing and aggregates surface and graph scores for PPO updates.

## Key Results
- Structurally aligned models outperform standard RLHF baselines on ROUGE metrics for long-document summarization
- Graph-level alignment yields the strongest improvements, increasing human-distinctive motif usage while standard RLHF causes decline
- Dense token-level rewards improve training stability compared to sparse episodic rewards in long-context RL
- ROUGE scores achieved: R-1 55.86, R-2 21.72, R-L 52.81 on GOVREPORT dataset

## Why This Works (Mechanism)

### Mechanism 1
Dense, token-level reward signals may stabilize long-context RL training where sparse episodic rewards fail. The system assigns rewards to individual tokens if they contribute to "discourse motifs" (recurring rhetorical structures) found to be distinctively human-like via MF-IDF scoring. This shapes the credit assignment process, guiding the policy not just on *what* was said, but *how* it was structurally positioned. Core assumption: Tokens within specific Elementary Discourse Units (EDUs) can be mapped to high-level rhetorical structures (motifs), and that optimizing for these local structural signals aggregates into global coherence. Evidence: Section 3.2 describes assigning ½ × num_tokens rewards to tokens contributing to distinctive motifs to address "sparsity of rewards" in standard PPO. Break condition: If the RST parser fails to accurately segment long-form text into EDUs, the mapping between tokens and motifs becomes noisy, rendering the dense reward signal ineffective or misleading.

### Mechanism 2
Optimizing for hierarchical discourse motifs improves global coherence better than optimizing for surface-level text features alone. A Longformer-based classifier is trained to distinguish human-authored text from LLM-generated text based on distributions of hierarchical discourse motifs (extracted via RST). The probability of "human" classification serves as the reward signal, forcing the policy to adopt complex rhetorical strategies (e.g., "elaboration," "antithesis") rather than simple sequencing. Core assumption: The distribution of discourse motifs is a robust proxy for "human-like" rhetorical sophistication and global coherence. Evidence: Section 4.4 analysis shows graph-level alignment increases the proportion of human-distinctive motifs, while standard RLHF causes a decline. Break condition: If the classifier relies on spurious correlations (e.g., specific vocabulary rather than structure) to detect AI text, the policy may learn to game the classifier without improving structural coherence.

### Mechanism 3
Decoupling structural alignment into surface-level (readability) and graph-level (coherence) objectives allows for flexible, complementary optimization. The framework utilizes two distinct reward models: one uses a large LLM (72B) to score explicit structural features (flow, organization), the other uses the motif classifier. These act as complementary forces—one handling local logic, the other global structure. Core assumption: Surface markers (connectives, headings) and deep discourse structures are sufficiently independent to be optimized via separate or sequential reward signals without conflicting gradient updates. Evidence: Figure 5 shows that while two-stage alignment yields marginal gains, the low correlation between surface and graph scores suggests they capture different dimensions of quality. Break condition: If the policy capacity is too small (e.g., 1.5B parameters), it may struggle to simultaneously satisfy both reward dimensions, leading to mode collapse where the model prioritizes the easier reward (surface features) over the harder one (deep structure).

## Foundational Learning

- **Concept**: Rhetorical Structure Theory (RST)
  - **Why needed here**: RST is the theoretical backbone of the "Graph-Level" alignment. Understanding how text is segmented into Elementary Discourse Units (EDUs) and organized into nuclearity (nucleus vs. satellite) is required to debug the dense reward assignments.
  - **Quick check question**: Can you identify the nucleus and satellite in a sentence like "Although the model is accurate, it is slow"?

- **Concept**: Proximal Policy Optimization (PPO)
  - **Why needed here**: This is the optimization loop used. The paper specifically modifies the standard PPO reward tensor to include dense signals. You must understand the clipped surrogate objective to diagnose training stability issues.
  - **Quick check question**: What happens to the policy update if the advantage estimate $\hat{A}_t$ has high variance due to sparse rewards?

- **Concept**: MF-IDF (Motif Frequency-Inverse Document Frequency)
  - **Why needed here**: This metric determines *which* discourse motifs are considered "human-distinctive" and thus worthy of a dense reward. It is the filter for the reward signal.
  - **Quick check question**: If a motif appears frequently in both human and LLM-generated text, would its MF-IDF score mark it as distinctive?

## Architecture Onboarding

- **Component map**: Policy Model (QWEN2-1.5B) -> Text Structure Evaluator (QWEN2-72B) and DMRST Parser -> Motif Extractor -> Authorship Classifier (Longformer) -> PPO Update

- **Critical path**:
  1. Policy generates text (sample)
  2. Text routed to Evaluator (Surface Score) AND Parser (Graph path)
  3. Parser -> Trees -> Motifs -> Classifier (Graph Score)
  4. Motifs compared against pre-computed corpus stats to identify distinctive tokens
  5. Surface Score + Graph Score + Dense Token Rewards aggregated
  6. PPO update step

- **Design tradeoffs**:
  - **Parsing Segmentation**: The parser handles only ~512 tokens. Text must be segmented at the paragraph level. *Tradeoff*: This maintains parser accuracy but risks losing cross-paragraph discourse links (global context fragmentation)
  - **Reward Source**: Using a 72B model as a judge is accurate but computationally expensive (requires 8×A100 GPUs just for serving the judge)

- **Failure signatures**:
  - **"Connective Stuffing"**: If the Surface Reward dominates, the model may over-use transition words ("moreover," "therefore") without logical backing
  - **Segmentation Artifacts**: If paragraph segmentation is poor, the RST parser may produce malformed trees, causing the Authorship Classifier to output erratic rewards (high variance)

- **First 3 experiments**:
  1. **Reward Correlation Check**: Replicate Section 4.2. Verify that the 72B Evaluator's scores actually correlate with human judgment on a small held-out set before running full PPO
  2. **Ablation on Reward Density**: Train two models—one with episodic rewards only, one with the dense token rewards. Compare training curve stability (as hinted in Figure 8) to validate the core claim about stability
  3. **Parser Constraint Test**: Generate a 1500-token essay and inspect the RST parsing quality at the 512-token boundaries. If the parser fails to link ideas across boundaries, the global coherence reward may be optimizing for disjoint segments

## Open Questions the Paper Calls Out
- Does structural alignment scale effectively to models significantly larger than 1.5B parameters?
- What is the most effective strategy for combining surface-level and graph-level reward models to optimize long-form generation?
- How does structural alignment perform when implemented with alternative optimization techniques like Direct Preference Optimization (DPO) rather than PPO?

## Limitations
- The dense reward mechanism relies on MF-IDF to identify "human-distinctive" discourse motifs, which may not generalize across different domains
- The DMRST parser is limited to ~512 tokens, requiring paragraph-level segmentation that may fragment discourse structures spanning paragraphs
- Deploying a 72B frozen LLM judge and Longformer authorship classifier imposes significant computational overhead with 8×A100 GPUs required

## Confidence
- **High Confidence**: The claim that surface-level and graph-level rewards capture distinct dimensions of text quality (Section 4.2, Figure 5)
- **Medium Confidence**: The claim that dense token-level rewards stabilize PPO training (Figure 8 shows smoother training curves but lacks isolated ablations)
- **Low Confidence**: The claim that the authorship classifier robustly captures "human-like" discourse sophistication (paper doesn't validate against spurious feature reliance)

## Next Checks
1. **Cross-domain motif distinctiveness**: Retrain the authorship classifier on a different human corpus (e.g., news articles instead of essays) and test whether the set of "human-distinctive" motifs changes significantly
2. **Parser boundary analysis**: Generate long essays (>1000 tokens) and manually inspect RST parsing quality at the 512-token boundaries
3. **Spurious correlation test for authorship classifier**: Train a baseline classifier using only surface features (e.g., sentence length, punctuation frequency) and compare its performance to the motif-based classifier