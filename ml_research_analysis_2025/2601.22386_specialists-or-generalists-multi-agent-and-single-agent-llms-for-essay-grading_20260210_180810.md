---
ver: rpa2
title: Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading
arxiv_id: '2601.22386'
source_url: https://arxiv.org/abs/2601.22386
tags:
- multi-agent
- score
- essays
- single-agent
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares single-agent and multi-agent LLM architectures
  for automated essay grading using the ASAP 2.0 corpus. The multi-agent system uses
  three specialist agents (Content, Structure, Language) coordinated by a Chairman
  Agent with rubric-aligned veto and capping logic.
---

# Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading

## Quick Facts
- **arXiv ID**: 2601.22386
- **Source URL**: https://arxiv.org/abs/2601.22386
- **Reference count**: 9
- **Primary result**: Few-shot prompting improves QWK by ~26% for both architectures; multi-agent excels at weak essay detection, single-agent better on mid-range essays

## Executive Summary
This paper compares single-agent and multi-agent LLM architectures for automated essay grading using the ASAP 2.0 corpus. The multi-agent system uses three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent with rubric-aligned veto and capping logic. Both architectures were tested in zero-shot and few-shot conditions using GPT-5.1. Few-shot prompting significantly improved performance for both architectures (approximately 26% QWK improvement), emerging as the dominant factor in system performance. The multi-agent system excelled at identifying weak essays (73.3% accuracy for score 1 vs. 46.7% for single-agent), while the single-agent system performed better on mid-range essays. Both architectures struggled with high-quality essays. The study concludes that architectural choice should align with deployment priorities, with multi-agent AI suited for diagnostic screening of at-risk students and single-agent models offering cost-effective general assessment.

## Method Summary
The study compared single-agent and multi-agent LLM architectures for automated essay scoring on the ASAP 2.0 corpus. The single-agent approach used one GPT-5.1 call with rubric and calibration examples to produce a holistic score. The multi-agent system employed three specialist agents (Content, Structure, Language) each with dimension-specific negative constraints, coordinated by a Chairman Agent applying veto (any specialist scores 1 → final 1) and capping (any specialist scores 2 → final capped at 2-3) logic. Both architectures were tested in zero-shot and few-shot conditions, with few-shot using 2 calibration examples per score level (12 total, non-overlapping with test set). Primary evaluation used Quadratic Weighted Kappa (QWK) on a stratified 450-essay test set.

## Key Results
- Few-shot prompting improved QWK by ~26% for both architectures (single-agent: 26.5%, multi-agent: 25.96%)
- Multi-agent system achieved 73.3% accuracy on score-1 essays vs. 46.7% for single-agent
- Single-agent performed better on score-4 essays (61.2% accuracy vs. 57.3% for multi-agent)
- Both architectures struggled with high-quality essays (scores 5-6), showing systematic under-prediction
- Conservative bias from capping logic artificially suppressed scores for essays requiring nuanced trade-off assessment

## Why This Works (Mechanism)

### Mechanism 1: Few-Shot Calibration Anchors Grading Standards
Providing just 2 examples per score level dramatically improves performance across both architectures by grounding abstract scoring criteria in concrete instantiations. The model learns the implicit boundary conditions between adjacent scores that the rubric text alone fails to convey.

### Mechanism 2: Veto Logic Creates Conservative Score Floor
The multi-agent Chairman's veto rule (if any specialist scores 1, final score = 1) drives superior detection of critically weak essays by converting any single specialist's detection of critical failure into a hard constraint on the final score.

### Mechanism 3: Dimensional Isolation Enables Precision at Extremes, Constrains Mid-Range Judgment
Specialists develop heightened sensitivity to their assigned dimension by being explicitly instructed to ignore competing concerns. This sharpened attention improves detection at distribution extremes but degrades performance in the mid-range where quality involves subtle compensations across dimensions.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK)**: Primary evaluation metric measuring inter-rater agreement while penalizing large disagreements more heavily than small ones. *Quick check*: If a system predicts score 5 when human gave 1, QWK penalizes this more than predicting 4 when human gave 3.

- **Few-Shot Prompting vs. Zero-Shot**: Few-shot uses calibration examples to improve performance; the paper identifies this as the "dominant factor" in system performance. *Quick check*: What is the minimum number of examples per score level used, and what happens if you omit them?

- **Negative Constraint Prompting**: Technique where agents are explicitly told to ignore certain dimensions (e.g., "ignore grammar") to enforce specialist isolation. *Quick check*: Why would explicitly telling an agent to ignore grammar improve its assessment of structure?

## Architecture Onboarding

- **Component map**: Essay → Three parallel specialist calls (Content, Structure, Language) → Chairman Agent applies veto/capping logic → Final Score

- **Critical path**: 1) Curate calibration examples (2 per score level, n=12 total, no overlap with test) 2) Construct dimension-specific prompts with explicit negative constraints 3) Implement Chairman logic as conditional rules, not averaging 4) Validate on stratified test set with score-level breakdown

- **Design tradeoffs**:
  - Multi-agent: 4x API cost, ~4% relative QWK gain, +26.6 percentage points on score-1 detection, conservative bias on high scores, dimension-specific interpretability
  - Single-agent: 1x API cost, competitive mid-range performance, weaker at-risk detection, holistic-only rationale

- **Failure signatures**:
  - Under-prediction on score 5-6 essays (both architectures): systems assign 3-4 to essays humans rated 5-6
  - Score-6 sparsity: only 3 essays in test set (0.7%), making conclusions unreliable
  - Conservative bias from capping logic: a single specialist's concern can artificially suppress scores

- **First 3 experiments**:
  1. **Calibration sensitivity test**: Vary number of calibration examples (0, 1, 2, 5 per level) to validate ~26% improvement claim
  2. **Score-stratified error analysis**: Run both architectures on held-out set and compute confusion matrices by true score level
  3. **Ablation of veto/capping logic**: Run multi-agent system with simple averaging instead of rule-based aggregation

## Open Questions the Paper Calls Out
- **Generalizability**: Do single-agent and multi-agent architectures generalize to diverse datasets, genres, and student populations outside of U.S. secondary students? (Section 5.3)
- **Fairness**: Are certain student populations or writing styles systematically disadvantaged by the "conservative bias" and capping logic inherent in the multi-agent architecture? (Section 5.3)
- **Actionability**: Does the multi-agent system's dimension-specific feedback offer superior utility and actionability for students compared to single-agent rationales? (Section 5.3)

## Limitations
- Test set contains only 3 essays at score level 6 (0.7%), making high-quality essay performance conclusions unreliable
- Exclusive focus on single LLM (GPT-5.1) and one rubric (SAT-aligned) limits generalizability claims
- Veto/capping logic may introduce systematic bias that masks actual essay quality

## Confidence
- **High Confidence**: Few-shot prompting dramatically improves QWK for both architectures (26% improvement); multi-agent significantly outperforms single-agent on score-1 essay detection (73.3% vs 46.7% accuracy)
- **Medium Confidence**: Single-agent advantage on mid-range essays (scores 3-4) and both architectures' struggle with high-quality essays (5-6), given limited score-6 test data
- **Low Confidence**: Generalizability of architectural advantages across different rubrics, student populations, or LLM models

## Next Checks
1. **Score-level robustness test**: Replicate study with balanced test set containing ≥20 essays per score level (especially levels 5-6) to validate conclusions about high-quality essay performance
2. **Ablation of Chairman logic**: Compare multi-agent performance with and without veto/capping rules using same calibration and test sets to isolate architectural contribution to weak-essay detection
3. **Cross-rubric validation**: Apply both architectures to different essay scoring rubric (e.g., AP English) with same few-shot protocol to assess generalizability beyond SAT-aligned rubric used in this study