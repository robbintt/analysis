---
ver: rpa2
title: 'RECODE: Reasoning Through Code Generation for Visual Question Answering'
arxiv_id: '2510.13756'
source_url: https://arxiv.org/abs/2510.13756
tags:
- code
- image
- reasoning
- visual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECODE addresses the challenge of precise visual reasoning on structured
  infographics by reverse-engineering images into executable code, enabling verifiable
  perception and reasoning. It uses an agentic pipeline that generates multiple candidate
  programs, selects the most faithful reconstruction via a pixel-based critic (MSE),
  and iteratively refines the code to correct discrepancies.
---

# RECODE: Reasoning Through Code Generation for Visual Question Answering

## Quick Facts
- **arXiv ID**: 2510.13756
- **Source URL**: https://arxiv.org/abs/2510.13756
- **Reference count**: 40
- **Primary result**: RECODE achieves 77% accuracy on CharXiv-Reasoning (19% gain over direct prompting) and 93.2% on ChartQA.

## Executive Summary
RECODE addresses the challenge of precise visual reasoning on structured infographics by reverse-engineering images into executable code. The approach uses an agentic pipeline that generates multiple candidate programs, selects the most faithful reconstruction via a pixel-based critic (MSE), and iteratively refines the code to correct discrepancies. On CharXiv-Reasoning, RECODE achieves 77% accuracy, a 19% gain over direct prompting; on ChartQA, it reaches 93.2% accuracy, surpassing chart-pretrained models; and on Geometry3K, it attains 94.2% accuracy, significantly outperforming baselines. The method demonstrates that derendering into code, combined with iterative self-correction, provides a robust path to accurate, interpretable multimodal reasoning.

## Method Summary
RECODE transforms visual question answering into a code generation and verification task. The method takes an image and question as input, generates multiple candidate code programs to reproduce the visual, selects the best candidate using pixel-based MSE evaluation, and iteratively refines the code to correct discrepancies. Finally, it uses the refined code and original image to answer the question. The pipeline leverages OCR to extract text elements, hierarchical task decomposition for code generation, and a refinement loop that progressively improves code fidelity.

## Key Results
- On CharXiv-Reasoning, RECODE achieves 77% accuracy, a 19% gain over direct prompting.
- On ChartQA, RECODE reaches 93.2% accuracy, surpassing chart-pretrained models.
- On Geometry3K, RECODE attains 94.2% accuracy, significantly outperforming baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Derendering transforms ambiguous pixel-based perception into verifiable symbolic reasoning.
- Mechanism: By forcing the model to generate executable code that reproduces the visual, the agent externalizes its understanding into a structured representation. This code can be executed to produce a reconstruction that is directly comparable to the original image, creating a verifiable feedback signal. Once in code form, downstream reasoning leverages exact numerical values rather than visual estimation.
- Core assumption: MLLMs possess sufficient code generation capabilities to capture the generative logic of structured visuals (charts, diagrams).
- Evidence anchors:
  - [abstract] "This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on."
  - [Section 3.1, Table 1] Code-Only setting achieves 93% vs Image-Only 75% on CharXiv-Mini, demonstrating the reasoning advantage of symbolic representations.
- Break condition: If the visual contains elements that cannot be expressed in standard plotting libraries (e.g., hand-drawn annotations, complex 3D renderings), derendering fidelity degrades.

### Mechanism 2
- Claim: Pixel-based critic selection (MSE) provides a cost-effective signal for identifying the most faithful code reconstruction.
- Mechanism: Among multiple candidate code programs, the critic evaluates each by executing the code and computing a similarity metric between the rendered output and the original image. MSE directly measures pixel-level discrepancy, penalizing both semantic errors (wrong data) and stylistic mismatches.
- Core assumption: Lower MSE correlates with higher downstream reasoning accuracy.
- Evidence anchors:
  - [Section 3.2.2, Table 3] MSE achieves 92% accuracy on CharXiv-Mini, matching PSNR and outperforming embedding-based metrics (88-89%).
  - [Section 3.2.2] "MSE Strikes a Balance Between Efficiency and Effectiveness" — selected as default critic for simplicity and performance.
- Break condition: If stylistic differences (colors, line thickness) dominate semantic errors in pixel space, MSE may select candidates with correct style but incorrect data.

### Mechanism 3
- Claim: Iterative self-refinement progressively corrects perception errors by explicit discrepancy identification.
- Mechanism: After initial derendering, the agent compares the rendered image against the original, identifies specific discrepancies (e.g., misaligned labels, incorrect bar heights), and revises the code. This loop repeats, with each round producing higher-fidelity code.
- Core assumption: MLLMs can reliably identify discrepancies between two images when explicitly prompted.
- Evidence anchors:
  - [Section 3.2.3, Table 4] Two refinement rounds improve accuracy from 92% to 95% and reduce MSE from 2325 to 1913.
  - [Section 3.2.3] "This iterative correction is critical to capture fine-grained details and correct errors that one-shot code generation cannot resolve."
- Break condition: If discrepancies are too subtle for the model to verbalize (e.g., slight font differences), refinement may stall without meaningful improvement.

## Foundational Learning

- Concept: **Derendering / Code Generation for Visuals**
  - Why needed here: The core innovation requires understanding how to reverse-engineer visuals into executable programs. Without this, the entire pipeline fails at step one.
  - Quick check question: Given a simple bar chart image, can you write Python/matplotlib code that reproduces it?

- Concept: **Best-of-N Candidate Generation**
  - Why needed here: Single-shot code generation is unreliable; generating multiple candidates and selecting the best enables robustness.
  - Quick check question: Why might generating 5 candidate programs outperform generating 1, even with the same model?

- Concept: **Iterative Refinement with External Feedback**
  - Why needed here: One-shot derendering cannot capture all nuances; a feedback loop using rendered-vs-original comparison enables progressive improvement.
  - Quick check question: What information must be provided to the model in each refinement round?

## Architecture Onboarding

- Component map:
  - **OCR Module**: Extracts text (labels, values) from input image; uses pytesseract or Gemini.
  - **Multi-Candidate Generator**: Produces k candidate code programs using hierarchical task decomposition (subplot-level → component-level).
  - **Code Executor**: Runs generated Python code to produce rendered images.
  - **Critic (MSE)**: Computes pixel-wise similarity between original and rendered images; selects top candidate.
  - **Refinement Loop**: Prompts model with original image, best code, and rendered image to identify discrepancies and revise code.
  - **QA Module**: Uses final refined code + original image to answer the question.

- Critical path: Image → OCR → Code Generation (k candidates) → Execution → MSE Selection → Refinement (T rounds) → QA Answer

- Design tradeoffs:
  - **Critic choice**: MSE is cheap and effective; LLM-as-a-Judge achieves comparable accuracy but requires more API calls.
  - **Number of candidates (k)**: Paper uses k=5; more candidates increase coverage but linearly increase cost.
  - **Refinement rounds (T)**: Paper uses T=2; diminishing returns observed (Table 4).
  - **OCR tool**: pytesseract is free but less accurate; Gemini OCR is more accurate but requires API access.

- Failure signatures:
  - **Stochastic code**: Model generates `np.random` calls instead of hard-coded values; breaks determinism.
  - **Missing text elements**: OCR fails to extract labels/values; code lacks critical annotations.
  - **Style vs semantic confusion**: MSE penalizes stylistic differences (colors) that don't affect reasoning.
  - **Refinement plateau**: MSE stops improving but accuracy doesn't reach ground-truth code level (94%).

- First 3 experiments:
  1. **Proof-of-concept validation**: Replicate Table 1 on a small curated dataset (Image-Only vs Code-Only vs Image+Code) to confirm the reasoning advantage of code.
  2. **Critic ablation**: Compare MSE, SSIM, PSNR, and LLM-as-a-Judge on candidate selection; measure correlation with downstream QA accuracy.
  3. **Refinement scaling**: Sweep T ∈ {0, 1, 2, 3} and plot MSE vs QA accuracy to identify the point of diminishing returns.

## Open Questions the Paper Calls Out
- **Question**: Can reinforcement learning (RL) on collected agent trajectories optimize the derendering process more effectively than the current prompting-based approach?
- **Question**: Does optimizing for pixel-based fidelity metrics like Mean Squared Error (MSE) inadvertently penalize stylistic variations that do not affect semantic correctness?
- **Question**: Is the effectiveness of the RECODE pipeline fundamentally dependent on the proprietary capabilities of Gemini 2.5 Pro, or is it agnostic to the underlying MLLM?
- **Question**: Can the derendering approach scale to unstructured natural images where executable code representations (like Matplotlib) are ill-defined?

## Limitations
- The core mechanism relies on MLLMs' ability to generate deterministic, executable code that faithfully reproduces complex visual structures, untested for non-standard or hand-drawn elements.
- The pixel-based critic (MSE) assumes pixel similarity correlates with semantic correctness, but high MSE can result from harmless stylistic variations.
- The refinement loop's effectiveness depends on the model's ability to identify and verbalize discrepancies, which may fail for subtle errors.

## Confidence
- **High Confidence**: The reasoning advantage of symbolic code over direct image-based reasoning (Table 1, Code-Only vs Image-Only). The iterative refinement mechanism improves both MSE and accuracy (Table 4).
- **Medium Confidence**: The MSE critic's effectiveness in selecting faithful code candidates. While it outperforms other metrics in the paper, no ablation study compares it against semantic critics on downstream accuracy.
- **Low Confidence**: Generalization claims to arbitrary visual reasoning tasks. All results are on structured infographics; no evidence supports performance on unstructured or 3D visuals.

## Next Checks
1. **Critic Semantic Validation**: Replace MSE with a semantic critic (e.g., CLIP-score) on a subset of CharXiv-Reasoning. Measure correlation between critic score and downstream QA accuracy to test whether pixel similarity proxies semantic correctness.
2. **Generalization Stress Test**: Apply RECODE to a dataset of unstructured or hand-drawn diagrams (e.g., educational sketches from online forums). Measure performance degradation and identify failure modes (e.g., missing text, complex shapes).
3. **Determinism Audit**: Generate 100 independent runs of RECODE on the same CharXiv-Reasoning sample. Measure variance in final code, rendered images, and QA accuracy to quantify the stochasticity of the pipeline and identify sources of irreproducibility.