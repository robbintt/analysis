---
ver: rpa2
title: Performance Consistency of Learning Methods for Information Retrieval Tasks
arxiv_id: '2509.20804'
source_url: https://arxiv.org/abs/2509.20804
tags:
- random
- methods
- performance
- seeds
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the stability of machine learning models
  for information retrieval tasks when different random seeds are used during training.
  The authors evaluate nine models (including traditional statistical methods and
  transformer-based approaches) across three tasks: sentiment analysis, fake news
  detection, and binary question answering.'
---

# Performance Consistency of Learning Methods for Information Retrieval Tasks

## Quick Facts
- arXiv ID: 2509.20804
- Source URL: https://arxiv.org/abs/2509.20804
- Authors: Meng Yuan; Justin Zobel
- Reference count: 14
- Key outcome: Transformer models show extreme sensitivity to random seeds, with F1-score standard deviations exceeding 0.075 in 9 out of 11 cases

## Executive Summary
This study investigates how random seed choices affect machine learning model performance in information retrieval tasks. The authors evaluate nine models (traditional statistical and transformer-based) across sentiment analysis, fake news detection, and binary question answering. They find that transformer models exhibit dramatically higher performance variance when random seeds are changed compared to traditional methods, with F1-score standard deviations exceeding 0.075 in most cases versus under 0.01 for statistical models. The study uses bootstrapping to distinguish between test-set sampling variability and training-induced variance.

## Method Summary
The authors evaluate nine machine learning models across three IR tasks using 10 different random seeds per model-task combination. They employ bootstrapping of test sets to estimate natural variability independent of training randomness. Models include traditional statistical approaches (SVM, logistic regression, Naive Bayes) and transformer-based architectures (BERT, RoBERTa, XLNet, BART, T5). Performance metrics include F1-score, precision, and recall. The study compares observed variance from seed changes against bootstrap variance to quantify the relative contribution of training instability versus test-set sampling uncertainty.

## Key Results
- Transformer models show F1-score standard deviations over 0.075 in 9 out of 11 cases, compared to under 0.01 for traditional models
- Precision standard deviations exceed 0.125 in 7 out of 11 transformer cases versus under 0.02 for traditional methods
- T5 demonstrates relative stability (σ_F1 = 0.020) compared to other transformers like BART (σ_F1 = 0.210)
- Bootstrapping variance remains consistently low (~0.01-0.02) across all models, confirming training-induced rather than test-set variability

## Why This Works (Mechanism)

### Mechanism 1
Random seed variation propagates through multiple training pipeline components, causing amplified performance variance in transformer models. The seed influences weight initialization, dropout layer behavior, training data ordering, and GPU-level scheduling simultaneously. These coupled sources of randomness compound during optimization, creating divergent convergence paths to different local minima. The observed variance stems from seed-dependent factors rather than implementation bugs or hardware non-determinism alone.

### Mechanism 2
Test-set bootstrapping captures natural data variability independent of training randomness, providing a baseline for comparison. Sampling test instances with replacement creates alternative test distributions while keeping the trained model fixed. The resulting variance reflects measurement uncertainty from finite test sets, not training instability. Bootstrap variance approximates the true sampling distribution of test performance metrics.

### Mechanism 3
Architectural complexity correlates with seed sensitivity—encoder-decoder models show different stability profiles than encoder-only transformers. T5's relative stability (σ_F1 = 0.020) versus BART's instability (σ_F1 = 0.210) suggests architectural choices (text-to-text framing, pre-training scale) modulate how seed-induced noise propagates through optimization landscapes. The observed T5 stability is not task-specific but reflects architectural properties.

## Foundational Learning

- Concept: **Bootstrapping for variance estimation**
  - Why needed here: Distinguishes test-set sampling uncertainty from training-induced variance; critical for interpreting whether seed effects are practically significant
  - Quick check question: If bootstrap σ = 0.01 and seed-run σ = 0.15, which source dominates measurement uncertainty?

- Concept: **Optimization landscape and local minima**
  - Why needed here: Explains why small seed-induced perturbations lead to divergent final models—different initialization/ordering pushes optimization toward different basins
  - Quick check question: Why might a wider, flatter loss basin produce more stable results across seeds?

- Concept: **Statistical significance vs. practical significance**
  - Why needed here: Reported improvements of 0.008-0.02 F1 are meaningless when σ > 0.075; need to understand confidence intervals
  - Quick check question: With σ_F1 = 0.08, what minimum mean difference would you need to claim one method outperforms another at 95% confidence?

## Architecture Onboarding

- Component map: Random seed → [Data loader shuffling] → Training batch order → [Weight init] → Starting point in loss landscape → [Dropout mask] → Regularization stochasticity → [GPU kernel scheduling] → Non-deterministic operations → [Optimizer state] → Momentum/Adam terms

- Critical path: Seed → Weight initialization → Early gradient updates → Final converged weights → Inference predictions. The first few epochs likely determine which basin the model enters.

- Design tradeoffs:
  - Averaging over 5-10 seeds vs. 100 seeds: More seeds improve variance estimates but multiply compute costs
  - Reporting mean±std vs. median±IQR: Robust to outliers but less familiar to readers
  - Fixing seed for reproducibility vs. random sampling: Ensures exact replication but masks instability

- Failure signatures:
  - σ_F1 > 0.05: High instability—treat single-run results as unreliable
  - Outliers beyond 1.5×IQR: Some seeds produce catastrophically bad models (see Figure 1, XLNet range 0.26-0.92)
  - Bootstrap σ ≈ seed σ: Problem likely in test data, not training

- First 3 experiments:
  1. Run your existing model with 10 different seeds on the same data split. Compute σ for your primary metric. If σ > 0.02, you must report variance.
  2. Add bootstrapping on test predictions (100 resamples). Compare bootstrap σ to seed σ to quantify training vs. measurement uncertainty.
  3. If using transformers, run with frozen seed across all components (torch, numpy, CUDA) to verify infrastructure determinism before attributing variance to the model.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope to only three specific IR tasks with relatively small datasets
- Fixed model hyperparameters may mask stability differences from different training regimes
- Does not systematically verify whether variance persists across different hardware or frameworks

## Confidence

*High Confidence:* The core finding that transformer models show substantially higher seed sensitivity than traditional models (σ_F1 > 0.075 vs σ_F1 < 0.01) is well-supported by extensive empirical evidence across multiple datasets and metrics.

*Medium Confidence:* The architectural explanations for why T5 is more stable than other transformers are plausible but not definitively proven.

*Low Confidence:* The claim that reported performance differences of 0.008-0.02 F1 are "meaningless" when σ > 0.075 may overstate the practical implications.

## Next Checks

1. **Cross-domain validation**: Test the seed sensitivity findings on at least two additional IR tasks (e.g., document classification, relevance ranking) using datasets with different characteristics to assess generalizability beyond the original three tasks.

2. **Hyperparameter sensitivity analysis**: Systematically vary key hyperparameters (learning rate, batch size, number of epochs) for the most unstable transformer models to determine whether seed-induced variance is consistent across training configurations.

3. **Implementation verification**: Replicate the experiments using different deep learning frameworks (TensorFlow/PyTorch) and hardware configurations to distinguish between true model instability and framework/hardware-induced non-determinism.