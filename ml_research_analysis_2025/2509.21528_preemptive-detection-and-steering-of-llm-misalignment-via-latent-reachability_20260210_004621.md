---
ver: rpa2
title: Preemptive Detection and Steering of LLM Misalignment via Latent Reachability
arxiv_id: '2509.21528'
source_url: https://arxiv.org/abs/2509.21528
tags:
- safety
- unsafe
- alignment
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BRT-ALIGN, a reachability-based framework
  for runtime detection and steering of unsafe LLM outputs. It models autoregressive
  generation as a dynamical system in latent space, learns a safety value function
  via backward reachability, and uses this to both monitor for harmful completions
  several tokens in advance and minimally perturb embeddings to steer away from unsafe
  trajectories.
---

# Preemptive Detection and Steering of LLM Misalignment via Latent Reachability

## Quick Facts
- **arXiv ID:** 2509.21528
- **Source URL:** https://arxiv.org/abs/2509.21528
- **Reference count:** 17
- **One-line primary result:** BRT-ALIGN achieves F1 scores of 0.73–0.91 for early detection of unsafe LLM outputs and reduces unsafe generations by up to 85% with 2–4× faster inference than gradient-based steering.

## Executive Summary
This paper introduces BRT-ALIGN, a reachability-based framework for runtime detection and steering of unsafe LLM outputs. It models autoregressive generation as a dynamical system in latent space, learns a safety value function via backward reachability, and uses this to both monitor for harmful completions several tokens in advance and minimally perturb embeddings to steer away from unsafe trajectories. Experiments across five LLMs and three toxicity benchmarks show that BRT-ALIGN significantly improves early and accurate detection of unsafe continuations compared to prior control-theoretic baselines.

## Method Summary
BRT-ALIGN extracts layer-20 embeddings from LLM outputs and trains a safety value network (MLP) to predict whether a trajectory will lead to toxic content. The value function is trained using either supervised learning (SAMPLE-BRT-ALIGN) or reinforcement learning with Bellman updates (RL-BRT-ALIGN). During monitoring, the system flags unsafe trajectories when the value function falls below a threshold α. During steering, it minimally perturbs embeddings via random sampling from an L2 ball to maximize safety while preserving coherence. The approach leverages backward reachability analysis to detect hazards several tokens before they appear textually.

## Key Results
- BRT-ALIGN achieves F1 scores of 0.73–0.91 across five LLMs and three toxicity benchmarks
- Early detection capability: flags unsafe content 7–10 tokens in advance (First-Token Index)
- Steering mode reduces unsafe generations by up to 85% while maintaining coherence
- Inference speeds 2–4× faster than gradient-based steering baselines

## Why This Works (Mechanism)

### Mechanism 1: Preemptive Hazard Prediction via Backward Reachability
The framework predicts unsafe outputs several tokens before they appear textually by identifying the "inevitability" of a failure state. BRT-ALIGN computes a "safety value function" V(z_t) that determines if the current latent state z_t falls within the Backward Reachable Tube (BRT)—the set of all states that evolve into a "failure set" under the LLM's dynamics. If V(z_t) ≤ 0, the system flags the trajectory as unsafe immediately.

### Mechanism 2: Least-Restrictive Steering Filter
When monitoring detects an unsafe trajectory (V(z_t) ≤ α), the system applies a control input u_t to the latent state using random search rather than gradient descent. It draws random perturbations ε from an L2-norm ball and selects the one that maximizes the safety value V(z_t + ε). If V(z_t) > α, it does nothing (u_t = 0), maintaining output coherence by intervening only when necessary.

### Mechanism 3: Temporal Credit Assignment via Bellman Updates
RL-based training (RL-BRT-ALIGN) provides better early detection than single-step supervision by propagating risk backward through generation steps. The value function is trained using a Bellman recursion where the "reward" is the proximity to the failure set. This forces the model to learn that a seemingly safe state at time t is actually unsafe if it leads to a failure state at t+5.

## Foundational Learning

- **Concept: Backward Reachable Tube (BRT)**
  - **Why needed here:** This is the core mathematical object defining "risk." Unlike a static safety boundary, the BRT expands backward in time. Understanding this is crucial to distinguishing this method from simple output filtering.
  - **Quick check question:** Does the BRT represent the set of *currently unsafe* tokens, or the set of *currently safe* tokens that will inevitably lead to unsafe ones?

- **Concept: Latent Space Dynamics (Control Theory View)**
  - **Why needed here:** The paper treats LLM generation not as text prediction, but as a trajectory z_0 → z_1 → ... → z_T. Understanding this abstraction is necessary to see why "steering" a vector works to change text.
  - **Quick check question:** In this paper's formulation, does the system state grow with context length (key-value cache) or remain fixed dimension?

- **Concept: Least-Restrictive Control**
  - **Why needed here:** This explains the trade-off between safety and utility. The goal is *minimal* intervention.
  - **Quick check question:** If the safety value V(z_t) is high (indicating safety), what is the magnitude of the control input u_t applied by BRT-ALIGN?

## Architecture Onboarding

- **Component map:** Frozen LLM -> Layer-20 embedding extraction -> Safety Value Network (MLP) -> Steering Filter (sampling logic) -> LLM input perturbation
- **Critical path:** Training the Safety Value Network. This requires generating a dataset of (prompt, response) pairs, embedding them, labeling them with the target function, and training the MLP via Bellman updates (RL-BRT-ALIGN) or direct supervision (SAMPLE-BRT-ALIGN).
- **Design tradeoffs:**
  - Layer Selection: Uses Layer 20; earlier layers may be too abstract; later layers may be too token-specific
  - RL vs. Sample Training: RL-BRT is harder to train but offers earlier detection; Sample-BRT is easier but less temporally aware
  - Sampling Radius (R): Too small = fails to escape unsafe regions; too large = destroys coherence
- **Failure signatures:**
  - Skewed Monitoring: SAP baseline shows 100% true positives but ~0% true negatives (too conservative). RE-CONTROL shows 0% true positives (too optimistic). BRT-ALIGN seeks a balance (F1 0.73–0.91).
  - Coherence Collapse: If steering perturbation is too large, the text becomes gibberish (seen in Ministral-8B results where coherence dropped).
- **First 3 experiments:**
  1. Dataset Construction: Generate completions for BeaverTails prompts, embed the last token at Layer 20, and label trajectories using the RoBERTa toxicity classifier.
  2. Ablation on Value Learning: Train two MLPs—one using Bellman updates (RL) and one using only terminal labels—to verify that RL enables earlier detection (lower First-Token Index).
  3. Steering Efficiency Check: On a held-out toxic prompt set, compare the inference time and "Safety Rate" of BRT-ALIGN (sampling-based steering) against RE-CONTROL (gradient-based steering) to validate the 2–4× speedup claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the BRT-ALIGN framework effectively handle multi-dimensional notions of harm (e.g., misinformation, self-harm, jailbreaks) beyond the single-dimensional offensive language classification currently implemented?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "LLM misalignment spans many other forms of harm... extending it to richer and multi-dimensional notions of harm remains essential."
- **Why unresolved:** The current instantiation relies solely on a binary offensive language classifier (CardiffNLP RoBERTa) to define the failure set.
- **What evidence would resolve it:** Successful application of the framework using composite or multi-label target functions on datasets covering diverse harm categories like medical misinformation or adversarial attacks.

### Open Question 2
- **Question:** Can advanced control strategies, such as smooth blending filters or control barrier functions, mitigate the semantic coherence trade-offs observed in specific model architectures?
- **Basis in paper:** [explicit] The authors note reduced semantic coherence for certain models (e.g., Ministral-8B) and suggest exploring "more advanced control-theoretic strategies—such as smooth blending filters or control barrier functions" in future work.
- **Why unresolved:** The current least-restrictive filter (LRF) applies perturbations that sometimes degrade coherence, and the suggested advanced methods have not yet been integrated or tested.
- **What evidence would resolve it:** Experiments showing that CBT-based or blended steering maintains or improves coherence scores while preserving the high safety rates of the baseline LRF method.

### Open Question 3
- **Question:** How does the assumption of deterministic, greedy decoding affect the reliability of the safety value function when LLMs generate text using stochastic sampling methods?
- **Basis in paper:** [inferred] Section 3 states, "we assume that LLM deterministically selects the most likely token... removing the stochasticity... yielding deterministic latent dynamics," which simplifies analysis but diverges from typical inference usage (e.g., temperature > 0).
- **Why unresolved:** The backward reachable tube is computed for deterministic dynamics; stochastic sampling introduces noise (ω_t) that the current value function V(z_t) does not explicitly account for.
- **What evidence would resolve it:** An analysis of monitoring and steering performance when generation temperature is varied, or an extension of the value function formulation to include stochastic disturbance bounds.

## Limitations
- Unexplored failure modes in open-ended generation: Validated primarily on curated toxicity benchmarks that don't capture gradual toxicity emergence over extended dialogues
- Latent space dynamics assumption: Models generation as deterministic despite real LLMs using stochastic sampling and having context-dependent attention dynamics
- Scalability and efficiency concerns: Sampling-based steering requires searching high-dimensional L2 balls without specifying sample count or computational overhead

## Confidence

**High confidence:** The core monitoring mechanism (backward reachability for preemptive detection) is well-supported by the theoretical framework and experimental results. The F1 scores of 0.73-0.91 across five LLMs and three benchmarks demonstrate reliable performance. The speedup claims (2-4× over gradient-based methods) are plausible given the sampling-based approach versus iterative optimization.

**Medium confidence:** The steering effectiveness (85% reduction in unsafe outputs) is demonstrated but relies on careful hyperparameter tuning (α, R values per model). The paper shows these parameters are crucial but does not provide a systematic method for setting them on new models or domains. The coherence preservation claims are supported but show model-dependent variation (Ministral-8B coherence drops to 0.37).

**Low confidence:** The Bellman recursion training methodology is described but not fully validated. The paper claims RL-BRT-ALIGN provides earlier detection than SAMPLE-BRT-ALIGN, but the exact training procedure for obtaining V(z_{t+1}) during Bellman updates is unclear. The sampling-based steering's sample efficiency and convergence properties are not characterized.

## Next Checks

**Validation Check 1: Open-ended generation robustness.** Evaluate BRT-ALIGN on long-form conversational datasets (e.g., OpenSubtitles, Reddit conversations) where toxicity emerges over extended dialogues. Measure not just safety rate but also whether steering introduces new failure modes like topic drift, repetition, or semantic incoherence that weren't present in the original benchmarks.

**Validation Check 2: Ablation on sampling strategy.** Systematically vary the number of samples drawn from the L2 ball during steering and measure the trade-off between safety improvement and inference time. Compare uniform sampling versus importance sampling based on the gradient direction. Report the 90th percentile inference time across different embedding dimensions to characterize worst-case overhead.

**Validation Check 3: Cross-model transferability.** Train the value function on one LLM (e.g., Llama2-7b) and evaluate monitoring/steering performance on a different architecture (e.g., Mistral or GPT-Neo). This would test whether the latent dynamics assumption holds across model families and whether the approach requires per-model retraining or can generalize.