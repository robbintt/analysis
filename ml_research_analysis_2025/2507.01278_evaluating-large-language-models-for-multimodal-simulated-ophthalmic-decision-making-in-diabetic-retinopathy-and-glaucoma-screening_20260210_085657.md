---
ver: rpa2
title: Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making
  in Diabetic Retinopathy and Glaucoma Screening
arxiv_id: '2507.01278'
source_url: https://arxiv.org/abs/2507.01278
tags:
- metadata
- referral
- clinical
- diabetic
- glaucoma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated GPT-4\u2019s ability to simulate clinical\
  \ reasoning in ophthalmology using structured textual descriptions of retinal fundus\
  \ photographs. The model was tasked with assigning International Clinical Diabetic\
  \ Retinopathy (ICDR) scores, recommending diabetic retinopathy (DR) and glaucoma\
  \ referrals, under three metadata conditions: image-only, real patient data, and\
  \ synthetic data."
---

# Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening

## Quick Facts
- arXiv ID: 2507.01278
- Source URL: https://arxiv.org/abs/2507.01278
- Reference count: 22
- GPT-4 shows moderate accuracy for ICDR classification (67.5%) but poor performance for glaucoma referral (F1 < 0.04)

## Executive Summary
This study evaluates GPT-4's ability to simulate clinical reasoning in ophthalmology using structured textual descriptions of retinal fundus photographs. The model was tasked with classifying diabetic retinopathy severity and making referral recommendations under three metadata conditions. While performance was moderate for binary diabetic retinopathy referral (82.3% accuracy), it was poor for glaucoma screening (F1 < 0.04). Metadata inclusion did not significantly affect predictions, suggesting model outputs were driven primarily by image descriptions rather than clinical context.

## Method Summary
The study used GPT-4 to evaluate 300 retinal fundus images from the mBRSET dataset, providing structured textual descriptions of findings. Three tasks were assessed: International Clinical Diabetic Retinopathy (ICDR) scoring (5-class), binary diabetic retinopathy referral (ICDR ≥ 2 or macular edema), and binary glaucoma referral (cup-to-disc ratio > 0.6). Performance was measured across three metadata conditions (image-only, real patient data, synthetic data) using accuracy, F1 scores, Cohen's kappa, and McNemar's test. The model was accessed via ChatGPT API with default settings and no memory between prompts.

## Key Results
- ICDR classification accuracy: 67.5% (F1 0.33 macro, 0.67 weighted)
- Binary DR referral accuracy: 82.3% (F1 0.54)
- Glaucoma referral accuracy: ~78% (F1 < 0.04, kappa < 0.03)
- Metadata inclusion showed no significant effect on predictions (McNemar p > 0.05)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Binary classification tasks yield higher reliability than multi-class grading when LLMs simulate clinical decisions from structured text.
- **Mechanism:** Simplifying the output space reduces the burden on pattern matching within the model's pre-trained knowledge. The model maps textual descriptors to coarser decision boundaries rather than fine-grained distinctions between adjacent severity levels.
- **Core assumption:** The model's internal representations for medical concepts are sufficiently discriminative for coarse categorization but lack precision for ordinal differentiation.
- **Evidence anchors:**
  - [abstract]: "Performance was moderate for ICDR classification (accuracy 67.5%, F1 0.33) and improved for binary DR referral (accuracy 82.3%, F1 0.54)"
  - [results]: "Reframing the task to binary referral... substantially improved model performance compared to the multiclass ICDR task"
  - [corpus]: Related work on multimodal LLMs in ophthalmology (arXiv:2509.13234) similarly notes that minimal outputs may limit clinical utility, suggesting task complexity remains a broader challenge.
- **Break condition:** If the textual descriptions lack sufficient granularity to distinguish between adjacent severity levels (e.g., mild vs. moderate NPDR), the mechanism degrades.

### Mechanism 2
- **Claim:** LLM predictions are driven primarily by textual image descriptions rather than structured patient metadata.
- **Mechanism:** GPT-4 processes the descriptive content as the primary signal source, treating metadata as peripheral context. Without explicit training to weight metadata integration, the model defaults to pattern matching on the most information-dense portion of the prompt—the image description.
- **Core assumption:** The model lacks architectures for dynamic context integration that would allow metadata to modulate decision thresholds.
- **Evidence anchors:**
  - [abstract]: "Metadata inclusion did not significantly affect outcomes (McNemar p > 0.05)"
  - [discussion]: "This indicates that the model's outputs are primarily driven by the descriptive content of the image-based prompts rather than the structured clinical context"
  - [corpus]: Weak comparative evidence—neighboring papers focus on multimodal vision-language models rather than text-only approaches with metadata.
- **Break condition:** If metadata were formatted as explicit conditional logic (e.g., "if age > 60, increase suspicion threshold"), the mechanism might shift, but this was not tested.

### Mechanism 3
- **Claim:** Tasks requiring fine-grained spatial reasoning from textual descriptions remain unreliable for current LLMs.
- **Mechanism:** Translating subjective visual assessments (e.g., cup-to-disc ratio) into text loses spatial precision. The model must infer ratios from qualitative descriptors, introducing compounding approximation errors.
- **Core assumption:** The textual descriptions provided to the model do not encode sufficient spatial information for precise quantitative estimation.
- **Evidence anchors:**
  - [abstract]: "glaucoma referral (accuracy ~78%, F1 <0.04, kappa <0.03)"
  - [discussion]: "This likely reflects the inherent difficulty of translating subjective, fine-grained spatial assessments of the optic nerve into textual descriptions"
  - [corpus]: Related multimodal benchmarks (arXiv:2503.07094, arXiv:2505.19624) evaluate vision-language models on similar tasks, suggesting spatial reasoning remains an active research frontier.
- **Break condition:** If descriptions included precise numerical measurements (e.g., "vertical CDR = 0.7") rather than qualitative assessments, performance might improve.

## Foundational Learning

- **Concept: Cohen's Kappa (Inter-rater Reliability)**
  - **Why needed here:** The paper uses kappa to measure agreement between GPT-4 and ground truth beyond chance. Understanding that kappa = 0.25 indicates "fair" agreement while kappa = 0.44 indicates "moderate" agreement is essential for interpreting results.
  - **Quick check question:** If a model achieves 80% accuracy on a dataset where 78% of cases are negative, would you expect high or low kappa?

- **Concept: McNemar's Test for Paired Categorical Data**
  - **Why needed here:** The study uses McNemar's test to determine whether adding metadata significantly changes model predictions. A p-value > 0.05 indicates no statistically significant difference.
  - **Quick check question:** What does it mean when McNemar's p = 1.0 comparing image-only vs. metadata predictions?

- **Concept: Class Imbalance and Macro vs. Weighted F1**
  - **Why needed here:** The dataset was 74.72% normal cases. Macro F1 (0.33) treats all classes equally, while weighted F1 (0.67) accounts for class prevalence—explaining the large gap.
  - **Quick check question:** Why does the model achieve higher weighted F1 than macro F1 for ICDR classification?

## Architecture Onboarding

- **Component map:**
  ```
  Input: Structured text prompt → [Image description + optional metadata]
  Model: GPT-4 (transformer-based autoregressive LLM)
  Tasks: ICDR scoring (5-class) | DR referral (binary) | Glaucoma referral (binary)
  Output: Structured response with scores, findings, and recommendations
  Evaluation: Accuracy, F1 (macro/weighted), Cohen's kappa, McNemar's test
  ```

- **Critical path:** Prompt design → Textual description quality → Model inference → Output parsing → Metric calculation. The prompt template (Appendix) explicitly instructs the model on decision criteria, making prompt engineering the highest-leverage component.

- **Design tradeoffs:**
  - Text-only vs. multimodal: Using textual descriptions instead of direct image input enables evaluation of language-based reasoning but loses spatial information.
  - Static vs. dynamic prompts: Each prompt processed independently (no memory) ensures replicability but prevents context accumulation across cases.
  - Default temperature: Using default sampling parameters prioritizes reproducibility over output diversity.

- **Failure signatures:**
  - F1 = 0.00 for specific classes (e.g., mild NPDR, severe NPDR) indicates the model never correctly predicted these classes—likely due to class imbalance and feature overlap.
  - Kappa near 0 with high accuracy (glaucoma: 78% accuracy, kappa < 0.03) suggests the model is predicting the majority class without learning discriminative patterns.
  - Metadata showing no effect (McNemar p = 1.0) indicates the model is not integrating contextual information.

- **First 3 experiments:**
  1. **Prompt ablation:** Test whether explicit conditional logic (e.g., "if patient age > 60 and hypertension = yes, consider higher pretest probability") changes metadata utilization.
  2. **Description granularity sweep:** Provide progressively more detailed textual descriptions (qualitative → semi-quantitative → quantitative measurements) to identify the threshold where glaucoma referral improves.
  3. **Class-balanced subset:** Evaluate on a rebalanced dataset with equal class distribution to distinguish model capacity limitations from class imbalance effects.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can native multimodal models that process pixel data directly outperform text-based simulation for diabetic retinopathy grading?
  - **Basis in paper:** [explicit] The authors state in the Discussion that "Future work should explore the development of truly multimodal AI systems that combine the visual processing capabilities of image-based models with the reasoning abilities of LLMs."
  - **Why unresolved:** This study evaluated GPT-4, which operates exclusively on textual inputs and lacks the ability to process raw image pixels, creating an inherent information bottleneck.
  - **What evidence would resolve it:** A comparative study evaluating the performance of multimodal LLMs (e.g., GPT-4o) against text-only simulations on the same image set.

- **Open Question 2:** Why does GPT-4 fail to incorporate clinical metadata into its ophthalmic decision-making predictions?
  - **Basis in paper:** [explicit] The authors note that the model has a "limited capacity to dynamically integrate structured patient-specific metadata into its predictions," leading to results driven by image descriptions rather than context.
  - **Why unresolved:** It is unclear if this is a limitation of the model's attention mechanism or the specific prompt design, as the model did not statistically change outputs when provided with real versus synthetic metadata.
  - **What evidence would resolve it:** Ablation studies using prompts that explicitly force the model to reference specific metadata points in its reasoning chain before generating a conclusion.

- **Open Question 3:** Can improved prompt granularity overcome the LLM's poor performance in estimating cup-to-disc ratios for glaucoma screening?
  - **Basis in paper:** [inferred] The paper attributes poor glaucoma performance to the "inherent difficulty of translating subjective, fine-grained spatial assessments... into textual descriptions," suggesting a potential limitation of the input modality.
  - **Why unresolved:** It remains unknown if the failure is due to the fundamental inability of LLMs to handle spatial concepts or simply insufficient detail in the structured prompts provided.
  - **What evidence would resolve it:** Testing model performance using varying levels of descriptive specificity regarding optic disc features to see if a threshold of detail yields valid clinical predictions.

## Limitations
- Heavy class imbalance (74.7% normal ICDR) inflates accuracy metrics while obscuring performance on rare but clinically significant cases.
- The source and quality of textual image descriptions remain unspecified, creating potential reproducibility gaps.
- Metadata showed no significant effect on predictions, but this may reflect inadequate prompt engineering rather than true irrelevance of contextual information.

## Confidence
- **High confidence:** Binary DR referral performance (82.3% accuracy, 0.54 F1) reflects genuine model capability for coarse-grained clinical decisions from structured text.
- **Medium confidence:** ICDR classification results (67.5% accuracy, 0.33 F1) are reliable for the specific textual description format used, but generalizability to other description styles is uncertain.
- **Low confidence:** Glaucoma referral performance conclusions, given the extreme F1 (<0.04) and kappa (<0.03) values, may reflect dataset-specific factors rather than fundamental LLM limitations for spatial reasoning.

## Next Checks
1. **Description quality audit:** Compare model performance using original human-generated descriptions versus standardized structured reports with quantitative measurements to isolate the impact of description format on glaucoma referral accuracy.

2. **Metadata integration test:** Modify prompts to explicitly incorporate metadata as conditional decision factors (e.g., "Given patient age > 60 and hypertension, adjust suspicion threshold upward") and measure changes in metadata utilization.

3. **Balanced subset evaluation:** Train and evaluate the model on a class-balanced subset of the data to distinguish whether poor minority class performance stems from class imbalance versus fundamental model limitations in recognizing subtle clinical features.