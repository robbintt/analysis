---
ver: rpa2
title: 'MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference'
arxiv_id: '2511.06010'
source_url: https://arxiv.org/abs/2511.06010
tags:
- shared
- attention
- data
- unique
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoSKA introduces Mixture of Shared KV Attention to solve the KV
  cache bottleneck in long-sequence LLM inference by differentiating between unique
  and massively reused shared context data. Its core innovation is Shared KV Attention,
  which converts attention on shared data from memory-bound GEMV operations into compute-bound
  GEMM operations by batching concurrent requests.
---

# MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference

## Quick Facts
- arXiv ID: 2511.06010
- Source URL: https://arxiv.org/abs/2511.06010
- Reference count: 14
- MoSKA achieves up to 538.7× throughput gains by solving KV cache memory bandwidth bottleneck through shared context batching

## Executive Summary
MoSKA addresses the critical bottleneck in long-sequence LLM inference where KV cache memory bandwidth limits performance. The key insight is that in many real-world applications, a large portion of context is shared across multiple inference requests while only a small portion is unique to each request. By distinguishing between these two types of context and processing them differently, MoSKA converts memory-bound operations into compute-bound operations through intelligent batching of shared context requests.

The architecture introduces Shared KV Attention, which leverages the observation that shared context queries can be computed once and reused across multiple requests. This enables conversion from GEMV (memory-bound) to GEMM (compute-bound) operations when sufficient concurrent requests are available. The system uses MoE-inspired sparse routing to direct queries to either unique or shared attention mechanisms, while a disaggregated infrastructure separates the processing of unique and shared data to optimize resource utilization.

## Method Summary
MoSKA introduces Mixture of Shared KV Attention (MoSKA) to solve the KV cache bottleneck in long-sequence LLM inference by differentiating between unique and massively reused shared context data. Its core innovation is Shared KV Attention, which converts attention on shared data from memory-bound GEMV operations into compute-bound GEMM operations by batching concurrent requests. This is supported by MoE-inspired sparse routing and a disaggregated infrastructure specialized for unique vs. shared data. Evaluated analytically under realistic conditions (Llama 3.1 8B, FP8, 75% sparsity, up to 16M shared tokens), MoSKA achieved throughput gains up to 538.7× over baselines by solving memory bandwidth scaling while enabling efficient, large-scale shared context serving.

## Key Results
- Achieved up to 538.7× throughput improvement over baselines
- Maintains similar latency to vanilla KV attention for unique data
- Effectively solves memory bandwidth bottleneck through shared context batching
- Demonstrates scalability to 16M shared tokens with 75% sparsity assumption

## Why This Works (Mechanism)
MoSKA works by recognizing that in many LLM applications, a large portion of the context is shared across multiple inference requests (like knowledge bases or common documents), while only a small portion is unique to each request. By processing shared and unique contexts differently, the system can batch multiple requests together for shared context computation, converting memory-bound operations into compute-bound operations that better utilize modern hardware.

The key mechanism is the conversion from GEMV (Generalized Matrix-Vector multiplication) to GEMM (Generalized Matrix-Matrix multiplication) for shared context. When multiple requests share the same context, their query vectors can be stacked into a matrix and processed together, amortizing the memory access cost across many computations. The MoE-inspired routing ensures that each query is directed to the appropriate attention mechanism based on whether it needs unique or shared context.

## Foundational Learning
**Attention Mechanism**: Why needed - Core operation in transformers for capturing relationships between tokens; Quick check - Verify self-attention computation matches standard formulation with Q, K, V matrices.
**KV Cache**: Why needed - Stores key and value vectors to avoid recomputation during autoregressive generation; Quick check - Confirm cache size scales linearly with sequence length and batch size.
**Memory Bandwidth vs Compute Bound**: Why needed - Understanding hardware limitations is crucial for optimization; Quick check - Benchmark GEMV vs GEMM performance on target hardware to verify theoretical predictions.
**Sparsity in Attention**: Why needed - Many real-world attention patterns have significant redundancy that can be exploited; Quick check - Analyze actual attention patterns in target workloads to validate sparsity assumptions.
**MoE Routing**: Why needed - Enables conditional computation paths based on input characteristics; Quick check - Verify routing mechanism correctly directs queries to appropriate attention paths with minimal overhead.

## Architecture Onboarding

**Component Map**: Input Queries -> MoE Router -> Unique Attention (KV cache) + Shared Attention (Batched GEMM) -> Output

**Critical Path**: Query → MoE Router → Attention Computation → Output
The critical path involves routing each query to determine whether it needs unique or shared attention processing. For shared attention, multiple queries are batched together to maximize GEMM efficiency. The system must maintain consistency between unique and shared attention outputs while minimizing synchronization overhead.

**Design Tradeoffs**: The primary tradeoff is between batching efficiency and latency. Larger batches improve throughput for shared attention but increase latency for individual requests. The disaggregated architecture reduces memory contention but introduces coordination overhead. The 75% sparsity assumption optimizes for specific workload patterns but may not generalize to all applications.

**Failure Signatures**: 
- Degraded performance when shared context sparsity drops below threshold
- Increased latency spikes during bursty request patterns
- Memory consistency issues in disaggregated cache
- Routing errors causing incorrect attention computations

**First Experiments**:
1. Measure baseline KV cache bandwidth utilization under various sequence lengths
2. Characterize actual shared context patterns in target workloads
3. Benchmark GEMM performance with different batch sizes to find optimal configuration

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalization of its approach to different workload patterns and the optimal configuration of the disaggregated infrastructure. Specifically, it questions how the system performs under varying degrees of shared context and how to dynamically adjust the routing mechanism based on runtime conditions.

## Limitations
- Strong assumptions about 75% sparsity may not hold across all applications
- Disaggregated architecture introduces significant system complexity
- Limited evaluation of tail latency impacts on interactive applications
- Potential consistency challenges during concurrent access to shared KV cache

## Confidence
- **High Confidence**: The fundamental observation that KV cache memory bandwidth becomes the bottleneck for long-sequence inference is well-established in the literature and the mathematical derivation of shared attention computation is sound.
- **Medium Confidence**: The proposed routing mechanism and sparse attention patterns appear technically valid, but their effectiveness depends heavily on the assumed workload characteristics which may not generalize across all use cases.
- **Medium Confidence**: The analytical throughput gains are compelling but represent idealized conditions that may be challenging to achieve in real-world deployment scenarios with varying request patterns and infrastructure constraints.

## Next Checks
1. **Sparsity Sensitivity Analysis**: Conduct experiments measuring throughput and latency across a range of sparsity levels (50-90%) to understand performance sensitivity to the key assumption about shared context distribution.

2. **Tail Latency Characterization**: Implement and measure end-to-end latency distributions for both batched and single-request scenarios to quantify the trade-offs between throughput optimization and response time guarantees.

3. **Concurrent Access Consistency**: Develop and evaluate mechanisms for maintaining consistency and handling race conditions in the disaggregated shared KV cache when serving multiple concurrent requests with overlapping but non-identical shared contexts.