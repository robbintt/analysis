---
ver: rpa2
title: 'HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers'
arxiv_id: '2509.02837'
source_url: https://arxiv.org/abs/2509.02837
tags:
- labeled
- information
- arxiv
- unlabeled
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of combining evidence from heterogeneous
  sources (labeled and unlabeled data) for fact verification in retrieval augmented
  generation (RAG). It proposes a hierarchical fusion-based RAG approach (HF-RAG)
  that first aggregates top-documents from multiple rankers within each source using
  reciprocal rank fusion (RRF), then standardizes the retrieval score distributions
  within each source using z-score transformation, and finally merges the top-retrieved
  documents from the two sources.
---

# HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers

## Quick Facts
- **arXiv ID:** 2509.02837
- **Source URL:** https://arxiv.org/abs/2509.02837
- **Reference count:** 40
- **Primary result:** HF-RAG consistently outperforms single-ranker, single-source RAG baselines and improves out-of-domain generalization on Climate-FEVER and SciFact datasets

## Executive Summary
This paper introduces HF-RAG, a hierarchical fusion-based RAG approach that combines evidence from labeled (FEVER training set) and unlabeled (Wikipedia) sources using multiple heterogeneous rankers. The method first aggregates top-documents from multiple rankers within each source using reciprocal rank fusion (RRF), then standardizes retrieval score distributions within each source using z-score transformation, and finally merges the top-retrieved documents from the two sources. The approach achieves better macro F1 scores than parametric baselines (RoBERTa, LoRA) and non-parametric RAG-based methods, particularly in out-of-domain settings.

## Method Summary
HF-RAG employs a two-stage fusion architecture: first aggregating outputs from four diverse rankers (BM25, Contriever, ColBERT, MonoT5) within each source using RRF, then standardizing these RRF scores with z-score normalization before merging across sources. The labeled source (L-RAG) uses FEVER training data while the unlabeled source (U-RAG) uses Wikipedia. The final merged context is fed to an LLM (LLaMA 2.0 or Mistral) for fact verification. The approach outperforms single-source and single-ranker baselines, with z-score transformation proving more effective than simple linear interpolation.

## Key Results
- HF-RAG consistently outperforms single-ranker, single-source RAG configurations
- Achieves better out-of-domain generalization on Climate-FEVER and SciFact datasets
- Ablation study shows z-score transformation is more effective than simple linear combination of labeled and unlabeled sources
- Robust to changes in the number of retrieved examples

## Why This Works (Mechanism)

### Mechanism 1: Intra-Source Rank Aggregation
- **Claim:** Aggregating outputs of multiple diverse IR models reduces retrieval error variance compared to single rankers
- **Mechanism:** HF-RAG uses Reciprocal Rank Fusion (RRF) to merge top-k lists from BM25, Contriever, ColBERT, and MonoT5, prioritizing documents consistently ranked highly across different relevance signals
- **Core assumption:** Retrieval models make independent errors; consensus across heterogeneous models indicates true relevance
- **Evidence anchors:** Abstract states RRF improves RAG effectiveness; section 2 describes RRF application per source; related work notes single re-rankers have specific vulnerabilities

### Mechanism 2: Inter-Source Score Standardization
- **Claim:** Z-score transformation enables fair comparison of documents from heterogeneous sources with incomparable raw score distributions
- **Mechanism:** Raw RRF scores from labeled (L-RAG) and unlabeled (U-RAG) sources cannot be directly compared due to differing collection statistics; z-score normalization maps scores to standard normal distribution N(0,1)
- **Core assumption:** Document scores within each source list follow a distribution that can be adequately standardized by mean and variance
- **Evidence anchors:** Abstract mentions z-score standardization before merging; section 2 describes z-score transformation removing collection-specific bias

### Mechanism 3: Complementary Contextual Grounding
- **Claim:** Combining labeled data (task format) and unlabeled data (broad knowledge) improves out-of-domain generalization
- **Mechanism:** L-RAG provides task-specific grounding (what "refute" looks like), while U-RAG provides factual evidence; prevents overfitting to training label idiosyncrasies
- **Core assumption:** Optimal solutions require both knowing how to answer (labeled) and what the facts are (unlabeled)
- **Evidence anchors:** Section 1 explains L-RAG/U-RAG complementary roles; section 4 shows HF-RAG relies more on external knowledge sources for SciFact due to domain shift

## Foundational Learning

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** Mathematical glue binding multiple rankers; understanding RRF is essential for debugging fused lists
  - **Quick check question:** If Document A is Rank 1 in List 1 but missing from List 2, and Document B is Rank 5 in both lists, which has higher RRF score? (Answer: Doc A wins slightly since $1/61 > 1/65 + 1/65$)

- **Concept: Z-score Normalization**
  - **Why needed here:** Enables inter-source merge by making incomparable score distributions comparable
  - **Quick check question:** Document with raw score 0.8 from Source A (Mean 0.9, StdDev 0.05) vs. 0.4 from Source B (Mean 0.3, StdDev 0.2). Which selected? (Answer: Source B doc is +0.5 std dev above mean, Source A is -2 std dev below; Source B selected)

- **Concept: In-Context Learning (L-RAG) vs. Parametric Knowledge**
  - **Why needed here:** Paper reframes In-Context Learning as "Labeled RAG"; understanding distinction is vital for prompt engineering
  - **Quick check question:** If only L-RAG context provided, what specific capability is missing? (Answer: Access to external, updated, or specific factual evidence not in training set examples)

## Architecture Onboarding

- **Component map:** Retrievers (BM25, Contriever, ColBERT, MonoT5) → Intra-Source Fusers (RRF L-RAG, RRF U-RAG) → Standardizer (Z-score) → Merger (top-k selection) → Generator (LLM)
- **Critical path:** Retrieval → RRF (L+U) → Z-Score → Merge; bottleneck likely multi-retriever ensemble latency, especially cross-encoder (MonoT5)
- **Design tradeoffs:**
  - Precision vs. Latency: 4 retrievers increase robustness but add latency; tradeoff exists between fusion robustness and single dense retriever speed
  - Source Bias: Z-score assumes relative rank value; if L-RAG uniformly low quality, z-score may force poor documents into context
- **Failure signatures:**
  - Score Collapse: One source returns uniformly irrelevant (low variance); z-score inflates "best of worst" scores, polluting context
  - Context Window Overflow: Merging top-k from two sources may exceed LLM window or dilute attention
- **First 3 experiments:**
  1. Ablate Rankers: HF-RAG with only BM25+Contriever vs. full ensemble; verify if cross-encoder necessary for F1 gains
  2. Score Distribution Analysis: Visualize RRF score histograms for L-RAG vs. U-RAG; confirm different means/variances justifying z-score over linear interpolation
  3. Context Composition Audit: For error case, check merged context; did z-score favor L-RAG when U-RAG facts needed?

## Open Questions the Paper Calls Out
- **Multi-agent extension:** The authors plan to extend hierarchical fusion to multi-agent RAG with a reasoner component (e.g., search-R1), but current implementation is single-agent without reasoning module
- **Task generalization:** Whether HF-RAG framework generalizes beyond fact verification to other KILT benchmark tasks like open-domain QA
- **Distribution assumptions:** How sensitive z-score fusion is to violations of Gaussian distribution assumptions for retrieval scores; paper assumes Gaussian but doesn't empirically validate for specific rankers

## Limitations
- Z-score effectiveness depends on distributional assumptions that may not hold if score distributions are skewed or contain outliers
- Paper doesn't explore latency implications of using four retrievers and complex fusion operations
- Robustness to systematic biases across rankers is not explicitly tested; RRF may reinforce errors if all rankers share same failure mode

## Confidence
- **High confidence:** Empirical improvements over single-ranker, single-source baselines on FEVER, Climate-FEVER, and SciFact are well-supported by reported macro F1 scores
- **Medium confidence:** Mechanism explanation for z-score superiority relies on distributional assumptions that are reasonable but not explicitly validated
- **Medium confidence:** Claim that labeled data provides task format while unlabeled provides factual grounding is supported by qualitative reasoning but could benefit from more granular ablation studies

## Next Checks
1. **Ablate Rankers:** Run HF-RAG with only BM25+Contriever (fast) vs. full ensemble; verify if cross-encoder necessary for reported F1 gains or if lighter fusion suffices
2. **Score Distribution Analysis:** Visualize RRF score histograms for L-RAG vs. U-RAG on validation set; confirm different means/variances justifying z-score approach over simple linear interpolation
3. **Context Composition Audit:** For specific error case, check merged context; did z-score favor L-RAG (task examples) when U-RAG (facts) was needed?