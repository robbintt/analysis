---
ver: rpa2
title: Enhancing Health Fact-Checking with LLM-Generated Synthetic Data
arxiv_id: '2508.20525'
source_url: https://arxiv.org/abs/2508.20525
tags:
- data
- synthetic
- fact
- original
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited annotated data for
  health-related fact-checking. The authors propose an LLM-driven synthetic data generation
  pipeline that creates sentence-fact entailment tables from summarized documents
  and uses them to generate labeled text-claim pairs.
---

# Enhancing Health Fact-Checking with LLM-Generated Synthetic Data

## Quick Facts
- **arXiv ID**: 2508.20525
- **Source URL**: https://arxiv.org/abs/2508.20525
- **Reference count**: 12
- **Primary result**: Synthetic data pipeline improves F1 scores by up to 0.049 on SciFact dataset

## Executive Summary
This paper addresses the challenge of limited annotated data for health-related fact-checking by proposing an LLM-driven synthetic data generation pipeline. The approach creates sentence-fact entailment tables from summarized documents and uses them to generate labeled text-claim pairs. Evaluated on PubHealth and SciFact datasets, the pipeline demonstrates modest but consistent improvements in fact-checking accuracy, with F1 score gains of up to 0.049. The method also shows promise for detecting hallucinations in LLM-generated text.

## Method Summary
The authors develop a pipeline that first summarizes health-related documents using an LLM, then generates entailment tables mapping sentences to facts within those summaries. These tables are used to create labeled text-claim pairs for training fact-checking models. The synthetic data is combined with original training data to enhance model performance. The approach leverages LLM capabilities for both summarization and entailment generation, creating a scalable method for augmenting limited health fact-checking datasets.

## Key Results
- F1 score improvements of up to 0.019 on PubHealth dataset
- F1 score improvements of up to 0.049 on SciFact dataset
- Pilot study shows effectiveness in detecting hallucinations in LLM-generated text
- Largest improvements observed on SciFact, which contains shorter, more structured scientific abstracts

## Why This Works (Mechanism)
The pipeline works by addressing the fundamental data scarcity problem in health fact-checking through automated synthetic data generation. By leveraging LLMs to summarize documents and generate entailment relationships, the method creates a scalable way to produce labeled training examples. The approach is particularly effective on structured scientific abstracts (like SciFact) where document structure is predictable, allowing the entailment generation to be more accurate. The combination of summarization and entailment table creation creates a rich source of training signals that complement the limited original data.

## Foundational Learning
- **Entailment detection**: Understanding whether a claim is supported by evidence - needed to train fact-checking models, check by verifying models can correctly identify entailment relationships in validation sets
- **LLM summarization**: Extracting key information from long documents - needed to create manageable input for entailment generation, check by measuring ROUGE scores of generated summaries
- **Synthetic data generation**: Creating artificial training examples - needed to augment limited labeled datasets, check by comparing model performance with and without synthetic data
- **Health misinformation patterns**: Recognizing common health claim structures - needed to evaluate domain-specific effectiveness, check by analyzing error patterns on different claim types
- **Hallucination detection**: Identifying false or unsupported content in LLM outputs - needed for quality control of synthetic data, check by measuring precision/recall on manually annotated samples
- **Sentence-fact alignment**: Mapping specific sentences to supporting facts - needed for creating training pairs, check by evaluating alignment accuracy on held-out data

## Architecture Onboarding

**Component Map**
LLM Summarization -> Entailment Table Generation -> Labeled Pair Creation -> Model Training

**Critical Path**
The pipeline follows a sequential flow where document summarization is the first bottleneck, followed by entailment table generation, then labeled pair creation. Each stage depends on the successful completion of the previous one, with LLM summarization being the most computationally intensive step.

**Design Tradeoffs**
The approach trades computational cost and potential noise introduction for scalability in data generation. Using LLM-generated summaries introduces the risk of omitting critical details, but enables processing of large document collections. The method prioritizes quantity and diversity of training examples over perfect quality of individual synthetic samples.

**Failure Signatures**
- Poor summarization quality leading to incomplete entailment tables
- Incorrect entailment relationships generating misleading training labels
- Domain mismatch between summary content and original document facts
- Overfitting to synthetic data patterns at the expense of generalization
- Hallucination propagation from LLM outputs to training data

**3 First Experiments**
1. Compare fact-checking model performance with synthetic data augmentation versus original data only on PubHealth dataset
2. Evaluate entailment generation accuracy on a held-out set of manually annotated summaries
3. Test hallucination detection precision on a small set of LLM-generated summaries with known errors

## Open Questions the Paper Calls Out
None

## Limitations
- Modest F1 score improvements suggest incremental rather than transformative gains
- Limited evaluation dataset sizes (1,000 claims for PubHealth, 1,409 for SciFact) raise questions about scalability
- Dependence on summarization models introduces potential noise from omitted critical details
- Pilot hallucination detection study lacks detailed error analysis and false positive/negative rates

## Confidence
**Medium** for the synthetic data pipeline's effectiveness in improving fact-checking accuracy, based on consistent but modest F1 gains. **Low** for the hallucination detection results, given the limited evaluation scope and lack of detailed error analysis.

## Next Checks
1. Evaluate the pipeline on a larger, more diverse health misinformation dataset (e.g., social media claims) to test generalizability.
2. Conduct a comprehensive error analysis on hallucination detection, including false positive/negative rates and impact on downstream fact-checking.
3. Test the robustness of the approach with different summarization models to assess sensitivity to summary quality.