---
ver: rpa2
title: 'COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models
  with Provable Risk Guarantees'
arxiv_id: '2506.20178'
source_url: https://arxiv.org/abs/2506.20178
tags:
- uni00000013
- uni00000011
- uni00000010
- uni00000018
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles uncertainty quantification for foundation models
  to control hallucinations in question answering with provable false discovery rate
  (FDR) guarantees. The proposed COIN framework calibrates uncertainty thresholds
  using split conformal prediction and confidence interval methods (Clopper-Pearson
  or Hoeffding) to filter answers while ensuring FDR control at user-specified risk
  levels.
---

# COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees

## Quick Facts
- arXiv ID: 2506.20178
- Source URL: https://arxiv.org/abs/2506.20178
- Authors: Zhiyuan Wang; Jinhao Duan; Qingni Wang; Xiaofeng Zhu; Tianlong Chen; Xiaoshuang Shi; Kaidi Xu
- Reference count: 40
- Key outcome: COIN provides provable FDR control for foundation models in QA by calibrating uncertainty thresholds using split conformal prediction and confidence intervals.

## Executive Summary
This paper addresses the critical challenge of controlling hallucinations in foundation model question answering through a provable selective prediction framework called COIN. The method guarantees False Discovery Rate (FDR) control at user-specified risk levels by constructing high-probability upper bounds on the true conditional failure rate using split conformal prediction. Through modular design integrating uncertainty quantification with statistical guarantees, COIN significantly outperforms prior methods in balancing risk control with answer retention across multiple QA benchmarks.

## Method Summary
COIN operates in three stages: (1) Error Analysis - computes empirical error rates on calibration data for candidate uncertainty thresholds; (2) Bound Construction - applies Clopper-Pearson or Hoeffding confidence intervals to establish high-probability upper bounds on the true failure rate; (3) Threshold Selection - selects the largest threshold satisfying the risk constraint. The framework uses different uncertainty quantification methods (predictive entropy for closed-ended, semantic entropy for open-domain) and can be extended to other tasks through its modular architecture. The statistical guarantees hold under the assumption that calibration and test data are i.i.d. samples from the same distribution.

## Key Results
- COIN achieves strict FDR control below user-specified risk levels (α=0.15, 0.20, 0.25) across CommonsenseQA, TriviaQA, and MMVet datasets
- Clopper-Pearson bound construction provides tighter FDR control and higher power compared to Hoeffding bounds
- COIN maintains robust performance with limited calibration data (1:9 split ratio) and varying sampling strategies
- Modular design enables flexible integration of different uncertainty quantification methods while preserving statistical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COIN provides provable FDR control by constructing high-probability upper bounds on the true conditional failure rate through split conformal prediction.
- Mechanism: A held-out calibration set quantifies empirical error rates at candidate thresholds; Clopper-Pearson or Hoeffding bounds then convert these finite-sample observations into statistically valid upper bounds with confidence ≥1−δ, enabling threshold selection that guarantees test-time risk control.
- Core assumption: Calibration and test samples are i.i.d. draws from the same underlying distribution; uncertainty scores are deterministic functions of inputs.
- Evidence anchors: Abstract states "COIN estimates the empirical error rate on a calibration set and applies confidence interval methods such as Clopper–Pearson to establish a high-probability upper bound on the true error rate (i.e., FDR)." Lemma 1 establishes Bernoulli property of correctness indicators under conditional selection.
- Break condition: Distribution shift between calibration and test; non-i.i.d. sampling; or uncertainty scores that do not deterministically depend on inputs.

### Mechanism 2
- Claim: Selecting the largest threshold satisfying the risk constraint maximizes sample retention while maintaining FDR guarantees.
- Mechanism: The TCFR is typically non-decreasing with respect to the uncertainty threshold; by exploiting this monotonicity, COIN identifies the supremum threshold that ensures the upper bound remains below α.
- Core assumption: Monotonic relationship between uncertainty threshold and conditional failure rate holds empirically.
- Evidence anchors: Abstract mentions "selection of the largest uncertainty threshold that ensures FDR control" to increase sample retention. Equation 15 states the guarantee Pr(R(t̂) ≤ α) ≥ 1−δ.
- Break condition: Non-monotonic TCFR behavior or multiple hypothesis testing without proper correction.

### Mechanism 3
- Claim: Modular architecture enables independent optimization of uncertainty quantification, bound construction, and threshold selection stages without breaking statistical guarantees.
- Mechanism: Three-stage design treats each component as interchangeable; different UQ methods and bound types can be substituted while preserving the PAC-style risk guarantee.
- Core assumption: Substituted components satisfy the same statistical requirements (uncertainty scores correlate with correctness; bounds provide valid coverage).
- Evidence anchors: Abstract highlights "flexible integration of different uncertainty quantification and bound construction methods." Figure 15 shows power improvements when swapping UQ methods while maintaining FDR control.
- Break condition: Substituted UQ method produces miscalibrated scores or bound construction violates coverage requirements.

## Foundational Learning

- Concept: Split Conformal Prediction (SCP)
  - Why needed here: COIN builds on SCP's core idea—reserving a calibration set to convert heuristic uncertainty into rigorous statistical guarantees. Understanding exchangeability, prediction sets, and coverage properties is essential to interpret why the FDR guarantee holds.
  - Quick check question: Given a calibration set of 1000 samples with 50 errors at threshold t=0.3, what does a 95% Clopper-Pearson upper bound tell you about the true failure rate?

- Concept: Binomial Confidence Intervals (Clopper-Pearson, Hoeffding)
  - Why needed here: Stage 2 constructs upper bounds on TCFR using exact (CP) or concentration-based (Hoeffding) methods. Knowing when each is appropriate—exactness vs. computational cost—directly impacts deployment decisions.
  - Quick check question: If you observe 5 failures in 50 trials, which bound (CP or Hoeffding) would be tighter at confidence level 0.99, and why might you choose the looser one anyway?

- Concept: False Discovery Rate (FDR) and Selective Prediction
  - Why needed here: The paper's core guarantee is FDR control among selected answers. Understanding the difference between per-sample error rate and FDR, plus the power-coverage trade-off, clarifies the objective function.
  - Quick check question: In a test set of 1000 questions, if COIN selects 400 answers with target FDR α=0.1, what is the maximum expected number of incorrect selections?

## Architecture Onboarding

- Component map:
  Calibration data → Model generates answers → UQ module computes uncertainty scores → Filter by candidate threshold → Count errors → Apply CP/Hoeffding bound → Binary search for max threshold → Test-time: Compute U → Compare to calibrated threshold → Accept/Reject

- Critical path:
  1. Calibration data quality (i.i.d. representative of test distribution)
  2. UQ method reliability (scores must correlate with correctness)
  3. Bound computation accuracy (correct BetaInv or Hoeffding implementation)
  4. Threshold monotonicity validation (verify R̂^upper_{1−δ}(t) is non-decreasing)

- Design tradeoffs:
  - CP vs. Hoeffding: CP provides exact coverage but requires Beta quantile computation (slower for many threshold evaluations); Hoeffding is closed-form but looser, reducing power
  - Calibration set size: Larger → tighter bounds → higher power; but reduces test data availability
  - Sampling budget (black-box UQ): More samples → better uncertainty estimates; power stability down to 10 samples for semantic entropy

- Failure signatures:
  - ECFR consistently above target α: Likely distribution shift or miscalibrated UQ; inspect calibration-test overlap
  - Near-zero power despite low α: Overly conservative bounds or poor UQ calibration
  - High variance in ECFR across trials: Insufficient calibration data; increase set size or use ensemble bounds
  - Monotonicity violation in R̂^upper_{1−δ}(t): Numerical instability or edge cases (zero selections); add regularization

- First 3 experiments:
  1. Replicate FDR control (Figure 3/4): Run COIN-CP on CommonsenseQA with 50/50 split, verify ECFR ≤ α across α ∈ {0.15, 0.20, 0.25} for 100 trials
  2. Compare CP vs. Hoeffding power (Figure 5): On TriviaQA with Qwen2.5-7B, measure power difference at fixed α; quantify computational cost ratio
  3. Sensitivity to calibration ratio (Figure 6): Test 1:9, 3:7, 1:1 splits on CommonsenseQA; report ECFR variance and power degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the COIN framework maintain its FDR guarantees under distribution shift between calibration and test data?
- Basis in paper: The methodology relies on i.i.d. assumption between calibration and test data (Section 3.1)
- Why unresolved: Paper validates on specific datasets but doesn't evaluate performance under distribution shift
- What evidence would resolve it: Experiments with synthetic distribution shifts or out-of-domain test sets to measure FDR upper bound violations

### Open Question 2
- Question: How can COIN be extended to long-form or open-ended generation tasks where correctness is not strictly binary?
- Basis in paper: Authors state "We hope COIN can be further applied to other downstream tasks" but current methodology relies on binary admission indicator
- Why unresolved: Current statistical formulation assumes Bernoulli distribution which may not capture partial correctness
- What evidence would resolve it: Extension to continuous correctness metrics and validation on long-form generation benchmarks

### Open Question 3
- Question: Is there a principled, automatic method for selecting the optimal uncertainty quantification strategy to maximize power?
- Basis in paper: Abstract highlights modularity and Figure 15 shows different UQ methods yield different power results
- Why unresolved: Framework treats UQ method as plug-in without guidance on optimal selection
- What evidence would resolve it: Theoretical analysis or empirical study correlating UQ properties with resulting power

### Open Question 4
- Question: Is COIN robust to adversarial inputs designed to manipulate uncertainty estimates and bypass the risk filter?
- Basis in paper: Introduction identifies "reliable deployment in risk-sensitive domains" as motivation; method depends on heuristic UQ methods
- Why unresolved: Paper evaluates natural hallucinations but not security against adversarial attacks
- What evidence would resolve it: Evaluation under adversarial attack scenarios to determine if FDR guarantees hold

## Limitations
- Strong i.i.d. assumption required between calibration and test sets; distribution shift breaks statistical guarantees
- Performance heavily dependent on quality of uncertainty quantification - poorly calibrated scores degrade performance
- Open-domain question handling relies on Sentence-BERT similarity thresholds and unspecified NLI models, not fully characterized

## Confidence

- **High Confidence**: FDR control mechanism through split conformal prediction and binomial confidence intervals is well-established theoretically. Mathematical framework is sound and follows standard PAC-style guarantees. Empirical FDR control across multiple datasets provides strong validation.
- **Medium Confidence**: Power gains depend on specific uncertainty quantification methods and may not generalize uniformly. Superiority of Clopper-Pearson over Hoeffding bounds is context-dependent. Robustness claims to limited calibration data hold within tested ranges.
- **Low Confidence**: Handling of open-domain questions using semantic entropy relies on Sentence-BERT similarity thresholds and unspecified NLI models. Performance on multimodal datasets is limited by small number of experiments shown.

## Next Checks

1. **Distribution Shift Stress Test**: Evaluate COIN's FDR control and power when calibration and test sets are drawn from different domains. Measure ECFR violation rates and power degradation across 10 random domain shift configurations.

2. **Uncertainty Calibration Analysis**: Systematically vary the Sentence-BERT similarity threshold (0.6 to 0.9) for open-domain correctness and measure its impact on FDR control and power. Include analysis of calibration curve quality for both predictive entropy and semantic entropy methods.

3. **Cross-Model Transferability**: Apply COIN thresholds calibrated on one model (e.g., LLaMA-3.1-8B) to test sets from different models (e.g., Qwen2.5-7B) without recalibration. Quantify performance drop in both FDR control and power to assess sensitivity to model-specific uncertainty calibration.