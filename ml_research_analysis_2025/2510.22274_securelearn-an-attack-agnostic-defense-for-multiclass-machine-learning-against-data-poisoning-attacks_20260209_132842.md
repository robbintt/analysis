---
ver: rpa2
title: SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against
  Data Poisoning Attacks
arxiv_id: '2510.22274'
source_url: https://arxiv.org/abs/2510.22274
tags:
- data
- poisoning
- attacks
- securelearn
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SecureLearn is an attack-agnostic defense framework designed to\
  \ protect multiclass machine learning models from data poisoning attacks. It combines\
  \ data sanitization\u2014using nearest neighbor voting and statistical deviation\
  \ detection\u2014with a novel feature-oriented adversarial training (FORT) method\
  \ that leverages feature importance scores to generate adversarial examples."
---

# SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks

## Quick Facts
- arXiv ID: 2510.22274
- Source URL: https://arxiv.org/abs/2510.22274
- Authors: Anum Paracha; Junaid Arshad; Mohamed Ben Farah; Khalid Ismail
- Reference count: 40
- Maintains accuracy >90% and recall/F1 >75% against data poisoning attacks

## Executive Summary
SecureLearn is an attack-agnostic defense framework designed to protect multiclass machine learning models from data poisoning attacks. It combines data sanitization—using nearest neighbor voting and statistical deviation detection—with a novel feature-oriented adversarial training (FORT) method that leverages feature importance scores to generate adversarial examples. Evaluated across four ML algorithms (RF, DT, GNB, MLP) on three datasets (IRIS, MNIST, USPS) against three poisoning attacks (OOP, SubP, RLPA), SecureLearn maintained accuracy above 90% and recall/F1-score above 75% across all models. For neural networks, recall and F1-score reached at least 97%. The approach reduced false discovery rates to 0.06 and improved adversarial robustness with only a 3% accuracy trade-off.

## Method Summary
SecureLearn employs a two-layer defense: first, data sanitization removes poisoned samples using k-NN relabeling (k=7, γ≥40%) and z-score outlier detection (|g|=3); second, feature-oriented adversarial training (FORT) generates perturbations based on feature importance scores to augment the sanitized dataset. The approach trains models on this combined set to achieve robustness against various poisoning attacks without requiring gradient-based optimization.

## Key Results
- Maintained accuracy above 90% and recall/F1-score above 75% across all models
- For neural networks, recall and F1-score reached at least 97%
- Reduced false discovery rates to 0.06
- Improved adversarial robustness with only a 3% accuracy trade-off
- Outperformed existing defenses and generalized across different attack vectors and model types

## Why This Works (Mechanism)

### Mechanism 1: k-NN Voting-Based Label Correction
- Corrects mislabeled training points via neighbor consensus before model training
- For each data point, computes confidence from k=7 nearest neighbors; relabels if ≥40% disagree
- Assumes poisoned points are locally inconsistent with their assigned labels

### Mechanism 2: Statistical Deviation Detection for Outlier Removal
- Removes points with extreme deviation from normalized dataset distribution
- Uses z-score filtering with |g|=3 threshold to target outlier-oriented poisoning
- Assumes poisoned points exhibit statistical abnormality in feature space

### Mechanism 3: Feature-Oriented Adversarial Training (FORT)
- Perturbs high-importance features near decision boundaries to widen classification margins
- Generates perturbations using feature importance scores without gradient-based optimization
- Assumes features with high importance are primary attack targets

## Foundational Learning

- **Concept: k-Nearest Neighbors (k-NN) Classification**
  - Why needed here: SecureLearn's relabeling relies on computing distances and aggregating neighbor labels
  - Quick check question: Given a point with 3 neighbors of class A and 4 neighbors of class B, what is the majority class? What if k=7 and threshold γ=40%—would relabeling occur?

- **Concept: Feature Importance Scoring (Gini, Probability-based)**
  - Why needed here: FORT requires extracting feature importance differently per algorithm
  - Quick check question: For a Random Forest with 100 trees, how would you compute feature importance? Why does FORT use different methods for tree-based vs. probabilistic models?

- **Concept: Z-Score Normalization and Outlier Detection**
  - Why needed here: The deviation detection assumes standardized data
  - Quick check question: If a dataset has mean μ=100 and std σ=20, what z-score corresponds to a value of 160? Would it exceed the |g|=3 threshold?

## Architecture Onboarding

- **Component map:** Input Dataset → k-NN Relabeling → Z-Score Filtering → Feature Importance Extraction → FORT Perturbation Generation → Adversarial Augmentation → Robust Model Training

- **Critical path:** The k-NN relabeling step is computationally dominant (O(n²) for naive distance computation)

- **Design tradeoffs:**
  - k value: k=7 balances sensitivity to local noise vs. smoothing boundaries
  - Deviation threshold: |g|=3 is 99.7% confidence interval; stricter thresholds remove more points
  - Perturbation constant: c=0.01 is small to avoid over-regularization; larger c improves robustness but may harm clean accuracy

- **Failure signatures:**
  - Detection rate < 60%: likely indicates poisoning rate >20% or poisoned points clustered near clean data
  - FDR > 0.15 after FORT: perturbation magnitude may be insufficient
  - GNB recall < 60%: Naive Bayes assumptions may not align with FORT perturbations

- **First 3 experiments:**
  1. Apply SecureLearn to clean IRIS dataset to verify sanitization doesn't harm clean data
  2. Poison IRIS with 15% OOP attack and measure DR, CR, accuracy, F1 vs. baseline
  3. Train RF on poisoned MNIST (15% RLP) and compare SecureLearn vs. no defense

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SecureLearn perform when applied to complex deep learning architectures (e.g., CNNs, Transformers) compared to the traditional ML models and MLPs tested?
- Basis in paper: The conclusion states, "In the future, we will expand our research and examine SecureLearn in DL and complex ML models"
- Why unresolved: The FORT method relies on feature importance scores, which may behave differently in deep architectures
- What evidence would resolve it: Empirical evaluation on complex DNN benchmarks against standard poisoning attacks

### Open Question 2
- Question: Can the SecureLearn framework be effectively adapted for regression algorithms?
- Basis in paper: The discussion notes that experiments were "conducted to mitigate data poisoning attacks in classification algorithms, which can be further extended to regression algorithms"
- Why unresolved: Current data sanitization relies on k-NN voting and label confidence thresholds designed for discrete class labels
- What evidence would resolve it: Modification of the label correction component for continuous outputs and subsequent testing on regression datasets

### Open Question 3
- Question: Can SecureLearn maintain its effectiveness and computational efficiency when scaling to datasets significantly larger than MNIST or USPS?
- Basis in paper: Results observe "inverse relation between SecureLearn performance and the dataset size"
- Why unresolved: The method relies on k-NN calculations which typically incur high computational costs as data grows
- What evidence would resolve it: Time-complexity analysis and accuracy benchmarks on datasets with >100k samples

## Limitations

- k=7 parameter and 40% confidence threshold lack sensitivity analysis
- Evaluation focuses on gray-box attacks with known poisoning rates (10-20%)
- Claims about FORT's superiority over gradient-based methods lack comparative analysis
- Feature importance-based perturbations may not generalize well to complex deep architectures

## Confidence

- **High Confidence:** Combining data sanitization with adversarial training is well-established
- **Medium Confidence:** Empirical results showing >90% accuracy are impressive but require replication
- **Low Confidence:** Claims about FORT's general superiority lack sufficient comparative analysis

## Next Checks

1. Systematically vary k (3-15), confidence thresholds (30-60%), and deviation thresholds (2.5-3.5) across all datasets
2. Implement white-box poisoning attacks where adversary knows SecureLearn's exact defense parameters
3. Apply SecureLearn to non-image, non-tabular domains (e.g., text classification) to test transferability