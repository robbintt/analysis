---
ver: rpa2
title: 'Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with
  AlphaEvolve'
arxiv_id: '2601.21096'
source_url: https://arxiv.org/abs/2601.21096
tags:
- compiler
- magellan
- llvm
- optimization
- heuristics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Magellan introduces an autonomous framework for discovering compiler
  optimization heuristics by evolving compiler passes using large language models
  (LLMs) and evolutionary search. The system generates executable C++ decision logic
  that can be directly integrated into existing compilers, targeting heuristic design
  as a program-synthesis problem rather than generating per-program optimization sequences.
---

# Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve

## Quick Facts
- arXiv ID: 2601.21096
- Source URL: https://arxiv.org/abs/2601.21096
- Reference count: 40
- Primary result: Discovers inlining heuristics achieving 5.23% binary size reduction and 0.61% performance improvement over expert baselines

## Executive Summary
Magellan is an autonomous framework that discovers compiler optimization heuristics by evolving executable C++ decision logic through a combination of large language models and evolutionary search. The system treats heuristic design as a program-synthesis problem, generating code that can be directly integrated into existing compilers like LLVM and XLA. By using macro-benchmarks for evaluation and a hierarchical search strategy that separates logic design from parameter tuning, Magellan discovers heuristics that are both compact and effective, demonstrating competitive performance while maintaining code maintainability.

## Method Summary
Magellan couples an LLM coding agent with evolutionary search in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement. The framework uses AlphaEvolve to propose code modifications within EVOLVE-BLOCK regions of compiler source files, specifically targeting LLVM's InlineAdvisor interface. The hierarchical search strategy separates high-level policy design (LLM proposes code templates with symbolic constants) from low-level parameter tuning (Vizier autotuner optimizes the constants). For each candidate heuristic, the system recompiles LLVM and evaluates performance on benchmarks measuring binary size and runtime. The approach starts from naive policies and iteratively refines through 50-100 iterations, with each iteration generating multiple hyperparameter configurations.

## Key Results
- Discovers inlining heuristics achieving 5.23% binary size reduction compared to expert-crafted baselines
- Achieves 0.61% performance improvement while maintaining code compactness (143-line policy)
- Successfully generalizes across different optimization tasks (LLVM inlining and XLA register allocation)
- Reduces invalid code rate from >65% to 13% through hierarchical search decomposition
- Demonstrates practical deployment in evolving compiler toolchains with maintainable code

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Search Decomposition
Separating high-level policy synthesis from low-level numerical tuning improves sample efficiency by preventing LLM token waste on arithmetic trial-and-error. The decoupled strategy accelerates convergence and reduces invalid rates from over 65% to only 13%. The core assumption is that optimal logic structure and numerical thresholds can be optimized independently, though this may fail if the best logic depends critically on specific constant values.

### Mechanism 2: Grounding in Macro-Benchmarks
Using end-to-end binary size or runtime as direct reward signals produces deployable heuristics that generalize better than proxy models. The approach recompiles the entire compiler with candidate heuristics and executes against real applications, grounding evaluation in realistic workloads. The main risk is the high compute overhead (approximately 1.5 days per study) and potential representativeness issues with the benchmark suite.

### Mechanism 3: Context-Constrained Mutation
Restricting LLM modifications to EVOLVE-BLOCK regions while preserving surrounding compiler infrastructure maintains system stability. This approach ensures generated code conforms to existing LLVM APIs without breaking build systems or type signatures. The assumption is that optimal heuristics can be expressed within the provided API context, though architectural changes might be necessary for more complex optimizations.

## Foundational Learning

- **LLVM Pass Infrastructure & InlineAdvisor**: Understanding how `InlineAdvisor`, `CallBase`, and `FunctionAnalysis` interact is required to define the search space and interpret synthesized code. Quick check: Can you identify which method acts as the "decision point" that Magellan tries to replace?

- **Evolutionary Program Synthesis**: The core loop is an evolutionary strategy requiring understanding of "population," "mutation" (LLM proposing diffs), and "fitness" (benchmark scores). Quick check: How does the system handle candidates that compile but produce invalid/crashing binaries?

- **Black-Box Optimization**: Magellan uses Vizier for hierarchical search, treating heuristics as black boxes and constants as dimensions. Quick check: Why is a "black-box" optimizer preferred over asking the LLM to "guess better numbers" for constants?

## Architecture Onboarding

- **Component map**: Controller (AlphaEvolve Orchestrator & LLM Prompter) -> Search Space (LLVM Source Code) -> Evaluator (Build System + Benchmark Runner) -> Optimizer (Vizier)
- **Critical path**: The Evaluation Loop is the bottleneck, requiring 1.5 days for size task and approximately 1 week for performance. Optimizing build time or using proxy benchmarks is the primary speed lever.
- **Design tradeoffs**: Partial heuristics (using predefined features) are faster but saturate early, while full heuristics (using raw APIs) have higher potential but require more exploration and have higher invalid code rates.
- **Failure signatures**: High invalid rates (>60%) indicate insufficient LLM prompt context or overly complex EVOLVE-BLOCK regions; plateauing suggests search space exhaustion or local optima requiring continuation strategies.
- **First 3 experiments**: 1) Hello World Loop: verify AlphaEvolve-to-LLVM-build pipeline with trivial function evolution; 2) Feature-Based Inlining: replicate "Partial Heuristic" configuration on small benchmark suite; 3) Ablation on Autotuning: compare with/without Vizier to verify hierarchical search claims.

## Open Questions the Paper Calls Out

1. **LLM vs Neural-Policy Tradeoffs**: Under what conditions is LLM-driven pass evolution preferable to neural-policy approaches for compiler heuristic discovery? The paper notes this comparison hasn't been systematically studied.

2. **Green-Field Discovery**: Can Magellan discover effective heuristics for problems with no pre-existing literature or expert baselines? Current experiments targeted mature optimizations where LLMs likely encountered related code during training.

3. **Multi-Objective Policies**: Would multi-objective evolved policies (jointly optimizing size and performance) retain the compactness and maintainability observed in single-objective cases? The 143-line policy optimized only binary size, and complex tradeoffs may require more intricate logic.

4. **Cold-Start Performance**: Why does performance optimization require seeding from prior heuristics, and can this limitation be overcome? The paper doesn't explain whether this reflects fundamental search difficulty or methodology limitations.

## Limitations

- Hierarchical search effectiveness is validated only within the compiler optimization domain and may depend on LLVM's specific API structure
- Macro-benchmark evaluation creates significant practical barriers with 1.5-day evaluation windows and uncertain benchmark representativeness
- Performance results (0.61% improvement) are modest, suggesting the approach may be hitting fundamental limits of what can be discovered through this search space
- AlphaEvolve framework internals are described through reference to another paper, creating uncertainty about implementation details

## Confidence

- **High Confidence**: Hierarchical search decomposition is technically sound and demonstrates measurable improvement in sample efficiency
- **Medium Confidence**: Macro-benchmark evaluation produces more deployable heuristics, though results could be influenced by benchmark selection bias
- **Low Confidence**: The approach will scale to discover fundamentally novel optimization strategies beyond incremental improvements to existing heuristics

## Next Checks

1. **Ablation Study on Search Granularity**: Run Magellan with varying EVOLVE-BLOCK sizes to quantify the relationship between search granularity and convergence speed, measuring both invalid rate and time-to-convergence.

2. **Benchmark Suite Sensitivity Analysis**: Evaluate discovered heuristics across different benchmark subsets to determine whether the macro-benchmarks used are representative or cherry-picked, reporting variance across workload categories.

3. **Cross-Compiler Generalization Test**: Apply the same hierarchical search framework to a different compiler infrastructure (e.g., GCC or MLIR) with comparable optimization problems to validate whether benefits are architecture-specific or transferable.