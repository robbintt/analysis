---
ver: rpa2
title: 'Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference
  to Provable Guarantees'
arxiv_id: '2506.11033'
source_url: https://arxiv.org/abs/2506.11033
tags:
- safety
- safe
- cost
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles safe reinforcement learning under hidden-parameter
  dynamics shifts, where unobserved factors like mass or friction vary across episodes.
  The authors propose an adaptive shielding framework that combines a safety-regularized
  optimization (SRO) with an online adaptive shield using conformal prediction.
---

# Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees

## Quick Facts
- **arXiv ID**: 2506.11033
- **Source URL**: https://arxiv.org/abs/2506.11033
- **Reference count**: 40
- **Primary result**: Adaptive shielding with function encoders and safety-regularized training significantly reduces safety violations in Safe-Gym benchmarks with hidden-parameter dynamics shifts.

## Executive Summary
This paper addresses safe reinforcement learning in environments where unobserved physical parameters (e.g., friction, mass) vary across episodes. The authors propose a two-part framework: a safety-regularized optimization (SRO) that proactively trains policies to avoid high-cost regions, and an adaptive conformal shield that enforces probabilistic safety bounds at runtime. The shield uses a function encoder to infer hidden dynamics online and filter unsafe actions based on predicted safety margins with uncertainty quantification. Experiments show substantial reductions in safety violations while maintaining performance, even in out-of-distribution settings.

## Method Summary
The method combines safety-regularized optimization (SRO) during training with an adaptive conformal shield during deployment. SRO augments the reward objective with a safety regularizer derived from local cost integration, encouraging policies toward zero-violation behavior. The adaptive shield uses a pre-trained function encoder to infer hidden dynamics online via neural basis functions, then applies conformal prediction to filter unsafe actions at runtime. The approach is evaluated on Safe-Gym benchmarks with varying hidden parameters, showing improved safety without significant performance degradation.

## Key Results
- Adaptive shielding significantly reduces safety violations compared to baselines across Safe-Gym benchmarks
- The method generalizes well to out-of-distribution hidden parameters (e.g., friction in [0.15, 0.3] and [1.7, 2.5])
- Computational overhead remains modest compared to non-shielded baselines
- Theoretical analysis provides bounds on average cost rate based on prediction error in the learned dynamics model

## Why This Works (Mechanism)

### Mechanism 1: Online Dynamics Inference via Function Encoders
The system infers unobserved physical parameters (e.g., friction, mass) online without fine-tuning neural networks by projecting transition data onto pre-trained neural basis functions. The resulting coefficients form a low-dimensional linear representation of current dynamics, conditioning both policy and shield. This works because the space of possible dynamics lies within the Hilbert space spanned by the basis functions.

### Mechanism 2: Safety-Regularized Optimization (SRO)
SRO modifies the training objective with a safety regularizer derived from integrating the cost Q-value over local action neighborhoods. This creates gradients that push probability mass away from locally high-cost regions, driving the policy toward zero-violation behavior without degrading performance within that set.

### Mechanism 3: Adaptive Conformal Shielding
The runtime shield samples candidate actions, predicts future states using the function encoder, and applies Adaptive Conformal Prediction to compute dynamic uncertainty thresholds. It filters actions where predicted safety margins fall below uncertainty bounds, enforcing probabilistic safety guarantees based on prediction error.

## Foundational Learning

**Concept: Conformal Prediction (CP) & Adaptive CP**
- *Why needed*: Required to understand how the shield converts heuristic prediction errors into statistically rigorous safety bounds
- *Quick check*: How does the ACP threshold Γ_t adapt if the model consistently underestimates risk?

**Concept: Hilbert Spaces & Basis Functions**
- *Why needed*: Essential for understanding the Function Encoder architecture, which assumes dynamics can be linearly represented by neural basis functions
- *Quick check*: Why does the FE allow for "zero-shot" adaptation compared to a standard recurrent network?

**Concept: Constrained Markov Decision Processes (CMDPs)**
- *Why needed*: Provides mathematical formulation for separating "reward" (task) from "cost" (safety) and understanding Lagrangian baselines
- *Quick check*: In the SRO objective, what does the hyperparameter α trade off?

## Architecture Onboarding

**Component map**: Function Encoder (FE) -> Policy Network -> Critics (Reward/Cost) -> Shielding Module

**Critical path**: Pre-train function encoder on diverse dynamics dataset, then train RL policy with SRO while freezing FE. At runtime, FE infers dynamics, shield filters actions, policy selects from safe set.

**Design tradeoffs**: Shield horizon (h=1-2) balances lookahead safety against compounding prediction errors. ACP failure probability (δ) trades safety guarantees against action set restriction and task performance.

**Failure signatures**: Shield paralysis (100% trigger rate blocking exploration), prediction drift (Γ_t growing indefinitely), SRO over-regularization (conservative policies with low return).

**First 3 experiments**: 1) FE validation: verify prediction accuracy on held-out hidden parameters, 2) SRO ablation: compare training curves of SRO-only vs shield-only vs combined, 3) Stress test: evaluate on OOD intervals [0.15, 0.3] to confirm generalization.

## Open Questions the Paper Calls Out
- Can the adaptive shielding mechanism maintain safety guarantees and low computational overhead on physical robotic platforms with sensor noise and actuator delays?
- Is it possible to learn the function encoder's basis functions online or remove the dependency on a pre-collected offline dataset?
- How can the term ε̄ in the average cost rate bound be quantified or minimized to ensure the bound approaches the failure probability δ?
- Can the safety guarantees be extended to environments where the cost function is non-Lipschitz or discontinuous?

## Limitations
- Hidden parameter scope limited by pre-trained basis functions; performance may degrade with highly non-linear or structurally different dynamics
- Theoretical guarantees depend on ACP calibration and Lipschitz assumptions that may not hold under severe non-stationarity
- Computational overhead scales with action space size and horizon length despite being modest compared to baselines

## Confidence
- **High**: Experimental results show consistent reduction in safety violations across Safe-Gym benchmarks
- **Medium**: Connection between prediction error and cost rate bounds is theoretically sound but depends on calibration assumptions
- **Medium**: SRO mechanism is supported by Proposition 1 but practical impact on exploration requires further validation

## Next Checks
1. Evaluate on wider range of out-of-distribution hidden parameters (e.g., friction ∈ [0.1, 2.0]) to stress-test function encoder limits
2. Measure empirical coverage of ACP uncertainty set under varying non-stationarity levels and compare against theoretical bounds
3. Systematically vary shield prediction horizon h to quantify trade-off between safety guarantees and prediction error