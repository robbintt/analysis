---
ver: rpa2
title: 'FedSV: Byzantine-Robust Federated Learning via Shapley Value'
arxiv_id: '2502.17526'
source_url: https://arxiv.org/abs/2502.17526
tags:
- clients
- malicious
- learning
- client
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Byzantine-robust federated learning by introducing
  FedSV, a defense mechanism that leverages Shapley Value (SV) to detect and mitigate
  malicious clients. The core idea is to iteratively estimate each client's SV during
  training, which quantifies their contribution to the global model's accuracy.
---

# FedSV: Byzantine-Robust Federated Learning via Shapley Value

## Quick Facts
- **arXiv ID**: 2502.17526
- **Source URL**: https://arxiv.org/abs/2502.17526
- **Reference count**: 30
- **Primary result**: FedSV achieves Byzantine-robust federated learning by leveraging Shapley Value to detect malicious clients, maintaining high accuracy even with up to 50% malicious clients.

## Executive Summary
This paper introduces FedSV, a novel defense mechanism for Byzantine-robust federated learning that leverages Shapley Value (SV) to detect and mitigate malicious clients. The approach iteratively estimates each client's SV during training, quantifying their contribution to the global model's accuracy. FedSV employs a clustering-based strategy (CLUS FED) to group clients into clusters based on their SVs, allowing the server to select only honest clients for global model aggregation. Experiments on MNIST demonstrate that FedSV maintains high accuracy even under severe attack conditions, outperforming existing defenses and achieving accuracy comparable to a federated averaging baseline without attacks.

## Method Summary
FedSV combines Shapley Value estimation with clustering-based client selection to achieve Byzantine-robust federated learning. The server estimates each client's SV using Truncated Antithetic Monte Carlo (TAMC) combined with Stratified Random Sampling (SRS) to reduce variance. Clients are then grouped into clusters based on their SV values through a regularized clustering strategy (CLUS FED). The server selects the high-SV cluster for aggregation, effectively filtering out malicious clients. This approach is designed to work with non-IID data distributions and does not require prior knowledge of the number of malicious clients.

## Key Results
- FedSV maintains high accuracy even with up to 50% malicious clients, outperforming existing defenses like Multi-Krum and Median
- Achieves accuracy comparable to federated averaging baseline without attacks under severe attack conditions
- Effective in non-IID data scenarios where traditional defenses often struggle
- SV-based clustering successfully separates honest from malicious clients in controlled experiments

## Why This Works (Mechanism)

### Mechanism 1: Shapley Value for Contribution Quantification
- Claim: Estimating each client's Shapley Value (SV) provides a discriminative signal for distinguishing honest from malicious clients, even under non-IID data distributions.
- Mechanism: SV measures the marginal accuracy improvement when a client's update joins any coalition of other clients. Malicious updates systematically degrade model performance, yielding lower or negative SV scores. The paper computes SV iteratively using TAMC combined with SRS to reduce variance and computational cost.
- Core assumption: Malicious client updates negatively affect model accuracy on a held-out validation set available to the server, and this effect is consistent across different coalitions.
- Evidence anchors: [abstract] "estimates the contribution of each client according to the different groups to which the target client belongs"; [Section III-B] Describes TAMC and SRS for variance reduction, with bounds on required samples.
- Break condition: If malicious clients craft updates that improve validation accuracy (e.g., by overfitting to validation data), SV-based detection may fail.

### Mechanism 2: Clustering-Based Client Selection (CLUS FED)
- Claim: A regularized clustering strategy over SV time series can separate honest from malicious clients without prior knowledge of the number of attackers.
- Mechanism: The server sorts clients by SV and evaluates whether splitting into two clusters (with penalty term λ) better explains the distribution than a single cluster. If the two-cluster solution is optimal, only the higher-SV cluster participates in aggregation; otherwise, all clients are used.
- Core assumption: Honest clients have SVs that cluster together and separate from malicious client SVs with sufficient margin.
- Evidence anchors: [Section III-C] Formal optimization problem for cluster selection; Algorithm 2 provides pseudocode; [Figure 1] Visualizes SV separation between honest (cyan) and malicious (red) clients across rounds.
- Break condition: If malicious clients collude to produce diverse SVs that blend into the honest cluster (e.g., via stealthy or sporadic attacks), the single-cluster assumption fails.

### Mechanism 3: Robust Aggregation via Honest Subset
- Claim: Aggregating only over the selected honest subset achieves accuracy comparable to FedAvg without attacks, even with ≥50% malicious clients.
- Mechanism: After CLUS FED identifies the honest subset, the server performs weighted averaging using only their model updates. This removes malicious influence without requiring explicit anomaly thresholds.
- Core assumption: The honest subset remains a majority of high-SV clients after clustering; the paper assumes malicious clients cannot spoof high SV.
- Evidence anchors: [abstract] "FedSV achieves accuracy comparable to a federated averaging baseline without attacks"; [Figures 2-4] Loss and accuracy comparisons showing FedSV matching FedAvg baseline under 40-55% malicious clients.
- Break condition: If malicious clients can adapt their updates to achieve high SV (e.g., by mimicking honest behavior initially), they may be included in aggregation and later corrupt the model.

## Foundational Learning

- **Federated Averaging (FedAvg)**
  - Why needed here: FedSV modifies FedAvg by adding SV-based client filtering before aggregation.
  - Quick check question: Can you sketch the standard FedAvg local training and aggregation loop?

- **Shapley Value (Cooperative Game Theory)**
  - Why needed here: Core metric for contribution estimation; understanding marginal contribution over coalitions is essential.
  - Quick check question: Explain in one sentence why SV requires exponential permutations and how Monte Carlo approximates it.

- **Byzantine Attacks in FL (Poisoning, Backdoor, Sign-Flipping)**
  - Why needed here: Threat model defines what FedSV defends against; sign-flipping was the primary attack evaluated.
  - Quick check question: What distinguishes a targeted backdoor attack from an untargeted sign-flipping attack?

## Architecture Onboarding

- **Component map:** Server (global model, SV estimation, clustering) <- Client updates -> Clients (local training) ; Server uses validation data for SV computation
- **Critical path:** 1) Global model broadcast to all clients 2) Local training and update upload 3) SV estimation via TAMC + SRS 4) CLUS_FED clustering on SV vector 5) Conditional aggregation over selected subset
- **Design tradeoffs:** SV approximation quality vs. communication/compute overhead (more Monte Carlo samples reduce variance but increase server-side computation); Clustering sensitivity (λ) balances detection aggressiveness vs. false positives; Validation set requirement may not hold in privacy-sensitive domains
- **Failure signatures:** Sudden drop in all client SVs (model collapse or attack on validation data); No cluster separation (single cluster persists, attack too subtle); Accuracy degrades despite high-SV selection (adaptive attack spoofing high SV)
- **First 3 experiments:** 1) Baseline reproduction on MNIST with 20 clients (4-10 malicious, sign-flipping attack); verify SV separation in Figure 1 and accuracy matching Figure 4 2) Hyperparameter sensitivity sweep of λ ∈ [−1, 1] and Monte Carlo sample count m; measure false positive/negative rates and convergence speed 3) Adaptive adversary test with malicious clients alternating between honest and malicious updates across rounds to probe SV clustering adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Shapley Value (SV) approximation framework be scaled to handle large-scale cross-device Federated Learning with thousands of clients while remaining computationally tractable?
- Basis in paper: [explicit] The conclusion explicitly identifies the need for "new SV approximation frameworks to handle a larger number of clients in a reasonable time."
- Why unresolved: The proposed estimation methods (TAMC, SRS) were validated on only N=20 clients, and the complexity of SV estimation typically grows rapidly with population size.
- What evidence would resolve it: Theoretical bounds and empirical results demonstrating convergence speed and latency on datasets with N > 1000 clients.

### Open Question 2
- Question: Can a selection strategy be developed that identifies the optimal subset of honest clients to improve performance beyond the baseline, rather than simply selecting all non-malicious clients?
- Basis in paper: [explicit] The conclusion proposes designing strategies that "find the optimal combination of clients at each round... to perform better than the baseline."
- Why unresolved: The current CLUS FED strategy focuses on binary clustering to prune malicious nodes but does not optimize the selection among the remaining honest participants.
- What evidence would resolve it: An algorithm that actively selects a specific subset of honest clients to maximize a utility function, demonstrating higher accuracy than the FedAvg baseline.

### Open Question 3
- Question: Is FedSV effective in scenarios where the server lacks a clean, representative validation dataset to compute the value function?
- Basis in paper: [inferred] The authors state in Section IV.D that FedSV calculates the SV based on model performance "when tested against a set of test data," relying on a server-side dataset that may not exist in all FL contexts.
- Why unresolved: The robustness of the SV estimation and subsequent clustering depends on the quality of this validation data, a limitation not addressed for data-scarce environments.
- What evidence would resolve it: Experimental evaluation of detection accuracy and model convergence when the server's validation set is noisy, small, or unrepresentative.

## Limitations
- Adaptive attack robustness is not validated; paper assumes malicious clients consistently degrade validation accuracy
- Requires server to have a held-out validation dataset, which may not be feasible in privacy-constrained environments
- Computational overhead and communication costs for scaling to large-scale deployments (thousands of clients) are not quantified
- Experiments limited to MNIST with sign-flipping attacks; performance on complex datasets and other attack types remains unverified

## Confidence
- **High confidence**: Core SV-based contribution estimation mechanism and its role in Byzantine defense (supported by game theory literature and FedSV's formal derivation)
- **Medium confidence**: Clustering-based honest client selection (CLUS FED) achieves robust separation in controlled MNIST experiments, but lacks validation under adaptive or stealthy attacks
- **Low confidence**: Claim that FedSV matches FedAvg baseline accuracy under 50% malicious clients without explicit validation under adaptive adversaries or alternative attack models

## Next Checks
1. **Adaptive adversary stress test**: Implement malicious clients that alternate between honest and Byzantine updates across rounds. Measure whether SV clustering adapts and maintains accuracy over time.
2. **Cross-dataset evaluation**: Replicate experiments on CIFAR-10 with backdoor and label-flipping attacks. Compare FedSV's SV separation and accuracy retention against baseline defenses.
3. **Overhead profiling**: Quantify server-side computation (Monte Carlo samples) and communication costs for scaling FedSV to 100+ clients. Identify bottlenecks and optimization opportunities.