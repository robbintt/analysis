---
ver: rpa2
title: 'DSGym: A Holistic Framework for Evaluating and Training Data Science Agents'
arxiv_id: '2601.16344'
source_url: https://arxiv.org/abs/2601.16344
tags:
- data
- tasks
- task
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DSGym is a unified framework for evaluating and training data science
  agents in isolated execution environments. It standardizes task representations,
  agent interfaces, and runtime environments, enabling reproducible and fair comparison
  across diverse data science benchmarks.
---

# DSGym: A Holistic Framework for Evaluating and Training Data Science Agents

## Quick Facts
- arXiv ID: 2601.16344
- Source URL: https://arxiv.org/abs/2601.16344
- Reference count: 40
- A unified framework enabling reproducible evaluation and training of data science agents in isolated Docker environments.

## Executive Summary
DSGym is a holistic framework for evaluating and training data science agents in isolated execution environments. It standardizes task representations, agent interfaces, and runtime environments, enabling reproducible and fair comparison across diverse data science benchmarks. We introduce DSGym-Tasks, a curated suite that refines existing benchmarks via quality and shortcut filtering, and expands coverage with DSGio (expert-derived bioinformatics tasks) and DSPredict (real-world modeling challenges). On standardized analysis benchmarks, a 4B model trained with DSGym’s execution-verified synthetic data achieves competitive performance against GPT-4o. Evaluation of frontier models reveals that over 80% of failures in scientific workflows stem from domain-grounding errors, and that agents often stop after producing a valid but sub-optimal solution. DSGym supports both rigorous evaluation and scalable synthetic training data generation, positioning it as a live, extensible testbed for advancing data science agents.

## Method Summary
DSGym standardizes data science agent evaluation and training via a Docker-based manager-worker architecture. Tasks are executed in isolated Jupyter kernel containers with read-only data mounts, preventing unauthorized actions. Agents interact via a CodeAct interface, interleaving reasoning, code, and answers. The framework introduces DSGym-Tasks, a curated benchmark suite spanning analysis and prediction challenges. Training leverages execution-verified synthetic data (DSGym-SFT), generated via a multi-stage pipeline including automated execution checks. A 4B model (Qwen3-4B) is fine-tuned on this data and evaluated against GPT-4o, achieving competitive results on analysis benchmarks.

## Key Results
- A 4B model trained with DSGym’s execution-verified synthetic data matches GPT-4o performance on standardized analysis benchmarks.
- Over 80% of agent failures in scientific workflows are due to domain-grounding errors (misinterpreting metadata or hallucinating API arguments).
- Agents often stop after producing a valid but sub-optimal solution, rather than iterating toward above-median or medal-level performance.

## Why This Works (Mechanism)
The framework’s success rests on three pillars: (1) isolated, reproducible execution via Docker containers prevents environmental drift and ensures fair comparison; (2) the CodeAct interface structures agent reasoning and code execution, enabling precise error attribution; (3) execution-verified synthetic data filters out invalid or low-quality trajectories, improving training signal fidelity.

## Foundational Learning
- **Docker containerization** — Isolates agent execution, enforces read-only data access, and ensures reproducibility across environments.
  - *Why needed:* Prevents agents from circumventing data constraints or installing unauthorized libraries.
  - *Quick check:* Verify worker containers run with `read-only` mounts and no package installation permissions.

- **CodeAct interface** — Structures agent interaction into reasoning, code, and answer phases.
  - *Why needed:* Enables precise error attribution and consistent agent behavior across tasks.
  - *Quick check:* Inspect agent logs for `<reasoning>`, `<code>`, `<answer>` tags in correct sequence.

- **Execution-verified synthetic data** — Filters training trajectories by running code and validating outputs.
  - *Why needed:* Ensures only valid, executable solutions contribute to training, avoiding noise from hallucinated code.
  - *Quick check:* Confirm synthetic samples execute without errors in the worker container.

- **Quality and shortcut filtering** — Refines existing benchmarks to remove ambiguous or trivially solvable tasks.
  - *Why needed:* Prevents agents from exploiting task shortcuts rather than demonstrating genuine data science skill.
  - *Quick check:* Compare pre- and post-filtered benchmark distributions for task difficulty and ambiguity.

## Architecture Onboarding

**Component Map**
Manager -> Worker (Jupyter kernel) -> CodeAct Agent -> Execution Environment

**Critical Path**
Task submission → Docker container launch → CodeAct reasoning → Code execution → Output validation → Result aggregation

**Design Tradeoffs**
- *Isolation vs. performance:* Docker sandboxing ensures reproducibility but introduces overhead.
- *Exact-match vs. semantic evaluation:* Deterministic metrics enable fair comparison but miss nuanced solution quality.
- *Synthetic data scale vs. quality:* Large-scale generation is efficient but risks introducing subtle biases without rigorous filtering.

**Failure Signatures**
- `ModuleNotFoundError`: Agent attempts unauthorized library installation.
- `TimeoutError`: Agent stalls on large datasets or infinite loops.
- Domain grounding errors: Misinterpretation of scientific metadata or API arguments.

**3 First Experiments**
1. Run default CodeAct agent with Qwen3-4B-Instruct on DSGym-Tasks to establish baseline scores.
2. Fine-tune Qwen3-4B-Instruct on DSGym-SFT-2k and re-evaluate to measure training impact.
3. Inspect failed trajectories for domain-grounding errors to validate failure attribution.

## Open Questions the Paper Calls Out
- **Reinforcement learning for agent training:** How can RL be effectively applied within DSGym’s execution environment, given challenges in reward design and credit assignment under sparse, long-horizon rewards?
- **Extending evaluation to visualization and open-ended discovery:** Can data science agent evaluation be extended to visualization-centric and open-ended discovery tasks while maintaining reproducibility?
- **Domain transfer of DSBio pipeline:** To what extent does the DSBio construction pipeline transfer to other scientific domains (chemistry, materials science, astronomy)?
- **Overcoming simplicity bias:** How can agents be trained to overcome simplicity bias and develop verification behaviors that match expert data scientists?

## Limitations
- Exact prompt templates and LLM-based judge configurations for synthetic data filtering are not disclosed, limiting reproducibility.
- Domain-grounding failure statistics depend on undisclosed evaluation rubrics and judge criteria.
- The framework does not yet support visualization-centric or open-ended discovery tasks.

## Confidence
- **High Confidence:** Core architectural claims (Docker-based manager-worker separation, CodeAct interface, Jupyter kernel execution) are well-specified and directly verifiable.
- **Medium Confidence:** Synthetic data generation pipeline is described, but exact prompt and judge details are omitted; performance claims relative to GPT-4o rest on these unprovided components.
- **Low Confidence:** Domain-grounding failure statistics (80% of failures) are plausible but depend on the undisclosed evaluation rubric and judge criteria.

## Next Checks
1. Request the complete synthetic data generation prompt templates and LLM-based judge configuration from the authors to verify the reported filtering criteria.
2. Replicate the worker container builds using the specified Dockerfiles and validate that the environment matches the reported execution constraints (e.g., read-only mounts, installed libraries).
3. Perform a small-scale ablation: evaluate the 4B model trained with and without the Stage 3 filtering to quantify the impact of the undisclosed judge decisions on downstream benchmark accuracy.