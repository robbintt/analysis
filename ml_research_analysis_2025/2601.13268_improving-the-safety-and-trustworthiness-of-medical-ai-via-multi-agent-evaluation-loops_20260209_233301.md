---
ver: rpa2
title: Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation
  Loops
arxiv_id: '2601.13268'
source_url: https://arxiv.org/abs/2601.13268
tags:
- safety
- medical
- risk
- language
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-agent evaluation loop for improving
  the safety and ethical compliance of medical LLMs through iterative refinement rather
  than retraining. Two generative models, DeepSeek R1 and Med-PaLM, are paired with
  two evaluator agents that assess responses against the AMA Principles of Medical
  Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol.
---

# Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops

## Quick Facts
- arXiv ID: 2601.13268
- Source URL: https://arxiv.org/abs/2601.13268
- Reference count: 33
- Primary result: 89% reduction in ethical violations and 92% risk downgrade rate through inference-time multi-agent refinement

## Executive Summary
This study introduces a multi-agent evaluation loop for improving the safety and ethical compliance of medical LLMs through iterative refinement rather than retraining. Two generative models, DeepSeek R1 and Med-PaLM, are paired with two evaluator agents that assess responses against the AMA Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. The framework processes 900 adversarial medical queries, using structured feedback to guide multiple rounds of response revision until both ethical and safety thresholds are met. Results show DeepSeek R1 achieves faster convergence (mean 2.34 vs 2.67 iterations) and both models reach an 89% reduction in ethical violations and a 92% risk downgrade rate. This demonstrates that inference-time safety refinement is scalable, regulator-aligned, and cost-efficient for governing medical AI deployment.

## Method Summary
The framework implements an iterative multi-agent refinement loop where two medical LLMs (DeepSeek R1, Med-PaLM) generate responses to adversarial medical queries, which are then evaluated by two separate agents: LLaMA 3.1 assesses AMA ethical compliance (0-9 scale) while Phi-4 assigns SRA-5 risk levels (1-5 scale). A consensus record merges these evaluations, and if thresholds aren't met (AMA score ≤2, SRA level ≤2), a revision plan is generated and fed back to the generator for refinement. The process repeats for up to 5 iterations per query, processing 900 prompts balanced across 9 AMA principle categories from MedSafetyBench.

## Key Results
- DeepSeek R1 converges faster than Med-PaLM (mean 2.34 vs 2.67 iterations per query)
- Both models achieve 89% reduction in ethical violations through iterative refinement
- 92% of responses successfully downgrade risk levels across iterations
- Non-convergence rate remains low at 5.8-8.2% across categories
- Mean latency of 8.5 seconds per converged query on 4× A100 hardware

## Why This Works (Mechanism)

### Mechanism 1: Iterative Refinement via Structured Feedback
- Claim: Passing structured critique back to the generator enables safety improvement without weight updates.
- Mechanism: Each evaluation round produces a consensus record containing AMA score, SRA level, and violation reasons. A revision plan is generated from this consensus, instructing the generator to remove unsafe instructions, avoid diagnostic claims, and align with professional ethics. The generator revises its response, and the loop repeats until thresholds are met or the iteration budget is exhausted.
- Core assumption: Generators can reliably interpret and follow textual feedback to reduce violations in subsequent outputs.
- Evidence anchors:
  - [abstract] "The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate."
  - [section III.C] "If the thresholds are not met, the system generates structured feedback and the generator revises the response. We cap the process at five iterations."
  - [corpus] Weak direct corpus evidence; related work (Reflexion, Self-Refine) suggests verbal feedback improves behavior, but medical-domain validation remains limited.
- Break condition: If generators ignore or misinterpret feedback (e.g., superficial rewrites), iteration gains diminish. Non-convergence rates (5.8–8.2% in this study) indicate this can occur.

### Mechanism 2: Dual-Evaluator Consensus for Safety Coverage
- Claim: Using two evaluators with distinct roles improves coverage of ethical and risk dimensions.
- Mechanism: LLaMA 3.1 evaluates against AMA principles (ethics-focused). Phi-4 assigns SRA-5 risk levels (safety-focused). Their outputs are merged into a consensus record. This separation reduces single-evaluator blind spots and provides both normative (ethical) and consequential (risk) assessments.
- Core assumption: Each evaluator specializes sufficiently in its assigned dimension, and merging outputs yields a more complete safety signal than either alone.
- Evidence anchors:
  - [abstract] "Two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol."
  - [section III.A] "LLaMA 3.1 performs ethics evaluation against the AMA principles. Phi-4 focuses on safety risk detection and assigns an SRA-5 risk level."
  - [corpus] No direct corpus comparison of dual vs. single evaluators in medical settings.
- Break condition: If evaluators produce conflicting signals or share systematic blind spots (e.g., both miss subtle privacy violations), consensus provides false confidence.

### Mechanism 3: Inference-Time Alignment Decouples Policy from Model Weights
- Claim: Safety refinement at inference time allows policy updates without retraining the generator.
- Mechanism: The generator remains unchanged throughout the loop. Safety criteria (AMA thresholds, SRA levels, feedback templates) live in evaluator prompts and feedback construction logic. Updating policies requires modifying prompts or swapping evaluators, not fine-tuning the generator.
- Core assumption: Evaluators correctly encode policy intent, and generators respond predictably to feedback across policy changes.
- Evidence anchors:
  - [abstract] "Introduces a multi-agent refinement framework to improve the safety and trustworthiness of medical LLMs without expensive retraining."
  - [section V.B] "In contrast, our approach operates at inference time. The generator can be left unchanged, while the evaluation policy and the feedback style can be adjusted by changing the evaluator prompts or swapping evaluator models."
  - [corpus] Related work (Constitutional AI, MedSafetyBench fine-tuning) shows retraining-based alignment is costly and inflexible; corpus supports the motivation but not direct comparison of efficacy.
- Break condition: If policy changes are complex (e.g., nuanced subspecialty criteria), evaluator prompts may fail to capture them accurately, requiring evaluator retraining or replacement.

## Foundational Learning

- **Iterative Multi-Agent Refinement (e.g., Reflexion, Self-Refine)**
  - Why needed here: The entire framework assumes generators improve when given textual feedback over multiple rounds. Understanding how feedback formatting, iteration limits, and convergence thresholds affect outcomes is prerequisite.
  - Quick check question: If a generator ignores revision instructions after 3 iterations, should you increase the budget or restructure the feedback template?

- **Medical Ethics and Risk Stratification**
  - Why needed here: Evaluator judgments are grounded in AMA principles and SRA-5 levels. Without understanding these frameworks, you cannot debug why a response receives a high score or fails to converge.
  - Quick check question: What SRA level should a response receive if it suggests a patient take a specific medication dosage without consulting a clinician?

- **LLM-as-Judge Reliability**
  - Why needed here: Evaluators are themselves LLMs, not ground truth. Their judgments require calibration and may exhibit systematic biases or blind spots.
  - Quick check question: If Phi-4 systematically underestimates risk in emergency scenarios, how would you detect and correct this without clinician annotations?

## Architecture Onboarding

- **Component map:**
  - Generator (DeepSeek R1 or Med-PaLM) -> Initial response
  - Ethics Evaluator (LLaMA 3.1) -> AMA violation score
  - Risk Evaluator (Phi-4) -> SRA-5 risk level
  - Consensus Merger -> Combined safety record
  - Feedback Constructor -> Revision plan
  - Loop Controller -> Iteration management and termination

- **Critical path:**
  1. Generator receives query → produces initial response
  2. Both evaluators assess in parallel
  3. Consensus merger combines scores and reasons
  4. If thresholds met → return response. If not → feedback constructor creates revision plan
  5. Generator revises response using feedback
  6. Repeat until convergence or 5-iteration limit

- **Design tradeoffs:**
  - Latency vs. safety: More iterations improve safety but increase latency (mean 8.5s per converged query on 4× A100)
  - Threshold strictness: Lower thresholds (AMA ≤ 1, SRA ≤ 1) would improve safety but reduce convergence and increase over-refusal
  - Evaluator choice: Stronger evaluators may catch more violations but increase cost and potential over-correction

- **Failure signatures:**
  - Non-convergence: Response oscillates between acceptable and unacceptable across iterations
  - Over-refusal: Refined responses become excessively vague or decline benign queries
  - Evaluator disagreement: Consensus mechanism fails to reconcile conflicting scores
  - Feedback ignored: Generator revisions do not address cited violations

- **First 3 experiments:**
  1. **Convergence analysis by category:** Run 100 queries per AMA principle category and measure iteration distribution, non-convergence rate, and residual violations. Identify which categories (e.g., Patient Welfare, Legal) require the most refinement.
  2. **Evaluator calibration check:** Sample 50 responses at each SRA level and have clinicians annotate ground-truth risk. Measure over/under-estimation bias per evaluator and per risk category.
  3. **Feedback template ablation:** Compare convergence rate and violation reduction using (a) detailed revision plans vs. (b) minimal feedback (score only). Quantify the contribution of structured feedback to iterative improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do LLM-based evaluator judgments align with clinician-labeled safety assessments for medical outputs?
- Basis in paper: [explicit] The authors state: "Validation against clinician annotations is necessary to understand where the evaluators are reliable and where they may miss subtle harms."
- Why unresolved: The evaluators (LLaMA 3.1 and Phi-4) are themselves language models; their judgments are not treated as ground truth, and no human expert calibration was performed.
- What evidence would resolve it: A study comparing evaluator scores against board-certified clinician annotations across the same 900 adversarial prompts, measuring agreement rates and identifying systematic blind spots.

### Open Question 2
- Question: Does the multi-agent refinement loop maintain safety improvements in multi-turn conversational contexts where prior dialogue history influences risk?
- Basis in paper: [explicit] The authors note: "we evaluate single turn prompts, while many real deployments involve multi turn conversations where earlier context can change risk."
- Why unresolved: Real clinical interactions involve accumulated context; a safe response in isolation may become unsafe when combined with prior exchanges.
- What evidence would resolve it: Extending the evaluation to multi-turn dialogue scenarios from MedSafetyBench or similar datasets, measuring violation rates across conversation depth.

### Open Question 3
- Question: How robust is the refinement framework against targeted prompt injection attacks and tool-integrated deployment scenarios?
- Basis in paper: [explicit] The authors state: "we do not fully characterize adversarial robustness against targeted prompt injection and tool use scenarios, which is important when LLMs are embedded in larger applications."
- Why unresolved: The study only covers adversarial medical prompts, not injection attacks that manipulate the evaluator agents or exploit tool-use interfaces.
- What evidence would resolve it: Red-teaming experiments using OWASP-style prompt injection vectors against both generator and evaluator agents, measuring bypass success rates.

### Open Question 4
- Question: Do the convergence and safety improvement patterns generalize across a broader range of generator and evaluator model families?
- Basis in paper: [explicit] The authors acknowledge: "we only evaluate two generators and two evaluators, so the conclusions may not generalize to other model families."
- Why unresolved: DeepSeek R1 and Med-PaLM may have idiosyncratic behaviors; it is unclear whether reasoning-optimized vs. domain-trained differences hold universally.
- What evidence would resolve it: Replicating the evaluation loop with additional generators (e.g., GPT-4, Claude, Mistral) and diverse evaluator configurations, comparing convergence metrics.

## Limitations
- The study relies on LLM-as-judge methodology without external clinician validation, creating potential blind spots in safety assessment
- The consensus mechanism between two evaluators lacks explicit resolution rules for conflicting judgments, which could mask systematic evaluator biases
- The non-convergence rate of 5.8-8.2% across categories suggests fundamental limitations in the approach's ability to handle complex medical queries

## Confidence
- **High confidence**: The iterative refinement mechanism achieving 89% reduction in ethical violations and 92% risk downgrade rate - directly measured outcomes with clear methodology
- **Medium confidence**: Dual-evaluator consensus improving coverage - conceptually sound but lacks direct comparative evidence against single-evaluator baselines
- **Low confidence**: Inference-time alignment being more scalable than retraining - motivational claim supported by related work but not directly validated through cost/complexity comparison

## Next Checks
1. Conduct clinician review of 100 converged responses to validate evaluator judgments and measure true safety improvement beyond LLM assessments
2. Implement controlled comparison with single-evaluator baseline to quantify the actual contribution of dual-evaluator consensus to performance gains
3. Test policy update flexibility by modifying evaluator prompts to reflect new safety criteria and measuring generator compliance without retraining