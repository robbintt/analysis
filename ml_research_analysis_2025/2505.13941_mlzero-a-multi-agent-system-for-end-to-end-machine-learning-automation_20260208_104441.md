---
ver: rpa2
title: 'MLZero: A Multi-Agent System for End-to-end Machine Learning Automation'
arxiv_id: '2505.13941'
source_url: https://arxiv.org/abs/2505.13941
tags:
- data
- dataset
- code
- agent
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLZero is a multi-agent system that automates end-to-end machine
  learning workflows with minimal human intervention. It employs specialized perception
  agents to analyze raw multimodal data and select appropriate ML libraries, while
  dual memory modules (semantic and episodic) enhance iterative code generation and
  debugging.
---

# MLZero: A Multi-Agent System for End-to-end Machine Learning Automation

## Quick Facts
- arXiv ID: 2505.13941
- Source URL: https://arxiv.org/abs/2505.13941
- Reference count: 40
- MLZero achieves 92% success rate (+263% over competitors) on MLE-Bench Lite

## Executive Summary
MLZero is a multi-agent system that automates end-to-end machine learning workflows with minimal human intervention. It employs specialized perception agents to analyze raw multimodal data and select appropriate ML libraries, while dual memory modules (semantic and episodic) enhance iterative code generation and debugging. Evaluated on MLE-Bench Lite and a new Multimodal AutoML Agent Benchmark, MLZero demonstrates superior performance compared to state-of-the-art agents in both solution quality and computational efficiency, even when using compact 8B LLMs.

## Method Summary
MLZero implements a multi-agent architecture where perception agents analyze raw data to recommend appropriate ML libraries and frameworks. The system uses a dual memory approach combining semantic memory for domain knowledge and episodic memory for tracking past solutions. These components work together to iteratively generate, test, and refine ML pipelines. The agents coordinate through a central orchestrator that manages the workflow from data preprocessing through model deployment. The system is designed to work with smaller language models (8B parameters) while maintaining competitive performance.

## Key Results
- Achieves 92% success rate on MLE-Bench Lite (+263% improvement over competitors)
- Outperforms state-of-the-art agents in both solution quality and computational efficiency
- Demonstrates effectiveness with compact 8B LLMs without sacrificing performance

## Why This Works (Mechanism)
MLZero's effectiveness stems from its specialized perception agents that can analyze raw multimodal data and make informed decisions about ML library selection. The dual memory architecture allows the system to learn from both general domain knowledge and specific past experiences, enabling more efficient problem-solving over time. The multi-agent coordination enables specialization where different agents focus on specific aspects of the ML workflow, reducing the complexity each agent needs to handle.

## Foundational Learning
1. **Multi-agent coordination**: Why needed - to divide complex ML workflows into manageable subtasks; Quick check - observe communication patterns between agents during problem-solving
2. **Dual memory systems**: Why needed - to balance general knowledge with specific experience; Quick check - compare performance with and without episodic memory on similar problems
3. **Perception-driven library selection**: Why needed - to match problem characteristics with appropriate tools; Quick check - validate perception agent recommendations against human expert choices

## Architecture Onboarding

**Component Map**: Raw Data -> Perception Agents -> Library Selector -> Code Generator -> Memory Modules -> Debugger -> Solution

**Critical Path**: Data Analysis → Library Selection → Pipeline Generation → Iterative Refinement → Final Solution

**Design Tradeoffs**: Compact 8B LLMs for efficiency vs. larger models for potentially better performance; specialized agents for focused expertise vs. general agents for flexibility

**Failure Signatures**: Incorrect library selection leading to poor model performance; memory retrieval errors causing repeated mistakes; perception agent misclassification of data types

**First Experiments**: 1) Run perception agents on benchmark datasets to verify library recommendations; 2) Test memory retrieval on previously solved problems; 3) Evaluate end-to-end pipeline generation on simple classification tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark Representativeness: Evaluation relies heavily on simplified Kaggle tasks (MLE-Bench Lite), lacking evidence for real-world complex ML problems
- Model Size Trade-offs: Inadequate exploration of performance degradation with smaller models and the efficiency-quality balance
- Memory System Evaluation: Limited quantitative evidence of dual memory architecture's specific contributions to overall performance

## Confidence
- **High Confidence**: MLZero's superior performance on MLE-Bench Lite benchmarks compared to existing agents
- **Medium Confidence**: Claims about computational efficiency and effectiveness with 8B models
- **Low Confidence**: Claims about effectiveness on real-world, complex ML problems and multimodal data handling capabilities

## Next Checks
1. **Real-world Deployment Test**: Evaluate MLZero on at least 10 complex, real-world ML problems from industry or research, including larger datasets (>100K samples) and longer training requirements (>24 hours)

2. **Memory System Isolation**: Conduct controlled experiments isolating the semantic and episodic memory modules to quantify their individual contributions to code generation quality and debugging efficiency

3. **Cross-Domain Robustness**: Test MLZero's performance across diverse ML domains (computer vision, NLP, time series, multimodal) using standardized datasets to assess generalization beyond the primarily tabular/text focus of the current evaluation