---
ver: rpa2
title: 'Beyond Transcription: Mechanistic Interpretability in ASR'
arxiv_id: '2508.15882'
source_url: https://arxiv.org/abs/2508.15882
tags:
- layer
- whisper
- layers
- arxiv
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work applies interpretability methods to modern ASR systems,
  revealing internal mechanisms behind hallucinations, repetitions, and contextual
  errors. Using logit lens, linear probing, and activation patching adapted from LLM
  analysis, the authors show that: (1) decoder residual streams encode hallucination
  signals with up to 93.4% accuracy; (2) encoder layers capture speaker gender (94.6%),
  acoustic conditions (90%), and semantic categories (up to 97% per pair); (3) repetition
  hallucinations localize to specific cross-attention components, with one head in
  layer 18 suppressing 78.1% of cases; (4) encoders contribute semantic bias, not
  just acoustic processing.'
---

# Beyond Transcription: Mechanistic Interpretability in ASR

## Quick Facts
- arXiv ID: 2508.15882
- Source URL: https://arxiv.org/abs/2508.15882
- Reference count: 40
- One-line primary result: Mechanistic interpretability methods reveal internal mechanisms behind hallucinations, repetitions, and contextual errors in ASR systems

## Executive Summary
This work applies interpretability methods from large language models to modern ASR systems, demonstrating that these models encode rich internal representations beyond simple transcription. Using logit lens, linear probing, and activation patching, the authors reveal that decoder residual streams encode hallucination signals with up to 93.4% accuracy, encoder layers capture semantic categories with up to 97% accuracy, and repetition hallucinations localize to specific cross-attention components. The findings challenge assumptions about encoder roles and demonstrate that ASR models maintain internal quality signals and semantic biases that can override acoustic evidence.

## Method Summary
The authors apply three core interpretability techniques to ASR: (1) logit lens, which projects hidden states to vocabulary space at each layer to identify when outputs stabilize, (2) linear probing, which trains linear classifiers on frozen activations to detect encoded attributes, and (3) activation patching, which blends activations from different inputs to test causal relationships. These methods are applied to Whisper-large-v3 and Qwen2-Audio-7B-Instruct models across multiple datasets including LibriSpeech, CommonVoice, and MUSAN. The analysis examines encoder and decoder layers separately, identifying specific components responsible for hallucination detection, repetition control, and semantic encoding.

## Key Results
- Decoder residual streams encode hallucination signals with up to 93.4% accuracy, enabling internal quality detection
- Encoder layers capture semantic categories with up to 97% accuracy per pair, demonstrating semantic processing beyond acoustics
- Layer 18 head 13 cross-attention suppresses repetition hallucinations by 78.1% when targeted
- Mid-encoder layers generate grammatically coherent but ungrounded text, while final layers produce accurate transcriptions

## Why This Works (Mechanism)

### Mechanism 1: Hallucination Detection via Residual Stream Probing
Linear probes trained on decoder residual streams at layer 22 distinguish high-WER from zero-WER samples by learning decision boundaries in activation space that correlate with internal quality signals. The model represents confidence/grounding quality in linearly separable directions even when outputs appear confident.

### Mechanism 2: Repetition Suppression via Localized Cross-Attention Heads
Repetition hallucinations are causally controlled by specific cross-attention components, particularly head 13 in layer 18. When cross-attention alignment degrades, self-attention sustains repetitive patterns. Patching or ablating specific heads restores alignment, breaking the repetition loop.

### Mechanism 3: Semantic-Contextual Bias Originating in Encoder
The ASR encoder encodes semantic and contextual expectations that can override acoustic evidence, challenging assumptions that encoders are purely acoustic processors. Encoder representations at mid-to-late layers linearly separate semantic categories, and when contextual expectations conflict with acoustic input, encoder-encoded biases influence decoder output.

## Foundational Learning

- **Linear Probing**: Core method for testing whether attributes are linearly decodable from hidden states. Quick check: Can a single linear layer trained on frozen activations predict the target attribute above chance?

- **Residual Stream**: Intervention point for hallucination detection and logit lens analysis. Quick check: At a given layer, what information persists from earlier layers versus what is newly added by the current sublayer?

- **Cross-Attention in Encoder-Decoder Architectures**: Identified locus of repetition control. Quick check: If cross-attention to encoder is disrupted, what happens to output grounding?

## Architecture Onboarding

- **Component map**: Audio → Encoder layers 0–31 (semantic buildup at 18–21, repetition risk at 27–30, grounding at 31–32) → Cross-attention (critical at L18, L23) → Decoder layers → Residual stream at L22 (quality signal) → Unembedding → Tokens

- **Critical path**: Audio enters encoder (32 layers), building semantic representations through mid-layers, with repetition risks emerging at late layers. Decoder processes through 32 layers with cross-attention maintaining encoder grounding, quality signals encoded at layer 22, and final output generated.

- **Design tradeoffs**: Grounding vs. fluency (mid-encoder produces grammatical but ungrounded text; final layers resolve grounding), acoustic fidelity vs. contextual plausibility (encoder semantic bias can override acoustic evidence), intervention precision vs. coverage (single-head patching is precise but may miss distributed failures).

- **Failure signatures**: Repetition loops (triggered by cross-attention alignment breakdown at L18, sustained by self-attention, most severe at L27–30), hallucinations on non-speech (model produces coherent text despite non-speech input, detectable via residual stream probes), contextual errors (encoder semantic bias overrides acoustic truth).

- **First 3 experiments**: 1) Train linear probes on encoder activations for new attribute (e.g., speaker emotion) and verify layer-wise accuracy progression; 2) Apply activation patching to cross-attention head 13 (layer 18) on constructed repetition-prone dataset; 3) Run encoder lens analysis: pass intermediate encoder representations to decoder and categorize output types to map representational hierarchy.

## Open Questions the Paper Calls Out

### Open Question 1
Can targeted fine-tuning or surgical ablation of the identified repetition-suppressing cross-attention head permanently eliminate repetition hallucinations without inducing catastrophic forgetting or degrading performance on standard benchmarks? The paper only demonstrates inference-time patching effects.

### Open Question 2
To what extent can the semantic bias identified in the encoder be localized to specific sub-components, and does this semantic encoding serve a functional role in fluency or is it strictly a source of error? The interventions are applied broadly, leaving the mechanism unclear.

### Open Question 3
Do the specific memorized "fallback" phrases observed in Qwen2-Audio's intermediate layers originate from over-represented samples in training data, and do similar memorization circuits exist in Whisper? The authors link phrases to potential datasets but do not confirm this linkage.

## Limitations
- Analysis focuses primarily on English and limited language sets, raising questions about cross-linguistic applicability
- Linear probe accuracy doesn't establish whether representations are causally necessary or merely correlational
- Practical utility of targeted interventions without degrading overall transcription quality remains unclear
- Mechanism by which encoder representations override acoustic evidence lacks rigorous causal validation

## Confidence
- **High confidence**: Basic methodology is sound and produces interpretable results; decoder residual streams encoding hallucination signals (93.4% accuracy) is well-supported
- **Medium confidence**: Localization of repetition hallucinations to specific cross-attention heads (78.1% suppression) is supported but broader claims about distributed mechanisms are less certain
- **Low confidence**: Precise mechanisms of encoder semantic override and general applicability across diverse ASR architectures and languages are not well-established

## Next Checks
1. Apply the same mechanistic interpretability pipeline to diverse ASR models (different architectures, training objectives, language families) to test generality of findings
2. Conduct ablation studies where identified representations are systematically removed or modified to measure actual impact on model outputs
3. Test whether identified mechanisms generalize across languages with different phonological and syntactic structures, particularly non-Indo-European languages