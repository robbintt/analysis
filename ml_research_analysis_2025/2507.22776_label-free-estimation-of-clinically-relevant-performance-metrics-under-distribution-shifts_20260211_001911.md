---
ver: rpa2
title: Label-free estimation of clinically relevant performance metrics under distribution
  shifts
arxiv_id: '2507.22776'
source_url: https://arxiv.org/abs/2507.22776
tags:
- performance
- test
- accuracy
- metrics
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating clinically relevant
  performance metrics for medical image classification models when ground-truth labels
  are unavailable in the target dataset. The authors generalize two existing confidence-based
  performance estimation methods (ATC and DoC) to directly estimate the full confusion
  matrix, enabling the calculation of metrics like precision, recall, F1-score, and
  AUC without labeled test data.
---

# Label-free estimation of clinically relevant performance metrics under distribution shifts

## Quick Facts
- arXiv ID: 2507.22776
- Source URL: https://arxiv.org/abs/2507.22776
- Reference count: 26
- Primary result: CM-ATC and CM-DoC generalize confidence-based methods to estimate full confusion matrices, enabling label-free performance monitoring under distribution shifts

## Executive Summary
This paper addresses the challenge of estimating clinically relevant performance metrics for medical image classifiers when ground-truth labels are unavailable in the target dataset. The authors extend two existing confidence-based performance estimation methods (ATC and DoC) to directly estimate the full confusion matrix, enabling calculation of metrics like precision, recall, F1-score, and AUC without labeled test data. Through extensive benchmarking on chest x-ray datasets under real-world and controlled distribution shifts, CM-ATC and CM-DoC consistently outperform baseline methods, with CM-ATC showing the lowest overall error. However, all methods struggle with prevalence shifts, highlighting important limitations that require further research.

## Method Summary
The paper generalizes confidence-based performance estimation methods to directly predict the full confusion matrix. CM-ATC learns class-specific thresholds on validation data such that the fraction of scores above each threshold matches observed PPV and NPV respectively. CM-DoC recalibrates confidence scores by computing offsets between validation and test average confidences for each class. Both methods estimate PPV and NPV on test data, derive confusion matrix entries, and compute any counting metric including multi-threshold AUC. The approach is benchmarked against CBPE and naive baselines on three chest x-ray datasets under real-world distribution shifts and controlled covariate/prevalence shifts.

## Key Results
- CM-ATC and CM-DoC outperformed CBPE and naive baselines across most metrics and settings
- CM-ATC showed the lowest overall mean absolute error (MAE) for estimating clinically relevant metrics
- All methods struggled with prevalence shifts, showing elevated MAE for prevalence-dependent metrics (PPV, F1)
- Methods handled covariate shifts reasonably well, with MAE typically 0.05-0.15 for out-of-distribution performance estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating positive and negative prediction sets with class-specific thresholds enables confusion matrix estimation
- Mechanism: CM-ATC learns two thresholds (t⁺_ATC, t⁻_ATC) on validation data such that the fraction of scores exceeding each threshold matches the observed PPV and NPV respectively. Test-set estimates are then computed as PPV̂ = E_{s∼I⁺_test}[I[s > t⁺_ATC]] and NPV̂ = E_{s∼I⁻_test}[I[s > t⁻_ATC]].
- Core assumption: The relationship between confidence score distributions and predictive values generalizes from validation to test data under distribution shift.
- Break condition: When class-wise calibration degrades severely (e.g., under prevalence shift), the PPV/NPV relationships no longer transfer—observed in Figure 4 with high MAE for prevalence-dependent metrics.

### Mechanism 2
- Claim: Recalibrating confidence scores by the gap between validation performance and validation confidence improves estimation robustness
- Mechanism: CM-DoC computes offsets ∆⁺ = E_{s∼I⁺_val}[s] - E_{s∼I⁺_test}[s] and ∆⁻ = E_{s∼I⁻_val}[s] - E_{s∼I⁻_test}[s], then estimates PPV̂ = PPV_val - ∆⁺ and NPV̂ = NPV_val - ∆⁻.
- Core assumption: Shifts in average confidence between validation and test reflect changes in actual performance; this linear recalibration captures the degradation.
- Break condition: Under prevalence shifts, calibration error changes non-linearly (Figure 4h shows RBS/ACE varying with prevalence), violating the linear offset assumption.

### Mechanism 3
- Claim: Full confusion matrix estimation enables computation of any counting metric, including multi-threshold AUC
- Mechanism: From PPV and NPV estimates, confusion matrix entries are derived via: tp̂ = n⁺_test · PPV̂, fp̂ = n⁺_test - tp̂, tn̂ = n⁻_test · NPV̂, fn̂ = n⁻_test - tn̂. These enable recall, F1, specificity. AUC is approximated by evaluating TPR/FPR at 100 threshold quantiles and numerical integration.
- Core assumption: PPV/NPV estimates are sufficiently accurate to propagate through confusion matrix calculations without compounding error.
- Break condition: When base PPV/NPV estimates have high error (Figure 2f shows CBPE MAE~0.47 for PPV), propagated metrics like F1 also fail catastrophically.

## Foundational Learning

- Concept: Model calibration (P(Y=1|S=s) = s)
  - Why needed here: All confidence-based methods assume or implicitly rely on calibrated confidence scores; miscalibration directly causes estimation error.
  - Quick check question: If your model outputs 0.8 confidence for 100 samples, should ~80 of them be positive? What happens if only 50 are?

- Concept: Confusion matrix decomposition (PPV = TP/(TP+FP), NPV = TN/(TN+FN))
  - Why needed here: The method recovers full confusion matrix from PPV/NPV estimates plus prediction counts (n⁺, n⁻); understanding this enables extending to any derived metric.
  - Quick check question: Given n⁺=100 predictions above threshold with estimated PPV=0.7, how many true positives are estimated?

- Concept: Distribution shift types (covariate shift vs. prevalence/label shift)
  - Why needed here: Results show methods handle covariate shift reasonably but degrade under prevalence shift (Figure 4); practitioners must identify which shift type they face.
  - Quick check question: If your test set has the same feature distribution but 90% positives vs. 30% in validation, which shift type is this and what estimation challenges should you expect?

## Architecture Onboarding

- Component map: Pretrained classifier f(x) -> validation predictions (compute PPV_val, NPV_val, t⁺_ATC, t⁻_ATC or ∆⁺, ∆⁻) -> unlabeled test set -> test prediction partitioning (I⁺_test, I⁻_test) -> PPV̂, NPV̂ estimation -> confusion matrix assembly -> metric computation

- Critical path: (1) Proper threshold selection on validation -> (2) Correct partitioning of test predictions -> (3) Accurate PPV/NPV estimation -> (4) Confusion matrix assembly -> (5) Metric computation

- Design tradeoffs:
  - CM-ATC vs. CM-DoC: CM-ATC showed lowest overall MAE but requires threshold learning; CM-DoC is simpler but slightly higher error
  - Naive vs. CM variants: Naive ATC/DoC work well for prevalence-independent metrics but fail for prevalence-dependent ones (Figure 4e-g)
  - Calibration pre-processing: Paper found temperature scaling provided no systematic improvement (Supplementary Fig. S1)—adds complexity without clear benefit

- Failure signatures:
  - Prevalence shift: All methods show elevated MAE, especially for PPV/F1 (Figure 4f-g); calibration error varies with prevalence (Figure 4h)
  - Covariate shift with spurious correlations: Overconfident predictions on minority groups cause overestimation (Figure 3)
  - Severe miscalibration: CBPE fails dramatically for PPV (MAE~0.47 in Figure 2f) when ACE/RBS is high

- First 3 experiments:
  1. Reproduce in-distribution validation: Train classifier on CheXpert train split, compute PPV_val/NPV_val on validation, verify CM-ATC/CM-DoC estimates match realized metrics on held-out test (target MAE<0.02)
  2. Test cross-dataset generalization: Apply CheXpert-trained model to PadChest/NIH test sets; compare CM-ATC, CM-DoC, CBPE across all metrics (expect MAE~0.05-0.15 for o.o.d. per Table S2)
  3. Controlled prevalence shift: Sample CheXpert test at 5%, 38%, 95% prevalence; plot estimated vs. realized metrics to reproduce Figure 4 failure mode and identify break-even prevalence range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can performance estimation methods be made robust to prevalence (label) shifts?
- Basis in paper: [explicit] The authors state their "simulated shift scenarios exposed important failure modes of current performance estimation techniques" and show that "all estimation methods struggled under prevalence shifts, especially for prevalence-dependent metrics."
- Why unresolved: The paper demonstrates systematic failures under prevalence shifts but does not propose solutions; the methods assume stable class distributions between validation and test sets.
- What evidence would resolve it: Development of estimation methods that maintain low MAE across varying prevalence levels (5%-95%), or integration with label shift adaptation techniques.

### Open Question 2
- Question: Can domain adaptation techniques effectively mitigate the calibration degradation caused by distribution shifts?
- Basis in paper: [explicit] The authors explicitly recommend: "Especially under prevalence shifts, domain adaptation techniques could help counter the systematic negative impact on model calibration."
- Why unresolved: The paper identifies calibration error as a key failure mode but does not evaluate whether domain adaptation could address this.
- What evidence would resolve it: Empirical evaluation of methods like maximum likelihood bias correction combined with CM-ATC/CM-DoC under controlled prevalence shifts.

### Open Question 3
- Question: How can distribution shift detection and identification be optimally integrated with label-free performance monitoring?
- Basis in paper: [explicit] The authors conclude that "performance monitoring should be accompanied by distribution shift detection, identification, and mitigation" and note their estimators' accuracy depends on shift type.
- Why unresolved: The paper evaluates estimation methods in isolation without coupling them with automated shift detection mechanisms.
- What evidence would resolve it: A unified framework where shift detection triggers appropriate estimation strategies, with demonstrated improvements in clinical deployment simulations.

## Limitations

- All methods struggle with prevalence shifts, showing high MAE for prevalence-dependent metrics (PPV, F1) when test prevalence deviates significantly from validation
- Methods rely on the assumption that confidence score distributions generalize from validation to test data, which breaks down under severe distribution shifts
- Controlled experiments use simplified covariate shifts (binary artefact) that may not capture real-world complexity

## Confidence

- High Confidence: CM-ATC/CM-DoC outperform CBPE and naive baselines on covariate shifts for most metrics (Table S2, Figure 2)
- Medium Confidence: Methods work reasonably for prevalence-independent metrics (accuracy, recall, specificity) across shift types
- Medium Confidence: Failure under prevalence shifts is real but may be improvable with calibration or adaptation techniques

## Next Checks

1. Test calibration preprocessing (temperature scaling, isotonic regression) to see if it improves CM-ATC/CM-DoC performance under prevalence shifts
2. Evaluate on more complex real-world distribution shifts beyond binary artefacts and simple prevalence resampling
3. Assess computational efficiency and runtime overhead compared to CBPE for clinical deployment scenarios