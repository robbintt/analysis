---
ver: rpa2
title: 'Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning
  Models'
arxiv_id: '2505.17225'
source_url: https://arxiv.org/abs/2505.17225
tags:
- reasoning
- arxiv
- question
- original
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit reasoning rigidity, overriding explicit
  user instructions in favor of familiar reasoning patterns, leading to incorrect
  conclusions. This behavior is particularly problematic in domains like mathematics
  and logic puzzles where precise adherence to constraints is critical.
---

# Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models

## Quick Facts
- **arXiv ID:** 2505.17225
- **Source URL:** https://arxiv.org/abs/2505.17225
- **Reference count:** 40
- **Key outcome:** Large language models override explicit user instructions in favor of familiar reasoning patterns, leading to incorrect conclusions, particularly in mathematics and logic puzzles.

## Executive Summary
Large language models exhibit reasoning rigidity, systematically overriding explicit user instructions to follow familiar reasoning patterns. This behavior leads to incorrect conclusions in domains requiring precise adherence to constraints, such as mathematics and logic puzzles. The authors introduce ReasoningTrap, a diagnostic dataset comprising modified mathematical benchmarks and puzzles with subtle alterations designed to require deviation from familiar reasoning strategies. Analysis reveals three contamination patterns where models ignore or distort provided instructions, with base models generally outperforming reasoning-tuned counterparts, suggesting current reinforcement learning methods may exacerbate this rigidity.

## Method Summary
The authors developed ReasoningTrap, a diagnostic dataset created by modifying established mathematical benchmarks (AIME, MATH500) and puzzles with subtle alterations that require deviation from familiar reasoning strategies. They introduced the Contamination Ratio metric to quantify the proportion of reasoning steps contaminated by familiar patterns. The dataset was evaluated across various reasoning models, comparing base models with their reasoning-tuned counterparts. Analysis focused on identifying specific contamination patterns through systematic examination of model outputs, revealing how models systematically override explicit instructions in favor of familiar reasoning approaches.

## Key Results
- Models exhibit three recurring contamination patterns: Interpretation Overload, Input Distrust, and Partial Instruction Attention
- Base models generally outperform their reasoning-tuned counterparts on the diagnostic set
- Current reinforcement learning methods may exacerbate reasoning rigidity rather than improve instruction adherence

## Why This Works (Mechanism)
The phenomenon of instruction overriding stems from models' learned preference for familiar reasoning patterns over explicit instructions. When faced with modified problems that require deviation from established strategies, models default to their trained reasoning approaches rather than following provided constraints. This creates a systematic bias where the model's confidence in familiar patterns overrides explicit user directives, particularly when the new instructions conflict with established reasoning strategies.

## Foundational Learning
- **Contamination Ratio metric**: Quantifies instruction adherence by measuring reasoning steps contaminated by familiar patterns; needed to systematically evaluate instruction-following capability
- **Reasoning rigidity**: Models' tendency to default to familiar reasoning approaches; explains why models override explicit instructions
- **Instruction overriding**: Systematic behavior where models prioritize learned patterns over user constraints; fundamental limitation in current reasoning models

## Architecture Onboarding
- **Component map:** Input processing -> Reasoning pattern matching -> Contamination detection -> Output generation
- **Critical path:** Modified problem input → Familiar pattern recognition → Instruction conflict → Output override
- **Design tradeoffs:** Model performance on familiar patterns vs. instruction adherence; established reasoning strategies vs. flexibility
- **Failure signatures:** Systematic rejection of explicit constraints, partial attention to instructions, input distrust
- **3 first experiments:** 1) Test base models vs. reasoning-tuned variants on modified benchmarks, 2) Analyze Contamination Ratio across different problem types, 3) Identify specific contamination patterns in model outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may reflect dataset-specific biases rather than general superiority of base models
- Analysis focuses on post-hoc evaluation rather than intervention strategies
- Limited exploration of whether behavior stems from architectural constraints versus training methodology

## Confidence
- **High:** Evidence for instruction overriding behavior in reasoning models; validity of Contamination Ratio metric
- **Medium:** Generalizability of reasoning rigidity beyond mathematical domains; superiority of base models
- **Low:** Long-term persistence of this behavior with future architectural advances

## Next Checks
1. Test reasoning models on diverse instruction-following benchmarks outside mathematics to assess domain generality
2. Conduct ablation studies isolating the impact of specific RLHF components on instruction adherence
3. Evaluate whether fine-tuning with explicit instruction-following objectives can reduce contamination ratios