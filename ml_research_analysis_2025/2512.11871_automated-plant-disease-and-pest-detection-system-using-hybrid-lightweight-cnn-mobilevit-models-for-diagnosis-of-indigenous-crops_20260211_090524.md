---
ver: rpa2
title: Automated Plant Disease and Pest Detection System Using Hybrid Lightweight
  CNN-MobileViT Models for Diagnosis of Indigenous Crops
arxiv_id: '2512.11871'
source_url: https://arxiv.org/abs/2512.11871
tags:
- dataset
- disease
- plant
- global
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of automated plant disease detection
  for indigenous crops, specifically cactus-fig, in resource-constrained and infrastructure-limited
  regions like Tigray, Ethiopia. To overcome the limitations of existing models designed
  for global commercial crops, the authors curated a new dataset of 3,587 field images
  of Opuntia ficus-indica and benchmarked three mobile-efficient architectures: a
  custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS.'
---

# Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops

## Quick Facts
- arXiv ID: 2512.11871
- Source URL: https://arxiv.org/abs/2512.11871
- Reference count: 15
- Primary result: MobileViT-XS achieves 97.3% accuracy for indigenous cactus-fig disease detection on resource-constrained devices

## Executive Summary
This paper addresses the challenge of automated plant disease detection for indigenous crops, specifically cactus-fig (Opuntia ficus-indica), in resource-constrained regions like Tigray, Ethiopia. The authors developed a new dataset of 3,587 field images and benchmarked three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. The results show a clear trade-off between accuracy and deployment efficiency, with MobileViT-XS delivering superior accuracy through multi-head self-attention mechanisms while the lightweight CNN offers the most favorable deployment profile. The models are deployed in a localized Flutter application supporting fully offline inference on Cortex-A53 class devices.

## Method Summary
The study involved curating a new dataset of 3,587 field images of cactus-fig plants across three classes: Affected (cochineal infestation or fungal rot), Healthy, and No Cactus. Models were pre-trained on 26,394 PlantVillage images before fine-tuning on the cactus dataset. Three architectures were evaluated: a custom lightweight CNN (1.2M parameters), EfficientNet-Lite1, and MobileViT-XS (2.3M parameters). Training used PyTorch with timm library, AdamW optimizer, cosine decay learning rate scheduler, and data augmentation applied only to training splits. Models were exported to TensorFlow Lite with Float16 post-training quantization for deployment on ARM Cortex-A53 devices.

## Key Results
- MobileViT-XS achieved 97.3% mean cross-validation accuracy, significantly outperforming both CNN (89.5%) and EfficientNet-Lite1 (90.7%)
- Lightweight CNN provided optimal deployment profile: 42 ms inference latency and 4.8 MB model size
- MobileViT-XS demonstrated superior ability to disambiguate pest clusters from fungal lesions through global reasoning
- All models successfully deployed in a Tigrigna and Amharic localized Flutter application for offline inference

## Why This Works (Mechanism)

### Mechanism 1: MHSA-based Global Context Disambiguation
Multi-Head Self-Attention enables more reliable discrimination between visually similar 3D pest clusters and 2D fungal lesions compared to local convolutional kernels alone. MHSA computes pairwise relationships between all image patches simultaneously, allowing the model to recognize that cochineal infestations exhibit spatially correlated cluster patterns, whereas scarring and fungal lesions appear as random, isolated patches.

### Mechanism 2: Hybrid CNN-Transformer Feature Fusion
MobileViT's interleaved convolution and transformer blocks preserve local texture extraction while adding global reasoning, without requiring pure-ViT data volumes. Standard convolutions process local neighborhoods for edge/texture features; these are then fed to transformer blocks that model long-range dependencies across the feature map, combining inductive bias with learned attention.

### Mechanism 3: Post-Training Quantization for Edge Deployment
Float16 quantization enables offline inference on ARM Cortex-A53 devices with acceptable accuracy retention. PTQ maps 32-bit floating-point weights to 16-bit floats via scaling factor and zero-point, reducing memory bandwidth and enabling mobile-optimized arithmetic.

## Foundational Learning

- **Multi-Head Self-Attention (MHSA)**
  - Why needed here: Explains why MobileViT-XS achieves 97.3% accuracy where pure CNNs fail on spatially ambiguous symptoms.
  - Quick check question: Can you explain why attending to all patch pairs simultaneously helps distinguish clustered pests from random scars?

- **Inductive Bias vs Learned Attention**
  - Why needed here: Understanding the CNN vs Transformer trade-off is essential for selecting the right architecture for edge deployment constraints.
  - Quick check question: What inductive bias do convolutions provide that pure ViTs lack, and why does MobileViT preserve it?

- **Post-Training Quantization (PTQ)**
  - Why needed here: Deployment on Cortex-A53 devices with 4.8–9.3 MB models requires understanding compression-accuracy trade-offs.
  - Quick check question: What is the difference between FP32, FP16, and INT8 quantization, and which does this paper use?

## Architecture Onboarding

- **Component map:**
  Field Images (3,587) → Resize 256×256 → Augmentation (train only) → Pre-training on Multi-Crop Backbone (26,394 images) → Fine-tune: Lightweight CNN (1.2M params) | MobileViT-XS (2.3M params) → TensorFlow Lite Export → Float16 PTQ → Flutter App (Tigrigna/Amharic UI) → Offline Inference on ARM Cortex-A53

- **Critical path:**
  1. Dataset curation with proper class balance (Affected/Healthy/No Cactus)
  2. Augmentation applied to training split only (sanitized validation protocol)
  3. Architecture selection based on deployment constraints
  4. Quantization and on-device latency validation

- **Design tradeoffs:**
  | Model | Accuracy | Latency | Size | Use Case |
  |-------|----------|---------|------|----------|
  | Custom CNN | 89.5% | 42 ms | 4.8 MB | Real-time scanning on legacy devices |
  | EfficientNet-Lite1 | 90.7% | 55 ms | 19 MB | Mid-tier devices |
  | MobileViT-XS | 97.3% | 68 ms | 9.3 MB | High-precision verification |

- **Failure signatures:**
  - CNN false positives: Older scarred cladodes misclassified as Affected (68% of CNN errors)
  - Specular highlight confusion: Waxy cuticle reflections trigger false lesion detection in CNNs
  - Background artifacts: Mitigated by explicit "No Cactus" rejection class

- **First 3 experiments:**
  1. Reproduce the 3-fold cross-validation benchmark on the open-sourced cactus-fig dataset, confirming the 89.5% vs 97.3% accuracy gap between CNN and MobileViT-XS.
  2. Ablate the augmentation pipeline to measure domain shift sensitivity (train with/without augmentation, test on held-out field images).
  3. Profile on-device inference on a Cortex-A53 class device, measuring actual latency and memory footprint post-quantization against reported 42 ms / 68 ms figures.

## Open Questions the Paper Calls Out

### Open Question 1
Can a cascaded deployment strategy, where the lightweight CNN acts as a trigger for the MobileViT model, effectively optimize battery and computational resources on legacy edge devices? The current study benchmarks models in isolation but does not test the system-level overhead or energy consumption of switching between models on a single device.

### Open Question 2
Can the feature extractors trained on Opuntia ficus-indica transfer effectively to structurally distinct regional staples like Teff and Sorghum? The authors list "Dataset Expansion" to include Teff and Sorghum as a specific goal, but it's unclear if the "indigenous morphology" learned here hinders or helps the transfer learning process for grain crops.

### Open Question 3
Can the MobileViT architecture distinguish between simultaneous co-infections (fungal rot and cochineal clusters) without merging them into a single "Affected" class? While the model demonstrates high accuracy for the binary "Affected" vs. "Healthy" distinction, the paper does not evaluate the model's multi-label classification capability when a single plant exhibits both 3D pest clusters and 2D lesions simultaneously.

## Limitations

- The custom lightweight CNN architecture is underspecified (only "3 conv blocks" described), preventing exact reproduction
- Domain shift from lab-captured to field-captured images is partially mitigated by augmentations but not fully characterized
- Field evaluation only confirms deployment feasibility on Cortex-A53 class devices without benchmarking against other edge hardware

## Confidence

- MobileViT-XS accuracy superiority (97.3% vs 89.5% CNN): High confidence based on cross-validation protocol and clear architectural distinction
- Deployment on Cortex-A53 with Float16 quantization: Medium confidence—quantization scheme validated, but on-device profiling data limited
- Generalizability to other indigenous crops: Low confidence—results specific to cactus-fig morphology and pest types

## Next Checks

1. Replicate the 3-fold cross-validation benchmark using the open-sourced dataset, measuring the exact accuracy gap between CNN and MobileViT-XS models
2. Conduct ablation studies on the augmentation pipeline to quantify domain shift mitigation effectiveness
3. Profile on-device inference latency and memory usage on a Cortex-A53 class device, comparing against reported 42 ms (CNN) and 68 ms (MobileViT-XS) figures