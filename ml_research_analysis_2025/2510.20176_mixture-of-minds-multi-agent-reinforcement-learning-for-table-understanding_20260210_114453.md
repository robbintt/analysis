---
ver: rpa2
title: 'Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding'
arxiv_id: '2510.20176'
source_url: https://arxiv.org/abs/2510.20176
tags:
- agent
- arxiv
- table
- reasoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of table understanding, where
  large language models struggle with arithmetic errors and hallucinations in reasoning
  tasks. The authors propose Mixture-of-Minds, a multi-agent framework that decomposes
  table reasoning into three specialized roles: planning, coding, and answering.'
---

# Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding

## Quick Facts
- arXiv ID: 2510.20176
- Source URL: https://arxiv.org/abs/2510.20176
- Reference count: 19
- Primary result: 62.13% accuracy on TableBench, surpassing OpenAI-o4-mini-high

## Executive Summary
Mixture-of-Minds addresses table understanding challenges where large language models struggle with arithmetic errors and hallucinations in reasoning tasks. The authors propose a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. By leveraging Monte Carlo Tree Search rollouts to generate pseudo-gold intermediate supervision for reinforcement learning optimization, the framework achieves state-of-the-art performance on the TableBench benchmark while demonstrating strong generalization on FinQA.

## Method Summary
The framework employs a sequential multi-agent pipeline where a Planning agent generates reasoning steps, a Coding agent translates these steps into executable Python code, and an Answering agent synthesizes the final response. Training uses MCTS rollouts to sample candidate trajectories, filtering for those matching ground truth answers to create pseudo-gold intermediate supervision. The agents are then optimized sequentially using Group Relative Policy Optimization (GRPO) with carefully designed reward functions for each role, including BLEU scores for plan matching, execution accuracy for code, and exact match for final answers.

## Key Results
- Achieves 62.13% accuracy on TableBench benchmark
- Outperforms OpenAI-o4-mini-high on the same task
- Demonstrates strong generalization on FinQA dataset

## Why This Works (Mechanism)
The approach works by decomposing complex table reasoning into specialized subtasks that can be optimized independently while maintaining end-to-end coherence. The MCTS rollouts provide diverse candidate trajectories that capture different valid reasoning paths, while the filtering mechanism ensures only successful trajectories contribute to training. The sequential GRPO training allows each agent to specialize in its role while benefiting from the optimized performance of previous agents in the pipeline.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: A search algorithm that balances exploration and exploitation to find optimal decision paths. Needed to generate diverse candidate trajectories for training data augmentation.
- **Group Relative Policy Optimization (GRPO)**: An RL algorithm that optimizes policies by comparing group performance rather than individual trajectories. Quick check: Verify gradient updates use group-level statistics.
- **Pandas Operation Extraction**: Parsing Python AST to identify data manipulation operations. Quick check: Ensure operation normalization handles equivalent Pandas expressions consistently.

## Architecture Onboarding
- **Component Map**: Planning Agent -> Coding Agent -> Answering Agent -> Table + Question
- **Critical Path**: Query → Plan → Code → Execute → Answer
- **Design Tradeoffs**: Sequential vs parallel agent execution; specialized vs unified agents; reward function complexity vs training stability
- **Failure Signatures**: Planning errors cascade to coding; code execution failures halt pipeline; answer synthesis may introduce hallucinations
- **First Experiments**: 1) Test MCTS rollout success rate on training data, 2) Validate reward function implementations, 3) Benchmark individual agent performance

## Open Questions the Paper Calls Out
None

## Limitations
- MCTS rollout procedure may not scale well to complex or open-ended tasks with multiple valid answers
- Sequential training creates cascading dependencies where early agent errors cannot be recovered
- Reliance on Pandas-based code execution limits applicability to non-dataframe tabular structures

## Confidence
- **High Confidence**: Core architectural innovation of task decomposition
- **Medium Confidence**: Empirical results and performance improvements
- **Low Confidence**: Reproducibility of exact training procedure without additional implementation details

## Next Checks
1. Implement MCTS rollout procedure on TableInstruct and measure success rate of generating correct answer trajectories
2. Evaluate trained agents on an additional table understanding dataset not mentioned in the paper (e.g., WikiTableQuestions)
3. Create ablation study comparing full multi-agent pipeline against simplified versions (single agent, two-agent combinations)