---
ver: rpa2
title: Arabic Metaphor Sentiment Classification Using Semantic Information
arxiv_id: '2504.19590'
source_url: https://arxiv.org/abs/2504.19590
tags:
- sentiment
- metaphor
- classification
- arabic
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Arabic sentiment classification
  method using semantic tags for metaphors, addressing the lack of tools for Arabic
  metaphor identification in sentiment analysis. The approach leverages the Arabic
  Metaphor Corpus (AMC) and the Arabic Semantic Tagger (AraSAS) to classify sentiment
  by counting and scoring emotional tags with varying polarity intensities.
---

# Arabic Metaphor Sentiment Classification Using Semantic Information

## Quick Facts
- **arXiv ID**: 2504.19590
- **Source URL**: https://arxiv.org/abs/2504.19590
- **Reference count**: 9
- **Primary result**: F-scores range from 0.09 to 0.95 across review lengths, with shorter reviews yielding higher accuracy

## Executive Summary
This paper introduces the first Arabic sentiment classification method using semantic tags for metaphors, addressing the lack of tools for Arabic metaphor identification in sentiment analysis. The approach leverages the Arabic Metaphor Corpus (AMC) and the Arabic Semantic Tagger (AraSAS) to classify sentiment by counting and scoring emotional tags with varying polarity intensities. Two tools are developed: one for overall sentiment and another incorporating metaphor sentiment. Evaluation using precision, recall, and F-score shows F-scores ranging from 0.09 to 0.95 across review lengths, with shorter reviews yielding higher accuracy. The results demonstrate that semantic tags can effectively support metaphor-based sentiment analysis in Arabic, highlighting the impact of review length on classification performance.

## Method Summary
The method uses AraSAS to annotate Arabic text with semantic tags, focusing on emotional (E) tags that carry polarity markers. A scoring function weights these tags (E+ = +0.5, E++ = +1, E+++ = +1.5, and negative equivalents) and sums them to determine overall sentiment. For metaphor-aware classification, a ±2 boost is added based on metaphor sentiment annotations. The approach is evaluated on the AMC corpus using precision, recall, and F-score metrics across different review lengths, revealing that shorter texts achieve significantly higher accuracy.

## Key Results
- F-scores range from 0.09 to 0.95, with shorter reviews (1-4 tokens) achieving 0.661 F-score
- Review length inversely affects accuracy, with long reviews (≥1000 tokens) dropping to 0.33 F-score
- The metaphor-aware tool achieves highest F-score of 0.9 for reviews between 5-100 tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counting and weighting semantic emotional (E) tags with polarity signs produces a sentiment score that correlates with gold-standard annotations, particularly for shorter texts.
- Mechan: AraSAS tags text with E-tags carrying polarity markers (+, ++, +++, -, –, —). A scoring function adds 0.5/1.0/1.5 for positive intensities and subtracts equivalent values for negatives. The sum determines final polarity (positive if >0, negative if <0, neutral otherwise).
- Core assumption: Emotional tag density and polarity intensity aggregate meaningfully to reflect overall sentiment; neutral E-tags can be safely excluded without major signal loss.
- Evidence anchors:
  - [section 2.1]: "For a positive emotional tag (item) from the pre-defined list, I added 0.5 to the count... for a double positive E++ the function adds 1... for triple positive emotional tags (E+++) the function adds 1.5"
  - [Table 1]: Short reviews (1–4 tokens) achieved F-score 0.661 with the overall sentiment tool
  - [corpus]: Weak direct evidence—neighbor papers address metaphor sentiment but not this specific scoring scheme
- Break condition: When neutral tags dominate or when positive/negative E-tags are equal in count, the simplified aggregation may default to neutral incorrectly.

### Mechanism 2
- Claim: Explicitly boosting metaphor sentiment scores (±2) changes classification outcomes, but performance varies substantially by review length and annotation type.
- Mechan: The metaphor-aware tool adds 2 to the sentiment score if metaphor polarity is positive and subtracts 2 if negative, overlaying this on the semantic tag sentiment base. This reflects an assumption that metaphor sentiment carries disproportionate weight.
- Assumption: Metaphor sentiment is a stronger signal than non-metaphorical emotional tags; the ±2 boost is appropriate across all contexts.
- Evidence anchors:
  - [section 2.2]: "a score of 2 is added to the sentiment score if the polarity is positive and subtracted 2 if it is negative"
  - [section 4]: "The highest F-score is 0.9 from the second tool for the review length between 5 and 1"
  - [corpus]: "Cultural Bias Matters" paper notes metaphor importance in NLP but highlights cross-cultural variation, suggesting the ±2 weight may not generalize
- Break condition: When metaphor annotations are sparse, absent, or noisy relative to other signals, the boost can distort overall polarity.

### Mechanism 3
- Claim: Review length inversely affects classification accuracy with this semantic-tag counting approach.
- Mechan: Shorter texts have fewer E-tags, reducing aggregation noise and equalizing polarity scenarios. Longer texts accumulate more tags, increasing chances of conflicting signals and neutral tie-breaks.
- Core assumption: The scoring function's linear aggregation does not scale well with token count; longer texts require more sophisticated handling.
- Evidence anchors:
  - [section 4]: "the lowest F-score has 0.33 for the tokens above or equal to 1000, which means the tool's prediction is not highly accurate with the high number of tokens"
  - [Table 2]: F-scores degrade from 0.95 (1–4 tokens) to 0.33 (≥1000 tokens)
  - [corpus]: "Multilingual Sentiment Analysis of Summarized Texts" examines length effects but for summarization, not this specific mechanism—weak direct support
- Break condition: If longer texts have consistent dominant polarity (e.g., strongly positive throughout), the inverse relationship may not hold.

## Foundational Learning

- Concept: Semantic tagging (semantic categories assigned to tokens/units)
  - Why needed here: AraSAS outputs depend on understanding what semantic tags represent and how E-tags differ from other tag types.
  - Quick check question: Can you explain the difference between a semantic tag and a POS tag?

- Concept: Polarity intensity weighting
  - Why needed here: The scoring mechanism uses graduated weights (0.5, 1.0, 1.5) based on sign repetition.
  - Quick check question: Why might triple-weighted tags (+++) not be 3× the single-weight value?

- Concept: Precision/Recall/F-score evaluation
  - Why needed here: All tool comparisons use these metrics; understanding their tradeoffs is essential for interpreting results.
  - Quick check question: If a system has high precision but low recall on negative reviews, what does that mean practically?

## Architecture Onboarding

- Component map:
  - **AMC Corpus** → 1,000 Arabic book reviews with manual metaphor, sentiment, and gold-standard annotations
  - **AraSAS Tagger** → Assigns semantic tags including E-tags with polarity markers; batch limit of 100 reviews
  - **Tool 1 (Overall Sentiment)** → Counts/weights E-tags, excludes neutrals, outputs polarity
  - **Tool 2 (Metaphor-Aware)** → Adds ±2 metaphor boost to Tool 1's base score
  - **Evaluation Module** → Python-based precision/recall/F-score computation against gold standard

- Critical path:
  1. Preprocess AMC reviews (replace punctuation that AraSAS treats as line breaks)
  2. Batch submit to AraSAS (100-review chunks)
  3. Parse tagged output into data frame
  4. Apply scoring function for E-tag polarity aggregation
  5. (For Tool 2) Integrate metaphor gold-standard annotations with ±2 boost
  6. Compare against gold standard; compute metrics

- Design tradeoffs:
  - Excluding neutral E-tags simplifies logic but risks losing signal in balanced texts
  - ±2 metaphor boost assumes metaphor is consistently more important than other sentiment cues
  - No automatic metaphor detection—relies on pre-annotated AMC gold standard
  - AraSAS batch limits require manual chunking; not pipeline-scalable without wrapping

- Failure signatures:
  - F-score drops sharply for texts ≥500 tokens (0.33–0.53 range)
  - Neutral classification becomes unreliable when E-tag counts are equal
  - Tool 2 underperforms Tool 1 for some categories despite metaphor boost (e.g., neutral reviews: 0.19 vs 0.11)
  - AraSAS fails on special characters like asterisks

- First 3 experiments:
  1. **Baseline replication**: Run Tool 1 on a fresh sample from AMC to verify F-score distribution by length; confirm the 0.09–0.95 range is reproducible.
  2. **Ablation on metaphor boost**: Test Tool 2 with ±1, ±1.5, and ±2 boost values to determine if the 2-point weight is optimal or arbitrary.
  3. **Neutral tag handling**: Include neutral E-tags in scoring (e.g., +0 contribution but counted for tie-breaking) and compare F-scores for balanced-sentiment reviews.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantic tags effectively function as the sole feature for automatic Arabic metaphor detection, removing the need for pre-annotated metaphor data?
- Basis in paper: [explicit] The conclusion states that "semantic tags can be used to even test the existence of metaphor, which is an alternative solution for pre-annotation data for Arabic metaphor detection."
- Why unresolved: The current study utilized the pre-annotated Arabic Metaphor Corpus (AMC) to classify sentiment but did not implement or test a module for detecting the metaphors themselves.
- What evidence would resolve it: An experiment evaluating the precision and recall of identifying metaphorical text using only AraSAS tags compared to the AMC gold standard.

### Open Question 2
- Question: How does incorporating neutral tags and resolving polarity equality cases impact the overall F-score compared to the current method of exclusion?
- Basis in paper: [explicit] The author notes, "I will evaluate these methods further... as suggestions for further research, because for better sentiment classification in regards to metaphor, all the polarity signs should be considered in the classification."
- Why unresolved: To simplify the initial tool, neutral tags were removed and equality cases were glossed over to avoid "complications," leaving the impact of these elements unknown.
- What evidence would resolve it: A comparative evaluation of the current tool against a modified version that includes logic to weigh neutral tags and break polarity ties.

### Open Question 3
- Question: Which specific textual features or semantic patterns are required to maintain classification accuracy in longer reviews (>500 tokens) where the tool currently struggles?
- Basis in paper: [inferred] The results show F-scores dropping significantly (to 0.33) for long texts compared to short ones (0.95), and the author notes the need for "tools that contain more features to capture other aspects of the text."
- Why unresolved: The paper identifies the performance gap based on review length but does not isolate the linguistic factors (e.g., complex negation, mixed topics) causing the degradation in longer texts.
- What evidence would resolve it: Feature ablation studies on long-text subsets to identify which semantic or syntactic elements, if added to the tagger, recover the lost accuracy.

## Limitations

- The approach relies entirely on pre-annotated metaphor sentiment rather than automatic metaphor detection, limiting real-world applicability
- The ±2 boost for metaphor sentiment is empirically chosen without comparative analysis of alternative weights
- Performance degrades significantly for longer reviews (F-scores drop from 0.95 to 0.33 as token count increases)
- Evaluation is limited to the AMC corpus without testing on independent Arabic datasets

## Confidence

**High Confidence** (Supported by direct evidence and clear methodology):
- The two-tool architecture (overall sentiment and metaphor-aware sentiment) is correctly implemented and evaluated
- Review length inversely correlates with classification accuracy (F-scores 0.95 to 0.33 across token ranges)
- Precision, recall, and F-score metrics are correctly computed against gold standard

**Medium Confidence** (Reasonable but with gaps in evidence):
- The ±2 metaphor boost improves performance relative to no boost (though Tool 2 underperforms Tool 1 on some categories)
- Excluding neutral E-tags does not significantly harm performance
- AraSAS semantic tagging provides meaningful sentiment signals

**Low Confidence** (Significant assumptions or missing evidence):
- The specific ±2 weight for metaphor sentiment is optimal
- Results generalize beyond the AMC corpus to other Arabic domains
- The approach would perform similarly on automatically detected metaphors versus gold-standard annotations

## Next Checks

1. **Weight Optimization Experiment**: Test the metaphor boost with multiple values (±1, ±1.5, ±2, ±2.5) on the same AMC dataset to determine if ±2 is truly optimal or arbitrary. Compare F-scores across all categories to identify the weight that maximizes overall performance.

2. **Cross-Dataset Validation**: Apply both tools to an independent Arabic sentiment dataset (such as hotel reviews or social media data) without metaphor annotations to test whether the semantic tag approach generalizes beyond the AMC corpus. Document any domain-specific performance variations.

3. **Automatic Metaphor Detection Integration**: Replace gold-standard metaphor annotations with an automatic metaphor detection system (such as the mBERT-based approach from the Konkani paper) and evaluate whether the semantic tag method maintains comparable performance. This would test real-world applicability of the approach.