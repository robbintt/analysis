---
ver: rpa2
title: 'MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal
  LLMs'
arxiv_id: '2406.17126'
source_url: https://arxiv.org/abs/2406.17126
tags:
- spurious
- core
- attributes
- object
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MM-SpuBench, a benchmark to study multimodal
  spurious biases in MLLMs. The benchmark includes 10,773 images and 2,400 VQA questions
  with nine types of spurious correlations.
---

# MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2406.17126
- **Source URL:** https://arxiv.org/abs/2406.17126
- **Reference count:** 40
- **Primary result:** MM-SpuBench reveals state-of-the-art MLLMs still rely on spurious correlations, with accuracy ranging from 39.54% to 83.04% across models

## Executive Summary
This work introduces MM-SpuBench, a benchmark designed to study multimodal spurious biases in Multimodal Large Language Models (MLLMs). The benchmark consists of 10,773 images and 2,400 VQA questions annotated with nine types of spurious correlations. Using two evaluation metrics—standard VQA accuracy and Conditional Generation Likelihood Advantage (CGLA)—the study demonstrates that MLLMs exhibit significant reliance on spurious features rather than core attributes. Visualization and prompting experiments confirm that spurious features influence model reasoning, highlighting the need for improved multimodal alignment and robustness in MLLMs.

## Method Summary
The MM-SpuBench benchmark is constructed by first identifying images where CLIP's zero-shot predictions rank between top-k and top-l positions (k=5,l=10 for ObjectNet, k=3,l=40 for ImageNet-Hard), indicating confusion. GPT-4o then extracts core and spurious attributes from these misclassified image-class pairs. The benchmark includes 2,400 VQA questions with human-verified core/spurious attribute annotations. Models are evaluated using standard VQA accuracy and CGLA, which measures the likelihood difference between core and spurious attribute conditions for generating object labels. Evaluation is performed zero-shot with greedy decoding, and answer choices are shuffled per question.

## Key Results
- MLLMs show accuracy ranging from 39.54% to 83.04% across different models on MM-SpuBench
- CGLA reveals significant misalignment between core and spurious attributes in MLLMs
- Visualization and prompting experiments confirm spurious features influence model reasoning
- Relative Size and Colorization categories consistently show 30-40% lower performance than Background and Co-occurring Objects
- Scaling alone is insufficient to mitigate spurious correlation impacts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying robust multimodal failure modes requires targeting the "confusion zone" of pre-trained vision encoders rather than random sampling.
- **Mechanism:** The benchmark construction uses CLIP zero-shot logits to filter images where the ground truth ranks low (e.g., 5th-20th). These specific failures suggest the visual features present are statistically associated with the wrong class (spurious) rather than the ground truth (core). GPT-4o then extracts attributes from these (Ground Truth, Misclassified) pairs to isolate the specific semantic conflict (e.g., "fire hydrant" vs. "white color").
- **Core assumption:** The failure modes of the CLIP vision encoder (specifically ViT-L/14) are representative of the spurious correlations learned by downstream MLLMs, given that CLIP is a standard vision backbone.
- **Evidence anchors:** [abstract]: "...human-verified benchmark... consisting of image-class pairs annotated with core and spurious attributes..."; [section]: "We utilize the logit vectors... to find samples where CLIP's true class prediction is in the range of top-$k$ and top-$l$..."; [corpus]: Hosseini et al. (2025) confirm that MLLMs suffer from spurious correlations, validating the need for such targeted identification.
- **Break condition:** If an MLLM uses a fundamentally different visual backbone (e.g., a purely self-supervised non-contrastive encoder like DINOv2), the CLIP-curated spurious correlations may not trigger the same biases.

### Mechanism 2
- **Claim:** Comparing conditional generation likelihoods isolates internal concept associations from instruction-following capabilities.
- **Mechanism:** The Conditional Generation Likelihood Advantage (CGLA) measures the difference in log-probability of generating the object label when conditioned on a core attribute versus a spurious attribute. A negative value indicates the model structurally prefers the spurious shortcut.
- **Core assumption:** The token likelihood distribution reflects the model's learned causal graph between visual concepts and text labels, independent of the final sampling strategy.
- **Evidence anchors:** [abstract]: "Conditional Generation Likelihood Advantage (CGLA)... reveals misalignment between core and spurious attributes."; [section]: "CGLA measures the likelihood difference between attributes as conditions for generating a particular text sequence..."; [corpus]: Assumption: The related work "COPO" (Causal-Oriented Policy Optimization) implicitly supports this by assuming causal intervention is needed to fix these likelihood associations.
- **Break condition:** If the model's tokenizer splits attributes into uninterpretable sub-words, or if the prompt format induces significant distributional shift, the likelihood comparison loses semantic meaning.

### Mechanism 3
- **Claim:** Visual attention alignment serves as a proxy for verifying the reliance on spurious textual attributes.
- **Mechanism:** Grad-CAM visualizations on the vision encoder layers reveal where the model "looks." If the model predicts based on a spurious attribute (e.g., "grass background"), the attention heatmap should concentrate on the background rather than the object, correlating with the textual reliance identified by CGLA.
- **Core assumption:** Gradient-based saliency maps (Grad-CAM) accurately represent the features the model uses for classification, rather than just high-frequency noise.
- **Evidence anchors:** [abstract]: "Visualization... confirm that spurious features influence model reasoning..."; [section]: "The Grad-CAM heatmaps pay more attention to irrelevant elements... The token-level text attention heatmap further supports this observation..."; [corpus]: Corpus signals (e.g., Causal-LLaVA) suggest visual disentanglement is key to mitigating hallucinations, supporting the mechanism's focus on visual attention.
- **Break condition:** If the MLLM utilizes a non-attention based architecture (e.g., Mamba/SSM) or has multi-head attention where heads specialize strictly (one for core, one for context), a single aggregated Grad-CAM may obscure the true reasoning path.

## Foundational Learning

- **Concept:** **Spurious Correlations (Shortcut Learning)**
  - **Why needed here:** This is the central problem the paper addresses. You must understand that models often optimize for easy predictors (e.g., snow for snowboard) rather than the semantic target.
  - **Quick check question:** If a model classifies all images with snow as "Wolf" and fails on a wolf in a desert, is this a failure of generalization or a spurious correlation?

- **Concept:** **Contrastive Learning (CLIP)**
  - **Why needed here:** The benchmark relies on CLIP's zero-shot failure modes to curate the dataset. You need to understand that CLIP aligns image-text pairs, which allows it to be "fooled" by text-associated backgrounds.
  - **Quick check question:** Why does a contrastive model associate "backgrounds" with labels more strongly than a pure supervised classifier might?

- **Concept:** **Generative Log-Likelihood**
  - **Why needed here:** The paper introduces CGLA. You need to understand that autoregressive models assign probabilities to tokens, and we can use these numbers to measure the strength of an association without asking the model to produce an answer directly.
  - **Quick check question:** If P(Object | Spurious) > P(Object | Core), what does that imply about the model's internal representation?

## Architecture Onboarding

- **Component map:** Vision Encoder (CLIP ViT-L/14) -> Projector (MLP/Q-Former) -> LLM Backbone -> Evaluation Pipeline

- **Critical path:**
  1. **Data Curation:** Run CLIP zero-shot on ImageNet/ObjectNet → Filter top-$k$ to top-$l$ errors → GPT-4o Attribute Extraction
  2. **Benchmarking:** Feed Image + Question (with options) to MLLM → Record Choice
  3. **Deep Analysis:** Force-prefix generation with Core/Spurious attributes → Calculate CGLA scores

- **Design tradeoffs:**
  - **VQA vs. CGLA:** VQA accuracy captures the end-user experience (instruction following + logic), but is noisy. CGLA captures pure associative strength but requires white-box access to token logits.
  - **Automation vs. Human Verification:** The pipeline automates attribute extraction to scale (10k+ images) but relies on human verification to ensure the "core" vs. "spurious" labels are semantically valid.

- **Failure signatures:**
  - **High VQA / Low CGLA:** The model is guessing correctly (perhaps via prompt heuristics) but doesn't actually associate the object with its core attributes.
  - **Low VQA / High CGLA:** The model knows the association but fails to follow instructions or select the correct multiple-choice option.

- **First 3 experiments:**
  1. **Baseline Profiling:** Run LLaVA-1.5 and InternVL3 on the MM-SpuBench VQA set. Establish if your model is closer to random chance (25%) or GPT-4o levels (~80%).
  2. **Attribute Disentanglement:** Pick a subset of 100 failure cases. Run CGLA analysis to see if the model has a "positive preference" for spurious attributes (positive CGLA implies spurious bias is the cause of failure).
  3. **Prompt Engineering:** Apply the "No-Bias" prompt strategy (from Table 4) to see if the failure is a reasoning error that can be verbally corrected, or a fundamental visual encoding failure (in which case prompting won't help).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-grained spurious attribute annotations (core vs. spurious) be leveraged during MLLM training or fine-tuning to proactively prevent spurious bias learning, beyond inference-time prompting?
- **Basis in paper:** [inferred] The paper notes "the difficulty of mitigation on our benchmark" and that "the rest of the tested models do not show significant performance gain from simple prompting, motivating research in advanced mitigation methods."
- **Why unresolved:** The authors only explored inference-time prompting strategies (guiding, explain, no-bias), which showed mixed effectiveness. Training-time interventions remain unexplored.
- **What evidence would resolve it:** Experiments comparing MLLMs trained with spurious-aware objectives against baseline models on MM-SpuBench, showing significant accuracy improvements.

### Open Question 2
- **Question:** How do spurious biases propagate differently when spurious attributes are shared versus unshared between the vision encoder and language model components of MLLMs?
- **Basis in paper:** [explicit] The appendix states: "Future work could explore more complex scenarios where the spurious attribute is not shared and investigate whether such cases further impact the robustness of MLLMs."
- **Why unresolved:** The paper's theoretical framework assumes shared spurious attributes; the unshared case requires different formalization and empirical investigation.
- **What evidence would resolve it:** An extended benchmark with cross-modal mismatch spurious correlations, plus analysis of model behavior differences across conditions.

### Open Question 3
- **Question:** What architectural or training innovations in multimodal alignment can reduce spurious correlation transfer from pre-trained vision encoders to the full MLLM pipeline?
- **Basis in paper:** [inferred] The visualization results show "scaling alone is not sufficient to mitigate the impact of spurious correlations" and results "highlight the need for improved multimodal alignment techniques and more robust architectures."
- **Why unresolved:** Current alignment methods (e.g., projection layers) appear to propagate spurious biases from vision encoders like CLIP, but alternative approaches are not tested.
- **What evidence would resolve it:** Comparative studies of different alignment architectures evaluated on CGLA metrics, showing reduced spurious reliance.

## Limitations
- Dataset coverage may be limited to CLIP-specific failure modes, potentially missing spurious patterns that CLIP handles correctly
- CGLA metric interpretation may conflate genuine spurious correlations with learned linguistic conventions
- Benchmark may overestimate bias severity in unconstrained real-world deployment scenarios

## Confidence
- **High confidence:** The existence of spurious correlations in MLLMs is well-established and the benchmark successfully demonstrates this phenomenon
- **Medium confidence:** CGLA metric provides meaningful insights into internal model associations, though interpretation requires careful consideration
- **Low confidence:** Claim that CLIP-curated failure modes are representative of all MLLM spurious correlations across different architectural families

## Next Checks
1. **Cross-backbone validation:** Test the same MM-SpuBench dataset on MLLMs using non-contrastive vision encoders (e.g., DINOv2, MAE) to determine whether CLIP's specific spurious patterns transfer across architectural families.
2. **Distribution shift robustness:** Evaluate model performance on a holdout set of images where spurious attributes are deliberately removed or altered to test whether improvements on MM-SpuBench translate to genuine causal reasoning rather than pattern matching.
3. **Token-level intervention study:** Systematically mask or replace spurious attribute tokens during generation to measure whether CGLA scores accurately predict intervention effectiveness in reducing spurious bias reliance.