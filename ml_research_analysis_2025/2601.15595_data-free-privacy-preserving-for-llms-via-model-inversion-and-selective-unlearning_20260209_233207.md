---
ver: rpa2
title: Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning
arxiv_id: '2601.15595'
source_url: https://arxiv.org/abs/2601.15595
tags:
- unlearning
- data
- privacy
- uni00000013
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of privacy-preserving in large
  language models (LLMs) by introducing a data-free approach for selective unlearning
  of sensitive personally identifiable information (PII). The proposed method, Data-Free
  Selective Unlearning (DFSU), synthesizes pseudo-PII through model inversion, constructs
  token-level privacy masks, and performs selective unlearning via a contrastive mask
  loss within a LoRA subspace.
---

# Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning

## Quick Facts
- arXiv ID: 2601.15595
- Source URL: https://arxiv.org/abs/2601.15595
- Authors: Xinjie Zhou; Zhihui Yang; Lechao Cheng; Sai Wu; Gang Chen
- Reference count: 28
- Key outcome: Data-free selective unlearning removes target PII while maintaining model utility, achieving near-oracle performance without access to original training data

## Executive Summary
This paper introduces a data-free approach for selective unlearning of personally identifiable information (PII) from large language models (LLMs). The method, called Data-Free Selective Unlearning (DFSU), synthesizes pseudo-PII through model inversion, constructs token-level privacy masks, and performs selective unlearning via a contrastive mask loss within a LoRA subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that DFSU effectively removes target PII while maintaining model utility, achieving near-oracle performance without access to original training data.

## Method Summary
The method involves a three-stage pipeline: (1) Train a sequence-to-sequence inverter model to map target model logits back to input tokens, (2) Synthesize pseudo-PII by querying the target model with entity-swapped candidates, decoding with the inverter, and annotating with privacy masks, and (3) Apply Privacy-Selective Contrastive Unlearning (PSCU) using LoRA adapters on MLP modules with a dual-objective loss that maximizes privacy loss for sensitive tokens while minimizing utility loss for context tokens.

## Key Results
- Achieves near-oracle performance on PII removal while maintaining model utility
- Outperforms baseline methods in both privacy preservation and utility retention
- Demonstrates effectiveness across different model sizes (160M/410M/1.4B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Logit-based Model Inversion
If a target LLM has memorized sensitive PII, its output logit distributions contain recoverable traces of that information, allowing for the synthesis of surrogate training data without accessing the original corpus. A sequence-to-sequence inverter model (e.g., Flan-T5) is trained to map the target model's output logits back to input tokens. By querying the target model with entity-swapped candidates and inverting the resulting logits, the system generates "pseudo-PII" that approximates the statistical footprint of the original memorized data. The core assumption is that the model's internal logit distributions serve as a "holographic storage" of the training distribution, meaning optimized inversion can recover semantic approximations of sensitive data even without the exact inputs.

### Mechanism 2: Token-Level Contrastive Unlearning
If unlearning is restricted to specific tokens identified by a privacy mask, the model can suppress sensitive outputs while maintaining the semantic coherence of surrounding context. The Privacy-Selective Contrastive Unlearning (PSCU) method partitions the token-wise loss, maximizing the loss for tokens marked as "sensitive" (erasing PII) while simultaneously minimizing the loss for "non-sensitive" context tokens (anchoring utility), preventing the model from drifting into incoherence. The core assumption is that PII is localized to specific tokens within a sequence, and the representation of these tokens can be decorrelated from the general language modeling capabilities required for the non-sensitive context.

### Mechanism 3: LoRA Subspace Regularization
If gradient updates are constrained to a low-rank subspace (LoRA), the unlearning process is less likely to cause "catastrophic collapse" of the model's general knowledge compared to full-parameter gradient ascent. The base model weights are frozen, and adaptation is restricted to low-rank matrices, limiting the "directionality" of the forgetting signal and forcing the model to find a solution that fits the unlearning objective within a constrained manifold, which implicitly preserves the pre-trained knowledge structure. The core assumption is that the dimensions required to represent the "forgetting" of specific PII are lower than the full rank of the model, and these dimensions can be modified without overwriting the dense manifold of general language knowledge.

## Foundational Learning

- **Concept: Logit-based Model Inversion**
  - Why needed here: This is the engine of the "data-free" approach. You must understand how to train a decoder (the "inverter") to map frozen output probability vectors back to probable input text to create the synthetic unlearning dataset.
  - Quick check question: Given a vocabulary $V$ and a sequence of logits $L$, how would you construct the input for a Seq2Seq model designed to reconstruct the original prompt?

- **Concept: Contrastive Learning Objectives**
  - Why needed here: The core PSCU mechanism relies on a dual-objective loss (one for privacy, one for utility). You need to understand how to balance pushing away one set of tokens while pulling in others simultaneously.
  - Quick check question: In the loss function $J(\phi) = \alpha L_{gen} - \beta L_{priv}$, what is the expected behavior of the gradients with respect to the "privacy" tokens versus the "context" tokens?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The method utilizes LoRA not just for efficiency, but as a regularization technique to prevent utility collapse. Understanding the rank $r$ and scaling $\alpha$ is critical for tuning the privacy-utility trade-off.
  - Quick check question: If you observe that the model's perplexity on general text is spiking during unlearning, should you increase or decrease the LoRA rank $r$, or adjust the scaling factor $\alpha$?

## Architecture Onboarding

- **Component map**: Target LLM ($M_\theta$) -> Inverter Model ($I_\phi$) -> Annotator -> LoRA Adapter -> Target LLM ($M_\theta$)

- **Critical path**:
  1. Inversion Training: Train $I_\phi$ on a generic corpus to reconstruct text from $M_\theta$'s logits
  2. Surrogate Generation: Feed entity-swapped prompts to $M_\theta$, capture logits, use $I_\phi$ to generate pseudo-PII, and annotate with masks
  3. Selective Unlearning: Fine-tune LoRA adapters on pseudo-PII using the contrastive PSCU loss

- **Design tradeoffs**:
  - MLP vs. Attention LoRA: MLP-only adaptation is more stable for utility, while Attention-based adaptation may offer slight reasoning improvements but risks higher perplexity
  - Pseudo-Data Scale: Privacy saturation occurs quickly (~100 samples), but utility scales linearly
  - Inverter Fidelity: A more powerful inverter creates better pseudo-data but increases the "attack" surface if the inverter itself leaks data

- **Failure signatures**:
  - Catastrophic Collapse: Perplexity explodes ($>10^4$), indicating $\beta$ is too high or LoRA rank is too high
  - Under-Unlearning: Entity-Level Hit Rate (E-Hit) remains high ($>1\%$), indicating $\beta$ is too low or pseudo-data failed to trigger the memorized PII
  - Context Drift: The model generates grammatically correct but irrelevant text, suggesting the "utility stream" ($L_{gen}$) is under-weighted

- **First 3 experiments**:
  1. Inverter Validation: Verify the Inverter ($I_\phi$) achieves decent BLEU/F1 scores on a held-out set of text-to-logit-to-text reconstructions
  2. Privacy-Utility Sweep: Run PSCU on a small subset sweeping $\beta \in [0.2, 1.0, 5.0]$ to identify the collapse threshold
  3. Module Ablation: Compare training LoRA on MLP-only vs. Full (MLP + Attention) to determine optimal trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How can DFSU be adapted to operate in black-box settings where only API access is available? The current method requires explicit log-probability distributions ($P_t$) to train the inversion model; APIs typically hide these internal states.

### Open Question 2
To what extent does the inversion model transfer across distinct LLM architectures? It is unclear if the logit-to-text mapping learned by the inverter generalizes to models with different vocabularies or architectural inductive biases.

### Open Question 3
How robust is DFSU when the defender lacks prior knowledge of the sensitive data's syntactic templates? The pseudo-data synthesis depends on entity-swapping within specific templates; if templates are unavailable, the surrogate data may fail to trigger the target memorized PII.

## Limitations

- Data-Free Premise Ambiguity: The inversion model training requires a generic corpus, creating a potential gap between claimed methodology and practical implementation
- Entity Substitution Strategy Gap: The source of "public, disjoint pool" for substitutes is not disclosed, making it unclear whether the approach can handle edge cases
- Privacy Annotation Reliability: The PII annotation process uses LLM prompting without specifying the model, prompt template, or evaluation of annotation accuracy

## Confidence

- **High Confidence**: The core mechanism of using model inversion to synthesize pseudo-training data is technically sound and well-supported by the logit-based inversion literature
- **Medium Confidence**: The claim that LoRA regularization prevents catastrophic collapse is supported by ablation studies, but the optimal rank (r=4) appears somewhat arbitrary
- **Low Confidence**: The data-free nature of the approach is questionable given the inversion training requirements, and the privacy annotation process lacks transparency

## Next Checks

1. **Inversion Model Quality Threshold Validation**: Establish a minimum reconstruction quality threshold (e.g., BLEU > 25, F1 > 40) on held-out data and verify that pseudo-PII quality correlates with downstream unlearning effectiveness

2. **Entity Substitution Robustness Test**: Implement multiple entity substitution strategies (random public pool, semantic similarity-based, and named entity category matching) and measure how substitution quality affects pseudo-PII generation and subsequent unlearning performance

3. **Annotation Error Sensitivity Analysis**: Systematically introduce controlled annotation errors (random mask flips, systematic over/under-masking) into the privacy masks and measure the impact on E-Hit reduction and utility preservation