---
ver: rpa2
title: Effect of Model Merging in Domain-Specific Ad-hoc Retrieval
arxiv_id: '2509.21966'
source_url: https://arxiv.org/abs/2509.21966
tags:
- retrieval
- merging
- data
- source
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of model merging for domain-specific
  ad-hoc retrieval tasks. The authors propose merging a retrieval model with a domain-specific
  model using linear interpolation of their weights, requiring no additional fine-tuning.
---

# Effect of Model Merging in Domain-Specific Ad-hoc Retrieval

## Quick Facts
- arXiv ID: 2509.21966
- Source URL: https://arxiv.org/abs/2509.21966
- Reference count: 29
- Key outcome: Model merging outperforms source retrieval models on 3/4 datasets and significantly outperforms LoRA fine-tuning under limited data (50 queries) with nDCG@10 improvements of 4.27, 4.03, and 4.81 points on NFCorpus, MIRACL, and JQaRA respectively.

## Executive Summary
This study evaluates model merging as a parameter-efficient approach for domain-specific ad-hoc retrieval. The authors propose linearly interpolating weights between a retrieval-capable model and a domain-specialized model, requiring no fine-tuning. Experiments in medical and Japanese domains demonstrate that merged models outperform both source retrieval models and LoRA fine-tuned models, particularly under limited development data. The approach shows particular robustness when training data is scarce, with significantly lower variance than LoRA fine-tuning.

## Method Summary
The method linearly interpolates weights between a source retrieval model (e5-mistral-7b-instruct) and a domain-specific model (BioMistral-7B or japanese-stablelm-base-gamma-7b) using tunable interpolation coefficients α. The model is split into two segments (lower: layers 1-16, upper: layers 17-32) with separate α values for each segment. Hyperparameters are optimized via grid search over 23 valid configurations using development data. The tokenizer and embedding layer are adopted from the source retrieval model. For comparison, LoRA fine-tuning is performed with 80:20 dev train/validation split and early stopping.

## Key Results
- Merged models outperformed source retrieval models on three out of four datasets (NFCorpus, MIRACL, JQaRA)
- Under limited data (50 queries), merged models significantly outperformed LoRA fine-tuning with 4.27, 4.03, and 4.81 nDCG@10 improvements
- Merged models showed substantially lower variance than LoRA under limited data (std dev ≤0.76 vs. up to 4.56)
- Optimal α values were predominantly in the 0.75-1.0 range, favoring the retrieval model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear interpolation of weights between a retrieval-capable model and a domain-specialized model can yield domain-adapted retrieval without fine-tuning.
- Mechanism: Weights from two source models (θ₁ and θ₂) are combined using tunable interpolation coefficients (α). The merged weights θ_new = αθ₁ + (1-α)θ₂ inherit retrieval capability from the source retrieval model while absorbing domain-specific representations from the domain-pretrained model. No backpropagation occurs; only α is optimized via grid search on development data.
- Core assumption: Both source models share a compatible architecture (here, Mistral-7B variants) and that domain knowledge is encoded in weights transferable via linear combination.
- Evidence anchors:
  - [abstract]: "We merged the weights of a source retrieval model and a domain-specific (non-retrieval) model using a linear interpolation approach."
  - [section 3, Equations 1-2]: Explicit formulas for θ_new using α_lower and α_upper.
  - [corpus]: Limited direct corpus support; neighbor papers focus on LoRA merging and multimodal merging rather than retrieval-specific weight interpolation.

### Mechanism 2
- Claim: Segmenting layers and assigning different interpolation weights improves retrieval effectiveness over uniform merging.
- Mechanism: The model is split into lower layers (1-16) and upper layers (17-32), each with independent α values. This accounts for evidence that different LLM layers serve distinct functions in language understanding. The paper found optimal α values predominantly in 0.75-1.0, favoring the retrieval model more heavily.
- Core assumption: Lower and upper layers encode differentially useful information for retrieval vs. domain knowledge, and segment-specific weighting captures this better than a single global α.
- Evidence anchors:
  - [section 3]: "We divided the model into two segments: the 1st to 16th layers and the 17th to 32nd layers."
  - [section 5, Table 2]: Selected hyperparameters show α values in 0.5-1.0 range, with α_upper often at 1.0 (fully retaining retrieval model weights).
  - [corpus]: No direct corpus evidence on layer-segmented merging for retrieval; assumption draws on general LLM layer-role literature cited (Meng et al., 2022).

### Mechanism 3
- Claim: Model merging is more robust than LoRA fine-tuning under limited development data due to drastically fewer tunable parameters.
- Mechanism: Merging requires optimizing only 2 hyperparameters (α_lower, α_upper) via grid search, whereas LoRA optimizes low-rank adapter weights with many parameters. With limited data (50 queries), LoRA exhibits high variance (std dev up to 4.56 nDCG@10), while merging remains stable (std dev ≤0.76).
- Core assumption: The stability stems from reduced overfitting risk when tuning 2 parameters vs. thousands of LoRA adapter parameters.
- Evidence anchors:
  - [section 4.1]: "Our proposed method requires learning only two parameters, which is significantly fewer than those in LoRA."
  - [section 5, Table 3]: Limited data results show merged models with 4.27, 4.03, and 4.81 nDCG@10 improvements over LoRA, with smaller standard deviations.
  - [corpus]: Neighbor papers (e.g., "Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging") address LoRA merging challenges, supporting that LoRA introduces complexity requiring careful management.

## Foundational Learning

- Concept: Bi-encoder architecture for dense retrieval
  - Why needed here: The paper uses e5-mistral-7b-instruct, a bi-encoder that independently encodes queries and documents into embeddings. Understanding this architecture clarifies why weight interpolation affects embedding quality.
  - Quick check question: Can you explain why bi-encoders trade some accuracy for efficiency compared to cross-encoders?

- Concept: Weight-space vs. layer-space model merging
  - Why needed here: The paper employs weight-level merging (linear interpolation). Distinguishing this from layer-level merging (selecting whole layers) is necessary to understand the method's constraints and generality.
  - Quick check question: If two models have different layer counts, which merging paradigm could still apply?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: The baseline comparison is against LoRA fine-tuned models. Understanding LoRA's parameter efficiency and overfitting behavior contextualizes why merging competes favorably in low-data regimes.
  - Quick check question: What is the rank hyperparameter in LoRA, and how does it control the capacity of adaptations?

## Architecture Onboarding

- Component map:
  - Source Retrieval Model: e5-mistral-7b-instruct
  - Source Domain-Specific Model: BioMistral-7B (medical) or japanese-stablelm-base-gamma-7b (Japanese)
  - Merging Engine: MergeKit performing linear interpolation
  - Hyperparameter Search: Grid over α_lower, α_upper ∈ {0.00, 0.25, 0.50, 0.75, 1.00}
  - Tokenizer/Embedding Layer: Adopted unchanged from source retrieval model

- Critical path:
  1. Verify both source models share the same base architecture (layer count, hidden size)
  2. Load weights from both models using MergeKit
  3. Define layer segments (lower: 1-16, upper: 17-32)
  4. Run grid search over 23 valid (α_lower, α_upper) combinations on development data
  5. Select configuration maximizing nDCG@10 on development set
  6. Export merged model and evaluate on test set

- Design tradeoffs:
  - Simplicity vs. optimality: Linear interpolation is simple but may underperform advanced methods (SLERP, TIES, DARE) noted in future work
  - Segment granularity: Two-segment split is heuristic; finer segmentation or per-layer α could improve but increases search cost
  - Tokenizer choice: Using the retrieval model's tokenizer ensures consistency but may mismatch the domain model's vocabulary (mitigated by shared Mistral base)

- Failure signatures:
  - nDCG@10 degrades below source retrieval model: α values too low, over-weighting domain model that lacks retrieval training
  - High variance across development splits: Insufficient development data or search space too coarse
  - SciFact-style stagnation (no improvement): Domain model may not encode relevant knowledge for the corpus, or retrieval model already near-optimal

- First 3 experiments:
  1. Replicate the medical domain merging (e5-mistral-7b-instruct + BioMistral-7B) on NFCorpus with the documented 23-configuration grid search to validate baseline reproduction
  2. Ablate layer segmentation by testing a single global α across all layers; compare nDCG@10 and standard deviation against the segmented approach
  3. Stress-test the limited-data regime by sampling 25 queries (vs. 50 in paper) and comparing merging vs. LoRA stability; document variance inflation in both approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced model merging techniques (e.g., TIES, SLERP, evolutionary merging) yield greater improvements in retrieval effectiveness compared to simple linear interpolation?
- Basis in paper: [explicit] The authors explicitly propose exploring "more advanced model merging techniques beyond simple linear interpolation" as a primary future direction in Section 6.
- Why unresolved: This study only evaluated linear interpolation, leaving the potential benefits of sophisticated merging algorithms for retrieval tasks untested.
- What evidence would resolve it: Comparative experiments on the same domains (medical, Japanese) utilizing algorithms like TIES or evolutionary merging to measure performance deltas against the linear baseline.

### Open Question 2
- Question: What specific characteristics of source models and target corpora determine the success of model merging for domain-specific retrieval?
- Basis in paper: [explicit] Section 6 highlights the need to understand the "interplay between source models and target corpora" to explain inconsistent results, such as the limited improvement observed on the SciFact dataset.
- Why unresolved: The study observed variance in success across datasets but did not analyze the underlying features (e.g., vocabulary overlap, domain density) that cause these differences.
- What evidence would resolve it: An analysis correlating corpus statistics (e.g., term specificity) and source model features with the resulting retrieval performance gains.

### Open Question 3
- Question: Does a more granular layer-wise merging strategy improve performance compared to the fixed two-segment split used in this study?
- Basis in paper: [inferred] The methodology fixes the merging split between the 16th and 17th layers; however, the optimal distribution of retrieval vs. domain knowledge across layers was not optimized, only the interpolation weights (α) were.
- Why unresolved: It is unclear if the fixed segmentation constrained the model's ability to balance general retrieval capabilities with domain-specific knowledge integration.
- What evidence would resolve it: Experiments treating the layer split point as a tunable hyperparameter or applying distinct weights to every individual layer.

## Limitations

- The method assumes identical base architectures for merging, limiting generalizability to cross-architecture scenarios
- Layer segmentation boundary (1-16 vs. 17-32) is heuristic without theoretical grounding for retrieval tasks
- Hard negative mining relies on BM25, which may not capture semantic relevance needed for dense retrieval evaluation
- Limited exploration of alternative merging techniques (SLERP, TIES, DARE) that might outperform linear interpolation

## Confidence

- High confidence in mechanism 1 (linear interpolation works)
- Medium confidence in mechanism 2 (layer segmentation helps)
- High confidence in mechanism 3 (robustness to limited data)

## Next Checks

1. Conduct ablation study comparing single global α vs. layer-segmented α merging to quantify the actual contribution of segmentation to performance gains
2. Test merging robustness with more extreme limited data scenarios (25 queries vs. 50) to establish minimum viable data requirements and variance behavior
3. Experiment with alternative interpolation methods (SLERP, TIES) on the same datasets to benchmark whether linear interpolation is optimal or merely sufficient