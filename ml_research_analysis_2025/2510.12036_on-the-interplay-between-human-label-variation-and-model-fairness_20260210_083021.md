---
ver: rpa2
title: On the Interplay between Human Label Variation and Model Fairness
arxiv_id: '2510.12036'
source_url: https://arxiv.org/abs/2510.12036
tags:
- fairness
- training
- each
- section
- sbic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interplay between human label variation
  (HLV) and model fairness, a previously unexplored area. HLV has been shown to improve
  model generalization by preserving minority views, but its impact on fairness is
  unknown.
---

# On the Interplay between Human Label Variation and Model Fairness

## Quick Facts
- arXiv ID: 2510.12036
- Source URL: https://arxiv.org/abs/2510.12036
- Reference count: 28
- Primary result: HLV training methods improve performance without harming fairness, and can even improve fairness for certain configurations

## Executive Summary
This paper investigates the previously unexplored relationship between human label variation (HLV) and model fairness. While HLV has been shown to improve model generalization by preserving minority views, its impact on fairness remained unknown. The study compares training on majority-vote labels with four HLV methods (repeated labelling, soft labelling, minimizing Jensen-Shannon divergence, and maximizing soft micro F1 score) across offensive classification and legal area classification tasks. Results show that HLV training methods consistently improve overall performance without harming fairness across most configurations, and can even improve fairness when all groups/classes are weighted equally. The analysis reveals that fairness improvements stem from minority annotations, as demonstrated by temperature scaling experiments that weight minority annotations more heavily.

## Method Summary
The study compares five training approaches across two tasks: offensive classification using the SBIC dataset and legal area classification using the TAG dataset. The baseline uses majority-vote labels, while four HLV methods are tested: repeated labelling (duplicating minority annotations), soft labelling (using annotator distribution), minimizing Jensen-Shannon divergence between soft labels and model predictions, and maximizing soft micro F1 score. Fairness is evaluated using three configurations based on p-values: Rawlsian (p = -10, prioritizing lowest-scoring groups), mixed (p = 0, equal weighting), and utilitarian (p = 10, prioritizing high-scoring groups). Performance is measured using F1 scores across demographic groups for SBIC and legal areas for TAG.

## Key Results
- HLV training methods consistently improve overall performance without harming fairness across most configurations
- Training with HLV can improve fairness for a substantial number of configurations, particularly when all groups/classes are weighted equally
- Temperature scaling experiments confirm that weighting minority annotations more heavily leads to higher fairness scores

## Why This Works (Mechanism)
The paper does not explicitly explain the mechanism behind HLV's positive impact on fairness. However, the results suggest that preserving minority views in the training data helps models better represent underrepresented perspectives, leading to more equitable performance across different demographic groups.

## Foundational Learning
- **Human Label Variation (HLV)**: The variation in annotations provided by different human annotators for the same data point. Understanding HLV is crucial because it represents the inherent subjectivity and diversity in human judgment that can affect model learning and fairness.
- **Demographic Parity**: A fairness metric requiring that different demographic groups receive similar outcomes. This is needed to evaluate whether models treat different groups equitably.
- **Equal Opportunity**: A fairness metric requiring that models have similar true positive rates across demographic groups. This ensures models don't systematically fail to identify positive cases for certain groups.
- **Soft Labeling**: A method where the model learns from the distribution of annotator labels rather than a single majority-vote label. This preserves the uncertainty and diversity in human judgments.
- **Temperature Scaling**: A technique for adjusting the confidence of model predictions, used here to weight minority annotations more heavily. This helps investigate the relationship between annotation weighting and fairness outcomes.

## Architecture Onboarding

Component Map: Data -> Annotation Collection -> HLV Methods -> Model Training -> Fairness Evaluation

Critical Path: The critical path involves collecting multiple annotations per data point, applying HLV methods to generate training labels, training the model, and evaluating fairness across demographic groups using various p-value configurations.

Design Tradeoffs: The study balances model performance with fairness considerations, choosing HLV methods that preserve minority views while maintaining overall accuracy. The choice of fairness metrics (demographic parity and equal opportunity) focuses on group-level fairness rather than individual fairness.

Failure Signatures: When HLV methods fail to improve fairness, it typically occurs in Rawlsian configurations where the lowest-scoring groups are prioritized. In these cases, standard HLV methods show comparable performance to majority voting, suggesting they don't actively address fairness deficits for the worst-off groups.

First Experiments:
1. Replicate the SBIC experiment with Rawlsian configuration (p = -10) to verify the claim that HLV is comparable to MV when prioritizing lowest-scoring groups
2. Apply temperature scaling with minority annotation weighting to a different dataset to confirm the correlation between weighted minority annotations and improved fairness
3. Test the four HLV methods on a third classification task outside the offensive and legal domains to assess generalizability

## Open Questions the Paper Calls Out
### Open Question 1
Does the positive interplay between Human Label Variation (HLV) and fairness generalize to non-English languages and culturally distinct contexts?
The authors explicitly state in the Limitations section that they "encourage future work to develop such datasets for non-English languages to test this empirically." The study is restricted to English datasets (SBIC and TAG) due to the scarcity of non-English corpora containing both disaggregated annotations and the demographic attributes necessary for fairness evaluation.

### Open Question 2
What is the specific mechanism by which minority annotations contribute to fairness improvements in specialized domains?
In Section 5.2, the authors note they "leave the full investigation on this direction for future work," expanding in Appendix K that they only "hypothesize" a correlation between rare specializations and minority annotations. While temperature scaling experiments confirm a correlation between weighting minority annotations and higher fairness scores, the paper does not establish a causal link or the underlying semantic reason for this improvement.

### Open Question 3
Can HLV training methods be modified to improve fairness under "Rawlsian" configurations (low p) where the lowest-scoring groups are prioritized?
Section 5.1 concludes that when prioritizing the lowest-scoring groups (low p), "training with HLV is comparable to MV," suggesting standard HLV methods fail to improve fairness in this specific, strict configuration. The paper demonstrates that standard HLV methods do not actively solve fairness deficits for the worst-off groups in the way they do for average-case fairness.

## Limitations
- The study focuses primarily on demographic parity and equal opportunity fairness metrics, excluding other notions such as individual fairness or counterfactual fairness
- Results are based on two specific datasets (SBIC and TAG) which may not generalize to all classification tasks or domains
- Temperature scaling experiments use an ad-hoc approach to weight minority annotations rather than deriving theoretically grounded weighting schemes

## Confidence
- **High**: Core finding that HLV methods generally maintain or improve fairness for tested configurations
- **Medium**: Generalizability of findings to other fairness definitions and real-world deployment scenarios
- **Medium**: Claim that HLV can improve fairness specifically through minority annotation weighting

## Next Checks
1. Replicate the experiments across additional fairness metrics including individual fairness, counterfactual fairness, and equalized odds to verify robustness across different fairness definitions
2. Test the HLV methods on additional datasets spanning different domains and task types to assess generalizability beyond offensive classification and legal area classification
3. Conduct ablation studies varying the degree of label disagreement and annotator expertise levels to understand how different sources of HLV affect fairness outcomes