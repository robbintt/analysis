---
ver: rpa2
title: Some Attention is All You Need for Retrieval
arxiv_id: '2510.19861'
source_url: https://arxiv.org/abs/2510.19861
tags:
- retrieval
- attention
- during
- generation
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the functional roles of self-attention and
  state-space model (SSM) layers in hybrid architectures by systematically ablating
  attention mechanisms while measuring retrieval performance. Using entropy-based
  sparsification to progressively disable attention heads, the study finds that retrieval
  fails completely (0% accuracy) when all attention is removed, while as few as 15%
  of attention heads maintain near-perfect retrieval with 84% MMLU performance.
---

# Some Attention is All You Need for Retrieval

## Quick Facts
- arXiv ID: 2510.19861
- Source URL: https://arxiv.org/abs/2510.19861
- Reference count: 21
- Primary result: Hybrid architectures show complete functional segregation—attention exclusively implements retrieval while SSMs handle other language modeling tasks.

## Executive Summary
This work investigates the functional roles of self-attention and state-space model (SSM) layers in hybrid architectures by systematically ablating attention mechanisms while measuring retrieval performance. Using entropy-based sparsification to progressively disable attention heads, the study finds that retrieval fails completely (0% accuracy) when all attention is removed, while as few as 15% of attention heads maintain near-perfect retrieval with 84% MMLU performance. No compensatory retrieval mechanisms emerge in SSM layers, even with prompting strategies designed to enhance SSM performance. Mechanistic analysis reveals that successful retrieval requires needle token exposure during generation and sufficient contextual information during either prefill or generation phases.

## Method Summary
The study uses entropy-based top-k sparsification to progressively disable attention heads across three hybrid SSM-Transformer models (RecurrentGemma-2B/9B, Jamba-Mini-1.6). Attention heads are ranked by entropy, with k lowest-entropy heads retained. Retrieval performance is measured using the NIAH benchmark with Paul Graham essays as haystack and a specific needle about San Francisco. The experiments test both generation-only and prefill+generation sparsification, and include attention manipulation techniques (Only, Omit, Binary, Null) to isolate mechanism requirements. Models are evaluated at multiple k thresholds to identify tipping points where retrieval fails.

## Key Results
- Complete retrieval failure at k=0 across all models (0% accuracy)
- Near-perfect retrieval maintained with as few as 15% of attention heads
- No compensatory retrieval mechanisms in SSM layers, even with optimized prompting
- Sliding window attention restricts retrieval to active window during generation
- Kv-cache preserves retrieval information for tokens outside sliding window

## Why This Works (Mechanism)

### Mechanism 1: Attention-Exclusive Retrieval
- Claim: Retrieval in hybrid SSM-Transformer architectures depends exclusively on self-attention layers with zero SSM contribution.
- Mechanism: Self-attention implements direct token-to-token comparisons via attention weights; SSM layers rely on compressed recurrent states that cannot perform exact positional lookups.
- Core assumption: The "fuzzy memory" property of SSMs (capturing patterns but not precise positions) is structural, not recoverable via prompting.
- Evidence anchors:
  - [abstract] "attention ablation causes catastrophic retrieval failure (0% accuracy), while SSM layers show no compensatory mechanisms even with improved prompting"
  - [Section 3.2] "all models failed to retrieve the needle at k=0, and applying JRT did not recover this retrieval capability"
  - [corpus] Related work (arXiv:2510.23006) confirms ICL mechanisms differ across architectures, but does not test retrieval specifically—evidence is indirect.
- Break condition: If a pure SSM model (no attention) achieves non-zero NIAH accuracy, functional exclusivity fails.

### Mechanism 2: Precision-Critical Attention Weights
- Claim: Successful retrieval requires not just needle token exposure but precise attention weight distributions.
- Mechanism: The Binary manipulation (uniform weights across needle tokens) catastrophically degraded outputs, suggesting attention weights encode relational structure beyond binary relevance.
- Core assumption: Attention peaks track the target token dynamically during generation.
- Evidence anchors:
  - [Section 5.2] "the values of the attention weights are important... It is not enough to expose necessary tokens more than unnecessary ones"
  - [Section 5.2] Binary during prefill "produced nonsensical tokens"; Binary during generation yielded "off-topic but grammatical" outputs
  - [corpus] No direct corpus evidence on weight precision; this is a novel finding in this paper.
- Break condition: If a binarized attention pattern (all relevant tokens get equal weight) maintains retrieval accuracy ≥80%, precision is not required.

### Mechanism 3: Phase-Dependent Context Requirements
- Claim: Retrieval requires needle token exposure during generation, plus sufficient context exposure during prefill OR generation.
- Mechanism: Sliding-window attention (2048 tokens in RecurrentGemma) restricts active access; kv-cache preserves retrieval information for tokens outside the window if properly exposed during prefill.
- Core assumption: The kv-cache encodes retrieval-relevant structure even when tokens exit the sliding window.
- Evidence anchors:
  - [Section 5.2] "Omit-Keep... partial success... successful retrievals occur only when needles fall within this window during generation"
  - [Section 5.2] "Retrieval beyond the window requires prefill exposure, suggesting the kv-cache preserves implicit retrieval information"
  - [corpus] arXiv:2509.24552 confirms sliding-window + linear RNN interactions matter but does not isolate retrieval—weak indirect support.
- Break condition: If Keep-Null (no prefill context, full generation context) achieves baseline accuracy for needles beyond the sliding window, the kv-cache hypothesis fails.

## Foundational Learning

- Concept: **Sliding Window Attention**
  - Why needed here: RecurrentGemma uses a 2048-token window; understanding retrieval failures beyond this window requires knowing that attention only "sees" local context during generation.
  - Quick check question: For a 4096-token prompt with the needle at position 500, can a 2048-token sliding window attending from the end access it during generation?

- Concept: **KV-Cache**
  - Why needed here: The paper argues prefill exposure populates the cache with retrieval-relevant structure; ablations during prefill corrupt all downstream queries.
  - Quick check question: If you sparsify attention during prefill only, does the cached key-value structure reflect original or sparsified attention patterns?

- Concept: **Entropy-Based Sparsification**
  - Why needed here: The paper selects low-entropy heads (focused attention) as retrieval-critical; high-entropy heads (diffuse attention) are ablated.
  - Quick check question: A head with uniform attention weights over 1000 tokens has entropy ≈10 bits. Is this head more or less likely to be retained than a head attending to 5 tokens?

## Architecture Onboarding

- Component map: RecurrentGemma-2B/9B -> 26/38 layers, pattern "2× SSM, 1× Attention", sliding window 2048, 10/16 heads per attention layer; Jamba-Mini-1.6 -> 32 layers, pattern "3× SSM, 1× Attention, 4× SSM", global attention, 32 heads, MoE MLPs; Attention layers -> Exclusive retrieval implementation; SSM layers -> General language modeling, no retrieval contribution; KV-cache -> Stores key/value projections from prefill; sensitive to prefill manipulation

- Critical path:
  1. Prefill phase populates kv-cache with attention-computed key/value projections
  2. Generation phase attends over cached keys + new tokens
  3. Needle must be in active attention window OR cached with valid structure
  4. Low-entropy attention heads execute retrieval; ablate these → failure

- Design tradeoffs:
  - **Sparsification during prefill vs. generation only**: Prefill sparsification corrupts kv-cache in RecurrentGemma, causing worse degradation; Jamba more robust
  - **Global vs. sliding window attention**: Global attention (Jamba) is more robust to prefill sparsification; sliding window (RG) depends heavily on cache integrity
  - **Head count vs. redundancy**: More heads (Jamba: 32) → higher k threshold before failure; but 15% of heads suffices across all models

- Failure signatures:
  - **Retrieval failure**: 0% accuracy at k=0; JRT prompting cannot recover
  - **Binary manipulation**: Nonsensical outputs (prefill) or off-topic coherent text (generation)
  - **Prefill sparsification (RG models)**: Sharp accuracy drop even at low sparsity due to corrupted kv-cache
  - **Sliding window edge**: Partial retrieval only for needles within window during generation

- First 3 experiments:
  1. **Baseline NIAH sweep**: Run NIAH at k ∈ {0, 1, 2, 4, 8, 16, N} on your target model to identify the tipping point (model-specific, correlates with head count).
  2. **Phase isolation test**: Compare sparsification during prefill-only vs. generation-only at k near tipping point to quantify kv-cache sensitivity.
  3. **Manipulation probe**: Apply Only/Omit/Binary/Null combinations to isolate whether your model requires precise weights or mere token exposure (replicate Table 1 on your architecture).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does functional segregation emerge from architectural constraints or training dynamics?
- Basis in paper: [explicit] Conclusion states: "Future work may explore whether this specialization emerges from architectural constraints or training dynamics."
- Why unresolved: Current study only examines trained models; cannot isolate whether specialization is inherent to architecture or learned during training.
- What evidence would resolve it: Training identical hybrid architectures with different attention/SSM ratios or training pure SSM models then adding attention layers mid-training.

### Open Question 2
- Question: How do retrieval mechanisms differ between pure SSM models and hybrid SSM-Transformer variants?
- Basis in paper: [explicit] Section 3.2: "Future work could compare state transition matrices between pure and hybrid SSM variants to uncover how retrieval mechanisms differ when attention is available versus absent."
- Why unresolved: Study only tested hybrid models with attention ablated post-hoc, not pure SSMs trained without attention.
- What evidence would resolve it: Comparative analysis of SSM state matrices from pure Mamba models versus hybrid models' SSM components.

### Open Question 3
- Question: Can lightweight retrieval-specific mechanisms replace full attention layers while preserving retrieval performance?
- Basis in paper: [explicit] Conclusion: "whether lightweight retrieval-specific mechanisms could replace full attention layers."
- Why unresolved: Current findings show 15% of heads suffice, but whether smaller specialized modules could substitute remains untested.
- What evidence would resolve it: Designing and training hybrid models with reduced attention capacity targeted specifically at retrieval functions.

## Limitations

- Model Generalization Gap: Findings based exclusively on three hybrid SSM-Transformer architectures; functional exclusivity claim may not generalize to other architectures or pure SSMs.
- Quantitative Robustness Concerns: NIAH benchmark uses only one needle-question pair across all experiments; the 15% tipping point might be specific to this particular retrieval task.
- Statistical Power Limitations: With 100 prompts total, some results may lack statistical power to capture high-variance behaviors like partial success in binary manipulation experiments.

## Confidence

**High Confidence**: The functional exclusivity claim (attention-only retrieval with zero SSM contribution) is strongly supported by the ablation experiments showing complete retrieval failure at k=0 across all models, with no compensatory mechanisms even under prompting optimization.

**Medium Confidence**: The phase-dependent context requirements (needle exposure during generation plus context during prefill/generation) are well-supported by the Keep-Null and Omit-Keep experiments, though the kv-cache mechanism remains somewhat indirect.

**Low Confidence**: The precision-critical attention weights claim (beyond binary relevance) is based on limited binary manipulation experiments. The claim that weight values matter, not just token exposure, needs broader validation across different retrieval tasks.

## Next Checks

1. **Cross-Architecture Validation**: Test the 15% tipping point and retrieval failure pattern on pure transformer models and other hybrid architectures (e.g., Mamba-2, RWKV) to verify the generality of functional exclusivity.

2. **Mechanism Isolation Experiment**: Create a pure SSM model (no attention layers) and attempt NIAH retrieval to definitively test whether SSMs can implement retrieval independently, which would falsify the exclusivity claim.

3. **Multi-Needle Generalization**: Repeat the entropy-based sparsification and attention manipulation experiments across 10-20 diverse needle-question pairs spanning different semantic domains to validate that the 15% threshold and mechanism findings are not specific to the Paul Graham essay retrieval task.