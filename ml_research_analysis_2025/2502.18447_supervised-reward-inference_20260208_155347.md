---
ver: rpa2
title: Supervised Reward Inference
arxiv_id: '2502.18447'
source_url: https://arxiv.org/abs/2502.18447
tags:
- reward
- learning
- tasks
- behavior
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Supervised Reward Inference (SRI), a method
  that learns to map behavior trajectories directly to reward functions using supervised
  learning, bypassing traditional inverse reinforcement learning assumptions about
  demonstrator behavior. The authors prove that SRI is asymptotically Bayes-optimal
  under mild assumptions, converging to optimal policies as training data increases.
---

# Supervised Reward Inference

## Quick Facts
- arXiv ID: 2502.18447
- Source URL: https://arxiv.org/abs/2502.18447
- Authors: Will Schwarzer; Jordan Schneider; Philip S. Thomas; Scott Niekum
- Reference count: 40
- One-line primary result: Learns reward functions from arbitrary behavioral patterns via supervised learning, achieving Bayes-optimal performance with relatively few labeled samples

## Executive Summary
This paper introduces Supervised Reward Inference (SRI), a method that learns to map behavior trajectories directly to reward functions using supervised learning, bypassing traditional inverse reinforcement learning assumptions about demonstrator behavior. The authors prove that SRI is asymptotically Bayes-optimal under mild assumptions, converging to optimal policies as training data increases. Experiments on simulated robotic manipulation tasks demonstrate that SRI can infer accurate reward functions from a wide variety of suboptimal demonstrations, including gestures, noisy actions, and systematically incorrect demonstrations, where traditional imitation learning methods fail.

## Method Summary
SRI learns a reward function by treating it as a supervised learning problem, mapping trajectories to rewards directly rather than assuming demonstrator optimality. The method uses a two-component architecture: a trajectory encoder that processes sets of demonstrations into a task embedding, and a reward head that predicts rewards for states conditioned on this embedding. The model is trained to minimize mean-squared error on ground-truth rewards, which the authors prove converges to the Bayes-optimal posterior mean. During deployment, SRI provides rewards for reinforcement learning agents without requiring assumptions about demonstrator rationality.

## Key Results
- SRI achieves comparable performance to ground-truth reinforcement learning with fewer labeled state-reward samples than traditional methods
- The method successfully handles diverse behavioral patterns including gestures (pointing without touching), noisy actions, and systematically incorrect demonstrations
- SRI demonstrates robust performance across different task complexities, from simple reaching to multi-step pick-and-place operations

## Why This Works (Mechanism)

### Mechanism 1: Direct Inversion of the Behavior-Reward Map
Traditional IRL infers rewards by assuming optimal demonstrator behavior with noise. SRI bypasses this by directly regressing from trajectories to rewards as a supervised learning problem, avoiding fragile optimality assumptions. This works because the training dataset contains representative pairs of behaviors and their ground-truth rewards, allowing the model to learn the true mapping regardless of demonstration quality.

### Mechanism 2: Amortized Task Inference via Factorized Architecture
SRI uses a set transformer to encode multiple trajectories into a fixed-size task embedding, then conditions reward prediction on this embedding. This factorization allows expensive trajectory processing to happen once per task while enabling cheap reward queries for any state during RL. The architecture efficiently handles variable numbers of demonstrations without reprocessing them at every timestep.

### Mechanism 3: Convergence to Bayesian Posterior Expectation
Minimizing MSE on reward prediction forces the model to output the expected value of the reward posterior. The paper proves this is theoretically optimal for imitation learning under mild conditions. This works because the mean-squared error objective naturally estimates the posterior mean, which maximizes expected imitation return when the function class is sufficiently expressive.

## Foundational Learning

### Inverse Reinforcement Learning & Assumption Misspecification
Why needed: SRI positions itself as a solution to IRL's fragility from assuming demonstrator optimality. Understanding IRL's assumption of "Boltzmann rationality" reveals why SRI's direct mapping approach is novel.
Quick check: Why would a robot fail to learn the correct goal if a human points at the goal but the robot assumes the human is trying to reach it?

### Set Transformers / Permutation Invariance
Why needed: SRI inputs are sets of trajectories where order shouldn't matter. Set transformers are critical for aggregating information from multiple demonstrations into a single task embedding.
Quick check: If you feed 5 demonstrations into the encoder in different orders, should the task embedding change?

### Bayes Optimality & The Posterior Mean
Why needed: The paper claims SRI is "Bayes-optimal," which relies on the mathematical property that minimizing MSE predicts the mean of a distribution.
Quick check: If the true reward is either +1 or -1 with 50% probability, what reward value will an MSE-minimizing model predict?

## Architecture Onboarding

### Component map:
Trajectory Encoder (Transformer) -> Set Transformer -> Task Encoder -> State Encoder -> Reward MLP

### Critical path:
Ground Truth Reward Generator (simulation) → Reward MLP (training). Behavior Demonstrations → Task Encoder → Reward MLP (deployment), which feeds into RL algorithm.

### Design tradeoffs:
- Simulation dependency: Requires ground-truth rewards, easy in simulation but hard in real world
- State sampling strategy: Must sample states from all regions the policy might explore to prevent reward hacking
- Model capacity: Must be sufficient to distinguish complex behavioral patterns like gestures vs failed attempts

### Failure signatures:
- Reward hacking: If training data doesn't cover explored states, model may create incorrect local optima
- Gesture misinterpretation: If trained mostly on optimal reaches, may fail to recognize pointing gestures
- Distribution shift: Poor performance in states not well-represented in training data

### First 3 experiments:
1. Train SRI on "Noisy" demonstrations (ε=0.87) and compare performance against AIRL/GAIL baselines
2. Train SRI with decreasing numbers of labeled state-reward pairs to identify minimum data threshold
3. Train on optimal demos, test on pure gestures to verify goal interpolation from incomplete trajectories

## Open Questions the Paper Calls Out

### Open Question 1
Can SRI maintain asymptotic Bayes-optimality and data efficiency when applied to high-dimensional visual tasks without engineered state representations? The authors suggest visual tasks would require image encoders but haven't studied this transition.

### Open Question 2
To what extent does integrating pre-trained visual foundation models mitigate SRI's high data requirements for complex real-world tasks? The paper proposes this enhancement but hasn't validated it empirically.

### Open Question 3
How sensitive is SRI to reward hacking when training state-reward samples don't fully cover states explored by the resulting policy? The authors identify this as a failure mode but haven't quantified the sensitivity.

## Limitations
- Requires ground-truth reward signals in simulation, creating scalability barriers for real-world deployment
- Theoretical convergence assumes equicontinuous function classes and compact state spaces that may not hold in practice
- Performance degrades when RL agents explore states poorly represented in training data, potentially causing reward hacking

## Confidence

- **High Confidence**: Supervised learning framework for reward inference is technically sound and well-implemented
- **Medium Confidence**: Theoretical Bayes-optimality claim holds under stated conditions but practical relevance depends on realistic scenarios
- **Medium Confidence**: Empirical results show SRI's robustness to behavioral patterns, but comparisons are limited to specific baselines and distortions

## Next Checks

1. Systematically evaluate SRI's performance as the proportion of states visited during RL that were absent from training increases, measuring reward prediction error and policy degradation.

2. Implement SRI in a physical robotic manipulation task where ground-truth rewards must be approximated through human supervision, comparing data efficiency and performance against traditional IRL methods.

3. Create a benchmark suite with demonstrations spanning from near-optimal to completely random, including novel behavioral patterns not seen in training, to evaluate SRI's advantage across diverse demonstration qualities.