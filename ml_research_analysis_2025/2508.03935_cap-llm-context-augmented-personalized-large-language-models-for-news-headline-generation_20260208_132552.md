---
ver: rpa2
title: 'CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline
  Generation'
arxiv_id: '2508.03935'
source_url: https://arxiv.org/abs/2508.03935
tags:
- user
- generation
- cap-llm
- factual
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personalized news headline
  generation, aiming to create customized headlines that are both appealing to users
  and factually consistent with news articles. The proposed CAP-LLM framework leverages
  the power of large language models by integrating user preferences and factual consistency
  constraints.
---

# CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation

## Quick Facts
- **arXiv ID:** 2508.03935
- **Source URL:** https://arxiv.org/abs/2508.03935
- **Reference count:** 39
- **Primary result:** State-of-the-art performance on personalized news headline generation with FactCC 87.50, outperforming BART while improving personalization and content coverage.

## Executive Summary
CAP-LLM addresses the challenge of generating personalized news headlines that balance user appeal with factual consistency. The framework integrates a User Preference Encoder to capture long-term user interests, a Context Injection Adapter to seamlessly integrate these preferences with article context, and a Fact-Consistency Reinforcement Module employing contrastive loss to mitigate hallucination. Evaluated on the PENS dataset, CAP-LLM achieves state-of-the-art performance with significant improvements in factual consistency (FactCC of 87.50 vs. BART's 86.67) while enhancing personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01).

## Method Summary
CAP-LLM leverages a frozen Llama-2 7B backbone with lightweight LoRA adapters to generate personalized headlines. The architecture processes user history through a 2-layer Transformer User Preference Encoder to create a user interest vector, which is then projected and injected into the LLM via a Context Injection Adapter. A Fact-Consistency Reinforcement Module applies contrastive loss to ensure generated headlines remain faithful to source articles. The model is trained with a joint loss function combining generation, factual consistency, and personalization objectives, using AdamW optimizer with learning rate 5×10⁻⁵ for 10 epochs on the PENS dataset.

## Key Results
- Achieves state-of-the-art FactCC score of 87.50, significantly outperforming baseline models
- Improves personalization metrics with Pc(avg) 2.73 and Pc(max) 17.25
- Maintains strong content coverage with ROUGE-1 26.55, ROUGE-2 9.95, and ROUGE-L 23.01
- Ablation studies confirm effectiveness of each component, with contrastive loss being primary driver of factual consistency gains

## Why This Works (Mechanism)

### Mechanism 1
Aggregating user history into a fixed vector allows the model to condition headline generation on long-term interests, provided the history is sufficiently rich. A User Preference Encoder uses self-attention over historical click embeddings, followed by pooling to create a user vector $v_u$. This vector is projected and injected into the LLM, biasing token generation toward user-relevant topics. Core assumption: Historical clicks accurately reflect genuine user intent rather than noise or accidental exposure.

### Mechanism 2
Lightweight adapters allow a frozen LLM to be steered by user preferences and article context without catastrophic forgetting or full retraining. The Context Injection Adapter takes the user vector ($v_u$) and the current article representation ($e_{Dc}$), projecting them into the LLM's hidden layers. It modulates the LLM's internal states ($H_l$) to align generation with the injected context. Core assumption: The pre-trained LLM backbone has sufficient semantic capacity to generate the headline, requiring only "steering" rather than structural modification.

### Mechanism 3
Contrastive learning enforces factual consistency by penalizing generated segments that deviate from the source text. The Fact-Consistency Reinforcement Module constructs positive (consistent) and negative (hallucinated/irrelevant) segment pairs. It maximizes similarity between generated segments and factual anchors while minimizing similarity to negatives via a contrastive loss ($L_{fact}$). Core assumption: Factual consistency can be effectively learned via segment-level similarity maximization rather than just token-level likelihood.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - Why needed here: The architecture relies on adapting a massive LLM (Llama-2 7B) using "lightweight adapter modules" rather than retraining all weights. Understanding low-rank matrix injection is essential to grasp how the Context Injection Adapter functions.
  - Quick check question: How does injecting a small rank-decomposition matrix into the attention weights alter the model's output distribution without changing the pre-trained frozen weights?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The core innovation for factual consistency is $L_{fact}$. You must understand how contrastive loss pulls positive pairs closer and pushes negative pairs apart in embedding space to see how "hallucination" is mathematically penalized.
  - Quick check question: In the context of Equation (12), what defines a "negative sample" ($s_{neg}$), and how does the temperature parameter $\tau$ affect the strictness of this separation?

- **Concept: Multi-head Self-Attention for User Modeling**
  - Why needed here: The User Preference Encoder (Section III.B) aggregates a set of historical articles. Understanding how self-attention creates a weighted representation of history is key to understanding how the model summarizes a user's "long-term interests."
  - Quick check question: How does the Pooling operation (Equation 8) collapse the variable-length sequence of historical attention outputs into a single user vector $v_u$?

## Architecture Onboarding

- **Component map:** User History $U$ + Current Article $D_c$ -> User Preference Encoder -> $v_u$ -> Projection -> Context Injection Adapter -> Llama-2 7B (Frozen) -> LM Head -> Headline

- **Critical path:** The flow of the user vector $v_u$ is critical. It moves from the Preference Encoder -> Projection -> Adapter -> LLM Hidden States. If this projection is misaligned, personalization fails.

- **Design tradeoffs:**
  - $\lambda_1$ (Factual) vs. $\lambda_2$ (Personalization): Table V and VI show a direct tension. High $\lambda_1$ boosts FactCC but can dampen personalization scores (Pc) slightly. The default $\lambda_1=0.5, \lambda_2=0.2$ is a tuned balance, not a global optimum.
  - History Length ($N$): Table IV shows diminishing returns after 50 articles. Processing 100 articles adds compute cost without significant Pc gain.

- **Failure signatures:**
  - Generic Headlines: High FactCC, low Pc. Likely caused by insufficient user history ($N<5$) or $\lambda_2$ set too low.
  - Hallucinated Details: Low FactCC. Likely caused by disabled/weak $L_{fact}$ ($\lambda_1 \approx 0$) or poor negative sampling in contrastive loss.
  - Incoherent Style: Conflicting gradients. Check if adapter learning rate is too high relative to the encoder.

- **First 3 experiments:**
  1. Ablation Sanity Check: Replicate the "w/o Fact-Consistency" row in Table II to verify that the contrastive loss is the primary driver of the high FactCC score (87.50 -> 82.10).
  2. History Sensitivity: Run inference on users with only 1-3 historical clicks vs. 50 clicks to quantify the performance degradation on Pc(avg) and verify the "cold start" behavior.
  3. Hyperparameter Sweep: Vary $\lambda_1$ (0.0 to 1.0) on a validation set to observe the trade-off curve between FactCC and ROUGE scores, confirming the paper's claim of a "superior balance."

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be adapted to capture dynamic, short-term shifts in user preferences rather than relying solely on long-term history? The conclusion states, "For future work, we plan to explore more dynamic user interest modeling that adapts to short-term changes in preferences." The current User Preference Encoder aggregates historical clicks into a static long-term interest vector, potentially missing immediate intent shifts.

### Open Question 2
Can the CAP-LLM architecture be effectively extended to process multimodal news content (e.g., images and videos)? The conclusion suggests "extending this framework to incorporate multimodal news content" as a valuable direction. The current model relies exclusively on text-based inputs for both the article body and user history.

### Open Question 3
Does the proposed Context Injection Adapter and Fact-Consistency Reinforcement Module generalize to other LLM architectures? The authors propose investigating "the impact of different LLM architectures... could yield further improvements." The experimental validation is restricted to the Llama-2 7B model, leaving performance on encoder-decoder or larger decoder-only models unknown.

## Limitations
- Key LoRA configuration parameters (rank, alpha, target modules) and contrastive loss segmentation strategy are not detailed, creating uncertainty about faithful reproduction
- Cold-start scenarios with minimal user history are not addressed, representing a significant practical limitation
- Generalization to other news corpora or user populations remains untested, as validation is restricted to the PENS dataset

## Confidence

- **High Confidence:** The core architectural concept of combining user preference encoding with factual consistency constraints is well-supported. Ablation studies demonstrate that each component contributes measurably to performance gains.
- **Medium Confidence:** Reported performance improvements are convincing within the PENS dataset context, but generalization to other news corpora remains untested. Hyperparameter sensitivity analysis shows robustness to reasonable variations.
- **Low Confidence:** The mechanism for handling cold-start scenarios (users with minimal history) is not addressed. The paper assumes 50 historical articles are sufficient but provides no strategy for users with sparse interaction histories.

## Next Checks

1. **Contrastive Loss Implementation Verification:** Reconstruct the Fact-Consistency Reinforcement Module using the paper's description and verify that the negative sampling strategy effectively captures hallucinated content. Test whether random negative sampling from the batch achieves comparable FactCC scores to the claimed method.

2. **Cold-Start Performance Analysis:** Evaluate CAP-LLM on users with progressively fewer historical articles (1, 5, 10, 50) to quantify the performance degradation in Pc scores. This will reveal whether the fixed-length user vector aggregation creates a practical limitation for new users.

3. **Cross-Dataset Generalization:** Apply the trained CAP-LLM model to a different news headline dataset (e.g., Newsroom or CNN/DailyMail) without fine-tuning to assess whether the factual consistency gains transfer across domains, or if they are specific to the PENS dataset distribution.