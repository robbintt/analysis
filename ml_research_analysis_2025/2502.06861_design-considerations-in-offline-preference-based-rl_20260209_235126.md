---
ver: rpa2
title: Design Considerations in Offline Preference-based RL
arxiv_id: '2502.06861'
source_url: https://arxiv.org/abs/2502.06861
tags:
- loss
- policy
- which
- learning
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework for understanding offline
  reinforcement learning from human preferences, focusing on how design choices in
  algorithms like DPO and IPO affect learned policy quality. The authors develop a
  general framework that encompasses most existing offline RLHF methods and establish
  a benchmark policy to measure performance against.
---

# Design Considerations in Offline Preference-based RL

## Quick Facts
- arXiv ID: 2502.06861
- Source URL: https://arxiv.org/abs/2502.06861
- Reference count: 17
- This paper provides a theoretical framework for understanding offline reinforcement learning from human preferences, focusing on how design choices in algorithms like DPO and IPO affect learned policy quality.

## Executive Summary
This paper develops a theoretical framework for offline reinforcement learning from human preferences (RLHF), unifying most existing methods under a common optimization objective. The authors establish a benchmark policy π* that represents the ideal KL-regularized reward maximizer and derive a bound on the KL divergence between learned policies and this benchmark. Their analysis reveals that squared loss (used in IPO) provides superior stability compared to logistic loss (used in DPO) due to better curvature properties, and that normalizing by a reference policy rather than uniform distribution improves performance. Empirical experiments on summarization confirm these theoretical predictions, showing logistic loss variants suffer catastrophic collapse while squared loss maintains stable performance.

## Method Summary
The authors develop a unified framework for offline preference-based RL that encompasses methods like DPO, IPO, and SLiC. The core objective minimizes a proper loss function applied to preference labels, with key design choices being the loss function (logistic vs squared) and normalization base policy (uniform vs reference). They theoretically analyze the KL divergence between learned policies and a benchmark policy π*, which depends on loss curvature and data coverage properties. Empirically, they compare logistic and squared loss variants on TL;DR summarization using T5 Large, measuring preference score against a reference policy fine-tuned on winning responses.

## Key Results
- Squared loss (IPO) maintains constant curvature across optimization landscape while logistic loss (DPO) curvature degrades exponentially as log-likelihoods grow large
- Using a reference policy for normalization (μ=π_ref) improves performance by positioning the benchmark policy as the KL-regularized reward maximizer
- Empirical experiments show logistic loss variants suffer catastrophic collapse during training with log-likelihoods crashing to zero, while squared loss variants maintain stable performance
- The theoretical bound on KL divergence depends on both the curvature constant c_μ of the loss function and the coverage constant C of the offline dataset

## Why This Works (Mechanism)

### Mechanism 1: Loss Curvature Determines Stability
- Claim: Squared loss (IPO) provides more stable training than logistic loss (DPO) because it maintains constant curvature across the optimization landscape.
- Mechanism: The theoretical bound on KL divergence to the benchmark policy π* scales inversely with the curvature constant c_μ. Squared loss satisfies the curvature assumption with c_μ = 1/2 uniformly, while logistic loss curvature degrades exponentially as log-likelihood ratios grow large.
- Core assumption: Assumption 3.5 requires curvature around the optimal policy π* for the bound to hold.
- Evidence anchors:
  - [abstract] "squared loss performs better than logistic loss due to superior curvature properties"
  - [section 5, Figure 2] Logistic loss sends log-likelihoods of both preferred and dispreferred responses crashing sharply to zero, while squared loss shows mild, stable decline
  - [corpus] Related work on Orthogonalized Policy Optimization (arxiv 2601.12415) also discusses decoupling optimization geometry, suggesting curvature concerns are recognized in the field
- Break condition: When ωπ,μ becomes very large (driving loss to near-zero), logistic loss curvature collapses (c_μ ≈ 0.00066 empirically vs. 2.0 for squared loss in Table 2).

### Mechanism 2: Reference Policy Normalization Anchors the Optimal Policy
- Claim: Normalizing by reference policy π_ref (rather than uniform μ=1) positions the benchmark policy π* as the KL-regularized reward maximizer.
- Mechanism: Under proper loss and BTL preference model, realizability (Assumption 3.2) requires the learned reward to match R* + log(μ). Setting μ = π_ref makes π* naturally correspond to the optimal softmax policy rather than an arbitrary reward-shifting of it.
- Core assumption: The preference data follows a BTL model with some underlying reward R*, and π_ref provides reasonable coverage of good responses.
- Evidence anchors:
  - [abstract] "using a reference policy for normalization improves results"
  - [section 3.1] "When μ = π_0, then we see that the policy π* exactly corresponds to this optimal policy for an appropriately chosen loss function"
  - [corpus] Limited direct corpus evidence on normalization effects; most related papers focus on optimization algorithms rather than base policy choice
- Break condition: If π_ref has very different support from the optimal policy, the realizability assumption fails since π* may fall outside the constrained policy class Π.

### Mechanism 3: Data Coverage Bounds Generalization to Optimal Policy
- Claim: Generalization from training data to the benchmark policy π* depends on coverage of π*'s response space by the offline dataset.
- Mechanism: The coverage assumption (3.4) requires that squared error under the data distribution bounds squared error under π*. This translates empirical loss minimization into a KL guarantee. The constant C captures how well Dy(·|x) covers π*(·|x).
- Core assumption: The data sampling policy provides sufficient coverage such that for reward class R, the covariance structure satisfies λ_max(Σ_y^(-1/2) Σ_{π*μ} Σ_y^(1/2)) ≤ C.
- Evidence anchors:
  - [abstract] "bound depends on the curvature of the loss function, as well as the coverage of the offline dataset"
  - [section 3.1, Assumption 3.4] Ex_y~π*(·|x)[ΔR̄(x,y)²] ≤ C · Ex_y~D[ΔR̄(x,y)²]
  - [corpus] Adversarial Policy Optimization paper (arxiv 2503.05306) addresses conservatism under coverage limitations, confirming coverage is an active concern
- Break condition: If Dy places zero probability on responses that π* would highly rank, the coverage constant C becomes unbounded and no generalization guarantee holds.

## Foundational Learning

- Concept: **Bradley-Terry-Luce (BTL) Model**
  - Why needed here: The theoretical results assume preference labels are generated from a BTL model where P(ω=1|x,y,y') = σ(R*(x,y) - R*(x,y')). This connects the loss function to a probabilistic model of preferences.
  - Quick check question: Given two responses with rewards R*(x,y) = 2.0 and R*(x,y') = 0.5, what is the probability that y is preferred over y'?

- Concept: **KL-Regularized Policy Optimization**
  - Why needed here: The benchmark policy π* is defined as the KL-regularized reward maximizer: π*(y|x) ∝ π_0(y|x)exp(βR*(x,y)). This is what offline methods approximate without online sampling.
  - Quick check question: Why use KL regularization instead of directly maximizing reward? What failure mode does it prevent?

- Concept: **Proper Loss Functions**
  - Why needed here: Assumption 3.3 requires proper losses where argmin_v[ηℓ(v) + (1-η)ℓ(-v)] recovers a function of η. This ensures the loss minimizer aligns with true preference probabilities.
  - Quick check question: Is cross-entropy loss a proper loss? What about hinge loss?

## Architecture Onboarding

- Component map:
  - Loss function ℓ: Maps log-likelihood ratio differences to scalar loss. Choices: logistic (DPO), squared (IPO), hinge (SLiC).
  - Base policy μ: Normalization for log-likelihoods. Choices: uniform (μ=1), reference policy (μ=π_ref).
  - Policy class Π: Constrained set of policies. Includes implicit constraints from early stopping, explicit regularization (CE to π_0).
  - Benchmark π*: The target policy defined by realizability and properness conditions. KL(π*||π) is the performance metric.

- Critical path:
  1. Collect offline preference dataset with inputs x, response pairs (y, y'), and preference labels ω
  2. Choose base policy μ (recommend μ = π_ref) and loss function ℓ (recommend squared loss)
  3. Initialize policy π = π_ref (SFT policy)
  4. Minimize empirical loss in Equation (1) with early stopping or explicit constraints
  5. Monitor log-likelihoods of preferred/dispreferred responses to detect collapse

- Design tradeoffs:
  - **Logistic vs. Squared loss**: Logistic is theoretically motivated by BTL model but suffers curvature collapse. Squared is less realistic probabilistically but empirically more stable.
  - **μ = π_ref vs. μ = uniform**: π_ref normalization improves performance by ~5-10% in win rate but requires storing/computing reference log-probabilities during training.
  - **Constraint strength**: Tighter constraints (more regularization, earlier stopping) reduce risk of distribution shift but may underfit preferences.

- Failure signatures:
  - **Catastrophic collapse**: Log-likelihoods of both preferred and dispreferred responses crash to zero simultaneously while loss decreases (seen in DPO after ~5K steps). Caused by logistic loss curvature → 0.
  - **Distribution shift**: Learned policy places mass on responses outside dataset support. Caused by insufficient coverage in Dy.
  - **Underfitting**: Loss plateaus early without matching preference signal. Caused by over-regularization or policy class too constrained.

- First 3 experiments:
  1. **Replicate the loss comparison**: Train with logistic loss (DPO) vs. squared loss (IPO) on TL;DR summarization, plot log-likelihoods of preferred/dispreferred responses over training steps. Expect DPO collapse after 5-10K steps.
  2. **Ablate base policy**: Compare μ = π_ref vs. μ = uniform with squared loss. Measure win rate against reference policy at multiple checkpoints. Expect small but consistent improvement from π_ref normalization.
  3. **Stress test coverage**: Create a dataset with deliberately limited response diversity. Compare performance of both loss types as coverage decreases. Expect squared loss to degrade more gracefully.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can active data collection strategies or experimental design techniques effectively mitigate the coverage limitations inherent in fixed offline preference datasets?
- Basis in paper: [explicit] The Discussion section suggests that "using experimental design techniques for data collection before offline RLHF" is a fruitful avenue for addressing the limited coverage obstacle identified.
- Why unresolved: The current theoretical framework assumes a fixed dataset $\mathcal{D}$, leaving the impact of active sampling on the coverage constant $C$ and subsequent policy quality unexplored.
- What evidence would resolve it: An empirical or theoretical analysis showing that adaptively collected data reduces the coverage constant $C$ in Assumption 3.4, leading to tighter KL bounds compared to static datasets.

### Open Question 2
- Question: Are there alternative loss functions that combine the theoretical stability (curvature) of squared loss with the realistic preference modeling of logistic loss?
- Basis in paper: [explicit] The authors suggest identifying "losses which do not decay to zero at a slow rate" as a direction for future work to prevent the degeneration of log-likelihoods.
- Why unresolved: The paper demonstrates that squared loss is theoretically superior due to constant curvature ($c_\mu$) but acknowledges its underlying probabilistic model is less realistic than the Bradley-Terry-Luce (BTL) model associated with logistic loss.
- What evidence would resolve it: The derivation and empirical validation of a novel loss function that maintains a bounded curvature constant $c_\mu$ across the full range of $\omega_{\pi,\mu}$ while adhering to a BTL-like preference generation process.

### Open Question 3
- Question: Does the theoretical framework for offline RLHF hold when length normalization is applied to log-likelihoods, as seen in methods like SimPO?
- Basis in paper: [inferred] The paper notes in Table 1 and Footnote 2 that SimPO's length normalization is "currently not included in our theoretical setup," creating a gap in the unified treatment of methods.
- Why unresolved: The theoretical bounds rely on the relationship between rewards and log-likelihoods which length normalization alters by dividing by $|y|$, potentially affecting the realizability and curvature assumptions.
- What evidence would resolve it: An extension of Theorem 3.6 that successfully incorporates response length into the benchmark policy definition or empirical results showing the theorem's predictions hold despite length-normalized optimization.

## Limitations

- The theoretical analysis relies heavily on Assumptions 3.2-3.5, particularly the curvature assumption for loss functions and the coverage assumption for data, which may not hold in practice
- Results are validated only on one summarization task with a single model size (T5 Large), limiting generalizability to other domains and model scales
- The mechanism for logistic loss collapse is demonstrated qualitatively through log-likelihood monitoring rather than through rigorous characterization of the curvature degradation

## Confidence

- **High confidence**: The empirical observation that squared loss variants maintain stable log-likelihoods while logistic loss variants collapse (verified in Figure 2)
- **Medium confidence**: The theoretical bound relating KL divergence to curvature and coverage constants, as the assumptions are reasonable but not exhaustively validated
- **Medium confidence**: The claim that reference policy normalization improves performance by ~5-10%, as this requires precise experimental control and the effect size is modest

## Next Checks

1. **Cross-task validation**: Replicate the loss comparison (logistic vs squared) on at least two additional tasks (e.g., dialogue, instruction following) with different model scales to assess generalizability of the collapse phenomenon
2. **Coverage quantification**: Systematically vary the diversity of response pairs in the training data and measure how C (coverage constant) relates to empirical performance degradation, particularly for squared loss
3. **Curvature analysis**: Empirically measure the curvature constant c_μ during training for both losses across different iterations and response pairs to validate the theoretical prediction that c_μ → 0 for logistic loss under extreme conditions