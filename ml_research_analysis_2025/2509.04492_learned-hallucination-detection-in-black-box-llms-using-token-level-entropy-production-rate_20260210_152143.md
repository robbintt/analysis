---
ver: rpa2
title: Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy
  Production Rate
arxiv_id: '2509.04492'
source_url: https://arxiv.org/abs/2509.04492
tags:
- entropy
- uncertainty
- token
- detection
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a practical method for detecting hallucinations
  in black-box large language models (LLMs) during question answering tasks. The core
  idea is to derive uncertainty indicators directly from the top-K log-probabilities
  exposed by API-constrained LLMs during generation.
---

# Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate

## Quick Facts
- arXiv ID: 2509.04492
- Source URL: https://arxiv.org/abs/2509.04492
- Authors: Charles Moslonka; Hicham Randrianarivo; Arthur Garnier; Emmanuel Malherbe
- Reference count: 40
- Primary result: WEPR consistently outperforms EPR and state-of-the-art methods using only top-10 log-probabilities

## Executive Summary
This paper presents a practical method for detecting hallucinations in black-box large language models (LLMs) during question answering tasks. The core idea is to derive uncertainty indicators directly from the top-K log-probabilities exposed by API-constrained LLMs during generation. The method first computes an Entropy Production Rate (EPR) from token-level entropy estimates and then refines it with supervised learning (WEPR) using features from top-ranked token contributions. Experiments across multiple QA datasets and models show that WEPR consistently outperforms both unsupervised EPR and state-of-the-art detection methods like SelfCheckGPT and HalluDetect. Importantly, strong performance is maintained using as few as K≤10 log-probabilities per token, confirming the method's efficiency and suitability for real-world API deployments.

## Method Summary
The method extracts top-K log-probabilities from black-box LLM API responses and computes token-level entropy contributions. EPR averages these entropies across the sequence as an unsupervised uncertainty measure. WEPR enhances this by learning weighted combinations of mean and max-pooled entropic contributions across ranks via logistic regression. The approach works with API constraints (K≤20 log-probs) and decouples uncertainty measurement from sampling temperature by using raw-logit log-probs. The method is validated on TriviaQA, WebQuestions, and ArGiMi-Ardian Finance datasets across multiple model sizes.

## Key Results
- WEPR achieves 3-6 ROC-AUC points improvement over EPR across TriviaQA models
- Performance plateaus at K≈8-10 log-probabilities per token
- Outperforms SelfCheckGPT and HalluDetect on TriviaQA and WebQuestions
- Maintains strong performance in specialized RAG domain (financial reports)
- Temperature invariance confirmed: T_samp=0.6 vs 1.0 shows <1 ROC-AUC difference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Top-K log-probabilities (K ≤ 10) provide sufficient signal for hallucination detection because probability mass concentrates heavily in top-ranked tokens.
- **Mechanism:** The estimator computes per-token entropy from accessible log-probs; the missing tail mass Q_K drops below 10⁻⁴ median by K=10, making truncation error negligible for the practically sampleable token pool.
- **Core assumption:** The relevant uncertainty is captured within the sampleable vocabulary (K_samp ≈ 50), not the full |V|.
- **Evidence anchors:**
  - [abstract]: "high performance is demonstrated using only the typically small set of available log-probabilities (e.g., top <10 per token)"
  - [section 3.2]: Equation (5) bounds missing entropy; Figure 2(b) shows Q_K decay
  - [corpus]: Moderate support—related methods (HALT, ECLIPSE) also exploit top-K log-probs, but no direct replication of the K-sufficiency bound
- **Break condition:** If APIs further restrict K < 3, or if models exhibit heavy-tailed distributions where rank-10+ tokens carry significant probability mass, estimation quality degrades.

### Mechanism 2
- **Claim:** Supervised weighting of per-rank entropic contributions improves detection over uniform averaging because hallucinations often manifest as localized "spike" uncertainty.
- **Mechanism:** WEPR learns weights (β_k) for each rank's entropy contribution and adds max-pooled terms (γ_k · max_j s_k,j) to capture sequence-level uncertainty spikes; trained via logistic regression on binary annotations.
- **Core assumption:** Annotated hallucination labels reliably proxy ground-truth factual correctness; spike patterns generalize across queries.
- **Evidence anchors:**
  - [abstract]: "estimator significantly improves hallucination detection over using EPR alone"
  - [section 4.3.1]: WEPR ROC-AUC improves 3–6 points over EPR across models on TriviaQA
  - [corpus]: Indirect—Semantic Entropy Probes (SEPs) similarly show learned probes outperform raw entropy, supporting learnability of uncertainty signals
- **Break condition:** If training annotations are noisy (LLM-as-judge errors) or test domains have different hallucination patterns (e.g., long reasoning chains), learned weights may overfit or underperform.

### Mechanism 3
- **Claim:** Entropy computed from raw-logit log-probabilities remains discriminative regardless of sampling temperature used for generation.
- **Mechanism:** APIs (OpenAI, vLLM) expose log-probs at T=1 (unscaled) even when sampling uses T_samp ≠ 1; this decouples measured uncertainty from decoding stochasticity.
- **Core assumption:** Model internals' raw-logit distributions reflect intrinsic uncertainty, not sampling artifacts.
- **Evidence anchors:**
  - [section 3.3]: Explicitly states log-probs are "computed from raw logits (equivalent to fixing T=1 for calculation)"
  - [section 4.3.1]: T_samp = 0.6 vs. 1.0 yields <1.0 point ROC-AUC difference on Falcon-3-10B
  - [corpus]: Weak—no corpus papers explicitly validate temperature-invariance of log-prob-based entropy
- **Break condition:** If APIs begin returning temperature-scaled log-probs, or if providers change logit exposure behavior, calibration may shift.

## Foundational Learning

- **Concept: Shannon entropy for discrete distributions**
  - Why needed here: Core mathematical object; EPR is average per-token entropy H(q) = -Σ p_i log₂(p_i)
  - Quick check question: If a token has probabilities [0.7, 0.2, 0.1] for top-3 candidates, calculate its entropy contribution.

- **Concept: Logistic regression for binary classification**
  - Why needed here: WEPR training uses sigmoid loss on sequence-level scores; understanding calibration is essential
  - Quick check question: Why might max-pooled features (max_j s_k,j) help separate hallucinated vs. faithful sequences compared to averaging alone?

- **Concept: Black-box API constraints in LLM deployment**
  - Why needed here: Method is designed for K≤20 log-probs access; understanding API limitations prevents misapplication
  - Quick check question: Your API only returns top-5 log-probs. Does the paper's K-sufficiency claim (K≈10) still apply? What degradation might you expect?

## Architecture Onboarding

- **Component map:**
  Input: query q → LLM generation (T_samp=1.0, K_samp=50) → Extraction: top-K log-probs per token (K≤15 typical) → Per-token features: s_k,j = -p_r(k),j · log₂(p_r(k),j) for k∈[1,K] → Sequence aggregation: EPR (uniform average) or WEPR (β-weighted average + γ-weighted max-pool) → Output: hallucination score σ(WEPR) ∈ [0,1]; optional token-level σ(S_β)

- **Critical path:**
  1. Ensure API exposes log-probs (not just tokens); verify K ≥ 5–10
  2. Use non-greedy decoding (T_samp > 0) to ensure meaningful distributions
  3. Train WEPR on domain-specific annotations if available; otherwise use EPR as unsupervised baseline

- **Design tradeoffs:**
  - EPR vs. WEPR: EPR requires no training data but lower accuracy; WEPR needs ~hundreds of annotations but gains 3–6 ROC-AUC points
  - K selection: Higher K improves coverage but increases API data volume; paper shows plateau at K≈8–10
  - Token vs. sequence scores: Token-level enables fine-grained highlighting but may flag stylistic hesitation (false positives at sequence start)

- **Failure signatures:**
  - High-certainty hallucinations: confident incorrect outputs with low entropy (memorization errors); method explicitly notes this limitation
  - Stylistic hesitation flagged as factual uncertainty: observe elevated scores at sequence beginnings
  - Domain shift: WEPR trained on TriviaQA drops ~10 ROC-AUC points on WebQuestions (generalization gap)

- **First 3 experiments:**
  1. Baseline validation: On your LLM + QA dataset, extract top-10 log-probs, compute EPR, plot hallucinated vs. faithful score distributions (replicate Figure 3 separation)
  2. K-sensitivity sweep: Vary K∈{3,5,8,10,15} on held-out set; confirm ROC-AUC plateaus near K≈8–10 for your model
  3. WEPR transfer test: Train WEPR on general QA (e.g., TriviaQA subset), evaluate on domain-specific RAG (e.g., your financial reports); measure ROC-AUC gap vs. in-domain training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the entropy-based signal (EPR/WEPR) remain discriminative when applied to frontier models with 100B+ parameters or closed-source APIs like GPT-4?
- **Basis in paper:** [explicit] Section 5 (Limitations) states that "Performance on bigger models (e.g., 100B+ parameters or closed-source APIs like GPT-5) remains to be tested."
- **Why unresolved:** Current experiments are restricted to mid-sized models (8B–24B); larger models may exhibit different calibration properties or entropy distributions.
- **What evidence would resolve it:** ROC-AUC evaluations of WEPR on TriviaQA/WebQuestions using generations from models like GPT-4 or Llama-3-70B.

### Open Question 2
- **Question:** Can the method be effectively extended to tasks requiring extensive multi-step reasoning or long-form generation?
- **Basis in paper:** [explicit] The authors note in Section 5 that they "did not study tasks requiring extensive multi-step reasoning... where different types of uncertainty could dominate."
- **Why unresolved:** Long reasoning chains may involve valid high-entropy "thinking" steps, potentially causing false positives in a detector tuned for short, factual answers.
- **What evidence would resolve it:** Experiments on chain-of-thought reasoning datasets (e.g., GSM8K) analyzing the correlation between entropy spikes and logical errors versus valid reasoning steps.

### Open Question 3
- **Question:** How can the approach be augmented to detect "high-certainty hallucinations" where the model generates incorrect information with low entropy (high confidence)?
- **Basis in paper:** [explicit] Section 5 identifies the inability to detect confident errors as an inherent limitation, noting the method fails on "instances where the model generates incorrect information with low output entropy."
- **Why unresolved:** The core methodology relies on entropy as a proxy for uncertainty; it mathematically cannot flag errors the model is "confident" about due to memorization.
- **What evidence would resolve it:** A hybrid system combining WEPR with consistency-based checks (like SelfCheckGPT) that demonstrates improved recall on datasets of confident factual errors.

## Limitations

- Cannot detect "high-certainty hallucinations" - confident incorrect outputs with low entropy
- Performance degrades 10+ ROC-AUC points when transferring from general QA to specialized domains
- Relies on API providers exposing top-K log-probabilities; performance may degrade if K is restricted below 5

## Confidence

**High Confidence:** The unsupervised EPR mechanism works as described - top-K log-probabilities (K≥10) provide sufficient signal for uncertainty estimation, and the mathematical derivation of entropy from log-probabilities is sound. The temperature-invariance claim is supported by empirical evidence.

**Medium Confidence:** The WEPR supervised learning approach improves detection over EPR, but performance gains (3-6 ROC-AUC points) are modest and may not justify the annotation cost. Domain transfer performance suggests learned weights don't generalize as robustly as claimed.

**Low Confidence:** The paper's claims about hallucination detection in RAG settings are weakly supported - only one specialized dataset is evaluated, and no comparison is made against RAG-specific baselines. The method's performance on long-form generation (beyond single answers) remains untested.

## Next Checks

1. **API Constraint Sensitivity:** Test the method across different API providers (OpenAI, Anthropic, Google) with varying K limits (top-5, top-10, top-20). Measure ROC-AUC degradation as K decreases below 10, and verify that temperature-scaling changes don't affect results.

2. **Annotation Robustness:** Replicate the Gemma-3-12b-it annotation process with different prompts and compare WEPR performance. Test whether training on human-annotated labels (if available) yields better generalization than LLM-as-a-judge annotations.

3. **Long-form Generation:** Evaluate the method on extended reasoning tasks (e.g., MATH, GSM8K) where hallucinations occur across multiple reasoning steps rather than single answers. Measure whether token-level uncertainty scores remain discriminative when sequences span hundreds of tokens.