---
ver: rpa2
title: Automatic Rank Determination for Low-Rank Adaptation via Submodular Function
  Maximization
arxiv_id: '2507.01841'
source_url: https://arxiv.org/abs/2507.01841
tags:
- lora
- rank
- determination
- loss
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automatic rank determination for Low-Rank
  Adaptation (LoRA) in transfer learning settings, focusing on when to prune LoRA
  components to maintain efficiency without sacrificing performance. The authors propose
  SubLoRA, which uses submodular function maximization based on second-order information
  from the Hessian matrix, contrasting with prior first-order approaches like AdaLoRA.
---

# Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization

## Quick Facts
- arXiv ID: 2507.01841
- Source URL: https://arxiv.org/abs/2507.01841
- Reference count: 40
- Primary result: SubLoRA achieves superior rank determination for LoRA through Hessian-based submodular optimization, outperforming first-order methods on physics-informed neural networks.

## Executive Summary
This paper introduces SubLoRA, a method for automatic rank determination in Low-Rank Adaptation (LoRA) that leverages second-order Hessian information through submodular function maximization. Unlike existing first-order approaches like AdaLoRA, SubLoRA addresses the fundamental limitation that linearization becomes inaccurate near convergence points. The method formulates rank selection as a combinatorial optimization problem, projects the Hessian to ensure submodularity, and employs efficient greedy algorithms with theoretical approximation guarantees. Experiments on solving partial differential equations with physics-informed neural networks demonstrate consistent improvements over existing methods across elliptic, parabolic, and hyperbolic equations.

## Method Summary
SubLoRA determines LoRA rank by maximizing a submodular function derived from the Hessian of the loss with respect to singular values. The method operates in three stages: (1) fine-tune LoRA to convergence, (2) compute gradient and Hessian w.r.t. singular values and project the Hessian to ensure submodularity, and (3) greedily select singular values to maximize the submodular objective under a rank budget constraint. An alternating optimization variant iteratively prunes and re-trains to escape suboptimal rank allocations. The approach is validated on physics-informed neural networks solving three types of PDEs, comparing against first-order methods like AdaLoRA.

## Key Results
- SubLoRA consistently outperforms first-order methods (AdaLoRA, DiagLoRA) on PINNs across elliptic, parabolic, and hyperbolic equations
- The method shows robustness to overestimated rank budgets, maintaining performance when more capacity is allocated than needed
- Alternating optimization variant achieves lower relative error than post-hoc pruning while requiring additional computation
- Second-order information becomes critical near convergence, where first-order methods fail to provide meaningful pruning signals

## Why This Works (Mechanism)

### Mechanism 1: Hessian-Guided Curvature Sensing
When LoRA parameters converge near a stationary point, first-order gradient information becomes unreliable for rank determination because gradients approach zero. SubLoRA uses second-order Hessian information to capture curvature, ensuring singular values are retained based on their actual impact on the loss geometry. This quadratic approximation remains informative when linear approximations vanish.

### Mechanism 2: Submodular Relaxation via Hessian Projection
The method projects the Hessian matrix to satisfy submodularity conditions, transforming an NP-hard quadratic optimization into a tractable problem with theoretical guarantees. By ensuring off-diagonal elements satisfy specific sign conditions, the greedy algorithm can efficiently select singular values with provable approximation bounds instead of requiring exact combinatorial search.

### Mechanism 3: Alternating Optimization for Joint Training
Rather than performing rank determination as a one-shot post-processing step, SubLoRA alternates between pruning and fine-tuning. This allows the model to re-optimize remaining low-rank components after pruning, minimizing the performance drop typically associated with aggressive rank reduction and escaping suboptimal rank allocations found in static approaches.

## Foundational Learning

- **Concept:** Taylor Expansion (First vs. Second Order)
  - **Why needed here:** To understand why AdaLoRA (first-order) fails at convergence when gradients vanish
  - **Quick check question:** If ∇L ≈ 0 at a minimum, why does the first-order term ⟨∇L, Δσ⟩ fail to rank the importance of singular values?

- **Concept:** Submodular Functions
  - **Why needed here:** To grasp why the "Greedy Algorithm" works here but generally fails for NP-hard problems
  - **Quick check question:** Does the greedy algorithm provide an optimal solution or an approximation guarantee for submodular maximization?

- **Concept:** Hessian Projection / Positive Semi-Definiteness
  - **Why needed here:** To implement the core contribution (Algorithm 3), specifically transforming the Hessian
  - **Quick check question:** In the projection logic, what happens to an off-diagonal Hessian entry Hij if the product σiσj has the "wrong" sign?

## Architecture Onboarding

- **Component map:** Gradient/Hessian Calculator -> Submodular Projector -> Greedy Solver -> LoRA Layer
- **Critical path:** The calculation of the Hessian w.r.t singular values (∇²σLft). This is O(L²) where L is number of layers, not O(params), making it feasible.
- **Design tradeoffs:**
  - **Greedy vs. Randomized Greedy:** Standard Greedy is theoretically guaranteed only if monotonicity holds (Lemma 1 says it does if pre-training converged). Randomized Greedy is safer for non-monotone objectives but performed worse in experiments.
  - **Post-hoc vs. Alternating:** Post-hoc is faster (one-shot). Alternating yields lower error but requires re-training loops.
- **Failure signatures:**
  - **Diagonal Dominance:** If SubLoRA performs identically to DiagLoRA, check if the projected Hessian is effectively diagonal (off-diagonals zeroed out)
  - **Divergence in Alternating Mode:** Training loss oscillates or grows; implies rank budget b is too strict or learning rate is too high for the reduced capacity
- **First 3 experiments:**
  1. **Stationary Gradient Test:** Train a small MLP to convergence, apply LinearLoRA vs. SubLoRA. Verify LinearLoRA returns zero importance while SubLoRA distinguishes ranks.
  2. **Projection Ablation:** Compare SubLoRA (projected Hessian) vs. HessLoRA (raw Hessian) to verify if the submodular projection actually stabilizes the greedy selection.
  3. **Budget Sensitivity Sweep:** Run Algorithm 4 on the Allen-Cahn equation (Sec 5.1.2) sweeping budget b ∈ {20, 40, 80} to verify the "robustness to overestimation" claim.

## Open Questions the Paper Calls Out
The authors identify extending SubLoRA to large language models and vision tasks as a promising direction for future work, noting that replacing ReLU activations with smooth alternatives may enable application in these contexts.

## Limitations
- The method requires twice-differentiable activation functions, limiting direct application to architectures with ReLU without modification
- Computational overhead from Hessian calculation, though manageable due to O(L²) scaling with layers rather than parameters
- All experiments are confined to physics-informed neural networks solving PDEs, limiting generalizability to broader machine learning tasks

## Confidence
- **High Confidence:** The submodularity framework and greedy algorithm approximation guarantees (Theorem 2, Lemma 1) are mathematically rigorous and well-established in the optimization literature
- **Medium Confidence:** The experimental results on PINNs demonstrate clear performance improvements over first-order methods, but the evaluation is limited to a narrow set of physics-informed problems with known solutions
- **Low Confidence:** Claims about robustness to overestimated rank budgets and the general applicability of alternating optimization require broader empirical validation across different model architectures and task types

## Next Checks
1. **Architecture Generalization Test:** Apply SubLoRA to standard vision or language tasks (e.g., fine-tuning ResNet on CIFAR-10 or BERT on GLUE) to verify performance gains extend beyond PINNs
2. **Projection Impact Analysis:** Systematically vary the fraction of off-diagonal Hessian terms that violate the submodularity condition and measure the resulting performance degradation compared to full Hessian usage
3. **Convergence Robustness Sweep:** Test alternating optimization with increasingly aggressive rank budgets (b ≪ original rank) across multiple random seeds to quantify failure rates and identify stability thresholds