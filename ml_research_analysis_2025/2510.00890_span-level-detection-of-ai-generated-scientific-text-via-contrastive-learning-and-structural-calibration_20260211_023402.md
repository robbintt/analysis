---
ver: rpa2
title: Span-level Detection of AI-generated Scientific Text via Contrastive Learning
  and Structural Calibration
arxiv_id: '2510.00890'
source_url: https://arxiv.org/abs/2510.00890
tags:
- detection
- text
- across
- span-level
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting AI-generated scientific
  text at the span level, aiming to improve localization accuracy and robustness compared
  to existing document-level methods. The authors propose Sci-SpanDet, a structure-aware
  framework that combines section-conditioned stylistic modeling with multi-level
  contrastive learning to capture nuanced human-AI differences while mitigating topic
  dependence.
---

# Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration

## Quick Facts
- **arXiv ID**: 2510.00890
- **Source URL**: https://arxiv.org/abs/2510.00890
- **Reference count**: 40
- **Primary result**: Span-level detection framework achieving F1(AI) of 80.17 and AUROC of 92.63 on 100K annotated samples

## Executive Summary
This paper addresses the challenge of detecting AI-generated scientific text at the span level, significantly improving localization accuracy and robustness compared to document-level methods. The authors propose Sci-SpanDet, a structure-aware framework that combines section-conditioned stylistic modeling with multi-level contrastive learning to capture nuanced human-AI differences while mitigating topic dependence. The framework integrates BIO-CRF sequence labeling with pointer-based boundary decoding and confidence calibration to enable precise span-level detection and reliable probability estimates. Evaluated on a cross-disciplinary dataset of 100,000 annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek, LLaMA), the model demonstrates state-of-the-art performance with F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36, while showing strong resilience under adversarial rewriting.

## Method Summary
Sci-SpanDet is a span-level detection framework that combines section-conditioned stylistic modeling with multi-level contrastive learning. The architecture uses SciBERT as a backbone, with a Graph Attention Network (GAT) to encode paragraph representations conditioned on section information and document structure. The model employs BIO-CRF sequence labeling for tag sequence consistency and pointer networks for precise boundary detection. Training involves a joint loss function combining CRF loss, pointer loss, instance-level contrastive loss, and cluster-level prototype alignment. Temperature scaling is applied for confidence calibration to ensure reliable probability estimates. The model is evaluated on a newly constructed cross-disciplinary dataset with 100,000 annotated samples generated by multiple LLM families.

## Key Results
- Achieved F1(AI) of 80.17 and AUROC of 92.63 on cross-disciplinary dataset
- Span-F1 of 74.36 demonstrating precise localization capability
- Strong resilience under adversarial rewriting with maintained performance
- Balanced accuracy across IMRaD sections and diverse scientific disciplines
- Low Expected Calibration Error (0.06) indicating reliable probability estimates

## Why This Works (Mechanism)

### Mechanism 1: Section-Conditioned Contrastive Style Separation
The model treats document sections (IMRaD) as distinct stylistic clusters, allowing separation of human/AI authorship signals from topic-specific vocabulary. A Graph Attention Network propagates context across paragraphs, using multi-level contrastive learning (InfoNCE + Prototype alignment) to pull same-source/same-section embeddings together while pushing different sources apart within the same section. This captures consistent style differences across disciplines.

### Mechanism 2: Joint BIO-CRF and Pointer Decoding
The framework combines sequence labeling with boundary detection to improve span localization. BIO-CRF ensures tag transition validity and captures internal span consistency, while pointer networks predict start/end probabilities. A joint scoring function merges CRF sequence likelihood with pointer boundary confidence to filter candidates and improve localization accuracy.

### Mechanism 3: Boundary Confidence Calibration
Raw model confidence scores are often over-confident, so temperature scaling is used to align predicted probabilities with actual accuracy. A boundary confidence scorer aggregates CRF and pointer signals, and a temperature parameter learned on validation data scales the logits before the final sigmoid, enabling reliable thresholding for deployment.

## Foundational Learning

- **Concept: Conditional Random Fields (CRF)**
  - **Why needed here**: Standard classifiers tag tokens independently, leading to illegal sequences. The CRF layer is required to model the dependency between adjacent tags in a span.
  - **Quick check question**: Why would a standard Softmax layer fail to enforce the rule that an "Inside" tag must follow a "Begin" tag?

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here**: The model must learn a representation space where "AI-ness" is clustered separately from "Human-ness," even if topics differ. Contrastive loss explicitly shapes this geometry.
  - **Quick check question**: In this context, what defines a "positive" pair vs. a "negative" pair during training? (Answer: Same source & section vs. Different source).

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here**: A high F1 score is insufficient for deployment if probability scores are untrustworthy. ECE measures the gap between confidence and accuracy.
  - **Quick check question**: If a model predicts "AI" with 90% confidence but is only correct 60% of the time, is it well-calibrated?

## Architecture Onboarding

- **Component map**: SciBERT (Token/Paragraph embedding) -> Graph Attention Network (GAT) -> Contrastive Head (InfoNCE + Prototype) -> Localization Heads (BIO-CRF + Pointer) -> Calibration (Temperature Scaling)

- **Critical path**: The fusion of section context ($h_{sec}$) into the paragraph embedding ($h_{para}$) is the most critical step. If section information is noisy or graph attention fails to propagate context, the contrastive learning objective will optimize for topic noise rather than style.

- **Design tradeoffs**: The model explicitly relies on IMRaD structure and struggles if section labels are missing or incorrect. The dual-head (CRF+Pointer) architecture is heavier than a simple classifier but necessary for span-level output.

- **Failure signatures**: 
  - High ECE despite high F1 indicates calibration temperature was fit on non-representative validation set
  - Oscillating predictions within a paragraph suggest GraphEnc weight is too low or CRF transitions are under-trained

- **First 3 experiments**:
  1. Context Ablation: Set $\omega_{ctx} = 0$ to verify marginal gain from Graph structure vs. single-paragraph classification
  2. Calibration Drift: Train on GPT-data, calibrate on GPT-validation, test on LLaMA-data to observe calibration degradation across generators
  3. Visualizing Separability: t-SNE plot of embeddings colored by "Section" vs. "Source" to ensure clusters form around Authorship rather than Topic

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How robust is Sci-SpanDet when processing scientific documents that lack explicit section headers or deviate from standard IMRaD structures?
- **Basis in paper**: The Conclusion states the framework "relies on accurate section segmentation," implying performance may degrade with structural errors.
- **Why unresolved**: The model conditions stylistic modeling on IMRaD sections; it is unclear how the graph encoder handles missing or noisy structural nodes.
- **What evidence would resolve it**: Evaluation results on scientific preprints or non-standard formats where section boundaries are obfuscated or removed.

### Open Question 2
- **Question**: Can the framework be extended to accurately attribute AI-generated spans to specific source models (e.g., GPT vs. LLaMA) rather than just binary classification?
- **Basis in paper**: The Conclusion lists "identifying the source generator model" as a specific avenue for future work.
- **Why unresolved**: While the model distinguishes human vs. AI via contrastive learning, the shared "AI" cluster may obscure generator-specific stylistic fingerprints.
- **What evidence would resolve it**: A multi-class extension showing high accuracy in distinguishing between different LLM families within detected spans.

### Open Question 3
- **Question**: How does the section-conditioned stylistic modeling generalize to non-English scientific texts?
- **Basis in paper**: The Conclusion proposes extending the framework to "cross-lingual... data."
- **Why unresolved**: Stylistic cues and function words used for contrastive learning are language-specific; the current SciBERT backbone is English-centric.
- **What evidence would resolve it**: Performance metrics of the model applied to scholarly corpora in languages like Chinese or German without retraining.

## Limitations
- The framework's performance is highly dependent on the accuracy of section classification and the presence of standard IMRaD section headers
- The calibration approach relies on a single temperature parameter that may not generalize well to entirely unseen LLM families
- The model's complexity (dual-head architecture with GAT) makes it computationally heavier than simpler document-level classifiers

## Confidence
- **High Confidence**: The overall improvement in F1(AI) (80.17) and AUROC (92.63) over existing document-level methods is well-supported by experimental results
- **Medium Confidence**: The claim of "strong resilience under adversarial rewriting" is supported but requires reference to supplementary materials for full details
- **Medium Confidence**: The assertion of "balanced accuracy across IMRaD sections" is supported but exact methodology for ensuring balanced sampling is not explicitly stated

## Next Checks
1. **Cross-Generator Calibration Stability**: Retrain on GPT-generated data, calibrate on GPT validation set, evaluate calibration (ECE) on LLaMA test set to test temperature scaling robustness to generator distribution shifts

2. **Section Header Ablation**: Evaluate performance on dataset where standard IMRaD section headers are removed or replaced with non-standard labels to quantify dependence on explicit section information

3. **Robustness to Mixed Authorship Granularity**: Test ability to detect AI-generated spans in documents with varying levels of mixed authorship (e.g., paragraphs with both human and AI sentences) to assess performance beyond binary span classification