---
ver: rpa2
title: Fooling Algorithms in Non-Stationary Bandits using Belief Inertia
arxiv_id: '2511.05620'
source_url: https://arxiv.org/abs/2511.05620
tags:
- regret
- algorithm
- algorithms
- non-stationary
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new method, called belief inertia, for\
  \ deriving worst-case lower bounds in non-stationary multi-armed bandits. Unlike\
  \ prior infrequent sampling arguments, the approach exploits how algorithms\u2019\
  \ empirical beliefs\u2014encoded via historical reward averages\u2014resist updating\
  \ even after environment changes."
---

# Fooling Algorithms in Non-Stationary Bandits using Belief Inertia

## Quick Facts
- **arXiv ID:** 2511.05620
- **Source URL:** https://arxiv.org/abs/2511.05620
- **Reference count:** 3
- **Primary result:** Introduces belief inertia framework to prove worst-case linear regret for classical bandit algorithms in piecewise-stationary settings

## Executive Summary
This paper introduces a novel belief inertia framework for proving worst-case lower bounds in non-stationary multi-armed bandits. Unlike prior approaches relying on infrequent sampling, this method exploits how algorithms' empirical beliefs resist updating after environment changes. The paper demonstrates that classical algorithms (Explore-Then-Commit, ε-greedy, UCB) can be made to incur linear regret with substantial constant factors, even with a single change point. The analysis also shows that periodically restarted versions of these algorithms still suffer linear worst-case regret when changes are frequent.

## Method Summary
The paper constructs adversarial instances that exploit how cumulative empirical averages create momentum resisting new evidence after distribution changes. For UCB algorithms, the confidence radius shrinks as an arm is sampled, locking in preferences even after changes. The analysis proves that ETC algorithms incur at least (1-1/K)T regret, ε-greedy at least T/8, and UCB at least 0.07(1-1/K)T. The framework also quantifies the cost of periodic restarts, showing they incur at least a √d-fold increase in regret in worst cases.

## Key Results
- Belief inertia causes classical MAB algorithms to suffer linear regret in piecewise-stationary environments
- UCB's confidence radius trap locks algorithms into suboptimal arms post-change
- Periodic restarts incur √d penalty in stationary phases and fail when change frequency exceeds restart frequency
- The framework provides sharp lower bounds for worst-case regret in non-stationary settings

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Belief Inertia
- **Claim:** Cumulative empirical averages create momentum that resists adaptation to new reward distributions
- **Mechanism:** Large historical sample counts make new rewards barely affect the average, keeping suboptimal arms' means artificially high after changes
- **Core assumption:** Algorithms use non-discounted statistics without explicit forgetting mechanisms
- **Evidence anchors:** Abstract states belief inertia captures how "historical reward averages create momentum that resists new evidence"; Section 1 explains conviction overturning requires many additional observations
- **Break condition:** Fails if algorithm resets statistics at distribution change or uses aggressive sliding windows

### Mechanism 2: The Confidence Radius Trap (UCB Specific)
- **Claim:** UCB's confidence bonus shrinks with sampling, locking in preferences for historically optimal arms
- **Mechanism:** Before change, frequent sampling makes confidence radius small; after change, this arm's index remains stable while new optimal arm has high confidence bonus but needs exploration
- **Core assumption:** UCB doesn't know horizon is dynamic or adapt confidence bounds to drift
- **Evidence anchors:** Section 5.3 explains index remains artificially high; Figure 1 shows UCB index of suboptimal arm B remaining higher than optimal arm A for >500 rounds post-change
- **Break condition:** Fails if confidence bonus is volatility-scaled or algorithm is restarted

### Mechanism 3: The Restart Cost Paradox
- **Claim:** Periodic restarts impose √d regret penalty in stationary phases and fail if restart frequency mismatches true breakpoint frequency
- **Mechanism:** Restarting every T/d rounds multiplies regret by √d in stationary settings; if changes exceed d, regret remains linear within changing segments
- **Core assumption:** Restart policy is blind (time-based) rather than adaptive
- **Evidence anchors:** Section 6 Theorem 4 quantifies √d increase; Theorem 5 shows linear regret when ΓT > d
- **Break condition:** Mitigated only with oracle knowledge of true volatility or adaptive detection

## Foundational Learning

- **Concept: Piecewise-Stationary Bandits**
  - **Why needed here:** This is the environment model used, allowing distinct stable phases separated by sudden breakpoints
  - **Quick check question:** If a reward distribution changes at t=100 and stays fixed until T=1000, is this an adversarial, stationary, or piecewise-stationary setting?

- **Concept: Regret (Instantaneous vs. Cumulative)**
  - **Why needed here:** The paper proves "linear regret" (Ω(T)); efficient algorithms typically achieve O(√T) or O(ln T) regret
  - **Quick check question:** If an algorithm chooses a suboptimal arm with constant gap Δ for 50% of total rounds T, what is the order of its cumulative regret?

- **Concept: Exploration-Exploitation Trade-off**
  - **Why needed here:** "Belief Inertia" is an exploitation trap where algorithms become over-confident based on stale data and stop exploring
  - **Quick check question:** In ε-greedy, what parameter controls the trade-off between exploring new arms and exploiting the best-known arm?

## Architecture Onboarding

- **Component map:** Instance Generator -> Agent (ETC/ε-greedy/UCB) -> State Buffer (counts and means) -> Regret Tracker

- **Critical path:** Vulnerability triggered when State Buffer accumulates large counts before Instance Generator triggers breakpoint; high counts dampen update signal, causing Agent to persist with old optimal arm

- **Design tradeoffs:**
  - Forgetting Factor vs. Stability: Adding forgetting factor γ helps but destroys stability for low regret in stationary phases
  - Tuning ε (in ε-greedy): High ε detects changes faster but hurts stationary performance; low ε improves stationary performance but increases susceptibility to early two-arm switch trap

- **Failure signatures:**
  - UCB: "Index Crossover" failure where suboptimal arm's index doesn't drop below optimal arm post-change
  - ETC: "Blind Commitment" where algorithm pulls single arm repeatedly despite clear reward shifts
  - Restarting: "Segmentation Drag" showing distinct hops every T/d rounds due to re-exploration

- **First 3 experiments:**
  1. Replicate UCB Trap: Implement 2-arm UCB with rewards (0,0)→(1,0.1) at t=200, verify arm 1 rarely selected post-breakpoint
  2. Sensitivity to ε: Implement ε-greedy with Instance 1 (1,0)→(0,1), plot regret vs. ε to verify V-shaped trade-off
  3. Cost of Restarting: Implement UCB with periodic restarts on stationary instance, compare regret against standard UCB to verify √d penalty

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the belief inertia framework be extended to derive sharp lower bounds for algorithms specifically designed for non-stationarity?
- **Basis in paper:** Authors state "A natural next step is to examine whether the same argument can be extended to algorithms specifically designed for non-stationarity"
- **Why unresolved:** Paper analyzes classical algorithms but excludes adaptive methods like Sliding-Window UCB or Discounted UCB
- **What evidence would resolve it:** Formal proofs showing whether adaptive algorithms suffer similar linear regret or if active forgetting breaks inertia

### Open Question 2
- **Question:** Does belief inertia persist in active forgetting algorithms that rely on sliding windows or discounting?
- **Basis in paper:** Authors suggest "Our framework may be able to formalize this trade-off by showing that even with active forgetting, belief inertia persists whenever past data retain enough influence"
- **Why unresolved:** Unproven whether momentum of empirical averages survives mechanisms designed to discard outdated data
- **What evidence would resolve it:** Analysis of how window size or discount factors balance variance (responsiveness) and anchoring (inertia)

### Open Question 3
- **Question:** What is the minimax lower bound for algorithms in the broader variation budget setting?
- **Basis in paper:** Paper focuses on piecewise-stationary models noting related work on "drifting bandit frameworks" but not analyzing them
- **Why unresolved:** Belief inertia construction relies on sudden distribution changes; effect of gradual drift remains unquantified
- **What evidence would resolve it:** Adversarial instances constructed for continuous variation budgets rather than discrete change points

## Limitations
- The belief inertia argument relies on exact knowledge of breakpoints for optimal adversarial placement
- Analysis assumes cumulative, non-discounted statistics without empirical validation on real-world non-stationary datasets
- Restart cost analysis assumes blind time-based resets rather than adaptive detection methods

## Confidence
- **High Confidence:** Mechanism of belief inertia creating linear regret for ETC and ε-greedy is well-established in bandit literature with mathematically sound formalization
- **Medium Confidence:** UCB-specific confidence radius trap is rigorously proven but exact numerical bounds depend on parameter choices within intervals
- **Medium Confidence:** Restart cost analysis correctly identifies √d penalty but worst-case linear regret assumes adversarial construction that may be overly pessimistic

## Next Checks
1. Implement belief inertia instances on synthetic data with varying change patterns and measure actual regret convergence rates versus theoretical predictions
2. Systematically vary the confidence parameter c in UCB construction across valid interval to quantify impact on regret bounds and identify optimal choices
3. Implement adaptive restart algorithm (e.g., CUSUM change-point detection) and compare regret against blind restarts to validate theoretical trade-off