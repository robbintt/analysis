---
ver: rpa2
title: Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient
  Reweighting with Similar Labels
arxiv_id: '2503.13917'
source_url: https://arxiv.org/abs/2503.13917
tags:
- data
- unlearning
- q-mul
- quantized
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of implementing machine unlearning
  in quantized neural networks under data privacy regulations. The core method, Q-MUL,
  introduces two key innovations: Similar Labels that replace random labels with semantically
  consistent alternatives to minimize noise injection during data processing, and
  Adaptive Gradient Reweighting that dynamically balances parameter update contributions
  from forgotten and retained data during training.'
---

# Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels

## Quick Facts
- arXiv ID: 2503.13917
- Source URL: https://arxiv.org/abs/2503.13917
- Reference count: 40
- Primary result: Q-MUL achieves only 3.11% average performance gap on CIFAR-100 vs 6.05%+ for baseline methods

## Executive Summary
This paper addresses the challenge of implementing machine unlearning in quantized neural networks under data privacy regulations. The core method, Q-MUL, introduces two key innovations: Similar Labels that replace random labels with semantically consistent alternatives to minimize noise injection during data processing, and Adaptive Gradient Reweighting that dynamically balances parameter update contributions from forgotten and retained data during training. Extensive experiments on benchmark datasets demonstrate Q-MUL's superiority, achieving an average performance gap of only 3.11% compared to retraining on CIFAR-100, significantly outperforming existing methods which showed gaps of 6.05% or higher.

## Method Summary
Q-MUL operates in two stages: (1) Similar Labels module replaces forgotten sample labels with semantically similar alternatives by finding the class with probability closest to the true label probability, and (2) Adaptive Gradient Reweighting module computes gradient norms for forgotten and retained data before each epoch, then applies weighted loss to balance their contributions during SGD updates. The method is designed specifically for quantized models, addressing noise amplification and gradient imbalance issues that plague existing unlearning approaches in low-bit parameter spaces.

## Key Results
- Q-MUL achieves only 3.11% average performance gap on CIFAR-100 vs 6.05%+ for baseline methods
- Similar Labels alone reduces average gap from 6.05% to 3.52% on CIFAR-100
- Adaptive Gradient Reweighting alone reduces average gap from 6.05% to 7.90% on CIFAR-100
- Q-MUL outperforms existing methods across multiple datasets (CIFAR-10, CIFAR-100, SVHN, Tiny-ImageNet)

## Why This Works (Mechanism)

### Mechanism 1: Similar Labels for Noise Reduction
- Claim: Using semantically similar labels instead of random labels reduces noise injection during unlearning in quantized models.
- Mechanism: For each forgotten sample, calculate distance between each class's predicted probability and true label probability, then select class with minimum distance as new label.
- Core assumption: Similar labels maintain gradient direction consistency with true labels, avoiding gradient conflicts that random labels introduce.
- Evidence anchors: Abstract states "Similar Labels assignment replaces random labels with semantically consistent alternatives to minimize noise injection"; Section 4.2 notes random labels "introduce a large amount of noise inconsistent with the original data".

### Mechanism 2: Adaptive Gradient Reweighting
- Claim: Dynamically weighting losses based on gradient norms balances contributions from forgotten and retained data during parameter updates.
- Mechanism: Compute expected gradient norms Gf and Gr for forgotten and retained data, set weights αf = Gr/(Gf+Gr), αr = Gf/(Gf+Gr) to create complementary weighting.
- Core assumption: Gradient norm imbalance (observed ratio ~2.5-4.0x) causes uneven parameter update contributions that degrade unlearning quality.
- Evidence anchors: Abstract states "Adaptive Gradient Reweighting dynamically aligns parameter update contributions from forgotten and retained data"; Section 4.3 describes AGR "adaptively adjusts the contributions of forgotten data and retained data to model updates to achieve balance".

### Mechanism 3: Quantization-Specific Vulnerability Exploitation
- Claim: Quantized models exhibit amplified sensitivity to noise and gradient imbalance, making specialized unlearning mechanisms more impactful than in full-precision models.
- Mechanism: Constrained parameter representation space (e.g., 2-4 bits) limits model's ability to absorb perturbations; discrete optimization via STE creates gradient dynamics where imbalances are magnified.
- Core assumption: Performance degradation gap between quantized and full-precision models under standard MU methods is causal, not correlative.
- Evidence anchors: Abstract notes "These issues are exacerbated by quantized models' constrained parameter space and discrete optimization"; Figure 1 shows gradient norm ratio for quantized model (~3.5) exceeds full-precision (~2.75).

## Foundational Learning

- Concept: **Quantization-Aware Training (QAT)**
  - Why needed here: Q-MUL operates during QAT-style unlearning where fake quantization nodes simulate quantization error and STE enables gradient flow through discrete operations.
  - Quick check question: Can you explain why the backward pass uses STE (gradients flow unchanged within clamping range) while forward uses actual quantization?

- Concept: **Exact vs Approximate Machine Unlearning**
  - Why needed here: Q-MUL is an approximate unlearning method that trades theoretical guarantees for computational efficiency; understanding this tradeoff is essential for deployment decisions.
  - Quick check question: What is the "golden standard" the paper compares against, and what does the "Average Gap" metric measure?

- Concept: **Cross-Entropy Loss Gradient Dynamics**
  - Why needed here: The AGR mechanism depends on understanding how ∇_w L changes with different labels and why gradient magnitude affects update stability.
  - Quick check question: Why does the paper use cosine similarity (Eq. 9) to compare gradient directions, and what does cos θ_sl ≈ 1 imply?

## Architecture Onboarding

- Component map:
  Data Processing Stage: Similar Labels module (Algorithm 1) → Unlearning Training Stage: AGR module → Quantization Layer: LSQ+/PACT/DSQ

- Critical path:
  1. Original quantized model M_0 with weights w_0
  2. Forward pass forgotten data → probability distribution p_Q(x_i; w)
  3. For each forgotten sample: find argmin_{k≠y_i} |p_Q(k|x_i) - p_Q(y_i|x_i)|
  4. Assemble D' = D'_r ∪ D'_f with modified labels for D_f only
  5. Per epoch: compute G_f, G_r → calculate α_f, α_r → SGD update: w_{t+1} = w_t - η_t(α_f∇L_f + α_r∇L_r)

- Design tradeoffs:
  - SL vs RL: Less noise injection but potentially slower "forgetting signal"; SL preserves more semantic structure
  - AGR overhead: Requires computing gradients on both datasets before each training epoch—~2x forward/backward passes per epoch vs 1x without AGR
  - Bit-width selection: Lower bits (2) make Q-MUL more critical; higher bits (8+) reduce need for specialized MU

- Failure signatures:
  - FA stays high + RA drops significantly → Check AGR weight calculation; α_f and α_r may be inverted
  - All metrics degrade uniformly → Similar Label selection may be failing; verify model predictions are not uniformly distributed
  - MIA remains near 100% → Forgetting incomplete; increase epochs or check learning rate range [1e-3, 1e-1]

- First 3 experiments:
  1. Baseline validation: Replicate Figure 1—run Random Labels on full-precision vs 4-bit quantized ResNet18 with CIFAR-10 (10% random forgetting). Confirm gradient norm ratio difference and ~1.92% vs ~1.05% AG gap.
  2. Component ablation: Run Q-MUL variants on CIFAR-100 per Table 2—compare full Q-MUL vs w/o SL vs w/o AGR. Verify RA gap increases from 2.09% to 3.52% (without SL) and 7.90% (without AGR).
  3. Cross-QAT validation: Test Q-MUL across PACT, DSQ, and LSQ+ quantization methods on 4-bit ResNet18 with CIFAR-100 (10% forgetting). Target AG ≤ 8.26% as shown in Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Q-MUL framework be effectively transferred to Large Language Models (LLMs) and Transformer architectures?
- Basis in paper: [Explicit] The paper evaluates only CNNs (ResNet, MobileNet) on computer vision tasks. While the Introduction acknowledges the success of DNNs in NLP, the methodology and experiments are confined to vision datasets (CIFAR, SVHN, Tiny-ImageNet).
- Why unresolved: The "Similar Labels" mechanism relies on output probability distributions which differ structurally between classification logits and generative language modeling.
- What evidence would resolve it: Application of Q-MUL to quantized Transformers (e.g., LLaMA, BERT) demonstrating that semantically consistent label replacement effectively unlearns text data without catastrophic degradation of language reasoning.

### Open Question 2
- Question: Can the computational overhead of Adaptive Gradient Reweighting (AGR) be reduced to match the speed of simpler unlearning methods?
- Basis in paper: [Explicit] The efficiency analysis in Appendix E admits that Q-MUL has "slightly larger time overhead compared to methods like SalUn and RL" because it requires calculating gradient norms for both forgotten and retained datasets before updating weights.
- Why unresolved: Calculating norms for the retained set adds a significant pass over the data, which may be prohibitive for extremely large datasets or resource-constrained edge devices.
- What evidence would resolve it: Development of an approximation technique (e.g., sampling-based gradient estimation) that maintains the balance of contributions without computing full dataset gradients, achieving time parity with baseline methods.

### Open Question 3
- Question: How robust is the "Similar Labels" mechanism when the pre-trained model exhibits low predictive confidence?
- Basis in paper: [Inferred] Equation 4 selects the label with the probability closest to the ground truth. If the model is poorly trained or the data is ambiguous, the "closest" probability may still be low, effectively introducing a label that is only marginally better than random noise.
- Why unresolved: The paper assumes the model has sufficient discriminative power to generate meaningful probability distances, but this assumption is not verified for low-accuracy scenarios.
- What evidence would resolve it: Experiments evaluating Q-MUL on models with deliberately reduced accuracy (e.g., under-trained or heavily pruned networks) to test if the Similar Label strategy fails when the probability signal is weak.

### Open Question 4
- Question: Can formal differential privacy guarantees be established for the Q-MUL mechanism?
- Basis in paper: [Inferred] The paper evaluates privacy compliance using empirical Membership Inference Attacks (MIA) but does not provide a theoretical analysis or bounds regarding privacy loss (ε) for the gradient reweighting process.
- Why unresolved: Empirical MIA success rates do not constitute a formal proof of privacy, which is often required for strict regulatory compliance (e.g., GDPR).
- What evidence would resolve it: A theoretical proof showing the upper bound of information leakage for the Q-MUL update step, specifically accounting for the discrete nature of quantized parameters.

## Limitations

- The paper's privacy claims rely on empirical MIA evaluation rather than formal differential privacy guarantees, which may be insufficient for strict regulatory compliance
- Computational overhead of AGR (requiring two gradient computations per epoch) may be prohibitive for large-scale or resource-constrained applications
- Similar Labels mechanism effectiveness depends on model having sufficient predictive confidence; performance in low-confidence scenarios is not validated

## Confidence

- **High**: Q-MUL outperforms baseline unlearning methods on CIFAR-100 (Table 2, AG = 3.11% vs 6.05%+ for baselines)
- **Medium**: Similar Labels mechanism reduces noise injection (mechanism plausible but limited empirical validation of semantic consistency)
- **Medium**: Adaptive Gradient Reweighting improves unlearning quality (supported by ablation but gradient norm analysis is correlational)

## Next Checks

1. **Cross-dataset generalization**: Apply Q-MUL to SVHN and Tiny-ImageNet to verify performance gains extend beyond CIFAR datasets
2. **Bit-width sensitivity analysis**: Test Q-MUL effectiveness across 2, 4, and 8-bit quantization to validate the claimed quantization-specific advantages
3. **Gradient norm correlation study**: Systematically measure the relationship between gradient norm ratios, α_f/α_r weights, and actual parameter update contributions to confirm AGR's balancing effect