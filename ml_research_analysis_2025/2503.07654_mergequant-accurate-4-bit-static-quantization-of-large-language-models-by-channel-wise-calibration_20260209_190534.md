---
ver: rpa2
title: 'MergeQuant: Accurate 4-bit Static Quantization of Large Language Models by
  Channel-wise Calibration'
arxiv_id: '2503.07654'
source_url: https://arxiv.org/abs/2503.07654
tags:
- quantization
- layers
- proj
- value
- mergequant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MergeQuant addresses the challenge of accurate 4-bit static quantization
  for large language models (LLMs) by introducing a per-channel quantization framework
  that eliminates the overhead of repeated quantization steps during autoregressive
  generation. The core method, Quantization Step Migration (QSM), integrates quantization
  scaling factors with model parameters like RMSNorm and linear layers, enabling efficient
  alignment with integer acceleration kernels.
---

# MergeQuant: Accurate 4-bit Static Quantization of Large Language Models by Channel-wise Calibration

## Quick Facts
- arXiv ID: 2503.07654
- Source URL: https://arxiv.org/abs/2503.07654
- Reference count: 40
- Achieves up to 1.77× speedup in decoding and 2.06× speedup in end-to-end inference on Llama-2-7B models

## Executive Summary
MergeQuant introduces a novel per-channel quantization framework that eliminates the overhead of repeated quantization steps during autoregressive generation in LLMs. The core innovation, Quantization Step Migration (QSM), algebraically integrates quantization scaling factors with model parameters like RMSNorm and linear layers, enabling efficient alignment with integer acceleration kernels. By combining dimension reconstruction and adaptive clipping to address non-uniform quantization scale factors across channels, along with learnable quantization compensation parameters, MergeQuant achieves up to 1.77× speedup in decoding while reducing the accuracy gap to FP16 baseline to just 1.3 points on zero-shot tasks for Llama-2-70B.

## Method Summary
MergeQuant is a 4-bit static quantization framework for LLMs that eliminates quantization overhead through three core mechanisms. First, Quantization Step Migration (QSM) algebraically merges per-channel activation scaling factors into adjacent model parameters (RMSNorm and linear weights), enabling purely integer-based computation compatible with INT4 kernels. Second, dimension reconstruction identifies channels with extreme scaling factors (outliers) and splits them while pruning less important neighbor channels using Hessian sensitivity analysis to maintain tensor dimensions. Third, learnable quantization compensation parameters (LoRA) are trained to minimize reconstruction error between original FP16 outputs and quantized outputs. The method uses 32 sentences for calibration and 256 sentences for LoRA training, achieving significant speedups while maintaining accuracy close to FP16 baselines.

## Key Results
- Achieves 1.77× speedup in decoding and 2.06× speedup in end-to-end inference on Llama-2-7B models
- Reduces accuracy gap to FP16 baseline to 1.3 points on zero-shot tasks for Llama-2-70B
- Eliminates quantization overhead through algebraic integration of scaling factors with model parameters
- Demonstrates effective handling of structured outliers through dimension reconstruction

## Why This Works (Mechanism)

### Mechanism 1: Quantization Step Migration (QSM)
The framework identifies that for a linear layer $Y=XW$, per-channel activation scaling factors ($s_{\tilde{X}}$) can be pre-computed and algebraically merged into RMSNorm parameters ($\gamma/s_{\tilde{X}}$) and weight matrix ($W \cdot s_{\tilde{X}}$). This converts the operation into purely integer-based computation with a single external scaling, compatible with INT4 kernels. The core assumption is that floating-point RMSNorm has sufficient precision headroom to absorb quantization scaling division without overflow.

### Mechanism 2: Dimension Reconstruction
Channels with extreme scaling factors (outliers) are split and reconstructed to reduce non-uniformity of parameter distribution. The paper identifies "strong" channels where scales exceed threshold $T$ and splits these channels to cap their scale at $T$. To maintain tensor dimensions, it prunes "neighbor" channels deemed less important via Hessian-based sensitivity analysis. The core assumption is that channels adjacent to outliers carry less critical information and can be pruned to offset dimension increase.

### Mechanism 3: Learnable Quantization Compensation
Low-Rank Adaptation (LoRA) recovers information loss induced by static quantization and dimension pruning without full-model retraining. Learnable low-rank matrices $A$ and $B$ are trained to minimize reconstruction error between original FP16 block output and quantized block output, effectively "learning" to invert quantization noise. The core assumption is that quantization error resides in a low-dimensional subspace recoverable by low-rank matrices.

## Foundational Learning

- **Concept: Static vs. Dynamic Quantization**
  - Why needed: MergeQuant argues dynamic quantization (calculating scales per token) is too slow for long-sequence autoregressive decoding
  - Quick check: Why does per-token dynamic calibration become a bottleneck specifically during decoding phase rather than prefill phase?

- **Concept: Integer Kernel Constraints**
  - Why needed: Standard INT4 GEMM kernels expect single global scale or simple vector scales applied after dot product
  - Quick check: Why can't standard integer kernels handle per-channel input scaling efficiently inside accumulation loop?

- **Concept: Outlier Dominance in LLMs**
  - Why needed: LLM activations contain "structured outliers" that stretch quantization range, crushing small values into zero
  - Quick check: If a single channel has value 100 while others are around 0.1, how does this affect quantization scale for per-tensor vs per-channel approach?

## Architecture Onboarding

- **Component map:** Input RMSNorm -> Dimension Reconstruction Layer -> INT4 Linear Layers -> Clipping Manager
- **Critical path:** The Dimension Reconstruction logic - if split/prune operation doesn't match modified weight matrix dimensions, model will crash
- **Design tradeoffs:** Speed vs. Accuracy (Hadamard) - without rotation it's faster but less accurate; Memory vs. Distribution - splitting outlier channels increases width requiring neighbor pruning
- **Failure signatures:** Perplexity Explosion (>100) from incorrect QSM math or extreme outlier scaling; Shape Mismatch Errors from misaligned dimension reconstruction; Slow Inference from dynamic reconstruction loops
- **First 3 experiments:** 1) Math Verification - implement QSM for single Linear layer in isolation, compare FP16 vs INT4 outputs; 2) Scale Distribution Analysis - visualize per-channel scaling factors for Llama-2-7B to confirm long tail distribution; 3) Ablation on Dimension Threshold - test different thresholds on WikiText-2 to find sweet spot between memory cost and perplexity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MergeQuant be extended to enable accurate fully static quantization for all linear layers, specifically including down-proj and out-proj layers that currently require dynamic calibration?
- **Basis:** Authors state in Section 5 that for activations "without structured outliers" (down-linear and out-linear layers), they apply per-token dynamic quantization to maintain model accuracy
- **Why unresolved:** Current framework relies on hybrid approach, leaving feasibility of purely static 4-bit quantization for output layers unresolved
- **What evidence would resolve it:** A static-only variant achieving comparable perplexity on down-proj and out-proj layers without reverting to per-token calibration

### Open Question 2
- **Question:** Is incorporation of Hadamard rotation strictly necessary for MergeQuant to maintain performance on smaller models (e.g., 7B parameters), or can core calibration be refined to remove this dependency?
- **Basis:** Table 1 reveals significant accuracy drop (approx. 8-9%) for "MergeQuant$_{n-h}$" (no rotation) variant compared to full method on Llama-2/3 models
- **Why unresolved:** Performance gap suggests core channel-wise calibration may be insufficient on its own, relying on rotation to smooth errors
- **What evidence would resolve it:** A methodological improvement that achieves accuracy comparable to rotation-inclusive baseline using only channel-wise calibration and dimension reconstruction

### Open Question 3
- **Question:** Does dimension reconstruction strategy generalize effectively to LLM architectures with different outlier distributions or normalization layers?
- **Basis:** Section 4.1 tailors QSM for RMSNorm, and Section 4.2 relies on specific "structured outlier" patterns to determine pruning thresholds
- **Why unresolved:** Method is evaluated exclusively on Llama family; transferability of Hessian-based pruning and splitting logic to other architectures is unverified
- **What evidence would resolve it:** Evaluation of MergeQuant on diverse architectures such as Mixture-of-Experts (MoE) or LayerNorm-based models

## Limitations
- Hessian-based sensitivity pruning lacks implementation details, making dimension reconstruction difficult to replicate faithfully
- LoRA compensation configuration (rank and learning rate) is unspecified, creating ambiguity in achieving reported accuracy gains
- Generalization across architectures remains untested beyond Llama-2 and Llama-3 family models

## Confidence
- **High Confidence**: QSM mechanism for eliminating quantization overhead is well-specified and mathematically sound
- **Medium Confidence**: Dimension reconstruction effectively addresses outlier-induced accuracy loss, but implementation details are incomplete
- **Medium Confidence**: Learnable compensation improves accuracy, though specific LoRA parameters are unspecified

## Next Checks
1. **Isolation Test**: Implement QSM on a single linear layer with controlled input distributions to verify mathematical equivalence between floating-point and integer computation paths
2. **Scale Distribution Analysis**: Measure per-channel scaling factors on Llama-2-7B to confirm the "long tail" outlier distribution that justifies dimension reconstruction
3. **Ablation Study**: Systematically vary the dimension reconstruction threshold (α) on WikiText-2 to quantify the trade-off between memory overhead and perplexity improvement