---
ver: rpa2
title: A Practical Approach to using Supervised Machine Learning Models to Classify
  Aviation Safety Occurrences
arxiv_id: '2504.09063'
source_url: https://arxiv.org/abs/2504.09063
tags:
- incident
- safety
- aviation
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a practical application of supervised machine
  learning to assist aviation safety investigators in classifying occurrences as either
  incidents or serious incidents. Using a dataset derived from 475 aviation investigation
  reports, five ML models were evaluated: Support Vector Machine, Logistic Regression,
  Random Forest Classifier, XGBoost, and K-Nearest Neighbors.'
---

# A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences

## Quick Facts
- arXiv ID: 2504.09063
- Source URL: https://arxiv.org/abs/2504.09063
- Reference count: 20
- Primary result: Random Forest Classifier achieved best performance (accuracy 0.77, F1 0.78, MCC 0.51) for classifying aviation occurrences

## Executive Summary
This study presents a practical application of supervised machine learning to assist aviation safety investigators in classifying occurrences as either incidents or serious incidents. Using a dataset derived from 475 aviation investigation reports, five ML models were evaluated: Support Vector Machine, Logistic Regression, Random Forest Classifier, XGBoost, and K-Nearest Neighbors. The Random Forest Classifier performed best with accuracy of 0.77, F1 Score of 0.78, and Matthews Correlation Coefficient of 0.51. The study also explored Synthetic Minority Over-sampling Technique (SMOTE) for handling the slightly imbalanced dataset, finding it did not improve performance and sometimes degraded results. The ML web application was benchmarked against human expert classification, showing comparable results with only minor differences in three of eight test cases.

## Method Summary
The research developed a web-based ML application to classify aviation safety occurrences using supervised learning algorithms. The methodology involved extracting text from 475 aviation investigation reports, preprocessing the data through cleaning and vectorization, and training five different classification models. The dataset was split into training (80%) and test (20%) sets, with performance evaluated using accuracy, F1 score, and Matthews Correlation Coefficient. The study also implemented SMOTE to address class imbalance and compared ML predictions against human expert classifications across eight test cases to validate practical utility.

## Key Results
- Random Forest Classifier achieved highest performance: accuracy 0.77, F1 score 0.78, MCC 0.51
- SMOTE did not improve model performance and sometimes reduced accuracy and F1 scores
- ML application matched human expert classification on 5 of 8 test cases
- Support Vector Machine performed worst with accuracy of 0.66 and F1 score of 0.69

## Why This Works (Mechanism)
The classification approach works by converting aviation occurrence narratives into numerical features through text vectorization, then applying ensemble learning techniques that can capture complex patterns in safety incident descriptions. Random Forest's success stems from its ability to handle non-linear relationships and reduce overfitting through ensemble averaging of multiple decision trees. The model's robustness to feature noise and ability to rank feature importance makes it particularly suitable for text classification tasks where certain words or phrases strongly indicate incident severity.

## Foundational Learning
1. **Text Vectorization** - Converting narrative text to numerical features using TF-IDF or similar methods; needed because ML models require numerical input rather than raw text
   * Quick check: Visualize feature importance to ensure key safety-related terms are captured

2. **Class Imbalance Handling** - Techniques like SMOTE to balance minority class samples; needed because imbalanced datasets can bias models toward majority class predictions
   * Quick check: Compare model performance with and without oversampling techniques

3. **Ensemble Learning** - Combining multiple models (Random Forest) to improve generalization and reduce overfitting; needed for handling complex, non-linear patterns in text data
   * Quick check: Analyze individual tree performance to ensure diversity in ensemble

4. **Performance Metrics** - Using multiple metrics (accuracy, F1, MCC) to evaluate classification; needed because different metrics capture different aspects of model performance
   * Quick check: Create confusion matrix to visualize prediction patterns

## Architecture Onboarding

**Component Map:**
Text Preprocessing -> Feature Extraction -> Model Training -> Prediction Service -> User Interface

**Critical Path:**
Raw text -> TF-IDF vectorization -> Random Forest training -> Classification API -> Web interface

**Design Tradeoffs:**
- Balanced accuracy vs. interpretability: Random Forest provides good performance but complex decision boundaries
- Training time vs. prediction speed: Ensemble methods require more training but fast predictions
- Data size vs. generalization: Small dataset limits model robustness but enables rapid iteration

**Failure Signatures:**
- High false positive rate indicates model overgeneralizing from limited training data
- Low recall suggests missing critical incident indicators in text features
- Performance degradation with SMOTE indicates sensitivity to synthetic sample quality

**3 First Experiments:**
1. Test feature importance ranking to identify which words most influence classification decisions
2. Evaluate model performance with different text preprocessing parameters (tokenization, stopword removal)
3. Compare binary vs. multi-class classification performance with additional severity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (475 reports) limits model generalizability and robustness
- Class imbalance ratio of 2.63:1 may affect model performance despite SMOTE attempts
- Australian aviation data focus may not translate to other regulatory environments

## Confidence
- Primary classification results: Medium (MCC 0.51 indicates room for improvement)
- Human expert comparison: Medium (small sample size of 8 test cases)
- SMOTE findings: Medium (warrants further investigation with different datasets)

## Next Checks
1. Test model performance across multiple aviation regulatory jurisdictions to assess generalizability
2. Conduct blind validation with independent human experts using a larger, diverse test set
3. Evaluate model performance with real-time streaming data to assess practical deployment scenarios