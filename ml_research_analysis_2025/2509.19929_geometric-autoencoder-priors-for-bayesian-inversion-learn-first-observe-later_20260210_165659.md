---
ver: rpa2
title: 'Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later'
arxiv_id: '2509.19929'
source_url: https://arxiv.org/abs/2509.19929
tags:
- learning
- bayesian
- prior
- inverse
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to learn geometry-aware priors for
  Bayesian inversion in engineering systems. The key idea is to use a geometric autoencoder
  to encode physical fields on varying geometries into a latent space, which serves
  as a highly informative prior for Bayesian inference.
---

# Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later

## Quick Facts
- arXiv ID: 2509.19929
- Source URL: https://arxiv.org/abs/2509.19929
- Reference count: 40
- Primary result: Geometry-aware autoencoder learns a latent prior that enables efficient Bayesian inversion across varying geometries with well-calibrated uncertainty.

## Executive Summary
This paper introduces Geometric Autoencoder Bayesian Inversion (GABI), a method to learn physics-informed priors for Bayesian inversion in engineering systems with varying geometries. The key innovation is decoupling learning from inference: a geometry-conditioned autoencoder is trained once on large datasets to encode physical fields into a shared latent space, creating a rich prior that captures the underlying physics. During inference, Approximate Bayesian Computation (ABC) efficiently samples from this learned prior using modern GPU parallelization, making the approach scalable to large problems. The method demonstrates strong predictive accuracy and calibrated uncertainty quantification across multiple physical setups including heat diffusion, fluid flow, and acoustic resonance.

## Method Summary
GABI learns a geometry-conditioned autoencoder that maps physical fields on varying geometries to a shared latent space. The encoder processes geometry and field data through graph neural networks to produce a fixed-dimensional latent vector, while the decoder reconstructs geometry-specific fields from this latent representation. The latent space is regularized to follow a standard normal distribution, creating a tractable prior. During inference, ABC sampling draws from this prior, decodes to physical fields, applies the observation operator, and accepts samples with residuals closest to the actual observations. This approach decouples training from the specific observation process, allowing the model to be trained once and used for any observation configuration.

## Key Results
- Predictive accuracy comparable to deterministic supervised learning methods across all tested physical systems
- Well-calibrated uncertainty quantification with ~80% of true values within 1 standard deviation
- ABC sampling achieves 400x speedup versus traditional NUTS MCMC (0.9s vs 410s for comparable accuracy)
- Demonstrates scalability to large problems with 5,000+ samples and complex terrain geometries
- Train-once-use-anywhere flexibility enables inference with varying observation locations and quantities

## Why This Works (Mechanism)

### Mechanism 1: Geometry-Conditioned Latent Space Unification
Physical fields on different geometries are mapped to a shared latent space through a geometry-conditioned autoencoder. The encoder takes both the solution field and geometry as input, outputting a fixed-dimensional latent vector. The decoder reconstructs geometry-specific fields from this shared latent representation. This creates a unified prior distribution across varying domains, assuming physical systems share latent structure despite geometric variation.

### Mechanism 2: Pushforward Posterior Equivalence
The decoder acts as a pushforward map that transforms samples from the latent posterior into the true Bayesian posterior. Lemma 2.1 proves that sampling from the latent posterior and decoding yields the same distribution as the true Bayesian posterior with the learned pushforward prior. This theoretical result enables efficient inference in the low-dimensional latent space rather than the high-dimensional physical space.

### Mechanism 3: ABC Sampling via GPU Parallelization
Approximate Bayesian Computation with rejection sampling efficiently approximates the posterior when the prior is highly informative. By sampling in the low-dimensional latent space, decoding to physical fields, and computing residuals in parallel across GPU cores, ABC achieves massive computational speedup. The well-tailored prior ensures high acceptance rates and efficient sampling.

## Foundational Learning

- **Bayesian Inversion Basics** - Why needed: The entire method relies on combining prior knowledge with observed data through Bayes' theorem. Quick check: Can you explain why inverse problems are ill-posed and how priors regularize them?

- **Autoencoders and Latent Priors** - Why needed: Understanding how compression to latent space creates a generative model via the decoder. Quick check: What is the difference between a VAE's per-sample KL regularization versus GABI's distributional MMD regularization?

- **Graph Neural Networks** - Why needed: Geometries are represented as computational meshes (graphs) processed through message passing. Quick check: How does message passing aggregate information from neighboring nodes?

## Architecture Onboarding

- **Component map:** Input coordinates and solution field → Graph with node features → GCN layers with non-local pooling → latent z ∈ R^{d_z} → Decoder GCN layers conditioned on (z, geometry) → Reconstructed field → ABC sampling → Posterior inference

- **Critical path:** 1) Prepare dataset as graphs with node coordinates and solution fields 2) Train autoencoder with reconstruction + MMD loss 3) Verify latent distribution approximates N(0,I) 4) At inference: specify observation locations, run ABC sampling

- **Design tradeoffs:** Higher latent dimension improves reconstruction but slows ABC convergence. GCN architecture sufficient for simple geometries; GEN GNN needed for complex edge features. ABC offers speed/parallelization versus NUTS' lower-variance estimates with small samples.

- **Failure signatures:** High reconstruction loss indicates physically implausible decoded fields. Latent histogram not Gaussian suggests prior mismatch and poor posterior coverage. ABC acceptance rate near zero indicates prior too diffuse or likelihood too narrow.

- **First 3 experiments:** 1) Replicate 2D heat problem: train on 1k rectangles, verify ~80% coverage within 1 std 2) Ablate latent dimension: sweep d_z ∈ {10, 50, 100, 200} and plot MAE vs inference time 3) Test observation generalization: train with 10 observations, test with 5, 20, 50, compare to Direct Map baseline

## Open Questions the Paper Calls Out

### Open Question 1
How should the latent space dimension be optimally selected for a given class of physical problems? The paper demonstrates sensitivity to this hyperparameter across dimensions 10-100 but offers no theoretical or heuristic guidance for new problem domains. A systematic ablation study across problem types with analysis of reconstruction error versus sampling efficiency trade-offs would be needed.

### Open Question 2
Can GABI priors trained on one physical domain transfer effectively to related but distinct physical phenomena? The paper describes the method as providing a "train-once-use-anywhere foundation model" but does not explore cross-domain transfer capabilities. Transfer learning experiments where models trained on one PDE type are evaluated on physically distinct but geometrically similar problems would resolve this.

### Open Question 3
Under what conditions does ABC sampling outperform MCMC methods for posterior inference in this framework? While the paper states ABC is advantageous due to parallelization and well-tailored prior, performance across observation sparsity levels, noise magnitudes, and problem complexity remains unexplored. Comparative benchmarks across all four problem domains would be needed.

## Limitations

- Theoretical guarantees about pushforward equivalence are mathematically correct but lack empirical validation in practice
- Critical implementation details like non-local pooling and MMD kernel configuration are underspecified
- Scalability claims to "large problems" are only tested on one moderately large problem (terrain airflow)
- Out-of-distribution robustness is untested; no analysis of failure modes when true posterior lies in low-density regions

## Confidence

- **High confidence:** Core architectural approach is technically sound and well-grounded in prior literature; parallel ABC sampling advantage over MCMC is empirically demonstrated
- **Medium confidence:** Theoretical claim about pushforward decoder recovering Bayesian posterior is mathematically correct but practically unverified; "train-once-use-anywhere" flexibility is supported but not comprehensively tested
- **Low confidence:** Claims about scaling to "large problems" and robustness to out-of-distribution geometries lack empirical validation; relative advantage over diffusion-based methods is not directly benchmarked

## Next Checks

1. **Latent space coverage test:** Generate 10,000 random samples from the latent prior, decode them, and visualize the distribution of reconstructed fields. Verify that the training data manifold is well-covered and that no mode collapse has occurred.

2. **Out-of-distribution geometry test:** Hold out an entire class of geometries during training (e.g., exclude all L-shaped domains) and test GABI's performance on these. Compare to Direct Map baseline to quantify the value of the learned prior.

3. **Computational scaling benchmark:** Measure wall-clock time and memory usage of the encoder/decoder as mesh resolution increases from 100 to 10,000 nodes. Compare to the claimed advantage of ABC's parallelization by measuring strong scaling with increasing GPU cores.