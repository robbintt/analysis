---
ver: rpa2
title: 'Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy
  Explanations in AutoML'
arxiv_id: '2507.14744'
source_url: https://arxiv.org/abs/2507.14744
tags:
- rashomon
- explanations
- dependence
- partial
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explanation uncertainty in
  AutoML systems by proposing a novel framework that incorporates model multiplicity
  into explanation generation. The core idea is to aggregate partial dependence profiles
  (PDP) from a set of near-optimal models, known as the Rashomon set, to create a
  Rashomon PDP.
---

# Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML

## Quick Facts
- **arXiv ID:** 2507.14744
- **Source URL:** https://arxiv.org/abs/2507.14744
- **Reference count:** 36
- **Primary result:** Aggregating partial dependence profiles from a set of near-optimal models (Rashomon set) creates uncertainty-aware explanations that improve trustworthiness in AutoML.

## Executive Summary
This paper addresses the challenge of explanation uncertainty in AutoML systems by proposing a novel framework that incorporates model multiplicity into explanation generation. The core idea is to aggregate partial dependence profiles (PDP) from a set of near-optimal models, known as the Rashomon set, to create a Rashomon PDP. This approach captures interpretive variability and highlights areas of disagreement, providing users with a richer, uncertainty-aware view of feature effects. The framework is evaluated using two quantitative metrics: coverage rate and mean width of confidence intervals. Experiments on 35 regression datasets from the OpenML CTR23 benchmark suite show that in most cases, the Rashomon PDP covers less than 70% of the best model's PDP, underscoring the limitations of single-model explanations. The findings suggest that the Rashomon PDP improves the reliability and trustworthiness of model interpretations by adding additional information that would otherwise be neglected, particularly in high-stakes domains where transparency and confidence are critical.

## Method Summary
The method introduces Rashomon Partial Dependence Profiles as an ensemble-based approach to explainable AI in AutoML. The framework identifies a Rashomon set - models with performance close to the best model - and aggregates their individual PDPs to generate a confidence interval around the feature effect estimates. This aggregation captures both the central tendency and the variability of interpretations across the model set. The evaluation employs coverage rate (proportion of single-model PDPs covered by the Rashomon interval) and mean width of confidence intervals as quantitative metrics. The approach is tested across 35 regression datasets from the OpenML CTR23 benchmark suite, comparing the Rashomon PDP against traditional single-model explanations to quantify the uncertainty and interpretive variability introduced by model multiplicity.

## Key Results
- Rashomon PDP covers less than 70% of the best model's PDP in most cases, demonstrating significant interpretive variability
- The framework successfully quantifies explanation uncertainty through coverage rates and confidence interval widths
- The approach is particularly relevant for high-stakes domains where transparency and confidence are critical

## Why This Works (Mechanism)
The Rashomon effect in machine learning recognizes that multiple models can achieve similar performance on a given dataset. By leveraging this multiplicity, the framework captures the inherent uncertainty in feature effect interpretations that single-model approaches miss. When multiple near-optimal models disagree on feature effects, this disagreement represents genuine uncertainty about the underlying data relationships rather than noise. Aggregating these diverse perspectives through confidence intervals provides a more honest representation of what the models collectively "know" and "don't know" about feature relationships, leading to more trustworthy explanations.

## Foundational Learning
**Partial Dependence Profiles (PDP):** Visualization technique showing how a feature influences model predictions while averaging out the effects of other features. Why needed: Provides interpretable feature effect visualization. Quick check: PDP should show expected relationships (e.g., positive correlation for relevant features).

**Rashomon Set:** Collection of models with performance within a threshold of the best model. Why needed: Captures model multiplicity without including poor performers. Quick check: All models in set should have validation performance within specified tolerance of best model.

**Coverage Rate Metric:** Proportion of individual model PDPs covered by the aggregated Rashomon PDP confidence interval. Why needed: Quantifies how well the ensemble explanation represents the diversity of interpretations. Quick check: Coverage should be high when models agree, lower when they disagree.

**Mean Width of Confidence Intervals:** Average width of the uncertainty bands around the aggregated PDP. Why needed: Measures the magnitude of interpretive uncertainty. Quick check: Wider intervals should correspond to areas of high model disagreement.

**OpenML CTR23 Benchmark:** Collection of 35 regression datasets used for systematic evaluation. Why needed: Provides standardized, diverse test bed for comparing explanation methods. Quick check: Datasets should span different characteristics (size, dimensionality, noise levels).

## Architecture Onboarding

**Component Map:** Data -> Model Selection -> PDP Generation -> Rashomon Set Identification -> Confidence Interval Aggregation -> Rashomon PDP Output

**Critical Path:** The most time-consuming step is generating individual PDPs for all models in the Rashomon set, particularly for high-dimensional data or models with expensive prediction operations.

**Design Tradeoffs:** Using a fixed performance threshold for Rashomon set selection versus adaptive methods based on performance distribution. Fixed thresholds are simpler but may include too many or too few models depending on the problem.

**Failure Signatures:** If coverage rates are consistently very high (>90%), the Rashomon set may be too small, missing important interpretive diversity. If coverage rates are very low (<30%), the set may be too large, including models that are not truly near-optimal.

**First Experiments:**
1. Apply Rashomon PDP to a simple linear regression problem with known feature relationships to verify correct uncertainty quantification
2. Compare coverage rates between random forest and neural network ensembles on the same dataset
3. Test sensitivity of results to different Rashomon set size thresholds (e.g., top 10% vs top 25% of models)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on quantitative coverage metrics that may not fully capture practical value for end-users
- Sample size of 35 regression datasets may not represent all AutoML use cases
- Limited comparison metrics (coverage rate and mean width) may not reflect practical interpretation
- No user studies to demonstrate real-world impact on trust and understanding
- Theoretical claims about high-stakes domain value remain unverified with concrete case studies

## Confidence
- Core claim that Rashomon PDP provides richer explanations: **Medium**
- Single-model explanations are insufficient: **High**
- Approach valuable for high-stakes domains: **Low**

## Next Checks
1. Conduct user studies to assess whether Rashomon PDP improves trust and understanding compared to single-model explanations in practical settings
2. Extend experiments to include classification tasks and non-tabular data types to test generalizability
3. Compare Rashomon PDP against other uncertainty quantification methods for explanations, such as Bayesian approaches or ensemble-based techniques