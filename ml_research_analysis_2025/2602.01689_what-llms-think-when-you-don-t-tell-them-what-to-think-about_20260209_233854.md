---
ver: rpa2
title: What LLMs Think When You Don't Tell Them What to Think About?
arxiv_id: '2602.01689'
source_url: https://arxiv.org/abs/2602.01689
tags:
- text
- figure
- qwen
- gpt-oss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models were studied under minimal, topic-neutral
  prompts to reveal unconstrained generative behaviors. Across 256,000 samples from
  16 models, outputs spanned a broad semantic space even without explicit topics,
  with each model family showing systematic topical preferences: GPT-OSS predominantly
  generated programming (27.1%) and math content (24.6%), Llama favored literary content
  (9.1%), DeepSeek often produced religious content, and Qwen frequently generated
  multiple-choice questions.'
---

# What LLMs Think When You Don't Tell Them What to Think About?

## Quick Facts
- arXiv ID: 2602.01689
- Source URL: https://arxiv.org/abs/2602.01689
- Reference count: 40
- Large language models exhibit systematic topical preferences when given minimal prompts

## Executive Summary
This study investigates how large language models behave when given minimal, topic-neutral prompts without explicit instructions. By analyzing 256,000 outputs from 16 different models, the researchers reveal that models generate content spanning a broad semantic space even without specific topics, with each model family showing distinct topical preferences. The study demonstrates that minimally conditioned generation can effectively reveal model tendencies relevant to AI monitoring and safety.

## Method Summary
The researchers sampled 256,000 outputs from 16 LLMs using minimal, topic-neutral prompts at temperature 1.0. They employed semantic categorization through keyword-based methods and manual labeling of 2,000 samples to analyze topical preferences across model families. The study compared GPT-OSS, Llama, DeepSeek, and Qwen models, examining not only topic distribution but also subcategory depth and degenerate text patterns in the generated outputs.

## Key Results
- GPT-OSS models predominantly generated programming (27.1%) and math content (24.6%)
- Llama models favored literary content (9.1%), while DeepSeek produced religious content and Qwen generated multiple-choice questions
- GPT-OSS produced more technically advanced content (e.g., dynamic programming) compared to other models
- Different model families exhibited distinct degenerate text patterns, with GPT-OSS generating formatting artifacts and Llama producing URLs to personal social accounts

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanism driving why different model families exhibit distinct topical preferences when given minimal prompts. This represents a gap in understanding the fundamental reasons behind the observed systematic differences in model behavior.

## Foundational Learning
- **Semantic categorization**: Why needed - to classify model outputs into meaningful topics; Quick check - verify keyword matching accuracy through manual annotation
- **Temperature-based sampling**: Why needed - controls randomness in generation; Quick check - compare outputs across different temperature settings
- **Cross-model comparison**: Why needed - identifies systematic differences between model families; Quick check - ensure consistent prompt design across all models

## Architecture Onboarding
- **Component map**: Prompt design -> Model generation -> Semantic categorization -> Manual validation -> Statistical analysis
- **Critical path**: Minimal prompt → Model generation → Output collection → Categorization → Analysis of topical patterns
- **Design tradeoffs**: Temperature 1.0 maximizes variance but may introduce artifacts vs lower temperatures that could mask model tendencies
- **Failure signatures**: Degenerate text patterns (URLs, formatting artifacts) that reveal model-specific behaviors
- **First experiments**: 1) Compare single-model vs multi-model generation patterns; 2) Test different prompt lengths; 3) Vary temperature settings systematically

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions for future research. This represents a missed opportunity to highlight areas where further investigation could build upon the findings presented.

## Limitations
- Study uses temperature 1.0, which may amplify variance and affect reproducibility
- Semantic categorization relies on keyword-based methods and manual labeling, introducing potential bias
- Focus on English-language generation limits generalizability to multilingual contexts

## Confidence
- Primary finding (systematic topical preferences): High confidence
- Technical advancement measurement: Medium confidence
- Degenerate text patterns: Medium confidence

## Next Checks
1. Replicate the study with multiple temperature settings (0.1, 0.5, 1.0) to assess temperature sensitivity
2. Implement cross-validation of semantic categorization using independent annotators
3. Conduct time-series analysis by sampling the same models at different points to detect behavioral drift