---
ver: rpa2
title: 'MaiBERT: A Pre-training Corpus and Language Model for Low-Resourced Maithili
  Language'
arxiv_id: '2509.15048'
source_url: https://arxiv.org/abs/2509.15048
tags:
- language
- maithili
- languages
- corpus
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MaiBERT, a BERT-based language model specifically
  pre-trained for the Maithili language, which has been underserved in NLP research
  despite having millions of speakers. The authors constructed a high-quality Maithili
  corpus from diverse sources including Wikipedia, digitized books, and news articles,
  totaling approximately 18 million words.
---

# MaiBERT: A Pre-training Corpus and Language Model for Low-Resourced Maithili Language

## Quick Facts
- arXiv ID: 2509.15048
- Source URL: https://arxiv.org/abs/2509.15048
- Reference count: 10
- Outperformed NepBERTa and HindiBERT by 5-7% across news classification classes with 87.02% accuracy

## Executive Summary
MaiBERT is a BERT-based language model specifically pre-trained for the Maithili language, addressing the lack of computational resources for this low-resource Indic language despite its millions of speakers. The authors constructed an 18-million-word corpus from Wikipedia, digitized books, and news articles, then trained a BERT-base model using Masked Language Modeling. When evaluated on a 10-class news classification task, MaiBERT achieved 87.02% accuracy, outperforming existing regional models by 0.13% overall and 5-7% across various classes. The model was open-sourced on Hugging Face for further fine-tuning on downstream tasks.

## Method Summary
The authors built a Maithili corpus of approximately 18 million words from diverse sources: Wikipedia (~880K words), digitized books (418 documents, ~10.4M words), and news articles from three sources (~6.6M words). They trained a custom SentencePiece-BPE tokenizer with 30,000 vocabulary size optimized for Devanagari script features. Using this corpus, they pre-trained a BERT-base architecture (d=768) with MLM objective for 1 million steps on NVIDIA RTX 4090. The model was then fine-tuned on a labeled dataset of 11,959 news articles across 10 classes (70/15/15 train/val/test split) using early stopping with macro-F1 evaluation.

## Key Results
- MaiBERT achieved 87.02% accuracy on 10-class news classification, outperforming NepBERTa and HindiBERT by 0.13% overall
- Across individual classes, MaiBERT showed 5-7% improvement over HindiBERT on texts with Maithili-specific vocabulary and grammatical structures
- The model demonstrated effective learning of Maithili's unique morphological and lexical patterns through language-specific pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific pre-training captures morphological and lexical patterns that multilingual models miss when targeting low-resource languages.
- Mechanism: MaiBERT learns representations directly from Maithili text rather than relying on cross-lingual transfer from related languages (Hindi, Nepali), which share Devanagari script but differ in dialectal variations, unique lexical items, and grammatical structures.
- Core assumption: Maithili's linguistic distinctiveness is underrepresented in multilingual models; therefore, monolingual pre-training yields better embeddings for downstream tasks than transfer from related scripts.
- Evidence anchors:
  - [abstract] "outperforming existing regional models like NepBERTa and HindiBERT, with a 0.13% overall accuracy gain and 5–7% improvement across various classes"
  - [section 6.2] "maiBERT outperforms HindiBERT by 5–7% in texts where Maithili-specific vocabulary and grammatical structures are more significant"
  - [corpus] Limited external validation exists; neighbor papers show similar low-resource approaches but none validate MaiBERT specifically
- Break condition: If Maithili text distribution shifts dramatically from the news domain used for evaluation, generalization gains may not hold.

### Mechanism 2
- Claim: Subword tokenization tuned for Devanagari script improves embedding granularity for morphologically rich Maithili.
- Mechanism: SentencePiece-BPE builds a 30,000-token vocabulary optimized for Maithili's 47 phonemes and agglutinative morphology, preserving rare and compound words that generic multilingual tokenizers would fragment.
- Core assumption: Script-specific tokenization better handles Maithili's unique punctuation markers and word structures than off-the-shelf multilingual vocabularies.
- Evidence anchors:
  - [abstract] "custom tokenizer that preserves Devanagari script features"
  - [section 3.1] "customized BertTokenizer...preserved linguistic features specific to Devanagari script while filtering out non-Devanagari characters"
  - [corpus] No direct external validation of tokenizer effectiveness; mechanism is inferred from performance gains
- Break condition: If vocabulary size is insufficient for rare domain-specific terms (e.g., medical or legal Maithili), subword fragmentation could degrade performance.

### Mechanism 3
- Claim: Diverse corpus sources (Wikipedia, digitized books, news) provide broader syntactic and semantic coverage than single-domain datasets.
- Mechanism: Combining 880K Wikipedia words, 10.4M book words, and 6.6M news words exposes the model to formal, literary, and journalistic registers, yielding 1,357,081 unique tokens and reducing domain overfitting.
- Core assumption: Corpus diversity directly translates to improved generalization across downstream tasks; quality of OCR-extracted book text is sufficient for pre-training.
- Evidence anchors:
  - [abstract] "constructed a high-quality Maithili corpus from diverse sources...totaling approximately 18 million words"
  - [section 3.1] Corpus statistics showing 1,028,017 sentences with average 16.88 words per sentence
  - [corpus] Weak external validation; no neighbor papers directly assess this corpus
- Break condition: If OCR errors in digitized books introduce systematic noise, morphological patterns could be corrupted.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MaiBERT's pre-training objective masks 15% of tokens and predicts them; understanding this is essential for replicating or extending the model.
  - Quick check question: If you mask "मैथिली" in a sentence, what contextual signals would MaiBERT use to predict it?

- Concept: Subword Tokenization (BPE/SentencePiece)
  - Why needed here: The model uses a 30,000-token BPE vocabulary; engineers must understand how rare words decompose into subword units.
  - Quick check question: How would a tokenizer handle a Maithili compound word not in its vocabulary?

- Concept: Transfer Learning vs Language-Specific Pre-training
  - Why needed here: The paper's central claim is that monolingual pre-training outperforms multilingual transfer for Maithili.
  - Quick check question: Why might mBERT (trained on 104 languages) underperform on Maithili compared to MaiBERT?

## Architecture Onboarding

- Component map:
  Input Layer: SentencePiece-BPE tokenizer → 30K vocabulary → 512 max tokens
  Embedding Layer: Token + Positional encodings (768-dim)
  Encoder: 12-layer Transformer (standard BERT-base configuration, 110M params)
  Task Heads: MLM head (pre-training) → CLS token + Softmax classifier (fine-tuning)
  Training: Adam optimizer, LR=5e-5, batch=64, 1M steps, mixed-precision FP16

- Critical path: Corpus preprocessing (NFC normalization → Devanagari filtering) → Tokenization → MLM pre-training → Fine-tune on labeled classification data (70/15/15 split) → Evaluate macro-F1

- Design tradeoffs:
  - Vocabulary size (30K) balances coverage vs. embedding sparsity; smaller would underrepresent rare words.
  - Corpus composition (57% books, 37% news, 5% Wikipedia) emphasizes literary language; may bias toward formal registers.
  - Model scale (0.11B params) prioritizes efficiency over capacity; may limit performance on complex generative tasks.

- Failure signatures:
  - High training loss after epoch 10 (expected: ~0.01) → check tokenization or data corruption
  - mBERT outperforms MaiBERT → corpus may be too small or domain-mismatched to task
  - Poor performance on conversational text → pre-training corpus lacks informal registers

- First 3 experiments:
  1. Replicate news classification baseline: Fine-tune MaiBERT on the 12,383-instance labeled dataset and verify ~87% accuracy matches paper.
  2. Ablate tokenizer: Compare SentencePiece-BPE vs. standard mBERT tokenizer on same classification task to isolate tokenization contribution.
  3. Domain shift test: Evaluate MaiBERT on a non-news task (e.g., sentiment analysis if data available) to assess generalization limits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does transitioning from Masked Language Modeling (MLM) to Causal Language Modeling (CLM) affect MaiBERT's generative capabilities for Maithili text?
- Basis in paper: [explicit] The conclusion states, "limitations remain in generative capabilities... Future work will focus on... enhancing the model's generative capacity through causal language modeling."
- Why unresolved: The current study focused exclusively on an encoder architecture (BERT) using MLM, which is optimized for understanding rather than generation.
- What evidence would resolve it: A comparison of text generation metrics (e.g., BLEU, perplexity) between the current MLM-based MaiBERT and a new decoder-based or encoder-decoder variant trained with CLM.

### Open Question 2
- Question: To what extent does incorporating conversational and informal literary data improve the model's performance on non-news downstream tasks?
- Basis in paper: [explicit] The authors note "limitations... in creative or poetic contexts" and identify "expanding the diversity of the training corpus to include more conversational, literary, and informal texts" as a primary goal for future work.
- Why unresolved: The current pre-training corpus is dominated by formal sources (news, Wikipedia, digitized books), potentially limiting adaptability to colloquial or creative domains.
- What evidence would resolve it: Benchmarking the fine-tuned model on tasks involving informal dialogue or poetry analysis after re-pretraining with the expanded corpus.

### Open Question 3
- Question: What is the impact of instruction tuning on MaiBERT's zero-shot or few-shot performance compared to standard fine-tuning?
- Basis in paper: [explicit] The conclusion explicitly lists plans to "integrate instruction tuning for downstream applications" to enhance utility.
- Why unresolved: The paper currently evaluates the model only via standard supervised fine-tuning on a classification task, without assessing its ability to follow natural language prompts.
- What evidence would resolve it: Evaluating the model on a benchmark of instruction-following tasks specific to Maithili to measure task generalization without task-specific weight updates.

## Limitations
- Corpus quality and representativeness are unverified, with OCR digitization potentially introducing noise
- Evaluation limited to single 10-class news classification task without validation on other downstream tasks
- Architecture specification gaps leave critical hyperparameters (layers, attention heads) unspecified

## Confidence
- High Confidence: MaiBERT outperforming NepBERTa and HindiBERT on news classification (87.02% accuracy)
- Medium Confidence: Language-specific pre-training outperforming multilingual transfer (lacks multi-task validation)
- Low Confidence: Custom tokenizer contribution (no ablation study provided)

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate MaiBERT on a non-news Maithili dataset (if available) such as conversational text, social media posts, or literary analysis. Measure performance degradation compared to the news classification benchmark to assess real-world applicability beyond the training domain.

2. **Tokenizer Ablation Study**: Train an identical BERT-base model using the standard multilingual BERT tokenizer instead of MaiBERT's custom Devanagari tokenizer. Compare performance on the same classification task to isolate the contribution of script-specific tokenization to the reported accuracy gains.

3. **Multilingual vs. Monolingual Transfer Comparison**: Fine-tune mBERT (104 languages) and MaiBERT on the same Maithili classification dataset. Additionally, test zero-shot transfer from HindiBERT and NepBERTa (trained on related languages) to quantify the performance gap between cross-lingual transfer and monolingual pre-training approaches.