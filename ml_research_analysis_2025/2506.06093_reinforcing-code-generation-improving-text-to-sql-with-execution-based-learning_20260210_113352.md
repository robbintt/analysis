---
ver: rpa2
title: 'Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning'
arxiv_id: '2506.06093'
source_url: https://arxiv.org/abs/2506.06093
tags:
- athlete
- reward
- code
- query
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reinforcement learning can improve
  code generation by allowing a model to interact with a database engine and learn
  from execution-based feedback, rather than relying solely on supervised fine-tuning.
  The authors frame the problem as a reinforcement learning task where an LLM generates
  SQL queries from natural language questions and receives rewards based on execution
  success and answer correctness.
---

# Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning

## Quick Facts
- arXiv ID: 2506.06093
- Source URL: https://arxiv.org/abs/2506.06093
- Reference count: 12
- Primary result: RL-tuned SQLCoder-7B achieves 49.83% exact match accuracy, reducing syntax errors from 25.43% to 14.71%

## Executive Summary
This paper demonstrates that reinforcement learning with execution-based feedback can significantly improve text-to-SQL generation. The authors apply Group Relative Policy Optimization (GRPO) to train models using rewards derived from SQL query execution rather than supervised fine-tuning. Their approach leads to substantial performance gains on the TEMPTABQA-C benchmark, with SQLCoder-7B accuracy increasing from 31.49% to 49.83% in exact match score. The method is particularly effective at reducing syntax errors and improving performance on harder queries and counterfactual data.

## Method Summary
The method applies GRPO with execution-based rewards to train text-to-SQL models. For each natural language question and database schema, the model generates k=2 SQL candidates via beam search, executes them against a MariaDB database, and receives rewards based on execution success and answer correctness. The rewards include syntax error penalties (-100), execution success bonuses (+1), partial match rewards based on Relaxed Exact Match Score (REMSÃ—100), and exact match rewards (+1000). LoRA adapters are used for parameter-efficient fine-tuning, with a KL penalty term to prevent policy drift. The approach is tested on TEMPTABQA-C benchmark using SQLCoder-7B and CodeGemma-7B models.

## Key Results
- SQLCoder-7B exact match score improves from 31.49% to 49.83% after RL tuning
- Syntax error rate decreases from 25.43% to 14.71%, approaching SQLCoder-70B performance
- RL-tuned models show particular effectiveness on harder queries and counterfactual data
- CodeGemma-7B also shows improvements from 32.75% to 43.06% exact match score

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage Normalization
GRPO generates k=2 completions per prompt and normalizes rewards into advantages using $A_i = (r_i - \mu_r) / \sigma_r$. This creates a relative signal where the model learns to prefer better-than-average candidates within the group, even if absolute rewards are low or noisy. The variance within small groups (k=2) provides sufficient gradient signal for improvement.

### Mechanism 2: Execution-Grounded Feedback Loop
The model generates SQL which is executed by a MariaDB engine, receiving deterministic feedback. A large negative reward ($r_{err} = -100$) for syntax errors strongly penalizes invalid code, while positive rewards for correct answers reinforce valid logic. This creates selection pressure for executable and accurate programs, helping the model overcome hallucinations regarding syntax and schema.

### Mechanism 3: KL-Regularized Policy Constraint
The loss function includes a KL divergence penalty term $\beta \cdot KL(\pi_\theta \| \pi_0)$ that anchors the updated policy to the reference model. This retains syntactic priors learned during pre-training while optimizing for new rewards, preventing catastrophic forgetting of general SQL syntax during RL fine-tuning.

## Foundational Learning

### Concept: Proximal Policy Optimization (PPO) & Policy Gradients
**Why needed here:** GRPO is a variant of PPO. Understanding how policy gradients update model weights based on rewards is essential to debug why the model is generating specific SQL patterns.
**Quick check question:** Can you explain how the "advantage" $A_i$ in GRPO differs from the value function estimation used in standard PPO?

### Concept: SQL Execution Environments
**Why needed here:** The rewards are derived from query execution. Distinguishing between syntax errors (code fails) and logic errors (code runs but answer is wrong) is critical for designing the reward function.
**Quick check question:** Does the reward system penalize a query that runs successfully but returns the wrong rows, or only queries that crash the database?

### Concept: Reward Shaping (Sparse vs. Dense Rewards)
**Why needed here:** The paper uses a composite reward (syntax + partial + exact). Understanding why exact match alone is insufficient (sparse signal) explains the architecture's success.
**Quick check question:** In the paper's reward scheme, what is the specific reward value assigned for a syntax error versus a successful execution?

## Architecture Onboarding

### Component map:
LLM Backbone (SQLCoder-7B/CodeGemma-7B with LoRA) -> MariaDB Database Engine -> Python Reward Function -> GRPO Trainer -> Updated LLM Weights

### Critical path:
1. **Input:** Natural Language Question (NLQ) + Database Schema
2. **Generation:** Model generates k=2 SQL candidates via beam search
3. **Interaction:** Candidates are executed against the MariaDB instance
4. **Evaluation:** Results compared to ground truth; rewards calculated (-100 for error, +1000 for exact match)
5. **Update:** Advantages normalized across group; model weights update via GRPO loss

### Design tradeoffs:
- **Latency vs. Signal:** Generating multiple candidates and waiting for database execution significantly slows training compared to SFT
- **Reward Hacking vs. Partial Credit:** The paper notes CodeGemma exploited partial rewards by dumping excessive data (Section 6.4)

### Failure signatures:
- **Reward Hacking:** Models may generate `SELECT *` to maximize partial overlap scores rather than filtering logic
- **Hard-coded Logic:** Model may hard-code dates (e.g., 2010-2020) rather than computing from schema columns
- **Schema Hallucination:** Despite execution feedback, complex schemas might still trigger non-existent column references

### First 3 experiments:
1. **Sanity Check (Mock Execution):** Run training loop with "mock" database that always returns success to isolate RL mechanism
2. **Reward Sensitivity Analysis:** Ablate partial reward; train with only binary rewards vs. REMS-based partial rewards on TEMPTABQA-C
3. **Generalization Test:** Train on standard split but evaluate specifically on "Counterfactual" split to verify learning SQL logic vs. memorizing patterns

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can reward functions be redesigned to prevent reward hacking while maintaining learning signal?
**Basis in paper:** Section 6.4 documents CodeGemma exploiting partial rewards by generating overly broad queries that return many results for partial credit, and the Limitations section lists reward hacking as a key concern.
**Why unresolved:** The paper observes the behavior but does not propose or test mitigation strategies.
**What evidence would resolve it:** Experiments comparing alternative reward formulations (e.g., penalizing result set size, removing partial rewards, or adding structural constraints) on the same benchmark.

### Open Question 2
**Question:** Does execution-based RL scale effectively to models larger than 70B parameters?
**Basis in paper:** The Limitations section states: "Larger models may behave differently under similar training protocols. This may restrict the generalization of our findings to significantly larger models."
**Why unresolved:** All experiments used 7B and 70B models; no larger models were tested.
**What evidence would resolve it:** Applying the same GRPO training protocol to models exceeding 70B parameters and comparing gains relative to baseline.

### Open Question 3
**Question:** Can intermediate supervision over SQL structure enable learning of multi-step logical inference that answer-only rewards fail to teach?
**Basis in paper:** Section 6.3 shows the RL-tuned model failed to learn temporal reasoning requiring birth_year computation, and the Limitations section notes weak supervision may limit learning of concepts requiring multi-step inference.
**Why unresolved:** The current setup provides only final-answer rewards with no guidance on intermediate reasoning steps.
**What evidence would resolve it:** Augmenting the reward with intermediate signals (e.g., correct table joins, valid subquery structure) and testing on temporally complex queries.

### Open Question 4
**Question:** Does execution-based RL transfer to other symbolic reasoning domains beyond SQL (e.g., Python, APIs)?
**Basis in paper:** Section 2 mentions the goal of models that "generate executable code in a target programming language (e.g., Python, SQL, etc.)" and Section 7 discusses extending to "low-level APIs, databases, and external systems."
**Why unresolved:** Experiments were restricted to SQL generation only.
**What evidence would resolve it:** Applying the same GRPO framework with execution-based rewards to Python code generation or API interaction tasks.

## Limitations

- **Implementation details unclear:** Code is listed as "forthcoming" with unspecified prompt format for schema serialization
- **Reward hacking vulnerability:** Model can exploit partial rewards by generating overly broad queries that return many results
- **Limited scope:** Experiments restricted to SQL generation; scalability to larger models and other domains not tested

## Confidence

- **High Confidence:** The 18.34% improvement in Exact Match Score from 31.49% to 49.83% is well-supported by experimental results
- **Medium Confidence:** Performance approaching SQLCoder-70B levels is supported but lacks accounting for potential training differences
- **Low Confidence:** Effectiveness on harder queries and counterfactual data lacks detailed statistical analysis and quantification

## Next Checks

1. **Reward Function Sensitivity Test:** Run ablation studies removing each reward component to quantify their individual contributions to the 18.34% EMS improvement
2. **Generalization Stress Test:** Evaluate the trained model on a completely unseen database schema with similar structure but different data to verify if the model learned SQL logic patterns or merely memorized TEMPTABQA-C-specific patterns
3. **Latency-Performance Tradeoff Analysis:** Measure wall-clock time per training iteration with execution feedback versus supervised fine-tuning baseline, and calculate EMS improvement per unit of training time to assess practical efficiency