---
ver: rpa2
title: Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered
  Mobile-Use Agents
arxiv_id: '2510.02204'
source_url: https://arxiv.org/abs/2510.02204
tags:
- arxiv
- reasoning
- action
- agents
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new evaluation framework for diagnosing
  reasoning-execution gaps in VLM-powered mobile-use agents. The core contribution
  is the Ground-Truth Alignment (GTA) metric, which measures whether an agent's chain-of-thought
  reasoning aligns with ground-truth actions, enabling disentanglement of reasoning
  accuracy from execution accuracy.
---

# Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents

## Quick Facts
- arXiv ID: 2510.02204
- Source URL: https://arxiv.org/abs/2510.02204
- Reference count: 30
- Primary result: Introduces GTA metric to diagnose reasoning-execution gaps in VLM-powered mobile agents, showing execution gaps predominate over reasoning gaps even in 72B models.

## Executive Summary
This work introduces a novel evaluation framework for diagnosing reasoning-execution gaps in vision-language model (VLM) powered mobile-use agents. The core contribution is the Ground-Truth Alignment (GTA) metric, which measures whether an agent's chain-of-thought reasoning aligns with ground-truth actions, enabling disentanglement of reasoning accuracy from execution accuracy. Experimental results across three mobile interaction benchmarks show that reasoning-execution gaps are prevalent, with execution gaps (EG) occurring more frequently than reasoning gaps (RG). While scaling model size reduces overall gaps, sizable execution gaps persist even in the largest models. The framework reveals distinct failure modes and supports development of more trustworthy mobile-use agents.

## Method Summary
The paper introduces Ground-Truth Alignment (GTA) as a diagnostic metric that measures whether the action implied by a chain-of-thought (CoT) matches the ground-truth action. Using an instruction-following VLM (AgentCPM-GUI-8B) as an evaluator, the framework conditions on CoT, local history, and current screenshot to predict the implied action. This predicted action is compared to ground truth using exact match criteria. Combining GTA with standard execution match (EM) reveals four quadrants: correct reasoning/execution, execution gap (correct reasoning, wrong action), reasoning gap (correct action, wrong reasoning), and both incorrect. The framework is applied across three benchmarks (AITZ, CAGUI, AndroidControl) using models ranging from 2B to 72B parameters.

## Key Results
- Execution gaps (EG) occur more frequently than reasoning gaps (RG) across 18 model-dataset combinations
- Scaling improves both EM and GTA but execution gaps persist above 10% even in 72B models
- AgentCPM-GUI shows high RG on AITZ (relying on action shortcuts), while UI-TARS-7B shows high EG on CAGUI (OOD scenario)
- GTA surpasses EM in 14 of 18 cases, confirming EG > RG in most scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GTA metric disentangles reasoning correctness from execution correctness by mapping free-form CoT to implied actions and comparing against ground truth.
- Mechanism: An instruction-following VLM (AgentCPM-GUI-8B) conditions on the CoT, local history, and current screenshot to predict the action implied by the reasoning trace. This predicted action is then compared to ground truth using the same strict matching rule as EM.
- Core assumption: The evaluator VLM can reliably interpret underspecified or context-dependent references in CoT (e.g., "click the confirm button below") when grounded in the current screen and history.
- Evidence anchors:
  - [abstract]: "At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action."
  - [Section 3.3]: "We retain (H_n, o_n) in the evaluators input because CoT c_n often contains underspecified or context-dependent references... Conditioning on (c_n, H_n, o_n) therefore ensures that f(c_n) consistently maps reasoning traces to executable actions."
  - [corpus]: Related work "Large Language Models Often Say One Thing and Do Another" explores similar consistency issues but in general LLM contexts, not GUI agents specifically.
- Break condition: If the evaluator VLM has systematic biases or grounding failures for certain action types or GUI layouts, GTA scores may not reflect true reasoning alignment.

### Mechanism 2
- Claim: Combining GTA with EM reveals two distinct failure modes: Execution Gap (correct reasoning, wrong action) and Reasoning Gap (correct action, wrong reasoning).
- Mechanism: GTA measures whether the CoT implies the ground-truth action; EM measures whether the executed action matches ground truth. Their joint distribution across four quadrants isolates whether errors originate from reasoning-to-action translation (EG) or action shortcuts that bypass reasoning (RG).
- Core assumption: The action space and matching criteria are consistent between GTA and EM evaluation.
- Evidence anchors:
  - [abstract]: "This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution."
  - [Section 4.3]: "Across 18 model-dataset combinations, GTA surpasses EM in 14 cases, indicating that EG>RG in the majority of scenarios. This suggests that most models can reason correctly... but encounter difficulty when mapping these intermediate reasoning steps into the precise executable actions."
  - [corpus]: Weak direct corpus evidence for this specific four-quadrant decomposition in GUI agents; most related work focuses on general reasoning-action consistency.
- Break condition: If ground-truth annotations contain errors or if the action space is underspecified, quadrant assignments become unreliable.

### Mechanism 3
- Claim: Parameter scaling monotonically improves both reasoning and execution accuracy but does not eliminate execution gaps, which persist above 10% even in 72B models.
- Mechanism: Larger models better capture the mapping from visual GUI observations and instructions to both reasoning traces and actions. However, the reasoning-to-execution translation bottleneck remains because scaling does not directly address grounding fidelity.
- Core assumption: The scaling trend generalizes across model families and training paradigms beyond UI-TARS.
- Evidence anchors:
  - [abstract]: "While scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models."
  - [Section 4.4]: "Scaling consistently improves both EM and GTA... At the same time, EG and RG decrease monotonically with parameter growth... Notably, however, the largest 72B models still exhibit residual execution gaps (>10%), suggesting that scaling alone cannot fully eliminate misalignment."
  - [corpus]: "Thinking Short and Right Over Thinking Long" suggests efficient reasoning may matter more than scale, indirectly supporting that scaling has diminishing returns.
- Break condition: If training data quality or architecture differs significantly across scales, the scaling trend may not hold.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning in Vision-Language Models**:
  - Why needed here: The entire diagnostic framework depends on understanding how VLMs generate intermediate reasoning traces and how these traces relate to action prediction.
  - Quick check question: Can you explain why CoT might improve execution accuracy but also create opportunities for reasoning-execution misalignment?

- **GUI Action Spaces and Grounding**:
  - Why needed here: Evaluating both EM and GTA requires understanding what constitutes an action (type + parameters) and how free-form text descriptions map to executable operations like click coordinates or swipe directions.
  - Quick check question: Given a CoT stating "tap the submit button," what information would you need to determine the implied action's type and parameters?

- **Exact Match vs. Semantic Alignment Metrics**:
  - Why needed here: The paper's core innovation is distinguishing between execution correctness (EM) and reasoning correctness (GTA); understanding why these can diverge is essential.
  - Quick check question: If a model predicts the correct click location but its CoT describes clicking a different UI element, which quadrant does this fall into and why?

## Architecture Onboarding

- **Component map**: Input CoT → GTA Evaluator (AgentCPM-GUI-8B) → Predicted Action → GTA Score; Input Predicted Action → EM Score; GTA + EM → Quadrant Classification

- **Critical path**:
  1. Run inference on target model to collect (CoT, predicted action) pairs
  2. Apply GTA Evaluator to each CoT to extract implied action
  3. Compute EM (predicted action vs. ground truth) and GTA (implied action vs. ground truth)
  4. Classify into quadrants and compute EG/RG rates

- **Design tradeoffs**:
  - **Evaluator model choice**: Using AgentCPM-GUI-8B provides domain-specific grounding but may inherit biases from its training data; a general VLM might be more neutral but less accurate for GUI-specific references
  - **Strict matching vs. semantic similarity**: The paper uses strict type+parameter matching for both EM and GTA; relaxing this could capture near-miss errors but would complicate interpretation
  - **Deterministic vs. sampled decoding**: Greedy decoding ensures reproducibility but may miss valid action interpretations that would appear under sampling

- **Failure signatures**:
  - **High EG, Low RG**: Model reasons correctly but fails at action grounding (common in OOD scenarios like CAGUI for untrained models)
  - **High RG, Low EG**: Model relies on action shortcuts from SFT, ignoring CoT consistency (observed in AgentCPM-GUI on AITZ)
  - **Evaluator disagreement**: If GTA Evaluator accuracy drops significantly for certain action types (e.g., LONG POINT with few training examples), validation against human annotations is required

- **First 3 experiments**:
  1. **Validate GTA Evaluator reliability**: Replicate the stratified sampling procedure (200 samples per dataset) and compare evaluator predictions against human annotations; target >85% agreement as reported in Table 3
  2. **Establish baseline quadrant distribution**: Run inference on a target model (e.g., UI-TARS-7B) across all three benchmarks and compute EM, GTA, EG, RG; verify that EG > RG as the paper reports
  3. **Test scaling hypothesis**: Compare UI-TARS models at 2B, 7B, and 72B on AndroidControl; confirm monotonic improvement in EM/GTA and reduction in EG/RG, with residual EG > 10% at 72B

## Open Questions the Paper Calls Out
- What training strategies or architectural modifications can specifically reduce execution gaps (EG) beyond what parameter scaling achieves?
- How does the choice of GTA evaluator model affect gap measurements and diagnostic conclusions?
- Can causal CoT training reduce reasoning gaps (RG) caused by action shortcut learning without degrading execution accuracy?

## Limitations
- GTA evaluator reliability depends on a single VLM's ability to interpret underspecified CoT references across diverse GUI layouts
- Results based on three mobile interaction benchmarks may not generalize to other GUI domains or non-GUI reasoning tasks
- Several evaluated models may have been exposed to evaluation data during training, though the paper acknowledges this for CAGUI

## Confidence
- **High Confidence**: The core claim that reasoning-execution gaps exist and can be diagnosed using the GTA framework
- **Medium Confidence**: The claim that execution gaps predominate over reasoning gaps across benchmarks
- **Low Confidence**: The assertion that scaling alone cannot eliminate execution gaps, based on limited model scales and single model family

## Next Checks
1. **Evaluator Robustness Validation**: Conduct human annotation studies on 300-500 randomly sampled CoT-action pairs across all three benchmarks, focusing on edge cases (LONG POINT actions, complex reasoning traces) to establish confidence intervals for GTA accuracy.

2. **Cross-Domain Generalization Test**: Apply the GTA framework to a non-mobile GUI benchmark (e.g., WebShop or Mind2Web) to verify whether EG > RG holds across different interaction paradigms and whether scaling trends persist.

3. **Ablation on Decoding Strategy**: Compare deterministic vs. sampled decoding for the GTA evaluator on a subset of the data to quantify how evaluation stability affects gap measurements and quadrant classifications.