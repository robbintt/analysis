---
ver: rpa2
title: 'Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational
  Geometry'
arxiv_id: '2511.21703'
source_url: https://arxiv.org/abs/2511.21703
tags:
- embedding
- embeddings
- structure
- arxiv
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines how different embedding architectures\u2014\
  non-LLM encoders, LLM-based encoders, and their merged variants\u2014represent abstract,\
  \ non-linguistic numeric sequences. Using a controlled dataset of number-theoretic\
  \ sequences, the authors evaluate embedding generalization through clustering metrics\
  \ (Silhouette, Davies\u2013Bouldin) and KMeans labels."
---

# Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry

## Quick Facts
- **arXiv ID**: 2511.21703
- **Source URL**: https://arxiv.org/abs/2511.21703
- **Reference count**: 40
- **Key outcome**: LLM-based embeddings (especially Qwen3) outperform non-LLM models in clustering numeric sequences, with SLERP merging preserving base-model geometry while retaining task gains

## Executive Summary
This study systematically compares embedding generalization across different architectures—non-LLM encoders, LLM-based encoders, and their merged variants—using a controlled dataset of number-theoretic sequences. The authors evaluate how different models represent abstract numeric patterns through clustering metrics and KMeans labels. Their experiments reveal that LLM-based embeddings consistently outperform non-LLM models in clustering separability and structural fidelity. The research also investigates SLERP-based merging strategies versus model soups, finding that SLERP more reliably recovers base-model geometry while incorporating task-specific improvements.

## Method Summary
The authors construct a controlled dataset of abstract numeric sequences with mathematical properties, then evaluate embeddings from various architectures using clustering metrics (Silhouette, Davies-Bouldin) and KMeans labels. They compare non-LLM encoders against LLM-based encoders, with and without LoRA adaptation, and explore different merging strategies including SLERP and model soups. The evaluation focuses on embedding generalization—how well representations transfer to abstract, non-linguistic patterns—rather than task-specific performance. The study uses quantitative metrics to assess clustering quality and representational geometry preservation across different architectural choices.

## Key Results
- LLM-based embeddings (particularly Qwen3 series) consistently outperform non-LLM models on clustering metrics for abstract numeric sequences
- SLERP merging strategies preserve base-model geometry while retaining task-specific improvements better than model soups
- In model soups, LoRA adapters tend to dominate the merged representation, potentially limiting generalization capacity

## Why This Works (Mechanism)
The superior performance of LLM-based embeddings stems from their pretraining on diverse, large-scale data that captures richer semantic relationships and structural patterns. These models develop more generalizable representations that transfer well to abstract numeric sequences, even without explicit numerical pretraining. The SLERP merging mechanism works effectively because it interpolates in the tangent space of the embedding manifold, preserving geometric properties while allowing controlled blending of representations. This geometric preservation maintains the structural fidelity that enables good generalization, whereas model soups can suffer from adapter dominance that distorts the underlying representational space.

## Foundational Learning
**Embedding Generalization**: Understanding how representations transfer across domains
- *Why needed*: Core to evaluating whether models learn transferable features or task-specific memorization
- *Quick check*: Compare clustering metrics across domain-shifted test sets

**LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning technique
- *Why needed*: Enables task-specific adaptation without full model retraining
- *Quick check*: Measure performance drop when removing LoRA layers

**SLERP (Spherical Linear Interpolation)**: Geometric merging strategy for model parameters
- *Why needed*: Preserves manifold geometry during interpolation, unlike linear interpolation
- *Quick check*: Visualize interpolation paths in embedding space

**Model Soups**: Combining multiple adapted models into a single model
- *Why needed*: Leverages diversity of multiple specialized models
- *Quick check*: Compare individual vs. merged model performance

**Clustering Metrics (Silhouette, Davies-Bouldin)**: Quantitative measures of cluster quality
- *Why needed*: Objective evaluation of embedding separation and cohesion
- *Quick check*: Correlate metric values with downstream task performance

## Architecture Onboarding
**Component Map**: Non-LLM Encoder -> LLM Encoder -> LoRA Adapter -> SLERP Merger -> Model Soup -> Clustering Evaluation
**Critical Path**: Embedding Generation -> Clustering Evaluation -> Generalization Assessment
**Design Tradeoffs**: LLM backbones offer better generalization but higher computational cost; SLERP preserves geometry but requires careful parameter tuning; LoRA enables efficient adaptation but risks adapter dominance in soups
**Failure Signatures**: Poor clustering metrics indicate representation collapse; adapter dominance in soups shows as loss of base-model characteristics; SLERP failures manifest as geometric distortion in embedding space
**First Experiments**:
1. Compare clustering metrics (Silhouette, Davies-Bouldin) between non-LLM and LLM embeddings on the numeric sequence dataset
2. Evaluate SLERP merging quality by measuring geometric preservation against linear interpolation
3. Test adapter dominance in model soups by ablating individual LoRA components and measuring performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled numeric sequence dataset may not generalize to more complex or naturalistic linguistic tasks
- Clustering metrics provide only a partial view of embedding quality, potentially missing semantic coherence aspects
- The comparison between SLERP and model soups is limited to specific architectures and may not capture the full spectrum of merging strategies

## Confidence
- **High confidence**: LLM-based embeddings outperform non-LLM models on the controlled dataset, consistent across multiple architectures (Qwen3 series)
- **Medium confidence**: SLERP merging preserves base-model geometry while retaining task gains, though generalizability to other domains is uncertain
- **Medium confidence**: LoRA adapters dominate in model soups based on specific experimental setup, may vary with different hyperparameters

## Next Checks
1. Expand dataset diversity by validating findings on naturalistic language data and multimodal inputs to assess robustness
2. Benchmark computational efficiency by comparing SLERP versus model soups across different scales and architectures
3. Explore sequence length and pretraining data diversity effects on embedding generalization, particularly for non-LLM encoders