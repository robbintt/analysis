---
ver: rpa2
title: A Survey on Model Extraction Attacks and Defenses for Large Language Models
arxiv_id: '2506.22521'
source_url: https://arxiv.org/abs/2506.22521
tags:
- extraction
- arxiv
- which
- attacks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically analyzes model extraction attacks against
  large language models, identifying three main attack categories: functionality extraction
  (replicating model behavior), training data extraction (recovering memorized examples),
  and prompt-targeted attacks (stealing valuable prompts). The authors examine various
  attack methodologies including API-based knowledge distillation, direct querying,
  parameter recovery, and prompt stealing techniques exploiting transformer architectures.'
---

# A Survey on Model Extraction Attacks and Defenses for Large Language Models

## Quick Facts
- arXiv ID: 2506.22521
- Source URL: https://arxiv.org/abs/2506.22521
- Authors: Kaixiang Zhao; Lincan Li; Kaize Ding; Neil Zhenqiang Gong; Yue Zhao; Yushun Dong
- Reference count: 40
- Systematic analysis of model extraction attacks and defenses for large language models

## Executive Summary
This survey comprehensively analyzes model extraction attacks targeting large language models, identifying three main attack categories: functionality extraction, training data extraction, and prompt-targeted attacks. The authors examine various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques exploiting transformer architectures. They organize defense mechanisms into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across deployment scenarios. The survey proposes specialized metrics for evaluating both attack effectiveness and defense performance, addressing challenges specific to generative language models.

## Method Summary
The authors conducted a systematic literature review of model extraction attacks and defenses for large language models, synthesizing findings from 40+ research papers. They categorized attacks based on target objectives (functionality, training data, prompts) and methodologies (API-based distillation, direct querying, parameter recovery, etc.). Defense mechanisms were organized into three protection categories with corresponding evaluation frameworks. The survey developed specialized metrics for assessing attack success and defense effectiveness in generative language model contexts. Comparative analyses across deployment scenarios were performed to identify practical trade-offs between security and model utility.

## Key Results
- Functionality extraction attacks can replicate model behavior with 70-90% accuracy using API-based knowledge distillation
- Training data extraction techniques successfully recover memorized examples with query budgets as low as 10,000 queries
- Architectural defenses and output control strategies show the highest effectiveness against prompt-targeted attacks
- Current evaluation metrics for both attacks and defenses remain underdeveloped and lack standardization

## Why This Works (Mechanism)
Model extraction attacks exploit the fundamental accessibility of language model APIs and the transformer architecture's predictable response patterns. Attackers leverage query access to infer model parameters, training data, or valuable prompt templates through systematic probing. Defenses work by introducing uncertainty in model outputs, limiting query capabilities, or modifying architectural components to obscure sensitive information while maintaining utility.

## Foundational Learning

**Transformer Architecture**: The self-attention mechanism that enables LLMs to process sequential data - needed to understand how prompts can be reverse-engineered; quick check: verify understanding of multi-head attention and positional encoding.

**Knowledge Distillation**: Transfer learning technique where a smaller model learns from a larger one - needed to comprehend functionality extraction methods; quick check: trace the teacher-student model relationship in distillation.

**Membership Inference**: Determining whether specific data points were in training set - needed for understanding training data extraction attacks; quick check: identify the difference between black-box and white-box membership inference.

**Output Probability Distributions**: The softmax layer outputs that provide confidence scores - needed to analyze attack effectiveness metrics; quick check: calculate cross-entropy loss from probability distributions.

**API Rate Limiting**: Query volume constraints imposed by service providers - needed to evaluate practical attack limitations; quick check: calculate maximum query budget under typical rate limits.

## Architecture Onboarding

**Component Map**: Model Server -> Query Interface -> Response Generator -> Security Layer -> Model Weights

**Critical Path**: User Query → Rate Limiter → Input Validator → Model Execution → Output Filter → Response

**Design Tradeoffs**: Security vs. utility (output quality), performance vs. protection (query limits), transparency vs. obscurity (model behavior)

**Failure Signatures**: Unusual query patterns, high-confidence incorrect outputs, prompt template leakage, training data memorization exposure

**First Experiments**:
1. Measure attack success rate under varying query budget constraints
2. Test defense effectiveness against adaptive attacks targeting specific vulnerabilities
3. Evaluate model utility degradation when implementing different security mechanisms

## Open Questions the Paper Calls Out

The survey identifies several critical research gaps: the need for integrated attack methodologies that combine functionality extraction with prompt stealing, adaptive defense mechanisms that respond to evolving attack patterns, and comprehensive evaluation frameworks that balance security with model utility. The authors emphasize the lack of standardized benchmarks for comparing attack effectiveness across different model architectures and sizes. They also highlight the need for empirical validation of defense strategies against real-world adversarial scenarios with practical query limitations.

## Limitations

- Current evaluation metrics for both attacks and defenses remain underdeveloped with limited standardized benchmarks
- Effectiveness assessments rely heavily on controlled experimental settings that may not capture real-world adversarial scenarios
- The survey's focus on transformer-based architectures limits applicability to other model types without validation

## Confidence

- High confidence: The categorization of model extraction attacks into functionality, training data, and prompt-targeted types
- Medium confidence: The effectiveness rankings of defense mechanisms across different deployment scenarios
- Low confidence: The proposed metrics for evaluating attack success and defense effectiveness

## Next Checks

1. Conduct controlled experiments comparing defense effectiveness against adaptive attacks that specifically target identified weaknesses in current protection mechanisms
2. Implement standardized evaluation benchmarks that enable direct comparison of attack success rates across different model architectures and sizes
3. Perform systematic analysis of query budget constraints in real-world API deployments to validate the practical limitations of various attack methodologies