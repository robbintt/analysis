---
ver: rpa2
title: Attention-Only Transformers via Unrolled Subspace Denoising
arxiv_id: '2506.03790'
source_url: https://arxiv.org/abs/2506.03790
tags:
- representations
- token
- each
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a minimalistic transformer architecture consisting
  of only self-attention layers with skip connections, designed to denoise token representations
  modeled as a mixture of low-rank Gaussians. By unrolling iterative denoising operations
  via multi-head subspace self-attention, the authors construct an interpretable transformer
  architecture that achieves denoising performance at a linear rate in terms of signal-to-noise
  ratio improvement per layer.
---

# Attention-Only Transformers via Unrolled Subspace Denoising

## Quick Facts
- **arXiv ID:** 2506.03790
- **Source URL:** https://arxiv.org/abs/2506.03790
- **Reference count:** 40
- **Primary result:** Attention-only transformer achieves comparable performance to standard transformers on ImageNet and language tasks while using fewer parameters

## Executive Summary
This paper proposes a minimalistic transformer architecture consisting of only self-attention layers with skip connections, designed to denoise token representations modeled as a mixture of low-rank Gaussians. By unrolling iterative denoising operations via multi-head subspace self-attention, the authors construct an interpretable transformer architecture that achieves denoising performance at a linear rate in terms of signal-to-noise ratio improvement per layer. Experimental results on vision (ImageNet) and language tasks (language modeling, in-context learning) demonstrate that this attention-only transformer achieves performance comparable to standard transformer architectures like GPT-2 and ViT, while using significantly fewer parameters (e.g., 22M vs 39M for vision tasks). The work provides theoretical justification for the effectiveness of self-attention as a denoising operator and offers insights into the role of MLP layers in transformers.

## Method Summary
The proposed attention-only transformer (AoT) architecture consists of self-attention layers with skip connections that implement iterative subspace denoising. Each layer updates token representations as Z^(l+1) = Z^(l) + η · MSSA(Z^(l)), where MSSA computes similarity between tokens projected onto learned subspace bases U_k. The architecture removes MLP layers while maintaining competitive performance through theoretical SNR improvement guarantees. Training uses standard optimizers (Lion for vision, AdamW for language) with hyperparameters specified for each task.

## Key Results
- AoT-MHSA-L achieves 0.5 nT5 higher accuracy than standard GPT-2 on WikiText-2 with 22M parameters vs 39M
- AoT-MHSA-V achieves 78.3% ImageNet accuracy with 24 layers, matching CRATE's performance
- MLPs accelerate training but don't improve final zero-shot performance in AoT architectures
- Linear SNR improvement rate verified experimentally on synthetic data matching theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-attention operates as a subspace denoising operator that projects noisy token representations onto their corresponding low-dimensional subspaces.
- **Mechanism:** The multi-head subspace self-attention (MSSA) computes similarity between tokens projected onto each subspace via U_k^T Z, converts similarities to a membership distribution through softmax with thresholding, and aggregates tokens weighted by membership. This amplifies intra-subspace signal while suppressing cross-subspace noise.
- **Core assumption:** Token representations follow a mixture of low-rank Gaussian distributions (Definition 1), with each cluster's signal confined to an orthogonal subspace U_k.
- **Evidence anchors:**
  - [abstract]: "To compress these noisy token representations, an associated denoising operation naturally takes the form of a multi-head (subspace) self-attention."
  - [section 2.3, Eq. 4]: Formal definition of MSSA showing how U_k U_k^T Z projects and φ(·) thresholds membership.
  - [corpus]: Weak direct evidence—related papers discuss unrolling and interpretability but do not independently validate subspace denoising as the primary attention mechanism.
- **Break condition:** If token representations do not approximately lie on a union of low-dimensional subspaces, or if subspaces are highly non-orthogonal (violating U_k^T U_j ≈ 0 for k ≠ j), the denoising decomposition fails and cross-subspace interference dominates.

### Mechanism 2
- **Claim:** Each layer improves the signal-to-noise ratio (SNR) of token representations at a linear rate with respect to layer depth.
- **Mechanism:** Under the thresholded softmax φ, intra-subspace token similarities are preserved (soft-max values exceed threshold τ) while inter-subspace noise similarities are suppressed (values below τ). This yields multiplicative SNR growth: SNR(Z_k^{l+1}) = (1 + ητ) · SNR(Z_k^l).
- **Core assumption:** Noise level satisfies δ ≲ √(log N)/√p and subspace dimensionality p ≳ log N, ensuring sufficient concentration for reliable similarity discrimination.
- **Evidence anchors:**
  - [abstract]: "it improves the signal-to-noise ratio of token representations at a linear rate with respect to the number of layers."
  - [section 3.2, Theorem 1]: Formal proof showing SNR(Z_k^{l+1}) = (1 + ητ) SNR(Z_k^l) with probability 1 - KLN^{-Ω(1)} under stated conditions.
  - [corpus]: No independent verification of the linear rate claim in related literature; remains theoretically proposed within this work.
- **Break condition:** When noise level δ exceeds the theoretical threshold, or when the number of tokens N is too small for statistical concentration, the similarity-based discrimination fails and SNR improvement saturates or degrades.

### Mechanism 3
- **Claim:** Stacking attention-only layers unrolls an iterative subspace denoising algorithm, progressively compressing representations toward their supporting subspaces.
- **Mechanism:** Each layer implements one denoising step: Z^{(l+1)} = Z^{(l)} + η · MSSA(Z^{(l)}). Skip connections preserve gradient flow and accumulated signal while allowing iterative refinement. The subspace bases {U_k^{(l)}} are learned via backpropagation rather than pre-specified.
- **Core assumption:** The forward denoising structure provides a good inductive bias; backpropagation can discover subspace bases aligned with data structure even when initialized randomly.
- **Evidence anchors:**
  - [abstract]: "By unrolling such iterative denoising operations into a deep network, we arrive at a highly compact architecture that consists of only self-attention operators with skip connections."
  - [section 2.1]: "this approach constructs each layer of a neural network according to a step of an iterative optimization algorithm."
  - [corpus]: Moderate support—"Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps" similarly frames transformers as unrolled inference, providing independent conceptual alignment.
- **Break condition:** If learned subspace bases do not converge to meaningful structures, or if depth L is insufficient for the iterative process to reach adequate SNR, the architecture underperforms relative to standard transformers with MLPs.

## Foundational Learning

- **Concept: Union of Subspaces Model**
  - **Why needed here:** The entire theoretical framework assumes data lies on or near multiple low-dimensional linear subspaces; understanding this geometric structure is essential for grasping why projection-based denoising works.
  - **Quick check question:** Given a dataset of face images under varying illumination, explain why they might approximately lie on a low-dimensional subspace, and how multiple such subspaces could capture different identities.

- **Concept: Signal-to-Noise Ratio in Projections**
  - **Why needed here:** SNR is the core metric for quantifying denoising efficiency; you must understand how orthogonal projections affect signal and noise differently to follow Theorem 1.
  - **Quick check question:** If a d-dimensional vector has signal component in a p-dimensional subspace (p ≪ d) plus isotropic Gaussian noise, what happens to SNR when you project onto that subspace?

- **Concept: Algorithm Unrolling for Deep Networks**
  - **Why needed here:** The architecture is explicitly constructed by unrolling iterative denoising; this design paradigm explains the layer structure and provides interpretability.
  - **Quick check question:** How would you convert one iteration of the update rule x_{t+1} = x_t - η∇f(x_t) into a neural network layer? What becomes the learnable parameter?

## Architecture Onboarding

- **Component map:** Input token embeddings Z^(0) ∈ R^(d×N) → LayerNorm → MSSA/MHSA → Skip connection addition → Output Z^(L) → Task head
- **Critical path:**
  1. Initialize token embeddings and subspace bases {U_k^(0)}
  2. Train via backpropagation to learn bases that align with data subspaces
  3. Tune step size η and threshold τ for stable multiplicative SNR growth
  4. Ensure sufficient depth L to reach target SNR for downstream task
- **Design tradeoffs:**
  - **Depth vs. parameters:** Linear SNR growth favors depth, but AoT requires ~2× layers to match standard transformer performance (Table 4: 24 vs 12 layers for comparable size)
  - **Threshold τ:** Must lie in [1/2, 1/(1 + Ne^(-9p/32))]; too low admits noise, too high discards valid tokens
  - **MLP removal:** Halves parameters but slows convergence; MLPs accelerate training but not final zero-shot performance (Figure 5, Table 3)
  - **Head count K:** Assumption: K should match number of true subspaces; mismatch causes suboptimal clustering
- **Failure signatures:**
  - **Rank collapse:** Tokens converge to single representation if skip connections removed or τ misconfigured
  - **SNR plateau:** Noise level δ exceeds theoretical bound, stopping linear improvement
  - **Training divergence:** Large η or missing LayerNorm causes gradient explosion
  - **Semantic incoherence:** Learned heads don't capture consistent features (check attention heatmaps as in Figure 7)
- **First 3 experiments:**
  1. **Synthetic validation of Theorem 1:** Sample Z^(0) from mixture of low-rank Gaussians (Definition 1) with known U_k, track SNR(Z_k^l) across layers, verify multiplicative factor (1 + ητ) matches theory.
  2. **MLP ablation study:** Train AoT-MHSA-L and GPT-2 Base side-by-side on OpenWebText, compare training dynamics (Figure 5) and zero-shot benchmarks (Table 3) to quantify MLP contribution to convergence speed vs. final performance.
  3. **Attention head interpretability:** Train AoT-MSSA-V on ImageNet, visualize attention patterns from penultimate layer (following Figure 7), verify that different heads consistently attend to semantically meaningful regions across images.

## Open Questions the Paper Calls Out
- **Open Question 1:** What is the specific mechanistic role of MLP layers in standard transformers if they accelerate training but do not necessarily improve final zero-shot accuracy?
  - **Basis in paper:** [explicit] The authors observe that "incorporating MLP layers can accelerate the training process" but "adding MLP layers to AoT does not improve the zero-shot performance."
  - **Why unresolved:** The paper establishes that attention-only models suffice for final performance but does not fully explain the optimization dynamics that make MLPs beneficial for speed during training.
  - **What evidence would resolve it:** A comparative analysis of the loss landscape geometry and gradient flow in AoT versus standard Transformers during the initial training phases.

## Limitations
- Theoretical framework relies heavily on union-of-subspaces model which may not hold for all real-world data distributions
- Linear SNR improvement rate proven under strict conditions (orthogonality, noise bounds) that may be too restrictive for practical applications
- Claims about MLP unnecessariness based on ablation studies don't fully establish that MLPs serve no purpose beyond training speed

## Confidence
- **High confidence:** Experimental results demonstrating competitive performance on ImageNet and OpenWebText with AoT architectures
- **Medium confidence:** Theoretical SNR improvement rate (Theorem 1) with assumptions about subspace orthogonality and noise concentration
- **Low confidence:** Claim that MLPs are unnecessary for transformer function based on ablation studies showing acceleration vs. final performance

## Next Checks
1. **Empirical SNR validation:** Design a controlled experiment where synthetic token representations are generated from a known union-of-subspaces model (matching Definition 1). Track SNR improvement across AoT layers and verify the predicted multiplicative factor (1 + ητ) holds empirically across different noise levels and subspace configurations.

2. **MLP removal impact analysis:** Systematically vary MLP presence in standard transformers (0%, 50%, 100%) and measure both training speed and final task performance on multiple benchmarks. Quantify the exact contribution of MLPs to convergence rate versus representational capacity, and determine if there are tasks where MLPs are essential rather than merely beneficial.

3. **Subspace interpretability validation:** For AoT trained on ImageNet, conduct a comprehensive analysis of learned subspace bases {U_k} across all layers. Use techniques like singular value decomposition and visualization to determine whether the bases capture semantically meaningful features (e.g., edges, textures, objects) and whether this interpretability degrades with depth or varies by head index.