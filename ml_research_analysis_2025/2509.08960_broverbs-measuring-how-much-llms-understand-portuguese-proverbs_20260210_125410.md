---
ver: rpa2
title: BRoverbs -- Measuring how much LLMs understand Portuguese proverbs
arxiv_id: '2509.08960'
source_url: https://arxiv.org/abs/2509.08960
tags:
- arxiv
- portuguese
- llms
- proverbs
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BRoverbs, a benchmark designed to evaluate
  how well large language models (LLMs) understand Brazilian Portuguese proverbs.
  The dataset contains 193 proverbs, each paired with three short narratives that
  illustrate its meaning.
---

# BRoverbs -- Measuring how much LLMs understand Portuguese proverbs

## Quick Facts
- arXiv ID: 2509.08960
- Source URL: https://arxiv.org/abs/2509.08960
- Authors: Thales Sales Almeida; Giovana Kerche Bonás; João Guilherme Alves Santos
- Reference count: 10
- Key outcome: BRoverbs benchmark reveals larger models understand Portuguese proverbs better than smaller models, regardless of Portuguese-specific training

## Executive Summary
This paper introduces BRoverbs, a benchmark designed to evaluate how well large language models understand Brazilian Portuguese proverbs. The dataset contains 193 proverbs, each paired with three short narratives illustrating its meaning. Two tasks are proposed: Proverb-to-Story (PtS) and Story-to-Proverb (StP), where models must correctly match proverbs to their corresponding stories and vice versa. Commercial models like GPT-4o and Sabiá-3 achieved near-perfect scores (>95%), while open-source models showed varying performance, with larger models (e.g., Qwen 2.5 14B) performing best. Smaller models (e.g., TinyLlama, Tucano) performed close to random chance.

## Method Summary
The BRoverbs benchmark consists of 193 Brazilian proverbs, each paired with three short narratives generated by GPT-4 and validated by humans. Models are evaluated using a 1-fewshot approach on two multiple-choice tasks: matching proverbs to stories (PtS) and matching stories to proverbs (StP). Each task presents 5 options (1 correct, 4 distractors). The dataset is publicly available on HuggingFace, and evaluation was performed using RTX A6000 GPUs with commercial and open-source models ranging from 1.1B to 14B parameters.

## Key Results
- Commercial models (GPT-4o, Sabiá-3) achieved >95% accuracy on both tasks
- Larger open-source models (Qwen 2.5 14B) performed close to commercial models
- Smaller models (TinyLlama 1.1B, Tucano 2.4B) scored near random chance (~20%)
- StP task was systematically easier than PtS across all model sizes
- Portuguese-specialized models (Curió, Tucano) did not outperform general models despite native training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A model's ability to map figurative language to concrete scenarios appears contingent on reaching a scale threshold, rather than just exposure to the target language.
- **Mechanism:** Larger models (e.g., Qwen 2.5 14B) likely utilize distributed semantic representations to link abstract proverbs to narrative outcomes. Smaller models (e.g., TinyLlama 1.1B, Tucano 2.4B) may lack the capacity to resolve non-compositional meanings, defaulting to surface-level keyword matching even when trained extensively on Portuguese data.
- **Core assumption:** The performance plateau observed in smaller, Portuguese-native models (like Tucano) is due to architectural capacity limits rather than solely data sparsity.
- **Evidence anchors:**
  - [abstract] "Smaller models (e.g., TinyLlama, Tucano) performed close to random chance."
  - [section 4.1] "Qwen models clearly outperform their counterparts... with Qwen 2.5 14B reaching performance close to commercial models."
  - [corpus] Related work (*MasalBench*, *Jawaher*) confirms evaluating figurative language in LLMs is an active research area, though BRoverbs specifically highlights the failure of small models regardless of native training.
- **Break condition:** If a sub-2B parameter model trained specifically on proverb datasets achieves high accuracy, the capacity threshold assumption is invalid.

### Mechanism 2
- **Claim:** The task of identifying a proverb from a story (StP) is structurally easier than matching a story to a proverb (PtS) due to recognition vs. recall dynamics.
- **Mechanism:** In StP, the model must recognize a distinct, memorable phrase (the proverb) among distractors. In PtS, the model must generate or verify an abstract narrative implication from a concise phrase, requiring deeper semantic expansion.
- **Core assumption:** Proverbs act as "distinct tokens" or highly salient sequences in the model's latent space, whereas stories are compositionally complex.
- **Evidence anchors:**
  - [section 4.1] "The StP variant... is generally easier... likely because many proverbs are distinct and memorable phrases, making them easier for models to recognize."
  - [section 4.1] "PtS... demands a deeper interpretation of the story’s nuances."
  - [corpus] *Beyond Understanding: Evaluating the Pragmatic Gap...* supports the difficulty of pragmatic processing required for PtS tasks.
- **Break condition:** If specific distractor design (e.g., adversarial stories with similar keywords) equalizes StP and PtS difficulty, the "recognition ease" mechanism fails.

### Mechanism 3
- **Claim:** Mere volume of language-specific pre-training data does not linearly transfer to cultural competence.
- **Mechanism:** Cultural competence (proverb understanding) likely requires specific exposure to idiomatic or cultural corpora, or sufficient general reasoning capabilities (acquired at scale) to infer context. Continued pre-training on generic text (e.g., Curió) without targeted cultural signals shows negligible gains.
- **Core assumption:** Generic Portuguese web text (Common Crawl) contains insufficient density of cultural proverbs to shift model weights meaningfully for this task.
- **Evidence anchors:**
  - [section 4.2] "Curió-1.1B... continued pretraining in Portuguese, revealed no significant gains... despite its exposure to 150B additional tokens."
  - [section 5] "A more direct approach... would be to measure the actual presence of Brazilian proverbs in large Portuguese corpora."
  - [corpus] *Advancing Equitable AI* supports the disparity between data volume and effective cultural representation.
- **Break condition:** If analysis of the Curió training corpus reveals the proverbs were explicitly excluded or filtered out, the "data quality" argument would need re-evaluation regarding "insufficient density."

## Foundational Learning

- **Concept:** **Non-compositional Semantics (Idiomaticity)**
  - **Why needed here:** Proverbs differ from literal text; their meaning cannot be derived by simply summing the definitions of their words. Understanding BRoverbs results requires grasping that models must "learn" the mapping of the whole phrase to a concept (e.g., "Don't judge a book by its cover" ≠ books/covers).
  - **Quick check question:** Can a model that perfectly predicts next tokens in formal text still fail to explain why "batatinha quando nasce" relates to spreading/expansion?

- **Concept:** **Evaluation Contamination (Data Leakage)**
  - **Why needed here:** Since proverbs are static cultural artifacts and GPT-4o was used to generate stories, there is a risk the model "memorized" the associations during training. The paper argues against this for GPT-4o's specific advantage, but the concept is central to interpreting high scores.
  - **Quick check question:** Did the authors test GPT-4o on *novel*, unpublished proverbs to ensure it is reasoning rather than recalling?

- **Concept:** **Zero-shot vs. Few-shot Evaluation**
  - **Why needed here:** The paper uses a 1-fewshot setting. Understanding this methodology is crucial for reproducing the results and distinguishing it from fine-tuning.
  - **Quick check question:** Does providing a single example (1-fewshot) teach the model the *meaning* of proverbs, or merely the *format* of the multiple-choice question?

## Architecture Onboarding

- **Component map:**
  Source (15 curated websites) -> Generator (GPT-4) -> Validator (Human review) -> Distractor Sampler -> Evaluator (Benchmark script) -> Models (Open-source vs Commercial)

- **Critical path:**
  1. **Collection:** Web scrape → Lexical clustering
  2. **Generation:** GPT-4 prompt (Proverb → 3 Stories)
  3. **Filtering:** Manual check for 193 valid proverbs
  4. **Distractor Sampling:** Random selection of 4 distractors (validated manually to ensure uniqueness)
  5. **Inference:** Run PtS and StP tasks; measure accuracy

- **Design tradeoffs:**
  - **Synthetic vs. Authentic Data:** Using GPT-4 to write stories ensures narrative clarity but risks "modeling the model" (synthetic artifacts). The authors chose this over scraping news/articles to ensure clear, direct proverb illustrations.
  - **Manual Validation Cost:** The authors manually checked 12% of questions (those missed by models) and distractors. This limits dataset scale (193 proverbs) in favor of high-quality distractors.

- **Failure signatures:**
  - **Random Chance Baseline:** Accuracy ≈ 20% (1 in 5). Observed in TinyLlama, Tucano, and Curió.
  - **StP >> PtS Gap:** A large gap indicates a model relies on recognizing the proverb string (StP) but cannot reason about its application in a narrative (PtS).

- **First 3 experiments:**
  1. **Sanity Check (Random Baseline):** Run TinyLlama-1.1B on the benchmark to confirm ~20% accuracy, validating the distractor quality.
  2. **Language Specificity Ablation:** Compare Llama-3-8B vs. Sabiá-7B vs. Qwen-2.5-7B. Does "Portuguese-specialized" (Sabiá) beat "General High-Resource" (Qwen)? (Result: No, Qwen wins, signaling scale/data quality matters more than label).
  3. **Data Contamination Check:** Test GPT-4o (story generator) vs. Claude-3.5-Sonnet (blind to generation). If scores are similar (>95%), the generation process likely didn't "leak" the specific answers into GPT-4o's training memory.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent are specific Brazilian proverbs actually represented in the large Portuguese corpora (e.g., Gigaverbo, Clueweb22) used to train models?
- **Basis in paper:** [explicit] The authors state in Section 5 that "A more direct approach for future work would be to measure the actual presence of Brazilian proverbs in large Portuguese corpora... used for training," rather than using data volume as a proxy.
- **Why unresolved:** The study currently assumes that increased Portuguese training data correlates with proverb exposure, but the failure of Portuguese-heavy models like Curió to improve suggests the proverbs may be absent from the training data itself.
- **What evidence would resolve it:** A quantitative audit of common Portuguese training datasets to identify the frequency and distribution of the exact proverbial expressions contained in BRoverbs.

### Open Question 2
- **Question:** Does targeted pretraining on idiomatic expressions and cultural narratives enhance proverb comprehension more effectively than general language exposure?
- **Basis in paper:** [explicit] The Conclusion explicitly notes that "Future work should explore whether specific exposure to idiomatic expressions and cultural narratives could enhance proverb comprehension."
- **Why unresolved:** The paper demonstrates that general continued pretraining on Portuguese text (Curió-1.1B) yielded no performance gains, leaving the efficacy of targeted cultural fine-tuning unknown.
- **What evidence would resolve it:** A comparison of models fine-tuned on general news/wikipedia text versus those fine-tuned specifically on folklore and idiom datasets, evaluated on the BRoverbs benchmark.

### Open Question 3
- **Question:** Is there a specific parameter capacity threshold required for models to solve the proverb-to-story matching task, regardless of language-specific pretraining?
- **Basis in paper:** [inferred] Section 4.2 suggests that "models may need to reach a certain capacity threshold to handle our task effectively," noting that smaller models (1.1B–2.4B parameters) performed at random chance even when heavily trained on Portuguese data.
- **Why unresolved:** The study observed that larger general models (Qwen) outperformed smaller language-specific models, but did not isolate whether this was due to reasoning capabilities inherent to scale or data composition.
- **What evidence would resolve it:** A controlled ablation study testing models across a continuous range of parameter sizes (e.g., 1B to 14B) using identical training data to identify the inflection point where performance exceeds random chance.

## Limitations

- **Synthetic Data Validity:** The paper relies entirely on GPT-4-generated stories, which may be too clean or literal, potentially inflating model performance on this synthetic dataset while underestimating real-world challenges.
- **Distractor Quality:** Manual review was performed on distractors, but the methodology for ensuring semantic separation between correct answers and plausible alternatives is not fully specified.
- **Generation Contamination:** Using GPT-4o for both story generation and evaluation creates potential data leakage concerns, though the paper claims similar performance to Claude-3.5-Sonnet.

## Confidence

**High Confidence**
- Larger models consistently outperform smaller ones on this task
- Commercial models (GPT-4o, Sabiá-3) achieve near-perfect scores
- StP task is systematically easier than PtS across all model sizes

**Medium Confidence**
- Portuguese-specific pre-training data doesn't guarantee cultural competence
- Scale thresholds matter more than language-specific training for this task
- The performance gap between StP and PtS reflects genuine reasoning differences

**Low Confidence**
- The specific architectural features that enable proverb understanding in large models
- Whether synthetic narratives accurately represent authentic cultural usage
- The long-term stability of these performance differences as models evolve

## Next Checks

1. **Cross-Lingual Generalization Test:** Evaluate the same benchmark on Spanish or Italian proverbs (similar cultural/linguistic distance to Portuguese). If models trained on Portuguese perform similarly to multilingual models, it suggests the task tests general figurative reasoning rather than Portuguese-specific knowledge.

2. **Human Baseline Validation:** Have native Portuguese speakers complete the PtS and StP tasks. Compare human accuracy rates and response times to model performance. This would establish whether high model scores reflect genuine comprehension or exploit patterns in the synthetic narratives.

3. **Adversarial Distractor Analysis:** Create a new benchmark version with intentionally challenging distractors that are semantically related but incorrect (e.g., stories that fit the proverb's general theme but contradict its specific meaning). Re-evaluate top-performing models to determine if their accuracy stems from deep understanding or superficial pattern matching.