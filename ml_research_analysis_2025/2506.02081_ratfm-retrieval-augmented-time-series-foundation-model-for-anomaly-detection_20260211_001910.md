---
ver: rpa2
title: 'RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection'
arxiv_id: '2506.02081'
source_url: https://arxiv.org/abs/2506.02081
tags:
- time
- series
- anomaly
- ratfm
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RATFM, a retrieval-augmented time series foundation
  model for anomaly detection. It addresses the challenge that time series foundation
  models lack the ability to interpret or utilize examples or instructions during
  test-time adaptation.
---

# RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection

## Quick Facts
- **arXiv ID:** 2506.02081
- **Source URL:** https://arxiv.org/abs/2506.02081
- **Reference count:** 40
- **Primary result:** RATFM achieves comparable performance to in-domain fine-tuning for anomaly detection without domain-specific training, using retrieval-augmented examples.

## Executive Summary
This paper addresses the challenge that time series foundation models lack the ability to interpret or utilize examples during test-time adaptation for anomaly detection. The proposed RATFM method retrieves a similar example from the same domain and concatenates it with the target input, enabling the model to leverage retrieved examples as references for forecasting. The approach consistently outperforms baseline models including Zero-shot and Out-domain fine-tuning across all domains, achieving performance comparable to in-domain fine-tuning while avoiding domain-dependent training. Additionally, a simple moving average-based post-processing method significantly improves anomaly detection performance by mitigating peak-related false positives.

## Method Summary
RATFM fine-tunes time series foundation models (Time-MoE or Moment) on out-of-domain data using a concatenation schema: `[Example Input] ⊕ [Example Future] ⊕ [Target Input]`. The retriever uses cross-correlation to find the most similar series from the same domain. During inference, the model forecasts the next 96 steps and computes raw deviation scores, which are then smoothed with a Simple Moving Average (SMA) filter using a window size equal to the estimated period via Fourier Transform. The method is evaluated using VUS-ROC, VUS-PR, and Point-wise F1 metrics with a threshold of μ + 3σ of test scores.

## Key Results
- RATFM achieves performance comparable to in-domain fine-tuning while avoiding domain-dependent training
- Simple Moving Average post-processing significantly improves VUS-ROC (e.g., 65.7% → 68.7% for Time-MoE)
- The approach consistently outperforms baseline models including Zero-shot and Out-domain fine-tuning across all domains
- Cross-correlation retrieval provides examples with average similarity of 0.974 to forecast targets

## Why This Works (Mechanism)

### Mechanism 1
Time series foundation models can perform test-time adaptation using retrieved examples, but only if explicitly trained on a specific concatenation schema. The model is fine-tuned on inputs structured as `[Example Input] ⊕ [Example Future] ⊕ [Target Input]`, forcing the attention mechanism to attend to the "Example Future" as a reference pattern when generating the forecast for the "Target Input." Core assumption: the model possesses sufficient capacity to learn a domain-independent mapping where "similar input patterns imply similar future trajectories" across different time series domains.

### Mechanism 2
High-similarity retrieval via cross-correlation provides superior references for forecasting compared to the target's own historical context in cyclic data. Anomaly detection relies on deviation from "normal," and in Zero-shot settings, the model may fail to forecast periodic peaks accurately, causing high deviation (false positives). RATFM retrieves an example where that specific peak was "normal," lowering the deviation score for the target's peak and isolating true anomalies. Core assumption: the retrieved example contains a segment with a similar pattern that was successfully forecasted during training/inference.

### Mechanism 3
Simple Moving Average (SMA) post-processing reduces false positives caused by periodic signal peaks. Raw anomaly scores are computed as absolute deviation |x̂ - x|. At high-amplitude periodic peaks, even small fractional errors result in large absolute deviations. Smoothing these scores over a window (equal to the estimated period) dampens these transient spikes, preventing them from triggering the threshold. Core assumption: anomalies manifest as sustained deviations or errors that survive the averaging window, whereas peak errors are transient.

## Foundational Learning

- **Concept: Forecast-based Anomaly Detection**
  - **Why needed here:** The entire RATFM architecture is built on the premise that "anomaly = deviation from forecast." You cannot understand the retrieval logic without understanding that the goal is to minimize deviation for normal data.
  - **Quick check question:** If the model perfectly reconstructed the input (including the anomaly), would this mechanism still work? (Answer: No, see Reconstruction failure in Appendix E).

- **Concept: Cross-Correlation**
  - **Why needed here:** This is the specific distance metric used to retrieve the "example." Unlike Euclidean distance, cross-correlation normalizes for magnitude, focusing on shape similarity which is crucial for cyclic time series (ECG, Gait).
  - **Quick check question:** Why would Euclidean distance fail for two ECG signals with the same shape but different amplitudes?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The paper attempts to port ICL (standard in LLMs) to Time Series. You must understand that ICL relies on "demonstrations" to shift the model's output distribution without weight updates.
  - **Quick check question:** Why does the paper claim standard TSFMs fail at ICL? (Answer: They lack the semantic "instruction" tokens found in NLP).

## Architecture Onboarding

- **Component map:** Retriever -> Pre-processor (concatenates tensors) -> Backbone (Time-MoE or Moment) -> Post-processor (SMA filter + Thresholding)
- **Critical path:** The concatenation format is the "secret sauce." Do not feed the example and target as separate batches; they must be a single sequence tensor for the attention mechanism to link them.
- **Design tradeoffs:**
  - **Forecast vs. Reconstruction:** RATFM works for Forecasting (Time-MoE) but fails for Reconstruction (Moment) because reconstruction models copy the anomaly, resulting in low deviation scores.
  - **Latency:** Retrieval adds a search step. For real-time detection, the database must be indexed for fast similarity search.
- **Failure signatures:**
  - **"RATFM w/o training":** Performance drops to near Zero-shot levels, proving you cannot just "plug in" examples; the model must be fine-tuned to understand the retrieval schema.
  - **False Positives from Lag:** If the retrieved example is shifted in time relative to the target, the forecast may be temporally misaligned, triggering false anomaly signals.
- **First 3 experiments:**
  1. **Baseline Verification:** Run Time-MoE in Zero-shot mode on a single UCR domain to confirm the "high deviation at peaks" problem.
  2. **Ablation Check:** Implement the "RATFM w/o training" setup (feed examples to a vanilla model) to confirm performance drops.
  3. **Retrieval Robustness:** Reduce the candidate pool size to 25% to see how much the system degrades with sparse data availability.

## Open Questions the Paper Calls Out

- Can the retrieval-augmented approach used for anomaly detection be effectively generalized to other time series tasks?
- Can time series foundation models be adapted to understand natural language instructions alongside numerical examples?
- How can the retrieval mechanism be improved to account for future trajectory similarity rather than just input similarity?
- How can forecast-based anomaly detection models be made robust to anomalies present in the input context window?

## Limitations

- The method relies on retrieving examples from the same domain, assuming sufficient labeled database per domain exists
- Cross-correlation assumes cyclic patterns have stable periods and may fail on non-stationary or aperiodic series
- The paper uses Fourier analysis for period estimation but provides no sensitivity analysis for window size choices

## Confidence

- **High Confidence:** The mechanism of concatenation-based training (Mechanism 1) and the effectiveness of SMA post-processing (Mechanism 3) are well-supported by ablation studies and empirical results
- **Medium Confidence:** The claim that cross-correlation retrieval provides superior references (Mechanism 2) is supported by similarity metrics but lacks analysis of failure cases
- **Low Confidence:** The claim that RATFM works for forecasting but not reconstruction backbones lacks detailed architectural justification

## Next Checks

1. **Retrieval Pool Sensitivity:** Systematically vary the size and diversity of the retrieval database (e.g., 10%, 50%, 100% of available examples) to quantify performance degradation when examples are sparse or from different sub-domains.

2. **Cross-Correlation Failure Analysis:** Construct test cases where retrieved examples have high input similarity but divergent future trajectories (e.g., different cycle phases). Measure false positive rates to validate the assumption that "similar input implies similar future."

3. **SMA Window Robustness:** Replace Fourier-based period estimation with fixed windows (1, 2, 5x estimated period) and random windows. Compare performance to determine if the method is sensitive to accurate period detection or if window size is a hyperparameter that can be tuned independently.