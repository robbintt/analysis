---
ver: rpa2
title: 'NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with
  Linguistic Insights and Temporal Trends'
arxiv_id: '2512.22823'
source_url: https://arxiv.org/abs/2512.22823
tags:
- emotion
- sentiment
- nepali
- classification
- sadness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NepEMO, a novel dataset for multi-label emotion
  and sentiment analysis in Nepali social media text, addressing the lack of resources
  for emotion detection in low-resource languages. The dataset comprises 4,462 Reddit
  posts annotated for five emotions (fear, anger, sadness, joy, depression) and three
  sentiments (positive, negative, neutral).
---

# NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends

## Quick Facts
- **arXiv ID**: 2512.22823
- **Source URL**: https://arxiv.org/abs/2512.22823
- **Reference count**: 40
- **Primary result**: NepEMO introduces the first multi-label emotion and sentiment dataset for Nepali, showing transformer models outperform traditional ML and deep learning approaches

## Executive Summary
This paper introduces NepEMO, a novel dataset for multi-label emotion and sentiment analysis in Nepali social media text, addressing the lack of resources for emotion detection in low-resource languages. The dataset comprises 4,462 Reddit posts annotated for five emotions (fear, anger, sadness, joy, depression) and three sentiments (positive, negative, neutral). The authors employ traditional ML models (SVM, KNN, RF, etc.), deep learning architectures (CNN, LSTM, Bi-LSTM), and transformer-based models (XLM-RoBERTa, DistilBERT) for classification tasks. Results show that transformer models, particularly XLM-RoBERTa, consistently outperform other approaches, achieving high accuracy and F1-scores across emotion and sentiment classification. The study also provides insights into linguistic patterns, temporal trends, and co-occurrence of emotions, contributing valuable resources for NLP research in Nepali and similar low-resource languages.

## Method Summary
The authors created NepEMO by collecting 4,462 Reddit posts in Nepali, which were manually annotated for five emotions and three sentiment categories using a multi-label scheme. For classification, they implemented a comprehensive evaluation pipeline including traditional machine learning models (SVM, KNN, Random Forest, Logistic Regression), deep learning architectures (CNN, LSTM, Bi-LSTM), and transformer-based models (XLM-RoBERTa, DistilBERT). The models were trained and evaluated using standard metrics including accuracy, precision, recall, and F1-score. The study also conducted linguistic analysis examining word frequencies, n-grams, and temporal patterns across different emotion categories.

## Key Results
- Transformer-based models, particularly XLM-RoBERTa, achieved the highest performance across all emotion and sentiment classification tasks
- XLM-RoBERTa consistently outperformed traditional ML and deep learning approaches, demonstrating the effectiveness of pre-trained multilingual models for low-resource languages
- The dataset reveals distinct linguistic patterns and temporal trends in how different emotions are expressed in Nepali social media discourse

## Why This Works (Mechanism)
The success of transformer models on this low-resource language task stems from their ability to leverage pre-training on massive multilingual corpora, which includes Nepali text. XLM-RoBERTa's architecture captures contextual relationships and semantic nuances that are particularly important for emotion detection, where subtle linguistic cues can indicate different emotional states. The multi-label nature of the dataset reflects the complexity of real emotional expression, where posts often contain mixed emotions, making transformer models' contextual understanding especially valuable.

## Foundational Learning
- **Multi-label classification**: Why needed - emotions often co-occur in text; Quick check - verify each post can have multiple emotion labels assigned
- **Transformer architecture**: Why needed - captures contextual relationships crucial for emotion detection; Quick check - examine attention weights for emotion-related tokens
- **Cross-lingual transfer learning**: Why needed - enables effective modeling of low-resource languages using pre-trained multilingual models; Quick check - test model performance on similar low-resource languages
- **Linguistic feature analysis**: Why needed - identifies patterns specific to emotional expression in Nepali; Quick check - compare word frequency distributions across emotion categories
- **Temporal trend analysis**: Why needed - reveals how emotional expression varies over time; Quick check - visualize emotion distributions across different time periods
- **Sentiment-Emotion relationship**: Why needed - emotions and sentiments are related but distinct constructs; Quick check - analyze correlation between sentiment and emotion labels

## Architecture Onboarding
**Component Map**: Raw text -> Preprocessing -> Feature Extraction (Traditional/Deep/Transformer) -> Classification -> Evaluation
**Critical Path**: Nepali Reddit posts → Annotation → Model Training → Performance Evaluation → Linguistic Analysis
**Design Tradeoffs**: Multi-label annotation increases realism but complicates evaluation; Reddit-only data limits generalizability but ensures platform consistency
**Failure Signatures**: Poor performance on rare emotions indicates insufficient training data; high confusion between similar emotions suggests feature overlap
**3 First Experiments**:
1. Train baseline traditional ML models (SVM, Random Forest) with TF-IDF features
2. Implement CNN and LSTM architectures with word embeddings
3. Fine-tune XLM-RoBERTa with emotion-specific classification heads

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 4,462 posts may limit generalizability to broader Nepali social media discourse
- Reddit-only data source introduces platform-specific biases not representative of general Nepali language usage
- Multi-label annotation scheme complicates performance evaluation and makes direct comparison with single-label datasets challenging

## Confidence
- **High**: Dataset creation value, relative performance rankings of model architectures
- **Medium**: Specific F1-score values, absolute performance differences between models
- **Low**: Generalizability to other Nepali text domains beyond Reddit

## Next Checks
1. Evaluate model performance on additional Nepali social media platforms (Facebook, Twitter) to assess cross-platform generalizability
2. Conduct inter-annotator agreement analysis to quantify annotation reliability for the multi-label scheme
3. Test few-shot learning approaches to determine minimum dataset requirements for achieving similar performance levels