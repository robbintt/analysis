---
ver: rpa2
title: 'd$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching'
arxiv_id: '2509.23094'
source_url: https://arxiv.org/abs/2509.23094
tags:
- uni00000013
- tokens
- uni00000048
- decoding
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Dual aDaptive Cache (d\xB2Cache), a training-free\
  \ approximate KV cache framework for accelerating diffusion-based large language\
  \ model (dLLM) inference. d\xB2Cache addresses the inefficiency of dLLMs caused\
  \ by bidirectional attention, which prevents standard KV caching."
---

# d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching

## Quick Facts
- **arXiv ID:** 2509.23094
- **Source URL:** https://arxiv.org/abs/2509.23094
- **Reference count:** 18
- **Key result:** 3.2×–4.0× inference speedup over vanilla decoding while maintaining or improving accuracy on LLaDA and Dream models

## Executive Summary
d$^2$Cache introduces a training-free framework for accelerating diffusion-based LLM (dLLM) inference through approximate KV caching. Unlike autoregressive models, dLLMs use bidirectional attention which prevents standard KV caching. The method employs a two-stage fine-grained token selection strategy: certainty prior-guided selection for masked tokens and attention-aware selection for prompt/decoded tokens. Experiments show 3.2×–4.0× speedup over vanilla decoding with maintained or improved accuracy across multiple benchmarks.

## Method Summary
d$^2$Cache is a training-free approximate KV cache framework that addresses the incompatibility of standard KV caching with bidirectional attention in dLLMs. At each decoding step, it computes KV states for all tokens but selectively updates only a subset. Stage 1 uses certainty prior-guided selection to identify masked tokens likely to be decoded soon, based on prediction confidence and local context density. Stage 2 uses attention-aware selection to identify salient prompt and decoded tokens via attention rollout scores. The method achieves quasi left-to-right generation and enables efficient KV state reuse while maintaining generation quality.

## Key Results
- Achieves 3.2×–4.0× inference speedup over vanilla decoding on LLaDA and Dream models
- Maintains or improves accuracy on GSM8K, MBPP, HumanEval, and Math-500 benchmarks
- Outperforms concurrent baselines like dLLM-Cache and Fast-dLLM in both speed and accuracy
- Certainty prior-guided decoding produces more sequential output order compared to vanilla confidence decoding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KV states of masked tokens exhibit high similarity across most decoding steps, enabling safe caching and reuse.
- **Mechanism:** The model computes KV states for all tokens at each step. For masked tokens not about to be decoded, their KV states are reused from cache. Only KV states for tokens identified as needing updates are recomputed.
- **Core assumption:** Masked token representations evolve slowly through most of decoding, changing significantly only in final steps.
- **Evidence anchors:** The paper states that KV states "often exhibit high similarity across consecutive decoding steps," supported by related work like dLLM-Cache observing similar patterns.

### Mechanism 2
- **Claim:** A two-stage fine-grained selection strategy effectively identifies which tokens' KV states must be updated.
- **Mechanism:** Stage 1 targets masked tokens using certainty prior (combining prediction confidence and local context density). Stage 2 targets prompt/decoded tokens using attention rollout to identify critical tokens for KV state refresh.
- **Core assumption:** Decoding order is localized and attention distribution is highly uneven in dLLMs.
- **Evidence anchors:** Empirical evidence shows dLLMs tend to decode masked tokens near previously decoded tokens, and queries attend to a small subset of key positions.

### Mechanism 3
- **Claim:** Certainty prior decoding provides more reliable quasi left-to-right generation than standard confidence-based decoding.
- **Mechanism:** Instead of decoding the masked token with highest single prediction confidence, d2Cache decodes the token with highest certainty prior, which incorporates local context density to promote sequential generation.
- **Core assumption:** Sequential decoding mitigates premature overconfidence and leads to more coherent text.
- **Evidence anchors:** The paper shows certainty prior-guided decoding yields more natural left-to-right generation compared to NAR decoding's "U-shaped" trajectory.

## Foundational Learning

- **Bidirectional vs. Causal Attention**: Why needed: dLLMs use bidirectional attention, preventing standard KV caching used in ARMs. Quick check: Why can a standard autoregressive LLM reuse its KV cache while a standard diffusion LLM cannot?

- **KV State Dynamics**: Why needed: The method relies on the empirical observation that KV states for masked tokens evolve in a specific three-phase pattern. Quick check: What are the three phases a masked token's KV state goes through during decoding?

- **Attention Rollout**: Why needed: This technique identifies the most important prompt and decoded tokens for KV state updates. Quick check: How does the attention rollout algorithm help determine which non-masked tokens should have their KV states recomputed?

## Architecture Onboarding

- **Component map**: Model attention layers -> Token Selector (two-stage selection) -> Cache Manager (store/retrieve KV tensors) -> Decoding scheduler

- **Critical path**: At each step t: 1) Forward pass produces logits and attention scores. 2) Stage 1 Selector computes certainty priors for masked tokens, selects top-k. 3) Stage 2 Selector computes attention rollout for prompt/decoded tokens, selects salient subset. 4) Cache Manager determines which KV states to recompute vs. load from cache. 5) Partial forward pass/update for selected tokens.

- **Design tradeoffs**:
  - Speed vs. Quality: Increasing k or p increases computation but may improve accuracy
  - σ (sigma) controls locality of decoding: smaller σ enforces more sequential generation, larger σ allows more parallel decoding
  - Choice depends on model and task requirements

- **Failure signatures**:
  - Degraded Accuracy: Throughput increases but scores drop significantly (selection thresholds too aggressive)
  - Unstable/U-shaped Decoding: Erratic decoding order (certainty prior calculation misconfigured)
  - No Speedup: High latency (cache manager not correctly implemented or selection overhead consumes gains)

- **First 3 experiments**:
  1. Reproduce baseline comparison on GSM8K with Vanilla, dLLM-Cache, and Fast-dLLM baselines
  2. Ablation on selection stages: run d2Cache using only Stage 1 selection, then only Stage 2 selection
  3. Hyperparameter sensitivity sweep: vary k, p, and σ to plot throughput-accuracy trade-off curves

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does d$^2$Cache performance scale efficiently to larger dLLMs (>8B parameters) or different architectural backbones?
- **Basis:** Experimental evaluation restricted to LLaDA-8B and Dream-v0-7B models
- **Why unresolved:** Balance between memory bandwidth and compute intensity shifts with larger models
- **Evidence needed:** Benchmark results on 30B+ parameter dLLMs measuring latency, throughput, and memory footprint

### Open Question 2
- **Question:** Can d$^2$Cache hyperparameters (k, p, σ) be adapted dynamically during decoding?
- **Basis:** Paper uses fixed hyperparameters (k=32, p=0.1, σ=10.0) without extensive sensitivity analysis
- **Why unresolved:** Static hyperparameters require manual tuning for different datasets/models
- **Evidence needed:** Comparative study with heuristic-based adaptive parameter selection vs. static baseline

### Open Question 3
- **Question:** How does d$^2$Cache interact with KV cache quantization?
- **Basis:** Paper focuses on reusing KV states but doesn't address memory footprint reduction via quantization
- **Why unresolved:** Quantization of cached KV states might degrade generation quality or destabilize selection mechanism
- **Evidence needed:** Experiments applying INT4/INT8 quantization to cached KV states measuring accuracy degradation and memory savings

### Open Question 4
- **Question:** Can d$^2$Cache selection strategy be integrated into training phase of dLLMs?
- **Basis:** Method is explicitly defined as "training-free"
- **Why unresolved:** Training with d$^2$Cache logic might allow model to learn robust representations suited for caching strategy
- **Evidence needed:** Results from dLLM fine-tuned with d$^2$Cache selection mask during forward pass

## Limitations

- **Architecture dependency:** The method's effectiveness depends on specific assumptions about KV state dynamics that may not generalize to all dLLM architectures
- **Hyperparameter sensitivity:** Fixed values for k, p, and σ may not be optimal across different models and tasks
- **Domain specificity:** Performance on specialized domains or long-context scenarios (>2K tokens) remains unverified

## Confidence

**High Confidence Claims:**
- d$^2$Cache achieves 3.2×–4.0× inference speedup on tested benchmarks
- Two-stage selection strategy is novel and effective for tested model families
- Certainty prior decoding produces more sequential output order

**Medium Confidence Claims:**
- Three-phase KV state evolution pattern generalizes across all dLLMs
- σ=10.0 and p=0.1 are universally optimal hyperparameters
- Speedup-quality trade-off curve is stable across task types

**Low Confidence Claims:**
- d$^2$Cache maintains similar trade-offs on non-tested dLLM architectures
- Method generalizes to long-context scenarios without modification
- Certainty prior decoding prevents all forms of premature overconfidence

## Next Checks

1. **Architecture Transfer Validation**: Implement and test d$^2$Cache on at least two additional dLLM architectures not covered in the original paper to verify three-phase KV state pattern and hyperparameter optimality.

2. **Long-Context Stress Test**: Evaluate d$^2$Cache on tasks requiring generation of sequences longer than 1024 tokens to monitor degradation in speedup efficiency and accuracy.

3. **Cross-Domain Robustness Analysis**: Test d$^2$Cache on specialized domains like medical text or legal documents to compare speed-quality trade-off curves against original results.