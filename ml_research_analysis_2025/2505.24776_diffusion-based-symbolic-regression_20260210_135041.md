---
ver: rpa2
title: Diffusion-Based Symbolic Regression
arxiv_id: '2505.24776'
source_url: https://arxiv.org/abs/2505.24776
tags:
- symbolic
- ddsr
- expressions
- diffusion
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a diffusion-based symbolic regression approach
  that leverages discrete diffusion and denoising to generate mathematical expressions
  from data. The method employs a random mask-based diffusion process to gradually
  reconstruct expressions and integrates this with a token-wise Group Relative Policy
  Optimization (GRPO) for efficient reinforcement learning.
---

# Diffusion-Based Symbolic Regression

## Quick Facts
- arXiv ID: 2505.24776
- Source URL: https://arxiv.org/abs/2505.24776
- Reference count: 37
- The paper introduces a diffusion-based symbolic regression approach that significantly improves solution accuracy and symbolic recovery rate compared to existing approaches, including DSR and genetic programming methods.

## Executive Summary
This paper presents a novel symbolic regression method that leverages discrete diffusion and denoising to generate mathematical expressions from data. The approach employs a random mask-based diffusion process to gradually reconstruct expressions and integrates this with a token-wise Group Relative Policy Optimization (GRPO) for efficient reinforcement learning. A long short-term risk-seeking policy is also used to expand the pool of top-performing candidates. Experiments on the SRBench benchmark show that the proposed method significantly improves solution accuracy and symbolic recovery rate compared to existing approaches, including DSR and genetic programming methods.

## Method Summary
The method constructs a random mask-based diffusion and denoising process where only one token is masked at each diffusion step, preserving expression structure while enabling diverse sampling. A transformer with 2D positional encoding (combining token position and diffusion timestep) learns to denoise expressions. The training uses token-wise GRPO with trust region constraints and a long short-term risk-seeking policy that maintains a historical pool of top candidates across epochs. Expressions are sampled via backward diffusion from masked states, with constraints on maximum depth (32 tokens) and constant tokens (10). Constants are optimized using Levenberg-Marquardt after expression generation.

## Key Results
- Achieves 27.3% improvement in accuracy rate (R²>0.999) and 6.5% improvement in symbolic solution rate compared to DSR baseline on SRBench
- Outperforms genetic programming methods and other SR approaches on black-box problems while maintaining favorable trade-off between expression complexity and predictive performance
- Ablation studies show random mask-based diffusion improves accuracy by 2.2% (Feynman) and 35.7% (Strogatz) vs. D3PM, while GRPO converges ~30 epochs faster than RSPG

## Why This Works (Mechanism)

### Mechanism 1: Random Mask-Based Discrete Diffusion
Single-token masking per diffusion step preserves expression structure and enables stable RL integration, outperforming full-sequence perturbation approaches. The forward process uniformly samples one token index to mask at each timestep t, applying Qt = I - diag(eqt) to X0. Generation begins from an all-masked (zero) matrix and reconstructs tokens sequentially. This gradual blurring maintains most structural information while enabling diverse expression sampling.

### Mechanism 2: Token-Wise Group Relative Policy Optimization (GRPO)
Per-token likelihood optimization within a trust region stabilizes RL training and accelerates convergence relative to standard risk-seeking policy gradients. For each top-α% expression τ(i), the model maximizes scaled log-likelihood Ai · log p(X0|φθ(Xt)) where Ai = R(τ(i)) - Rα. The update uses likelihood ratio hθkt = p(xk|θ)/p(xk|θold) with clipping to [1-ε, 1+ε] and KL divergence regularization against a reference model θref updated every G epochs.

### Mechanism 3: Long Short-Term Risk-Seeking Policy
A persistent candidate pool across epochs prevents premature abandonment of hard-to-sample but high-quality expressions. At epoch k, the pool updates as Sα ← Sα ∪ Skα (union with current top-α%). Rα is recomputed as the minimum reward in the expanded pool. Bottom α% expressions are pruned after each epoch to bound pool size.

## Foundational Learning

- **Concept: Discrete Denoising Diffusion Probabilistic Models (D3PM)**
  - Why needed here: DDSR modifies D3PM's uniform-to-categorical transition to a single-token masking scheme. Understanding the baseline helps distinguish what's novel.
  - Quick check question: Can you explain why D3PM's Qt = βtI + (1-βt)11⊤/d converges to uniform noise?

- **Concept: Risk-Seeking Policy Gradients**
  - Why needed here: The paper extends DSR's risk-seeking strategy with long-term memory. The baseline gradients in Equation 4 are the foundation for GRPO modifications.
  - Quick check question: Why does risk-seeking (selecting top α%) differ from standard policy gradient, and what problem does it solve for symbolic regression?

- **Concept: Transformer Positional Encodings for Mixed Dimensions**
  - Why needed here: Equation 9 defines a 2D positional encoding PE(l, t) combining token position l and diffusion timestep t—this is non-standard and critical to the architecture.
  - Quick check question: How does the 2D encoding in Equation 9 differ from standard sinusoidal positional encoding in vanilla Transformers?

## Architecture Onboarding

- **Component map**: Input (one-hot token matrix + 2D positional encoding) -> Encoder (1 attention block) -> Decoder (1 attention block + feed-forward layer) -> Output (linear layer + softmax)
- **Critical path**: 1) Sample batch of B=1000 expressions via backward diffusion, 2) Select top α=5% by reward, 3) Merge with historical pool Sα, prune bottom 5%, 4) For C=5 steps: sample t, compute JGRPO, update θ, 5) Every G=5 epochs, refresh θref ← θ
- **Design tradeoffs**: BFS vs. POT ordering (BFS keeps siblings adjacent for attention, POT matches DSR convention), single-token masking vs. D3PM (fewer steps but slower mixing), constant token limit (10) restricts expressiveness for polynomial-heavy targets
- **Failure signatures**: Invalid expression loops suggest token library mismatch, slow convergence on complex problems indicates insufficient depth/constant budget, high noise sensitivity shows reward susceptibility to variance
- **First 3 experiments**: 1) Reproduce ablation on Strogatz dataset (target 35.7% accuracy gap), 2) Token masking rate sweep (test 1-3 tokens per step), 3) Pool size sensitivity (vary α from 1%-10% on black-box problems)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the random mask-based discrete diffusion framework be adapted for supervised learning to build foundational symbolic regression models?
- Basis in paper: [explicit] The authors state future work may explore "extending the discrete diffusion framework to supervised learning of foundational models."
- Why unresolved: The current method relies on reinforcement learning (GRPO) to train from scratch on specific datasets rather than leveraging pre-training on large-scale equation corpora.
- What evidence would resolve it: Successful pre-training of the diffusion model on a large synthetic dataset demonstrating zero-shot or few-shot generalization to unseen SRBench problems.

### Open Question 2
- Question: How can the reward function be refined to improve robustness in high-noise environments (e.g., 10% target noise)?
- Basis in paper: [explicit] The conclusion lists "reduced robustness to high levels of noise" as a limitation and suggests "refining the reward function" as a solution.
- Why unresolved: In Table 1, DDSR underperforms compared to DSR-W/OC at 10% noise, implying the current reward strategy is susceptible to noise-induced variance.
- What evidence would resolve it: A modified reward formulation that achieves a higher symbolic solution rate than DSR-W/OC on the 10% noise SRBench subset.

### Open Question 3
- Question: Can the efficiency of the diffusion transformation process be improved to reduce runtimes for complex expressions?
- Basis in paper: [explicit] The authors acknowledge the method "tends to require longer runtimes to discover solutions for complex problems" and list "improving the efficiency of the diffusion transformation process" as future work.
- Why unresolved: The sequential nature of the single-token masking process may inherently limit speed for long sequences compared to parallelized methods.
- What evidence would resolve it: A variation of the algorithm that discovers solutions of equivalent accuracy to the baseline DDSR in significantly fewer epochs or less wall-clock time.

## Limitations

- The single-token masking mechanism lacks theoretical grounding for why this specific perturbation rate optimizes the trade-off between structural preservation and diversity
- Arbitrary constraints (32-token max depth, 10-constant limit) may artificially constrain solution space for polynomial-heavy problems
- Claims about robustness to noise and interpretability through complexity simplification lack rigorous ablation and validation against human interpretability studies

## Confidence

- **High confidence**: The empirical improvements on SRBench (27.3% accuracy, 6.5% symbolic solution rate over DSR baseline) are well-documented with specific numbers. The mechanism of random mask-based diffusion preserving structure is clearly described and validated through ablation.
- **Medium confidence**: The claim that token-wise GRPO stabilizes RL training and accelerates convergence (~30 epochs faster than RSPG) is supported by Figure 5 but lacks statistical significance testing. The LST policy's improvement (1.3-3.5%) is documented but the interaction effects with GRPO are unclear.
- **Low confidence**: The paper's claims about robustness to noise (10% noise performance) and interpretability through complexity simplification lack rigorous ablation—the complexity metric itself is poorly defined and not validated against human interpretability studies.

## Next Checks

1. **Statistical significance testing**: Run DDSR vs. DSR-W/OC on 20 randomly selected SRBench problems with 5 random seeds each. Compute t-tests for accuracy and solution rates, and report effect sizes with confidence intervals. This addresses whether the 27.3% accuracy improvement is reproducible or dataset-dependent.

2. **Perturbation rate sweep**: Implement and test masking rates of 1, 2, 3, and 4 tokens per diffusion step on 5 Feynman and 5 Strogatz problems. Measure reconstruction fidelity (token accuracy), expression validity rates, and final solution quality. This validates whether single-token masking is truly optimal or if the improvement comes from reduced optimization steps.

3. **Constraint relaxation experiment**: Run DDSR on 10 polynomial-heavy black-box problems with modified constraints: increase max constants from 10 to 20 and max depth from 32 to 64. Compare solution rates and expression complexities against original constraints. This tests whether the current limits artificially constrain expressiveness or if the improvements come from architectural innovation.