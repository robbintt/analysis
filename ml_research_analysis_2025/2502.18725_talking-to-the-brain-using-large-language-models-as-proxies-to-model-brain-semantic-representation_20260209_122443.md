---
ver: rpa2
title: 'Talking to the brain: Using Large Language Models as Proxies to Model Brain
  Semantic Representation'
arxiv_id: '2502.18725'
source_url: https://arxiv.org/abs/2502.18725
tags:
- semantic
- activation
- brain
- patterns
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel paradigm that leverages large language
  models (LLMs) as proxies to extract semantic information from naturalistic images
  for analyzing human visual semantic representation. Using a Visual Question Answering
  (VQA) strategy, LLMs generate rich semantic representations that successfully predict
  established neural activity patterns measured by fMRI, such as face-selective activation
  in the fusiform face area and building-selective activation in the parahippocampal
  place area.
---

# Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation

## Quick Facts
- **arXiv ID:** 2502.18725
- **Source URL:** https://arxiv.org/abs/2502.18725
- **Reference count:** 40
- **Primary result:** LLM-derived semantic features successfully predict established fMRI activation patterns (FFA for faces, PPA for buildings)

## Executive Summary
This study introduces a novel paradigm that leverages large language models (LLMs) as proxies to extract semantic information from naturalistic images for analyzing human visual semantic representation. Using a Visual Question Answering (VQA) strategy, LLMs generate rich semantic representations that successfully predict established neural activity patterns measured by fMRI, such as face-selective activation in the fusiform face area and building-selective activation in the parahippocampal place area. The method reveals hierarchical semantic organization across cortical regions and constructs a brain semantic network identifying meaningful clusters reflecting functional and contextual associations. This innovative approach overcomes limitations of traditional annotation methods and enables more ecologically valid explorations of human cognition using complex naturalistic stimuli.

## Method Summary
The method extracts semantic features from naturalistic images using a multimodal LLM (BLIP) in VQA mode. For each semantic label, BLIP is prompted with image-specific questions (e.g., "Is there a face in the image?") and returns binary "yes/no" responses. These binary vectors are balanced via random undersampling and used as predictors in voxel-wise GLM regression against fMRI BOLD signals. Cluster correction based on Random Field Theory (RFT) with Monte Carlo simulations establishes significance thresholds. The approach successfully predicts established neural activation patterns and reveals hierarchical semantic organization across cortical regions.

## Key Results
- LLM-derived semantic features successfully predict established neural activity patterns (FFA for faces, PPA for buildings)
- Semantic hierarchies are reflected in progressively recruiting cortical regions as specificity increases
- Brain semantic network reveals meaningful clusters reflecting functional and contextual associations

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-Semantic-Proxy via VQA Binary Encoding
- Claim: Multimodal LLMs can extract semantically meaningful representations from naturalistic images that predict voxel-wise fMRI responses.
- Mechanism: For each semantic label (e.g., "face"), a targeted prompt queries the LLM (BLIP) about image content. The binary "yes/no" response encodes semantic presence. These binary vectors across labels serve as predictors in voxel-wise GLM regression against BOLD signals.
- Core assumption: LLM semantic representations share sufficient structure with human visual semantic representations to serve as proxies.

### Mechanism 2: Voxel-wise GLM Regression with Cluster Correction
- Claim: LLM-derived semantic features can statistically explain BOLD variance at individual voxels when properly corrected for multiple comparisons.
- Mechanism: Each voxel's time series is regressed onto semantic binary predictors via GLM. R² and t-values quantify fit. Random Field Theory (RFT) and Monte Carlo simulations establish cluster-size thresholds (minimum 77.84 mm³ at p < 0.05) to control false positives.
- Core assumption: Hemodynamic responses to semantic features are sufficiently linear and time-locked to image presentation for GLM assumptions to hold.

### Mechanism 3: Hierarchical Semantic Organization in Cortex
- Claim: Semantic hierarchies (e.g., "animal" → "mammal" → "human" → "man") are reflected in progressively recruiting cortical regions as specificity increases.
- Mechanism: WordNet provides hierarchical label structure. GLM-derived activation maps for each level are overlaid and subtracted. Adjacent hierarchy levels share more co-activated regions than non-adjacent levels.
- Core assumption: WordNet's linguistic hierarchy approximates neural semantic hierarchy.

## Foundational Learning

- **Concept: Visual Question Answering (VQA)**
  - Why needed here: Core method for extracting semantic features from images using LLMs with targeted prompts
  - Quick check question: Can you explain why binary VQA outputs (yes/no) are sufficient for semantic encoding rather than dense embeddings?

- **Concept: GLM regression in fMRI analysis**
  - Why needed here: Statistical engine mapping semantic predictors to voxel-wise BOLD responses
  - Quick check question: Why does fMRI analysis require multiple-comparison correction (e.g., RFT), and what happens if you skip it?

- **Concept: Semantic hierarchy (hyponymy/hypernymy)**
  - Why needed here: Structural assumption linking WordNet's lexical hierarchy to cortical organization
  - Quick check question: If "animal" activates region A and "human" activates regions A+B, what does the subtraction "human - animal" reveal?

## Architecture Onboarding

- **Component map:**
  Input images → BLIP VQA with category-specific prompts → binary semantic vectors → balancing (undersampling) → voxel-wise GLM regression → RFT + Monte Carlo cluster correction → cortical activation maps and semantic similarity matrix

- **Critical path:**
  1. Design prompts for each semantic label (e.g., 80 COCO categories)
  2. Run BLIP VQA inference on all images → binary semantic matrix [images × labels]
  3. Balance yes/no counts per label via undersampling
  4. Fit GLM per voxel per participant → t-maps
  5. Group-level GLM + cluster correction
  6. Build similarity matrix from activation profiles → hierarchical clustering

- **Design tradeoffs:**
  - Binary VQA vs. dense embeddings: Binary is interpretable and efficient but loses nuance; dense captures richer semantics but is harder to interpret
  - Undersampling vs. weighted regression: Undersampling reduces sample size; weighting preserves data but may introduce bias
  - BLIP vs. other LLMs (GPT-4, Gemini, CLIP): BLIP is efficient; larger models may capture finer semantics but increase cost

- **Failure signatures:**
  - Low t-values in known category-selective regions (e.g., FFA for faces) → prompt design issue or LLM failure
  - Inflated false positives despite correction → spatial correlation underestimated; adjust FWHM or cluster threshold
  - Collapsed intra-cluster similarity → semantic labels too heterogeneous; refine category definitions

- **First 3 experiments:**
  1. Validate on held-out categories: Train GLM on 60 COCO labels, test prediction accuracy on remaining 20 to assess generalization
  2. Prompt sensitivity analysis: Compare activation maps from paraphrased prompts (e.g., "Is there a face?" vs. "Can you see a person's face?") to test robustness
  3. LLM ablation: Compare BLIP VQA vs. CLIP feature similarity (cosine) vs. dense BLIP embeddings on same data; quantify FFA/PPA prediction accuracy (Table S1 shows VQA mean t=2.82, CLIP feature similarity t=2.10)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the "talking to the brain" paradigm maintain its predictive validity when applied to more complex cognitive task designs beyond simple image recognition?
  - Basis: Authors note simple recognition tasks may not capture full semantic processing depth
  - Why unresolved: Current study used passive viewing/ simple recognition tasks only
  - Evidence needed: Successful replication in active tasks (decision-making, memory encoding)

- **Open Question 2:** Can findings from this study, based on 11 participants, be generalized to broader population?
  - Basis: Authors list "relatively small sample size" as limitation
  - Why unresolved: Small N=11 subjects from specific datasets introduces idiosyncratic neural representation risk
  - Evidence needed: Replication with significantly larger participant pool in pre-registered study

- **Open Question 3:** Can advanced proprietary models (e.g., GPT-4, Gemini) decode semantic representations with higher fidelity than current open-source models?
  - Basis: Authors suggest GPT-4 and Gemini could offer even greater potential
  - Why unresolved: Study used BLIP and CLIP; proprietary models may have superior contextual understanding
  - Evidence needed: Comparative analysis benchmarking neural prediction accuracy of open-source vs. proprietary models

## Limitations
- **Dependence on LLM Semantic Alignment**: Method validity hinges on untested assumption that LLM semantic representations align with human representations
- **Potential False Positives in Cluster Correction**: Effectiveness depends critically on accurate spatial smoothness estimation (FWHM=3mm)
- **Prompt Design Opacity**: Only one example prompt provided; 79 other categories unspecified, creating reproducibility gap

## Confidence
- **High Confidence**: Face and building category predictions (FFA/PPA activation) - replicate well-established findings
- **Medium Confidence**: Hierarchical semantic organization - observed pattern but relies on strong assumption about WordNet mapping
- **Low Confidence**: Generalizability to novel categories - paper doesn't test prediction for categories not used in GLM training

## Next Checks
1. Cross-validation with held-out categories: Train GLM on 60 COCO labels and test prediction accuracy on remaining 20 to assess generalization
2. Prompt sensitivity analysis: Systematically vary phrasing of VQA prompts and quantify how activation map consistency changes for established regions
3. Alternative semantic feature comparison: Replace BLIP VQA binary vectors with CLIP feature similarity scores or dense BLIP embeddings on same dataset and compare prediction accuracy for FFA/PPA activation