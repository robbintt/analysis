---
ver: rpa2
title: Complexity-Driven Policy Optimization
arxiv_id: '2509.20509'
source_url: https://arxiv.org/abs/2509.20509
tags:
- entropy
- policy
- cdpo
- complexity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of balancing exploration and\
  \ exploitation in reinforcement learning by proposing a novel complexity-based regularization\
  \ term. The authors replace the standard entropy bonus in PPO with an LMC (L\xF3\
  pez-Ruiz, Mancini, and Calbet) complexity measure, defined as the product of Shannon\
  \ entropy and disequilibrium."
---

# Complexity-Driven Policy Optimization

## Quick Facts
- arXiv ID: 2509.20509
- Source URL: https://arxiv.org/abs/2509.20509
- Reference count: 40
- This paper proposes replacing entropy regularization in PPO with LMC complexity (entropy × disequilibrium) to achieve structured exploration in discrete action spaces.

## Executive Summary
This paper addresses the challenge of balancing exploration and exploitation in reinforcement learning by proposing a novel complexity-based regularization term. The authors replace the standard entropy bonus in PPO with an LMC (López-Ruiz, Mancini, and Calbet) complexity measure, defined as the product of Shannon entropy and disequilibrium. This encourages policies to maintain structured stochasticity rather than pure randomness or determinism. The proposed Complexity-Driven Policy Optimization (CDPO) algorithm was tested across multiple discrete-action environments, including CartPole, CarRacing, Atari games, CoinRun, and a newly designed multi-cart CartPole variant. Results show that CDPO is more robust to the choice of regularization coefficient than entropy-based PPO, particularly in complex tasks requiring nuanced exploration, and achieves comparable or superior performance with less hyperparameter sensitivity.

## Method Summary
CDPO modifies PPO's objective by replacing the entropy bonus with LMC complexity: L = L_CLIP - c_vf·L_VF + c_reg·C[π], where C = entropy × disequilibrium. Entropy is computed as -Σ π(a|s)log π(a|s) and disequilibrium as Σ(π(a|s) - 1/|A|)². The algorithm uses standard PPO training with stable-baselines3 implementations, employing MLP networks (64×2) for simpler tasks and CNN architectures (Nature/IMPALA) for visual environments. Tested on discrete action spaces including CartPole, CarRacing, Atari games (AirRaid, Asteroids, Riverraid), CoinRun, and a custom CARTerpillar environment with 6-11 carts.

## Key Results
- CDPO achieves comparable or superior performance to entropy-based PPO across tested environments
- CDPO shows significantly reduced sensitivity to the regularization coefficient (c_reg) compared to entropy-based methods
- Performance gains are most pronounced in complex environments requiring nuanced exploration
- The method successfully balances structured stochasticity, avoiding both pure randomness and determinism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMC complexity regularization creates a self-regulating gradient that dynamically adjusts exploration pressure based on current policy state.
- Mechanism: The gradient ∇θC[πθ](s) = S[πθ](s)∇θD[πθ](s) + D[πθ](s)∇θS[πθ](s) has two competing terms. When the policy is near-deterministic (S≈0), the entropy gradient dominates, encouraging exploration. When near-uniform (D≈0), the disequilibrium gradient dominates, encouraging structure. This creates negative feedback loops around both pathological extremes.
- Core assumption: The training dynamics naturally traverse states where one term dominates sufficiently to steer away from extremes.
- Evidence anchors:
  - [abstract] "This regularizer encourages policies that balance stochasticity (high entropy) with structure (high disequilibrium)... suppresses both extremes"
  - [section 4.2] "This self-regulating mechanism, which encourages exploration when the policy is nearly deterministic and imposes structure when it approaches pure randomness"
  - [corpus] "When Maximum Entropy Misleads Policy Optimization" (arXiv:2506.05615) confirms entropy maximization can fail in practice, supporting the need for alternatives, though does not directly validate CDPO's specific mechanism.
- Break condition: If training converges so rapidly that the policy never spends sufficient iterations in either extreme region, self-regulation may not trigger meaningfully.

### Mechanism 2
- Claim: The complexity landscape has |A| local maxima rather than a single uniform attractor, allowing policies to maintain near-deterministic behavior on some actions while exploring others.
- Mechanism: Unlike entropy (single maximum at uniform distribution), complexity has multiple maxima where some actions can have very low probability while others remain differentiated. This permits "structured stochasticity"—the policy can be nearly deterministic on clearly suboptimal actions while maintaining exploration on uncertain ones.
- Core assumption: Discrete action spaces have actions that can be confidently suppressed without harming exploration of the remaining action space.
- Evidence anchors:
  - [abstract] "encourages policies that balance stochasticity (high entropy) with structure (high disequilibrium)"
  - [section 4.2] "complexity optimization landscape contains multiple maxima and lowers the probability of deterministic and purely random behaviors"
  - [corpus] Weak direct evidence; corpus focuses on entropy control issues in LLM-RL rather than complexity measures.
- Break condition: In action spaces where all actions are potentially useful (no clearly "bad" actions), the multi-peak landscape advantage diminishes.

### Mechanism 3
- Claim: Scaling entropy by disequilibrium reduces sensitivity to the regularization coefficient by softening the optimization landscape's gradient magnitude.
- Mechanism: When entropy is high but disequilibrium is low (near-uniform policy), the product C = H·D remains low, naturally damping the regularization signal. When disequilibrium is high (peaked distribution), entropy is typically lower, again constraining the product. This inherent scaling limits how strongly the regularizer can push the policy, making c_reg less critical.
- Core assumption: The product structure provides a natural damping effect across different policy states that standard entropy lacks.
- Evidence anchors:
  - [abstract] "CDPO is more robust to the choice of regularization coefficient than entropy-based PPO"
  - [section 6] "c_reg coefficient still influences the final result, though less significantly than for entropy"
  - [corpus] "Distribution-Centric Policy Optimization" (arXiv:2601.12730) addresses EE trade-offs through distributional approaches, suggesting broader interest in alternative regularization, though not direct validation.
- Break condition: If an environment requires very strong exploration pressure that complexity's self-damping would inherently limit, performance could suffer relative to well-tuned entropy.

## Foundational Learning

- Concept: **Shannon Entropy H(X) = -Σ p(x)log p(x)**
  - Why needed here: Measures policy stochasticity; maximizing it pushes toward uniform randomness, which CDPO explicitly avoids.
  - Quick check question: What distribution maximizes entropy for a 4-action discrete space?

- Concept: **Disequilibrium D = Σ(pi - 1/|A|)²**
  - Why needed here: Quantifies distance from uniform distribution; CDPO multiplies entropy by this to suppress both random and deterministic extremes.
  - Quick check question: What is the disequilibrium of a deterministic policy [1, 0, 0, 0]?

- Concept: **PPO Clipped Surrogate Objective**
  - Why needed here: CDPO modifies PPO's objective by replacing entropy bonus with complexity; understanding the base algorithm is essential.
  - Quick check question: Why does PPO clip the probability ratio rather than use a hard KL constraint?

## Architecture Onboarding

- Component map:
  Policy network πθ(a|s) -> action probabilities (softmax) -> entropy and disequilibrium computation -> complexity loss (entropy × disequilibrium) -> PPO clipped objective + complexity bonus

- Critical path: (1) Forward pass produces action logits → (2) Softmax yields probabilities → (3) Compute entropy AND disequilibrium in parallel → (4) Multiply for complexity → (5) Add to PPO clipped objective with c_reg scaling

- Design tradeoffs:
  - Discrete action spaces only: Disequilibrium formula assumes finite |A|; continuous extension is non-trivial
  - Additional computation: O(|A|) per state for disequilibrium vs. entropy-only baseline; negligible for small |A|, may matter for large action spaces
  - c_reg still matters: Robustness is improved but not eliminated; some tuning still required

- Failure signatures:
  - Near-zero complexity loss throughout training: May indicate disequilibrium collapsing (policy stuck near uniform) or entropy collapsing (policy over-deterministic); check both components separately
  - Performance degrades with higher c_reg: Unlike reported robustness, suggests environment may require deterministic policies; try c_reg=0 baseline
  - Slow convergence in simple environments: Complexity bonus may be unnecessary overhead; verify against PPOwoEnt

- First 3 experiments:
  1. Baseline comparison on CartPole: Run CDPO vs. PPOwEnt vs. PPOwoEnt with c_reg ∈ {1e-3, 1e-2, 1e-1}; confirm CDPO matches baseline performance (Figure 2 replication)
  2. Coefficient sensitivity sweep: On CoinRun or Asteroids, test 5+ c_reg values for both CDPO and PPOwEnt; plot performance variance to verify reduced sensitivity claim
  3. Gradient component analysis: Log entropy, disequilibrium, and complexity throughout training; verify self-regulation (entropy dominates early, disequilibrium increases as policy sharpens)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the LMC complexity regularization be extended to continuous action spaces, and what mathematical formulation would replace the discrete disequilibrium term?
  - Basis in paper: [explicit] The authors state "due to the mathematical formulation of disequilibrium, CDPO currently applies only to environments with discrete action spaces. Extending our approach to continuous actions represents a compelling direction for future work."
  - Why unresolved: The disequilibrium term D[πθ](s) = Σ(πθ(a|s) − 1/|A|)² explicitly depends on |A| (the discrete action space cardinality), which has no direct analogue in continuous settings.
  - What evidence would resolve it: A reformulation of disequilibrium for continuous distributions (e.g., using KL divergence from a reference distribution) and empirical evaluation on continuous control benchmarks like MuJoCo.

- **Open Question 2**: Do the robustness benefits of CDPO persist as the action space dimensionality grows large (e.g., hundreds or thousands of actions)?
  - Basis in paper: [explicit] The authors note "our evaluation is limited to classic RL scenarios with a relatively small action space. Whether the observed benefits scale up with the number of actions is an open question."
  - Why unresolved: Tested environments have at most 18 actions (Riverraid); the complexity landscape with many maxima (|A| maxima per the gradient analysis) may behave differently in high-dimensional action spaces.
  - What evidence would resolve it: Systematic evaluation on tasks with large discrete action spaces, such as language modeling with vocabulary sizes in the tens of thousands.

- **Open Question 3**: Does complexity regularization improve stability and performance when applied to off-policy algorithms like SAC or the maximum-entropy framework?
  - Basis in paper: [explicit] The conclusion lists "extending the scope of complexity regularization to different policy gradient methods and to the maximum-entropy framework" as part of the research agenda.
  - Why unresolved: CDPO was only tested on on-policy PPO; the interaction between complexity regularization and off-policy learning dynamics or entropy-constrained objectives remains unexplored.
  - What evidence would resolve it: Comparative experiments replacing entropy terms in SAC or other maximum-entropy methods with LMC complexity, evaluated across standard benchmarks.

## Limitations
- The method is currently limited to discrete action spaces due to the mathematical formulation of disequilibrium
- Evaluation is limited to relatively small action spaces (≤18 actions); scaling to larger discrete spaces remains untested
- While robust, the regularization coefficient still influences final performance, requiring some hyperparameter tuning

## Confidence
- **High**: CDPO implementation and basic performance (matches or exceeds entropy-based PPO in tested environments)
- **Medium**: Coefficient robustness claim (supported by results but not rigorously quantified)
- **Low**: Theoretical mechanisms (self-regulation, multi-peak landscape) lack direct empirical validation

## Next Checks
1. **Coefficient Sensitivity Analysis**: Run systematic sweeps of c_reg ∈ {1e-4, 1e-3, 1e-2, 1e-1, 1e-0} for both CDPO and PPOwEnt on CoinRun, plotting mean±std across 5+ seeds to quantify robustness improvement
2. **Gradient Component Tracking**: Log entropy, disequilibrium, and complexity loss components throughout training; verify that entropy dominates early (high entropy, low disequilibrium) and disequilibrium increases as policy sharpens, confirming self-regulation
3. **Landscape Visualization**: For a simple 3-action environment, plot complexity landscape and policy trajectories to verify multiple maxima and that CDPO policies settle near these peaks rather than uniform distribution