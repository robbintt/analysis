---
ver: rpa2
title: Revisit the Stability of Vanilla Federated Learning Under Diverse Conditions
arxiv_id: '2502.19849'
source_url: https://arxiv.org/abs/2502.19849
tags:
- fedavg
- learning
- performance
- methods
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the stability and performance of the vanilla
  FedAvg algorithm in federated learning, comparing it against advanced FL methods
  on medical image classification tasks. The study uses two medical datasets (blood
  cell and skin lesion classification) with Vision Transformer (ViT) models across
  non-IID data distributions.
---

# Revisit the Stability of Vanilla Federated Learning Under Diverse Conditions

## Quick Facts
- arXiv ID: 2502.19849
- Source URL: https://arxiv.org/abs/2502.19849
- Authors: Youngjoon Lee; Jinu Gong; Sun Choi; Joonhyuk Kang
- Reference count: 31
- Primary result: FedAvg achieves comparable performance to advanced FL methods on medical image classification without hyperparameter tuning

## Executive Summary
This paper evaluates the stability and performance of vanilla FedAvg algorithm against advanced federated learning methods across medical image classification tasks. The study compares FedAvg with FedProx, FedDyn, and FedSAM on two medical datasets using Vision Transformer models under non-IID data distributions. The research demonstrates that FedAvg consistently delivers competitive accuracy without requiring hyperparameter tuning, making it a practical choice for medical FL deployments.

The findings challenge the common assumption that advanced FL algorithms necessarily outperform simpler approaches. FedAvg achieved top-1 test accuracies exceeding 98% for blood cell classification and 77% for skin lesion classification across all tested conditions, while advanced methods showed significant performance variations depending on their hyperparameter settings.

## Method Summary
The study employs a comparative evaluation framework using two medical image classification datasets: blood cell classification (3 classes, 364 images) and skin lesion classification (7 classes, 1,800 images). Vision Transformer models are trained using FedAvg, FedProx, FedDyn, and FedSAM algorithms under non-IID data distributions. The evaluation measures test accuracy, convergence speed, and computational efficiency across multiple hyperparameter configurations. Experiments run for 100 communication rounds with 10 clients participating in each round, maintaining class distribution ratios of 0.2, 0.3, and 0.5 for non-IID settings.

## Key Results
- FedAvg achieves consistent top-1 test accuracies above 98% (blood cell) and 77% (skin lesion) without any hyperparameter tuning
- Advanced FL methods show significant performance variations across different hyperparameter settings
- FedAvg requires no hyperparameter tuning while maintaining competitive convergence speed and accuracy
- Computational efficiency favors FedAvg due to reduced per-round processing time

## Why This Works (Mechanism)
FedAvg's stability stems from its simple aggregation mechanism that averages model updates from participating clients, inherently providing regularization against extreme updates. The algorithm's lack of complex penalty terms or adaptive learning rates means it avoids overfitting to specific hyperparameter configurations. This simplicity translates to consistent performance across diverse data distributions and client participation patterns, making it particularly suitable for medical applications where reliability and ease of deployment are critical.

## Foundational Learning
- **Non-IID data distributions**: Why needed - Medical data often exhibits heterogeneity across institutions; Quick check - Verify class distribution ratios match real-world medical data splits
- **Vision Transformer architecture**: Why needed - State-of-the-art performance on medical imaging tasks; Quick check - Confirm ViT model size and configuration match standard implementations
- **Federated learning communication rounds**: Why needed - Understand convergence behavior across distributed training; Quick check - Validate round counts and client participation rates
- **Hyperparameter sensitivity**: Why needed - Assess algorithm robustness to configuration changes; Quick check - Compare performance variance across different parameter settings
- **Computational efficiency metrics**: Why needed - Evaluate practical deployment feasibility; Quick check - Measure per-round processing times across algorithms

## Architecture Onboarding

Component Map: Data Distribution -> Client Sampling -> Local Training -> Model Aggregation -> Global Model Update

Critical Path: Local client training → Model upload → Server aggregation → Global model broadcast → Next round initiation

Design Tradeoffs:
- Simplicity vs. optimization: FedAvg sacrifices potential performance gains for reliability
- Computational efficiency vs. convergence speed: Fewer operations per round reduce resource requirements
- Hyperparameter-free operation vs. fine-tuning capability: Eliminates tuning complexity but limits optimization potential

Failure Signatures:
- Performance degradation when client participation drops below threshold
- Convergence issues with highly skewed data distributions
- Communication bottlenecks with large model sizes

First Experiments:
1. Run FedAvg with 5, 10, and 20 clients per round to assess participation impact
2. Test different non-IID ratios (0.1, 0.5, 0.9) to evaluate data heterogeneity effects
3. Compare convergence behavior on IID vs. non-IID data distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparison to only three advanced FL methods rather than broader algorithm spectrum
- Evaluation restricted to two medical datasets and Vision Transformer models
- Absence of security and privacy analysis specific to medical data deployments
- No ablation studies on varying levels of data heterogeneity

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| FedAvg stability across hyperparameter settings | High |
| Computational efficiency advantages | Medium |
| Universal applicability across medical FL scenarios | Low |

## Next Checks
1. **Broader Algorithm Comparison**: Evaluate FedAvg against additional FL algorithms (SCAFFOLD, FedNova, FedBN) across multiple medical datasets and model architectures to confirm competitive advantage.

2. **Real-World Deployment Testing**: Conduct federated learning experiments on distributed medical institutions with varying computational resources and network conditions to validate efficiency claims under realistic constraints.

3. **Robustness Analysis**: Investigate FedAvg's performance under different levels of data heterogeneity, label distribution skew, and potential adversarial attacks to assess stability in challenging federated learning environments.