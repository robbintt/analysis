---
ver: rpa2
title: 'Condition Weaving Meets Expert Modulation: Towards Universal and Controllable
  Image Generation'
arxiv_id: '2508.17364'
source_url: https://arxiv.org/abs/2508.17364
tags:
- condition
- image
- conditional
- visual
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces UniGen, a unified image-to-image generation
  framework that addresses the inefficiency and redundancy of existing methods that
  use separate modules for different conditional inputs. The core innovation is the
  Condition Modulated Expert (CoMoE) module, which aggregates semantically similar
  visual tokens and assigns them to dedicated experts for efficient conditional modeling,
  thereby reducing parameter redundancy and improving computational efficiency.
---

# Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation

## Quick Facts
- **arXiv ID:** 2508.17364
- **Source URL:** https://arxiv.org/abs/2508.17364
- **Reference count:** 40
- **Primary result:** UniGen achieves superior performance on MultiGen-20M and Subjects-200K datasets while reducing model complexity and inference time through Condition Modulated Expert (CoMoE) and WeaveNet modules.

## Executive Summary
UniGen introduces a unified image-to-image generation framework that addresses the inefficiency and redundancy of existing methods that use separate modules for different conditional inputs. The core innovation is the Condition Modulated Expert (CoMoE) module, which aggregates semantically similar visual tokens and assigns them to dedicated experts for efficient conditional modeling, thereby reducing parameter redundancy and improving computational efficiency. Additionally, WeaveNet is proposed to bridge the semantic gap between global text-guided features and local condition-guided features through a dynamic interaction mechanism. Extensive experiments on the MultiGen-20M and Subjects-200K datasets show that UniGen consistently outperforms state-of-the-art methods in both single- and multi-condition control tasks, achieving superior performance in FID, CLIP-I, CLIP-T, DINO, and other metrics, while significantly reducing model complexity and inference time.

## Method Summary
UniGen is a unified image-to-image generation framework that combines Condition Modulated Expert (CoMoE) modules with WeaveNet for efficient conditional modeling. The CoMoE module uses a Mixture-of-Experts approach to route semantically similar visual tokens to specialized experts, reducing cross-condition interference while enabling parameter sharing. WeaveNet interleaves control-branch and backbone computations to reduce the semantic gap between text-guided global features and condition-guided local features. The framework is built on Stable Diffusion 3.5 DiT blocks and supports 12 conditional inputs, achieving superior performance while reducing parameter count and inference latency by approximately 50% compared to previous methods.

## Key Results
- UniGen achieves state-of-the-art performance on MultiGen-20M and Subjects-200K datasets, outperforming ControlNet, IP-Adapter, and OmniControl2 across multiple metrics.
- The framework reduces parameter count and inference latency by approximately 50% compared to previous unified methods like UniControl.
- CoMoE with 6 experts provides optimal balance between diversity and fragmentation, while WeaveNet's 12 control layers effectively bridge semantic gaps without overfitting.

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Expert Routing via Semantic Clustering
- Claim: Routing visually similar tokens to specialized experts reduces cross-condition interference while enabling parameter sharing.
- Mechanism: The CoMoE module fuses global visual features (Fn) with condition-specific features (Fc) to compute expert scores via a linear projection. Tokens are routed based on these scores—foreground regions with similar representations go to dedicated Modulated Experts, while background regions are grouped separately. A Shared Expert processes the full token set holistically to preserve global structure.
- Core assumption: Semantic similarity in patch-level features correlates with processing requirements across different condition types.
- Evidence anchors:
  - [abstract] "aggregates semantically similar patch features and assigns them to dedicated expert modules"
  - [section III-A] Equation 1 shows Se = Linear(Fn + Fc) and routing via Max operation; ablation (Table VIII) confirms RoPE before expert routing is critical for spatial preservation
  - [corpus] OminiControl2 addresses conditioning efficiency but via different architectural choices; corpus does not validate token-routing claims directly
- Break condition: If visual tokens from different condition types lack shared semantic structure, expert specialization degrades and routing becomes near-uniform.

### Mechanism 2: Interleaved Feature Weaving for Global-Local Alignment
- Claim: Alternating control-branch and backbone computations reduces the semantic gap between text-guided global features and local condition-guided features.
- Mechanism: WeaveNet uses a "snake-like" data flow where conditional features from the control branch are integrated into the backbone at staggered layers, rather than processed independently and added at the end. The Zero ResProjection module aligns outputs before injection. This allows text prompt semantics to condition the control branch and vice versa during intermediate representations.
- Core assumption: Multi-step interaction between branches yields better alignment than single-point fusion.
- Evidence anchors:
  - [abstract] "bridges the information gap between the backbone and control branches" via "dynamic interaction"
  - [section IV-F1, Table VII] WeaveNet alone (without CoMoE) already outperforms ControlNet; combined yields best FID gains
  - [corpus] DivControl addresses representation entanglement in unified architectures but does not test interleaved injection specifically
- Break condition: If the number of control layers is too high, the model overfits conditioning signals and degrades generation quality (see ablation Fig. 4).

### Mechanism 3: Condition-Modulated Parameter Generation
- Claim: Condition-specific feature transformation can be achieved via learnable parameter matrices modulated by semantic embeddings, avoiding separate branches per condition.
- Mechanism: A learnable matrix LWc is modulated by condition semantic embedding Ec (pooled CLIP features of condition type). This produces condition-aware weights for transforming visual features without duplicating the full branch. Similarly, LWh modulates global features using condition features for directional control.
- Core assumption: Condition semantics provide sufficient signal to specialize shared parameters for diverse visual constraints.
- Evidence anchors:
  - [section III-A] Equation 2: Ec = Pooling(CLIP([condition types])), LWc ~ N(0, 0.1²), F'c = LWc · Ec · F'c
  - [section I] "reducing both parameter count and inference latency by approximately 50%" vs. UniControl
  - [corpus] No direct corpus validation; related work (CPO, DivControl) focuses on preference optimization or knowledge diversion, not parameter modulation
- Break condition: If condition types are too diverse or semantically ambiguous, modulation fails to differentiate and reverts to near-identity transforms.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**
  - Why needed here: CoMoE relies on sparse expert activation; understanding router load balancing and expert capacity prevents collapse where one expert dominates.
  - Quick check question: Can you explain why MoE models need an auxiliary load-balancing loss, and what happens without it?

- **Rotary Position Embeddings (RoPE)**
  - Why needed here: Tokens are scattered across experts by semantic similarity, which destroys spatial adjacency. RoPE encodes position into the representation so spatial information survives redistribution.
  - Quick check question: How does RoPE differ from absolute positional embeddings, and why is it preferred for variable-length sequences?

- **Conditional Diffusion Control Mechanisms**
  - Why needed here: UniGen builds on ControlNet-style control branches; understanding how conditions are injected (zero convolutions, feature addition) clarifies why WeaveNet's interleaving is a modification, not a replacement.
  - Quick check question: What is the role of zero convolutions in ControlNet, and how does WeaveNet's Zero ResProjection differ?

## Architecture Onboarding

- **Component map:**
  - VAE encoder → patch embedding → CoMoE (layer 0 only) → WeaveNet interleaving → VAE decoder

- **Critical path:**
  1. Encode conditional image via VAE → patch tokens Fc
  2. At DiT block 0: Fuse Fn + Fc → compute expert scores → route tokens → Modulated Expert processing (with RoPE + LWc modulation) → aggregate + Shared Expert output
  3. For blocks 1–11: Backbone processes Fn; WeaveNet injects control features at each block via Zero ResProjection
  4. Final denoising prediction used for Flow Matching loss

- **Design tradeoffs:**
  - Expert count: Paper finds 6 experts optimal; fewer reduces diversity, more fragments tokens excessively (Table X)
  - Control layers: 12 layers balances accuracy and compute; full replication overfits conditions (Fig. 4)
  - Base model: SD 3.5 chosen for efficiency over FLUX; FLUX version (UniGen†) improves metrics but at 3× parameters

- **Failure signatures:**
  - Dense expert activation: Router collapses; all tokens go to one expert → regularization loss should spike during training
  - Spatial inconsistency: Generated subject looks "detached" from background → RoPE may be missing or incorrectly applied before token scattering
  - Text instruction degradation: Generated image ignores prompt → WeaveNet over-prioritizes condition features; check CLIP-T scores vs. baseline

- **First 3 experiments:**
  1. **Reproduce single-condition Depth control on Subjects-200K test split** using provided weights; verify FID < 12, CLIP-I > 89, DINO > 94 to confirm implementation correctness.
  2. **Ablate WeaveNet vs. ControlNet-style injection** with CoMoE disabled; expect Table VII pattern: WeaveNet alone outperforms parallel ControlNet injection.
  3. **Test multi-condition (Depth + Canny)** and compare subject consistency vs. spatial alignment; expect condition forgetting in sparse spatial regions per failure analysis (Section IV-H2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can dynamically adjusting semantic control strength based on the similarity between visual and semantic representations prevent semantic degradation in WeaveNet?
- **Basis in paper:** [explicit] Section VI states the authors plan to investigate dynamically adjusting control strength to "avoid semantic degradation caused by excessive reliance on spatial constraints."
- **Why unresolved:** The current WeaveNet architecture unintentionally diminishes text-based constraints during training, leading to inferior alignment with textual prompts.
- **What evidence would resolve it:** Successful implementation of a dynamic adjustment mechanism that yields higher CLIP-T scores without sacrificing spatial FID or DINO scores.

### Open Question 2
- **Question:** What alternative condition injection strategies can maintain effective spatial constraints while mitigating the "spatial-level semantic gap" observed in element-wise addition?
- **Basis in paper:** [explicit] Section VI notes that while element-wise addition creates semantic gaps, the authors' preliminary tests with attention-based injection "significantly weakened spatial constraints."
- **Why unresolved:** There is currently a trade-off where attention-based methods fail to preserve the strict spatial alignment required for controllable generation.
- **What evidence would resolve it:** A novel injection method that outperforms element-wise addition in global coherence metrics while maintaining the spatial consistency of the condition image.

### Open Question 3
- **Question:** Can the parameter footprint of the UniGen framework be reduced to match LoRA-based approaches while retaining the benefits of unified conditional processing?
- **Basis in paper:** [explicit] Section VI highlights that despite efficiency gains over ControlNet, the UniGen footprint "remains relatively large" compared to LoRA-based methods.
- **Why unresolved:** The current architecture relies on duplicating independent branches of the backbone, which is inherently more parameter-heavy than low-rank adaptation techniques.
- **What evidence would resolve it:** A compressed variant of UniGen that achieves comparable performance metrics with a parameter count comparable to LoRA adapters (e.g., < 0.5B additional parameters).

## Limitations

- Scalability concerns for CoMoE routing with heterogeneous condition types lacking shared semantic structure.
- Limited external validation on diverse, real-world datasets beyond curated MultiGen-20M and Subjects-200K domains.
- Parameter footprint remains relatively large compared to LoRA-based approaches despite efficiency gains.

## Confidence

- **High** for the core architecture design (MoE + WeaveNet) and its computational efficiency gains (parameter count and inference latency reduction).
- **Medium** for the superiority claims over state-of-the-art methods, given the controlled benchmark settings and lack of external validation on diverse, real-world datasets.
- **Low** for the universality claim, as the evaluation is constrained to image-to-image generation with curated conditions, not open-world generation scenarios.

## Next Checks

1. **External Dataset Generalization:** Evaluate UniGen on a held-out, real-world dataset with diverse condition types (e.g., outdoor scenes with Depth and Semantic Segmentation) not seen during training to test true universality.

2. **Routing Behavior Analysis:** Visualize and quantify the distribution of tokens across experts during inference on multi-condition tasks to confirm that semantically similar tokens are consistently routed to the same expert, and that no expert is overloaded.

3. **Condition Forgetting in Sparse Regions:** Systematically test multi-condition generation in areas where condition signals are spatially sparse (e.g., Subject + Depth in open backgrounds) and measure subject consistency (CLIP-T) vs. spatial accuracy (Depth metrics) to identify the failure mode described in the paper.