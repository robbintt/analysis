---
ver: rpa2
title: 'A Single Direction of Truth: An Observer Model''s Linear Residual Probe Exposes
  and Steers Contextual Hallucinations'
arxiv_id: '2507.23221'
source_url: https://arxiv.org/abs/2507.23221
tags:
- hallucination
- probe
- arxiv
- linear
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a practical, generator-agnostic approach for
  detecting contextual hallucinations in AI-generated text. The method uses a frozen
  "observer" model to process concatenated source and continuation text, applying
  a linear probe to its residual stream at the final sentence boundary.
---

# A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations

## Quick Facts
- arXiv ID: 2507.23221
- Source URL: https://arxiv.org/abs/2507.23221
- Reference count: 40
- A linear probe on a frozen observer model's residual stream detects contextual hallucinations with F1 scores up to 0.99

## Executive Summary
This paper introduces a generator-agnostic approach for detecting contextual hallucinations in AI-generated text using a frozen "observer" model and linear probe on residual activations. The method achieves state-of-the-art performance on news summarization tasks (F1 up to 0.99) and synthetic logical contradiction datasets (ContraTales, F1=0.84), significantly outperforming baseline approaches including lexical overlap, entity verification, and attention-based methods. Crucially, gradient-times-activation analysis reveals that hallucination signals localize to sparse late-layer MLP activity rather than attention patterns, and causal interventions demonstrate that manipulating this direction in generators can steer hallucination rates, proving its functional role.

## Method Summary
The method processes concatenated source and continuation text through a frozen observer transformer, extracting post-layer-norm residual activations at the final sentence boundary. A logistic regression probe trained on these activations identifies contextual hallucinations with high accuracy. The approach uses inner-fold validation to select optimal layers (typically mid-to-late layers) and L2 regularization for probe training. For steering, the normalized probe direction is injected into generator residual streams at generation time, demonstrating bidirectional control over hallucination and repetition rates.

## Key Results
- Linear residual probe achieves F1 scores up to 0.99 on CNN/DM news summarization
- Outperforms baseline methods by 5-27 F1 points on news tasks and 0.21-0.31 on ContraTales
- Gradient attribution reveals hallucination signal localizes to late-layer MLP activity (L7-L9) not attention patterns
- Causal steering via direction injection modulates hallucination rates from 0.35-0.86 while trading off with repetition

## Why This Works (Mechanism)

### Mechanism 1: Linear Residual-Stream Hallucination Direction
- Claim: A single linear direction in the residual stream separates hallucinated from faithful text with high accuracy.
- Evidence: F1 > 0.95 sustained from layers 17-37 on CNN/DM for Gemma-2-9B; performance degrades on ContraTales (F1=0.84 vs news F1=0.99).
- Break condition: Direction may not capture all hallucination types; extrinsic hallucinations remain untested.

### Mechanism 2: Late-Layer MLP Attribution Circuit
- Claim: Hallucination signal localizes to sparse, late-layer MLP outputs rather than attention patterns.
- Evidence: Consistent {positive (L7) → strong-positive (L8) → negative (L9)} MLP attribution pattern across datasets; attention attributions show no layer-consistent pattern.
- Break condition: First-order gradient approximation may miss nonlinear interactions; attention-based methods won't transfer.

### Mechanism 3: Causal Steering via Direction Injection
- Claim: Injecting/ablating the identified direction causally modulates hallucination rates.
- Evidence: α=+60 yields hallucination rate 0.86, repetition <0.05; α=-60 yields repetition 0.84, hallucination 0.35; unpatched baseline shows intermediate trade-off.
- Break condition: Cannot simultaneously minimize both hallucination and repetition; fundamental trade-off exists.

## Foundational Learning

- **Residual stream and layer normalization**: Essential for understanding probe operation on post-layer-norm activations at specific token positions. Quick check: What operations normalize residual activations across different token positions?

- **Logistic regression probing**: Core to detection method using L2 regularization and threshold selection. Quick check: How do probe score thresholds >0 vs <0 relate to hallucination detection, and how to calibrate for asymmetric precision/recall?

- **Gradient-times-activation attribution**: Critical for localizing hallucination signals to specific MLP layers. Quick check: Why does positive MLP attribution at layer 8 imply increasing its activation would increase probe score?

## Architecture Onboarding

- **Component map**: Source + continuation text -> Frozen observer transformer -> Post-layer-norm residual stream at final token -> Linear logistic regression probe -> (Optional) Gradient-times-activation attribution -> (Optional) Direction injection into generator

- **Critical path**: 1) Concatenate source and continuation; 2) Forward pass through observer; 3) Extract residual activation at final token; 4) Compute probe logit with threshold; 5) (Optional) Attribution analysis; 6) (Optional) Steering intervention

- **Design tradeoffs**: Mid-to-late layers offer stable performance; larger observers improve ContraTales performance but increase cost; detection requires only observer while steering needs generator access

- **Failure signatures**: High false-positive rates on repetitive text; degraded performance on logical contradictions vs news; steering trade-off between hallucination and repetition

- **First 3 experiments**: 1) Layer sweep replication to verify mid-layer plateau; 2) Baseline comparison confirming 5-27 point F1 improvement; 3) Attribution sanity check verifying MLP pattern before steering

## Open Questions the Paper Calls Out

- Does the probe generalize to extrinsic hallucinations introducing novel, unverifiable information from outside source context? The paper states this applicability "remains untested."

- Can the probe maintain high performance on organically generated, human-verified hallucinations versus synthetically generated ones? The paper calls for validation on "organically generated, human-verified hallucinations."

- Is the hallucination direction truly unique or do multiple independent linear directions encode contextual inconsistency? The paper notes "The uniqueness of this linear direction is not established here."

## Limitations

- Performance degrades significantly on out-of-distribution logical contradictions (ContraTales F1=0.84 vs news F1=0.99), suggesting limited generalizability.
- Steering intervention reveals a fundamental trade-off: reducing hallucination increases repetition rates substantially.
- Attribution analysis relies on first-order gradient approximations that may not capture full nonlinear interactions.

## Confidence

- **High Confidence**: Linear probe achieves state-of-the-art detection performance on news summarization (F1 > 0.99), outperforming baselines by 5-27 points.
- **Medium Confidence**: MLP attribution pattern (L7 positive → L8 strong-positive → L9 negative) is robust across datasets, though exact layer positions vary.
- **Low Confidence**: Probe performance on entity-confusion and extrinsic hallucinations remains largely untested; attribution method's reliability for identifying causal units is limited.

## Next Checks

1. **Out-of-Distribution Generalization Test**: Evaluate CNN/DM probe on entity-confusion and extrinsic hallucinations from other datasets to measure F1 degradation and transferability.

2. **Attribution Ablation Validation**: Implement targeted ablation of identified MLP layers (L7-L9) in observer model during inference and compare performance degradation against random layer ablations.

3. **Steering Optimization Study**: Systematically sweep α values for both positive and negative steering to quantify the hallucination-repetition trade-off curve and test multi-objective optimization approaches.