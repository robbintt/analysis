---
ver: rpa2
title: 'SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant
  Navigation with Reinforcement Fine-Tuning'
arxiv_id: '2512.14757'
source_url: https://arxiv.org/abs/2512.14757
tags:
- navigation
- socially
- compliant
- vision
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing efficient vision
  language models for socially compliant robot navigation in human-populated environments.
  Current large-scale models incur high computational overhead, making them unsuitable
  for real-time deployment on resource-constrained robotic platforms.
---

# SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning

## Quick Facts
- arXiv ID: 2512.14757
- Source URL: https://arxiv.org/abs/2512.14757
- Reference count: 40
- SocialNav-MoE achieves 0.506 BERTScore-F1 while running at 1.709 FPS with only 5.74B parameters for socially compliant navigation

## Executive Summary
SocialNav-MoE addresses the challenge of deploying efficient vision language models for socially compliant robot navigation in human-populated environments. Current large-scale models are computationally expensive, making them unsuitable for real-time deployment on resource-constrained robotic platforms. The authors propose an efficient Mixture-of-Experts vision language model that incorporates reinforcement fine-tuning with a novel semantic similarity reward function. Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves a balance between navigation accuracy and efficiency, outperforming large model baselines in both semantic similarity metrics and inference speed while using significantly fewer parameters.

## Method Summary
SocialNav-MoE uses a three-stage training pipeline: supervised fine-tuning (SFT) establishes image-text alignment and action generation, reinforcement fine-tuning (RFT) with GSPO and semantic similarity reward (SSR) aligns outputs with semantic preferences, and MoE fine-tuning transfers the refined policy to MoE architecture with multi-turn conversational fine-tuning. The model uses SigLIP vision encoder (frozen) with Phi-2-2.7B language model and 4-expert MoE with top-k=1 routing. SSR uses BERTScore-F1 to provide denser gradients than hard-level rewards, while the MoE structure enables efficient inference by activating only a subset of experts per token.

## Key Results
- Achieves 0.506 BERTScore-F1 compared to 0.254-0.387 for baseline models
- Runs at 1.709 FPS versus 0.087-0.212 FPS for large model baselines
- Uses only 5.74B parameters compared to 175-200B in baseline models
- Top-k=1 routing with 4 experts optimal; higher k values degrade performance due to feature divergence under data scarcity

## Why This Works (Mechanism)

### Mechanism 1: Sparse MoE Activation
Sparse MoE activation enables efficient inference by activating only a subset of experts per token while maintaining representational capacity. Top-k routing selects k highest-scoring experts from K total experts, reducing FLOPs while preserving model capacity. Core assumption: Navigation decisions can be decomposed into specialized subtasks that benefit from expert specialization.

### Mechanism 2: Semantic Similarity Reward (SSR)
SSR provides denser, more informative gradients than hard-level or character-level rewards by evaluating semantic correspondence rather than surface-level matching. Uses BERTScore-F1 with contextual token embeddings from BERT to compute cosine similarity between generated and ground truth outputs. Core assumption: Semantic similarity correlates with socially appropriate navigation decisions even when lexical overlap is low.

### Mechanism 3: Three-Stage Training Pipeline
Three-stage training (SFT → RFT → MoEFT) progressively specializes the model from language grounding to socially compliant decision-making. Sequential specialization prevents catastrophic forgetting and enables stable MoE training. Core assumption: Each stage builds upon the previous one without undermining learned capabilities.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding how tokens are routed to specialized experts is essential for debugging routing collapse and optimizing expert count. Quick check: Given 4 experts with top-k=2 and router logits [2.1, 0.5, 1.8, 0.3], which experts are activated?

- **Reinforcement Learning from Verifiable Rewards (RLVR)**: RFT in this paper adapts GSPO, which is a group-based policy optimization method. Understanding KL-constrained policy updates is critical for tuning β and ε. Quick check: Why does GSPO apply length normalization to the importance sampling ratio s(θ)?

- **BERTScore and Semantic Similarity Metrics**: SSR uses BERTScore-F1 as the reward signal. Understanding how contextual embeddings enable soft token matching is necessary for interpreting reward behavior. Quick check: How does BERTScore handle synonyms like "stop" and "halt" compared to character-level overlap?

## Architecture Onboarding

- **Component map**: Input Image → Vision Encoder (SigLIP/CLIP, frozen) → Visual Tokens Vp → Vp + Tq → Multimodal Context αt → MoE Layers (alternating FFN/MoE) → SLM Head → Action Output

- **Critical path**: Vision encoder quality determines visual grounding, SFT establishes baseline action generation capability, RFT with SSR aligns outputs with semantic preferences, MoE fine-tuning distributes specialized knowledge across experts

- **Design tradeoffs**: Expert count: 4 experts with top-k=1 optimal; more experts with higher k degrade performance due to "feature divergence among experts" under data scarcity. Vision encoder: Frozen encoder better than fine-tuned (0.523 vs 0.515 SMS) in low-data regime. SLM backbone: Phi-2-2.7B outperforms Qwen-1.8B and StableLM-1.6B across all metrics

- **Failure signatures**: Routing collapse if top-k=1 consistently selects same expert, reward hacking if SSR rewards verbose but semantically empty responses, speed mismatches in velocity estimation without RFT

- **First 3 experiments**: 1) Ablate expert count with top-k=1 to verify SMS improvement plateaus beyond 4 experts. 2) Compare SSR vs hard-level reward on same SFT checkpoint measuring SMS, BERTScore-F1, and qualitative action appropriateness. 3) Compare frozen vs fine-tuned SigLIP vision encoder under 530-image training set to confirm frozen achieves higher SMS with fewer parameters

## Open Questions the Paper Calls Out

- Can SocialNav-MoE maintain its efficiency and social compliance when deployed on physical robotic platforms in dynamic, real-world environments? The authors plan to deploy the model on real robotic platforms but current evaluation is offline using image descriptions.

- How can evaluation metrics be adapted to account for ambiguity of social norms where multiple actions are valid but differ from single ground truth? The paper acknowledges inherent ambiguity in defining universally valid navigation norms.

- Does increasing training data volume mitigate performance degradation observed when activating more experts (higher top-k values)? The authors attribute performance drops with higher k to data scarcity introducing noise during output aggregation.

## Limitations

- Small dataset size (325 images total, 530 with augmentation) may limit generalization and explains why top-k=1 routing outperforms higher values
- Semantic similarity reward (BERTScore-F1) may not perfectly capture navigation-relevant semantics compared to alternatives like CLIPScore
- Frozen vision encoder outperforming fine-tuned version suggests model may memorize patterns rather than learn meaningful visual representations

## Confidence

**High Confidence**: Efficiency gains (FPS, parameter counts) and three-stage training pipeline are well-supported by experimental data. Superiority of Phi-2-2.7B over other SLMs and SigLIP over CLIP are high confidence claims with clear numerical backing.

**Medium Confidence**: SSR's effectiveness relative to hard-level and character-level rewards is supported by SMS scores but BERTScore-F1 may not perfectly align with socially compliant navigation quality. Optimal MoE configuration (4 experts, top-k=1) is empirically validated but may be dataset-dependent.

**Low Confidence**: Claims about generalization beyond SNEI dataset are low confidence given limited training data. Assertion that sparse expert activation maintains representational capacity while improving efficiency lacks mechanistic depth and direct evidence.

## Next Checks

1. **Dataset scaling experiment**: Train SocialNav-MoE on larger navigation dataset (e.g., augmented with HM3D or RoboTHOR) to test whether 4-expert, top-k=1 configuration remains optimal or if more experts become beneficial with increased data diversity.

2. **Reward function ablation with alternative metrics**: Replace BERTScore-F1 with CLIPScore and RoBERTa-based similarity metrics while keeping all other training conditions constant. Compare SMS scores and qualitative navigation outputs to determine if BERTScore captures navigation-relevant semantics better than alternatives.

3. **Expert routing analysis**: During inference on test set, log which expert is selected for each token across all samples. Analyze whether routing shows consistent patterns or collapses to random selection, indicating MoE structure isn't learning meaningful specialization for this task.