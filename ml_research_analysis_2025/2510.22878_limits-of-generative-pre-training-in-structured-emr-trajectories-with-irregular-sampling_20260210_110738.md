---
ver: rpa2
title: Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular
  Sampling
arxiv_id: '2510.22878'
source_url: https://arxiv.org/abs/2510.22878
tags:
- acute
- hypotension
- datasets
- autoregressive
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated whether autoregressive generative pre-training
  from natural language processing effectively transfers to structured electronic
  medical record (EMR) trajectories with irregular sampling. Using two longitudinal
  datasets (ART for HIV and Acute Hypotension), we trained a sequence-to-sequence
  LSTM and a reduced-scale ETHOS Transformer under controlled temporal irregularity.
---

# Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling

## Quick Facts
- arXiv ID: 2510.22878
- Source URL: https://arxiv.org/abs/2510.22878
- Reference count: 11
- This study evaluated whether autoregressive generative pre-training from natural language processing effectively transfers to structured electronic medical record (EMR) trajectories with irregular sampling.

## Executive Summary
This study evaluated whether autoregressive generative pre-training from natural language processing effectively transfers to structured electronic medical record (EMR) trajectories with irregular sampling. Using two longitudinal datasets (ART for HIV and Acute Hypotension), we trained a sequence-to-sequence LSTM and a reduced-scale ETHOS Transformer under controlled temporal irregularity. Models were assessed on their ability to generate realistic patient trajectories by measuring distributional fidelity (reproducing feature distributions) and correlational fidelity (preserving cross-feature dependencies). Results showed that both models successfully reproduced feature distributions under severe irregularity (up to 35-month gaps for HIV, 28-hour gaps for hypotension) but failed to maintain cross-feature coherence. The LSTM retained limited dependencies under moderate gaps, while ETHOS-lite produced inconsistent results. These findings indicate that autoregressive pre-training does not transfer cleanly from text to structured EMRs, yielding locally realistic but clinically incoherent trajectories, and highlight the need for domain-specific evaluation before deployment.

## Method Summary
The study trained LSTM and reduced ETHOS Transformer models on two EMR datasets (ART for HIV and Acute Hypotension) with controlled temporal irregularity. Both datasets were standardized to fixed-length trajectories with no missing values. Models were trained autoregressively with random inter-visit gaps during training only, while test sequences remained complete. The elapsed interval (Δt) between retained visits was included as an input feature. Models were evaluated on their ability to generate realistic patient trajectories by measuring distributional fidelity (reproducing marginal distributions) and correlational fidelity (preserving cross-feature dependencies) on prediction windows.

## Key Results
- Both models reproduced feature distributions under severe irregularity (35 months / 28 hours gaps) but failed to preserve cross-feature structure
- LSTM retained limited dependencies under moderate gaps while ETHOS-lite produced inconsistent results
- Autoregressive pre-training yields local realism but limited clinical coherence in structured EMRs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autoregressive pre-training preserves marginal feature distributions but degrades cross-feature correlational structure under irregular sampling.
- **Mechanism:** The model learns to predict each feature independently based on temporal context by minimizing per-feature reconstruction loss. When inter-visit gaps are introduced during training, the autoregressive context weakens, and the model defaults to reproducing univariate marginals rather than joint dependencies. This yields trajectories that look realistic feature-by-feature but violate known clinical relationships.
- **Core assumption:** Cross-feature correlations require consistent temporal context, which irregular sampling disrupts; the loss function does not explicitly penalize correlation drift.
- **Evidence anchors:**
  - [abstract] "Both reproduced feature distributions but failed to preserve cross-feature structure -- showing that generative pre-training yields local realism but limited clinical coherence."
  - [section 3] "correlational fidelity was inconsistent (Figure 3), indicating that both models failed to capture underlying clinical dependencies"
  - [corpus] Related work on longitudinal flow matching (arXiv:2510.03569) addresses sparse trajectories through joint multi-marginal learning, implicitly acknowledging that pairwise transitions lose global structure.

### Mechanism 2
- **Claim:** Patient-trajectory synthesis functions as a diagnostic probe for representation quality before downstream use.
- **Mechanism:** By autoregressively generating unseen trajectory windows and comparing synthesized vs. held-out distributions and correlations, the evaluation surface exposes whether the model has learned clinically meaningful structure or merely statistical shortcuts. This probes the embedding space without requiring ground-truth phenotypes.
- **Core assumption:** Distributional and correlational fidelity on synthetic trajectories predicts downstream utility for phenotype discovery; failure in synthesis implies unreliable representations.
- **Evidence anchors:**
  - [abstract] "support trajectory synthesis as a practical probe before fine-tuning or deployment"
  - [section 1] "Probing embeddings is crucial when such models are used for phenotype discovery, ensuring learned structure reflects true clinical relations"
  - [corpus] Diffusion-based trajectory synthesis for MCI prediction (arXiv:2506.05428) similarly uses plausibility of generated futures as a proxy for model quality.

### Mechanism 3
- **Claim:** Temporal irregularity encoding (Δt features) enables handling of variable gaps but does not compensate for context loss at extreme sparsity.
- **Mechanism:** The elapsed interval Δt between retained visits is passed as an input feature, conditioning the model on gap duration. This informs the model how much "virtual time" has passed, but when gaps become large (e.g., 35 months), the effective context for predicting cross-feature dependencies erodes.
- **Core assumption:** Δt encoding is sufficient signal for the model to weight recent vs. distant history; the model architecture can integrate this signal meaningfully.
- **Evidence anchors:**
  - [section 2.2] "Controlled irregularity was introduced only during training by sampling random inter-visit gaps... The elapsed interval between retained visits (Δt) was included as an input feature"
  - [section 3] Models "reproduce test-set marginals even under severe irregularity (35 months / 28 hours), showing strong distributional fidelity" but correlational fidelity failed
  - [corpus] Weak direct evidence; neighbor papers do not explicitly evaluate Δt encoding under irregular EMR sampling.

## Foundational Learning

- **Concept: Autoregressive generative pre-training**
  - Why needed here: The paper explicitly tests whether the NLP-origin paradigm of "predict next token from context" transfers to EMR sequences, which lack the sequential coherence of language.
  - Quick check question: Can you explain why predicting the next lab value from prior visits differs fundamentally from predicting the next word in a sentence?

- **Concept: Marginal vs. joint distribution fidelity**
  - Why needed here: The central finding is that models preserve marginals (individual feature distributions) while losing joint structure (cross-feature correlations); understanding this distinction is essential for interpreting results.
  - Quick check question: If a model generates realistic blood pressure values and realistic heart rates but produces implausible combinations (e.g., hypotension with tachycardia absent when clinically expected), which fidelity type has failed?

- **Concept: Informative missingness in clinical data**
  - Why needed here: EMR trajectories reflect clinical decision-making—tests are ordered when clinically indicated, creating systematic gaps that are not random noise but signal.
  - Quick check question: Why might a patient with no lactate measurements for 6 hours have a different underlying state than one with hourly lactate checks?

## Architecture Onboarding

- **Component map:** Fixed-length trajectories with Δt encoding -> LSTM or ETHOS-lite encoder-decoder -> Feature reconstruction loss over observation window -> Generated trajectories compared to held-out test
- **Critical path:**
  1. Standardize trajectories to fixed length, no missing values
  2. Split: 80-20 train-test + temporal 2:1 observation-prediction split
  3. During training only: subsample visits with random gaps ~ Uniform(1, G_max)
  4. Include Δt as input feature at each retained visit
  5. Train autoregressively for 10 epochs with Adam (lr=10⁻³)
  6. Generate prediction window trajectories and compare to held-out test
- **Design tradeoffs:**
  - Reduced ETHOS-lite vs. full ETHOS: Downscaled to 2 layers to prevent overfitting on small datasets, trading off representational capacity
  - Irregularity only during training: Simulates real-world missingness while keeping test sequences complete for clean evaluation
  - No explicit correlation loss: Standard per-feature reconstruction is simple but does not enforce joint structure
- **Failure signatures:**
  - Marginals match but correlation matrix tiles show mismatch or are uncomputable (single-level categoricals)
  - LSTM retains some dependencies under moderate gaps; ETHOS-lite produces inconsistent results even at lower irregularity
  - Generated trajectories contain clinically incoherent feature combinations (e.g., drug combinations that would never co-occur)
- **First 3 experiments:**
  1. **Baseline regularity test:** Train and evaluate with G_max=1 (no irregularity) to establish upper bound on correlational fidelity; compare against G_max=10 and G_max=35 conditions.
  2. **Correlation-aware loss ablation:** Add a correlation matrix reconstruction term to the loss (e.g., Frobenius norm between real and synthetic correlation matrices) and measure whether correlational fidelity improves without degrading marginals.
  3. **Architecture comparison at matched capacity:** Equalize parameter counts between LSTM and ETHOS-lite to isolate whether the failure is architecture-specific or a fundamental autoregressive limitation on EMR data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger-scale foundation models trained on massive, heterogeneous EMR datasets overcome the correlational fidelity limitations observed in reduced-scale models, or is this a fundamental limitation of autoregressive pre-training for structured clinical data?
- Basis in paper: [explicit] The authors used a "reduced ETHOS Transformer (ETHOS-lite, 2 layers)" to mitigate overfitting on smaller datasets and concluded that autoregressive pre-training "does not transfer cleanly from natural language to structured EMRs."
- Why unresolved: The study was constrained to two moderate-sized datasets with downscaled architectures; scaling effects remain untested.
- What evidence would resolve it: Training full-scale models on diverse, large-scale EMR corpora and systematically evaluating cross-feature correlation preservation across increasing model and data sizes.

### Open Question 2
- Question: What domain-specific evaluation metrics beyond distributional and correlational fidelity are necessary to rigorously assess clinical coherence in generated patient trajectories?
- Basis in paper: [explicit] The paper "examines...how its 'goodness-of-fit' should be evaluated" and states results "highlight the need for domain-specific evaluation," yet only evaluates marginal distributions and pairwise correlations.
- Why unresolved: The authors propose trajectory synthesis as a probe but do not establish a comprehensive evaluation framework capturing clinically meaningful dependencies.
- What evidence would resolve it: Development and validation of evaluation metrics grounded in clinical knowledge (e.g., treatment guidelines, physiological constraints), tested for sensitivity to clinically incoherent synthetic trajectories.

### Open Question 3
- Question: Can architectural modifications or alternative training objectives explicitly designed to model cross-feature dependencies overcome the observed correlational fidelity failures?
- Basis in paper: [inferred] Both tested architectures (LSTM and Transformer) failed to preserve correlational structure despite successful distributional fidelity, suggesting the limitation may stem from the autoregressive objective rather than model capacity alone.
- Why unresolved: The study tested standard autoregressive training without modifications targeting multi-feature dependencies or clinical constraints.
- What evidence would resolve it: Comparing standard autoregressive models against variants with correlation-preserving regularization, graph-based dependency modeling, or clinically-informed constraints on the same benchmarks.

### Open Question 4
- Question: To what extent does preserved distributional fidelity enable effective downstream prediction tasks despite the loss of cross-feature coherence in learned representations?
- Basis in paper: [explicit] The authors conclude that "such models may offer a foundation for downstream prediction, but their use for knowledge discovery or patient phenotyping remains premature."
- Why unresolved: The study evaluated trajectory synthesis quality but did not assess transfer performance on downstream clinical prediction tasks.
- What evidence would resolve it: Fine-tuning the pre-trained models on clinically relevant prediction tasks (e.g., treatment response, readmission risk) and comparing performance against models trained from scratch or with alternative pre-training strategies.

## Limitations
- The paper demonstrates that autoregressive pre-training from NLP does not cleanly transfer to structured EMRs, but the evaluation relies on visual distributional comparisons without specifying quantitative fidelity thresholds or statistical tests.
- ETHOS-lite architecture details (attention heads, FFN dimensions, position encoding with Δt) are unspecified, limiting exact reproduction.
- No ablation on correlation-aware loss terms or matched-capacity comparisons between LSTM and Transformer, leaving open whether the correlational failure is architectural or fundamental to autoregressive modeling on EMRs.

## Confidence

- **High confidence:** Models reproduce marginal feature distributions under severe irregularity but fail to preserve cross-feature correlations
- **Medium confidence:** Temporal gap encoding (Δt) informs the model of interval duration but does not compensate for context loss at extreme sparsity
- **Medium confidence:** Trajectory synthesis is a valid diagnostic probe for representation quality before downstream use

## Next Checks

1. Implement correlation matrix reconstruction loss (e.g., Frobenius norm between real and synthetic correlation matrices) and measure whether correlational fidelity improves without degrading marginals.
2. Equalize parameter counts between LSTM and ETHOS-lite to isolate whether the failure is architecture-specific or a fundamental autoregressive limitation on EMR data.
3. Add quantitative fidelity metrics (e.g., KS test for marginals, correlation coefficient differences for cross-feature dependencies) and establish statistical thresholds for "clinically coherent" vs. incoherent trajectories.