---
ver: rpa2
title: Self-Correcting Code Generation Using Small Language Models
arxiv_id: '2505.23060'
source_url: https://arxiv.org/abs/2505.23060
tags:
- code
- reward
- training
- accuracy
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores self-correction in code generation for small
  language models (SLMs), addressing the challenge that existing prompting- and training-based
  self-correction methods fail to generalize effectively to SLMs. The authors propose
  COCOS, a reinforcement learning framework that enhances intrinsic self-correction
  by using an accumulated reward function with a discount factor and a progressive
  reward that evaluates incremental improvements across multiple turns.
---

# Self-Correcting Code Generation Using Small Language Models

## Quick Facts
- arXiv ID: 2505.23060
- Source URL: https://arxiv.org/abs/2505.23060
- Reference count: 31
- This paper proposes COCOS, a reinforcement learning framework for self-correction in small language models, achieving up to 35.8% improvement on MBPP and 27.7% on HumanEval.

## Executive Summary
This paper addresses the challenge of self-correction in code generation for small language models (SLMs), where existing prompting- and training-based methods fail to generalize effectively. The authors propose COCOS, a reinforcement learning framework that uses an accumulated reward function with discount factor Î³=0.5 and progressive reward based on unit test pass ratios. Experiments on 1B-scale models show substantial improvements in self-correction across multiple datasets, with the approach maintaining correct responses while progressively correcting incorrect ones.

## Method Summary
The method employs a two-stage approach: first, an SFT pretraining stage on KodCode (180k filtered from 484k samples) creates a "boost model" for initialization and KL regularization reference. Second, online RL training uses REINFORCE with RLOO estimator, sampling k=2 trajectories per problem from the current policy. The accumulated reward function R(Å·â‚:T) = Î³^(T-1)râ‚ + Î£Î³^(T-t)(râ‚œ - râ‚œâ‚‹â‚) with Î³=0.5 balances first-turn quality and correction gains, while progressive reward computes râ‚œ as the unit test pass ratio. Training runs for 1500 steps with batch 128, learning rate 1e-5, and sampling temperature 0.9, using greedy decoding during evaluation.

## Key Results
- COCOS achieves 35.8% Î”Acc improvement over Turn-SFT on MBPP dataset
- Shows 27.7% Î”Acc improvement on HumanEval dataset
- Maintains stable training with accumulated reward (Î³=0.5) versus collapse in standard KL-penalized approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulated reward function with discount factor Î³ prevents training collapse while jointly optimizing initial and corrected responses.
- Mechanism: By computing R(Å·â‚:T) = Î³^(T-1)râ‚ + Î£Î³^(T-t)(râ‚œ - râ‚œâ‚‹â‚) with Î³=0.5, the objective balances first-turn quality (through weighted râ‚) and correction gains (through râ‚‚ - râ‚). This avoids the KL-regularization conflict observed in SCoRe, where overlapping outputs cause regularization on turn-1 to inadvertently constrain turn-2.
- Core assumption: SLMs produce near-identical outputs across turns when incorrect (edit distance â‰¤ 0.05 in 93% of cases), causing gradient coupling that destabilizes separate-turn regularization.
- Evidence anchors: Section 5.3 introduces COCOS with accumulated reward; Section 8.1 Figure 3 shows training collapse when Î³=0 or Î³=1, stable learning at Î³=0.5; MURPHY corpus validates multi-turn RL for code correction.
- Break condition: If edit distance between turns increases substantially (>0.3 ratio), the coupling assumption weakens and standard per-turn optimization may suffice.

### Mechanism 2
- Claim: Progressive reward based on unit test pass ratio provides denser learning signal than binary correctness, enabling stable policy updates for low-capacity models.
- Mechanism: Instead of r âˆˆ {0, 1}, progressive reward computes râ‚œ = (1/K)Î£ðŸ™{uâ‚–(Å·â‚œ) passes}, giving partial credit. This expands the exploration spaceâ€”correcting one additional test case yields positive reward even if full solution remains incorrect.
- Core assumption: SLMs receiving persistently low binary rewards cannot explore effectively; sparse rewards prevent associating actions with outcomes.
- Evidence anchors: Section 8.2 shows progressive reward with 34.2% test case improvement on ODEX with 0% degradation; binary causes ~40% regression; Section 5.2 notes binary rewards severely limit exploration space.
- Break condition: If test cases are highly correlated (passing one implies passing all), progressive reward degenerates to binary and loses advantage.

### Mechanism 3
- Claim: Online RL with self-generated data enables generalization beyond training distribution, unlike SFT-based correction methods.
- Mechanism: REINFORCE with RLOO estimator trains on trajectories sampled from the current policy, exposing the model to its own error patterns rather than fixed correction pairs. This creates a feedback loop where learned corrections address actual failure modes.
- Core assumption: SFT memorizes training distribution corrections but fails to generalize; online learning adapts to the model's evolving error profile.
- Evidence anchors: Abstract states COCOS trains model to confidently maintain correct outputs while progressively correcting incorrect ones; Section 7.1 shows SFT baselines comparable or degraded on unseen datasets while COCOS maintains positive Î”Acc; Beyond Output Critique corpus notes similar generalization concerns with output-level correction.
- Break condition: If training and test distributions are nearly identical, SFT may match RL performance with lower compute cost.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation for text generation**
  - Why needed here: Self-correction is framed as sequential decision-making where state = (problem, history, prompts) and actions = generated code. Understanding this abstraction is required to follow the policy gradient derivation.
  - Quick check question: Can you explain why the policy conditions on Å·â‚:â‚œâ‚‹â‚ rather than just the current prompt?

- Concept: **KL-divergence regularization in RLHF**
  - Why needed here: The paper's central failure analysis (Â§5.2) hinges on why KL penalties cause collapse when outputs overlap across turns. Without this background, the accumulated reward motivation is opaque.
  - Quick check question: Why would constraining Ï€(Â·|sâ‚) toward a reference policy inadvertently affect outputs at turn 2?

- Concept: **REINFORCE with baseline (RLOO estimator)**
  - Why needed here: The training objective uses leave-one-out baseline across sampled trajectories to reduce variance. Implementation requires understanding how k samples are generated and how rewards are centered.
  - Quick check question: Given 3 sampled trajectories with rewards [0.3, 0.5, 0.4], what baseline would RLOO use for the first sample?

## Architecture Onboarding

- Component map: Boost model (SFT on KodCode) â†’ Environment (unit test executor) â†’ Reward computer (progressive + accumulated reward) â†’ Policy optimizer (RLOO-based REINFORCE with KL penalty) â†’ Inference (greedy decoding over T=2+ turns)

- Critical path:
  1. Pre-train boost model on KodCode (ensures parseable code for test execution)
  2. Sample k=2 trajectories per problem from current policy
  3. Execute unit tests, compute progressive rewards per turn
  4. Aggregate into trajectory-level reward R(Å·â‚:â‚‚)
  5. Compute RLOO baseline, policy gradient, KL penalty
  6. Update for 1500 steps, checkpoint by highest reward

- Design tradeoffs:
  - Î³=0.5 vs. alternatives: Î³=0 risks reward hacking (degrading râ‚ to inflate râ‚‚-râ‚); Î³=1 ignores first turn; Î³=2 creates credit assignment ambiguity
  - Progressive vs. binary reward: Denser signal but requires multiple test cases per problem (may not exist for all benchmarks)
  - T=2 training vs. more turns: Limited by compute; analysis suggests gains continue beyond turn 2 but not validated

- Failure signatures:
  - Training collapse: Reward â†’ 0, KL divergence â†’ âˆž (indicates Î³ misconfiguration or excessive KL coefficient)
  - Low Î”iâ†’c with high Î”câ†’i: Model changes correct outputs more than incorrect ones (reward hacking or insufficient Î³)
  - No edit distance change between turns: Model generating identical outputs (may need stronger prompt separation or SFT warmstart)

- First 3 experiments:
  1. Reproduce Table 3 on MBPP: Train CoCoS vs. Turn-SFT baseline on Qwen2.5-1.5B; verify Î”Acc gap ~9%. This validates the full pipeline.
  2. Ablate Î³ âˆˆ {0, 0.5, 1, 2}: Plot learning curves (reward, KL) to confirm Figure 3 patterns. This diagnoses training stability.
  3. Progressive vs. binary reward comparison: Train two models differing only in reward function; evaluate on ODEX (where test case granularity matters). This isolates reward design contribution.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the CoCoS framework maintain its efficacy and stability when applied to models significantly larger than the 1B parameter scale?
  - Basis in paper: The authors explicitly state in the Limitations section that "evaluating its scalability to models larger than 1B parameters remains an interesting aspect to observe in future work."
  - Why unresolved: Experiments were constrained to 1B-scale models due to infrastructural limitations; thus, it is unknown if larger models require less specific reward shaping or exhibit different failure modes.
  - What evidence would resolve it: Empirical results showing the Î”Acc and training stability of CoCoS on 7B or 70B parameter models compared to standard baselines.

- **Open Question 2**: Can the accumulated reward approach be extended to support stable multi-turn training beyond two turns without performance saturation?
  - Basis in paper: The authors note, "future work should extend our approach to support multi-turn training beyond two turns," noting that current analysis was limited by costs.
  - Why unresolved: The training formulation was restricted to T=2, so the behavior of the accumulated reward function and credit assignment over longer trajectories remains untested.
  - What evidence would resolve it: Successful training runs with T > 2 showing consistent accuracy gains per turn without training collapse or reward hacking.

- **Open Question 3**: How can the credit assignment problem be resolved to allow joint optimization of responses when using a discount factor Î³ > 1?
  - Basis in paper: Section 8.1 notes that setting Î³ = 2 causes instability because "it becomes unclear which of the two responses contributed positively," identifying this as a credit assignment problem beyond the scope of current research.
  - Why unresolved: The current method avoids this by setting Î³=0.5, which focuses on recent changes, but does not solve the underlying difficulty of balancing the weight of initial versus corrected responses in the reward signal.
  - What evidence would resolve it: A modified reward mechanism or structural architecture that enables stable training with Î³ > 1, allowing the model to place valid emphasis on early turns.

## Limitations

- The approach requires unit tests for every problem, limiting applicability to domains without natural test cases.
- Training assumes exactly two correction turns, though analysis suggests potential gains from longer sequences remain unexplored.
- The accumulated reward mechanism specifically addresses SLM characteristics (near-identical repeated outputs) that may not generalize to larger models with more diverse turn-to-turn variation.

## Confidence

**High Confidence**: The empirical improvements over baselines (Î”Acc gains of 35.8% on MBPP and 27.7% on HumanEval) are well-documented and reproducible given the detailed methodology. The failure analysis of existing approaches and the mechanism explaining why COCOS succeeds (Î³=0.5 with progressive reward) are convincing.

**Medium Confidence**: The claims about generalization to unseen datasets (ODX) are supported but based on limited samples. The assertion that online RL with self-generated data enables better generalization than SFT-based correction methods is plausible but requires more rigorous ablation studies comparing different training distributions.

**Low Confidence**: The scalability analysis is incomplete. While results show COCOS works across different 1B-scale models (Qwen2.5-1.5B, Llama-3.2-1B, DeepSeek-Coder-1.3B), there is no systematic investigation of model capacity thresholds or performance degradation on smaller models (e.g., 500M parameters).

## Next Checks

1. **Ablation on test case availability**: Train COCOS with varying numbers of unit tests per problem (K=1, 3, 5, 10) to quantify how progressive reward benefits scale with test granularity. This validates the core mechanism claim about dense vs. sparse rewards.

2. **Cross-distribution robustness test**: Evaluate COCOS trained on MBPP against a completely different code generation dataset (e.g., APPS or CodeContests) without fine-tuning. This directly tests the generalization advantage over SFT-based methods claimed in Â§7.1.

3. **Multi-turn extension validation**: Implement T=3+ turn correction and compare against the T=2 baseline on the same datasets. Measure whether accumulated reward continues to prevent collapse and whether progressive reward maintains exploration benefits in longer sequences.