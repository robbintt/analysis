---
ver: rpa2
title: Language-Agnostic Suicidal Risk Detection Using Large Language Models
arxiv_id: '2505.20109'
source_url: https://arxiv.org/abs/2505.20109
tags:
- suicidal
- speech
- risk
- language
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a language-agnostic framework for suicidal
  risk detection using large language models (LLMs). The method transcribes speech
  to Chinese text using an ASR system, then employs LLMs with prompt-based queries
  to extract suicidal risk-related features in both Chinese and English.
---

# Language-Agnostic Suicidal Risk Detection Using Large Language Models

## Quick Facts
- arXiv ID: 2505.20109
- Source URL: https://arxiv.org/abs/2505.20109
- Authors: June-Woo Kim; Wonkyo Oh; Haram Yoon; Sung-Hoon Yoon; Dae-Jin Kim; Dong-Ho Lee; Sang-Yeol Lee; Chan-Mo Yang
- Reference count: 0
- Primary result: Proposed language-agnostic framework using LLMs achieves performance comparable to direct fine-tuning, demonstrating potential to overcome language constraints in suicidal risk detection.

## Executive Summary
This paper introduces a novel language-agnostic approach for detecting suicidal risk in speech using large language models (LLMs). The method transcribes speech to Chinese text using an ASR system, then employs LLMs with prompt-based queries to extract suicidal risk-related features in both Chinese and English. These features are used to fine-tune corresponding pretrained language models independently. Experiments on the SpeechWellness Challenge dataset show the proposed method achieves performance comparable to direct fine-tuning with ASR results or models trained solely on Chinese features, demonstrating the potential of cross-linguistic analysis to overcome language constraints and improve robustness in suicidal risk assessment.

## Method Summary
The framework transcribes speech to Chinese text using Paraformer ASR, then uses LLMs (GPT-4o/Qwen-Plus) with prompts to extract suicidal risk-related features in both Chinese and English. These features fine-tune monolingual language models (Chinese-BERT for Chinese, DepRoBERTa for English). A speech branch uses frozen XLSR/WavLM encoders. The system fuses text and speech embeddings through a multimodal hidden layer with logit voting across three tasks (ER, PR, ED). The approach aims to overcome language barriers by leveraging LLMs as semantic aligners for cross-lingual feature extraction.

## Key Results
- Proposed method achieves accuracy comparable to direct fine-tuning with ASR results (73% vs 57% for Chinese features alone)
- Cross-linguistic approach demonstrates potential to overcome language constraints in suicidal risk assessment
- Integration of speech and text modalities improves detection compared to unimodal baselines
- Significant performance gap observed between development (68%) and test (54%) sets

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Semantic Transfer via LLMs
If an LLM summarizes and translates risk markers from Chinese to English, monolingual models can perform cross-lingual classification without multilingual pre-training. The LLM acts as a semantic aligner, extracting suicidal risk-related features and regenerating them in the target language, bypassing grammar dependencies. This works if the LLM successfully captures all relevant risk markers without hallucinating or suppressing existing ones.

### Mechanism 2: Noise Filtering and Signal Amplification
Prompt-based feature extraction improves detection by filtering out non-relevant conversational noise and normalizing input. By instructing the LLM to "summarize key points related to suicidal thoughts," the system discards low-information tokens, providing downstream classifiers with higher signal-to-noise ratio inputs focused on risk indicators.

### Mechanism 3: Multimodal Error Compensation
Combining text-based features with speech-based features provides robustness because acoustic and semantic channels offer independent evidence of risk. The architecture uses late fusion where speech captures prosodic features while text captures semantic intent, allowing compensation when one modality is ambiguous.

## Foundational Learning

- **Concept: Automatic Speech Recognition (ASR) Error Propagation**
  - Why needed: The entire text pipeline depends on Paraformer ASR output. You must understand that text is an estimation with specific Word Error Rate.
  - Quick check: How does the system handle homophones or slang in Chinese transcript before LLM processing?

- **Concept: Prompt Engineering for Feature Extraction**
  - Why needed: Quality of cross-lingual features depends entirely on prompt design (e.g., "first-person perspective").
  - Quick check: What happens to feature extraction if user provides very short, one-word answer to interviewer's question?

- **Concept: Transfer Learning vs. Fine-Tuning**
  - Why needed: Paper relies on adapting general models (RoBERTa, BERT). Distinguishing between "frozen" extraction (speech) and "fine-tuned" adaptation (text) is critical.
  - Quick check: Why fine-tune text models but only extract embeddings from speech models?

## Architecture Onboarding

- **Component map:** Audio -> Paraformer (ASR) -> GPT-4o/Qwen (LLM Prompter) -> Bilingual Summaries -> [Chinese BERT / English DepRoBERTa]; Audio -> XLSR/WavLM (Frozen Encoder) -> Speech Embeddings; Concatenate(Text Embeddings, Speech Embeddings) -> Multimodal Hidden Layer -> Classifier
- **Critical path:** LLM Feature Extraction step is the bottleneck, introducing latency and cost (API calls to GPT-4o/Qwen) for every training sample.
- **Design tradeoffs:** Cost vs. Accuracy - using LLMs for preprocessing is expensive and slow compared to direct fine-tuning but yields ~16% accuracy gains (Table 3: 57% â†’ 73%).
- **Failure signatures:** Performance gap between Dev (68%) and Test (54%) suggests overfitting to training interview styles or LLM summaries too specific to training distribution.
- **First 3 experiments:**
  1. Run ASR -> Chinese BERT pipeline without LLM summarization to reproduce 57% accuracy baseline.
  2. Compare GPT-4o vs Qwen-Plus outputs on small subset (N=10) to verify "first-person perspective" constraint and check for hallucination.
  3. Train fusion layer using only text vs only speech embeddings to quantify marginal contribution to final 69% accuracy.

## Open Questions the Paper Calls Out
- How can the generalization gap between development and test set performance be effectively minimized? The authors report a "significant performance gap between the development and test sets" and list "enhancing generalization" as specific future work.
- Does transitioning from first-person perspective prompts to general summarization prompts improve the quality of extracted suicidal risk features? The conclusion suggests "refining prompt strategies, transitioning from first-person perspective prompts to general summarization prompts" for future work.
- Does the proposed framework maintain robustness when applied to native non-Chinese speech or low-resource languages? While claiming to be "language-agnostic," the experimental validation relies exclusively on the Chinese SW1 dataset.

## Limitations
- Entire pipeline depends on Paraformer ASR accuracy, with no mechanism for correction or uncertainty quantification.
- Using GPT-4o/Qwen-Plus for feature extraction introduces significant computational expense and latency, making approach impractical for real-time deployment.
- Method validated only on Chinese adolescents dataset; performance on other languages, cultures, or age groups remains unknown.

## Confidence
- High Confidence: Cross-lingual feature extraction mechanism is technically sound and experimental results show consistent improvements over baselines.
- Medium Confidence: Performance gains are promising but large dev-test gap suggests potential overfitting to development set or sensitivity to interview style variations.
- Low Confidence: Generalizability of LLM prompts across different cultural contexts and robustness when ASR quality degrades significantly are not thoroughly tested.

## Next Checks
1. Systematically inject controlled errors into ASR transcripts and measure degradation in downstream classification performance to quantify ASR-LLM error propagation effect.
2. Test the same LLM prompts on suicidal risk transcripts from different cultural contexts (e.g., Western hotline data) to assess prompt generalizability.
3. Benchmark the complete pipeline (ASR + LLM + fine-tuning + fusion) on GPU/CPU setup to measure inference latency and computational requirements for clinical deployment.