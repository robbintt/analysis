---
ver: rpa2
title: 'PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized
  feedback'
arxiv_id: '2502.18425'
source_url: https://arxiv.org/abs/2502.18425
tags:
- feedback
- students
- grading
- tutors
- pyevalai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PyEvalAI, an AI-assisted evaluation system
  for Jupyter Notebooks that combines unit tests with locally hosted language models
  to provide immediate personalized feedback while preserving privacy. The system
  enables students to receive rapid feedback on assignments, reducing the typical
  week-long grading delay.
---

# PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback

## Quick Facts
- arXiv ID: 2502.18425
- Source URL: https://arxiv.org/abs/2502.18425
- Reference count: 40
- Primary result: AI-assisted grading achieved 65.7% identical grading compared to human tutors with only -0.14% mean difference

## Executive Summary
PyEvalAI is an AI-assisted evaluation system for Jupyter Notebooks that combines unit tests with locally hosted language models to provide immediate personalized feedback while preserving privacy. The system reduces typical week-long grading delays to approximately 88 seconds, enabling students to iterate and improve their solutions rapidly. In a university numerics course case study with 20 students, PyEvalAI demonstrated strong grading accuracy while maintaining tutor oversight and reducing grading workload.

## Method Summary
The system uses a Tornado web server to coordinate notebook submissions via WebSocket connections, executing unit tests client-side for security. Solutions are then graded using a local LLM (AWQ-quantized Mistral Large) with multi-step prompting that incorporates test results for informed grading decisions. The pipeline includes automatic feedback generation, tutor review through a web interface, and persistent storage using pickle files. The system supports both text and code exercises with customizable test specifications.

## Key Results
- 65.7% identical grading compared to human tutors with mean difference of -0.14% and standard deviation of 20.73%
- Average feedback latency of 88.2 seconds compared to typical week-long delays
- Student scores improved by 25-30% from second-last to final submissions across multiple attempts
- Tutors accepted AI output unchanged in 57.8% of cases, reducing grading workload

## Why This Works (Mechanism)

### Mechanism 1
Combining unit tests with LLM-based evaluation improves grading accuracy over either approach alone. Unit tests provide deterministic criteria while the LLM handles qualitative assessment of explanations and derivations. Test results are fed as context to the LLM before it generates final scores and feedback.

### Mechanism 2
Immediate feedback enables iterative improvement, with students showing 25-30% score gains between penultimate and final attempts. Reducing feedback latency from ~1 week to ~88 seconds allows students to revise solutions while concepts remain fresh, creating an effective practice-feedback-revision loop.

### Mechanism 3
Human tutor oversight maintains grading quality while reducing workload through semi-automated pipeline. AI generates initial scores and feedback, tutors review via web interface and correct when needed. Tutors accepted AI output unchanged in 57.8% of cases.

## Foundational Learning

- **Asynchronous WebSocket communication**: Students submit exercises directly from Jupyter notebooks via `handin_exercise()` calls that use WebSocket connections to transmit solutions and receive feedback without blocking notebook execution. *Quick check: Can you explain why the system uses asynchronous submission rather than blocking until feedback is ready?*

- **LLM prompt engineering for structured grading**: The system uses multi-step prompting—task description → sample solution → student solution → test results → comparison → scoring → feedback generation—to produce consistent, actionable feedback. *Quick check: What might happen if you skipped the "compare sample and student solution" step in the prompt chain?*

- **Local LLM deployment for privacy**: Student data never leaves university infrastructure; the system uses AWQ-quantized Mistral Large model hosted on in-house A100 GPU, accessed via Huggingface's TGI toolkit with OpenAI-compatible API. *Quick check: What are the tradeoffs of using a quantized model versus full-precision model for grading accuracy?*

## Architecture Onboarding

- **Component map**: Tornado server -> WebSocket communication -> Notebook integration (pyevalai package) -> LDAP authentication -> Pickle storage -> Evaluation pipeline -> Local LLM (TGI server)

- **Critical path**: Student calls `handin_exercise()` → WebSocket transmits solution to Tornado server → Server stores submission, queues for evaluation → Test function sent back to notebook, executed client-side → Results returned → Server constructs prompt with task/sample/student/test results → LLM generates score + feedback → Results stored and displayed

- **Design tradeoffs**: Pickle storage is simple but not production-scalable; client-side test execution provides security but requires active connection; greedy sampling ensures determinism but may reduce feedback variety; AWQ quantization enables faster inference but may impact accuracy

- **Failure signatures**: LLM timeout/hallucination (feedback takes >3 minutes or contains fabricated test results); WebSocket disconnection during test execution (submission stuck in "pending" state); grade drift across similar submissions (minor solution changes produce inconsistent scores)

- **First 3 experiments**: 1) Latency profiling: Submit 50 test exercises and measure end-to-end feedback time breakdown; 2) Grading accuracy calibration: Have 2 human tutors independently grade 30 submissions blind, compare inter-tutor agreement rate versus AI-human agreement rate; 3) Prompt ablation: Run grading with and without unit test context provided to LLM to quantify unit test contribution

## Open Questions the Paper Calls Out

- How does the grading variance of PyEvalAI compare to the natural inter-rater reliability among human tutors? The study compared AI scores against an average of human grades but did not measure variance between human tutors themselves.

- Can prompting strategies or fine-tuning prevent grading instability caused by minor, non-semantic changes in student submissions? Despite using greedy sampling, the model exhibited sensitivity to small input changes, leading to inconsistent scores for similar solutions.

- Does PyEvalAI maintain its grading accuracy and latency when scaled to larger class sizes and applied to non-numerics disciplines? The case study was limited to 20 students in a numerics course; it's unknown if performance degrades for other subjects or larger cohorts.

## Limitations

- Single-course evaluation with only 20 students limits generalizability to larger cohorts and different subject domains
- AI-over-tutor accuracy reported rather than inter-rater reliability among human graders, which would establish baseline grading variance
- Analysis focuses on final outcomes without exploring intermediate feedback quality or detailed student learning trajectories

## Confidence

- **High confidence**: System architecture description, core technical mechanisms (WebSocket submission, prompt construction, local LLM deployment), and basic performance metrics
- **Medium confidence**: Pedagogical claims about feedback quality and student experience, based on self-reported perceptions from small sample
- **Low confidence**: Generalizability to other courses/disciplines and long-term learning impact, given single-course, single-term study design

## Next Checks

1. Conduct inter-rater reliability study with multiple human graders on same submissions to establish baseline grading variance before comparing against AI performance
2. Deploy in second course with different content and at least 50 students to test generalizability of grading accuracy and student experience
3. Implement A/B testing where half students receive immediate AI feedback and half receive traditional week-delayed human feedback, measuring both learning outcomes and student satisfaction