---
ver: rpa2
title: 'Causal LLM Routing: End-to-End Regret Minimization from Observational Data'
arxiv_id: '2505.16037'
source_url: https://arxiv.org/abs/2505.16037
tags:
- cost
- routing
- regret
- each
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first end-to-end causal framework for
  learning large language model (LLM) routing policies directly from observational
  data, without requiring full-feedback datasets. The proposed method minimizes decision-making
  regret by integrating counterfactual estimation and regret optimization into a single
  differentiable objective.
---

# Causal LLM Routing: End-to-End Regret Minimization from Observational Data

## Quick Facts
- arXiv ID: 2505.16037
- Source URL: https://arxiv.org/abs/2505.16037
- Reference count: 40
- One-line primary result: First end-to-end causal framework for LLM routing from observational data, achieving state-of-the-art performance without full-feedback datasets.

## Executive Summary
This paper introduces a causal framework for learning LLM routing policies from observational data, where only one model's outcome is observed per query. The method minimizes decision-making regret by integrating counterfactual estimation with regret optimization into a differentiable objective. It achieves this through doubly robust counterfactual estimation and two surrogate loss functions: a classification-based upper bound and a softmax-weighted regret approximation. Experiments on public benchmarks show the method outperforms existing baselines, including cost-aware routers, across different embedding models.

## Method Summary
The framework learns routing policies from observational data by first estimating counterfactual utilities using a doubly robust estimator that combines outcome and propensity models. It then trains a policy network to minimize decision regret using either a classification-based surrogate loss or a softmax-weighted differentiable approximation. The approach handles heterogeneous user cost preferences through an interval-conditioned architecture that interpolates between endpoint models. The method is trained in two stages: first estimating counterfactual utilities from observational data, then optimizing the routing policy with the differentiable surrogate loss.

## Key Results
- Achieves state-of-the-art performance on RouterBench and SPROUT benchmarks
- RM-Softmax generally outperforms RM-Classification in both utility and variance
- Interval-conditioned models generalize remarkably well to unseen budget levels
- Baseline methods ignoring treatment bias perform worst across most cost trade-off values

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Utility Estimation via Doubly Robust (DR) Estimation
The DR estimator corrects for treatment selection bias in observational data, enabling accurate counterfactual utility estimation without full-feedback datasets. It combines an outcome model with a propensity model, yielding consistent estimates if either is correctly specified. The core assumptions are ignorability (treatments independent of potential outcomes conditional on observed covariates) and support (all treatments have non-zero probability given covariates).

### Mechanism 2: Softmax-Weighted Regret Surrogate Recovers Optimal Policy at Convergence
The softmax-weighted surrogate loss provides a differentiable approximation to discrete regret minimization and provably concentrates probability mass on the optimal treatment at convergence. Instead of optimizing the non-differentiable argmax-based regret, the policy outputs a softmax distribution over treatments. Under fixed utility estimates, the inner product monotonically increases until the softmax becomes one-hot on the optimal treatment.

### Mechanism 3: Piecewise Constant Optimal Policy Enables Efficient Interpolation with Two Endpoint Models
Because utility is affine in the cost parameter λ, the optimal treatment is piecewise constant in λ, requiring only two endpoint models per interval for exact interpolation. With utility y = a - λc, each treatment's utility is a line in λ-space. The argmax of finitely many lines changes only at intersection points, so a linear interpolation of endpoint model outputs suffices within any interval.

## Foundational Learning

- **Concept: Inverse Propensity Weighting (IPW) and Doubly Robust Estimation**
  - Why needed here: Observational data contains treatment bias (historical policies preferentially routed queries to certain models). Without correction, learned routers inherit this bias. DR combines IPW with direct regression for robustness.
  - Quick check question: Given a dataset where GPT-4 was selected 80% of the time by the logging policy, how would you estimate the counterfactual accuracy of a smaller model that was rarely deployed?

- **Concept: Regret vs. Accuracy in Decision-Focused Learning**
  - Why needed here: Prior routing methods optimize prediction accuracy (predicting each model's performance) but ultimately care about decision quality. Regret directly measures the utility gap between the chosen and optimal treatment.
  - Quick check question: If a router achieves 95% accuracy in predicting model scores but consistently mispredicts the best model by small margins, would it have high or low regret?

- **Concept: Softmax Temperature and Differentiable Approximations to Discrete Decisions**
  - Why needed here: Routing is a discrete selection (argmax), which has zero gradients almost everywhere. The softmax-weighted surrogate smooths this, with temperature controlling the sharpness of the approximation.
  - Quick check question: As temperature τ → ∞, what does the softmax distribution approach? As τ → 0, what happens?

## Architecture Onboarding

- **Component map:** Query embeddings (BERT or LLaMA) -> Counterfactual Estimator (DR estimator) -> Policy Network (MLP with softmax) -> Interval-conditioned extension (joint model)

- **Critical path:**
  1. Precompute embeddings for all queries
  2. Train propensity model on logged data
  3. Train per-treatment outcome models on observed outcomes
  4. Compute DR estimates for all samples
  5. Identify pseudo-optimal treatments
  6. Train policy network via RM-Softmax or RM-Classification loss
  7. For interval models: train endpoint routers, then fine-tune joint interpolator

- **Design tradeoffs:**
  - RM-Classification vs. RM-Softmax: Classification is simpler but provides only an upper bound on regret. Softmax directly approximates regret but requires careful temperature tuning.
  - Embedding choice: BERT is faster; LLaMA embeddings capture more semantic nuance.
  - Clipping propensity weights: Reduces variance but introduces bias. Required for stability.

- **Failure signatures:**
  - High variance in DR estimates: Extreme propensity weights cause instability. Check propensity distribution; increase clipping or regularize propensity model.
  - RM-Softmax converges to uniform distribution: Temperature too high or utility estimates near-constant. Reduce τ or inspect Ŷ variance.
  - RM-Interval fails on intermediate λ: Endpoints not trained to convergence, or affine assumption violated. Verify endpoint performance first.

- **First 3 experiments:**
  1. Ablation on bias correction: Compare Baseline vs. R&C vs. RM-Softmax on same observational split. Expect: Baseline << R&C < RM-Softmax in utility.
  2. Temperature sweep for RM-Softmax: Test τ ∈ {10, 50, 100, 500, 1000} on validation regret.
  3. Interval generalization test: Train RM-Interval on λ ∈ {0, 200, 400, 600, 800, 1000}, evaluate on held-out λ ∈ {100, 300, 500, 700, 900}.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to accommodate hard constraints, such as strict latency limits or budget caps, which cannot be readily incorporated as soft penalties in the objective? The current methodology optimizes a utility function y = a - λc, which treats cost as a trade-off penalty rather than a strict cutoff.

### Open Question 2
Can the causal regret minimization approach be effectively adapted for online or adaptive routing in non-stationary environments where model performance or costs shift over time? The current framework is designed for batch learning from static observational data.

### Open Question 3
How can the framework be extended to multi-turn conversational settings where the routing decision for the current turn impacts the state and optimal decision for subsequent turns? The current formulation relies on SUTVA, implying independence between samples.

### Open Question 4
How does the framework perform on naturally occurring observational logs compared to the simulated observational data used in the experiments? Real-world observational data often contains unobserved confounders and complex selection biases.

## Limitations

- Reliance on ignorability and support assumptions for DR estimation may not hold in real-world routing scenarios with unobserved confounders
- Piecewise constant policy structure assumes affine utility in λ, which may not generalize to non-linear user preferences
- Performance depends heavily on propensity score overlap, which could be problematic in highly skewed logging policies

## Confidence

- **High Confidence:** Mechanism 1 (DR estimation corrects treatment bias) - well-established in causal inference literature and supported by ablation results
- **Medium Confidence:** Mechanism 2 (softmax-weighted surrogate converges to optimal policy) - theoretically sound but sensitive to temperature hyperparameter and utility estimate quality
- **Medium Confidence:** Mechanism 3 (interval interpolation sufficiency) - theoretically proven under affine utility assumption, but this may not generalize to all preference structures

## Next Checks

1. **Propensity Score Analysis:** Examine propensity score distributions across treatments to verify overlap assumptions. Compute propensity overlap statistics to quantify potential bias.

2. **Temperature Sensitivity Study:** Conduct systematic sweep of τ values (10-1000) to identify optimal settings and characterize convergence behavior across different utility landscapes.

3. **Generalization to Non-Affine Preferences:** Test interval models on datasets with non-linear utility functions (e.g., step functions or risk-averse preferences) to evaluate robustness when the affine assumption breaks.