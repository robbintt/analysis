---
ver: rpa2
title: 'TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health
  Record Time Series Generation'
arxiv_id: '2504.17613'
source_url: https://arxiv.org/abs/2504.17613
tags:
- data
- diffusion
- influence
- samples
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TarDiff addresses the challenge of generating synthetic EHR time
  series that enhance downstream clinical model performance, particularly for rare
  conditions. Unlike conventional methods that focus on mimicking data distributions,
  TarDiff integrates task-specific influence guidance into the diffusion process.
---

# TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation

## Quick Facts
- **arXiv ID:** 2504.17613
- **Source URL:** https://arxiv.org/abs/2504.17613
- **Reference count:** 29
- **Primary result:** Generates synthetic EHR time series that improve downstream clinical model performance by up to 20.4% in AUPRC and 18.4% in AUROC

## Executive Summary
TarDiff addresses the challenge of generating synthetic EHR time series that enhance downstream clinical model performance, particularly for rare conditions. Unlike conventional methods that focus on mimicking data distributions, TarDiff integrates task-specific influence guidance into the diffusion process. It quantifies each synthetic sample's impact on downstream loss via influence functions, steering generation toward samples that maximize utility. Evaluated on six clinical datasets, TarDiff outperforms baselines by up to 20.4% in AUPRC and 18.4% in AUROC, demonstrating improved fidelity and task alignment. It also mitigates class imbalance by naturally prioritizing minority-class guidance, without explicit reweighting. The approach incurs minimal computational overhead and offers scalable, privacy-preserving generation of clinically useful synthetic EHR data.

## Method Summary
TarDiff modifies the standard diffusion reverse process by injecting task-specific guidance gradients into the denoising mean. It pre-computes a cached gradient vector from a guidance set using a frozen downstream classifier, then during sampling modifies the predicted denoising mean with this influence signal. The approach combines a pre-trained conditional 1D U-Net diffusion model with a frozen downstream task model (TimesNet), using influence functions to guide generation toward samples that maximize downstream task utility rather than simply mimicking the training distribution.

## Key Results
- Outperforms baselines by up to 20.4% in AUPRC and 18.4% in AUROC on six clinical datasets
- Demonstrates improved fidelity and task alignment through target-oriented guidance
- Mitigates class imbalance naturally by prioritizing minority-class guidance without explicit reweighting
- Incurs minimal computational overhead compared to traditional generation approaches

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Steering of the Reverse Process
TarDiff modifies the mean of the Gaussian transition in the reverse diffusion process by adding a guidance term $\alpha \nabla_{x_t} \Delta L_T$. This steers generation toward samples that maximize utility for the specific downstream task. The gradient of the influence function at an intermediate noisy state $x_t$ is assumed to be sufficiently correlated with the utility of the final denoised sample $x_0$. If guidance scale $\alpha$ is too high, samples may be pushed out of the valid data manifold or cause mode collapse.

### Mechanism 2: Implicit Minority-Class Upsampling
The influence guidance mechanism naturally prioritizes rare conditions because minority samples exhibit higher gradient norms during loss computation. These samples are harder to classify and thus generate larger gradient magnitudes, receiving stronger guidance signals during generation. This assumes gradient norm is a reliable proxy for sample hardness or informational value in EHR contexts. If the downstream model overfits to the minority class, gradient norms might become erratic and generate erroneous samples.

### Mechanism 3: Efficient Influence Approximation
TarDiff avoids the computational cost of retraining the downstream model for every generated sample by using a cached gradient vector $G$ derived from a guidance set. This pre-computed vector is leveraged during sampling to guide the diffusion model. The approach assumes the cached vector generalizes well across diffusion time steps and captures essential optimization direction. If the guidance set is too small or unrepresentative, the cached vector will provide a biased steering signal.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - Why needed: TarDiff operates by reversing a forward noise process, requiring understanding of transition kernels $q(x_t|x_{t-1})$ and noise predictor $\hat{\epsilon}_\theta$
  - Quick check: Can you explain why the guidance term is added to the mean of the reverse transition distribution rather than the variance?

- **Concept: Influence Functions (Koh & Liang, 2017)**
  - Why needed: TarDiff adapts influence functions—typically used to debug static datasets—to guide a generative process
  - Quick check: How does Equation 20 approximate the parameter change $\delta_\phi$ without explicitly calculating the inverse Hessian matrix?

- **Concept: Classifier Guidance vs. Classifier-Free Guidance**
  - Why needed: TarDiff is a variant of "Classifier Guidance" using task-specific influence functions instead of generic classifiers
  - Quick check: In standard classifier guidance, what does the gradient $\nabla_{x_t} \log p(y|x_t)$ represent geometrically in the sample space?

## Architecture Onboarding

- **Component map:** Pre-trained Diffusion Backbone (1D U-Net) -> Downstream Task Model (TimesNet) -> Guidance Gradient Cache (G) -> Influence-Guided Sampler

- **Critical path:**
  1. Pre-train Diffusion Model on full EHR dataset
  2. Train Downstream Model on training split and compute cached gradient vector G using validation split as guidance set
  3. Run reverse diffusion loop, computing influence gradient at each step and updating denoising mean

- **Design tradeoffs:**
  - Guidance Set Size: Larger sets offer more stable gradient G but increase preprocessing time
  - Guidance Scale (w): Low w mimics real data; high w prioritizes task performance at risk of realism (Figure 3)

- **Failure signatures:**
  - Gradient Mismatch: Under-trained downstream model creates noisy gradients, leading to unstable generation
  - Unrealistic Artifacts: High guidance scale generates physiologically implausible spikes designed solely to minimize downstream loss

- **First 3 experiments:**
  1. TSTR Baseline: Generate samples using TarDiff and only these samples to train a new classifier; test on real data to verify learnable signal
  2. Ablation on Guidance: Run sampling with guidance scale w=0 (standard diffusion) vs. w=1000 (TarDiff) to quantify performance lift
  3. Class Balance Check: Visualize gradient norms of guidance set; verify minority classes have higher norms in your dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several implications arise regarding generalization to different clinical tasks and model architectures.

## Limitations
- Relies heavily on accurate influence function approximations that may degrade for complex, non-convex loss landscapes
- Implicit minority-class balancing assumes gradient norms reliably indicate sample hardness across all architectures and tasks
- Guidance mechanism effectiveness depends on careful tuning of guidance scale parameter with no clear universal heuristic

## Confidence
- **High Confidence:** Core diffusion guidance mechanism and its integration with influence functions
- **Medium Confidence:** Implicit minority-class balancing claims - theoretically sound but needs broader empirical validation
- **Medium Confidence:** Computational efficiency claims - sound approach but real-world overhead depends on implementation details

## Next Checks
1. Test TarDiff's performance when the downstream model exhibits significant class imbalance in the guidance set itself
2. Evaluate whether TarDiff-generated samples maintain utility when the downstream model architecture changes
3. Measure the sensitivity of downstream performance to guidance scale parameter across multiple clinical datasets to establish robust selection heuristics