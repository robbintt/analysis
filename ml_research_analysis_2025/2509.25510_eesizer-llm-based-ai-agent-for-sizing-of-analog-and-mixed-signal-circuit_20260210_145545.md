---
ver: rpa2
title: 'EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit'
arxiv_id: '2509.25510'
source_url: https://arxiv.org/abs/2509.25510
tags:
- circuit
- performance
- design
- sizing
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents EEsizer, an LLM-based AI agent for automated
  transistor sizing in analog and mixed-signal (AMS) circuits. The core method employs
  Chain-of-Thought prompting combined with function calling to integrate LLMs with
  circuit simulators and custom data analysis functions, enabling closed-loop optimization
  without external knowledge.
---

# EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit

## Quick Facts
- **arXiv ID:** 2509.25510
- **Source URL:** https://arxiv.org/abs/2509.25510
- **Reference count:** 34
- **Primary result:** LLM-based agent achieves automated transistor sizing for AMS circuits across 180 nm, 130 nm, and 90 nm nodes with 76–100% pass rates for key metrics.

## Executive Summary
EEsizer is an LLM-based AI agent that automates transistor sizing in analog and mixed-signal circuits through a closed-loop integration with SPICE simulators. Using Chain-of-Thought prompting and function calling, the agent iteratively explores design spaces, evaluates performance, and refines solutions without requiring external knowledge bases. The system was validated on six basic circuits and a 20-transistor CMOS opamp, demonstrating the ability to meet target specifications across multiple technology nodes while reducing manual design effort.

## Method Summary
EEsizer employs a Reasoning and Acting (ReAct) loop where an LLM receives structured Chain-of-Thought prompts containing circuit context, previous results, and performance targets. The agent uses function calling to interact with Ngspice simulator and custom analysis functions, enabling automated DC, AC, and transient simulations. Each iteration involves verifying transistor operating regions, identifying failing specifications, mapping parameter-performance relationships from historical data, and proposing netlist modifications. The process continues until all targets are met or iteration limits are reached, with results feeding back into the next prompt for in-context learning.

## Key Results
- OpenAI o3 achieved target performance at 90 nm within 20 iterations across three test groups.
- Variation analysis using Gaussian-distributed transistor sizes and threshold voltages showed robustness, with minimum 76% pass rate for gain.
- The agent successfully sized a 20-transistor CMOS opamp at 180 nm, 130 nm, and 90 nm technology nodes.
- All basic circuits (5T-OTA, folded-cascode, current-mirror OTA) met specifications within 20-25 iterations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting enables LLMs to capture performance trade-offs and make informed sizing decisions without domain-specific fine-tuning.
- Mechanism: The CoT template injects four structured reasoning steps—verify transistor operating regions, identify failing specifications, observe parameter-performance relationships from history, and propose constrained optimizations. This transforms implicit LLM knowledge into explicit, verifiable reasoning chains that guide each iteration.
- Core assumption: LLMs possess sufficient pre-trained analog design knowledge to generate valid parameter adjustments when properly prompted with circuit context and historical feedback.
- Evidence anchors:
  - [abstract] "By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention."
  - [Section II.D] "The CoT instructions include four steps: (1) verify transistor regions, (2) identify performances needing improvement, (3) connect design parameters with performance metrics, and (4) propose optimization within constraints."
  - [corpus] Neighbor paper AnaFlow (arxiv:2511.03697) similarly employs "reasoning-driven" workflows, suggesting CoT-style prompting is an emerging pattern for AMS sizing—but effectiveness varies by model and circuit complexity.
- Break condition: CoT guidance degrades when (a) circuit topology exceeds LLM's pre-trained knowledge (e.g., novel architectures not in training data), or (b) the design space requires precise mathematical relationships the LLM cannot infer from context alone.

### Mechanism 2
- Claim: Function calling creates a closed-loop simulator integration that compensates for LLMs' limited mathematical precision while maintaining interpretability.
- Mechanism: The agent constructs structured prompts → LLM responds with function calls and arguments → Ngspice executes simulation → custom analysis functions extract metrics → results return as natural language feedback. This ReAct-style loop (Reasoning + Acting) grounds LLM proposals in physical simulator outputs, preventing drift into invalid design regions.
- Core assumption: Simulation time per iteration remains tractable, and the feedback loop converges before hitting iteration or cost limits.
- Evidence anchors:
  - [abstract] "integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge."
  - [Section II.E] "Function calling is a structured communication protocol which allows LLMs to interact with external tools, thereby improving their performance in specialized domains."
  - [Section II.G] "The agent communicates with different LLMs by constructing structured prompts and passing them through the API, which then returns responses that guide subsequent actions in the simulation loop."
  - [corpus] Corpus evidence on function-calling mechanisms is sparse—neighbor papers do not explicitly detail simulator integration protocols. This limits cross-validation of the mechanism's generality.
- Break condition: The loop fails to converge when (a) LLM proposals oscillate between similar design points without progress, (b) iteration/cost budgets are exhausted before meeting specifications, or (c) the performance metric landscape has local minima the LLM cannot escape via contextual reasoning alone.

### Mechanism 3
- Claim: In-context learning from simulation history enables the LLM to infer parameter-performance trade-offs without explicit training data.
- Mechanism: Each iteration's prompt includes previous results (netlist + performance) alongside current results. This accumulating history provides examples of parameter changes and their effects, allowing the LLM to detect patterns (e.g., "W ↑ ⇒ gm ↑ ⇒ gain ↑") through few-shot-style learning within the context window.
- Core assumption: The LLM can reliably infer causal relationships from noisy, sparse examples in the prompt history, and context window limits do not truncate critical information.
- Evidence anchors:
  - [Section II.D] "The previous results, current results and target performance provide highly relevant context for LLMs, enabling effective in-context learning, which helps LLMs better understand the relationship between performance and parameters."
  - [Fig. 5 example] "Observed parameter–performance relationships: Differential-pair PMOS load (M3, M4): W ↑ ⇒ gm_p ↑, resulting differential-pair transconductance rises ⇒ overall gain ↑"
  - [corpus] EEschematic (arxiv:2510.17002) and AnaFlow both leverage in-context learning for AMS tasks, suggesting this is a common strategy, but corpus does not provide rigorous analysis of context-length scaling or failure modes.
- Break condition: In-context learning degrades when (a) history exceeds context window capacity, losing earlier informative iterations, (b) the parameter-performance relationships are too complex or non-monotonic to infer from few examples, or (c) LLM misattributes correlations as causation, leading to counterproductive adjustments.

## Foundational Learning

- Concept: **Analog/Mixed-Signal Circuit Sizing and PPA Trade-offs**
  - Why needed here: The agent operates on SPICE netlists and must understand that transistor dimensions (W, L) affect gain, bandwidth, phase margin, power, and area—often with conflicting requirements. Without this foundation, you cannot interpret the agent's reasoning or diagnose failures.
  - Quick check question: Given a simple common-source amplifier, if you increase the input transistor width by 2× while keeping length constant, what happens to gain, bandwidth, and power? (Answer: gm increases → gain and bandwidth typically increase, but power also increases due to higher drain current.)

- Concept: **ReAct Pattern (Reasoning + Acting) and Tool-Using Agents**
  - Why needed here: EEsizer implements a ReAct loop where the LLM reasons about circuit state, calls simulator functions, observes results, and iterates. Understanding this pattern is essential for debugging the agent's decision flow and extending it to new tools or metrics.
  - Quick check question: In a ReAct loop, what is the risk if the "observation" step provides noisy or misleading feedback? (Answer: The LLM may reinforce incorrect parameter-performance beliefs, leading to oscillation or divergence from the target.)

- Concept: **Chain-of-Thought Prompting and In-Context Learning**
  - Why needed here: The agent relies on CoT to structure its reasoning and on in-context examples (previous iteration results) to infer design relationships. You need to understand how prompt construction affects LLM behavior to tune the system.
  - Quick check question: If the CoT prompt instructs the LLM to "verify all transistors are in saturation" but the simulator reports subthreshold operation, what should the next reasoning step prioritize? (Answer: Adjust bias voltages or transistor dimensions to bring devices into strong inversion before optimizing other metrics.)

## Architecture Onboarding

- Component map:
  User Input -> Task Decomposition -> LLM Engine -> Function Calling Layer -> Ngspice Simulator -> Analysis Functions -> Comparison & Feedback Loop -> Output

- Critical path:
  1. User provides netlist + targets
  2. Task decomposition generates context (circuit type, nodes, mapped functions)
  3. LLM receives CoT prompt with history + constraints
  4. LLM outputs reasoning + function calls
  5. Simulator executes, analysis functions extract metrics
  6. Comparison checks pass/fail; if fail, loop to step 3 with updated history
  7. If pass or max iterations reached, output final netlist + reasoning

- Design tradeoffs:
  - **Model choice:** Larger reasoning models (o3) → higher success rate but longer latency and cost (~27s/iteration vs. ~1.5s for Gemini 2.0 Flash). Faster models → lower success rate, more variability.
  - **Iteration budget:** Paper uses 20–25 iterations; more iterations do not guarantee convergence (LLM can oscillate). Budget constrained by API costs.
  - **Context window:** Accumulating history improves in-context learning but risks truncation; balancing recency vs. completeness is unresolved.
  - **Strong-inversion constraint:** Enforced to reduce Monte Carlo complexity, but limits exploration of subthreshold designs.

- Failure signatures:
  - **Oscillation:** LLM alternates between similar W/L values without metric improvement (visible in iteration logs as repetitive patterns).
  - **Early saturation:** Performance plateaus below target, and LLM reasoning becomes circular ("increase W to improve gain" repeatedly without effect).
  - **Constraint violation:** LLM proposes bias voltages outside valid range or W/L ratios that push transistors into unintended regions.
  - **Node-specific failure:** Advanced nodes (90 nm) show higher failure rates due to short-channel effects; if 180 nm succeeds but 90 nm fails, suspect model physics not captured in LLM's reasoning.

- First 3 experiments:
  1. **Baseline replication on simple circuit:** Run EEsizer on a 5T-OTA with 180 nm PTM models using OpenAI o3. Target: gain ≥40 dB, bandwidth ≥1 MHz, phase margin ≥60°. Measure iterations to convergence and compare to paper's reported ~10–15 iterations. This validates your environment setup.
  2. **Model comparison on identical task:** Run the same 5T-OTA optimization with Gemini 2.0 Flash. Expect higher variability and potentially more iterations. Log where reasoning diverges from o3's to understand model-dependent behavior.
  3. **Node transfer stress test:** Take a successful 180 nm opamp configuration and run optimization at 90 nm with identical targets. Observe if the agent adapts or fails, and which metrics degrade first (paper suggests gain/UGBW are most sensitive). This tests generalization claimed in the abstract.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating Monte Carlo simulation directly into the LLM agent's optimization loop improve robustness pass rates for gain and bandwidth beyond the current 76–78%?
- Basis in paper: [explicit] "a full Monte Carlo simulation with accurate device models incorporating Monte Carlo parameters should also be used to rigorously evaluate design robustness... not all performance metrics achieved a 100% pass rate, highlighting the need to integrate Monte Carlo simulation into the agent's workflow."
- Why unresolved: Current optimization uses a simplified $V_{gs}-V_{th} > 0$ check to enforce strong inversion and reduce simulation burden, rather than true process variation analysis during sizing.
- What evidence would resolve it: Implementation of in-loop Monte Carlo sampling with foundry statistical models, comparing pass rates for gain/UGBW against the current 76–78% baseline.

### Open Question 2
- Question: Does incorporating layout constraints and post-layout simulation into EEsizer's optimization preserve the convergence efficiency observed in schematic-level sizing?
- Basis in paper: [explicit] "layout and post-layout simulation were not considered in this study... Incorporating area and layout constraints in future work would enable a more complete PPA trade-off."
- Why unresolved: The 20-transistor opamp was optimized at schematic level only; parasitic extraction effects on the 10 targeted metrics (especially UGBW and phase margin) remain unquantified.
- What evidence would resolve it: Running EEsizer with parasitic-aware netlists and comparing iteration counts and success rates against schematic-only baseline.

### Open Question 3
- Question: Can LLMs automatically generate analog testbenches and analysis functions that match the performance of the manually designed functions in Table III?
- Basis in paper: [explicit] "the analysis functions for processing simulation results are currently designed manually... Future work could explore leveraging LLMs for automatic analog testbench generation to improve adaptability."
- Why unresolved: The 16 pre-defined functions (simulation, gain, offset, ICMR, THD, etc.) require developer expertise; whether LLMs can synthesize equivalent measurement setups for arbitrary circuits is unknown.
- What evidence would resolve it: Benchmarking LLM-generated testbenches against hand-crafted ones across diverse circuit types, measuring extraction accuracy for each metric.

### Open Question 4
- Question: Would integrating noise analysis (input-referred noise, SNR) with accurate foundry models reveal fundamental trade-offs not captured in the current 10-metric optimization?
- Basis in paper: [explicit] "The PTM model we used may not include noise parameters, making accurate noise simulation infeasible. An accurate noise model will enable for an input referred noise and SNR performance measurement."
- Why unresolved: Gain-bandwidth optimization may inadvertently degrade noise performance, but this cannot be assessed without noise-enabled models.
- What evidence would resolve it: Re-optimizing the opamp with foundry noise models, comparing noise figures between EEsizer solutions and noise-aware manual designs.

## Limitations
- **Bias circuit netlist unspecified:** Variation analysis uses a specific bias circuit not fully detailed in text, requiring manual reconstruction.
- **Manual analysis functions:** The 16 pre-defined metric extraction functions require developer expertise and limit adaptability to new circuits.
- **Model-dependent convergence:** OpenAI o3 performs best, but lack of rigorous statistical comparison across all tested models (GPT-4.1, Gemini 2.0 Flash) leaves convergence reliability uncertain.

## Confidence
- **High:** Core ReAct loop integration with simulator, use of PTM models for multiple nodes, and basic functionality on simple circuits (5T-OTA, folded-cascode) are well-documented and reproducible.
- **Medium:** The 90 nm success rates and variation analysis results are plausible but require careful replication of the bias circuit and exact PTM models; model-specific behaviors (e.g., GPT-4.1 variability) are described but not deeply analyzed.
- **Low:** Claims about generalization to "any circuit" or "any technology node" are aspirational; the corpus shows limited evidence of EEsizer's performance beyond the six tested circuits and three PTM nodes.

## Next Checks
1. **Prompt Fidelity Test:** Reproduce the 5T-OTA optimization at 180 nm using OpenAI o3 with the reconstructed CoT prompt. Measure iterations to convergence and compare reasoning quality to paper's reported ~10–15 iterations.
2. **Model Variability Benchmark:** Run the same 5T-OTA task with Gemini 2.0 Flash and GPT-4.1. Log iteration counts, convergence status, and reasoning patterns to quantify model-dependent performance gaps.
3. **Node Transfer Stress Test:** Take a successful 180 nm opamp configuration and optimize at 90 nm with identical targets. Track which metrics (gain, bandwidth, phase margin) degrade first and whether the agent adapts or fails, validating the paper's claim of technology-node adaptability.