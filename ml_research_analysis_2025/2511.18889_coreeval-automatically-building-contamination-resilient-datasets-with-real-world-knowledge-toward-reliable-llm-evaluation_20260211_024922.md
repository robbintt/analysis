---
ver: rpa2
title: 'CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World
  Knowledge toward Reliable LLM Evaluation'
arxiv_id: '2511.18889'
source_url: https://arxiv.org/abs/2511.18889
tags:
- data
- contamination
- performance
- test
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data contamination in large language model
  evaluation by proposing CoreEval, a framework that automatically builds contamination-resilient
  datasets. The core method extracts entity relationships from original data, retrieves
  real-world knowledge from the GDELT database, recontextualizes this knowledge with
  the original data, and employs a reflection mechanism to ensure label consistency.
---

# CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation

## Quick Facts
- **arXiv ID:** 2511.18889
- **Source URL:** https://arxiv.org/abs/2511.18889
- **Reference count:** 38
- **Primary result:** Reduces data contamination overestimation by 5.42% for proprietary models vs. 3.62% for open-source models

## Executive Summary
CoreEval is a framework designed to automatically build contamination-resilient datasets for LLM evaluation by leveraging real-world knowledge from the GDELT database. The method extracts entity relationships from original data, retrieves up-to-date knowledge that postdates model training cutoffs, recontextualizes this knowledge with the original data, and employs a reflection mechanism to ensure label consistency. Experiments across five NLP tasks show CoreEval significantly reduces performance overestimation caused by data contamination, with proprietary models exhibiting higher contamination sensitivity than open-source models.

## Method Summary
CoreEval addresses data contamination by automatically updating existing benchmark datasets using real-world knowledge. The pipeline extracts entities from original text, queries the GDELT database for recent events (post-dating model training cutoffs), summarizes retrieved knowledge, and replaces relational triples in the original text with updated triples while preserving semantic style. A reflection mechanism iteratively verifies factuality and label alignment, regenerating samples that fail checks. The framework was evaluated on five classification tasks from TweetEval and GLUE, using zero-shot inference on 11 LLMs and LoRA fine-tuning to simulate contamination effects.

## Key Results
- CoreEval significantly reduces performance overestimation caused by data contamination
- Proprietary models show higher contamination sensitivity (5.42% performance drop) compared to open-source models (3.62%)
- The framework demonstrates superior contamination resistance compared to existing methods like data rewriting and generation
- Human evaluation confirms high data quality with substantial inter-annotator agreement (κ = 0.73-0.86)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieving temporally-bounded real-world knowledge from GDELT creates data samples unlikely to exist in pre-training corpora.
- **Mechanism:** Entity extraction → GDELT query with temporal bounds (post-dating latest model release) → knowledge summarization. The temporal constraint ensures retrieved events occurred after model training cutoffs, preventing memorization-based shortcuts.
- **Core assumption:** Models cannot generalize to genuinely novel entity-relation combinations they haven't encountered during training.
- **Evidence anchors:** [abstract] "leveraging the GDELT database to retrieve relevant, up-to-date knowledge" [section 3.2] "we chose the release date of the latest open-source model as the starting point for retrieval to prevent overlap with the model's training data"

### Mechanism 2
- **Claim:** Structured triple substitution preserves task-relevant semantic complexity while replacing factual content.
- **Mechanism:** Extract relational triples Ti = ⟨entity, relation, entity⟩ from original text → generate replacement triples using new knowledge → synthesize updated sentence combining original semantic style with new triples. This maintains cognitive task difficulty (irony, stance detection) while changing surface-level facts.
- **Core assumption:** Task difficulty stems from reasoning patterns (stance inference, irony detection) rather than specific entity knowledge.
- **Evidence anchors:** [abstract] "recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence" [section 3.3] "du_i ← f(d_i, T̂_i) where f is the replacement operation"

### Mechanism 3
- **Claim:** Iterative reflection with explicit verification criteria reduces label drift and factual hallucination.
- **Mechanism:** LLM agent evaluates generated text against two criteria—(1) factual consistency with retrieved knowledge, (2) label alignment with ground truth—and flags samples requiring regeneration. This creates a feedback loop missing in one-shot generation approaches.
- **Core assumption:** LLMs can reliably assess their own outputs when given explicit evaluation criteria and retrieved evidence.
- **Evidence anchors:** [abstract] "a robust data reflection mechanism is employed to iteratively verify and refine labels" [section 3.4] "The assessment focuses on two key criteria: Incorrect Information... Label Alignment"

## Foundational Learning

- **Concept: Data Contamination vs. Generalization**
  - Why needed here: The paper's core premise distinguishes legitimate generalization from memorization of test data; confusing these leads to misinterpreting results.
  - Quick check question: If a model scores 95% on a benchmark released before its training cutoff, can you conclude it has contaminated training data?

- **Concept: Relation Triple Extraction (Subject-Relation-Object)**
  - Why needed here: CoreEval's recontextualization depends on decomposing sentences into structured triples before substitution.
  - Quick check question: What triples can you extract from "Apple announced the iPhone 15 in September 2023"?

- **Concept: n-gram Overlap Detection Limitations**
  - Why needed here: The paper critiques simple contamination detection; understanding why paraphrasing evades detection contextualizes the problem scope.
  - Quick check question: Why would "The movie was fantastic" and "The film was excellent" show low n-gram overlap despite identical meaning?

## Architecture Onboarding

- **Component map:**
  1. **Entity Extractor** (LLM-based): Input original sentence → Output entity set Ei
  2. **Knowledge Retriever** (GDELT + BigQuery): Input entities + time window → Output raw knowledge Ki
  3. **Knowledge Summarizer** (LLM): Input raw knowledge → Output condensed knowledge K̂i
  4. **Triple Extractor** (LLM): Input original sentence → Output triples Ti
  5. **Triple Updater** (LLM): Input original triples + new knowledge → Output replacement triples T̂i
  6. **Sentence Generator** (LLM): Input original sentence + updated triples + semantic style → Output updated sentence d̂i
  7. **Reflection Agent** (LLM): Input generated text + knowledge + label → Output accept/regenerate decision

- **Critical path:** Entity extraction → GDELT retrieval (external API dependency) → Triple substitution → Reflection loop. The GDELT query is the primary external dependency; failures here cascade downstream.

- **Design tradeoffs:**
  - GDELT recency vs. coverage: Very recent time windows ensure contamination resistance but may return sparse results for niche entities
  - Reflection iterations vs. cost: More iterations improve quality but increase LLM API costs linearly
  - Semantic preservation vs. novelty: Aggressive triple replacement increases contamination resistance but risks incoherence

- **Failure signatures:**
  - Empty GDELT results → downstream hallucination (no factual grounding)
  - High variance across reflection runs → unstable dataset quality
  - Label alignment failures on irony/stance tasks → semantic complexity lost during rewriting

- **First 3 experiments:**
  1. **Sanity check:** Run entity extraction on 10 samples from each dataset; verify GDELT returns non-empty results for >80% of entity sets
  2. **Contamination validation:** Fine-tune a small model (e.g., Llama3-8B with LoRA) on original test set → measure δ1 on original vs. CoreEval-updated test set; expect higher δ1 on original (confirming contamination sensitivity)
  3. **Reflection ablation:** Generate dataset with reflection disabled; compare label accuracy against human-verified subset; quantify error rate increase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CoreEval framework be effectively adapted for generative tasks like question answering and summarization?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "In this study, CoreEval is applied only to classification tasks. In the future, we plan to extend its application to more complex tasks such as question answering and summarization."
- Why unresolved: The current Data Reflection mechanism relies on verifying alignment with ground truth labels (e.g., "entailment" vs. "not entailment"), a validation method that does not apply to open-ended text generation.
- What evidence would resolve it: A modification of the reflection component to assess semantic consistency and factual accuracy in generative outputs, validated on a benchmark like LiveBench.

### Open Question 2
- Question: Why do proprietary models exhibit a significantly higher performance drop (5.42%) compared to open-source models (3.62%) on updated datasets?
- Basis in paper: [inferred] The paper reports this discrepancy in the experimental results, suggesting it implies proprietary models may suffer from more severe data contamination due to opaque training data, but it does not verify the cause.
- Why unresolved: The "black box" nature of proprietary models prevents the authors from determining if the drop is caused by larger training corpora, different architectures, or specific data-cleaning strategies.
- What evidence would resolve it: A controlled analysis comparing model performance relative to known training data cutoffs or a study correlating model size and training data opacity with contamination sensitivity.

### Open Question 3
- Question: To what extent does the reliance on the GDELT database limit the framework's applicability to tasks requiring static or non-event-based knowledge?
- Basis in paper: [inferred] The method is explicitly built on retrieving "real-time knowledge" from GDELT (a database of global events) to update data. While effective for dynamic topics, the paper does not test tasks requiring stable, encyclopedic knowledge that may not appear in recent event logs.
- Why unresolved: It is unclear if the Real-World Knowledge Attainment component can retrieve sufficient context for tasks unrelated to specific recent events (e.g., scientific reasoning or static historical fact-checking).
- What evidence would resolve it: Experiments applying CoreEval to domains outside of news/current events, utilizing a broader range of knowledge sources beyond GDELT.

## Limitations
- Framework relies heavily on GDELT database coverage, which may be sparse for niche entities
- Current implementation limited to classification tasks; generative task adaptation remains unexplored
- Reflection mechanism effectiveness depends on LLM evaluation capabilities and prompt quality

## Confidence
- **High confidence**: Contamination resistance metrics (δ1, δ2) showing CoreEval datasets reduce performance overestimation
- **Medium confidence**: Semantic preservation during triple substitution - while reflection reduces label drift, the quality of integrated knowledge remains dependent on GDELT coverage
- **Low confidence**: Generalizability to non-classification tasks - the framework is validated only on five classification benchmarks

## Next Checks
1. **GDELT Coverage Analysis**: For each entity extracted from the original datasets, measure the percentage of successful GDELT retrievals with non-empty results. Analyze whether entity specificity correlates with retrieval failure rates.
2. **Human Evaluation of Generated Samples**: Conduct blind human assessments comparing original vs. CoreEval-updated samples for factual consistency and semantic coherence, focusing on samples flagged during reflection iterations.
3. **Cross-Task Generalization**: Apply CoreEval to a different NLP task type (e.g., summarization or question answering) to evaluate whether the triple-substitution approach generalizes beyond classification benchmarks.