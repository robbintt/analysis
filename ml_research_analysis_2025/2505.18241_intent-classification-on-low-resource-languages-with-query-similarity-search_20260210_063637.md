---
ver: rpa2
title: Intent Classification on Low-Resource Languages with Query Similarity Search
arxiv_id: '2505.18241'
source_url: https://arxiv.org/abs/2505.18241
tags:
- classification
- intent
- query
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of intent classification for
  low-resource languages, where data annotation is difficult and expensive. The authors
  propose a novel approach that casts intent classification as a query similarity
  search problem.
---

# Intent Classification on Low-Resource Languages with Query Similarity Search

## Quick Facts
- arXiv ID: 2505.18241
- Source URL: https://arxiv.org/abs/2505.18241
- Authors: Arjun Bhalla; Qi Huang
- Reference count: 23
- Primary result: Achieves 48-55% accuracy on low-resource intent classification without requiring any annotated data from target language

## Executive Summary
This paper addresses the challenge of intent classification for low-resource languages where data annotation is difficult and expensive. The authors propose a novel approach that casts intent classification as a query similarity search problem, using pre-existing example queries to define intents and classifying incoming queries based on the labels of their most similar queries in latent space. The method demonstrates that reasonable performance can be achieved for low-resource languages in a zero-shot setting, without requiring annotated data from the target language. The approach outperforms alternatives like machine translation and direct classification training in low-resource scenarios, while being simpler to implement and requiring no retraining or re-indexing when adding new languages.

## Method Summary
The method casts intent classification as a k-nearest neighbor similarity search problem in multilingual embedding space. Pre-trained multilingual sentence encoders (LaBSE, XLM-RoBERTa) map queries to dense vectors where semantically similar queries cluster together regardless of language. The system indexes labeled queries from high-resource languages, then classifies incoming low-resource queries by computing their embeddings and finding the k most similar indexed queries. A majority vote among the retrieved neighbor labels produces the final intent prediction. The approach requires no fine-tuning, no target-language data, and no retraining when new languages are added.

## Key Results
- Zero-shot accuracy of 48-55% on low-resource languages (Swahili, Urdu, Indonesian) without any target-language training data
- Outperforms translation-based baselines and direct classification training on low-resource languages
- Indexing only high-resource language queries provides comparable performance to indexing all available languages
- Method achieves reasonable performance across multiple datasets (ATIS, HINT3, CLINC-150, MASSIVE) with C-way N-shot sampling

## Why This Works (Mechanism)

### Mechanism 1
- Multilingual sentence encoders create language-agnostic semantic representations where queries with similar intent cluster together regardless of language
- Pre-trained encoders map semantically similar text to proximate vectors in shared latent space, enabling cross-lingual similarity matching
- Works because encoders have aligned representations across high-resource and low-resource languages, though alignment quality varies by language pair
- Breaks when target language is poorly represented in encoder pre-training data (e.g., XLM-RoBERTa achieves only 11.9% on Swahili vs. 56.1% on Indonesian)

### Mechanism 2
- k-nearest neighbor voting provides robust label prediction by aggregating signals from multiple similar examples
- Majority voting over k=31 neighbors smooths over individual retrieval errors and reduces sensitivity to noisy matches
- Assumes optimal k transfers across datasets and languages, found through grid search on ATIS but not universally validated
- Breaks when intent classes have uneven distributions or semantically overlapping definitions, requiring lower k (e.g., k=5 for HINT3)

### Mechanism 3
- Indexing only high-resource language queries provides sufficient semantic coverage for zero-shot classification
- Cross-lingual alignment in multilingual encoders enables semantic matching without target-language examples in index
- Assumes semantic concepts are language-independent and encoder has learned to map equivalent concepts from different languages to similar embeddings
- Breaks when culture-specific or language-specific intent formulations lack semantic equivalents in high-resource languages

## Foundational Learning

- **Approximate Nearest Neighbor (ANN) Search with FAISS**
  - Why needed: Efficiently finding k-nearest neighbors in high-dimensional embedding space across potentially millions of indexed queries
  - Quick check: Given a query embedding of dimension 768 and an index of 100,000 labeled queries, how would you efficiently retrieve the 31 nearest neighbors without exhaustive distance computation?

- **Multilingual Sentence Embeddings and Cross-Lingual Alignment**
  - Why needed: Understanding why LaBSE and XLM-RoBERTa can match a Swahili query to English queries requires knowing how these models are trained to align semantic representations across languages
  - Quick check: If LaBSE is trained on parallel sentence pairs across 50+ languages, what geometric property would you expect in the embedding space for translations of the same sentence?

- **Zero-Shot Learning and Transfer**
  - Why needed: The method claims zero-shot capability—predicting intents for languages never seen during any training phase
  - Quick check: In this paper's context, does "zero-shot" mean (a) no model training at all, (b) no training data in the target language, or (c) no labeled examples of any kind?

## Architecture Onboarding

- **Component map:** Raw query (low-resource language) → Sentence Encoder → Query embedding → FAISS ANN search (k=31) → Retrieve neighbor labels → Majority vote → Predicted intent

- **Critical path:** Incoming query encoded to 768/1024-dim vector, retrieved via cosine similarity ANN search, majority vote resolves final intent label

- **Design tradeoffs:**
  - LaBSE vs. XLM-RoBERTa: LaBSE more consistent across low-resource languages (48-55% accuracy); XLM-R shows high variance (12% Swahili vs. 56% Indonesian)
  - Index composition: Indexing all non-target languages vs. high-resource only yields comparable results, suggesting high-resource-only is more practical
  - k value: Higher k (31) smooths predictions but requires more indexed examples per class; sparse datasets need lower k (5)
  - No fine-tuning vs. classification head: Classification head achieves 70-77% accuracy but requires labeled target data; proposed method achieves 48-56% with zero target labels

- **Failure signatures:**
  - Language poorly represented in encoder pre-training (e.g., XLM-RoBERTa's 11.9% accuracy on Swahili)
  - Highly imbalanced intent distribution (method underperforms on HINT3 at 55.4% vs. CLINC-150 at 91.2%)
  - Semantically similar intent classes may conflate similar intents through mixed neighbor retrieval
  - Insufficient index size per class reduces accuracy, though minimum viable size not quantified

- **First 3 experiments:**
  1. Validate encoder selection for your target languages: Encode small set of target-language queries and high-resource translations, compute cosine similarity—values above 0.7 suggest adequate cross-lingual alignment
  2. Determine optimal k for your intent taxonomy: Run grid search k ∈ {5, 10, 15, 25, 31, 50} on held-out validation set, monitor both accuracy and F1
  3. Profile index size vs. accuracy trade-off: Build indexes with N ∈ {5, 10, 20, 31, 50} examples per class, plot accuracy vs. index size to identify diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative relationship between the number of indexed queries per class and classification accuracy, particularly in scenarios with heavily imbalanced class labels?
- Basis: Section 5.2 notes experiments maintained balanced query index, leaving behavior in imbalanced distributions untested
- Why unresolved: Current experiments didn't test performance degradation as ratio of indexed samples per intent diverges from uniform distribution
- Evidence needed: Experiments measuring performance as class label distributions become increasingly imbalanced

### Open Question 2
- Question: To what extent does fine-tuning the pre-trained sentence encoder models on available high-resource data improve zero-shot performance on low-resource target languages?
- Basis: Section 5.2 notes authors "are still yet to explore how fine-tuning such encoder models with available data will help"
- Why unresolved: Study isolated zero-shot capability of frozen embeddings without assessing if supervised tuning on source language creates domain shift
- Evidence needed: Comparison of zero-shot test accuracy using frozen vs. fine-tuned encoders on low-resource MASSIVE subset

### Open Question 3
- Question: Can the similarity search method be modified to close the performance gap with translation-based baselines without sacrificing deployment advantages?
- Basis: Results in Section 4.3 show proposed method lags translation-based alternatives by over 20 points in accuracy for some languages
- Why unresolved: Paper concludes method is practical "rapidly deployed" alternative but doesn't explore architectural enhancements (e.g., weighted voting, re-ranking)
- Evidence needed: Ablation studies testing alternative label resolution strategies against baseline majority vote

## Limitations
- Performance highly dependent on quality of cross-lingual semantic alignment in chosen multilingual encoder
- Method not universally reliable across arbitrary low-resource languages given wide performance variance observed
- Foundational assumption that all intents have semantic equivalents across languages not validated
- Optimal k value (31) may not generalize to other intent taxonomies or low-resource languages with different semantic characteristics

## Confidence
- High confidence: Core claim that query similarity search in multilingual embedding space can achieve non-trivial zero-shot intent classification accuracy (48-55% on MASSIVE)
- Medium confidence: Claim that indexing only high-resource languages is sufficient for zero-shot classification holds for tested scenarios but may break down for languages with significantly different semantic structures
- Low confidence: Assertion that approach will work reliably across arbitrary low-resource languages without per-language validation

## Next Checks
1. Before deploying on new low-resource language, encode 50-100 target-language queries and high-resource translations, compute average cosine similarity between query pairs with confirmed semantic equivalence—if average similarity < 0.5, either switch encoders or collect some target-language examples for fine-tuning
2. For your specific intent taxonomy, compute average distance between queries from different intent classes in embedding space—if average inter-class distance < 0.3, consider whether intents need consolidation or if different similarity threshold/k value should be used
3. Measure percentage of test queries where all top-5 neighbors belong to correct intent class—if < 80%, either increase number of examples per class in your index or consider whether some intents are genuinely ambiguous and may require additional context or disambiguation mechanisms