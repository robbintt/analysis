---
ver: rpa2
title: A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability
arxiv_id: '2512.22205'
source_url: https://arxiv.org/abs/2512.22205
tags:
- malaria
- image
- used
- parasitized
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a deep learning-based approach for malaria
  diagnosis using a custom Convolutional Neural Network (CNN) to classify blood cell
  images as parasitized or uninfected. The proposed CNN model achieves an accuracy
  of 96%, with precision and recall scores exceeding 0.95 for both classes.
---

# A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability

## Quick Facts
- arXiv ID: 2512.22205
- Source URL: https://arxiv.org/abs/2512.22205
- Reference count: 17
- Primary result: Custom CNN achieves 96% accuracy with SHAP/LIME explainability for malaria diagnosis

## Executive Summary
This study proposes a deep learning-based approach for malaria diagnosis using a custom Convolutional Neural Network (CNN) to classify blood cell images as parasitized or uninfected. The proposed CNN model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied, providing transparency into the decision-making process. The model outperforms several transfer learning architectures (ResNet50, VGG16, MobileNetV2, DenseNet121) and balances high accuracy with model explainability, which is crucial for medical applications. The results demonstrate the potential of deep learning in providing quick, accurate, and understandable malaria diagnosis, especially in resource-limited settings.

## Method Summary
The study uses the NIH/NLM Malaria Dataset containing 27,558 images, split into 82% training, 9% validation, and 9% test sets. Images are resized to 100×100 pixels and normalized. The custom CNN architecture consists of four convolutional blocks with increasing filter counts (32→64→128→256), batch normalization, max pooling, global average pooling, dropout, and dense layers. The model is trained using Adamax optimizer with learning rate scheduling and early stopping. Explainable AI techniques (SHAP, LIME, Saliency Maps) are applied to validate the model's decision-making process. The approach is compared against four transfer learning models (ResNet50, VGG16, MobileNetV2, DenseNet121) and demonstrates superior performance.

## Key Results
- Custom CNN achieves 96% accuracy with precision and recall >0.95 for both parasitized and uninfected classes
- Outperforms transfer learning models: ResNet50 (92.30%), VGG16 (95.28%), MobileNetV2 (80.85%), DenseNet121 (93.74%)
- XAI techniques validate model attention on diagnostically relevant regions rather than spurious correlations
- Model maintains low computational complexity with 625,378 parameters (2.39 MB)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The custom CNN's hierarchical convolutional structure enables progressive feature extraction from blood cell images, allowing discrimination between parasitized and uninfected cells.
- Mechanism: Four convolutional blocks with increasing filter counts (32→64→128→256) extract features at multiple scales. Early layers capture low-level patterns (edges, textures) while deeper layers identify high-level structures (parasite morphology). Batch normalization stabilizes activations, and max pooling (2,2) provides translation invariance while reducing spatial dimensions.
- Core assumption: The visual features distinguishing parasitized cells (purple-stained Plasmodium parasites) are learnable through convolution operations at 100×100 pixel resolution.
- Evidence anchors: [abstract] "custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%"; [section 3.2] "The number of filters increases with each block 32, 64, 128, and 256 to allow the network to learn both low-level and high-level features"; [corpus] Related work (DCENWCNet) demonstrates similar CNN ensemble approaches achieve strong WBC classification.

### Mechanism 2
- Claim: Explainable AI techniques (SHAP, LIME, Saliency Maps) validate that the model attends to diagnostically relevant regions rather than spurious correlations.
- Mechanism: SHAP computes Shapley values per pixel, showing positive (red) and negative (blue) contributions to class predictions. LIME generates local surrogate models highlighting magenta regions that maximally influence classification. Saliency maps compute gradients of output w.r.t. input, revealing attention hotspots. Together, these confirm the model focuses on parasite structures in infected cells.
- Core assumption: The explainability methods faithfully represent the model's decision process; their highlighted regions correlate with actual diagnostic features used by the CNN.
- Evidence anchors: [abstract] "Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied, providing transparency into the decision-making process"; [section 4.2] "In the figure 6 middle image shows red color that indicate it positively influence the classification as parasitized... The bright spot in the center indicates where the model focused to classify the cell as parasitized"; [corpus] Related paper "Bridging Accuracy and Interpretability" demonstrates XAI integration for breast cancer detection.

### Mechanism 3
- Claim: Regularization strategies (dropout, L2, early stopping, batch normalization) enable a relatively small model (625K parameters) to generalize without overfitting on the 27K image dataset.
- Mechanism: Dropout (0.25 rate) randomly zeros neurons during training, preventing co-adaptation. L2 regularization (0.01) in dense layers penalizes large weights. A custom callback halts training at 99% accuracy to prevent overfitting. Batch normalization reduces internal covariate shift. Global Average Pooling replaces flatten+large-dense, reducing parameters.
- Core assumption: The dataset is sufficiently representative of real-world variability that regularization-induced generalization transfers to deployment contexts.
- Evidence anchors: [section 3.2] "a Dropout layer with a 0.25 rate was included to prevent overfitting... L2 regularization (0.01) to improve generalization"; [section 5] "maintaining lower computational complexity, total params 625,378 (2.39 MB)" while achieving 96% accuracy; [corpus] UltraLightSqueezeNet paper achieves malaria classification with 54x fewer parameters.

## Foundational Learning

- Concept: **Convolutional Neural Network fundamentals**
  - Why needed here: Understanding how convolution, pooling, and activation functions combine to extract hierarchical features is essential before modifying the 4-block architecture.
  - Quick check question: Can you explain why filter counts typically increase (32→64→128→256) rather than decrease as network depth increases?

- Concept: **Explainable AI methods (SHAP, LIME, Saliency)**
  - Why needed here: The paper's contribution emphasizes interpretability; implementing these correctly requires understanding their theoretical foundations and limitations.
  - Quick check question: What is the difference between SHAP (global feature importance via Shapley values) and LIME (local surrogate model explanations)?

- Concept: **Transfer learning and fine-tuning strategies**
  - Why needed here: The paper compares custom CNN against ImageNet-pretrained models (ResNet50, VGG16, MobileNetV2, DenseNet121); understanding why these underperform requires knowledge of domain shift and feature transferability.
  - Quick check question: Why might ImageNet features (trained on natural images) transfer poorly to medical microscopy images?

## Architecture Onboarding

- Component map: Input(100×100×3 RGB) -> Conv Block 1 (32 filters) -> Conv Block 2 (64 filters) -> Conv Block 3 (128 filters) -> Conv Block 4 (256 filters) -> Global Average Pooling -> Dropout(0.25) -> Dense(128, ReLU, L2=0.01) -> Dense(64, ReLU, L2=0.01) -> Dense(1, Sigmoid)

- Critical path: Data preprocessing (resize, normalize, stratified split 82/9/9) -> Model compilation with binary crossentropy loss -> Training with callbacks (LR scheduling, early stopping) -> Evaluation via classification report + confusion matrix -> Explainability analysis (SHAP, LIME, Saliency) on test samples

- Design tradeoffs: Custom CNN vs. transfer learning: Custom model (625K params, 96% acc) outperforms pre-trained models (VGG16: 95.28%, ResNet50: 92.30%) despite simpler architecture—likely due to domain-specific feature learning from scratch. Accuracy vs. interpretability: Paper accepts slightly lower accuracy (96% vs. 97.72% in literature) to gain SHAP/LIME explainability, critical for clinical adoption. Model size vs. deployment: 2.39MB model is deployment-friendly for resource-limited settings, unlike deeper transfer learning architectures.

- Failure signatures: Low validation accuracy but high training accuracy → overfitting (increase dropout, add augmentation). SHAP/LIME highlighting image borders or artifacts → model learning spurious features (review data quality). Large class imbalance in errors (e.g., high false negatives on parasitized) → adjust class weights or threshold. MobileNetV2-level performance (80.85%) suggests architecture-dataset mismatch—verify preprocessing matches pre-trained expectations.

- First 3 experiments: 1) Baseline reproduction: Train custom CNN on NIH malaria dataset with documented hyperparameters; verify ~96% accuracy and generate SHAP/LIME visualizations on 10 test samples to confirm explainability pipeline works. 2) Ablation study: Remove one regularization component at a time (dropout→0, L2→0, no early stopping) to quantify each's contribution to generalization. 3) Data augmentation impact: Add rotation, flip, brightness augmentation; measure whether validation accuracy improves and whether SHAP attention becomes more robust to spatial variation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of SHAP and LIME explanations significantly improve diagnostic accuracy or confidence for medical professionals compared to standard "black-box" predictions?
- Basis in paper: [explicit] The authors state their goal is to enhance "user trust" and ensure "transparency, particularly essential in medical applications."
- Why unresolved: The paper provides visual explanations but does not conduct user studies with clinicians to quantify the actual utility of these explanations in a diagnostic workflow.
- What evidence would resolve it: A controlled user study measuring diagnostic speed or error rates among pathologists using the XAI-enhanced tool versus the raw model output.

### Open Question 2
- Question: Can the proposed custom CNN maintain high inference speeds and low power consumption when deployed on actual edge devices in resource-limited settings?
- Basis in paper: [inferred] The abstract claims the solution is suitable for "resource-limited settings," yet experiments utilized an NVIDIA Tesla P100 GPU. Additionally, MobileNetV2 (an architecture optimized for mobile) performed poorly (80.85%), creating uncertainty regarding the custom model's efficiency on low-power hardware.
- Why unresolved: The paper evaluates parameter count (2.39 MB) but provides no benchmarks for latency or energy consumption on target edge hardware (e.g., smartphones or embedded systems).
- What evidence would resolve it: Benchmarks of inference time (FPS) and battery usage running on a Raspberry Pi or standard mobile device.

### Open Question 3
- Question: How robust is the model against variations in blood smear staining quality, lighting conditions, and camera hardware found in real-world clinical environments?
- Basis in paper: [inferred] The study relies entirely on the curated NIH dataset, which uses standardized acquisition protocols that may not reflect the noisy, variable conditions of field clinics.
- Why unresolved: The model was tested on data that was normalized and resized from a single source, potentially masking vulnerabilities to domain shift or artifacts often present in remote diagnostics.
- What evidence would resolve it: Performance metrics (accuracy/F1-score) derived from external datasets containing variable image quality, stains, and camera resolutions.

## Limitations
- The custom CNN architecture's exact hyperparameter configuration remains partially unspecified (ReduceLROnPlateau parameters, early stopping monitoring frequency)
- The SHAP/LIME implementation details are not provided, making exact reproduction of the interpretability results uncertain
- While the NIH dataset is publicly available, potential preprocessing differences could affect performance

## Confidence
- High confidence: The core mechanism that hierarchical convolutional feature extraction enables malaria classification (96% accuracy) is well-supported by the architectural description and evaluation metrics.
- Medium confidence: The claim that XAI techniques (SHAP, LIME, saliency) provide clinically meaningful explanations is supported by the methodology description but requires verification of actual visualization quality.
- Medium confidence: The assertion that the custom CNN outperforms transfer learning models is documented in results, but the reasons (domain shift, feature transferability) are theoretical rather than empirically proven in this paper.

## Next Checks
1. Reproduce the baseline model on the NIH dataset with documented hyperparameters and verify the 96% accuracy claim, generating SHAP/LIME visualizations on test samples to validate the explainability pipeline.
2. Conduct an ablation study removing individual regularization components (dropout, L2, early stopping) to quantify their contribution to generalization performance.
3. Test the model's robustness to staining protocol variations by training on augmented data (rotation, brightness, contrast) and measuring whether SHAP attention becomes more consistent across transformations.