---
ver: rpa2
title: 'ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural Faithfulness
  in SpArX'
arxiv_id: '2503.03693'
source_url: https://arxiv.org/abs/2503.03693
tags:
- layer
- structural
- compression
- activation
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ILLC (Iterative Layer-by-Layer Compression),
  a novel model compression method that sequentially compresses each layer of a neural
  network to minimize information loss and maintain structural fidelity. Unlike traditional
  compression that simplifies all layers simultaneously, ILLC compresses each layer
  individually and immediately compensates for errors in the next layer, improving
  both input-output and structural faithfulness.
---

# ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural Faithfulness in SpArX

## Quick Facts
- arXiv ID: 2503.03693
- Source URL: https://arxiv.org/abs/2503.03693
- Authors: Ungsik Kim
- Reference count: 26
- One-line primary result: ILLC reduces input-output unfaithfulness by 26% and structural unfaithfulness by 6% compared to conventional compression on Breast Cancer Diagnosis dataset

## Executive Summary
ILLC (Iterative Layer-by-Layer Compression) is a novel model compression method that sequentially compresses each layer of a neural network while immediately compensating for errors in subsequent layers. Unlike traditional compression that simplifies all layers simultaneously, ILLC compresses each layer individually and propagates the updated activations through the compressed model before proceeding to the next layer. This approach significantly improves both input-output and structural faithfulness preservation compared to conventional methods, with experimental results showing 26% reduction in input-output unfaithfulness and 6% reduction in structural unfaithfulness on the Breast Cancer Diagnosis dataset.

## Method Summary
ILLC operates by compressing neural networks layer-by-layer rather than globally. For each layer, it computes activations across the dataset, clusters neurons based on activation similarity, and aggregates weights and biases within clusters. Crucially, after compressing layer l, it immediately recalculates activations using the compressed weights and passes these updated activations to layer l+1. This allows the next compression step to account for errors introduced in the previous layer, preventing error accumulation. The method uses activation-value-based clustering where neurons with similar activation patterns are grouped together, with edge weight aggregation that sums outgoing weights and averages incoming weights within clusters.

## Key Results
- Input-output unfaithfulness improves from 0.0131 to 0.0104 (26% improvement) on 20-layer, 100-neuron architecture
- Structural unfaithfulness improves from 0.4342 to 0.4084 (6% improvement) on same architecture
- ILLC maintains consistent attack-support relationships in Argumentative Explanation scheme
- Method prevents "Bi-Local-Maxima" effect where structural unfaithfulness spikes at specific layers

## Why This Works (Mechanism)

### Mechanism 1
- Sequential layer-by-layer compression with immediate error compensation preserves structural faithfulness better than simultaneous global compression
- After compressing layer l, the method recalculates activations using compressed weights and passes these to layer l+1, allowing next compression step to account for previous layer errors
- Core assumption: Activation distributions successfully clustered at layer l can represent that layer's structural features, enabling error readjustment at layer l+1
- Break condition: If activation distributions become highly non-Gaussian or multi-modal in ways clustering cannot capture, error compensation assumption degrades

### Mechanism 2
- Activation-value-based clustering preserves functional similarity better than weight-based clustering for maintaining argumentative explanation structure
- Neurons with similar activation patterns across dataset are grouped together, with edge weight aggregation summing outgoing weights and averaging incoming weights
- Core assumption: Neurons that co-activate across dataset serve similar functional roles in argumentative structure
- Break condition: If neurons have similar activation patterns but semantically distinct roles (e.g., opposite argument polarities), clustering destroys interpretability

### Mechanism 3
- Immediate forward propagation with compressed weights prevents "Bi-Local-Maxima" effect where structural unfaithfulness spikes at specific layers
- By computing X_cur with compressed parameters before next clustering step, method avoids propagating stale activation estimates that diverge from compressed model's actual behavior
- Core assumption: Compressed model's forward pass sufficiently approximates original model behavior at each layer
- Break condition: When compression ratio γ is too aggressive, even immediate forward propagation cannot recover sufficient information

## Foundational Learning

- Concept: Multi-Layer Perceptron (MLP) forward propagation
  - Why needed here: Entire method operates on layer-wise activations; understanding how W^l and b^l produce activations from inputs is essential
  - Quick check question: Can you trace how an input vector transforms through two layers with weight matrices and ReLU activations?

- Concept: Clustering for dimensionality reduction (e.g., k-means)
  - Why needed here: Compression relies on clustering neurons by activation similarity; understanding cluster assignment and centroid-based aggregation is required
  - Quick check question: Given a set of neuron activation vectors, how would you group them into k clusters?

- Concept: Quantitative Bipolar Argumentation Framework (QBAF)
  - Why needed here: Paper's motivation is preserving argumentative structure; QBAF maps attack/support relationships from network weights
  - Quick check question: How would a positive weight between two neurons be interpreted in an argumentation framework?

## Architecture Onboarding

- Component map: Original MLP → Layer 1 activation extraction → Clustering → Weight/bias aggregation → Compressed Layer 1 → Forward pass with compressed weights → Layer 2 activation (updated) → [repeat for all layers] → Final compressed MLP

- Critical path: 1) Extract activations A_l from current input X_cur using current (possibly compressed) weights, 2) Cluster neurons by activation similarity into |C_l| = γ · |V_l| clusters, 3) Aggregate weights: incoming weights averaged within cluster, outgoing weights summed, 4) Update X_cur with compressed layer's output before proceeding to next layer

- Design tradeoffs:
  - Compression ratio γ: Higher values preserve faithfulness but reduce interpretability gains
  - Clustering algorithm choice: Current implementation unspecified; choice affects cluster quality
  - Global vs local aggregation: Global uses full dataset, local uses neighborhood-weighted samples

- Failure signatures:
  - Dead Neurons: ReLU units consistently near zero; clustering them with active neurons introduces error
  - Bi-Local-Maxima: Sharp unfaithfulness spikes at specific layers due to activation distribution shifts
  - Structural unfaithfulness remaining high while input-output unfaithfulness drops (seen in Table 2 for deeper networks)

- First 3 experiments:
  1. Replicate the 20-layer, 100-neuron experiment on Breast Cancer dataset with γ=0.8; verify input-output unfaithfulness improvement matches ~26%
  2. Ablation: Compare ILLC against variant that uses original (not updated) activations for each layer's clustering; quantify unfaithfulness gap
  3. Stress test: Vary γ from 0.3 to 0.9 and plot faithfulness-compression frontier; identify tipping point where Bi-Local-Maxima emerges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ILLC be adapted to handle skip connections in residual networks?
- Basis in paper: The conclusion states: "As future work, we aim to develop methods for explaining skip connections. When skip connections are present, clustering becomes more complex because it must be performed over the entire parallel branch rather than simply following the sequential order of layers."
- Why unresolved: Current ILLC algorithm assumes strictly sequential layer-wise compression, but skip connections create parallel pathways that break this sequential assumption
- What evidence would resolve it: Modified ILLC algorithm that can cluster neurons across parallel branches while maintaining error compensation mechanism, demonstrated on ResNet-style architectures

### Open Question 2
- Question: Does the assumption that clustered activation distributions accurately represent structural features hold across different activation functions and normalization layers?
- Basis in paper: The conclusion notes: "Since the current approach relies solely on the activation values of hidden neurons, additional processing may be necessary when specialized activation functions (e.g., ReLU6) or normalization layers (e.g., BatchNorm) are involved."
- Why unresolved: ILLC was validated only on standard activation functions without normalization layers; modern architectures commonly use BatchNorm, LayerNorm, and bounded activations like ReLU6 that may alter activation distributions in ways clustering cannot capture
- What evidence would resolve it: Experiments applying ILLC to networks with BatchNorm, LayerNorm, and ReLU6, showing whether 26% input-output and 6% structural unfaithfulness improvements persist

### Open Question 3
- Question: Can ILLC maintain its structural faithfulness improvements on more complex, higher-dimensional datasets beyond tabular medical data?
- Basis in paper: All experiments use only Breast Cancer Diagnosis dataset (30 features, 569 samples). Paper does not test on image, text, or larger tabular datasets where activation patterns may be more complex
- Why unresolved: 6% structural unfaithfulness improvement may not generalize when hidden layer activations have more complex, multi-modal distributions that are harder to cluster meaningfully
- What evidence would resolve it: Experiments on image datasets (e.g., MNIST, CIFAR) or larger tabular datasets, showing whether structural unfaithfulness improvements scale with data complexity

## Limitations

- Clustering algorithm not explicitly specified, creating ambiguity in reproducing exact implementation
- Compression rate interpretation unclear (80% in experiments vs. γ ∈ (0,1] suggests γ=0.2 but not explicitly stated)
- Method validated only on tabular medical data; scalability to image, text, or larger datasets untested
- Assumes strictly sequential layer architecture; cannot handle skip connections or parallel pathways

## Confidence

- **High**: Sequential layer-by-layer compression with error compensation preserves structural faithfulness better than global compression
- **Medium**: Activation-value-based clustering preserves argumentative explanation structure effectively
- **Medium**: Immediate forward propagation prevents Bi-Local-Maxima effect in deeper networks

## Next Checks

1. Verify that core insight (updating X_cur with compressed weights before next layer) is primary driver of improved structural faithfulness by comparing against variant that skips this update
2. Test whether method maintains advantages with alternative clustering algorithms (e.g., hierarchical vs. k-means) on same dataset
3. Evaluate whether ILLC's improvements persist when applied to non-MLP architectures like CNNs or transformers