---
ver: rpa2
title: Kronecker Factorization Improves Efficiency and Interpretability of Sparse
  Autoencoders
arxiv_id: '2505.22255'
source_url: https://arxiv.org/abs/2505.22255
tags:
- kronsae
- topk
- feature
- base
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KronSAE, a sparse autoencoder architecture
  that improves computational efficiency and interpretability of latent representations
  through Kronecker factorization. The key innovation is factorizing the latent space
  into head-wise components using a differentiable mAND activation function that approximates
  logical AND operations.
---

# Kronecker Factorization Improves Efficiency and Interpretability of Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.22255
- Source URL: https://arxiv.org/abs/2505.22255
- Reference count: 40
- Key outcome: KronSAE achieves similar explained variance to TopK SAEs while using fewer parameters and FLOPs, with improved interpretability through reduced feature absorption

## Executive Summary
This paper introduces KronSAE, a sparse autoencoder architecture that improves computational efficiency and interpretability of latent representations through Kronecker factorization. The key innovation is factorizing the latent space into head-wise components using a differentiable mAND activation function that approximates logical AND operations. KronSAE reduces encoder computational cost from O(Fd) to O(h(m+n)d) while maintaining reconstruction quality comparable to standard TopK SAEs. Experimental results on multiple language models show that KronSAE achieves similar explained variance to baselines while demonstrating lower feature absorption scores and improved interpretability metrics.

## Method Summary
KronSAE modifies the sparse autoencoder architecture by factorizing the encoder matrix into head-wise Kronecker components. The encoder decomposes F = h·m·n latent dimensions into h heads, each with two thin matrices P_k ∈ R^{m×d} and Q_k ∈ R^{n×d}. Pre-latents p_k = ReLU(P_k·x) and q_k = ReLU(Q_k·x) are computed per head, then combined via mAND composition: z_{i,j} = √(u_i·v_j) when both u_i > 0 and v_j > 0, else 0. The decoder matrix is constructed additively from the factorized components. Standard TopK selection is applied across the flattened post-latent grid. Training uses AdamW optimizer with cosine learning rate schedule, batch size 8192, and auxiliary loss coefficient 0.03125.

## Key Results
- KronSAE reduces encoder computational cost from O(Fd) to O(h(m+n)d) while maintaining comparable reconstruction quality
- Lower feature absorption scores compared to TopK SAEs, with mean absorption fraction consistently reduced across experiments
- Improved interpretability metrics showing post-latents are more monosemantic than pre-latents
- Performance gap with TopK narrows at larger dictionary sizes but efficiency advantage persists

## Why This Works (Mechanism)

### Mechanism 1
KronSAE reduces encoder computational cost from O(Fd) to O(h(m+n)d) while maintaining comparable reconstruction quality. Instead of a single dense encoder matrix W_enc ∈ R^{F×d}, the architecture decomposes into h independent heads, each with thin matrices P_k ∈ R^{m×d} and Q_k ∈ R^{n×d}, where F = h·m·n. Pre-latents are computed separately, then combined via mAND. The core assumption is that the latent space admits a compositional structure where features can be expressed as interactions between lower-dimensional primitives.

### Mechanism 2
The mAND activation function creates AND-like compositional features where post-latents activate only when both parent pre-latents are active. mAND(u_i, v_j) = √(u_i · v_j) when both u_i > 0 and v_j > 0, else 0. This enforces F = P ∩ Q where F, P, Q are input sets activating post-latent, base pre-latent, and extension pre-latent respectively. The core assumption is that interpretable features in language models have compositional structure amenable to logical combination semantics.

### Mechanism 3
Head-wise Cartesian decomposition reduces feature absorption compared to standard TopK SAEs. Dividing latent space into h independent subspaces confines specialized concepts to single heads, preventing them from fully absorbing more general concepts in other heads. The AND-like hierarchy also ensures post-latents cannot subsume pre-latents. The core assumption is that feature absorption arises from unstructured competition in a monolithic latent space.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Why needed: KronSAE modifies the encoder architecture of standard TopK SAEs; understanding the baseline reconstruction objective and sparsity constraint is prerequisite. Quick check: Can you explain why SAEs use sparsity (e.g., TopK or L1 penalty) and how reconstruction quality trades off with sparsity?

- **Kronecker Product Decomposition**: Why needed: Core mathematical operation enabling the factorization; understanding how vec(p ⊗ q) relates to outer products and the mAND formulation. Quick check: Given vectors p ∈ R^m and q ∈ R^n, what is the dimensionality of p ⊗ q, and how does element-wise square root connect to the mAND definition?

- **Feature Absorption in SAEs**: Why needed: Key interpretability failure mode KronSAE addresses; understanding why a "Lion" feature might absorb a "starts with L" feature. Quick check: In an SAE with features {f_1, ..., f_F}, what does it mean for feature f_i to be "absorbed" by f_j, and why does this harm interpretability?

## Architecture Onboarding

- **Component map**: Input x → per-head projections (P_k·x, Q_k·x) → ReLU pre-latents → mAND composition → flatten + TopK → sparse f → W_dec·f reconstruction

- **Critical path**: Input x → per-head projections (P_k·x, Q_k·x) → ReLU pre-latents → mAND composition → flatten + TopK → sparse f → W_dec·f reconstruction

- **Design tradeoffs**: Lower m, higher h → better reconstruction but potentially more feature splits; m = 1 gives best EV but may reduce interpretability relative to m = 2, 4; larger dictionary sizes narrow performance gap with TopK but reduce relative efficiency gains

- **Failure signatures**: Reconstruction quality significantly below TopK under iso-FLOPs; pre-latents remaining monosemantic; high feature split rates despite low absorption

- **First 3 experiments**: 1) Train KronSAE (m=2, h=4096, F=2^16) on Qwen-2.5-1.5B layer 14 with 500M tokens; verify EV within 2% of TopK baseline; 2) Compare mAND vs. ReLU(u)·ReLU(v) vs. u·v under fixed budget; confirm mAND advantage; 3) Run SAEBench absorption metrics on trained KronSAE vs. TopK; verify lower absorption scores

## Open Questions the Paper Calls Out

- **Alternative logical gating functions**: Can XOR or composite gates be integrated into transcoders to improve interpretability and circuit analysis? The paper explicitly proposes investigating alternative logical gating functions beyond mAND in transcoder architectures.

- **Crosscoder generalization**: Does generalizing Kronecker factorization to a crosscoder setting successfully uncover interpretable, cross-level compositionality? The authors list "Crosscoding" as a future direction to reveal cross-level compositionality via logic operations.

- **Dynamic composition**: Can a learnable mechanism for dynamically tuning the number of heads (h) and their dimensionality (m, n) improve performance by adapting to varying feature scales? The paper suggests exploring "Dynamic Composition" via learnable tuning of both the number of attention heads and their dimensionality.

## Limitations

- Computational efficiency analysis is based on FLOPs counting for encoder only, without accounting for potential additional overhead from mAND composition or flattened output management
- Interpretability improvements rely heavily on SAEBench metrics, which measure automated interpretability proxies rather than direct human evaluation of feature semantics
- Performance gap narrowing at larger dictionary sizes suggests the efficiency advantage diminishes for applications requiring very high feature counts

## Confidence

**High Confidence**: Computational efficiency improvements (O(Fd) → O(h(m+n)d)) and reconstruction quality parity with TopK SAEs
**Medium Confidence**: Interpretability improvements via feature absorption reduction
**Medium Confidence**: mAND composition creating AND-like feature semantics

## Next Checks

1. **Human evaluation study**: Recruit 3-5 NLP experts to manually annotate 50-100 post-latent features from both KronSAE and TopK SAE trained on the same model/layer. Compare agreement rates and qualitative assessments of feature monosemanticity to validate automated interpretability metrics.

2. **Ablation on composition mechanism**: Train variants using ReLU(u)·ReLU(v) and u·v activations with identical computational budgets on Qwen-2.5-1.5B. Quantify differences in absorption scores, EV, and interpretability metrics to isolate mAND's contribution.

3. **Transferability analysis**: Evaluate KronSAE features trained on Qwen-2.5-1.5B layer 14 on downstream tasks (e.g., probing for factual knowledge, syntactic structures) and compare performance against TopK SAE features.