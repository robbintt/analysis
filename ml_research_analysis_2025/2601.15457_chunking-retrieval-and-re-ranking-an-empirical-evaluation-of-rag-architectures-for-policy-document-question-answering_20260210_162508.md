---
ver: rpa2
title: 'Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures
  for Policy Document Question Answering'
arxiv_id: '2601.15457'
source_url: https://arxiv.org/abs/2601.15457
tags:
- policy
- retrieval
- system
- cross-encoder
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated three RAG architectures for policy document
  question answering, comparing a Vanilla LLM against Basic RAG and Advanced RAG with
  cross-encoder re-ranking. Using CDC policy documents, the research found that Basic
  RAG improved faithfulness from 0.35 to 0.62 and relevance from 0.45 to 0.70, while
  Advanced RAG achieved the highest scores at 0.80 for both metrics.
---

# Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering

## Quick Facts
- **arXiv ID**: 2601.15457
- **Source URL**: https://arxiv.org/abs/2601.15457
- **Reference count**: 17
- **Primary result**: Advanced RAG with cross-encoder re-ranking achieved 0.80 faithfulness and 0.80 relevance scores, a 28% relative improvement over Basic RAG on CDC policy documents.

## Executive Summary
This study evaluates three RAG architectures for policy document question answering: a Vanilla LLM, Basic RAG with vector retrieval, and Advanced RAG with cross-encoder re-ranking. Using CDC policy documents and 10 complex policy questions, the research demonstrates that Basic RAG improves faithfulness from 0.35 to 0.62 and relevance from 0.45 to 0.70. Advanced RAG with dual-stage retrieval achieves the highest scores at 0.80 for both metrics. The cross-encoder re-ranking mechanism significantly reduces noisy context retrieval, proving essential for precision in domain-specific policy question answering, though document chunking remains a bottleneck for complex reasoning tasks.

## Method Summary
The study compares three RAG architectures on CDC policy documents: Vanilla LLM (direct inference), Basic RAG (vector retrieval with bi-encoder embeddings), and Advanced RAG (bi-encoder top-10 followed by cross-encoder re-ranking to top-3). The system uses Mistral-7B-Instruct-v0.2 as generator, all-MiniLM-L6-v2 for bi-encoder embeddings, ms-marco-MiniLM-L-6-v2 for cross-encoder re-ranking, and FAISS for vector indexing. Two chunking strategies were tested: recursive character-based and token-based semantic splitting. Performance is measured using faithfulness and relevance scores on 10 curated policy questions.

## Key Results
- Basic RAG improved faithfulness from 0.35 to 0.62 and relevance from 0.45 to 0.70 compared to Vanilla LLM
- Advanced RAG achieved highest scores at 0.80 for both faithfulness and relevance metrics
- Cross-encoder re-ranking provided a 28% relative improvement in faithfulness by reducing noisy context retrieval
- Document chunking remains a bottleneck for complex reasoning tasks in policy documents

## Why This Works (Mechanism)
The dual-stage retrieval system (bi-encoder + cross-encoder) filters out semantically similar but contextually irrelevant chunks that would otherwise mislead the generator. By first retrieving top-10 candidates via fast vector similarity and then re-ranking with a more computationally expensive cross-encoder, the system ensures that only the most relevant context is injected into the prompt. This approach addresses the fundamental limitation of vector similarity alone, which can retrieve topically related but factually incorrect or incomplete passages for complex policy reasoning.

## Foundational Learning
- **Vector embeddings and FAISS indexing** - Why needed: Efficient semantic search requires converting text to numerical representations and fast approximate nearest neighbor search. Quick check: Can you build a FAISS index and retrieve top-k results for a test query?
- **Cross-encoder re-ranking vs bi-encoder retrieval** - Why needed: Cross-encoders capture inter-sentence context that bi-encoders miss, crucial for complex reasoning. Quick check: Can you implement a simple cross-encoder re-ranker and compare outputs with bi-encoder results?
- **Faithfulness vs relevance metrics** - Why needed: Policy Q&A requires both contextual accuracy (faithfulness) and answer completeness (relevance). Quick check: Can you distinguish between a faithful but incomplete answer versus a relevant but hallucinated one?
- **Chunking strategies for long documents** - Why needed: Policy documents contain hierarchical structures that simple splitting may fragment. Quick check: Can you identify logical sections in a sample policy document that should remain together?
- **RAG evaluation frameworks (RAGAS)** - Why needed: Automated metrics are essential for comparing architectures across multiple queries. Quick check: Can you run RAGAS on a sample RAG pipeline and interpret the scores?

## Architecture Onboarding

**Component map**: Query -> Bi-encoder embeddings -> FAISS retrieval (top-10) -> Cross-encoder re-ranking (top-3) -> Prompt injection -> Mistral-7B-Instruct-v0.2 -> Answer

**Critical path**: Document chunking → Vector indexing → Query processing → Dual-stage retrieval → Answer generation

**Design tradeoffs**: Speed vs accuracy (bi-encoder fast but less precise vs cross-encoder slow but more accurate), chunk size vs context completeness (smaller chunks faster but may fragment reasoning), retrieval depth vs computational cost (more candidates better recall but higher latency)

**Failure signatures**: Bi-encoder retrieves topically similar but contextually wrong chunks (faithfulness drops to 0.0), cross-encoder fails to distinguish relevant from irrelevant context (relevance scores plateau), prompt injection format confuses generator (coherence metrics decline)

**First experiments**: 1) Implement single-stage vector retrieval and measure faithfulness/relevance baseline; 2) Add cross-encoder re-ranking and compare improvement on queries that scored poorly in single-stage; 3) Test different chunk sizes (256, 512, 1024 tokens) and measure impact on multi-hop reasoning accuracy

## Open Questions the Paper Calls Out

**Open Question 1**: Can "structure-aware" chunking strategies mitigate the fragmentation of logical policy workflows better than the recursive character-based and token-based semantic splitting methods evaluated? The current study found that complex reasoning tasks still suffer, likely because these methods ignore the hierarchical logical dependencies inherent in policy documents. Evidence needed: Comparative study measuring multi-hop reasoning accuracy and faithfulness using structure-aware chunker versus baseline semantic splitters.

**Open Question 2**: Can the integration of the Model Context Protocol (MCP) effectively abstract data retrieval in government RAG systems while maintaining strict security boundaries? While MCP is proposed as a solution for standardizing data exchange, it is untested whether this abstraction layer introduces new latency or security vulnerabilities in high-stakes public health environments. Evidence needed: Empirical security and performance audit of a RAG pipeline implementing MCP, specifically testing for adversarial payload resistance and retrieval latency under load.

**Open Question 3**: To what degree can hybrid retrieval mechanisms or knowledge graph integration reduce hallucination rates toward zero compared to the dual-stage vector retrieval used in this study? The current Advanced RAG architecture achieved a faithfulness score of 0.797, leaving a significant gap (approx. 20%) in factual grounding that vector-only retrieval could not close. Evidence needed: Experimental results from a Graph-RAG or hybrid sparse-dense system on the same CDC corpus, demonstrating statistically significant increase in faithfulness scores above the 0.80 baseline.

## Limitations
- Chunking hyperparameters (sizes, overlap values, and which strategy produced each score) are unspecified, creating uncertainty in reproducing the 28% faithfulness improvement
- Evaluation methodology details (exact RAGAS version, scoring formula implementation) are absent from the paper
- CDC document corpus specifics (total count, size distribution, content domains) are not disclosed, limiting generalizability assessment
- Cross-encoder re-ranking's computational overhead and latency impact are mentioned but not quantified

## Confidence
- **High confidence**: Basic RAG outperforms Vanilla LLM (0.62→0.35 faithfulness; 0.70→0.45 relevance) — methodological difference is clearly specified and aligns with established RAG literature
- **Medium confidence**: Advanced RAG achieves highest scores (0.80/0.80) — two-stage retrieval mechanism is well-defined, but chunk quality and exact evaluation protocol remain uncertain
- **Low confidence**: 28% relative improvement claim — depends on unknown chunking parameters and evaluation details not specified in the paper

## Next Checks
1. Reconstruct the CDC corpus from publicly available sources and apply recursive character splitting (chunk_size=512, overlap=50) to match typical defaults, then measure faithfulness/relevance across all three architectures
2. Implement the exact two-stage retrieval pipeline (bi-encoder top-10 → cross-encoder re-rank to top-3) using specified models (all-MiniLM-L6-v2, ms-marco-MiniLM-L-6-v2) and validate against the reported performance gap
3. Conduct ablation studies on chunking strategy and overlap parameters to identify their impact on faithfulness scores, particularly for queries like Q8 that scored 0.0 in Basic RAG