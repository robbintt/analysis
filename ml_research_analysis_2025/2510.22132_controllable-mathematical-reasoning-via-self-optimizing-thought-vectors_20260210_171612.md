---
ver: rpa2
title: Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors
arxiv_id: '2510.22132'
source_url: https://arxiv.org/abs/2510.22132
tags:
- thought
- reasoning
- vectors
- control
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for controlling mathematical reasoning
  in language models using self-optimizing thought vectors and entropy minimization.
  The approach employs eight learnable thought vectors that represent different reasoning
  strategies (e.g., direct computation, sequential tracking, algebraic reasoning),
  with their selection guided by entropy-based rewards to encourage focused thinking.
---

# Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors

## Quick Facts
- arXiv ID: 2510.22132
- Source URL: https://arxiv.org/abs/2510.22132
- Authors: Xuying LI
- Reference count: 19
- Gemma-2-9B achieves 90.1% accuracy on GSM8K with entropy-based controllable reasoning

## Executive Summary
This paper introduces a method for controlling mathematical reasoning in language models using self-optimizing thought vectors and entropy minimization. The approach employs eight learnable thought vectors that represent different reasoning strategies (e.g., direct computation, sequential tracking, algebraic reasoning), with their selection guided by entropy-based rewards to encourage focused thinking. Tested on GSM8K with Gemma-2-9B, the method achieves 90.1% accuracy and a controllability score of 0.42, demonstrating improved reasoning control without requiring external reward annotations. Analysis shows that thought vectors cluster by reasoning depth and exhibit selective, bimodal activation patterns, validating the framework's effectiveness for controllable AI reasoning.

## Method Summary
The method introduces eight learnable thought vectors initialized orthogonally to represent distinct mathematical reasoning strategies. These vectors are injected at Layer 21 of Gemma-2-9B and modulated by a control encoder that expands 3D control signals (Depth, Length, Path) into 4096D representations. A gating mechanism computes attention weights over the thought vectors, with entropy minimization serving as an internal reward signal that encourages decisive strategy selection. The model is trained using LoRA adaptation with 1.06B trainable parameters, combining cross-entropy loss with the entropy-based reward. This creates a self-optimizing system that learns to select appropriate reasoning strategies while maintaining controllability over the reasoning process.

## Key Results
- Achieves 90.1% accuracy on GSM8K benchmark with Gemma-2-9B
- Depth control achieves 81.3% match rate; path control shows 41.2% success
- Controllability score of 0.42 with weights 0.6/0.2/0.2 for depth/length/path
- Bimodal activation patterns: 2-3 vectors active (mean ~0.7) while others dormant (mean ~0.08)
- Entropy correlates negatively with accuracy (ρ=-0.71), validating the reward mechanism

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy minimization acts as an internal reward signal that forces the model to commit to specific reasoning strategies rather than remaining uncertain.
- **Mechanism:** The system calculates the entropy H(p) of the thought vector selection distribution. By optimizing a reward R = -H(p), the model is penalized for "wavering" (high entropy selection) and rewarded for "decisiveness" (low entropy selection). This creates a feedback loop where confident strategy selection correlates with correct answers.
- **Core assumption:** Focused, low-entropy thinking is causally linked to higher accuracy in mathematical reasoning tasks.
- **Evidence anchors:**
  - [abstract] "...entropy-based rewards effectively guide focused reasoning patterns without requiring external reward annotations."
  - [section 5] "Low entropy captures this mathematically—strong commitment to specific thought vectors correlates with better outcomes (correlation: ρ=-0.71)."
  - [corpus] "Steering LLM Reasoning Through Bias-Only Adaptation" supports the efficacy of steering vectors for reasoning control.
- **Break condition:** If a problem genuinely requires exploring multiple ambiguous paths simultaneously, forcing premature commitment (low entropy) might suppress necessary exploration.

### Mechanism 2
- **Claim:** Reasoning processes can be decomposed into a bank of eight orthogonal, learnable vectors that function as internal "skills" or modes.
- **Mechanism:** Eight vectors are initialized orthogonally to represent distinct strategies (e.g., t_{1-2} for direct computation, t_{5-6} for algebra). A gating mechanism blends these vectors into the model's hidden state at Layer 21. The model learns to retrieve and combine these vectors based on the input problem.
- **Core assumption:** Mathematical reasoning can be effectively represented as a linear combination of a small set of discrete, latent strategies.
- **Evidence anchors:**
  - [section 3.1] "These vectors are initialized orthogonally to ensure diversity... The selection mechanism uses the current hidden state to compute attention weights."
  - [section 5] "Analysis reveals bimodal activation patterns: models strongly activate 2-3 vectors... while keeping others dormant."
  - [corpus] Evidence on specific "thought vector" decomposition is weak in neighbors; related work focuses on prompt-based control or full weight adaptation.
- **Break condition:** If the fixed bank of 8 vectors cannot represent a novel type of reasoning required by an out-of-distribution problem.

### Mechanism 3
- **Claim:** Control signals (Depth, Length, Path) are effectively transmitted by expanding low-dimensional control inputs into high-dimensional steering vectors.
- **Mechanism:** A control encoder expands a 3D control vector into a 4096D representation. This high-dim vector modulates the attention weights over the thought vectors. Specifically, it biases the selection probability p toward vectors that match the requested reasoning depth or style.
- **Core assumption:** The control dimensions (Depth, Path) map linearly or predictably to the learned thought vector space.
- **Evidence anchors:**
  - [section 4.4] "Depth control achieves 81.3% match rate... Path control shows 41.2% success."
  - [section 3.2] "...progressively transforming the 3D control vector into a 4096D representation that modulates thought vector selection."
  - [corpus] "Control-R" suggests structured control signals can guide reasoning, supporting the general premise.
- **Break condition:** When control dimensions conflict (e.g., High Depth + Low Length), causing "Control Signal Saturation" or incoherent outputs.

## Foundational Learning

- **Concept: Entropy (Information Theory)**
  - **Why needed here:** The core self-optimization loop relies on minimizing entropy H(p) as a reward. You must understand that low entropy = high confidence (peaked distribution) and high entropy = uncertainty (flat distribution).
  - **Quick check question:** If a model selects 4 thought vectors with probabilities [0.25, 0.25, 0.25, 0.25], is the entropy high or low?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The method trains 1.06B parameters on top of Gemma-2-9B. Understanding LoRA is critical to distinguishing which weights are frozen and which are updated during the entropy-reward optimization.
  - **Quick check question:** Does LoRA update the pre-trained weights of the base LLM directly?

- **Concept: Orthogonal Initialization**
  - **Why needed here:** The paper emphasizes that thought vectors are initialized orthogonally. This prevents the vectors from starting too similar to one another, ensuring they learn distinct "roles" (e.g., Algebra vs. Arithmetic) early in training.
  - **Quick check question:** Why would initializing two control vectors randomly (potentially close together) hurt specialization?

## Architecture Onboarding

- **Component map:** Input Prompt + Control Signal → Control Encoder → Biasing of Thought Vector Selection → Injection at Layer 21 → Output Generation → Entropy Calculation → Loss Backprop
- **Critical path:** Input Prompt + Control Signal → Control Encoder → Biasing of Thought Vector Selection → Injection at Layer 21 → Output Generation → Entropy Calculation → Loss Backprop
- **Design tradeoffs:**
  - Layer Selection: Layer 21 is optimal; earlier layers lack abstraction, later layers are too output-focused
  - Vector Count: Ablation shows 2 vectors achieve full accuracy (90.0%), while 8 are kept primarily for interpretability/controllability
  - Control Strength: Depth control works well (81%), but Length control fails (2.7%), indicating internal reasoning is easier to steer than output verbosity
- **Failure signatures:**
  - Thought Vector Oscillation: Rapid switching between vectors (e.g., Algebra ↔ Sequential) leading to incoherent outputs
  - Control Signal Saturation: At extreme settings (Depth=5), the model ignores the signal and reverts to safe, low-entropy patterns
  - Overthinking: Applying high depth to trivial problems, causing spurious complexity
- **First 3 experiments:**
  1. Verify Bimodal Activation: Run inference on a validation set and plot the magnitude of thought vector activations. Confirm that 2-3 vectors are active (mean ~0.7) while others are dormant (mean ~0.08)
  2. Layer Ablation Reproduction: Move the injection point from Layer 21 to Layer 10 and 35. Confirm the paper's finding that accuracy drops to 86.7% at non-optimal layers
  3. Entropy-Reward Correlation: Scatter plot the entropy of the selection distribution vs. answer correctness. Verify the claimed ρ = -0.71 correlation to ensure the reward signal is valid

## Open Questions the Paper Calls Out
- Can the self-optimizing thought vector framework effectively transfer to non-mathematical reasoning domains?
- Would implementing hierarchical thought vector structures improve the system's reasoning capabilities?
- What architectural modifications are required to achieve functional control over output length?

## Limitations
- Generalizability Gap: The method achieves 90.1% accuracy on GSM8K but hasn't been validated on other mathematical reasoning benchmarks or out-of-distribution problems
- Control Signal Effectiveness: The controllability scores reveal significant asymmetry—depth control achieves 81.3% match rate while length control fails at 2.7%
- Black Box Reasoning: The paper demonstrates that thought vectors cluster by reasoning depth but doesn't establish clear semantic mappings between specific vectors and reasoning strategies

## Confidence
- **High Confidence**: The entropy-minimization reward mechanism correlates with improved accuracy (ρ=-0.71), bimodal activation patterns are consistently observed, and Layer 21 injection point provides optimal performance
- **Medium Confidence**: Eight thought vectors are necessary for controllability (vs. two for accuracy), orthogonal initialization ensures vector diversity, and the gating mechanism effectively blends thought vectors
- **Low Confidence**: Control dimensions map predictably to thought vector space, self-optimizing thought vectors will generalize to non-mathematical reasoning, and the 1.06B LoRA parameter budget is optimal

## Next Checks
1. **Cross-Benchmark Validation**: Evaluate the trained model on GSM8K+, MATH, and other mathematical reasoning datasets to assess generalization. Compare accuracy drop and controllability degradation across benchmarks to quantify domain transfer limits.
2. **Thought Vector Interpretability Study**: Perform ablation studies where individual thought vectors are frozen or removed. Document which vector removals cause the largest accuracy drops for specific problem types (e.g., algebra vs. arithmetic) to establish semantic mappings.
3. **Control Signal Robustness Test**: Systematically test control signal saturation by applying extreme depth (5) to trivial problems and minimal depth (1) to complex problems. Measure output quality degradation and determine if the model learns to override conflicting control signals.