---
ver: rpa2
title: Steering Over-refusals Towards Safety in Retrieval Augmented Generation
arxiv_id: '2510.10452'
source_url: https://arxiv.org/abs/2510.10452
tags:
- benign
- harmful
- context
- refusal
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses over-refusals in retrieval-augmented generation
  (RAG) pipelines, where safety-aligned LLMs decline benign requests due to aggressive
  filters, especially when retrieved contexts contain harmful content. To mitigate
  this, the authors introduce RAGREFUSE, a domain-stratified benchmark with 2,970
  samples spanning six domains and fifteen contamination patterns, designed to evaluate
  over-refusal under controlled context contamination.
---

# Steering Over-refusals Towards Safety in Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2510.10452
- Source URL: https://arxiv.org/abs/2510.10452
- Reference count: 26
- One-line primary result: SafeRAG-Steering reduces over-refusal rates from 53.4% to 4.3% on Llama-3.1-8B and from 4.7% to 0% on Qwen1.5-7B while preserving legitimate refusals.

## Executive Summary
This paper addresses the problem of over-refusals in retrieval-augmented generation (RAG) pipelines, where safety-aligned LLMs decline benign requests due to aggressive filters when retrieved contexts contain harmful content. The authors introduce RAGREFUSE, a domain-stratified benchmark with 2,970 samples across six domains and fifteen contamination patterns, to systematically evaluate over-refusal under controlled context contamination. They propose SafeRAG-Steering, a lightweight inference-time embedding steering method that shifts model representations toward empirically safe regions, reducing over-refusals while preserving legitimate refusals. Experiments on Llama-3.1-8B and Qwen1.5-7B demonstrate significant ORR reductions while maintaining safety alignment.

## Method Summary
The method employs SafeRAG-Steering, an inference-time intervention that computes steering vectors by extracting hidden states from middle layers (8-23 of 32) for benign and over-refusal examples. The steering vector v^(ℓ) is calculated as the centroid difference between safe responses (benign queries + benign contexts) and over-refusal responses (benign queries + contaminated contexts). At inference, this vector is added to hidden states before post-LayerNorm with scaling factor α. The approach involves grid search over layers and α values to optimize performance on validation sets, targeting reduction of over-refusal rates while preserving legitimate safety refusals.

## Key Results
- ORR reduction: Llama-3.1-8B from 53.4% to 4.3%, Qwen1.5-7B from 4.7% to 0%
- Largest gains observed in Chemical, Medical, and Legal domains where base models exhibited highest refusal rates
- Monotonic relationship between harmful context density and refusal frequency (k=3→7 adds 20-25% more refusals)
- Domain sensitivity varies: Llama-3.1-8B refuses Chemical > Medical > Legal > Cybersecurity/Other requests

## Why This Works (Mechanism)

### Mechanism 1: Context Contamination Triggers Over-Refusal
Safety alignment creates aggressive filters that respond to harmful-text density in context regardless of query intent. Refusal rates increase monotonically with harmful context frequency—adding harmful contexts from k=3 to k=7 increases refusals by ≈20–25%.

### Mechanism 2: Linear Separability of Safe vs. Over-Refusal Representations
Safe and over-refusal responses occupy distinguishable regions in hidden state space, enabling steering. The method computes a steering vector as the difference between centroids of safe and over-refusal representations, then shifts intermediate layers toward safe regions.

### Mechanism 3: Domain-Specific Alignment Priors
Refusal sensitivity varies by domain due to alignment training data and model architecture. Llama-3.1-8B refuses Chemical > Medical > Legal > Cybersecurity/Other requests, suggesting alignment procedures embed domain-specific priors.

## Foundational Learning

- **RAG (Retrieval-Augmented Generation)**
  - Why needed: Over-refusal in RAG is the core problem; understanding how retrieved contexts augment prompts is essential.
  - Quick check: Given a user query "How do I synthesize aspirin?", what would a RAG system retrieve, and how might context contamination affect response?

- **Safety Alignment & Over-Refusal**
  - Why needed: Safety alignment creates the refusal behavior; over-refusal is the unintended side effect this paper addresses.
  - Quick check: Why might a model refuse a benign request like "Explain how penicillin works" if retrieved contexts mention controlled substances?

- **Activation Steering / Representation Engineering**
  - Why needed: SafeRAG-Steering manipulates hidden states; you need to understand where and how to intervene.
  - Quick check: At which layer would you intervene to affect semantic vs. output-level behavior, and why?

## Architecture Onboarding

- **Component map**: RAGREFUSE benchmark (2,970 samples) -> LLM judge (over-refusal classification) -> Hidden state extraction (layers 8-23) -> Steering vector computation -> Inference-time intervention (αv^(ℓ) addition)

- **Critical path**: 1) Build Target/OverRefusal sets from model behavior on benign queries 2) Extract and normalize hidden states at middle layers 3) Compute steering vector as centroid difference 4) Grid search over layer ℓ and scaling α 5) Apply at inference: ˜h(ℓ)_t = h(ℓ)_t + αv(ℓ)

- **Design tradeoffs**: Layer choice balances semantic stability vs. output commitment; scaling α balances effectiveness vs. collateral distortion; linear separability assumption enables efficiency but may fail on adversarial inputs.

- **Failure signatures**: Over-correction (legitimate harmful queries no longer refused), domain shift (vectors calibrated on one domain fail on another), context-length sensitivity (vectors trained on k=3 fail at k=7)

- **First 3 experiments**: 1) Reproduce baseline over-refusal rates on RAGREFUSE test set 2) Grid search layer & α on validation split 3) Safety preservation check on harmful-query subset

## Open Questions the Paper Calls Out
1. To what extent does the linear separability assumption fail against adversarially optimized contexts?
2. How does the steering vector's efficacy degrade when applied to domains or text distributions not represented in the RAGREFUSE training set?
3. Does the intervention introduce safety vulnerabilities by shifting representations away from refusal regions for genuinely harmful queries?

## Limitations
- Linear separability assumption may not hold for adversarially crafted prompts or complex contamination patterns
- Domain transferability concerns—vectors trained on specific domains may not generalize to unseen domains or distributions
- Limited analysis of safety vs. utility tradeoff—focus on over-refusal reduction without extensive harmful-query safety preservation testing

## Confidence
- **High confidence**: Empirical effectiveness of SafeRAG-Steering (ORR reduction from 53.4%→4.3% and 4.7%→0%) is well-supported by controlled experiments
- **Medium confidence**: Linear separability assumption is reasonable given results but may not generalize to all input types
- **Low confidence**: Exact layer α configuration selected through grid search is unspecified, and LLM judge choice for ORR evaluation is unclear

## Next Checks
1. Apply SafeRAG-Steering to RAGREFUSE test samples with mixed contamination patterns to evaluate effectiveness degradation
2. Systematically evaluate the steered model on harmful queries to measure if legitimate refusals are preserved
3. Train steering vectors on one domain and apply to another to quantify cross-domain performance degradation