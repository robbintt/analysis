---
ver: rpa2
title: 'RegCL: Continual Adaptation of Segment Anything Model via Model Merging'
arxiv_id: '2507.12297'
source_url: https://arxiv.org/abs/2507.12297
tags:
- regcl
- learning
- tasks
- continual
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RegCL is a continual learning framework for the Segment Anything
  Model (SAM) that integrates model merging techniques to prevent catastrophic forgetting
  across multiple domains. The method leverages LoRA adapter modules and employs RegMean-based
  weight optimization to minimize prediction discrepancies between merged and domain-specific
  models.
---

# RegCL: Continual Adaptation of Segment Anything Model via Model Merging

## Quick Facts
- **arXiv ID:** 2507.12297
- **Source URL:** https://arxiv.org/abs/2507.12297
- **Reference count:** 35
- **Key outcome:** RegCL integrates LoRA adapter modules with RegMean-based weight optimization to prevent catastrophic forgetting across multiple domains, achieving superior performance over existing continual learning baselines on five diverse datasets.

## Executive Summary
RegCL is a continual learning framework that enables the Segment Anything Model (SAM) to adapt across multiple domains without catastrophic forgetting. The method employs LoRA adapters and a novel RegMean-based weight optimization strategy that minimizes prediction discrepancies between merged and domain-specific models. By accumulating inner product matrices rather than storing historical data, RegCL achieves memory efficiency while maintaining strong segmentation performance across diverse domains including medical, camouflaged, and shadow imagery.

## Method Summary
RegCL operates on LoRA adapter modules added to SAM's image encoder, training each task independently before merging weights using a closed-form RegMean solution. The merging process optimizes linear layer weights by minimizing prediction discrepancies between merged and task-specific models, using an inner product accumulator to maintain historical knowledge without data replay. For non-linear layers, simple averaging is used. After each task, the accumulator updates by summing the current task's inner product matrix, enabling order-independent integration of new knowledge while preserving previously learned capabilities.

## Key Results
- **Superior performance:** Outperforms existing continual learning baselines on five diverse datasets with notable improvements in average accuracy, backward and forward transfer metrics
- **Memory efficiency:** Achieves knowledge retention without data replay by storing compressed inner product matrices comparable in size to LoRA parameters
- **Robust adaptation:** Effectively balances knowledge retention and adaptation across domains including medical (Kvasir, ISIC), camouflaged (CAMO), and shadow (ISTD) imagery

## Why This Works (Mechanism)

### Mechanism 1: RegMean-Based Weight Optimization for Knowledge Consolidation
- **Claim:** RegCL minimizes prediction discrepancies between merged and domain-specific models through closed-form weight optimization
- **Mechanism:** Reformulates model merging as linear regression using inner product matrices Ct = X^T X, computing merged weights as W_merged = (P + Ct)^(-1)(P*W_prev + Ct*W_new)
- **Core assumption:** Linear layers in neural networks approximate linear behavior sufficiently for closed-form merging solutions to preserve functional outputs
- **Evidence anchors:** [abstract] shows merging guided by weight optimization minimizing prediction discrepancies; [section 3.2.2, Eq. 4] demonstrates decomposition into historical and new knowledge terms
- **Break condition:** Conflicting weight configurations across domains may produce unstable or degenerate solutions due to opposing signs on critical features

### Mechanism 2: Non-Replay Historical Knowledge Accumulation
- **Claim:** RegCL maintains multi-domain knowledge without storing or replaying historical data by accumulating compressed feature statistics
- **Mechanism:** Maintains P_t = ΣC_i (sum of inner product matrices) across all tasks, requiring memory proportional to O(J*d_j²)
- **Core assumption:** Inner product matrices capture sufficient information about data distributions to preserve learned behaviors without accessing actual samples
- **Evidence anchors:** [abstract] states no historical data storage is required; [section 3.2.4] notes memory overhead is comparable to LoRA parameters
- **Break condition:** Highly non-Gaussian feature distributions or rare but critical samples may cause inner product statistics to fail capturing essential decision boundaries

### Mechanism 3: Order-Independent Task Integration
- **Claim:** Merged weights are invariant to task order, unlike sequential fine-tuning which is sensitive to training sequence
- **Mechanism:** Since P_t = ΣC_i and weight merging depends only on the sum (commutative), final merged model is theoretically identical regardless of task order
- **Core assumption:** Task-specific models can be trained independently without interference, and their combination is purely additive
- **Evidence anchors:** [section 3.2.4] states merging depends solely on sum of Ct, which is order independent due to commutative property of addition
- **Break condition:** Numerical precision issues in matrix inversion or extreme condition numbers in P_t may introduce order-dependent artifacts

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** Core problem RegCL addresses—neural networks overwrite previously learned weights when trained on new tasks, causing performance collapse on earlier domains
  - **Quick check question:** Can you explain why sequential fine-tuning on domain A then domain B typically degrades performance on domain A?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** RegCL operates specifically on LoRA adapter modules rather than full model weights. Understanding LoRA's decomposition W = W_0 + BA is essential to grasp what parameters are being merged
  - **Quick check question:** How does LoRA reduce trainable parameters while preserving model capacity?

- **Concept: Model Merging via Weight Interpolation**
  - **Why needed here:** RegCL builds on prior work (RegMean, Fisher Averaging, TIES) that combine multiple fine-tuned models. Understanding why simple averaging often fails helps explain RegCL's design
  - **Quick check question:** Why might naively averaging weights from two fine-tuned models produce worse results than either individual model?

## Architecture Onboarding

- **Component map:**
  SAM Image Encoder (ViT-B/16) -> LoRA Modules -> Inner Product Accumulator (P_t) -> SAM Mask Decoder + Prompt Encoder

- **Critical path:**
  1. Initialize LoRA with Kaiming initialization, P ← 0
  2. For each task D_t: Train LoRA on D_t → compute C_t = X^T X from final features
  3. Merge: W_merged = (P + C_t)^(-1)(P × W_prev + C_t × W_new)
  4. Update: P ← P + C_t
  5. Repeat for all tasks; final model uses last merged weights

- **Design tradeoffs:**
  - Memory vs. data access: Stores inner product matrices (~same size as LoRA) instead of raw data, but cannot benefit from replay-based refinement
  - Linear-only optimization: Uses closed-form RegMean for linear layers but falls back to simple averaging for non-linear layers—may lose optimization benefits in complex components
  - Order independence vs. potential interference: Design guarantees order independence but cannot selectively weight certain domains based on importance

- **Failure signatures:**
  - Backward transfer (BWT) approaching task count × large negative values indicates severe forgetting (merging insufficient)
  - Large performance variance across domains in final model suggests imbalance in inner product magnitudes
  - Numerical instability during matrix inversion (singular or near-singular P_t) indicates insufficient or highly correlated feature distributions

- **First 3 experiments:**
  1. Single-domain LoRA fine-tuning: Establish baseline performance on each domain independently to set upper bounds
  2. Two-domain sequential fine-tuning vs. RegCL: Validate merging preserves both domains where sequential training fails (measure mIoU gap on domain 1 after training on domain 2)
  3. Five-domain continual learning (Kvasir→CAMO→ISTD→ISIC→COD): Replicate paper's main result, tracking ACC, BWT, and FWT metrics to verify knowledge retention and forward transfer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can RegCL be effectively extended to Class-Incremental Learning (CIL) scenarios where the label space evolves?
- **Basis in paper:** [explicit] Section 3.2.1 explicitly defines the problem setup as Domain-Incremental Learning (DIL), noting that tasks share the same data label space, leaving the more complex CIL setup unaddressed
- **Why unresolved:** The merging objective minimizes prediction discrepancies assuming a shared output space; evolving classes would require architectural changes to the mask decoder or a reformulation of the merging objective
- **What evidence would resolve it:** Experiments on standard CIL benchmarks (e.g., sequential segmentation of distinct object classes) demonstrating mIoU retention without fixed labels

### Open Question 2
- **Question:** Does the simple averaging strategy for non-linear layers (Eq. 6) create a performance bottleneck compared to the optimized linear merging?
- **Basis in paper:** [explicit] Section 3.2.2 states that for non-linear layers, "a simpler averaging strategy is adopted" rather than the closed-form RegMean solution used for linear layers
- **Why unresolved:** It is unclear if the discrepancy between using optimized RegMean for linear components and naive averaging for non-linear components results in sub-optimal knowledge fusion
- **What evidence would resolve it:** Ablation studies replacing the averaging heuristic with alternative non-linear merging techniques to isolate the impact on segmentation accuracy

### Open Question 3
- **Question:** How does RegCL perform regarding numerical stability and accuracy when scaling to significantly longer sequences of tasks (e.g., >10 domains)?
- **Basis in paper:** [inferred] The experiments are limited to a sequence of 5 datasets. The accumulation of the inner product accumulator ($P_t$) and the inverse weighting term over many steps is theoretically stable but empirically unverified at scale
- **Why unresolved:** Cumulative rounding errors or diminishing returns in the adaptive weighting term $(P_t + C_t)^{-1}$ may arise as the denominator grows over extended time horizons
- **What evidence would resolve it:** Evaluation on a large-scale continual learning benchmark with many tasks to verify if Backward Transfer (BWT) remains consistent

## Limitations

- **LoRA dependency:** Method's reliance on LoRA modules limits applicability to other parameter-efficient tuning methods or full fine-tuning scenarios
- **Hyperparameter sensitivity:** Lack of ablation studies on LoRA hyperparameters (rank, alpha, target modules) creates uncertainty about performance sensitivity
- **Inner product sufficiency:** Claim that inner product matrices capture sufficient distributional information lacks direct validation, particularly for tasks with conflicting or highly complex feature distributions

## Confidence

- **High Confidence:** Core mechanism of RegMean-based weight optimization and its order-independent property are mathematically sound and clearly demonstrated on five-domain benchmark
- **Medium Confidence:** Non-replay memory efficiency claim is valid given stated O(J*d_j²) complexity, but requires empirical validation across diverse task distributions and sequence lengths
- **Low Confidence:** Claim that inner product matrices sufficiently capture task-specific knowledge for indefinite task sequences lacks validation, particularly for tasks with conflicting or highly complex feature distributions

## Next Checks

1. **Ablation on LoRA Hyperparameters:** Systematically vary LoRA rank (8, 16, 32) and alpha scaling across all five domains to establish performance sensitivity and optimal configurations
2. **Stress Test on Task Sequence Length:** Evaluate RegCL on extended task sequences (10+ domains) to empirically validate whether inner product accumulation maintains performance without degradation or numerical instability
3. **Cross-Domain Generalization Test:** After training on the five specified domains, evaluate the final RegCL model on entirely unseen segmentation datasets (e.g., COCO, Cityscapes) to assess true generalization beyond the training distribution