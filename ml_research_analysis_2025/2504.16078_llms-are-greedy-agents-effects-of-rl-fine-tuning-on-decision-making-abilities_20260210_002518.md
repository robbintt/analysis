---
ver: rpa2
title: 'LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities'
arxiv_id: '2504.16078'
source_url: https://arxiv.org/abs/2504.16078
tags:
- action
- arxiv
- reward
- actions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically examines why LLMs perform suboptimally
  in decision-making scenarios. It identifies three prevalent failure modes: greediness
  (overly favoring high-reward actions), frequency bias (repeatedly selecting the
  most frequent action regardless of reward), and the knowing-doing gap (models know
  how to solve a task but fail to act on that knowledge).'
---

# LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities

## Quick Facts
- arXiv ID: 2504.16078
- Source URL: https://arxiv.org/abs/2504.16078
- Reference count: 40
- LLMs exhibit three key decision-making failure modes: greediness, frequency bias, and knowing-doing gap

## Executive Summary
This work systematically examines why LLMs perform suboptimally in decision-making scenarios. It identifies three prevalent failure modes: greediness (overly favoring high-reward actions), frequency bias (repeatedly selecting the most frequent action regardless of reward), and the knowing-doing gap (models know how to solve a task but fail to act on that knowledge). The paper proposes Reinforcement Learning Fine-Tuning (RLFT) on self-generated Chain-of-Thought rationales as a solution. Experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe demonstrate that RLFT enhances LLMs' decision-making abilities by increasing exploration and narrowing the knowing-doing gap.

## Method Summary
The paper evaluates LLMs' decision-making abilities across multiple environments including multi-armed bandits, contextual bandits, and Tic-tac-toe. It identifies three failure modes: greediness (overly favoring high-reward actions), frequency bias (selecting most frequent actions regardless of reward), and knowing-doing gap (knowing how to solve but failing to act). The proposed solution involves Reinforcement Learning Fine-Tuning (RLFT) using self-generated Chain-of-Thought rationales. The method is evaluated against baseline approaches including in-context learning, classic exploration mechanisms like ε-greedy, and LLM-specific approaches like self-consistency.

## Key Results
- RLFT reduces regret by 30-50% compared to in-context learning
- RLFT improves action coverage by 12-20%
- Exploration bonuses are particularly effective for fine-tuning LLMs in decision-making scenarios

## Why This Works (Mechanism)
RL Fine-Tuning (RLFT) with self-generated Chain-of-Thought rationales works by providing structured feedback that bridges the gap between a model's knowledge and its decision-making behavior. The Chain-of-Thought rationales allow the model to articulate reasoning before making decisions, which can then be reinforced through RLFT. This process helps the model learn to balance exploration and exploitation more effectively, addressing the greediness failure mode. Additionally, by explicitly training on the reasoning process rather than just final actions, RLFT helps close the knowing-doing gap where models understand solutions but fail to implement them.

## Foundational Learning
- Multi-armed bandits: A decision-making framework where agents must choose between multiple options with unknown reward distributions
  *Why needed:* Provides a controlled environment to test exploration-exploitation tradeoffs
  *Quick check:* Can measure regret and action coverage to quantify decision quality
- Contextual bandits: Extension of multi-armed bandits where decisions depend on context or state
  *Why needed:* Tests whether models can adapt decisions based on situational information
  *Quick check:* Evaluate if models select different actions for different contexts
- Reinforcement Learning Fine-Tuning (RLFT): Process of further training models using RL algorithms
  *Why needed:* Allows models to learn from interaction rather than just static examples
  *Quick check:* Compare performance before and after RLFT on held-out tasks
- Chain-of-Thought reasoning: Explicit step-by-step reasoning process before decision-making
  *Why needed:* Makes decision process transparent for reinforcement learning
  *Quick check:* Verify rationales are generated and used in the RLFT process

## Architecture Onboarding
Component map: Environment -> Model (LLM) -> Decision -> Reward -> RLFT -> Updated Model
Critical path: Observation → Reasoning (CoT) → Action → Reward → RL Update → New Policy
Design tradeoffs: Chain-of-Thought adds computational overhead but provides better supervision signals for RLFT
Failure signatures: Over-greedy selection, repetitive action patterns, inconsistency between stated reasoning and actual decisions
First experiments:
1. Test greedy agent behavior on simple bandit tasks to establish baseline failure modes
2. Apply ε-greedy exploration to measure improvement over greedy baseline
3. Evaluate RLFT with and without Chain-of-Thought rationales to isolate their contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on relatively simple environments may limit generalizability to complex real-world scenarios
- Sample of 25 related papers is modest, limiting contextualization within broader literature
- Evaluation framework relies on specific metrics that may not capture all aspects of decision-making quality

## Confidence
High confidence: The identification of greediness, frequency bias, and knowing-doing gap as failure modes is well-supported by experimental evidence across multiple domains
Medium confidence: The effectiveness of RLFT with self-generated Chain-of-Thought rationales, as results show improvement but may be sensitive to implementation details and hyperparameter choices
Medium confidence: The superiority of exploration bonuses for fine-tuning, though this conclusion is based on comparison with limited alternative mechanisms

## Next Checks
1. Test RLFT performance in more complex decision-making environments with non-stationary rewards and partial observability to assess generalizability
2. Conduct ablation studies to determine the specific contribution of Chain-of-Thought rationales versus RLFT alone in improving decision-making
3. Evaluate whether the identified failure modes persist in larger, more capable LLMs (e.g., GPT-4, Claude) compared to the models tested in this study