---
ver: rpa2
title: Performance Comparison of Large Language Models on Advanced Calculus Problems
arxiv_id: '2503.03960'
source_url: https://arxiv.org/abs/2503.03960
tags:
- answer
- correct
- found
- sqrt
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated seven Large Language Models (LLMs) on 32 advanced
  calculus problems, totaling 320 points, to assess their mathematical problem-solving
  capabilities. The models tested were ChatGPT 4o, Gemini Advanced with 1.5 Pro, Copilot
  Pro, Claude 3.5 Sonnet, Meta AI, Mistral AI, and Perplexity.
---

# Performance Comparison of Large Language Models on Advanced Calculus Problems

## Quick Facts
- arXiv ID: 2503.03960
- Source URL: https://arxiv.org/abs/2503.03960
- Authors: In Hak Moon
- Reference count: 0
- Primary result: Seven LLMs evaluated on 32 advanced calculus problems, scoring between 84.4% and 96.9% accuracy

## Executive Summary
This study evaluated seven Large Language Models (LLMs) on 32 advanced calculus problems to assess their mathematical problem-solving capabilities. The models tested included ChatGPT 4o, Gemini Advanced with 1.5 Pro, Copilot Pro, Claude 3.5 Sonnet, Meta AI, Mistral AI, and Perplexity. Performance varied significantly across models, with ChatGPT 4o and Mistral AI achieving the highest scores of 96.9% (310/320 points), while Perplexity scored lowest at 84.4% (270/320 points). The average score across all models was 90.4% (290/320 points). All models performed well on vector operations, partial derivatives, and line integrals, but struggled with complex integral evaluations and optimization problems. The study found that re-prompting was effective in correcting initial errors for some models, highlighting its importance in achieving accurate solutions.

## Method Summary
The study employed a systematic evaluation approach using 32 carefully selected advanced calculus problems, totaling 320 points across all questions. Each model was prompted with the same problems under similar conditions, and responses were scored using a standardized rubric. The evaluation included both exact numerical answers and mathematical reasoning tasks. Re-prompting strategies were tested to assess whether initial errors could be corrected through iterative prompting. Performance was measured across different calculus topics including vector operations, partial derivatives, line integrals, and optimization problems. The study maintained consistency in evaluation conditions while acknowledging potential limitations in problem difficulty distribution and scoring subjectivity.

## Key Results
- ChatGPT 4o and Mistral AI achieved highest scores of 96.9% (310/320 points)
- Perplexity scored lowest at 84.4% (270/320 points)
- Average score across all models was 90.4% (290/320 points)
- All models performed well on vector operations, partial derivatives, and line integrals
- Models struggled with complex integral evaluations and optimization problems

## Why This Works (Mechanism)
The mechanisms underlying LLM performance in calculus problems appear to be driven by their training on mathematical datasets and ability to recognize problem patterns. However, the study did not explicitly investigate these mechanisms, making it difficult to determine why certain models performed better than others on specific problem types.

## Foundational Learning
The paper does not discuss how the LLMs were originally trained on mathematical concepts or how their foundational learning relates to calculus problem-solving. The study focuses on performance evaluation rather than examining the learning processes that enable mathematical reasoning.

## Architecture Onboarding
The study does not address how the architectural differences between models (such as transformer configurations or fine-tuning approaches) influenced their calculus problem-solving abilities. This information was not investigated as part of the performance comparison.

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions for future research. However, implicit questions arise from the findings, such as why certain models struggled with optimization problems and how architectural differences might influence mathematical reasoning capabilities.

## Limitations
- Small sample size of 32 problems may not comprehensively represent advanced calculus challenges
- Scoring methodology relied on human judgment for partial credit, introducing potential subjectivity
- Study did not account for variations in problem difficulty, potentially biasing results
- Re-prompting effectiveness varied across models but was not systematically analyzed
- Limited exploration of why certain models performed better on specific problem types

## Confidence
- High Confidence: Performance rankings among models for vector operations and partial derivatives
- Medium Confidence: Overall accuracy scores and average performance metrics
- Low Confidence: Comparative effectiveness of re-prompting strategies across different model types
- Low Confidence: Understanding of why models struggled with optimization problems

## Next Checks
1. Expand the problem set to include 100+ diverse calculus problems with varying difficulty levels and problem types
2. Implement double-blind scoring with multiple evaluators to reduce subjectivity in grading
3. Conduct time-controlled evaluations with standardized prompting strategies across all models to ensure fair comparison of performance metrics
4. Investigate the relationship between model architecture and performance on specific calculus problem types
5. Systematically analyze the effectiveness of re-prompting strategies across different models and problem categories