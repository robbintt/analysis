---
ver: rpa2
title: 'Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating
  Reward Hacking'
arxiv_id: '2510.13694'
source_url: https://arxiv.org/abs/2510.13694
tags:
- responses
- rlhf
- reward
- hacking
- inform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in RLHF, where RL policies
  exploit misgeneralized reward models, leading to degraded alignment. The authors
  propose InfoRM, an information-theoretic reward modeling framework that filters
  out preference-irrelevant information using the Information Bottleneck principle,
  thereby reducing reward misgeneralization.
---

# Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking

## Quick Facts
- arXiv ID: 2510.13694
- Source URL: https://arxiv.org/abs/2510.13694
- Reference count: 40
- Key outcome: InfoRM+IBL consistently outperforms baselines in RLHF performance and reward hacking mitigation

## Executive Summary
This paper addresses reward hacking in RLHF where RL policies exploit misgeneralized reward models, leading to degraded alignment. The authors propose InfoRM, an information-theoretic reward modeling framework that filters out preference-irrelevant information using the Information Bottleneck principle, thereby reducing reward misgeneralization. They introduce IBL, a distribution-level RL regularization derived from InfoRM's IB latent space, which mitigates reward hacking while preserving policy exploration flexibility. Additionally, they propose MOP, a statistical metric based on Mahalanobis distance to quantify reward hacking severity. Extensive experiments across diverse LLMs and datasets show InfoRM and IBL consistently outperform baselines in RLHF performance and reward hacking mitigation, with MOP serving as a reliable diagnostic tool.

## Method Summary
InfoRM applies the Information Bottleneck principle to reward modeling by maximizing mutual information between latent representations and preference labels while minimizing mutual information with raw inputs. This forces the encoder to retain only preference-relevant features. IBL computes the Mahalanobis distance between each RL response's IB representation and the SFT-induced distribution, then penalizes large deviations. MOP uses the chi-squared distribution of squared Mahalanobis distances to flag outliers as potential reward hacking instances. The framework integrates into standard RLHF: SFT training, InfoRM training with IB objective, PPO with IBL regularization, and MOP monitoring.

## Key Results
- InfoRM with IBL regularization consistently outperforms standard reward modeling plus KL regularization across multiple datasets
- MOP metric successfully tracks reward hacking severity and enables early stopping that improves final win rates
- InfoRM substantially reduces length bias compared to standard reward models while maintaining or improving accuracy on OOD benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Filtering preference-irrelevant information from reward model representations reduces reward misgeneralization susceptibility. InfoRM applies the Information Bottleneck principle by maximizing mutual information between latent representations and preference labels while minimizing mutual information between representations and raw inputs conditioned on labels. This forces the encoder to retain only preference-relevant features. Core assumption: reward hacking arises partly because standard RMs overfit to spurious correlations that don't reflect true human preferences.

### Mechanism 2
Distribution-level regularization in the IB latent space mitigates reward hacking while preserving more policy exploration flexibility than token-level KL constraints. IBL computes the Mahalanobis distance between each RL response's IB representation and the SFT-induced distribution, then penalizes large deviations. Core assumption: reward-hacked responses consistently manifest as outliers in InfoRM's IB latent space - a structural property specific to how the IB shapes the representation space.

### Mechanism 3
Mahalanobis Outlier Probability (MOP) provides a statistical, training-free diagnostic for reward hacking severity. Under the multivariate Gaussian assumption, squared Mahalanobis distances follow a chi-squared distribution, enabling p-value computation. MOP is defined as the proportion of RL samples flagged as outliers. Core assumption: The IB representations of SFT responses are approximately Gaussian, and reward-hacked samples are the dominant source of outliers.

## Foundational Learning

- **Concept: Information Bottleneck principle**
  - Why needed: InfoRM's core objective directly implements IB; understanding the compression-prediction trade-off is essential
  - Quick check: Can you explain why maximizing I(S;Y) while minimizing I(X;S|Y) filters irrelevant information?

- **Concept: Mahalanobis distance and covariance structure**
  - Why needed: Both IBL regularization and MOP detection depend on Mahalanobis distance being sensitive to low-variance directions
  - Quick check: Why does Mahalanobis distance outperform Euclidean distance when features have different variances and correlations?

- **Concept: RLHF pipeline (SFT → RM → PPO)**
  - Why needed: InfoRM replaces standard RM training; IBL integrates into PPO's objective; MOP monitors the RL phase
  - Quick check: At what point in the standard RLHF pipeline does reward hacking typically manifest, and why?

## Architecture Onboarding

- **Component map:** SFT model → InfoRM (encoder + decoder) → IB latent representations → PPO + IBL regularization → RL responses → MOP computation

- **Critical path:**
  1. Train InfoRM with IB objective (β ∈ {0.001, 0.01, 0.1}); validate on RM benchmarks
  2. Extract and cache IB representations from SFT model; compute μ, Σ
  3. Integrate IBL into PPO: for each generated response, compute Mahalanobis distance and add penalty γ · IBL(x) to reward
  4. Monitor MOP during training; use for hyperparameter tuning or early stopping

- **Design tradeoffs:**
  - β (IB coefficient): Higher values increase compression but risk losing preference-relevant signal
  - γ (IBL strength): Higher values better suppress hacking but may overly constrain exploration
  - Latent dimension k: Must be sufficient for preference information; paper uses 64-256
  - Significance level α for MOP: Lower values (e.g., 0.01) are more conservative

- **Failure signatures:**
  - InfoRM generalization poor: β may be too low (insufficient filtering) or too high (over-compression)
  - IBL doesn't help: Check if outliers actually appear in IB space; may need different β or architecture
  - MOP uncorrelated with quality: Gaussian assumption may be violated; check SFT representation distribution
  - Performance degrades vs Standard RM: IB objective may be dominated by compression term

- **First 3 experiments:**
  1. Reproduce IB outlier phenomenon: Train InfoRM (β=0.1), visualize SFT vs RLHF vs hacking samples in IB space using t-SNE; verify outliers correlate with GPT-4-identified hacks
  2. Ablate IBL vs KL: Train RLHF with InfoRM + IBL vs InfoRM + KL across 2 datasets; compare final win rates and training stability curves
  3. Validate MOP early stopping: Run RLHF with Standard RM, use MOP threshold to trigger early stopping; compare final GPT-4 win rate vs full training

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the MOP metric remain reliable for detecting reward hacking when the SFT-induced latent distribution is multimodal rather than Gaussian?
  - Basis: Section III-B and V-A state MOP relies on Gaussian distribution assumption
  - Why unresolved: Real-world instruction-tuning datasets often cover diverse tasks, potentially inducing multimodal latent structures
  - What evidence would resolve: Experiments on datasets constructed to elicit multimodal latent distributions in IB space

- **Open Question 2:** Can IBL regularization be effectively combined with token-level KL penalties to achieve superior stability compared to IBL alone?
  - Basis: Paper contrasts IBL with KL as binary choices rather than continuous spectrum
  - Why unresolved: Trade-off between IBL's distribution-level flexibility and KL's token-level constraints unexplored
  - What evidence would resolve: Ablation studies adding varying coefficients of KL regularization to IBL objective

- **Open Question 3:** Is the emergence of reward hacking as outliers in the latent space a unique property of the Information Bottleneck, or does it generalize to other compressed representation learning methods?
  - Basis: Section V-F shows outliers appear in InfoRM's IB space but not Standard RM space
  - Why unresolved: Unclear if outlier phenomenon is caused by IB's mutual information objective or any compact representation learning
  - What evidence would resolve: Comparative experiments applying MOP to reward models with VAE or contrastive losses

## Limitations

- The effectiveness of MOP detection depends on the Gaussian assumption of SFT IB representations, which may be violated in real-world multimodal datasets
- IBL regularization's effectiveness is specifically tied to InfoRM's unique IB latent space structure, potentially limiting generalizability to other reward modeling approaches
- The coupling between InfoRM and IBL creates a potential dependency where the outlier phenomenon may not generalize to different IB implementations

## Confidence

- **High Confidence**: The fundamental information-theoretic framework (InfoRM with IB objective) is sound and well-grounded in established theory. The experimental methodology is rigorous.
- **Medium Confidence**: The empirical effectiveness of IBL regularization and MOP detection across diverse datasets. While results are strong, generalizability needs further validation.
- **Low Confidence**: The robustness of MOP under distributional violations (non-Gaussian SFT representations) and its sensitivity to the choice of significance level α. Long-term stability of InfoRM's improvements.

## Next Checks

1. **Distributional Assumption Validation**: Systematically test the Gaussianity assumption of SFT IB representations across different datasets and model scales using Q-Q plots and goodness-of-fit tests to quantify deviations from normality.

2. **Generalization of IBL Effectiveness**: Evaluate IBL regularization when applied to IB latent spaces from different architectures and when the underlying reward model is not InfoRM but a standard transformer with added IB layers.

3. **MOP as Early Stopping Signal**: Conduct controlled experiments comparing MOP-guided early stopping against standard early stopping across multiple random seeds and datasets, measuring both final win-rates and response quality at stopping points.