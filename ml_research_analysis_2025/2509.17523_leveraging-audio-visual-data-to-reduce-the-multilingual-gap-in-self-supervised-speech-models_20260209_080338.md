---
ver: rpa2
title: Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised
  Speech Models
arxiv_id: '2509.17523'
source_url: https://arxiv.org/abs/2509.17523
tags:
- speech
- bilingual
- multilingual
- monolingual
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether limited visual grounding can reduce
  the multilingual gap in self-supervised speech models. Specifically, the authors
  explore whether adding visual information during pretraining can help bilingual
  models overcome cross-lingual interference and perform closer to monolingual models.
---

# Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models

## Quick Facts
- arXiv ID: 2509.17523
- Source URL: https://arxiv.org/abs/2509.17523
- Reference count: 0
- Primary result: Visual grounding reduces multilingual gap from 31.5% to 8.04% in zero-shot phonetic discrimination

## Executive Summary
This work investigates whether limited visual grounding can reduce the multilingual gap in self-supervised speech models. The authors explore whether adding visual information during pretraining helps bilingual models overcome cross-lingual interference and perform closer to monolingual models. Through sequential training of HuBERT-base models (first audio-only, then audio-visual), they demonstrate that visual grounding substantially reduces the performance gap between monolingual and bilingual models, particularly benefiting bilingual models which show 26.74% relative improvement compared to 10.84% for monolingual models.

## Method Summary
The authors use a sequential training approach: first training HuBERT-base models on audio-only data, then continuing training with audio-visual paired data using the FaST-VGS+ architecture. Visual grounding is limited to using image-caption pairs from the ML-COCO dataset, with speech synthesized from captions. Both monolingual (single language) and bilingual (two languages) models are trained with equal total training hours to isolate the effect of visual grounding from data quantity effects. The audio encoder's CNN is frozen during the visual grounding stage, and k-means pseudo-labels from Stage 1 are reused to preserve phonetic learning.

## Key Results
- Visual grounding reduces multilingual gap from 31.5% to 8.04% in zero-shot phonetic discrimination
- Bilingual models show 26.74% relative improvement with visual grounding vs. 10.84% for monolingual models
- Visually grounded models demonstrate better language discrimination and greater robustness to domain shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual grounding functions as an interlingual bridge that reduces cross-lingual interference in bilingual models.
- Mechanism: The visual modality provides a language-agnostic semantic anchor. When both languages map to the same visual representations (e.g., images of "a dog"), the model learns to associate different phonetic patterns with shared semantics, reducing competition between language-specific representations.
- Core assumption: Cross-lingual interference is the primary cause of the multilingual gap, not data sparsity alone.
- Evidence anchors:
  - [abstract] "reducing the multilingual performance gap on zero-shot phonetic discrimination from 31.5% for audio-only models to 8.04% with grounding"
  - [Section 1] "visual information could serve as an additional signal to help models better discriminate between languages during training, potentially reducing cross-lingual interference"
  - [corpus] Weak direct evidence; corpus papers address multilingual interference but not visual grounding specifically.
- Break condition: If visual representations differ substantially across languages/cultures (e.g., culturally-specific objects), the interlingual bridge effect may weaken.

### Mechanism 2
- Claim: Sequential training (audio-only → audio-visual) preserves phonetic learning while adding semantic grounding.
- Mechanism: Audio-only pretraining establishes robust phonetic representations via masked token prediction. The subsequent audio-visual stage adds semantic alignment without disrupting learned phonetic patterns because the audio encoder's CNN is frozen and the k-means pseudo-labels are reused.
- Core assumption: Early phonetic learning transfers to the multimodal stage without catastrophic forgetting.
- Evidence anchors:
  - [Section 2.2] "we adopt a sequential training schedule where models are first trained on audio-only data before introducing audio-visual paired data"
  - [Section 2.4] "we use the same k-means model derived from the first stage of audio-only training for processing the pseudo-labels"
  - [corpus] No direct corpus evidence on sequential vs. simultaneous training for multilingual SSL.
- Break condition: If the audio-visual dataset differs drastically in acoustic properties from the audio-only pretraining data, domain shift could degrade phonetic representations.

### Mechanism 3
- Claim: Bilingual models benefit disproportionately from visual grounding compared to monolingual models.
- Mechanism: Monolingual models face only within-language phonetic competition. Bilingual models face cross-lingual interference where similar-sounding phonemes across languages compete. Visual grounding provides disambiguating signal specifically where interference is highest—hence the larger relative gain (26.74% vs. 10.84%).
- Core assumption: The visual signal is informative enough to disambiguate phonetically similar but linguistically distinct content.
- Evidence anchors:
  - [Section 3, Results] "bilingual setting... yields a relative improvement z of 26.74%" vs. "monolingual... relative improvement x of 10.84%"
  - [Section 3, language discrimination] "visually grounded models (VGS+) show substantially lower error rates... indicating that vision strengthens the separation between languages"
  - [corpus] "MiLorE-SSL" addresses multilingual scaling but does not test visual grounding mechanisms.
- Break condition: If language pairs have minimal phonetic overlap, the differential benefit for bilingual models may diminish.

## Foundational Learning

- Concept: **Masked Token Prediction (HuBERT objective)**
  - Why needed here: The audio encoder learns by predicting clustered pseudo-labels for masked speech segments. Understanding this clarifies why reusing k-means labels in Stage 2 preserves phonetic structure.
  - Quick check question: Can you explain why freezing the CNN encoder and reusing k-means pseudo-labels preserves Stage 1 learning?

- Concept: **Contrastive Learning for Cross-Modal Alignment**
  - Why needed here: The audio-visual loss (L_av) uses contrastive learning where correct audio-image pairs are positive samples and mismatched pairs are negatives. This drives semantic alignment.
  - Quick check question: In a batch of 8 audio-image pairs, how many negative samples would each pair have?

- Concept: **ABX Discrimination Metric**
  - Why needed here: The paper's core claims rest on ABX phonetic and language discrimination scores. Understanding ABX (within-speaker vs. across-speaker, DTW alignment) is essential for interpreting results.
  - Quick check question: If a model has ABX error of 7%, what does that mean about the distance relationship between A-X and B-X in embedding space?

## Architecture Onboarding

- Component map:
  - Audio Encoder: HuBERT-base (12 transformer layers) → outputs to Res-DAVEnet + 1 transformer layer → CLS token = audio embedding
  - Image Encoder: Faster RCNN RoI features → 6-layer transformer → CLS token = image embedding
  - Fusion: Contrastive loss between audio CLS and image CLS embeddings; combined loss L = 0.5·L_a + 0.5·L_av
  - Training stages: Stage 1 (audio-only, 90k steps, LR=0.0005) → Stage 2 (audio-visual, 2k steps, LR=0.0001)

- Critical path:
  1. Train HuBERT-base on LibriLight (EN) or Audiocite (FR) or both for bilingual
  2. Extract k-means pseudo-labels from Stage 1
  3. Initialize FaST-VGS+ with Stage 1 weights; freeze CNN encoder
  4. Train on ML-COCO synthesized speech + LXMERT image embeddings using dual loss
  5. Extract layer-12 representations for ABX evaluation

- Design tradeoffs:
  - **α=0.5 weighting**: Equal audio and audio-visual loss. Paper notes this may not be optimal—alternative weightings are unexplored.
  - **Synthesized speech for visual grounding**: Uses TTS with 2 voices (1 male, 1 female). Domain mismatch with read speech is acknowledged; visual grounding may help robustness here.
  - **Limited visual data**: Only 275 hours per language vs. 1000 hours audio-only. Visual grounding is intentionally "limited."

- Failure signatures:
  - If ABX phonetic error increases after Stage 2: Check if L_av weight is too high, overwhelming L_a
  - If language discrimination doesn't improve: Visual embeddings may not be informative; verify LXMERT features are loading correctly
  - If bilingual model degrades more than monolingual: Check data balance—each language should have equal hours in bilingual setting

- First 3 experiments:
  1. **Replicate monolingual vs. bilingual gap**: Train HuBERT-base on 1k hours EN only vs. 500h EN + 500h FR. Verify ABX gap ~31% relative.
  2. **Add visual grounding to monolingual**: Train Stage 2 with ML-COCO EN pairs. Confirm ~10% relative improvement in ABX.
  3. **Add visual grounding to bilingual**: Train Stage 2 with ML-COCO EN+FR pairs. Confirm gap reduces to ~8% and bilingual improvement > monolingual improvement.

## Open Questions the Paper Calls Out
- What is the minimum amount of audio-visual data required to achieve meaningful reduction in the multilingual gap? The authors state this is an open question, as they used a fixed ~275 hours per language without systematically varying this amount.
- Does improved language discrimination directly cause reduced multilingual gap, or is it merely correlated? The study shows correlation but does not establish causation through controlled intervention.
- Do gains in zero-shot ABX phonetic discrimination transfer to practical downstream tasks such as automatic speech recognition? The evaluation focused only on zero-shot ABX tasks, not on supervised downstream tasks.
- Does visual grounding reduce the multilingual gap for language pairs with varying degrees of phonetic overlap, or only for typologically similar pairs like English-French? The study only tested English and French, leaving generalization to phonetically distant languages untested.

## Limitations
- Data synthesis assumptions: Specific TTS models, voices, and preprocessing details are not disclosed, creating uncertainty about generalizability to natural speech.
- Architecture specificity gaps: Exact implementation details for Res-DAVEnet (CNN depth, layer configurations) are not provided, potentially affecting reproducibility.
- Limited cross-lingual scope: The study only examines English-French pairs, leaving generalizability to language pairs with greater typological distance untested.

## Confidence
- Visual grounding reduces multilingual gap (High): The 31.5% to 8.04% reduction is well-documented with clear metrics and multiple validation checks.
- Bilingual models benefit more than monolingual (Medium): The relative improvement difference is demonstrated, but the underlying mechanism is inferred rather than directly measured.
- Sequential training preserves phonetic learning (Medium): Preservation is implied by performance metrics, but there's no direct analysis of representation drift between stages.

## Next Checks
1. **Cross-linguistic generalization test**: Replicate the study with a language pair having minimal phonetic overlap (e.g., English-Japanese) to verify whether the bilingual advantage persists and whether visual grounding remains effective.
2. **Direct interference measurement**: Add probing experiments to measure cross-lingual interference directly (e.g., phonetic confusion matrices between languages) to validate that visual grounding specifically reduces interference rather than improving representations generally.
3. **Simultaneous vs sequential training comparison**: Train models with audio-visual data from the start versus the sequential approach to determine whether staged learning is necessary or if benefits can be achieved through joint training.