---
ver: rpa2
title: 'Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate
  Relation Extraction'
arxiv_id: '2510.06198'
source_url: https://arxiv.org/abs/2510.06198
tags:
- relation
- summarization
- sentence
- llms
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes CogRE, a cognitive-structured framework for\
  \ explainable relation extraction that improves both accuracy and explainability.\
  \ The framework decomposes relation extraction into three steps\u2014chunking, keyword\
  \ anchoring, and integrative reasoning\u2014inspired by cognitive science models\
  \ of human text processing."
---

# Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction

## Quick Facts
- arXiv ID: 2510.06198
- Source URL: https://arxiv.org/abs/2510.06198
- Reference count: 40
- Primary result: Improves F1 scores by up to 48.11% and human explanation quality ratings by 54% relative in one-shot relation extraction

## Executive Summary
This paper introduces CogRE, a cognitive-structured framework that improves both accuracy and explainability in relation extraction by decomposing the task into three sequential steps inspired by human text processing: chunking, keyword anchoring, and integrative reasoning. The framework uses reinforcement learning with a HIT@DICT reward that matches explanation keywords against a self-generated dictionary, enabling automatic supervision of explanation quality without human annotation. Experiments on TACRED and NYT29 datasets with Phi-4 and Qwen2.5-14B-Instruct show significant improvements in both task performance and explanation quality compared to accuracy-only training.

## Method Summary
CogRE decomposes relation extraction into three cognitive-inspired steps: first chunking sentences into relational propositions to reduce token-level matching errors, then anchoring keywords that explicitly identify relation-carrying terms, and finally integrative reasoning to combine these into coherent chains producing both labels and explanations. The framework employs reinforcement learning with Group Relative Policy Optimization (GRPO) using a combined reward of accuracy (asymmetric weighting for 1:7 positive:negative imbalance) and HIT@DICT (keyword matching against a self-generated dictionary normalized by output length). A relational keywords dictionary is automatically constructed from high-quality self-generated explanations using GPT-4o keyword extraction.

## Key Results
- Improves F1 scores by up to 48.11% relative compared to accuracy-only training
- Increases human explanation quality ratings by 54% relative
- Achieves faster convergence with HIT@DICT reward (+0.15, +0.06 higher final reward values)
- Demonstrates that removing any decomposition step causes F1 drops of -1.66% to -21.51%

## Why This Works (Mechanism)

### Mechanism 1: Cognitive-Structured Decomposition
- Claim: Decomposing relation extraction into chunking, keyword anchoring, and integrative reasoning improves accuracy and reduces reasoning errors
- Mechanism: The three-step approach reduces attention dispersion by first compressing sentences, then focusing on relation-carrying terms, and finally combining them into coherent chains
- Core assumption: LLMs suffer from attention dispersion on long sequences; explicit intermediate steps act as scaffolding
- Evidence: F1 drops of -1.66% to -21.51% when removing any step; recall drops -44.27% to -21.95% on TACRED when chunking removed
- Break condition: If sentences are already short and proposition-like, chunking adds computational overhead without accuracy gains

### Mechanism 2: HIT@DICT Reward Signal
- Claim: Lightweight rule-based reward matching model explanations against self-generated keyword dictionary improves both accuracy and explanation quality
- Mechanism: The reward reinforces outputs containing relation-specific keywords extracted from true positive samples, creating feedback loop that aligns reasoning vocabulary with gold labels
- Core assumption: Keywords from correct model predictions sufficiently capture relation identity; dictionary coverage is adequate
- Evidence: Training with HIT@DICT converges faster and achieves higher final reward values; no direct corpus validation found
- Break condition: If dictionary lacks coverage for rare relations or contains noisy keywords, reward may reinforce spurious vocabulary

### Mechanism 3: Combined Reward with GRPO Optimization
- Claim: GRPO with jointly weighted accuracy and explanation rewards enables stable training that balances task performance with interpretability
- Mechanism: GRPO normalizes rewards within sampled groups, asymmetric accuracy reward counters 1:7 positive:negative imbalance, KL penalty prevents policy drift
- Core assumption: Group-based normalization provides meaningful gradient signal; combined rewards prevent reward hacking
- Evidence: Qwen2.5-14B with HIT@DICT + Acc achieves 48.11% F1 on NYT29 versus 27.69% with accuracy-only
- Break condition: If group size is too small or rewards too sparse, variance in advantage estimates destabilizes training

## Foundational Learning

- Concept: One-shot Relation Extraction
  - Why needed here: Determines if test sentence expresses same relation as support sentence given only one labeled example per relation type
  - Quick check question: Can you explain why one-shot RE with 1:7 positive:negative imbalance requires asymmetric reward weighting?

- Concept: Reinforcement Learning from Verifiable Rewards
  - Why needed here: HIT@DICT exemplifies verifiable rewards - automatically checkable signals that don't require human annotation during training
  - Quick check question: What distinguishes a "verifiable" reward from one requiring human judgment or LLM-as-judge?

- Concept: Explanation-Quality Evaluation
  - Why needed here: Paper introduces dual evaluation - automatic F1 plus human ratings on 3-point rubric (summarization correctness + conciseness + label alignment)
  - Quick check question: Why might optimizing only for accuracy produce explanations that humans rate poorly?

## Architecture Onboarding

- Component map:
  1. Relational Keywords Dictionary (offline): GPT-4o extracts keywords from vanilla LLM's true positive outputs
  2. CogRE Prompting Module: Three-step prompt with few-shot summarization examples
  3. Reward Calculator: R = R_Acc + R_HIT@DICT with asymmetric weights and weighted keyword matches
  4. GRPO Trainer: Samples m outputs per input, computes group-relative advantages, optimizes with KL penalty
  5. Inference Pipeline: Trained model generates explanation + label using same three-step structure

- Critical path:
  1. Run vanilla LLM on training positives → filter true positives → GPT-4o extracts keywords → build dictionary
  2. Construct three-step prompt with summarization examples
  3. Initialize GRPO with combined reward; sample groups of m outputs per input
  4. Train 4-18 hours on 4×H100 (14B model: ~20 GPU-hours total)
  5. Evaluate F1 + human ratings using rubric

- Design tradeoffs:
  - Dictionary size (K=1-5): More samples increase coverage but may introduce noise
  - Keyword weights (w_entity=0.4, w_relation=1.0): Prioritizes relation-specific terms
  - Accuracy asymmetry: +3/-3 for Yes, +1/-1 for No counters 1:7 imbalance
  - Model scale: Appendix A.12 shows models <10B fail; minimum 14B recommended

- Failure signatures:
  - Stagnant rewards + near-zero KL: Policy not updating (accuracy-only Qwen)
  - High precision, low recall: Over-conservative; HIT@DICT too restrictive
  - Skipping reasoning steps: Qwen-NYT29 occasionally jumps from chunking to answer
  - Abstraction errors: Confusing similar relations; dictionary lacks discriminative keywords

- First 3 experiments:
  1. Baseline comparison: Run vanilla LLM with direct-matching, simple-reasoning, and CogRE (no RL) on sampled test set; compute F1 to isolate decomposition gains
  2. Reward ablation: Train accuracy-only vs. HIT@DICT-only vs. combined; plot reward/KL/response-length curves; measure convergence speed and final F1
  3. Dictionary sensitivity: Vary K ∈ {1, 3, 5} samples per label; measure F1 breakdown by relation frequency to verify rare-relation coverage

## Open Questions the Paper Calls Out

- **Question 1**: How can the framework be stabilized to prevent the model from skipping the integrative reasoning step while maintaining conciseness?
  - Basis: Section 5.2 notes model "skips the reasoning after chunking in some cases" to produce shorter outputs
  - Why unresolved: Current reward structure doesn't enforce structural constraint on reasoning chain length or completeness
  - What evidence would resolve it: Modified training run with penalty for missing reasoning steps, evaluated on NYT29

- **Question 2**: How sensitive is performance to quality and coverage of the "vanilla" LLM used to construct the Relational Keywords Dictionary?
  - Basis: Algorithm 1 relies on sampling explanations from untrained LLM; errors could propagate through training
  - Why unresolved: Paper assumes sufficient "good cases" but doesn't test scenarios with poor initial model
  - What evidence would resolve it: Experiments varying quality of dictionary generation model and measuring RL convergence and F1 scores

- **Question 3**: Can HIT@DICT reward be generalized to tasks with less distinct "relational keywords" like event extraction or implicit sentiment analysis?
  - Basis: Method specialized for Relation Extraction where keywords map clearly to labels
  - Why unresolved: Efficacy depends on existence of finite, predictable set of trigger words which may not exist for all NLP tasks
  - What evidence would resolve it: Applying CogRE framework to ACE05 (event extraction) and evaluating if dictionary-based reward yields comparable gains

## Limitations

- Framework validated only on one-shot RE with TACRED and NYT29 datasets; performance on standard supervised RE or multi-hop reasoning untested
- Quality of relational keywords dictionary depends on vanilla LLM's precision and GPT-4o's keyword extraction accuracy; errors propagate through training
- Framework fails on models below 10B parameters (Qwen-3B: 8.32% F1, Llama-8B: 1.40%), limiting applicability to smaller models and edge deployments

## Confidence

- **High Confidence**: Accuracy improvements from cognitive decomposition (Table 1, ablation study), human evaluation reliability (Cohen's κ=0.693), and convergence speed with HIT@DICT (Figure 2)
- **Medium Confidence**: Specific mechanism by which HIT@DICT improves explanation quality (requires human validation across relation types), and generalizability of asymmetric accuracy rewards
- **Low Confidence**: Claims about attention dispersion in LLMs and whether three-step decomposition is optimal for all RE scenarios

## Next Checks

1. **Per-Relation Breakdown Analysis**: Evaluate F1 and explanation quality by relation frequency and similarity (e.g., org:city vs. org:country) to identify abstraction-level failure modes

2. **Dictionary Coverage Study**: Vary K ∈ {1, 3, 5} samples per label and measure F1 degradation on rare relations to quantify dictionary completeness impact

3. **Direct Comparison to Human-Annotated Explanations**: Train a subset with human-written keywords as ground truth and compare F1 gains to HIT@DICT to assess automatic keyword extraction quality