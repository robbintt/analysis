---
ver: rpa2
title: Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection
arxiv_id: '2510.08073'
source_url: https://arxiv.org/abs/2510.08073
tags:
- real
- nsg-vd
- fake
- video
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting AI-generated videos
  that achieve near-perfect visual realism, particularly those from advanced models
  like Sora. The authors propose a physics-driven detection method based on Normalized
  Spatiotemporal Gradient (NSG), a statistic that quantifies deviations from natural
  video dynamics using probability flow conservation principles.
---

# Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection

## Quick Facts
- arXiv ID: 2510.08073
- Source URL: https://arxiv.org/abs/2510.08073
- Reference count: 40
- Primary result: Achieves 16.00% higher Recall and 10.75% higher F1-Score than state-of-the-art baselines in detecting AI-generated videos, including Sora

## Executive Summary
This paper addresses the challenge of detecting AI-generated videos that achieve near-perfect visual realism, particularly those from advanced models like Sora. The authors propose a physics-driven detection method based on Normalized Spatiotemporal Gradient (NSG), a statistic that quantifies deviations from natural video dynamics using probability flow conservation principles. The method leverages pre-trained diffusion models to estimate spatial gradients and motion-aware temporal derivatives, then computes Maximum Mean Discrepancy (MMD) between NSG features of test and real videos. The approach is theoretically grounded with an upper bound showing amplified discrepancies in generated videos. Extensive experiments demonstrate the method outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score, with strong performance even under data-imbalanced scenarios and on challenging closed-source generators like Sora.

## Method Summary
The proposed NSG-VD method detects AI-generated videos by computing a physics-based statistic called Normalized Spatiotemporal Gradient. This involves using a pre-trained diffusion model to estimate the spatial gradient of the data distribution, approximating temporal derivatives using inter-frame differences, and combining these to form the NSG feature. A deep kernel (Swin Transformer) is trained to optimize MMD between NSG features of real and generated videos. Detection is performed by comparing a test video's NSG features against a reference set of real videos using MMD, with a threshold determining the final classification.

## Key Results
- Outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score
- Maintains strong performance under data-imbalanced scenarios with Recall of 74.90% and F1-Score of 72.43%
- Successfully detects videos from closed-source generators like Sora, which previous methods struggle with

## Why This Works (Mechanism)

### Mechanism 1: Physics-Driven Distribution Shift Detection via NSG
The method detects AI-generated videos by identifying violations of physical continuity laws modeled as probability flow conservation. The Normalized Spatiotemporal Gradient (NSG) acts as a "dual field" to the probability velocity, with real videos theoretically preserving the constraint $v \cdot g \approx 1$. Generated videos exhibit amplified discrepancies in this metric due to distributional shifts. The theoretical upper bound in Theorem 1 shows NSG distances expand with distribution shift $\phi$. This approach is consistent with neighbor papers identifying temporal inconsistency and physical implausibility as key detection vectors.

### Mechanism 2: Diffusion-Guided Gradient Approximation
Pre-trained diffusion models estimate the spatial gradients of the data distribution ($\nabla_x \log p$) without training on the specific detection task. The method uses the score function $s_\theta(x_t)$ from a pre-trained diffusion model to approximate $\nabla_x \log p$, while temporal derivatives are approximated using inter-frame differences and a brightness constancy constraint, avoiding explicit optical flow computation.

### Mechanism 3: Amplified Discrepancy Measurement via MMD
The distributional discrepancy in NSG features is inherently larger for generated videos than real ones, making it a robust metric for detection. The method computes the Maximum Mean Discrepancy (MMD) between the NSG features of a test video and a reference set of real videos using a deep kernel. A threshold on this MMD score classifies the video, leveraging the separability of NSG features in the Reproducing Kernel Hilbert Space.

## Foundational Learning

**Concept: Score-Based Generative Models (Diffusion)**
- **Why needed here:** The method relies on using the diffusion model's score function ($s_\theta \approx \nabla_x \log p$) as a feature extractor. Understanding that this score represents the gradient of the log-likelihood is essential.
- **Quick check question:** Can you explain why a denoising autoencoder (diffusion model) effectively learns the gradient of the data distribution $\nabla_x \log p(x)$?

**Concept: Continuity Equation / Fluid Dynamics**
- **Why needed here:** The core theoretical justification for NSG is derived from the conservation of probability mass, analogous to fluid dynamics ($\partial_t \rho + \nabla \cdot (\rho v) = 0$).
- **Quick check question:** In the context of this paper, what does the "velocity field" $v(x,t)$ represent in terms of video evolution?

**Concept: Maximum Mean Discrepancy (MMD)**
- **Why needed here:** The final detection is not a simple classifier but a statistical test comparing the distribution of NSG features against a reference set using MMD.
- **Quick check question:** How does a "deep kernel" differ from a standard Gaussian kernel in the context of MMD optimization?

## Architecture Onboarding

**Component map:** Input frames -> Pre-trained Guided Diffusion Model -> Score estimation $s_\theta(x_t)$ -> Inter-frame difference calculator -> NSG calculator -> Deep Kernel (Swin Transformer) -> MMD Calculator -> Binary decision (Fake/Real)

**Critical path:** The estimation of the spatial gradient $s_\theta(x_t)$ is the heavy lifting. The choice of diffusion model (Guided Diffusion 256x256) dictates the resolution and quality of the physics proxy features.

**Design tradeoffs:**
- **Pre-trained vs. Custom:** Uses a frozen diffusion model (high inference cost, $0.36s$/video) to avoid training a large detector on potentially limited synthetic data
- **Approximation vs. Precision:** Approximates $\partial_t \log p$ via brightness constancy to avoid complex optical flow decomposition, risking error in rapidly changing scenes

**Failure signatures:**
- **High dynamics:** Scenes with explosive motion or cuts may violate the "smoothly varying" assumption of the continuity equation derivation
- **Numerical Instability:** If temporal derivatives approach zero (static scenes), the NSG denominator becomes unstable, though the paper claims $\lambda$ regularization mitigates this

**First 3 experiments:**
1. **Unit Test (NSG Estimation):** Visualize the NSG map $g(x,t)$ for a known AI-generated video (e.g., Sora) vs. a real video. Confirm that "fake" regions show higher NSG variance.
2. **Ablation (Spatial vs. Temporal):** Run the detector using only the spatial score $s_\theta$ vs. only the temporal derivative to verify the paper's claim that their combination is necessary for high F1-score.
3. **Threshold Sensitivity:** Validate the stability of the decision threshold $\tau$ across different generators (e.g., Pika vs. SEINE) as shown in Figure 3.

## Open Questions the Paper Calls Out

**Open Question 1:** How does the assumption that the divergence of the velocity field is subdominant (incompressible flow) affect detection performance when analyzing videos with highly discontinuous motion or abrupt scene changes? The paper acknowledges in Appendix F that simplified physical assumptions may fail to capture highly dynamic or discontinuous motion patterns, but doesn't quantify performance degradation on such content.

**Open Question 2:** To what extent does a domain shift between the pre-trained diffusion model and the target video distribution degrade the reliability of the Normalized Spatiotemporal Gradient (NSG) estimation? While the paper uses a general pre-trained diffusion model, the sensitivity of the NSG statistic to domain mismatch between the score model's training data and test video domain remains unquantified.

**Open Question 3:** Can the NSG estimation framework be adapted for real-time applications using lightweight diffusion models without violating the theoretical upper bound on detection discrepancies? The authors note in Appendix F that the reliance on diffusion models introduces computational overhead, but the minimum model capacity required to maintain theoretical guarantees while enabling real-time detection is unknown.

## Limitations

**Strong theoretical assumptions:** The method relies on continuity equation and incompressible flow assumptions that may not hold for all video types, particularly those with discontinuous motion or rapid scene changes.

**Diffusion model dependency:** The method's effectiveness critically depends on the quality of pre-trained diffusion models, and domain shifts or limited training data may degrade the reliability of estimated NSG features.

**Computational overhead:** The reliance on diffusion models introduces significant computational overhead, making it less suitable for large-scale real-time detection tasks without model compression.

## Confidence

**High confidence:** The method's superior performance metrics (16.00% Recall improvement, 10.75% F1-Score improvement) over baselines are well-supported by extensive experiments across 10 different generators. The ablation studies in Table 4 and threshold stability analysis in Figure 3 provide strong empirical validation.

**Medium confidence:** The theoretical derivation connecting NSG to probability flow conservation is mathematically sound, but its practical applicability to complex, real-world videos with varying content types remains to be fully validated. The upper bound in Theorem 1 provides theoretical justification but may not capture all failure modes.

**Low confidence:** The paper's claims about zero-shot generalization to unseen generators like Sora are based on a single dataset evaluation. While promising, broader testing across more diverse, closed-source generators would strengthen this claim.

## Next Checks

1. **Domain Shift Evaluation:** Test the method's performance when the pre-trained diffusion model is trained on a domain (e.g., synthetic renders) significantly different from the test videos (e.g., real-world footage). Measure degradation in detection accuracy to quantify domain transfer limitations.

2. **Temporal Dynamics Stress Test:** Create test sets with controlled temporal discontinuities (rapid cuts, explosive motion) and measure NSG stability. Compare detection performance between smoothly varying and discontinuous motion content to validate the continuity equation assumptions.

3. **Reference Set Size Sensitivity:** Systematically vary the reference set size (e.g., 10, 50, 100, 200) and measure MMD stability and detection accuracy. Identify the minimum viable reference set size that maintains performance within 5% of the optimal configuration.