---
ver: rpa2
title: 'Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes
  via Language Modeling of Sensor Data & Activities'
arxiv_id: '2507.21964'
source_url: https://arxiv.org/abs/2507.21964
tags:
- activity
- sensor
- zero-shot
- data
- smart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot human activity recognition (HAR)
  method for smart homes that avoids reliance on external Large Language Models (LLMs).
  Instead, it uses language modeling of sensor data and activity labels by generating
  textual summaries and descriptions, then leveraging embeddings and similarity metrics
  for classification.
---

# Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities

## Quick Facts
- arXiv ID: 2507.21964
- Source URL: https://arxiv.org/abs/2507.21964
- Authors: Sourish Gunesh Dhekane; Thomas Ploetz
- Reference count: 20
- Primary result: Zero-shot HAR method achieves comparable performance to LLM-based approaches without external prompting

## Executive Summary
This paper introduces a novel zero-shot human activity recognition (HAR) approach for smart homes that eliminates the need for external Large Language Models (LLMs). The method leverages language modeling of sensor data and activity labels by generating textual summaries and descriptions, then using embeddings and similarity metrics for classification. Evaluated across six diverse datasets, the approach demonstrates strong performance without requiring model prompting or external services.

The work addresses a critical limitation in current HAR systems that rely heavily on LLMs for activity recognition. By using pre-trained sentence encoders and carefully constructed textual representations of sensor data, the method achieves state-of-the-art performance while maintaining computational efficiency and data privacy. The approach also offers flexibility through optional few-shot extensions when labeled data becomes available.

## Method Summary
The proposed method generates textual summaries from sensor data streams and activity labels, then uses pre-trained sentence encoders to create embeddings in a shared semantic space. Classification is performed by computing similarity metrics between sensor data embeddings and activity label embeddings. The system operates entirely within the local environment without external LLM calls, using only the pre-trained sentence encoder weights. For scenarios where some labeled data is available, the method can be extended with few-shot learning techniques to further improve accuracy.

## Key Results
- Achieves comparable performance to state-of-the-art LLM-based approaches without external prompting
- Demonstrates strong generalization across six diverse smart home datasets
- Maintains computational efficiency by avoiding external API calls and LLM dependencies
- Shows optional few-shot extension capability for improved accuracy when labeled data is available

## Why This Works (Mechanism)
The method works by creating a shared semantic space where both sensor data representations and activity labels can be meaningfully compared. By generating descriptive text summaries of sensor patterns and using pre-trained sentence encoders, the system leverages the semantic understanding captured in these models to bridge the gap between raw sensor data and human-interpretable activity labels. The similarity-based classification approach allows for flexible recognition of activities without requiring explicit training on every possible activity type.

## Foundational Learning

**Sensor Data Embeddings** - Why needed: To convert time-series sensor data into a format comparable with textual descriptions. Quick check: Verify embedding dimensionality matches across sensor and text representations.

**Sentence Encoder Models** - Why needed: Pre-trained models provide semantic understanding without requiring task-specific training. Quick check: Confirm encoder supports the vocabulary needed for sensor descriptions.

**Textual Summary Generation** - Why needed: Creates human-interpretable descriptions that bridge sensor data and activity labels. Quick check: Ensure summaries capture essential temporal and spatial patterns.

## Architecture Onboarding

**Component Map:** Sensor Data Streams -> Textual Summary Generator -> Sentence Encoder -> Embedding Space -> Similarity Computation -> Activity Classification

**Critical Path:** The core inference pipeline follows: sensor data acquisition → summary generation → embedding computation → similarity matching → classification output. This path must maintain low latency for real-time applications.

**Design Tradeoffs:** The method trades potential accuracy gains from fine-tuned LLMs against computational efficiency, privacy, and independence from external services. Using pre-trained sentence encoders limits customization but provides strong generalization.

**Failure Signatures:** Performance degradation may occur when sensor data patterns don't map well to textual descriptions, when activities have similar sensor signatures, or when the pre-trained encoder lacks relevant semantic knowledge. Ambiguity in sensor-to-text mapping can lead to misclassification.

**First Experiments:**
1. Test embedding similarity between known sensor patterns and their corresponding activity labels
2. Evaluate classification accuracy with synthetic sensor data and ground-truth activity descriptions
3. Measure inference latency across different sentence encoder model sizes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Assumes meaningful alignment between textual descriptions and sensor data embeddings, which may not hold for all activity types
- Relies on pre-trained sentence encoders that weren't specifically trained on sensor data representations
- Six dataset evaluation may not capture full real-world variability including sensor failures and environmental noise
- Zero-shot performance may be suboptimal compared to few-shot or fully supervised approaches

## Confidence
- High confidence in technical implementation and evaluation methodology
- Medium confidence in generalizability claims across diverse smart home environments
- Medium confidence in performance comparability to LLM approaches without detailed benchmarking
- Low confidence in robustness to sensor noise and real-world deployment challenges

## Next Checks
1. Conduct cross-dataset validation where models trained on one dataset are evaluated on entirely different datasets to assess true zero-shot generalization capabilities
2. Perform extensive ablation studies removing different components (textual summaries, sensor data descriptions, embedding methods) to quantify their individual contributions to performance
3. Test the method under simulated sensor noise conditions and sensor failure scenarios to evaluate robustness in real-world deployment settings