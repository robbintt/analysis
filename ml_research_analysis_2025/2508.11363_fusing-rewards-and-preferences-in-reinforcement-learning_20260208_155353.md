---
ver: rpa2
title: Fusing Rewards and Preferences in Reinforcement Learning
arxiv_id: '2508.11363'
source_url: https://arxiv.org/abs/2508.11363
tags:
- preference
- policy
- reward
- learning
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning reinforcement learning
  agents with human preferences without requiring a separate reward model. It introduces
  Dual-Feedback Actor (DFA), which unifies scalar rewards and pairwise preferences
  in a single policy update rule using the policy's log-probabilities to model preference
  probabilities directly.
---

# Fusing Rewards and Preferences in Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.11363
- Source URL: https://arxiv.org/abs/2508.11363
- Reference count: 25
- One-line primary result: DFA matches or exceeds SAC on 6 MuJoCo tasks while showing more stable training and outperforms reward-modeling baselines in stochastic GridWorld using semi-synthetic human feedback.

## Executive Summary
This paper introduces Dual-Feedback Actor (DFA), a method that unifies scalar rewards and pairwise preferences in a single policy update rule for reinforcement learning. Unlike classical RLHF, DFA uses the policy's log-probabilities directly to model preference probabilities, avoiding a separate reward-modeling step. The method can synthesize preferences from Q-values and works both on-policy and off-policy for sample efficiency. Under a Bradley-Terry model, minimizing DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC) policy. Empirically, DFA matches or exceeds SAC on six control tasks while showing more stable training, and outperforms reward-modeling baselines in a stochastic GridWorld environment using semi-synthetic human feedback.

## Method Summary
DFA defines preference probability as P_θ(a+ ≻ a⁻ | s) = π_θ(a+ | s)^α / [π_θ(a+ | s)^α + π_θ(a⁻ | s)^α], using the negative log-likelihood of this probability as a loss that directly increases the likelihood of preferred actions. For synthetic preferences, DFA samples states from a replay buffer, finds nearest neighbors, and compares Q-values to label preferred actions. The policy is updated by minimizing this preference loss, while Q-networks are updated via TD learning if rewards are available. The temperature parameter α controls exploration-exploitation balance, with optimal values typically in the 0.001-0.6 range depending on the environment.

## Key Results
- DFA matches or exceeds SAC performance on all six MuJoCo control tasks tested
- DFA shows more stable training with lower variance in learning curves compared to SAC
- In stochastic GridWorld with semi-synthetic human feedback, DFA outperforms reward-modeling baselines
- The method successfully unifies rewards and preferences without requiring a separate reward model

## Why This Works (Mechanism)

### Mechanism 1: Direct Preference Modeling via Policy Log-Probabilities
DFA eliminates the need for a separate reward model by using policy log-probabilities directly to model preference probability. The preference probability P_θ(a+ ≻ a⁻ | s) = π_θ(a+ | s)^α / [π_θ(a+ | s)^α + π_θ(a⁻ | s)^α] creates a loss that directly increases the likelihood of preferred actions. This works when the policy's probability assignments can capture preference relationships without an intermediate reward function. The mechanism fails if the policy's softmax outputs cannot express meaningful preference orderings (e.g., identical action probabilities across states).

### Mechanism 2: Entropy-Preserving Exploration via Bradley-Terry Preference Model
Minimizing DFA's preference loss under a Bradley-Terry model recovers the entropy-regularized SAC solution. The preference loss is minimized at a Gibbs distribution π*(a|s) ∝ exp((β/α) Q*(s,a)), which coincides with SAC's optimal policy when λ = α/β, preserving entropy-driven exploration. This mechanism relies on the assumption that human or synthetic preferences follow a Bradley-Terry model with the soft-optimal Q-function as the latent utility. The theoretical equivalence may not hold if preferences deviate significantly from Bradley-Terry structure (e.g., intransitive preferences).

### Mechanism 3: Preference Synthesis from Q-Values for Unified Learning
DFA synthesizes preferences from Q-values stored in a replay buffer by comparing Q(s_i, a_i) with Q(s_i, a'_i) from a nearby state. The action with higher Q-value becomes the preferred action, creating synthetic preference pairs for training. This mechanism assumes Q-value differences reliably proxy preference relationships and that early Q-estimates are sufficiently accurate to guide learning. If early Q-estimates are highly inaccurate, synthesized preferences introduce noise that destabilizes training.

## Foundational Learning

- **Entropy-Regularized Reinforcement Learning (SAC):**
  - Why needed here: DFA's theoretical foundation depends on understanding how entropy regularization affects policy optimization and connects to preference learning.
  - Quick check question: Why does adding entropy H(π(·|s)) to the RL objective encourage exploration, and how does the temperature parameter λ control this?

- **Bradley-Terry Model for Preferences:**
  - Why needed here: The mathematical proof connecting DFA's preference loss to SAC policy relies on the Bradley-Terry assumption for preference generation.
  - Quick check question: What does the Bradley-Terry model assume about the relationship between preference probabilities and latent utility values?

- **Off-Policy Learning and Replay Buffers:**
  - Why needed here: DFA synthesizes preferences from Q-values in replay buffers and operates off-policy for sample efficiency.
  - Quick check question: How does off-policy learning with a replay buffer differ from on-policy learning in terms of data efficiency and value estimation stability?

## Architecture Onboarding

- **Component map:**
  Policy Network (π_θ) -> Preference Loss Module -> Policy Update
  Q-Network(s) -> Preference Synthesis Module -> Preference Dataset
  Replay Buffer -> Preference Synthesis Module -> Preference Dataset
  Preference Dataset -> Preference Loss Module -> Policy Update

- **Critical path:**
  1. Collect transitions and store in replay buffer
  2. Generate preferences (human annotations or synthesize from Q-values)
  3. Compute preference loss using policy log-probabilities
  4. Update policy by minimizing preference loss
  5. If rewards available, update Q-networks via TD learning

- **Design tradeoffs:**
  - α (temperature): Controls exploration-exploitation balance. Too high (α=1.0) → near-zero learning signal; too low (α=10⁻⁸) → overly stochastic policy. Empirically optimal range: 0.001-0.6.
  - State-wise vs. trajectory-level preferences: State-wise easier to collect; trajectory-level captures temporal structure.
  - Off-policy vs. on-policy: Off-policy enables replay buffer reuse; on-policy more stable but less sample-efficient.

- **Failure signatures:**
  - Mode collapse: Policy becomes near-deterministic if α too high (similar to DPO issues)
  - Overly stochastic policy: Near-uniform action probabilities if α too low
  - Noisy synthetic preferences: Early inaccurate Q-estimates propagate errors
  - Computational overhead: Nearest-state lookup in replay buffer can be expensive for large buffers

- **First 3 experiments:**
  1. Replicate SAC baseline on MuJoCo (e.g., Walker2d) using Table 2 hyperparameters to establish reference performance.
  2. Implement DFA preference loss only on GridWorld with synthetic preferences from true Q-values to verify Theorem 5.2 connection to SAC.
  3. Ablate α parameter: Run DFA with α ∈ {0.001, 0.01, 0.1, 0.5, 1.0} and observe convergence behavior (should replicate Figure 3b pattern where moderate values work best).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DFA perform when trained on actual, noisy human feedback in complex, high-dimensional environments compared to the semi-synthetic or simulated preferences used in the paper?
- Basis in paper: [explicit] The conclusion states, "The future work can be... evaluating DFA on larger, real human-in-the-loop tasks."
- Why unresolved: The experiments rely either on synthetic preferences derived from Q-values (MuJoCo) or semi-synthetic feedback generated via a Bradley-Terry model (GridWorld). Real human feedback introduces inconsistencies and noise patterns not fully captured by the semi-synthetic model.
- What evidence would resolve it: Empirical results from training a DFA agent on a robotics or language task using a dataset of actual human annotator preferences, analyzing robustness to human label noise.

### Open Question 2
- Question: Can the theoretical guarantees and empirical stability of DFA be maintained if the human preference model deviates from the Bradley-Terry assumption?
- Basis in paper: [explicit] The conclusion identifies "investigating other assumptions" as a specific avenue for future work, noting the Bradley-Terry assumption as a limitation.
- Why unresolved: Theoretical results (Theorem 5.2) rely on Assumption 5.1 (Bradley-Terry model on the soft-optimal Q-function). Real human preferences may exhibit intransitivity or biases not captured by this model.
- What evidence would resolve it: A theoretical analysis of DFA’s error bounds under alternative preference models (e.g., Plackett-Luce) or empirical tests where the feedback generation process violates the Bradley-Terry assumptions.

### Open Question 3
- Question: How can the computational cost of synthesizing preferences be reduced for high-dimensional state spaces where finding the "nearest state" in the replay buffer is expensive?
- Basis in paper: [explicit] The conclusion lists "the computational cost of finding the nearest state in the replay buffer" as a main limitation.
- Why unresolved: The method requires finding a nearest neighbor in the replay buffer to generate the rejected action $a'$. As the buffer grows or state dimensionality increases, this search becomes a computational bottleneck.
- What evidence would resolve it: The proposal and validation of an approximate nearest neighbor search method or a different sampling strategy that maintains performance while reducing the time complexity of the preference synthesis step.

## Limitations
- The method relies heavily on the Bradley-Terry assumption for preference modeling, which may not hold for real human preferences
- Computational cost of nearest-neighbor search in replay buffer scales poorly with buffer size and state dimensionality
- Performance is highly sensitive to the temperature parameter α, requiring careful tuning
- The synthetic preference synthesis mechanism's effectiveness in noisy, real-world settings remains unverified

## Confidence
- High confidence: Theoretical connection between DFA preference loss and SAC policy (Theorem 5.2)
- Medium confidence: Empirical performance claims on MuJoCo tasks (6/6 matches or exceeds SAC)
- Medium confidence: Stability improvements in training (lower variance observed)
- Low confidence: Synthetic preference synthesis mechanism generalization beyond controlled settings

## Next Checks
1. **Ablation study on α temperature:** Run DFA across the full α range (0.001, 0.01, 0.1, 0.5, 1.0) on Walker2d with 5 seeds each, measuring both convergence speed and final performance to map the critical sensitivity region.
2. **Preference synthesis robustness test:** Implement DFA with synthetic preferences on a simple tabular environment (e.g., GridWorld) where ground-truth Q-values are known, then systematically corrupt early Q-estimates to measure propagation of errors through preference synthesis.
3. **Nearest-neighbor search efficiency validation:** Compare exact nearest-neighbor (brute-force) vs. approximate methods (FAISS) on the 20,000-buffer size, measuring both computational overhead and any performance degradation from approximation errors.