---
ver: rpa2
title: 'Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in
  Trace-Based Knowledge Distillation'
arxiv_id: '2505.13792'
source_url: https://arxiv.org/abs/2505.13792
tags:
- traces
- intermediate
- final
- correct
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the disconnect between intermediate reasoning
  traces and final solution correctness in knowledge distillation for small language
  models. The authors propose a rule-based problem decomposition method for Open Book
  QA, breaking problems into classification and information retrieval steps to generate
  verifiable intermediate traces.
---

# Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation

## Quick Facts
- arXiv ID: 2505.13792
- Source URL: https://arxiv.org/abs/2505.13792
- Reference count: 31
- Primary result: Correct intermediate reasoning traces do not guarantee correct final solutions in knowledge distillation for small language models

## Executive Summary
This paper investigates knowledge distillation (KD) for small language models (SLMs) using interpretable intermediate reasoning traces. The authors propose a rule-based problem decomposition approach for Open Book QA tasks, breaking problems into classification and information retrieval steps to generate verifiable intermediate traces. Through supervised fine-tuning experiments on Llama-3.2-1B-Instruct and Qwen3-1.7B models across three datasets, they discover a surprising disconnect: correct intermediate traces do not reliably predict correct final solutions, and vice versa. This challenges the assumption that intermediate reasoning traces serve as reliable scaffolding for knowledge distillation in SLMs.

## Method Summary
The authors employ rule-based problem decomposition to break Open Book QA into Classification and Information Retrieval steps, generating verifiable intermediate traces. They construct SFT datasets with Input-Trace-Output tuples using correct and incorrect traces, then fine-tune Llama-3.2-1B-Instruct and Qwen3-1.7B models with QLoRA. The method enables automatic trace evaluation at inference time and tests whether trace correctness correlates with final solution accuracy across three datasets (CoTemp QA, Microsoft MARCO QA, Facebook bAbI QA).

## Key Results
- Correct intermediate traces do not guarantee correct final solutions (e.g., 52.88% final accuracy despite 47.06% classification accuracy on CoTemp QA with correct traces)
- Low correlation between trace correctness and solution correctness observed across all datasets and models
- SFT with incorrect traces sometimes outperforms correct traces on final accuracy, though with less interpretable reasoning
- Models frequently produce correct answers with incorrect traces (high False Positive rates in confusion matrices)

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Problem Decomposition for Trace Verifiability
Decomposing Open Book QA into Classification + Information Retrieval steps produces objectively evaluable intermediate traces. Given a query and context passage, the system first identifies the question type via classification, then retrieves the relevant supporting fact. Both sub-task outputs have ground-truth labels, enabling automatic trace evaluation at inference time. The decomposition captures meaningful reasoning sub-structure while remaining evaluable.

### Mechanism 2: Trace-Augmented Supervised Fine-Tuning for Knowledge Distillation
SFT datasets are constructed with structured traces (classification label + retrieved fact) paired with correct final answers. Models are fine-tuned to predict the full trace-output sequence, learning to emit verifiable steps before answering. The assumption is that the model learns to condition its final answer on the emitted trace, using the trace as a reasoning scaffold.

### Mechanism 3: Trace-Solution Decoupling (Observed Phenomenon)
Correct intermediate traces do not reliably predict correct final solutions because SLMs may not causally depend on emitted traces to generate answers. The trace may be a post-hoc rationalization or a learned pattern unconnected to the answer generation pathway. Empirical evidence shows high False Positive rates and error analysis revealing most correct traces are followed by incorrect solutions.

## Foundational Learning

- **Knowledge Distillation (KD) for SLMs**: KD transfers reasoning capabilities from structured traces to SLMs; understanding KD distinctions clarifies why Input-Trace-Output tuples are used rather than teacher model outputs. *Quick check: Can you explain why black-box KD (using only outputs) is more applicable than white-box KD when distilling from rule-generated traces rather than a teacher model?*

- **Chain-of-Thought (CoT) and Reasoning Traces**: The paper positions itself against CoT/R1 traces, which are verbose and hard to evaluate; understanding CoT's promise and limitations motivates the structured trace approach. *Quick check: What makes CoT traces difficult to evaluate automatically, and how does rule-based decomposition address this?*

- **Open Book QA vs. Closed Book QA**: The decomposition (Classification + IR) is specific to Open Book QA, where external context is provided; the task structure determines what sub-problems are decomposable. *Quick check: Why would the Classification + IR decomposition be less applicable to Closed Book QA tasks?*

## Architecture Onboarding

- **Component map**: Raw QA dataset -> Rule-based decomposition -> (Classification label, Retrieved fact) -> Format as Input-Trace-Output tuple -> SFT with QLoRA -> Model generates trace and answer

- **Critical path**: 
  1. Implement decomposition rules per dataset (CoTemp QA: temporal relations; MARCO: question categories; bAbI: reasoning types)
  2. Generate SFT datasets with correct and incorrect trace variants
  3. Run SFT with QLoRA, monitoring both trace and solution metrics
  4. At inference, parse model output to extract classification, IR, and final answer for evaluation

- **Design tradeoffs**:
  - Trace granularity: Fininer decomposition increases verifiability but raises sequence length and annotation cost
  - Correct vs. incorrect traces for training: Incorrect traces yield comparable final accuracy but produce unreliable explanations
  - Model size: 1B-1.7B models may lack capacity to reliably condition on traces

- **Failure signatures**:
  - High final solution accuracy with low trace accuracy → model ignores its own traces (decoupling)
  - Correct trace followed by incorrect answer → trace not causally used
  - Truncated traces at inference → sequence length insufficient

- **First 3 experiments**:
  1. Replicate SFT w/ Correct Traces on CoTemp QA with Llama-3.2-1B-Instruct; verify Classification Step Accuracy (~39-47%) and final solution accuracy (~40-53%) match paper baseline.
  2. Ablate trace format: Train on Input-Output only (SFT-Vanilla) and compare final solution accuracy to trace-augmented variants.
  3. Stress-test decoupling: On held-out examples where the model produces correct final answers, manually inspect whether retrieved facts are plausibly supportive.

## Open Questions the Paper Calls Out

### Open Question 1
Does mixing correct and incorrect intermediate traces in varying proportions affect the correlation between trace correctness and final solution accuracy? The authors only tested extreme conditions (100% correct vs. 100% incorrect traces), not realistic mixed settings. Evidence would require SFT experiments with controlled ratios (e.g., 25%, 50%, 75% correct) measuring both trace and solution accuracy.

### Open Question 2
Do alternative problem decomposition techniques at different granularities show stronger trace-solution correlations than the two-step classification+IR approach? The paper only tested one decomposition method; finer or coarser decompositions might yield different correlation patterns. Evidence would require comparative experiments using alternative decomposition schemes on the same datasets.

### Open Question 3
Does the trace-solution disconnect generalize to other reasoning domains beyond Open Book QA? The study focuses exclusively on Open Book QA with a specific decomposition; whether findings apply to mathematical reasoning, planning, or other task types remains unknown. Evidence would require replicating the methodology on diverse reasoning benchmarks with domain-appropriate decompositions.

## Limitations
- The paper documents trace-answer decoupling but doesn't establish whether this reflects true causal independence or model capacity limitations
- Results are limited to 1B-1.7B models; scaling to larger models may alter the observed decoupling pattern
- The approach relies on task-specific decomposition rules that may not generalize to more complex reasoning tasks

## Confidence

- **High Confidence**: The empirical observation that correct traces do not guarantee correct final answers is well-supported by confusion matrices and accuracy metrics across all three datasets
- **Medium Confidence**: The claim that incorrect traces can yield comparable final accuracy is supported but requires careful interpretation about whether models "guess correctly despite wrong reasoning" versus "learn to ignore traces"
- **Low Confidence**: The mechanism explanation for why decoupling occurs is speculative; the paper documents the phenomenon but does not establish causal mechanisms

## Next Checks

1. **Causal Intervention Test**: Design an experiment where you manually correct model-generated traces at inference time and measure whether this improves final answer accuracy, testing whether traces causally influence answers or merely accompany them.

2. **Capacity-Aware Scaling Study**: Repeat the SFT experiments with progressively larger models (2B, 7B, 13B) to determine whether the trace-answer decoupling weakens or strengthens with model capacity, indicating whether it's a fundamental limitation or artifact of model size.

3. **Process Supervision Ablation**: Implement a simple process reward model that gives bonus rewards for correct traces during SFT (or fine-tune with DPO on trace correctness) and measure whether this enforces stronger trace-answer correlation, testing whether the decoupling is training-method dependent.