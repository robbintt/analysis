---
ver: rpa2
title: 'ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters'
arxiv_id: '2510.18431'
source_url: https://arxiv.org/abs/2510.18431
tags:
- scalenet
- layers
- training
- pretrained
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScaleNet introduces a parameter-efficient method for expanding
  pretrained vision transformers by inserting additional layers that share weights
  with corresponding layers in the original model. To preserve model capacity and
  avoid performance degradation, lightweight parallel adapter modules are introduced,
  providing layer-specific adjustments for each shared weight instance.
---

# ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters

## Quick Facts
- arXiv ID: 2510.18431
- Source URL: https://arxiv.org/abs/2510.18431
- Authors: Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang
- Reference count: 40
- Primary result: Achieves 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs when scaling DeiT-Base to 2× depth

## Executive Summary
ScaleNet introduces a parameter-efficient method for expanding pretrained vision transformers by inserting additional layers that share weights with corresponding layers in the original model. To preserve model capacity and avoid performance degradation, lightweight parallel adapter modules are introduced, providing layer-specific adjustments for each shared weight instance. Experiments on ImageNet-1K demonstrate that a 2× depth-scaled DeiT-Base model using ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs. The method also shows strong generalization to downstream tasks including object detection, semantic segmentation, and language modeling, with consistent performance gains and minimal parameter overhead.

## Method Summary
ScaleNet scales up pretrained vision transformers by interleaving additional layers with shared weights from the original model, preventing capacity collapse through parallel adapter modules that provide layer-specific adjustments. The method uses a cyclic mapping function for isotropic architectures (like DeiT) where all layers can share weights, and stage-wise mapping for hierarchical architectures (like Swin) where sharing is restricted to within stages due to resolution differences. Adapters are lightweight non-linear modules attached to all linear layers, allowing the model to differentiate the functional output of shared-weight layers while maintaining parameter efficiency.

## Key Results
- 2× depth-scaled DeiT-Base achieves 82.53% accuracy versus 75.11% for training from scratch, with training time reduced from 89.5 to 19.8 hours
- ScaleNet generalizes effectively to downstream tasks: object detection (+1.6% AP), semantic segmentation (+0.5% mIoU), and language modeling (2.7% perplexity reduction)
- Parameter efficiency is demonstrated with only 50% unique parameters compared to standard expansion, while FLOPs still double due to increased depth
- Ablation studies show adapters are critical for performance, with parallel adapters outperforming LoRA variants at similar parameter counts

## Why This Works (Mechanism)

### Mechanism 1: Optimization Landscape Shrinkage via Weight Sharing
ScaleNet reduces the number of unique trainable parameters by mapping new layers directly to existing parameter tensors in the pretrained model, effectively shrinking the optimization landscape. This allows the model to leverage existing basins of attraction in the parameter space rather than exploring a massive new parameter space from scratch.

### Mechanism 2: Differentiation via Parallel Adapters
Lightweight parallel adapter modules prevent model collapse by providing non-linear, layer-specific modifications to the shared weight output. While weights are shared, adapters introduce adjustment parameters that ensure the functional output of layer i differs from layer j.

### Mechanism 3: Gradient Flow Stabilization
The inclusion of adjustment modules stabilizes gradient flow, mitigating the gradient instability often associated with recurrent or weight-shared architectures. Adapters act as residual pathways that modify the local gradient landscape, ensuring stable backward passes through identical transforms.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT)**: Understanding the difference between reparameterization (LoRA) and parallel insertion (Adapter) is critical for ScaleNet's adapter implementation. *Quick check: Can you explain why a parallel adapter introduces distinct inference overhead compared to a LoRA module that can be merged into weights?*

- **Isotropic vs. Hierarchical Vision Transformers**: The effectiveness of layer mapping depends on architecture type. Isotropic models (DeiT) can share all layers, while hierarchical models (Swin) must restrict sharing to within stages due to resolution differences. *Quick check: Why would sharing a weight tensor trained on 56x56 feature maps be detrimental for a layer processing 14x14 feature maps?*

- **Progressive Training (Net2Net)**: ScaleNet evolves progressive training concepts. Understanding baseline "Stack" and "Interpolation" methods is necessary to interpret ablation results. *Quick check: How does ScaleNet differ from Net2Net regarding the independence of new layers during the training phase?*

## Architecture Onboarding

- **Component map**: Image patches + Positional Embeddings -> Base Blocks (Pretrained) -> Scaled Blocks (Inserted, weight-sharing) -> Adjustment Modules (Parallel Adapters) -> Head (Classification/Language)

- **Critical path**:
  1. Select Layer Mapping: Define $g(l')$ (Cyclic for DeiT, Stage-local for Swin)
  2. Construct Scaled Model: Insert new blocks and tie weights ($\theta$) to source blocks
  3. Inject Adapters: Add parallel branches to all linear layers in the MLPs of both original and new blocks
  4. Freeze Strategy: Freeze shared weights ($\theta$). Train only Adapters ($\Delta \theta$) and Layer Norms
  5. Optimization: Train with high Drop Path ratio (0.5 for Base, 0.2 for Small)

- **Design tradeoffs**:
  - LoRA vs. Parallel Adapter: LoRA is parameter-efficient and mergeable, but Parallel Adapters with non-linearity provide better representation capacity
  - Parameter Efficiency vs. FLOPs: Drastically reduces unique parameter storage (~1/2) but FLOPs still double due to depth
  - Drop Path Ratio: Scaling depth increases overfitting risk, requiring significantly larger drop path ratios than standard training

- **Failure signatures**:
  - Architecture Mismatch: Sharing weights across different stages in hierarchical models causes performance degradation
  - Gradient Instability: Removing adapters in deep shared networks causes gradient norms to spike and destabilize

- **First 3 experiments**:
  1. Implement weight-sharing loop on DeiT-Tiny, compare "Shared Weights only" vs. "Shared Weights + Adapters"
  2. Sweep adapter intermediate dimensions (4, 8, 16, 32) on DeiT-Base to locate redundancy point
  3. Train 2× scaled DeiT-Small from scratch vs. ScaleNet, plot Accuracy vs. Training Hours

## Open Questions the Paper Calls Out
- Exploring dynamic weight-sharing mechanisms that adapt to varying input complexities
- Extending the framework to hybrid architectures combining ViTs with convolutional networks
- Understanding why performance degrades when layer reuse exceeds 2× and whether increasing adapter capacity can overcome this limit

## Limitations
- Effectiveness relies heavily on pretrained weights containing transferable knowledge for multiple functional stages
- Theoretical understanding of why non-linear adapters outperform linear alternatives remains limited
- Paper doesn't extensively explore scenarios where pretrained models are significantly out-of-distribution from target tasks

## Confidence
- **High Confidence**: Empirical efficiency claims (training time reduction from 89.5 to 19.8 hours while improving accuracy) are well-supported by controlled experiments
- **Medium Confidence**: Gradient stabilization mechanism demonstrated through gradient norm analysis, but alternative initialization strategies not explored
- **Medium Confidence**: Architectural constraints empirically validated but lack theoretical justification for optimality

## Next Checks
1. Evaluate ScaleNet when scaling models pretrained on ImageNet to tasks with significantly different data distributions (medical imaging, satellite imagery)
2. Systematically measure contribution of each adapter layer by removing them individually to determine position-specific requirements
3. Compare ScaleNet against a variant where new layers are initialized with same weights but trained independently to isolate gains from weight sharing vs. constrained optimization