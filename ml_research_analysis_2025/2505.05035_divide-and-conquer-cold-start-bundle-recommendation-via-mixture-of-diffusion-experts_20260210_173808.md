---
ver: rpa2
title: 'Divide-and-Conquer: Cold-Start Bundle Recommendation via Mixture of Diffusion
  Experts'
arxiv_id: '2505.05035'
source_url: https://arxiv.org/abs/2505.05035
tags:
- bundle
- cold-start
- recommendation
- diffusion
- bundles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cold-start problem in bundle recommendation,
  where new bundles lack sufficient information for effective recommendation. The
  authors propose MoDiffE, a framework that employs a divide-and-conquer strategy
  using diffusion models and a Mixture of Experts (MoE) architecture.
---

# Divide-and-Conquer: Cold-Start Bundle Recommendation via Mixture of Diffusion Experts

## Quick Facts
- arXiv ID: 2505.05035
- Source URL: https://arxiv.org/abs/2505.05035
- Authors: Ming Li; Lin Li; Xiaohui Tao; Dong Zhang; Jimmy Xiangji Huang
- Reference count: 40
- Primary result: MoDiffE achieves up to 0.1027 absolute gain in Recall@20 for cold-start bundle recommendation

## Executive Summary
This paper addresses the cold-start problem in bundle recommendation, where new bundles lack sufficient user interaction data for effective recommendation. The authors propose MoDiffE, a divide-and-conquer framework that leverages diffusion models and a Mixture of Experts (MoE) architecture to generate high-quality representations for cold bundles without requiring interaction history. The approach divides the cold-start problem into sub-problems by level (bundle and item) and view (embedding and diffusion), solves them uniformly using diffusion models, and combines results with a cold-aware hierarchical MoE. Extensive experiments on three real-world datasets demonstrate that MoDiffE significantly outperforms existing methods, achieving up to a 0.1027 absolute gain in Recall@20 in cold-start scenarios and up to a 47.43% relative improvement in all-bundle scenarios.

## Method Summary
MoDiffE employs a multi-stage decoupled pipeline: first, a prior-embedding model (LightGCN) generates embedded representations for warm entities; second, conditional DDPMs (diffusion models) are trained to predict original representations from noisy inputs, conditioned on item affiliations; third, hierarchical MoE gating networks (view-layer with Softmax, output-layer with Tanh) are trained using pseudo-cold bundles synthesized via interpolation to handle cold-start routing; finally, during inference, diffusion representations are generated from similarity-based anchors and combined via the trained gating networks. The framework uses a divide-and-conquer strategy, solving bundle cold-start problems at both bundle and item levels, and across embedding and diffusion views, uniformly with diffusion models.

## Key Results
- MoDiffE achieves up to 0.1027 absolute gain in Recall@20 for cold-start bundle recommendation
- Significant performance improvements over existing methods: 47.43% relative improvement in all-bundle scenarios
- The gating augmentation is essential: without it, Recall@20 drops to 0 in cold-start scenarios
- The framework effectively handles dual-level multi-view complexity of bundle cold-start recommendation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diffusion models can generate useful representations for cold bundles/items without requiring their interaction features.
- **Mechanism**: The model learns the distribution of well-represented warm entities through a forward diffusion process (adding noise) and reverse denoising process (removing noise). During inference, it starts from a similarity-based anchor (not random noise) and denoises step-by-step, conditioned on available features (bundle item composition), producing representations that approximate what the entity would have if it had interactions.
- **Core assumption**: The learned distribution of warm representations generalizes meaningfully to cold entities with similar item compositions.
- **Evidence anchors**:
  - [abstract]: "directly generates diffusion representations using diffusion models without depending on specific features"
  - [Section 4.2]: "diffusion models are trained to capture the underlying distribution of well-represented representations through a diffusion-denoise process"
  - [corpus]: Limited corpus evidence; related work "Modeling Item-Level Dynamic Variability with Residual Diffusion for Bundle Recommendation" uses diffusion but for different purposes.
- **Break condition**: If cold bundles have item compositions with no similarity to any warm bundles, the similarity-based anchor will be meaningless and generated representations may be arbitrary.

### Mechanism 2
- **Claim**: Hierarchical MoE with cold-aware gating adaptively routes bundles to appropriate experts based on their cold-start situation.
- **Mechanism**: Two-level gating: (1) View-layer gating uses Softmax to competitively select between embedding and diffusion experts per view, weighted by cold-aware features (interaction counts); (2) Output-layer gating uses Tanh to cooperatively weight predictions across views. Warm entities route primarily to embedding experts; cold entities route primarily to diffusion experts.
- **Core assumption**: Cold-aware features (interaction counts) are reliable indicators of which expert will perform better.
- **Evidence anchors**:
  - [abstract]: "cold-aware hierarchical Mixture of Experts (MoE) is employed to combine results"
  - [Section 4.3]: "Warm bundle #494... leans on embedding expert, whereas cold bundle #1471 relies mainly on diffusion expert"
  - [Section 5.6, Figure 6]: Gating case study shows adaptive routing patterns
  - [corpus]: No direct corpus evidence for this specific gating design.
- **Break condition**: If cold-aware features don't correlate with representation quality (e.g., bundles with few interactions but highly informative item compositions), gating decisions may be suboptimal.

### Mechanism 3
- **Claim**: Interpolation-based pseudo cold bundles enable the gating network to learn cold-start routing without actual cold data.
- **Mechanism**: Sample pairs of cold bundles, interpolate their representations and set cold-aware features to zero via `λ·r_x + (1-λ)·r_y` where λ ~ Beta(0.9, 0.9). This creates training examples that simulate cold-start conditions, teaching gating networks appropriate routing without introducing noise into original bundle representations.
- **Core assumption**: Linear interpolation of representations preserves semantic meaning and creates valid pseudo-cold training examples.
- **Evidence anchors**:
  - [Section 4.4.2]: "interpolation-based constructing pseudo bundles does not introduce a negative impact on the distribution"
  - [Table 4]: w/o aug achieves 0 Recall@20 in cold-start scenarios, confirming augmentation is necessary
  - [corpus]: No corpus evidence for this specific augmentation approach.
- **Break condition**: If the interpolation ratio η is too high, training time increases significantly with diminishing returns; if too low (or zero), gating cannot handle cold bundles at inference.

## Foundational Learning

- **Concept: Diffusion Models (DDPM)**
  - **Why needed here**: The core representation generation mechanism requires understanding forward diffusion (adding Gaussian noise gradually) and reverse denoising (neural network predicts original data from noisy input).
  - **Quick check question**: Can you explain why diffusion models use a Markov chain with T steps rather than adding noise in one step?

- **Concept: Mixture of Experts (MoE) with Gating Networks**
  - **Why needed here**: The combination strategy relies on understanding how gating networks assign weights to expert outputs and why Softmax vs. Tanh matter for competitive vs. cooperative fusion.
  - **Quick check question**: Why would you use Softmax for view-layer gating (competitive) but Tanh for output-layer gating (cooperative)?

- **Concept: Cold-Start Problem in Collaborative Filtering**
  - **Why needed here**: The fundamental motivation depends on understanding why traditional embedding-based methods fail when entities lack interaction history.
  - **Quick check question**: Why can't standard collaborative filtering methods (e.g., matrix factorization) handle items with zero interactions?

## Architecture Onboarding

- **Component map**: Prior-Embedding Model (LightGCN) -> Diffusion Models (DDPM, per view) -> View-Layer Gating Networks (Softmax) -> Output-Layer Gating Network (Tanh) -> Cold-Start Gating Augmentation (interpolation)

- **Critical path**:
  1. Train prior-embedding model on original data (BPR loss)
  2. Train diffusion models on warm embedded representations (L2 loss)
  3. Generate pseudo-cold bundles via interpolation
  4. Train gating networks on augmented data (BPR loss)
  5. Inference: Generate diffusion representations -> Gate -> Predict

- **Design tradeoffs**:
  - T (training steps) vs. T' (inference steps): More steps improve quality but slow inference (T=500, T'=20 works well)
  - η (augmentation ratio): 0.3-0.5 for cold-start, 0 for warm-start scenarios
  - Top-n for similarity sampling: n=5-10 balances anchor quality vs. smoothness
  - Softmax vs. Tanh activation: Softmax for competitive (within-view), Tanh for cooperative (cross-view)

- **Failure signatures**:
  - Zero recall on cold-start scenario -> Gating augmentation disabled or η=0
  - Performance worse than backbone -> MoE removed (simple averaging doesn't work)
  - Slow inference -> T' too large; use fast ODE solver with fewer steps
  - Unstable training -> Diffusion and BPR losses conflict; use decoupled multi-stage training

- **First 3 experiments**:
  1. **Ablation on gating augmentation**: Set η=0 and verify Recall@20 drops to 0 in cold-start scenario (confirms necessity per Table 4).
  2. **Gating activation comparison**: Test Softmax+Softmax vs. Softmax+Tanh on validation set to understand competitive vs. cooperative fusion (Table 5 shows 0.1210 vs. 0.1171 on Youshu cold-start).
  3. **Similarity sampling analysis**: Vary top-n from 1 to 50 and plot Recall@20; verify peak at n=5-10 (Figure 3c) to understand anchor sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can multi-modal diffusion models be integrated into MoDiffE to leverage non-interaction data (e.g., text/images) for more robust representation generation?
- **Basis in paper**: [explicit] Section 6 states the current uni-modal approach "cannot fully utilize the multi-modal information of the bundle" and proposes introducing multi-modal diffusion models as a future direction.
- **Why unresolved**: The current framework architecture and experiments are restricted to interaction views; integrating multi-modal inputs requires redefining the condition mechanisms within the diffusion process to handle heterogeneous data.
- **What evidence would resolve it**: Experiments on datasets with rich multi-modal metadata demonstrating that a multi-modal extension of MoDiffE outperforms the uni-modal baseline in cold-start scenarios.

### Open Question 2
- **Question**: Can the divide-and-conquer strategy be effectively adapted to solve the user cold-start problem alongside the bundle cold-start problem?
- **Basis in paper**: [explicit] Section 6 explicitly lists "integrate the user cold-start problem into the same divide-and-conquer paradigm" as a goal to provide a comprehensive solution.
- **Why unresolved**: The current MoDiffE design specifically targets bundle entities; applying the "level-and-view" division to users requires defining distinct sub-problems and training experts for user representation generation.
- **What evidence would resolve it**: A unified model that applies diffusion experts to user features, demonstrating significant improvements in Recall/NDCG for cold-user scenarios without sacrificing bundle performance.

### Open Question 3
- **Question**: Can the diffusion inference process be optimized to meet the strict latency requirements of real-time online recommendation systems?
- **Basis in paper**: [inferred] The paper acknowledges in Section 4.2.5 that inference "can be time-consuming" and employs a fast ODE solver, but provides no latency benchmarks or trade-off analysis regarding the computational cost of the denoising steps.
- **Why unresolved**: While accuracy gains are shown, the computational overhead of iterative denoising may render the model impractical for high-throughput, low-latency industrial serving environments.
- **What evidence would resolve it**: A detailed latency-performance trade-off analysis, reporting throughput (QPS) and inference time relative to the number of denoising steps (T'), showing viability for real-time use.

## Limitations

- The interpolation-based cold-start augmentation assumes linear interpolation preserves semantic meaning without introducing noise, an assumption with limited empirical validation.
- The gating mechanism's reliance on interaction counts as cold-aware features may not capture all relevant cold-start indicators, particularly for bundles with sparse but informative item compositions.
- The conditional diffusion model's conditioning mechanism on item affiliations is described but not fully detailed, raising questions about its effectiveness across diverse bundle structures.

## Confidence

- **High confidence**: The overall framework architecture and multi-stage training pipeline are well-specified and theoretically sound. The performance improvements over baselines are clearly demonstrated with quantitative metrics.
- **Medium confidence**: The core mechanisms (diffusion representation generation, hierarchical MoE gating) are supported by ablation studies and case studies, but some implementation details remain underspecified.
- **Low confidence**: The interpolation-based augmentation strategy and its assumption about preserving semantic meaning through linear interpolation lack direct empirical validation beyond ablation results.

## Next Checks

1. **Semantic preservation analysis**: Test the interpolation-based augmentation by measuring cosine similarity between interpolated representations and their source bundles to quantify semantic drift.
2. **Gating feature sensitivity**: Evaluate gating performance using alternative cold-aware features (e.g., bundle size, item popularity) to determine if interaction counts are optimal indicators.
3. **Conditioning mechanism validation**: Compare diffusion performance with and without item affiliation conditioning on a held-out test set to measure the actual contribution of conditional information.