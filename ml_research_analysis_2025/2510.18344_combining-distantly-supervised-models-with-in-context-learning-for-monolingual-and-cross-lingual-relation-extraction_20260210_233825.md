---
ver: rpa2
title: Combining Distantly Supervised Models with In Context Learning for Monolingual
  and Cross-Lingual Relation Extraction
arxiv_id: '2510.18344'
source_url: https://arxiv.org/abs/2510.18344
tags:
- head
- tail
- relation
- hydre
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HYDRE is a hybrid framework that combines the high-recall candidate
  relation selection of fine-tuned DSRE models with the reasoning capabilities of
  LLMs via in-context learning. It retrieves semantically aligned exemplars from training
  data to guide LLMs in disambiguating relations at the sentence level.
---

# Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction

## Quick Facts
- **arXiv ID:** 2510.18344
- **Source URL:** https://arxiv.org/abs/2510.18344
- **Reference count:** 40
- **Primary result:** HYDRE achieves up to 20 F1 point gains in English and on average 17 F1 points on four low-resource Indic languages over prior DSRE models.

## Executive Summary
HYDRE is a hybrid framework that combines the high-recall candidate relation selection of fine-tuned DSRE models with the reasoning capabilities of LLMs via in-context learning. It retrieves semantically aligned exemplars from training data to guide LLMs in disambiguating relations at the sentence level. HYDRE achieves up to 20 F1 point gains in English and on average 17 F1 points on four low-resource Indic languages over prior DSRE models.

## Method Summary
HYDRE uses a two-stage pipeline: first, a fine-tuned DSRE model (PARE) identifies the top-k candidate relations for a query. Then, a novel dynamic exemplar retrieval strategy selects semantically aligned training examples to guide an LLM in resolving the correct relation. The approach uses confidence-aggregated retrieval to surface cleaner exemplars and in-context learning to disambiguate fine-grained relations. For cross-lingual settings, HYDRE leverages either Translate-Train or Translate-Test paradigms with appropriate multilingual retrievers.

## Key Results
- HYDRE achieves up to 20 F1 point gains in English over prior DSRE models
- On average 17 F1 points improvement on four low-resource Indic languages
- Ablations show removing exemplar retrieval degrades performance by up to 7 F1 points

## Why This Works (Mechanism)

### Mechanism 1: Recall-Precision Decomposition via Candidate Filtering
HYDRE improves performance by decomposing the extraction task into a high-recall filtering phase (DSRE model) followed by a high-precision disambiguation phase (LLM). A fine-tuned DSRE model retrieves the top-$k$ candidate relations for a query, narrowing the search space for the LLM to focus its reasoning capacity on disambiguating a small subset of likely relations.

### Mechanism 2: Noise-Robust Exemplar Retrieval via Confidence Aggregation
Selecting in-context exemplars based on aggregate model confidence rather than just semantic similarity mitigates noise in distantly supervised training data. HYDRE selects bags where the DSRE model is confident about the relation, then selects representative sentences that maximize coverage of confident labels.

### Mechanism 3: Contextual Disambiguation of Fine-Grained Relations
Providing the LLM with retrieved exemplars for top candidate relations enables resolution of subtle semantic differences that confuse standard classification models. The prompt includes query sentence alongside selected exemplars for top-$k$ candidate relations, allowing the LLM to distinguish relations like *Nationality* vs. *Place_of_Birth*.

## Foundational Learning

- **Concept: Distant Supervision (DSRE) & The Multi-Instance Problem**
  - Why needed here: Standard supervised learning assumes sentence-level labels. HYDRE operates on DSRE data where labels are "bag-level." Understanding this challenge is critical to understanding why Stage 2 (bag/sentence selection) is necessary.
  - Quick check question: Why can't we simply treat all sentences in a positively labeled bag as positive examples for training?

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - Why needed here: HYDRE uses an LLM as a "reasoning engine" via ICL rather than fine-tuning it. Distinguishing between updating model weights and steering behavior via prompt examples is essential to understanding latency/efficiency trade-offs.
  - Quick check question: If we added 1000 new training examples, would HYDRE require retraining the LLM weights or just updating the retrieval index?

- **Concept: Cross-Lingual Transfer (Translate-Train vs. Translate-Test)**
  - Why needed here: The paper extends to low-resource Indic languages. It relies on translating English data to target language (Translate-Train) or translating query to English (Translate-Test).
  - Quick check question: In the "Translate-Test" setting, which component performs the actual relation prediction: an English model or a multilingual model?

## Architecture Onboarding

- **Component map:** Input Query + DSRE Corpus → DSRE Model → Scores → Semantic Encoder + Confidence Aggregator → Top-$k$ Exemplars → LLM → Final Relation Set

- **Critical path:**
  1. Candidate Generation: PARE scores all relations; filter to Top-$k$
  2. Hybrid Retrieval: For each candidate $r$, find best Bag $B_r$ using (Similarity × Confidence)
  3. Sentence Selection: From $B_r$, select sentence $s^*$ with highest aggregate confidence
  4. Prompting: Construct prompt with $(s^^*, r)$ pairs + Query; LLM outputs final relation

- **Design tradeoffs:**
  - Retrieval Strategy: Semantic similarity helps English but hurts performance in low-resource "Translate-Train" settings
  - Choice of $k$: Setting $k$ too low misses valid relations; too high increases prompt length and confuses the LLM. Paper finds $k=5$ as robust equilibrium

- **Failure signatures:**
  - Position Bias: LLM defaults to predicting PARE's top-1 candidate regardless of query
  - Lost-in-the-Middle: Including full bags instead of representative sentences degrades performance
  - Translation Noise: Errors in entity projection during translation lead to invalid exemplars

- **First 3 experiments:**
  1. Sanity Check (Stage 1): Evaluate PARE Recall@$k$ on target test set to ensure candidate selection phase is capable
  2. Ablation (Stage 2): Run HYDRE using random exemplars vs. confidence-selected exemplars on English dev set
  3. Cross-Lingual Robustness: Execute "Translate-Test" vs. "Translate-Train" pipeline on single Indic language

## Open Questions the Paper Calls Out
- Can HYDRE effectively generalize to specialized, low-resource domains such as biomedical or financial text?
- Does the efficacy of HYDRE's cross-lingual transfer extend to low-resource languages outside the Indic family?
- Would replacing the static HYDRE pipeline with an iterative agentic workflow improve relation extraction performance?

## Limitations
- The hybrid retrieval score formulation is under-specified, making exact replication difficult
- Cross-lingual evaluations depend on translation quality and entity projection fidelity, which are not systematically measured
- The ablation of exemplar retrieval is performed only at the bag level, not at the sentence level

## Confidence
- **High Confidence:** The core mechanism of combining DSRE candidate filtering with LLM in-context reasoning is well-supported by ablation studies
- **Medium Confidence:** The specific formulation of the hybrid retrieval score and its parameterization across languages is supported by results but not fully specified
- **Low Confidence:** The generalizability of HYDRE to truly native low-resource languages and to relation schemas beyond NYT/FB-AUTO is not established

## Next Checks
1. Precision-Recall Tradeoff Validation: Re-run HYDRE with varying $k$ (e.g., $k \in \{3, 5, 10\}$) on English dev to confirm optimal tradeoff
2. Cross-Lingual Generalization Test: Apply HYDRE to a truly low-resource language dataset to test whether Translate-Train advantage holds without synthetic translation
3. Retrieval Ablation Granularity: Implement and test an ablation that removes exemplar retrieval at the sentence level (using full bags) to isolate contribution of sentence selection heuristic