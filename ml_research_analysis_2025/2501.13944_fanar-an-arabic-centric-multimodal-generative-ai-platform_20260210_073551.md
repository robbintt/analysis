---
ver: rpa2
title: 'Fanar: An Arabic-Centric Multimodal Generative AI Platform'
arxiv_id: '2501.13944'
source_url: https://arxiv.org/abs/2501.13944
tags:
- arabic
- data
- fanar
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fanar introduces a comprehensive Arabic-centric multimodal generative\
  \ AI platform featuring two large language models\u2014Fanar Star (7B parameters,\
  \ trained from scratch) and Fanar Prime (9B parameters, fine-tuned from Gemma-2-9B)\u2014\
  trained on 1 trillion Arabic, English, and code tokens. The platform integrates\
  \ speech and image generation, four retrieval-augmented generation systems (Islamic,\
  \ Recency, Biography, and Attribution RAG), and an attribution service for fact\
  \ verification."
---

# Fanar: An Arabic-Centric Multimodal Generative AI Platform

## Quick Facts
- arXiv ID: 2501.13944
- Source URL: https://arxiv.org/abs/2501.13944
- Reference count: 17
- Two highly capable Arabic LLMs (Fanar Star 7B, Fanar Prime 9B) trained on 1 trillion tokens with superior Arabic benchmark performance

## Executive Summary
Fanar introduces a comprehensive Arabic-centric multimodal generative AI platform featuring two large language models—Fanar Star (7B parameters, trained from scratch) and Fanar Prime (9B parameters, fine-tuned from Gemma-2-9B)—trained on 1 trillion Arabic, English, and code tokens. The platform integrates speech and image generation, four retrieval-augmented generation systems (Islamic, Recency, Biography, and Attribution RAG), and an attribution service for fact verification. Fanar addresses Arabic's unique morphological challenges with a novel tokenizer and provides inclusive dialectal Arabic speech recognition. Evaluation shows superior performance on Arabic benchmarks and cultural tasks compared to peer models, with strong bilingual instruction-following capabilities. The platform aims to empower Arabic-speaking communities through sovereign AI development while maintaining cultural alignment and multimodal accessibility.

## Method Summary
Fanar employs a dual-model architecture with Fanar Star (7B, from-scratch) and Fanar Prime (8.7B, fine-tuned from Gemma-2-9B), both using decoder-only Transformers with 4096 context length. The models undergo a curriculum pre-training pipeline: 4 epochs on multi-epoch corpus → 100B token cool-down on curated high-quality data → 2-stage SFT (3.6M→834k samples) → DPO (250k samples) → annealing. A custom MorphBPE tokenizer (76,800 vocab for Star, 128,256 for Prime) addresses Arabic morphology. The platform includes specialized RAG systems, multimodal generation (dialectal ASR, diffusion-based TTS, Stable Cascade image generation), and an attribution service for fact verification.

## Key Results
- Fanar Star and Prime achieve best-in-class performance on Arabic benchmarks including Arabic MMLU (64.90) and CulturalMCQ (70.60)
- Dual-model orchestration with specialized routing outperforms single-model deployment on task-specific metrics
- Morphology-aware MorphBPE tokenization demonstrates faster convergence and better morphological alignment than vanilla BPE
- Platform successfully integrates speech, image, and text generation with cultural alignment and dialectal inclusivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphology-aware tokenization improves representation efficiency for Arabic LLMs.
- Mechanism: MorphBPE constrains byte-pair merges to respect morphological boundaries, preventing semantically incoherent token splits (e.g., splitting "Al-Rahman" into unrelated fragments like "min" + "al" + incomplete root). This preserves derivational morphology where words derive from root-template combinations.
- Core assumption: Arabic's non-linear root-and-pattern morphology requires explicit structural constraints that standard BPE's greedy frequency-based merging cannot capture.
- Evidence anchors:
  - [abstract] "At the heart of Fanar are Fanar Star and Fanar Prime, two highly capable Arabic Large Language Models (LLMs) that are best in the class on well established benchmarks"
  - [section 5.6] "Morph-BPE Cross-Entropy Loss demonstrates faster convergence and smaller loss than vanilla-BPE for the same vocabulary size" and achieves "highest alignment with morphology while maintaining reasonable fertility"
  - [corpus] Weak direct evidence—no corpus papers evaluate morphology-aware tokenization for Arabic specifically
- Break condition: If morphological segmentation introduces errors or if downstream tasks show no perplexity/fertility improvement over well-tuned standard BPE.

### Mechanism 2
- Claim: Dual-model orchestration with specialized routing improves task-specific performance over single-model deployment.
- Mechanism: A classifier routes prompts to either Fanar Star (general queries, cultural/religious content) or Fanar Prime (STEM, reasoning, coding). Fanar Prime benefits from Gemma-2's extensive pre-training (8T tokens via distillation), while Fanar Star provides native Arabic grounding from scratch training.
- Core assumption: Task specialization combined with complementary model strengths (from-scratch Arabic grounding vs. strong reasoning transfer from larger teacher) yields better aggregate performance than either approach alone.
- Evidence anchors:
  - [abstract] "Both models are concurrently deployed and designed to address different types of prompts transparently routed through a custom-built orchestrator"
  - [section 8.3.2] Fanar Prime Instruct achieves top scores on Arabic MMLU (64.90), PIQA-Ar (67.82), and cultural MCQ (70.60), while maintaining competitive English performance
  - [corpus] "UI-Level Evaluation of ALLaM 34B" discusses Arabic-centric LLM evaluation but does not validate orchestration specifically
- Break condition: If classifier misrouting rate exceeds acceptable threshold or if single-model fine-tuning matches dual-model performance at lower inference cost.

### Mechanism 3
- Claim: Two-stage curriculum pre-training with cool-down on curated high-quality data enhances downstream benchmark performance.
- Mechanism: Stage 1 uses multi-epoch training (4 epochs) on full corpus; Stage 2 ("cool-down") trains on ~100B curated high-quality tokens with learning rate annealed to zero. Final epochs apply additional filtering (education classifier) and increase Arabic proportion (40% → 50%).
- Core assumption: High-quality data exposure during learning rate decay phases consolidates knowledge more effectively than uniform training.
- Evidence anchors:
  - [section 6.3.1] "The cool-down phase has also given a strong boost in downstream performance" with Figure 9 showing Arabic MMLU accuracy jump from ~42% to ~48% during cool-down
  - [section 6.2.1] Ablation shows Fanar filtering recipe achieves ~4-point improvement over Jais filters on Arabic HellaSwag
  - [corpus] "Harmonizing the Arabic Audio Space with Data Scheduling" mentions data scheduling for Arabic audio LLMs, providing indirect support for curriculum approaches
- Break condition: If cool-down phase shows no benchmark improvement or if alternative approaches (longer uniform training, different data mixing) achieve equivalent results.

## Foundational Learning

- Concept: **Arabic morphology (root-and-pattern system)**
  - Why needed here: Arabic words derive from consonantal roots (typically 3 letters) fitted into templates with affixes. Understanding this is essential for grasping why standard BPE fails and how MorphBPE addresses it.
  - Quick check question: Can you explain why the word "kitāb" (book) and "maktaba" (library) share the root k-t-b and how this differs from English concatenative morphology?

- Concept: **Decoder-only Transformer architecture**
  - Why needed here: Both Fanar models use this architecture; understanding attention mechanisms, RoPE embeddings, and GQA is necessary for interpreting architecture choices and ablation studies.
  - Quick check question: What is the difference between full attention and grouped-query attention (GQA), and why might GQA be preferred for inference efficiency?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Post-training uses DPO for alignment. Understanding preference learning, on-policy vs. off-policy data, and reference model updates is critical for reproducing the alignment pipeline.
  - Quick check question: How does DPO differ from RLHF with explicit reward models, and what happens if the reference model is not updated during training?

## Architecture Onboarding

- Component map:
  - User input → Safety filter → Classifier (determines routing) → Appropriate service (Fanar Star, Fanar Prime, RAG systems, speech/image generation) → LLM → Safety filter → Return to user → Attribution service (optional)

- Critical path:
  1. User input → Safety filter
  2. Classifier determines routing (Islamic/STEM/Recency/Biography/General)
  3. If RAG needed → retrieve context → construct prompt → LLM
  4. LLM generates response → Safety filter → Return to user
  5. Attribution service optionally validates facts post-generation

- Design tradeoffs:
  - **From-scratch vs. continual training**: Star provides native Arabic grounding but limited reasoning; Prime leverages Gemma-2's strong reasoning but requires vocab pruning
  - **Tokenizer vocabulary size**: 76,800 (Star) vs. 128,256 (Prime)—larger vocab captures more Arabic morphology but increases embedding parameters
  - **Multi-stage SFT vs. single-stage**: Two-stage SFT separates general capabilities from value-aligned data, reducing task interference

- Failure signatures:
  - **Loss spikes during pre-training**: Indicates bad batches (improper language filtering, deduplication failures); use OLMo's back-tracing to identify
  - **Cultural misalignment in outputs**: Addresses via annealing stage with targeted samples; monitor via user dislike feedback (reported ~13-15% rate)
  - **Language switching**: DPO data imbalance causes model to default to majority language; balance Arabic/English samples and add mismatched-language rejected responses

- First 3 experiments:
  1. **Tokenizer ablation**: Compare MorphBPE vs. vanilla BPE on fertility, morphological alignment score, and downstream Arabic MMLU using 1B-parameter proxy models (following Section 6.2 methodology)
  2. **Data filtering validation**: Train small models (1B, 50-100B tokens) with Fanar filters vs. baseline (e.g., Jais filters) on Arabic HellaSwag to reproduce claimed ~4-point improvement
  3. **RAG retrieval calibration**: Test Islamic RAG with varying chunk sizes (512, 1024, 2048 tokens), top-k values (5, 10, 20), and similarity thresholds on domain-specific benchmark to validate retrieval precision before full integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified autoregressive model effectively integrate speech, image, and video generation capabilities while maintaining cultural alignment?
- Basis in paper: [explicit] Section 11 states that current multimodal capabilities are not intrinsic to the autoregressive model and outlines a plan to "make speech, image and text generation as part of a unified generative model" and support video generation.
- Why unresolved: The current Fanar platform relies on separate modules for these tasks coordinated by an orchestrator, rather than a single model handling all modalities natively.
- What evidence would resolve it: The development and evaluation of a native multimodal model that outperforms the current modular system on culturally aligned generation tasks.

### Open Question 2
- Question: To what extent can enhanced test-time computation strategies improve performance on difficult reasoning tasks compared to increasing model parameters?
- Basis in paper: [explicit] Section 11 identifies exploring "the use of enhanced test time computation for hard tasks (e.g., math reasoning) where multiple responses are generated and an external reward models selects the best answer" as a future research direction.
- Why unresolved: The current models, Fanar Star and Prime, rely on standard parameter scaling and pre-training/post-training recipes without implementing test-time search or verification strategies.
- What evidence would resolve it: Benchmarks demonstrating that specific test-time compute techniques yield higher accuracy on mathematical or reasoning benchmarks than simply scaling the model size.

### Open Question 3
- Question: How can the accuracy of hate speech detection in Arabic be improved to effectively filter training data without removing culturally relevant nuance?
- Basis in paper: [explicit] Section 4.3.2 states, "We plan to increase the accuracy of hate speech detection in ASAD and use it to filter out hateful content," acknowledging current limitations.
- Why unresolved: While the authors use ASAD for semantic filtering, they imply the current precision is insufficient for robust filtering without potential data loss or missed detections.
- What evidence would resolve it: A refined classifier that demonstrates higher F1-s scores on Arabic hate speech detection and a resulting dataset with lower toxicity rates while retaining linguistic diversity.

## Limitations

- Data provenance and replicability: Heavy reliance on proprietary datasets (Al Jazeera, Qatar National Library, Ministry of Islamic Affairs) creates significant barriers to independent replication
- Evaluation context: Most Arabic benchmark evaluations were conducted in controlled settings with selected comparisons, potentially inflating relative performance metrics
- Multimodal integration maturity: Limited detail on cross-modal performance benchmarks and system-level integration testing for speech, image, and text generation components

## Confidence

**High Confidence**: The architectural decisions (MorphBPE tokenizer, dual-model orchestration, curriculum pre-training) are technically sound and well-documented. The evaluation methodology for language models follows established practices, and the performance improvements on Arabic benchmarks are measurable and reproducible given access to comparable training data.

**Medium Confidence**: The claims of cultural alignment and inclusive dialectal support are supported by benchmark results but rely heavily on internal evaluation frameworks. The effectiveness of the attribution service for fact verification and the practical utility of the specialized RAG systems in real-world deployment scenarios require further independent validation.

**Low Confidence**: The generalization of Fanar's performance to real-world Arabic-speaking communities and diverse use cases remains uncertain without broader deployment data. The long-term sustainability of the platform's performance given evolving language patterns and cultural contexts is not addressed.

## Next Checks

1. **Independent tokenizer evaluation**: Replicate the MorphBPE ablation study by training small Arabic language models (1B parameters) with MorphBPE versus standard BPE tokenization on publicly available Arabic datasets, measuring fertility scores, morphological alignment, and downstream performance on Arabic MMLU to verify the claimed improvements.

2. **Cross-platform deployment testing**: Deploy Fanar's dual-model architecture in a controlled environment with real-world Arabic prompts across different domains (religious, educational, technical) to measure classifier routing accuracy, response quality consistency, and inference efficiency compared to single-model alternatives.

3. **Cultural alignment community validation**: Conduct user studies with diverse Arabic-speaking participants across different regions and dialects to evaluate Fanar's outputs on cultural appropriateness, dialectal accuracy, and alignment with local values, comparing results against ALLaM and other Arabic LLMs to verify benchmark-based claims.