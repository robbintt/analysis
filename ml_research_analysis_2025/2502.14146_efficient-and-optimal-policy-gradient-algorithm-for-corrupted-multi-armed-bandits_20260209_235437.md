---
ver: rpa2
title: Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits
arxiv_id: '2502.14146'
source_url: https://arxiv.org/abs/2502.14146
tags:
- corruption
- regret
- time
- samba
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies the policy gradient algorithm SAMBA to stochastic\
  \ multi-armed bandits with adversarial corruptions, where an adversary can modify\
  \ rewards up to a corruption level C. SAMBA, a Markovian policy, reduces the regret\
  \ upper bound from O(K log^2 T / \u0394) + O(C) to O(K log T / \u0394) + O(C/\u0394\
  ), achieving the optimal O(log T + C) bound."
---

# Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits

## Quick Facts
- arXiv ID: 2502.14146
- Source URL: https://arxiv.org/abs/2502.14146
- Reference count: 40
- This paper applies the policy gradient algorithm SAMBA to stochastic multi-armed bandits with adversarial corruptions, where an adversary can modify rewards up to a corruption level C. SAMBA, a Markovian policy, reduces the regret upper bound from O(K log^2 T / Δ) + O(C) to O(K log T / Δ) + O(C/Δ), achieving the optimal O(log T + C) bound.

## Executive Summary
This paper presents SAMBA, a policy gradient algorithm for stochastic multi-armed bandits with adversarial corruptions. The algorithm achieves an optimal regret bound of O(log T + C), where C is the corruption level, improving upon existing combinatorial algorithms by removing an extra log T factor. SAMBA uses a Markovian policy that updates probabilities based on the leading arm and observed rewards, limiting the impact of corruptions to a single step.

## Method Summary
SAMBA is a Markovian policy gradient algorithm that maintains a probability distribution over arms and updates it based on observed rewards. The algorithm initializes uniform probabilities and identifies a leading arm each round. When the leading arm is played, it receives a probability reduction proportional to the observed reward, while other arms' probabilities are adjusted upward. When non-leading arms are played, their probabilities increase based on the reward. This Markovian update structure ensures that corruptions only affect one step, enabling the optimal regret bound.

## Key Results
- Achieves optimal O(log T + C) regret bound for corrupted multi-armed bandits
- First combinatorial algorithm to reach this asymptotic optimality
- Reduces one log T factor compared to prior combinatorial algorithms
- Superior empirical performance, especially with no corruption
- Matches the lower bound with O(C + log T) regret

## Why This Works (Mechanism)
The Markovian update structure is the key innovation. By limiting probability updates to affect only the current step's leading arm or the played arm, SAMBA ensures that corruptions cannot propagate their effects across multiple time steps. This containment strategy is what enables the removal of the extra log T factor that plagues other combinatorial approaches.

## Foundational Learning
- **Multi-armed bandit problem**: Sequential decision-making framework where an agent chooses actions to maximize cumulative reward. Why needed: Forms the basic problem structure.
- **Adversarial corruptions**: Model where an adversary can modify rewards up to a certain budget. Why needed: Represents realistic scenarios where observations may be unreliable.
- **Policy gradient methods**: Optimization techniques that directly update policies using gradient information. Why needed: Enables efficient learning without requiring explicit value function estimation.
- **Combinatorial algorithms**: Approaches that maintain and update probability distributions over action sets. Why needed: Provides framework for maintaining uncertainty about optimal actions.
- **Regret analysis**: Framework for measuring algorithm performance relative to optimal policy. Why needed: Standard metric for evaluating bandit algorithms.

## Architecture Onboarding

**Component Map**: Initialize uniform probabilities -> Identify leading arm -> Sample arm -> Observe reward -> Markovian update -> Update probabilities -> Repeat

**Critical Path**: The core algorithm loop where probabilities are updated based on leading arm identification and reward observations. The Markovian update ensures corruptions are contained.

**Design Tradeoffs**: SAMBA trades computational simplicity for theoretical guarantees. The Markovian updates are computationally efficient but require careful parameter tuning (α < Δ/(r*-Δ)).

**Failure Signatures**: 
- Probability values becoming negative or exceeding 1 after updates
- Non-convergence to optimal arm due to improper parameter settings
- Poor performance when corruption levels approach the theoretical limits

**3 First Experiments**:
1. Verify probability simplex constraints are preserved across all update scenarios
2. Test algorithm performance on simple Bernoulli bandits without corruption
3. Validate that corruptions are indeed contained to single steps by comparing updates with and without corruptions

## Open Questions the Paper Calls Out
None

## Limitations
- Corruption model is underspecified - doesn't specify which arms are targeted or direction of corruption
- Baseline algorithm implementations (BARBAR, CBARBAR, OMD) only referenced via citations without implementation details
- Random seed handling and standard deviation computation methodology is unclear

## Confidence
- **Theoretical claims**: High confidence - the O(log T + C) bound is well-established
- **Empirical claims**: Medium confidence - corruption schemes and baseline implementations are underspecified
- **Implementation feasibility**: Medium confidence - core algorithm is clear but details for baselines are missing

## Next Checks
1. Verify the Markovian update preserves probability simplex constraints across all possible reward sequences and corruption patterns
2. Replicate the empirical comparison using the 4 corruption schemes, measuring cumulative regret over time and vs corruption level across 100 runs
3. Confirm the OMD baseline indeed requires ~1700 seconds per run as Table 2 suggests, checking optimization convergence settings