---
ver: rpa2
title: 'Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and
  Defenses'
arxiv_id: '2504.04033'
source_url: https://arxiv.org/abs/2504.04033
tags:
- attribute
- attack
- attacks
- dataset
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of targeted attribute inference
  attacks in machine learning models, where an adversary seeks to infer sensitive
  attributes of training data. The authors introduce a novel "disparity inference
  attack" that leverages "angular difference," a metric based on confidence score
  distributions, to identify high-risk groups within the dataset.
---

# Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses

## Quick Facts
- arXiv ID: 2504.04033
- Source URL: https://arxiv.org/abs/2504.04033
- Reference count: 40
- One-line primary result: Novel targeted attribute inference attacks leveraging "angular difference" vulnerability metric achieve up to 20.48% higher accuracy than untargeted methods, with a novel BCorr defense reducing attack success rate differences across groups from 12.52 to 2.06.

## Executive Summary
This paper addresses targeted attribute inference attacks in machine learning, where adversaries seek to infer sensitive attributes of training data. The authors introduce a novel "disparity inference attack" that uses "angular difference," a metric based on confidence score distributions, to identify high-risk groups within datasets. They propose two targeted variations of attribute inference attacks that exploit these vulnerable groups, achieving significantly higher accuracy than untargets methods. The paper also presents a "Balanced Correlation Defense" (BCorr) to mitigate disparity while preserving model performance, demonstrating effectiveness through experimental results.

## Method Summary
The method introduces disparity inference attack using angular difference metric derived from confidence score distributions to identify vulnerable groups. Two targeted attack variants are proposed: single attribute-based targeting and nested attribute-based targeting, which concentrate queries on high-vulnerability subsets. The BCorr defense mitigates disparity by re-sampling training data to equalize sensitive-output correlation across groups while maintaining class balance. Experiments use three datasets (Census19, Texas-100X, Adult) with MLP models (3 hidden layers, 32→16→8 neurons, Adam optimizer) to evaluate attack accuracy, F1-score, and ranking quality, along with defense metrics including ASRD, EOD, and DPD.

## Key Results
- Targeted attribute inference attacks achieve up to 20.48% higher accuracy than untargeted methods (LOMIA variant)
- BCorr defense reduces attack success rate differences across groups from 12.52 to 2.06 for CSMIA
- Angular difference metric successfully identifies vulnerable groups with Kendall's Tau up to 0.85 for ranking quality
- Single attribute-based targeting outperforms nested targeting in most experimental conditions

## Why This Works (Mechanism)

### Mechanism 1: Angular Difference as Vulnerability Proxy
- Claim: Angular difference (derived from model confidence score distributions) correlates with group-level vulnerability to attribute inference attacks.
- Mechanism: When a sensitive attribute is highly correlated with the output label, the model learns to predict with higher confidence for records where the sensitive value matches the training distribution's majority. This creates distinct patterns in confidence scores when querying the same record with different sensitive values. The angle between regression lines fitted to these confidence score distributions across output classes serves as a proxy for this underlying correlation.
- Core assumption: The adversary has black-box query access to obtain confidence scores, knows all possible sensitive attribute values, and can identify group membership from non-sensitive attributes.
- Evidence anchors: [abstract] "leverages 'angular difference,' a metric based on confidence score distributions, to identify high-risk groups"; [section 4.2] "We term this the angular difference... An adversary could attempt to compare correlations between groups by computing the difference in confidence score distributions"

### Mechanism 2: Targeted Attack via Vulnerability Stratification
- Claim: Identifying and focusing attack queries on high-vulnerability groups substantially improves attack success rates compared to dataset-wide untargeted attacks.
- Mechanism: The adversary first computes angular differences for groups defined by non-sensitive attributes. Groups are ranked by angular difference (higher = more vulnerable). The attack then restricts attribute inference queries to records from the most vulnerable groups, avoiding wasted queries on low-vulnerability records where predictions would be near-random. Nested attribute-based targeting intersects multiple high-risk group segments to further concentrate queries.
- Core assumption: The training data exhibits heterogeneous correlation between sensitive and output attributes across demographic or attribute-defined groups, creating exploitable disparity.
- Evidence anchors: [abstract] "achieving significantly higher accuracy than untargeted methods (e.g., up to 20.48% for LOMIA variant)"; [section 5.3] "Condition 2 ensures that the adversary performs better in the target subset than in any subset of equal or greater size"

### Mechanism 3: Balanced Correlation Defense (BCorr)
- Claim: Re-sampling training data to equalize sensitive-output correlation across groups reduces disparate vulnerability without degrading model utility.
- Mechanism: The defender identifies the group with the lowest sensitive-output correlation (cm) as the target correlation level. For each higher-correlation group, records are sub-sampled to achieve correlation equal to cm while maintaining class balance. This removes the root cause of disparity—differential correlation—so the model cannot exploit group-specific sensitive-output associations. Since sampling preserves overall output class distributions, model accuracy is maintained.
- Core assumption: The defender has full access to training data, knows group membership and sensitive attribute values, and can identify vulnerable groups (via simulated attacks or correlation analysis).
- Evidence anchors: [abstract] "efficacy of BCorr in reducing attack success rate differences across groups (e.g., from 12.52 to 2.06 for CSMIA)"; [section 7.2] "BCorr comprises of the following steps... Sample records from each Di to form D′i such that the correlation of D′i is equal to cm"

## Foundational Learning

- **Concept: Attribute Inference Attack (Model Inversion)**
  - Why needed here: This paper's attacks assume familiarity with the basic threat model—adversary queries a trained model using public non-sensitive attributes to infer private sensitive attributes. Without this, the motivation for targeting vulnerable subgroups is unclear.
  - Quick check question: Given a trained classifier and a record's non-sensitive features, how would you predict a binary sensitive attribute using only the model's confidence scores?

- **Concept: Pearson Correlation for Binary/Categorical Variables**
  - Why needed here: The paper's central finding is that sensitive-output correlation drives vulnerability. Understanding how correlation is computed and interpreted for binary attributes (e.g., marital status vs. income class) is essential to follow the attack logic and defense design.
  - Quick check question: If 80% of married individuals in a dataset have high income while only 40% of single individuals do, what would you expect the correlation sign and approximate magnitude to be?

- **Concept: Ranking Evaluation Metrics (Kendall's Tau, Spearman's Rho)**
  - Why needed here: The disparity inference attack is evaluated on how well it ranks groups by vulnerability. These rank correlation metrics quantify ranking quality.
  - Quick check question: If an attack correctly ranks the 5 most vulnerable groups but reverses the order of the 5 least vulnerable, would Spearman's Rho be positive, negative, or near zero?

## Architecture Onboarding

- **Component map:**
  - Confidence Matrix Generator (Algorithm 1) -> Angular Difference Computer (Algorithm 2) -> Single Attribute Targeter -> Nested Attribute Targeter -> CSMIA/LOMIA Attack Implementation
  - BCorr Sampler -> Correlation Balancing -> Retrained Model

- **Critical path:**
  1. Threat assessment: Run disparity inference attack on deployed model using query budget q to identify vulnerable groups.
  2. Attack simulation: If angular differences exceed threshold, simulate targeted CSMIA/LOMIA to quantify exposure.
  3. Defense deployment: If ASRD > ε, apply BCorr to training data and retrain model before production release.

- **Design tradeoffs:**
  - Query budget (q) vs. ranking accuracy: Higher q improves angular difference estimates but increases attack detectability. Paper uses q=1.0 in experiments; lower values (0.1–0.3) may suffice for coarse ranking.
  - Attack budget (κ) vs. attack success rate: Lower κ (smaller target set) increases ASR but reduces coverage. κ=0.1 yields ~15–20% ASR gain; κ<0.05 may overfit to outlier groups.
  - BCorr sampling vs. data efficiency: BCorr discards records from high-correlation groups (up to ~33% in experiments). If training data is scarce, consider DAMIR (mutual information regularization) despite its weaker disparity reduction.

- **Failure signatures:**
  - Angular difference yields flat ranking (all groups similar): Either the dataset has genuinely uniform correlation (attack not viable) or the prediction correctness filter (t vector) excludes too many records—check filter pass rate.
  - Targeted attack underperforms untargeted baseline: Budget κ may be too aggressive, or group attribute selection picked a low-variance attribute—verify angular difference range before committing.
  - BCorr degrades model accuracy by >5%: Minimum correlation group may have correlation too close to zero; consider relaxing ASRD target or using partial correlation balancing.

- **First 3 experiments:**
  1. Reproduce angular difference–ASR correlation (Section 6.3, Figure 5): Train model on Census19 with controlled per-group correlations; compute angular differences; plot against CSMIA/LOMIA accuracy per group. Verify linear trend.
  2. Single vs. nested targeting comparison (Tables 2–3): On Adult dataset, run both targeting strategies at κ∈{1.0, 0.5, 0.25, 0.1}. Confirm nested targeting achieves higher peak ASR at equivalent κ.
  3. BCorr efficacy baseline (Table 5): Train model on Census19 with Male/Female correlation disparity; apply BCorr; report ASRD, model accuracy, and fairness metrics (EOD, DPD). Confirm ASRD drops to <5 while accuracy change is <1%.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The approach relies on binary/categorical attribute correlation with the output label, limiting applicability to multi-class sensitive attributes or continuous attributes
- Assumes adversaries can perfectly identify group membership from non-sensitive attributes, which may not hold in real-world deployments
- Scalability to very large datasets or high-dimensional feature spaces is not explored, and query complexity (O(|D| × |S|)) could become prohibitive

## Confidence

- **High confidence**: The mathematical derivation of angular difference as a vulnerability proxy (Mechanism 1) and the BCorr defense design are well-specified and theoretically grounded.
- **Medium confidence**: The targeted attack methodology (Mechanisms 2-3) shows promising results but depends on the assumption that angular difference ranking translates to attack success ranking, which while supported by experiments, could vary with different model architectures or data distributions.
- **Low confidence**: The scalability of the approach to very large datasets or high-dimensional feature spaces is not explored, and the query complexity (O(|D| × |S|)) could become prohibitive.

## Next Checks
1. **Correlation generalization test**: Apply the disparity inference framework to datasets with multi-class sensitive attributes (e.g., race with >2 categories) and validate whether angular difference still correlates with attack success.
2. **Defense robustness check**: Test BCorr against adaptive adversaries who modify their attack strategy after observing the defense (e.g., switching to gradient-based methods or federated learning settings).
3. **Query efficiency analysis**: Experiment with varying query budgets (q) below 0.1 to determine the minimum budget required for reliable angular difference estimation, and assess the corresponding trade-off with attack detectability.