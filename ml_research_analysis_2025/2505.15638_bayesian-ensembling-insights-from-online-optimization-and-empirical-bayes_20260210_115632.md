---
ver: rpa2
title: 'Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes'
arxiv_id: '2505.15638'
source_url: https://arxiv.org/abs/2505.15638
tags:
- online
- learning
- bayesian
- regret
- o-bma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Online Bayesian Stacking (OBS), a method that
  optimizes the log-score over predictive distributions to adaptively combine Bayesian
  models in online, continual learning settings. OBS is shown to correspond exactly
  to the well-studied problem of (universal) online portfolio selection, enabling
  efficient algorithms with strong theoretical regret bounds.
---

# Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes

## Quick Facts
- **arXiv ID:** 2505.15638
- **Source URL:** https://arxiv.org/abs/2505.15638
- **Reference count:** 40
- **Primary result:** Online Bayesian Stacking (OBS) outperforms Online Bayesian Model Averaging (O-BMA) in M-open settings by reframing model combination as portfolio selection.

## Executive Summary
This paper introduces Online Bayesian Stacking (OBS), a novel approach for adaptively combining Bayesian models in online and continual learning settings. OBS reframes the problem of model ensembling as a portfolio selection task, where predictive densities serve as "returns" and the ensemble weights are optimized to maximize cumulative log-score. The key insight is that OBS optimizes the log-score of the ensemble mixture, while O-BMA optimizes a linear combination of individual log-scores, leading to fundamental differences in behavior. Empirically, OBS demonstrates superior performance across multiple datasets, maintaining proper mixtures of models even in M-open scenarios where O-BMA collapses to single models.

## Method Summary
The method implements OBS by treating predictive densities from multiple Bayesian models as "returns" in a portfolio selection framework. Using Online Convex Optimization (OCO) algorithms like Exponentiated Gradient (EG) or Online Newton Step (ONS), OBS iteratively updates ensemble weights to maximize cumulative log-score. The core implementation involves a model bank producing predictive densities, a likelihood vectorizer converting these to "returns," an OBS optimizer (OCO algorithm) that updates weights based on observed returns, and a mixer that forms the final predictive distribution. The method is particularly effective in M-open settings where the true data-generating process is not among the candidate models.

## Key Results
- OBS achieved median predictive log-likelihood of -0.847 ± 0.02 on MNIST, compared to -0.932 ± 0.02 for O-BMA
- OBS maintains proper mixtures of models while O-BMA collapses to single models in M-open scenarios
- OBS demonstrates superior performance in variational Bayesian neural networks, stochastic volatility forecasting, and non-stationary Gaussian process regression

## Why This Works (Mechanism)

### Mechanism 1
OBS reframes model combination as portfolio selection, treating predictive densities as "returns" and using OCO algorithms to maximize cumulative log-score. This avoids multiplicative weight collapse seen in standard Bayesian Model Averaging. Core assumption: predictive densities must be non-zero and properly normalized.

### Mechanism 2
OBS optimizes log-score of ensemble mixture (log ∑ wₖpₖ), while O-BMA optimizes linear combination of individual log-scores (∑ wₖ log pₖ). This allows OBS to maintain mixtures even in infinite data limit. Core assumption: M-open environment where true model is not in candidate set.

### Mechanism 3
BMA collapses to single model due to implicit Empirical Bayes optimization over discrete indicator variable, while OBS treats mixture weights as hyperparameters to optimize, preserving diverse model contributions. Core assumption: infinite data regime where marginal likelihoods diverge.

## Foundational Learning

- **M-open vs. M-closed Assumptions**
  - Why needed: Choice between OBS and BMA depends entirely on this distinction
  - Quick check: Do you believe one of your K candidate models is the "true" generator of the data?

- **Log-Score (Predictive Likelihood)**
  - Why needed: Objective function for OBS; measures surprise of observed data
  - Quick check: Can you calculate probability density p(y|x) for every model in your ensemble?

- **Online Convex Optimization (OCO) Regret**
  - Why needed: OBS relies on OCO algorithms for regret bounds guaranteeing near-optimal performance
  - Quick check: Are you optimizing for best performance at final time step T or adapting to changing environment?

## Architecture Onboarding

- **Component map:** Model Bank -> Likelihood Vectorizer -> OBS Optimizer -> Mixer
- **Critical path:** Ensuring Likelihood Vectorizer produces valid, normalized densities
- **Design tradeoffs:**
  - EG: Simple, linear runtime O(K), good for large K
  - ONS: O(K²) runtime, faster theoretical convergence but computationally heavier
  - Soft-Bayes: Robust to extreme market variability but may adapt slower
- **Failure signatures:**
  - Weight Collapse: O-BMA weights hitting 0 for all but one model
  - Numerical Underflow: Log-sum-exp calculations underflowing with very small likelihoods
  - Lag: Standard OBS sticking to old weights in highly non-stationary environments
- **First 3 experiments:**
  1. Synthetic M-Open Validation: Generate data from model outside ensemble, verify OBS maintains mixture weights
  2. Stationary Benchmark: Train variational BNNs on MNIST, compare PLL of OBS vs O-BMA
  3. Non-Stationary Stress Test: Apply concept drift, compare standard ONS against D-ONS

## Open Questions the Paper Calls Out

### Open Question 1
Does a hybrid approach combining O-BMA and OBS offer superior practical performance in ambiguous M-closed/M-open settings compared to standalone methods? The paper proposes a hybrid method with theoretical regret bounds but notes it was not tested empirically.

### Open Question 2
How can online Bayesian stacking be optimized using algorithms designed for dynamic or adaptive regret to better handle regime changes? The paper evaluates discounted ONS but identifies this as a future direction.

### Open Question 3
What are the optimal strategies for initializing ensemble weights in Online Bayesian Stacking? The paper uses uniform weights or pre-training but lacks systematic study on initialization effects.

## Limitations

- Performance heavily depends on choice of prior variances for ensemble members
- Quadratic complexity of ONS limits practical applicability for large ensembles
- Limited empirical validation of D-ONS effectiveness in highly dynamic environments

## Confidence

- **Prior Specification Dependency**: Medium
- **Computational Scaling**: High
- **Non-stationary Adaptation**: Low

## Next Checks

1. **Prior Sensitivity Analysis**: Systematically vary prior variance parameters across multiple orders of magnitude to quantify impact on OBS performance relative to O-BMA.

2. **Scaling Experiment**: Evaluate OBS with larger ensembles (K=10, K=20) using both EG and ONS to verify computational tradeoffs and performance scaling.

3. **Extreme Non-stationarity Test**: Design synthetic dataset with abrupt, repeated concept shifts to compare adaptation speed of standard OBS versus D-ONS against sliding-window baseline.