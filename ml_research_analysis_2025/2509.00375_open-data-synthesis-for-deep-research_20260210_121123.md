---
ver: rpa2
title: Open Data Synthesis For Deep Research
arxiv_id: '2509.00375'
source_url: https://arxiv.org/abs/2509.00375
tags:
- research
- arxiv
- reasoning
- deep
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoSeek introduces a scalable framework for synthesizing complex
  Deep Research tasks by formalizing them as Hierarchical Constraint Satisfaction
  Problems (HCSPs). It uses a dual-agent system to recursively build research trees
  from large-scale webpages, blurring intermediate nodes into valid sub-problems and
  converting these into natural language questions requiring multi-step reasoning.
---

# Open Data Synthesis For Deep Research

## Quick Facts
- **arXiv ID:** 2509.00375
- **Source URL:** https://arxiv.org/abs/2509.00375
- **Reference count:** 20
- **Primary result:** 3B model trained on InfoSeek outperforms 32B baselines and rivals commercial APIs on BrowseComp-Plus

## Executive Summary
InfoSeek introduces a scalable framework for synthesizing complex Deep Research tasks by formalizing them as Hierarchical Constraint Satisfaction Problems (HCSPs). It uses a dual-agent system to recursively build research trees from large-scale webpages, blurring intermediate nodes into valid sub-problems and converting these into natural language questions requiring multi-step reasoning. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On BrowseComp-Plus, a 3B LLM trained with InfoSeek surpasses much larger 32B models and achieves performance comparable to leading commercial APIs like Gemini 2.5-Pro. The dataset includes over 50K training examples, 16.5K reasoning trajectories, and is fully open-source.

## Method Summary
InfoSeek formalizes Deep Research as Hierarchical Constraint Satisfaction Problems (HCSP), where complex questions are recursively decomposed into sub-problems with constraints. A dual-agent system (Planner and Browser) constructs Research Trees from raw text, using "Action 2" to blur parent nodes with constraints, converting them to questions via LLMs. The training pipeline consists of supervised fine-tuning (SFT) using rejection sampling with a teacher model (Qwen2.5-72B) and reinforcement learning (RL) with Group Relative Policy Optimization (GRPO). A Refiner Agent (Qwen2.5-7B-Inst) summarizes search results to prevent context overflow. The dataset contains 50K+ QA pairs and 16.5K reasoning trajectories synthesized from Wikipedia and webpages.

## Key Results
- 3B LLM trained with InfoSeek surpasses much larger 32B models on BrowseComp-Plus
- Achieves performance comparable to leading commercial APIs like Gemini 2.5-Pro
- Dataset includes over 50K training examples and 16.5K reasoning trajectories

## Why This Works (Mechanism)
The framework works by transforming complex research tasks into structured HCSPs that can be systematically decomposed. The dual-agent system enables recursive tree construction where each node represents a well-defined sub-problem. The blurring mechanism (Action 2) ensures that parent nodes with constraints are converted into specific questions requiring multi-step reasoning. The Refiner Agent prevents context overflow by aggressively summarizing search results, maintaining the 16k token limit while preserving essential evidence.

## Foundational Learning
- **Hierarchical Constraint Satisfaction Problems (HCSP):** A formal framework for decomposing complex problems into hierarchical sub-problems with constraints. Needed to structure Deep Research tasks systematically. Quick check: Verify that all generated sub-problems satisfy their parent constraints.
- **Dual-Agent System:** Planner creates research trees while Browser handles search and evidence collection. Needed for separating high-level planning from low-level execution. Quick check: Ensure the Browser agent correctly parses search results into structured evidence.
- **Group Relative Policy Optimization (GRPO):** A reinforcement learning algorithm that compares multiple outputs from the same policy. Needed to optimize the model using binary rewards. Quick check: Verify that reward calculation correctly handles the 5-output sampling strategy.
- **Research Tree Construction:** Recursive building of hierarchical question-answer structures. Needed to represent the multi-step reasoning process. Quick check: Validate that tree depth and branching follow expected patterns.
- **Context Management:** Techniques to prevent token overflow in long research chains. Needed to maintain performance within model limits. Quick check: Monitor average context length during training rollouts.
- **Rejection Sampling:** Filtering teacher-generated trajectories to select high-quality examples. Needed to create a clean training dataset. Quick check: Calculate acceptance rate and verify trajectory quality.

## Architecture Onboarding

**Component Map:** Planner -> Browser -> Refiner Agent -> LLM (Student) -> GRPO Reward

**Critical Path:** Data Synthesis (Planner + Browser) → SFT Training → RL Optimization → Inference

**Design Tradeoffs:**
- Small model (3B) vs large models (32B+) - InfoSeek enables smaller models to match larger ones through better data
- Context length 16k - requires aggressive summarization via Refiner Agent
- Binary rewards - simpler than continuous rewards but may provide less gradient information
- Wikipedia-only corpus vs web corpus - affects generalization and domain specificity

**Failure Signatures:**
- Context overflow errors during inference
- Underdetermined problems with multiple valid answers
- Inconsistent tree structures from the dual-agent system
- Low acceptance rates during rejection sampling

**First Experiments:**
1. Test the Refiner Agent prompt reconstruction to verify it correctly summarizes search results within token limits
2. Validate the Research Tree generation process on a small Wikipedia subset to check HCSP formulation
3. Run a single GRPO iteration with 5 sampled outputs to confirm reward calculation and policy update

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates for internal agents (Refiner and Teacher) are not provided, potentially affecting reproducibility
- Missing implementation details for search tool interface and result parsing
- Heavy reliance on LLM-generated data may introduce bias or noise
- Limited external validation beyond Wikipedia and BrowseComp-Plus corpus

## Confidence
- **High Confidence:** Overall methodology and experimental results are clearly specified and robust across multiple benchmarks
- **Medium Confidence:** Synthetic data generation process is well-described but missing key prompt details for internal agents
- **Low Confidence:** Generalization to real-world domains beyond provided corpus is uncertain

## Next Checks
1. Reconstruct and test Refiner Agent and Teacher prompts using described models to verify trajectory quality matches reported metrics
2. Run trained model on extended research trees to ensure Refiner Agent consistently prevents context overflow
3. Evaluate trained model on independent Deep Research dataset (e.g., DRBench or Geo-Temporal Deep Research) to assess robustness beyond InfoSeek corpus