---
ver: rpa2
title: KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs
arxiv_id: '2512.11851'
source_url: https://arxiv.org/abs/2512.11851
tags:
- cache
- prompt
- arxiv
- tokens
- reuse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that cached key-value (KV) activations
  from a small GPT-2-style language model can be reused across different prompts to
  accelerate inference, provided the new prompt starts with an exact prefix of a previously
  processed prompt. Using DialoGPT-medium (345M parameters) as a testbed, we built
  a cache of past activations from a set of prompts, retrieved the most similar cached
  prompt via sentence embeddings, and reused its KV states to skip redundant attention
  computations.
---

# KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs

## Quick Facts
- **arXiv ID**: 2512.11851
- **Source URL**: https://arxiv.org/abs/2512.11851
- **Authors**: Prashant Pandey
- **Reference count**: 21
- **Primary result**: Demonstrated 46.46% latency reduction on DialoGPT-medium by reusing KV cache activations from cached prompts that are exact prefixes of new prompts

## Executive Summary
This study introduces a method for cross-prompt key-value (KV) cache recycling in low-parameter language models to accelerate inference. Using DialoGPT-medium (345M parameters), cached KV activations from previously processed prompts are reused when new prompts share an exact token-level prefix. The method employs sentence embeddings to retrieve the most similar cached prompt, verifies exact prefix match at the token level, and injects the cached KV states to skip redundant attention computations. Results show that test prompts reused 35-38 tokens on average, achieving 46.46% latency reduction without degrading output quality. The recycled outputs matched baseline outputs with 0.594 average cosine similarity, confirming semantic equivalence.

## Method Summary
The method builds a cache of past KV activations by running forward passes with `use_cache=True` on a set of prompts, extracting and serializing the KV tensors to CPU. For each new test prompt, a sentence embedding is computed and the most similar cached prompt is retrieved via dot product. Exact token-level prefix matching verifies if reuse is possible. If a match is found, the cached KV states are loaded and injected into the generation process, allowing the model to skip attention computation over the prefix tokens. The approach requires no model modifications and uses standard HuggingFace generation with the `past_key_values` argument.

## Key Results
- Achieved 46.46% average reduction in generation latency through KV cache recycling
- Reused 35-38 tokens on average per test prompt
- Recycled outputs maintained semantic equivalence with baseline outputs (0.594 average cosine similarity)
- Prompt embeddings of matched pairs showed high similarity (0.819 average)
- Exact prefix requirement ensured no output degradation while enabling computational savings

## Why This Works (Mechanism)

### Mechanism 1: Prefix-Exact KV State Reuse
When a cached prompt is an exact token-level prefix of a new prompt, the computed key-value pairs remain mathematically valid and can be reloaded to skip redundant attention computation. Autoregressive decoder attention computes K,V tensors per token position. If positions 1:k are identical between two prompts, their K,V tensors are identical (assuming deterministic attention). Loading `past_key_values` allows generation to resume from position k+1. The core assumption is that token identity implies representation identity—positional encoding and layer-wise transformations are deterministic and stable across separate forward passes.

### Mechanism 2: Semantic Embedding Retrieval for Cache Selection
Sentence embeddings efficiently surface cached prompts with high prefix overlap probability. The embedding for a test prompt is computed, and the cached prompt with highest dot product similarity is retrieved, then verified at the token level. Embedding similarity acts as a coarse filter; token matching is the correctness gate. The core assumption is that semantically similar prompts (at embedding level) are more likely to share token prefixes. This is a heuristic, not guaranteed.

### Mechanism 3: Compute Reduction Translates to Latency Reduction
Skipping attention computation over k prefix tokens reduces end-to-end latency proportionally to k. Baseline time ≈ T_enc(m) + T_dec(g). Recycled time ≈ T_enc(m−k) + T_dec(g) + T_loadKV. Since T_loadKV (CPU tensor load) is negligible vs. multi-layer attention, net speedup scales with k. The core assumption is that KV loading overhead is small; no GPU-CPU transfer bottleneck at tested cache sizes.

## Foundational Learning

- **Concept: KV Caching in Autoregressive Decoding**
  - Why needed here: The entire method depends on understanding what `past_key_values` contains and why it can be injected into `model.generate()`.
  - Quick check question: During incremental decoding, what does the model avoid recomputing when `past_key_values` is provided?

- **Concept: Positional Encoding and Token-Position Binding**
  - Why needed here: Prefix reuse assumes that K,V at position i depends only on tokens 1:i, not on future tokens. Causal masking ensures this.
  - Quick check question: Why does GPT-2's causal attention mask guarantee that position 5's K,V tensor is identical across any prompts sharing tokens 1-5?

- **Concept: Embedding Similarity vs. Token Similarity**
  - Why needed here: The method uses semantic embeddings for retrieval but requires exact token match for correctness. Understanding this two-stage filter is critical.
  - Quick check question: Could two prompts with 0.95 cosine embedding similarity have zero token overlap? Give an example.

## Architecture Onboarding

- **Component map**: Cache Construction Pipeline -> Retrieval Index -> Prefix Validator -> Generation Engine
- **Critical path**: New prompt → embedding computation → top-1 retrieval → token prefix check → (if r=k) load KVs → generate with reduced input_ids. If prefix check fails, fall back to baseline generation.
- **Design tradeoffs**:
  - Exact prefix vs. fuzzy match: Current implementation requires r=k (cached prompt is full prefix). More aggressive reuse (partial prefix) would increase hit rate but risks incorrect context injection.
  - CPU storage vs. GPU retention: Serializing to CPU reduces GPU memory pressure but adds I/O latency. At small cache sizes, this is negligible; at scale, becomes bottleneck.
  - Cache size vs. retrieval latency: Larger cache increases hit probability but slows embedding search; FAISS mitigates but does not eliminate.
- **Failure signatures**:
  - Silent quality degradation: If prefix check is buggy and non-prefix KVs are injected, outputs may diverge subtly. Monitor output similarity scores.
  - No speedup despite cache hit: If T_loadKV > T_enc(k), speedup is negative. Profile KV loading time separately.
  - Retrieval returns wrong cache: High embedding similarity but zero prefix overlap. Log retrieval scores and prefix depths to diagnose.
- **First 3 experiments**:
  1. Reproduce baseline vs. recycled latency on provided test prompts: Verify 40-50% speedup with exact prefix matches. Log per-prompt reuse depth to confirm proportional scaling.
  2. Stress test prefix strictness: Synthesize prompts with 1-token prefix deviation. Confirm fallback to baseline and no output corruption.
  3. Scale cache size to 100-500 prompts: Measure retrieval latency growth and any change in hit rate. Compare brute-force dot product vs. FAISS indexing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-prompt KV cache recycling maintain output fidelity and latency improvements when applied to contemporary large language models (e.g., 7B–70B parameters) with significantly larger memory footprints?
- Basis in paper: The authors state in Section 6.1 that the "study covers only one model" (DialoGPT-medium) and that "cross architecture validation remains future work."
- Why unresolved: The study was restricted to a 345M parameter model; it is unknown if the stability of attention representations and the I/O overhead scale effectively to multi-billion parameter architectures.
- What evidence would resolve it: Successful replication of the recycling methodology on larger architectures (e.g., LLaMA-3 or Mistral) showing similar cosine similarity scores and latency reduction percentages.

### Open Question 2
- Question: Is it possible to relax the "exact prefix" constraint to allow recycling for semantically similar or partially overlapping prompts without causing output divergence?
- Basis in paper: Section 6.1 notes that the prototype "enforces an exact prefix condition" and that this "conservative rule... does not utilize the potential overlap between semantically similar prompts."
- Why unresolved: The current implementation strictly requires $x_{1:r}^{(t)} = x_{1:r}^{(c)}$; allowing fuzzy matching introduces the risk of "hallucination" or context collapse, which has not yet been tested.
- What evidence would resolve it: A study analyzing output degradation rates when using approximate token matching (e.g., Levenshtein distance or embedding thresholds) rather than exact equality for cache retrieval.

### Open Question 3
- Question: At what scale does the I/O latency of loading serialized KV caches from the CPU negate the computational gains achieved by skipping attention layers?
- Basis in paper: The paper mentions in Section 6.1 that caches are stored on CPU and "adding minor I/O latency... becomes non-negligible when caches grow large," yet the experiment used only a very small dataset.
- Why unresolved: The experiments used a cache of only 10 prompts, preventing an analysis of the trade-off between cache search/retrieval time and inference speedup in a production-sized cache (e.g., thousands of entries).
- What evidence would resolve it: Benchmarking results plotting retrieval latency and generation speedup against cache database size to identify the break-even point for net efficiency.

## Limitations
- **Exact prefix requirement**: The method only reuses KV states when there is an exact token-level prefix match, missing potential reuse opportunities from semantically similar but non-identical prompts
- **Small model scope**: Validated only on DialoGPT-medium (345M parameters), leaving uncertainty about scalability to larger models and different architectures
- **Tokenization brittleness**: The method's reliance on exact token matching may fail when tokenization differs slightly between semantically identical prompts

## Confidence

- **KV cache recycling provides 40-50% latency reduction** (High confidence): This is directly measured and reproducible given the exact cache/prompts used. The proportional relationship between reuse depth and speedup is well-established.
- **Recycled outputs are semantically equivalent to baseline outputs** (Medium confidence): The 0.594 average cosine similarity suggests reasonable alignment, but this metric alone doesn't guarantee functional equivalence. No human evaluation or downstream task performance validation is reported.
- **Cross-prompt prefix matching is a general technique applicable to larger models** (Low confidence): While the mechanism is sound, the empirical validation is limited to a single small model. No evidence is provided about performance on models with different architectures, tokenizers, or vocabulary sizes.

## Next Checks

1. **Tokenization robustness test**: Create pairs of prompts that are semantically identical but differ by one character (e.g., "machine learning" vs. "machine-learning"). Run the exact prefix matching algorithm and verify whether reuse occurs. Measure the fraction of semantically similar prompts that fail due to tokenization differences.

2. **Embedding-retrieval accuracy analysis**: For the existing cache/prompts, systematically test all 10×6=60 possible cached-test prompt pairs. Compute the correlation between embedding similarity and prefix overlap depth. Report false positive rate (high embedding similarity but zero prefix overlap) and false negative rate (prefix overlap but low embedding similarity).

3. **Scale model size experiment**: Repeat the entire pipeline with GPT-2-small (124M) and GPT-2-large (774M) using the same cache/prompts. Measure whether the 46.46% speedup holds, and separately profile T_loadKV and T_enc(k) to identify the scaling breakpoint where loading overhead dominates.