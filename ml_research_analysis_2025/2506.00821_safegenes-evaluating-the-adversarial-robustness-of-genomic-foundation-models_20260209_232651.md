---
ver: rpa2
title: 'SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models'
arxiv_id: '2506.00821'
source_url: https://arxiv.org/abs/2506.00821
tags:
- adversarial
- attack
- prompt
- fgsm
- soft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SafeGenes, a framework for evaluating the\
  \ adversarial robustness of Genomic Foundation Models (GFMs) like ESM. The study\
  \ uses two attack strategies\u2014Fast Gradient Sign Method (FGSM) and soft prompt\
  \ attacks\u2014to assess model vulnerability under adversarial perturbations."
---

# SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models

## Quick Facts
- **arXiv ID:** 2506.00821
- **Source URL:** https://arxiv.org/abs/2506.00821
- **Reference count:** 34
- **Primary result:** Both input-space (FGSM) and embedding-space (soft prompt) attacks cause significant performance degradation across all tested GFMs, with targeted soft prompt attacks being especially effective.

## Executive Summary
This paper introduces SafeGenes, a framework for evaluating the adversarial robustness of Genomic Foundation Models (GFMs) like ESM. The study uses two attack strategies—Fast Gradient Sign Method (FGSM) and soft prompt attacks—to assess model vulnerability under adversarial perturbations. Results show that both input-space and embedding-space attacks cause significant performance degradation across all tested GFMs, with the targeted soft prompt attack being especially effective. Even high-capacity models like ESM1b and ESM1v exhibit large drops in AUC and AUPR, particularly in smaller or MLM-based models like ProteinBERT. These findings highlight the critical need for robust adversarial evaluation and defense mechanisms in clinical genomic applications.

## Method Summary
The study evaluates adversarial robustness of four GFMs (ESM1b, ESM1v, ESM2, ProteinBERT) for variant effect prediction using two attack strategies. Input-space attacks employ FGSM with ε=0.01 on embeddings, while embedding-space attacks use soft prompt attacks with 10 learnable tokens prepended. A siamese network with PLLR-based scoring classifies variants as pathogenic/benign, trained on cardiomyopathy and arrhythmia datasets with BCE loss and calibration. Performance is measured via AUC, AUPR, PLLR, Brier score, ECE, and FPR@TPR=0.95 across clean and adversarial conditions.

## Key Results
- Both FGSM and soft prompt attacks cause significant AUC and AUPR degradation across all tested GFMs
- Targeted soft prompt attacks are particularly effective, outperforming untargeted variants
- Even high-capacity models like ESM1b and ESM1v show large performance drops, especially smaller or MLM-based models like ProteinBERT
- Gene-level analysis reveals specific genes (e.g., MYH7) are disproportionately vulnerable to adversarial perturbations

## Why This Works (Mechanism)
The adversarial attacks exploit the continuous embedding space of GFMs by either perturbing input embeddings directly (FGSM) or injecting learnable soft prompts that modify the model's internal representation. The PLLR scoring mechanism amplifies small changes in the embedding space into large classification differences, making the models sensitive to adversarial perturbations. The soft prompt attack is particularly effective because it operates within the semantic space of the model itself, bypassing traditional input-level defenses.

## Foundational Learning
- **Siamese network architecture**: Needed to compare wild-type and mutant protein embeddings; quick check: verify paired input processing and shared weights
- **PLLR scoring mechanism**: Computes log-likelihood ratio between wild-type and mutant sequences; quick check: ensure separation between pathogenic and benign classes
- **Soft prompt attacks**: Prepend learnable tokens to modify model behavior; quick check: verify embedding dimension alignment across different GFM architectures
- **FGSM perturbation on embeddings**: Adds small noise in embedding space rather than token space; quick check: confirm perturbation doesn't produce invalid amino acid representations
- **Model calibration**: Sigmoid calibration ensures proper probability outputs; quick check: verify Brier score and ECE metrics are reasonable
- **Gene-level vulnerability analysis**: Identifies specific genes more susceptible to attacks; quick check: confirm consistent vulnerability patterns across multiple runs

## Architecture Onboarding

**Component map:** Data → GFM Encoder → Siamese Network → PLLR Scoring → Classification Head → Metrics

**Critical path:** Input sequences → GFM backbone → PLLR computation → BCE loss → Model updates (for both clean training and adversarial attacks)

**Design tradeoffs:** The study uses a simple siamese architecture with PLLR scoring rather than more complex approaches, prioritizing interpretability and computational efficiency over potentially higher baseline accuracy. The choice of soft prompt length (10 tokens) represents a balance between attack effectiveness and computational cost.

**Failure signatures:** PLLR values failing to separate classes, training instability with NaN loss during soft prompt attacks, or FGSM perturbations producing invalid embeddings.

**Exactly 3 first experiments:**
1. Train baseline siamese network on clean data and verify AUC/AUPR scores match reported baselines within ±0.02
2. Apply FGSM attack with ε=0.01 and measure performance degradation
3. Implement soft prompt attack with 10 tokens and compare effectiveness against FGSM

## Open Questions the Paper Calls Out
**Open Question 1:** What defense mechanisms can effectively protect GFMs against embedding-space adversarial attacks like soft prompt manipulations? The paper demonstrates vulnerability but does not propose or evaluate any defensive strategies, only noting that standard input-level defenses are insufficient.

**Open Question 2:** Why are certain genes (e.g., MYH7) disproportionately vulnerable to adversarial perturbations compared to others? The paper identifies the phenomenon but doesn't investigate whether vulnerability correlates with sequence features, mutation frequency, structural properties, or representation geometry.

**Open Question 3:** Do adversarial vulnerabilities generalize to other genomic foundation model applications beyond variant effect prediction? The study focuses only on two disease-specific variant classification tasks without testing other clinical applications.

## Limitations
- Potential architectural mismatch between soft prompt embeddings and diverse GFM backbone dimensions could bias attack success rates
- Evaluation focuses on only two disease-specific variant datasets with binary labels, limiting generalizability
- Only two attack strategies are tested, leaving gaps in understanding robustness against more sophisticated or black-box adversaries

## Confidence
- **Adversarial vulnerability findings:** High confidence - methodology is standard and reproducible
- **Relative model robustness rankings:** Medium confidence - baseline performance differences may be influenced by unexplained architectural details
- **Clinical implications:** Low confidence - study doesn't model real-world clinical deployment constraints

## Next Checks
1. Reimplement the siamese network classification head (pooling method, hidden dimensions, activation functions) and verify clean AUC/AUPR scores match reported baselines within ±0.02 before applying attacks
2. Cross-validate soft prompt embedding alignment by testing attacks with fixed vs. adaptive prompt dimensionality per GFM architecture to isolate architectural bias from true adversarial susceptibility
3. Benchmark against an additional GFM variant (e.g., ESM3 or retrained ESM1b) using the same hyperparameters to confirm observed performance gaps are not model-specific artifacts