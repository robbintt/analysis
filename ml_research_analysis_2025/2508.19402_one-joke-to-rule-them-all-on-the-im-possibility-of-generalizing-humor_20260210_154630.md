---
ver: rpa2
title: One Joke to Rule them All? On the (Im)possibility of Generalizing Humor
arxiv_id: '2508.19402'
source_url: https://arxiv.org/abs/2508.19402
tags:
- humor
- transfer
- training
- datasets
- jokes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether competence on specific humor tasks
  enables generalization to novel humor types using transfer learning across four
  datasets representing different humor styles (Amazon questions, Dad Jokes, sarcasm
  headlines, and one-liners). The authors fine-tune two LLMs (Mistral-7B and LLaMA-2-7B)
  under varied diversity settings and evaluate on unseen datasets.
---

# One Joke to Rule them All? On the (Im)possibility of Generalizing Humor

## Quick Facts
- **arXiv ID:** 2508.19402
- **Source URL:** https://arxiv.org/abs/2508.19402
- **Reference count:** 40
- **Primary result:** LLMs can transfer humor knowledge across different humor types, with diverse training improving generalization

## Executive Summary
This paper investigates whether competence on specific humor tasks enables generalization to novel humor types using transfer learning across four datasets representing different humor styles. The authors fine-tune two LLMs (Mistral-7B and LLaMA-2-7B) under varied diversity settings and evaluate on unseen datasets. Results show that models can transfer humor knowledge, achieving up to 75% accuracy on unseen datasets, with training on diverse sources improving transferability (1.88-4.05%) while maintaining in-domain performance. Dad Jokes emerge as the most effective humor type for enabling transfer to others, though it is difficult to generalize to.

## Method Summary
The study uses LoRA fine-tuning on pre-trained LLMs (Mistral-7B-v0.1 and LLaMA-2-7B-hf) with 4-bit quantization. Four humor datasets (Amazon Questions, Reddit Dad Jokes, Sarcasm Headlines, One Liners) are downsampled to 6,250 examples each and split 80/2/18 for train/val/test. For multi-dataset training, examples are sampled equally to maintain 5,000 total training examples. Hyperparameter search covers learning rates {3e-4, 5e-5, 6e-5}, LoRA rank {32, 64, 128}, and alpha {8, 16, 32}, with 4-fold CV per dataset. Models are evaluated on held-out datasets across single, double, and triple dataset training scenarios.

## Key Results
- Models achieve up to 75% accuracy on unseen humor datasets
- Training on diverse sources improves transferability by 1.88-4.05%
- Dad Jokes enable the strongest transfer to other humor types (68-71%) but are difficult to transfer from
- Increasing training diversity generally enhances generalization, particularly for simpler humor types

## Why This Works (Mechanism)

### Mechanism 1: Diverse Training for Enhanced Transferability
Training LLMs on multiple humor types teaches models to recognize general, transferable humor features (e.g., incongruity, wordplay patterns) rather than overfitting to single-dataset surface patterns. This works when target humor types share underlying linguistic or structural features with training humor types.

### Mechanism 2: Asymmetric Humor Type Hierarchy
Transfer performance is asymmetric, governed by complexity and generality. Complex, diverse humor types (e.g., Dad Jokes) contain broader feature ranges, making them good sources. Simple, constrained types (e.g., One Liners) are easier targets but poor sources for generalization.

### Mechanism 3: In-Domain Robustness to Data Dilution
Models maintain high in-domain performance even with significantly reduced in-domain data when mixed with other relevant tasks. Multi-task learning acts as a regularizer, preventing overfitting and providing beneficial inductive bias.

## Foundational Learning

**Fine-tuning**: Why needed? The core experimental method adapts pre-trained LLMs to specific humor tasks rather than training from scratch. Quick check: How does LoRA enable efficient fine-tuning of large models?

**Transfer Learning**: Why needed? The entire paper investigates applying knowledge from one humor dataset (source) to a different, unseen humor dataset (target). Quick check: What would be an example of a "source" and "target" domain in this context?

**Generalization Gap**: Why needed? The key metric is performance on novel, unseen humor types. A smaller gap indicates better generalization. Quick check: If a model achieves 99% accuracy on training data but 50% on unseen data, what does this indicate?

## Architecture Onboarding

**Component map**: Pre-trained LLMs (Mistral-7B, LLaMA-2-7B) -> LoRA adapter -> Data Pipeline (sampling from 1-3 datasets) -> Binary classification evaluation

**Critical path**:
1. Data Construction: Sample and balance data from chosen datasets, generate GPT-4 Turbo negatives for Dad Jokes
2. Hyperparameter Selection: Cross-validation to find optimal LoRA configuration
3. Model Fine-tuning: Train on constructed training set
4. Transfer Evaluation: Test on held-out dataset from completely different humor type

**Design tradeoffs**: Diversity vs. In-Domain Data (favorable tradeoff: diversity gains outweigh reduced in-domain samples); Model Choice (Mistral-7B outperformed LLaMA-2 on transfer tasks)

**Failure signatures**: Overfitting to source (high training accuracy, random unseen performance); Negative transfer (target performance drops with added source); Asymmetric transfer failure (excellent A→B but poor B→A)

**First 3 experiments**:
1. Establish baseline: Fine-tune Mistral-7B on Amazon dataset, evaluate in-domain accuracy
2. Test zero-shot transfer: Evaluate Amazon-trained model on Dad Jokes without further training
3. Introduce diversity: Fine-tune on Amazon + Dad Jokes, evaluate on One Liners to measure diversity impact

## Open Questions the Paper Calls Out

**Open Question 1**: Do transfer patterns in textual humor extend to multimodal humor (memes, cartoons, videos) and cross-lingual/cross-cultural settings? The study is limited to short-form English textual humor.

**Open Question 2**: What specific linguistic or structural features make Dad Jokes effective enablers of transfer while being difficult to transfer to? The paper identifies asymmetry but doesn't isolate driving features.

**Open Question 3**: To what extent is humor transferability intrinsic to humor types versus dependent on model architecture and pre-training? Only two models tested, making it unclear whether patterns reflect humor-relevant architecture or pre-training variations.

## Limitations

- Dataset Representativeness: Downsampling to 6,250 examples may not capture full humor type distributions; Amazon uses automatically generated negatives
- Transfer Asymmetry Mechanisms: Identifies asymmetric patterns but lacks theoretical framework explaining why certain types transfer better
- Generalization Beyond Binary Classification: All experiments focus on binary classification, not nuanced humor understanding tasks

## Confidence

**High Confidence**: Core finding that diverse training improves transfer (1.88-4.05% gain) is well-supported by systematic experiments
**Medium Confidence**: Dad Jokes as most effective transfer enabler is supported but relies on specific dataset construction methodology
**Low Confidence**: General assertion that humor transfer is "possible" requires more validation beyond binary classification

## Next Checks

1. **Cross-Dataset Negative Sampling Validation**: Replicate experiments using naturally occurring negatives for all datasets rather than GPT-4 generated negatives for Dad Jokes
2. **Multi-Class Humor Classification Extension**: Extend framework to multi-class setup distinguishing different humor types rather than just humorous vs. non-humorous
3. **Zero-Shot Transfer Robustness Test**: Systematically evaluate zero-shot transfer performance across all dataset pairs to understand baseline transfer capability