---
ver: rpa2
title: 'Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on
  Reducing Patch-Level Embeddings'
arxiv_id: '2506.04997'
source_url: https://arxiv.org/abs/2506.04997
tags:
- merging
- colqwen2
- performance
- pruning
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the excessive memory usage of ColPali/ColQwen2,
  which encodes each document page into multiple patch-level embeddings for visual
  document retrieval. The authors systematically investigate token reduction strategies,
  finding that token pruning is inherently unsuitable for VDR due to the unpredictability
  of query-conditioned patch importance.
---

# Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings

## Quick Facts
- **arXiv ID**: 2506.04997
- **Source URL**: https://arxiv.org/abs/2506.04997
- **Reference count**: 20
- **Primary result**: Light-ColPali/ColQwen2 achieves 98.2% retrieval performance with only 11.8% memory usage using semantic clustering

## Executive Summary
This study addresses the excessive memory usage of ColPali/ColQwen2 visual document retrieval systems, which encode each document page into multiple patch-level embeddings. Through systematic investigation of token reduction strategies, the authors demonstrate that traditional pruning approaches are inherently unsuitable for VDR due to query-dependent patch importance. Instead, they develop Light-ColPali/ColQwen2, which employs semantic clustering for token merging at the final stage of the retrieval pipeline. The approach maintains 98.2% of retrieval performance with only 11.8% of original memory usage, establishing a competitive baseline for efficient visual document retrieval.

## Method Summary
The method involves reducing patch-level embeddings through semantic clustering rather than pruning. Documents are encoded through the full LVLM pipeline (Vision Encoder → LLM → Projector), then patch embeddings undergo hierarchical clustering based on cosine similarity. The merged embeddings are stored offline, and the retriever is fine-tuned with LoRA (r=32, α=32) using InfoNCE loss. The system is evaluated on three benchmarks (ViDoRE, VisRAG, MMLongBench-Doc) covering 9 datasets, comparing performance against the original ColPali/ColQwen2 models.

## Key Results
- Light-ColPali/ColQwen2 maintains 98.2% retrieval performance with only 11.8% memory usage (merging factor 4)
- At extreme compression (merging factor 49), the system preserves 94.6% effectiveness at just 2.8% memory footprint
- Fine-tuning recovers 61-67% of performance degradation caused by token merging at high compression ratios
- Semantic clustering outperforms spatial pooling by 2-3% across multiple datasets and merging factors

## Why This Works (Mechanism)

### Mechanism 1: Query-Dependent Patch Informativeness Invalidates Offline Pruning
Pruning strategies are inherently unsuitable for VDR because patch importance cannot be determined without query-specific information. Different queries activate different subsets of document patches, making any fixed pruning criterion fundamentally limited. The paper shows that different queries on the same page activate nearly disjoint patch sets (only 1 out of 736 patches overlapped in one example).

### Mechanism 2: Semantic Clustering Preserves Information While Reducing Dimensionality
Merging via representation-similarity clustering better preserves retrieval-relevant information during token reduction. Hierarchical clustering groups patches with similar embeddings regardless of spatial position, preserving semantic content because patches within clusters share representation space and likely encode similar visual/textual features.

### Mechanism 3: Fine-tuning Recovers Performance by Adapting to Compressed Representations
During fine-tuning, the model learns to extract relevant signals from "blurred" (merged) representations. The loss function optimizes query-document similarity using already-compressed embeddings, forcing the model to redistribute attention across merged token groups. At extreme compression (merging factor 49), fine-tuning recovers 61-67% of the performance drop.

## Foundational Learning

- **Concept: ColPali/ColQwen2 Late Interaction Architecture**
  - Why needed here: Light-ColPali builds directly on this architecture; understanding how Nₚ patch embeddings interact with Nq query embeddings via MaxSim is essential.
  - Quick check question: Given a query with 20 tokens and a document page with 768 patches, how many similarity computations does one query-document pair require?

- **Concept: Token Reduction Strategies (Pruning vs. Merging)**
  - Why needed here: The paper's core contribution is demonstrating why merging succeeds where pruning fails for VDR.
  - Quick check question: Why does pruning work for generation tasks but not for retrieval indexing?

- **Concept: Hierarchical Clustering for Embedding Aggregation**
  - Why needed here: The optimal merging approach uses hierarchical clustering; understanding distance metrics and linkage methods is necessary for implementation.
  - Quick check question: When clustering 768 patch embeddings into 156 clusters (factor 4), what determines which patches merge together?

## Architecture Onboarding

- **Component map**:
```
Input Document Image
    ↓
Vision Encoder (e.g., SigLIP)
    ↓ [Nₚ visual tokens]
LLM Backbone (e.g., Qwen2-VL)
    ↓ [Nₚ contextualized tokens]
Projection Layer (128-dim)
    ↓ [Nₚ patch embeddings]
Semantic Clustering Module (NEW)
    ↓ [N'ₚ merged embeddings]
Offline Storage Index
```

- **Critical path**:
1. Document encoding through full LVLM pipeline (Vision Encoder → LLM → Projector)
2. Semantic clustering on 128-dim embeddings using cosine similarity
3. Hierarchical merging to target N'ₚ clusters
4. Fine-tuning with LoRA (r=32, α=32) on merged embeddings
5. Indexing compressed embeddings for retrieval

- **Design tradeoffs**:
| Decision | Option A | Option B | Tradeoff |
|----------|----------|----------|----------|
| Merging location | Post-LLM | Post-Projector | Post-Projector gives +0.4% but requires full forward pass |
| Merging approach | 2D pooling | Semantic clustering | Clustering gives +2-3% but adds ~50% offline time |
| Fine-tuning | Training-free | LoRA fine-tuning | Fine-tuning recovers 8.4% at high compression but costs 72 GPU-hours |

- **Failure signatures**:
1. **Pruning-style degradation**: If performance drops >15% at low compression (factor 4), likely using pruning instead of merging
2. **Spatial pooling underperformance**: If 2D pooling significantly trails clustering on text-heavy documents, confirms spatial assumptions don't hold
3. **Pre-LLM merging collapse**: If merging before LLM causes >20% drop, confirms LVLM needs full visual context
4. **High information-density documents**: DocVQA/TAT-DQA show larger drops (text-rich); if these fail disproportionately, may need adaptive merging factors

- **First 3 experiments**:
1. **Baseline reproduction**: Run ColQwen2-v1.0 on ViDoRE benchmark subset (InfoVQA, DocVQA) to establish baseline NDCG@5 and memory footprint (target: ~81.4% average, 64.4x memory vs DSE).
2. **Ablation on merging factor**: Test semantic clustering at factors [4, 9, 25, 49] without fine-tuning to replicate training-free curve and identify cliff edge (expected: cliff between factor 25-49).
3. **Fine-tuning recovery test**: Fine-tune with merged embeddings at factor 25 on 130k query training set (5 epochs, batch 256), measure absolute NDCG@5 gain vs training-free (target: +3-4% recovery).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can a mechanism be developed to adaptively adjust the token merging factor based on the specific information density of each document page?
- **Basis in paper**: Section 6 notes that performance retention varies by dataset and states, "how to adaptively adjust the merging factor... remains an open challenge. We leave this as future work."
- **Why unresolved**: The current Light-ColPali/ColQwen2 uses a fixed merging factor, which fails to account for the varying complexity and information density of different document types (e.g., text-heavy vs. visual-heavy).
- **What evidence would resolve it**: A dynamic merging model that assigns variable reduction ratios based on page content, outperforming fixed-factor baselines on heterogeneous benchmarks.

### Open Question 2
- **Question**: How can orthogonal efficiency techniques like dimension reduction, vector quantization, or model distillation be integrated with token merging to achieve greater storage efficiency?
- **Basis in paper**: The Limitations section lists these methods as "unexplored" and "orthogonal," suggesting they could complement the proposed token reduction strategies.
- **Why unresolved**: This study exclusively focused on reducing the number of patch embeddings ($N_p$), leaving the reduction of embedding dimensionality ($d$) or model size untouched.
- **What evidence would resolve it**: Experiments demonstrating that combining Light-ColPali with product quantization (reducing $d$) yields multiplicative storage savings without catastrophic performance loss.

### Open Question 3
- **Question**: Can the computational overhead of offline hierarchical clustering be reduced using approximate methods without sacrificing retrieval performance?
- **Basis in paper**: Table 3 shows that clustering adds significant offline latency (increasing embedding generation time by ~50%), yet Section 5.1 establishes it as the superior merging approach over faster pooling methods.
- **Why unresolved**: While merging reduces storage, the paper highlights a trade-off in offline processing time; it is undetermined if this specific bottleneck can be optimized without harming the semantic quality of the merged embeddings.
- **What evidence would resolve it**: An approximate clustering algorithm that matches the NDCG@5 of hierarchical clustering while reducing the offline embedding generation time to be comparable to spatial pooling.

## Limitations

- The analysis of query-dependent patch activation is based on a small sample (N=7) and needs broader statistical validation across the full query distribution.
- Performance degradation at extreme compression (merging factor 49) remains significant (21% absolute drop), with unclear characterization of which query types suffer most.
- The training dataset composition and representativeness across document types and query complexities is not fully specified, potentially limiting generalizability.

## Confidence

- **Query-dependent patch importance**: High - multiple empirical observations support this claim with concrete evidence.
- **Semantic clustering superiority**: High - ablation studies show statistically significant improvements across datasets.
- **Fine-tuning effectiveness**: Medium - shows promise but absolute performance levels remain substantially below uncompressed baseline.

## Next Checks

1. **Query-patch dependency statistical validation**: Sample 1,000 random query-document pairs from ViDoRE benchmarks and compute pairwise patch activation overlap distributions. Test whether the observed low overlap (1-2 patches) is statistically significant compared to random chance across different document types and query categories.

2. **Adaptive merging factor experiment**: Implement document-adaptive merging where merging factor varies based on document complexity metrics (text density, layout complexity, visual entropy). Evaluate whether adaptive merging outperforms fixed-factor approaches, particularly for text-heavy documents (DocVQA) where current methods show largest degradation.

3. **Cross-dataset training transfer**: Train Light-ColPali on a subset of ViDoRE datasets (e.g., InfoVQA, RVL-DIP) and evaluate zero-shot performance on VisRAG and MMLongBench-Doc. Measure whether fine-tuning benefits transfer across datasets or require dataset-specific adaptation, validating the generalizability of the training approach.