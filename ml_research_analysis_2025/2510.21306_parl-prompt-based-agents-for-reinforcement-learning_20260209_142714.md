---
ver: rpa2
title: 'PARL: Prompt-based Agents for Reinforcement Learning'
arxiv_id: '2510.21306'
source_url: https://arxiv.org/abs/2510.21306
tags:
- parl
- learning
- agent
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARL, a method for using large language models
  (LLMs) as reinforcement learning (RL) agents through prompting, without any fine-tuning.
  PARL encodes states, actions, and rewards in prompts, enabling in-context learning
  through iterative interaction with an environment.
---

# PARL: Prompt-based Agents for Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.21306
- Source URL: https://arxiv.org/abs/2510.21306
- Reference count: 0
- Primary result: PARL matches or outperforms traditional RL agents in simple environments using prompt-based in-context learning without fine-tuning

## Executive Summary
PARL introduces a novel approach to reinforcement learning that leverages large language models as agents through prompting rather than fine-tuning. The method encodes states, actions, and rewards in prompts, enabling LLMs to learn through iterative trial-and-error interaction with environments. Evaluated on three standard RL tasks—Blackjack, Frozen Lake, and Taxi—PARL demonstrates competitive performance, particularly in Blackjack where it achieves an average reward of 0.20 compared to 0.04 for the best baseline (PPO). The approach shows that pretrained knowledge can provide immediate policy advantages in knowledge-aligned tasks while revealing limitations in complex environments requiring symbolic computation.

## Method Summary
PARL formulates the LLM as a frozen policy that generates actions based on prompts containing task descriptions and accumulated interaction history. The prompt policy P_PARL concatenates task description T with tuples of (state, action, reward) from previous episodes. The method uses script-based state decoding functions to convert raw environment states into natural language descriptions, significantly improving learning speed and performance compared to LLM self-decoding. Training involves 100 episodes of interaction, with the LLM improving its policy through pattern recognition in the accumulated prompt history. Three configurations are tested: Full History (accumulates all episodes), Random Rewards (tests dependence on actual reward signals), and No History (only current episode).

## Key Results
- PARL achieves 0.20 average reward in Blackjack vs. 0.04 for PPO, the best-performing baseline
- Script-based state decoding reaches 0.6 average reward in Blackjack after 23 episodes vs. 0.3 after 50 episodes with self-decoding
- PARL fails to solve the Taxi task while baselines learn to minimize penalties, highlighting limitations in complex environments

## Why This Works (Mechanism)

### Mechanism 1: In-Context Policy Optimization via Cumulative Prompt History
The LLM improves its policy through pattern recognition by treating successful trajectories as few-shot examples in the prompt. Each episode's history (state, action, reward) is appended to the prompt, and the LLM's attention mechanism identifies reward-correlated action patterns. Random reward ablation confirms genuine learning occurs, as performance collapses when rewards are randomized.

### Mechanism 2: Script-Based State Decoding Bridges Non-Linguistic to Linguistic Representations
External Python scripts convert raw numeric states into human-readable descriptions before prompt inclusion, dramatically improving learning speed and performance. This reduces the LLM's interpretive burden since LLMs trained on natural language struggle with symbolic/numeric reasoning. Script decoding reaches 0.6 average reward in Blackjack after 23 episodes vs. 0.3 with self-decoding.

### Mechanism 3: Pre-Trained Domain Knowledge Provides Zero-Shot Initialization Advantage
When task semantics align with pre-training data, the LLM bootstraps reasonable policies from task description alone before accumulating history. In Blackjack, "No History" achieves -0.07 average reward vs. PPO's 0.04, demonstrating partial knowledge transfer. However, in Frozen Lake and Taxi where no applicable knowledge exists, "No History" fails completely.

## Foundational Learning

- **Markov Decision Process (MDP) Formulation**: PARL inherits RL's formalism—understanding (S, A, P, R, γ) is required to map environment outputs to prompt components. *Quick check*: Can you identify the state space, action space, and reward function for a Gymnasium environment from its documentation?

- **In-Context Learning in Transformers**: PARL's core mechanism relies on LLMs learning from prompt examples without gradient updates—distinct from fine-tuning or RAG. *Quick check*: If you double the number of few-shot examples in a prompt, would you expect a linear performance improvement? Why or why not?

- **Prompt Engineering for Structured Data**: Encoding tuples, grids, and reward signals as text requires deliberate formatting choices that affect LLM comprehension. *Quick check*: How would you format the state `(10, 6, 1), [7]` (player sum, dealer card, usable ace) as a natural language sentence?

## Architecture Onboarding

- **Component map**: Environment (Gymnasium API) -> State Decoder (Python script) -> Prompt Builder -> LLM (frozen weights) -> Action Parser
- **Critical path**: State decoder quality → LLM comprehension → action correctness. If the decoder produces ambiguous descriptions, the entire pipeline degrades.
- **Design tradeoffs**: Context length vs. history depth (long histories may exceed token limits or introduce noise), script vs. LLM self-decoding (scripts are accurate but require engineering vs. self-decoding scales but fails on complex encodings), episode history during inference (full history improves Blackjack but harms Taxi).
- **Failure signatures**: Random reward injection collapses learning, long episodes with large state-action spaces introduce noise leading to repeated sub-optimal actions, token overflow truncates early episodes.
- **First 3 experiments**: 1) Replicate Blackjack baseline with script-based decoding, 100 episodes, GPT-4o, target ~0.20 avg reward. 2) Ablate state decoder: compare script-based vs. raw state vs. LLM self-decoding on Frozen Lake. 3) Context length stress test on Taxi: truncate history to last N episodes (N=5, 10, 20, 50), plot avg reward vs. N.

## Open Questions the Paper Calls Out

- **Retrieval-based episode selection**: Can retrieval mechanisms effectively mitigate noise and context length limits in PARL? Current implementation concatenates full history, causing context overflow and noise in complex tasks like Taxi.

- **Multimodal extensions**: Can multimodal PARL enable effective visual reinforcement learning? Current text-based state encoding limits applicability to visual environments like Atari games.

- **Smaller model scalability**: How does PARL perform with smaller language models compared to GPT-4o? It's unclear if performance gains are inherent to the method or rely on the vast scale of GPT-4o.

## Limitations

- Context accumulation impact remains unclear due to lack of ablation studies on optimal window size across tasks
- State decoding requires environment-specific engineering without systematic frameworks for automatic decoder generation
- Exploration-exploitation transition mechanism is not characterized or quantified in the trial-and-error approach

## Confidence

- **High confidence**: In-context learning via prompt history works for simple tasks (Blackjack), script-based decoding provides measurable benefits, random reward ablation confirms genuine learning
- **Medium confidence**: Claims about pretrained knowledge advantage are supported by Blackjack but lack systematic analysis of domain feature importance
- **Low confidence**: Extrapolation to more complex environments, scalability claims, and generalization beyond tested tasks lack empirical validation

## Next Checks

1. **Context window ablation study**: Systematically vary history length (last N episodes) across all three tasks to identify optimal context windows and quantify noise thresholds

2. **Intermediate state decoding comparison**: Compare script-based decoding against simpler approaches (formatted numeric state descriptions, template-based conversions) to establish a spectrum of decoding complexity vs. performance

3. **Cross-task knowledge transfer**: Train PARL on one task (e.g., Blackjack), then evaluate zero-shot performance on a semantically similar task (e.g., simplified card games) to measure actual pretrained knowledge utility