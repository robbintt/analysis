---
ver: rpa2
title: 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image
  Analysis'
arxiv_id: '2505.09358'
source_url: https://arxiv.org/abs/2505.09358
tags:
- depth
- image
- diffusion
- marigold
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Marigold, a resource-efficient fine-tuning
  protocol for converting pretrained text-to-image latent diffusion models (LDMs)
  into powerful generative models for dense image analysis tasks. The key innovation
  is minimal architectural modification of the U-Net to accept concatenated latent
  codes from both the input image and target modality (e.g., depth, surface normals,
  intrinsic decomposition), enabling the model to learn the conditional distribution
  rather than just its mode.
---

# Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis

## Quick Facts
- arXiv ID: 2505.09358
- Source URL: https://arxiv.org/abs/2505.09358
- Reference count: 40
- Primary result: Sub-100ms depth estimation (AbsRel ~5.5%) via DDIM sampling and LCM distillation

## Executive Summary
Marigold is a resource-efficient fine-tuning protocol that converts pretrained text-to-image latent diffusion models into powerful generative models for dense image analysis tasks. The key innovation is minimal architectural modification of the U-Net to accept concatenated latent codes from both input images and target modalities (e.g., depth, surface normals), enabling the model to learn conditional distributions rather than just modes. By training exclusively on small synthetic datasets, Marigold preserves prior knowledge from large-scale pretraining while achieving strong zero-shot generalization to unseen real-world data, with state-of-the-art performance on tasks including monocular depth estimation, surface normals prediction, and intrinsic image decomposition.

## Method Summary
Marigold fine-tunes a pre-trained Stable Diffusion v2 model by modifying its U-Net architecture to accept concatenated latent codes from both input images and target modalities. The input channels of the first convolutional layer are doubled and initialized by duplicating weights and dividing by two. The model is trained exclusively on synthetic datasets (HyperSim, Virtual KITTI, InteriorVerse, Sintel) using standard diffusion denoising objectives in latent space. Training takes approximately 2.5 days on a single RTX 4090 GPU. Inference leverages DDIM sampling with trailing timesteps for single-step capability and LCM distillation for further speed improvements, achieving sub-100ms inference times.

## Key Results
- Achieves AbsRel ~5.5% on NYUv2 depth estimation
- Mean angular error ~14.9° on ScanNet surface normals prediction
- Sub-100ms inference speed through DDIM sampling and LCM distillation
- Strong zero-shot generalization from synthetic-only training to real-world data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concatenating image and target latent codes enables the pretrained LDM to learn conditional distributions for dense prediction tasks while preserving prior knowledge.
- **Mechanism:** The input channels of the U-Net's first convolutional layer are doubled to accept `cat(z(d)_t, z(x))` where `z(x)` is the encoded image condition and `z(d)_t` is the noisy target latent. Weights are duplicated and divided by two to maintain activation statistics from pretraining.
- **Core assumption:** The VAE encoder, designed for RGB, can faithfully encode single-channel modalities (depth, normals) when replicated to three channels.
- **Evidence anchors:** Page 5, Section III-B: "To implement the conditioning of the latent denoiser... we concatenate the image and depth latent codes into a single input zt = cat(z(d)_t, z(x)) along the feature dimension."
- **Break condition:** If the target modality cannot be faithfully reconstructed through the RGB-trained VAE, the concatenated conditioning will propagate artifacts rather than useful signal.

### Mechanism 2
- **Claim:** Training exclusively on synthetic data with a short fine-tuning protocol preserves the pretrained visual prior while enabling task adaptation.
- **Mechanism:** Synthetic data provides dense, noise-free, pixel-complete ground truth that reduces noise in gradient updates. The short protocol (~18K iterations, 2.5 days on single GPU) prevents catastrophic forgetting of the LAION-5B pretrained knowledge.
- **Core assumption:** The synthetic training distribution is sufficiently diverse to cover real-world test scenarios without requiring real data.
- **Evidence anchors:** Page 5, Section III-C: "Real depth datasets suffer from missing depth values... synthetic depth is inherently dense and complete... It provides the cleanest examples and reduces noise in gradient updates during the short fine-tuning protocol."
- **Break condition:** If synthetic-to-real domain gap is large (e.g., synthetic lacks humans, animals, specific textures), zero-shot generalization degrades.

### Mechanism 3
- **Claim:** Modeling the full conditional distribution via diffusion enables uncertainty quantification and ambiguity handling that deterministic regression cannot provide.
- **Mechanism:** Multiple predictions from different noise initializations sample the conditional distribution `P(d|x)`. Ensembling via scale-shift alignment approximates the mode while preserving the ability to quantify uncertainty from prediction variance.
- **Core assumption:** For ill-posed problems (transparent objects, occlusions), multiple plausible interpretations exist and the ensemble median better approximates ground truth than any single sample.
- **Evidence anchors:** Page 3, Section I: "Modeling the distribution of depth conditioned on the input image with an LDM allows for multiple plausible interpretations of the input. This ability is essential for solving ill-posed problems."
- **Break condition:** If NFE budget is too low (1-step without proper DDIM-trailing scheduler), the sampled distribution collapses to a biased estimate.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** Marigold inherits the forward/reverse process formulation. Understanding `d_t = √ᾱ_t·d_0 + √(1-ᾱ_t)·ε` (Eq. 1) is essential for interpreting training objectives.
  - **Quick check question:** Can you explain why the noise schedule {β₁,...,β_T} matters for both training stability and inference quality?

- **Concept: Latent Diffusion and VAE Architecture**
  - **Why needed here:** Marigold operates entirely in the VAE bottleneck latent space. You must understand why compression there enables affordable single-GPU training.
  - **Quick check question:** What is the compression ratio of Stable Diffusion v2's VAE, and why does operating in latent space reduce memory by ~8× compared to pixel-space diffusion?

- **Concept: DDIM vs. DDPM Sampling**
  - **Why needed here:** Inference uses DDIM with "trailing" timesteps for 1-step capability. The Garcia et al. correction [23] was critical for v1.1 performance.
  - **Quick check question:** Why does DDIM enable deterministic sampling with fewer steps than DDPM, and what does "trailing timesteps" mean?

## Architecture Onboarding

- **Component map:** Input Image (RGB) -> VAE Encoder (frozen) -> z(x) -> Concat -> U-Net (fine-tuned) -> z(d)_0 -> VAE Decoder -> Prediction
- **Critical path:**
  1. Verify VAE can encode/decode your target modality (Sec III-B "first check")
  2. Implement affine-invariant normalization (Eq. 3) for depth; raw encoding for normals
  3. Apply DDIM-trailing scheduler with zero SNR for 1-step inference
  4. Ensemble 10 predictions with scale-shift alignment for benchmark evaluation
- **Design tradeoffs:**
  - 1-step vs. multi-step: 1-step is ~100ms but slightly worse AbsRel; 10×4 ensemble is highest quality but ~40× slower
  - TAESD vs. full VAE: Tiny VAE gives 0.082s inference but introduces artifacts; full VAE is 0.568s
  - Synthetic-only training: Eliminates real-data noise but risks domain gap; paper shows 74K synthetic samples suffice
- **Failure signatures:**
  - Flat surface artifacts in depth: Usually indicates insufficient training diversity or wrong normalization percentile (should use 2%/98%)
  - Inconsistent predictions across ensemble members: Check that scale-shift alignment regularization λR in Eq. 4 is applied
  - Blurry high-resolution outputs: Base model resolution bias; must use Marigold-HR with MultiDiffusion (Sec VII)
- **First 3 experiments:**
  1. VAE reconstruction sanity check: Take a depth map from HyperSim, encode-decode through the frozen SD VAE, verify `d ≈ D(E(d))` with MSE < 0.01
  2. Single-image overfit test: Fine-tune on ONE image-depth pair for 500 iterations. The model should predict that exact depth perfectly
  3. Scheduler ablation: Run inference with DDIM-leading (v1.0 default) vs. DDIM-trailing (v1.1 default) at 1, 4, 10 steps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is latent consistency model (LCM) distillation a viable approach for achieving fast inference in diffusion-based image analysis models, given that DDIM with trailing timesteps achieves strong single-step performance?
- **Basis in paper:** Section VI.D states "the viability of LCM distillation remains an open question for future research" given that Marigold with DDIM trailing timesteps performs well.
- **Why unresolved:** LCM distillation requires additional training complexity (teacher-student-target setup, ~1 GPU-day), while Garcia et al. [23] showed that simple DDIM scheduler tuning can achieve competitive single-step inference without distillation.
- **What evidence would resolve it:** Systematic comparison of LCM vs. DDIM-trailing across multiple modalities (depth, normals, IID) showing whether LCM provides meaningful quality or speed advantages beyond what scheduler tuning alone achieves.

### Open Question 2
- **Question:** Under what conditions does the generative formulation (modeling conditional distributions) provide practical advantages over end-to-end deterministic approaches for dense prediction tasks?
- **Basis in paper:** The paper notes E2E-FT [23] and GenPercept [26] showed "end-to-end networks can score higher in zero-shot benchmarks than the similar generative model," yet Marigold maintains the generative formulation. The stated benefit—handling ambiguity through sampling—is primarily useful for ill-posed cases but comes at higher computational cost.
- **Why unresolved:** Benchmark evaluation uses ensembling to approximate the mode, which negates the generative advantage. The practical utility of uncertainty quantification and multi-sample diversity in downstream applications remains underexplored.
- **What evidence would resolve it:** Downstream task evaluations (e.g., 3D reconstruction, novel view synthesis) comparing generative vs. deterministic models when predictive uncertainty matters, plus analysis of cases where multi-modal outputs provide meaningful information.

### Open Question 3
- **Question:** What properties of synthetic training data maximize zero-shot generalization to real-world images, and how can the domain gap be systematically characterized?
- **Basis in paper:** The paper trains exclusively on synthetic data but notes: "the remaining concern is the sufficient diversity or domain gaps between synthetic and real data, which sometimes limits generalization ability." Dataset ablation (Table III) shows combining indoor+outdoor synthetic data improves both domains, but the underlying principles remain unclear.
- **Why unresolved:** The paper demonstrates that synthetic data works but doesn't establish which aspects (photorealism, scene diversity, rendering quality, annotation completeness) are most critical, or whether there's a point of diminishing returns.
- **What evidence would resolve it:** Controlled experiments varying individual synthetic dataset properties while measuring real-world transfer; systematic analysis of failure cases attributable to synthetic-real domain gaps.

## Limitations
- Assumes VAE-trained RGB encoders can faithfully reconstruct single-channel modalities without architectural changes
- Synthetic-only training may suffer from domain gaps in complex real-world scenarios involving humans, animals, or uncommon materials
- 1-step inference claims rely heavily on DDIM-trailing scheduler variant, which is not yet standard in most diffusion implementations

## Confidence
- **High confidence** in the architectural modification mechanism (concatenation and weight initialization), as this follows standard diffusion conditioning principles with explicit mathematical justification
- **Medium confidence** in the synthetic-only training effectiveness, given the empirical results but limited discussion of synthetic dataset diversity requirements
- **Low confidence** in the claimed 1-step inference quality without seeing more extensive ablation studies across different noise schedules and task types

## Next Checks
1. **VAE Reconstruction Fidelity Test:** Systematically measure reconstruction MSE for depth, normals, and intrinsic decomposition maps through the frozen VAE across the full test set, not just single examples
2. **Synthetic-to-Real Domain Gap Analysis:** Evaluate performance degradation when training on synthetic data but testing on real data with increasing domain shift
3. **1-Step Inference Robustness Study:** Compare 1-step DDIM-trailing against 4-step and 10-step variants across all three tasks with statistical significance testing