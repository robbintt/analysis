---
ver: rpa2
title: 'EngiBench: A Benchmark for Evaluating Large Language Models on Engineering
  Problem Solving'
arxiv_id: '2509.17677'
source_url: https://arxiv.org/abs/2509.17677
tags:
- engineering
- reasoning
- level
- problem
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EngiBench introduces a hierarchical benchmark to evaluate LLMs
  on engineering problem-solving across three difficulty levels: foundational knowledge
  retrieval, multi-step contextual reasoning, and open-ended modeling. The benchmark
  includes controlled problem variants (perturbed, knowledge-enhanced, and math abstraction)
  to isolate reasoning abilities and test robustness.'
---

# EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving

## Quick Facts
- arXiv ID: 2509.17677
- Source URL: https://arxiv.org/abs/2509.17677
- Reference count: 40
- LLMs show sharp performance degradation on higher-level engineering tasks, especially with perturbed problem variants

## Executive Summary
EngiBench is a hierarchical benchmark designed to evaluate large language models' capabilities in engineering problem solving across three difficulty levels. The benchmark systematically tests foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling tasks. Through controlled problem variants including perturbed, knowledge-enhanced, and math abstraction modifications, the benchmark isolates and evaluates specific reasoning capabilities. Experiments reveal that while models perform well on basic knowledge retrieval, their accuracy drops significantly on complex reasoning tasks and remains far below human expert performance on open-ended engineering problems.

## Method Summary
The benchmark employs a hierarchical structure with three levels of engineering problem difficulty, using controlled variants to systematically test different reasoning capabilities. Problems are generated from public sources and modified to create perturbed versions that test robustness, knowledge-enhanced versions that assess integration of external information, and math abstraction versions that evaluate reasoning transfer. Evaluation combines accuracy metrics, pass@k scoring, and LLM-as-a-judge methods for open-ended responses, with comparisons to human expert performance providing contextual baselines.

## Key Results
- Models perform well on Level 1 foundational knowledge tasks but accuracy drops sharply on Levels 2 and 3
- Performance significantly degrades when problems are slightly perturbed, revealing robustness limitations
- On open-ended Level 3 tasks, all models fall well below human expert performance
- Multi-step contextual reasoning remains a persistent challenge for current LLMs

## Why This Works (Mechanism)
EngiBench works by systematically isolating different aspects of engineering reasoning through hierarchical task design and controlled problem variants. The benchmark separates knowledge retrieval from complex reasoning, allowing clear identification of where models fail. By introducing perturbations and knowledge enhancements, it reveals whether failures stem from shallow pattern matching or genuine reasoning limitations. The open-ended modeling tasks expose the gap between current LLM capabilities and expert-level problem solving.

## Foundational Learning
- Hierarchical problem design - needed to systematically test reasoning progression from basic to complex tasks; quick check: verify three distinct difficulty levels are clearly separated
- Controlled problem variants - needed to isolate specific reasoning capabilities; quick check: ensure perturbations are minimal but effective
- LLM-as-a-judge methodology - needed for evaluating open-ended responses; quick check: validate judge consistency across similar responses

## Architecture Onboarding

Component map: Problem generation -> Controlled variants creation -> Model evaluation -> Performance analysis -> Human expert comparison

Critical path: Problem generation and variant creation must precede all evaluation, as these form the foundation for systematic testing. The evaluation pipeline must maintain consistency across all difficulty levels and variant types to ensure valid comparisons.

Design tradeoffs: The benchmark prioritizes systematic evaluation over scale, using 50 test cases to ensure controlled variation rather than comprehensive coverage. This allows precise isolation of reasoning capabilities but may limit generalizability.

Failure signatures: Sharp performance drops on perturbed problems indicate shallow pattern matching rather than robust reasoning. Consistent failure on multi-step problems reveals limitations in contextual reasoning. Large gaps between model and human performance on open-ended tasks indicate missing high-level reasoning capabilities.

First experiments:
1. Test model performance on basic Level 1 problems to establish baseline knowledge retrieval
2. Evaluate robustness by comparing performance on original vs. perturbed problems
3. Assess multi-step reasoning by analyzing step-by-step solution accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale of 50 test cases may limit generalizability across engineering domains
- Reliance on GPT-4o-mini as evaluation judge for open-ended responses may introduce bias
- Exact methodology for establishing human expert performance baseline is not fully detailed

## Confidence

**High confidence**: Claims about LLM performance degradation on perturbed problems and multi-step reasoning tasks

**Medium confidence**: Claims about the benchmark's ability to isolate reasoning capabilities

**Medium confidence**: Comparisons to human expert performance

## Next Checks
1. Expand test case coverage to include more diverse engineering domains and problem types
2. Validate LLM-as-a-judge results using multiple independent evaluators
3. Test benchmark performance with emerging reasoning-focused models and chain-of-thought prompting variations