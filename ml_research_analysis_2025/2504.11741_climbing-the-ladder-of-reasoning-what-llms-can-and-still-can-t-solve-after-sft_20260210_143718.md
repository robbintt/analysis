---
ver: rpa2
title: 'Climbing the Ladder of Reasoning: What LLMs Can-and Still Can''t-Solve after
  SFT?'
arxiv_id: '2504.11741'
source_url: https://arxiv.org/abs/2504.11741
tags:
- questions
- each
- problem
- reasoning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how supervised fine-tuning (SFT) enhances
  language models'' reasoning capabilities on AIME24 math problems. The authors discover
  a "ladder-like" structure in problem difficulty, categorizing questions into four
  tiers: Easy, Medium, Hard, and Extremely Hard (Exh).'
---

# Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?

## Quick Facts
- arXiv ID: 2504.11741
- Source URL: https://arxiv.org/abs/2504.11741
- Authors: Yiyou Sun; Georgia Zhou; Hao Wang; Dacheng Li; Nouha Dziri; Dawn Song
- Reference count: 40
- Primary result: SFT with 500-1K R1-style trajectories enables ~90% accuracy on Medium-level math problems, but Hard-level problems plateau at ~65% despite logarithmic scaling

## Executive Summary
This paper analyzes how supervised fine-tuning (SFT) enhances language models' reasoning capabilities on AIME24 math problems, revealing a "ladder-like" structure in problem difficulty across four tiers: Easy, Medium, Hard, and Extremely Hard. The authors discover that base models can achieve Medium-level performance (P≥90%) with minimal SFT (500-1K R1-style trajectories), but Hard-level problems present a fundamental plateau (~65%) due to compounding errors across sequential reasoning steps. Exh-level questions requiring unconventional strategies uniformly fail at 0% accuracy regardless of scaling.

## Method Summary
The authors systematically evaluate Qwen2.5-32B-Instruct models trained with varying amounts of SFT data from OpenR1-Math-220k (R1-style trajectories on NuminaMath1.5), testing on AIME24. They employ two metrics: avg@8 (average pass rate over 8 samples) and cov@8 (coverage, at least one success). Training uses LR=1e-5, weight decay=1e-4, batch size=32, 5 epochs. The analysis stratifies problems into difficulty tiers based on multi-model performance and examines scaling behavior, dataset curation effects, and solution strategy convergence across different training conditions.

## Key Results
- 500-1K R1-style trajectories enable base models to achieve ~90% accuracy on Medium-level problems regardless of math domain
- Hard-level problems follow logarithmic scaling with accuracy plateauing at ~65%, driven by multiplicative step failure
- Carefully curated small-scale datasets offer only ~1% performance advantage over random sampling
- SFT models adopt similar solution strategies regardless of training data diversity, converging on familiar approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small-scale SFT (500-1K R1-style trajectories) enables base models to solve Medium-level math problems, achieving ~90% accuracy regardless of specific math domain
- Mechanism: SFT activates latent cognitive behaviors (verification, backtracking, subgoal setting) present in Qwen2.5 base models by teaching extended chain-of-thought with self-reflection patterns. The training signal is primarily *stylistic*—models learn to "reason aloud" rather than acquiring domain-specific knowledge
- Core assumption: The base model already possesses sufficient mathematical knowledge; SFT unlocks expression of reasoning rather than injecting new capabilities
- Evidence anchors:
  - [abstract] "progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances)"
  - [section 2.3] Performance P≥90% on Medium-level achieved with P=f(C=*,N>500,L=nm/lg,S=R1)—independent of math category C
  - [corpus] Advancing Mathematical Reasoning paper confirms two-stage training (pre-training + post-training) paradigm effectiveness
- Break condition: Short trajectories (<normal length), fewer than 500 samples, or non-R1 style (e.g., Gemini-style) reduces accuracy below threshold

### Mechanism 2
- Claim: Hard-level problems follow logarithmic scaling with accuracy plateauing at ~65%, driven by compounding errors across sequential reasoning steps
- Mechanism: Multi-step reasoning creates multiplicative failure modes—overall success = ∏s_i where each step's success probability compounds. More SFT data improves *stability* (consistency across attempts) but cannot overcome fundamental step-level error rates. Small-scale SFT models match DeepSeek-R1's *potential* (cov@8) but lack *reliability* (avg@8)
- Core assumption: Step-level errors are approximately independent; improving individual step accuracy has diminishing returns
- Evidence anchors:
  - [abstract] "Hard-level questions suffer from frequent model's errors at each step...accuracy plateauing at ~65% despite logarithmic scaling"
  - [section 2.4.1] Subquestion analysis shows multiplicative degradation (e.g., AIME #1: 100%×100%×100%×25% = 25% final accuracy)
  - [corpus] Learning What RL Can't paper suggests RL alone cannot overcome fundamental capability limitations
- Break condition: Exh-level problems requiring unconventional strategies show 0% accuracy regardless of scaling; external tools (STILL3) or RL (QwQ) needed to exceed plateau

### Mechanism 3
- Claim: Careful dataset curation yields only ~1% improvement; models trained on different math categories converge to similar solution strategies
- Mechanism: SFT primarily transfers *reasoning templates* rather than domain-specific heuristics. Models learn to apply familiar strategies (coordinate geometry, inclusion-exclusion) even when suboptimal—suggesting SFT refines existing reasoning modes rather than expanding strategic diversity
- Core assumption: Base model's strategic repertoire is bounded; SFT cannot induce genuinely novel problem-solving approaches
- Evidence anchors:
  - [abstract] "carefully curated small-scale datasets offer limited advantage (only ~1% performance difference)"
  - [section 2.3.1] Trajectory similarity scores: 50% "almost identical," 50% "mostly similar" across models trained on different math categories
  - [section 3, Table 2d] Curated vs. random dataset shows 56.0% vs. 55.7% accuracy (within standard deviation)
  - [corpus] Corpus lacks direct replication; related work focuses on data synthesis methods, not curation effects
- Break condition: Exh-level problems requiring "out-of-box" insights uniformly fail, suggesting current SFT cannot transfer unconventional strategies

## Foundational Learning

- **Concept: Chain-of-Thought Reasoning with Self-Reflection**
  - Why needed here: R1-style trajectories combine extended reasoning with explicit verification steps—this is the *causal mechanism* enabling Easy→Medium progression
  - Quick check question: Can you explain why short trajectories fail even when containing correct solutions?

- **Concept: Compounding Error in Sequential Reasoning**
  - Why needed here: Hard-level plateau emerges from multiplicative step failure—understanding this explains why simple data scaling has diminishing returns
  - Quick check question: If each of 4 reasoning steps has 90% accuracy, what's the maximum expected final accuracy?

- **Concept: Reasoning Style vs. Strategic Diversity**
  - Why needed here: SFT appears to teach *how to reason* (style) not *what strategies to apply* (diversity)—critical for understanding Exh-level limitations
  - Quick check question: Why might models trained on geometry and algebra produce nearly identical solutions to the same problem?

## Architecture Onboarding

- **Component map:**
  Base Model (Qwen2.5-32B-Instruct) -> SFT Data Selection {category, size N, trajectory length L, style S} -> Training {LR=1e-5, weight_decay=1e-4, batch=32, 5 epochs} -> Evaluation {avg@8, cov@8} -> Difficulty Stratification {Easy → Med → Hard → Exh}

- **Critical path:**
  1. Ensure base model has latent cognitive behaviors (Qwen2.5 recommended; Llama lacks verification/backtracking)
  2. Select R1-style trajectories (not Gemini); normal/long length; N≥500 for Medium, N≥10K for Hard
  3. Evaluate using *both* avg@8 (accuracy) and cov@8 (coverage) to distinguish stability vs. capability gaps

- **Design tradeoffs:**
  - **Scale vs. curation:** 2K random samples outperform 1K curated samples (38.5% vs 33.6% on Hard)—invest in scale first
  - **Potential vs. stability:** Small-scale SFT achieves R1-level coverage (cov@8) but 20%+ lower accuracy—accept instability or scale up
  - **SFT vs. RL/Tools:** Hard plateau (~65%) broken by RL (QwQ: 70.8%) or tools (STILL3: 72.9%)—SFT alone insufficient

- **Failure signatures:**
  - **Easy→Med failure:** Using Gemini-style trajectories, short trajectories, or N<500
  - **Hard plateau:** Accuracy stalls at 60-65% despite data scaling; models fail on computational substeps
  - **Exh-level collapse:** 0% accuracy across all SFT scales; models apply rigid strategies (coordinate geometry, inclusion-exclusion) inappropriately
  - **Curation illusion:** <2% improvement from similarity-based dataset selection vs. random sampling

- **First 3 experiments:**
  1. **Replicate threshold finding:** Train Qwen2.5-32B on exactly 500 vs. 200 R1-style trajectories; verify Medium accuracy crosses 90% only at N≥500
  2. **Test scaling law:** Train models at N={1K, 2K, 5K, 10K}; fit logarithmic curve to Hard-level accuracy; confirm plateau ~65%
  3. **Probe strategy convergence:** Train on geometry-only vs. algebra-only trajectories; compare solution similarity scores on shared test problems; expect >4/5 similarity rating

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific advantages do reinforcement learning methods offer over supervised fine-tuning for surpassing the ~65% accuracy plateau on Hard-level mathematical reasoning?
- Basis in paper: [explicit] Section 2.4.2 states: "Since the precise amount of data used in training Qwq-32B is not publicly available, understanding the specific advantages that RL methods offer over SFT remains an important open question for future research."
- Why unresolved: RL models (QwQ, STILL3) clearly outperform SFT-only models on Hard questions, but the paper cannot isolate whether gains come from training methodology, data volume, or other factors.
- What evidence would resolve it: Controlled experiments comparing SFT vs RL on matched data scales, with ablations isolating the contribution of reward signals, exploration strategies, and training dynamics.

### Open Question 2
- Question: Can higher-level reasoning capabilities, such as deploying unconventional or ingenious problem-solving strategies, be developed through supervised fine-tuning alone?
- Basis in paper: [explicit] Section 3 states: "a natural question arises regarding whether the higher-level intelligence (e.g., utilizing uncommon yet ingenious solutions) can be developed through SFT." This builds on Section 2.3.1's finding that "models trained via SFT adopt similar solution strategies."
- Why unresolved: The paper demonstrates SFT models converge on similar strategies regardless of training data diversity, but does not identify mechanisms to promote strategic creativity.
- What evidence would resolve it: Training interventions that explicitly reward strategy diversity or novelty, evaluated on whether models discover qualitatively different solution approaches on benchmark problems.

### Open Question 3
- Question: What training paradigms or architectural modifications are required to solve Extremely Hard (Exh-level) problems that depend on geometric intuition, strategic flexibility, or extended exploration?
- Basis in paper: [inferred] Sections 2.5 and Appendix H document comprehensive failure modes on Exh problems—rigid adherence to common strategies, deficiency in geometric intuition, and insufficient reasoning context—while demonstrating that scaling SFT data yields 0% accuracy on this tier.
- Why unresolved: The paper identifies the limitations but offers no path forward; scaling, curation, and extended SFT all fail to address these fundamental barriers.
- What evidence would resolve it: Methods that integrate visual-spatial representations, enforce exploration of alternative solution paths, or enable substantially longer coherent reasoning chains, tested specifically on Exh-level problems with fine-grained error analysis.

## Limitations

- The analysis relies heavily on AIME24 as the benchmark, which may not generalize to broader mathematical reasoning tasks
- Claims about dataset curation effectiveness (1% difference) are based on limited direct comparison data and similarity scores from GPT-4o-mini rather than systematic ablation studies
- The Hard-level plateau at ~65% is primarily inferred from scaling behavior rather than proven through alternative methodologies like probe training or error analysis decomposition

## Confidence

- **High Confidence**: The Easy→Medium progression mechanism (500-1K R1-style trajectories enabling ~90% accuracy) is well-supported by the empirical threshold behavior and is consistent with prior SFT literature
- **Medium Confidence**: The logarithmic scaling law for Hard-level problems and the ~65% plateau are plausible but could be influenced by dataset quality variations and evaluation noise across different SFT runs
- **Low Confidence**: The claim that curated datasets offer only ~1% advantage over random sampling needs stronger empirical validation, as the paper provides limited direct comparison data and the similarity-based curation methodology is not fully specified

## Next Checks

1. **Replicate the Medium threshold**: Systematically train models at exactly 200, 500, and 1000 R1-style trajectories to verify the 90% accuracy threshold only emerges at N≥500, controlling for trajectory length and style

2. **Test scaling law robustness**: Conduct additional scaling experiments at intermediate dataset sizes (3K, 7K, 15K) to confirm the logarithmic fit and determine if the Hard-level plateau is truly asymptotic or simply slow-growing

3. **Probe strategy diversity**: Train multiple models on highly curated domain-specific datasets (e.g., only geometry or only algebra) and conduct blinded solution comparison with R1 on shared test problems to quantify whether the 50% "almost identical" similarity score holds under stricter evaluation criteria