---
ver: rpa2
title: Gradient Projection onto Historical Descent Directions for Communication-Efficient
  Federated Learning
arxiv_id: '2511.05593'
source_url: https://arxiv.org/abs/2511.05593
tags:
- epochs
- diana
- communication
- projfl
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses communication efficiency in federated learning
  by proposing two novel algorithms, ProjFL and ProjFL+EF, that project local gradients
  onto a shared subspace spanned by historical descent directions. ProjFL is designed
  for unbiased compressors, while ProjFL+EF incorporates error feedback for biased
  compressors.
---

# Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning

## Quick Facts
- arXiv ID: 2511.05593
- Source URL: https://arxiv.org/abs/2511.05593
- Authors: Arnaud Descours; Léonard Deroose; Jan Ramon
- Reference count: 40
- Primary result: Reduces communication costs by up to 8× while maintaining accuracy comparable to state-of-the-art baselines in federated learning

## Executive Summary
This paper introduces ProjFL and ProjFL+EF, two novel algorithms for communication-efficient federated learning that project local gradients onto a shared subspace spanned by historical descent directions. By transmitting only a scalar and a compressed orthogonal component instead of full gradients, these methods achieve significant communication savings. ProjFL works with unbiased compressors while ProjFL+EF incorporates error feedback for biased compressors. Theoretical analysis establishes convergence guarantees across strongly convex, convex, and non-convex settings, and extensive experiments on MNIST and CIFAR-10 demonstrate up to 8× communication reduction without sacrificing accuracy.

## Method Summary
The proposed algorithms decompose each gradient into a component aligned with the average of K historical descent directions and an orthogonal component. The aligned component is transmitted as a single scalar α, while the orthogonal component is compressed using techniques like Top-k sparsification. Both client and server maintain identical K-length histories of descent directions, enabling efficient reconstruction without additional communication. ProjFL+EF adds error feedback to handle biased compressors by accumulating and reinjecting compression errors, preventing divergence while maintaining the projection mechanism's benefits.

## Key Results
- Achieves accuracy comparable to FedAvg baselines while reducing communication costs by up to 8×
- Demonstrates convergence guarantees for strongly convex, convex, and non-convex settings
- Works effectively with both unbiased (Rand-k, QSGD) and biased (Top-k) compressors
- Empirically validated on MNIST (LeNet-5) and CIFAR-10 (ResNet-20) with varying client counts (3, 10, 100, 1000)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Projecting gradients onto a shared subspace spanned by historical descent directions reduces communication while preserving optimization information.
- **Mechanism**: Instead of compressing the full gradient, ProjFL decomposes it into a component aligned with historical descent directions (transmitted as a scalar) and an orthogonal component (compressed). The aligned component is "free" in terms of communication since both client and server can reconstruct the direction from shared history.
- **Core assumption**: Gradients at consecutive iterations share sufficient directional alignment with historical descent directions, making the orthogonal component smaller than the full gradient.
- **Evidence anchors**:
  - [abstract]: "Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead."
  - [section 3.1]: "If gi^t+1 is nearly aligned with D̄i^t, then αi^t+1 captures nearly all the gradient information, and the compressed component is negligible."
  - [corpus]: Related work on gradient compression (EF21, DIANA) confirms that compressing smaller-magnitude residual vectors is more efficient than full gradients.

### Mechanism 2
- **Claim**: Error feedback (EF) with projection enables convergence guarantees for biased compressors that would otherwise diverge.
- **Mechanism**: ProjFL+EF accumulates compression error from the orthogonal component and reinjects it into subsequent updates. This prevents the compressor's systematic bias from accumulating unboundedly, as the error is eventually transmitted when it aligns with the projection direction.
- **Core assumption**: The error accumulator remains bounded; the projection subspace eventually captures accumulated error components.
- **Evidence anchors**:
  - [abstract]: "ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism."
  - [section 4.2, Theorem 2]: Convergence bounds include (1-δ) terms showing that error is controlled by compression quality parameter δ.
  - [corpus]: BiCoLoR and related bidirectional compression methods confirm EF is essential for biased compressor convergence.

### Mechanism 3
- **Claim**: Averaging K historical descent directions creates a more stable projection subspace than single-direction projection.
- **Mechanism**: Using K=3 historical directions (empirically selected) smooths noise in individual gradient estimates. The averaged direction D̄i^t is less sensitive to stochastic gradient variance than the most recent gradient alone.
- **Core assumption**: Historical directions remain approximately co-linear with near-future descent directions; the optimization trajectory has local directional consistency.
- **Evidence anchors**:
  - [section 5.3.2]: "In our experiments, we fix k = 0.01 and K = 3."
  - [Figure C1]: K=3 shows best trade-off between convergence speed and communication cost across datasets.

## Foundational Learning

- **Concept: Federated Learning Optimization Formulation**
  - **Why needed here**: Understanding that FL optimizes f(w) = (1/M)Σfi(w) with local objectives fi helps explain why gradient compression must preserve the aggregate update direction.
  - **Quick check question**: Can you explain why compression bias accumulates differently in FL than in single-node SGD?

- **Concept: Unbiased vs. Biased Compressors**
  - **Why needed here**: The distinction determines whether error feedback is required. Unbiased compressors (Rand-k, QSGD) have E[C(g)]=g; biased compressors (Top-k) do not, requiring EF for convergence.
  - **Quick check question**: Given Top-k retains largest-magnitude coordinates, why does this introduce bias rather than just reducing precision?

- **Concept: Gradient Decomposition and Orthogonal Projection**
  - **Why needed here**: The core operation g = αd + g⊥ where g⊥·d = 0. Understanding that ||g⊥|| ≤ ||g|| always holds, but ||g⊥|| << ||g|| requires directional alignment.
  - **Quick check question**: If the new gradient is orthogonal to all historical directions, what happens to the communication cost?

## Architecture Onboarding

- **Component map**:
  - Client: Gradient computation → Projection onto D̄ (compute α) → Compress orthogonal component → Transmit (α, compressed_orthogonal)
  - Server: Receive all (α_i, M_i) → Reconstruct D_i = α_i D̄ + M_i → Average and update weights → Broadcast new weights
  - Shared: Both maintain identical K-length history of descent directions

- **Critical path**:
  1. Initialization: w_0, D_i^0 = 0 for all clients
  2. Each round: All clients compute D̄ identically from K most recent D values
  3. Projection step: α_i^t+1 = (g_i^t+1 · D̄) / ||D̄||²
  4. Orthogonal component: g_⊥ = g_i^t+1 - α_i^t+1 D̄
  5. Compression: Apply Top-k or chosen compressor to g_⊥ only
  6. Update shared direction: D_i^t+1 = α_i^t+1 D̄ + C(g_⊥)

- **Design tradeoffs**:
  - **K (history length)**: Larger K → more stable projection but slower adaptation to direction changes. Paper finds K=3 optimal empirically.
  - **Compression rate k**: Lower k → more communication savings but higher noise ball radius in convergence. Paper uses k=0.01 (1% sparsity).
  - **ProjFL vs. ProjFL+EF**: ProjFL simpler but requires unbiased compressor; ProjFL+EF handles biased compressors but adds error state overhead.

- **Failure signatures**:
  - **Divergence with biased compressor + no EF**: Loss increases monotonically (use ProjFL+EF instead)
  - **Slow convergence with K too large**: Training progresses but requires many more epochs than baselines
  - **No communication benefit**: ||g_⊥|| ≈ ||g|| indicates gradients aren't aligning with history; consider smaller K or check data heterogeneity
  - **Memory growth**: Ensure K-length buffer is bounded; D_i vectors should be overwritten, not accumulated

- **First 3 experiments**:
  1. **Sanity check**: Run ProjFL with identity compressor (C=Id) on MNIST; should match FedAvg convergence exactly since ||g_⊥|| = 0 when no compression applied.
  2. **Ablation on K**: Compare K ∈ {1, 3, 5, 10} with Top-k (k=0.01) on CIFAR-10; expect K=3 to show best communication-to-accuracy ratio.
  3. **Biased vs. unbiased**: Compare ProjFL (Rand-k) vs. ProjFL+EF (Top-k) with same sparsity; ProjFL+EF should converge faster but both should reach similar final accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does projecting local gradients onto the full K-dimensional subspace spanned by historical descent directions improve convergence speed or accuracy compared to projecting onto the one-dimensional average? The proposed ProjFL algorithm currently reduces the communication overhead to a single scalar by averaging the last K directions into one vector, but it has not explored the trade-off of transmitting slightly more data to utilize the full subspace information.

- **Open Question 2**: Can ProjFL be successfully combined with optimization acceleration techniques, such as Nesterov momentum or heavy-ball methods, to further reduce communication rounds? The authors explicitly list "combined with acceleration techniques, such as those proposed in [33, 34]" as a promising direction for future work in Section 6.

- **Open Question 3**: Does applying bidirectional compression (compressing both client-to-server and server-to-client messages) preserve the stability and convergence guarantees of ProjFL in large-scale systems? Section 6 suggests that "for a large number of clients, one could apply bidirectional compression, as done in [21]."

## Limitations

- Limited ablation studies on history length K beyond K=3; optimal value likely dataset-dependent
- Analysis assumes IID data distribution; non-IID settings may affect gradient alignment and compression efficiency
- No comparison with state-of-the-art bidirectional compression methods like DIANA or L-SVRCD beyond FedAvg baselines
- Computational overhead of projection operations (O(d) per client) not fully characterized

## Confidence

- High confidence: Convergence guarantees for convex and non-convex settings, communication reduction claims
- Medium confidence: Empirical performance superiority claims due to limited baseline comparisons
- Medium confidence: K=3 optimal selection based on single dataset validation

## Next Checks

1. Test ProjFL+EF with varying K values (1, 3, 5, 10) on non-IID data partitions to assess robustness to data heterogeneity
2. Compare communication-to-accuracy trade-off against bidirectional compression baselines (DIANA, L-SVRCD) on same benchmarks
3. Measure per-round computational overhead of projection operations relative to communication savings across different model sizes