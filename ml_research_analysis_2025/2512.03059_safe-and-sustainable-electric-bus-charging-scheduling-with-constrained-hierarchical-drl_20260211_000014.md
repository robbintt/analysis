---
ver: rpa2
title: Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical
  DRL
arxiv_id: '2512.03059'
source_url: https://arxiv.org/abs/2512.03059
tags:
- time
- charging
- safe
- safety
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Electric Bus Charging Scheduling Problem
  (EBCSP) by proposing a safe Hierarchical Deep Reinforcement Learning (HDRL) framework.
  The problem involves optimizing charging schedules for electric buses to minimize
  operational costs while ensuring battery safety under uncertainties in electricity
  prices, renewable energy generation, and travel times.
---

# Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL

## Quick Facts
- arXiv ID: 2512.03059
- Source URL: https://arxiv.org/abs/2512.03059
- Reference count: 40
- One-line primary result: A safe Hierarchical Deep Reinforcement Learning framework optimizes electric bus charging schedules, outperforming baselines in cost and safety while enabling fast convergence.

## Executive Summary
This paper tackles the Electric Bus Charging Scheduling Problem (EBCSP) by formulating it as a Constrained Markov Decision Process (CMDP) with temporal abstraction via options. The proposed solution, DAC-MAPPO-Lagrangian, uses a hierarchical architecture where a centralized high-level agent allocates chargers and decentralized low-level agents determine charging power, all trained under the Centralized Training with Decentralized Execution (CTDE) paradigm. By integrating Lagrangian relaxation for safety constraint enforcement, the method dynamically balances cost minimization with battery safety, achieving superior performance compared to existing approaches on real-world data.

## Method Summary
The method builds a hierarchical CMDP environment for M electric buses and N chargers (N < M), using real-world electricity prices, PV generation, and transit schedules. The high-level PPO-Lagrangian agent selects charger allocation options that persist over multiple time steps, while low-level MAPPO-Lagrangian agents determine charging power for each bus. Centralized critics estimate state values for both operational reward and safety cost, with Lagrangian multipliers dynamically adjusting to enforce safety constraints. The architecture employs shared parameters for low-level agents to accelerate training while maintaining decentralized execution during real operation.

## Key Results
- DAC-MAPPO-Lagrangian outperforms MILP-D and AADP baselines in both operational cost minimization and safety compliance rates on real-world test data.
- The method achieves fast convergence speed while maintaining constraint satisfaction through dynamic Lagrangian multiplier adaptation.
- Experimental results on two fleet sizes (M=6, N=3 and M=20, N=10) demonstrate scalability and effectiveness of the hierarchical approach.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporal abstraction via options accelerates convergence by reducing high-level decision frequency while retaining granular low-level control.
- **Mechanism:** The architecture decouples charger allocation (high-level options) from power adjustment (low-level actions). High-level options persist over multiple time steps until a termination condition $\beta$ is met, preventing instability from re-allocating chargers at every minor fluctuation.
- **Core assumption:** Charger assignments are sticky and do not require per-minute re-evaluation, whereas power levels benefit from fine-grained adjustment.
- **Evidence anchors:**
  - [abstract] "...formulate EBCSP as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making."
  - [Section IV.A] "...options can be regarded as temporally extended 'actions,' which can last for multiple time steps... This temporal abstraction enables better exploration and faster convergence."
- **Break condition:** If the environment dynamics shift so rapidly that optimal charger allocation changes every single time step, the overhead of the option mechanism may hinder rather than help performance.

### Mechanism 2
- **Claim:** Lagrangian relaxation dynamically enforces safety constraints more effectively than fixed penalty weights by adapting to the cost of violation.
- **Mechanism:** Instead of a hard-coded penalty $\lambda_{safe}$, the system learns a Lagrange multiplier $\lambda$ that increases when safety constraints are violated and decreases when they are satisfied. This converts the constrained optimization into an unconstrained dual problem, balancing cost minimization against the risk of battery depletion automatically.
- **Core assumption:** Slater's condition holds, meaning a strictly feasible policy exists where safety violations are zero.
- **Evidence anchors:**
  - [abstract] "...integrates Lagrangian relaxation into the Double Actor-Critic framework... outperforms existing baselines in both cost minimization and safety compliance."
  - [Section V.B] "$\lambda_H$ can be viewed as a dynamic penalty coefficient that adjusts adaptively during training... This eliminates the need for manual tuning."
- **Break condition:** If the tolerance $d$ is set too low or the environment is too volatile for any policy to be safe, the multiplier may diverge or destabilize learning.

### Mechanism 3
- **Claim:** Centralized Training with Decentralized Execution (CTDE) solves the "curse of dimensionality" in fleet scaling by allowing agents to cooperate during training while acting independently.
- **Mechanism:** Low-level agents (buses) share parameters and are guided by a global critic that observes the full state during training. This allows them to learn cooperative behaviors (e.g., not overloading grid capacity) without requiring explicit communication channels or a centralized controller during real-time execution.
- **Core assumption:** The global value function can be adequately approximated by a centralized critic even with decentralized actors.
- **Evidence anchors:**
  - [abstract] "...learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm."
  - [Section V.C] "...all low-level agents share the same actor network... to expedite training. Additionally, a pair of centralized critic networks is used..."
- **Break condition:** If agents have strictly conflicting objectives or the environment is non-stationary beyond the critic's ability to model, decentralized policies may collapse.

## Foundational Learning

- **Concept: Constrained Markov Decision Process (CMDP)**
  - **Why needed here:** Standard RL maximizes reward, but bus scheduling requires hard limits (safety). CMDP separates "operational cost" (reward) from "safety violation" (cost/constraint), which is the mathematical foundation of the paper's solution.
  - **Quick check question:** Can you distinguish between the *objective function* (cost to minimize) and the *constraint function* (limit to satisfy) in this paper?

- **Concept: Options Framework (Temporal Abstraction)**
  - **Why needed here:** The system uses a hierarchy where high-level decisions (options) last for variable durations. Understanding this is critical to grasping how the "Double Actor-Critic" architecture functionsâ€”it isn't just two agents acting at the same frequency.
  - **Quick check question:** What triggers the termination of a high-level "option" in this architecture: a fixed timer or a learned termination condition $\beta$?

- **Concept: Policy Gradient Methods (PPO/MAPPO)**
  - **Why needed here:** The algorithm builds on Proximal Policy Optimization (PPO). Understanding the clipping mechanism and the "Actor-Critic" split is necessary to interpret the loss functions defined in Section V.
  - **Quick check question:** Why does the PPO algorithm use a clipped surrogate objective rather than a standard policy gradient?

## Architecture Onboarding

- **Component map:**
  - High-Level Agent (Centralized): Actor ($\mu$) -> selects option $\omega$ (charger allocation) -> Termination Network ($\beta$) -> decides if current option persists
  - Low-Level Agents (Decentralized, one per EB): Actor ($\pi_L$) -> selects power $p$ (continuous action) -> Shared Critics ($V_R, V_C$) -> estimate value of states for Reward and Cost
  - Dual Variables: $\lambda_H, \lambda_L$ (Lagrangian multipliers regulating safety)

- **Critical path:**
  1. Observe State $S_t$: Gather SoC, time, prices, PV data.
  2. High-Level Decision: Check termination $\beta$. If terminated, select new option $\omega$ via $\mu$.
  3. Low-Level Decision: Agents with active chargers select power $p$ via $\pi_L$.
  4. Environment Step: Execute actions, calculate operational cost and safety cost.
  5. Update Loop:
     - Update Critics ($V_R, V_C$) via MSE loss.
     - Update Lagrangian Multipliers ($\lambda$) via gradient ascent on constraint violation.
     - Update Actors using adjusted advantage functions (Advantage - $\lambda \times$ Cost).

- **Design tradeoffs:**
  - **Centralized vs. Decentralized High-Level:** The paper uses a centralized high-level agent for stability, but this could become a bottleneck for massive fleets (e.g., >100 buses).
  - **Lagrangian vs. Fixed Penalty:** The Lagrangian method automates tuning but adds hyperparameters (learning rate for $\lambda$) and potential oscillation risks.
  - **Shared Parameters:** Low-level agents share network weights. This speeds up learning but assumes all buses behave similarly (may fail if buses have vastly different battery health).

- **Failure signatures:**
  - **Constraint Oscillation:** Safety violation rate bounces between 0% and high values; likely $\lambda$ learning rate is too high or tolerance $d$ is too strict.
  - **Convergence to Zero Power:** Agents learn to never charge to avoid "switching costs" or complexity; reward shaping for service completion is likely insufficient.
  - **Option Sticking:** High-level agent never terminates options; check initialization of $\beta$ network.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run DAC-MAPPO-Lagrangian against MILP-D (Deterministic Oracle) on a small fleet (M=6). Confirm the gap is <5% as claimed in Table V.
  2. **Constraint Ablation:** Compare the "Lagrangian" version against a "Fixed Penalty" version. Vary the penalty weight in the fixed version to show that the Lagrangian version achieves comparable costs with lower manual tuning effort.
  3. **Stress Test:** Increase fleet size (M=20) and reduce chargers (N=5). Monitor if the "Shared Critic" can still guide decentralized agents effectively under resource contention.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the explicit integration of stationary Energy Storage Systems (ESS) impact the optimal charging schedules and grid interaction managed by the DAC-MAPPO-Lagrangian algorithm?
- **Basis in paper:** [explicit] Footnote 1 states that the authors restrict the model to PV, grid, and EBs, explicitly leaving "the explicit modeling of stationary ESS for future work."
- **Why unresolved:** The current system model treats EBs as the only mobile storage resources, omitting stationary storage which could provide additional flexibility but adds modeling complexity.
- **What evidence would resolve it:** An extension of the CMDP formulation including ESS state variables and constraints, followed by comparative simulation results against the current baseline.

### Open Question 2
- **Question:** Can adaptive constraint thresholds and online safety monitoring mechanisms further improve the practicality and safety guarantees of the proposed safe HDRL framework?
- **Basis in paper:** [explicit] The conclusion explicitly identifies the need to "explore adaptive constraint thresholds and online safety monitoring mechanisms" as future work.
- **Why unresolved:** The current method uses a fixed tolerance $d$ and relies on the Lagrangian relaxation method which allows occasional violations during training, potentially requiring more dynamic safety management for real-world deployment.
- **What evidence would resolve it:** Development of an adaptive threshold policy and empirical results demonstrating reduced safety violation rates compared to the fixed tolerance approach.

### Open Question 3
- **Question:** How effectively do the policies learned in the simulated environment transfer to real-world physical electric bus charging stations?
- **Basis in paper:** [inferred] The paper relies on a "simulated environment" based on historical data and notes the method allows constraint violations during exploration, which is acceptable in simulation but not physical systems.
- **Why unresolved:** While real-world data traces are used, the paper does not address the "sim-to-real" gap or validate the closed-loop control on physical hardware.
- **What evidence would resolve it:** Hardware-in-the-loop simulations or results from a pilot deployment on a physical bus fleet demonstrating safety compliance and cost reduction.

## Limitations

- Performance claims rely on comparisons against heuristic baselines without ablation studies isolating contributions of hierarchical structure, Lagrangian constraints, and CTDE.
- Scalability analysis is limited to only two fleet sizes (M=6, M=20), with computational complexity of the high-level centralized policy for large-scale deployments not quantified.
- Real-world validation uses a single transit agency's data (Guelph Transit Routes 17 & 18), limiting generalizability across different route patterns and operational constraints.

## Confidence

- **High Confidence:** The mechanism of using temporal abstraction via options to reduce decision frequency is well-established in HDRL literature and the paper's architecture follows standard CMDP formulations.
- **Medium Confidence:** The claim that Lagrangian relaxation outperforms fixed penalty weights for safety constraint enforcement is supported by general Safe RL theory but lacks direct comparative ablation results in the paper's experiments.
- **Low Confidence:** The assertion that CTDE solves the "curse of dimensionality" for fleet scaling is not empirically validated beyond the two test scenarios provided.

## Next Checks

1. **Ablation Study:** Compare DAC-MAPPO-Lagrangian against (a) non-hierarchical CMDP with fixed penalty constraints, (b) hierarchical version without Lagrangian relaxation, and (c) fully decentralized training to isolate each architectural contribution to performance.
2. **Scalability Stress Test:** Evaluate the algorithm on fleet sizes beyond M=20 (e.g., M=50, M=100) with proportional charger constraints to identify computational bottlenecks and verify if the centralized high-level policy remains tractable.
3. **Robustness Analysis:** Test the trained policies under unseen price patterns, PV generation profiles, and travel time distributions not present in the training data to assess generalization capability.