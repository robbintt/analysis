---
ver: rpa2
title: Towards Understanding the Shape of Representations in Protein Language Models
arxiv_id: '2509.24895'
source_url: https://arxiv.org/abs/2509.24895
tags:
- protein
- space
- proteins
- structure
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the lack of understanding of how protein
  language models (PLMs) transform the entire space of protein sequences and their
  interrelations, focusing on the geometry of representations in ESM2 models. The
  authors propose two approaches: using square-root velocity (SRV) representations
  and graph filtrations to compare protein structures and PLM embeddings.'
---

# Towards Understanding the Shape of Representations in Protein Language Models

## Quick Facts
- arXiv ID: 2509.24895
- Source URL: https://arxiv.org/abs/2509.24895
- Reference count: 6
- Primary result: ESM2 models show non-linear effective dimension patterns with expansion-contraction behavior, encoding protein structure optimally at intermediate layers rather than final ones

## Executive Summary
This paper addresses the gap in understanding how protein language models (PLMs) transform the entire space of protein sequences and their interrelations. The authors analyze the geometry of representations in ESM2 models using two novel approaches: square-root velocity (SRV) representations for comparing protein structures and PLM embeddings, and graph filtrations to study context lengths at which PLMs encode structural features. The study reveals that larger models exhibit dimension expansion in initial layers followed by contraction, and that PLMs preferentially encode local and immediate residue relations, with optimal structural encoding occurring in intermediate layers rather than the final layer.

## Method Summary
The method involves extracting ESM2 embeddings at each layer for proteins from the SCOP dataset, then analyzing the geometric properties of these representations. First, both 3D protein coordinates and PLM embeddings are interpolated into curves using quadratic splines and transformed into SRV representations to enable metric space analysis on an infinite-dimensional sphere. The Fréchet mean and effective dimension are computed using tangent PCA on the shape manifold. Second, graph filtrations are constructed by building k-nearest neighbor graphs for both 3D structures and embeddings at varying context lengths, with normalized graph filtration moments revealing which context scales are preferentially encoded. The analysis tracks layer-wise changes across four ESM2 model sizes.

## Key Results
- ESM2 models exhibit a non-linear effective dimension pattern: expansion in early layers followed by contraction in later layers, with larger models showing more pronounced peaks
- The Fréchet radius and effective dimension follow distinct layer-wise patterns that differ from baseline 3D structure analysis
- PLMs preferentially encode immediate (~2 neighbors) and local (~8 neighbors) residue relations, with optimal structural encoding occurring in intermediate layers rather than the final layer
- Larger models expand the effective dimension more than the 3D structure baseline, while smaller models stay below it

## Why This Works (Mechanism)

### Mechanism 1: SRV Shape Space Projection for Cross-Length Protein Comparison
- Claim: The square-root velocity (SRV) representation enables principled comparison of proteins with different numbers of amino acids by projecting interpolated curves onto an infinite-dimensional sphere
- Mechanism: Proteins are interpolated as curves γ: [0,1] → R³ (or R^m for embeddings), then transformed via q(t) = γ̇(t)/√‖γ̇(t)‖, which normalizes velocity magnitude. This maps all curves to S^∞, where geodesic distances are computed via L² norm after aligning rotations through SVD optimization: R̂ = argmin_R∈SO(n) ‖q₁ − Rq₂‖
- Core assumption: Assumption: Quadratic spline interpolation between residues preserves biologically meaningful shape structure; reparameterization invariance is unnecessary given consistent residue-based parameterization
- Evidence anchors:
  - [Section 2.1]: "this approach projects curves to an infinite-dimensional sphere S^∞, which makes computing geodesics, and thereby measuring distances, straightforward"
  - [Section 2.1]: "we quotient out the remaining actions of the SE(m) group, namely the rotations generated by SO(m). This is done using SVD"
  - [corpus]: Weak/missing—corpus papers focus on PLM applications rather than geometric shape analysis methods
- Break condition: If proteins with dramatically different lengths require different interpolation densities, SRV distances may reflect interpolation artifacts rather than biological shape similarity

### Mechanism 2: Dimension Expansion-Contraction Reflects Abstraction-to-Semantic Processing
- Claim: ESM2 models exhibit a non-linear effective dimension pattern—expansion in early layers (high abstraction) followed by contraction in later layers (semantic specialization)—with larger models showing more pronounced peaks
- Mechanism: Early layers project sequences into a high-dimensional shape space where many deformation modes separate proteins. Later layers collapse this space onto a low-dimensional manifold where few shape transformations suffice to distinguish representations. Effective dimension λ_eff = (Σλ_k)²/Σλ_k² is computed via tangent PCA at the Fréchet mean
- Core assumption: Assumption: The tangent space at the Fréchet mean adequately captures the intrinsic dimensionality of the curved shape manifold; this dimensionality reflects task-relevant representational capacity
- Evidence anchors:
  - [Abstract]: "Karcher mean and effective dimension of the SRV shape space follow a non-linear pattern as a function of the layers in ESM2 models of different sizes, with larger models exhibiting dimension expansion in initial layers and contraction later"
  - [Section 3.1]: "larger models expand the dimension more than the 3d structure baseline, whereas the smaller models stay below it"
  - [Section 4.1]: References Cheng et al. (2024) and Valeriani et al. (2023) showing similar patterns, suggesting "a general high-abstraction regime in the dimension-expansion phase and a specific semantically rich regime in the contraction phase"
  - [corpus]: Weak/missing—corpus does not discuss intrinsic dimensionality patterns in PLM layers
- Break condition: If the Fréchet mean computation converges to poor local minima on the curved manifold, tangent PCA will measure noise rather than true dimensionality

### Mechanism 3: Graph Filtrations Reveal Multi-Scale Structural Encoding
- Claim: PLMs encode protein structure preferentially at two context scales—very local (~2 neighbors) and moderately local (~8 neighbors)—with optimal encoding occurring in intermediate layers, not the final layer
- Mechanism: k-nearest neighbor graphs are constructed for both 3D protein structures and PLM embeddings at increasing k values. The normalized graph filtration moment E[d(P,φ(P))]_norm = E[d_A(P,φ(P))]/E[d_A(P,R)] compares structural similarity to random baselines. Bimodal curves indicate two preferred context lengths
- Core assumption: Assumption: Adjacency matrix distance captures biologically relevant structural correspondence; normalization by random point cloud distances provides a meaningful baseline
- Evidence anchors:
  - [Abstract]: "PLMs preferentially encode immediate as well as local relations between residues, but start to degrade for larger context lengths"
  - [Section 3.2]: "many of the curves have a bimodal shape throughout the filtration. This implies that PLM representations encode 3d protein structure at both a very local level, at about 2 neighbors, as well as at a slightly less local level, at about 8 neighbors"
  - [Section 4.2]: "structure is not optimally encoded in the last layer, but rather in the layers that immediately precede it"
  - [corpus]: Consistent with Structure-Aligned Protein Language Model (arxiv:2505.16896) finding that PLMs "lack structural knowledge essential for some biological applications"
- Break condition: If protein classes have fundamentally different optimal context lengths (as suggested by Alpha/Beta vs. small proteins), averaging across classes obscures class-specific patterns

## Foundational Learning

- Concept: **Riemannian manifolds and geodesic distance**
  - Why needed here: The SRV shape space is curved (a quotient of S^∞ by SO(m)), so standard Euclidean PCA fails; you must compute geodesics and use log maps to project to tangent spaces
  - Quick check question: Can you explain why computing the mean on a sphere requires iterative optimization rather than coordinate-wise averaging?

- Concept: **Fréchet mean and tangent PCA**
  - Why needed here: The paper computes the "center" of protein shapes on a curved manifold via gradient descent (Fréchet mean), then performs PCA in the tangent space to estimate effective dimension
  - Quick check question: How does tangent PCA differ from standard PCA, and why is the log map necessary?

- Concept: **Graph filtrations and topological data analysis**
  - Why needed here: Filtrations systematically vary a parameter (k-nearest neighbors) to study multi-scale structure; this reveals which context lengths PLMs encode best
  - Quick check question: If a filtration curve is monotonically decreasing, what does that imply about the similarity between two graph families?

## Architecture Onboarding

- Component map:
  - **Input pipeline**: SCOP protein structures → 3D coordinates + ESM2 embeddings (L × d tensors per protein)
  - **SRV transformation**: Quadratic spline interpolation (1000 points) → SRV curves → rotation alignment → shape space points
  - **Geometric metrics**: Fréchet radius r_F = E[d(y_i, p_F)], effective dimension λ_eff via tangent PCA eigenvalues
  - **Graph filtration pipeline**: k-NN graphs at k ∈ {1, 2, ..., K} for both structures and embeddings → adjacency matrix distances → normalized moments
  - **Analysis layer**: Layer-wise tracking across 4 ESM2 model sizes (8M to 650M parameters)

- Critical path:
  1. Extract ESM2 embeddings at each layer for all proteins
  2. Interpolate both 3D coordinates and embeddings to curves (1000 points)
  3. Compute SRV representations and align rotations
  4. Compute Fréchet mean via gradient descent on shape manifold
  5. Project to tangent space, compute PCA, extract effective dimension
  6. Separately: build k-NN graphs, compute filtration moments, identify optimal k and layer

- Design tradeoffs:
  - **Interpolation density vs. computational cost**: 1000 points provides smooth curves but increases memory; fewer points may miss fine structure
  - **Rotation alignment via SVD vs. closed-form solutions**: SVD is robust but O(n³) per protein pair; closed-form exists for specific cases
  - **Normalization by random baselines vs. absolute distances**: Random baselines enable cross-model comparison but add variance; absolute distances are more stable but harder to interpret
  - **Layer selection for downstream tasks**: Intermediate layers show best structural encoding but final layers are standard for most applications

- Failure signatures:
  - **Fréchet mean non-convergence**: If gradient descent oscillates or diverges, check learning rate and initialization; try multiple random seeds
  - **Effective dimension exceeds ambient dimension**: Indicates numerical instability in tangent PCA; verify eigenvalues sum correctly
  - **Filtration moment > 1 for all k**: PLM representations are no better than random; check embedding extraction pipeline
  - **No bimodal pattern in filtrations**: May indicate model hasn't learned structure or protein class is fundamentally different

- First 3 experiments:
  1. **Reproduce Fréchet radius and effective dimension curves** for ESM2-650M on a small SCOP subset (50 proteins, 4 classes). Validate that you observe the expansion-contraction pattern and identify the peak layer.
  2. **Ablate interpolation density** (100, 500, 1000, 2000 points) on 20 proteins. Check stability of effective dimension estimates and computational time scaling.
  3. **Extract embeddings from layer with optimal structural encoding** (identified via filtration) and from final layer. Train a simple contact prediction head on both. Compare accuracy to test if intermediate layers genuinely improve downstream performance as the paper suggests.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the interpretable geometric transformations that describe the differences between protein representation shapes in the low-dimensional subspace observed in later PLM layers?
- Basis in paper: [explicit] The authors state, "Understanding the precise nature of these transformations would be an exciting direction for future work" regarding the sharp reduction in dimensionality in later layers
- Why unresolved: While the paper measures the effective dimensionality of the shape space, it does not identify the semantic or structural meaning of the specific axes of variation (transformations) that define this space
- What evidence would resolve it: An analysis identifying basis vectors in the tangent space of the shape manifold and mapping them to physical protein deformations or specific structural motifs

### Open Question 2
- Question: Which specific structural or functional features of protein classes (e.g., Alpha/Beta vs. Small proteins) determine the degree to which their 3D structure is encoded by PLMs?
- Basis in paper: [explicit] The authors note that representation similarity varies by class and state, "understanding what features of these protein classes determine how much of the structure is encoded by PLMs is an exciting direction for future research"
- Why unresolved: The paper establishes that structural encoding fidelity varies across the SCOP classes but does not isolate the features (e.g., contact density, secondary structure prevalence) causing this variance
- What evidence would resolve it: A systematic ablation or correlation study linking specific protein features (hydrophobicity, solvent accessibility, etc.) to the graph filtration moment metrics

### Open Question 3
- Question: Does initializing folding models (like ESMFold) on intermediate layers with optimal structural encoding, rather than the final layer, improve folding performance?
- Basis in paper: [explicit] The abstract and discussion suggest that "training a folding model ontop of these layers might lead to improved folding performance" because the optimal encoding occurs close to, but before, the last layer
- Why unresolved: The paper provides geometric evidence that intermediate layers are more structurally faithful, but does not empirically validate if this geometric advantage translates into better downstream folding accuracy
- What evidence would resolve it: Benchmarking the RMSD and TM-scores of protein structure prediction models trained on embeddings from layer $N-k$ compared to those trained on the final layer $N$

## Limitations
- The paper does not identify which specific structural or functional features of protein classes determine their encoding fidelity by PLMs
- The geometric analysis provides evidence but does not empirically validate whether intermediate layers with optimal structural encoding improve downstream folding performance
- The study uses a fixed interpolation density (1000 points) without analyzing sensitivity to this parameter choice

## Confidence

| Claim | Confidence |
|-------|------------|
| ESM2 models exhibit non-linear effective dimension patterns | High |
| Larger models show more pronounced dimension expansion peaks | High |
| PLMs encode structure optimally at intermediate layers | Medium |
| Graph filtrations reveal bimodal encoding patterns at specific context lengths | Medium |
| SRV representations enable meaningful cross-length protein comparison | Medium |

## Next Checks
1. Verify the expansion-contraction pattern in effective dimension across ESM2 model sizes using Fréchet radius and tangent PCA on a small protein subset
2. Check sensitivity of effective dimension estimates to interpolation density by comparing results at 100, 500, 1000, and 2000 points
3. Validate that embeddings from intermediate layers with optimal structural encoding improve contact prediction accuracy compared to final layer embeddings