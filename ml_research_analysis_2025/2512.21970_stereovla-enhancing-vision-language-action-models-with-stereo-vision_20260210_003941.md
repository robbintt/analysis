---
ver: rpa2
title: 'StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision'
arxiv_id: '2512.21970'
source_url: https://arxiv.org/abs/2512.21970
tags:
- stereo
- arxiv
- depth
- vision
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StereoVLA integrates stereo vision into vision-language-action
  models by extracting geometric features from stereo image pairs using FoundationStereo
  and semantic features from monocular views using SigLIP and DINOv2, then fusing
  them into hybrid visual tokens. It also introduces an auxiliary Interaction-Region
  Depth Estimation task that focuses depth prediction on the gripper-object interaction
  area to enhance spatial perception and accelerate convergence.
---

# StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision

## Quick Facts
- **arXiv ID:** 2512.21970
- **Source URL:** https://arxiv.org/abs/2512.21970
- **Reference count:** 40
- **Primary result:** 33% higher success rate than baselines on robotic manipulation with stereo vision

## Executive Summary
StereoVLA enhances Vision-Language-Action (VLA) models by integrating stereo vision for improved spatial perception in robotic manipulation tasks. The method fuses geometric features from stereo image pairs (via FoundationStereo) with semantic features from monocular views, creating hybrid visual tokens that provide both precise spatial information and semantic context. An auxiliary Interaction-Region Depth Estimation task focuses depth prediction on gripper-object interaction areas, accelerating convergence and improving manipulation precision. Evaluated on a Franka robot arm, StereoVLA achieves 33% higher success rates than monocular baselines under stereo settings while demonstrating strong robustness to camera pose variations.

## Method Summary
StereoVLA processes stereo image pairs by extracting geometric features from a filtered cost volume using FoundationStereo and semantic features from SigLIP and DINOv2 on the left view. These features are spatially pooled to 14-pixel stride and fused channel-wise through an MLP projector. The fused features feed into an InternLM-1.8B backbone with a 300M action expert using flow-matching for delta end-effector pose prediction. The model is trained on 5M synthetic trajectories from MuJoCo/Isaac Sim with auxiliary tasks including interaction-region depth estimation, 2D bounding box prediction, and keyframe pose prediction. The training uses a 5:2:2:1 loss ratio (action:depth:bbox:pose) over 160k steps.

## Key Results
- Achieves 33% higher success rate than monocular baselines under stereo vision settings
- Maintains 61.3% success under large camera pose randomization compared to 24.1% for multi-view monocular approaches
- Demonstrates faster convergence with interaction-region depth estimation versus uniform depth sampling
- Shows robustness to camera pose variations while maintaining precision for small object manipulation

## Why This Works (Mechanism)

### Mechanism 1: Geometric-Semantic Feature Fusion
The Geometric-Semantic Feature Extraction module fuses dense geometric cost volumes with semantic features to improve spatial grounding. By extracting a filtered cost volume ($V'_c$) from FoundationStereo and combining it with high-level semantic features from SigLIP and DINOv2, the model gains both precise spatial cues and semantic context. Table I validates that this fusion yields 77.0% success rate, outperforming other feature representations.

### Mechanism 2: Interaction-Region Depth Estimation
The auxiliary depth supervision focused on the interaction region accelerates convergence by restricting depth prediction to areas around the gripper and target object. This forces the model to allocate representational capacity to task-critical spatial relationships rather than background geometry. Figure 6 shows this approach achieves higher success rates faster than uniform sampling, which negatively impacts early training.

### Mechanism 3: Stereo Vision Robustness
Stereo configurations provide better robustness to camera pose variations than multi-view monocular setups because the fixed relative geometry of stereo cameras maintains consistent depth priors. Table II demonstrates StereoVLA maintains 61.3% success under large pose randomization, whereas multi-view approaches like Front+Side GraspVLA drop to 24.1%.

## Foundational Learning

- **Cost Volumes in Stereo Matching**
  - Why needed here: StereoVLA uses filtered cost volumes rather than final depth maps to preserve geometric density
  - Quick check: Can you explain why a 4D cost volume captures more geometric information than a 2D disparity map? (Answer: It retains probability distributions over depths/disparities rather than collapsing to a single "best guess")

- **Auxiliary Tasks / Multi-task Learning**
  - Why needed here: Interaction-Region Depth Estimation shapes the visual encoder's features without increasing inference cost
  - Quick check: Why does the depth task use an interaction-region mask rather than standard dense depth loss? (Answer: To focus model capacity on gripper-object relationships and avoid gradient noise from irrelevant backgrounds)

- **Action Chunking with Flow Matching**
  - Why needed here: StereoVLA uses flow-matching action expert instead of standard autoregressive token prediction
  - Quick check: How does predicting a "delta end-effector pose" via flow matching differ from predicting discretized position tokens? (Answer: Flow matching allows continuous, smooth trajectory generation in one shot rather than step-by-step)

## Architecture Onboarding

- **Component map:** Stereo Pair ($I_L, I_R$) → FoundationStereo (Geometric branch) + SigLIP/DINOv2 (Semantic branch) → Spatial Pooling → Channel Concatenation → MLP Projector → InternLM-1.8B Backbone → Action Expert + Depth Head
- **Critical path:** Alignment of FoundationStereo features to 14-pixel stride of semantic features is critical; misalignment breaks spatial reasoning required for grasping
- **Design tradeoffs:** Channel concatenation reduces token count (efficiency) but requires spatial alignment; synthetic data enables perfect depth ground truth but introduces Sim-to-Real gap
- **Failure signatures:** Early gripper closing indicates depth perception failure; grasping wrong object suggests semantic grounding failure; small object failure (<2cm) expected at 224×224 resolution
- **First 3 experiments:** 1) Visualize spatially pooled geometric features alongside semantic attention maps to verify interaction region highlighting, 2) Train with uniform vs. interaction-region depth sampling to reproduce Figure 6 convergence curves, 3) Systematically vary camera extrinsics to confirm Table II robustness claims

## Open Questions the Paper Calls Out
- **Trade-off optimization:** How to balance high input resolution for small object perception with computational efficiency
- **Long-horizon dependencies:** Extending the framework to capture temporal dependencies for complex multi-stage tasks
- **Full stereo pipeline evaluation:** Impact of including FoundationStereo's iterative refinement module on high-precision tasks
- **Generalization to different embodiments:** Effectiveness across diverse robot morphologies beyond the Franka arm

## Limitations
- Limited to single robot embodiment (Franka arm), with generalization to other robots unexplored
- 224×224 resolution creates hard limit on minimum object size (2cm), affecting small object manipulation
- Performance evaluation relies primarily on synthetic data, with real-world deployment data limited to qualitative demo
- Interaction-region depth estimation depends on accurate object detection, with failure modes under detection errors unexplored

## Confidence
- **High Confidence:** Geometric-semantic fusion effectiveness (33% improvement) well-supported by Table I comparisons
- **Medium Confidence:** Camera pose robustness claims supported by Table II but tested within narrow baseline variations
- **Low Confidence:** Real-world performance claims limited by primarily synthetic data evaluation

## Next Checks
1. Deploy trained model on real robot with Zed Mini camera to measure actual Sim-to-Real performance gap
2. Systematically induce object detection failures during interaction-region depth estimation to quantify performance degradation
3. Train models at 224×224, 448×448, and 896×896 resolutions to empirically validate 2cm size limitation and quantify resolution-compute trade-offs