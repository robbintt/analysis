---
ver: rpa2
title: What to Do Next? Memorizing skills from Egocentric Instructional Video
arxiv_id: '2507.02997'
source_url: https://arxiv.org/abs/2507.02997
tags:
- action
- memory
- planning
- actions
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces interactive action planning, a task where
  an agent must plan high-level goal-oriented actions in a simulation environment
  from an egocentric perspective. The key challenge is to generate action sequences
  that account for the current environment and are robust to deviations during execution.
---

# What to Do Next? Memorizing skills from Egocentric Instructional Video

## Quick Facts
- **arXiv ID**: 2507.02997
- **Source URL**: https://arxiv.org/abs/2507.02997
- **Reference count**: 37
- **Primary result**: Proposes topological affordance memory for interactive action planning from egocentric video, achieving 64% LCS on VirtualHome

## Executive Summary
This paper introduces interactive action planning, a task where an agent must plan high-level goal-oriented actions in a simulation environment from an egocentric perspective. The key challenge is to generate action sequences that account for the current environment and are robust to deviations during execution. The authors propose a novel approach that combines a topological affordance memory with a transformer architecture. The memory module learns to associate visual observations with environment affordances and goals, enabling efficient retrieval of relevant past experiences. A replanning algorithm based on adversarial attack is introduced to handle action deviations. The method is evaluated on the VirtualHome environment, demonstrating significant improvements in terms of LCS and executability compared to baseline methods, with robust performance even under attack scenarios.

## Method Summary
The proposed method combines a Topological Affordance Memory (TAM) with a transformer-based planner. TAM consists of three components: a localization network that maps visual observations to a learned topological space using a Siamese ResNet18 architecture, an affordance learning module that encodes visual-action relationships using R2+1D18 with contrastive loss, and a goal association module that conditions action planning on specific goals using a discriminator network. The transformer decoder generates action sequences auto-regressively with cross-attention to retrieved memory nodes. A replanning algorithm based on iterative target class method handles deviations by maximizing goal association scores through gradient-based updates. The method is trained on egocentric instructional videos from VirtualHome and evaluated on four settings: pure-text, vis-static, vis-interactive, and vis-interactive-attack.

## Key Results
- Achieves 64% LCS on pure-text setting, outperforming pixel-based localization (24%) and non-goal-conditioned methods (33%)
- Demonstrates 64% executability rate in vis-interactive setting, maintaining performance under adversarial attacks
- Shows robust replanning capability, recovering from action deviations to achieve 64% LCS compared to 37% without replanning

## Why This Works (Mechanism)
The topological affordance memory addresses the core challenge of interactive action planning by learning a continuous space where visually similar environments with similar affordances are close together. This enables efficient retrieval of relevant past experiences when planning new actions. The goal association component ensures that retrieved actions are relevant to the specific task, while the replanning algorithm provides robustness against execution errors. By combining visual perception with high-level planning in a unified framework, the method can adapt to novel environments while maintaining task-oriented behavior.

## Foundational Learning
- **Topological affordance memory**: A learned continuous space mapping visual observations to environment affordances and goals, needed to retrieve relevant past experiences for planning in novel environments. Quick check: Verify that similar visual scenes cluster together in the learned representation using t-SNE visualization.
- **Contrastive learning for affordance encoding**: Uses InfoNCE loss to learn representations where actions with similar affordances are close together across different episodes, needed to generalize action skills to new environments. Quick check: Test retrieval accuracy on held-out action categories.
- **Siamese network for localization**: Trains a ResNet18 to map stacked frame observations to a topological space where temporally close frames are close in representation, needed for efficient memory retrieval. Quick check: Visualize embedding distances between consecutive and distant frames.
- **Transformer cross-attention with memory retrieval**: Combines auto-regressive action generation with attention to retrieved memory nodes, needed to incorporate past experiences into current planning. Quick check: Verify attention weights focus on relevant memory nodes for each generated action.
- **Iterative target class replanning**: Uses gradient-based updates to modify actions when goal association scores fall below threshold, needed to recover from execution deviations. Quick check: Test replanning success rate under varying attack strengths.
- **Goal-conditioned action planning**: Conditions action generation on task descriptions through goal association module, needed to ensure planned actions are relevant to specific objectives. Quick check: Compare performance with and without goal conditioning on multi-task scenarios.

## Architecture Onboarding

**Component Map**: [o^s_t, o^e_t] stacked frames → Localization Network → Topological Space → K-nearest memory nodes → Affordance Encoder + Goal Association → Cross-attention → Transformer Decoder → Action sequence

**Critical Path**: Visual observation → Topological Affordance Memory retrieval → Goal-conditioned action generation → Execution with replanning

**Design Tradeoffs**:
- Learned topological space vs. pixel-based distance: Reduces noise from visual variations but requires additional training
- Goal-conditioned vs. unconditioned planning: Improves task relevance but adds complexity to training
- Replanning with adversarial attack vs. passive recovery: Provides robustness but increases computational cost

**Failure Signatures**:
- Poor localization leading to irrelevant memory retrieval (LCS drops to 24-33%)
- Goal association failure causing task-irrelevant actions (executability drops)
- Replanning instability when gradient updates overshoot (recovery fails)

**First Experiments**:
1. **Ablation: Localization baseline** - Replace learned localization with pixel-based distance and measure LCS degradation
2. **Ablation: Goal conditioning** - Train without goal association and evaluate task relevance on multi-goal scenarios
3. **Stress test: Replanning limits** - Apply increasing attack strengths and measure replanning success rate

## Open Questions the Paper Calls Out
- Can integrating a "bad" memory module containing failed cases improve the agent's ability to avoid undesirable actions in noisy real-world scenarios?
- How robust is the localization network in distinguishing visually similar but semantically different locations (visual aliasing) as the environment scale increases?
- Can the proposed Topological Affordance Memory generalize to real-world egocentric video domains without reliance on simulation-based training?

## Limitations
- Requires perfect expert demonstrations for training, limiting real-world applicability
- Evaluation restricted to VirtualHome simulation environment, not tested on real-world egocentric videos
- Several critical architectural details and hyperparameters remain unspecified

## Confidence
- **High confidence**: Problem formulation and evaluation methodology are clearly defined
- **Medium confidence**: Core TAM architecture design is well-described with appropriate loss functions
- **Low confidence**: Exact implementation details and hyperparameter choices are underspecified

## Next Checks
1. **Ablation study validation**: Verify that the proposed method outperforms the pixel-based localization baseline (LCS 64% vs 24%) and that replanning provides the stated improvement (64% vs 37% under attack). Test whether goal-conditioned goal association is necessary by comparing with a baseline without goal conditioning.
2. **Memory retrieval visualization**: Implement the localization network and visualize retrieved memory nodes using t-SNE or similar dimensionality reduction. Confirm that visually similar environments and affordances produce similar representations, validating the learned topological affordance memory.
3. **Robustness testing**: Evaluate the replanning algorithm under various attack scenarios beyond those presented. Test whether the iterative gradient-based update (n^{k+1}_t ← clip(n^k - α·sign(∇_n Loss))) consistently recovers from deviations and whether performance degrades gracefully as attack strength increases.