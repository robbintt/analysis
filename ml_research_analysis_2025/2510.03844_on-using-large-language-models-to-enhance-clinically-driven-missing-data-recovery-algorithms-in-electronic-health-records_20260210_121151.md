---
ver: rpa2
title: On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery
  Algorithms in Electronic Health Records
arxiv_id: '2510.03844'
source_url: https://arxiv.org/abs/2510.03844
tags:
- data
- missing
- health
- chart
- electronic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A roadmap-driven algorithm based on ICD-10 codes was developed
  to recover missing electronic health record data, mimicking expert chart reviews.
  The roadmap was iteratively refined using large language models to expand auxiliary
  diagnoses.
---

# On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records

## Quick Facts
- arXiv ID: 2510.03844
- Source URL: https://arxiv.org/abs/2510.03844
- Reference count: 40
- Primary result: LLM-augmented ICD-10 roadmap algorithm recovered as much or more missing EHR data than expert reviewers in 1000-patient study

## Executive Summary
This paper presents a clinically-driven algorithm for recovering missing electronic health record data using large language models to enhance ICD-10 code-based roadmaps. The approach mimics expert chart reviews by defining phenotypic criteria and identifying auxiliary diagnoses that increase likelihood of meeting those criteria. Through iterative refinement with LLM assistance, the roadmap expands to include relevant auxiliary diagnoses that would be labor-intensive to identify manually. The algorithm was validated against expert reviewers on 1000 patients, demonstrating comparable or superior performance in data recovery while offering a scalable, cost-effective alternative to manual chart review.

## Method Summary
The study developed a roadmap-driven algorithm using ICD-10 codes to recover missing EHR data by mimicking expert chart review processes. Researchers defined specific phenotypic criteria (e.g., rheumatoid arthritis, major depressive disorder, stroke) and used LLMs to iteratively expand lists of auxiliary diagnoses that increase likelihood of meeting those criteria. The algorithm matches patient ICD-10 codes against these expanded roadmaps to recover missing data. The approach was tested on 1000 patients, comparing algorithmic recovery performance against expert reviewers across multiple roadmap versions, with the LLM playing a key role in expanding and refining the auxiliary diagnosis lists.

## Key Results
- Algorithm recovered as much or more missing data than expert reviewers, depending on roadmap version
- LLM-assisted roadmap refinement expanded auxiliary diagnoses beyond manual identification capabilities
- The approach offers scalable, cost-effective method for improving EHR data completeness without extensive manual chart review

## Why This Works (Mechanism)
The algorithm leverages structured ICD-10 coding systems to create clinically-driven recovery patterns. By defining phenotypes through explicit criteria and using LLMs to identify auxiliary diagnoses that correlate with those phenotypes, the approach creates a systematic method for inferring missing information. The iterative refinement process allows the roadmap to capture complex clinical relationships that would be difficult to identify through manual review alone, while maintaining clinical validity through the underlying ICD-10 framework.

## Foundational Learning
- **ICD-10 coding system**: International classification of diseases, 10th revision - why needed: provides standardized clinical vocabulary for phenotype definition; quick check: verify codes match clinical concepts
- **Phenotype definition**: Specific clinical criteria for conditions - why needed: establishes the target for data recovery; quick check: ensure criteria are clinically valid and complete
- **Auxiliary diagnosis concept**: Secondary conditions that increase likelihood of primary phenotype - why needed: enables inference of missing data through clinical relationships; quick check: validate auxiliary-diagnostic relationships with clinical expertise
- **Roadmap algorithm**: Structured approach matching patient codes against phenotype criteria - why needed: provides systematic framework for automated recovery; quick check: test algorithm on known cases with complete data
- **LLM-assisted refinement**: Using language models to expand diagnosis lists - why needed: overcomes manual limitations in identifying complex clinical relationships; quick check: evaluate relevance of LLM-suggested diagnoses
- **Data completeness vs. plausibility**: Different dimensions of data quality - why needed: distinguishes between missing and erroneous data; quick check: clarify which quality dimension the algorithm addresses

## Architecture Onboarding
- **Component map**: Phenotypes -> ICD-10 codes -> LLM refinement -> Roadmap algorithm -> Data recovery -> Validation
- **Critical path**: Define phenotypes → Generate initial auxiliary diagnoses → LLM refinement → Algorithm implementation → Validation against experts
- **Design tradeoffs**: Structured codes (reliability) vs. unstructured text (completeness); manual review (accuracy) vs. automation (scalability); single phenotype version (simplicity) vs. multiple versions (precision)
- **Failure signatures**: Incomplete auxiliary diagnosis lists → missed recoveries; overly broad LLM suggestions → false positives; poor phenotype definition → systematic errors; single-site validation → limited generalizability
- **First experiments**: 1) Test algorithm on patients with known complete data to verify recovery accuracy; 2) Compare LLM-suggested auxiliary diagnoses against clinical expert consensus; 3) Evaluate performance across different EHR systems and clinical contexts

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can this clinically-driven algorithm approach be extended to monitor and correct other dimensions of data quality, such as plausibility, rather than just completeness?
- Basis in paper: [explicit] The Discussion states, "Extending them to monitor other dimensions of data quality (e.g., plausibility) is a promising future direction."
- Why unresolved: The current study focused exclusively on the "completeness" dimension by recovering missing values; it did not test the algorithm's ability to identify or correct implausible (erroneous) values.
- What evidence would resolve it: A modified version of the roadmap algorithm applied to detect known erroneous values (e.g., biologically impossible vital signs) with sensitivity and specificity metrics.

### Open Question 2
- Question: Does training an LLM specifically on medical terminology and ICD-10 codes improve the validity of generated search terms compared to general-purpose models like Gemini?
- Basis in paper: [explicit] The Discussion notes that "Training an LLM specifically on ICD-10 codes and medical terminology... could improve the validity... potentially reducing the noise inherent in broad keyword generation."
- Why unresolved: The study utilized a general-purpose model (Gemini-2.5-Flash) for accessibility, leaving the performance delta with domain-specific models unknown.
- What evidence would resolve it: A comparative study evaluating the clinical relevance and matching accuracy of search terms generated by general-purpose versus medically fine-tuned LLMs.

### Open Question 3
- Question: Does applying the LLM directly to unstructured free-text notes (rather than structured ICD-10 codes) significantly increase the recovery rate of missing data?
- Basis in paper: [explicit] The Discussion suggests that "text-mining unstructured fields (like free-text notes) could uncover additional sources of auxiliary information" beyond the structured data used in the study.
- Why unresolved: The current implementation relied solely on structured ICD-10 codes; the utility of unstructured text remains untested in this context.
- What evidence would resolve it: An evaluation of an LLM-based text miner on patient notes to see if it identifies valid auxiliary diagnoses missed by the ICD-10 matching algorithm.

### Open Question 4
- Question: Does incorporating multiple versions of the phenotype (extracted EHR, chart review, and algorithmic) into statistical models reduce bias or improve precision compared to using a single version?
- Basis in paper: [explicit] The Discussion posits that "Incorporating multiple versions of the same phenotype... instead could reduce bias and improve precision in statistical models."
- Why unresolved: The study replaced missing values with the algorithmic output but did not analyze the statistical benefit of retaining and utilizing multiple phenotype versions simultaneously.
- What evidence would resolve it: Statistical simulations or analyses comparing the error rates and confidence intervals of models using single-source versus multi-source phenotype integration.

## Limitations
- Validation conducted on single dataset of 1000 patients, limiting generalizability across different EHR systems and clinical contexts
- Comparison to expert reviewers was retrospective rather than prospective, with potential variability in "recovered data" interpretation
- LLM-assisted roadmap refinement process lacks detailed documentation of specific prompts and refinement cycles used
- Performance comparison across roadmap versions presented without statistical significance testing

## Confidence
- High: The core concept of using ICD-10-based roadmaps for structured data recovery is sound and methodologically clear
- Medium: The comparative performance claims against expert reviewers are reasonable but require independent replication
- Medium: The scalability and cost-effectiveness assertions are logical extensions but not empirically validated across multiple institutions

## Next Checks
1. Prospective validation: Apply the algorithm to new patient cohorts and compare real-time recovery results with ongoing expert chart reviews
2. Multi-site replication: Test the roadmap algorithm across different EHR systems and healthcare institutions to assess generalizability
3. Statistical rigor: Conduct formal statistical analysis of performance differences between roadmap versions and expert reviewers, including confidence intervals and effect sizes