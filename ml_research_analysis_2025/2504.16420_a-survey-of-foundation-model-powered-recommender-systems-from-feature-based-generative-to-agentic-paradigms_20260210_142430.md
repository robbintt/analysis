---
ver: rpa2
title: 'A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based,
  Generative to Agentic Paradigms'
arxiv_id: '2504.16420'
source_url: https://arxiv.org/abs/2504.16420
tags:
- recommendation
- user
- language
- data
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of Foundation Models
  (FMs) for Recommender Systems (RS), examining three integration paradigms: Feature-Based,
  Generative, and Agentic. It highlights how FMs enhance recommendation tasks like
  Top-N, sequential, conversational, and cross-domain recommendation, while addressing
  challenges in data representation, fairness, multimodal integration, and system
  efficiency.'
---

# A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms

## Quick Facts
- arXiv ID: 2504.16420
- Source URL: https://arxiv.org/abs/2504.16420
- Reference count: 40
- Foundation Models enhance recommender systems across three paradigms: Feature-Based, Generative, and Agentic

## Executive Summary
This survey provides a comprehensive overview of how Foundation Models (FMs) are transforming recommender systems (RS) through three distinct integration paradigms. The Feature-Based paradigm leverages FMs as high-quality feature extractors, the Generative paradigm uses FMs to directly produce recommendations or explanations, and the Agentic paradigm enables autonomous, interactive recommendation agents. The survey examines applications across Top-N, sequential, conversational, and cross-domain recommendation tasks while addressing critical challenges in data representation, fairness, multimodal integration, and system efficiency.

## Method Summary
The survey systematically categorizes FM4RecSys research along three paradigms and application scenarios. It analyzes the integration methods, architectural designs, and evaluation frameworks for each paradigm. The authors synthesize findings from 40 references to identify current trends, technical challenges, and future research directions. The survey includes empirical comparisons of generative models and discusses theoretical frameworks for agentic systems, while acknowledging limitations in real-world deployment studies.

## Key Results
- Three integration paradigms identified: Feature-Based (FM as feature extractor), Generative (FM produces recommendations directly), and Agentic (autonomous recommendation agents)
- Generative models show superior performance in diverse recommendation environments compared to traditional approaches
- Agentic models demonstrate potential for interactive scenarios but face significant real-world deployment challenges
- Major challenges include long-sequence modeling, multimodal fusion, and ensuring privacy and fairness in FM4RecSys applications

## Why This Works (Mechanism)
Foundation Models work in recommender systems by leveraging their pre-trained knowledge to capture complex user-item interactions, semantic relationships, and behavioral patterns. Their large-scale pretraining enables them to understand context, generate explanations, and adapt to diverse recommendation scenarios. The integration paradigms allow different levels of FM utilization: Feature-Based approaches extract rich representations from user-item interactions, Generative approaches directly produce recommendations using FM's generation capabilities, and Agentic approaches create autonomous systems that can interact with users and make decisions independently.

## Foundational Learning
**Foundation Models**: Large-scale models pretrained on massive corpora that can be adapted for downstream tasks through fine-tuning or prompting.
*Why needed*: Provide strong generalization capabilities and rich semantic understanding for complex recommendation scenarios.
*Quick check*: Verify if FM4RecSys approaches demonstrate improved performance on out-of-distribution data compared to traditional methods.

**Prompt Engineering**: Designing input prompts to guide FMs toward specific recommendation behaviors without extensive fine-tuning.
*Why needed*: Enables efficient adaptation of FMs to recommendation tasks while preserving their generalization capabilities.
*Quick check*: Compare recommendation quality across different prompting strategies and few-shot examples.

**Retrieval-Augmented Generation (RAG)**: Combining FMs with retrieval mechanisms to access external knowledge or long interaction histories beyond context limits.
*Why needed*: Addresses the fixed context window limitation of FMs when dealing with life-long user interaction sequences.
*Quick check*: Measure performance improvements when incorporating retrieved relevant items into recommendation generation.

## Architecture Onboarding
**Component Map**: User Interaction Data -> FM Backbone (Pretrained Model) -> Integration Layer -> Recommendation Output -> Evaluation Metrics
**Critical Path**: User-Item Interaction History → FM Processing → Recommendation Generation → User Feedback → System Update
**Design Tradeoffs**: Static FMs offer computational efficiency but lack real-time adaptation, while dynamic adaptation requires expensive fine-tuning. Multimodal integration improves recommendation quality but increases system complexity. Large context windows improve performance but limit scalability.

**Failure Signatures**:
- Poor recommendation quality when user interaction history exceeds FM context window
- Inconsistent recommendations across similar user queries due to FM stochasticity
- Performance degradation when domain shifts significantly from pretraining distribution
- Privacy violations when FM memorizes sensitive user information

**First Experiments**:
1. Compare Top-N recommendation accuracy between FM-based feature extraction and traditional collaborative filtering on MovieLens dataset
2. Evaluate sequential recommendation performance using RAG-based approaches vs. standard transformer models on Amazon review data
3. Test conversational recommendation effectiveness with different prompting strategies on dialogue-based datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers effectively detect and mitigate data pollution to ensure evaluation benchmarks were not inadvertently used during Foundation Model pre-training?
- Basis in paper: [explicit] Section 9.4.1 identifies "data pollution" as a significant challenge, noting that ensuring evaluation data has not been used during FM training is difficult in the current setting.
- Why unresolved: The sheer scale and opacity of pre-training corpora make it nearly impossible to verify if specific benchmark samples were memorized by the model, threatening evaluation validity.
- Evidence: Development of rigorous decontamination protocols or "clean" benchmark datasets with verifiable exclusion from pre-training corpora.

### Open Question 2
- Question: How can static Foundation Models be adapted to handle drastic, real-time shifts in user preference within a single session and evolve across sessions?
- Basis in paper: [explicit] Section 9.2.5 notes that current LLMs are static (producing similar outputs for identical inputs), whereas RecSys require dynamic adaptation within sessions and behavioral evolution over time.
- Why unresolved: Standard fine-tuning is often too resource-intensive for real-time adaptation, and static model weights fail to capture ephemeral user intent without external memory or state mechanisms.
- Evidence: Architectures demonstrating dynamic adaptation without weight updates, or novel memory mechanisms that successfully capture evolving user intent in longitudinal studies.

### Open Question 3
- Question: How can FM4RecSys overcome fixed context window limitations to effectively model and reason over life-long user interaction sequences?
- Basis in paper: [explicit] Section 9.3.1 highlights that long user interaction histories often exceed FMs' context capacity, leading to less effective recommendations. Section 9.2.3 also posits Retrieval-Augmented Generation (RAG) as a potential solution to this context constraint.
- Why unresolved: Transformers have quadratic complexity with sequence length, and naive truncation or summarization often discards fine-grained temporal signals essential for sequential recommendation.
- Evidence: Frameworks that successfully utilize RAG or hierarchical memory to process interaction sequences exceeding 10,000 items with high recall.

## Limitations
- Empirical comparisons rely on limited benchmark datasets that may not capture real-world complexity and diversity
- Agentic paradigm section remains largely theoretical with few validated real-world implementations
- Privacy and fairness considerations discussed qualitatively without comprehensive empirical validation
- Rapid evolution of foundation models means some approaches may already be outdated

## Confidence
- Feature-Based Paradigm: High - Well-established integration method with extensive empirical validation
- Generative Paradigm: Medium - Strong theoretical foundation but limited real-world deployment studies
- Agentic Paradigm: Low - Largely theoretical with few practical implementations to validate claims

## Next Checks
1. Conduct empirical benchmarking of representative models from each paradigm across diverse recommendation tasks and domains to verify comparative performance claims
2. Implement and test agentic recommendation systems in real-world deployment scenarios to validate their practical effectiveness and identify deployment challenges
3. Evaluate the efficiency and scalability of foundation model integration approaches on industrial-scale datasets to assess real-world feasibility