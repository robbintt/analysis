---
ver: rpa2
title: 'MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation'
arxiv_id: '2601.01753'
source_url: https://arxiv.org/abs/2601.01753
tags:
- merging
- mergerec
- domains
- data
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MergeRec, a model merging framework designed
  for data-isolated cross-domain sequential recommendation. The key idea is to construct
  a universal recommender system without sharing raw user interaction data across
  domains.
---

# MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2601.01753
- Source URL: https://arxiv.org/abs/2601.01753
- Reference count: 40
- Key outcome: MergeRec achieves up to 17.21% improvement in Recall@10 over existing model merging methods for cross-domain sequential recommendation without sharing raw data

## Executive Summary
MergeRec introduces a model merging framework for cross-domain sequential recommendation that operates without sharing raw user interaction data across domains. The approach constructs a universal recommender by initializing with task vectors, creating pseudo-user data from single-item sequences, and optimizing merging weights through a joint recommendation and distillation objective. Extensive experiments on eight Amazon datasets demonstrate consistent improvements over existing methods and strong generalizability to unseen domains.

## Method Summary
MergeRec merges K fine-tuned domain models into a single universal recommender without accessing cross-domain user data. The framework initializes a merged model using task vectors (parameter differences between fine-tuned and pre-trained models), constructs pseudo-user data by treating each item as a single-item sequence, and optimizes merging weights through a joint objective combining recommendation loss and knowledge distillation loss. The method can operate with domain-wise or layer-wise weight optimization and demonstrates effective transfer of collaborative filtering knowledge while preserving ranking performance.

## Key Results
- Achieves average improvements of up to 17.21% in Recall@10 over existing model merging methods
- Demonstrates superior generalizability to unseen domains compared to Task Arithmetic and TIES baselines
- Shows robust performance under data-scarce conditions across eight Amazon datasets
- Layer-wise merging provides slight advantages over domain-wise merging for some backbone architectures

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Arithmetic for Domain Knowledge Consolidation
Parameter differences between fine-tuned and pre-trained models encode transferable domain-specific collaborative filtering patterns. Task vectors (τ_k = θ_k - θ_base) capture the directional shift needed to specialize in each domain, and linear combinations with learned weights integrate multiple domains without interference from raw data overlap.

### Mechanism 2: Pseudo-User Probes Elicit Latent CF Knowledge
Single-item sequences serve as sufficient probes to extract collaborative filtering signals from frozen domain models without exposing real user data. Each item i ∈ I_k becomes a pseudo-user sequence [i], revealing the conditional distribution P(i'|i) over related items and encoding local co-consumption patterns.

### Mechanism 3: Dual-Objective Distillation Captures Multi-Intent User Behavior
Entropy minimization alone fails for recommender systems because it only amplifies top-1 confidence. A joint distillation+recommendation loss preserves ranking diversity: KL divergence aligns merged model with teacher distributions (capturing multi-intent patterns), while recommendation loss uses teacher's top-1 predictions as hard pseudo-labels to sharpen ranking.

## Foundational Learning

- **Task Vectors (Model Merging)**
  - Why needed here: MergeRec's entire premise is combining domain-specific adaptations without data. Task vectors provide the mathematical object for this combination.
  - Quick check question: Given a pre-trained model θ_base and fine-tuned model θ_A, what does τ_A = θ_A - θ_base represent operationally?

- **Knowledge Distillation (Teacher-Student Alignment)**
  - Why needed here: The L_KD loss uses domain-specific models as teachers. Understanding soft labels and temperature scaling is essential for implementing the distillation component.
  - Quick check question: Why might a student model trained with soft labels (KL divergence) generalize differently than one trained with hard labels?

- **Collaborative Filtering Signals in Sequential Models**
  - Why needed here: The paper argues CF knowledge is "encapsulated" in fine-tuned models and extractable via pseudo-users. Distinguishing sequence modeling from CF patterns clarifies what's being transferred.
  - Quick check question: In a sequential recommender, what's the difference between modeling sequential transitions (i_t → i_{t+1}) vs. collaborative co-consumption (users who liked i also liked j)?

## Architecture Onboarding

- **Component map:** Pre-trained base model + K fine-tuned domain models → Task vectors computation → Merging initialization → Pseudo-user construction → Collaborative merging optimization → Single merged model
- **Critical path:** Ensure all domain models share identical architecture and were initialized from the same θ_base; verify pseudo-user sequences correctly index into domain-specific item vocabularies; monitor both L_KD and L_Rec during optimization
- **Design tradeoffs:** Domain-wise vs. layer-wise merging (K weights vs. K×L weights); λ balance for distillation vs. ranking discrimination; temperature T for distillation softening
- **Failure signatures:** Merged model underperforms all fine-tuned models (conflicts in task vectors); L_KD decreases but L_Rec plateaus (distillation dominant); performance varies across domains (weights not converging)
- **First 3 experiments:** (1) Reproduce single-backbone, two-domain merge with RecFormer-base on Arts and Beauty; (2) Ablate pseudo-user design comparing single-item vs. multi-item sequences; (3) Stress test on unseen domain training on 5 domains and evaluating on held-out domain

## Open Questions the Paper Calls Out

- **Open Question 1:** Does constructing longer or multi-item pseudo-user sequences significantly improve the transfer of sequential dependency knowledge compared to the single-item sequences currently used? The current approach treats single items as sequences to simulate cold-start users, but this simplification fails to capture complex transition dynamics or high-order co-occurrence patterns found in real interaction logs.

- **Open Question 2:** Can the MergeRec framework be adapted for ID-based sequential recommendation models where a shared semantic pre-trained backbone is unavailable? The method relies on Task Vectors derived from text-based PLMs and does not address ID-based models where domains have disjoint vocabularies and lack a shared semantic initialization.

- **Open Question 3:** Is domain-wise or layer-wise scalar weight optimization sufficient to suppress negative transfer when merging domains with highly conflicting user behaviors? The paper discusses the "negative transfer problem" but proposes optimizing only scalar weights, which may not be robust enough to handle strong interference without granular parameter masking.

## Limitations

- The framework assumes all domains share the same item vocabulary, which may not hold in real-world deployments where domains have non-overlapping catalogs
- The assumption that task vectors capture "disentangled" domain-specific patterns without interference is largely untested across domains with overlapping item sets or conflicting user preferences
- Single-item pseudo-sequences may under-represent complex transition dynamics and high-order co-occurrence patterns found in real interaction logs

## Confidence

- High confidence: The merging mechanism works when domains are sufficiently compatible and the dual-objective distillation provides measurable improvements over entropy-only approaches
- Medium confidence: The pseudo-user construction reliably extracts CF knowledge across all domains (particularly for sequence-dependent patterns)
- Medium confidence: Layer-wise merging provides consistent advantages over domain-wise merging across all backbone architectures

## Next Checks

1. Test pseudo-user quality by comparing single-item probes against random 2-3 item sequences on domains with known CF patterns (measure recall gap)
2. Conduct controlled interference experiments: merge models trained on overlapping item sets with deliberately conflicting preferences to measure weight divergence
3. Validate cross-vocabulary generalization: merge models trained on disjoint item sets, then evaluate on a domain containing items from multiple source domains