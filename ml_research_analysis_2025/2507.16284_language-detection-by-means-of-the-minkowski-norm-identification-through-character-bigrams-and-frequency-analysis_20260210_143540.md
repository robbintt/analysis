---
ver: rpa2
title: 'Language Detection by Means of the Minkowski Norm: Identification Through
  Character Bigrams and Frequency Analysis'
arxiv_id: '2507.16284'
source_url: https://arxiv.org/abs/2507.16284
tags:
- language
- texts
- frequency
- character
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a non-AI-based language detection method using
  character n-grams and frequency analysis. The approach leverages monograms and bigrams
  frequency rankings derived from linguistic research, combined with diacritic pattern
  evaluation.
---

# Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis

## Quick Facts
- arXiv ID: 2507.16284
- Source URL: https://arxiv.org/abs/2507.16284
- Authors: Paul-Andrei Pogăcean; Sanda-Maria Avram
- Reference count: 23
- Non-AI method achieves >80% accuracy for texts <150 chars, 100% for longer texts across four datasets

## Executive Summary
This paper presents a non-AI-based language detection method using character n-grams and frequency analysis. The approach leverages monograms and bigrams frequency rankings derived from linguistic research, combined with diacritic pattern evaluation. The method achieves over 80% accuracy for texts shorter than 150 characters and 100% accuracy for longer texts across four diverse datasets including Romanian stories, multilingual parallel corpora, and fairy tales. The algorithm employs a Minkowski norm-based scoring system that integrates character frequency vectors, bigram distributions, and diacritic occurrence thresholds.

## Method Summary
The method uses a Minkowski norm-based scoring system that combines three features: character frequency distributions, bigram patterns, and diacritic occurrence thresholds. For each language, reference profiles are compiled with top 25 characters and top 10 bigrams. Input text is scored by matching its top 10 characters against each language's reference list (weighted by position), adding bigram occurrence scores, and applying diacritic bonuses (100 points for 5-10% diacritics, 200 points for >10%). The language with the highest total score is selected. The approach is implemented in C++ for speed, achieving sub-0.5ms/KB processing time.

## Key Results
- Over 80% accuracy for texts shorter than 150 characters using character and bigram analysis
- 100% accuracy for longer texts when combining all features (characters + bigrams + diacritics)
- Perfect accuracy across all datasets (ROST, FTDS, OPUS, LDDS) with the unified scoring approach
- Processing speed under 0.5ms per KB of text, eliminating GPU dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character frequency distributions alone can distinguish many languages when compared via a norm-based scoring system.
- Mechanism: Extract top 10 characters from input text; match against pre-built reference profiles (top 25 per language) with scores weighted by rank position; sum produces language-specific scores via Minkowski-inspired ℓ₁ norm.
- Core assumption: Languages have sufficiently distinct character frequency signatures that remain stable across genres and text lengths.
- Evidence anchors:
  - [abstract] Reports "over 80% accuracy on texts shorter than 150 characters" using monograms and bigrams derived from linguistic research.
  - [Section 3.3.2] Empirical testing found top-10 character threshold balances accuracy and robustness; top-25 reference list provides ≥25! permutations for discrimination.
  - [corpus] Related work "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts" validates n-gram effectiveness for Romanian text classification, though focused on authorship rather than language detection.
- Break condition: Short informal texts with missing diacritics or slang deviate from reference profiles, reducing accuracy (Section 3.3.3).

### Mechanism 2
- Claim: Bigram frequency patterns resolve ambiguities where monogram distributions overlap across languages.
- Mechanism: For each language, compile top-10 bigram list with scores 1–10; sum occurrences in input text; add to character frequency score. This captures sequential character patterns that single characters miss.
- Core assumption: Bigram distributions provide orthogonal discriminatory signal to monograms, particularly for closely related languages or diacritic-stripped texts.
- Evidence anchors:
  - [abstract] Method "reaches 100% accuracy for longer texts" when combining features.
  - [Section 4.6] Table 3 shows Method 1 (characters + diacritics) at 80–88% accuracy vs. Method 2 (+bigrams) at 100% across all datasets; bigrams critical for "noisy data" and "archaic texts."
  - [corpus] "Detection of LLM-Generated Java Code Using Discretized Nested Bigrams" demonstrates bigram efficacy for code attribution, suggesting transferability of n-gram patterns to classification tasks—though different domain.
- Break condition: Assumption: Languages with highly overlapping bigram profiles (untested in paper's 5-language scope) may require trigram or longer sequences.

### Mechanism 3
- Claim: Diacritic occurrence thresholds provide decisive signal for languages with distinctive orthographic markers.
- Mechanism: If diacritic proportion exceeds 5% → +100 bonus; if >10% → +200 bonus. Bonus dominates scoring for short texts; becomes negligible for long texts where frequency signal is stronger.
- Core assumption: Diacritics are preserved in formal texts and appear at language-specific rates that exceed thresholds.
- Evidence anchors:
  - [Section 4.1] Diacritic bonus "help determine the language of shorter texts (fewer than 150 characters), where statistical reliability of character frequency is otherwise limited."
  - [Section 4.5] Dutch (≤2% diacritics) initially misclassified without bigrams; Romanian benefits from diacritic signal.
  - [corpus] Corpus evidence for diacritic-specific mechanisms is weak; no directly comparable diacritic threshold approaches found in neighbor papers.
- Break condition: Informal texts with missing diacritics (common in Romanian digital communication) lose this signal; archaic texts may use different diacritic conventions.

## Foundational Learning

- Concept: **Minkowski/ℓₚ Norm Distance Metrics**
  - Why needed here: The unified scoring function (Eq. 1) is a norm-based combination of character and bigram vectors; understanding norm properties (definiteness, homogeneity, triangle inequality) is required to verify mathematical soundness.
  - Quick check question: If you double all input frequencies, does the score double? (It should—this is homogeneity.)

- Concept: **N-gram Language Profiles**
  - Why needed here: The algorithm depends on pre-compiled frequency rankings (monograms: top 25, bigrams: top 10) per language; these are the reference vectors against which input is scored.
  - Quick check question: Why might a top-5 character list fail where top-25 succeeds? (Fewer permutations → higher collision probability across languages.)

- Concept: **Statistical Reliability vs. Text Length**
  - Why needed here: Short texts (<150 chars) have noisy frequency estimates; the paper addresses this via diacritic bonuses and proves that noise operations reduce expected ℓ₁ norm (Section 3.3.3).
  - Quick check question: What happens to expected frequency score when a high-frequency vowel is deleted from a short text? (Expected change is negative; detection accuracy degrades.)

## Architecture Onboarding

- Component map:
  1. Preprocessing: Normalize text, extract characters and bigrams
  2. Reference Store: Per-language profiles (top-25 chars, top-10 bigrams, diacritic thresholds)
  3. Scoring Engine: Compute ∥(X,F)∥ per language using Eq. 1 + diacritic bonus
  4. Decision Layer: Argmax over language scores

- Critical path: Text → Character/Bigram extraction → Frequency vector construction → Norm computation per language → Diacritic bonus addition → Max score selection

- Design tradeoffs:
  - Top-10 vs. top-N characters: Paper empirically found 10 optimal for accuracy/robustness tradeoff (Figure 1)
  - Bigram list size: Limited to 10 for "bounded, interpretable contributions" and to prevent score inflation
  - Diacritic bonus magnitude (100/200): Chosen to dominate short-text scoring without overwhelming long-text frequency signal
  - C++ implementation: Chosen for speed over Python; paper claims <0.5ms/KB processing (Section 4.8)

- Failure signatures:
  - Short informal texts without diacritics → low confidence scores across all languages
  - Mixed-language text (code-switching) → scores split between languages, no clear argmax
  - Historical/archaic texts with obsolete spellings → frequency profiles mismatched to modern references
  - Logographic scripts (Chinese, Japanese) → current UTF-16 implementation fails (Section 5.2)

- First 3 experiments:
  1. **Baseline validation**: Replicate Method 1 vs. Method 2 comparison on LDDS dataset; confirm 80% → 100% accuracy delta when adding bigrams to Romanian stories with/without diacritics.
  2. **Noise robustness test**: Apply synthetic noise operations (vowel deletion, substitution, symbol corruption) to clean texts at 5%, 10%, 20% rates; measure accuracy degradation vs. theoretical ℓ₁ norm reduction (Section 3.3.3 proof).
  3. **Language expansion pilot**: Add a 6th language (e.g., Spanish or Portuguese); compile top-25 character and top-10 bigram profiles; test whether monogram+bigram+diacritic combination maintains high accuracy, or if new ambiguities emerge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the method scale effectively to highly inflectional (e.g., Finnish) or logographic (e.g., Chinese) languages without extensive feature engineering?
- Basis in paper: [explicit] Section 4.7 states that the primary limitation is restricted language coverage and that expanding to logographic or highly inflectional languages "may require additional feature engineering."
- Why unresolved: The empirical validation was limited to five languages (Romanian, English, German, etc.) utilizing Latin scripts, leaving the algorithm's efficacy on diverse linguistic structures unproven.
- What evidence would resolve it: Empirical accuracy results from testing the current algorithm on Finnish and Chinese corpora, followed by an analysis of any necessary modifications to the bigram or scoring components.

### Open Question 2
- Question: Is migrating to UTF-32 sufficient to generalize the algorithm to Cyrillic scripts and non-Latin alphabets?
- Basis in paper: [explicit] Section 5.2 identifies encoding constraints (current UTF-16 implementation) as a barrier that excludes languages like Chinese and Cyrillic scripts, proposing "Future work will implement UTF-32 support."
- Why unresolved: The paper identifies the encoding limitation but does not demonstrate whether simply changing the encoding standard resolves the identification challenges for these scripts.
- What evidence would resolve it: Successful detection of Russian and Greek texts using a UTF-32 implementation of the proposed Minkowski norm scoring system.

### Open Question 3
- Question: How robust is the unified scoring formula against intra-sentence code-switching?
- Basis in paper: [inferred] Section 2.3 notes that statistical methods are often challenged by "code-switching," and Section 3.3.3 confirms that noise (typos, non-standard forms) reduces the expected norm value.
- Why unresolved: While the paper claims robustness against "mixed-language content" in Section 4.5, it does not provide specific metrics or testing methodologies focused on complex code-switching scenarios common in informal text.
- What evidence would resolve it: Evaluation results on a dataset specifically containing sentences with alternating language varieties (e.g., Spanglish or mixed-script texts) to test if the scoring thresholds remain accurate.

## Limitations

- The method's performance depends on pre-built reference frequency profiles that are not fully specified in the paper, creating reproducibility challenges.
- The diacritic bonus mechanism has limited validation scope—the 5% and 10% thresholds appear arbitrary without sensitivity analysis.
- The approach assumes diacritic preservation in formal texts, yet modern digital communication often strips diacritics (particularly in Romanian), creating a fundamental limitation for real-world deployment.
- The method's 100% accuracy claims for longer texts lack statistical significance testing across diverse text genres and quality levels.

## Confidence

- **High confidence** in the mathematical framework and norm-based scoring mechanism, as these follow established principles of vector comparison.
- **Medium confidence** in empirical results due to limited dataset diversity and lack of cross-validation.
- **Low confidence** in scalability to languages beyond the tested six, particularly those with logographic writing systems or extensive diacritic use (e.g., Vietnamese).

## Next Checks

1. **Statistical significance validation**: Replicate experiments with k-fold cross-validation on each dataset, reporting confidence intervals and p-values for accuracy differences between Method 1 (characters + diacritics) and Method 2 (characters + bigrams + diacritics). This addresses whether claimed 100% accuracy is statistically robust or dataset-specific.

2. **Noise robustness benchmarking**: Systematically test the method against controlled noise injections (5%, 10%, 20% deletion/substitution) on clean texts from multiple languages. Compare observed accuracy degradation against the theoretical ℓ₁ norm reduction proof, validating whether the mathematical analysis accurately predicts real-world performance degradation.

3. **Language expansion stress test**: Implement the method for Spanish and Portuguese, languages with similar Romance language characteristics to Romanian. Evaluate whether the monogram + bigram + diacritic combination maintains high accuracy, or if new ambiguities emerge requiring algorithmic modifications such as trigram incorporation or weighted scoring adjustments.