---
ver: rpa2
title: 'Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning'
arxiv_id: '2601.16163'
source_url: https://arxiv.org/abs/2601.16163
tags:
- policy
- cosmos
- training
- value
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cosmos Policy fine-tunes a pretrained video model (Cosmos-Predict2)
  into a robot policy through a single stage of post-training, with no architectural
  changes. The method injects robot actions, future states, and values as latent frames
  within the model's latent diffusion sequence, enabling direct policy learning and
  future state/value prediction.
---

# Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning

## Quick Facts
- **arXiv ID:** 2601.16163
- **Source URL:** https://arxiv.org/abs/2601.16163
- **Reference count:** 21
- **Primary result:** Achieves 98.5% success rate on LIBERO, 67.1% on RoboCasa, and highest average score on real-world bimanual tasks via latent frame injection into pretrained video models

## Executive Summary
Cosmos Policy fine-tunes pretrained video models into robot policies through latent frame injection, achieving state-of-the-art performance across multiple simulation and real-world benchmarks. The method encodes robot actions, future states, and values as latent frames within the video model's diffusion sequence, enabling direct policy learning without architectural changes. This approach leverages video priors learned from Internet-scale data to capture complex action distributions and temporal causality, outperforming models trained from scratch and other video-based policies.

## Method Summary
Cosmos Policy learns to generate robot actions, future states, and values by encoding these modalities as latent frames within a pretrained video diffusion model's latent sequence. The method injects normalized action and value data into blank placeholder latents, then jointly trains policy, world model, and value function through diffusion denoising. At inference, actions are extracted via latent averaging and un-normalization, while test-time planning uses best-of-N search with future state and value predictions to select higher-likelihood successful actions.

## Key Results
- Achieves 98.5% average success rate on LIBERO simulation benchmark
- Achieves 67.1% average success rate on RoboCasa simulation benchmark
- Achieves highest average score on challenging real-world bimanual manipulation tasks
- Model-based planning (V(s')) outperforms model-free (Q(s,a)) by 12.5 points on average across tasks

## Why This Works (Mechanism)

### Mechanism 1: Latent Frame Injection
Encoding robot actions, proprioceptive states, and values as latent frames within the video model's diffusion sequence enables multi-modal generation without architectural changes. The model inserts blank placeholder images into the input sequence, encodes them via the VAE tokenizer, then completely overwrites these placeholder latent frames with normalized, duplicated copies of action/value data. The pretrained denoiser learns to recover these injected latents through the same diffusion objective used for video. At inference, extracting actions requires averaging across the latent volume and un-normalizing—no VAE decoding needed for non-image modalities.

### Mechanism 2: Spatiotemporal Prior Transfer
Pretrained video models capture physical dynamics and temporal causality that transfer to robot control, outperforming models trained from scratch or on static images. Video models trained on Internet-scale data learn implicit physics, motion patterns, and temporal causality through future frame prediction. These representations encode object interactions and deformation—knowledge directly relevant to predicting robot actions and environmental effects.

### Mechanism 3: Test-Time Planning via Future State and Value Prediction
Generating future states and values enables best-of-N search over action candidates, improving success rates on challenging tasks when refined with rollout data. After fine-tuning on policy rollouts (including failures), the model: (1) samples N candidate actions, (2) predicts future state for each via the world model, (3) predicts value (expected return) for each future state, and (4) executes the highest-value action. Rollout data is critical—demonstrations only cover successful trajectories, while rollouts expose the model to failure modes and broader state distributions.

## Foundational Learning

- **Concept: Latent Diffusion Models**
  - Why needed here: Cosmos Policy operates entirely in the VAE latent space, denoising latent frames rather than pixels. The noise schedule modification (hybrid log-normal-uniform) and inference σ_min adjustment are key implementation details.
  - Quick check question: Why does the paper use σ_min=4 at inference instead of σ_min≈0, and what problem does the hybrid noise distribution solve?

- **Concept: World Models and Value Functions**
  - Why needed here: The paper jointly trains a policy, world model p(s'|s,a), and value function V(s'). Understanding the difference between model-based planning (using world model + V(s')) and model-free planning (using Q(s,a)) is essential for Section 5.3.
  - Quick check question: What is the difference between conditioning value prediction on s' only versus on (s,a), and which does the paper find more effective for planning?

- **Concept: Imitation Learning Distribution Shift**
  - Why needed here: The base policy is trained on successful demonstrations only, which creates a narrow state distribution. The paper explicitly addresses this by collecting rollout data (including failures) to refine the world model and value function.
  - Quick check question: Why might a policy trained only on successful demonstrations struggle with test-time planning, and how does rollout data collection address this?

## Architecture Onboarding

- **Component map:**
  1. **VAE Tokenizer (Wan2.1):** Temporal compression 4×, spatial compression 8×; first frame uncompressed for single-image conditioning
  2. **Diffusion Transformer (2B params):** Cosmos-Predict2 backbone; cross-attends to T5-XXL text embeddings; adaptive layer norm for noise conditioning
  3. **Latent Frame Sequence (11 frames for 3-camera):** [blank, proprio, wrist_cam, cam1, cam2, actions, future_proprio, future_wrist, future_cam1, future_cam2, value]
  4. **Noise Schedule:** Hybrid log-normal-uniform (0.7 original + 0.3 uniform over [1.0, 85.0]); inference σ_min=4

- **Critical path:**
  1. Format demonstrations as (s, a, s', V(s')) tuples with all camera images
  2. Encode images via VAE; overwrite placeholder latents with normalized action/value data via duplication to match latent shape
  3. Train with batch split: 50% policy / 25% world model / 25% value function (conditioning scheme determines which)
  4. Direct policy: parallel decoding (~0.6-0.95s); Planning: autoregressive (actions → future state → value, ~4.9s with N=8)

- **Design tradeoffs:**
  - **Parallel vs. autoregressive decoding:** Parallel faster; autoregressive higher quality for planning
  - **V(s') vs. Q(s,a) for planning:** Model-based outperforms with limited rollout data; model-free simpler but overfits
  - **Noise schedule modification:** Uniform tail improves action accuracy but shifts from pretrained distribution

- **Failure signatures:**
  - **Jerky motions:** Training from scratch without pretrained weights (Section 5.2)
  - **Value prediction bimodality:** Requires "majority mean" aggregation (determine majority success/fail threshold, then average within majority)
  - **World model blind to failures:** Base model trained only on demonstrations fails to predict error states like grasp loss (Figure 6)

- **First 3 experiments:**
  1. **Validate latent injection/extraction:** Train on small dataset, generate action latents, verify averaging + un-normalizing produces reasonable action values
  2. **Ablate noise schedule:** Compare base log-normal vs. hybrid on action L1 loss to validate the claim about high-noise training
  3. **Minimal planning loop:** Collect 50-100 rollouts with failure labels, fine-tune world model/value function, run best-of-N=4 planning on held-out task vs. base policy

## Open Questions the Paper Calls Out

### Open Question 1
Can model-based planning inference be accelerated to enable deployment on dynamic manipulation or locomotion tasks?
- Basis in paper: [explicit] "We observe substantially lower inference speed when using model-based planning (e.g., around 5 seconds to produce one action chunk), which may limit applicability to dynamic tasks. How to speed up the search is an important direction for future study."
- Why unresolved: The current best-of-N search requires parallel GPU inference and multiple denoising steps per prediction type (10 for actions, 5 each for future state and value), creating a latency bottleneck.
- What evidence would resolve it: Demonstration of model-based planning achieving sub-500ms latency while maintaining success rate improvements, tested on dynamic tasks like high-speed throwing or locomotion.

### Open Question 2
Can accurate world models and value functions be learned from substantially fewer policy rollouts?
- Basis in paper: [explicit] "Learning from fewer rollouts would increase the accessibility of our approach."
- Why unresolved: The current method required 648 rollouts (505 aggregated from evaluations plus 143 additional for the ziploc bag task) to achieve effective planning, and the authors note that "training on demonstrations alone is insufficient for effective planning since the data only covers successful outcomes."
- What evidence would resolve it: Achieving comparable planning improvements (e.g., the observed 12.5% score increase) using 10× fewer rollouts through techniques like data augmentation, offline data, or more sample-efficient learning.

### Open Question 3
Does extending the world model's prediction horizon beyond a single future state improve planning effectiveness?
- Basis in paper: [explicit] "extending the world model's prediction horizon and planning to greater depths could potentially lead to more effective search."
- Why unresolved: The current architecture predicts only s' (the state at t+K, where K is the action chunk size) and does not model multi-step future trajectories, limiting search to one layer in the tree.
- What evidence would resolve it: Ablation studies comparing single-step vs. multi-step prediction horizons (e.g., predicting s', s'', s''') on long-horizon tasks, measuring both success rate and planning compute trade-offs.

### Open Question 4
Why does model-based planning (V(s')) outperform model-free planning (Q(s,a)) given limited rollout data?
- Basis in paper: [inferred] The authors hypothesize that the Q-function "may overfit given higher input dimensionality" but do not conclusively determine whether the gap stems from data scarcity, architecture, or fundamental differences between the approaches.
- Why unresolved: The comparison was conducted with only 648 rollouts; it remains unclear whether the model-free variant would match or exceed model-based performance with more data, or whether the gap is fundamental.
- What evidence would resolve it: Systematic comparison of V(s') vs. Q(s,a) planning across varying rollout dataset sizes (100, 500, 2000, 5000 rollouts) to identify data scaling regimes where each approach dominates.

## Limitations
- **Data generalization limits:** Real-world tasks outside the training distribution may reveal brittleness in transferred video priors
- **Planning horizon constraints:** Effectiveness at longer horizons or with larger N remains untested; computational cost scales linearly with N
- **Latent modality fidelity:** Uncertain whether discretized latent space preserves critical continuous features for precise control

## Confidence

- **High:** The latent frame injection mechanism and its implementation details (noise schedule, conditioning masks, normalization) are well-specified and grounded in the diffusion framework. The performance improvements on established benchmarks are clearly demonstrated.
- **Medium:** The claim that pretrained video priors transfer to robot control is supported by ablation studies but relies on implicit assumptions about Internet video distribution matching manipulation dynamics. Real-world bimanual tasks show promise but involve limited task diversity.
- **Medium:** The planning framework (world model + value prediction) is theoretically sound, but its robustness depends on rollout data quality and quantity. The paper shows planning helps but doesn't explore failure modes systematically.

## Next Checks

1. **Distribution shift test:** Evaluate Cosmos Policy on tasks with manipulated objects that differ substantially from Internet video content (e.g., transparent/reflective surfaces, deformable materials) to quantify the limits of video prior transfer.
2. **Planning robustness analysis:** Systematically vary N (best-of-N) and planning horizon length on challenging tasks to identify at what point prediction errors accumulate and planning degrades. Include stochastic environments to test robustness.
3. **Latent space fidelity audit:** Visualize and quantify the reconstruction error for non-image modalities (actions, proprioception, values) after latent injection/extraction across the full value range to ensure critical control features aren't lost in quantization.