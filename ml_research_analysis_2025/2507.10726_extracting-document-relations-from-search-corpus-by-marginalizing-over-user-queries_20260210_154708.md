---
ver: rpa2
title: Extracting Document Relations from Search Corpus by Marginalizing over User
  Queries
arxiv_id: '2507.10726'
source_url: https://arxiv.org/abs/2507.10726
tags:
- document
- queries
- query
- relationships
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EDR-MQ addresses the challenge of understanding relationships between
  documents in large-scale corpora without relying on manual annotation or predefined
  taxonomies. The core method idea is to discover document relationships through query
  marginalization, leveraging the insight that strongly related documents often co-occur
  across diverse user queries.
---

# Extracting Document Relations from Search Corpus by Marginalizing over User Queries

## Quick Facts
- arXiv ID: 2507.10726
- Source URL: https://arxiv.org/abs/2507.10726
- Reference count: 19
- Primary result: EDR-MQ discovers document relationships through query marginalization, revealing cross-domain connections and bridge documents without manual annotation

## Executive Summary
EDR-MQ (Extracting Document Relations from Search Corpus by Marginalizing over User Queries) addresses the challenge of understanding relationships between documents in large-scale corpora without relying on manual annotation or predefined taxonomies. The core method idea is to discover document relationships through query marginalization, leveraging the insight that strongly related documents often co-occur across diverse user queries. The approach employs Multiply Conditioned Retrieval-Augmented Generation (MC-RAG), where subsequent document retrievals depend on previously retrieved content, enabling the estimation of joint probabilities between document pairs. Experimental results on the SciFact dataset demonstrate that query marginalization successfully identifies meaningful document relationships, revealing topical clusters and cross-domain connections that are not apparent through traditional similarity-based methods.

## Method Summary
The method estimates document relationships by marginalizing joint probabilities over user queries. It uses MC-RAG with two ColBERT retrievers: the first retrieves documents based on the query alone, and the second retrieves documents conditioned on both the query and previously retrieved documents. The joint probability p(zi, zj) is estimated as (1/N) Σ pη1(zi|xn)pη2(zj|zi, xn) across N queries, and accumulated into a relationship matrix. Top-5 retrievals are performed at each stage, resulting in 25 document pairs per query. The approach requires pre-computed ColBERT embeddings and operates on the SciFact dataset (1,409 claims, 5,183 abstracts) using 300 diverse queries for comprehensive relationship extraction.

## Key Results
- Query marginalization successfully identifies meaningful document relationships, revealing topical clusters and cross-domain connections
- MC-RAG with conditional retrieval captures document dependencies that independent retrieval cannot
- The method excels at discovering bridge documents that connect different parts of the corpus and revealing interdisciplinary relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional retrieval captures document dependencies that independent retrieval cannot.
- Mechanism: MC-RAG employs two retrievers where the second retriever conditions on both the original query x and the first retrieved document zi. The second retriever uses concatenated context [zi; x] to find documents zj that are jointly relevant, enabling the system to model p(zj|zi, x) rather than treating each retrieval independently.
- Core assumption: Documents that are semantically related will be co-retrievable when conditioned on each other, even if they don't share surface-level lexical similarity.
- Evidence anchors:
  - [abstract]: "MC-RAG, which employs conditional retrieval where subsequent document retrievals depend on previously retrieved content"
  - [section 3.1]: "The key innovation of MC-RAG lies in its conditional retrieval mechanism pη2(zj|zi, x). Unlike standard RAG models that perform independent retrieval, the second retriever conditions its search on both the user input x and the previously retrieved passage zi."
  - [corpus]: Related work on Chain of Retrieval (arXiv:2507.10057) supports iterative retrieval expansion, though not directly citing this conditional mechanism.
- Break condition: If documents are topically unrelated, conditioning zj on zi will not improve retrieval over independent retrieval—joint probability p(zi, zj) will approach zero across most queries.

### Mechanism 2
- Claim: Marginalizing over diverse queries reveals stable document co-occurrence patterns that indicate underlying relationships.
- Mechanism: The framework estimates joint probability p(zi, zj) ≈ (1/N) Σ pη1(zi|xn)pη2(zj|zi, xn) across N queries. As query diversity increases, noise from individual query biases cancels out while true document relationships accumulate, revealing latent structure.
- Core assumption: The query distribution p(x) sufficiently covers the semantic space such that related documents will co-occur across multiple query perspectives.
- Evidence anchors:
  - [abstract]: "By observing co-occurrence patterns across diverse queries, EDR-MQ estimates joint probabilities between document pairs without requiring labeled training data or predefined taxonomies."
  - [section 4.2]: "With 300 queries, the relationships become more comprehensive, connecting previously isolated clusters and revealing the overall corpus structure through richer inter-document connections."
  - [corpus]: No direct corpus validation of the marginalization approach; this appears to be a novel contribution.
- Break condition: If query set X is too small or semantically narrow, joint probability estimates will be unreliable, producing fragmented clusters (as shown in Figure 3a with only 30 queries).

### Mechanism 3
- Claim: Cross-domain relationships emerge from query-driven retrieval rather than from categorical or similarity-based organization.
- Mechanism: Because retrievals are conditioned on semantic relevance to queries rather than document metadata, documents from different categories can be linked if they share conceptual content relevant to the same queries. This reveals bridge documents and interdisciplinary connections.
- Core assumption: Scientific concepts span categorical boundaries, and query formulations naturally cut across these boundaries.
- Evidence anchors:
  - [section 4.4]: "Despite being classified into different categories, our method successfully identifies the underlying conceptual connections between these documents. The extracted relationships reveal a coherent research narrative centered around neutrophil extracellular traps (NETs)."
  - [section 4.3]: "The TF-IDF-based network exhibits scattered edge patterns with poor bundling characteristics... In contrast, the EDR-MQ network demonstrates clear and well-defined edge bundling patterns."
  - [corpus]: Weak direct validation—no corpus papers explicitly address cross-domain relationship discovery through query marginalization.
- Break condition: If corpus has no true cross-domain conceptual overlap, or if queries are overly category-specific, cross-domain connections will not emerge.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: MC-RAG extends standard RAG by making retrieval conditional. Understanding baseline RAG (retriever + generator) is prerequisite to understanding why conditional retrieval matters.
  - Quick check question: Can you explain why standard RAG retrieves documents independently and how this differs from MC-RAG's conditional approach?

- Concept: **ColBERT Late Interaction**
  - Why needed here: The paper uses ColBERT as its retrieval backbone specifically for its fine-grained token-level matching, which is critical for the conditional retrieval mechanism to work effectively.
  - Quick check question: How does ColBERT's late interaction (computing max similarity per query token) differ from dense retrieval with single vector representations?

- Concept: **Probability Marginalization**
  - Why needed here: The entire framework rests on marginalizing p(zi, zj) over query distribution p(x). Without understanding marginalization, the joint probability estimation appears arbitrary.
  - Quick check question: Given p(zi, zj) = Σx p(zi|x)p(zj|zi, x)p(x), why does increasing query diversity improve the estimate?

## Architecture Onboarding

- Component map:
  1. **ColBERT Encoder**: Token-level embeddings for queries and documents
  2. **First Retriever pη1(zi|x)**: Standard retrieval based on query alone, returns top-k documents
  3. **Second Retriever pη2(zj|zi, x)**: Conditional retrieval using concatenated [zi; x], returns top-k documents
  4. **Generator pθ(y|zi, zj, x)**: Language model producing output from retrieved context (not central to relationship extraction)
  5. **Relationship Matrix R**: M×M matrix accumulating joint probabilities across all queries

- Critical path:
  Query → First Retriever → Top-k docs → For each zi: Conditional query [zi; x] → Second Retriever → Top-k docs → Accumulate p1×p2 to R[i,j]

- Design tradeoffs:
  - **k (retrieval depth)**: Higher k captures more relationships but increases O(N·k²·C) compute cost. Paper uses k=5.
  - **Query diversity vs. coverage**: More queries improve relationship quality but require more compute. Paper shows 30 queries produce fragmented clusters; 300 queries produce connected structure.
  - **ColBERT vs. simpler retrievers**: ColBERT's fine-grained matching enables better conditional retrieval but has higher memory/compute requirements than single-vector dense retrieval.

- Failure signatures:
  - Fragmented clusters in relationship matrix → Query set too small or too narrow
  - Dense, uninterpretable networks (like TF-IDF baseline) → Retrieval not discriminative enough; may need better retriever or different similarity threshold
  - No cross-domain connections emerging → Queries may be too category-specific; corpus may lack true interdisciplinary overlap
  - Memory issues with large corpora → Relationship matrix is O(M²); may need sparse storage or thresholding for large M

- First 3 experiments:
  1. **Vary query count**: Run Algorithm 1 with N=30, 100, 300 queries on a held-out corpus subset. Visualize relationship networks and measure cluster connectivity (number of connected components, average path length). Should reproduce paper's finding that more queries → better connected structure.
  2. **Compare retriever backbones**: Swap ColBERT for BM25 or standard DPR. Measure relationship quality by checking if known related document pairs (from labeled data or citations) have higher p(zi, zj) than random pairs. Hypothesis: ColBERT's fine-grained matching is necessary for conditional retrieval to work.
  3. **Cross-domain validation**: On a corpus with known cross-domain connections (e.g., interdisciplinary citations), measure recall: what fraction of known cross-domain pairs are discovered by EDR-MQ vs. TF-IDF similarity? This tests the core claim that query-driven discovery reveals connections similarity-based methods miss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does end-to-end training of the MC-RAG framework improve the quality of extracted document relationships compared to using frozen retrievers?
- Basis in paper: [explicit] The conclusion states that "end-to-end training of the entire MC-RAG framework could potentially improve the quality of extracted relationships by jointly optimizing the retrieval and relationship extraction objectives."
- Why unresolved: The current implementation utilizes pre-trained ColBERT encoders without jointly optimizing the retrieval components specifically for the relationship extraction task.
- What evidence would resolve it: Quantitative evaluation showing higher accuracy or coherence in the relationship matrix when the model is trained end-to-end versus the current modular approach.

### Open Question 2
- Question: How does the performance of EDR-MQ generalize to document corpora outside of the scientific domain?
- Basis in paper: [explicit] The authors note that "comprehensive evaluation across diverse datasets... would strengthen the validation of our approach’s generalizability."
- Why unresolved: All reported experiments were conducted exclusively on the SciFact dataset, which focuses on scientific claims and abstracts.
- What evidence would resolve it: Successful extraction of meaningful document relationships and corpus structures in non-scientific datasets (e.g., legal, news, or organizational documents).

### Open Question 3
- Question: How does EDR-MQ compare to a wider range of baseline methods for document relationship extraction?
- Basis in paper: [explicit] The conclusion identifies the need for "comparison with additional baseline methods" to validate the approach.
- Why unresolved: The paper primarily compares the method against TF-IDF similarity and visualization techniques, leaving its relative performance against other unsupervised relation extraction methods undetermined.
- What evidence would resolve it: Benchmarking results against established unsupervised relationship extraction or graph construction baselines on standard datasets.

## Limitations
- The approach relies heavily on the quality and diversity of the query set, with no clear guidance on optimal query selection
- The conversion from ColBERT scores to probabilities lacks explicit specification, which could significantly affect relationship estimation
- The relationship matrix R grows quadratically with corpus size, creating computational and memory constraints for large-scale applications

## Confidence

- High confidence: Conditional retrieval mechanism and joint probability estimation formula
- Medium confidence: Query marginalization effectively reveals document relationships
- Medium confidence: Cross-domain relationships emerge from query-driven retrieval
- Low confidence: Specific hyperparameter choices (k=5, query count thresholds) generalize well

## Next Checks

1. Implement quantitative metrics for relationship quality, such as recall of known related document pairs (from citations or labeled data) to compare EDR-MQ against TF-IDF and other baselines.
2. Conduct ablation studies varying query diversity systematically (e.g., 10, 30, 100, 300 queries) while measuring relationship matrix sparsity and cluster connectivity to establish clear diversity requirements.
3. Test the approach on a different corpus type (e.g., news articles or patents) with known categorical structure to verify cross-domain relationship discovery claims across domains.