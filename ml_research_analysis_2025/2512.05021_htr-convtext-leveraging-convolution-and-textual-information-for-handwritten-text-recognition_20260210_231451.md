---
ver: rpa2
title: 'HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten
  Text Recognition'
arxiv_id: '2512.05021'
source_url: https://arxiv.org/abs/2512.05021
tags:
- recognition
- text
- context
- dataset
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose HTR-ConvText, a hybrid architecture for handwritten
  text recognition that combines a CNN-based feature extractor with MobileViT and
  positional encoding, a ConvText encoder using self-attention and convolution, and
  a training-only Textual Context Module to inject linguistic context. The approach
  achieves state-of-the-art performance on IAM (CER 4.0%, WER 12.9%), LAM (CER 2.7%,
  WER 7.0%), READ2016 (CER 3.6%, WER 15.7%), and HANDS-VNOnDB (CER 3.45%, WER 8.9%),
  demonstrating strong generalization across English, German, Italian, and Vietnamese
  scripts, especially with limited training data.
---

# HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition

## Quick Facts
- **arXiv ID:** 2512.05021
- **Source URL:** https://arxiv.org/abs/2512.05021
- **Reference count:** 40
- **Primary result:** State-of-the-art HTR performance across four languages with novel hybrid CNN-ViT architecture

## Executive Summary
HTR-ConvText introduces a hybrid architecture for handwritten text recognition that combines a CNN-based feature extractor with MobileViT and positional encoding, a ConvText encoder using self-attention and convolution, and a training-only Textual Context Module to inject linguistic context. The approach achieves state-of-the-art performance on IAM (CER 4.0%, WER 12.9%), LAM (CER 2.7%, WER 7.0%), READ2016 (CER 3.6%, WER 15.7%), and HANDS-VNOnDB (CER 3.45%, WER 8.9%), demonstrating strong generalization across English, German, Italian, and Vietnamese scripts, especially with limited training data. Ablation studies confirm the effectiveness of each component. Qualitative analysis shows errors mainly stem from intrinsic handwriting variability rather than model limitations.

## Method Summary
The HTR-ConvText model uses a hybrid MVP block (ResNet-18 with interleaved MobileViT blocks and Conditional Positional Encoding) to extract visual features, which are then processed by a ConvText encoder with 8 stacked hybrid blocks in a U-Net structure that combines self-attention, feed-forward networks, and depthwise separable convolutions. A training-only Textual Context Module injects linguistic context via an auxiliary cross-entropy loss. The model is trained with AdamW optimization and combines CTC loss with auxiliary TCM loss, achieving strong performance across multiple languages without pre-training or synthetic data.

## Key Results
- Achieves state-of-the-art performance on four benchmark datasets: IAM (CER 4.0%, WER 12.9%), LAM (CER 2.7%, WER 7.0%), READ2016 (CER 3.6%, WER 15.7%), HANDS-VNOnDB (CER 3.45%, WER 8.9%)
- Demonstrates strong generalization across English, German, Italian, and Vietnamese scripts
- Ablation studies confirm effectiveness of each component (MVP block, ConvText encoder, TCM)
- Performance particularly strong with limited training data, without relying on pre-training or synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid CNN-ViT feature extraction with positional encoding captures both local stroke-level details and global contextual dependencies in handwriting.
- **Mechanism:** ResNet-18 backbone provides strong local feature extraction through stacked convolutional layers, while strategically interleaved MobileViT blocks introduce Vision Transformer layers to capture global context. Conditional Positional Encoding (CPE) maintains spatial awareness during the patch transformations (Fold/Unfold operations) that MobileViT performs.
- **Core assumption:** Handwritten text recognition requires both fine-grained stroke discrimination (diacritics, character-level details) and global contextual understanding (word-level, sequence-level patterns).
- **Evidence anchors:**
  - [abstract] "We integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details."
  - [section 3.1] "This intelligent hybrid structure capitalizes on the complementary nature of local features from the CNN and global context captured by the Vision Transformer (ViT) layers, allowing the model to effectively capture the interplay between local details and global scene context."
  - [corpus] Corpus evidence for MobileViT specifically in HTR is limited; however, "WriteViT: Handwritten Text Generation with Vision Transformer" (arXiv:2505.13235) demonstrates successful application of Vision Transformers to handwriting tasks, supporting the viability of ViT-based approaches in this domain.
- **Break condition:** If the MobileViT blocks do not effectively capture the specific types of global dependencies in handwriting (e.g., inter-character relationships across long lines) due to the patch-based nature of ViT processing, or if CPE fails to preserve essential spatial ordering during the Fold/Unfold operations, the mechanism's benefit will be diminished or negated.

### Mechanism 2
- **Claim:** The ConvText encoder's sequential combination of self-attention, feed-forward networks, and depthwise separable convolutions within a U-Net-like hierarchical structure efficiently captures both global dependencies and fine-grained local features while reducing computational cost.
- **Mechanism:** Multi-Head Self-Attention (MHSA) captures global dependencies. A subsequent lightweight 1D depthwise separable convolution module specifically captures fine-grained local spatial context (e.g., diacritics, stroke connections). Framing these with half-step Feed-Forward Networks (FFNs) and using a U-Net-like downsampling/upsampling structure with residual connections compresses the sequence length for the attention mechanism (O(L²) cost reduction) while enabling recovery of fine spatial details for character boundary prediction. Relative Positional Encoding ensures robustness to sequence length changes across U-Net stages.
- **Core assumption:** Effective HTR requires explicit modeling of both long-range character/word dependencies and highly localized stroke patterns, and the Conformer-style sequential interleaving is a superior arrangement to alternatives like the Macaron structure for this task.
- **Evidence anchors:**
  - [abstract] "We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency."
  - [section 3.2] "Our encoder therefore interleaves (i) a multi-head self-attention and (ii) a depthwise-separable convolutions along the sequence axis... This effectively intertwines local and global feature extraction without the structural redundancy of the Macaron design."
  - [section 3.2.2] "The encoder begins by processing the input feature sequence X at its original resolution (L=128). It then temporally downsamples the sequence by a factor of two... effectively quartering the computational cost of the attention mechanism while increasing the effective receptive field."
  - [corpus] Corpus evidence for "ConvText" specifically is absent (novel proposal). The Conformer architecture [26] (cited in the paper) from ASR provides a strong precedent for hybrid attention-convolution models, but direct corpus evidence for this specific ConvText variant in HTR is weak.
- **Break condition:** If the sequential ordering of MHSA and convolution is not optimal for the specific feature hierarchies in handwriting, or if the U-Net downsampling causes irreversible loss of fine-grained spatial information critical for character discrimination, performance gains will be limited.

### Mechanism 3
- **Claim:** A training-only Textual Context Module (TCM) that injects bidirectional linguistic priors into the visual encoder via an auxiliary cross-entropy loss improves recognition by mitigating the conditional independence assumption of CTC.
- **Mechanism:** For each ground-truth character, the TCM constructs left and right context windows. These textual contexts are embedded, processed by a lightweight convolution and projection, and fused with the visual encoder's features via a Cross-Attention mechanism. The module produces an auxiliary character prediction loss that guides the visual encoder to learn text-consistent representations. Crucially, the TCM is discarded at inference, adding no latency.
- **Core assumption:** CTC's independence assumption is a significant bottleneck, and forcing the visual encoder to produce representations that are consistent with their linguistic context (as an auxiliary training task) yields a more robust feature space for the primary CTC decoder.
- **Evidence anchors:**
  - [abstract] "Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification."
  - [section 3.3] "Standard CTC-based recognition operates under a conditional independence assumption, often leading to predictions that lack linguistic coherence. To address this, we introduce a Textual Context Module, a training-only auxiliary branch designed to inject explicit linguistic priors into the visual encoder."
  - [Table 6] Ablation study shows adding the TCM head (C) reduces CER from 4.87% to 4.19% on IAM, demonstrating its positive contribution.
  - [corpus] Corpus evidence for training-only auxiliary linguistic modules in CTC-based HTR is limited. Related work like TrOCR uses pre-trained language models but is a full encoder-decoder architecture, not a CTC auxiliary module. Evidence is weak in the corpus.
- **Break condition:** If the visual features and linguistic contexts are not effectively fused by the Cross-Attention, or if the auxiliary loss signal is too weak/strong and destabilizes training, the mechanism will fail to improve the CTC head's performance.

## Foundational Learning

- **Concept: Connectionist Temporal Classification (CTC) Loss**
  - **Why needed here:** CTC is the primary loss function for the model. Understanding its conditional independence assumption is critical to appreciating why the Textual Context Module is proposed as a remedy.
  - **Quick check question:** Given a feature sequence, does CTC loss model the probability of the entire output sequence directly, or does it sum over all possible alignments between the input and output sequences?

- **Concept: Vision Transformer (ViT) and Inductive Bias**
  - **Why needed here:** The MVP Block and ConvText encoder are fundamentally hybrid architectures. You must understand that standard ViTs lack the "inductive bias" of CNNs (translation invariance, locality) and typically require massive data to learn these patterns from scratch.
  - **Quick check question:** Why might a pure Vision Transformer struggle on a small dataset like IAM (6,482 training lines) compared to a CNN-based model?

- **Concept: Self-Attention and Convolution for Sequence Modeling**
  - **Why needed here:** The core of the ConvText block is the interplay between these two operations. You need to understand that self-attention excels at capturing long-range global dependencies with O(L²) complexity, while 1D convolutions are excellent for local patterns with O(L) complexity.
  - **Quick check question:** In the ConvText block, which operation is primarily responsible for learning features like the shape of a diacritic mark on a character, and which is primarily responsible for understanding that a character is the end of a word?

## Architecture Onboarding

- **Component Map:**
  1. Input Preprocessing: Line image (H×W) → grayscale, resize to 512×64, normalization, data augmentation
  2. MVP Block (Feature Extractor): ResNet-18 backbone with MobileViT Blocks and Conditional Positional Encoding (CPE) interleaved in final stages. Output: 2D feature map
  3. Flattening: 2D feature map → 1D sequence X (length L, dimension D)
  4. ConvText Encoder: 8 stacked ConvText blocks in a U-Net structure (downsample → process → upsample + residual)
     - Each block: LayerNorm → MHSA (with Relative PE) → Residual+LayerScale → Half-FFN → Residual+LayerScale → Depthwise Conv (SiLU) → Residual+LayerScale → Half-FFN → Residual+LayerScale
  5. CTC Head: Projects encoder output to character logits. Decoded via CTC
  6. Textual Context Module (Training Only):
     - Input: Ground-truth text (left/right context windows)
     - Process: Embedding → 1D Conv → Linear → LayerNorm → Cross-Attention with encoder output → Auxiliary classifier
     - Loss: Auxiliary cross-entropy loss (LT C M) combined with primary CTC loss (LCT C )

- **Critical Path:** The flow of gradients during training is critical. The primary CTC loss provides the main learning signal. The TCM's auxiliary loss provides a secondary signal that *must* effectively flow back through the cross-attention into the ConvText encoder to regularize the visual features. Ensure this gradient path is correctly implemented.

- **Design Tradeoffs:**
  - **ConvText Block Structure:** The authors chose a sequential MHSA-FFN-Conv-FFN structure over the Conformer's Macaron (FFN-MHSA-FFN-Conv) design. This may reduce computational redundancy but is a key architectural choice to verify for your specific data
  - **Hierarchical U-Net:** Downsampling the sequence length reduces attention's O(L²) cost but risks losing fine-grained spatial information. The residual connection from the pre-downsampling features is essential to mitigate this. Validate its contribution with an ablation
  - **Training-only TCM:** This design avoids inference latency but means the model cannot adapt to linguistic context at test time. The linguistic "knowledge" is baked into the encoder weights

- **Failure Signatures:**
  - **High CER on Diacritics:** This may indicate the convolution module in the ConvText block is not effectively capturing local stroke details, or the U-Net downsampling is losing spatial resolution
  - **Poor Performance on Long Lines:** This could be a failure of the relative positional encoding to generalize to unseen sequence lengths, or a symptom of the O(L²) attention cost becoming a bottleneck if the U-Net structure isn't effectively managing sequence length
  - **No Gain from TCM:** If adding the TCM doesn't improve validation CER, check the loss weighting (λCT C, λT C M). The auxiliary loss may be too weak, or the cross-attention fusion may be failing

- **First 3 Experiments:**
  1. **Baseline Reproduction:** Implement the model with only the ResNet-18 backbone (no MVP, no ConvText blocks, no TCM) feeding into a CTC head. Establish a performance baseline on your validation set. Compare against the paper's "Vision Transformer with ResNet backbone (A)" ablation result (CER 6.25%)
  2. **Component-wise Ablation:** Add components one by one as per the paper's ablation study (Table 6): first the ConvText Block, then the MVP Block (with CPE), then the TCM. Track CER/WER at each step to confirm each proposed mechanism adds value on your data
  3. **Hyperparameter Sensitivity:** The paper's ablation (Table 7) suggests 8 layers and 8 heads is a good tradeoff. Run a small grid search around these values (e.g., 6, 8, 10 layers and 6, 8, 10 heads) to find the optimal configuration for your specific computational budget and dataset size

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Textual Context Module (TCM) be modified for use during inference to improve recognition accuracy without negating the efficiency benefits of the CTC decoder?
- **Basis in paper:** [explicit] The conclusion states that "exploring the feasibility of integrating the TCM's linguistic modeling capabilities into an inference-time component remains a promising avenue for improving real-world HTR application efficiency."
- **Why unresolved:** The current design discards the TCM at inference to ensure fast, parallel decoding, creating a gap between the linguistic context available during training and inference
- **What evidence would resolve it:** Implementation of a lightweight inference-time TCM that demonstrates a reduction in Word Error Rate (WER) without significantly increasing latency compared to the baseline CTC-only inference

### Open Question 2
- **Question:** What specific architectural or data augmentation strategies are required to achieve invariance to severe stroke distortion and character overlap?
- **Basis in paper:** [explicit] The conclusion suggests "future efforts should concentrate on enhancing model robustness against challenging intrinsic data properties, such as severe stroke distortion and character overlap." Section 4.6 identifies these as the primary causes of residual errors
- **Why unresolved:** The qualitative analysis suggests that errors are driven by intrinsic data challenges rather than current architectural limitations, implying the model has reached a performance ceiling with the current feature extraction methods for these specific cases
- **What evidence would resolve it:** Introduction of new augmentation techniques or invariance-inducing modules that specifically lower error rates on a curated subset of "heavily distorted" or "overlapping" text lines

### Open Question 3
- **Question:** Can HTR-ConvText effectively scale to leverage massive synthetic data pre-training, or is the hybrid architecture strictly optimized for data-efficient, non-pre-trained scenarios?
- **Basis in paper:** [inferred] The paper highlights its success "without relying on pre-training, synthetic data" (Section 4) and contrasts this with data-hungry models like TrOCR. It remains untested whether this specific hybrid CNN-ViT architecture is compatible with the massive pre-training regimes used by current state-of-the-art commercial models
- **Why unresolved:** The experiments are limited to real, low-to-mid-sized datasets. The authors do not evaluate if the ConvText encoder can absorb the representations learned from hundreds of millions of synthetic images to further push state-of-the-art benchmarks
- **What evidence would resolve it:** A training run utilizing a standard large-scale synthetic dataset (e.g., Synth90k or Google's synthetic data) to determine if the relative gain over baselines persists or diminishes compared to pure Transformer models

## Limitations
- **Unknown hyperparameters:** Loss weights (λCTC, λTCM) and context window size (k) are mentioned but not specified in the paper
- **Computational overhead:** Hybrid MVP architecture increases model complexity compared to pure CNN baselines, with unclear practical efficiency trade-offs
- **Dataset bias:** All results come from standardized benchmark datasets; generalization to truly out-of-distribution handwriting styles remains untested

## Confidence

- **High Confidence:** The hybrid MVP architecture combining CNN and ViT components effectively captures both local and global features (supported by ablation study showing MVP block improves performance from 6.25% to 4.87% CER on IAM)
- **Medium Confidence:** The ConvText encoder's U-Net structure with interleaved MHSA and convolution provides efficient feature extraction (mechanism is sound, but specific architectural choices like sequential vs. Macaron ordering lack direct comparative evidence)
- **Medium Confidence:** The training-only TCM provides meaningful linguistic regularization to the CTC decoder (demonstrated by ablation, but mechanism relies on auxiliary loss signal that may not transfer across languages/domains)

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary λCTC, λTCM, and the number of ConvText layers/heads around the reported values to establish robust performance bounds and identify brittle configurations
2. **Cross-Domain Transfer Test:** Evaluate the pre-trained model on a held-out dataset with substantially different handwriting styles (e.g., historical documents, medical prescriptions) to verify generalization claims beyond benchmark test sets
3. **Computational Efficiency Benchmark:** Measure wall-clock training and inference times against a strong CNN baseline (e.g., CRNN) on identical hardware to quantify the practical cost of the hybrid architecture's performance gains