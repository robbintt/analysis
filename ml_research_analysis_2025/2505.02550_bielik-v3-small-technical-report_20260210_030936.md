---
ver: rpa2
title: 'Bielik v3 Small: Technical Report'
arxiv_id: '2505.02550'
source_url: https://arxiv.org/abs/2505.02550
tags:
- polish
- bielik
- language
- instruct
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces Bielik v3, a series of Polish
  language models (1.5B and 4.5B parameters) that demonstrate exceptional parameter
  efficiency. By building on Qwen2.5 architecture with depth up-scaling and replacing
  the tokenizer with a custom Polish tokenizer (APT4), the models achieve strong performance
  on multiple Polish benchmarks including Open PL LLM Leaderboard, CPTUB, Polish EQ-Bench,
  and Polish Medical Leaderboard.
---

# Bielik v3 Small: Technical Report

## Quick Facts
- arXiv ID: 2505.02550
- Source URL: https://arxiv.org/abs/2505.02550
- Reference count: 14
- Bielik-4.5B-v3.0-Instruct achieves 56.13 on Open PL LLM Leaderboard, outperforming Qwen2.5-7B-Instruct (54.93) despite having fewer parameters

## Executive Summary
Bielik v3 introduces a series of Polish language models (1.5B and 4.5B parameters) that demonstrate exceptional parameter efficiency by building on Qwen2.5 architecture with depth up-scaling and custom Polish tokenizer. The models achieve strong performance on multiple Polish benchmarks including Open PL LLM Leaderboard, CPTUB, Polish EQ-Bench, and Polish Medical Leaderboard. The Bielik-4.5B-v3.0-Instruct model scores 56.13 on the Open PL LLM Leaderboard, competitive with models 2-3 times its size. The 1.5B model also delivers strong performance, demonstrating that well-optimized smaller models can match or exceed larger counterparts. The training methodology incorporates quality-focused data curation with 95% accuracy classification, Adaptive Learning Rate, and innovative preference optimization techniques.

## Method Summary
Bielik v3 builds on Qwen2.5 architecture with depth up-scaling, increasing model size from 1.5B/3B to 1.5B/4.5B parameters. The approach involves replacing the original tokenizer with a custom Polish tokenizer (APT4) using FOCUS initialization, followed by depth up-scaling (for 4.5B: n=36, m=8, s=56). The training pipeline includes pretraining on 292B tokens (237B Polish from SpeakLeash, 55B English from SlimPajema), supervised fine-tuning on 19M instructions with Adaptive Learning Rate, and preference optimization using both DPO-P and GRPO. The models are evaluated on multiple Polish benchmarks including Open PL LLM Leaderboard (5-shot), Polish EQ-Bench, CPTUB, Polish Medical Leaderboard, PLCC, and Berkeley Function-Calling Leaderboard.

## Key Results
- Bielik-4.5B-v3.0-Instruct achieves 56.13 on Open PL LLM Leaderboard, outperforming Qwen2.5-7B-Instruct (54.93) despite having fewer parameters
- Bielik-1.5B-v3.0-Instruct achieves 49.73 on Open PL LLM Leaderboard, matching performance of models 2-3× larger
- Strong performance across specialized benchmarks: 82.67 on CPTUB, 58.48 on Polish EQ-Bench, 69.78 on Polish Medical Leaderboard
- Demonstrates exceptional parameter efficiency with competitive results using 4.5B parameter models

## Why This Works (Mechanism)
The Bielik v3 models achieve strong performance through a combination of architectural scaling and data optimization. Depth up-scaling effectively increases model capacity while maintaining efficiency, allowing the models to capture more complex patterns without proportional parameter growth. The custom APT4 tokenizer with FOCUS initialization ensures optimal representation of Polish language features, while the quality-focused data curation pipeline (95% accuracy classifier) ensures training on high-quality Polish text. The Adaptive Learning Rate and preference optimization techniques further refine model capabilities, resulting in models that perform competitively with larger counterparts despite having fewer parameters.

## Foundational Learning
- **Depth Up-Scaling**: Architectural technique that increases model depth while maintaining efficiency. Why needed: To expand model capacity without proportional parameter growth. Quick check: Verify model depth increase matches specified n=36, m=8, s=56 for 4.5B version.
- **Tokenization Optimization**: Custom APT4 tokenizer with FOCUS initialization for Polish language. Why needed: Standard tokenizers are suboptimal for less-represented languages. Quick check: Confirm 32K vocabulary size and successful embedding initialization.
- **Quality Classification**: XGBoost classifier with 200 stylometric features achieving 95% accuracy. Why needed: Ensures training on high-quality Polish text while filtering noise. Quick check: Validate classification accuracy on held-out test set.
- **Adaptive Learning Rate**: LR × √(T/BS) scheduling during fine-tuning. Why needed: Optimizes learning dynamics for instruction tuning. Quick check: Monitor loss curves for expected adaptive behavior.
- **Preference Optimization**: DPO-P and GRPO techniques for alignment. Why needed: Improves model responses through preference learning. Quick check: Verify preference pair effectiveness on validation set.
- **Linear Model Merging**: Technique for combining checkpoints. Why needed: Preserves capabilities while integrating new knowledge. Quick check: Confirm successful merge without performance degradation.

## Architecture Onboarding

**Component Map:**
Qwen2.5 Base Model -> Depth Up-Scaling -> Tokenizer Replacement (APT4) -> Pretraining (292B tokens) -> SFT (19M instructions) -> DPO-P (126K pairs) -> GRPO (12K math) -> Linear Merge

**Critical Path:**
Pretraining → SFT → DPO-P → GRPO → Linear Merge

**Design Tradeoffs:**
- Smaller parameter count (1.5B/4.5B) vs. larger models (7B+)
- Polish language focus vs. multilingual capabilities
- Quality-focused curation (95% accuracy) vs. data quantity
- Preference optimization (DPO-P/GRPO) vs. standard SFT

**Failure Signatures:**
- High initial training loss after tokenizer replacement indicates poor FOCUS initialization
- Catastrophic forgetting during continued pretraining suggests insufficient English data inclusion
- GRPO instability manifests as policy divergence from reference model

**First 3 Experiments:**
1. Test FOCUS embedding initialization by replacing tokenizer on small Qwen2.5 model and monitoring initial loss
2. Validate quality classifier pipeline by implementing 200 stylometric features and testing classification accuracy
3. Implement minimal GRPO training run with specified KL coefficient to verify stability on small math dataset

## Open Questions the Paper Calls Out
**Open Question 1:** What specific architectural or training modifications would most effectively improve performance on complex reasoning tasks in small parameter-efficient Polish models? The authors note "the area with the most room for improvement is in handling tricky questions, where Bielik-4.5B-v3.0-Instruct scores 2.46" and future work will focus on enhancing complex reasoning capabilities.

**Open Question 2:** To what extent do the techniques developed for Polish (custom tokenizer APT4, depth up-scaling, DPO-P preference optimization) transfer to other less-resourced languages? While the paper claims these advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, no cross-lingual validation is provided.

**Open Question 3:** How does the quality classifier's low recall on MEDIUM-quality texts (0.51) affect downstream model performance, and would improving this recall yield better models? The paper notes this issue but does not analyze downstream impact on model robustness or training data diversity.

## Limitations
- Missing complete pretraining hyperparameters (learning rate schedule, batch size progression)
- Incomplete GRPO implementation details (group size, clipping epsilon)
- Reliance on custom components (APT4 tokenizer, FOCUS initialization) creating replication barriers
- No cross-lingual validation to demonstrate transferability to other less-resourced languages

## Confidence

**High confidence** in the core architectural innovation (Depth Up-Scaling + tokenizer replacement methodology) and benchmark performance claims

**Medium confidence** in the SFT and DPO-P implementation details

**Low confidence** in the exact pretraining configurations and GRPO hyperparameters

## Next Checks

1. Reproduce the tokenizer replacement procedure with FOCUS initialization on a small-scale model to verify embedding alignment and training stability

2. Validate the quality classifier pipeline by implementing the 200 stylometric features and testing classification accuracy on a sample Polish corpus

3. Implement a minimal GRPO training run with the specified KL coefficient to verify stability and effectiveness on a small math dataset