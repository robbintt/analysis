---
ver: rpa2
title: 'Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level
  Aggregation via Markov Likelihood'
arxiv_id: '2510.09369'
source_url: https://arxiv.org/abs/2510.09369
tags:
- entropy
- policy
- arxiv
- grpo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEPO addresses instability in GRPO-based reinforcement learning
  for LLM reasoning, particularly entropy collapse and high-variance gradients due
  to sparse rewards in long chain-of-thought sequences. It introduces a Markov Likelihood-based
  importance sampling scheme that links group-level rewards to token-level updates,
  reducing gradient variance while maintaining exploration.
---

# Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood

## Quick Facts
- **arXiv ID**: 2510.09369
- **Source URL**: https://arxiv.org/abs/2510.09369
- **Reference count**: 18
- **Primary result**: TEPO achieves 77.20% accuracy on MATH-500, setting a new state of the art

## Executive Summary
TEPO addresses key stability issues in GRPO-based reinforcement learning for LLM reasoning, particularly entropy collapse and high-variance gradients arising from sparse rewards in long chain-of-thought sequences. It introduces a Markov Likelihood-based importance sampling scheme that links group-level rewards to token-level updates, reducing gradient variance while maintaining exploration. On seven math benchmarks, TEPO achieves an average accuracy of 32.04%, a 1.74-point improvement over GRPO, and sets a new state of the art on MATH-500 (77.20% accuracy). Training stability is enhanced, as evidenced by steadier rewards and higher gradient norms.

## Method Summary
TEPO introduces a Markov Likelihood-based importance sampling scheme to address the instability and high variance of gradients in GRPO-based reinforcement learning for LLM reasoning. By linking group-level rewards to token-level updates, TEPO reduces gradient variance and prevents entropy collapse, while maintaining exploration. The method is evaluated on seven math benchmarks, showing significant improvements in both accuracy and training stability compared to GRPO.

## Key Results
- TEPO achieves an average accuracy of 32.04% across seven math benchmarks, a 1.74-point improvement over GRPO.
- Sets a new state of the art on MATH-500 with 77.20% accuracy.
- Demonstrates enhanced training stability, evidenced by steadier rewards and higher gradient norms.

## Why This Works (Mechanism)
TEPO's Markov Likelihood-based importance sampling scheme addresses the core issue of sparse rewards in long chain-of-thought sequences by creating a more informative gradient signal. This mechanism reduces gradient variance and prevents the entropy collapse that often occurs in GRPO, thereby maintaining exploration and improving learning stability. By linking group-level rewards to token-level updates, TEPO provides a more stable and efficient learning signal for each token in the sequence.

## Foundational Learning
- **Importance Sampling**: A technique for reducing variance in Monte Carlo estimates by reweighting samples based on their importance. *Why needed*: To focus learning on more informative tokens and reduce the impact of sparse rewards. *Quick check*: Verify that the sampling weights are properly normalized and that the variance reduction is measurable.
- **Markov Likelihood**: A probabilistic model that captures the likelihood of a sequence based on local dependencies. *Why needed*: To model the relationship between token-level updates and group-level rewards in a way that respects the sequential nature of reasoning. *Quick check*: Confirm that the Markov assumptions are valid for the reasoning tasks at hand.
- **Entropy Collapse**: A phenomenon where the policy becomes too deterministic too quickly, reducing exploration. *Why needed*: To understand and prevent the loss of exploration that can hinder learning in RL. *Quick check*: Monitor entropy over training to ensure it remains above a minimum threshold.

## Architecture Onboarding
- **Component Map**: Reward Aggregation -> Markov Likelihood Sampling -> Token-Level Policy Update -> Gradient Computation
- **Critical Path**: Token generation → Reward assignment → Markov Likelihood weighting → Policy gradient update
- **Design Tradeoffs**: Balancing exploration (via entropy regularization) against exploitation (via reward maximization) is critical. The Markov Likelihood sampling introduces additional computational overhead but yields more stable gradients.
- **Failure Signatures**: High gradient variance, rapid entropy collapse, or stagnant rewards may indicate issues with the importance sampling or reward aggregation.
- **First Experiments**:
  1. Compare gradient norms and entropy trajectories between TEPO and GRPO during training.
  2. Perform an ablation study removing the Markov Likelihood component to assess its impact on stability and accuracy.
  3. Test TEPO on a non-mathematical reasoning task (e.g., code generation) to evaluate generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- The trade-off between exploration and exploitation in the Markov Likelihood-based sampling scheme is not fully addressed.
- Long-term generalization to out-of-distribution reasoning tasks remains untested.
- Experimental analysis is limited to mathematical reasoning benchmarks, leaving unclear whether improvements extend to other reasoning domains.
- The impact of TEPO on inference efficiency and computational overhead is not discussed.

## Confidence
- **High Confidence**: The empirical improvements in training stability (steadier rewards, higher gradient norms) and the reported accuracy gains over GRPO on MATH-500 and other math benchmarks are well-supported by the results presented.
- **Medium Confidence**: The claim that TEPO achieves the new state of the art on MATH-500 is credible, but the robustness of these results to hyperparameter tuning and different model scales is uncertain.
- **Medium Confidence**: The assertion that TEPO reduces gradient variance and prevents entropy collapse is plausible given the experimental evidence, but the underlying mechanism is not fully explained or validated across different reasoning tasks.

## Next Checks
1. Test TEPO on non-mathematical reasoning benchmarks (e.g., code generation, commonsense reasoning) to assess generalizability.
2. Conduct a detailed ablation study to isolate the impact of the Markov Likelihood sampling component and other design choices on performance and stability.
3. Measure and report the computational overhead and inference efficiency of TEPO relative to GRPO in practical deployment scenarios.