---
ver: rpa2
title: 'Direct Confidence Alignment: Aligning Verbalized Confidence with Internal
  Confidence In Large Language Models'
arxiv_id: '2512.11998'
source_url: https://arxiv.org/abs/2512.11998
tags:
- confidence
- verbalized
- internal
- calibration
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Direct Confidence Alignment (DCA) uses Direct Preference Optimization\
  \ (DPO) to align verbalized confidence (Cv) with internal confidence (Ci) in LLMs\
  \ by training on preference pairs where Cv is replaced by Ci. The method improves\
  \ Confidence-Probability Alignment, measured by Spearman's \u03C1, and reduces calibration\
  \ error metrics (|\u03B5| and \u03C3\u03B5) across datasets, with Gemma-2-9B-Instruct\
  \ showing consistent improvements."
---

# Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models

## Quick Facts
- **arXiv ID:** 2512.11998
- **Source URL:** https://arxiv.org/abs/2512.11998
- **Reference count:** 40
- **Primary result:** DCA improves confidence-probability alignment (Spearman's ρ) and reduces calibration error (|ε|, σ_ε) on Gemma-2-9B-Instruct but shows mixed results on other architectures.

## Executive Summary
Direct Confidence Alignment (DCA) addresses the misalignment between verbalized confidence (Cv) and internal confidence (Ci) in large language models by using Direct Preference Optimization (DPO). The method generates preference pairs where Cv is replaced with Ci, then trains the model to prefer responses where verbalized confidence matches internal confidence. DCA improves confidence-probability alignment metrics without affecting model accuracy, though its effectiveness varies significantly across different model architectures.

## Method Summary
DCA uses DPO to align verbalized confidence with internal confidence in LLMs. The method generates responses on QA tasks, extracts Cv by parsing "Probability: X%" from outputs and Ci from softmax probabilities of answer tokens. Preference pairs are created where the "chosen" response has Cv replaced with Ci and the "rejected" response keeps original Cv. The model is trained with DPO using LoRA fine-tuning on attention and MLP layers, optimizing for preference of aligned responses. Evaluation measures Spearman's ρ correlation between Cv and Ci, along with calibration error metrics including mean absolute error |ε| and standard deviation σ_ε.

## Key Results
- Gemma-2-9B-Instruct showed consistent improvements in Spearman's ρ and reduced calibration error metrics across all evaluation datasets
- DCA did not improve model accuracy on any tested architecture
- Mistral-7B-Instruct and Llama-3.2-3B-Instruct exhibited degraded or mixed alignment performance on certain tasks, demonstrating architecture-dependent effectiveness

## Why This Works (Mechanism)
DCA works by creating preference pairs where the model learns to prefer responses where verbalized confidence matches internal confidence. Through DPO training, the model adjusts its generation to align Cv with Ci, reducing the gap between what the model says about its confidence and what its internal probability distribution indicates. This alignment improves the reliability of confidence estimates without requiring changes to the base model's accuracy.

## Foundational Learning
- **DPO (Direct Preference Optimization)**: Why needed - provides framework for training on preference pairs rather than direct labels. Quick check - verify loss function correctly implements IPO.
- **LoRA (Low-Rank Adaptation)**: Why needed - enables efficient fine-tuning by modifying attention and MLP layers without full model training. Quick check - confirm rank r=16 and α=16 parameters match implementation.
- **Spearman's ρ correlation**: Why needed - measures monotonic relationship between verbalized and internal confidence. Quick check - validate calculation uses correct paired samples.
- **Calibration error metrics (|ε|, σ_ε)**: Why needed - quantify misalignment between Cv and Ci. Quick check - ensure ε = Cv - Ci calculation matches paper definition.
- **Preference pair construction**: Why needed - creates training signal for alignment. Quick check - verify "chosen" vs "rejected" response assignment follows substitution rule.
- **Token position extraction for Ci**: Why needed - determines how internal confidence is computed from logits. Quick check - confirm whether single token or averaged sequence used.

## Architecture Onboarding

**Component Map**: QA prompt generation -> Response completion -> Cv extraction (parsing) -> Ci extraction (softmax logits) -> Preference pair creation -> DPO training (LoRA) -> Evaluation (ρ, |ε|, σ_ε)

**Critical Path**: The essential sequence is prompt generation → response completion → confidence extraction → preference pair creation → DPO training. Each step must succeed for the next to function, with Ci extraction from logits being particularly critical as it provides the training signal.

**Design Tradeoffs**: The method trades model accuracy (which remains unchanged) for improved confidence alignment. It requires access to model logits, limiting applicability to open-weight models. Architecture-specific effectiveness suggests the approach may need customization for different model families.

**Failure Signatures**: Underconfidence collapse where verbalized confidence shifts to 40-50% regardless of actual confidence (observed in Mistral/Llama models). High extraction failure rates (>5%) indicate issues with response parsing or token positioning. Degraded alignment on certain tasks reveals architecture-dependent limitations.

**First Experiments**: 1) Test Cv parsing extraction rate on sample responses to verify <5% failure threshold. 2) Validate Ci extraction by comparing softmax probabilities against known answer positions. 3) Run small-scale DPO training on Gemma-2-9B-Instruct to verify preference pair construction and initial alignment improvements.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Method effectiveness varies significantly across model architectures, working well on Gemma-2-9B-Instruct but degrading performance on Mistral-7B-Instruct and Llama-3.2-3B-Instruct
- Cannot improve base model accuracy, only aligns confidence estimates
- Requires access to model logits, limiting applicability to proprietary models
- Architecture-specific biases may cause underconfidence collapse or other unintended behaviors

## Confidence

**High**: The mathematical formulation of DCA using DPO with preference pairs is sound, and the confidence-probability alignment metrics (Spearman's ρ, |ε|, σ_ε) are properly computed and interpreted.

**Medium**: The preference dataset construction and training methodology are reproducible, though exact sampling distributions across MMLU subjects remain unspecified.

**Medium**: The degradation patterns in Mistral and Llama models are documented, but the underlying causes (architecture-specific biases, training dynamics) require further investigation.

## Next Checks

1. Replicate the full pipeline on Gemma-2-9B-Instruct with exact sampling distributions from MMLU subjects to verify reported improvements hold across all evaluation tasks.

2. Conduct ablation studies on Mistral-7B-Instruct to determine whether degraded performance stems from architectural differences or training configuration mismatches.

3. Test DCA on additional open-weight models (e.g., Qwen2, Phi-3) to map the boundary conditions where DCA succeeds or fails.