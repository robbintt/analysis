---
ver: rpa2
title: 'RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG
  Motor-Imagery Classification'
arxiv_id: '2510.21841'
source_url: https://arxiv.org/abs/2510.21841
tags:
- temporal
- neural
- motor
- imagery
- convolutional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RatioWaveNet, a trainable wavelet-transform-based
  front-end architecture for EEG motor-imagery classification. The core innovation
  is the use of a rational-dilated wavelet transform (RDWT) with learnable dilation
  parameters to decompose raw EEG into scale-separated, shift-invariant subbands that
  enhance sensorimotor rhythms while mitigating nonstationarity and artifacts.
---

# RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG Motor-Imagery Classification

## Quick Facts
- arXiv ID: 2510.21841
- Source URL: https://arxiv.org/abs/2510.21841
- Reference count: 40
- Primary result: Improves worst-subject accuracy over Transformer baseline by +0.17/+0.42 pp on BCI-IV-2a and +1.07/+2.54 pp on BCI-IV-2b

## Executive Summary
RatioWaveNet introduces a trainable wavelet-transform-based front-end architecture for EEG motor-imagery classification. The core innovation is a rational-dilated wavelet transform (RDWT) with learnable dilation parameters that decompose raw EEG into scale-separated, shift-invariant subbands. These subbands enhance sensorimotor rhythms while mitigating nonstationarity and artifacts. Evaluated on BCI-IV-2a and BCI-IV-2b datasets, RatioWaveNet improves worst-subject accuracy over transformer baselines with consistent average-case gains and modest computational overhead.

## Method Summary
RatioWaveNet processes raw EEG through a four-level RDWT front-end with learnable rational dilation factors, followed by soft-thresholding per subband. The reconstructed subbands are fused via grouped convolutions, processed by a multi-kernel CNN, passed through a grouped-query attention encoder with rotary positional embeddings, and finally integrated by a causal temporal convolutional network head. The model is trained with Adam (lr=0.001, batch size=64) for up to 1000 epochs with early stopping, evaluated across five seeds under both intra- and inter-subject protocols.

## Key Results
- Worst-subject accuracy improves by +0.17/+0.42 percentage points on BCI-IV-2a and +1.07/+2.54 percentage points on BCI-IV-2b
- Consistent average-case accuracy gains observed across all evaluation protocols
- Modest computational overhead compared to transformer baselines

## Why This Works (Mechanism)

### Mechanism 1
Learnable rational dilation factors adapt the spectral resolution to subject-specific sensorimotor rhythms better than fixed dyadic scales. The RDWT front-end uses a tempered logistic map to learn continuous scale parameters $s_l \in [1, S_{max}]$, creating band-pass filters fine-tuned to the precise frequency location of mu/beta rhythms which vary across individuals.

### Mechanism 2
Undecimated decomposition preserves temporal alignment and shift-invariance, preventing phase distortions that would otherwise hinder the Transformer's temporal attention. By avoiding subsampling through "same" padding, subbands remain perfectly aligned with raw EEG events, ensuring attention attends to correct temporal context.

### Mechanism 3
Grouped-Query Attention with Rotary Positional Embeddings captures long-range dependencies efficiently, mitigating quadratic cost of standard attention. GQA reduces memory overhead by sharing Key-Value projections across Query head groups, while RoPE injects relative positional information directly into attention scores.

## Foundational Learning

- **Rational Dilation Wavelet Transform (RDWT)**
  - Why needed: Standard DWT uses fixed integer scales creating "frequency gaps"; RDWT allows fractional scales providing dense time-frequency tiling essential for non-stationary EEG.
  - Quick check: Why would standard dyadic scale (2, 4, 8) fail to capture subject's motor rhythm at 11Hz?

- **Undecimated (Stationary) Wavelet Transform**
  - Why needed: Standard DWT downsamples causing shift-variance; RatioWaveNet requires shift-invariance for robustness to timing variations in EEG.
  - Quick check: If you shift input signal by 1 sample, does standard DWT output change? How does RatioWaveNet prevent this?

- **Causal Convolutions (TCN)**
  - Why needed: TCN head uses dilated causal convolutions preventing future time step access, required for online/real-time BCI systems.
  - Quick check: In causal convolution with kernel size 3 and dilation 2, does calculation for time t access t+1 or t+2?

## Architecture Onboarding

- **Component map:** Input -> RDWT Front-end -> MK-CNN -> GQA Encoder -> TCN Head -> Classifier
- **Critical path:** RDWT scale learning is the critical novelty; if logistic map temperature κ or spread regularizer λ_spr are misconfigured, front-end learns trivial filters.
- **Design tradeoffs:** Interpretability vs. Trainability (light trainable Daubechies-4 prototypes risk destroying perfect reconstruction), Efficiency vs. Redundancy (undecimated transforms produce L× more data).
- **Failure signatures:** Scale Collapse (all learned scales converge to same value), Boundary Artifacts (incorrect "same" padding), Attention Degrade (misconfigured RoPE).
- **First 3 experiments:** Ablation on "Learnable" vs. Fixed scales, Scale Visualization for good vs. bad subjects, Robustness to Jitter with artificial temporal shifts.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can selection of RDWT priors (initial scale seeds, regularization strengths) be fully automated to eliminate manual tuning while maintaining or improving convergence?
- **Open Question 2:** What compression techniques (distillation, structured pruning) can reduce computational footprint for edge deployment without degrading worst-subject robustness?
- **Open Question 3:** Does learned RDWT front end transfer effectively to BCI paradigms beyond motor imagery (P300, SSVEP, error-related potentials)?

## Limitations

- Architecture specifics (MK-CNN, GQA, TCN hyperparameters, RDWT regularization) are unspecified requiring assumptions
- EEG preprocessing details (epoch length, bandpass settings, artifact rejection) not explicitly stated
- Performance validated only on two BCI datasets; generalization to other EEG tasks untested

## Confidence

- **Primary performance claims:** Medium - Well-supported by evaluation protocol but rely on assumptions for unspecified hyperparameters
- **Interpretability and robustness:** Low - Claims made but lack detailed ablation studies or visualizations
- **Computational overhead:** Medium - Modest overhead claimed but memory usage scaling noted as potential concern

## Next Checks

1. Run ablation comparing fixed rational scales vs. learned scales to isolate gain from training mechanism and verify subject-specific adaptation
2. Apply artificial temporal shifts to test trials and compare performance degradation against CNN baseline to verify shift-invariant claim
3. Systematically vary RDWT regularization weights and temperature to diagnose and prevent scale collapse or boundary artifacts