---
ver: rpa2
title: 'StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight
  Safety Rails'
arxiv_id: '2509.02982'
source_url: https://arxiv.org/abs/2509.02982
tags:
- sleep
- adaptation
- entropy
- staging
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces StableSleep, a source-free test-time adaptation
  method for sleep staging that improves performance on unseen patients. It combines
  BatchNorm statistic refresh and entropy minimization (Tent) with two safety rails:
  an entropy gate to pause updates during uncertainty and an EMA-based reset to recover
  from drift.'
---

# StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails

## Quick Facts
- arXiv ID: 2509.02982
- Source URL: https://arxiv.org/abs/2509.02982
- Reference count: 40
- Primary result: Source-free test-time adaptation improves single-lead EEG sleep staging on unseen patients with minimal latency and memory

## Executive Summary
StableSleep introduces a streaming test-time adaptation method for sleep staging that improves performance on unseen patients without requiring source data or patient-specific calibration. The approach combines BatchNorm statistic refresh and entropy minimization (Tent) with two safety rails: an entropy gate to pause updates during uncertainty and an EMA-based reset to recover from drift. Evaluated on Sleep-EDF Expanded with single-lead EEG, StableSleep consistently outperforms a frozen baseline while maintaining seconds-level latency and minimal memory overhead.

## Method Summary
The method adapts a pre-trained compact 1D CNN for single-lead EEG sleep staging by updating only BatchNorm layers during streaming inference. It uses entropy minimization to improve target alignment while safety rails prevent catastrophic drift. The system processes 30s epochs from Fpz-Cz channel (100 Hz), applies preprocessing (notch, bandpass, standardization), and outputs predictions after causal median filtering. No source data or target labels are accessed during adaptation.

## Key Results
- Improves over frozen baseline across all metrics (per-stage accuracy, macro/weighted F1, Cohen's kappa, balanced accuracy, MCC, ECE)
- Maintains seconds-level latency suitable for on-device deployment
- Demonstrates robust performance across diverse patient cohorts without calibration
- Effective across sleep stages with minimal confusion between adjacent stages

## Why This Works (Mechanism)

### Mechanism 1: BatchNorm Statistics Refresh for Distribution Alignment
Updating BatchNorm running statistics during inference adapts the model to target distribution shifts without requiring source data. BN layers maintain running mean/variance computed during training. At test time, refreshing these statistics with target data momentum realigns feature normalization to the deployment distribution, correcting for shifts in signal amplitude, noise characteristics, or physiology.

### Mechanism 2: Entropy Minimization via Tent
Minimizing prediction entropy on unlabeled target data improves adaptation beyond statistics refresh alone. Tent updates only the affine parameters (γ, β) in BN layers by backpropagating entropy loss computed on streaming micro-batches. Lower entropy indicates more confident predictions, which under the assumption correlates with better target alignment.

### Mechanism 3: Safety Rails for Streaming Stability
An entropy gate and EMA-based reset prevent catastrophic drift and erroneous updates during streaming adaptation. The entropy gate maintains EMA of batch entropy and skips updates if outside [h_min, h_max]. The EMA reset maintains a snapshot of adapted BN parameters and rolls back when drift criteria trigger, preventing accumulated error.

## Foundational Learning

- **Batch Normalization and Test-Time Distribution Alignment**
  - Why needed: The entire adaptation mechanism depends on understanding how BN statistics encode distribution properties and how refreshing them shifts the effective feature space.
  - Quick check: If a model trained on source data with μ_s, σ_s sees target data with μ_t ≠ μ_s, what happens to activations when using frozen vs. refreshed BN statistics?

- **Entropy as a Proxy for Model Confidence and Alignment**
  - Why needed: Tent uses entropy minimization as the sole learning signal; understanding when low entropy correlates with correct predictions vs. confident errors is critical.
  - Quick check: Why does minimizing entropy on unlabeled target data help adaptation, and what failure mode does this enable?

- **Source-Free vs. Traditional Domain Adaptation Constraints**
  - Why needed: This method operates under strict constraints—no source data access, no target labels, streaming inference. These constraints define what information is available for adaptation.
  - Quick check: What can the model observe and modify during source-free TTA that enables adaptation without labels?

## Architecture Onboarding

- Component map: Pre-trained 1D CNN -> BatchNorm layers (adaptable) -> Entropy computation -> Gradient updates -> Safety rails (entropy gate, EMA reset) -> Causal median filter -> Output prediction

- Critical path: Load pre-trained source model (backbone frozen; only BN layers adaptable) → Receive streaming 30s EEG epoch → Preprocess (notch, bandpass, standardize) → Forward pass → Logits → Compute prediction entropy → Gate check against EMA bounds → If gate passes: compute entropy loss → Backprop to BN affine params → Refresh running stats → Monitor drift → Trigger EMA reset if criterion met → Apply causal median filter → Output stage prediction

- Design tradeoffs:
  - BN-only vs. BN+Tent: BN-only is simpler (no gradients) but Tent adds gains with minimal compute overhead
  - Gate threshold tuning: Tight bounds reduce update frequency (conservative) vs. loose bounds risk harmful updates
  - EMA momentum: Higher momentum = more stable but slower adaptation; lower = faster but noisier
  - Micro-batch size: Larger batches improve entropy estimate but increase latency

- Failure signatures:
  - Catastrophic drift: Rapid performance degradation with confident wrong predictions → EMA reset should trigger
  - Stagnant adaptation: No improvement over frozen baseline → gate may be too restrictive
  - Excessive flicker: Rapid prediction oscillations between stages → median filter width may need increase
  - Persistent N1 confusion: High N1→N2/REM errors → inherent single-lead limitation

- First 3 experiments:
  1. Baseline comparison: Frozen model vs. BN-only vs. BN+Tent on held-out subjects; report macro F1, Cohen's κ, and ECE to quantify adaptation gains and calibration.
  2. Safety rail ablation: Run full pipeline with (a) no rails, (b) entropy gate only, (c) EMA reset only, (d) both; measure performance stability over long streams and count reset triggers.
  3. Gate threshold sensitivity: Sweep h_min ∈ [0.5, 1.5] and h_max ∈ [1.5, 2.5]; plot adaptation frequency vs. performance to identify operating range where updates are neither blocked nor harmful.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on entropy as proxy for alignment, creating risks when model is confidently wrong
- Architecture details for compact 1D CNN are not fully disclosed
- Performance on diverse real-world EEG recordings beyond Sleep-EDF Expanded remains untested
- Single-lead constraint inherently limits performance on stages requiring multi-channel information

## Confidence
- **High Confidence**: Core contribution of combining BatchNorm refresh with lightweight safety rails is well-grounded in established TTA literature
- **Medium Confidence**: Entropy minimization mechanism (Tent) effectiveness specifically for sleep staging with proposed safety rails requires more rigorous ablation
- **Low Confidence**: Optimal hyperparameter settings (entropy gate bounds, EMA decay, reset thresholds) are not systematically explored

## Next Checks
1. Conduct hyperparameter sensitivity analysis over entropy gate thresholds and EMA decay rates to identify robust operating ranges
2. Simulate gradual distribution shifts and measure EMA reset trigger frequency and recovery time compared to baseline without safety rails
3. Evaluate adapted model on separate sleep staging dataset (MASS or SHHS) without re-training to assess real-world robustness