---
ver: rpa2
title: LLM Assistance for Pediatric Depression
arxiv_id: '2501.17510'
source_url: https://arxiv.org/abs/2501.17510
tags:
- symptoms
- patient
- notes
- clinical
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the feasibility of using state-of-the-art
  Large Language Models (LLMs) to extract depressive symptoms from pediatric electronic
  health records (EHRs) to assist in depression screening. Three LLMs (FLAN-T5, Llama
  3, and Phi) were tested for their ability to identify symptoms based on PHQ-9 criteria.
---

# LLM Assistance for Pediatric Depression

## Quick Facts
- arXiv ID: 2501.17510
- Source URL: https://arxiv.org/abs/2501.17510
- Reference count: 39
- Primary result: FLAN-T5 achieved F1 0.65 for pediatric depression symptom extraction from EHRs

## Executive Summary
This study demonstrates the feasibility of using large language models to extract depressive symptoms from pediatric electronic health records, achieving promising results with FLAN-T5 (F1: 0.65) for symptom detection. The extracted symptoms were successfully used as features in a machine learning model that differentiated depression cases from controls with high precision (0.78). The approach shows particular strength in detecting rare symptoms like sleep problems and self-loathing, though performance on common symptoms remains limited.

## Method Summary
The study evaluated three LLMs (FLAN-T5, Llama 3, and Phi) for extracting PHQ-9 depression symptoms from pediatric EHRs. FLAN-T5 achieved the highest precision (0.78) and overall F1 score (0.65), while Llama 3 showed the highest recall (0.90) but tended to overgeneralize. The extracted symptom annotations were then used as features in a machine learning model to classify depression cases versus controls, achieving high precision (0.78) in this task.

## Key Results
- FLAN-T5 achieved highest precision (0.78) and F1 score (0.65) among tested LLMs
- Llama 3 showed highest recall (0.90) but overgeneralized symptoms
- Extracted symptoms enabled ML model to differentiate depression cases with 0.78 precision
- FLAN-T5 excelled at rare symptoms: sleep problems (F1: 0.92), self-loathing (F1: 0.8)

## Why This Works (Mechanism)
LLMs can effectively parse clinical narratives in EHRs to identify structured symptom data that would otherwise require manual review. The PHQ-9 framework provides a standardized symptom ontology that LLMs can learn to recognize within free-text clinical documentation. By converting unstructured clinical narratives into structured symptom features, LLMs enable the application of traditional ML classification techniques to identify depression cases, bridging the gap between unstructured clinical data and structured diagnostic criteria.

## Foundational Learning
- PHQ-9 depression screening criteria - Standardized 9-item questionnaire for depression assessment
  - Why needed: Provides consistent symptom framework for LLM training and evaluation
  - Quick check: Does the model recognize all 9 PHQ-9 symptom categories?

- EHR narrative extraction - Process of identifying structured information from unstructured clinical text
  - Why needed: Clinical notes contain rich symptom data not captured in structured fields
  - Quick check: Can the model handle varied clinical writing styles and abbreviations?

- Symptom annotation quality - Accuracy of extracted symptom labels compared to ground truth
  - Why needed: Determines reliability of LLM output for downstream ML tasks
  - Quick check: What is the inter-rater agreement between LLM and human annotators?

## Architecture Onboarding

Component map: EHR Clinical Notes -> LLM Symptom Extractor -> Symptom Annotations -> ML Classifier -> Depression Classification

Critical path: Clinical notes must be successfully parsed by LLM to generate accurate symptom annotations, which then serve as input features for the ML classifier. The quality of symptom extraction directly determines classification performance.

Design tradeoffs: The study prioritized precision over recall (FLAN-T5 vs Llama 3), accepting potentially missed symptoms to avoid false positives. This conservative approach may be appropriate for clinical screening but could miss some depression cases.

Failure signatures: Overgeneralization (Llama 3) leads to false positive symptom detections. Poor performance on common symptoms suggests limitations in handling frequently occurring clinical descriptions. Institutional specificity indicates potential adaptation requirements for different clinical settings.

First experiments:
1. Test FLAN-T5 on a small sample of EHRs with known diagnoses to verify symptom extraction accuracy
2. Compare LLM-extracted symptoms against structured PHQ-9 questionnaire responses when available
3. Evaluate model performance across different age subgroups within the pediatric population

## Open Questions the Paper Calls Out
None

## Limitations
- Single institutional dataset limits generalizability across different clinical settings
- Modest F1 scores (maximum 0.65) indicate substantial room for improvement
- Focus exclusively on PHQ-9 criteria limits applicability to other depression screening instruments
- Validation only through ML classification, not direct clinical diagnosis comparison

## Confidence

Major Claim Confidence Assessment:
- LLM feasibility for symptom extraction: Medium - Demonstrated technical capability but with limited precision and institutional specificity
- Clinical utility as complementary screening tool: Low - Promising ML model results but no direct clinical outcome validation
- FLAN-T5 superiority for rare symptoms: High - Statistically significant performance differences clearly demonstrated

## Next Checks

1. External validation on multi-institutional EHR datasets to assess generalizability across different clinical documentation styles and populations
2. Direct comparison of LLM-extracted symptoms against structured clinical interviews and diagnostic gold standards rather than proxy machine learning classifications
3. Evaluation of model performance across diverse pediatric age groups, comorbidities, and demographic characteristics to identify potential bias or blind spots