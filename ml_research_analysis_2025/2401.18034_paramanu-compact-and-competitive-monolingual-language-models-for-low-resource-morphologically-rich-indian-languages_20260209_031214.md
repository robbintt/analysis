---
ver: rpa2
title: 'Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource
  Morphologically Rich Indian Languages'
arxiv_id: '2401.18034'
source_url: https://arxiv.org/abs/2401.18034
tags:
- language
- languages
- hindi
- linguistics
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PARAMANU introduces the first family of compact, from-scratch
  Indian language autoregressive models for the five most spoken Indian languages:
  Bengali, Hindi, Marathi, Tamil, and Telugu. These models range from 108M to 367M
  parameters and are trained under $1,000 on a single GPU.'
---

# Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages

## Quick Facts
- arXiv ID: 2401.18034
- Source URL: https://arxiv.org/abs/2401.18034
- Authors: Mitodru Niyogi; Eric Gaussier; Arnab Bhattacharya
- Reference count: 40
- Paramanu models outperform larger multilingual models despite being 20x smaller

## Executive Summary
PARAMANU introduces the first family of compact, from-scratch Indian language autoregressive models for Bengali, Hindi, Marathi, Tamil, and Telugu. These models range from 108M to 367M parameters and are trained under $1,000 on a single GPU. To address low-resource challenges, the authors develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE scaling for efficient long-sequence training, and create instruction-tuning datasets in Bangla translated to other languages. Despite their small size, Paramanu models outperform most larger multilingual models (up to 8B parameters) across all five languages.

## Method Summary
Paramanu trains decoder-only transformer models (108M-367M params) on cleaned monolingual corpora using hybrid BPE+Unigram tokenizers for better morphological handling. Models use RMSNorm, SwiGLU activation, and RoPE with position interpolation for longer contexts. Training employs AdamW with cosine learning rate decay, BF16 mixed precision, and μP transfer from 15M models. Instruction tuning uses machine-translated datasets from Bangla. The approach emphasizes depth (quality tokenization, cleaning) over breadth (multilinguality, scale) to achieve competitive performance on zero-shot benchmarks.

## Key Results
- Paramanu-Bangla (108M) outperforms 10 out of 13 LLMs in the 500M-3B range and rivals Sarvam 2B (2B) despite being 20x smaller
- Instruction-tuned models show significant gains: Hindi improves by 9% and Bangla by 4.3% on average benchmarks
- Effective cross-lingual transfer within the Devanagari script family, with Hindi→Marathi transfer helping and Marathi→Hindi transfer hurting
- Tokenizers achieve lowest fertility scores across all five languages compared to Sarvam 2B, Llama-3.1, Gemma-2, and GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid BPE+Unigram tokenizers reduce fertility and improve morphological coherence for Indian languages
- Mechanism: Unigram LM produces subword units that better capture morphological structure through global optimization; augmenting BPE vocabulary with these tokens allows the tokenizer to preserve stems and frequent suffixes as atomic units rather than over-fragmenting
- Core assumption: Morphologically coherent subwords lead to better downstream task performance because the model learns semantically meaningful units rather than arbitrary character sequences
- Evidence anchors:
  - [abstract] "we develop morphology-aligned, low-fertility tokenizers"
  - [Section 3.3] "Our tokenizers achieve the least fertility scores across all five languages compared to Sarvam 2B, Llama-3.1, Gemma-2, and GPT-4o"
  - [Table 4] Ablation shows 1.91% (Bangla) to 2.02% (Telugu) improvement from Unigram tokens
  - [corpus] Related work on tokenization inefficiency for morphologically complex languages (The Token Tax) supports this mechanism
- Break condition: If BPE vocabulary already covers morphological patterns adequately, Unigram augmentation yields diminishing returns

### Mechanism 2
- Claim: Position interpolation enables training longer sequences on memory-constrained hardware
- Mechanism: Scale token position indices by factor α = y/L (target length / permissible length) before RoPE application; RoPE's reliance on relative positions through trigonometric functions means fractional positions remain valid, preserving long-range dependency learning
- Core assumption: Linear interpolation of position indices does not catastrophically disrupt the model's ability to learn positional relationships
- Evidence anchors:
  - [abstract] "propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently"
  - [Table 5] Average benchmark scores generally increase with larger context sizes (α=4, ctx=1024 performs best for most languages)
  - [Figure 2] Illustrates how relative offsets are preserved through scaling
  - [corpus] No direct corpus validation of this specific interpolation method; related work on context extension exists but doesn't validate this exact approach
- Break condition: If α becomes too large (extreme compression), positional resolution may degrade, injecting noise

### Mechanism 3
- Claim: Small monolingual models outperform larger multilingual models when trained on clean, language-specific data with appropriate tokenization
- Mechanism: Avoiding multilingual data imbalance, English-centric bias, and tokenizer over-segmentation allows compact models to efficiently utilize parameters; cross-lingual transfer within script families (Devanagari) provides additional generalization
- Core assumption: Limited compute budget is better spent on depth (quality, tokenization, cleaning) than breadth (multilinguality, scale)
- Evidence anchors:
  - [abstract] "Paramanu-Bangla (108M) outperforms 10 out of 13 LLMs in the 500M-3B range"
  - [Section 5.3] "Paramanu-Sanskrit 139M, trained without Hindi, achieves 31.05 avg on Hindi, outperforming xGLM 564M"
  - [Table 4] Data cleaning adds 1-3.5% improvement across languages
  - [corpus] Related work (Multilingual Tokenization through the Lens of Indian Languages) confirms tokenizer bias against morphologically rich languages
- Break condition: When tasks require broad world knowledge or cross-script transfer beyond the training language, compact monolingual models plateau

## Foundational Learning

- **Subword Tokenization (BPE vs Unigram):**
  - Why needed here: The hybrid tokenizer requires understanding how BPE's greedy merging differs from Unigram's probabilistic approach
  - Quick check question: Given the Telugu word example in Table 7, why does BPE split the root but the hybrid tokenizer preserves it?

- **Rotary Position Embeddings (RoPE):**
  - Why needed here: Position interpolation mechanism depends on understanding that RoPE operates on relative positions via trigonometric functions
  - Quick check question: If you double the context length with interpolation factor α=2, what happens to the relative distance between tokens at positions 100 and 101?

- **Morphological Richness in Indian Languages:**
  - Why needed here: Understanding why standard tokenizers fail for agglutinative languages motivates the hybrid approach
  - Quick check question: Why might a tokenizer trained primarily on English produce more tokens per word for Tamil than for English?

## Architecture Onboarding

- **Component map:**
  - Data cleaning (regex filtering → language ID → deduplication) → Tokenizer training (BPE + Unigram merge) → Pretraining (100K-150K steps, single A100 40GB) → Instruction tuning (23K examples, translated from Bangla)

- **Critical path:**
  Tokenizer training → Pretraining → Instruction tuning

- **Design tradeoffs:**
  - Smaller models (108M) train faster but require longer training to match larger models (367M) at fixed step counts
  - Machine-translated instruction data introduces artifacts; Hindi gains (+9%) exceed Tamil (+1.46%) due to translation quality
  - Cross-lingual transfer works within script (Devanagari) but is asymmetric: Hindi→Marathi helps, Marathi→Hindi hurts

- **Failure signatures:**
  - MMLU scores near random baseline (models trained on limited-domain corpora: news, Wikipedia, literature)
  - N-shot degradation (0-shot → 25-shot performance drops on XNLI, XStoryCloze)
  - Overfitting risk from multi-epoch training on small corpora

- **First 3 experiments:**
  1. Replicate the tokenizer ablation: train a 15M param model with BPE-only vs BPE+Unigram on the same corpus, compare fertility and downstream task scores
  2. Test context interpolation: pretrain with α=1 (ctx=256) vs α=4 (ctx=1024), measure perplexity and benchmark performance to validate the tradeoff
  3. Validate cross-script transfer: take Paramanu-Marathi and evaluate zero-shot on Hindi benchmarks, compare against a fresh Hindi model of equivalent size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance-efficiency tradeoff of monolingual models hold when scaling the methodology to all 22 official Indian languages?
- Basis in paper: [explicit] The conclusion states: "In the future, we aim to scale our methodology to all 22 official Indian languages..."
- Why unresolved: The current study is limited to the five most spoken Indian languages, leaving the remaining 17 officially recognized languages untested with this specific architecture.
- What evidence would resolve it: Benchmark results for Paramanu models trained on the remaining 17 languages compared against multilingual baselines.

### Open Question 2
- Question: To what extent does the lack of domain diversity in training data cause performance degradation on academic benchmarks like MMLU?
- Basis in paper: [inferred] The Limitations section notes the dataset lacks "law, science, education... As a result, the models perform sub-optimally on benchmarks like MMLU."
- Why unresolved: It is unclear if the sub-optimal MMLU performance is due to model size (compactness) or solely the absence of diverse domain-specific training data.
- What evidence would resolve it: Evaluation of models pretrained on a diversified corpus including scientific and legal texts compared to the current news/Wikipedia-heavy corpus.

### Open Question 3
- Question: How significant is the performance gap between machine-translated instruction data and human-curated instruction data for low-resource Dravidian languages?
- Basis in paper: [explicit] The authors note "Gains are smaller for Tamil (+1.46%) and Telugu (+2.28%), likely due to lower-quality machine translation artifacts in instruction tuning datasets."
- Why unresolved: The specific performance ceiling imposed by translation noise versus model capacity has not been isolated for these script families.
- What evidence would resolve it: Ablation studies comparing current models against versions fine-tuned on human-verified instruction sets for Tamil and Telugu.

### Open Question 4
- Question: Why do Paramanu models exhibit performance degradation in few-shot settings (n-shot) compared to zero-shot settings?
- Basis in paper: [explicit] The paper observes: "performance drops from 0-shot to 25-shot on XNLI-Hindi... This may result from in-context examples acting as soft constraints that hinder generation."
- Why unresolved: The authors identify the phenomenon but do not determine if it is a symptom of model compactness, tokenizer alignment, or specific architectural constraints.
- What evidence would resolve it: Analysis of attention patterns and output generation in few-shot vs. zero-shot settings across different model sizes.

## Limitations

- Tokenizer design gaps: Lack of systematic analysis of optimal vocabulary sizes across languages; modest improvements (1.91-2.02%) may not justify added complexity
- Context interpolation validation: Novel approach with preliminary validation but lacking edge case analysis and analysis of when interpolation becomes counterproductive
- Instruction tuning artifacts: Machine-translated datasets introduce systematic bias; reported improvements likely reflect translation quality differences rather than inherent language characteristics
- Narrow evaluation scope: Models trained exclusively on news, Wikipedia, and literature data, explaining near-random MMLU performance and raising questions about real-world applicability

## Confidence

**High Confidence (Mechanisms Well-Supported)**:
- Mechanism 1 (Hybrid tokenization): Strong empirical support through fertility comparisons and ablation studies
- Mechanism 3 (Monolingual advantage): Clear quantitative evidence against multilingual baselines

**Medium Confidence (Promising but Under-Validated)**:
- Mechanism 2 (Position interpolation): Novel approach with preliminary validation but lacking edge case analysis

## Next Checks

1. **Tokenizer Complexity Validation**: Replicate the 15M parameter model ablation with BPE-only (large vocab) versus BPE+Unigram configurations on identical corpora. Measure both fertility scores and downstream task performance to determine if hybrid complexity yields meaningful gains over simpler approaches.

2. **Context Interpolation Stress Test**: Systematically vary interpolation factor α from 1 to 8 while measuring perplexity, benchmark scores, and positional resolution degradation. Identify the point where positional compression introduces measurable noise versus performance gains.

3. **Translation Quality Impact Analysis**: Generate a small human-verified instruction dataset for one language (e.g., Hindi) and compare instruction-tuned model performance against the machine-translated version. Quantify the translation noise impact on the reported instruction-tuning improvements.