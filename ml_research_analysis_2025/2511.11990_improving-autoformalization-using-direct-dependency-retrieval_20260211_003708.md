---
ver: rpa2
title: Improving Autoformalization Using Direct Dependency Retrieval
arxiv_id: '2511.11990'
source_url: https://arxiv.org/abs/2511.11990
tags:
- dependency
- arxiv
- retrieval
- formal
- autoformalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dependency hallucination
  in statement autoformalization, where language models often generate incorrect or
  non-existent formal dependencies during translation from natural language to formal
  representations. The proposed Direct Dependency Retrieval (DDR) method generates
  candidate formal dependencies directly from informal mathematical descriptions,
  followed by verification against the formal library using an efficient Suffix Array
  Check (SAC).
---

# Improving Autoformalization Using Direct Dependency Retrieval

## Quick Facts
- arXiv ID: 2511.11990
- Source URL: https://arxiv.org/abs/2511.11990
- Reference count: 32
- Primary result: Reduces dependency hallucination from ~30% to <2% while improving autoformalization success rates by up to 15 percentage points

## Executive Summary
This paper addresses the critical challenge of dependency hallucination in statement autoformalization, where language models generate incorrect or non-existent formal dependencies during translation from natural language to formal representations. The authors propose Direct Dependency Retrieval (DDR), a generation-plus-verification pipeline that fine-tunes LLMs to directly predict candidate dependencies from informal statements, followed by efficient verification using a suffix array check (SAC). DDR achieves state-of-the-art performance with precision and recall of 0.84-0.91 and 0.82-0.92 respectively across different difficulty levels, while reducing hallucination rates to nearly zero (<0.02) compared to approximately 30% in existing methods.

## Method Summary
DDR implements a generation-plus-verification pipeline for dependency retrieval in statement autoformalization. The method fine-tunes a Qwen3-32B model to directly extract candidate dependencies from informal mathematical statements, then verifies these candidates against the formal library using an efficient suffix array check (SAC). SAC constructs a suffix array over all library identifiers and uses binary search to verify candidate dependencies in O(sM log N) time, where s is the number of candidates, M is the maximum identifier length, and N is the number of library objects. The training data is automatically generated by processing existing informal-formal statement pairs from FineLeanCorpus and extracting verified dependencies via SAC, eliminating the need for manual annotation.

## Key Results
- DDR achieves precision and recall of 0.84-0.91 and 0.82-0.92 respectively across different difficulty levels
- Hallucination rate reduced to nearly zero (mean < 0.02) compared to approximately 30% in existing ICL methods
- Improves pass@8 success rates by up to 15 percentage points when integrated with downstream autoformalization tasks
- SAC verification operates in O(sM log N) time versus O((s+d)MN) for brute-force methods

## Why This Works (Mechanism)

### Mechanism 1: Generation-Plus-Verification Pipeline for Dependency Retrieval
DDR fine-tunes LLMs to directly predict likely dependency names from informal statements, then verifies existence using SAC. This outperforms embedding-based selection methods by learning the mapping from mathematical language patterns to formal identifier conventions. The approach assumes LLMs can better capture these mappings than embedding similarity.

### Mechanism 2: Suffix Array Check for Hallucination Detection
SAC provides O(sM log N) verification by constructing a suffix array over all library identifiers and using binary search to verify candidates. This catches hallucinated identifiers before they reach the autoformalizer. The approach assumes the formal library is relatively static, allowing precomputation.

### Mechanism 3: Training Data Construction via SAC-Based Labeling
SAC enables automatic extraction of ground-truth dependencies from existing formal code, producing 500K+ training pairs without manual annotation. The approach processes informal-formal statement pairs from FineLeanCorpus, extracting identifiers from formal code and verifying them via SAC.

## Foundational Learning

- **Statement Autoformalization**: Translating natural language mathematics into formal theorem prover syntax. Why needed: The entire paper targets this task, and understanding it is essential for evaluating dependency retrieval quality.
- **Dependency in Formal Libraries**: Library definitions/theorems that a statement needs. Why needed: DDR's core contribution is identifying these dependencies, making this concept critical for evaluation.
- **Hallucination in Generative Retrieval**: Generating non-existent identifiers. Why needed: The paper's headline result is reducing hallucination from ~30% to <2%, so understanding this concept is critical.

## Architecture Onboarding

- Component map: SAC construction -> Training data generation via SAC -> DDR fine-tuning -> Inference (DDR -> SAC verification -> Autoformalizer)
- Critical path: SAC construction enables training data generation, which enables DDR fine-tuning, which enables inference with SAC verification
- Design tradeoffs: DDR outputs abbreviated names (high recall, needs suffix-matching) vs full qualified names (lower recall, exact matching); single retrieval per problem (efficiency) vs per-attempt retrieval (higher pass@k but slower)
- Failure signatures: High hallucination rate (>10%) indicates SAC integration issues; low recall on complex statements suggests DDR undertraining; type checking passes but BEq fails indicates semantic errors not DDR issues
- First 3 experiments: 1) Reproduce hallucination baseline with ICL methods, 2) SAC verification in isolation to measure false negative rate, 3) Ablate DDR training data size to validate scalability

## Open Questions the Paper Calls Out
None

## Limitations

- Dependency extraction logic from formal Lean code is not fully specified, creating uncertainty about training data quality
- BEq verification implementation details are incomplete, affecting exact pass@8 metric calculation
- Library naming conventions and edge cases for abbreviated vs. fully-qualified names are not fully characterized

## Confidence

- **High Confidence**: Core mechanism of suffix array verification (SAC) and its O(sM log N) complexity analysis
- **Medium Confidence**: DDR training data construction process, though implementation details are unspecified
- **Low Confidence**: Integration with downstream autoformalizer performance due to unspecified baseline model details

## Next Checks

1. **SAC False Negative Rate Analysis**: Extract 100 formal statements from FineLeanCorpus, run SAC verification on all identifiers, and measure the false negative rate to validate suffix-based matching criterion

2. **Training Data Quality Assessment**: Take a stratified sample of 200 DDR training pairs, manually verify informal statements contain sufficient mathematical terminology, and check for systematic extraction errors

3. **Ablation on DDR Output Format**: Run DDR inference with both abbreviated and fully-qualified dependency names, measure SAC verification success rates for each format, and determine whether suffix-matching handles all edge cases