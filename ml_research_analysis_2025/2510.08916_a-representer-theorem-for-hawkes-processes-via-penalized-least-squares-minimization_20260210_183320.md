---
ver: rpa2
title: A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization
arxiv_id: '2510.08916'
source_url: https://arxiv.org/abs/2510.08916
tags:
- hawkes
- kernels
- kernel
- processes
- bonnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a novel representer theorem for estimating
  triggering kernels in linear multivariate Hawkes processes using penalized least
  squares minimization. Under an RKHS framework, the optimal estimators of triggering
  kernels are expressed as linear combinations of transformed kernels defined through
  a system of Fredholm integral equations, with all dual coefficients analytically
  fixed to unity.
---

# A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization

## Quick Facts
- arXiv ID: 2510.08916
- Source URL: https://arxiv.org/abs/2510.08916
- Reference count: 40
- Primary result: Novel representer theorem for Hawkes process triggering kernel estimation using penalized least squares with closed-form solutions

## Executive Summary
This paper establishes a representer theorem for estimating triggering kernels in linear multivariate Hawkes processes using penalized least squares minimization in a Reproducing Kernel Hilbert Space (RKHS) framework. The key innovation is that under this formulation, the dual coefficients in the optimal estimator are analytically fixed to unity, eliminating the need for costly iterative optimization. The paper also proposes an efficient algorithm using random feature map approximation that computes all required integrals in closed form, avoiding discretization.

## Method Summary
The method estimates triggering kernels g_ij(s) in linear multivariate Hawkes processes by minimizing a penalized least squares functional over an RKHS. Using Gaussian RKHS kernels, the paper derives a representer theorem showing optimal estimators are linear combinations of transformed kernels defined through Fredholm integral equations. The dual coefficients are analytically fixed to unity. For computational efficiency, random Fourier features approximate the RKHS kernel, allowing closed-form computation of integrals and a single matrix inversion to obtain both baseline intensities and triggering kernels.

## Key Results
- Dual coefficients in the optimal estimator are analytically fixed to unity (α = 1), eliminating iterative optimization
- Random feature approximation computes all integrals in closed form, avoiding discretization
- Method achieves competitive predictive accuracy compared to state-of-the-art while improving computational efficiency
- Empirical results show the method scales favorably with sequence length compared to likelihood-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The least squares formulation enables closed-form solutions for dual coefficients in the Hawkes process triggering kernel estimation.
- Mechanism: The least squares loss functional takes a quadratic form with respect to both the triggering kernels and baseline intensities. When taking functional derivatives and setting them to zero (calculus of variations), the resulting optimality conditions yield a system of linear equations rather than non-linear ones. This linearity allows the dual coefficients to be analytically determined as unity (α = 1) without iterative optimization.
- Core assumption: The intensity function can be modeled linearly (identity link function), which does not guarantee non-negativity of the intensity function.

### Mechanism 2
- Claim: Fredholm integral equations of the second kind define the equivalent kernels required by the representer theorem.
- Mechanism: The optimality conditions derived from the functional derivatives result in a system of simultaneous Fredholm integral equations (Equations 6 and 7). Solving this system yields a set of transformed kernel functions, termed "equivalent kernels" (h_j(·,·)). The optimal triggering kernel estimator is then expressed as a linear combination of these equivalent kernels evaluated at event times.
- Core assumption: The equivalent kernels exist and can be obtained by solving the coupled integral equations.

### Mechanism 3
- Claim: Random Fourier features approximation allows for a closed-form computation of the required integrals, avoiding discretization.
- Mechanism: By approximating the RKHS kernel as a finite sum of trigonometric feature maps (Random Fourier Features), the integral operations in the matrix Ξ (Equation 12) and the estimator (Equation 14) can be computed analytically. This is because the integrals of products of sine and cosine functions have known closed-form solutions. This converts the problem into one involving only matrix additions and a single matrix inversion.

## Foundational Learning
- **Reproducing Kernel Hilbert Spaces (RKHS)**: Why needed - The theoretical foundation of the method relies on assuming the triggering kernels lie in an RKHS. The representer theorem guarantees a finite-dimensional solution in this space. Quick check - Can you explain why functions in an RKHS have the "reproducing" property and how it allows for a "kernel trick"?
- **Hawkes Processes**: Why needed - This is the underlying temporal point process model. Understanding its conditional intensity function and the role of triggering kernels is essential to grasping the estimation problem. Quick check - In a multivariate Hawkes process, how does an event of type j influence the intensity of events of type i?
- **Calculus of Variations**: Why needed - The core of the paper's derivation involves solving a functional optimization problem by taking functional derivatives and setting them to zero. Quick check - How does a functional derivative differ from a standard derivative, and what does setting it to zero signify?

## Architecture Onboarding
- **Component map**: Input event sequences -> Feature map computation -> Matrix construction (Ξ) -> Baseline estimation (μ̂) -> Kernel estimation (ĝ_ij)
- **Critical path**: The inversion of the (1/γ I_MU + Ξ) matrix is the central computational step. Both baseline and kernel estimation depend on it. The accuracy depends heavily on the choice of M (number of features) and the kernel hyperparameters (γ, β).
- **Design tradeoffs**: Scalability vs. Accuracy (increasing M improves kernel approximation but increases matrix size and inversion cost); Speed vs. Theoretical Optimality (least squares loss is computationally faster but may have higher estimation bias compared to maximum likelihood methods).
- **Failure signatures**: High estimation error on small datasets (method may underperform likelihood-based methods when data is sparse); Negative intensity values (linear model doesn't enforce non-negativity, requiring post-hoc clipping); Cubic scaling in U (performance degrades rapidly for very high-dimensional processes).
- **First 3 experiments**: (1) Reproduce synthetic data results comparing estimated kernels to ground truth; (2) Ablation study on M varying the number of random features to observe trade-off between computational cost and estimation accuracy; (3) Scalability test measuring CPU time for increasing sequence lengths and comparing to baseline methods.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the analytical representer theorem be extended to non-linear Hawkes processes (e.g., using soft-plus link functions) to guarantee intensity non-negativity without post-hoc clipping? The paper notes the linear assumption "does not guarantee the non-negativity of the intensity function" and suggests enforcing non-negativity directly if excitatory interactions are known a priori.
- **Open Question 2**: Can iterative solvers, such as the conjugate gradient method, effectively mitigate the cubic computational scaling with process dimensionality U caused by matrix inversion? The paper suggests this issue "may be mitigated using iterative solvers" but provides no empirical validation.
- **Open Question 3**: How does the least squares-based estimator compare to likelihood-based methods in terms of statistical efficiency and robustness on large-scale, real-world datasets? The paper evaluates on synthetic data and notes Maximum Likelihood Estimation is "statistically efficient asymptotically," implying a potential trade-off with the least squares loss used here.

## Limitations
- The linear assumption in the least squares formulation does not guarantee non-negative intensity values, requiring post-hoc clipping
- The method's computational efficiency advantage diminishes for very high-dimensional processes due to cubic scaling in the number of dimensions
- The random feature approximation introduces an approximation error that must be balanced against computational cost

## Confidence
- **Closed-form dual coefficients**: High confidence - The analytical solution follows directly from the quadratic form of the least squares objective
- **Fredholm integral equations**: Medium confidence - The derivation appears sound, but existence/uniqueness depends on specific kernel properties
- **Computational efficiency**: High confidence - The O(N²M²U² + M³U³) complexity is clearly derived and empirically validated
- **Predictive accuracy**: Medium confidence - Competitive on synthetic data but shows higher estimation bias than likelihood-based methods for small datasets

## Next Checks
1. Apply the method to real-world event sequence datasets (e.g., social media interactions, financial transactions) to verify theoretical advantages translate to practical settings
2. Systematically vary Gaussian kernel bandwidth β and regularization parameter γ to characterize their impact on estimation accuracy and computational stability
3. Modify the framework to incorporate exponential or soft-plus link functions for intensity, analyzing how this affects the closed-form solution and computational advantages