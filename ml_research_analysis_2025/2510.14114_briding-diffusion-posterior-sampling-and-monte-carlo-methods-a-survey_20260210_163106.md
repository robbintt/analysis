---
ver: rpa2
title: 'Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey'
arxiv_id: '2510.14114'
source_url: https://arxiv.org/abs/2510.14114
tags:
- distribution
- sampling
- where
- diffusion
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models enable sampling from complex distributions and
  have been adapted for Bayesian inverse problems by serving as priors. This survey
  reviews methods that use pre-trained diffusion models alongside Monte Carlo techniques
  to solve such problems without additional training.
---

# Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey

## Quick Facts
- arXiv ID: 2510.14114
- Source URL: https://arxiv.org/abs/2510.14114
- Reference count: 40
- The survey reviews methods using pre-trained diffusion models with Monte Carlo techniques to solve Bayesian inverse problems without additional training.

## Executive Summary
This survey explores the intersection of diffusion models and Monte Carlo methods for Bayesian inverse problems. The authors review how pre-trained diffusion models can serve as expressive priors for complex data distributions, and how various Monte Carlo techniques can be used to sample from the posterior distributions induced by these priors. The core innovation involves "twisting" intermediate distributions within the diffusion process to guide simulations toward the posterior, enabling efficient sampling without retraining the diffusion model.

The review covers guidance-based sampling using approximate guidance terms from pre-trained denoisers, sequential Monte Carlo methods for constructing intermediate distributions, and Markov chain Monte Carlo approaches for these intermediate targets. Special attention is given to inpainting problems and methods like SMCDiff and MCGDiff. The survey identifies key trade-offs between computational cost and sampling accuracy, and proposes strategies for handling posterior intractability.

## Method Summary
The survey synthesizes methods that combine pre-trained diffusion models with Monte Carlo techniques to solve Bayesian inverse problems. The fundamental approach involves using the diffusion model as a prior distribution and then employing various Monte Carlo methods to sample from the posterior given observed data. The key mechanism is "twisting" - constructing intermediate distributions between the prior and posterior that can be more easily sampled. These twisted distributions are then targeted using sequential Monte Carlo, MCMC, or guidance-based approaches. The methods leverage the pre-trained denoiser from the diffusion model to provide approximate guidance without requiring additional training.

## Key Results
- Development of efficient, learning-free algorithms for posterior sampling under diffusion priors
- Identification of computational trade-offs between sampling accuracy and efficiency across different Monte Carlo approaches
- Demonstration of diffusion models as expressive priors enabling diverse, high-fidelity reconstructions compared to traditional priors

## Why This Works (Mechanism)
The approach works by leveraging the powerful generative capabilities of pre-trained diffusion models as priors for Bayesian inverse problems. By constructing intermediate "twisted" distributions between the prior and posterior, the methods create tractable targets for Monte Carlo sampling. The pre-trained denoiser provides crucial information about the data distribution that guides the sampling process toward the posterior without requiring additional model training. This combination of expressive priors with flexible Monte Carlo sampling schemes enables efficient posterior exploration in high-dimensional spaces.

## Foundational Learning

**Diffusion Models** - Generative models that learn to denoise data through a Markov chain. Why needed: Serve as expressive priors for complex data distributions. Quick check: Verify the diffusion model can generate realistic samples from the prior distribution.

**Bayesian Inverse Problems** - Framework for inferring parameters from observed data using probabilistic models. Why needed: Provides the mathematical foundation for posterior inference. Quick check: Confirm the observation model correctly represents the measurement process.

**Sequential Monte Carlo** - Particle-based methods for sampling from sequences of distributions. Why needed: Enables sampling from intermediate twisted distributions. Quick check: Validate particle diversity and effective sample size across iterations.

**Markov Chain Monte Carlo** - Stochastic sampling methods for generating samples from complex distributions. Why needed: Provides alternative approach for sampling from twisted distributions. Quick check: Check MCMC convergence diagnostics and autocorrelation.

## Architecture Onboarding

**Component Map**: Data/Observations -> Observation Model -> Posterior Distribution -> Twisted Distributions -> Monte Carlo Sampler -> Posterior Samples

**Critical Path**: The critical path involves computing the twisted distributions from the posterior and prior, then applying the chosen Monte Carlo method to sample from these twisted targets. The efficiency bottleneck typically occurs in computing guidance terms or evaluating intermediate distributions.

**Design Tradeoffs**: SMC offers theoretical guarantees but requires many particles; MCMC is more sample-efficient but may mix slowly; guidance-based methods are fast but rely on quality denoiser approximations. The choice depends on problem dimensionality, available computational resources, and required sample quality.

**Failure Signatures**: Poor mixing in MCMC indicates inadequate twisting; particle degeneracy in SMC suggests insufficient diversity in intermediate distributions; guidance approximation errors manifest as biased samples or mode collapse.

**First Experiments**: 1) Test SMC on a simple linear inverse problem with known ground truth. 2) Compare MCMC and guidance-based sampling on an inpainting task. 3) Evaluate computational scaling as dimensionality increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we rigorously compare and select the optimal sequence of intermediate distributions for twisting in diffusion posterior sampling?
- Basis in paper: The authors state that identifying a method for comparing different sequences and quantifying their proximity is an open problem, questioning if Ï€^y_k is truly the optimal target.
- Why unresolved: There is no consensus on the suitable divergence measures or distance metrics to evaluate these sequences effectively.
- What evidence would resolve it: A theoretical framework or empirical benchmark establishing that a specific sequence minimizes sampling variance or error compared to existing baselines.

### Open Question 2
- Question: Which Monte Carlo sampling scheme (SMC, Gibbs, Langevin, or VI) offers the best trade-off between computational efficiency and sampling accuracy?
- Basis in paper: The authors ask which method provides the best trade-off, noting that the efficiency and scalability of non-amortized VI compared to SMC or Langevin dynamics are not fully understood.
- Why unresolved: Each method has distinct strengths (e.g., SMC theoretical guarantees vs. VI parallelism), but a definitive comparative analysis is lacking.
- What evidence would resolve it: Comprehensive benchmarks across diverse inverse problems measuring FID, LPIPS, and runtime for each sampling family under identical conditions.

### Open Question 3
- Question: Can vector-Jacobian product-free guidance methods match the performance of DPS-type potentials while reducing computational costs?
- Basis in paper: The authors highlight that while DPS-type potentials outperform others in metrics, they are memory-intensive and slow; they ask if Jacobian-free methods can bridge this gap.
- Why unresolved: Current high-fidelity methods rely on expensive gradient computations (vector-Jacobian products) that limit scalability in resource-constrained environments.
- What evidence would resolve it: Development of an algorithm that achieves equivalent Frechet Inception Distance and Learned Perceptual Image Patch Similarity scores without relying on expensive backpropagation through the denoiser.

## Limitations
- Computational trade-offs between different Monte Carlo methods are not rigorously quantified across diverse problem settings
- Performance guarantees rely heavily on quality of pre-trained denoisers without systematic evaluation of failure modes
- Survey focuses on theoretical formulations without extensive empirical validation across benchmark inverse problems

## Confidence
- Computational trade-off quantification: Medium
- Guidance quality dependency analysis: Low
- Empirical validation comprehensiveness: Medium

## Next Checks
1. Conduct controlled experiments comparing SMC, MCMC, and guidance-based approaches on standardized inverse problem benchmarks with known ground truth
2. Analyze sensitivity of sampling quality to guidance approximation errors across different levels of posterior complexity
3. Evaluate computational scaling of twisted distribution sampling methods as problem dimensionality increases beyond typical image-based applications