---
ver: rpa2
title: 'SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling'
arxiv_id: '2601.01943'
source_url: https://arxiv.org/abs/2601.01943
tags:
- reaction
- data
- page
- chemical
- reactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SynRXN, a comprehensive benchmarking framework
  for computer-aided synthesis planning (CASP) that addresses the critical challenge
  of inconsistent evaluation methods in the field. The framework decomposes synthesis
  planning into five distinct task families: reaction rebalancing, atom-to-atom mapping,
  reaction classification, reaction property prediction, and synthesis route design,
  providing standardized datasets and evaluation metrics for each.'
---

# SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling

## Quick Facts
- arXiv ID: 2601.01943
- Source URL: https://arxiv.org/abs/2601.01943
- Reference count: 20
- Introduces SynRXN, a benchmarking framework for computer-aided synthesis planning

## Executive Summary
This paper presents SynRXN, a comprehensive benchmarking framework designed to address the challenge of inconsistent evaluation methods in computer-aided synthesis planning (CASP). The framework standardizes the evaluation of computational methods by decomposing synthesis planning into five distinct task families and providing curated datasets with deterministic splitting functions. By implementing provenance tracking and technical validation, SynRXN ensures reproducibility and transparency while preventing data leakage through held-out test sets. The framework aims to enable fair longitudinal comparisons of CASP methods across the entire reaction-informatics pipeline.

## Method Summary
SynRXN decomposes synthesis planning into five task families: reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. The framework provides curated, provenance-tracked datasets from heterogeneous public sources with deterministic splitting functions to prevent data leakage. Technical validation using the SynKit pipeline ensures dataset consistency and transparency about limitations. Held-out gold-standard test sets are distributed only as evaluation sets rather than training data, particularly for sensitive tasks like reaction rebalancing and atom-to-atom mapping. The entire resource is released under permissive open licenses with machine-readable manifests tracking checksums and metadata.

## Key Results
- Atom-mapping accuracy reaches 97.77% on certain datasets
- Reaction classification achieves weighted F1 scores above 0.9
- Property prediction attains single-digit to low-tens MAE across various targets
- Framework standardizes evaluation and removes dataset heterogeneity for fair method comparison

## Why This Works (Mechanism)
SynRXN addresses the critical challenge of inconsistent evaluation methods in computer-aided synthesis planning by creating a standardized benchmarking framework. The framework's effectiveness stems from its systematic decomposition of complex synthesis planning tasks into five manageable categories, each with standardized datasets and evaluation metrics. By implementing provenance tracking and deterministic data splitting, the framework ensures reproducibility and prevents data leakage. The technical validation through SynKit pipeline guarantees dataset consistency, while the release of held-out test sets as evaluation-only resources prevents overfitting during method development.

## Foundational Learning
- **Task Decomposition**: Breaking synthesis planning into five distinct families (reaction rebalancing, atom-to-atom mapping, reaction classification, property prediction, and route design) allows for targeted evaluation and method development
- **Provenance Tracking**: Recording data source information and transformation history ensures transparency and enables validation of dataset quality
- **Deterministic Splitting**: Using fixed, reproducible data splits prevents data leakage and enables fair comparison between different methods
- **Technical Validation**: Implementing pipeline checks ensures dataset consistency and identifies potential limitations before deployment
- **Open Licensing**: Releasing resources under permissive licenses with machine-readable manifests promotes community adoption and standardization

## Architecture Onboarding
- **Component Map**: Data Sources -> Curation Pipeline -> Task-Specific Datasets -> Evaluation Framework -> Method Benchmarking
- **Critical Path**: Raw reaction data → Provenance tracking → Task decomposition → Dataset validation → Performance evaluation
- **Design Tradeoffs**: The framework prioritizes reproducibility and fairness over flexibility, sacrificing some adaptability for standardized evaluation
- **Failure Signatures**: Inconsistent results across tasks may indicate data quality issues, while poor generalization suggests overfitting to specific reaction types
- **First Experiments**:
  1. Test atom-mapping accuracy on diverse reaction types to verify generalizability
  2. Evaluate reaction classification performance across different chemical domains
  3. Assess property prediction accuracy for underrepresented reaction conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on source data quality and completeness may introduce biases and gaps
- Held-out test sets may not fully represent real-world synthetic challenges for underrepresented reaction types
- Framework effectiveness for synthesis route design and property prediction requires further validation across diverse chemical spaces

## Confidence
- **High Confidence**: Methodological design for task decomposition and standardized evaluation metrics
- **Medium Confidence**: Performance claims for atom-mapping and reaction classification tasks
- **Low Confidence**: Framework effectiveness for synthesis route design and property prediction tasks

## Next Checks
1. Conduct cross-validation studies using independent reaction datasets from different sources to verify generalizability
2. Implement user studies with practicing chemists to evaluate practical utility in real synthesis planning scenarios
3. Perform systematic analysis of failure cases across all five task families to identify potential blind spots