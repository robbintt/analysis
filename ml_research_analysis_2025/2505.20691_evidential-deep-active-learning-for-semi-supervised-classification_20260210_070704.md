---
ver: rpa2
title: Evidential Deep Active Learning for Semi-Supervised Classification
arxiv_id: '2505.20691'
source_url: https://arxiv.org/abs/2505.20691
tags:
- learning
- uncertainty
- active
- samples
- edalssc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an evidential deep active learning approach
  for semi-supervised classification (EDALSSC) that addresses the problem of traditional
  methods ignoring uncertainty estimation during learning, leading to model overconfidence
  and ineffective sample selection. EDALSSC introduces a comprehensive uncertainty-aware
  framework that quantifies uncertainty for both labeled and unlabeled samples.
---

# Evidential Deep Active Learning for Semi-Supervised Classification

## Quick Facts
- arXiv ID: 2505.20691
- Source URL: https://arxiv.org/abs/2505.20691
- Reference count: 31
- Primary result: EDALSSC achieves 0.3-2.4% classification accuracy improvements over state-of-the-art methods across multiple datasets

## Executive Summary
This paper introduces an evidential deep active learning framework for semi-supervised classification (EDALSSC) that addresses the problem of model overconfidence in traditional active learning methods. The proposed approach integrates evidential deep learning with active learning to quantify uncertainty for both labeled and unlabeled samples using a combination of ignorance and conflict information. EDALSSC employs a dynamic scaling mechanism for Dirichlet density parameters and a sample selection strategy that prioritizes uncertain samples when training loss increases in the latter half of training cycles. The method demonstrates consistent improvements over existing semi-supervised and supervised active learning approaches on CIFAR-10, CIFAR-100, SVHN, and Fashion-MNIST datasets.

## Method Summary
EDALSSC combines evidential deep learning with active learning for semi-supervised classification. The framework uses a ResNet-18 backbone to output evidence values, which are transformed into Dirichlet distribution parameters with dynamic scaling (r) based on the top-two evidence scores. Uncertainty is computed by combining ignorance (lack of evidence) and conflict (contradictory evidence) using a T-conorm operator. The training process includes evidence-based cross-entropy loss for labeled data and uncertainty-based loss for unlabeled data, with sample selection triggered when training loss increases in the latter half of training cycles. The method iteratively labels data, with initial labeled sets of 10% and 5% of the unlabeled pool selected per cycle.

## Key Results
- EDALSSC achieves 0.3-2.4% classification accuracy improvements over state-of-the-art semi-supervised and supervised active learning methods
- The framework shows consistent performance gains across CIFAR-10, CIFAR-100, SVHN, and Fashion-MNIST datasets
- Ablation studies confirm the effectiveness of each component, with the full framework showing 0.3-2.1% accuracy gains over variants missing key uncertainty estimation mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing uncertainty into ignorance and conflict signals via T-conorm operator provides more reliable sample selection than Shannon entropy
- **Mechanism:** Standard entropy registers low values for misclassified samples. EDALSSC calculates uncertainty using $u_i = I_i + C_i - I_i \times C_i$, ensuring high uncertainty when model lacks data or sees conflicting patterns
- **Core assumption:** Misclassified samples exhibit either high ignorance (low evidence) or high conflict (evenly distributed evidence), which standard entropy fails to capture
- **Evidence anchors:** Abstract states uncertainty is "modeled by combining ignorance information and conflict information from the perspective of the T-conorm operator"; Section 3.2 describes the aggregation achieving "aggregation from the perspective of OR"
- **Break condition:** If conflict metric saturates for all difficult samples regardless of distribution shape, differentiation fails

### Mechanism 2
- **Claim:** Dynamic scaling of Dirichlet parameters prevents counterintuitive uncertainty estimates in high-class-count scenarios
- **Mechanism:** Standard EDL uncertainty linked to sum of evidence S can remain high even with strong evidence for one class. Dynamic scaling factor r derived from gap between top two evidence scores relaxes Dirichlet parameter constraints
- **Core assumption:** Relative gap between highest and second-highest evidence scores is reliable proxy for adjusting uncertainty estimation strictness
- **Evidence anchors:** Abstract mentions "dynamic scaling mechanism to balance evidence strength and class count"; Section 3.1 provides formula $r = \frac{(e_{max}+e_{second})^2}{2\cdot(e^2_{max}+e^2_{second})}$
- **Break condition:** If r is not differentiable or stable during training, gradients may explode or loss landscape becomes erratic

### Mechanism 3
- **Claim:** Restricting sample selection to epochs with training loss increases in latter half prevents selection based on noisy early predictions
- **Mechanism:** Instead of selecting after full convergence, monitors loss fluctuation and selects samples when model shows instability or learning plateaus in later stages
- **Core assumption:** Training loss spikes in latter half indicate presence of samples challenging current decision boundary effectively
- **Evidence anchors:** Abstract states method "selects the sample with the greatest uncertainty estimation when the training loss increases in the latter half"; Section 3.3 explains focusing on increased losses prevents model from being "overly conservative"
- **Break condition:** If model converges too quickly or loss never increases in latter half, selection pool becomes empty or random

## Foundational Learning

- **Concept: Evidential Deep Learning (EDL) & Dirichlet Distribution**
  - **Why needed here:** Entire architecture replaces softmax/cross-entropy with Dirichlet-based evidence framework to quantify uncertainty; must understand how network outputs represent "evidence" and map to class probabilities
  - **Quick check question:** How does a Dirichlet distribution represent "uncertainty" differently than a Softmax probability vector?

- **Concept: T-conorm Operators**
  - **Why needed here:** Paper uses T-conorm (probabilistic sum) to aggregate ignorance and conflict as fuzzy logic operation for combining probabilities in "OR" manner
  - **Quick check question:** If Ignorance is 0.5 and Conflict is 0.5, what is resulting Uncertainty using $I + C - I \times C$?

- **Concept: Semi-Supervised Active Learning Cycles**
  - **Why needed here:** Method iteratively labels data; must grasp loop: Train -> Select (using Uncertainty) -> Label -> Update Pool -> Retrain
  - **Quick check question:** Why is "Coefficient of Variation of parameter space difference" good metric for measuring sample selection strategy effectiveness?

## Architecture Onboarding

- **Component map:** Backbone (ResNet-18 features) -> EDL Head (maps to Evidence e) -> Dynamic Scaler (calculates r) -> Uncertainty Estimator (computes I, C, u) -> Loss Aggregator (CE, U, CS)
- **Critical path:**
  1. Implement dynamic scaling r: Calculate correctly to avoid division by zero when $e_{max}=e_{second}=0$
  2. T-conorm aggregation: Implement $u_i = I_i + C_i - I_i C_i$ for unsupervised loss
  3. Selection logic: Track training loss history to trigger selection only in latter half upon loss increase
- **Design tradeoffs:**
  - Heuristic r vs. Standard EDL: Scaling factor prevents over-conservatism but introduces heuristic dependency on top-2 evidence gap
  - Timing of Selection: Selecting only on loss increase might miss informative samples if loss is monotonically decreasing, but prevents "overly conservative" selection
- **Failure signatures:**
  - Uncertainty Collapse: If $u_i$ stays at 1.0 for all samples, check dynamic scaling r or evidence activation range
  - No Selection: If training loss never increases in second half, selection logic fails; implement fallback using end-of-cycle uncertainty
  - Performance Drop on High Classes: If CIFAR-100 performs poorly, dynamic scaling r is likely not adjusting Î± correctly
- **First 3 experiments:**
  1. Sanity Check (Visualize Uncertainty): Train on small labeled subset, visualize I vs C for misclassified vs correct samples, verify high u for errors
  2. Ablation on Scaling: Run CIFAR-100 with and without dynamic scaling factor r to confirm "counter-intuitive uncertainty" issue
  3. Timing Analysis: Compare selection "at end of cycle" vs "on loss increase" to validate heuristic strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can heuristic dynamic scaling factor r be replaced by theoretically grounded mechanism to optimize balance between evidence strength and class count?
- Basis: Section 3.1 introduces scaling factor r from "heuristic perspective" to prevent counter-intuitive uncertainty estimates without theoretical proof
- Why unresolved: Authors provide functional formula but no theoretical derivation of optimality compared to other normalization techniques
- What evidence would resolve it: Theoretical derivation grounded in Dirichlet distribution properties or comparative study against non-heuristic baselines

### Open Question 2
- Question: Is linear calculation of conflict information optimal for all data distributions compared to non-linear or entropy-based conflict measures?
- Basis: Section 3.2 states conflict calculation is "Inspired by the linear relationship between $u_i$ and the evidence," chosen to ensure independence from ignorance
- Why unresolved: While linear method ensures independence, paper doesn't validate if linear proxy accurately captures complex conflict scenarios better than established non-linear metrics
- What evidence would resolve it: Ablation experiments comparing linear conflict metric against non-linear alternatives on datasets with high label noise

### Open Question 3
- Question: Does EDALSSC framework maintain effectiveness when applied to non-CNN architectures or non-image domains?
- Basis: Section 4.1 limits experimental scope to ResNet-18 and image classification benchmarks
- Why unresolved: Method relies on Dirichlet density parameters from logits; unclear if dynamic scaling and uncertainty aggregation behave similarly with attention mechanisms or feature distributions of Transformers
- What evidence would resolve it: Experimental results applying EDALSSC to Vision Transformers or NLP tasks to verify generalization capability

## Limitations
- Dynamic scaling mechanism r lacks theoretical grounding and may not generalize to datasets with very different class distributions
- Selection strategy dependent on training loss increases could fail if models converge monotonically without loss spikes
- Computational overhead of uncertainty estimation and EMA consistency loss may limit scalability to larger datasets

## Confidence

- **High Confidence**: Performance improvements over baselines (0.3-2.4% accuracy gains), effectiveness of uncertainty decomposition, and need for dynamic scaling in high-class-count scenarios
- **Medium Confidence**: Selection strategy timing heuristic and specific form of T-conorm aggregation have moderate support but could benefit from additional theoretical justification
- **Low Confidence**: Long-term stability of heuristic scaling mechanism r across diverse domains and potential for overfitting to specific datasets tested

## Next Checks

1. **Cross-Domain Generalization Test**: Apply EDALSSC to medical imaging or satellite imagery datasets with different characteristics to evaluate robustness of dynamic scaling mechanism

2. **Convergence Behavior Analysis**: Systematically vary convergence rate through learning rate scheduling to test reliability of loss-increase selection trigger across different training dynamics

3. **Theoretical Bounds Validation**: Derive theoretical bounds on uncertainty estimates produced by ignorance+conflict decomposition to verify guaranteed improvements over standard entropy-based measures