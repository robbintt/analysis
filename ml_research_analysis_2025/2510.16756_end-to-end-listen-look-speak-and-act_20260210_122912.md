---
ver: rpa2
title: End-to-end Listen, Look, Speak and Act
arxiv_id: '2510.16756'
source_url: https://arxiv.org/abs/2510.16756
tags:
- speech
- action
- ellsa
- expert
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELLSA is the first end-to-end model capable of simultaneous listening,
  looking, speaking, and acting, enabling human-like full-duplex multimodal interaction.
  It introduces SA-MoE, a self-attention mixture-of-experts architecture that routes
  each modality to specialized experts and fuses them through a unified attention
  backbone, mitigating modality interference while enabling efficient cross-modal
  integration.
---

# End-to-end Listen, Look, Speak and Act

## Quick Facts
- **arXiv ID**: 2510.16756
- **Source URL**: https://arxiv.org/abs/2510.16756
- **Reference count**: 40
- **Primary result**: First end-to-end model enabling simultaneous listening, looking, speaking, and acting with competitive performance across speech interaction and robotic manipulation tasks.

## Executive Summary
ELLSA introduces the first end-to-end model capable of full-duplex multimodal interaction, simultaneously processing speech, vision, text, and action within a single architecture. The key innovation is SA-MoE (Self-Attention Mixture-of-Experts), which routes each modality to specialized experts and fuses them through a unified attention backbone, mitigating modality interference while enabling efficient cross-modal integration. Trained through a three-stage process, ELLSA achieves competitive performance on speech QA and robotic manipulation while uniquely supporting advanced capabilities like dialogue turn-taking, defective instruction rejection, and speaking-while-acting. The model represents a significant step toward artificial general intelligence by unifying multimodal perception and concurrent generation in a streaming framework.

## Method Summary
ELLSA uses a three-stage training procedure: (1) Train independent experts—speech expert (Mamba encoder + LLaMA-3.1-8B backbone with LoRA) on ASR and speech QA, and action expert (UniVLA) on robotic manipulation; (2) Integrate via SA-MoE with unified attention and shared KV cache, training both experts with LoRA on diverse multimodal tasks; (3) Attach CosyVoice2-0.5B speech synthesizer with MLP adapter. The model processes 1-second streaming blocks with interleaved multimodal tokens (speech→image→text→action) and uses modality-specific boundary tokens. Training uses bfloat16 on A100 GPUs with AdamW optimizer and linear warmup.

## Key Results
- Achieves 74.7% accuracy on Llama Questions speech QA benchmark
- Reaches 90.8% success rate on LIBERO robotic manipulation tasks
- SA-MoE outperforms dense models by 10-20% across tasks
- Demonstrates advanced capabilities: turn-taking (dialogue/action), defective instruction rejection, speaking-while-acting, context-grounded VQA, and action barge-ins

## Why This Works (Mechanism)

### Mechanism 1
SA-MoE routing reduces modality interference while enabling cross-modal integration, conditional on well-initialized experts. Each modality routes to specialized experts via learned routing, processing independently but sharing a unified KV cache across all attention layers. This enables information flow without dense weight blending. Core assumption: pretrained experts retain sufficient domain knowledge that LoRA fine-tuning can bridge modalities without catastrophic forgetting. Break condition: poor expert pretraining or routing collapse to single-expert dominance degrades cross-modal capabilities.

### Mechanism 2
Interleaved temporal sequences enable full-duplex behavior, contingent on appropriate time-block granularity. Within each 1-second block, multimodal tokens follow fixed sequence: speech input → image input → text output → action output. Modality-specific boundary tokens delimit segments. Model learns turn-taking by observing temporal patterns. Core assumption: 1-second blocks are sufficiently granular for responsive interaction but coarse enough to simplify learning. Break condition: blocks >2s create perceptible latency; blocks <0.2s cause token sparsity issues.

### Mechanism 3
Three-stage training preserves expert capabilities while building cross-modal coordination, assuming sufficient task diversity. Stage 1 trains experts independently. Stage 2 integrates via SA-MoE with LoRA on diverse tasks. Stage 3 connects speech synthesizer end-to-end. Progressive complexity prevents early interference. Core assumption: Stage 1 experts achieve sufficient quality that Stage 2's LoRA fine-tuning can integrate without degrading core skills. Break condition: Stage 2 overfitting to narrow task distributions or insufficient LoRA rank (256) for cross-expert alignment causes advanced capabilities to fail.

## Foundational Learning

- **Mixture-of-Experts (MoE) with routing**: Why needed here: ELLSA's core architecture; without understanding expert routing, the mechanism for modality separation is opaque. Quick check question: Can you explain how token-to-expert routing differs from dense feedforward layers?

- **Full-duplex vs. half-duplex communication**: Why needed here: The paper's central claim; distinguishing simultaneous bidirectional flow from turn-taking is essential. Quick check question: Why can't half-duplex models handle barge-ins without auxiliary modules?

- **Attention KV cache sharing**: Why needed here: SA-MoE's cross-expert fusion relies on shared historical context via KV cache. Quick check question: How does sharing KV cache across experts enable cross-modal attention without weight sharing?

## Architecture Onboarding

- **Component map**: Speech Expert (Mamba encoder → 2-layer MLP adapter → LLaMA-3.1-8B backbone) → SA-MoE Fusion (unified attention with shared KV cache) → Action Expert (Emu3-VisionTokenizer + FAST action tokenizer → Emu3-Base backbone) → Speech Synthesizer (CosyVoice2-0.5B with MLP adapter)

- **Critical path**: 1) Stage 1 expert pretraining/validation (ensures expert quality) → 2) SA-MoE integration with LoRA on mixed tasks → 3) Speech synthesizer attachment with streaming codec alignment (25 speech codecs per 8 text tokens)

- **Design tradeoffs**: 1-second time blocks simplify full-duplex modeling vs. higher latency than 0.16s alternatives; 2-expert design provides clear modality separation vs. potential for more granular expert specialization; LoRA-only fine-tuning offers parameter efficiency vs. possible under-capacity for complex cross-modal reasoning

- **Failure signatures**: Modality collapse (model ignores one expert—check routing distribution); turn-taking failure (responses delayed >1s after input ends); action barge-in missed (model continues action despite "pause" command); speech quality degradation (Stage 3 synthesizer misalignment produces artifacts)

- **First 3 experiments**: 1) Ablate SA-MoE vs. dense: Replicate Table 7 comparison on LIBERO subset to validate 10-20% gain claim; 2) Time-block sensitivity: Test 0.5s, 1s, 2s blocks on dialogue turn-taking success rate; 3) Cross-modal transfer probe: Evaluate action expert's speech understanding before/after Stage 2 (zero-shot spoken instruction execution)

## Open Questions the Paper Calls Out

- **Real-world deployment**: ELLSA has yet to be validated in real-world settings; current evaluation restricted to simulated environments (LIBERO). What evidence would resolve it: Successful task completion and duplex interaction metrics achieved by ELLSA running on physical robotic arms in real-world scenarios.

- **Nuanced conversational dynamics**: Many aspects of natural communication, such as user and assistant backchanneling, remain unaddressed by the current implementation. What evidence would resolve it: Demonstration of appropriate non-interrupting verbal or action-based feedback generated by the model while the user is speaking.

- **Alternative architecture**: The paper proposes an alternative "brain-hand-mouth" architecture with a universal backbone for inputs as potentially more elegant and efficient than current SA-MoE design. What evidence would resolve it: Comparative benchmarking of the proposed universal backbone architecture against current SA-MoE on multimodal integration tasks.

## Limitations

- Evaluation relies heavily on automated metrics rather than human evaluation, potentially missing quality aspects of full-duplex interactions
- LIBERO manipulation dataset contains only 3386 samples, limiting statistical power for certain advanced capability evaluations
- Lacks comprehensive ablation studies on critical design choices like time-block duration, expert routing strategies, and LoRA rank impact

## Confidence

- **High Confidence**: SA-MoE architecture design and basic functionality claims are well-specified and theoretically sound; implementation details sufficient for core component reproduction
- **Medium Confidence**: Quantitative performance improvements (74.7% Llama Questions accuracy, 90.8% LIBERO success rate) based on reported results but lack independent verification; 10-20% improvement over dense models cited but dense baseline configuration unspecified
- **Low Confidence**: Advanced capabilities (defective instruction rejection, context-grounded VQA, speaking-while-acting) demonstrated but evaluation methodology underspecified; limited quantitative evidence of reliability

## Next Checks

1. **Ablation Study on Time-Block Duration**: Systematically evaluate ELLSA's performance across multiple time-block durations (0.5s, 1s, 2s) on turn-taking success rate and barge-in accuracy metrics to validate the 1-second choice and understand latency-quality tradeoffs

2. **Independent Dense Baseline Comparison**: Implement and train a dense multimodal transformer with identical architecture (except without MoE routing) using same pretraining components and training procedure to verify claimed 10-20% performance advantage of SA-MoE

3. **Expert Capability Preservation Test**: Before and after SA-MoE integration (Stage 2), evaluate speech expert on LibriSpeech test sets and action expert on zero-shot manipulation tasks to quantify any performance degradation from cross-modal training