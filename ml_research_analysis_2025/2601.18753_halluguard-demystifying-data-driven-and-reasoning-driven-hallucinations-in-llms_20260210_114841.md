---
ver: rpa2
title: 'HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in
  LLMs'
arxiv_id: '2601.18753'
source_url: https://arxiv.org/abs/2601.18753
tags:
- hallucination
- hallucinations
- across
- data-driven
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of detecting hallucinations
  in large language models (LLMs), which arise from two distinct sources: data-driven
  hallucinations due to flawed knowledge encoding, and reasoning-driven hallucinations
  due to inference-time instabilities. Existing detection methods typically target
  only one source and lack a unified theoretical foundation.'
---

# HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs

## Quick Facts
- **arXiv ID**: 2601.18753
- **Source URL**: https://arxiv.org/abs/2601.18753
- **Reference count**: 39
- **Key outcome**: Introduces HalluGuard, an NTK-based score that jointly detects data-driven and reasoning-driven hallucinations, achieving state-of-the-art performance across 10 benchmarks and 9 LLM backbones.

## Executive Summary
This paper addresses the challenge of detecting hallucinations in large language models (LLMs), which arise from two distinct sources: data-driven hallucinations due to flawed knowledge encoding, and reasoning-driven hallucinations due to inference-time instabilities. Existing detection methods typically target only one source and lack a unified theoretical foundation. The authors introduce Hallucination Risk Bound, a theoretical framework that formally decomposes overall hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled understanding of how hallucinations emerge and evolve during generation. Building on this foundation, they propose HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the Neural Tangent Kernel to jointly identify both types of hallucinations. HalluGuard combines three interpretable terms: det(K) for representational adequacy, log σmax for rollout amplification, and -log κ² for conditioning-induced variance. Evaluated on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, HalluGuard consistently achieves state-of-the-art performance in detecting diverse forms of LLM hallucinations. It demonstrates particularly strong improvements on reasoning-oriented tasks and maintains robust detection across different model scales. The method also enables effective test-time inference guidance, improving downstream reasoning accuracy by up to 15% on complex tasks.

## Method Summary
HalluGuard introduces a theoretically grounded approach to detecting both data-driven and reasoning-driven hallucinations in LLMs. The method leverages the Neural Tangent Kernel (NTK) to compute three interpretable terms that jointly capture both sources of hallucination risk. The framework begins with a formal decomposition of hallucination risk into data-driven and reasoning-driven components, establishing a theoretical foundation for understanding how these distinct sources contribute to overall model uncertainty. The NTK-based score combines determinant of the kernel matrix (det(K)) for measuring representational adequacy, log of the maximum eigenvalue (log σmax) for capturing rollout amplification, and negative log of the condition number (-log κ²) for quantifying conditioning-induced variance. This geometric approach enables the method to identify hallucinations arising from both flawed knowledge encoding during training and inference-time instabilities during generation, providing a unified detection mechanism that outperforms existing approaches focused on only one type of hallucination source.

## Key Results
- HalluGuard achieves state-of-the-art performance in detecting diverse forms of LLM hallucinations across 10 diverse benchmarks and 11 competitive baselines.
- The method demonstrates particularly strong improvements on reasoning-oriented tasks, showing consistent performance across 9 different LLM backbones.
- HalluGuard enables effective test-time inference guidance, improving downstream reasoning accuracy by up to 15% on complex tasks.

## Why This Works (Mechanism)
HalluGuard works by leveraging the Neural Tangent Kernel's ability to capture the geometry of the model's learned representations and the induced geometry of the data distribution. The NTK provides a linearized approximation of the neural network that preserves key properties of the original model while enabling tractable analysis of uncertainty. The three terms in HalluGuard's score each target specific sources of hallucination: det(K) measures whether the model has sufficient representational capacity to encode the input data correctly, log σmax captures the amplification of errors during autoregressive generation (particularly problematic in reasoning tasks), and -log κ² quantifies the variance introduced by poor conditioning of the kernel matrix. By combining these geometric properties, HalluGuard can identify both training-time knowledge gaps and inference-time instabilities that lead to hallucinations. The method's effectiveness stems from its ability to detect subtle changes in the model's confidence landscape that correlate with hallucination risk, rather than relying on surface-level patterns or post-hoc corrections.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: A kernel that describes the behavior of infinitely wide neural networks during training, providing a linearization of the model's dynamics. *Why needed*: Enables tractable analysis of model uncertainty and representation geometry that would be intractable in the original nonlinear space. *Quick check*: Verify that the NTK approximation remains valid for the finite-width models tested.

**Condition Number (κ)**: A measure of how sensitive a matrix is to numerical operations, defined as the ratio of the largest to smallest singular values. *Why needed*: Quantifies the amplification of errors during matrix inversion and influences the stability of predictions. *Quick check*: Confirm that high condition numbers correlate with increased hallucination risk in empirical evaluations.

**Representational Adequacy**: The degree to which a model's feature space can capture the structure of the input data distribution. *Why needed*: Determines whether the model has sufficient capacity to represent the input without introducing systematic errors. *Quick check*: Validate that det(K) correlates with known cases of underfitting or distributional mismatch.

## Architecture Onboarding

**Component Map**: Input Data -> NTK Computation -> Three Term Calculation (det(K), log σmax, -log κ²) -> HalluGuard Score -> Hallucination Detection

**Critical Path**: The most time-consuming step is the NTK computation, which requires evaluating the kernel matrix for the entire input sequence. This computation scales quadratically with sequence length, making it the primary bottleneck for long-context applications.

**Design Tradeoffs**: The method trades computational efficiency for theoretical rigor and interpretability. While the NTK-based approach is more computationally intensive than heuristic methods, it provides principled uncertainty quantification and enables targeted intervention strategies.

**Failure Signatures**: HalluGuard may underperform when the NTK linearization assumption breaks down (e.g., for very deep or narrow networks), when the input distribution is far from the training distribution, or when hallucinations arise from sources not captured by the three NTK terms (such as external knowledge conflicts).

**First Experiments**: 1) Compute HalluGuard scores on known hallucination cases from existing benchmarks to establish baseline detection rates. 2) Perform ablation studies removing each NTK term to quantify their individual contributions to overall performance. 3) Test the method's sensitivity to sequence length by evaluating detection accuracy across progressively longer inputs.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical decomposition assumes clean separation between data-driven and reasoning-driven sources, but in practice these sources may interact in complex ways not captured by the current formulation.
- The assumption of linear approximation through NTK may not fully represent the behavior of highly nonlinear LLM representations, particularly for larger models or more complex reasoning tasks.
- The evaluation focuses primarily on English-language benchmarks and may not generalize to multilingual contexts.

## Confidence

**High confidence** in the theoretical framework's mathematical consistency and the interpretability of the three NTK-based components. The mathematical formulation is rigorous and the relationship between each term and specific hallucination types is clearly established.

**Medium confidence** in the empirical performance claims, given the extensive benchmarking across 10 datasets and 11 baselines. However, the results may be influenced by specific dataset characteristics or evaluation protocols.

**Medium confidence** in the generalizability of the method to extreme model scales (beyond the 9 backbones tested) and to domains with significantly different data distributions than those in the evaluation.

## Next Checks

1. Test HalluGuard's performance on multilingual datasets to verify cross-lingual generalization of the detection method.

2. Conduct ablation studies that isolate each NTK component's contribution across different hallucination types to validate the theoretical decomposition empirically.

3. Evaluate the method on out-of-distribution data that exhibits systematic shifts from the training distribution to test robustness of the data-driven detection component.