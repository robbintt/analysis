---
ver: rpa2
title: Disentangled representations via score-based variational autoencoders
arxiv_id: '2512.17127'
source_url: https://arxiv.org/abs/2512.17127
tags:
- latent
- noise
- image
- diffusion
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAMI, a new approach for unsupervised representation
  learning that integrates diffusion models and VAEs. By formulating a principled
  ELBO objective, SAMI uses conditional diffusion as the generative component and
  leverages score-based guidance to learn semantically meaningful latent representations.
---

# Disentangled representations via score-based variational autoencoders

## Quick Facts
- arXiv ID: 2512.17127
- Source URL: https://arxiv.org/abs/2512.17127
- Authors: Benjamin S. H. Lyo; Eero P. Simoncelli; Cristina Savin
- Reference count: 40
- Key outcome: Introduces SAMI, integrating diffusion models and VAEs to learn semantically meaningful latent representations with improved disentanglement

## Executive Summary
This paper presents SAMI (Score-based Autoencoder with Manifold Inference), a new approach for unsupervised representation learning that combines diffusion models and variational autoencoders. SAMI formulates a principled ELBO objective where conditional diffusion serves as the generative component and score-based guidance helps learn semantically meaningful latent representations. The method demonstrates superior disentanglement performance on synthetic data, factorizes semantic dimensions in complex natural images like CelebA, and produces straighter latent trajectories for video sequences without requiring temporal supervision.

## Method Summary
SAMI integrates diffusion models with VAEs by formulating a conditional ELBO that combines the denoising score matching objective with a variational inference objective over latent variables. The model uses an unconditional denoiser (typically a pre-trained UNet) alongside an inference network that maps both clean and noisy images to Gaussian latent posteriors. The key innovation is the "guidance score" mechanism that steers the diffusion reverse process toward regions of latent space consistent with the target image's semantics, while the dual encoding of clean and noisy images implicitly regularizes the latent space to be smooth and free of "holes."

## Key Results
- Recovers ground truth generative factors in synthetic datasets (Disks)
- Achieves FID of 16.25 on CelebA with semantically meaningful factorization
- Produces straighter latent trajectories for video sequences without temporal supervision
- Extracts interpretable latents from pre-trained diffusion models with minimal retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Latent variables guide the diffusion reverse process by adding a "guidance score" derived from the inference network to the unconditional denoising score.
- **Mechanism:** The model uses Bayes' rule to decompose the conditional score $\nabla_{x_t} \log p(x_t|z)$ into an unconditional denoising term $\nabla_{x_t} \log p(x_t)$ and a guidance term $\nabla_{x_t} \log p(z|x_t)$. The inference network $q_\phi(z|x_t)$ evaluates the likelihood of the clean image's latent $z$ given the noisy image $x_t$, and the gradient of this log-likelihood w.r.t $x_t$ steers generation toward the target semantic region.
- **Core assumption:** The unconditional denoiser and the inference network can be optimized jointly or separately to align their gradients.
- **Break condition:** If the inference network fails to generalize its likelihood estimation from clean to noisy images (distribution shift), the guidance score becomes noise, failing to steer the denoiser.

### Mechanism 2
- **Claim:** Optimizing the ELBO over both clean ($x_0$) and noisy ($x_t$) images implicitly smooths the latent space by penalizing the curvature of the encoder manifold.
- **Mechanism:** The paper proves that the reconstruction loss term minimizes the expected Frobenius norm of the encoder's Hessian. By mapping noisy versions of an image to the same latent region as the clean version, the encoder creates a "straighter" embedding space, mitigating the "latent holes" typical in standard VAEs.
- **Core assumption:** The noise schedule spans sufficient scales to enforce this smoothing across the entire data manifold.
- **Break condition:** If the encoder capacity is too low, it cannot maintain the mapping from diverse noisy states to a consistent latent, resulting in collapsed or fragmented latent clusters.

### Mechanism 3
- **Claim:** A diagonal covariance posterior combined with a multi-scale diffusion prior forces the model to disentangle features by their spatial scale (characteristic noise level).
- **Mechanism:** The diffusion process naturally separates features by scale (coarse features survive high noise, fine features appear at low noise). The paper argues that for a latent dimension to be useful across noise levels, it must consistently track a single feature scale; switching features violates the additive nature of the score updates.
- **Core assumption:** Semantic features are distributed hierarchically and are approximately orthogonal in the pixel space.
- **Break condition:** If the dataset lacks a clear hierarchy of spatial scales (e.g., specific synthetic datasets), the proof assumptions may not hold, potentially weakening disentanglement.

## Foundational Learning

- **Concept: Score-Based Generative Models (Diffusion)**
  - **Why needed here:** SAMI replaces the standard VAE decoder with a diffusion model. You must understand how diffusion models iteratively denoise data via score functions ($\nabla \log p(x)$) to grasp how SAMI modifies this trajectory with latent guidance.
  - **Quick check question:** How does adding a guidance score term $\nabla_{x_t} \log p(z|x_t)$ shift the mean of the reverse transition distribution?

- **Concept: Evidence Lower Bound (ELBO)**
  - **Why needed here:** The paper claims its primary contribution is formulating a principled ELBO that unifies VAE and Diffusion objectives. Understanding the trade-off between reconstruction likelihood and KL-divergence is essential for tuning the $\beta$ parameter.
  - **Quick check question:** In Eq. 4, which term corresponds to the standard VAE regularization and which corresponds to the diffusion denoising objective?

- **Concept: Reparameterization & Tweedie’s Formula**
  - **Why needed here:** The derivation relies on Tweedie's formula to relate the MMSE noise estimator to the score function. This mathematical bridge allows the inference network (which predicts $z$) to generate a gradient for the image (guidance score).
  - **Quick check question:** How does the derivation in Appendix A.1 use Tweedie's formula to connect the noise estimate $\hat{\epsilon}$ to the conditional score $\nabla \log p(x_t|z)$?

## Architecture Onboarding

- **Component map:**
  - Clean image $x_0$ -> Inference Net -> Latent $z$
  - Noisy image $x_t$ -> Inference Net -> Guidance score
  - Unconditional denoiser $\epsilon_\theta$ + Guidance score -> Updated $x_t$

- **Critical path:**
  1. Input clean image $x_0$ -> Inference Net -> Sample $z \sim q_\phi(z|x_0)$
  2. Forward diffuse $x_0$ to $x_t$
  3. Input noisy image $x_t$ -> Inference Net -> Compute log-likelihood $\log q_\phi(z|x_t)$
  4. Calculate guidance score via auto-diff: $g_t = \nabla_{x_t} \log q_\phi(z|x_t)$
  5. Combine Denoiser score and Guidance score -> Update $x_t$ to $x_{t-1}$

- **Design tradeoffs:**
  - **Frozen vs. Joint Training:** The paper shows you can train the inference net on top of a frozen denoiser (Section: Feature extraction), saving compute but potentially limiting maximum alignment
  - **KL Weight ($\beta$):** High $\beta$ improves factorization/disentanglement but risks posterior collapse. The paper suggests annealing is required

- **Failure signatures:**
  - **Posterior Collapse:** Generated samples ignore the latent $z$ and look like unconditional samples (Likely $\beta$ too high or latent dim too large)
  - **Artifacts/Contradiction:** Generated images look like a mix of the guide image and random noise (Likely misconfiguration of the noise schedule or gradient weighting $\gamma_t$)

- **First 3 experiments:**
  1. **Synthetic Disks Verification:** Train on the "Disks" dataset (3 latent dims) to confirm the model learns the exact Cartesian/Polar coordinates (Fig 2)
  2. **Ablation on Guidance:** Compare conditional generation FID scores when using the full guidance score vs. only the unconditional denoiser to quantify the information contribution of the latent $z$
  3. **Trajectory Straightness:** Encode a video sequence frame-by-frame using the trained model and plot the cosine similarity of latent deltas to verify the "straightening" claim (Fig 4)

## Open Questions the Paper Calls Out
None

## Limitations
- Disentanglement performance relies heavily on hierarchical spatial scales assumption, which may not generalize to all datasets
- Frozen denoiser approach could limit maximum achievable alignment between unconditional denoiser and inference network gradients
- Qualitative assessment of CelebA results lacks quantitative comparison to state-of-the-art disentanglement methods

## Confidence
- Core mathematical formulation: High confidence
- Synthetic dataset results: High confidence
- CelebA results: Medium confidence
- Video trajectory straightening: Medium confidence

## Next Checks
1. Ablation studies systematically varying the KL weight β to quantify the disentanglement-reconstruction trade-off across different datasets
2. Quantitative comparison of disentanglement metrics (e.g., DCI, SAP, or FactorVAE scores) against established baselines on standard benchmarks like dSprites and 3DShapes
3. Testing SAMI's performance on datasets specifically designed to violate the hierarchical scale assumption to validate the robustness of the proposed mechanism