---
ver: rpa2
title: More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization
  Problems
arxiv_id: '2505.02985'
source_url: https://arxiv.org/abs/2505.02985
tags:
- fractional
- gradient
- fosgd
- sedfosgd
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the instability and hyperparameter sensitivity
  of Fractional-Order Stochastic Gradient Descent (FOSGD) in non-convex optimization.
  It proposes 2SEDFOSGD, which integrates the Two-Scale Effective Dimension (2SED)
  algorithm to dynamically adapt the fractional exponent based on local curvature
  and sensitivity metrics derived from the Fisher Information Matrix.
---

# More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems

## Quick Facts
- **arXiv ID:** 2505.02985
- **Source URL:** https://arxiv.org/abs/2505.02985
- **Reference count:** 19
- **Primary result:** 2SEDFOSGD achieves faster convergence and more robust parameter estimation than FOSGD by dynamically adapting the fractional exponent based on local curvature and sensitivity metrics derived from the Fisher Information Matrix.

## Executive Summary
This work addresses the instability and hyperparameter sensitivity of Fractional-Order Stochastic Gradient Descent (FOSGD) in non-convex optimization by proposing 2SEDFOSGD, which integrates the Two-Scale Effective Dimension (2SED) algorithm to dynamically adapt the fractional exponent. The method computes a weighted combination of nominal dimension and curvature (via log-det of Fisher Information Matrix) to determine when to increase or decrease memory effects. Under smoothness and bounded-gradient assumptions, the approach achieves theoretical convergence guarantees with error bounds scaling as O(1/T^(1-ρ)) for 0.5 < ρ < 1. Empirical evaluation on an autoregressive model under both Gaussian and heavy-tailed noise demonstrates faster convergence and more robust parameter estimation compared to standard FOSGD.

## Method Summary
2SEDFOSGD dynamically modulates the fractional exponent α_t based on local curvature and sensitivity metrics derived from the Fisher Information Matrix. For each layer, it computes the 2SED metric d_ζ^(ℓ)(ε) = ζ·d + (1-ζ)·d_curv(ε), where d_curv involves the log-determinant of (I + ε^{-ξ} F^{1/2}). The fractional exponent is then adapted as α_t^(ℓ) = α₀ - β × (d_ζ^(ℓ)(ε)|_t / d_max), where d_max is tracked across all layers and time steps. The fractional update rule incorporates historical gradient information through the (|θ_t - θ_{t-1}| + δ)^(1-α_t) term, with step sizes decaying as μ_t = μ₀/t^ρ (0.5 < ρ < 1).

## Key Results
- 2SEDFOSGD achieves O(1/T^(1-ρ)) convergence rate under smoothness and bounded-gradient assumptions
- Dynamic adaptation of fractional exponent reduces oscillations in high-sensitivity regions
- Faster convergence and more robust parameter estimation compared to standard FOSGD under both Gaussian and heavy-tailed (α-stable) noise
- Maintains lower absolute estimation errors throughout training on autoregressive model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting the fractional exponent based on local curvature reduces oscillations in high-sensitivity regions
- Mechanism: High Fisher eigenvalues correlate with instability-prone directions where long memory harms convergence. When curvature is high, 2SED increases, which reduces α via α_t = α₀ - β × (d/d_max), damping oscillatory updates
- Core assumption: Fisher Information Matrix eigenvalues indicate instability-prone directions
- Evidence anchors: [abstract] tracks model sensitivity and effective dimensionality to mitigate oscillations; [section III-B] explains high curvature reduces α to prevent overshooting; related work notes lack of stable tuning mechanisms

### Mechanism 2
- Claim: Longer memory in flat regions accelerates convergence by leveraging accumulated gradient history
- Mechanism: In low-curvature regions, 2SED is smaller, leading to higher α. The fractional update incorporates more historical gradient information, smoothing noise and accelerating progress through shallow valleys
- Core assumption: Flat regions benefit from longer-range gradient accumulation
- Evidence anchors: [section III-B] explains raising α in low curvature regions leverages additional memory; [section V, Figure 3] shows lower absolute estimation errors under Gaussian noise; weak corpus support for long-memory benefit

### Mechanism 3
- Claim: Bounded iterates and decaying step sizes ensure convergence to critical points in non-convex settings
- Mechanism: Step size schedule satisfies ∑μ_t = ∞ (infinite travel) and ∑μ_t² < ∞ (noise averaging). Combined with bounded gradients and L-smoothness, this guarantees min_{1≤s≤T} E[‖∇f(θ_s)‖²] = O(1/T^{1-ρ})
- Core assumption: Gradients are uniformly bounded; loss is L-smooth; fractional term bounds hold throughout training
- Evidence anchors: [section IV, Theorem 3] provides formal proof of O(1/T^{1-ρ}) convergence; [section IV, Proposition 5.1] proves bounded iterates; novel to paper with no corpus validation

## Foundational Learning

- **Caputo Fractional Derivative**: The core update rule uses Caputo derivative to encode long-memory effects. Understanding D_t^α f(t) = 1/Γ(n-α) ∫₀ᵗ f^{(n)}(τ)/(t-τ)^{α-n+1} dτ is essential for grasping how fractional orders weight historical gradients. Quick check: For α ∈ (0,1), how does Caputo derivative differ from standard first derivative in terms of information used?

- **Fisher Information Matrix (FIM)**: 2SED derives curvature estimates from F = E[∇ℓ ⊗ ∇ℓ]. The log-det of (I + ε^{-ξ} F^{1/2}) captures effective dimensionality. You must understand why FIM eigenvalues indicate sensitivity. Quick check: If FIM has one large eigenvalue and many near-zero eigenvalues, what does that imply about parameter sensitivity in different directions?

- **Stochastic Approximation Step-Size Conditions**: Convergence proof relies on ∑μ_t = ∞ and ∑μ_t² < ∞. Recognizing why μ_t = μ₀/t^ρ with 0.5 < ρ < 1 satisfies these helps debug learning rate schedules. Quick check: For ρ = 0.6, does ∑_{t=1}^∞ t^{-0.6} converge or diverge? What about ∑_{t=1}^∞ t^{-1.2}?

## Architecture Onboarding

- **Component map**: Gradient Computer -> Fisher Estimator -> 2SED Calculator -> α Adapter -> Fractional Updater
- **Critical path**: Initialize θ₀ with standard SGD, then for each iteration: compute gradients → estimate Fisher → compute 2SED → adapt α → apply fractional update; track d_max across layers and time steps
- **Design tradeoffs**:
  - **ζ (0 ≤ ζ < 1)**: Controls dimension-curvature balance; higher ζ favors nominal dimension over curvature
  - **β**: Controls adaptation aggressiveness; too high may stall optimization, too low mimics fixed-α FOSGD
  - **δ > 0**: Prevents division-by-zero; smaller δ increases sensitivity to small step differences
  - **ε, ξ**: Control curvature weighting in log-det; unspecified in paper
- **Failure signatures**:
  - **α_t → 0**: Indicates consistently high 2SED; may signal gradient explosion or mis-scaled Fisher estimates
  - **Oscillating α_t**: Rapid 2SED fluctuations suggest noisy Fisher estimates; increase Fisher smoothing
  - **No improvement over FOSGD**: β may be too small; verify d_max not dominated by outlier layer
- **First 3 experiments**:
  1. **Sanity check on convex quadratic**: Minimize f(θ) = (1/2)θᵀAθ with known eigenvalues; verify 2SEDFOSGD adapts α inversely to eigenvalues
  2. **AR(2) reproduction**: Replicate y(k) = 1.5y(k-1) - 0.7y(k-2) + ξ(k) under Gaussian noise; confirm α_t stabilizes near 0.98
  3. **Heavy-tailed stress test**: Run same AR(2) under α-stable noise (α_stable = 1.8); compare error trajectories against Figure 5

## Open Questions the Paper Calls Out
None

## Limitations
- 2SED hyperparameters (ζ, ε, ξ) and step-size parameters (μ₀, ρ, δ) are unspecified, requiring empirical tuning
- Convergence guarantees rely on bounded gradients and L-smoothness assumptions that may not hold in deep neural networks
- No independent validation of the long-memory acceleration claim in flat regions; corpus lacks supporting papers

## Confidence
- **High Confidence**: Bounded iterate and convergence claims (Mechanism 3) supported by formal proofs under stated assumptions
- **Medium Confidence**: Oscillation damping via adaptive α (Mechanism 1) is theoretically sound but empirical validation is limited to synthetic AR(2) data
- **Low Confidence**: Long-memory acceleration in flat regions (Mechanism 2) lacks direct empirical or corpus support

## Next Checks
1. **Sanity Check on Convex Quadratic**: Minimize f(θ) = (1/2)θᵀAθ with known eigenvalues to verify 2SEDFOSGD adapts α inversely to eigenvalues and converges faster than fixed-α FOSGD
2. **AR(2) Reproduction**: Replicate paper's y(k) = 1.5y(k-1) - 0.7y(k-2) + ξ(k) under Gaussian noise, confirming α_t stabilizes near 0.98 and estimation error drops faster than baseline
3. **Heavy-Tailed Stress Test**: Run same AR(2) under α-stable noise (α_stable = 1.8) and compare error trajectories against Figure 5 to verify 2SEDFOSGD shows smoother decline