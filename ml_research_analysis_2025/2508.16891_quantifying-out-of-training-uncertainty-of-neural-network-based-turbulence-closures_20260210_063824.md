---
ver: rpa2
title: Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence
  Closures
arxiv_id: '2508.16891'
source_url: https://arxiv.org/abs/2508.16891
tags:
- uncertainty
- turbulence
- data
- deep
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Neural Network (NN)-based turbulence
  closures as pre-trained surrogates for traditional turbulence closures in CFD simulations,
  aiming to increase computational efficiency and prediction accuracy. A key bottleneck
  to the widespread adoption of these ML-based closures is the lack of uncertainty
  quantification (UQ), particularly for out-of-training inputs.
---

# Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence Closures

## Quick Facts
- arXiv ID: 2508.16891
- Source URL: https://arxiv.org/abs/2508.16891
- Reference count: 40
- Primary result: Deep Ensembles provide the best balance of accuracy and computational efficiency for uncertainty quantification in neural-network turbulence closures, with GP accuracy being slightly better but computationally prohibitive.

## Executive Summary
This paper addresses the critical need for uncertainty quantification (UQ) in neural-network-based turbulence closures used in CFD simulations. The study compares four UQ methods—Deep Ensembles, Monte-Carlo Dropout, Stochastic Variational Inference, and Gaussian Processes—using data from an algebraic turbulence closure model. Deep Ensembles emerge as the most practical solution, offering robust uncertainty estimates and computational efficiency while maintaining accuracy close to the theoretically superior Gaussian Processes. The work establishes a framework for evaluating epistemic uncertainty in out-of-training regions, which is essential for safe deployment of ML-based turbulence models in engineering applications.

## Method Summary
The study trains neural networks to predict turbulence closure coefficients (G₁, G₂, G₃) from strain and rotation rate invariants (η₁, η₂). Four uncertainty quantification methods are compared: Deep Ensembles (40 independently trained networks), Monte-Carlo Dropout (weight dropout during inference), Stochastic Variational Inference (learning weight distributions), and Gaussian Processes (both exact and approximate). The dataset consists of 80,000 training, 40,000 validation, and 490,000 testing points generated from an Algebraic Reynolds Stress Model. Inputs undergo log transformation followed by standard scaling, while outputs are standardized. Models are evaluated on both in-training and out-of-training performance using RMSE, MAE, NLL, miscalibration area, sharpness, and coefficient of variation metrics.

## Key Results
- Exact Gaussian Process achieves the lowest RMSE (2.14 × 10⁻⁵) for in-training predictions, followed by Deep Ensembles (4.59 × 10⁻⁴)
- Deep Ensembles provide the best balance of accuracy and computational efficiency for UQ
- In out-of-training scenarios, Deep Ensembles maintain performance close to Exact GP while being significantly more computationally tractable
- Deep Ensembles perform best in NLL for both out-of-training cases, indicating superior uncertainty calibration

## Why This Works (Mechanism)

### Mechanism 1: Functional Diversity via Deep Ensembles
Deep Ensembles capture epistemic uncertainty by approximating the posterior distribution of the turbulence closure function through multiple independently trained networks. The variance among predictions serves as a proxy for uncertainty because each member converges to different local minima in the loss landscape. This diversity is maintained through random initialization and data shuffling. The method assumes the loss landscape contains sufficiently diverse modes to prevent functional collapse.

### Mechanism 2: Distance-Sensitive Uncertainty in Gaussian Processes
Exact Gaussian Processes provide accurate uncertainty quantification by explicitly correlating prediction confidence with distance from training data. The covariance matrix (kernel) over inputs causes predictive variance to increase as test inputs move away from the convex hull of training data. This offers a rigorous mathematical guarantee of growing uncertainty during extrapolation, assuming the kernel correctly represents the underlying physics correlation structure.

### Mechanism 3: Stochastic Regularization (SVI & MCD)
Monte-Carlo Dropout and Stochastic Variational Inference approximate Bayesian inference by injecting noise during training or inference. MCD drops weights randomly during inference, sampling different network sub-structures, while SVI learns a distribution over weights. Both attempt to simulate a distribution of functions cheaply but tend to be "single-modal," underestimating uncertainty in complex extrapolation regimes compared to Deep Ensembles.

## Foundational Learning

**Concept: Epistemic vs. Aleatoric Uncertainty**
*Why needed*: The paper focuses on out-of-training inputs, which represent epistemic (reducible, model-based) rather than aleatoric (irreducible, noise-based) uncertainty. Understanding this distinction is critical for choosing appropriate UQ methods.
*Quick check*: Does adding more simulation data reduce the uncertainty? If yes, it is epistemic.

**Concept: The Predictive Distribution**
*Why needed*: All compared methods aim to estimate p(y|x,D)—the probability distribution of output given input and data—rather than a single point estimate.
*Quick check*: If you feed a trained model a specific input, does it return a single value or a mean and a variance (standard deviation)?

**Concept: Negative Log-Likelihood (NLL)**
*Why needed*: NLL is the strictly proper scoring rule used to rank methods, penalizing models that are confident but wrong.
*Quick check*: If a model predicts y=5 ± 0.1 when the true value is 10, is the NLL high or low? (Answer: High, because it was confidently wrong).

## Architecture Onboarding

**Component map**: Inputs (η₁, η₂) → Log transformation → Standard Scaling → 4-layer NN (2 hidden layers, 20 nodes each, ReLU) → Output (G₁, G₂, G₃) → UQ wrapper (DE, MCD, SVI, or GP)

**Critical path**: The input transformation is most overlooked. Transforming inputs to log-space reduces necessary learnable parameters from 40,000 to <1,000 and improves accuracy by an order of magnitude.

**Design tradeoffs**: Accuracy vs. Cost—GPs offer gold standard accuracy (RMSE 2.14 × 10⁻⁵) but are computationally prohibitive; Deep Ensembles offer best balance (RMSE 4.59 × 10⁻⁴) with manageable training time. Simplicity vs. Robustness—MCD is easy to implement but performs worst in out-of-training scenarios; DE is simple but requires training multiple models.

**Failure signatures**: Overconfidence—model outputs low standard deviation in high-error regions (observed in SVI/MCD near training borders). Underconfidence—model outputs high standard deviation in accurate prediction regions (observed in Approximate GP). Generalization Collapse—sharp RMSE increase when querying D<0 if trained only on D≥0.

**First 3 experiments**:
1. **Baseline Deterministic NN**: Train single network with Log-Transform architecture to replicate baseline RMSE (5.89 × 10⁻⁴), validating data pipeline.
2. **Deep Ensemble UQ**: Train 5-10 ensemble members, plot standard deviation over input space, verify uncertainty spikes in out-of-training regions (e.g., D<0 if trained on D≥0).
3. **NLL Comparison**: Compare NLL of Ensemble vs. simple Monte-Carlo Dropout network on test set, confirm DE provides lower (better) NLL.

## Open Questions the Paper Calls Out

**Open Question 1**: Will the performance ranking of UQ methods—specifically the dominance of Deep Ensembles over SVI and MCD—hold when models are trained on high-fidelity DNS data rather than algebraic proxy-physics?
*Basis*: The Conclusion states future work will "generalize these conclusions to turbulence models learned from high-fidelity Direct Numerical Simulations."
*Why unresolved*: Current study relies on algebraic Reynolds stress model lacking noise and high dimensionality of raw DNS datasets.
*Resolution evidence*: Comparative study replicating benchmarks using DNS training data.

**Open Question 2**: How does quantified epistemic uncertainty propagate to macroscopic engineering observables when the model is coupled with a RANS CFD solver?
*Basis*: Conclusion explicitly proposes implementing "learned surrogate models equipped with uncertainty quantification... in RANS CFD codes to propagate the model uncertainty into observables."
*Why unresolved*: Current paper evaluates surrogate model in offline capacity without investigating UQ behavior within live CFD simulation feedback loop.
*Resolution evidence*: Coupled CFD simulation results showing correlation between surrogate's epistemic uncertainty bounds and variance in final engineering output metrics.

**Open Question 3**: Can the sharp increase in prediction error in out-of-training regions be mitigated through architectural constraints, or is generalization failure inherent to data-driven closures?
*Basis*: Discussion notes "generalization of methods may be very difficult because error increases sharply when transitioning to the out-of-training region" and suggests this might not be achievable on complex problems.
*Why unresolved*: While UQ methods flag error via high uncertainty, paper doesn't explore methods (e.g., physics-informed constraints) to improve extrapolation capability itself.
*Resolution evidence*: Experiments incorporating physics-informed loss functions or tensor-basis constraints to determine if they smooth error transition at training boundaries.

## Limitations

- Computational cost analysis is incomplete; actual training times for all methods are not reported, making it difficult to assess claimed "computational efficiency" of Deep Ensembles
- Study focuses on synthetic algebraic turbulence model, which may not capture full complexity of real-world turbulence physics
- Use of uniform sampling in log-space is justified but may not represent realistic flow conditions

## Confidence

- **High**: Superiority of Deep Ensembles for computational efficiency and fundamental mechanism of epistemic uncertainty quantification are well-supported by experimental results and established literature
- **Medium**: Ranking of UQ methods (GP > DE > SVI > MCD) is based on specific metrics on synthetic dataset and may not generalize to more complex physical systems
- **Low**: Claim that "uncertainty should always increase" in out-of-training regions is not rigorously proven and may depend on specific kernel choice for GPs

## Next Checks

1. **Computational Cost Verification**: Report actual training times for each method on the same hardware to substantiate claimed efficiency gains of Deep Ensembles over GPs
2. **Generalization Test**: Apply best-performing UQ method (Deep Ensembles) to different turbulence closure model or real-world flow data to assess robustness beyond synthetic ARSM case
3. **Uncertainty Calibration**: Perform detailed analysis of when/why SVI/MCD underestimate uncertainty (overconfidence) versus when GPs might overestimate it, particularly near training domain boundaries