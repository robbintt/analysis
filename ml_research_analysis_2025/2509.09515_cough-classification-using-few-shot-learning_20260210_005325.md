---
ver: rpa2
title: Cough Classification using Few-Shot Learning
arxiv_id: '2509.09515'
source_url: https://arxiv.org/abs/2509.09515
tags:
- learning
- classification
- few-shot
- accuracy
- multi-class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of few-shot learning for cough-based
  respiratory illness classification, focusing on COVID-19, Flu, and healthy conditions.
  Using Prototypical Networks and Mel-spectrograms from three public datasets, the
  model achieves 74.87% accuracy in 3-way classification with only 15 support examples
  per class.
---

# Cough Classification using Few-Shot Learning

## Quick Facts
- arXiv ID: 2509.09515
- Source URL: https://arxiv.org/abs/2509.09515
- Reference count: 40
- Primary result: 74.87% accuracy in 3-way classification with 15 support examples per class

## Executive Summary
This study explores few-shot learning for classifying respiratory illnesses using cough sounds, focusing on COVID-19, Flu, and healthy conditions. The authors leverage Prototypical Networks with Mel-spectrograms from three public datasets, achieving 74.87% accuracy in 3-way classification with only 15 support examples per class. Binary classification models reach over 70% accuracy across all class pairs, with Flu vs. COVID-19 achieving the highest at 93.50%. Statistical equivalence tests confirm that multi-class performance is comparable to binary models within a 15% margin. The study demonstrates that few-shot learning can enable effective medical diagnostics with minimal labeled data, with Flu being the most distinguishable class and Healthy being the most challenging.

## Method Summary
The study uses Prototypical Networks with ResNet-18 backbone modified for single-channel Mel-spectrogram input (224×224, 128 frequency bands). Three datasets provide 100 samples each: Coswara (Healthy), COUGHVID (COVID-19), and FluSense (Flu). Audio is preprocessed to 1s duration at 22.05kHz, converted to Mel-spectrograms, and resized. The model uses episodic N-way K-shot training with Adam optimizer, evaluated at K=1,5,10,15 shots over 100 episodes per K-value. Performance is measured through accuracy and statistical equivalence testing (TOST, bootstrap) between binary and multi-class models.

## Key Results
- 3-way classification accuracy: 74.87% at 15-shot
- Best binary pair: Flu vs. COVID-19 at 93.50% accuracy
- Flu consistently most distinguishable class; Healthy most challenging (35-62% accuracy across K-values)
- Statistical equivalence between binary and multi-class models within 15% margin

## Why This Works (Mechanism)

### Mechanism 1
Prototypical Networks enable generalization from limited examples by learning class centroids in embedding space. Support set examples are encoded by a shared ResNet-18 backbone into embeddings; class prototypes are computed as per-class means; query samples are classified by Euclidean distance to the nearest prototype. This metric-based approach reduces the problem from learning a classifier to learning a transferable embedding space where similar samples cluster. Core assumption: The embedding space learned during training transfers to new class combinations during inference.

### Mechanism 2
ImageNet-pretrained convolutional features transfer to spectrogram classification despite domain gap. ResNet-18, pretrained on RGB images, is adapted by replacing the first convolutional layer to accept single-channel Mel-spectrograms. Early layers detect edges and textures that correspond to spectral patterns (frequency bands, temporal onsets); later layers compose these into disease-relevant features. Core assumption: Spectrograms share structural regularities with natural images that ImageNet features can exploit.

### Mechanism 3
Episodic training improves few-shot generalization by matching train and test distributions. Instead of batch gradient descent on the full dataset, training constructs episodes: randomly sample N classes, select K support and Q query examples per class, compute loss on query classification. This simulates the low-data inference condition and prevents overfitting to class-specific features. Core assumption: Episodes drawn from the training class set are representative of test-time tasks.

### Mechanism 4
Mel-spectrograms preserve disease-discriminative acoustic signatures with distinct separability per class. Cough audio is converted to 128-band Mel-spectrograms, capturing time-frequency energy patterns. Flu coughs form tight clusters; Healthy coughs show high variability and overlap with COVID-19. Core assumption: Disease-specific acoustic patterns are preserved through the Mel-scale transformation and duration normalization.

## Foundational Learning

- Concept: **Metric-based Few-Shot Learning**
  - Why needed here: Prototypical Networks rely on distance metrics in embedding space; understanding how prototypes are formed and compared is essential for debugging misclassifications.
  - Quick check question: Given support embeddings [0.2, 0.8], [0.3, 0.7] for Class A and [0.9, 0.1], [0.8, 0.2] for Class B, what is the predicted class for a query at [0.5, 0.5]?

- Concept: **Mel-spectrogram Representation**
  - Why needed here: Input modality is not raw audio but time-frequency images; understanding how audio maps to spectrograms helps diagnose whether preprocessing preserves discriminative features.
  - Quick check question: If a cough has strong high-frequency content at 0.3s, where would this appear in a Mel-spectrogram with 128 frequency bands and 1s duration?

- Concept: **Episodic Training vs. Batch Training**
  - Why needed here: The model is not trained on all data simultaneously; understanding episode construction (N-way K-shot) is critical for implementing the training loop correctly.
  - Quick check question: In a 3-way 5-shot task with 5 query samples per class, how many forward passes are needed per episode?

## Architecture Onboarding

- Component map:
  Input Pipeline -> Modified ResNet-18 -> Prototypical Network Head -> Distance-based Classification
  (Raw audio → Mel-spectrogram → Embedding → Prototype computation → Euclidean distance)

- Critical path:
  1. Preprocessing consistency: All audio must be resampled and padded identically; mismatch causes embedding misalignment.
  2. First-layer modification: ResNet-18 expects 3-channel input; failure to modify first conv for 1-channel breaks forward pass.
  3. Prototype computation: Must average only support embeddings per class; including query samples leaks labels.
  4. Distance metric: Euclidean distance (not cosine) per original Prototypical Networks formulation.

- Design tradeoffs:
  - K-shot value: Higher K (e.g., 15) improves accuracy (72% → 88% binary) but increases annotation cost; diminishing returns after K=10.
  - Binary vs. Multi-class: Binary models achieve higher accuracy (up to 93.5%), but multi-class (74.87%) is more practical for triage; paper shows statistical equivalence within 15% margin.
  - Backbone depth: ResNet-18 balances speed and capacity; deeper backbones may improve embeddings but increase computational cost (~6 hours training on Colab GPU noted).

- Failure signatures:
  - Healthy class underperformance (35-62% accuracy across K-values): High intra-class variability; confusion with COVID-19.
  - COVID-19 vs. Healthy confusion (74.8% binary at K=15): Shared acoustic characteristics; self-reported labels may introduce noise.
  - Low-shot degradation (1-shot: 60.67% multi-class): Insufficient support examples for reliable prototype estimation.

- First 3 experiments:
  1. Reproduce 3-way 5-shot classification with 100 episodes; verify accuracy falls within ±2% of reported 68.53% to confirm implementation correctness.
  2. Ablate ImageNet pretraining by training ResNet-18 from scratch; compare accuracy drop to quantify transfer learning contribution (Assumption: expected 5-10% degradation).
  3. Visualize t-SNE embeddings for held-out episodes; confirm Flu clusters tightly and Healthy/COVID-19 overlap, matching Figure 8, to validate that the encoder learns the expected structure.

## Open Questions the Paper Calls Out

### Open Question 1
Can self-supervised pretraining or attention mechanisms effectively improve discrimination between overlapping classes (Healthy vs. COVID-19)? The authors explicitly list "exploring self-supervised pretraining" and "combining Prototypical Networks with attention mechanisms" as future directions to address overlapping class scenarios. This is unresolved because the current t-SNE visualization shows significant overlap between Healthy and COVID-19 embeddings, and the Healthy class exhibits the lowest accuracy (62.6% at 15-shot). Experimental results showing reduced feature overlap in embedding visualizations and improved classification accuracy for the Healthy class after implementing these techniques would resolve this question.

### Open Question 2
Do MAML-based or transformer-backed meta-learners yield better generalization for cough classification than Prototypical Networks? The authors ask, "future studies could investigate whether MAML-based or transformer-backed meta-learners yield improved generalization for cough classification." This is unresolved because this study only evaluates Prototypical Networks with a ResNet-18 backbone, leaving the performance of other few-shot architectures in this domain unknown. A comparative study benchmarking Model-Agnostic Meta-Learning (MAML) or transformer models against the Prototypical Network baseline using the same cough datasets would resolve this question.

### Open Question 3
To what extent does using clinically verified labels improve model reliability compared to the self-reported labels used in this study? The paper notes that self-reported labels may introduce noise and that future work involves "expanding to larger and more clinically verified datasets." This is unresolved because the ambiguity of the "Healthy" class (sourced from a COVID-focused dataset) suggests that label noise may be a limiting factor in current performance. A comparison of model performance on the current dataset versus a dataset where cough labels are confirmed by clinical diagnosis rather than self-reporting would resolve this question.

## Limitations

- Limited corpus evidence for Prototypical Networks applied to cough audio and ImageNet-to-spectrogram transfer
- Unclear extent of label noise from self-reported COVID-19 status across datasets
- Statistical equivalence between binary and multi-class models relies on TOST test methodology not fully specified

## Confidence

- **High**: Few-shot learning feasibility with 15-shot performance (74.87% multi-class, 93.50% best binary)
- **Medium**: Flu as most distinguishable class, Healthy as most challenging (class-wise analysis supported but separability not directly validated)
- **Low**: Statistical equivalence between binary and multi-class models (TOST test methodology not fully specified)

## Next Checks

1. Replicate 3-way 5-shot classification with 100 episodes; verify accuracy within ±2% of reported 68.53%
2. Train ResNet-18 from scratch (no ImageNet pretraining); measure accuracy drop to quantify transfer learning contribution
3. Visualize t-SNE embeddings for held-out episodes; confirm Flu clusters tightly and Healthy/COVID-19 overlap matches Figure 8