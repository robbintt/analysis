---
ver: rpa2
title: Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition
arxiv_id: '2508.03780'
source_url: https://arxiv.org/abs/2508.03780
tags:
- adversarial
- interpretable
- more
- emotion
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether inherently interpretable deep
  learning models are more robust to adversarial perturbations compared to black-box
  models in the context of music emotion recognition. The authors compare three model
  types: a standard black-box model (A2E), an inherently interpretable concept bottleneck
  model (A2M2E) that predicts emotions through human-understandable mid-level features,
  and their adversarially trained counterparts.'
---

# Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition

## Quick Facts
- **arXiv ID**: 2508.03780
- **Source URL**: https://arxiv.org/abs/2508.03780
- **Reference count**: 0
- **Primary result**: Interpretable concept bottleneck models show significantly higher robustness to adversarial perturbations than black-box models in music emotion recognition, with performance comparable to adversarially trained models despite no adversarial training.

## Executive Summary
This paper investigates whether inherently interpretable deep learning models demonstrate greater robustness to adversarial perturbations compared to black-box models in the context of music emotion recognition. The authors compare three model types: a standard black-box model (A2E), an interpretable concept bottleneck model (A2M2E) that predicts emotions through human-understandable mid-level features, and their adversarially trained counterparts. Using an adapted Basic Iterative Method attack for regression tasks, they measure performance degradation after adversarial perturbation. Results show that the interpretable concept bottleneck model exhibits significantly higher robustness than the black-box model, with performance losses comparable to adversarially trained models despite not requiring adversarial training.

## Method Summary
The study uses a VGG-style CNN architecture for music emotion recognition, comparing four model variants: A2E (black-box), A2M2E (interpretable concept bottleneck), A2B2E (bottleneck without supervision), and their adversarially trained counterparts. The interpretable model (A2M2E) includes a bottleneck layer trained to predict 7 human-understandable mid-level features before mapping to 8 emotion targets. Models are trained on the Soundtracks dataset (360 clips for emotion, 5,000 for mid-level features) using MSE loss, with A2M2E using joint loss (0.5*mid-level MSE + 0.5*emotion MSE). Robustness is evaluated using an adapted Basic Iterative Method attack that maximizes MSE, with performance measured by ΔMAE (performance drop after attack).

## Key Results
- The interpretable concept bottleneck model (A2M2E) shows significantly higher robustness than the black-box model (A2E) with p < 0.05
- A2M2E achieves MAE of 0.11 ± 0.01 under attack, comparable to adversarially trained models
- Under attack, black-box models produce wild predictions in the range [-10, 10] while interpretable models maintain tighter bounds closer to valid [0,1] range

## Why This Works (Mechanism)

### Mechanism 1: Bottleneck Filtering Spurious Correlations
Forcing the model to predict human-understandable concepts acts as a filter against spurious correlations, inadvertently conferring robustness. The bottleneck constrains the model to a representation space aligned with human perception, preventing it from fitting brittle features that adversarial attacks exploit. This mechanism assumes the mid-level features are robust to minor perturbations and causally linked to emotions.

### Mechanism 2: Reduced Butterfly Effect
The interpretable model's architecture reduces the "butterfly effect" of input perturbations. With a linear layer connecting concepts to emotions, small changes in input spectrograms have limited impact on final emotion predictions. This contrasts with black-box models where perturbations propagate through deep non-linear layers, resulting in wildly out-of-range predictions.

### Mechanism 3: Computational Free Lunch
Concept bottleneck models provide a computational advantage by achieving parity with adversarial training without iterative training cost. The interpretable model effectively "skips" the vulnerability induction phase that adversarial training tries to correct by ignoring non-robust features during standard training.

## Foundational Learning

### Concept: Concept Bottleneck Models (CBM)
**Why needed**: This is the core architecture of the robust model (A2M2E). The network is forced to pass through an intermediate layer of human-labeled concepts (e.g., "tonal stability") which acts as the interpretability mechanism.
**Quick check**: Can you explain why the A2B2E model (same architecture, no concept supervision) was included in the study?

### Concept: Adversarial Attacks in Regression (BIM for Regression)
**Why needed**: Unlike classification attacks, this task uses regression where the attack tries to maximize Mean Squared Error rather than minimize classification probability.
**Quick check**: How does the "untargeted" attack strategy in this paper differ from a standard target misclassification attack?

### Concept: Spurious Correlations
**Why needed**: The paper attributes black-box vulnerability to models learning "shortcuts" (spurious correlations) that don't generalize. Understanding this explains why forcing "interpretable features" solves the problem.
**Quick check**: If a model predicts "sadness" based solely on low volume rather than harmonic content, is this a spurious correlation, and would it likely be robust to perturbations?

## Architecture Onboarding

### Component map:
Log-scaled spectrograms (10s snippets) -> VGG-style CNN backbone -> Bottleneck layer (7 neurons, A2M2E only) -> Linear layer -> 8 emotion outputs

### Critical path:
The Joint Loss Function is critical. A2M2E must be trained to minimize a weighted sum of MSE (Mid-level prediction) and MSE (Emotion prediction). If trained sequentially or mid-level loss is dropped (turning it into A2B2E), the robustness property disappears.

### Design tradeoffs:
- **Accuracy vs. Robustness**: A2M2E shows slightly lower average correlation (0.67) on clean data compared to A2E (0.73), but significantly higher resilience to attack
- **Complexity**: Requires dual datasets (Soundtracks for emotion, Mid-Level Features for concepts)

### Failure signatures:
- **Wild Predictions**: Under attack, black-box models predict emotions in the range [-10, 10] despite training targets being [0, 1]. Interpretable models maintain tighter bounds
- **High Variance**: Black-box models show high variance in robustness across random seeds; interpretable models are more consistent

### First 3 experiments:
1. **Reproduce the Baseline**: Train A2E and A2M2E on the Soundtracks dataset. Verify that A2M2E has slightly lower but comparable clean performance (Correlation ≈ 0.67-0.75)
2. **Implement Regression BIM**: Adapt the Basic Iterative Method to maximize MSE. Verify that for A2E, ΔMAE is high, while for A2M2E, it remains low
3. **Ablation on Bottleneck Supervision**: Compare A2M2E (Supervised Bottleneck) vs. A2B2E (Unsupervised Bottleneck). Confirm that A2B2E behaves like the brittle black-box A2E, proving that supervision drives robustness, not just architecture

## Open Questions the Paper Calls Out
The authors explicitly identify the limitation of using "only one single model and recognition task" in the conclusion, calling for "broader and more systematic analyses" to test whether the observed robustness generalizes to other MIR tasks, different deep learning architectures, and domains outside of music emotion recognition.

## Limitations
- Dataset size constraints: Results based on relatively small dataset (360 clips for emotion, 5,000 for mid-level features) may limit generalizability
- Attack specificity: The Basic Iterative Method may not capture all real-world adversarial scenarios in audio processing
- Mechanism isolation: The exact contribution of each architectural component to robustness remains unclear

## Confidence
- **High confidence**: The empirical observation that interpretable models show higher robustness than black-box models (p < 0.05) is well-supported
- **Medium confidence**: The claim that interpretable models achieve comparable robustness to adversarially trained models without computational cost is supported but could benefit from additional ablation studies
- **Medium confidence**: The mechanism explanation (bottleneck filtering spurious correlations) is plausible but not definitively proven

## Next Checks
1. **Ablation on bottleneck supervision**: Compare A2M2E (concept-supervised) against A2B2E (unsupervised bottleneck) to definitively prove that concept supervision, not just architecture, drives robustness
2. **Attack transferability**: Test whether robustness transfers across different attack types (e.g., FGSM, Carlini-Wagner) to confirm the phenomenon is not attack-specific
3. **Concept quality impact**: Evaluate model robustness when trained with noisy or inconsistent mid-level feature annotations to test the dependency on high-quality concept labels