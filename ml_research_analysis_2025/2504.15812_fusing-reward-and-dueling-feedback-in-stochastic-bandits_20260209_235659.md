---
ver: rpa2
title: Fusing Reward and Dueling Feedback in Stochastic Bandits
arxiv_id: '2504.15812'
source_url: https://arxiv.org/abs/2504.15812
tags:
- feedback
- dueling
- regret
- reward
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies fusing reward and dueling feedback in stochastic
  multi-armed bandits (DR-MAB), where both feedback types are gathered in each decision
  round. The authors derive a regret lower bound showing that an efficient algorithm
  may incur only the smaller among the reward and dueling-based regret for each individual
  arm.
---

# Fusing Reward and Dueling Feedback in Stochastic Bandits

## Quick Facts
- **arXiv ID:** 2504.15812
- **Source URL:** https://arxiv.org/abs/2504.15812
- **Reference count:** 40
- **One-line primary result:** Fuses reward and dueling feedback in stochastic multi-armed bandits; proposes two algorithms (ElimFusion and DecoFusion) with DecoFusion achieving near-optimal regret.

## Executive Summary
This paper studies the fusion of reward and dueling feedback in stochastic multi-armed bandits (DR-MAB). Both feedback types are gathered in each decision round. The authors derive a regret lower bound showing that an efficient algorithm may incur only the smaller among the reward and dueling-based regret for each individual arm. Two fusion approaches are proposed: (1) ElimFusion, a simple elimination fusion algorithm that leverages both feedback types to explore all arms and unifies collected information by sharing a common candidate arm set, and (2) DecoFusion, a decomposition fusion algorithm that selects the more effective feedback to explore the corresponding arms and randomly assigns one feedback type for exploration and the other for exploitation in each round. While ElimFusion experiences a suboptimal multiplicative term of the number of arms in regret due to the intrinsic suboptimality of dueling elimination, DecoFusion achieves regret matching the lower bound up to a constant under a common assumption. Extensive experiments confirm the efficacy of the algorithms and theoretical results. The key technical contributions include deriving regret lower bounds for DR-MAB, proposing two fusion algorithms (ElimFusion and DecoFusion), and proving that DecoFusion achieves near-optimal regret matching the lower bound up to a constant factor.

## Method Summary
The paper proposes two fusion algorithms for DR-MAB. ElimFusion maintains a shared candidate set of arms and eliminates an arm if either reward or dueling feedback provides sufficient evidence of suboptimality. DecoFusion approximates the optimal arm decomposition using empirical log-likelihoods and uses a randomized decision strategy with a specific probability threshold to balance exploration and exploitation across feedback types. DecoFusion aims to match the theoretical lower bound on regret up to a constant factor by decoupling exploration and exploitation phases.

## Key Results
- Derived a regret lower bound showing that an efficient algorithm may incur only the smaller among the reward and dueling-based regret for each individual arm.
- Proposed ElimFusion, a simple elimination fusion algorithm that unifies feedback by sharing a common candidate arm set.
- Proposed DecoFusion, a decomposition fusion algorithm that achieves regret matching the lower bound up to a constant under a common assumption.
- Extensive experiments confirm the efficacy of the algorithms and theoretical results.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Regret minimization in a heterogeneous feedback environment is achieved by paying only the minimum cost between available feedback types for each suboptimal arm, rather than summing them.
- **Mechanism:** The theoretical lower bound (Theorem 2.3) demonstrates that for any suboptimal arm $k$, the information required to identify it as suboptimal can be gathered additively from either reward or dueling feedback. However, an efficient algorithm minimizes the cost-weighted regret by exclusively selecting the feedback type (Reward or Dueling) that offers the smaller "regret cost" (weighted by $\alpha$) for that specific arm. This decomposes the problem into independent sub-problems for each arm.
- **Core assumption:** The algorithm is consistent (errors decay faster than polynomial time), and the KL-divergence between arm distributions accurately quantifies the "information" needed to distinguish them.
- **Evidence anchors:**
  - [abstract] "...efficient algorithm may incur only the smaller among the reward and dueling-based regret for each individual arm."
  - [section 2.1] Theorem 2.3 lower bound derivation using linear programming over the constraint line segment.
  - [corpus] Corpus does not contain direct precedents for this specific fusion; the mechanism relies on the paper's novel lower bound derivation.
- **Break condition:** If the parameter $\alpha$ (cost weight) is misspecified such that the algorithm chooses the "expensive" feedback type for an arm where gaps are significantly asymmetric, regret will scale with the worse feedback type.

### Mechanism 2
- **Claim:** Simple unification of feedback can be achieved by maintaining a single shared candidate set of arms, where elimination occurs if *either* feedback type provides sufficient evidence of suboptimality.
- **Mechanism:** ElimFusion maintains a single set $C$. An arm $k$ is removed if its Reward-UCB is lower than the best arm's Reward-LCB *or* if its Dueling-UCB against another arm is $<0.5$. This "OR" logic ensures that the algorithm benefits from the faster feedback channel without needing to know which one is faster a priori.
- **Core assumption:** Confidence bounds (Hoeffding-based) hold uniformly across time and arms.
- **Evidence anchors:**
  - [abstract] "...sharing a common candidate arm set..."
  - [section 3.1] Algorithm 1, Line 14 describes the elimination condition using logical "or" between reward and dueling bounds.
- **Break condition:** If the dueling feedback is extremely noisy or the number of arms $K$ is large, the $O(K)$ multiplicative regret factor (inherited from dueling elimination) will dominate, causing performance significantly worse than the optimal decomposition strategy.

### Mechanism 3
- **Claim:** Near-optimal fusion requires decoupling the exploration and exploitation phases across feedback types using a randomized decision rule to resolve "decomposition deadlock."
- **Mechanism:** DecoFusion approximates the optimal arm decomposition ($K^{(R)}$ vs $K^{(D)}$) using empirical log-likelihoods. It then uses a specific probability threshold $\frac{\alpha^2}{\alpha^2+(1-\alpha)^2}$ to decide the round's activity. If the random draw favors reward, it explores the target arm via *reward* (absolute) while simultaneously exploiting the best estimated arm via *dueling* (relative). This orthogonal usage allows the algorithm to maintain estimates for the decomposition without paying the "deadlock" cost of needing accurate estimates of *both* types for every arm.
- **Core assumption:** The estimated optimal arms $\hat{k}^{(R)}_t$ and $\hat{k}^{(D)}_t$ converge quickly to the true optimal arm 1.
- **Evidence anchors:**
  - [abstract] "...randomly assigns one feedback type for exploration and the other for exploitation in each round."
  - [section 4.2.2] Lines 12-22 detail the randomized decision logic and the specific threshold calculation.
- **Break condition:** If the "free exploration" property (when $\alpha=0$ or $\alpha=1$) is not leveraged correctly (i.e., if the randomized strategy fails to converge to deterministic exploration of the free channel), the algorithm will suffer $O(\log T)$ regret instead of the achievable constant regret.

## Foundational Learning

- **Concept: Multi-Armed Bandits (MAB) & Regret**
  - **Why needed here:** This is the base stochastic environment. You must understand "gap" ($\Delta$) as the difference between optimal and suboptimal arms and Regret as the cumulative loss from not picking the optimal arm.
  - **Quick check question:** If $\Delta_k$ is small, does an algorithm need more or fewer samples to eliminate arm $k$ with high confidence? (Answer: More samples).

- **Concept: Dueling Bandits (Relative Feedback)**
  - **Why needed here:** The paper fuses absolute (reward) with relative (dueling) feedback. You need to understand that dueling relies on probabilities $\nu_{k,\ell}$ (probability $k$ beats $\ell$) and identifying a Condorcet winner.
  - **Quick check question:** In a dueling bandit, if you compare two suboptimal arms, do you gather information about the optimal arm? (Answer: Typically no/limited; you usually need to duel against a strong candidate to gain information, hence the complexity).

- **Concept: KL-Divergence and Information Theory**
  - **Why needed here:** The core theoretical argument (Lower Bound) and the advanced algorithm (DecoFusion) rely on "Information" ($I^{(R)}_k, I^{(D)}_k$) defined via KL-divergence, not just simple gaps.
  - **Quick check question:** Why does the paper use KL-divergence in the lower bound instead of just the squared gap $\Delta^2$? (Answer: KL-divergence is tighter for Bernoulli distributions; $\Delta^2$ is a simplification for sub-Gaussian tails).

## Architecture Onboarding

- **Component map:**
  - **Feedback Collector:** Simultaneously pulls tuple $\{k_t, (k_{1,t}, k_{2,t})\}$.
  - **Statistics Module:** Tracks $\hat{\mu}_k$ (reward means), $\hat{\nu}_{k,\ell}$ (dueling probs), and crucially $\hat{I}^{(R)}_k, \hat{I}^{(D)}_k$ (Information/Log-likelihoods).
  - **Decomposition Engine:** Sorts arms into sets $\hat{K}^{(R)}$ (Reward-favorable) and $\hat{K}^{(D)}$ (Dueling-favorable) based on information measures.
  - **Decision Engine:** Implements the randomized strategy (Exploit via A, Explore via B).

- **Critical path:** The update of **Empirical Log-Likelihoods** ($\hat{I}$) $\to$ **Set Reconstruction** ($\hat{K}$) $\to$ **Randomized Selection**. If the Log-Likelihoods are stale or incorrect, the decomposition fails, and the randomization balance is broken.

- **Design tradeoffs:**
  - **ElimFusion vs. DecoFusion:** ElimFusion is significantly easier to implement (standard UCB/Elimination logic) but suffers a $O(K)$ regret penalty. DecoFusion is theoretically optimal (constant regret potential) but requires maintaining complex information metrics and careful tuning of the randomization threshold.

- **Failure signatures:**
  - **Regret spikes at $T_{start}$:** Indicates a failure in the Warm-up phase (Algorithm 3), leading to poor initial estimates for the Decomposition Engine.
  - **Linear Regret in DecoFusion:** Suggests the estimated optimal arms $\hat{k}^{(R)}$ or $\hat{k}^{(D)}$ are locking onto a suboptimal arm, causing the exploitation path to be permanently wrong.
  - **No improvement over Single-Feedback:** Suggests the fusion logic isn't triggering; e.g., the confidence bounds are too loose, or the decomposition sets are overlapping excessively (deadlock).

- **First 3 experiments:**
  1.  **Free Lunch Test ($\alpha=0$ or $\alpha=1$):** Run DecoFusion with reward cost set to 0 ($\alpha=0$). Verify that regret becomes a flat line (constant) after warm-up, proving the "free exploration" property.
  2.  **Asymmetric Gap Test:** Set reward gaps $\Delta^{(R)}$ to be tiny (hard to distinguish) and dueling gaps $\Delta^{(D)}$ to be large. Verify that DecoFusion shifts exploration primarily to the dueling channel.
  3.  **Scalability Test ($K$ variance):** Compare ElimFusion and DecoFusion as $K$ increases. Verify that ElimFusion's regret scales roughly linearly with $K$ while DecoFusion scales significantly better (logarithmically with respect to the sum of inverse gaps).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the principles of the DR-MAB fusion framework be extended to general online learning problems, such as reinforcement learning or contextual bandits?
- **Basis in paper:** [Explicit] Section 3.1 states that "the ultimate goal is to address the fusion in general online learning problems beyond bandits," suggesting the current work is a foundational step.
- **Why unresolved:** The theoretical analysis and algorithms (ElimFusion, DecoFusion) are derived specifically for stochastic multi-armed bandits, relying on stationary reward distributions and dueling probabilities.
- **Evidence to resolve:** Derivation of regret bounds for fusion algorithms in contextual settings or RL environments (e.g., RLHF) that handle heterogeneous feedback.

### Open Question 2
- **Question:** Can an algorithm be designed to achieve the general lower bound (Theorem 2.3) without assuming the optimal arm is the most effective competitor ($\ell^*_k = 1$) for every suboptimal arm?
- **Basis in paper:** [Inferred] Section 4.3 notes that DecoFusion achieves near-optimal regret matching the *simplified* lower bound (Corollary 2.4), but admits the bound is worse than the general lower bound when the most effective dueling arm is not the optimal arm.
- **Why unresolved:** DecoFusion uses the estimated optimal arm for dueling comparisons, which may be suboptimal if a different competitor arm provides more information per unit of regret.
- **Evidence to resolve:** An algorithm that dynamically identifies the most effective competitor arm for each suboptimal arm and matches the general lower bound expression.

### Open Question 3
- **Question:** How can reward and dueling feedback be effectively fused in non-stochastic or adversarial environments?
- **Basis in paper:** [Inferred] The Related Work section distinguishes between stochastic and adversarial bandits, but the paper's methodology is restricted entirely to the stochastic setting.
- **Why unresolved:** The current algorithms rely on confidence bounds and empirical divergence estimates that assume stationary (i.i.d.) feedback, which does not hold in adversarial settings.
- **Evidence to resolve:** A fusion algorithm for adversarial bandits with regret bounds competitive with baselines that use only a single feedback type.

## Limitations
- The regret lower bound (Theorem 2.3) relies on KL-divergence between Bernoulli distributions, which may not accurately reflect the information-theoretic cost in all practical scenarios.
- The "free exploration" property claimed for DecoFusion when $\alpha=0$ or $\alpha=1$ depends critically on the convergence of the estimated optimal arms $\hat{k}^{(R)}_t$ and $\hat{k}^{(D)}_t$. No explicit rate of convergence is provided.
- The paper assumes knowledge of the reward-dueling cost ratio $\alpha$, but does not address the impact of $\alpha$ being misspecified or needing to be learned online.

## Confidence
- **High Confidence:** The mechanism of DecoFusion's randomized decision rule (Mechanism 3) is clearly specified and the logic for using the threshold $\frac{\alpha^2}{\alpha^2+(1-\alpha)^2}$ is sound. The experimental results (Figure 2) showing DecoFusion's superiority over ElimFusion are consistent with the theoretical analysis.
- **Medium Confidence:** The regret lower bound (Theorem 2.3) is a novel and powerful result, but its practical tightness depends on the accuracy of the KL-divergence approximation for the specific arm distributions. The theoretical gap between DecoFusion's upper bound and the lower bound (constant factor) is acknowledged but not quantified.
- **Low Confidence:** The impact of the "fixed order" selection of arms from the exploration set $E$ in DecoFusion (Algorithm 2, Line 11) on regret performance is not rigorously analyzed. This could introduce bias or affect the convergence of the log-likelihood estimates.

## Next Checks
1. **Sensitivity Analysis of $\alpha$:** Implement DecoFusion with $\alpha$ values slightly perturbed from the true cost ratio. Measure the degradation in regret to quantify the algorithm's robustness to cost parameter misspecification.
2. **Convergence Rate of Optimal Arm Estimates:** For $\alpha=0$ or $\alpha=1$, track the convergence of $\hat{k}^{(R)}_t$ and $\hat{k}^{(D)}_t$ to the true optimal arm. Empirically determine the time required for these estimates to stabilize and the impact on regret.
3. **Generalization to Non-Condorcet Instances:** Extend the experimental evaluation to scenarios where no Condorcet winner exists (i.e., $\nu_{k,\ell} \leq 0.5$ for some pairs). Assess whether the fusion algorithms degrade gracefully or if the theoretical guarantees break down.