---
ver: rpa2
title: 'Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning
  in Large Language Models'
arxiv_id: '2506.05850'
source_url: https://arxiv.org/abs/2506.05850
tags:
- language
- reasoning
- grpo
- training
- gsm8k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual Collapse occurs when multilingual reasoning models,
  trained via reinforcement learning, systematically revert to their dominant pre-training
  language (usually English) during chain-of-thought generation, even when prompts
  are in other languages. This study trains Llama-3.2-3B and Qwen-2.5-1.5B with Group-Relative
  Policy Optimization on translated GSM8K and SimpleRL-Zoo datasets in Chinese, Korean,
  and Ukrainian, monitoring task accuracy and language consistency.
---

# Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2506.05850
- **Source URL:** https://arxiv.org/abs/2506.05850
- **Reference count:** 13
- **Primary result:** Multilingual reasoning models trained with GRPO systematically revert to their dominant pre-training language (usually English) during chain-of-thought generation, even when prompts are in other languages.

## Executive Summary
Cross-lingual Collapse is a phenomenon where multilingual reasoning models, trained via reinforcement learning, systematically revert to their dominant pre-training language during chain-of-thought generation. This study demonstrates that GRPO amplifies pre-training language imbalances, causing low-resource languages to collapse rapidly—Ukrainian word ratio dropped from 98% to 0.3% while accuracy rose +17.1%. A language-consistency reward mitigated this drift but reduced accuracy gains by 5-10 percentage points. Adding harder curriculum data induced collapse even in mid-resource Korean. Post-hoc fine-tuning on distilled LRMs failed to restore target-language reasoning, indicating the bias is self-reinforcing and largely irreversible.

## Method Summary
The study trains multilingual LLMs (Llama-3.2-3B, Qwen-2.5-1.5B, HyperCLOVA-X-1.5B) with Group-Relative Policy Optimization on translated GSM8K and SimpleRL-Zoo datasets in Chinese, Korean, and Ukrainian. Translation quality is filtered with COMET (threshold ≥0.70), with 15% held out for validation. Training runs up to 1K-2K steps while monitoring task accuracy (math-verify) and target-language word ratio (script-based token classification). Ablation studies include language-consistency rewards and curriculum difficulty variations. Post-hoc GRPO on DeepSeek-R1-Distilled Qwen tests reversibility.

## Key Results
- GRPO amplifies pre-training language imbalances, causing rapid collapse in low-resource languages
- Ukrainian word ratio collapsed from 98% to 0.3% while accuracy rose +17.1% on GSM8K
- Harder curriculum data (SimpleRL-Zoo) induced collapse even in mid-resource Korean
- Post-hoc fine-tuning on distilled LRMs failed to restore target-language reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Pre-training Prior Amplification via GRPO
- **Claim:** GRPO rapidly amplifies pre-training language imbalances, causing models to abandon low-resource languages in favor of the dominant pre-training language during reasoning.
- **Mechanism:** GRPO assigns advantage signals based on verifiable accuracy rewards. Since the dominant pre-training language (typically English) has stronger internal representations, reasoning traces in that language more reliably achieve correct answers. The optimizer reinforces these higher-reward pathways, progressively shifting token distributions toward the dominant language.
- **Core assumption:** The dominant pre-training language yields higher expected reward due to stronger learned representations for complex reasoning.
- **Evidence anchors:** Abstract states GRPO rapidly amplifies pre-training language imbalances; Table 1 shows Ukrainian word ratio collapsed from 98% → 0.3% while accuracy rose +17.1 pp on GSM8K; Schut et al. (2025) show models route through English-centric representation space even for non-English I/O.

### Mechanism 2: Task Difficulty as Collapse Trigger
- **Claim:** Harder training curricula accelerate cross-lingual collapse even in mid-resource languages that appeared stable on easier tasks.
- **Mechanism:** As task difficulty increases, the accuracy gap between reasoning in the target language vs. the dominant language widens. The optimizer more aggressively exploits any path to correct answers, and English provides a lower-friction reasoning substrate.
- **Core assumption:** Difficulty amplifies the reward differential between languages.
- **Evidence anchors:** Table 2 shows Korean model maintained >88% Korean word ratio on GSM8K alone, but collapsed to 2.1% on MATH500 when SimpleRL-Zoo was added; Section 4.1 states "task difficulty tilts the optimizer toward English reasoning."

### Mechanism 3: Self-Reinforcing Irreversibility
- **Claim:** Once cross-lingual collapse occurs, subsequent fine-tuning struggles to restore target-language reasoning capabilities.
- **Mechanism:** GRPO's advantage signals reinforce language-switching behaviors at the token level. These reinforced patterns become entrenched in the policy distribution. Post-hoc training on distilled LRMs cannot overwrite the self-reinforcing bias because the model's internal representations have already shifted.
- **Core assumption:** The collapse represents a stable equilibrium in policy space, not merely a transient drift.
- **Evidence anchors:** Abstract states "subsequent fine-tuning struggles to steer the model back toward its original target-language reasoning capabilities"; Figure 5 shows continued GRPO on DeepSeek-R1-Distilled Qwen showed steep decline in Korean word ratio; Lindsey et al. (2025) describe English as "mechanistically privileged" in multilingual circuits.

## Foundational Learning

- **Concept: Group-Relative Policy Optimization (GRPO)**
  - Why needed here: This is the core RL algorithm causing collapse. Understanding how verifiable rewards propagate through group-relative advantages is essential.
  - Quick check question: Can you explain why GRPO might favor higher-probability token sequences from pre-training over lower-probability target-language tokens?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Collapse manifests specifically in extended CoT traces. Longer reasoning = more tokens exposed to language drift.
  - Quick check question: How does increasing CoT length affect the cumulative probability of language switching?

- **Concept: Language Resource Levels**
  - Why needed here: Collapse severity correlates with resource level. High/mid/low-resource languages exhibit different collapse dynamics.
  - Quick check question: Why would Ukrainian (low-resource) collapse faster than Chinese (high-resource) even when both are non-English?

## Architecture Onboarding

- **Component map:** GSM8K/SimpleRL-Zoo datasets → GPT-4o translation → COMET filtering → GRPO training → accuracy and word ratio monitoring
- **Critical path:** Translate dataset → filter with COMET > 0.70 → Run GRPO training while logging rollouts → Compute word ratio per step (script detection: Hangul, CJK, Cyrillic, Latin) → Track accuracy vs. language drift tradeoff
- **Design tradeoffs:**
  - Language-consistency reward: Preserves target language but costs 5-10 pp accuracy
  - Harder curriculum: Improves accuracy but accelerates collapse in mid-resource languages
  - Target-language-optimized backbone (e.g., HCX): Delays collapse but doesn't eliminate it
- **Failure signatures:**
  - Sudden word ratio drop (e.g., 80% → 20%) coinciding with accuracy spike
  - English tokens appearing in early reasoning steps, then dominating
  - Post-hoc fine-tuning showing no recovery in target-language word ratio
- **First 3 experiments:**
  1. Baseline collapse: Train Llama-3.2-3B with GRPO on Ukrainian GSM8K; log word ratio and accuracy every 50 steps to identify collapse onset window
  2. Reward ablation: Add language-consistency reward (proportional to target-language word ratio) and measure accuracy drop vs. language preservation
  3. Curriculum stress test: Mix GSM8K + SimpleRL-Zoo for Korean; confirm if harder curriculum triggers collapse in a mid-resource language that was stable on GSM8K alone

## Open Questions the Paper Calls Out

- **Question:** What reward designs can successfully decouple reasoning accuracy from language fidelity without sacrificing 5-10 percentage points of performance?
  - Basis in paper: The conclusion states: "We release our code and dataset in the hope that they will spur research on reward designs that disentangle accuracy from language fidelity."
  - Why unresolved: The language-consistency reward preserves target-language word ratio but reduces accuracy gains; no alternative reward shaping approach was tested.
  - What evidence would resolve it: A new reward formulation that maintains both high accuracy (>80% on GSM8K) and high target-language word ratio (>90%) across low-resource languages like Ukrainian.

- **Question:** How does cross-lingual collapse unfold at the token level—do English fragments first appear at specific reasoning steps or conceptual categories?
  - Basis in paper: Section 4.1 asks: "Micro-level dynamics—how does collapse unfold token-by-token? We examine rollout samples to see when English fragments first appear and whether they align with specific reasoning steps."
  - Why unresolved: The paper shows aggregate word-ratio curves but does not analyze which reasoning steps (e.g., calculation, planning, verification) trigger the earliest language switches.
  - What evidence would resolve it: Token-level attribution analysis mapping language-switch points to reasoning step types across multiple problems and languages.

- **Question:** Does cross-lingual collapse behave differently in models larger than 3B parameters, or does the phenomenon scale predictably with model size?
  - Basis in paper: The limitations section states: "experiments stop at 3B parameters due to GPU limits, so collapse behaviour may differ for larger checkpoints."
  - Why unresolved: Larger models may have different pre-training language distributions or capacity to maintain multiple reasoning pathways.
  - What evidence would resolve it: Replication of GRPO experiments on Llama-3.1-70B or Qwen-2.5-72B using identical Ukrainian/Korean GSM8K training, comparing collapse rates and accuracy trajectories.

## Limitations
- Experiments limited to 3B parameter models due to GPU constraints
- Results based on three languages (Chinese, Korean, Ukrainian); generalization to other language families untested
- Specific GRPO hyperparameters and language-consistency reward formulations not fully detailed
- Irreversibility demonstrated post-hoc but exact point-of-no-return not mapped

## Confidence
- **High Confidence:** The core empirical finding that GRPO amplifies pre-training language imbalances (causing rapid collapse in low-resource languages) is well-supported by systematic experiments and consistent across datasets
- **Medium Confidence:** The mechanism linking task difficulty to collapse acceleration is plausible and supported by the Korean-MATH500 result, but lacks broader curriculum ablation studies to rule out other factors
- **Medium Confidence:** The claim of self-reinforcing irreversibility is demonstrated, but the extent of potential recovery under alternative reward shaping or larger-scale post-hoc training remains uncertain

## Next Checks
1. **Hyperparameter Sweep:** Systematically vary GRPO learning rate and rollout count to map the stability landscape of language consistency vs. accuracy
2. **Broader Language Test:** Replicate the core collapse experiment with a high-resource Romance language (e.g., Spanish) and a low-resource Semitic language (e.g., Hebrew) to test generalization
3. **Early Intervention Test:** Implement a "recovery reward" that penalizes language drift starting at the first sign of collapse (e.g., word ratio < 50%) and measure if this prevents full entrenchment