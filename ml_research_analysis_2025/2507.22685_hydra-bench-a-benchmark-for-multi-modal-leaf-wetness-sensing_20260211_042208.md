---
ver: rpa2
title: 'Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing'
arxiv_id: '2507.22685'
source_url: https://arxiv.org/abs/2507.22685
tags:
- leaf
- imaging
- wetness
- dataset
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-modal dataset for leaf wetness detection,
  combining mmWave SAR, RGB images, and raw mmWave data collected over six months
  from five plant species under both indoor and outdoor conditions. The dataset is
  designed to support benchmarking of machine learning models and SAR imaging algorithms
  for agricultural monitoring.
---

# Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing

## Quick Facts
- arXiv ID: 2507.22685
- Source URL: https://arxiv.org/abs/2507.22685
- Reference count: 0
- Primary result: Multi-modal dataset combining mmWave SAR, RGB images, and raw mmWave data for leaf wetness detection across five plant species

## Executive Summary
This paper introduces Hydra-Bench, a comprehensive dataset for leaf wetness detection that combines three sensing modalities: millimeter-wave synthetic aperture radar (SAR), RGB images, and raw mmWave data. The dataset was collected over six months from five different plant species under both indoor and outdoor conditions. The authors demonstrate that their Hydra model, which fuses these modalities, achieves 96% ± 2.14% accuracy in indoor settings and approximately 90% accuracy in outdoor farm environments, significantly outperforming single-modality baselines. This work provides a foundational resource for advancing multi-modal fusion techniques, explainable AI in agricultural sensing, and precision agriculture monitoring systems.

## Method Summary
The authors collected multi-modal data using mmWave SAR, RGB cameras, and raw mmWave sensors positioned to monitor five plant species. Data collection occurred across both controlled indoor environments and real-world outdoor farm settings over a six-month period. The Hydra model architecture fuses features from all three modalities through a multi-modal fusion framework. Indoor experiments achieved 96% ± 2.14% accuracy, while outdoor farm deployment showed approximately 90% accuracy. The dataset is specifically designed to support benchmarking of machine learning models and SAR imaging algorithms for agricultural applications, enabling systematic evaluation of multi-modal fusion approaches in leaf wetness detection.

## Key Results
- Hydra model achieves 96% ± 2.14% accuracy in controlled indoor settings
- Outdoor farm deployment shows approximately 90% accuracy
- Multi-modal fusion significantly outperforms single-modality baselines
- Dataset covers five plant species over six months with both indoor and outdoor conditions

## Why This Works (Mechanism)
The Hydra-Bench dataset's effectiveness stems from the complementary nature of the three sensing modalities. mmWave SAR provides high-resolution penetration through environmental obstructions and can detect moisture content through leaves, while RGB imaging captures visual surface characteristics and contextual plant features. The raw mmWave data offers additional signal processing opportunities for advanced feature extraction. By fusing these modalities, the Hydra model leverages both structural information (from SAR) and surface appearance (from RGB), creating a more robust representation that captures leaf wetness from multiple physical perspectives. This multi-modal approach is particularly effective in agricultural settings where environmental conditions vary significantly, as different modalities may perform better under different lighting, weather, or plant states.

## Foundational Learning
- **Multi-modal fusion techniques** - Why needed: Different sensors capture complementary information about leaf wetness that single modalities cannot fully represent. Quick check: Verify fusion architecture effectively combines modality-specific features without losing critical information.
- **Synthetic aperture radar principles** - Why needed: mmWave SAR provides unique penetration and moisture detection capabilities essential for accurate wetness sensing. Quick check: Confirm SAR resolution and penetration depth are sufficient for detecting leaf moisture variations.
- **Agricultural sensing challenges** - Why needed: Outdoor environments introduce variable lighting, weather, and plant conditions that affect sensor performance. Quick check: Validate model performance across different weather conditions and times of day.
- **Explainable AI in sensing** - Why needed: Understanding which modalities contribute to predictions helps diagnose failures and improve system reliability. Quick check: Implement attention mechanisms or feature importance analysis to identify modality contributions.
- **Seasonal plant physiology** - Why needed: Plant characteristics change over seasons, affecting how different sensing modalities detect wetness. Quick check: Test model generalization across different growth stages and seasonal conditions.

## Architecture Onboarding

**Component Map:** mmWave SAR sensor -> Feature Extractor -> Fusion Layer -> Classification Output; RGB Camera -> Feature Extractor -> Fusion Layer -> Classification Output; Raw mmWave Sensor -> Feature Extractor -> Fusion Layer -> Classification Output

**Critical Path:** Sensor data acquisition → Pre-processing → Multi-modal feature extraction → Fusion layer → Classification → Accuracy evaluation

**Design Tradeoffs:** Multi-modal fusion provides robustness but increases computational complexity and data storage requirements compared to single-modality approaches. The three-modality system balances penetration capabilities (mmWave SAR), visual context (RGB), and raw signal processing opportunities (raw mmWave), though at the cost of increased hardware complexity and data volume.

**Failure Signatures:** Performance degradation occurs under extreme weather conditions where mmWave signals may scatter or RGB imaging is compromised by lighting changes. The model shows sensitivity to environmental variability, with accuracy dropping from 96% to approximately 90% when moving from indoor to outdoor settings.

**3 First Experiments:**
1. Conduct modality ablation studies to quantify individual modality contributions under varying weather conditions
2. Test temporal generalization by evaluating model performance on a held-out season
3. Perform species-specific performance analysis to identify systematic accuracy differences across the five plant types

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Accuracy drops from 96% in indoor settings to approximately 90% in outdoor farm environments, indicating sensitivity to environmental variability
- Six-month collection period may not capture full seasonal dynamics across all five plant species
- Dataset composition between indoor/outdoor samples and across species is not detailed, raising concerns about potential class imbalance effects on model generalization

## Confidence

**High:** Indoor accuracy claims (96% ± 2.14%) - supported by controlled conditions and statistical reporting

**Medium:** Outdoor accuracy claims (~90%) - reported but environmental factors not fully characterized

**Low:** Cross-species generalization - insufficient data on performance variation across the five species

## Next Checks

1. Conduct ablation studies quantifying individual modality contributions across different weather conditions to verify fusion benefits

2. Test model performance on a held-out season to assess temporal generalization beyond the original six-month collection period

3. Evaluate species-specific performance to identify whether certain plants show systematic accuracy differences that could bias agricultural deployment