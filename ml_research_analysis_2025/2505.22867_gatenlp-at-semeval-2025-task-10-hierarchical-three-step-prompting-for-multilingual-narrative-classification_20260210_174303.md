---
ver: rpa2
title: 'GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual
  Narrative Classification'
arxiv_id: '2505.22867'
source_url: https://arxiv.org/abs/2505.22867
tags:
- classification
- narratives
- narrative
- h3prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of multilingual narrative classification
  in news articles, a key task for detecting misinformation. The proposed Hierarchical
  Three-Step Prompting (H3Prompt) method fine-tunes LLaMA 3.2 using both annotated
  and synthetically generated data, classifying articles in three steps: domain, main
  narrative, and sub-narrative.'
---

# GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification

## Quick Facts
- arXiv ID: 2505.22867
- Source URL: https://arxiv.org/abs/2505.22867
- Authors: Iknoor Singh; Carolina Scarton; Kalina Bontcheva
- Reference count: 2
- Primary result: First place on English test set with F1 scores of 0.577 (macro coarse) and 0.482 (samples fine)

## Executive Summary
This work addresses multilingual narrative classification in news articles for misinformation detection. The proposed Hierarchical Three-Step Prompting (H3Prompt) method fine-tunes LLaMA 3.2 using both annotated and synthetically generated data, classifying articles through a three-step process: domain, main narrative, and sub-narrative. The approach achieved first place on the English test set in the SemEval 2025 Task 10 competition, with significant performance improvements from synthetic data augmentation and ensemble methods.

## Method Summary
H3Prompt combines three key techniques: hierarchical taxonomy decomposition into three sequential classification steps, synthetic data augmentation using Vicuna-7B to generate 8,129 additional training samples, and LoRA fine-tuning of LLaMA-3.2-3B-Instruct with structured prompts. Non-English articles are translated to English using m2m100_418M before training. The three-step classification process constrains the output space at each level, reducing the multi-label search complexity. Training uses 5 epochs with batch size 8, gradient accumulation 8, and learning rate 2e-4 on 3× NVIDIA A100 40GB GPUs.

## Key Results
- First place on English test set with F1 Macro Coarse 0.577 and F1 Samples Fine 0.482
- Synthetic data augmentation improved fine-grained F1 by 23% (0.392 to 0.482)
- Union ensemble aggregation achieved 0.623 F1, outperforming majority vote (0.567) and intersection (0.458)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Taxonomy Decomposition
Decomposing multi-label classification into three sequential steps (domain → main narrative → sub-narrative) improves fine-grained classification by constraining the output space at each level. Step 1 filters documents into one of two domains or "Other." Step 2 selects main narratives from a domain-specific subset. Step 3 assigns sub-narratives only from those linked to identified main narratives. This cascades decisions, reducing the multi-label search space exponentially. Core assumption: Narratives follow a coherent hierarchical structure where sub-narratives are conditionally dependent on parent narratives. Evidence: Hierarchical prompting models outperformed binary classification baselines.

### Mechanism 2: Synthetic Data Augmentation with Domain-Specific LLMs
Generating synthetic news articles using Vicuna-7B with varied temperature sampling (1.0-1.5) improves model generalization for low-data multilingual settings. For each sub-narrative, 100 synthetic articles are generated, expanding training from 2,091 to 10,220 articles. Core assumption: Synthetic articles capture sufficient lexical and semantic patterns of real misinformation narratives. Evidence: Training with synthetic data improved fine-grained F1 score by 23% for LLaMA-3.2 H3Prompt. External validation is limited.

### Mechanism 3: LoRA Fine-Tuning with Structured Prompts
Low-Rank Adaptation enables efficient fine-tuning of LLaMA-3.2-3B-Instruct while preserving instruction-following capability for structured multi-step prompting. LoRA freezes pre-trained weights and learns low-rank decomposition (α=64, r=64), enabling adaptation without catastrophic forgetting. Core assumption: Pre-trained instruction-following capabilities transfer to the specialized narrative taxonomy. Evidence: Model achieved first-place results using LoRA fine-tuning with three-step prompts. Application to hierarchical narrative prompting is novel.

## Foundational Learning

- **Multi-label Hierarchical Classification**: Each article can have multiple main narratives, each with multiple sub-narratives. Standard single-label classification fails. Quick check: Can you explain why treating sub-narrative classification as independent binary classifiers might perform worse than conditioning on parent narratives?

- **Prompt Engineering for Structured Output**: The model must return hash-separated labels (e.g., "Narrative1#Narrative2") consistently across three steps. Quick check: What output format constraints would you add to the prompt to reduce parsing failures?

- **Ensemble Aggregation Strategies (Union vs. Majority vs. Intersection)**: Union-based aggregation (0.623 F1) outperforms majority vote (0.567) and intersection (0.458) for this task. Quick check: Why would union-based aggregation outperform intersection for fine-grained multi-label classification?

## Architecture Onboarding

- **Component map**: Data Pipeline (m2m100_418M translation → annotated + synthetic articles → combined training) → Model (LLaMA-3.2-3B-Instruct + LoRA) → Inference (three sequential prompts) → Ensemble (3 bagged models with union aggregation)

- **Critical path**: Synthetic data generation → LoRA fine-tuning with hierarchical prompts → inference cascade → ensemble aggregation

- **Design tradeoffs**: Union ensemble maximizes recall but may increase false positives; translation-first approach simplifies training but may lose language-specific nuances; 3B model enables single-GPU fine-tuning but may underperform larger models on complex reasoning

- **Failure signatures**: "Other" returned at Step 1 or 2 indicates document falls outside taxonomy or prompt unclear; empty sub-narrative predictions suggest main narrative correctly identified but sub-narrative prompt too restrictive; hash parsing errors occur when model adds explanatory text instead of returning only labels

- **First 3 experiments**: 1) Ablate synthetic data: Train on annotated data only vs. full dataset to isolate augmentation contribution. 2) Compare ensemble strategies on held-out set to validate union aggregation consistently outperforms majority/intersection across languages. 3) Test prompt variations: Remove narrative explanations from prompts to measure their contribution to classification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
Does native multilingual fine-tuning yield better performance than the translate-then-classify pipeline used in H3Prompt? The paper only evaluates the translated approach, leaving potential loss of nuance or context due to translation artifacts unquantified compared to native language processing. A comparison of current H3Prompt performance against a version fine-tuned directly on original, non-translated multilingual text would resolve this.

### Open Question 2
How does the choice of synthetic data generator impact the robustness of the classifier? The authors selected vicuna-7b-v1.5 because it "easily generates content containing disinformation," but they do not compare this against other generators. An ablation study training separate models using synthetic data generated by different LLMs while keeping the prompting strategy and volume constant would resolve this.

### Open Question 3
To what extent does error propagation in the hierarchical structure limit the final classification accuracy? The paper demonstrates that the hierarchical approach outperforms binary baselines, but it does not quantify how many valid sub-narratives were lost solely due to misclassification in earlier domain or main narrative steps. A granular error analysis reporting the percentage of false negatives directly attributable to failures in Step 1 or Step 2 would resolve this.

## Limitations
- Synthetic data augmentation lacks ablation studies to isolate Vicuna-generated samples' contribution versus real training data
- Hierarchical taxonomy assumption is not empirically validated and could impose artificial constraints if narratives form overlapping clusters
- Translation quality remains critical unknown, with ranking drops for Bulgarian and Hindi suggesting potential information loss

## Confidence

**High confidence** in hierarchical prompting effectiveness for English narrative classification, supported by first-place competition results and internal ablation showing binary classifiers underperform structured approaches.

**Medium confidence** in synthetic data augmentation benefits, as the 23% F1 improvement is demonstrated but lacks external validation or comparison to alternative augmentation strategies.

**Low confidence** in multilingual generalization, particularly for non-English languages where translated test sets and language-specific fine-tuning approaches were not directly compared.

## Next Checks

1. **Synthetic Data Ablation**: Train the same model architecture using only the original 2,091 annotated samples (without synthetic augmentation) on the English test set to quantify the true contribution of Vicuna-generated articles versus real data.

2. **Translation Quality Analysis**: Compare model performance on directly annotated non-English articles versus m2m100_418M translated versions to measure information loss during translation and identify language pairs where translation degrades performance.

3. **Taxonomy Independence Test**: Randomly shuffle sub-narrative assignments across main narratives and retrain the model to measure performance degradation—this would empirically validate whether the hierarchical constraint adds value or imposes artificial limitations.