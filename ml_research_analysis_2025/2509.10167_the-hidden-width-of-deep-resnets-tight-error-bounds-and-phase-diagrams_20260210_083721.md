---
ver: rpa2
title: 'The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams'
arxiv_id: '2509.10167'
source_url: https://arxiv.org/abs/2509.10167
tags:
- have
- error
- dynamics
- limit
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of deep residual networks
  (ResNets) as the depth L goes to infinity, establishing that for arbitrary hidden
  width M, the training dynamics converges to a "Neural Mean ODE" limit model. The
  key insight is that due to random initialization, forward and backward passes through
  ResNets behave as stochastic approximations of certain mean ODEs, and by propagation
  of chaos, this behavior is preserved through training dynamics.
---

# The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams

## Quick Facts
- **arXiv ID:** 2509.10167
- **Source URL:** https://arxiv.org/abs/2509.10167
- **Reference count:** 8
- **Primary result:** Deep ResNets converge to a Neural Mean ODE limit with error $O(1/L + 1/\sqrt{LM})$ regardless of hidden width $M$

## Executive Summary
This paper establishes that deep residual networks converge to a continuous "Neural Mean ODE" limit as depth $L$ approaches infinity, with error bounds that depend on the product $LM$ (depth × width) rather than on $M$ alone. The key insight is that random initialization causes forward and backward passes to behave as stochastic approximations of mean ODEs, with the effective width being $LM$. The analysis reveals a phase diagram where the residual scale parameter $\alpha$ determines whether networks exhibit complete feature learning ($\alpha = \Theta(1)$) or enter a lazy ODE regime ($\alpha \to \infty$).

## Method Summary
The paper analyzes the training dynamics of deep residual networks using stochastic approximation theory and propagation of chaos. It establishes convergence to a Neural Mean ODE limit by showing that random initialization causes each layer's forward and backward passes to act as Monte Carlo integration steps. The error bounds are derived through careful analysis of the discretization error (scaling as $1/L$) and sampling error (scaling as $1/\sqrt{LM}$), with the total error being the sum of these two terms. The analysis extends to two-layer perceptron blocks with specific scaling requirements on the residual scale parameter.

## Key Results
- Error bound between ResNet and its Neural Mean ODE limit is $O(1/L + 1/\sqrt{LM})$ for arbitrary hidden width $M$
- When residual scale $\alpha = \Theta(1)$, the limit exhibits complete feature learning with genuinely non-linear parameterization
- For $\alpha \to \infty$, the model enters a lazy ODE regime with linear parameterization
- For 2-layer perceptron blocks, critical residual scale is $\Theta(\sqrt{D}/L M)$, yielding error bound $O(1/L + \sqrt{D}/\sqrt{LM})$
- Phase diagram reveals feature learning behaviors depend on residual scale and width/depth ratios

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Approximation of Mean ODEs
- **Claim:** Finite ResNets trained with gradient descent approximate a continuous "Neural Mean ODE" limit, provided depth $L$ is sufficiently large, regardless of width $M$.
- **Mechanism:** Due to random initialization, the forward and backward passes through the network layers behave as stochastic Euler steps. The sum of random perturbations across layers acts as a Monte Carlo integration of an expected drift term (the mean ODE).
- **Core assumption:** Standard random initialization and sufficiently large depth $L$.
- **Evidence anchors:**
  - [abstract] "due to the randomness of the initialization, the forward and backward pass through the ResNet behave as the stochastic approximation of certain mean ODEs"
  - [section 2.3] "An important intermediate object... is the following stochastic approximation of the Neural mean ODE"
  - [corpus] "Frac-Connections" (related to residual connections) offers context on connection importance, but specific stochastic mechanisms are unique to this paper.
- **Break condition:** The mechanism fails if $L$ is too small (discretization error dominates) or if initialization is deterministic/correlated (preventing stochastic integration).

### Mechanism 2: Effective Width ($LM$) and Sampling Error
- **Claim:** The sampling error in the approximation to the limit model scales as $1/\sqrt{LM}$, meaning the product of depth and width acts as an "effective width" or effective sample size.
- **Mechanism:** The network utilizes $M$ units per layer over $L$ layers. The total variation in the forward/backward pass aggregates over $L \times M$ random variables. By the Central Limit Theorem (or propagation of chaos), fluctuations shrink inversely to the square root of the total number of variables.
- **Core assumption:** Units are approximately independent (propagation of chaos) and depth $L$ scales with or exceeds width $M$.
- **Evidence anchors:**
  - [abstract] "error bound is $O_D(1/L + 1/\sqrt{LM})$"
  - [section 2.3] "sampling error... only depends on the product $LM$ which can be interpreted as an effective width"
  - [corpus] Weak explicit support for the specific $LM$ scaling in neighbors; this is a primary theoretical contribution of the paper.
- **Break condition:** The scaling fails if $L$ and $M$ are decoupled in a way that violates the $LM$ product dominance (e.g., fixed $LM$ but extremely small $M$ might violate independence assumptions required for proof, though the paper claims it holds for $M=1$).

### Mechanism 3: Residual Scale and Regime Selection
- **Claim:** The scale of the residual branch ($\Theta(\frac{\alpha}{LM})$) determines whether the network enters a "feature learning" regime (finite $\alpha$) or a "lazy" regime (linearization, $\alpha \to \infty$).
- **Mechanism:** The parameter $\alpha$ controls the magnitude of updates relative to initialization. Large $\alpha$ suppresses feature evolution (linearization), while $\alpha = \Theta(1)$ allows the non-linearity to evolve significantly (feature learning). For 2-layer perceptron (2LP) blocks, the critical scale specifically depends on $\sqrt{D}$.
- **Core assumption:** The specific functional form of the block $\phi$ and the dimension $D$ (for 2LPs).
- **Evidence anchors:**
  - [abstract] "When $\alpha=\Theta(1)$, the limit exhibits complete feature learning... $\alpha \to \infty$ yields a lazy ODE regime"
  - [section 4.2] "tight scaling argument... yields the condition $\sigma_v = \Theta(\sqrt{D})$"
  - [corpus] "Frac-Connections" discusses fractional extension of connections but does not address the scaling laws of $\alpha$.
- **Break condition:** If the scale $\alpha$ is misconfigured relative to $D$ (e.g., scaling linearly with $D$ in a 2LP block without the $\sqrt{D}$ correction), the network may converge to a degenerate limit or fail to learn features.

## Foundational Learning

- **Concept:** Mean-Field Limits (in Neural Networks)
  - **Why needed here:** The paper derives a limit where individual neurons behave independently according to a distribution that evolves continuously in "depth-time." Understanding that the network represents a distribution of parameters rather than a fixed tensor is crucial for interpreting the limit model.
  - **Quick check question:** Can you explain why increasing the number of particles (neurons) in a mean-field system reduces the variance of the aggregate system state?

- **Concept:** Stochastic Euler Discretization
  - **Why needed here:** The paper models the forward pass as a discretization of an ODE where the update term includes a random component (from initialization).
  - **Quick check question:** How does the step size ($1/L$) relate to the discretization error in a standard Euler method, and how does adding noise change the error term?

- **Concept:** Propagation of Chaos
  - **Why needed here:** This is the mathematical phenomenon ensuring that while units interact during training, they remain asymptotically independent as $L, M \to \infty$, justifying the mean-field limit.
  - **Quick check question:** In a system of interacting particles, what condition typically allows the joint distribution of particles to factorize into a product of marginals as the system size grows?

## Architecture Onboarding

- **Component map:** $L$ (Depth) -> $M$ (Hidden Width) -> $D$ (Embedding Dim) -> $\alpha$ (Residual Scale) -> Limit Model (Mean ODE)
- **Critical path:**
  1. Configure depth $L$ and width $M$ to ensure the error term $1/L + 1/\sqrt{LM}$ is small (e.g., $LM \gg 1$).
  2. Select residual scale $\alpha$. For 2LP blocks, set $\sigma_v \approx \sqrt{D}$ (which implies $\alpha \approx 1$) to ensure "complete" feature learning.
  3. Verify that the "lazy" regime is avoided if non-linear feature evolution is desired.

- **Design tradeoffs:**
  - **Depth vs. Width:** The error bound depends on $LM$. Increasing Depth ($L$) helps both discretization error ($1/L$) and sampling error ($1/\sqrt{LM}$), whereas increasing Width ($M$) only helps sampling error. Deep networks are theoretically "richer" in this limit.
  - **Feature Learning vs. Stability:** The "Complete" regime ($\alpha=\Theta(1)$) offers maximal feature diversity but requires careful scaling of learning rates (e.g., $\eta \propto D$). The "Lazy" regime is more stable but may learn slower or get stuck in kernel regimes.

- **Failure signatures:**
  - **"Semi-Complete" Stagnation:** If $\sigma_v \ll \sqrt{D}$ (too small $\alpha$), the limit dynamics become indistinguishable from a deterministic (low-entropy) initialization, limiting capacity.
  - **Exploding Gradients/Outputs:** If $\sigma_v \gg \sqrt{D}$ (too large $\alpha$), the forward pass may explode or linearize prematurely (Lazy regime), and the bound $1/L + \alpha/\sqrt{LM}$ becomes loose.

- **First 3 experiments:**
  1. **Effective Width Validation:** Train ResNets with fixed $LM$ product but varying $(L, M)$ pairs (e.g., $L=100, M=10$ vs $L=1000, M=1$). Plot convergence of the output distribution to verify that $LM$ governs the rate, as claimed.
  2. **Regime Phase Diagram:** Vary the residual scale $\alpha$ (via initialization $\sigma_v$) for a 2LP ResNet. Plot the "feature movement" ($\|U_k - U_0\|$) vs $\alpha$ to confirm the transition at $\alpha \approx 1$ (or $\sigma_v \approx \sqrt{D}$).
  3. **Tightness Check:** Numerically estimate the error to the limit model (using a very deep, wide network as a proxy) and verify it scales linearly with $1/L$ and $1/\sqrt{LM}$ separately to confirm the tightness of the bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence results for generic ResNet blocks (Theorem 1) be extended to activation functions that are only pseudo-Lipschitz, rather than globally Lipschitz?
- Basis in paper: [explicit] The paper states in Section 2.3: "The assumed regularity on ϕ [Assumption A] is quite restrictive... In Section 4, we study the case of 2LP blocks where ϕ and Dϕ are only pseudo-Lipschitz (i.e. locally Lipschitz with a controlled growth)."
- Why unresolved: The proofs for generic blocks rely on global Lipschitzness to control error propagation, whereas two-layer perceptron blocks require a separate, more complex analysis (Theorem 4) to handle local growth.
- What evidence would resolve it: A proof of convergence for the generic case that relaxes Assumption A to cover pseudo-Lipschitz functions, or a counter-example showing that global Lipschitzness is necessary for the general bound.

### Open Question 2
- Question: Can the error bound for two-layer perceptron blocks be proven under the condition $D = O(LM)$ rather than the stricter assumption $M = \Omega(D)$?
- Basis in paper: [explicit] In Section 4.3 (Theorem 4 discussion), the author notes: "We believe however that this hypothesis [$M = \Omega(D)$] is not necessary to obtain a bound of this form and that $D = O(LM)$ is sufficient."
- Why unresolved: The current proof of Theorem 4 relies on the assumption $M = \Omega(D)$ to simplify the mathematical derivation, likely related to the concentration bounds used in the stochastic approximation steps.
- What evidence would resolve it: A derivation of Theorem 4's error bound that removes the assumption $\max\{D, \log(L)\} \le B M$ and holds for $M \ll D$ as long as $D = O(LM)$.

### Open Question 3
- Question: Does scaling the attention head dimension $d_k$ provide benefits in the Neural Mean ODE limit, or is a constant $d_k$ sufficient?
- Basis in paper: [explicit] In Section 2.1, regarding attention blocks, the paper states in Footnote 5: "It is in fact not clear whether scaling-up $d_k$ is beneficial."
- Why unresolved: The paper's analysis treats $d_k$ as a constant (specifically noting it is constant in Llama 3.1), leaving the dynamics of scaling this specific hyperparameter unexplored.
- What evidence would resolve it: An analysis of the Neural Mean ODE limit where $d_k$ scales with depth $L$ or width $M$, identifying any resulting changes to the phase diagrams or error bounds.

### Open Question 4
- Question: Under what precise conditions do subsequent gradient steps in the "semi-complete" regime (where $\sigma_v = o(\sqrt{D})$) achieve "complete" feature learning?
- Basis in paper: [inferred] The paper notes in Section 4.2 that Theorem 3-(iii) implies the first GD step is complete only if $\sigma_v = \Theta(\sqrt{D})$, yet the text states: "we expect... that the subsequent GD steps are also complete with $\sigma_v = o(\sqrt{D})$."
- Why unresolved: There is a gap between the theorem's characterization of the first step and the empirical/theoretical expectation for later steps in the semi-complete regime; the theory does not yet quantify the evolution of feature learning across multiple steps for small $\sigma_v$.
- What evidence would resolve it: Theoretical bounds characterizing the magnitude of feature learning (e.g., $\|U_k - U_0\|$) at arbitrary steps $k > 1$ in the semi-complete regime.

## Limitations
- The analysis relies on mean-field approximations and propagation of chaos assumptions that may not hold in practical finite-width settings
- Error bounds are asymptotic and may not capture transient behaviors or edge cases in training dynamics
- The analysis assumes continuous gradient flow rather than discrete SGD updates

## Confidence
- **High Confidence:** The core mechanism of stochastic approximation of mean ODEs and the $1/L + 1/\sqrt{LM}$ error scaling are well-supported by theoretical proofs and experimental validation
- **Medium Confidence:** The propagation of chaos assumption for practical network widths and the specific scaling requirements for 2LP blocks ($\sigma_v = \Theta(\sqrt{D})$) are theoretically sound but may require empirical verification
- **Low Confidence:** The assumption that the limit model accurately captures all relevant aspects of practical training dynamics, including phenomena like catastrophic forgetting or curriculum learning effects

## Next Checks
1. **Finite-Width Validation:** Systematically vary width $M$ (including very small values like $M=1$) and depth $L$ to empirically verify the $1/L + 1/\sqrt{LM}$ error scaling holds beyond asymptotic limits, particularly checking when $L$ is not much larger than $M$

2. **Initialization Sensitivity:** Test the theory with alternative initialization schemes (orthogonal, layer-wise scaled, etc.) to determine how robust the stochastic approximation mechanism is to initialization choices beyond standard random initialization

3. **Beyond Gradient Flow:** Compare the theoretical predictions based on gradient flow with actual SGD training, measuring discrepancies in convergence rates and final performance to quantify the impact of discretization and noise in practical optimization