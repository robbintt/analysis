---
ver: rpa2
title: 'EXECUTE: A Multilingual Benchmark for LLM Token Understanding'
arxiv_id: '2505.17784'
source_url: https://arxiv.org/abs/2505.17784
tags:
- word
- char
- languages
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the CUTE benchmark to multiple languages with
  diverse scripts and writing systems, introducing EXECUTE. The benchmark tests large
  language models on character, word, and sub-character manipulation tasks across
  eight languages.
---

# EXECUTE: A Multilingual Benchmark for LLM Token Understanding

## Quick Facts
- arXiv ID: 2505.17784
- Source URL: https://arxiv.org/abs/2505.17784
- Authors: Lukas Edman; Helmut Schmid; Alexander Fraser
- Reference count: 14
- Key outcome: Non-English languages often perform differently than English on token manipulation tasks, with performance correlating with character-word-token statistics and surprisingly better results for lower-resourced languages due to reduced language bias.

## Executive Summary
This work extends the CUTE benchmark to multiple languages with diverse scripts and writing systems, introducing EXECUTE. The benchmark tests large language models on character, word, and sub-character manipulation tasks across eight languages. Results show that non-English languages often perform differently than English, with performance correlating with character-word-token statistics. Surprisingly, lower-resourced languages achieve better results, likely due to reduced language bias. Models struggle significantly with sub-character tasks in Chinese, Japanese, and Korean, demonstrating limited understanding of character components. The findings suggest that while models can perform arbitrary manipulations, language knowledge and tokenization significantly impact performance, indicating a need for debiasing approaches in future research.

## Method Summary
The benchmark tests LLM token understanding through spelling manipulation tasks (spelling, inverse spelling, contains, insert, delete, substitute, swap) at character and word levels across eight languages. It uses 5000 stories from TinyStories translated via Google Translate, applies language-specific word segmentation (jieba/nagisa for CJK), and evaluates few-shot prompting with 4 examples per task. The study tests Aya Expanse, Gemma 2, Llama 3.1/3.3, Qwen 2.5, and Mistral models (7B-70B parameters) across languages including English, Arabic, Chinese, Japanese, Korean, Russian, Tamazight, Amharic, and Santali.

## Key Results
- Character insertion and swap tasks show near-zero performance for high c/t ratio languages (English, Arabic, Russian)
- Lower-resourced languages (Amharic, Tamazight, Santali) achieve near-perfect scores on character manipulation tasks
- Sub-character tasks (character→radical, Hangul→jamo) show significantly lower performance than character-level tasks for CJK languages
- Performance correlates with character-word-token statistics, with languages clustering into five groups by CWT ratios

## Why This Works (Mechanism)

### Mechanism 1
LLM token manipulation performance correlates with language-specific character-word-token (CWT) statistics. Languages cluster by CWT ratios (characters/word, tokens/word, characters/token). Where c/t ratios are high (English ~3.05), tokenizers segment more, making character-level operations harder. Where c/t is low (Chinese ~1.20), single tokens carry more complete morphemes, improving character-level task performance.

### Mechanism 2
Lower language knowledge paradoxically improves EXECUTE performance by reducing output bias. LLMs are biased toward generating grammatical, real-word outputs. When they lack strong language knowledge (Amharic, Tamazight, Santali), this bias weakens, allowing arbitrary character manipulation without the model "correcting" to known valid forms. Tokenizers fall back to byte-level for unseen scripts.

### Mechanism 3
LLMs lack robust understanding of sub-character components (radicals, jamo) in CJK languages. Subword tokenizers treat whole characters as atomic units. The model never learns to decompose Han characters into Kangxi radicals or Hangul into jamo during pretraining, so compositional understanding remains superficial.

## Foundational Learning

- **Subword tokenization (BPE/WordPiece/SentencePiece)**: Why needed: The entire benchmark tests what LLMs understand about their tokens. Quick check: Given "unhappiness," would a BPE tokenizer likely produce ["un", "happiness"] or ["un", "hap", "piness"]? What factors determine this?

- **Writing system typology (alphabet, abjad, abugida, logographic, featural)**: Why needed: The paper explicitly tests across five writing system types. Quick check: In an abugida, how are vowels typically represented compared to an alphabet?

- **Language model bias toward fluent output**: Why needed: The paradoxical low-resource advantage depends on understanding that LLMs default to generating probable sequences. Quick check: If asked to reverse "apple" character by character, why might an LLM output "apple" or "elppa" followed by an explanation rather than just "elppa"?

## Architecture Onboarding

- **Component map**: TinyStories subset → Google Translate → language-specific segmentation → vocabulary generation → task generation → few-shot prompts → LLM evaluation

- **Critical path**: Select source sentences from TinyStories subset → Translate to target language → Apply language-specific word segmentation → Generate tasks using vocabulary/character set → Construct few-shot prompts → Evaluate LLM outputs

- **Design tradeoffs**: Simplified framework vs. depth (removed similarity tasks for easy expansion), English prompts with target-language examples (keeps instructions consistent), grapheme-level diacritic handling (simplifies but may miss fine-grained errors)

- **Failure signatures**: Character insertion/swap near 0% (high c/t ratio languages), word deletion > char deletion (token-level operation), near-perfect performance on low-resource language (may indicate degenerate outputs)

- **First 3 experiments**:
  1. Establish baseline on your model: Run EXECUTE on your target LLM across all 8 languages. Identify which CWT cluster your model's failures align with.
  2. Probe the bias mechanism: Test ciphered English vs. byte-level English. If ciphered outperforms byte-level on word tasks, bias is the bottleneck.
  3. Sub-character sanity check: Run Contains-Rad vs. Char-to-Rad tasks for Chinese. If Contains-Rad >> Char-to-Rad, your model recognizes radicals but cannot compose them.

## Open Questions the Paper Calls Out

### Open Question 1
Can specific debiasing techniques enable LLMs to "forget" high-resource language patterns and improve performance on character manipulation tasks? The authors identify the bias but do not propose or test methods to mitigate it.

### Open Question 2
Does model scaling beyond 70B parameters resolve token understanding deficits in non-English languages? The study was limited to 7B-70B models, leaving frontier-scale performance unknown.

### Open Question 3
Will near-perfect performance on low-resource languages degrade as these languages become better represented in training data? The authors predict performance decline as language bias increases with more training data.

## Limitations
- Reliance on Google Translate introduces potential quality variations that could systematically bias results
- Simplified prompt approach using English instructions may not capture language-specific task understanding
- Benchmark focuses on spelling manipulation tasks, providing a narrow view of token understanding
- Does not address whether performance differences reflect genuine understanding versus pattern matching

## Confidence

**High Confidence**: The correlation between CWT statistics and benchmark performance; superior performance of lower-resourced languages on character manipulation tasks.

**Medium Confidence**: The proposed mechanism that reduced language bias explains low-resource advantage; LLMs lack robust sub-character understanding in CJK languages.

**Low Confidence**: The assertion that language knowledge "paradoxically" improves performance through bias; generalizability of CWT ratio effects to other token manipulation tasks.

## Next Checks

1. Probe the tokenization vs. bias mechanism: Test ciphered English vs. byte-level English on word manipulation tasks. If ciphered English outperforms byte-level on word tasks while both struggle on character tasks, this would confirm that bias—not tokenization—is the primary bottleneck for word-level manipulation in low-resource languages.

2. Validate the CWT correlation across broader model families: Evaluate the benchmark on diverse tokenizer architectures and model families with varying training data compositions. If CWT ratios predict performance across these diverse conditions, it strengthens the claim that tokenizer behavior fundamentally shapes manipulation capabilities.

3. Test sub-character understanding with explicit decomposition: Implement explicit radical/jamo tokenization for Chinese, Japanese, and Korean inputs, then re-run the benchmark. If performance on sub-character tasks improves significantly with explicit decomposition, this would confirm that the observed failures reflect tokenization choices rather than fundamental inability to understand character components.