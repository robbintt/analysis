---
ver: rpa2
title: 'Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge'
arxiv_id: '2512.06228'
source_url: https://arxiv.org/abs/2512.06228
tags:
- simplification
- sentence
- high
- penalty
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method for policy-driven sentence simplification\
  \ without parallel corpora or human annotation. It uses LLM-as-a-Judge to automatically\
  \ generate preference data for two simplification policies\u2014lexical paraphrasing\
  \ and overall rewriting."
---

# Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge

## Quick Facts
- **arXiv ID:** 2512.06228
- **Source URL:** https://arxiv.org/abs/2512.06228
- **Authors:** Xuanxin Wu; Yuki Arase; Masaaki Nagata
- **Reference count:** 40
- **One-line primary result:** Fine-tuned Phi-3-mini-3.8B outperforms GPT-4o on lexical paraphrasing (SARI: +8.0) and matches it on overall rewriting (LENS: +2.7) using LLM-generated preference data without parallel corpora.

## Executive Summary
This paper introduces a method for policy-driven sentence simplification that eliminates the need for parallel corpora or human annotation. The framework uses a reasoning-capable LLM to automatically generate pairwise preference data aligned with specific simplification policies—lexical paraphrasing and overall rewriting. Multiple small open-source LLMs generate candidate simplifications, which are evaluated by the judging LLM using explicit guidelines and feature extraction (word alignments and parse trees). The resulting preference dataset is used to fine-tune models via Adaptive Rejection Preference Optimization (ARPO), achieving state-of-the-art results with much smaller models than GPT-4o while demonstrating strong generalization to out-of-domain and document-level tasks.

## Method Summary
The method employs a three-step framework: First, generate a pool of candidate simplifications using four diverse LLMs (Qwen2.5-7B, Llama3.1-8B, Phi4-14B, Qwen3-32B) for each source sentence from the CoEdit corpus. Second, use a reasoning-capable LLM (Qwen3-32B in "think" mode) as a judge, equipped with lexical word alignments (via OTAlign) and syntactic parse trees (via Qwen3-32B), to apply explicit guidelines and select preferred/dispreferred simplification pairs. Third, train target models (Phi-3-mini-3.8B, etc.) using ARPO preference optimization on the generated dataset, with LoRA adapters and hyperparameters tuned for the task.

## Key Results
- Phi-3-mini-3.8B trained on LLM-generated preference data outperforms GPT-4o on lexical paraphrasing (SARI: +8.0 improvement)
- The same model matches GPT-4o performance on overall rewriting (LENS: +2.7 improvement)
- Performance improvements are consistent across model families and scales
- The framework generalizes to out-of-domain datasets and document-level simplification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated preference generation from a reasoning-capable LLM can replace costly human annotation for creating policy-aligned simplification training data.
- Mechanism: A reasoning LLM evaluates candidate simplifications from multiple models against explicit lexical and structural principles, selecting preferred and dispreferred candidates to form pairwise preference data without human involvement.
- Core assumption: The judging LLM can reliably interpret and apply simplification guidelines, and its preferences are a sufficient proxy for human judgment for the target policy.
- Evidence anchors:
  - [abstract]: "leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora."
  - [section 2.2, Step 2]: "For each source sentence x_i ∈ X, the judge LLM selects a preferred candidate y(i)_w and a dispreferred candidate y(i)_l from the candidate pool C(x_i) according to our guidelines G."
- Break condition: If the judging LLM's preferences show low correlation with human evaluation for a given policy, the mechanism may introduce systematic bias or noise into the preference dataset.

### Mechanism 2
- Claim: Training on automatically generated pairwise preference data via lightweight optimization algorithms effectively aligns small LLMs with specific simplification policies.
- Mechanism: The preference dataset (source, preferred, dispreferred) is used to fine-tune a model using Adaptive Rejection Preference Optimization (ARPO), encouraging the model to increase the likelihood of preferred outputs relative to dispreferred ones.
- Core assumption: The relative quality signal in the preference pairs is strong and consistent enough for the optimization process to converge on a policy-aligned behavior.
- Evidence anchors:
  - [abstract]: "The framework employs multiple small-scale open-source LLMs to generate candidates, uses a reasoning-capable LLM to judge and rank them, and then fine-tunes models via preference optimization."
  - [section 4.4, Results]: "The proposed method outperforms GPT-4o with much smaller scale models... SARI improves by +8.0 (Phi3-3.8B)... on lexical-paraphrasing."
- Break condition: If the preference dataset contains contradictory or low-quality pairs, optimization may fail to learn a coherent policy.

### Mechanism 3
- Claim: Employing a reasoning mode (explicit chain-of-thought) in the judging LLM leads to higher-quality preference data and better downstream model performance than a non-reasoning mode.
- Mechanism: The judging LLM is prompted to generate an analysis of edits before making a preference decision, helping it adhere to complex guidelines and reward nuanced, policy-compliant edits.
- Core assumption: Evaluating simplification edits against multi-dimensional guidelines requires explicit reasoning to achieve consistency and accuracy.
- Evidence anchors:
  - [section 4.4, Analysis]: "Training on reasoning-based preference data (PO Think) consistently outperforms those from the non-reasoning mode (PO No-think). This suggests that complex evaluations benefit from reasoning-enabled judges."
  - [appendix A.2, Case Studies]: Examples show the reasoning judge correctly rewarding high-quality paraphrases while the non-reasoning judge failed to do so.
- Break condition: If the reasoning process introduces hallucinations or incorrect linguistic analyses, it could lead to flawed preference signals.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) and its variants (CPO, ARPO)**
  - Why needed here: This is the core training paradigm replacing traditional supervised fine-tuning on parallel corpora. Understanding how these algorithms convert pairwise preferences into model updates is essential.
  - Quick check question: How does ARPO differ from standard DPO in handling marginally dispreferred responses?

- **Concept: LLM-as-a-Judge and Reasoning**
  - Why needed here: The entire data generation pipeline depends on the reliability of using an LLM as an evaluator. The paper specifically highlights the importance of a reasoning (chain-of-thought) mode.
  - Quick check question: What are the potential failure modes when using an LLM as a judge without an explicit reasoning process?

- **Concept: Simplification Edit Policies (Lexical vs. Structural)**
  - Why needed here: The framework is explicitly designed to adapt to distinct policies. Understanding the difference between lexical-paraphrasing (word-level) and overall-rewriting (sentence-level) is key to designing guidelines and evaluating outputs.
  - Quick check question: Which evaluation metric (SARI vs. LENS) is more appropriate for a lexical-paraphrasing policy and why?

## Architecture Onboarding

- **Component map:** Raw Source Sentences -> Candidate Generation -> (Alignment & Parsing) -> Reasoning Judge -> Preference Dataset -> ARPO Training -> Fine-tuned Model

- **Critical path:** `Raw Source Sentences -> Candidate Generation -> (Alignment & Parsing) -> Reasoning Judge -> Preference Dataset -> ARPO Training -> Fine-tuned Model`

- **Design tradeoffs:**
  1. **Judge Model Size vs. Cost:** A larger, reasoning-capable judge (e.g., 32B) yields better preference data but is slower and more expensive than a smaller, non-reasoning judge. The paper shows this tradeoff favors the reasoning judge for quality.
  2. **Candidate Pool Diversity:** Using models from different families/sizes increases the chance of having high-quality and diverse candidates for the judge to rank, but increases the upfront generation cost.
  3. **Data Quantity vs. Quality:** More preference pairs improve performance, but the quality of pairs from the judge is the limiting factor. Filtering heuristics are crucial.

- **Failure signatures:**
  1. **Policy Drift:** The fine-tuned model starts generating outputs that violate the policy (e.g., heavy deletions in lexical-paraphrasing mode). This indicates noisy preference data.
  2. **Low Metric Scores:** The model performs worse than vanilla or GPT-4o baselines. Could indicate a bug in the preference optimization code or poorly designed judging guidelines.
  3. **Overfitting to Judge Biases:** The model learns to exploit specific phrases or patterns rewarded by the judge LLM, not genuine simplification. This would show high automatic scores but low human evaluation scores.

- **First 3 experiments:**
  1. **Ablate the Reasoning Judge:** Train models using preference data from the same judge in "think" vs. "no-think" modes. Compare performance on SARI/LENS and a small human eval to validate the paper's finding on reasoning's importance.
  2. **Vary Candidate Pool Composition:** Test systems where the candidate pool is generated by (a) a single model, (b) same-family models of different sizes, and (c) diverse-family models as in the paper. Measure impact on downstream performance.
  3. **Stress Test on Out-of-Domain Data:** Take the fine-tuned models and evaluate them on a distinctly different domain (e.g., biomedical text from JEBS corpus) without any retraining to rigorously test the claimed generalization ability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework generalize to other controllable text generation tasks beyond simplification?
- Basis in paper: [explicit] The conclusion explicitly suggests future work should "explore its applicability beyond simplification, such as style transfer, lay-summarization, and other controllable text generation tasks."
- Why unresolved: The current study validates the framework exclusively on simplification policies; its efficacy on tasks requiring different constraints remains unknown.
- What evidence would resolve it: Successful application of the LLM-as-a-Judge preference optimization pipeline to style transfer or summarization benchmarks, demonstrating similar improvements over baselines.

### Open Question 2
- Question: Can integrating external linguistic resources improve the reasoning judge's sensitivity to context-specific word difficulty?
- Basis in paper: [explicit] Appendix A.2.3 notes that reasoning judges sometimes misjudge word difficulty and suggests "Future work could explore integrating external linguistic resources, such as CEFR-based wordlists... to enhance their sensitivity."
- Why unresolved: The current implementation relies solely on the LLM's internal parametric knowledge, which is prone to misclassifying word difficulty without standardized reference.
- What evidence would resolve it: A modified judge incorporating CEFR lexical databases showing higher alignment with human lexical complexity ratings compared to the baseline reasoning judge.

### Open Question 3
- Question: Does the policy-driven simplification framework generalize effectively to non-English languages?
- Basis in paper: [explicit] The conclusion states, "Future studies could extend our framework to policy-driven simplification in other languages," noting the current focus on English.
- Why unresolved: The experiments rely on English-specific datasets and metrics; the transferability of policy definitions and the reasoning judge's capability in other languages is unverified.
- What evidence would resolve it: Training and evaluation of models on non-English simplification corpora showing that the LLM-as-a-Judge can successfully enforce policies without human parallel data in the target language.

## Limitations

- The framework's performance critically depends on the reasoning LLM's ability to consistently apply complex guidelines across diverse inputs, but systematic analysis of judge reliability is not provided.
- The claim of out-of-domain generalization is based on limited testing (JEBs corpus), and behavior on highly technical or culturally specific domains remains unknown.
- The preference optimization process may be sensitive to hyperparameter choices that were tuned for the CoEdit corpus, potentially limiting robustness.

## Confidence

- **High Confidence:** The core methodology (LLM-as-Judge → preference data → preference optimization) is well-specified and the reported results show clear improvements over baselines for both policies.
- **Medium Confidence:** The claim about reasoning-mode judges producing higher-quality data is supported by case studies and ablation, but would benefit from larger-scale human evaluation of judge consistency.
- **Low Confidence:** The out-of-domain generalization claims need more rigorous validation across multiple domains and datasets.

## Next Checks

1. Conduct inter-annotator agreement study: Have human annotators evaluate a subset of the LLM-as-Judge's preference decisions to measure reliability and identify systematic biases.
2. Perform cross-domain stress test: Evaluate the fine-tuned models on at least three additional domains (e.g., news, legal, technical) to rigorously test generalization claims.
3. Implement judge quality monitoring: Track metrics like preference distribution balance, edit type consistency, and hallucination rates during the preference generation process.