---
ver: rpa2
title: 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL'
arxiv_id: '2512.04069'
source_url: https://arxiv.org/abs/2512.04069
tags:
- tool
- image
- depth
- reasoning
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of precise spatial reasoning
  in vision-language models for embodied applications. They introduce Double Interactive
  Reinforcement Learning (DIRL), a two-phase training framework that combines supervised
  fine-tuning on tool demonstrations with interactive reinforcement learning to teach
  models to coordinate multiple heterogeneous tools (depth estimation, segmentation,
  pose estimation, grasping).
---

# SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL

## Quick Facts
- arXiv ID: 2512.04069
- Source URL: https://arxiv.org/abs/2512.04069
- Reference count: 40
- Primary result: State-of-the-art performance on spatial reasoning benchmarks with 79.38% accuracy on RoboSpatial-Home

## Executive Summary
SpaceTools introduces a two-phase training framework (DIRL) that combines supervised fine-tuning on tool demonstrations with interactive reinforcement learning to teach vision-language models to coordinate multiple heterogeneous tools for precise spatial reasoning. The method achieves state-of-the-art performance on spatial reasoning benchmarks including RoboSpatial-Home, BLINK, and BOP-ASK, and demonstrates real-world robot manipulation with 86% success rate in pick-and-place tasks.

## Method Summary
SpaceTools employs a two-phase Double Interactive Reinforcement Learning (DIRL) framework. Phase 1 trains a pointing-only specialist via interactive RL on spatial tasks, generating 2k grounded traces. Phase 2 performs SFT on a combined dataset (2k IRL traces + 6k frontier model traces) followed by interactive RL with all tools using GRPO optimization. The system uses Toolshed infrastructure to host vision tools (SAM2, DepthPro, RoboRefer, GraspGen) and integrates with VERL for multi-turn rollouts.

## Key Results
- Achieves 79.38% accuracy on RoboSpatial-Home benchmark
- Demonstrates 86% success rate in real-world robot pick-and-place tasks
- Outperforms both proprietary models and open-source baselines across multiple spatial reasoning benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Progressive Search Space Reduction
Multi-tool RL fails due to combinatorial action space; bootstrapping from constrained single-tool learning makes optimization tractable. Phase 1 IRL trains a pointing-only specialist (small action space → reliable convergence). Its traces seed the SFT dataset. Phase 2 IRL then explores the full toolset from a strong initialization, avoiding exploration collapse.

### Mechanism 2: Interactive Tool Feedback During Training
Training with real tool outputs—not precomputed traces—teaches error recovery and state-dependent tool selection. Toolshed serves tools asynchronously during RL rollouts. The model experiences stochastic tool failures and learns fallback strategies. Reward signals backpropagate through full interaction histories.

### Mechanism 3: Heterogeneous Teacher Composition
Combining IRL-trained specialist traces with frontier-model multi-tool demonstrations covers both grounding precision and compositional diversity. IRL teacher provides grounded single-tool reasoning (2k traces); frontier model (Claude) provides multi-tool compositions (6k traces). SFT blends both, trading off grounding vs. diversity.

## Foundational Learning

- **Policy Gradient with Group Relative Advantages (GRPO)**
  - Why needed: Multi-turn RL with tool calls requires stable optimization; GRPO normalizes rewards across rollouts to reduce variance.
  - Quick check: Given 5 rollouts with rewards [0.2, 0.6, 0.4, 0.8, 0.4], compute the standardized advantage for the rollout with reward 0.6.

- **Combinatorial Action Spaces in Tool-Use**
  - Why needed: With N tools and T possible calls per trajectory, the action space explodes; standard RL exploration fails.
  - Quick check: If a model has 10 tools and can make up to 5 sequential calls, what is the approximate lower bound on distinct action sequences (ignoring argument variations)?

- **Reward Shaping for Spatial Tasks**
  - Why needed: Spatial outputs (points, boxes, poses) require differentiable or shaped rewards; binary rewards alone produce weak gradients.
  - Quick check: For a pointing task, why might NNDC (Normalized Negative Distance to Centroid) outperform a binary inside/outside reward?

## Architecture Onboarding

- **Component map**: VLM (Qwen2.5-VL-3B-Instruct) -> Toolshed (Ray-based distributed tool server) -> VERL (asynchronous rollout engine) -> Reward functions (task-specific)
- **Critical path**: 1) Launch Toolshed with GPU-backed tool actors; 2) Run Phase 1 IRL on pointing-only tasks; 3) Collect frontier-model traces; 4) Merge traces and run SFT; 5) Run Phase 2 IRL with all tools
- **Design tradeoffs**: Small KL coefficient (β=1e-4) enables exploration but causes initial reward drop; teacher data ratio (1:3 IRL:frontier) prioritizes compositional diversity; mock robot tools avoid latency but miss physical failure modes
- **Failure signatures**: Exploration collapse (Phase 2 rewards plateau early), tool overuse (model calls tools for simple tasks), pointing drift (accuracy degrades after Phase 2)
- **First 3 experiments**: 1) Single-tool IRL baseline on RoboSpatial; 2) Ablate teacher composition (only IRL vs only frontier traces); 3) Toolshed latency profiling under batch size 64

## Open Questions the Paper Calls Out

- **Tool Error Recovery**: How can VLMs systematically perceive, verify, and recover from tool errors or inaccuracies during spatial reasoning? [explicit]
- **Longer-Horizon Tasks**: How does the DIRL framework scale to longer-horizon, multi-stage tasks beyond short- or medium-horizon spatial reasoning? [explicit]
- **Framework Simplification**: Can the two-phase DIRL training paradigm be simplified or replaced while maintaining performance, given that direct IRL on all tools fails? [inferred]
- **Real Robot Integration**: How can Toolshed infrastructure support real robot-in-the-loop execution during interactive learning, rather than mock robot tools? [explicit]

## Limitations
- Tool integration overhead and computational cost of real-time tool orchestration are not quantified
- Generalization to novel environments and unseen object categories remains unverified
- Quality and error propagation of frontier model-generated traces are not assessed

## Confidence

- **High Confidence**: Progressive search space reduction mechanism (supported by ablation studies and prior work)
- **Medium Confidence**: Interactive tool feedback mechanism (theoretically sound but lacks isolated ablation studies)
- **Medium Confidence**: Heterogeneous teacher composition (performance justified by ablations but depends on unverified trace quality)

## Next Checks

1. Ablate real tool failures: Train SpaceTools with precomputed tool traces and compare performance on high-failure-rate tasks
2. Scale Toolshed: Profile latency and resource usage under increasing tool actor counts for larger models
3. Out-of-distribution testing: Evaluate SpaceTools on held-out dataset with novel object categories to assess generalization