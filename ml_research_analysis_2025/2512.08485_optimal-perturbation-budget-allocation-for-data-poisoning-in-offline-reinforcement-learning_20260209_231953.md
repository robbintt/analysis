---
ver: rpa2
title: Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement
  Learning
arxiv_id: '2512.08485'
source_url: https://arxiv.org/abs/2512.08485
tags:
- budget
- learning
- attack
- offline
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of offline reinforcement
  learning to data poisoning attacks, which can degrade policy performance. Existing
  attack strategies use locally uniform perturbations, treating all samples equally
  and leading to inefficiency and detectability.
---

# Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2512.08485
- **Source URL:** https://arxiv.org/abs/2512.08485
- **Reference count:** 4
- **Primary result:** Global Budget Allocation attack achieves up to 80% policy performance degradation by targeting high-TD-error samples under a global L2 constraint.

## Executive Summary
This paper addresses the vulnerability of offline reinforcement learning to data poisoning attacks. Existing attack strategies use locally uniform perturbations, which are inefficient and easily detectable. The authors propose a novel Global Budget Allocation attack strategy that leverages the insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error. This method formulates the attack as a global resource allocation problem and derives a closed-form solution where perturbation magnitudes are assigned proportional to TD-error sensitivity under a global L2 constraint. Experiments on D4RL benchmarks demonstrate that the proposed method significantly outperforms baseline strategies while maintaining stealth.

## Method Summary
The Global Budget Allocation attack operates by first computing TD-errors for all samples in the offline dataset using a proxy Q-network. It then allocates perturbation magnitudes proportionally to these TD-error magnitudes under a global L2 budget constraint. The optimal perturbation magnitude for each sample is derived as $\epsilon^*_i = |\delta_i| / (2\lambda)$, where $\lambda$ is determined by the total budget. Perturbations are applied in the gradient direction of the TD-error. The method is evaluated against baseline attacks (Random Noise, Random Subset, Local Greedy) on D4RL benchmarks with CQL, BCQ, BEAR, and IQL victim algorithms.

## Key Results
- The proposed method achieves up to 80% performance degradation on Walker2d, Hopper, and HalfCheetah tasks.
- Global allocation outperforms locally uniform attacks while using less total perturbation budget.
- The attack evades detection by state-of-the-art statistical and spectral defenses.
- Higher TD-error samples yield disproportionately larger policy degradation when targeted.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Allocating attack budget proportionally to TD-error magnitude maximizes cumulative model parameter distortion.
- **Mechanism:** The method uses Influence Functions to establish that a sample's impact on learned Q-values is proportional to its Temporal Difference (TD) error. By framing the attack as a global resource allocation problem under an L2 budget constraint, the Lagrangian derivation shows that optimal perturbation magnitude is directly proportional to TD-error ($\epsilon_i \propto |\delta_i|$). This concentrates attack energy on "leverage points" in the dataset that disproportionately influence the value function convergence.
- **Core assumption:** The inverse Hessian of the loss function varies smoothly and predictably, making TD-error a reliable first-order proxy for overall sample influence.
- **Evidence anchors:** [abstract] "Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem." [section 3.1] "Thus, the magnitude of the influence is directly proportional to the TD-Error magnitude |δ|... This theoretical derivation validates why prioritizing high-TD-error samples is mathematically optimal for maximizing parameter perturbation." [section 3.3.2] "Solving for ϵ_i, we derive the optimal perturbation magnitude: ϵ*_i = |δ_i| / 2λ... This yields a theoretically elegant property: ϵ*_i ∝ |δ_i|"

### Mechanism 2
- **Claim:** A global budget allocation strategy is more attack-efficient and stealthier than locally uniform or local-greedy constraints.
- **Mechanism:** Local constraints waste budget on low-impact samples and require higher overall noise levels to be effective, making them detectable. The global allocation strategy relaxes rigid per-sample boundaries, allowing larger perturbations on critical samples and smaller ones on negligible ones. This achieves a higher cumulative impact for a given total detectability budget (global L2 norm), effectively hiding the attack within the data's natural variance.
- **Core assumption:** Defenses primarily detect anomalies based on aggregate noise magnitude or distributional shifts across many samples rather than detecting a few highly perturbed outliers.
- **Evidence anchors:** [abstract] "Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations." [section 1] "To cause enough damage, uniform attacks often need a high noise level across the whole dataset. This creates statistical changes that are easy to detect by anomaly detectors..."

### Mechanism 3
- **Claim:** Targeting high-TD-error samples with gradient-aligned perturbations is sufficient to cause catastrophic policy failure.
- **Mechanism:** The attack first identifies critical samples with high TD errors. It then applies perturbations to these samples in the direction that maximally increases their TD error (gradient direction). Corrupting these samples directly attacks the Bellman optimality equation, leading to a divergent or highly suboptimal value function, which in turn produces a failed policy upon extraction.
- **Core assumption:** Offline RL algorithms, particularly value-based ones, rely heavily on the accurate estimation of value functions from the static dataset. Corrupting the learning signal (TD error) for critical transitions propagates errors throughout the value estimate.
- **Evidence anchors:** [abstract] "...achieving up to 80% performance degradation..." [section 4, Table 1] Empirical results show a hierarchy of damage where attacks targeting high-TD-error samples (Local Greedy and Global) cause drastically more performance reduction than random attacks.

## Foundational Learning

- **Concept: Temporal Difference (TD) Error**
  - **Why needed here:** It is the central signal used to identify vulnerable samples and allocate attack budget. The core theoretical claim hinges on its correlation with sample influence.
  - **Quick check question:** For a given state-action transition (s, a, r, s'), write the expression for the TD error δ. What does a large |δ| signify about the current Q-value estimate?

- **Concept: Influence Functions**
  - **Why needed here:** This is the theoretical tool used to justify why TD-error is a good proxy for sample importance. It provides the mathematical link between a sample and its impact on model parameters.
  - **Quick check question:** In the context of this paper, what does the influence function $I_{up, params}(z)$ approximately measure?

- **Concept: Constrained Optimization (Lagrange Multipliers)**
  - **Why needed here:** The paper formulates the attack as maximizing an objective (weighted distortion) subject to a constraint (global L2 budget). The optimal solution is derived using this method.
  - **Quick check question:** In the paper's Lagrangian (Eq. 10), what is the role of the Lagrange multiplier λ? How does the optimal solution $\epsilon^*_i$ relate to it?

## Architecture Onboarding

- **Component map:** Proxy Training/Inference -> Sample Selector -> Budget Allocator -> Perturbation Generator -> Data Poisoner
- **Critical path:**
  1. A clean offline dataset $D$ is provided.
  2. Train a proxy Q-network on $D$ (or a subset) to estimate Q-values.
  3. Forward pass all samples through the network to compute their TD-error magnitude $|\delta_i|$.
  4. Calculate the global scaling factor (related to the Lagrange multiplier $\lambda$) based on the total budget $C_{total}$.
  5. Allocate per-sample perturbation budgets $\epsilon^*_i$.
  6. For each sample, compute the gradient direction for the perturbation and scale it by $\epsilon^*_i$.
  7. Create the poisoned dataset $D'$ and train the victim agent on it.

- **Design tradeoffs:**
  - **Budget Magnitude ($C_{total}$):** A higher budget causes more damage but increases detectability. The paper shows a trade-off (e.g., Table 1, CQL with $\rho=0.01, \epsilon=0.5$ gives 81.7% reduction but with $\rho=0.1, \epsilon=0.05$ only 27.6%).
  - **Gradient Computation Cost:** Computing gradients for every sample can be expensive. The paper claims a closed-form solution is efficient, but the gradient calculation step ($\nabla_D |\delta_i|$) is still required.
  - **Model Dependence:** The attack's success depends on the quality of the proxy model used to compute TD-errors. If the proxy is poor, the TD-error signal will be noisy.

- **Failure signatures:**
  - **Low Performance Degradation:** Attack budget is too low, or the proxy model is poor, leading to misallocation of budget.
  - **Attack Detected:** Budget is too high, or perturbations are concentrated too heavily on a very small number of samples, creating statistical anomalies.
  - **Policy Not Affected:** Victim model may be robust or use an algorithm not reliant on the corrupted TD signal (e.g., BC).

- **First 3 experiments:**
  1. **Replicate the Main Result:** Implement the Global Budget Allocation attack on the D4RL Walker2d-medium dataset with a CQL victim. Compare performance degradation against the Random Noise, Random Subset, and Local Greedy baselines to verify the hierarchy of damage shown in Table 1.
  2. **Ablation on Budget Scaling:** Run the attack with varying global budget constraints ($C_{total}$). Plot the victim's final return against the total perturbation budget to visualize the efficiency curve compared to a uniform baseline.
  3. **Proxy Model Fidelity Test:** Train the proxy Q-network to different levels of convergence (e.g., 10%, 50%, 100% of training steps) before computing TD-errors for the attack. Measure how the quality of the proxy affects the final attack success rate to test the robustness of the mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can uncertainty-aware dynamics modeling or robust value estimation effectively distinguish between adversarially perturbed high-TD-error samples and naturally occurring "hard" transitions without discarding valuable learning data?
- **Basis in paper:** [explicit] The Conclusion states that defense mechanisms must go beyond outlier detection and utilize "robust value estimation and uncertainty-aware dynamics modeling" to counter the fragility exposed by the attack.
- **Why unresolved:** The authors demonstrate that current statistical and spectral defenses fail, but they do not implement or test the specific robust estimation defenses they suggest as the future direction.
- **What evidence would resolve it:** Experiments showing that a specific robust value estimation algorithm can maintain policy performance under Global Budget Allocation attacks better than standard CQL or IQL.

### Open Question 2
- **Question:** Does the Global Budget Allocation strategy retain its efficacy and stealthiness against certified defenses (e.g., COPA) that provide provable guarantees on policy performance?
- **Basis in paper:** [explicit] The Related Work section explicitly identifies Xu et al. [2025] as having proposed "certified defenses to guarantee performance against local corruptions," contrasting with the detection methods evaluated in the experiments.
- **Why unresolved:** The empirical evaluation is limited to statistical and spectral detection methods; the interaction between global L2 perturbations and provable certified defenses remains untested.
- **What evidence would resolve it:** Empirical results or theoretical proofs showing whether the global L2 constraint violates the assumptions of certified defenses or bypasses their performance guarantees.

### Open Question 3
- **Question:** Does the linear proportionality between perturbation magnitude and TD-error ($\epsilon_i \propto |\delta_i|$) remain optimal in sparse reward environments where high TD-errors may indicate terminal states rather than generalizable high-value leverage points?
- **Basis in paper:** [inferred] The experiments are conducted exclusively on D4RL MuJoCo tasks (Walker2d, Hopper, HalfCheetah), which are dense reward environments; the generalization to sparse reward structures is unstated.
- **Why unresolved:** In sparse settings, TD-errors are often zero until a reward is found. The mechanism for allocating budget when $\delta \approx 0$ for the majority of samples is not addressed by the current derivation.
- **What evidence would resolve it:** Evaluation of the attack strategy on sparse reward benchmarks (e.g., AntMaze or Adroit in D4RL) showing performance degradation comparable to dense environments.

## Limitations
- **Budget-Constraint Derivation Gap:** The paper presents a closed-form solution $\epsilon^*_i \propto |\delta_i|$, but the derivation of the Lagrange multiplier $\lambda$ from the total budget constraint is only implicitly defined.
- **Model Dependence:** The attack's effectiveness hinges on the accuracy of the proxy Q-network used to estimate TD-errors. The paper does not analyze how inaccuracies in this proxy affect the attack's performance.
- **Evaluation Scope:** All experiments use D4RL datasets with "medium" and "medium-replay" quality. It is unclear how the attack performs on higher-quality datasets or in continuous control tasks outside this benchmark suite.

## Confidence
- **High Confidence:** The core theoretical mechanism linking TD-error to sample influence (Mechanism 1) is well-justified by the influence function derivation and is internally consistent.
- **Medium Confidence:** The claim that global budget allocation is more efficient and stealthy than local strategies (Mechanism 2) is plausible but relies on unstated assumptions about the nature of detection methods.
- **Medium Confidence:** The empirical claim of "up to 80% performance degradation" (Mechanism 3) is supported by the results in Table 1, but the reproducibility is hampered by the ambiguities in the budget allocation formula.

## Next Checks
1. **Derive and Validate the Lagrange Multiplier:** Work through the full Lagrangian optimization to derive the explicit formula for $\lambda$ as a function of the total budget $C_{total}$ and the set of TD-errors. Implement this formula and verify that the resulting perturbations satisfy the global L2 constraint within a small numerical tolerance.
2. **Ablation on Proxy Model Quality:** Systematically vary the training stage of the proxy Q-network (e.g., 10%, 50%, 100% of convergence) and measure the attack's success rate. This will quantify the method's robustness to proxy model inaccuracies and identify the minimum viable training required.
3. **Detection Analysis with Varying Budget Magnitudes:** For a fixed dataset and victim algorithm, run the Global attack with a spectrum of total budget values ($C_{total}$). Plot the victim's performance degradation against a measure of statistical anomaly (e.g., total L2 perturbation norm or a detection score from a state-of-the-art defense). This will empirically validate the claimed stealth-efficiency trade-off.