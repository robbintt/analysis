---
ver: rpa2
title: 'Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style
  Control'
arxiv_id: '2601.03973'
source_url: https://arxiv.org/abs/2601.03973
tags:
- style
- song
- generation
- muse
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Muse, a fully open-source system for long-form
  song generation with fine-grained style control. The authors address the reproducibility
  bottleneck in song generation research by constructing a licensed synthetic dataset
  of 116k songs with structured lyrics and hierarchical style annotations.
---

# Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control

## Quick Facts
- arXiv ID: 2601.03973
- Source URL: https://arxiv.org/abs/2601.03973
- Reference count: 40
- Authors: Changhao Jiang; Jiahao Chen; Zhenghao Xiang; Zhixiong Yang; Hanchen Wang; Jiabao Zhuang; Xinmeng Che; Jiajun Sun; Hui Li; Yifei Cao; Shihan Dou; Ming Zhang; Junjie Ye; Tao Ji; Tao Gui; Qi Zhang; Xuanjing Huang
- Primary result: Open-source long-form song generation system achieving competitive performance through single-stage supervised fine-tuning of Qwen-based LLM with discrete audio tokens

## Executive Summary
Muse introduces a fully open-source system for long-form song generation with fine-grained style control, addressing the reproducibility bottleneck in music generation research. The authors construct a licensed synthetic dataset of 116k songs with structured lyrics and hierarchical style annotations, then train Muse via single-stage supervised fine-tuning of a Qwen-based language model extended with discrete audio tokens using MuCodec. Despite modest model size and data scale, Muse achieves competitive performance on phoneme error rate, text-music style similarity, and audio aesthetic quality while enabling controllable segment-level generation across different musical structures.

## Method Summary
Muse employs a single-stage supervised fine-tuning approach that extends a Qwen3-0.6B language model with discrete audio tokens from MuCodec, treating both text and audio as unified autoregressive sequences. The system uses a conversational prompting scheme where global style is defined first, followed by sequential segment-specific instructions, enabling fine-grained control over musical structure. The training data consists of 116k synthetic songs generated by Suno, which were re-annotated using Qwen3-Omni to correct style label inconsistencies. The model is trained with DeepSpeed ZeRO-3 using cross-entropy loss, without additional architectural components or task-specific losses.

## Key Results
- Achieves competitive performance on phoneme error rate, text-music style similarity (0.58 after re-annotation), and audio aesthetic quality
- Enables controllable segment-level generation across different musical structures through conversational prompting
- Demonstrates reproducibility through open data and licensing while maintaining performance comparable to commercial models

## Why This Works (Mechanism)

### Mechanism 1: Unified Autoregressive Token Modeling
The system integrates discrete audio tokens directly into the Qwen LLM vocabulary, modeling text and audio within a single causal sequence. This allows attention mechanisms to operate across lyrical semantics and acoustic features simultaneously, leveraging the same temporal dependencies learned for language in the audio domain.

### Mechanism 2: Conversational Structuring for Segment Control
Input is structured as a dialogue with global style as the first user message, followed by sequential user messages for specific segments containing local style and lyrics. This forces the model to attend to fresh local context at the start of every segment, enforcing structural boundaries better than monolithic prompts.

### Mechanism 3: Feedback-Guided Data Refinement
The authors generated 118k songs via Suno but found ~13% had low style similarity. They re-annotated these using Qwen3-Omni to correct labels to match actual audio content, improving annotation consistency and increasing text-music similarity to 0.58.

## Foundational Learning

- **Neural Audio Codecs (VQ-VAE):** Essential for understanding how continuous audio waves are compressed into discrete integers ("tokens") for LLM processing. *Quick check:* Why does the codebook size (16,384 in MuCodec) matter for reconstruction quality versus sequence length?

- **Supervised Fine-Tuning (SFT) vs. RLHF:** Muse relies entirely on SFT without Reinforcement Learning. *Quick check:* Why does the absence of a "reward model" (RL) simplify the reproducibility of the pipeline?

- **Phoneme Alignment:** Critical for understanding PER metrics and distinguishing between generating intelligible speech versus melodic audio. *Quick check:* How does the inclusion of explicit phoneme sequences in training data potentially aid lyric-to-vocal alignment?

## Architecture Onboarding

- **Component map:** Qwen Tokenizer (Text) -> MuCodec (Audio) -> Qwen3-0.6B (Transformer) -> Vocabulary Extension (Audio tokens + [BOA], [EOA]) -> DeepSpeed ZeRO-3 (Training)

- **Critical path:** Data generation (Suno) -> Re-annotation (Qwen3-Omni) -> Tokenization -> Packing into Conversation Format -> SFT

- **Design tradeoffs:** Synthetic data is licensable and clean-structured but risks inheriting teacher model biases; deterministic decoding at T=0 can cause collapse requiring T=0.9 for stability

- **Failure signatures:** Decoding collapse with immediate token repetition at T=0; style drift when global context becomes diluted over long songs

- **First 3 experiments:**
  1. Inference Stability Check: Run generation with Temperature=0 vs Temperature=0.9 to quantify repetition failure rate
  2. Context Ablation: Input a "Verse" prompt without preceding "Intro" context to test global style coherence without history
  3. Tokenizer Stress Test: Vary lyric length in single segments to identify PER threshold where alignment breaks down

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Model size (0.6B parameters) and data scale (116k songs) are significantly smaller than state-of-the-art models, potentially limiting generalization
- Reliance on synthetic data from commercial models (Suno) means inheriting potential biases and stylistic limitations
- Conversation-based control may struggle with very long songs where global style context becomes diluted

## Confidence

- **High Confidence:** Architectural design and training methodology are well-documented and technically sound; conversational prompting scheme is clearly specified
- **Medium Confidence:** Data quality improvement claims are supported by metrics but subject to annotation model bias; competitive performance claims based on benchmarks may not capture full musical quality
- **Low Confidence:** Scalability to complex genres or longer compositions is unproven; extent of true capability reproduction versus surface-level similarity is unclear

## Next Checks
1. Context Window Stress Test: Generate songs with varying segment numbers to quantify style drift and measure global style retention accuracy at different depths
2. Cross-Style Generalization: Test on underrepresented musical styles (classical, jazz, experimental electronic) to evaluate transferable musical representations
3. Phoneme Alignment Robustness: Systematically vary lyric complexity and phoneme density to identify PER threshold where alignment breaks down and compare against baseline without explicit phoneme sequences