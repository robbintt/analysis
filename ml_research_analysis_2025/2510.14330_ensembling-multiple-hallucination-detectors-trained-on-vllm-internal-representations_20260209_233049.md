---
ver: rpa2
title: Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations
arxiv_id: '2510.14330'
source_url: https://arxiv.org/abs/2510.14330
tags:
- hallucination
- task
- accuracy
- answer
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a solution to the Meta CRAG-MM Challenge at
  KDD Cup 2025, which focuses on reducing hallucinations in vision-language models
  (VLMs) for visual question answering. Their approach centers on training logistic
  regression models to detect hallucinations using internal representations from VLMs,
  specifically hidden states and attention head outputs.
---

# Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations

## Quick Facts
- **arXiv ID:** 2510.14330
- **Source URL:** https://arxiv.org/abs/2510.14330
- **Reference count:** 40
- **Primary result:** Ensembling 65 logistic regression models trained on Llama-3.2-11B-Vision-Instruct internal representations reduces hallucinations by ~94% while maintaining reasonable accuracy, achieving top-5 placement in KDD Cup 2025 Meta CRAG-MM Challenge.

## Executive Summary
This paper presents a solution to the Meta CRAG-MM Challenge at KDD Cup 2025, focusing on reducing hallucinations in vision-language models (VLMs) for visual question answering. The approach centers on training logistic regression models to detect hallucinations using internal representations from VLMs, specifically hidden states and attention head outputs. These detectors are trained on a validation set and then applied during inference to classify answers as hallucinated or not. An ensemble of multiple detectors is used to make the final decision, and if a hallucination is detected, the system outputs "I don't know" instead of the answer. This strategy successfully reduces hallucination rates significantly while maintaining reasonable accuracy, leading to a top-5 placement in the competition.

## Method Summary
The method extracts internal representations (hidden states and attention head outputs) from Llama-3.2-11B-Vision-Instruct during answer generation. These representations are averaged across all generated tokens for each VQA instance. Logistic regression classifiers are trained on these averaged features to detect hallucinations, using labels generated by an LLM-as-a-Judge on the validation set. The system selects 7 hidden state layers (13-19) and 58 attention heads with F1 > 0.5 for detection. During inference, all 65 detectors run in parallel, and their predictions are averaged. If the average probability of being a non-hallucination exceeds 0.65, the generated answer is accepted; otherwise, "I don't know" is output. PCA is applied to hidden state features to retain 95% cumulative variance.

## Key Results
- Reduced hallucinations by ~94% (from 0.278 to 0.017 on public_test)
- Maintained reasonable accuracy (0.207 to 0.082 on public_test)
- Achieved top-5 placement in KDD Cup 2025 Meta CRAG-MM Challenge
- Individual detectors achieved F1 > 0.5 for hallucination detection

## Why This Works (Mechanism)

### Mechanism 1
Internal representations of VLLMs encode discriminative signals that can predict whether a generated answer is a hallucination. Logistic regression probes trained on average pooled hidden states and attention head outputs learn to distinguish between "correct" and "hallucinated" classes based on geometric properties within these high-dimensional vectors. The ensemble aggregates these individual weak signals.

**Core assumption:** Hallucinations leave distinguishable traces in model internal states that can be captured via linear probing.

**Evidence anchors:**
- Logistic regression models trained to classify answers as hallucinations or not using hidden states and attention outputs as features
- Previous studies show hallucinations leave distinguishable traces in model internal states

**Break condition:** This mechanism would fail if internal representations of hallucinations are highly non-linear or differ significantly between training and deployment domains.

### Mechanism 2
Ensembling multiple specialized detectors improves robustness compared to a single probe. Instead of relying on one "best" layer or attention head, the system trains numerous independent logistic regression models (65 total) and averages their predicted probabilities. This averages out noise and specific failure modes of individual probes.

**Core assumption:** Errors of individual detectors are at least partially uncorrelated, allowing ensemble to improve signal-to-noise ratio.

**Evidence anchors:**
- Ensemble of multiple detectors used to make final decision by averaging outputs
- Competition results show improved performance with ensemble approach

**Break condition:** This mechanism could fail if selected detectors are highly correlated (all making same mistakes).

### Mechanism 3
Explicitly abstaining when a hallucination is detected maximizes the "trustfulness score" under a penalty-based evaluation metric. The competition's scoring function heavily penalizes incorrect answers (-1) versus missing/refusal answers (0). The system uses a binary filter: if ensemble-averaged hallucination score exceeds threshold (0.65), output "I don't know" instead of answer.

**Core assumption:** Penalty for incorrect answer is significantly higher than gain from correct one in high-uncertainty cases, making conservative strategy optimal.

**Evidence anchors:**
- System outputs "I don't know" when hallucination is detected
- Threshold of 0.65 for accepting answers based on non-hallucination probability

**Break condition:** This mechanism fails if detector has high false positive rate, rejecting many correct answers.

## Foundational Learning

**Linear Probing / Representation Engineering**
- Why needed here: Core technique of training linear classifier on frozen internal model activations to detect semantic features like "truthfulness"
- Quick check question: If hidden state of layer 17 is a 4096-dimension vector, what would be input and output dimensions of logistic regression probe trained on it?

**Ensembling / Model Averaging**
- Why needed here: System relies on aggregating predictions from dozens of separate probes to improve robustness
- Quick check question: Why might averaging predictions of 50 detectors be better than selecting single detector with highest validation accuracy?

**Evaluation Metrics & Utility Tradeoffs**
- Why needed here: Entire rationale based on optimizing specific asymmetric cost function (+1 correct, -1 incorrect, 0 abstain)
- Quick check question: Under what conditions would system's strategy of outputting "I don't know" decrease final trustfulness score?

## Architecture Onboarding

**Component map:**
VLLM (Generator) -> Probe Training Pipeline -> Hallucination Filter (Inference) -> Decision Logic

**Critical path:** Setting up VLLM with hooks to extract internal states, collecting (state, label) dataset on validation set, training simple logistic regression probes.

**Design tradeoffs:** Primary tradeoff is Recall vs. Precision in hallucination detection, manifesting as Accuracy vs. Trustfulness. Stricter threshold reduces hallucinations (increasing trustfulness) but also rejects more correct answers (decreasing accuracy).

**Failure signatures:**
- High False Positive Rate: System becomes overly conservative, constantly saying "I don't know" even for simple correct facts
- Detector Domain Shift: Probes trained on validation set fail to generalize to test set, causing hallucination rate to spike unexpectedly

**First 3 experiments:**
1. Baseline Probe: Train single logistic regression probe on final hidden state of last generated token; measure F1 score on held-out validation split
2. Layer/Head Search: Systematically evaluate probes trained on each individual layer and attention head; create ranked list based on F1 score
3. Ensemble vs. Single: Compare performance of single best probe against ensemble of top-k probes (k=10, 50, 65); sweep decision threshold to plot Accuracy vs. Trustfulness tradeoff curve

## Open Questions the Paper Calls Out

**Open Question 1**
Can incorporation of external Retrieval-Augmented Generation (RAG) sources improve or degrade performance of internal-state-based hallucination detectors?
- Basis in paper: No external sources were used at all, despite competition tasks providing mock APIs for image-KG and web retrieval
- Why unresolved: Unclear if adding external context shifts model's internal representations in way that invalidates logistic regression detectors trained on non-RAG outputs
- What evidence would resolve it: Evaluation of detection F1 scores and trustfulness metrics when VLM generates answers augmented by provided mock knowledge graphs or web search results

**Open Question 2**
Is it possible to mitigate significant loss of valid answers (accuracy drop) while maintaining high level of hallucination suppression achieved by ensemble?
- Basis in paper: Table 1 shows method reduced hallucinations by ~94% but also reduced accuracy by ~61% (from 0.207 to 0.082)
- Why unresolved: Paper optimizes purely for competition's "trustfulness" metric but does not explore methods to recover high rate of discarded correct answers
- What evidence would resolve it: Experiments using non-linear probes, calibrated thresholds, or reinforcement learning to refine decision boundary

**Open Question 3**
Do specific attention heads identified as effective detectors in Llama-3.2-11B-Vision-Instruct generalize to other VLLM architectures or modalities?
- Basis in paper: Tables 3 and 4 identify specific layers and heads that perform best, but study limited to single model checkpoint
- Why unresolved: Paper determines which heads work empirically but does not determine if they encode universal "truthfulness" features or are idiosyncratic to Llama-3.2 architecture
- What evidence would resolve it: Transferability study applying same logistic regression probing method to different VLLM backbones

## Limitations
- Accuracy significantly reduced (from 0.207 to 0.082) while reducing hallucinations by ~94%
- Does not explore use of provided external knowledge graph or web search APIs that could enhance factuality
- Relies on LLM-as-a-Judge for label generation without validating label quality or potential biases

## Confidence

**High Confidence:** Core claim that ensembling multiple logistic regression probes trained on VLLM internal representations can reduce hallucination rates while maintaining reasonable accuracy is well-supported by competition results and logical mechanism.

**Medium Confidence:** Specific selection of 7 hidden state layers (13-19) and 58 attention heads with F1 > 0.5 is based on empirical validation on validation set, though exact configuration may not be optimal.

**Low Confidence:** Claim that threshold of 0.65 for accepting answers maximizes trustfulness score is specific to competition's evaluation metric and test data distribution, may not generalize to other scenarios.

## Next Checks
1. **Domain Generalization Test:** Evaluate trained detectors on held-out domain or different VQA dataset to assess whether internal representation patterns generalize beyond CRAG-MM benchmark.

2. **Label Quality Analysis:** Manually inspect random sample of LLM-as-a-Judge labels to quantify label noise and assess impact on detector performance; compare detector performance when trained on clean vs. noisy labels.

3. **Detector Redundancy and Aggregation Optimization:** Analyze correlation between individual detector predictions and experiment with alternative aggregation strategies (weighted averaging, stacking) to determine if ensemble can be made more efficient or effective.