---
ver: rpa2
title: 'CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese
  Hokkien Speech Recognition'
arxiv_id: '2511.06860'
source_url: https://arxiv.org/abs/2511.06860
tags:
- taiwanese
- hokkien
- speech
- clift-asr
- tone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLiFT-ASR is a cross-lingual fine-tuning framework designed to
  address the scarcity of annotated data for Taiwanese Hokkien ASR by leveraging Mandarin-pretrained
  HuBERT models and applying a two-stage fine-tuning strategy. The framework first
  learns acoustic and tonal representations from phonetic Tai-lo annotations and then
  captures vocabulary and syntax from Han-character transcriptions, enabling effective
  alignment between speech sounds and orthographic structures.
---

# CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition

## Quick Facts
- **arXiv ID**: 2511.06860
- **Source URL**: https://arxiv.org/abs/2511.06860
- **Reference count**: 5
- **Primary result**: 24.88% relative reduction in character error rate (CER) compared with strong baselines

## Executive Summary
CLiFT-ASR is a cross-lingual fine-tuning framework designed to address the scarcity of annotated data for Taiwanese Hokkien ASR by leveraging Mandarin-pretrained HuBERT models and applying a two-stage fine-tuning strategy. The framework first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions, enabling effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88% relative reduction in character error rate (CER) compared with strong baselines, offering an effective and parameter-efficient solution for Taiwanese Hokkien ASR with potential to benefit other low-resource language scenarios.

## Method Summary
The CLiFT-ASR framework employs a two-stage fine-tuning approach on top of a Mandarin-pretrained HuBERT-base encoder within an RNN-Transducer architecture. In Stage 1, the model is trained for 20 epochs on phonetic Tai-lo romanization annotations to establish acoustic-phonetic-tonal mappings. In Stage 2, the model continues training for 40 epochs on Han-character transcriptions to capture lexical and syntactic information. The method uses byte-level BPE tokenization via Icefall, ScaledAdam optimizer (lr=0.0005, Eden scheduler, grad_accum=4), and maintains all components trainable throughout both stages. The framework is evaluated on the TAT-MOE corpus with mixed Hàn-Lô-Tâi-bûn orthography.

## Key Results
- Achieves 24.88% relative CER reduction compared to strong baselines
- Outperforms direct fine-tuning by 6.6% relative CER through two-stage approach
- Demonstrates superior performance of Mandarin-pretrained models (22.41% CER) over English-pretrained models (24.49% CER)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mandarin-pretrained encoders transfer more effectively to Taiwanese Hokkien than English-pretrained encoders.
- **Mechanism**: Taiwanese and Mandarin share similar morphological and syntactic structures despite tonal differences. Mandarin-pretrained acoustic representations capture phonological patterns (tone, syllable structure) that transfer better than English-based representations, which lack tonal modeling entirely.
- **Core assumption**: The phonological and structural similarities between source and target languages determine transfer efficiency more than raw pretraining data size.
- **Evidence anchors**: [Section 2.1] shows Mandarin-pretrained ASR models outperform English-pretrained models for romanized Taiwanese; [Table 2] demonstrates 22.41% vs 24.49% CER advantage; cross-lingual ASR transfer literature confirms dialect similarity matters beyond phylogenetic distance.

### Mechanism 2
- **Claim**: Two-stage fine-tuning (phonetic → orthographic) outperforms direct end-to-end fine-tuning for mixed-orthography low-resource ASR.
- **Mechanism**: Stage 1 on Tai-lo romanization forces the model to learn explicit acoustic-phonetic-tonal mappings without orthographic ambiguity. Stage 2 on Han characters then maps these stabilized acoustic representations to lexical-syntactic forms. Staging prevents the model from conflating acoustic learning with orthographic complexity.
- **Core assumption**: Phonetic annotations provide a cleaner learning signal for acoustic alignment than mixed Han-Lô text, and the model retains Stage 1 knowledge during Stage 2.
- **Evidence anchors**: [Abstract] notes direct fine-tuning often fails to capture detailed phonetic and tonal cues; [Table 3] shows two-stage approach achieves 20.94% CER vs 22.41% for direct fine-tuning; the strategy remains relatively unexplored in literature.

### Mechanism 3
- **Claim**: The RNN-Transducer joint network enables effective integration of acoustic and linguistic context for tonal languages.
- **Mechanism**: The joint network combines encoder hidden states (acoustic features including tone) with prediction network states (trigram token context) via element-wise addition before softmax. This allows each decoding step to condition on both current acoustic evidence and recent linguistic context—essential for disambiguating tone-dependent homophones.
- **Core assumption**: Short trigram context in the prediction network is sufficient for most tonal disambiguation; longer dependencies are less critical.
- **Evidence anchors**: [Section 3.1] explains how the architecture jointly leverages acoustic and linguistic context; [Figure 2] shows tone confusion analysis with improved acoustic-tonal discrimination; authors note stateless prediction network constrains long-range dependency modeling.

## Foundational Learning

- **Concept: RNN-Transducer (RNN-T) loss and decoding**
  - **Why needed here**: CLiFT-ASR is built on RNN-T architecture. Understanding how the transducer computes joint probabilities over all alignment paths—and how inference uses beam search with the autoregressive prediction network—is essential for debugging training convergence and decoding errors.
  - **Quick check question**: Can you explain why RNN-T training does not require forced alignment, and how the prediction network receives its inputs during training vs. inference?

- **Concept: Self-supervised speech representations (HuBERT)**
  - **Why needed here**: The audio encoder is initialized from Mandarin HuBERT, which learns discrete acoustic units via masked prediction. Understanding what HuBERT representations capture (phonetic content, speaker traits, prosody) helps diagnose transfer failures.
  - **Quick check question**: What is the difference between HuBERT and wav2vec 2.0 in terms of target construction for the masked prediction task?

- **Concept: Byte-level BPE tokenization for CJK languages**
  - **Why needed here**: The paper uses Icefall's byte-level BPE to handle large Han character vocabularies. This affects output vocabulary size, OOV handling, and CER computation.
  - **Quick check question**: How does byte-level BPE differ from character-level or word-level BPE for languages with large character sets like Chinese or Taiwanese Han characters?

## Architecture Onboarding

- **Component map**: Input Speech → Feature Extraction (HuBERT from raw waveform) → Audio Encoder (HuBERT-base-cmn Transformer, 96M params) → Encoder Hidden States H_enc; Previous Tokens (wn-2, wn-1) → Prediction Network (Stateless, 256-dim) → Prediction Hidden States h_pred; H_enc + h_pred → Joint Network → Softmax → Token Probabilities

- **Critical path**: 1) Initialize audio encoder with pretrained Mandarin HuBERT weights (not random). 2) Stage 1: Fine-tune entire model on Tai-lo phonetic annotations for 20 epochs. 3) Stage 2: Continue fine-tuning on Han character transcriptions for 40 epochs. 4) Ensure all components (encoder, prediction, joint) remain trainable—freezing any component in Stage 1 degrades performance.

- **Design tradeoffs**: HuBERT features vs FBank (HuBERT requires pretrained weights but provides better acoustic modeling); two-stage vs direct fine-tuning (staging adds complexity but yields ~6.6% relative CER reduction); parameter efficiency vs raw performance (CLiFT-ASR 96M params vs Whisper-small 244M params).

- **Failure signatures**: High CER (>35%) with frozen encoder indicates Stage 1 phonetic learning cannot adapt pretrained features; tone 5/7/8 confusion persists suggesting tone sandhi or speaker variation; training divergence in Stage 2 may indicate learning rate too high.

- **First 3 experiments**: 1) Ablation: Direct fine-tuning vs two-stage—replicate Table 3 to confirm staging benefit; 2) Encoder initialization comparison—train with English HuBERT, Mandarin HuBERT, and random initialization; 3) Tone confusion analysis—generate confusion matrices as in Figure 2 to identify high-confusion tone pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can explicit modeling of tone sandhi rules significantly reduce the confusion rates among tones 5, 7, and 8?
- **Basis in paper**: [explicit] Section 7 lists "explicit modeling of tone sandhi" as a future avenue to address tonal confusion.
- **Why unresolved**: The current framework does not model tone sandhi, resulting in frequent mutual misclassifications between specific tones.
- **What evidence would resolve it**: A comparative analysis of confusion matrices from models trained with and without explicit sandhi rule integration.

### Open Question 2
- **Question**: Does replacing the stateless prediction network with an architecture capable of modeling long-range dependencies improve recognition of complex tonal patterns?
- **Basis in paper**: [explicit] Section 8 notes the stateless RNN-Transducer constrains the ability to model long-range dependencies and complex tone sandhi.
- **Why unresolved**: The current architectural choice limits the model's capacity to utilize full contextual history for disambiguation.
- **What evidence would resolve it**: Performance benchmarks comparing the current stateless decoder against decoder architectures with extended context windows.

### Open Question 3
- **Question**: Is the two-stage fine-tuning strategy (phonetic-to-orthographic) effective for other low-resource languages with mixed-script traditions?
- **Basis in paper**: [explicit] The Abstract states the framework has "potential to benefit other low-resource language scenarios."
- **Why unresolved**: The experiments are restricted to Taiwanese Hokkien, leaving the generalizability of the specific staged alignment approach unverified.
- **What evidence would resolve it**: Successful application of CLiFT-ASR to a different low-resource language utilizing a similar cross-lingual transfer setup.

## Limitations

- Data availability and curation barriers: The method relies heavily on the TAT-MOE corpus with parallel Tai-lo and Han-character annotations, with exact preprocessing pipeline underspecified, limiting reproducibility and generalizability to other languages.
- Model architecture constraints: The RNN-T architecture with stateless prediction network limits ability to capture long-range dependencies, problematic for languages with complex syntactic or phonological patterns beyond trigram contexts.
- Comparison baseline selection: Comparing against models with significantly different parameter counts (e.g., Whisper-small at 244M vs CLiFT-ASR at 96M) makes it difficult to isolate the contribution of the fine-tuning framework from model size effects.

## Confidence

- **High confidence**: Superiority of Mandarin-pretrained models over English-pretrained models for Taiwanese Hokkien ASR (CER: 22.41% vs 24.49%). Directly supported by experimental results and aligns with cross-lingual transfer literature.
- **Medium confidence**: Two-stage fine-tuning yields 6.6% relative CER improvement over direct fine-tuning. Table 3 shows benefit, but lack of direct corpus evidence on staged phonetic-to-orthographic transfer and potential catastrophic forgetting concerns warrant cautious interpretation.
- **Medium confidence**: RNN-T joint network effectively integrates acoustic and linguistic context for tonal languages. Tone confusion analysis shows improvements, but limitation of trigram context for long-range dependencies suggests the mechanism is effective but incomplete.

## Next Checks

1. **Ablation study on staging**: Replicate the direct fine-tuning vs. two-stage comparison on your target language/data to verify the staging benefit. If no improvement is observed, investigate the quality of phonetic-to-orthographic alignment in your corpus and whether your language shares similar orthographic complexity to Taiwanese Hokkien.

2. **Encoder initialization cross-lingual test**: Train models with English HuBERT, Mandarin HuBERT, and random initialization on your target language to validate whether Mandarin pretraining advantage generalizes to your language family. This will confirm if phonological/structural similarity drives transfer success.

3. **Tone confusion analysis for your language**: Generate tone/similarity confusion matrices for your model outputs to identify systematic errors. If your language has tone sandhi or overlapping pitch contours, investigate whether explicit sandhi modeling or targeted data augmentation could improve performance, as CLiFT-ASR shows persistent confusion for tones 5, 7, and 8.