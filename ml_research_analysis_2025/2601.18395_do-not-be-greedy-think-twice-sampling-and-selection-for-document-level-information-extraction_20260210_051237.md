---
ver: rpa2
title: 'Do not be greedy, Think Twice: Sampling and Selection for Document-level Information
  Extraction'
arxiv_id: '2601.18395'
source_url: https://arxiv.org/abs/2601.18395
tags:
- reasoning
- greedy
- selector
- selection
- better
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces THINKTWICE, a sampling and selection framework
  for document-level Information Extraction (DocIE) that leverages the output variability
  of decoder-only LLMs to achieve better performance than greedy decoding. Instead
  of generating a single output, the method produces multiple candidate templates
  for a given document and applies a selection module to identify the most suitable
  one.
---

# Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction

## Quick Facts
- arXiv ID: 2601.18395
- Source URL: https://arxiv.org/abs/2601.18395
- Authors: Mikel Zubillaga; Oscar Sainz; Oier Lopez de Lacalle; Eneko Agirre
- Reference count: 12
- Primary result: THINKTWICE framework achieves F1 gains up to 4 points over greedy decoding in document-level IE tasks

## Executive Summary
This paper introduces THINKTWICE, a novel sampling and selection framework for document-level information extraction that addresses the limitations of greedy decoding in large language models. The method generates multiple candidate extraction templates for each document and employs a selection module to identify the optimal output. By leveraging output variability through sampling and applying either unsupervised F1 voting or supervised reward-based selection, the framework consistently outperforms greedy baselines across multiple datasets including MUC-4, MultiMUC, and BETTER.

The study demonstrates that reasoning models significantly outperform non-reasoning counterparts in DocIE tasks, with the supervised selection approach further improving performance. The framework shows particular strength in zero-shot and cross-lingual settings, achieving state-of-the-art results while addressing the trade-off between precision and recall that typically challenges greedy decoding approaches.

## Method Summary
THINKTWICE operates through a two-stage process: candidate generation followed by selection. During generation, multiple templates are produced for each document using sampling strategies from decoder-only LLMs, capturing diverse extraction possibilities. The selection stage employs two approaches: an unsupervised F1 Voting method that selects the candidate with highest average F1 score against others, and a supervised reward-based method trained on silver reasoning traces generated via rejection sampling. The silver data captures reasoning patterns that lead to successful extractions, enabling the reward model to evaluate and select optimal candidates. This framework effectively addresses the greedy decoding problem where LLMs may settle for suboptimal extractions by exploring the solution space more thoroughly.

## Key Results
- THINKTWICE achieves F1 improvements up to 4 points over greedy decoding baselines in zero-shot settings
- Supervised selection consistently outperforms unsupervised F1 voting across all evaluated datasets
- Reasoning models demonstrate superior performance compared to non-reasoning counterparts in DocIE tasks
- The framework achieves state-of-the-art results in both supervised and cross-lingual evaluation scenarios

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to overcome the inherent limitations of greedy decoding in information extraction tasks. Greedy decoding often produces suboptimal extractions by making locally optimal choices that don't account for global document context or alternative valid extractions. By generating multiple candidates, THINKTWICE explores the solution space more comprehensively, capturing different valid interpretations of the same document. The selection module then acts as a filter, using either unsupervised consensus (F1 voting) or learned preferences (reward-based selection) to identify the most accurate extraction. This approach is particularly valuable for document-level IE where entities and relations span multiple sentences and require holistic understanding.

## Foundational Learning

**Document-level Information Extraction**: Extracting entities and relations from entire documents rather than isolated sentences, requiring cross-sentence reasoning and global context understanding. Needed to capture complex relationships that span multiple sentences or paragraphs. Quick check: Can the model correctly extract event participants that appear in different sections of a document?

**Decoder-only Language Models**: Transformer models that generate text autoregressively without separate encoder and decoder components. Needed for efficient text generation and sampling capabilities. Quick check: Does the model maintain coherence when generating long extraction templates?

**Rejection Sampling for Silver Data**: A technique that generates synthetic training examples by repeatedly sampling until desired quality criteria are met. Needed to create high-quality training data for the reward model without expensive manual annotation. Quick check: Are the silver reasoning traces representative of successful human reasoning patterns?

**Template-based Extraction**: Representing extracted information as structured templates rather than raw text spans. Needed for consistent evaluation and downstream processing of extracted information. Quick check: Can the templates capture all relevant entity types and relations in the target domain?

## Architecture Onboarding

**Component map**: Document -> Sampling Module -> Multiple Candidates -> Selection Module -> Final Extraction

**Critical path**: The generation-selection pipeline where candidate quality directly impacts selection accuracy. The most critical components are the sampling strategy (which determines candidate diversity) and the selection module (which must accurately identify the best candidate).

**Design tradeoffs**: 
- More candidates improve coverage but increase computational cost
- Supervised selection requires silver data generation but provides better performance
- Reasoning models improve accuracy but may increase inference latency

**Failure signatures**: 
- Poor candidate diversity leading to selection among similar suboptimal options
- Noisy silver data causing reward model to learn incorrect patterns
- Selection module favoring candidates with high precision but low recall

**Exactly 3 first experiments**:
1. Compare F1 Voting vs reward-based selection on a held-out validation set to measure selection accuracy
2. Analyze candidate diversity metrics (e.g., pairwise F1 scores) to optimize the number of candidates generated
3. Evaluate performance degradation when using noisy vs clean silver reasoning traces for reward model training

## Open Questions the Paper Calls Out
None

## Limitations
- Selection module relies on silver data generated through rejection sampling, which may introduce biases or noise
- Computational overhead of generating multiple candidates per document not extensively analyzed
- Framework's generalization to other NLP tasks beyond document-level IE remains unexplored

## Confidence

**Performance claims**: High confidence - Experimental results are well-supported with consistent improvements across multiple datasets and settings

**Reasoning model superiority**: Medium confidence - While results show clear improvements, the analysis of specific reasoning patterns could be more comprehensive

**Zero-shot and cross-lingual transfer claims**: Medium confidence - Promising results but evaluation could benefit from more diverse language pairs and domain variations

## Next Checks

1. Conduct ablation studies to determine the optimal number of candidates to generate per document, analyzing the trade-off between performance gains and computational overhead.

2. Implement a comprehensive error analysis to identify failure modes of the selection module, particularly focusing on cases where it selects suboptimal candidates.

3. Test the framework's robustness by evaluating on out-of-domain documents and comparing performance degradation against baseline methods.