---
ver: rpa2
title: 'Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with
  Goal-Conditioned Reinforcement Learning'
arxiv_id: '2601.14693'
source_url: https://arxiv.org/abs/2601.14693
tags:
- symbolic
- expressions
- search
- expression
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of symbolic regression, where
  the goal is to discover compact mathematical expressions that model the relationship
  between input and output variables. The key problem is that most existing search-based
  methods rely on fitting error, which can lead to ambiguous search directions and
  convergence to suboptimal expressions due to structurally different expressions
  exhibiting similar error values.
---

# Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.14693
- **Source URL**: https://arxiv.org/abs/2601.14693
- **Reference count**: 8
- **Primary result**: EGRL-SR outperforms state-of-the-art methods in symbolic regression recovery rate and robustness using goal-conditioned RL with hindsight experience replay

## Executive Summary
This paper addresses the fundamental limitation of error-based optimization in symbolic regression: structurally different expressions can exhibit similar error values, leading to ambiguous search directions and convergence to suboptimal solutions. The authors propose EGRL-SR, a novel framework that formulates symbolic regression as a goal-conditioned reinforcement learning problem. By leveraging hindsight experience replay to convert failed trajectories into successful training samples and using an all-point satisfaction binary reward function, EGRL-SR focuses on structural patterns rather than error minimization. Experiments demonstrate superior performance on standard benchmarks with consistent recovery rates across expression complexities.

## Method Summary
EGRL-SR formulates symbolic regression as a goal-conditioned Markov decision process where the agent constructs expressions in postfix notation. The state consists of the current numeric output concatenated with the target output, while actions represent variables and operators from a predefined set. The framework employs hindsight experience replay to relabel failed trajectories with achieved intermediate outputs as new goals, enabling the action-value network to learn generalizable mapping patterns. An all-point satisfaction binary reward function assigns reward=1 only if the constructed expression meets accuracy threshold across all samples, otherwise reward=0. Structure-guided heuristic exploration partitions the search space into multiple directions based on structural attributes (unary operator count, nesting depth, sub-expression length), each with its own value network and replay buffer. The agent uses ε-greedy action selection combining structural constraints with learned value estimates.

## Key Results
- EGRL-SR consistently achieves higher exact expression recovery rates than state-of-the-art methods across Nguyen, Livermore, and Keijzer benchmarks
- The framework demonstrates superior robustness to noisy data compared to error-based optimization approaches
- EGRL-SR successfully recovers more complex expressions (length > 17) under the same search budget constraints
- Ablation studies validate the effectiveness of the action-value network, all-point satisfaction reward function, and structure-guided exploration strategy

## Why This Works (Mechanism)

### Mechanism 1: Hindsight Experience Replay for Pattern Generalization
Failed expression construction trajectories can be converted into successful training samples by relabeling achieved outputs as goals. HER replaces the original target y with intermediate outputs actually reached during search, transforming failed trajectories into goal-conditioned success experiences. This allows the action-value network to learn generalizable x→y mapping patterns across diverse targets rather than optimizing for a single target's error. The core assumption is that structural construction patterns transfer across different target outputs, where the way an expression transforms x to some y' contains reusable information for reaching similar y values.

### Mechanism 2: All-Point Satisfaction Binary Reward (APSR)
A sparse binary reward based on full-dataset accuracy prevents structurally incorrect expressions with low error from receiving misleading high rewards. APSR assigns reward=1 only if the constructed expression meets a predefined accuracy threshold across ALL input samples; otherwise reward=0. This eliminates gradient ambiguity from error-based rewards where structurally different expressions yield similar error values, forcing the network to learn structural correctness rather than local error minimization. The core assumption is that structurally correct expressions will achieve all-point satisfaction, while structurally incorrect expressions that happen to have low error on training data will fail on at least some points.

### Mechanism 3: Structure-Guided Heuristic Exploration (SGHE)
Partitioning the search space by structural attributes with independent value networks per partition improves coverage and prevents convergence to structurally biased local optima. SGHE defines exploration directions by combining three structural attributes: number of unary operators, nesting depth, and sub-expression length range. Each direction has its own action-value network and replay buffer. The core assumption is that different structural patterns require different exploration strategies, and a single value network will bias toward whichever structural patterns produce low-error expressions early in training.

## Foundational Learning

- **Concept**: **Goal-Conditioned Reinforcement Learning (GCRL)**
  - Why needed here: Standard RL requires a fixed reward function; GCRL allows the same policy to pursue different goals by conditioning on goal state. This is essential because symbolic regression targets vary per problem instance.
  - Quick check question: Can you explain how a universal value function Q(s, a, g) differs from a standard Q(s, a) function, and why the goal parameter enables multi-task learning?

- **Concept**: **Off-Policy Learning with Experience Replay**
  - Why needed here: HER generates experiences that were not produced by the current policy (goals are relabeled after the fact). On-policy algorithms (PPO, A3C) assume training data comes from the current policy and are incompatible.
  - Quick check question: Why would on-policy algorithms fail when trained on HER-relabeled experiences? What assumption do they make that HER violates?

- **Concept**: **Postfix Expression Generation**
  - Why needed here: The paper uses postfix (Reverse Polish Notation) for expression construction, where operands precede operators. This enables observing intermediate numeric outputs at each step for state computation.
  - Quick check question: Given the postfix sequence [x1, x2, +, sin, x3, *], can you compute the state after each token and explain why this representation supports incremental state tracking?

## Architecture Onboarding

- **Component map**:
  Agent (ε-greedy selector) -> Value Networks (one per SGHE direction) -> Environment (expression evaluator, APSR reward, goal checker)
  Value Networks: State encoder (s = x_now || y) -> Dueling streams (V(s) and A(s,a)) -> Output (Q(s,a) per action)
  Experience Management: Replay Buffers (one per direction) -> HER Relabeler (failed trajectory → successful with achieved goal)

- **Critical path**:
  1. Initialize N value networks (one per structural direction) with random weights
  2. For each target expression: sample initial x values, compute target y
  3. Agent constructs expression token-by-token in postfix form
  4. At each step: compute x_now (intermediate output), form state s = x_now || y
  5. With probability 1-ε: select action via Q(s,a); with probability ε: use SGHE-constrained exploration
  6. On failure: HER relabels trajectory with achieved x_now as new goal, stores in buffer
  7. Sample mini-batches from buffers, train Double Dueling DQN using equation (3)

- **Design tradeoffs**:
  - **More SGHE directions vs. compute budget**: Each direction needs its own network and buffer. Paper uses 8 directions with 200K steps each (1.6M total). More directions improve coverage but increase memory/training time.
  - **ε value vs. learning efficiency**: Lower ε trusts the learned network more. Figure 4 shows optimal TtS at ε≈0.2-0.4. Too low ε causes premature exploitation; too high wastes budget on random structures.
  - **Expression length threshold vs. search space**: Longer thresholds enable complex expressions but exponentially expand search space. Paper sets limits per direction based on structural attributes.

- **Failure signatures**:
  - **Network converges to low-reward expressions**: APSR too strict for noisy data; consider relaxing threshold or switching to shaped reward for initial warmup.
  - **All SGHE directions produce similar structures**: Structural attributes don't sufficiently partition space; redefine partitions or increase ε for diversity.
  - **Recovery rate drops sharply for length > 17**: Search budget insufficient; expression space exceeds what value networks can learn to navigate.
  - **HER experiences dominate buffer**: Failed trajectories vastly outnumber successes; sample ratio balancing needed to prevent negative learning.

- **First 3 experiments**:
  1. **Validate action-value guidance**: Run ablation with ε=1.0 (pure exploration) vs ε=0.2 on expressions from Table 2. Plot Trajectories-to-Success. Expected: ε=0.2 achieves lower TtS, confirming learned network provides useful guidance.
  2. **Isolate APSR contribution**: Replace APSR with r=1/(1+nRMSE) on Nguyen dataset (same setup as Table 2). Compare exact recovery rates. Expected: continuous reward causes recovery drops on structurally diverse expressions (M1, M2, M3).
  3. **Test noise robustness boundary**: Add Gaussian noise with σ proportional to RMSE(y) at levels [0, 0.01, 0.02, 0.05, 0.1]. Plot recovery rate curves. Expected: EGRL-SR maintains advantage up to ~0.05 noise level; beyond this, APSR provides no signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the EGRL-SR framework be adapted to efficiently recover expressions containing complex constants without relying on indirect combinations of variables and operators?
- **Basis in paper**: The Conclusion explicitly identifies the current indirect representation of constants as a limitation that increases search space and impairs performance, stating that future work will focus on strategies for recovering such expressions within the GCRL framework.
- **Why unresolved**: The current method constructs constants by combining variables and operators (e.g., generating 1 by $x/x$), which is inefficient and fails for arbitrary constants; the authors have not yet integrated a mechanism for direct constant handling.
- **What evidence would resolve it**: A modification of the framework that includes a specific mechanism for constant optimization (e.g., placeholder tokens with gradient-based fitting) and a demonstration of its performance on benchmarks specifically designed to test complex constant recovery.

### Open Question 2
- **Question**: How does the performance and convergence stability of the All-Point Satisfaction Reward (APSR) vary with the selection of the predefined accuracy threshold across different data scales?
- **Basis in paper**: The APSR function relies on a "predefined accuracy threshold" to generate binary rewards. While the paper demonstrates robustness to noise (RQ2), it does not analyze the sensitivity of the method to the specific value of this threshold, which is critical for balancing the sparse reward signal against the continuous nature of the error landscape.
- **Why unresolved**: The paper treats the threshold as a fixed hyperparameter; however, if the threshold is too tight relative to the data scale, the agent may never receive a positive reward, whereas a loose threshold might reinforce structurally incorrect expressions.
- **What evidence would resolve it**: An ablation study or sensitivity analysis showing the recovery rate and convergence speed of EGRL-SR when the accuracy threshold parameter is varied across datasets with different output magnitudes and noise levels.

### Open Question 3
- **Question**: Does the goal-conditioned state representation ($s = x_{now} || y$) and the action-value network scale effectively to high-dimensional problems with numerous input variables?
- **Basis in paper**: The state is defined as the concatenation of the current numeric output and the target output. While multi-variable tasks are mentioned, the experimental validation relies on standard low-dimensional benchmarks (Nguyen, Livermore, Keijzer), leaving the method's efficiency in high-dimensional search spaces unstated.
- **Why unresolved**: Concatenating high-dimensional target vectors ($y$) into the state space can increase the complexity of the learning task for the value network, potentially leading to the "curse of dimensionality" not observed in the low-dimensional tests presented.
- **What evidence would resolve it**: Experimental results applying EGRL-SR to datasets with a significantly larger number of input features (e.g., >10 variables) to validate if the structural guidance remains effective when the state representation becomes high-dimensional.

## Limitations

- The framework's performance depends heavily on the predefined accuracy threshold for APSR, which may be too strict for noisy real-world data
- The 1.6M-step budget may be insufficient for recovering highly complex expressions beyond those tested in benchmarks
- The current approach handles constants only through indirect combinations of variables and operators, limiting efficiency for expressions requiring specific numerical values

## Confidence

- **High**: The core RL framework using GCRL with HER is technically sound and implementable
- **Medium**: The APSR reward function's effectiveness in preventing error-based ambiguity is supported by ablation results but not comprehensively validated across noise levels
- **Low**: The optimal number and configuration of SGHE directions is not systematically explored beyond the chosen 8-direction setup

## Next Checks

1. **Noise Robustness Boundary**: Systematically test EGRL-SR on Nguyen benchmarks with increasing Gaussian noise levels (0-10% of target variance). Track exact recovery rate to identify the noise threshold where APSR fails to provide learning signals.

2. **SGHE Direction Sensitivity**: Run EGRL-SR with varying numbers of exploration directions (4, 8, 16, 32) on a subset of complex expressions. Measure recovery rate and compute resource usage to establish the tradeoff between directional coverage and computational efficiency.

3. **HER Relabeling Strategy Comparison**: Compare standard HER (random intermediate goals) against prioritized HER (choosing goals closest to target) and curriculum HER (starting with simple goals, progressing to complex). Evaluate impact on convergence speed and final recovery quality.