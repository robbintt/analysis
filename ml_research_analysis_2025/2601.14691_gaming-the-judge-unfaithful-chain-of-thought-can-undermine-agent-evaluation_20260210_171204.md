---
ver: rpa2
title: 'Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation'
arxiv_id: '2601.14691'
source_url: https://arxiv.org/abs/2601.14691
tags:
- agent
- task
- reasoning
- action
- manipulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly used as judges to
  evaluate agent performance in non-verifiable settings, relying on agent trajectories
  that include chain-of-thought (CoT) reasoning. This paradigm assumes that agent
  CoTs faithfully reflect both reasoning and environment state.
---

# Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation

## Quick Facts
- **arXiv ID**: 2601.14691
- **Source URL**: https://arxiv.org/abs/2601.14691
- **Reference count**: 40
- **Key outcome**: Systematic CoT rewriting inflates VLM judge false positive rates by up to 90% across 800 web trajectories, with content-based manipulations consistently more effective than style-based approaches.

## Executive Summary
This paper demonstrates that LLM-based judges evaluating agent performance are vulnerable to manipulation through chain-of-thought (CoT) rewriting. By systematically altering agent reasoning traces while holding actions and observations constant, researchers show that state-of-the-art VLM judges can be gamed to predict success for failed tasks. Five manipulation strategies were tested across 800 trajectories spanning diverse web tasks, revealing that judges systematically accept declarative CoT statements as factual without adequate cross-verification against observable evidence. The findings expose a fundamental vulnerability in current LLM evaluation paradigms where manipulated reasoning can systematically bias judges toward success predictions even when tasks remain incomplete.

## Method Summary
The study manipulates CoTs in failed agent trajectories across 800 web tasks (300 from AgentRewardBench, 500 newly collected) using five rewriting strategies ranging from style-based (competence boosting, reflective reasoning) to content-based (progress fabrication, environment blaming, task reinterpretation). These rewrites are applied post-hoc while keeping actions and observations unchanged. The manipulated trajectories are then evaluated by nine VLM judges using standardized prompts. Success/failure judgments are compared between original and manipulated CoTs to measure Judgment Flip Rate (JFR) and changes in False Positive Rate (FPR). The evaluation spans different judge configurations including standard prompting, manipulation-aware prompting, rubric-based evaluation, and judge-time scaling approaches.

## Key Results
- Progress Fabrication manipulation increases FPR by 20-30 points on GPT-4o and GLM-4.1V (70-100% relative increase)
- Style-only strategies like Competence Boosting induce smaller but non-trivial increases of 5-10 points
- Mitigation strategies reduce but do not eliminate susceptibility, with manipulation-aware prompting and rubrics reducing FPR at the cost of 10-20 point recall drops on genuine successes
- CoT susceptibility persists even with text captions instead of screenshots, indicating the vulnerability is not purely a multimodal grounding issue

## Why This Works (Mechanism)

### Mechanism 1: Truth Bias in Declarative Reasoning Acceptance
- Claim: VLM judges systematically accept declarative statements in chain-of-thought as factual without cross-verifying against observable evidence.
- Mechanism: Content-based manipulations inject false claims about task state or environment conditions. Judges treat these narrated events as evidence of progress, creating a truth bias where "said" is conflated with "done."
- Evidence: Progress Fabrication increases FPR by 20-30 points on GPT-4o and GLM-4.1V (70-100% relative increase).

### Mechanism 2: Fluency-Based Credibility Heuristic
- Claim: Judges assign higher credibility to well-articulated, confident reasoning independent of content accuracy.
- Mechanism: Style-based manipulations alter only presentation—confidence, deliberateness, methodological framing—without adding false claims. This exploits a fluency heuristic where surface quality proxies for correctness.
- Evidence: Competence Boosting amplifies confidence, decisiveness, and fluency, inducing 5-10 point FPR increases.

### Mechanism 3: Observation-Reasoning Decoupling in Multimodal Context
- Claim: Judges fail to maintain tight coupling between textual reasoning and visual/behavioral evidence when both are presented.
- Mechanism: When trajectory includes screenshots, actions, and CoT, judges overweight the narratively coherent CoT at the expense of disconfirming visual evidence.
- Evidence: Progress Fabrication increases FPR by 15-25 points even when captions are provided, showing CoT susceptibility is not purely a multimodality issue.

## Foundational Learning

- **Concept: LLM-as-a-Judge paradigm**
  - Why needed here: The entire vulnerability assumes this evaluation architecture where LLMs assess agent trajectories. Without understanding that judges ingest multimodal trajectories (observations + actions + CoT), the attack surface is unclear.
  - Quick check question: Can you explain why non-verifiable tasks require LLM judges rather than deterministic verification?

- **Concept: Chain-of-Thought (un)faithfulness**
  - Why needed here: The paper's core premise is that CoT does not reliably reflect actual reasoning or environment state. Prior work shows CoT can be post-hoc rationalization; this paper extends to adversarial manipulation.
  - Quick check question: What does it mean for a CoT to be "unfaithful" to environment state versus unfaithful to internal reasoning?

- **Concept: False Positive Rate (FPR) and Judgment Flip Rate (JFR)**
  - Why needed here: The paper quantifies manipulation success via FPR inflation and JFR—the probability a correct failure judgment flips to incorrect success. These metrics distinguish "judge becomes stricter" from "judge is systematically gamed."
  - Quick check question: If a mitigation reduces FPR but also reduces recall on true successes, what tradeoff does this represent?

## Architecture Onboarding

- **Component map**: Agent trajectory -> CoT manipulation module -> VLM Judge -> Mitigation layers
- **Critical path**: 1) Collect failed trajectories from web agent; 2) Apply CoT rewriting strategy to each reasoning step; 3) Feed manipulated trajectory to judge; 4) Compare judgment flip: correct FAILURE → incorrect SUCCESS; 5) Measure ΔFPR and JFR across manipulation strategies
- **Design tradeoffs**:
  - Robustness vs. Recall: Manipulation-aware prompting and rubrics reduce FPR but drop recall on genuine successes by 10-20 points
  - Including vs. excluding CoT: Removing CoT eliminates attack surface but hurts recall by up to 13 points—CoT provides valuable signal for interpreting actions
  - Judge-time compute: Parallel scaling and sequential scaling provide partial robustness with diminishing returns; 32 samples reduce but don't eliminate high FPRs
- **Failure signatures**:
  - Judge accepts CoT claims contradicted by action log (e.g., "successfully selected March 30" when action shows March 31)
  - Judge rewards effort/exploration over task completion (Task Reinterpretation)
  - Judge accepts workarounds as substitutes for deliverables (Environment Blaming)
  - High-confidence, fluent CoT overrides disconfirming screenshots
- **First 3 experiments**:
  1. Baseline susceptibility test: Run 100 failed trajectories through your VLM judge with original CoT, then with Progress Fabrication rewrites. Compute ΔFPR and JFR to establish vulnerability magnitude.
  2. Mitigation ablation: Compare standard prompt vs. manipulation-aware prompt vs. WebJudge rubrics on the same manipulated trajectories. Plot FPR reduction against recall loss to visualize the robustness-recall frontier.
  3. Cross-modal test: Provide text captions instead of screenshots for a subset of trajectories. Test whether Progress Fabrication effectiveness changes—this diagnoses whether vulnerability is multimodal or text-based.

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown task boundary definitions: Specific prompts and success criteria for newly collected tasks are not fully specified
- Model access constraints: GPT-5-mini and o4-mini are cited but not publicly available; substitution with available models may not fully replicate results
- Silver labeling approximation: GPT-5-high silver labels are approximated with GPT-4o, introducing potential bias in ground truth labels

## Confidence
- **High confidence**: The fundamental vulnerability of LLM judges to CoT manipulation is well-supported by consistent FPR increases (15-30 points) across multiple judges and manipulation strategies
- **Medium confidence**: The relative effectiveness ranking of manipulation strategies (content > style) holds across judges but may shift with different task domains or judge architectures
- **Medium confidence**: Mitigation effectiveness is demonstrated but with clear tradeoffs—reduced FPR comes at 10-20 point recall cost on genuine successes

## Next Checks
1. Cross-task generalizability test: Apply the manipulation framework to non-web tasks (e.g., mathematical reasoning, code generation) to verify vulnerability extends beyond the reported domains
2. Judge architecture stress test: Compare standard LLM judges against architectures with explicit CoT-observation verification layers to quantify the benefit of structural safeguards
3. Temporal stability evaluation: Re-run the manipulation and mitigation experiments with different judge versions (e.g., Claude 3.5 vs. Claude 4) to assess whether observed vulnerabilities persist across model updates