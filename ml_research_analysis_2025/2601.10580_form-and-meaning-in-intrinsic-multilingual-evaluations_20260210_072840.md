---
ver: rpa2
title: Form and Meaning in Intrinsic Multilingual Evaluations
arxiv_id: '2601.10580'
source_url: https://arxiv.org/abs/2601.10580
tags:
- language
- metrics
- languages
- computational
- latn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of comparing intrinsic evaluation
  metrics like perplexity and bits-per-character across languages in multilingual
  language modeling. The authors argue that these metrics, while useful in monolingual
  settings, rest on assumptions that break down when comparing parallel sentences
  across languages due to differences in form (even when meaning is consistent).
---

# Form and Meaning in Intrinsic Multilingual Evaluations

## Quick Facts
- arXiv ID: 2601.10580
- Source URL: https://arxiv.org/abs/2601.10580
- Reference count: 37
- Key outcome: Intrinsic metrics (NLL, BPC, MRR) are inconsistent when comparing parallel sentences across languages, even when meaning is preserved, due to differences in form rather than meaning.

## Executive Summary
This paper challenges the assumption that intrinsic evaluation metrics like perplexity and bits-per-character can be compared across languages in multilingual language modeling. Through experiments on parallel corpora (EuroParl and UNPC) and paraphrase data (WMT-19), the authors demonstrate that these metrics produce inconsistent values even when comparing semantically equivalent sentences in different languages. They find that approximately 47% of English-German paraphrase pairs show inconsistent NLL rankings between monolingual models, and all metrics become inconsistent when comparing average scores across different paraphrase splits. The authors connect these findings to the form-meaning debate in linguistics, arguing that current intrinsic metrics may be unsuitable for questions like "which language is easier to model?"

## Method Summary
The authors train causal language models (Llama-style) on multi-parallel corpora (EuroParl with 21 languages, UNPC with 6 languages) using both monolingual and multilingual setups. They evaluate using six intrinsic metrics (NLL, PPL, BPC, BPEC, IP, MRR) on FLORES-200 and test consistency on WMT-19 EN-DE paraphrases (4 German references per English source). For consistency testing, they compute metrics on paraphrase pairs and check if rankings are stable both at the sample level (is English NLL within the range of German paraphrases?) and split level (are German splits consistently easier/harder than English?). They also analyze BPC/BPEC/IP values across languages to assess tokenization effects.

## Key Results
- NLL is inconsistent for 47% of samples when comparing English to German paraphrases using monolingual models
- All metrics become inconsistent when comparing average scores across different splits of paraphrase data
- BPC shows Chinese as a clear outlier, likely due to tokenization artifacts rather than linguistic complexity
- Multilingual models systematically show higher NLL across all languages compared to monolingual counterparts

## Why This Works (Mechanism)

### Mechanism 1: Form-Meaning Mismatch in Intrinsic Metrics
Intrinsic metrics measure information-theoretic content (token predictability), not semantic meaning. Parallel sentences preserve semantic meaning but differ in form (word order, morphology, tokenization). Since metrics operationalize "surprise" at the token level, form differences propagate directly into metric values. The field's assumption that parallel meaning implies fair comparison is challenged by this form-meaning distinction.

### Mechanism 2: Paraphrase Inconsistency Within Languages
Intrinsic metrics produce inconsistent values for paraphrases within a single language. Given four German paraphrases of one English sentence, NLL values span ranges that overlap with English in ~47-51% of samples. Ranking (German above/below English) flips depending on which paraphrase is selected, undermining the assumption that metrics measure stable "language difficulty."

### Mechanism 3: Distribution Mismatch in Monolingual vs. Multilingual Comparisons
Monolingual models learn different distributions even when trained on parallel-aligned corpora. Each monolingual model sees only its target language corpus, with different tokenizers and vocabularies. Multilingual models merge distributions via shared vocabulary, enabling more comparable evaluation—but tokenization and script differences remain. This distribution mismatch makes cross-lingual metric comparisons inherently confounded.

## Foundational Learning

- **Concept: Cross-entropy loss and its relationship to NLL, PPL, BPC**
  - Why needed here: All intrinsic metrics are transformations of NLL (cross-entropy). Understanding that PPL = exp(NLL) and BPC rescales to bits per character is essential for interpreting Figure 2 and Table 3.
  - Quick check question: If Model A has NLL = 5.0 and Model B has PPL = 200, which assigns higher probability to the test set on average?

- **Concept: Tokenization and vocabulary allocation in multilingual models**
  - Why needed here: BPC, BPEC, and IP directly depend on segmentation. Chinese shows as a "clear outlier" in Figure 2 likely due to script/tokenization, not inherent difficulty.
  - Quick check question: Why might BPC favor character-based languages over morphologically rich languages with subword tokenization?

- **Concept: The form-meaning distinction in linguistics (Frege, Bender & Koller 2020)**
  - Why needed here: The paper connects its findings to debates about whether language models learn form vs. meaning. This frames why intrinsic metrics—grounded in form—cannot capture semantics.
  - Quick check question: If a model perfectly memorizes all training text (form), would it have low perplexity? Would it understand meaning?

## Architecture Onboarding

- **Component map:**
  Data pipeline: Multi-parallel corpus alignment (EuroParl, UNPC) → train/dev/test splits → FLORES-200 for evaluation, WMT-19 paraphrases for consistency tests → Tokenizers: Monolingual BPE (32k vocab) vs. Multilingual BPE (150k vocab), byte-level fallback → Models: Tiny Llama-style causal LMs (~143M mono, ~279M multi) → Metrics layer: NLL → PPL, BPC, BPEC, IP, MRR computed per-language per-checkpoint

- **Critical path:**
  1. Align multi-parallel data (pivot language determines row overlap)
  2. Train tokenizers and models separately for mono vs. multi setups
  3. Evaluate on FLORES-200; for consistency, run WMT-19 paraphrase experiments
  4. Compute sample-level and split-level consistency (ranking checks, row-wise sorting)

- **Design tradeoffs:**
  Small models limit absolute metric values but authors argue inconsistency patterns scale; not proven at larger scales. Multi-parallel corpora constrain language coverage (21 for EuroParl, 6 for UNPC); results may not generalize to all languages or domains. BPEC/IP normalize to English, embedding an anglocentric baseline—useful for relative comparison but not absolute claims.

- **Failure signatures:**
  Metric rankings flip when test split composition changes (e.g., "easiest" vs. "hardest" German paraphrase splits yield opposite conclusions vs. English). BPC outliers for non-Latin scripts (Chinese, Arabic) likely reflect tokenization artifacts, not linguistic complexity. Multilingual models show higher NLL across all languages compared to monolingual—suggesting capacity dilution or vocabulary pressure.

- **First 3 experiments:**
  1. Reproduce paraphrase consistency on a different language pair (e.g., English-French) to test whether the ~50% inconsistency rate generalizes beyond EN-DE
  2. Vary tokenizer vocabulary size (e.g., 50k, 100k, 200k for multilingual) to measure sensitivity of BPC/BPEC/IP to vocabulary allocation; expect BPC to stabilize as vocab grows
  3. Scale model size (e.g., 350M, 700M parameters) to test author's claim that inconsistency patterns persist at scale—verify whether sample-level inconsistency remains ~50% or decreases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a statistical linguistic law be formulated to account for paraphrases or semantics?
- Basis: The authors explicitly state in Section 7 that "How to do this is another question entirely, one that we do not have an answer to," regarding the creation of a law robust to form changes.
- Why unresolved: Existing laws (e.g., Zipf's law) hold for "gibberish" and do not capture semantic consistency, while current intrinsic metrics are sensitive to form rather than meaning.
- What evidence would resolve it: The derivation of a formal linguistic law that yields identical scores for sentences with consistent meaning but different forms (paraphrases).

### Open Question 2
- Question: Do specific technical decisions or languages exist that result in consistent intrinsic metrics robust to paraphrasing?
- Basis: In Section 7, the authors suggest "It could be that there exists a set of technical decisions and languages... that do show consistent values," noting their findings are limited to their specific experimental setup.
- Why unresolved: The paper only tested a specific architecture (tiny Llama) and tokenization scheme; it remains unknown if scaling model size or changing segmentation could neutralize the observed inconsistencies.
- What evidence would resolve it: An empirical study identifying a combination of architecture, tokenization, and language pairs where NLL/BPC scores are statistically consistent across parallel paraphrases.

### Open Question 3
- Question: Can we quantify cross-lingual information differences using morphosyntactic features to create a fairer evaluation standard?
- Basis: The authors cite Sproat et al. (2014) in Section 7, asking "Which languages convey the most information in a given amount of space?" and noting the required database was never completed.
- Why unresolved: Current metrics rely on information theory which conflates form and meaning; a "universal" standard requires a database of "close translations" with detailed feature marking.
- What evidence would resolve it: The construction of a multi-parallel database annotated with morphosyntactic and morphosemantic features to explicitly quantify information density differences.

## Limitations

- The findings are based on relatively small models (143M-279M parameters) and may not generalize to larger, more capable models
- The language coverage is limited to 21 languages for EuroParl and 6 for UNPC, heavily skewed toward European languages
- The WMT-19 EN-DE paraphrase dataset represents only one language pair and one specific type of paraphrasing

## Confidence

**High Confidence:**
- Intrinsic metrics produce inconsistent values when comparing parallel sentences across languages
- Multilingual models show systematically higher NLL than monolingual counterparts
- BPC is particularly sensitive to tokenization and script differences (Chinese as clear outlier)
- Split-level consistency can mask sample-level inconsistency

**Medium Confidence:**
- The ~47% inconsistency rate for EN-DE paraphrases generalizes to other language pairs
- The form-meaning mismatch is the primary driver of metric inconsistency (rather than other factors like corpus/domain mismatch)
- Small models are sufficient to demonstrate the core inconsistency phenomena

**Low Confidence:**
- Larger models would show identical inconsistency patterns
- The findings invalidate all cross-lingual metric comparisons (rather than suggesting careful interpretation is needed)
- Alternative metrics could fully resolve these consistency issues

## Next Checks

1. Replicate paraphrase consistency across diverse language pairs: Test the ~50% inconsistency rate on language pairs with different typological distances (e.g., English-French, English-Arabic, English-Chinese) and different script families to determine whether the finding is language-pair specific or a general phenomenon.

2. Vary tokenizer vocabulary size systematically: Train multiple multilingual tokenizers with different vocabulary sizes (e.g., 50k, 100k, 200k) and measure the impact on BPC/BPEC/IP consistency to quantify how much tokenization artifacts drive the observed inconsistencies versus genuine distributional differences.

3. Scale model size experimentally: Train both monolingual and multilingual models at multiple scales (e.g., 350M, 700M, 1.4B parameters) and measure whether the inconsistency rates decrease with model capacity to test the authors' claim that patterns would scale and help determine if small model artifacts are driving the results.