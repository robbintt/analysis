---
ver: rpa2
title: Relative Positioning Based Code Chunking Method For Rich Context Retrieval
  In Repository Level Code Completion Task With Code Language Model
arxiv_id: '2510.08610'
source_url: https://arxiv.org/abs/2510.08610
tags:
- code
- chunks
- context
- completion
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of effective context collection
  for repository-level code completion using large language models. The proposed method
  preprocesses code repositories into smaller chunks and retrieves relevant chunks
  based on syntactic and semantic similarity, enhanced with relative positioning (next
  chunks for prefix context, previous chunks for suffix context).
---

# Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model

## Quick Facts
- arXiv ID: 2510.08610
- Source URL: https://arxiv.org/abs/2510.08610
- Reference count: 17
- Third place in Kotlin code completion competition with chrF 0.660, fourth place in Python with chrF 0.636

## Executive Summary
This paper addresses repository-level code completion by proposing a context retrieval method that preprocesses code repositories into overlapping chunks and retrieves relevant chunks using both syntactic and semantic similarity measures. The key innovation is relative positioning, where the system retrieves neighboring code chunks based on cursor position—next chunks for prefix context and previous chunks for suffix context. This approach enhances large language models' understanding by providing coherent contextual information for fill-in-the-middle completion tasks. The method achieved competitive results in JetBrains Research code completion competitions, placing third for Kotlin and fourth for Python.

## Method Summary
The approach preprocesses code repositories by splitting source files into n-line chunks with m-line overlap, storing prev/next pointers for each chunk. It generates embeddings using `all-MiniLM-L6-v2` and indexes them in FAISS. For retrieval, it employs a LangChain Ensemble Retriever combining BM25 (weight 0.2) and FAISS (weight 0.8), reranked by Reciprocal Rank Fusion. The key innovation is relative positioning: when retrieving chunks for prefix queries, it fetches similar chunks plus their next chunks; for suffix queries, it retrieves similar chunks plus their previous chunks. The final context consists of the completion file, recent files, and retrieved chunks with their relative neighbors.

## Key Results
- Achieved chrF score of 0.660 for Kotlin code completion (third place in competition)
- Achieved chrF score of 0.636 for Python code completion (fourth place in competition)
- Demonstrated that code chunking and relative positioning significantly enhance code completion performance
- Showed that combining sparse (BM25) and dense (FAISS) retrieval captures complementary relevance signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving neighboring code chunks based on cursor position improves fill-in-the-middle completion because adjacent chunks encode likely continuations and antecedents.
- Mechanism: For prefix queries, retrieves similar chunks plus their next chunks; for suffix queries, retrieves similar chunks plus their prev chunks. This leverages local coherence of code.
- Core assumption: Code exhibits local sequential dependencies where adjacent chunks contain related logic.
- Evidence anchors: Section II-C describes using next/prev chunks; Section I states chunking and relative positioning improve performance; related work confirms retrieval-augmented context helps.
- Break condition: If code is highly non-sequential (reflection, dynamic dispatch without local patterns), adjacency signals may introduce noise.

### Mechanism 2
- Claim: Combining sparse (BM25) and dense (embedding) retrieval captures complementary relevance signals better than either alone.
- Mechanism: BM25 matches exact keywords and identifiers; FAISS matches semantic similarity via vector embeddings. Ensemble retriever reranks using Reciprocal Rank Fusion with weights 0.2 (BM25) and 0.8 (semantic).
- Core assumption: Semantic similarity is more important than syntactic overlap for code completion; keywords alone miss conceptual relevance.
- Evidence anchors: Section II-B explains 4:1 emphasis on semantic similarity; related work (LLavaCode, GrepRAG) supports hybrid approaches.
- Break condition: If embedding model is not code-specific, semantic retrieval may misrank domain-specific terms.

### Mechanism 3
- Claim: Chunking with line overlap preserves cross-chunk context that would otherwise be lost at boundaries.
- Mechanism: Files are split into n-line chunks with m-line overlap; consecutive chunks share context, and prev/next pointers are stored for relative positioning retrieval.
- Core assumption: Overlap prevents semantic breakage at chunk boundaries, maintaining coherence for retrieval.
- Evidence anchors: Section II-A describes overlapping chunking; Section IV acknowledges naive chunking may break coherent code; corpus suggests AST-based chunking may outperform naive methods.
- Break condition: If chunks span disconnected logic (multiple functions in one chunk), retrieval precision degrades.

## Foundational Learning

- Concept: **BM25 (Best Matching 25)**
  - Why needed here: Understanding sparse retrieval helps diagnose why keyword matching alone is insufficient and why semantic augmentation is necessary.
  - Quick check question: Given a query "getUserById" and documents ["fetchUser", "getUserByIdentifier", "retrieveUserByPK"], which would BM25 rank highest and why?

- Concept: **Vector Embeddings & FAISS**
  - Why needed here: Dense retrieval underpins the semantic similarity component; understanding embedding spaces explains how conceptual matches surface.
  - Quick check question: If two code snippets have high cosine similarity in embedding space but share no keywords, what does that imply about their relationship?

- Concept: **Reciprocal Rank Fusion (RRF)**
  - Why needed here: The ensemble retriever merges ranked lists from BM25 and FAISS; RRF determines final ordering.
  - Quick check question: If BM25 ranks document A at position 1 and FAISS ranks it at position 10, while document B is ranked 5 by both, which document wins under RRF with equal weights?

## Architecture Onboarding

- Component map: Preprocessor -> Embedding Generator -> Ensemble Retriever -> Context Collector -> Final Context
- Critical path:
  1. Repository ingestion → chunking with overlap → pointer assignment
  2. Embedding generation → FAISS indexing
  3. At inference: prefix/suffix query → hybrid retrieval → relative chunk expansion → context assembly
- Design tradeoffs:
  - Chunk size vs. retrieval precision: Smaller chunks increase granularity but may fragment logic; overlap mitigates but increases storage
  - BM25 vs. semantic weight: 0.2/0.8 split is heuristic; may need tuning per language or codebase
  - Embedding model choice: General-purpose NLP model is fast but suboptimal for code; code-specific models may improve but increase latency
- Failure signatures:
  - Irrelevant context retrieved → check embedding quality, verify chunk boundaries
  - High latency → profile FAISS index size and ensemble retriever configuration
  - Context window overflow → tune top-k and chunk size; prioritize recent files and completion file
- First 3 experiments:
  1. Ablation on relative positioning: Disable prev/next expansion; measure chrF drop to quantify contribution
  2. Embedding model swap: Replace `all-MiniLM-L6-v2` with a code-specific model; compare retrieval relevance and chrF
  3. Chunk size sensitivity: Vary n (lines per chunk) and m (overlap); plot chrF vs. latency to find Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a dynamic chunking strategy based on Abstract Syntax Trees (AST) outperform the static line-based chunking method used in this study?
- Basis in paper: The authors state they envision AST-based chunking would provide better performance through structured code representation.
- Why unresolved: They utilized static line-based chunking and attempted AST-based chunking but could not implement it fully for Kotlin in time.
- What evidence would resolve it: Comparative evaluation measuring chrF scores using AST-based chunking versus static line-based chunking on the same tasks.

### Open Question 2
- Question: What is the individual contribution of each context component to the model's performance?
- Basis in paper: The authors note that an ablation study might provide deeper insight into how much improvement each component provides.
- Why unresolved: The paper presents a unified solution but does not isolate the specific impact of the proposed "relative positioning" technique.
- What evidence would resolve it: Results from a rigorous ablation study where specific components are systematically removed to observe changes in completion accuracy.

### Open Question 3
- Question: How does performance change when using code-specific embedding models compared to the general-purpose NLP model used?
- Basis in paper: The authors used a general NLP model due to limited computing resources but acknowledged code-specific models should improve performance.
- Why unresolved: Conclusions are based on embeddings from a model not specifically designed for code.
- What evidence would resolve it: Benchmarks comparing retrieval quality and chrF scores when swapping the general embedding model for a code-pretrained model.

### Open Question 4
- Question: How do different hyperparameter settings influence the trade-off between context richness and computational efficiency?
- Basis in paper: The authors suggest understanding how factors like chunk size and top-k values influence performance would be interesting.
- Why unresolved: The solution used fixed parameters determined heuristically for the competition.
- What evidence would resolve it: Parameter sweep analysis varying chunk sizes and k values to identify optimal settings for accuracy and efficiency.

## Limitations
- Critical hyperparameters (chunk size, overlap, final k value) are not specified, making faithful reproduction challenging
- General-purpose NLP embedding model may limit semantic retrieval quality for programming constructs
- Relative positioning mechanism's contribution is not independently validated through ablation studies

## Confidence
- **High confidence**: The baseline methodology of chunk-based retrieval with overlap and pointer storage is clearly described and follows established patterns
- **Medium confidence**: The ensemble retrieval approach combining BM25 and FAISS with specified weights is theoretically sound
- **Low confidence**: The specific impact of relative positioning on completion quality is not independently quantified

## Next Checks
1. Conduct ablation study comparing chrF scores with and without relative positioning (next/prev chunk retrieval) to quantify its individual contribution
2. Replace the general NLP embedding model with a code-specific model (e.g., CodeBERT) and measure changes in retrieval relevance and final chrF scores
3. Perform sensitivity analysis on chunk size and overlap parameters to identify optimal configurations that balance retrieval precision against computational overhead