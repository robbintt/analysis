---
ver: rpa2
title: Task Decoding based on Eye Movements using Synthetic Data Augmentation
arxiv_id: '2509.11547'
source_url: https://arxiv.org/abs/2509.11547
tags:
- data
- synthetic
- used
- movement
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of decoding observer tasks from
  eye movement data, a claim initially made by Yarbus. The core method involves generating
  synthetic eye movement data using CTGAN and its variants (CopulaGAN, Gretel AI-based
  CTGAN) to augment limited real data.
---

# Task Decoding based on Eye Movements using Synthetic Data Augmentation

## Quick Facts
- arXiv ID: 2509.11547
- Source URL: https://arxiv.org/abs/2509.11547
- Reference count: 38
- This paper uses GAN-based synthetic data augmentation to improve eye movement-based task decoding accuracy from 28% to 82%

## Executive Summary
This paper addresses the problem of decoding observer tasks from eye movement data, a claim initially made by Yarbus. The core method involves generating synthetic eye movement data using CTGAN and its variants (CopulaGAN, Gretel AI-based CTGAN) to augment limited real data. This synthetic data is then combined with real eye movement data from 16 participants performing 4 tasks on 20 images. Five classification algorithms (Random Forest, LightGBM, XGBoost, HistGradientBoosting, InceptionTime) are used to decode tasks. Results show significant improvement in accuracy when synthetic data is added. For example, accuracy increases from 28.1% using Random Forest on real data alone to 82% using InceptionTime with five times more synthetic data added. This demonstrates that task decoding is possible from eye movement data with high accuracy when sufficient augmented data is used.

## Method Summary
The method uses synthetic data augmentation to improve task decoding from eye movement data. Real eye movement data (320 samples from 16 participants performing 4 tasks on 20 images) is augmented with synthetic samples generated by CTGAN variants. Three synthetic generators are compared: CTGAN, CopulaGAN, and Gretel-based CTGAN (G-CTGAN). Synthetic data quality is validated using two-sample Kolmogorov-Smirnov tests. Five classifiers are evaluated: Random Forest, LightGBM, XGBoost, HistGradientBoosting, and InceptionTime. The model is trained on real + synthetic data and tested on held-out real data. Augmentation ratios range from 1× to 5× the original dataset size.

## Key Results
- Task decoding accuracy improves from 28.1% (Random Forest, real data only) to 82% (InceptionTime, 5× synthetic augmentation)
- G-CTGAN achieves the highest KS test score (0.9) and best classification accuracy across all classifiers
- All five classifiers show significant accuracy improvements with synthetic data augmentation
- Accuracy scales with augmentation volume, with diminishing returns beyond 5× augmentation

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data augmentation improves task decoding accuracy when real eye movement data is severely limited. The 320-sample dataset causes underfitting across all classifiers (28-36% accuracy). Synthetic samples generated by GAN-based models expand the training distribution, allowing models—particularly InceptionTime's temporal convolutions—to learn separable task-specific patterns rather than memorizing sparse examples. The improvement scales with augmentation volume (up to 5× tested).

### Mechanism 2
Spatio-temporal eye movement patterns encode distinguishable task information, supporting Yarbus' hypothesis. Four cognitive tasks (decade identification, memorization, people recognition, wealth estimation) elicit different scanning strategies reflected in fixation coordinates, durations, and pupil diameter. These form separable distributions in 4D feature space that classifiers can partition given sufficient data.

### Mechanism 3
Synthetic data quality (distributional fidelity to real data) determines augmentation effectiveness. CTGAN variants differ in capturing the joint distribution of eye movement features. Higher KS test scores indicate better marginal/joint distribution matching, meaning synthetic samples provide valid training signal. G-CTGAN (KS=0.9) outperforms CopulaGAN (KS=0.83) and CTGAN (KS=0.73) across all classifiers.

## Foundational Learning

- **Yarbus' Hypothesis and Eye-Tracking for Task Inference**
  - Why needed here: The paper positions itself as resolving a 50+ year debate. Understanding that different visual tasks produce characteristic scanpaths is the conceptual foundation.
  - Quick check question: Why might "determine decade" and "memorize picture" produce different fixation patterns on the same image?

- **GANs for Tabular Data (CTGAN/CopulaGAN)**
  - Why needed here: You cannot evaluate the method without understanding how CTGAN handles mixed continuous features via mode-specific normalization and how CopulaGAN adds CDF-based transformations.
  - Quick check question: Why would a standard image GAN architecture fail on eye movement time-series data?

- **Time Series Classification with InceptionTime**
  - Why needed here: InceptionTime achieves 82% vs. ~67% for tree methods. Understanding its multi-scale convolutional kernels explains why temporal gaze dynamics matter more than static feature aggregation.
  - Quick check question: What inductive bias does InceptionTime have that Random Forest lacks for sequential fixation data?

## Architecture Onboarding

- **Component map:**
  Real Data (320 samples × 4 features) -> Synthetic Generators (CTGAN → CopulaGAN → G-CTGAN) -> Quality Validation (KS test) -> Classifiers (RF, LGBM, XGB, HGB, InceptionTime) -> Evaluation (80/20 split, 5 repetitions)

- **Critical path:**
  1. Load and validate real eye movement data (4 features, verify temporal ordering per sample)
  2. Fit synthetic generator on real training split; generate S samples at target ratio
  3. Compute KS scores comparing synthetic vs. real distributions (diagnostic check)
  4. Concatenate real + synthetic; train classifier
  5. Evaluate exclusively on held-out real test set (never synthetic)
  6. Sweep augmentation ratios (1× to 5×) and generator variants

- **Design tradeoffs:**
  - Generator quality vs. complexity: G-CTGAN achieves best KS and accuracy but requires external service; CTGAN/CopulaGAN are self-hosted but underperform
  - Augmentation volume vs. synthetic artifact risk: More data helps up to 5×, but untested beyond; diminishing returns likely
  - Classifier capacity vs. interpretability: InceptionTime (82%) is a 5-model deep ensemble; tree methods (~67%) offer feature importance

- **Failure signatures:**
  1. Baseline accuracy stuck at 25-30%: Real data alone is insufficient—expected; verify feature preprocessing
  2. Augmented accuracy plateaus below 50%: Generator quality too low; check KS scores, switch to G-CTGAN
  3. High train accuracy, low test accuracy: Synthetic data distribution mismatch; recompute KS on test set features
  4. Large std across repetitions (>5%): Insufficient ensemble stability; increase repetitions or check random seeding

- **First 3 experiments:**
  1. Reproduce baseline: Train all 5 classifiers on 320R only; expect 28-36% accuracy range per Table 1.
  2. Generator ablation: At 320R + 1600S, compare all three generators with InceptionTime; verify KS-accuracy correlation (0.73→0.83→0.9 KS corresponds to accuracy gains).
  3. Scaling curve: Using G-CTGAN, plot accuracy vs. synthetic multiplier (0×, 1×, 2×, 3×, 5×) for InceptionTime to confirm monotonic improvement trend.

## Open Questions the Paper Calls Out
- Does the inclusion of additional eye movement features—such as saccades, blinks, and scanpath structure—improve task decoding accuracy beyond the four features (x, y, duration, pupil) currently utilized?
- Can this synthetic augmentation framework be adapted to successfully identify individuals based on unique gaze patterns for biometric authentication?
- Does the high decoding accuracy (82%) generalize to new participants or images that were not present in the seed data used to generate the synthetic samples?
- Do the synthetic scanpaths generated by GANs preserve the physiological sequential dependencies required for deep time-series models to learn valid task strategies?

## Limitations
- Dataset accessibility issues due to the "anonymous" description of the Tatler et al. (2010) replication dataset
- Lack of reported hyperparameters for all classifiers makes exact reproduction difficult
- KS test validation may not capture all aspects of synthetic data quality relevant to downstream task performance
- Limited to four specific eye movement features (x, y, duration, pupil diameter), potentially missing other informative gaze patterns

## Confidence
- **High**: Synthetic data augmentation improves task decoding accuracy when real data is limited
- **Medium**: Spatio-temporal eye movement patterns encode distinguishable task information - limited by preprocessing transparency
- **Medium**: G-CTGAN achieves highest KS scores and accuracy - assumes KS test validity as quality proxy

## Next Checks
1. **KS-to-Accuracy Validation**: Generate synthetic samples using CTGAN, CopulaGAN, and G-CTGAN on a publicly available eye-tracking dataset, then measure both KS scores and task classification accuracy to verify correlation.
2. **Data Leakage Audit**: Reimplement the exact 80/20 split protocol and verify that synthetic generators are fitted exclusively on training data by computing KS scores between synthetic and test distributions.
3. **Ablation on Feature Types**: Train InceptionTime on subsets of features (x+y only, duration+pupil only, all four) to determine which contribute most to the observed accuracy gains.