---
ver: rpa2
title: Token-level Ensembling of Models with Different Vocabularies
arxiv_id: '2502.21265'
source_url: https://arxiv.org/abs/2502.21265
tags:
- ensembling
- translation
- each
- linguistics
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agreement-Based Ensembling (ABE), a method
  to combine text generation models with different subword vocabularies at inference
  time. Traditional ensembling requires identical vocabularies to combine probability
  distributions, which is not possible when models have distinct vocabularies.
---

# Token-level Ensembling of Models with Different Vocabularies

## Quick Facts
- **arXiv ID:** 2502.21265
- **Source URL:** https://arxiv.org/abs/2502.21265
- **Reference count:** 40
- **Primary result:** Agreement-Based Ensembling (ABE) enables inference-time ensembling of text generation models with different subword vocabularies, improving translation performance with BLEU gains up to +2.7 and consistent COMET improvements.

## Executive Summary
This paper introduces Agreement-Based Ensembling (ABE), a method to combine text generation models with different subword vocabularies at inference time. Traditional ensembling requires identical vocabularies to combine probability distributions, which is not possible when models have distinct vocabularies. ABE addresses this by coordinating token selection across models so they agree on the surface form of generated text. The method uses an efficient search algorithm to find agreeing tokens across vocabulary cross-products and handles varying token lengths through a stalling mechanism.

Applied to machine translation tasks, ABE enables ensembling across different architectures including traditional encoder-decoder models and decoder-only LLMs. The approach frequently improves translation performance over individual models, with BLEU score improvements of up to +2.7 and consistent positive gains in COMET scores. The method works particularly well when combining bilingual models with larger multilingual models or LLMs. The implementation is open-source and architecture-agnostic, requiring no model retraining or additional parameters.

## Method Summary
ABE coordinates token selection across models with different vocabularies to generate text that all models agree upon. The method maintains a global detokenized string and requires each model's local hypothesis to be a prefix of this global string (agreement). A heap-based cube pruning algorithm searches the V1 × V2 grid of vocabulary cross-products, efficiently finding agreeing tokens by tracking visited cells and using length-normalized scores. When models generate tokens of different lengths, a stalling mechanism allows trailing models to catch up by replacing the leading model's vocabulary with an ε-transition. For beam search, the space expands to k × V1 × V2, collecting k agreeing candidates per timestep. The approach is applied to machine translation, showing consistent improvements when combining bilingual models with larger multilingual models or LLMs.

## Key Results
- BLEU score improvements up to +2.7 over individual models in machine translation tasks
- Consistent positive gains in COMET scores across different model combinations
- Works effectively when combining bilingual models with larger multilingual models or LLMs
- Architecture-agnostic approach requiring no model retraining or additional parameters

## Why This Works (Mechanism)
The method works by constraining the search space through agreement requirements. By forcing all models to contribute tokens that maintain a shared global hypothesis, ABE effectively reduces the search space to the intersection of high-probability tokens across models. This is particularly effective in machine translation where the output space is constrained by meaning preservation requirements. The stalling mechanism handles the challenge of models with different tokenization schemes producing tokens of varying lengths, ensuring synchronization without forcing artificial token boundaries. The heap-based search efficiently navigates the exponential vocabulary cross-product space by prioritizing high-probability token combinations.

## Foundational Learning

**Cube pruning search:** A search algorithm that explores a grid of vocabulary cross-products efficiently by maintaining a heap of candidate cells and expanding only promising paths. Needed to handle the exponential search space when combining vocabularies; quick check: verify heap operations maintain the k-best candidates.

**Agreement constraint:** The requirement that all models' hypotheses must share a common prefix (the global hypothesis). Needed to ensure coherent outputs from models with different vocabularies; quick check: confirm detokenized strings match across models at each timestep.

**Stalling mechanism:** A synchronization technique where a leading model's vocabulary is temporarily replaced with ε-transitions when it reaches agreement but trailing models lag. Needed to handle different tokenization schemes producing tokens of varying lengths; quick check: verify models eventually catch up and produce agreeing tokens.

**Length-normalized scoring:** Probability scores adjusted by token length to fairly compare hypotheses of different lengths. Needed because longer tokens may have lower raw probabilities but represent more efficient encoding; quick check: confirm scores properly account for token length differences.

**Byte-string prefix checking:** Verification that detokenized candidates from different models share a common prefix at the byte level. Needed because different tokenization schemes may represent the same text differently; quick check: test with models using different whitespace handling.

## Architecture Onboarding

**Component map:** Token distributions from Model 1 and Model 2 → Cube pruning heap search → Agreement filter (byte-string prefix check) → Stalling mechanism (ε-transitions) → Global hypothesis update → Next timestep

**Critical path:** The core execution path is: get top-k tokens from each model → initialize heap with (1,1) cell → pop cells and check agreement → if agree, accept and expand neighbors → if stall detected, apply stalling → repeat until k agreeing items found or max_length reached.

**Design tradeoffs:** The method trades computational complexity (exploring V1 × V2 grid) for the ability to combine models with different vocabularies. The stalling mechanism adds complexity but enables synchronization across different tokenization schemes. Using equal weights (λ=0.5) simplifies the approach but may not be optimal for all model pairs.

**Failure signatures:** Poor performance when models rarely agree on tokens (indicated by excessive heap iterations), unresolvable stalls (models cannot catch up), or when models are fundamentally incompatible (negative Δ scores). The approach may also struggle with less constrained tasks where the agreement space is too sparse.

**First experiments:** 1) Test on synthetic data with known vocabulary overlap to verify search efficiency and agreement rates. 2) Implement stalling mechanism and test with models producing tokens of different lengths. 3) Apply to a simple MT task (e.g., WMT24 en-de) with two custom models of different vocabulary sizes.

## Open Questions the Paper Calls Out
**Can ABE be effectively applied to less constrained text generation tasks?** The authors note the approach "might falter in less constrained tasks" because machine translation benefits from a "narrow set of acceptable outputs" that constrains the generative space. ABE relies on the intersection of high-probability tokens across models; open-ended tasks may have sparser intersections, making agreement harder to achieve efficiently.

**How can the ABE algorithm be adapted to robustly support sampling-based generation?** Appendix B notes that experiments with sampling "did not work well," hypothesizing that tokenization schemes with leading whitespace cause synchronization failures during stochastic token selection. The current mechanism leads to states where models sample tokens that technically "agree" but represent incompatible word boundaries.

**What methodologies can optimize interpolation weights and predict successful model pairings?** Section 5.3 states the algorithm provides a framework for research into "recommendations for model combinations or lambda weights which we leave to future work." The experiments utilized static, evenly weighted ensembles (λ=0.5), yet performance gains were inconsistent across different model pairings.

## Limitations
- **Search space challenges:** The method may struggle when models rarely propose agreeing tokens, requiring potentially exhaustive search of the vocabulary cross-product space
- **Task specificity:** The approach works best in constrained domains like machine translation where output space is limited; less constrained tasks may have sparse agreement spaces
- **Edge case handling:** The algorithm's behavior at EOS tokens and max-length boundaries during stalling scenarios lacks complete specification

## Confidence

- **High confidence** in the core algorithm's validity and its ability to produce coherent outputs that combine models with different vocabularies. The search mechanism and stalling approach are well-defined and the empirical results on MT benchmarks are robust.
- **Medium confidence** in the generalizability to non-translation tasks and very large vocabularies. While the paper shows some non-MT examples, the MT domain provides strong constraints that may not apply elsewhere.
- **Medium confidence** in the claimed computational efficiency relative to naive approaches. The paper asserts sub-quadratic scaling but does not provide detailed runtime comparisons or show how performance degrades with vocabulary size or agreement frequency.

## Next Checks

1. Implement and test the algorithm on a controlled synthetic dataset where vocabulary overlap and agreement patterns are known, measuring iteration counts, agreement rates, and timing across different vocabulary sizes and model similarity levels.

2. Systematically test edge cases including: (a) scenarios where one model generates EOS while others are stalled, (b) max_length boundaries with unresolved stalls, and (c) vocabularies without byte-fallback support to verify the stalling mechanism's robustness.

3. Benchmark runtime and memory usage against a naive full enumeration baseline across varying vocabulary sizes (8k-64k) and beam widths (1-10), documenting how search space expansion and agreement frequency affect efficiency gains.