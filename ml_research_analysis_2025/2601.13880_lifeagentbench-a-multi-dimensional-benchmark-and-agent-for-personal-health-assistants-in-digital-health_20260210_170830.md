---
ver: rpa2
title: 'LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health
  Assistants in Digital Health'
arxiv_id: '2601.13880'
source_url: https://arxiv.org/abs/2601.13880
tags:
- health
- reasoning
- lifestyle
- lifeagent
- sleep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LifeAgentBench is a QA benchmark for long-horizon, cross-dimensional
  lifestyle health reasoning over heterogeneous multimodal records, containing 22,573
  questions covering tasks from basic retrieval to complex reasoning. It includes
  an extensible generation pipeline and standardized evaluation protocol to assess
  LLM-based health assistants.
---

# LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health

## Quick Facts
- **arXiv ID**: 2601.13880
- **Source URL**: https://arxiv.org/abs/2601.13880
- **Reference count**: 8
- **Key outcome**: LifeAgentBench is a QA benchmark for long-horizon, cross-dimensional lifestyle health reasoning over heterogeneous multimodal records, containing 22,573 questions covering tasks from basic retrieval to complex reasoning. It includes an extensible generation pipeline and standardized evaluation protocol to assess LLM-based health assistants. Systematic evaluation of 11 leading LLMs revealed bottlenecks in long-horizon aggregation and cross-dimensional reasoning. LifeAgent, a tool-augmented baseline agent, achieves significant improvements through multi-step evidence retrieval and deterministic aggregation, outperforming widely used baselines and demonstrating potential in realistic daily-life scenarios. The benchmark is publicly available.

## Executive Summary
LifeAgentBench introduces a comprehensive evaluation framework for personal health assistants that must reason across multiple dimensions of lifestyle data including diet, sleep, activity, and emotion. The benchmark contains 22,573 questions spanning basic fact retrieval to complex long-horizon aggregation and cross-dimensional reasoning tasks. LifeAgent, a tool-augmented baseline agent, demonstrates that combining multi-step evidence retrieval with deterministic aggregation significantly outperforms traditional prompting approaches, particularly on aggregation-intensive and multi-item answer tasks. The work highlights that structured tool interaction and SQL-grounded evidence retrieval can substantially benefit complex health reasoning when properly implemented.

## Method Summary
LifeAgentBench is built on AI4FoodDB, a one-month multimodal lifestyle dataset from 100 participants across four domains (diet, activity, sleep, emotion). Questions are mapped to executable SQL queries with verifiable ground-truth answers. The benchmark includes 22,573 questions across five task types: Fact Query, Aggregated Statistics, Numeric Comparison, Conditional Query, and Trend Analysis. LifeAgent is implemented using the smolagents framework with a thought-action-observation loop. It employs three tool categories: structured data retrieval via SQL, cohort-level aggregation, and deterministic computation. The agent uses iterative query decomposition with in-context demonstrations to generate sub-questions for multi-step evidence retrieval.

## Key Results
- LifeAgent significantly outperforms Context Prompting and Database-augmented Prompting baselines, particularly on Aggregated Statistics (accuracy improving from 5.21% to 48.77%) and Multi-item answers (from 0.36-3.04% to 32.31%).
- SQL execution accuracy is the primary bottleneck, with average Execution Accuracy across models at 25.94%, though GPT-4o achieves 45.97%.
- Multi-user reasoning remains challenging, with Context Prompting failing completely (0.00% accuracy) and Database-augmented Prompting achieving only 8.14% accuracy on numeric comparison tasks.
- Acc|EX metrics (accuracy given successful evidence retrieval) reach up to 95.65% for GPT-4o, indicating that retrieval failures are the primary limitation rather than reasoning errors.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-step query decomposition with iterative evidence retrieval improves performance on long-horizon, aggregation-intensive health reasoning tasks.
- **Mechanism**: LifeAgent decomposes a complex query Q into a retrieval agenda {q_i} via in-context decomposition. Each sub-question specifies target domain(s), time scope, and granularity. Evidence is retrieved iteratively, cached, and used to update intermediate context before synthesis.
- **Core assumption**: Complex health queries can be factorized into independently retrievable sub-questions whose answers compose reliably.
- **Evidence anchors**: [abstract] "LifeAgent...integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines." [Section 4.2] "This multi-step decomposition is evidence-driven and adaptive, rather than relying on a single-pass retrieval."
- **Break condition**: Queries requiring holistic pattern recognition across all dimensions simultaneously (not decomposable) may not benefit; failure likely when sub-question dependencies are cyclic.

### Mechanism 2
- **Claim**: Offloading aggregation and arithmetic to deterministic tools reduces computational hallucination errors in structured-output tasks.
- **Mechanism**: LifeAgent provides deterministic computation tools (basic arithmetic, aggregation) invoked by the LLM. Results are cached as verifiable observations, bypassing model-side calculation errors.
- **Core assumption**: The LLM can correctly identify when and which deterministic tool to invoke for a given sub-task.
- **Evidence anchors**: [Section 4.3] "These tools perform basic arithmetic operations deterministically, reducing errors caused by model-side calculation hallucinations." [Table 3] Multi-item answer accuracy improves from 0.36% (CP) / 3.04% (DP) to 32.31% (LifeAgent).
- **Break condition**: If tool invocation logic is mis-specified or the LLM fails to select correct tools, errors propagate; compositional outputs with many steps may still fail.

### Mechanism 3
- **Claim**: Structured, SQL-grounded evidence retrieval enables reliable cross-dimensional and multi-user reasoning when execution succeeds.
- **Mechanism**: The benchmark pipeline maps each question to an executable SQL query; agents retrieve structured records with SQL traces. High Acc|EX (up to 95.65% for GPT-4o) indicates that correct evidence retrieval strongly conditions successful final reasoning.
- **Core assumption**: The database schema is interpretable by the LLM, and SQL generation is correct for the query intent.
- **Evidence anchors**: [Section 5.2] "GPT-4o reaches 95.65% [Acc|EX]...suggesting that tool interaction and structured evidence retrieval can substantially benefit complex cross-domain, long-horizon health reasoning." [Table 2] SQL Validity and Execution Accuracy vary widely (VA: 28.46%–84.84%; EX: 11.48%–45.97%), indicating retrieval is the primary bottleneck.
- **Break condition**: Schema complexity or ambiguous query intent leads to invalid SQL; multi-user joins with cross-domain conditions remain error-prone (multi-user CP accuracy: 0.00%).

## Foundational Learning

- **Concept**: Relational data modeling and SQL query semantics (joins, aggregations, window functions)
  - **Why needed here**: LifeAgentBench grounds all questions in executable SQL; understanding schema relationships is required to debug retrieval failures and extend the pipeline.
  - **Quick check question**: Given a multi-table schema with user_id and timestamp keys, write a query aggregating sleep duration by week and joining with diet records.

- **Concept**: Agentic reasoning loops (thought–action–observation cycle)
  - **Why needed here**: LifeAgent implements this loop via smolagents; understanding state updates, tool selection, and termination conditions is essential for modifying agent behavior.
  - **Quick check question**: Sketch the state update equation s_{t+1} = T(s_t, a_t, o_t) for an agent that caches retrieved records and intermediate aggregates.

- **Concept**: Benchmark evaluation metrics for structured QA (accuracy by answer type, conditional accuracy given successful retrieval)
  - **Why needed here**: The paper reports Accuracy, SQL Validity, Execution Accuracy, and Acc|EX; distinguishing retrieval failures from reasoning failures requires fluency with these metrics.
  - **Quick check question**: If Acc|EX is high but overall Accuracy is low, what does this imply about the system's bottleneck?

## Architecture Onboarding

- **Component map**: User query Q → Query decomposition → Sub-question agenda {q_i} → Structured data retrieval (SQL) → Evidence cache E_t → Deterministic aggregation/computation → Intermediate context M_t → Final synthesis → Natural-language response

- **Critical path**: 1. Query parsing → intent identification (task type, domains, time window) 2. Retrieval agenda generation → correct sub-question formulation 3. Tool invocation → valid SQL execution and correct aggregation 4. Evidence accumulation → cross-domain alignment by timestamp 5. Final synthesis → faithful integration without hallucination

- **Design tradeoffs**: More decomposition steps increase retrieval granularity but raise latency and tool-calling overhead. Deterministic tools improve reliability but limit expressiveness to predefined operations. Context Prompting is simpler but fails on long-horizon/multi-user queries due to context window limits; Database-augmented Prompting scales better but depends on SQL generation quality.

- **Failure signatures**: Aggregated Statistics tasks: Near-random accuracy under CP (5.21%) indicates models cannot aggregate over long windows without tool support. Multi-user queries: 0.00% accuracy under CP (context overflow); low EX under DP indicates schema/join errors. Multi-item answers: CP/DP collapse (<4%); LifeAgent recovers to 32.31%, suggesting compositional output generation remains partially unsolved.

- **First 3 experiments**: 1. Baseline reproduction: Run Context Prompting and Database-augmented Prompting on a subset of LifeAgentBench with a small open-source model (e.g., Qwen-2.5-7B); measure Acc, VA, EX, Acc|EX to confirm reported bottlenecks. 2. Ablation on decomposition depth: Configure LifeAgent with single-step vs. multi-step retrieval on Aggregated Statistics and Numeric Comparison tasks; quantify accuracy gain per additional step. 3. Error analysis on multi-user queries: Manually inspect failed SQL queries for multi-user, cross-dimensional questions; categorize failures (schema misunderstanding, join errors, condition mismatches) to prioritize tool or prompt improvements.

## Open Questions the Paper Calls Out

- **Question**: How does LifeAgent performance degrade when applied to real-world data streams with significant sensor noise or missing entries, compared to the structured records used in LifeAgentBench?
- **Basis in paper**: [inferred] The benchmark generation pipeline maps natural language questions to executable SQL queries on aligned, clean records to ensure "verifiable ground-truth answers."
- **Why unresolved**: The current evaluation assumes the reliability and completeness of the underlying lifelog data, whereas real-world mobile sensing is often characterized by data gaps and noise.
- **What evidence would resolve it**: A robustness evaluation using a modified benchmark version containing simulated sensor dropouts, temporal misalignments, and noisy value perturbations.

- **Question**: Can the agent's deterministic aggregation and multi-step retrieval mechanisms scale effectively to longitudinal health reasoning spanning years rather than weeks?
- **Basis in paper**: [explicit] The authors identify "bottlenecks in long-horizon aggregation" in existing LLMs and note that performance degrades as evidence scope expands.
- **Why unresolved**: While LifeAgent improves performance on the benchmark, the source dataset (AI4FoodDB) covers only one month, leaving the agent's efficacy over truly long-term health trajectories untested.
- **What evidence would resolve it**: Evaluation results of LifeAgent on extended datasets spanning multiple years to assess if the multi-step retrieval remains accurate and computationally feasible.

- **Question**: To what extent can the LifeAgent architecture be adapted to incorporate implicit user feedback to refine its reasoning and personalization over time?
- **Basis in paper**: [inferred] The agent is described as "training-free" and relies on "in-context demonstrations" for decomposition.
- **Why unresolved**: The current framework operates on a static query-response basis; it is unclear if the agent can learn from past interactions to improve the relevance of "Targeted lifestyle recommendations" for specific users.
- **What evidence would resolve it**: A comparative study measuring user satisfaction and recommendation adherence between the static LifeAgent and a variant equipped with a user-feedback memory module.

## Limitations
- The benchmark's SQL-grounded evidence retrieval assumes fully clean, timestamp-aligned, anonymized data; real-world health datasets often contain missing values, sensor drift, and privacy-preserving noise that could degrade SQL execution accuracy.
- Multi-user reasoning fails entirely under Context Prompting (0.00% for numeric comparison), suggesting the agent struggles with join-heavy schemas and cross-user aggregation logic.
- Open-ended evaluation via G-Eval is weakly grounded in the paper and may introduce subjective bias.

## Confidence
- **High confidence**: SQL execution is the dominant bottleneck (average EX = 25.94%); structured tool-augmented retrieval substantially outperforms naive prompting baselines for multi-item and aggregation tasks.
- **Medium confidence**: Multi-step decomposition reliably improves accuracy on aggregation-intensive tasks, but the robustness of decomposition logic across unseen query patterns is not demonstrated.
- **Low confidence**: Generalization of LifeAgent's approach to non-SQL, free-text or multimodal evidence sources; scalability to larger populations or longer time horizons.

## Next Checks
1. **SQL Execution Stress Test**: Run a stratified sample of LifeAgentBench questions on a larger, noisier health dataset (e.g., with missing values or irregular timestamps); measure degradation in VA and EX to quantify real-world robustness.
2. **Cross-Domain Reasoning Ablation**: Systematically disable domain joins in multi-user queries; compare accuracy to isolate whether schema complexity or reasoning complexity is the limiting factor.
3. **Open-Ended Evaluation Replication**: Re-run the case study with a second independent G-Eval rubric and scorer; report inter-rater agreement to bound subjectivity in the final metric.