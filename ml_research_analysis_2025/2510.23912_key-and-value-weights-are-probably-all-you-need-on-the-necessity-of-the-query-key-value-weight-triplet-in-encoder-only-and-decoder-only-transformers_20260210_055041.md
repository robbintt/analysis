---
ver: rpa2
title: 'Key and Value Weights Are Probably All You Need: On the Necessity of the Query,
  Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers'
arxiv_id: '2510.23912'
source_url: https://arxiv.org/abs/2510.23912
tags:
- attention
- weight
- skip
- weights
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that Query weights (WQ) in multi-head attention
  are redundant and can be replaced with the identity matrix, reducing attention parameters
  by 25% per layer. The theoretical analysis shows that attention depends on input
  only through the products XWQ, XWK, and XWV, enabling this elimination through basis
  transformations.
---

# Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers

## Quick Facts
- arXiv ID: 2510.23912
- Source URL: https://arxiv.org/abs/2510.23912
- Authors: Marko Karbevski; Antonij Mijoski
- Reference count: 13
- Primary result: Query weights (WQ) in multi-head attention can be replaced with identity matrix, reducing parameters by 25% per layer while maintaining performance

## Executive Summary
This paper proves that Query weights (WQ) in multi-head attention are redundant and can be replaced with the identity matrix, reducing attention parameters by 25% per layer. The theoretical analysis shows that attention depends on input only through the products XWQ, XWK, and XWV, enabling this elimination through basis transformations. Under simplified architectural assumptions (no normalization, skip connections only around attention), the authors prove that all Query weights can be set to identity while maintaining identical outputs. Empirically, GPT-style models trained from scratch with WQ = I achieve comparable validation loss to standard baselines despite 8% fewer non-embedding parameters, and outperform parameter-matched baselines when saved parameters are reallocated to the MLP.

## Method Summary
The authors establish that attention computation depends on input X only through the products XWQ, XWK, and XWV. By performing a basis transformation on the Key and Value matrices using WQ⁻¹, they can eliminate the Query weight matrix entirely, replacing it with the identity matrix while maintaining identical attention outputs. The theoretical proof works under simplified transformer architectures without layer normalization and with skip connections only around attention blocks. For empirical validation, they train GPT-style models from scratch with WQ = I, comparing validation loss and training stability against standard baselines. They also experiment with reallocating the saved parameters to other components like the MLP.

## Key Results
- Query weights (WQ) can be replaced with identity matrix while maintaining identical attention outputs under simplified assumptions
- GPT-style models with WQ = I achieve comparable validation loss to standard baselines with 8% fewer non-embedding parameters
- Models with WQ = I and reallocated parameters outperform parameter-matched baselines when extra capacity goes to MLP
- Reduced models train stably at over 3× lower weight decay, suggesting implicit regularization benefits

## Why This Works (Mechanism)
The mechanism relies on the observation that attention computation only depends on the input X through the three products: XWQ, XWK, and XWV. Since attention is invariant to linear transformations of the Key and Value matrices when compensated appropriately in the Query space, the Query matrix can be eliminated through a basis transformation. Specifically, if we have original attention with WQ, WKv, and WV, we can define transformed Key and Value matrices as WKv WQ⁻¹ and WV WQ⁻¹, and use identity as the Query matrix, achieving the same attention output.

## Foundational Learning
- **Multi-head Attention**: Mechanism where queries, keys, and values are projected separately and combined; needed to understand how WQ, WKv, and WV interact; quick check: verify attention output formula
- **Basis Transformation**: Mathematical operation to change coordinate system; needed to understand how WQ can be eliminated; quick check: confirm that WQ⁻¹ exists and is well-defined
- **Transformer Architecture**: Standard encoder-decoder or decoder-only structure; needed to understand where attention blocks fit; quick check: identify attention block position in typical transformer
- **Parameter Sharing**: Concept of reducing parameters while maintaining functionality; needed to understand the practical implications; quick check: calculate parameter reduction percentage
- **Weight Decay**: Regularization technique that penalizes large weights; needed to understand training stability differences; quick check: compare weight decay values used in experiments
- **Skip Connections**: Residual connections that bypass layers; needed to understand architectural assumptions; quick check: verify where skip connections are placed in the model

## Architecture Onboarding
- **Component Map**: Input -> Linear Projections (WQ, WKv, WV) -> Scaled Dot-Product Attention -> Skip Connection -> Layer Norm -> MLP -> Skip Connection -> Output
- **Critical Path**: Input embedding -> Multi-head attention -> Layer normalization -> Feed-forward network -> Output
- **Design Tradeoffs**: Reduced parameters (25% per layer) vs. architectural simplicity and training stability; potential performance impact vs. efficiency gains
- **Failure Signatures**: If WQ is truly essential, models with WQ = I would show degraded performance or training instability; lack of parameter reallocation benefits would indicate WQ serves a unique purpose
- **First Experiments**: 1) Train standard vs. WQ = I models with identical initialization and compare convergence curves, 2) Measure attention pattern differences between standard and WQ = I models, 3) Test different weight decay values for both architectures to quantify stability differences

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes simplified architectures without layer normalization and with skip connections only around attention blocks
- Empirical validation focuses primarily on GPT-style models, not exploring encoder-only architectures or larger models beyond 2 billion parameters
- Does not provide mathematical justification for extending simplified proof to standard transformers with layer norms and residual connections around MLPs

## Confidence
*High confidence*: The mathematical proof under simplified assumptions is rigorous and the theoretical framework for basis transformations is sound. The observation that attention computations only depend on input through products XWQ, XWK, and XWV is correctly identified and exploited.

*Medium confidence*: The empirical results showing comparable validation loss with 8% fewer parameters are promising but limited to specific model scales and training configurations. The claim about improved stability at lower weight decay is supported but needs more systematic investigation across different training regimes.

*Low confidence*: The broader claim about architectural redundancy in the Query-Key-Value triplet lacks sufficient empirical support. The paper does not demonstrate whether these findings extend to vision transformers, multimodal models, or models with architectural modifications beyond the tested configurations.

## Next Checks
1. Test the WQ = I hypothesis in standard transformers with layer normalization and residual connections around both attention and MLP blocks, measuring both training stability and final performance across different model scales.

2. Reallocate the saved parameters to other components (MLP, feed-forward networks, or additional layers) and systematically evaluate whether the performance gains observed are due to the parameter reduction itself or the redistribution of capacity.

3. Extend the analysis to encoder-only architectures (BERT-style models) and evaluate whether the Query weight elimination holds under masked language modeling objectives and bidirectional attention patterns.