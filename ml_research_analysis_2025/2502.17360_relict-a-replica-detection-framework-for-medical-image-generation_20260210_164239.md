---
ver: rpa2
title: 'RELICT: A Replica Detection Framework for Medical Image Generation'
arxiv_id: '2502.17360'
source_url: https://arxiv.org/abs/2502.17360
tags:
- image
- images
- replica
- data
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes RELICT, a standardized framework for detecting\
  \ replicas\u2014nearly identical copies of training data\u2014in synthetic medical\
  \ images. The framework evaluates image similarity using three complementary approaches:\
  \ voxel-level analysis (MAE, RMSE, SSIM), feature-level analysis via a pretrained\
  \ medical foundation model (ResNet-50 MedicalNet), and segmentation-level analysis\
  \ (Dice coefficient, average surface distance)."
---

# RELICT: A Replica Detection Framework for Medical Image Generation

## Quick Facts
- arXiv ID: 2502.17360
- Source URL: https://arxiv.org/abs/2502.17360
- Reference count: 40
- Key outcome: Framework achieves perfect replica detection (balanced accuracy=1) for non-contrast head CT with ICH using image-level and feature-level analysis, while segmentation-level analysis achieves 0.79 balanced accuracy for detecting anatomical replicas in time-of-flight MR angiography.

## Executive Summary
RELICT is a standardized framework for detecting replicas—near-identical copies of training data—in synthetic medical images. The framework evaluates image similarity using three complementary approaches: voxel-level analysis (MAE, RMSE, SSIM), feature-level analysis via a pretrained medical foundation model (ResNet-50 MedicalNet), and segmentation-level analysis (Dice coefficient, average surface distance). Two clinically relevant 3D generative modeling use cases were investigated: non-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flight MR angiography of the Circle of Willis (N=1,782). Expert visual scoring served as the reference standard, with raters identifying 45/50 and 5/50 generated images as replicas for the NCCT and TOF-MRA use cases, respectively.

## Method Summary
The framework computes similarity measures between synthetic and training images at three levels: image-level (MAE, RMSE, SSIM), feature-level (MedicalNet embeddings with cosine/RMSE similarity), and segmentation-level (Dice, ASD). For each synthetic image, the closest training match is identified for each measure, and a distance ratio is calculated by dividing the nearest neighbor distance by the mean distance to the k-nearest neighbors (k=50). Thresholds are optimized via grid search (0.01 increments) to maximize balanced accuracy against expert-annotated ground truth. The framework is implemented in Python and available as open-source.

## Key Results
- For NCCT ICH use case: Image-level and feature-level measures achieved perfect replica classification (balanced accuracy=1) at optimal thresholds
- For TOF-MRA use case: Segmentation-level analysis achieved highest balanced accuracy of 0.79, as image-level and feature-level measures were insufficient for detecting abstract anatomical similarities
- Raters agreed in 92% of cases (46/50) for NCCT replica detection, with visual scores ≥3 classified as replicas
- For TOF-MRA, all 50 synthetic images had different closest training matches identified by different similarity measures

## Why This Works (Mechanism)

### Mechanism 1: Distance Ratio Captures "Abnormally Close" Matches
The ratio between a synthetic image's nearest training match and the mean distance to its k-nearest neighbors identifies replicas better than absolute distance alone. For each synthetic image, compute M(x̂, x_nearest) / mean(M(x̂, x_i)) for i ∈ k-nearest. A low ratio indicates the closest match is anomalously similar compared to the local neighborhood, suggesting memorization rather than generalization. This distribution differs from non-memorized generations when training data has low internal similarity.

### Mechanism 2: Complementary Analysis Levels Capture Different Similarity Modes
No single similarity measure detects all replica types; combining voxel-level, feature-level, and segmentation-level analyses provides complementary detection coverage. Voxel-level (MAE, RMSE, SSIM) detects near-identical copies with matching intensities; feature-level (MedicalNet embeddings) captures semantic/structural similarity robust to intensity variations; segmentation-level (Dice, ASD) isolates anatomical structure similarity independent of background. Different generative failure modes manifest at different abstraction levels.

### Mechanism 3: Expert Visual Ground Truth Calibrates Thresholds
Optimal replica detection thresholds are dataset-dependent and require visual calibration to establish ground truth. Raters score synthetic-real pairs on 4-point Likert scale; scores ≥3 classified as replicas. This creates binary labels for threshold optimization via balanced accuracy maximization. Human experts can reliably distinguish replicas from legitimate generations when inter-rater agreement validates this.

## Foundational Learning

- **Concept: Structural Similarity Index (SSIM)**
  - Why needed here: SSIM captures perceptual similarity better than pixel-wise metrics (MAE/RMSE) by accounting for luminance, contrast, and structure jointly
  - Quick check question: Would an image and its contrast-inverted version have low MAE but high SSIM, or vice versa?

- **Concept: Feature Embeddings via Pretrained Encoders**
  - Why needed here: MedicalNet ResNet-50 projects 3D volumes into a 2048×4×4×4 embedding space where semantic similarity is more meaningful than voxel correspondence
  - Quick check question: Why might two anatomically similar brain scans have different raw voxel values but similar embedding vectors?

- **Concept: Segmentation Metrics (Dice, ASD)**
  - Why needed here: Dice measures region overlap; ASD measures boundary proximity. Together they capture whether pathological/vascular structures are replicated independently of background
  - Quick check question: If a synthetic hemorrhage has the same volume but different shape than the real one, would Dice or ASD better detect this discrepancy?

## Architecture Onboarding

- **Component map:** Input layer (training + synthetic 3D volumes) -> Preprocessing (normalize, register) -> Analysis pipelines (parallel: image-level, feature-level, segmentation-level) -> Ranking engine (nearest neighbor identification) -> Distance ratio calculator -> Threshold optimizer (grid search) -> Output (ranked replicas + thresholds)

- **Critical path:** Load training and synthetic datasets -> Preprocess (normalize, register if needed) -> For each synthetic image: compute all similarity measures against all training images -> Identify nearest neighbor per measure -> Compute distance ratios -> If ground truth available: optimize thresholds; else: return ranked list

- **Design tradeoffs:** Runtime vs. coverage (image-level SSIM takes 76 min vs. embedding cosine at 4 min) vs. sensitivity vs. specificity (lower thresholds catch more replicas but increase false positives) vs. generalization vs. dataset-specific (paper recommends calibrating thresholds per dataset)

- **Failure signatures:** Different measures identify different "closest" images (occurred in 50/50 TOF-MRA cases) -> signals need for ensemble or segmentation-level analysis; high inter-rater disagreement -> ambiguous cases where threshold setting is unstable; balanced accuracy cannot reach 1.0 at any threshold (TOF-MRA case) -> replica definition may not align with available measures

- **First 3 experiments:**
  1. Reproduce NCCT results: Train diffusion model on 774 ICH scans, generate 50 samples, run all three analysis levels, confirm image/feature-level achieve balanced accuracy ≥0.95
  2. Ablate k-nearest parameter: Test k ∈ {10, 25, 50, 100} to assess sensitivity of distance ratio to neighborhood size
  3. Cross-domain threshold transfer: Take optimal NCCT thresholds and apply to TOF-MRA; measure performance drop to quantify domain specificity

## Open Questions the Paper Calls Out

- How does the choice of generative architecture (e.g., diffusion models vs. GANs) influence the rate and nature of memorization in medical imaging? The authors note memorization "could not be directly compared between model architectures" because each use case employed a different architecture (latent diffusion vs. StyleGAN).

- Do more recent medical foundation models (e.g., BiomedParse, MedSAM) provide superior feature-level replica detection compared to the ResNet-50 MedicalNet used in this study? The authors suggest "Future additions to the framework can be made using more recent foundation models such as BiomedParse or MedSAM."

- Does filtering out replicas from synthetic datasets improve the performance of downstream deep learning tasks? The authors hypothesize that "our framework can be used as a filter to select valuable and unique images" because memorized images "do not provide additional diversity," but they do not validate this experimentally.

## Limitations

- The framework's generalizability faces constraints when training data contains high internal similarity (e.g., patient follow-up scans), which may break down the distance ratio mechanism
- Segmentation-level analysis requires high-quality, consistent ROI definitions and segmentation models, which were not provided for reproduction
- Expert visual ground truth relies on subjective scoring that may not scale to larger datasets

## Confidence

- **High Confidence**: Image-level and feature-level measures achieving balanced accuracy=1 for NCCT replica detection; complementary analysis levels capturing different similarity modes
- **Medium Confidence**: Segmentation-level analysis achieving highest balanced accuracy (0.79) for TOF-MRA; distance ratio mechanism identifying "abnormally close" matches
- **Low Confidence**: Universal threshold applicability across datasets; scalability of visual scoring methodology

## Next Checks

1. **Distance Ratio Sensitivity Analysis**: Systematically vary k-nearest neighbor parameter (k ∈ {10, 25, 50, 100}) to quantify robustness of distance ratio to neighborhood size selection, particularly for datasets with varying internal similarity.

2. **Cross-Domain Threshold Transfer**: Apply optimal NCCT thresholds directly to TOF-MRA and other medical imaging modalities to measure performance degradation and quantify dataset-specific calibration requirements.

3. **Segmentation Model Dependency Test**: Train multiple segmentation models with varying architectures and quality levels on the same ROI to assess how segmentation quality impacts Dice/ASD-based replica detection performance.