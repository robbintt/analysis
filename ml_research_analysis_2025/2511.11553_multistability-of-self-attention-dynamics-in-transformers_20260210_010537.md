---
ver: rpa2
title: Multistability of Self-Attention Dynamics in Transformers
arxiv_id: '2511.11553'
source_url: https://arxiv.org/abs/2511.11553
tags:
- consensus
- equilibria
- bipartite
- matrix
- vkvt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies self-attention dynamics in transformers by modeling
  them as continuous-time multiagent systems on spheres. It establishes a connection
  between self-attention and a multiagent version of the Oja flow, a dynamical system
  computing principal eigenvectors.
---

# Multistability of Self-Attention Dynamics in Transformers

## Quick Facts
- arXiv ID: 2511.11553
- Source URL: https://arxiv.org/abs/2511.11553
- Authors: Claudio Altafini
- Reference count: 34
- Primary result: Self-attention dynamics exhibit multistability, with multiple stable equilibria often aligned with eigenvectors of the value matrix

## Executive Summary
This paper analyzes self-attention dynamics in transformers by modeling them as continuous-time multiagent systems on spheres. The work establishes a connection between self-attention and a multiagent version of the Oja flow, a dynamical system that computes principal eigenvectors. Through this lens, the paper reveals that self-attention dynamics are multistable, meaning multiple asymptotically stable equilibria can coexist. These equilibria fall into four categories: consensus, bipartite consensus, clustering, and polygonal equilibria. The analysis shows that while consensus at the principal eigenvector remains locally stable, other equilibria typically emerge due to the attention mechanism, suggesting each transformer layer may tilt token vectors toward eigenvectors of the value matrix.

## Method Summary
The paper approaches self-attention dynamics through the framework of continuous-time multiagent systems on spheres, establishing a mathematical connection to the Oja flow - a well-studied dynamical system for computing principal eigenvectors. The analysis classifies equilibria into four types (consensus, bipartite consensus, clustering, and polygonal) and proves conditions for their stability. The theoretical framework is validated through synthetic examples and empirical measurements on trained transformer models. The paper also introduces the concept of a value alignment matrix to quantify how token vectors align with eigenvectors of the value matrix during attention dynamics.

## Key Results
- Self-attention dynamics are multistable, with multiple asymptotically stable equilibria often coexisting
- Equilibria fall into four classes: consensus, bipartite consensus, clustering, and polygonal equilibria
- Stable equilibria correspond to low-rank attention matrices, with bipartite consensus and clustering equilibria being particularly prevalent
- Each transformer layer appears to tilt token vectors toward eigenvectors of the value matrix

## Why This Works (Mechanism)
The multistability arises from the interaction between the attention mechanism and the underlying value matrix structure. When tokens interact through self-attention, they form a dynamical system that can settle into various stable configurations depending on initial conditions and the spectral properties of the value matrix. The attention mechanism creates feedback loops that can reinforce certain alignment patterns while suppressing others, leading to multiple basins of attraction. The connection to the Oja flow explains why equilibria tend to align with eigenvectors of the value matrix - the dynamics naturally follow the spectral structure of the underlying matrix.

## Foundational Learning

1. Multiagent systems on spheres
   - Why needed: Provides the geometric framework for understanding how token vectors evolve while maintaining unit norm
   - Quick check: Verify that vector normalization preserves spherical geometry in attention computations

2. Oja flow and principal component analysis
   - Why needed: Establishes the mathematical connection between attention dynamics and eigenvector computation
   - Quick check: Confirm that attention updates follow similar gradient flow patterns as Oja's algorithm

3. Dynamical systems stability theory
   - Why needed: Enables classification of different equilibrium types and their stability properties
   - Quick check: Apply Lyapunov stability criteria to verify equilibrium classifications

4. Spectral graph theory
   - Why needed: Helps understand how the spectral properties of value matrices influence attention behavior
- Quick check: Analyze eigenvalue distributions of value matrices in trained transformers

## Architecture Onboarding

Component map:
Token embeddings -> Value matrix -> Self-attention dynamics -> Equilibrium states -> Output representations

Critical path:
Token vectors → Value matrix multiplication → Attention weight computation → Vector updates → Convergence to equilibrium

Design tradeoffs:
- Continuous vs discrete time modeling: Continuous provides analytical tractability but may miss implementation details
- Single vs multi-head attention: Single-head analysis simplifies theory but may not capture all attention behaviors
- Perfect vs approximate normalization: Perfect normalization enables clean mathematical analysis

Failure signatures:
- Divergence instead of convergence to equilibrium
- Oscillatory behavior instead of stable fixed points
- Unexpected alignment patterns not predicted by eigenvector structure

First experiments:
1. Track vector evolution in a trained transformer layer to verify convergence patterns
2. Measure eigenvector alignment scores across different attention heads
3. Test stability of different equilibrium types under parameter perturbations

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Assumes continuous-time dynamics that may not fully capture discrete implementations in actual transformers
- Limited empirical validation across diverse transformer architectures and attention mechanisms
- Practical implications for transformer training dynamics and generalization remain underexplored

## Confidence
- Mathematical proofs for equilibrium classifications: High
- Stability conditions and analysis: High
- Interpretation of multistability's role in transformer function: Medium
- Claims about layer-wise eigenvector alignment: Medium-Low

## Next Checks
1. Implement discrete-time versions of the self-attention dynamics to verify that continuous-time predictions hold under realistic computational constraints, measuring convergence rates and stability margins.

2. Conduct ablation studies on trained transformers to empirically measure eigenvector alignment at different layers, comparing against theoretical predictions across multiple attention heads and value matrices.

3. Test the multistability claims on diverse transformer architectures (different attention mechanisms, value projections) to determine whether the observed patterns generalize beyond the specific settings studied.