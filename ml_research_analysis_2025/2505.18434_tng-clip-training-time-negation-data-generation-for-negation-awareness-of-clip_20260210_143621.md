---
ver: rpa2
title: TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP
arxiv_id: '2505.18434'
source_url: https://arxiv.org/abs/2505.18434
tags:
- negation
- image
- caption
- clip
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP

## Quick Facts
- **arXiv ID:** 2505.18434
- **Source URL:** https://arxiv.org/abs/2505.18434
- **Reference count:** 14
- **Key outcome:** On-the-fly negation caption generation improves CLIP's negation understanding without LLM overhead

## Executive Summary
TNG-CLIP introduces a training-time negation data generation pipeline that dynamically creates negation captions during CLIP fine-tuning, achieving improved negation understanding while maintaining 2.5% extra training time. The method finds semantically plausible negation objects from similar images in each batch, generates compositional and full negation captions using templates, and employs asymmetric noise-augmented contrastive loss to prevent overfitting. Results show significant improvements on negation benchmarks compared to fixed dataset approaches.

## Method Summary
TNG-CLIP dynamically generates negation captions during training by finding similar image pairs within each batch, extracting nouns from their captions, and selecting semantically distant objects as negation targets. Using 46 compositional and 18 full negation templates, the method creates on-the-fly samples that improve generalization over fixed pre-generated datasets. The training employs asymmetric contrastive loss with random label noise in the image-to-text direction to prevent overfitting while the text-to-image direction learns proper negation alignment. The visual encoder remains frozen throughout training.

## Key Results
- Dynamic generation achieves 51.61 ± 0.96 vs fixed dataset 49.52 ± 1.27 on NegBench-MSCOCO matching
- Asymmetric noise-augmented loss improves retrieval to 61.11 vs 45.32-48.58 without noise
- TNG-CLIP achieves strong performance on Valse-Existence, NegBench (MSCOCO/VOC2007), and proposed NEG-TTOI benchmark

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Training-Time Negation Caption Generation
On-the-fly negation caption generation during training produces diverse samples that improve generalization over fixed pre-generated datasets. For each image-text pair, the method finds the most similar image-text pair via cosine similarity, extracts nouns, filters via WordNet to select semantically distant negation objects, then applies template-based patterns to create compositional and full negation captions.

### Mechanism 2: Asymmetric Noise-Augmented Contrastive Loss
Injecting random label noise into the image-to-text loss prevents overfitting while the text-to-image loss learns proper negation alignment. Images are randomly assigned captions from the entire batch as noise regularization, while all three caption types are aligned to their corresponding image normally. Visual encoder remains frozen.

### Mechanism 3: Dual Negation Caption Types for Complementary Learning
Compositional negation (partial scene negation) and full negation (complete caption negation) capture distinct semantic aspects needed for robust understanding. Compositional captions format as "A negation B" while full negation captions format as "negation A", providing both context-specific and absolute negation learning.

## Foundational Learning

- **Contrastive Learning (CLIP-style bidirectional alignment):**
  - Why needed here: TNG-CLIP modifies CLIP's contrastive framework with asymmetric losses
  - Quick check question: Given image features V ∈ ℝ^(B×D) and text features T ∈ ℝ^(B×D), how does CLIP compute the similarity matrix and what does each direction optimize?

- **Fine-tuning under Distribution Shift:**
  - Why needed here: Paper argues negation is OOD for CLIP; noise injection is motivated by overfitting risk under shift
  - Quick check question: Why might fine-tuning a strong pretrained model on data with large distribution shift yield worse performance than zero-shot?

- **Dynamic Data Augmentation / Curriculum:**
  - Why needed here: Core innovation is generating different negation samples each epoch rather than fixed pre-generation
  - Quick check question: What regularization effect does epoch-to-epoch data variation provide compared to a fixed augmented dataset?

## Architecture Onboarding

**Component map:**
Input Batch (images + captions) -> [Frozen] Visual Encoder → Image Features V_b -> Cosine Similarity Matrix → Most Similar Pair P_s for each P_o -> NLTK POS Tagging → Noun extraction from P_s caption -> WordNet Semantic Distance → Negation object O_n selection -> Template Engine (46 compositional + 18 full patterns) → T_nc, T_nf -> [Trainable] Text Encoder → Text Features for T_o, T_nc, T_nf -> Asymmetric Loss (L_t2i + L_i2t with noise) → Gradients to Text Encoder

**Critical path:**
1. Batch construction with sufficient diversity (similar pairs must exist within batch)
2. Efficient similarity computation (Eq. 1-2)
3. Template selection and caption formatting
4. Proper noise injection only in i2t direction

**Design tradeoffs:**
- Template-based vs LLM-based generation: Templates are 40x faster (2.5% overhead vs LLM generation) but may produce less natural phrasing
- Frozen vs trainable visual encoder: Freezing preserves pretrained features but limits adaptation; necessary for noise injection to work without corrupting visual space
- Dynamic vs fixed dataset per epoch: Dynamic provides regularization but requires online computation; fixed enables caching but harms generalization

**Failure signatures:**
- High affirmation accuracy drop → model biased toward negation, check if original caption alignment preserved
- Low retrieval scores (<50% Neg-R@5) → likely missing noise in i2t loss
- High variance across runs → fixed dataset being used instead of dynamic generation
- Training time >5% overhead → inefficient similar pair search

**First 3 experiments:**
1. Reproduce Table 4 (dynamic vs fixed) on 10K subset: Train with dynamic generation for 3 epochs, save generated sets, train separate model on saved sets
2. Noise injection ablation (Table 6 variant): Sweep random label percentage in i2t loss (0%, 33%, 67%, 100%) to find optimal noise level
3. Template diversity analysis: Train with only 10 randomly selected templates vs all 46 compositional templates; measure impact on NegBench-VOC2007 hybrid category

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the training-time negation data generation pipeline be effectively generalized to other Vision-Language Model (VLM) architectures beyond CLIP?
  - Basis in paper: Authors state they "mainly focus on the negation understanding of CLIP model" and that "further exploration on negation-awareness of diverse VLMs is necessary"
  - Why unresolved: The proposed pipeline is specifically tailored to CLIP's contrastive learning mechanism and architecture
  - What evidence would resolve it: Applying the TNG-CLIP pipeline to other VLM architectures (e.g., BLIP, ALBEF) and evaluating performance on standard negation benchmarks

- **Open Question 2:** Can the negation data generation mechanism be extended to support non-contrastive objective functions or datasets other than image-text pairs, such as Visual Question Answering (VQA)?
  - Basis in paper: Authors note the pipeline is "currently limited to image-text pair dataset" and suggest it "has the potential to be extended... [to] visual question answering dataset"
  - Why unresolved: The current implementation relies on the specific structure of image-text pairs within a batch to generate negations via contrastive similarity
  - What evidence would resolve it: A modified version of the pipeline that functions on VQA datasets using generation-based or QA-specific objective functions

- **Open Question 3:** To what extent does the reliance on template-based negation captions limit the model's ability to understand complex, free-form negation syntax compared to LLM-generated data?
  - Basis in paper: The method relies on a fixed set of templates (64 patterns) for efficiency, whereas previous works utilized computationally expensive LLMs for free-form generation
  - Why unresolved: The paper demonstrates efficiency but does not deeply analyze if the linguistic diversity of templates matches the semantic complexity of LLM-generated captions
  - What evidence would resolve it: A comparative study evaluating TNG-CLIP against a model trained on free-form LLM-generated negation captions on a benchmark specifically designed for complex linguistic nuance

## Limitations

- Strong assumptions about semantic distance via WordNet may not capture all valid negation objects
- Frozen visual encoder constraint prevents adaptation to negation-specific visual features
- Dynamic generation requires careful similarity search implementation to avoid quadratic complexity

## Confidence

**High confidence:** The dynamic training-time generation mechanism works as claimed (Table 4 shows 51.61 vs 49.52 accuracy improvement). The asymmetric noise-augmented loss provides measurable benefits (Table 6 ablation confirms performance degradation without noise).

**Medium confidence:** The core mechanism of finding negation objects via WordNet semantic distance and template-based generation produces valid samples. The frozen visual encoder + noise injection prevents overfitting.

**Low confidence:** The specific choice of 46 compositional and 18 full negation templates is optimal, and the random noise level of 100% in i2t loss is best. The WordNet distance metric's effectiveness for negation object selection lacks ablation studies.

## Next Checks

1. **Semantic distance ablation:** Replace WordNet distance with random noun selection from similar captions to test if semantic distance is truly necessary for the performance gain.

2. **Visual encoder fine-tuning study:** Train with trainable visual encoder (no noise injection) to quantify the trade-off between adaptation capability and overfitting risk.

3. **Template naturalness evaluation:** Human evaluation of generated captions to measure how often templates produce unnatural or contradictory sentences that could harm learning.