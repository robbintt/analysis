---
ver: rpa2
title: Greedy Sampling Is Provably Efficient for RLHF
arxiv_id: '2510.24700'
source_url: https://arxiv.org/abs/2510.24700
tags:
- preference
- policy
- general
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses theoretical understanding of Reinforcement
  Learning from Human Feedback (RLHF), focusing on KL-regularized contextual bandits
  with preference feedback. The key insight is that under KL regularization, every
  candidate optimal policy has a bounded likelihood ratio with respect to the reference
  policy, which enables provably efficient greedy sampling.
---

# Greedy Sampling Is Provably Efficient for RLHF

## Quick Facts
- **arXiv ID**: 2510.24700
- **Source URL**: https://arxiv.org/abs/2510.24700
- **Reference count**: 40
- **Key outcome**: Greedy sampling achieves O(log(T)) regret in online settings and O(ε⁻¹) sample complexity in offline settings for RLHF, without requiring optimism or pessimism.

## Executive Summary
This work provides the first theoretical analysis showing that greedy sampling is provably efficient for Reinforcement Learning from Human Feedback (RLHF). The key insight is that KL regularization constrains all candidate optimal policies to have bounded likelihood ratios with respect to the reference policy, enabling provably efficient greedy sampling. Under this constraint, the paper shows that greedy sampling with respect to empirical estimates achieves logarithmic regret in online settings and sample complexity matching optimistic methods in offline settings. Experiments confirm that greedy sampling performs comparably to optimism-based methods while being computationally simpler, as it avoids constructing confidence bounds.

## Method Summary
The method implements greedy sampling in KL-regularized contextual bandits with preference feedback. The algorithm maintains a reference policy π₀ and collects preference data by sampling actions (a₁~π̂, a₂~π₀) for each context. It performs maximum likelihood estimation (MLE) on the preference data to obtain empirical estimates P̂ or R̂, then directly extracts the optimal policy π̂ using the closed-form solution under KL regularization. In the online setting, this process repeats for T rounds; in the offline setting, it uses pre-collected data only. The method works for both general preference models and Bradley-Terry models, with slightly different analyses for each.

## Key Results
- Greedy sampling achieves O(log T) regret in online KL-regularized contextual bandits with preference feedback
- Greedy sampling achieves O(ε⁻¹) sample complexity in offline settings for both general and Bradley-Terry preference models
- These results match or improve upon previous bounds that required optimistic or pessimistic estimates
- Experimental results confirm greedy sampling performs comparably to optimism-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy sampling avoids the need for optimism/pessimism because KL-regularization constrains all candidate optimal policies to a bounded region around the reference policy.
- Mechanism: The KL penalty enforces that any optimal policy π* satisfies π*(a|x)/π₀(a|x) ∈ [exp(-η), exp(η)] for all (x,a) pairs. This bounded likelihood ratio prevents the policy from becoming deterministic or straying too far from π₀, which in turn bounds estimation error propagation when using empirical estimates directly.
- Core assumption: The reference policy π₀ is stochastic and has full support over actions; η is bounded.
- Evidence anchors: Abstract states "every candidate optimal policy has a bounded likelihood ratio with respect to the reference policy"; Lemma 1 provides explicit bounds.

### Mechanism 2
- Claim: A novel value decomposition lemma enables regret analysis without constructing confidence bounds.
- Mechanism: Lemma 2 establishes V(π_f*, f*) - V(π_f, f*) ≤ η · E_πf'[(f - f*)²] where f' is an interpolated function. This bounds suboptimality by squared estimation error rather than requiring explicit optimism.
- Core assumption: The true preference model P* or reward R* is realizable within the function class P or R.
- Evidence anchors: Section 4.2 proof sketch mentions leveraging this decomposition; Appendix B contains full derivation.

### Mechanism 3
- Claim: MLE under preference feedback concentrates sufficiently fast to enable O(log T) regret with only O(ε⁻¹) samples.
- Mechanism: For both general preference models and BT models, empirical MLE estimates satisfy squared error bounds of order O(log(N/δ)/n). The Eluder dimension d(P,λ,T) captures how quickly new samples reduce uncertainty over the function class.
- Core assumption: Finite (or coverable) function class P or R; bounded preference probabilities in [0,1].
- Evidence anchors: Section 5.2, Lemma 4 provides the concentration bound; Appendix C contains full proofs.

## Foundational Learning

- Concept: **KL-regularized contextual bandits**
  - Why needed here: The entire theoretical result hinges on the structural properties introduced by the KL penalty.
  - Quick check question: Given η and a reference policy π₀, can you write the closed-form optimal policy under reward function r(x,a)?

- Concept: **Preference feedback models (Bradley-Terry vs General)**
  - Why needed here: The paper proves results for both model types with slightly different analyses.
  - Quick check question: Does the general preference model assume transitivity of preferences?

- Concept: **Eluder dimension as a complexity measure**
  - Why needed here: The regret bounds are expressed in terms of d(P,λ,T), which quantifies how "hard" it is to learn in function class P.
  - Quick check question: For a linear function class in ℝᵈ, what is the Eluder dimension?

## Architecture Onboarding

- Component map:
  MLE Estimator -> Policy Extractor -> Action Sampler -> Reference Policy π₀

- Critical path:
  1. Initialize π̂₀ ← π₀
  2. Each round: sample context x~d₀, actions (a₁~π̂ₜ, a₂~π₀), observe preference y
  3. Update MLE: P̂ₜ or R̂ₜ ← argmax log-likelihood over history
  4. Extract policy: π̂ₜ₊₁ from P̂ₜ/R̂ₜ via fixed-point iteration (general) or closed-form (BT)
  5. Repeat for T rounds; output final π̂ₜ

- Design tradeoffs:
  - **η (regularization strength)**: Larger η → tighter reference adherence but potentially suboptimal if π₀ is far from true optimum
  - **Function class complexity |P|**: Larger classes → better expressivity but higher log(N_P) in regret
  - **Online vs Offline**: Online achieves O(log T) regret adaptively; offline requires single-policy coverage but no exploration needed

- Failure signatures:
  - Regret plateaus above zero: Check if realizability holds
  - Policy collapses to deterministic: η may be too large, or π₀ has near-zero probability on optimal actions
  - MLE fails to converge: Preference data may be inconsistent
  - Coverage coefficient C(D₀,(π₀,π₀)) explodes: Offline data insufficiently explores π₀'s support

- First 3 experiments:
  1. **Linear sanity check**: Implement Algorithm 1 with linear preference model, verify cumulative regret grows as O(log T)
  2. **Ablation on η**: Run with η ∈ {0.5, 1, 2, 3}, confirm exp(η) dependence in regret
  3. **Coverage stress test (offline)**: Generate offline datasets with varying coverage coefficients, verify linear scaling with sample complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the exp(η) multiplicative factor in the regret and sample complexity bounds fundamental, or can it be tightened or removed?
- Basis in paper: Appendix A.2 states "the bounds contain multiplicative constants of exp(η), which may be worth further studies on its necessity."
- Why unresolved: The current proof technique introduces exp(η) when converting intermediate policies to the reference policy using Lemma 1's bounded likelihood ratio property.
- What evidence would resolve it: Either a refined analysis showing tighter constants independent of exp(η), or a lower bound construction demonstrating that some exp(η) dependency is unavoidable.

### Open Question 2
- Question: Can the O(log T) regret and O(1/ε) sample complexity guarantees for greedy sampling be extended from contextual bandits to multi-step RL (MDPs) with KL regularization?
- Basis in paper: Appendix A.2 states "it would be a promising direction to extend the theoretical results in this work to multi-step RL."
- Why unresolved: The analysis fundamentally relies on properties of the optimal policy class for single-step decisions. The value decomposition and bounded likelihood ratio may not directly transfer to sequential settings.
- What evidence would resolve it: A theoretical extension proving logarithmic regret for KL-regularized MDPs, or a counterexample showing that greedy sampling fails to achieve similar rates in multi-step settings.

### Open Question 3
- Question: How does greedy sampling compare to optimism/pessimism-based methods in large-scale LLM alignment experiments beyond the small-scale linear setting tested in this work?
- Basis in paper: Appendix A.2 notes "more empirical, large-scale experiments would be helpful to compare greedy sampling with other methods further."
- Why unresolved: The computational simplicity of greedy sampling may have different trade-offs at scale. Optimism methods could provide better exploration in high-dimensional settings where empirical estimates are noisier.
- What evidence would resolve it: Comparative experiments on real LLM post-training tasks with standardized evaluation metrics.

### Open Question 4
- Question: What are the theoretical guarantees for greedy sampling under model misspecification, when the true preference model P* is not in the function class P?
- Basis in paper: Assumptions 1 and 2 require realizability (P* ∈ P and R* ∈ R), but this is not analyzed for misspecification.
- Why unresolved: Without realizability, the MLE estimate may not converge to the true preference model, and the decomposition in Lemma 2 may not bound the suboptimality gap correctly.
- What evidence would resolve it: A theoretical analysis characterizing regret and sample complexity under bounded misspecification error, or experiments showing robustness/fragility of greedy sampling when the function class is underspecified.

## Limitations
- The theoretical analysis assumes perfect realizability of the preference model, which may not hold in practice when modeling complex human preferences.
- The reference policy π₀ must be sufficiently stochastic to enable exploration; degenerate π₀ leads to undefined KL terms and breaks the bounded ratio property.
- The O(log T) regret bound includes a multiplicative exp(η) factor, which can dominate for large regularization strengths despite the logarithmic rate.

## Confidence
- **High Confidence**: The bounded likelihood ratio property under KL regularization and the resulting structural constraints on optimal policies are rigorously proven.
- **Medium Confidence**: The novel value decomposition appears sound but its tightness for specific preference models needs empirical validation.
- **Medium Confidence**: The online O(log T) regret bound assumes finite function classes; extensions to infinite classes introduce additional complexity terms.

## Next Checks
1. **Realizability Stress Test**: Evaluate performance when P* or R* lies outside the assumed function class, measuring degradation in regret and identifying breaking points.
2. **Coverage Coefficient Validation**: Empirically measure C(D₀,(π₀,π₀)) for various offline datasets and verify the predicted linear scaling with sample complexity.
3. **η Sensitivity Analysis**: Systematically vary η across multiple orders of magnitude to quantify the trade-off between reference adherence and optimal performance, validating the exp(η) dependence in bounds.