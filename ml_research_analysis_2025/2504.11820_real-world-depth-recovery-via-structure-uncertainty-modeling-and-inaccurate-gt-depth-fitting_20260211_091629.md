---
ver: rpa2
title: Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate
  GT Depth Fitting
arxiv_id: '2504.11820'
source_url: https://arxiv.org/abs/2504.11820
tags:
- depth
- structure
- maps
- recovery
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles real-world depth recovery by addressing structure
  misalignment in raw depth maps and inaccurate GT depth. The authors propose a novel
  framework with two main components: a raw depth generation pipeline that simulates
  random structure misalignments to prevent overfitting, and a structure uncertainty
  module that explicitly identifies misaligned structures in input raw depth maps
  using depth foundation models.'
---

# Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting

## Quick Facts
- arXiv ID: 2504.11820
- Source URL: https://arxiv.org/abs/2504.11820
- Authors: Delong Suzhang; Meng Yang
- Reference count: 40
- Key outcome: 18.75% improvement in AbsRel on RGBDD in-domain test, 81.5% δ1.05 accuracy on Middlebury 2014 generalization

## Executive Summary
This paper tackles real-world depth recovery by addressing two critical challenges: structure misalignment in raw depth maps and inaccurate ground truth depth. The authors propose a novel framework with a raw depth generation pipeline that simulates random structure misalignments to prevent overfitting, and a structure uncertainty module that explicitly identifies misaligned structures using depth foundation models. For output, they design a robust feature alignment module that aligns recovered depth with RGB image structure while mitigating interference from inaccurate GT depth.

## Method Summary
The method introduces a three-component pipeline: (1) a raw depth generation pipeline applying random elastic transformations to simulate diverse structural misalignments, (2) a structure uncertainty module that uses depth foundation model (Depth Anything V2) outputs to identify misaligned regions, and (3) a robust feature alignment module that aligns depth recovery with RGB structure using coordinate-based attention. The pipeline is trained on RGBDD for in-domain performance and mixed datasets for generalization, using a combination of L1, relative, and multi-scale gradient losses with stochastic depth regularization.

## Key Results
- Achieves 18.75% improvement in AbsRel on RGBDD in-domain test
- Demonstrates 81.5% δ1.05 accuracy on Middlebury 2014 generalization test
- Shows strong performance across various challenging raw depth conditions including ToF sensor data, stereo-matching distortions, and noisy depth super-resolution tasks

## Why This Works (Mechanism)

### Mechanism 1
- Random elastic transformation of ground truth depth maps during training reduces overfitting to specific misalignment patterns, improving generalization to unseen real-world distortions
- Core assumption: Elastic transformation approximates the distribution of real-world structural misalignments from ToF sensors and stereo matching errors
- Evidence: Table VI shows combining NSR + RSM achieves AbsRel 0.042 vs 0.108 baseline on Middlebury generalization

### Mechanism 2
- Depth foundation model as auxiliary input enables more accurate identification of structurally misaligned regions in raw depth maps
- Core assumption: DFMs capture generic depth structure that remains reliable even when raw depth sensors fail
- Evidence: Table VII shows DFM reduces AbsRel from 0.087 to 0.042 on Middlebury generalization

### Mechanism 3
- Coordinate-based feature alignment with stochastic depth regularization prevents overfitting to inaccurate ground truth depth boundaries
- Core assumption: RGB edges accurately correspond to depth discontinuities; GT depth contains random boundary errors
- Evidence: Table X shows stochastic depth achieves best AbsRel (0.013) vs dropout (0.014) or no regularization (0.017) on RGBDD

## Foundational Learning

- Concept: Elastic transformation for data augmentation
  - Why needed here: Core technique for simulating diverse structural misalignments; requires understanding of displacement field generation and Gaussian blur smoothing
  - Quick check question: Can you explain why applying Gaussian blur to random noise creates smooth, realistic deformation fields rather than chaotic distortions?

- Concept: Depth foundation models (MiDaS, Depth Anything V2)
  - Why needed here: Used as frozen feature extractors to provide generic depth structure guidance; understanding their pre-training and relative depth output is essential
  - Quick check question: Why do DFMs output relative depth rather than metric depth, and how does this affect their use as uncertainty guidance?

- Concept: Coordinate encoding in neural networks
  - Why needed here: The feature alignment module uses explicit coordinate relationships between query and target pixels; understanding this is critical for implementing the MLP-based attention mechanism
  - Quick check question: How does concatenating relative coordinates (X, Y offsets) to feature vectors help the MLP learn spatial relationships?

## Architecture Onboarding

- Component map:
```
Input: [RGB, Raw Depth] → [DFM (frozen)] → Structure Uncertainty Module
                                  ↓
                         [Uncertainty Mask]
                                  ↓
[RGB + Masked Raw Depth + DFM Output] → Backbone (U-Net/ViT/ConvNeXt)
                                  ↓
         [RGB Features + Depth Features] → Feature Alignment Module → Recovered Depth
```

- Critical path:
  1. Load pre-trained DFM (Depth Anything V2 recommended)
  2. Train structure uncertainty module first (cross-entropy, threshold τ = 0.1 × max(GT))
  3. Apply uncertainty mask to raw depth (set uncertain regions to 0)
  4. Train full pipeline with L1 + relative + multi-scale gradient loss (λ = 0.5)

- Design tradeoffs:
  - **Backbone choice**: ConvNeXt achieves best results (AbsRel 0.013) vs U-Net (0.015) but has higher compute cost
  - **Query distance f**: f=5 optimal; smaller values (f=1) lose context, larger (f=7) introduce noise
  - **Hidden dimension C**: C=64 best; C=128 overfits
  - **Regularization**: Stochastic depth outperforms dropout/dropblock for GT noise handling

- Failure signatures:
  - Over-smoothed boundaries: Feature alignment weight decay too high or query distance too large
  - Preserved wrong structures: Uncertainty module threshold too conservative; try reducing τ
  - Artifacts at mask boundaries: Binary masking too harsh; consider soft masking with uncertainty values
  - Poor generalization to new sensors: Elastic transformation parameters don't match new sensor characteristics

- First 3 experiments:
  1. **Ablation on uncertainty threshold τ**: Test values in [0.05, 0.1, 0.15, 0.2] × max(GT) on validation split
  2. **Backbone comparison**: Train identical pipeline with U-Net, ViT, ConvNeXt on mixed training data
  3. **Stress test on extreme distortions**: Generate synthetic depth with 2× the elastic transformation magnitude during testing

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do errors in the depth foundation model (DFM) propagate to the final depth recovery results, and can the learnable network fully compensate for incorrect relative depth estimations? The paper notes the auxiliary network is intended to mitigate errors but doesn't quantify robustness against DFM failure modes.

### Open Question 2
Can the proposed elastic transformation and random rectangular masking strategy accurately simulate physics-based sensor artifacts (e.g., ToF flying pixels or multi-path interference) which possess distinct spatial-frequency characteristics? The simulation relies on generic geometric deformations which may not capture physical generation mechanisms of sensor-specific artifacts.

### Open Question 3
What is the quantitative tolerance limit of the Robust Feature Alignment Module regarding Ground Truth (GT) noise before the stochastic depth regularization strategy fails to prevent model degradation? The paper demonstrates stochastic depth helps but doesn't define the breaking point where GT noise disrupts RGB-guided structural alignment.

## Limitations
- The approach's reliance on Depth Anything V2 introduces potential failure modes when the DFM's relative depth estimates are systematically biased in the same regions where raw depth sensors fail
- The structure uncertainty module's binary masking strategy may be too aggressive, potentially discarding useful raw depth information
- The method may not generalize well to completely different sensor types without retraining with appropriate elastic transformation parameters

## Confidence
- **High Confidence**: Claims about elastic transformation effectiveness for preventing overfitting are well-supported by ablation studies
- **Medium Confidence**: Claims about structure uncertainty module rely on assumption that DFM outputs remain reliable when raw depth fails
- **Medium Confidence**: Claims about feature alignment module performance are supported but lack comparison to alternative alignment methods

## Next Checks
1. **DFM Failure Mode Analysis**: Systematically test the pipeline on datasets where Depth Anything V2 is known to perform poorly to verify the structure uncertainty module doesn't propagate DFM errors
2. **Soft Masking Ablation**: Replace the binary masking strategy with soft uncertainty weighting to preserve potentially useful raw depth information
3. **Cross-Sensor Generalization**: Test the model trained on RGBDD on a completely different sensor type (e.g., structured light) to verify generalization beyond training sensor distribution