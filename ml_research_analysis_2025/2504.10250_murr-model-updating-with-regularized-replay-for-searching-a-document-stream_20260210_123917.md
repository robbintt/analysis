---
ver: rpa2
title: 'MURR: Model Updating with Regularized Replay for Searching a Document Stream'
arxiv_id: '2504.10250'
source_url: https://arxiv.org/abs/2504.10250
tags:
- retrieval
- session
- replay
- documents
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating neural retrieval
  models to handle evolving topics and language distributions in streaming document
  environments without requiring costly re-indexing of previously processed documents.
  The authors propose MURR (Model Updating with Regularized Replay), a strategy that
  combines continued fine-tuning with regularized replay of document representations
  to ensure compatibility between updated models and existing indexes.
---

# MURR: Model Updating with Regularized Replay for Searching a Document Stream

## Quick Facts
- arXiv ID: 2504.10250
- Source URL: https://arxiv.org/abs/2504.10250
- Reference count: 40
- MURR-CF achieves macro-averaged Success@5 of 0.483, significantly outperforming baseline strategies

## Executive Summary
This paper addresses the challenge of updating neural retrieval models to handle evolving topics and language distributions in streaming document environments without requiring costly re-indexing of previously processed documents. The authors propose MURR (Model Updating with Regularized Replay), a strategy that combines continued fine-tuning with regularized replay of document representations to ensure compatibility between updated models and existing indexes. MURR works by fine-tuning the model on new training data while applying a regularization loss that encourages the current model to reproduce the document representations generated by earlier models. Experiments using simulated document and query streams derived from the LoTTe dataset show that MURR significantly outperforms baseline strategies, achieving better and more stable retrieval effectiveness across sessions.

## Method Summary
MURR updates neural dual-encoder retrieval models in streaming document environments by combining continued fine-tuning with regularized replay. At each session boundary, the model is fine-tuned on new data using a contrastive loss while applying L2 regularization to maintain compatibility with previous document representations. A small replay buffer (200 triples) from prior sessions is used to mitigate catastrophic forgetting. Documents are indexed per session using the session-specific model, and queries are encoded with the latest model and searched across all indexes with simple score-based merging. The approach uses DPR with 768-dim [CLS] embeddings, CoCondenser initialization, and FAISS indexes with 4-bit product quantization.

## Key Results
- MURR-CF achieves macro-averaged Success@5 of 0.483, significantly outperforming baseline strategies
- Regularization loss is essential - without it, model performance degrades on old domains
- Even small replay buffers (200 triples) significantly improve stability and prevent catastrophic forgetting
- MURR maintains consistent performance improvements across all 5 sessions in the simulated streams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularizing document representations to remain consistent with past model outputs preserves backward compatibility with existing indexes without re-encoding.
- Mechanism: At session s, MURR applies an L2 regularization loss (Equation 2) that penalizes deviation between the current model's document embeddings and those stored from previous sessions. This anchors the representation space so queries encoded by the current model remain semantically aligned with older index partitions.
- Core assumption: Document vectors from earlier sessions can be approximated sufficiently well by a later model without degrading new-domain adaptation.
- Evidence anchors: [abstract] "regularization loss that encourages the current model to reproduce the document representations generated by earlier models" [section 3.2] Equation 2 defines the representation regularization term explicitly [corpus] No direct corpus validation; neighboring papers focus on retrieval coherence and contrastive learning but not streaming compatibility
- Break condition: If the representation drift required for new topics exceeds the regularization tolerance, the model may underfit both old and new domains.

### Mechanism 2
- Claim: Replaying a small subset of training triples from prior sessions mitigates catastrophic forgetting while preserving capacity for new domain learning.
- Mechanism: At each session, MURR samples 200 replay triples from prior sessions and combines them with new training data under a unified contrastive loss (Equation 3). The replay acts as a memory buffer, keeping decision boundaries calibrated across domains.
- Core assumption: A small replay buffer (e.g., 200 triples) is sufficient to approximate the joint distribution of past and present data.
- Evidence anchors: [section 3.2] "we retain 200 replay triples" and "aggregated replay training set" [section 5.2] Ablation shows 0 replay triples leads to degradation, but even small replay suffices [corpus] Related work on contrastive learning (e.g., DOGR) emphasizes document-oriented signals but does not address streaming replay
- Break condition: If replay buffer size is too small relative to distribution shift magnitude, the model may still forget rare but important patterns from early sessions.

### Mechanism 3
- Claim: Maintaining per-session indexes and merging results at search time avoids the cost of full re-indexing while supporting temporal heterogeneity.
- Mechanism: Each session's documents are encoded by that session's model and stored in a separate ANN index (Is). At query time, the current model encodes the query, searches all indexes I0...Is, and merges ranked lists by raw similarity score.
- Core assumption: Score distributions across indexes are comparable enough that simple score-based merging is effective without normalization or fusion heuristics.
- Evidence anchors: [section 3.3] "we do not need to retain or serve any previously trained model... or reprocess any documents" [section 4.2] "Since no document is indexed more than once, we do not need fusion approaches" [corpus] No corpus papers validate cross-index score compatibility; this remains an open assumption
- Break condition: If score scales drift significantly across models (e.g., due to representation norm changes), merged rankings may become unreliable.

## Foundational Learning

- Concept: Dual-encoder dense retrieval (e.g., DPR)
  - Why needed here: MURR operates on dual-encoder architectures where query and document representations must remain compatible across time.
  - Quick check question: Can you explain why a query encoded by model Ms may fail to retrieve documents encoded by model Mi (i < s)?

- Concept: Contrastive learning (SimCLR-style loss)
  - Why needed here: MURR's primary training objective is contrastive; understanding this loss is essential to see how replay data integrates.
  - Quick check question: What happens to the contrastive loss if all document embeddings collapse to a single point?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper frames the streaming problem as a forgetting risk; replay and regularization are countermeasures.
  - Quick check question: Why does fine-tuning only on new data ("LM without Replay") degrade performance on old documents even if the model architecture is unchanged?

## Architecture Onboarding

- Component map:
  - Session Manager -> Training Pipeline -> Replay Buffer -> Indexer -> Query Router
  - Session Manager triggers training and indexing at session boundaries
  - Training Pipeline combines contrastive loss (new + replay data) with representation regularization loss
  - Replay Buffer stores 200 triples per session with precomputed document embeddings
  - Indexer encodes incoming documents using current model and appends vectors to new FAISS index
  - Query Router encodes queries with latest model and dispatches to all existing indexes

- Critical path:
  1. At session end, sample replay triples and store their document embeddings
  2. Fine-tune Ms-1 using contrastive + regularization loss to produce Ms
  3. Encode and index new documents with Ms
  4. At query time, encode query with Ms, search all indexes, merge

- Design tradeoffs:
  - Replay buffer size vs. storage/latency: More replay improves stability (Figure 4a) but increases training time
  - Regularization strength (α): Too low loses compatibility; too high constrains new-domain adaptation (Figure 4b)
  - Index granularity: Finer session partitions improve model freshness but increase index count and search fanout

- Failure signatures:
  - Sudden retrieval drop for old queries: Indicates regularization too weak or replay too small
  - Stagnant performance on new domains: Suggests regularization too strong or learning rate too low
  - Inconsistent merged rankings: May signal score drift across indexes; consider per-index normalization

- First 3 experiments:
  1. Replicate the D2 scenario with α=0.01 and 200 replay triples; confirm Success@5 trajectory matches Figure 2b
  2. Ablate replay (set to 0) and observe degradation pattern; verify regularization alone is insufficient
  3. Vary α (e.g., 0, 0.0001, 0.1) and plot Success@5; identify the stability-effectiveness tradeoff curve

## Open Questions the Paper Calls Out

- Question: Why does the effectiveness of representation regularization depend on the specific stream characteristics?
  - Basis in paper: [explicit] In the ablation study, the authors observe that "the 0 replay triples case is significantly worse than any runs that use some replay" with varying gaps across scenarios (D1, D2, D3, DD), stating this "depends on the stream, which we leave for future investigation."
  - Why unresolved: The paper does not analyze the relationship between stream properties (e.g., topic overlap, shift magnitude) and regularization benefit.
  - What evidence would resolve it: Systematic experiments varying stream properties (domain similarity, shift abruptness) while measuring the contribution of the regularization term.

- Question: Do MURR's benefits generalize to real document streams with organic language distribution shifts?
  - Basis in paper: [explicit] The conclusion states that "experimenting on real document streams with real language distribution shifts can further empirically verify our findings" and mentions plans to evaluate on LongEval and Common Crawl.
  - Why unresolved: All experiments use simulated streams from LoTTe where distribution shifts are artificially controlled via beta-binomial sampling, not organic evolution.
  - What evidence would resolve it: Experiments on real-world streaming corpora (e.g., news archives, web crawls) with temporally-aligned queries.

- Question: How does MURR perform over longer streams with more sessions where catastrophic forgetting becomes more pronounced?
  - Basis in paper: [inferred] The authors note "the model memory likely has a limit and will eventually move on from knowledge acquired during the early sessions," but acknowledge "our experimental setup only simulates a short stream, it cannot clearly demonstrate this effect."
  - Why unresolved: Experiments only cover 5 sessions, limiting insight into long-term degradation or saturation effects.
  - What evidence would resolve it: Experiments extending to 20+ sessions with analysis of when replay and regularization fail to prevent forgetting.

- Question: Does MURR generalize to other dense retrieval architectures beyond single-vector DPR models?
  - Basis in paper: [inferred] The paper uses only DPR with [CLS] token representations, while other architectures like ColBERT use multi-vector late interaction. The regularization loss assumes single document vectors (Equation 2).
  - Why unresolved: Multi-vector models have different index structures and compatibility requirements that may not align with the L2 regularization approach.
  - What evidence would resolve it: Adapting and evaluating MURR on ColBERT or other multi-vector architectures.

## Limitations

- Score Distribution Compatibility: The paper assumes that simple score-based merging across per-session indexes is effective, but does not validate whether score distributions remain comparable across models as the stream progresses.
- Replay Buffer Adequacy: While 200 replay triples per session are shown to be sufficient in ablation studies, the paper does not explore whether this buffer size scales appropriately for larger datasets or more severe distribution shifts.
- Beta-Binomial Parameters: The exact parameters used to generate the simulated streams from LoTTe are not specified, making exact reproduction challenging.

## Confidence

- High Confidence: The core mechanism of using representation regularization to maintain backward compatibility with existing indexes is well-supported by the mathematical formulation and experimental results showing improved Success@5 metrics.
- Medium Confidence: The effectiveness of the small replay buffer (200 triples) is supported by ablation studies, but the generalizability to other datasets and stream dynamics remains uncertain.
- Low Confidence: The assumption that score distributions across indexes remain comparable enough for simple score-based merging without normalization is not validated and represents a potential failure point.

## Next Checks

1. **Cross-Index Score Validation**: Run MURR on a controlled dataset where the representation norms and score distributions are monitored across sessions. Verify that scores remain comparable without requiring normalization.

2. **Replay Buffer Scaling**: Test MURR with varying replay buffer sizes (e.g., 50, 200, 500, 1000 triples) on a dataset with known distribution shift characteristics to identify the minimum effective buffer size and scaling requirements.

3. **Real-World Streaming Test**: Apply MURR to a real document stream (e.g., news articles or social media posts) with temporal topic evolution, and measure both retrieval effectiveness and index-compatibility over extended time periods.