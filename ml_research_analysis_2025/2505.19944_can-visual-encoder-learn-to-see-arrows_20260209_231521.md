---
ver: rpa2
title: Can Visual Encoder Learn to See Arrows?
arxiv_id: '2505.19944'
source_url: https://arxiv.org/abs/2505.19944
tags:
- image
- diagram
- edge
- encoder
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether vision-language models can learn
  to recognize edges (lines/arrows) in diagrams. The authors hypothesize that failures
  stem from over-reliance on textual and positional biases in training data.
---

# Can Visual Encoder Learn to See Arrows?

## Quick Facts
- arXiv ID: 2505.19944
- Source URL: https://arxiv.org/abs/2505.19944
- Authors: Naoyuki Terashita; Yusuke Tozaki; Hideaki Omote; Congkha Nguyen; Ryosuke Nakamoto; Yuta Koreeda; Hiroaki Ozaki
- Reference count: 29
- One-line primary result: Fine-tuned VLMs can achieve >85% edge direction accuracy and 0.966 F1-score on diagram captioning by removing textual and positional biases through contrastive learning

## Executive Summary
This paper investigates whether vision-language models can learn to recognize edges (lines/arrows) in diagrams. The authors hypothesize that failures stem from over-reliance on textual and positional biases in training data. To test this, they create a synthetic dataset of diagrams with captions that eliminate these biases, then perform contrastive learning to fine-tune CLIP image encoders. They evaluate the trained models on three tasks: linear probing (edge classification), image retrieval (finding diagrams with identical graph structures), and diagram captioning (generating Mermaid-style edge descriptions). Results show the fine-tuned models significantly outperform pretrained CLIP on all tasks, with edge direction classification accuracy improving from near-chance to over 85%. On diagram captioning, the fine-tuned ViT-L/14 achieves an F1-score of 0.966, surpassing zero-shot GPT-4o and LLaVA-Mistral. This demonstrates that removing textual and positional biases enables VLMs to learn explicit edge representations, advancing diagram understanding.

## Method Summary
The authors generate 100k synthetic diagram-caption pairs by sampling random directed graphs (2-8 nodes labeled A-H) with force-directed layouts and Mermaid-format captions describing edges. They fine-tune CLIP image encoders via contrastive learning on these debiased pairs, then evaluate on three tasks: (1) linear probing for node/edge/direction classification, (2) image retrieval using cosine similarity between diagram embeddings, and (3) diagram captioning with a GPT-2 decoder using cross-attention over frozen encoder embeddings. The synthetic data removes textual biases (random node labels) and positional biases (randomized layouts) to force the encoder to learn visual edge features rather than exploiting correlations.

## Key Results
- Edge direction classification accuracy improves from near-chance (~50%) to over 85% with fine-tuned models
- Image retrieval MAP@100 increases from 0.131 to 0.996, demonstrating learned position-invariance
- Diagram captioning F1-score reaches 0.966 for fine-tuned ViT-L/14, outperforming zero-shot GPT-4o and LLaVA-Mistral
- Linear probing shows fine-tuned models encode edge features that pretrained CLIP lacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing textual and positional biases from training data forces visual encoders to learn actual edge features rather than relying on statistical shortcuts.
- Mechanism: When diagrams contain meaningful node labels (e.g., "HUB" connecting to components) or follow conventional layouts (top-down hierarchies), models exploit these correlations instead of learning visual edge representations. By generating synthetic diagrams with random node labels (A–H) and randomized force-directed layouts, the model cannot predict edges from text semantics or spatial priors—contrastive learning must encode the actual visual geometry of lines and arrows to succeed.
- Core assumption: The failure of pretrained CLIP on edge recognition stems from dataset biases, not fundamental architectural limitations of ViT encoders.
- Evidence anchors:
  - [abstract] "We hypothesize that these failures stem from an over-reliance on textual and positional biases, preventing VLMs from learning explicit edge features."
  - [Page 1, Fig. 2] GPT-4o succeeds when positional or textual cues exist but fails when neither is available.
  - [corpus] Related work (Rahmanzadehgervi et al.) confirms VLMs fail at simple visual tasks like line intersections; Singh et al. show layout flips degrade flowchart VQA.
- Break condition: If pretrained CLIP's edge performance remained poor even after debiased training, the hypothesis would be falsified—suggesting architectural rather than data-driven limitations.

### Mechanism 2
- Claim: Contrastive learning on debiased diagram–caption pairs induces position-invariant graph structure representations in image embeddings.
- Mechanism: Each training pair maps a diagram image to a Mermaid-format caption describing its directed edges (e.g., "A --> B"). Because identical graph structures can produce visually distinct layouts (random force-directed initialization), the contrastive loss pushes the encoder toward features invariant to node positions—capturing abstract connectivity rather than pixel-level layout.
- Core assumption: The encoder has sufficient capacity to learn edge representations when forced by the training signal.
- Evidence anchors:
  - [Page 2, Sec. 3.1] "This initialization ensures that the same graph can produce diagram images with different layouts."
  - [Page 3, Table 1] Image retrieval MAP improves from 0.131 to 0.996, demonstrating learned position-invariance.
  - [corpus] Weak direct corpus evidence on this specific mechanism; related work on graph self-supervised learning (Graph Positional Autoencoders) suggests similar principles but in different domains.
- Break condition: If retrieval performance remained layout-dependent, contrastive learning alone would be insufficient—suggesting need for explicit graph-aware architectures.

### Mechanism 3
- Claim: Learned edge representations transfer to downstream tasks via frozen encoder features, with the bottleneck in visual encoding rather than decoder capacity.
- Mechanism: The fine-tuned encoder produces embeddings rich in edge information; a lightweight GPT-2 decoder with cross-attention can extract this into text. Pretrained CLIP embeddings lack these features, so decoder adaptation alone cannot compensate.
- Core assumption: Edge information is fully encoded in the image embeddings, not requiring task-specific encoder fine-tuning for captioning.
- Evidence anchors:
  - [Page 4, Table 2] Fine-tuned ViT-L/14 + GPT-2 achieves F1=0.966 vs. pretrained ViT-L/14 + GPT-2 at F1=0.668 with identical decoder training.
  - [Page 4, Sec. 4.3] "The lower performance of the pretrained ViT-L/14...indicates that simply adapting the decoder is not enough; the bottleneck lies in the image encoder."
  - [corpus] Chain-of-Visual-Thought paper highlights VLMs' perceptual limitations, consistent with encoder bottlenecks.
- Break condition: If decoder-only baselines matched fine-tuned encoder performance, the mechanism would shift from encoder improvement to decoder adaptation.

## Foundational Learning

- Concept: **Contrastive Learning (CLIP-style)**
  - Why needed here: The training method aligns image and text embeddings by pulling matching pairs together and pushing non-matching pairs apart—understanding this is essential to see how debiased data creates the learning pressure.
  - Quick check question: Can you explain why contrastive loss on randomized-layout diagrams would force the encoder to ignore position?

- Concept: **Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: CLIP's image encoder is a ViT; understanding patch-based processing helps diagnose whether edge detection happens at patch level or requires cross-patch attention.
  - Quick check question: How might a ViT's self-attention mechanism capture arrowhead geometry that spans multiple patches?

- Concept: **Linear Probing for Feature Evaluation**
  - Why needed here: The paper uses linear probing to isolate whether features contain edge information, independent of downstream decoder capacity. Interpreting results requires understanding what linear separability implies about representation quality.
  - Quick check question: Why does linear probing rather than end-to-end fine-tuning better isolate the encoder's learned representations?

## Architecture Onboarding

- Component map:
  - Data Generator -> Contrastive Trainer -> CLIP Dual Encoder -> Evaluation Suite
  - Random directed graphs -> Batch-wise image-text alignment loss -> ViT-B/32 or ViT-L/14 image encoder + text encoder -> (1) Linear probes for node/edge/direction classification, (2) Image retrieval via cosine similarity, (3) Captioning with GPT-2 decoder + cross-attention

- Critical path:
  1. Generate 100k debiased diagram–caption pairs (random graphs, force-directed layouts, Mermaid format)
  2. Fine-tune CLIP image encoder via contrastive learning (text encoder frozen)
  3. Freeze fine-tuned encoder; train task-specific heads (linear probes or GPT-2 decoder)
  4. Evaluate on held-out test set (10k samples) with edge-focused metrics

- Design tradeoffs:
  - **Synthetic vs. real diagrams**: Synthetic ensures perfect bias control but may not transfer to real-world diagram styles (fonts, colors, noise). Paper shows generalization to non-isomorphic graphs but real-domain transfer untested.
  - **Encoder size**: ViT-L/14 outperforms ViT-B/32 on captioning (0.966 vs. 0.516 F1), suggesting capacity matters for edge representation.
  - **Frozen vs. fine-tuned encoder for downstream**: Paper freezes encoder after contrastive training; joint fine-tuning might improve captioning further but complicates evaluation isolation.

- Failure signatures:
  - **Near-chance edge direction accuracy (~50%)**: Encoder relying on text/position shortcuts (baseline CLIP signature)
  - **High node accuracy, low edge accuracy**: Model can read labels but not see connections
  - **Retrieval clusters by layout not structure**: Position-invariance not achieved
  - **Captioning outputs plausible but wrong edges**: Decoder hallucinating without visual grounding

- First 3 experiments:
  1. **Linear probing sanity check**: Train logistic regression on pretrained CLIP features for edge direction. If accuracy ≈ 50%, confirm baseline lacks edge features; then probe fine-tuned model to validate improvement.
  2. **Bias ablation**: Train with partially biased data (e.g., fixed layouts but random labels, or vice versa) to isolate which bias removal contributes most to edge learning.
  3. **Cross-domain transfer test**: Evaluate fine-tuned encoder on real diagrams (flowcharts, circuit diagrams) outside synthetic distribution to assess generalization limits.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset realism gap: Synthetic diagrams with A-H labels and uniform styling may not generalize to real-world diagrams with domain-specific symbols, text styles, and noise.
- Architecture ceiling: Improvements shown are from debiased training, but deeper architectural changes might yield further gains; the current approach assumes ViT capacity is sufficient once biases are removed.
- Hyperparameter sensitivity: Critical training details like edge generation probability, learning rates, and optimization schedules are unspecified, making exact reproduction difficult.

## Confidence
- **High confidence**: The core empirical findings showing fine-tuned models significantly outperform pretrained CLIP on all three tasks, with edge direction accuracy improving from near-chance to over 85%.
- **Medium confidence**: The claim that removing textual and positional biases is the primary mechanism for edge learning, as alternative explanations cannot be fully ruled out.
- **Low confidence**: Generalization to real-world diagrams beyond the synthetic domain tested, particularly for complex diagrams with varied visual styles and semantic complexity.

## Next Checks
1. **Real-domain transfer test**: Evaluate the fine-tuned encoder on a small benchmark of real flowcharts, circuit diagrams, or UML diagrams to measure performance drop from synthetic to natural distributions.

2. **Bias ablation study**: Train models with different bias configurations (fixed layouts with random labels, meaningful labels with random layouts, etc.) to quantify which bias removal contributes most to edge learning improvements.

3. **Architecture scaling study**: Compare the debiased CLIP approach against graph-specific architectures (Graph Transformers, GNNs) on the same tasks to establish whether debiasing alone matches architectural specialization.