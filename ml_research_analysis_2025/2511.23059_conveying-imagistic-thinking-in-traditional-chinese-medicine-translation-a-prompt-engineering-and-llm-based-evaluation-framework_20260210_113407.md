---
ver: rpa2
title: 'Conveying Imagistic Thinking in Traditional Chinese Medicine Translation:
  A Prompt Engineering and LLM-Based Evaluation Framework'
arxiv_id: '2511.23059'
source_url: https://arxiv.org/abs/2511.23059
tags:
- translation
- cognitive
- translations
- case
- metaphor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used prompt engineering and LLM-based evaluation to
  improve translation of TCM classics. DeepSeek V3.1 was guided to identify metaphor
  and metonymy in four Huangdi Neijing passages and generate translations.
---

# Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework

## Quick Facts
- arXiv ID: 2511.23059
- Source URL: https://arxiv.org/abs/2511.23059
- Authors: Jiatong Han
- Reference count: 0
- This study used prompt engineering and LLM-based evaluation to improve translation of TCM classics.

## Executive Summary
This study presents a novel framework for translating Traditional Chinese Medicine (TCM) classics by combining prompt engineering with large language model (LLM) evaluation. The research focuses on translating imagistic thinking patterns from the Huangdi Neijing, particularly metaphors and metonymies, into English while preserving cognitive accessibility for different reader types. Using DeepSeek V3.1 for translation and ChatGPT 5 Pro and Gemini 2.5 Pro for evaluation, the framework demonstrates that prompt-adjusted translations significantly outperform both baseline machine translations and human translations across multiple cognitive dimensions.

## Method Summary
The study employed DeepSeek V3.1 to generate translations of four selected passages from the Huangdi Neijing after being prompted to identify and address metaphor and metonymy challenges. Three English-speaking reader personas were simulated using ChatGPT 5 Pro and Gemini 2.5 Pro: a TCM beginner, an advanced practitioner, and a professional translator. These LLM evaluators assessed three translation versions (baseline, human, and prompt-adjusted) across five cognitive dimensions. The research combined quantitative scoring with thematic analysis to compare translation approaches and identify effective strategies for conveying imagistic thinking in TCM translation.

## Key Results
- Prompt-adjusted translations outperformed both baseline machine translations and human translations across all evaluation metrics
- High cross-model and cross-role consistency was observed among LLM evaluators, suggesting robust evaluation criteria
- Thematic analysis revealed distinct differences between human and machine translation approaches to handling metaphor-metonymy transfer
- Reader cognitive preferences were identified, providing insights into effective translation strategies for different audience levels

## Why This Works (Mechanism)
The framework works by explicitly addressing the cognitive challenges inherent in translating imagistic thinking from TCM classics. By prompting DeepSeek to identify metaphorical and metonymical elements before translation, the model can apply targeted strategies to preserve conceptual relationships. The LLM evaluators, simulating different reader perspectives, provide multidimensional feedback that captures both technical accuracy and cognitive accessibility. This approach bridges the gap between literal translation and meaningful cross-cultural communication of complex medical concepts.

## Foundational Learning
- **Imagistic Thinking in TCM**: TCM relies heavily on metaphorical and metonymical language to convey complex medical concepts; understanding these patterns is essential for accurate translation
- **Prompt Engineering for Translation**: Strategic prompting can guide LLMs to identify and address specific translation challenges, improving output quality
- **Multidimensional Evaluation**: Using multiple evaluator perspectives (beginner, practitioner, translator) provides comprehensive assessment of translation effectiveness
- **Cross-Model Consistency**: Agreement across different LLMs suggests reliable evaluation criteria and reduces bias from single-model assessment
- **Cognitive Accessibility**: Translations must preserve not just literal meaning but also conceptual relationships and accessibility for target readers
- **TCM-English Conceptual Mapping**: Successful translation requires bridging culturally specific medical concepts with their English conceptual equivalents

## Architecture Onboarding
Component Map: DeepSeek V3.1 (Translation) -> LLM Evaluators (ChatGPT 5 Pro, Gemini 2.5 Pro) -> Cognitive Assessment -> Thematic Analysis

Critical Path: Text selection → Prompt engineering → Translation generation → LLM evaluation → Scoring aggregation → Thematic analysis → Strategy identification

Design Tradeoffs: The framework prioritizes cognitive accessibility over literal accuracy, using simulated readers instead of human evaluators for scalability, and focuses on specific text passages rather than comprehensive corpus analysis.

Failure Signatures: Poor performance would manifest as low cross-model consistency, negative thematic analysis results, or failure to improve upon baseline translations.

First Experiments:
1. Test framework on a larger corpus of TCM texts to assess generalizability
2. Compare LLM evaluation results with actual human reader comprehension tests
3. Apply framework to other concept-dense traditional medical texts from different cultures

## Open Questions the Paper Calls Out
None

## Limitations
- Small corpus of only four Huangdi Neijing passages limits statistical power and may not represent full diversity of TCM imagistic language
- Evaluation relies on simulated reader responses from LLMs rather than actual human readers, raising ecological validity questions
- Study does not address potential cultural and philosophical nuances that may be lost when TCM concepts are mediated through English-speaking LLM evaluators
- Comparison focuses on translation quality without examining downstream impacts on clinical understanding or practice

## Confidence
- **High confidence**: Prompt engineering approach improves translation quality compared to baseline and human translations for the tested passages
- **Medium confidence**: Cross-model and cross-role consistency among LLM evaluators reflects meaningful evaluation criteria
- **Medium confidence**: Thematic analysis identifies genuine differences between human and machine translation approaches
- **Low confidence**: Findings generalize to broader TCM corpus or other concept-dense traditional texts

## Next Checks
1. Replicate the study with a larger, more diverse TCM corpus including multiple classical texts and contemporary medical literature
2. Conduct parallel evaluation using actual English-speaking TCM practitioners and scholars to validate LLM-simulated reader responses
3. Implement longitudinal study tracking whether prompt-engineered translations improve comprehension and retention among target readers over time