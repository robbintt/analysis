---
ver: rpa2
title: 'Shadows in the Attention: Contextual Perturbation and Representation Drift
  in the Dynamics of Hallucination in LLMs'
arxiv_id: '2505.16894'
source_url: https://arxiv.org/abs/2505.16894
tags:
- hallucination
- drift
- hallucinations
- attention
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically examines how incremental context injection
  induces internal-state drift leading to hallucinations in LLMs. Using TruthfulQA,
  it constructs 16-round "titration" tracks with relevant and misleading contexts
  for six open-source models.
---

# Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs

## Quick Facts
- arXiv ID: 2505.16894
- Source URL: https://arxiv.org/abs/2505.16894
- Reference count: 31
- Key finding: Systematic context injection induces internal-state drift leading to hallucinations in LLMs, with an attention-locking threshold beyond which errors solidify.

## Executive Summary
This study investigates how incremental context injection causes internal-state drift and hallucinations in large language models. Using a 16-round titration design with relevant and misleading contexts from TruthfulQA, the researchers track hallucination incidence and internal dynamics across six open-source models. They employ a tri-perspective detector and four drift metrics (cosine, entropy, JS, Spearman) applied to hidden states and attention maps. Results reveal that hallucination rates grow monotonically, plateauing after 5-7 rounds, with distinct error modes emerging based on context relevance. The study identifies a critical "attention-locking" threshold where hallucinations solidify and resist correction, providing empirical foundations for intrinsic hallucination prediction and context-aware mitigation strategies.

## Method Summary
The researchers constructed 16-round "titration" tracks using TruthfulQA contexts, injecting relevant and misleading information incrementally into six open-source LLM architectures. They employed a tri-perspective hallucination detector combining perplexity analysis, semantic consistency checking, and factuality verification to identify hallucination instances. Four drift metrics—cosine similarity, entropy, Jensen-Shannon divergence, and Spearman correlation—were applied to hidden states and attention maps to quantify internal representation changes. The methodology systematically compared how models processed relevant versus irrelevant contexts, tracking the progression of hallucinations through multiple rounds and identifying critical thresholds where internal representations converged toward hallucinatory states.

## Key Results
- Hallucination rates increase monotonically with context rounds, plateauing after 5-7 rounds regardless of context type
- Relevant context produces high-confidence "self-consistent" errors, while irrelevant context causes topic-drift hallucinations
- An attention-locking threshold emerges when JS-Drift (~0.69) and Spearman-Drift (~0) converge, beyond which hallucinations solidify and resist correction
- Larger models better distinguish relevant from irrelevant information, while smaller models compensate with increased attention diffusion, raising concatenation-type hallucination risk

## Why This Works (Mechanism)
The mechanism centers on how incremental context injection causes internal-state drift in LLM representations. As new contexts are added, hidden states and attention maps progressively diverge from their initial configurations. The study identifies that this drift follows predictable patterns: relevant contexts cause models to reinforce existing patterns (leading to self-consistent errors), while irrelevant contexts cause representation drift toward unrelated topics. The critical insight is that once internal representations cross specific divergence thresholds—measured by JS-Drift and Spearman-Drift—they become "locked" into hallucinatory patterns that resist subsequent correction attempts. This attention-locking phenomenon explains why early context contamination can have lasting effects on model outputs.

## Foundational Learning
**Attention mechanisms**: How self-attention computes weighted representations of input tokens through query-key-value operations, essential for understanding how models process context sequences and why attention maps are sensitive to context drift.

**Representation drift**: The progressive divergence of hidden states and attention patterns as new information is processed, fundamental to understanding how internal states transition from accurate to hallucinatory configurations.

**Context contamination**: The phenomenon where early misleading information influences subsequent processing and interpretation, creating cascading errors that become increasingly difficult to correct.

**Internal consistency vs external accuracy**: The tension between maintaining coherent internal representations versus producing factually accurate outputs, which explains why models often prefer self-consistent but incorrect answers.

**Drift metrics**: Mathematical tools (cosine similarity, entropy, JS divergence, Spearman correlation) for quantifying changes in internal representations, necessary for empirically tracking the progression toward hallucination.

**Titration methodology**: The systematic incremental injection of context to study cumulative effects, allowing researchers to map the temporal dynamics of hallucination development.

## Architecture Onboarding

**Component map**: Context injector -> LLM architecture -> Hidden state extractor -> Attention map analyzer -> Tri-perspective detector -> Drift metric calculator -> Hallucination classifier

**Critical path**: Context injection → hidden state/attention computation → drift metric calculation → hallucination detection → error mode classification

**Design tradeoffs**: The study prioritizes controlled experimental conditions over ecological validity, using synthetic contexts rather than natural language. This enables precise measurement of drift dynamics but may limit generalizability to real-world scenarios where context relationships are more complex.

**Failure signatures**: Hallucinations manifest as either self-consistent errors (high-confidence incorrect answers aligned with injected relevant context) or topic-drift errors (responses that veer into unrelated subjects due to irrelevant context contamination). The attention-locking threshold represents a critical failure point where internal states become resistant to correction.

**First experiments**: 1) Replicate the titration study with different model families to test threshold generalizability. 2) Apply the methodology to naturally occurring documents rather than synthetic contexts. 3) Implement real-time drift monitoring during open-ended generation tasks to test intervention efficacy.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Controlled synthetic contexts from TruthfulQA may not generalize to real-world open-ended generation tasks with complex context relationships
- The 16-round titration design represents an artificial scenario that may not mirror natural conversational or document processing flows
- Exclusive focus on open-source models limits generalizability to proprietary systems like GPT-4 or Claude
- The study does not address potential feedback loops where early hallucinations influence subsequent context interpretation

## Confidence

**High confidence**: Monotonic increase in hallucination rates with context rounds, existence of attention-locking threshold where hallucinations solidify, distinction between self-consistent and topic-drift error modes.

**Medium confidence**: Model-size-dependent error patterns and specific threshold values (JS-Drift ~0.69, Spearman-Drift ~0) may be influenced by architectural differences beyond parameter count.

**Low confidence**: Practical applicability of attention-locking threshold as real-time intervention mechanism requires further validation in dynamic, non-titration settings.

## Next Checks
1. Apply context titration methodology to naturally occurring documents and conversations rather than synthetic TruthfulQA contexts to assess pattern generalizability under realistic usage conditions.

2. Test whether attention-locking threshold (JS-Drift ~0.69, Spearman-Drift ~0) remains consistent across different model families and whether specific values are model-dependent.

3. Implement real-time monitoring using identified drift metrics during open-ended generation tasks and test whether interventions at predicted attention-locking threshold can prevent hallucination solidification in practice.