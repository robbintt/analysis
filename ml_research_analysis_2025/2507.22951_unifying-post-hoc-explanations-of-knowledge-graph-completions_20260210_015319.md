---
ver: rpa2
title: Unifying Post-hoc Explanations of Knowledge Graph Completions
arxiv_id: '2507.22951'
source_url: https://arxiv.org/abs/2507.22951
tags:
- explanations
- triples
- explanation
- prediction
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work unifies post-hoc explainability in Knowledge Graph Completion
  (KGC) by formalizing explanations as solutions to a multi-objective optimization
  problem balancing effectiveness and conciseness. Existing explainability methods
  are unified within this framework, addressing fragmentation in evaluation standards.
---

# Unifying Post-hoc Explanations of Knowledge Graph Completions

## Quick Facts
- **arXiv ID:** 2507.22951
- **Source URL:** https://arxiv.org/abs/2507.22951
- **Reference count:** 40
- **Primary result:** Unifies post-hoc explainability in Knowledge Graph Completion (KGC) by formalizing explanations as solutions to a multi-objective optimization problem balancing effectiveness and conciseness.

## Executive Summary
This work addresses the fragmentation in evaluating post-hoc explainability methods for Knowledge Graph Completion (KGC) by introducing a unified framework. The framework treats explanations as solutions to a multi-objective optimization problem, balancing effectiveness (rank change) and conciseness (explanation length). By providing a common formalization, it enables rigorous comparisons between diverse explainability algorithms and advocates for robust evaluation using complementary metrics like MΔR alongside MRR.

## Method Summary
The method involves training a ComplEx KGE model on FB15k-237, extracting explanations for 50 random top-ranked test triples using four algorithms (Kelpie, Data Poisoning, CRIAGE, AnyBURL Explainer), removing explanations from training, retraining the KGE, and evaluating with MΔR, MRR, Hits@1, and mean length. The framework defines a search space and uses a perturbator function to apply explanations, measuring effectiveness via rank change.

## Key Results
- Single metrics like MRR can be misleading; multi-metric evaluation is essential to capture explanation effectiveness and length trade-offs.
- The unified framework enables rigorous comparisons and reproducibility in KGC explainability research.
- Experiments show significant variations in explanation effectiveness across algorithms, with MΔR revealing nuances hidden by MRR.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Pareto Optimization
- **Claim:** If explanations are treated as solutions to a multi-objective problem balancing effectiveness and conciseness, cross-study comparisons become consistent even when algorithms output different explanation lengths.
- **Mechanism:** The framework defines a search space $\mathcal{Y}$ and an effectiveness function $\Psi$. It optimizes two competing objectives: minimizing explanation length $|X|$ and maximizing effectiveness $\Psi$. Solutions lie on a Pareto front, meaning a longer explanation must be significantly more effective to dominate a shorter one.
- **Core assumption:** Explanation quality is a trade-off; a user prefers a shorter explanation unless a longer one provides substantially more predictive power.
- **Evidence anchors:**
  - [Page 3, Problem 1] Formalizes the minimization of $|X|$ and maximization of $\Psi$.
  - [Page 3, Figure 1] Visualizes how restrictive algorithms solve single-objective problems while flexible ones trace the Pareto front.
  - [Corpus] Neighbors focus on KGC completion (e.g., KG-TRICK, LLM methods) rather than explainability theory, highlighting this framework's unique formalization.
- **Break condition:** If the effectiveness metric $\Psi$ (e.g., rank change) does not correlate with human interpretability, the optimization will mathematically converge on "optimal" explanations that are useless to users.

### Mechanism 2: Proxy-based C-Sufficiency
- **Claim:** If sufficient explanations are defined via transfer to a target set of entities $C$ (C-sufficiency), the framework avoids the intractability of retraining KGE models on minimal subgraphs.
- **Mechanism:** Instead of retraining on explanation $X$ alone (which fails due to lack of context), this mechanism swaps the subject $s_x$ in candidate triples to a new entity $c$. It measures if adding these swapped triples to the training set improves the rank of the target triple $(c, r_x, \hat{o}_x)$. If the prediction transfers, the explanatory logic is deemed sufficient.
- **Core assumption:** Assumption: If a pattern explains a prediction for entity $s_x$, it holds as a sufficient explanation if it can successfully transfer the prediction to a structurally similar entity $c$.
- **Evidence anchors:**
  - [Page 5, Definition 4] Defines C-sufficient explanation and the $s_x \to c$ swap mechanism.
  - [Page 5, Figure 3b] Illustrates the swap and retraining process.
  - [Corpus] Weak corpus support for this specific proxy; most related work focuses on completion accuracy rather than explanation transferability.
- **Break condition:** If the target entities $c$ are not semantically similar to $s_x$, the transferability test fails, potentially rejecting valid explanations or accepting spurious ones.

### Mechanism 3: M$\Delta$R (Mean Rank Difference) Sensitivity
- **Claim:** If evaluation relies solely on aggregate metrics like MRR or Hits@$k$, significant localized explanation effects may be hidden; M$\Delta$R restores sensitivity to individual rank shifts.
- **Mechanism:** MRR converts ranks to reciprocals (non-linear), dampening the perceived impact of rank changes in lower positions. M$\Delta$R calculates the arithmetic mean of raw rank differences ($\Delta R$), weighting a shift from rank 1 to 2 similarly to a shift from 100 to 101 in terms of relative displacement, capturing the raw perturbation effect.
- **Core assumption:** A consistent rank shift $\Delta R$ is equally meaningful regardless of the starting rank, which is true for measuring perturbation sensitivity but false for measuring user-facing utility (where top-rank changes matter more).
- **Evidence anchors:**
  - [Page 6, Table 1] Demonstrates how MRR can mislead by ordering algorithms differently than raw rank changes.
  - [Page 7, Table 2] Shows M$\Delta$R highlighting Kelpie's high perturbation impact vs. CRIAGE's low impact.
  - [Corpus] Nearby papers (e.g., "How Sharp and Bias-Robust is a Model?") discuss evaluation metrics but focus on predictive sharpness rather than explainability granularities.
- **Break condition:** In datasets with extreme rank outliers, M$\Delta$R becomes unstable and may over-represent the effectiveness of an explanation that only affects a few anomalous triples.

## Foundational Learning

- **Concept: Knowledge Graph Embeddings (KGE) & Score Functions**
  - **Why needed here:** The framework depends on retraining or perturbing a KGE model (specifically ComplEx in the paper) and measuring the output of its score function $\Phi_\theta(s, r, o)$. Without understanding that embeddings represent graph proximity, the "effectiveness" metric $\Psi$ lacks context.
  - **Quick check question:** If a KGE model scores a triple as 0.9, does that mean it is true? (Answer: Not necessarily; it indicates high plausibility relative to the model's training distribution).

- **Concept: Post-hoc vs. Ante-hoc Explainability**
  - **Why needed here:** The paper strictly limits scope to *post-hoc* methods (explaining an existing fixed model). Confusing this with interpretable model design (ante-hoc) will lead to misapplying the architecture, which assumes a black-box model $\theta$ is already trained and immutable except for the explanation extraction.
  - **Quick check question:** Does modifying the training data to find an explanation change the model parameters $\theta$? (Answer: In this framework, $F$ creates a new $\theta'$ for evaluation purposes only; the original model remains the reference point).

- **Concept: Adversarial Attacks vs. Generative Sampling**
  - **Why needed here:** The paper unifies "Data Poisoning" (adversarial) with "Latent Explanations" (generative). Understanding the difference is crucial for implementing the search space $\mathcal{Y}$.
  - **Quick check question:** Is a latent explanation sampled from a uniform distribution? (Answer: No, it must be sampled from a probabilistic distribution $q$ that prefers "likely" triples, making them plausible rather than random noise).

## Architecture Onboarding

- **Component map:**
  1. **Search Space ($\mathcal{Y}$):** The subset of triples (e.g., 1-hop neighbors, rules) to search. Constraints $C_k$ filter this space.
  2. **Perturbator ($F$):** A function that applies the explanation (removing $X$ for necessary, adding $X$ for latent/sufficient) and produces updated model parameters $\theta'$.
  3. **Evaluator ($\Psi$):** Computes effectiveness, typically rank change (Eq. 5).
  4. **Optimization Core:** A solver (e.g., Simulated Annealing) navigating the Pareto front.

- **Critical path:**
  Define Prediction $(s_x, r_x, \hat{o}_x)$ $\to$ Configure Search Constraints $\mathcal{Y}$ $\to$ Select Proxy Retraining Strategy $F$ (Crucial Choice) $\to$ Run Optimization $\to$ Report Pareto Front metrics ($|X|, \Psi$).

- **Design tradeoffs:**
  - **Retraining Proxy:** Exact retraining is accurate but too slow for large graphs. Taylor approximations (CRIAGE) are fast but linear; they fail to capture non-linear interactions in long explanations. Post-training (Kelpie) balances both but assumes local independence.
  - **Explanation Length:** Fixing $|X|=1$ allows comparison with older methods (CRIAGE) but misses complex multi-hop reasoning. Variable length is robust but computationally heavier.

- **Failure signatures:**
  - **Metric Masking:** High MRR with low M$\Delta$R indicates the explanation only affects tail-ranking triples, misleading evaluations focused solely on top-1 hits.
  - **Disconnected Components:** If Search Space $\mathcal{Y}$ includes triples in disconnected subgraphs (for non-GNN models), $\Psi$ will be 0 as they cannot affect the prediction entity $s_x$ (Rem. 2).

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run CRIAGE and Data Poisoning on a single top-ranked triple. Verify that the rank change reported by MRR is actually reflected in M$\Delta$R to confirm metric alignment.
  2. **Pareto Visualization:** Implement Kelpie (variable length) and plot Effectiveness vs. Length. Check if the "optimal" explanations lie on a distinct curve or if the algorithm gets stuck in local optima (single-objective behavior).
  3. **Latent Injection:** Replace the standard "unobserved triple" search in Data Poisoning with a simple probabilistic sampler (e.g., based on frequency). Compare if "likely" latent explanations shift the rank more effectively than random noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the unified framework be extended to optimize for multiple contrasting effectiveness functions (e.g., rank change vs. path-based preferences) simultaneously?
- **Basis in paper:** [explicit] Section 3 states, "An interesting direction is also to consider multiple effectiveness functions $\Psi_i$ encoding contrastive objectives... leading to an n-dimensional Pareto front. We leave this to future work."
- **Why unresolved:** Current implementations treat effectiveness as a scalar (rank change). Extending this to a vector of objectives complicates the optimization landscape and the definition of Pareto efficiency.
- **What evidence would resolve it:** An algorithm capable of generating explanation sets that lie on an n-dimensional Pareto front, balancing rank efficacy against semantic constraints like path coherence.

### Open Question 2
- **Question:** Can probabilistic generative models be effectively utilized to sample "latent explanations" (unobserved triples) that are both plausible and effective at altering predictions?
- **Basis in paper:** [explicit] The Conclusion identifies "studying latent explanations, e.g., by advancing probabilistic generative models for KGs" as a primary future methodological direction.
- **Why unresolved:** Current adversarial methods for adding triples often lack a principled sampling mechanism. While Section 3.3 defines latent explanations theoretically, it notes that specific sampling schemes and formal definitions for "likely" triples remain undefined design choices.
- **What evidence would resolve it:** A working implementation using a generative model (e.g., GeKCs) to identify missing triples that act as effective positive/negative latent explanations on standard datasets like FB15k-237.

### Open Question 3
- **Question:** How can proxies for sufficient explanations be refined to overcome the data contamination issues inherent in GNN distillation and the limited search space of C-sufficiency?
- **Basis in paper:** [explicit] The Conclusion lists "refining proxies for sufficient explanations" as a key research direction. [inferred] Section 3.2 highlights that GNN-based methods suffer from data contamination due to anchoring, while C-sufficient explanations rely on restrictive entity substitution.
- **Why unresolved:** There is currently a trade-off between structural flexibility (GNNs) and strict independence from the full KG (C-sufficiency). Existing methods struggle to isolate a minimal sufficient subgraph without leaking information from the original training data.
- **What evidence would resolve it:** A novel proxy method that generates sufficient explanations with strict independence from the full KG, validated by showing high fidelity without relying on frozen external embeddings or single-entity substitution tricks.

## Limitations
- The framework assumes C-sufficiency as a valid proxy for interpretability, but this transferability mechanism lacks strong empirical grounding in the literature.
- The evaluation relies heavily on FB15k-237 and ComplEx, limiting generalizability to other KGE architectures or datasets.
- The proposed MΔR metric, while addressing MRR limitations, remains sensitive to outliers and may overemphasize rare rank shifts.

## Confidence

- **High confidence:** The formalization of explanations as multi-objective optimization problems and the identification of metric limitations (MRR masking effects) are methodologically sound and well-supported by experimental evidence.
- **Medium confidence:** The C-sufficiency proxy mechanism is theoretically justified but lacks extensive validation against alternative interpretability frameworks or user studies.
- **Low confidence:** The generalizability of findings to KGE models beyond ComplEx or datasets beyond FB15k-237 remains uncertain without broader experimental coverage.

## Next Checks

1. Validate C-sufficiency transferability on datasets with known semantic similarity structures (e.g., biomedical KGs) to test if swapped entities maintain predictive logic.
2. Implement MΔR on a diverse set of KGE models (RotatE, DistMult) to confirm metric sensitivity is not ComplEx-specific.
3. Conduct ablation studies removing extreme rank outliers to assess MΔR stability and determine robust aggregation methods (median vs. mean).