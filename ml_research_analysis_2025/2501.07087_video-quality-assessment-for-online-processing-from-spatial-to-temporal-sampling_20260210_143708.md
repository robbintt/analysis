---
ver: rpa2
title: 'Video Quality Assessment for Online Processing: From Spatial to Temporal Sampling'
arxiv_id: '2501.07087'
source_url: https://arxiv.org/abs/2501.07087
tags:
- video
- quality
- temporal
- sampling
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how much video information can be discarded
  while maintaining acceptable performance in video quality assessment (VQA) models.
  The authors propose a joint spatial and temporal sampling approach that drastically
  reduces video information by sparsely sampling frames and extracting patches from
  each frame.
---

# Video Quality Assessment for Online Processing: From Spatial to Temporal Sampling

## Quick Facts
- arXiv ID: 2501.07087
- Source URL: https://arxiv.org/abs/2501.07087
- Reference count: 40
- Primary result: 99.83% computational cost reduction while maintaining competitive VQA performance through joint spatial-temporal sampling

## Executive Summary
This paper addresses the computational bottleneck in video quality assessment by proposing a joint spatial and temporal sampling approach that drastically reduces video information while maintaining acceptable performance. The authors demonstrate that videos can be compressed by extracting sparse keyframes and patches from each frame, then fed into a simplified VQA model consisting of MobileNet, Graph network, and FC layers. The approach achieves PLCC scores ranging from 0.69 to 0.87 across six public VQA datasets while reducing input data size by over 99.83% compared to traditional models.

## Method Summary
The proposed method involves two-step video compression: temporal sampling extracts sparse keyframes using segment-based strategies (TSN, TSM, or ECO), followed by spatial sampling using GMS to extract uniform grid patches from each keyframe. The compressed representation maintains sufficient statistical regularities for quality regression. A simplified VQA model with lightweight MobileNet spatial feature extraction, Graph network-based temporal fusion, and global quality regression then predicts quality scores from this compressed input. The approach exploits video redundancy in both spatial and temporal dimensions while learning quality-relevant patterns from heavily compressed representations.

## Key Results
- Achieves 99.83% computational cost reduction compared to traditional VQA models
- PLCC scores range from 0.69 to 0.87 across six VQA datasets (KoNViD-1k, LIVE-VQC, CVD2014, LIVE-Qualcomm, LSVQ, NTIRE)
- Video quality prediction shows minimal sensitivity to frame order, supporting orderless temporal sampling
- MobileNet + Graph network architecture outperforms other lightweight alternatives for this task

## Why This Works (Mechanism)

### Mechanism 1: Joint Spatio-Temporal Compression via Structured Sampling
The method applies two-step compression that preserves contextual relationships while discarding ~99.83% of input data. This works because videos contain highly redundant information in both dimensions, and quality-relevant distortions are spatially and temporally distributed rather than localized to specific regions/times. The compressed representation maintains sufficient statistical regularities for quality regression.

### Mechanism 2: Order-Independent Temporal Quality Aggregation
The model aggregates quality-relevant features across frames without relying on strict temporal sequence. Experiments show shuffled sequences perform comparably to ordered sequences, suggesting the temporal fusion module captures quality patterns rather than temporal dynamics. Perceptual video quality depends more on the distribution of quality across frames than on temporal progression or motion coherence.

### Mechanism 3: Lightweight Feature Extraction with Quality-Aware Representations
Pre-trained lightweight CNNs (MobileNet) combined with graph-based temporal fusion can achieve competitive VQA performance when input redundancy is reduced. The efficiency gain comes from both reduced input size and architectural simplicity, not just model shrinkage. Quality-relevant spatial features are learnable from small patches when training data provides sufficient diversity.

## Foundational Learning

- **Concept: Spatio-temporal redundancy in video data**
  - Why needed here: The entire approach assumes videos contain redundant information that can be discarded
  - Quick check question: Can you explain why sampling 4 patches from a frame might preserve enough information for quality assessment while discarding 99% of pixels?

- **Concept: Blind Video Quality Assessment (BVQA) paradigm**
  - Why needed here: The paper operates in the no-reference setting where reference videos are unavailable
  - Quick check question: How does a BVQA model predict quality without access to the original undistorted video?

- **Concept: Correlation metrics for quality prediction (PLCC, SRCC)**
  - Why needed here: All performance claims rest on PLCC (accuracy) and SRCC (monotonicity) values
  - Quick check question: If a model achieves PLCC 0.75 and SRCC 0.73, what does this tell you about its prediction accuracy versus ranking capability?

## Architecture Onboarding

- **Component map:** Input Video → Temporal Sampler (TSN/TSM/ECO) → Keyframes → Spatial Sampler (GMS) → Patches → MobileNet (Spatial Features) → Graph Network (Temporal Fusion) → FC Layers (Quality Regression) → Quality Score

- **Critical path:** The temporal sampling parameters (segments M, frames per segment η) and spatial sampling parameters (grid density β, patch size µ, patch count) directly control the efficiency-accuracy tradeoff. Table IX shows input size reduction from ~200×540×960 to 10-22×160×160.

- **Design tradeoffs:**
  - TSN vs. TSM vs. ECO: TSN extracts more frames (higher cost, better performance); TSM/ECO sample sparsely (lower cost, acceptable performance on spatial-distortion datasets)
  - Patch size vs. patch count: S2 (4×64×64) vs. S3 (25×32×32) represent different coverage strategies
  - MobileNet balances efficiency and performance; Table VI shows 5× parameter reduction vs. alternatives with competitive accuracy

- **Failure signatures:**
  - High-motion content: The approach may fail on "high-motion databases...due to the loss of temporal information"
  - Algorithm-related distortions: NTIRE dataset shows lower/more variable performance due to "complex and challenging algorithm-related distortions"
  - Insufficient spatial coverage: S1 (4 patches) performs substantially worse than S2/S3

- **First 3 experiments:**
  1. Reproduce the spatial sampling ablation: Implement GMS with S1-S4 configurations on KoNViD-1k using VSFA backbone to validate Table II results
  2. Test temporal sampling strategies with fixed spatial config: Keep S3 constant, compare TSN/TSM/ECO on LIVE-Qualcomm vs. CVD2014 to observe distortion-type sensitivity
  3. Validate order-independence claim: Implement four training/testing order modes (OO, OS, SO, SS) on LIVE-VQC to reproduce Table X results

## Open Questions the Paper Calls Out

### Open Question 1
How can spatio-temporal sampling strategies be adapted to maintain performance for high-motion or highly dynamic video content? The conclusion states that for high-motion databases, the proposed model might be unsuitable due to the loss of temporal information. The current joint sampling method disrupts temporal continuity, which is critical for high-motion scenes, and the lightweight temporal modeling module is too weak to compensate.

### Open Question 2
Can aggressive spatial sampling be effective for high-resolution, high-quality videos where distinguishing subtle quality differences is required? The authors note the model behaves "far from satisfactory" on the LSVQ 1080P dataset because high-resolution videos with relatively high quality are "hard to distinguish" when heavily compressed. Reducing input to small patches likely discards the fine-grained details necessary to differentiate quality in high-fidelity content.

### Open Question 3
Does the orderless nature of temporal modeling hold for videos with complex, algorithm-related distortions? While the paper confirms frame order independence on general datasets, it highlights that the model struggles with the "complex and challenging algorithm-related distortions" found in the NTIRE dataset. It is unclear if the "recency effect" or temporal evolution of specific synthetic distortions requires sequential processing.

## Limitations
- May not generalize well to videos with significant temporal distortions or high-motion content
- Performance on NTIRE (algorithm-related distortions) shows high variability
- Lacks detailed architectural specifications for Graph network and MobileNet configuration
- Order-independent temporal modeling assumption breaks down on motion-quality critical datasets

## Confidence

- **High confidence**: Core claim that joint spatio-temporal sampling can achieve 99.83% data reduction while maintaining competitive VQA performance
- **Medium confidence**: Order-independence finding, as corpus support is limited and temporal-distortion datasets show reduced robustness
- **Low confidence**: Reproducibility due to unspecified Graph network architecture, training hyperparameters, and MobileNet variant details

## Next Checks

1. **Spatial sampling ablation validation**: Reproduce Table II results by implementing GMS with S1-S4 configurations on KoNViD-1k using VSFA backbone to verify patch size/count sensitivity claims

2. **Temporal sampling strategy interaction**: Fix S3 spatial sampling and compare TSN/TSM/ECO performance on LIVE-Qualcomm vs. CVD2014 to observe distortion-type sensitivity

3. **Order-independence verification**: Implement the four training/testing order modes (OO, OS, SO, SS) on LIVE-VQC to reproduce Table X results and investigate any deviations from claimed order-independence