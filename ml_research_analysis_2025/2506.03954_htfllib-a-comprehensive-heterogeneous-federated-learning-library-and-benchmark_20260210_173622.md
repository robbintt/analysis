---
ver: rpa2
title: 'HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark'
arxiv_id: '2506.03954'
source_url: https://arxiv.org/abs/2506.03954
tags:
- htfe
- methods
- learning
- heterogeneous
- htfl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HtFLlib, the first comprehensive benchmark
  library for Heterogeneous Federated Learning (HtFL). It addresses the lack of standardized
  evaluation frameworks by providing 12 diverse datasets across image, text, and sensor
  signal modalities, 40 heterogeneous model architectures, and implementations of
  10 representative HtFL methods.
---

# HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark

## Quick Facts
- arXiv ID: 2506.03954
- Source URL: https://arxiv.org/abs/2506.03954
- Reference count: 40
- Introduces the first comprehensive benchmark library for Heterogeneous Federated Learning (HtFL) with 12 datasets, 40 model architectures, and 10 representative methods.

## Executive Summary
HtFLlib addresses the lack of standardized evaluation frameworks for Heterogeneous Federated Learning by providing a comprehensive benchmark library. The library includes 12 diverse datasets across image, text, and sensor signal modalities, 40 heterogeneous model architectures, and implementations of 10 representative HtFL methods. Through systematic evaluation across accuracy, convergence, and computational costs, the benchmark reveals key insights about method performance across different data and model heterogeneity scenarios. Experimental results demonstrate that FedTGP consistently performs well in image tasks, while FedKD excels in sensor signal applications.

## Method Summary
HtFLlib is a PyTorch-based framework for evaluating heterogeneous federated learning methods. The library supports 10 algorithms categorized into three sharing paradigms: partial-parameter methods (FedAvg, FedADP, FedORGP), mutual distillation methods (FML, FedKD, FedMRL), and prototype-sharing methods (FD, FedProto, FedTGP, FedKTL). Each method implements a standard interface for local training, knowledge uploading, server aggregation, and knowledge downloading. The framework enforces uniform output dimensions (K=512) across heterogeneous models and supports configurable data heterogeneity scenarios including label skew, feature shift, and real-world partitions.

## Key Results
- FedTGP achieves best accuracy on 7/8 image dataset settings (89.02-90.02% on CIFAR-10, 46.94% on CIFAR-100 Dirichlet)
- FedKD excels in sensor signal applications while FedTGP struggles with text modalities
- Prototype-sharing methods require 0.34-1.75 MB communication vs. 63-71 MB for mutual distillation methods
- Method performance is highly dependent on data modality and heterogeneity type

## Why This Works (Mechanism)

### Mechanism 1
Lightweight knowledge carriers enable heterogeneous model collaboration without sharing full model parameters. Instead of transmitting entire model weights (infeasible across different architectures), clients exchange compressed representations—either partial parameters, auxiliary models, or class prototypes—that the server aggregates into global knowledge for redistribution. Core assumption: Heterogeneous models can align on a shared representation space despite architectural differences in their feature extractors.

### Mechanism 2
Auxiliary-model mutual distillation provides robustness to high model heterogeneity but introduces communication overhead. A small homogeneous auxiliary model is co-trained with each heterogeneous client model; clients exchange auxiliary model weights or distilled logits, creating a shared reference point that all architectures can learn from regardless of their internal structure. Core assumption: The auxiliary model is small enough for efficient transmission yet expressive enough to capture transferable knowledge.

### Mechanism 3
Prototype quality determines method effectiveness, and discriminability-enhanced prototypes (FedTGP) outperform simple averaging in high-heterogeneity settings. Local prototypes are collected from clients, aggregated server-side, and broadcast back to guide local training; FedTGP adds adaptive-margin contrastive learning to improve inter-class separation in prototype space. Core assumption: Averaged feature representations capture meaningful class semantics across heterogeneous extractors.

## Foundational Learning

- **Federated Learning Fundamentals (FedAvg aggregation, client-server communication, non-IID data)**: Why needed: HtFL extends standard FL; understanding FedAvg's weighted averaging of model weights is prerequisite to grasping why heterogeneous architectures require different mechanisms. Quick check: Can you explain why FedAvg fails when client models have different layer dimensions?

- **Knowledge Distillation (teacher-student training, logit matching, feature alignment)**: Why needed: Mutual distillation methods rely on distillation principles; FedTGP uses contrastive learning for prototype discriminability. Quick check: What is the difference between distilling logits versus intermediate features?

- **Representation Learning (feature extractors, embedding spaces, prototype/centroid concepts)**: Why needed: Prototype-sharing methods assume feature extractors produce comparable embedding vectors; understanding feature-space geometry is essential for interpreting prototype aggregation and alignment challenges. Quick check: Why might two different CNN architectures produce incompatible feature spaces for the same image?

## Architecture Onboarding

- **Component map**: Data Module (12 datasets with heterogeneity configs) -> Model Registry (40 architectures in 19 groups) -> Method Implementations (10 algorithms in three categories) -> Server-Client Orchestration (communication round loop) -> Evaluation Suite (per-client accuracy, convergence, costs)

- **Critical path**: 1. Select dataset → configure heterogeneity 2. Assign model group → each client receives heterogeneous feature extractor 3. Initialize method → knowledge carrier type determined 4. Run federated loop → local training, upload knowledge carriers, server aggregation, broadcast global knowledge 5. Evaluate → aggregate test accuracy, track convergence, measure costs

- **Design tradeoffs**: Prototype vs. Auxiliary Model (communication cost vs. modality coverage), Feature Dimension K (accuracy vs. overhead), Local Epochs E (communication frequency vs. stability), Client Participation ρ (prototype diversity vs. robustness)

- **Failure signatures**: Prototype methods on text/sensor (slow convergence, unstable accuracy), mutual distillation with high E (accuracy drops), FedTGP with low client participation (accuracy collapses), feature-shift scenarios (prototype misalignment)

- **First 3 experiments**: 1. Baseline comparison on CIFAR-100 with HtFE_img8, Dirichlet α=0.1: Run all 10 methods for 1000 rounds; verify FedTGP achieves ~47% accuracy 2. Modality stress test: Compare FD vs. FedKD on AG News (text) and HAR (sensor); confirm modality-specific preferences 3. Scalability check with 100 clients, ρ=50%: Test FedTGP and FedKTL on CIFAR-100; observe participation sensitivity vs. generator-based robustness

## Open Questions the Paper Calls Out

- **Integration with pre-trained large models (PLMs)**: The authors suggest integrating HtFL frameworks with PLMs for large-scale scenarios with low client participation, but specific architectures remain undefined. Current methods like FedTGP struggle when client participation drops below 10%.

- **Black-box model collaboration**: Realistic black-box model setting is under-explored, with only a few methods applicable. Most existing HtFL methods require access to model weights, gradients, or intermediate features, which is incompatible with proprietary models.

- **Prototype alignment for text modalities**: Text models with different architectures have significant differences in their processing mechanisms, making feature alignment difficult. Prototype-sharing methods perform significantly worse on text tasks compared to image tasks.

## Limitations

- **Hyperparameter sensitivity**: Method rankings could shift significantly with different configurations of K dimension, local epochs E, and learning rate across different dataset characteristics.

- **Real-world scalability**: Experimental validation focuses on 20 clients while the library supports 100+ clients; communication and computational costs at scale remain theoretical extrapolations.

- **Architecture generalization**: The 40 models span common architectures but exclude newer designs like Vision Transformers; method effectiveness on emerging architectures is untested.

## Confidence

- **High Confidence**: Library implementation quality, dataset diversity, and basic benchmarking methodology
- **Medium Confidence**: Comparative performance claims across methods (small differences may not be practically meaningful)
- **Low Confidence**: Cross-modality generalization claims (demonstrates modality-specific preferences but doesn't establish why)

## Next Checks

1. **Hyperparameter sweep validation**: Re-run CIFAR-100 experiments with K ∈ {128, 256, 512} and E ∈ {1, 5, 10} to verify method rankings remain stable

2. **Scalability stress test**: Execute FedTGP and FedKTL with 100 clients and ρ=10% participation to confirm communication cost projections and identify bottlenecks

3. **Architecture generalization test**: Add ViT and modern Transformer to model registry, then evaluate FedTGP and FedKD on CIFAR-10 to verify prototype-space alignment assumptions