---
ver: rpa2
title: 'Visual RAG: Expanding MLLM visual knowledge without fine-tuning'
arxiv_id: '2501.10834'
source_url: https://arxiv.org/abs/2501.10834
tags:
- examples
- visual
- knowledge
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visual RAG introduces a retrieval-augmented generation framework
  that enhances Multimodal Large Language Models (MLLMs) for image classification
  without requiring fine-tuning. By combining a CLIP-based embedding model with FAISS
  retrieval, the system dynamically selects the most relevant demonstration images
  from a knowledge base, providing targeted in-context examples to the MLLM during
  inference.
---

# Visual RAG: Expanding MLLM visual knowledge without fine-tuning

## Quick Facts
- arXiv ID: 2501.10834
- Source URL: https://arxiv.org/abs/2501.10834
- Authors: Mirco Bonomo; Simone Bianco
- Reference count: 40
- Key outcome: Visual RAG achieves accuracy comparable to or better than many-shot ICL using only ~23% of the demonstration examples.

## Executive Summary
Visual RAG introduces a retrieval-augmented generation framework that enhances Multimodal Large Language Models (MLLMs) for image classification without requiring fine-tuning. By combining a CLIP-based embedding model with FAISS retrieval, the system dynamically selects the most relevant demonstration images from a knowledge base, providing targeted in-context examples to the MLLM during inference. Experiments on eight diverse datasets show that Visual RAG achieves accuracy comparable to or better than the state-of-the-art many-shot ICL approach, with an average improvement of +2% and using only about 23% of the demonstration examples (as low as 6.5% in most cases). This approach significantly reduces computational costs and data requirements while maintaining scalability and adaptability to new visual domains.

## Method Summary
Visual RAG employs a retrieval-augmented generation pipeline for image classification. First, a knowledge base of labeled images is indexed offline using CLIP embeddings stored in FAISS (IndexFlatL2). At inference, a query image is embedded and the top-k most similar examples are retrieved via L2 distance. These retrieved <image, label> pairs are then used as demonstrations in a prompt for Gemini 1.5 Pro, which performs in-context learning to predict the label. This approach avoids fine-tuning while dynamically adapting to new tasks and domains through targeted retrieval.

## Key Results
- Visual RAG achieves accuracy comparable to or better than state-of-the-art many-shot ICL, with an average improvement of +2%.
- The system uses only about 23% of the demonstration examples required by many-shot ICL (as low as 6.5% in most cases).
- Retrieval-augmented prompts reduce context noise, improving signal-to-noise ratio for the MLLM's in-context learning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving semantically similar images reduces context noise and improves the signal-to-noise ratio for the MLLM's in-context learning.
- Mechanism: The embedding model (CLIP) encodes images into a shared visual-semantic space. FAISS retrieves nearest neighbors via L2 distance. Only images most similar to the query enter the prompt, filtering out irrelevant demonstrations that many-shot ICL would include randomly.
- Core assumption: CLIP embeddings capture task-relevant visual similarity; semantic proximity in embedding space correlates with classification utility.
- Evidence anchors:
  - [abstract] "employs a robust embedding model and a retrieval mechanism to extract the most relevant examples from an image-based knowledge base, optimizing task-specific context"
  - [section 5, Results] "This behavior could be attributed to the reduction of noise (i.e., images irrelevant to the specific query) within the context, as previously observed and highlighted for textual RAGs"
  - [corpus] Related work (mKG-RAG, Re-ranking Reasoning Context) supports retrieval-augmented approaches for VQA but does not directly validate this specific image-classification mechanism.
- Break condition: If embedding similarity does not align with task-relevant features (e.g., background dominates subject, fine-grained distinctions needed), retrieval may return superficially similar but task-irrelevant examples.

### Mechanism 2
- Claim: Providing retrieved <image, label> pairs enables analogy-based learning without weight updates.
- Mechanism: The MLLM (Gemini 1.5 Pro) receives a prompt with k retrieved examples followed by the query image. The model predicts the label by pattern-matching across the demonstrated <image, label> mappings, leveraging its in-context learning capacity.
- Core assumption: The MLLM has sufficient ICL capability to generalize from few analogies; the prompt format is parseable and the model attends to all examples.
- Evidence anchors:
  - [abstract] "synergically integrates visual retrieval with MLLM in-context learning capabilities, enabling dynamic adaptation to new tasks and domains"
  - [section 3.3, Generator] "the RAG Solution employs the prompt detailed in Appendix A [17]. The retrieved examples are presented as enhanced context to the generator in the form of <image, label> pairs, serving as demonstrations."
  - [corpus] SMMILE benchmark shows multimodal ICL remains underexplored in domains like medicine, suggesting this mechanism is promising but not universally validated.
- Break condition: If context window saturates, "lost in the middle" effects degrade attention to retrieved examples; or if inter-class similarity is too high for the model to distinguish.

### Mechanism 3
- Claim: Example efficiency stems from targeted retrieval, enabling comparable accuracy with fewer demonstrations.
- Mechanism: Rather than randomly sampling many examples (many-shot ICL), Visual RAG selects the most informative subset. The ablation shows retriever-only performance is lower than combined retriever + generator, indicating the generator's reasoning adds value beyond nearest-neighbor voting.
- Core assumption: Retrieved examples are more informative per token than random examples; the generator can effectively leverage them.
- Evidence anchors:
  - [abstract] "achieves comparable or superior performance to the state-of-the-art many-shot ICL method with significantly fewer demonstrating examples (about 23% on average)"
  - [section 6, Ablation] "Retriever-only ablation study... reveals that the combined approach offers a significant advantage over using either component alone"
  - [corpus] No direct corpus evidence validates the 23% efficiency claim; it is specific to this paper's experiments.
- Break condition: If the knowledge base lacks diverse or representative examples, retrieval cannot surface informative demonstrations regardless of selection quality.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Visual RAG adapts textual RAG principles to visual knowledge bases; understanding indexing, retrieval, and generation stages is prerequisite.
  - Quick check question: Can you explain how a text-based RAG system retrieves and injects document chunks into an LLM prompt?

- Concept: In-Context Learning (ICL)
  - Why needed here: The generator learns from demonstration examples in the prompt without gradient updates; ICL capability determines system effectiveness.
  - Quick check question: What is the difference between few-shot ICL and fine-tuning in terms of how the model adapts to new tasks?

- Concept: Vector Embeddings and Similarity Search
  - Why needed here: CLIP encodes images; FAISS indexes and retrieves via L2 distance; understanding embedding spaces is essential for debugging retrieval quality.
  - Quick check question: Given two image embeddings, how would you compute their L2 (Euclidean) distance and interpret its meaning?

## Architecture Onboarding

- Component map:
  - Knowledge Base: Demo set images with labels → encoded by CLIP → stored in FAISS (IndexFlatL2)
  - Retriever: Query image → CLIP embedding → FAISS L2 nearest-neighbor search → top-k <image, label> pairs
  - Generator: Gemini 1.5 Pro → prompt with retrieved examples + query image → predicted label + confidence score

- Critical path:
  1. Index demo images offline (CLIP → FAISS)
  2. At inference, embed query image
  3. Retrieve k nearest neighbors
  4. Construct prompt with k examples + query
  5. Call MLLM API, parse structured response

- Design tradeoffs:
  - IndexFlatL2 (brute-force) vs. approximate indices: accuracy vs. latency at scale
  - Retrieval count (k): more examples improve accuracy up to context saturation; diminishing returns observed
  - Embedding model choice: CLIP generalizes well but may lack domain-specific granularity (e.g., fine-grained medical features)

- Failure signatures:
  - Background-dominant retrieval: camera-trap images (TerraIncognita) where background similarity overwhelms subject similarity
  - Multi-label imbalance: CheXpert retriever focuses on predominant label, neglecting secondary labels
  - Fine-grained confusion: OxfordPets high inter-class similarity; embeddings may not capture subtle breed differences
  - Context convergence: When k approaches demo set size, performance converges to random-selection many-shot ICL

- First 3 experiments:
  1. Baseline comparison: Run zero-shot, many-shot ICL (random selection), and Visual RAG (k=5, 10, 50) on a single dataset (e.g., EuroSAT) to reproduce efficiency and accuracy claims.
  2. Ablation study: Compare retriever-only (majority voting on retrieved labels) vs. full Visual RAG to quantify generator contribution.
  3. Scaling analysis: Vary k across 5–100 on a small dataset (e.g., FIVES) to observe accuracy trajectory and identify saturation point; log token counts and API costs per configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Visual RAG framework be effectively extended to dense prediction tasks like object detection or semantic segmentation?
- **Basis in paper:** [explicit] The authors state in the conclusion, "We will also extend Visual RAG to other computer vision tasks to evaluate its versatility beyond image classification."
- **Why unresolved:** The current experiments and prompt engineering are tailored exclusively for classification (multi-class, multi-label, fine-grained), and it is unclear if retrieving whole-image examples suffices for tasks requiring pixel-level or bounding-box outputs.
- **What evidence would resolve it:** Successful application of Visual RAG to standard detection/segmentation benchmarks (e.g., COCO) showing performance gains over zero-shot or random retrieval baselines.

### Open Question 2
- **Question:** How can the retrieval mechanism be refined to focus on specific visual elements rather than global image similarity to improve performance on multi-label or small-subject datasets?
- **Basis in paper:** [explicit] The analysis of CheXpert and TerraIncognita failures notes the retriever struggles with "images where the similarity is more related to the background than to the subject," and suggests future research could "refine the retriever to allow it to better focus on the main subject."
- **Why unresolved:** The current CLIP-based retriever uses global embeddings, causing it to miss small subjects (TerraIncognita) or neglect secondary labels in multi-label contexts (CheXpert).
- **What evidence would resolve it:** Implementation of a localized or attention-based retrieval mechanism that increases accuracy specifically on datasets with small objects or high label cardinality.

### Open Question 3
- **Question:** Does the use of hierarchical or semantic-aware indexing significantly outperform the flat L2 distance indexing used in the current study?
- **Basis in paper:** [explicit] The conclusion lists exploring "alternative retrieval methodologies and index configurations, such as hierarchical or semantic-aware indexing" as a specific avenue for future work to improve precision.
- **Why unresolved:** The authors utilized `IndexFlatL2` (brute-force) for accuracy on small datasets, but the benefits of more complex indexing structures for retrieval precision or scalability in larger knowledge bases remain untested.
- **What evidence would resolve it:** Comparative experiments showing that hierarchical indexing retrieves more contextually relevant examples for fine-grained tasks compared to the flat baseline.

## Limitations
- Data dependency and generalizability: System effectiveness depends on quality and representativeness of the knowledge base, especially for edge cases or rare classes.
- Embedding model sensitivity: CLIP embeddings may not capture task-specific discriminative features in specialized domains like fine-grained medical imaging.
- Multi-label task ambiguity: Retriever tends to match predominant label in multi-label tasks, neglecting secondary labels.

## Confidence
- High confidence: The core retrieval-augmented ICL mechanism works as described, supported by ablation studies and quantitative improvements over many-shot ICL on average.
- Medium confidence: The 2% average accuracy improvement is statistically significant but may not generalize across all domains, especially fine-grained or highly specialized tasks.
- Low confidence: Claims about scalability to new visual domains are theoretical; experiments cover only eight curated datasets and do not test on truly out-of-distribution or dynamic data streams.

## Next Checks
1. **Embedding sensitivity analysis**: Replace CLIP with a domain-specific encoder (e.g., CheXNet for CheXpert) and measure retrieval quality and end-to-end accuracy. Visualize retrieved neighbors for failures to diagnose embedding misalignment.
2. **Multi-label robustness test**: Design a controlled experiment where secondary labels are systematically varied in the knowledge base. Measure recall and F1 for both predominant and non-predominant labels to quantify retriever bias.
3. **Context saturation experiment**: Systematically increase k from 5 to 100 on a small, high-variation dataset (e.g., FIVES). Plot accuracy, token usage, and API cost per query to identify the optimal trade-off between accuracy and efficiency, and test for "lost in the middle" effects.