---
ver: rpa2
title: Investigating Syntactic Biases in Multilingual Transformers with RC Attachment
  Ambiguities in Italian and English
arxiv_id: '2504.09886'
source_url: https://arxiv.org/abs/2504.09886
tags:
- type
- attachment
- verb
- english
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses relative clause (RC) attachment ambiguity in Italian
  and English to probe syntactic knowledge in monolingual and multilingual language
  models. Using stimuli adapted from psycholinguistic studies, the authors test whether
  models exhibit human-like attachment preferences (high vs.
---

# Investigating Syntactic Biases in Multilingual Transformers with RC Attachment Ambiguities in Italian and English

## Quick Facts
- arXiv ID: 2504.09886
- Source URL: https://arxiv.org/abs/2504.09886
- Reference count: 17
- This paper uses relative clause (RC) attachment ambiguity in Italian and English to probe syntactic knowledge in monolingual and multilingual language models.

## Executive Summary
This paper investigates whether monolingual and multilingual language models exhibit human-like syntactic biases in resolving relative clause (RC) attachment ambiguities. Using stimuli adapted from psycholinguistic studies, the authors test whether models show preferences for high vs. low attachment and sensitivity to lexical factors like verb type and noun type. Experiments compare Italian models (GePpeTto, Alberto) and multilingual models (bert-base-multilingual-cased, xlm-mlm-17-1280, xlm-roberta-large) using surprisal measurements at the disambiguating verb. Results show no clear attachment preferences in Italian models and mixed results in English, with some models showing effects inconsistent with human data. The study highlights RC attachment as a useful benchmark for cross-linguistic syntactic evaluation and identifies item-level variability as a key area for future research.

## Method Summary
The study evaluates monolingual and multilingual LLMs' sensitivity to RC attachment ambiguities using surprisal measurements. Italian stimuli (Lee and De Santo, 2024) consist of 24 sentence sets × 4 conditions = 96 sentences in a 2×2 design (Verb Type: perceptual/non-perceptual × Attachment Type: HA/LA), disambiguated via singular/plural agreement on embedded verbs. English stimuli (Grillo et al., 2015) include two experiments: 24 sets (verb-type) and 24 sets (noun-type), each 2×2 design, disambiguated via auxiliary was/were. Surprisal is computed at the disambiguating verb using the minicons library, and analyzed via linear mixed-effects models (Surprisal ~ Verb/Noun Type + Attachment Type + interaction + (1|set)) in R (v4.4.1).

## Key Results
- Italian models (GePpeTto, Alberto) show no clear attachment preferences consistent with human data
- Multilingual models show mixed results in English, with some effects inconsistent with human patterns
- Item-level variability emerges as a key factor requiring further investigation
- RC attachment ambiguities prove useful as a cross-linguistic benchmark for syntactic evaluation

## Why This Works (Mechanism)
The study leverages the well-established psycholinguistic phenomenon of RC attachment ambiguity, where humans show systematic preferences for high vs. low attachment depending on lexical factors. By measuring surprisal at the disambiguating verb, the approach captures whether models assign lower probability to the attachment site humans prefer. The 2×2 factorial design isolates the effects of verb type and noun type, allowing detection of interactions that characterize human parsing strategies.

## Foundational Learning
- Surprisal theory: Why needed - to quantify the predictability of disambiguating tokens; Quick check - verify surprisal decreases when model encounters expected attachment site
- Linear mixed-effects models: Why needed - to account for item-level variability and nested data structure; Quick check - ensure model convergence and appropriate random effects specification
- Relative clause attachment ambiguity: Why needed - provides controlled test of syntactic knowledge across languages; Quick check - confirm stimuli correctly implement the 2×2 design

## Architecture Onboarding

**Component Map:**
minicons -> pretrained transformer models -> surprisal extraction -> LME modeling

**Critical Path:**
Stimulus loading → Model loading → Tokenization → Surprisal computation → Statistical analysis

**Design Tradeoffs:**
- Surprisal vs. prompting: Surprisal provides continuous measurement but requires exact token alignment; prompting allows zero-shot evaluation but may not capture fine-grained preferences
- Monolingual vs. multilingual: Monolingual models may better capture language-specific patterns; multilingual models test generalization across languages

**Failure Signatures:**
- Model convergence warnings in LME models indicate insufficient data or inappropriate random effects structure
- Unexpected token positions suggest misalignment between stimulus and model tokenization
- Null effects may indicate either true absence of preference or insufficient statistical power

**First Experiments:**
1. Compute surprisal for a single Italian sentence with known HA preference and verify expected pattern
2. Run LME model on English data to confirm basic attachment preference before examining interactions
3. Compare surprisal distributions across attachment conditions for one model to check for qualitative differences

## Open Questions the Paper Calls Out
- Do monolingual and multilingual LLMs exhibit verb-type modulations in other languages that allow pseudo-relative constructions, such as Spanish?
- Are LLMs sensitive to other psycholinguistic factors known to affect human attachment resolution, such as relative clause length?
- Do the observed results stem from the Transformer architecture itself or the use of surprisal measurements compared to prompting methods used in prior work?

## Limitations
- Exact stimuli items not fully reproduced, requiring manual extraction or request from source publications
- Small item sets (n=24) may lead to statistical convergence issues and unreliable effect estimates
- Checkpoint versions for GePpeTto and Alberto not specified, potentially introducing minor variations

## Confidence

**High confidence:** The overall finding that multilingual transformers do not consistently reproduce human-like RC attachment preferences across languages.

**Medium confidence:** The specific patterns of which models show which effects, due to item-level variability and small sample size.

**Low confidence:** Claims about exact effect sizes or the absence of effects, given statistical uncertainties with small N.

## Next Checks
1. Request or reconstruct complete stimulus sets from Lee and De Santo (2024) and Grillo et al. (2015) to enable exact replication
2. Verify token-level surprisal extraction by printing token indices and confirming alignment with disambiguating verbs
3. Test alternative statistical approaches (e.g., Bayesian mixed models) on Italian data to assess robustness of null findings