---
ver: rpa2
title: Better Prompt Compression Without Multi-Layer Perceptrons
arxiv_id: '2501.06730'
source_url: https://arxiv.org/abs/2501.06730
tags:
- prompt
- compression
- encoder
- xcompressor
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Attention-Only Compressor (AOC), a prompt\
  \ compression method that removes MLP layers from transformer blocks, reducing encoder\
  \ parameters by ~67% compared to baseline approaches. AOC achieves comparable or\
  \ better prompt regeneration performance than 500xCompressor across compression\
  \ ratios up to 480\xD7, with BLEU scores ranging from 0.588-0.984 and Exact-Match\
  \ scores from 0.082-0.889 depending on compression ratio and memory tokens used."
---

# Better Prompt Compression Without Multi-Layer Perceptrons

## Quick Facts
- arXiv ID: 2501.06730
- Source URL: https://arxiv.org/abs/2501.06730
- Authors: Edouardo Honig; Andrew Lizarraga; Zijun Frank Zhang; Ying Nian Wu
- Reference count: 4
- Removes MLP layers from transformer blocks, achieving 67% parameter reduction while maintaining compression performance

## Executive Summary
This paper introduces Attention-Only Compressor (AOC), a novel prompt compression method that eliminates Multi-Layer Perceptron (MLP) layers from transformer blocks in the encoder, reducing parameters by approximately 67% compared to baseline approaches. AOC achieves comparable or better prompt regeneration performance than 500xCompressor across compression ratios up to 480×, with BLEU scores ranging from 0.588-0.984 and Exact-Match scores from 0.082-0.889 depending on compression ratio and memory tokens used. The authors demonstrate that prompt compression encoders don't require identical architecture to their decoder language models, challenging the assumption that MLP layers are essential for effective compression.

## Method Summary
AOC modifies the standard transformer architecture by removing all MLP layers from the encoder blocks while retaining only the attention mechanism. The compressed prompt is then passed through a lightweight linear projection layer before being processed by the decoder language model. The approach is trained end-to-end using reconstruction loss, where the compressed prompt must regenerate the original text. The method is evaluated against 500xCompressor using standard metrics (BLEU, Exact-Match, ROUGE-L) across various compression ratios (10×, 50×, 100×, 480×) and different numbers of memory tokens. An ablation study compares full training with LoRA adaptation to understand the role of MLPs in compression effectiveness.

## Key Results
- AOC reduces encoder parameters by ~67% compared to baseline approaches
- Achieves comparable or better prompt regeneration performance than 500xCompressor across compression ratios up to 480×
- BLEU scores range from 0.588-0.984 and Exact-Match scores from 0.082-0.889 depending on compression ratio and memory tokens
- LoRA with AOC performs worse than full training, suggesting MLP layers contribute to LoRA effectiveness
- Demonstrates smooth interpolation between compressed prompts, indicating meaningful latent space structure

## Why This Works (Mechanism)
The paper challenges the conventional wisdom that prompt compression encoders must mirror the architecture of their decoder language models. By removing MLPs while retaining attention mechanisms, AOC achieves significant parameter reduction without sacrificing compression quality. The attention-only architecture appears sufficient for capturing the essential information needed for prompt reconstruction. The ablation study showing LoRA's reduced effectiveness without MLPs suggests that MLPs play a crucial role in parameter-efficient fine-tuning by providing additional representational capacity and regularization that attention-only architectures lack.

## Foundational Learning
- **Prompt Compression**: The task of reducing input text to a compact representation that can still regenerate the original content - needed to understand the problem AOC solves
- **Transformer Architecture**: Understanding the role of attention and MLP layers in transformers - needed to appreciate what AOC removes and why
- **Language Model Decoders**: Knowledge that the decoder is a frozen pre-trained model - needed to understand the asymmetry in the encoder-decoder setup
- **Compression Ratios**: The multiplicative factor by which input length is reduced - needed to interpret performance metrics
- **Parameter-Efficient Fine-Tuning (PEFT)**: Methods like LoRA that adapt models with fewer parameters - needed to understand the ablation study
- **Evaluation Metrics**: BLEU, Exact-Match, and ROUGE-L scores for text generation - needed to assess performance claims

## Architecture Onboarding

**Component Map:** Input Prompt -> Attention-Only Encoder -> Linear Projection -> Frozen Decoder LM

**Critical Path:** The attention-only encoder must capture sufficient information for the decoder to regenerate the original prompt. The linear projection layer serves as the bottleneck that enforces compression.

**Design Tradeoffs:** Removing MLPs reduces parameters and computational cost but may limit representational capacity. The approach trades architectural complexity for efficiency while maintaining performance through the powerful pre-trained decoder.

**Failure Signatures:** Poor regeneration quality suggests the attention-only encoder fails to capture necessary information. The LoRA ablation failure indicates MLPs contribute to effective fine-tuning, particularly for adaptation methods.

**3 First Experiments:**
1. Test AOC with different compression ratios (10×, 50×, 100×, 480×) on a held-out validation set to establish baseline performance
2. Compare full training vs. LoRA adaptation to verify the effectiveness tradeoff observed in the paper
3. Perform interpolation between compressed prompts to qualitatively assess latent space smoothness

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific role do MLP layers play in LoRA's effectiveness for prompt compression, and can alternative adaptation methods be designed for attention-only architectures?
- Basis in paper: The authors state that "LoRA-AOC tends to perform worse than AOC and the baseline 500xCompressor across all metrics, which suggests that the effectiveness of LoRA in Transformers relies in part on the frozen MLPs."
- Why unresolved: The paper documents the underperformance but does not investigate the mechanism behind why MLP removal degrades LoRA effectiveness.
- What evidence would resolve it: Ablation studies comparing different adapter architectures (e.g., full fine-tuning vs. LoRA vs. other PEFT methods) on attention-only models, with analysis of gradient flow and representation changes.

### Open Question 2
- Question: How does AOC performance scale to larger decoder models (e.g., 7B, 70B parameters) and across diverse domains beyond scientific abstracts?
- Basis in paper: "While this work was performed with limited computational resources, we aim to study more diverse and larger datasets, model architectures, and compression ratios in the future."
- Why unresolved: All experiments use only Llama 3.2 1B Instruct and the arXiv dataset, leaving generalization untested.
- What evidence would resolve it: Benchmarks across multiple model sizes (1B→70B) and diverse corpora (code, dialogue, multilingual text) with consistent metrics.

### Open Question 3
- Question: What are the geometric and semantic properties of the compressed prompt latent space, and can it support operations beyond linear interpolation?
- Basis in paper: "We seek to better understand the latent space formed by compressed prompts and extend the use of compressed prompts beyond the interpolation example presented in this work."
- Why unresolved: The interpolation demonstration is qualitative only; no quantitative analysis of latent space structure is provided.
- What evidence would resolve it: Probing tasks measuring semantic similarity preservation, arithmetic operations on compressed representations, and quantitative metrics (e.g., isotropy, clustering quality) of the latent space.

### Open Question 4
- Question: How well do AOC-compressed prompts transfer to downstream tasks (e.g., question answering, summarization) compared to regeneration alone?
- Basis in paper: The paper evaluates only regeneration performance (BLEU, EM, ROUGE-L), but real-world prompt compression aims to preserve information for task completion, not verbatim reproduction.
- Why unresolved: No downstream task evaluations are reported, leaving practical utility unclear.
- What evidence would resolve it: Comparing task performance (e.g., reading comprehension accuracy, summarization quality) using compressed vs. uncompressed prompts across multiple benchmarks.

## Limitations
- Results are limited to Llama 3.2 1B Instruct and scientific abstracts, raising questions about generalizability to other domains and larger models
- Only evaluates prompt regeneration performance, not downstream task effectiveness
- Interpolation experiments are qualitative without quantitative analysis of latent space properties
- The mechanism behind MLP's contribution to LoRA effectiveness remains unexplained

## Confidence
- **High confidence**: Parameter reduction claims (67% reduction is directly measurable)
- **Medium confidence**: Compression performance relative to 500xCompressor (supported by BLEU/EM metrics but limited to specific test conditions)
- **Medium confidence**: Architectural insights about encoder-decoder symmetry (well-reasoned but based on limited architectural variations)
- **Low confidence**: Latent space interpolation implications (preliminary exploratory results)

## Next Checks
1. Test AOC on diverse language models (beyond Llama variants) and across different domains (e.g., code generation, multilingual tasks) to assess generalizability.
2. Conduct ablation studies examining how MLP removal affects encoder behavior on non-prompt-compression tasks, particularly focusing on representation quality and downstream task performance.
3. Investigate the interpolation results with more complex prompt transformations and longer sequences to determine if smooth transitions persist under challenging conditions.