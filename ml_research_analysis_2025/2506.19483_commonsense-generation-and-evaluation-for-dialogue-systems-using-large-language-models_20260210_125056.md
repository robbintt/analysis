---
ver: rpa2
title: Commonsense Generation and Evaluation for Dialogue Systems using Large Language
  Models
arxiv_id: '2506.19483'
source_url: https://arxiv.org/abs/2506.19483
tags:
- commonsense
- arxiv
- user
- language
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  generate and evaluate commonsense-enriched dialogue responses. The authors propose
  a method that conditions GPT-3.5 and GPT-4 on 12 ATOMIC commonsense relations to
  augment dialogue turns, producing multiple context-aware alternative responses.
---

# Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models

## Quick Facts
- arXiv ID: 2506.19483
- Source URL: https://arxiv.org/abs/2506.19483
- Authors: Marcos Estecha-Garitagoitia; Chen Zhang; Mario Rodríguez-Cantelar; Luis Fernando D'Haro
- Reference count: 40
- Primary result: GPT-4 achieves 0.415 MRR in ranking commonsense attributes for dialogue turns, outperforming GPT-3.5 (0.388 MRR) and ACCENT baseline (0.08 MRR)

## Executive Summary
This paper explores the use of large language models (LLMs) to generate and evaluate commonsense-enriched dialogue responses. The authors propose a method that conditions GPT-3.5 and GPT-4 on 12 ATOMIC commonsense relations to augment dialogue turns, producing multiple context-aware alternative responses. To evaluate these synthetic turns, they design a prompt-based reranking task using the same LLMs to match generated responses to their target commonsense attributes. Experimental results on 200 dialogues from five datasets show that GPT-4 outperforms GPT-3.5 in both generation and evaluation, achieving up to 0.415 MRR in attribute ranking, with GPT-3.5 also showing strong performance (0.388 MRR). The findings demonstrate the viability of using LLMs for zero-shot commonsense data augmentation and evaluation in dialogue systems.

## Method Summary
The paper presents a two-stage prompt-based pipeline using GPT-3.5-turbo and GPT-4 for zero-shot commonsense data augmentation in dialogue systems. In the generation stage, LLMs are prompted with dialogue context plus 12 ATOMIC relation definitions to produce 12 alternative responses per target turn. In the evaluation stage, the same LLMs perform listwise reranking to identify which attribute best matches each generated response. The method is tested on 200 dialogues from five datasets (DailyDialog, Topical-Chat, EmpatheticDialogues, Persona-Chat, Wizard of Wikipedia), generating 1,721 augmented turns. Performance is measured via Mean Reciprocal Rank (MRR) and Top-k accuracy, with GPT-4 achieving 0.415 MRR and GPT-3.5 achieving 0.388 MRR.

## Key Results
- GPT-4 achieves 0.415 MRR in commonsense attribute ranking, significantly outperforming GPT-3.5 (0.388 MRR) and ACCENT baseline (0.08 MRR)
- Generated responses are 35% longer on average than original dialogue turns
- One-shot prompting with COMET examples provides marginal improvement (0.255→0.362 MRR for GPT-3.5 evaluation)
- Top-1 accuracy remains low (~0.10-0.15) across models, suggesting semantic overlap between ATOMIC relation definitions

## Why This Works (Mechanism)

### Mechanism 1
Conditioning LLMs on explicit commonsense relation definitions enables targeted dialogue augmentation along specific reasoning dimensions. The system provides GPT-3.5/GPT-4 with human-readable definitions for each of 12 ATOMIC relations (e.g., xWant, oReact, xEffect), asking the model to generate response variants that explicitly reflect each attribute. This leverages the LLM's pretrained world knowledge and instruction-following capacity without fine-tuning.

### Mechanism 2
LLMs can function as zero-shot evaluators by reranking commonsense attributes against their own generated responses. The evaluation stage presents the LLM with a generated response and all 12 attribute definitions, requiring it to rank definitions by relevance. Ranking quality is measured via MRR, treating the originally-conditioned attribute as ground truth.

### Mechanism 3
Listwise prompting (ranking multiple items simultaneously) improves evaluation coherence over pairwise or pointwise approaches. Rather than scoring each attribute independently, the evaluation prompt presents all 12 definitions together and requests an ordered ranking. This leverages relative comparison, reducing calibration drift across individual scoring decisions.

## Foundational Learning

- **ATOMIC commonsense relations**
  - Why needed here: The entire framework depends on understanding the 12 event-centered relations (xWant, xNeed, xEffect, xReact, xIntent, xAttr, oWant, oEffect, oReact, HasSubEvent, IsAfter, HinderedBy) and their semantic distinctions.
  - Quick check question: Can you explain the difference between xWant (what the speaker wants to do after) and xIntent (what the speaker wanted before the event)?

- **Mean Reciprocal Rank (MRR)**
  - Why needed here: The paper uses MRR as the primary evaluation metric for ranking quality; interpreting results requires understanding how MRR penalizes incorrect top rankings.
  - Quick check question: If the correct attribute is ranked 3rd out of 12, what MRR contribution does that sample provide?

- **Zero-shot vs. One-shot prompting**
  - Why needed here: The paper compares zero-shot generation against one-shot with COMET-generated examples; understanding the tradeoff is critical for cost/quality decisions.
  - Quick check question: What additional information does the one-shot condition provide, and what assumption does it make about COMET output quality?

## Architecture Onboarding

- **Component map**: Source datasets (5 dialogue corpora) -> Generation module (GPT-3.5/GPT-4 with structured prompts) -> Evaluation module (same LLMs with listwise ranking prompt) -> Metrics layer (MRR and Top-k accuracy)

- **Critical path**: 
  1. Sample partial dialogues (5-10 turns) from source datasets
  2. For each target turn, generate 12 response variants (one per ATOMIC relation)
  3. For each variant, run listwise reranking to predict which attribute was used
  4. Compute MRR comparing predicted rankings to true conditioned attributes

- **Design tradeoffs**:
  - GPT-4 vs. GPT-3.5: +2.7% MRR improvement but ~2× cost (authors halved GPT-4 sample size for budget)
  - Zero-shot vs. One-shot: One-shot with COMET examples showed marginal improvement (0.255→0.362 MRR when evaluated by GPT-3.5) but adds pipeline complexity
  - ACCENT vs. LLM evaluation: ACCENT requires complex event-relation tuple extraction; LLM evaluation is simpler but less interpretable

- **Failure signatures**:
  - Low Top-1 accuracy with reasonable Top-5/Top-10: suggests attribute definitions are semantically overlapping; the model cannot uniquely identify the correct dimension
  - Confusion matrix clustering on specific relations: indicates prompt bias or definition ambiguity (see Figure 2: GPT-3.5 shows sparser confusion patterns)
  - Generated responses averaging 35% longer than originals: may indicate verbose hallucination rather than focused commonsense integration

- **First 3 experiments**:
  1. Baseline validation: Replicate the zero-shot generation + GPT-4 evaluation pipeline on 20 dialogues; verify MRR falls within 0.40-0.42 range
  2. Definition ablation: Systematically remove 2-3 easily confused relations (e.g., xWant vs. oWant) and measure MRR change; this quantifies definition overlap impact
  3. Cross-model consistency check: Generate with GPT-3.5, evaluate with GPT-4 (and vice versa); compare diagonal vs. cross-diagonal confusion matrices to isolate generation vs. evaluation quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the LLM-based evaluation of commonsense attributes correlate with human judgment? The authors state, "In future work, we plan to... perform manual annotations to better detect which commonsense attributes are better reflected in the expanded turns." The current study relies solely on automatic evaluation (LLMs ranking themselves) and the ACCENT metric, lacking ground-truth human verification.

### Open Question 2
Can a fine-tuned small generative model effectively extract commonsense attributes with performance comparable to large proprietary models? The authors propose to "fine-tune a small generative model that can be used to automatically extract the attributes in chitchat dialogues." The preliminary results depend on GPT-3.5 and GPT-4, which are resource-intensive.

### Open Question 3
To what extent does the semantic overlap between ATOMIC relation definitions contribute to classification errors? The analysis of confusion matrices notes "biases towards certain relations" and suggests "an overlapping issue concerning the definitions of the commonsense relations" may be the cause. While the authors observe lower performance on specific relations, they do not isolate whether this is a failure of the model's reasoning or a flaw in the prompt definitions.

## Limitations

- The evaluation framework creates an artificial setup by assuming perfect knowledge of the "ground truth" attribute used during generation
- Generated responses show 35% length increase, suggesting potential verbosity issues
- Lack of human evaluation for naturalness and coherence of generated responses
- Reliance on self-referential evaluation (LLMs ranking themselves) raises potential bias concerns

## Confidence

- **High confidence**: Core finding that LLMs can perform zero-shot commonsense data augmentation for dialogue systems, as evidenced by consistent MRR scores across multiple datasets and clear performance differentiation between GPT-3.5 and GPT-4
- **Medium confidence**: Claim that LLMs can serve as reliable evaluators for commonsense-enriched dialogue responses, as lack of human baseline comparisons and self-referential evaluation introduce potential biases
- **Low confidence**: Practical utility of generated responses for improving end-user dialogue systems, as the paper does not demonstrate downstream task performance or user satisfaction improvements

## Next Checks

1. **Human evaluation validation**: Conduct blind human assessments comparing LLM-generated commonsense responses against original dialogue turns, measuring both semantic appropriateness and conversational naturalness across all 12 ATOMIC relations.

2. **Cross-model consistency testing**: Generate responses with GPT-3.5 and evaluate with GPT-4 (and vice versa) to determine whether the strong MRR results reflect genuine attribute-response alignment or model-specific idiosyncrasies in the ranking process.

3. **Definition overlap ablation study**: Systematically remove pairs of semantically similar relations (e.g., xWant vs oWant, xEffect vs oEffect) and measure the impact on MRR and Top-1 accuracy to quantify the extent to which definition ambiguity drives evaluation performance.