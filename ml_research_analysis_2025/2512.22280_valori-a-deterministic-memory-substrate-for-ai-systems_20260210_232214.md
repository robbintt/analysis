---
ver: rpa2
title: 'Valori: A Deterministic Memory Substrate for AI Systems'
arxiv_id: '2512.22280'
source_url: https://arxiv.org/abs/2512.22280
tags:
- memory
- valori
- deterministic
- floating-point
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Valori addresses the problem of non-deterministic AI memory, where
  floating-point arithmetic causes identical AI models and inputs to produce different
  memory states and retrieval results across hardware architectures (x86 vs. ARM),
  preventing replayability and safe deployment in regulated sectors.
---

# Valori: A Deterministic Memory Substrate for AI Systems

## Quick Facts
- arXiv ID: 2512.22280
- Source URL: https://arxiv.org/abs/2512.22280
- Reference count: 6
- Primary result: Valori achieves bit-identical memory states and Recall@10 of 0.998 across x86 and ARM architectures using fixed-point arithmetic

## Executive Summary
Valori addresses a fundamental challenge in AI systems: non-deterministic memory behavior across different hardware architectures due to floating-point arithmetic variations. The system replaces floating-point operations with deterministic Q16.16 fixed-point arithmetic and models memory as a replayable state machine, ensuring bit-identical results across platforms. By normalizing inputs at the kernel boundary and implementing pure state transitions, Valori guarantees reproducible memory states, snapshots, and search results while maintaining semantic retrieval quality with only 0.2% recall degradation.

## Method Summary
Valori implements a deterministic memory substrate by replacing all floating-point arithmetic with Q16.16 fixed-point operations and modeling memory as a replayable state machine. The system normalizes incoming float32 vectors to Q16.16 format at the kernel boundary, performs all subsequent operations using integer arithmetic, and constructs deterministic HNSW indices through fixed ordering and data-dependent decisions. The kernel is implemented as a `no_std` pure state machine, eliminating I/O side effects and ensuring reproducibility. Cross-platform consistency is verified through snapshot hash comparison and recall benchmark evaluation against floating-point baselines.

## Key Results
- Cross-platform snapshot hashes matched exactly between x86 and ARM machines
- Valori achieved Recall@10 of 0.998 compared to 1.000 for floating-point indices
- Bit-identical k-NN search results across different hardware architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing floating-point arithmetic with fixed-point (Q16.16) integer operations ensures cross-platform bit-identical results, because integer ALU instructions produce deterministic outputs across CPU architectures.
- Mechanism: IEEE 754 floating-point operations permit implementation-defined behavior in FMA usage and SIMD reduction ordering, causing different bit-level outcomes on x86 vs. ARM even with identical source code. Valori normalizes all input vectors to Q16.16 fixed-point at the memory boundary and performs subsequent distance calculations and index updates using only integer arithmetic. Since integer addition is associative and integer ALU instructions are architecturally deterministic, results are bit-identical across x86, ARM, RISC-V, and WASM.
- Core assumption: Integer arithmetic on major CPU architectures and WASM produces consistent, bit-identical results for the same inputs. Q16.16 precision (~0.000015 resolution) is sufficient to represent normalized embedding vectors (typically in [-1, 1]) without significant semantic distortion.
- Evidence anchors: [abstract] "The core method replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine, ensuring bit-identical memory states, snapshots, and search results across platforms." [section 5.1] "Since Q16.16 relies on standard integer ALU instructions (which are consistent across architectures), numerical results are guaranteed bit-identical on x86, ARM, RISC-V, and WASM."
- Break condition: If target platforms implement integer operations non-deterministically (unlikely for standard ALUs), or if Q16.16 resolution proves insufficient for an embedding model's dynamic range or granularity, semantic fidelity would degrade.

### Mechanism 2
- Claim: Modeling memory as a replayable state machine with a deterministic transition function guarantees that identical command sequences applied to an initial state produce the same final state across execution environments.
- Mechanism: The system defines `S_t` as memory state at logical time `t`, `C_t` as commands (insert, delete, link), and a transition function `F`. Determinism requires: `S_N = Apply(S_0, {C_1...C_N})` to be identical for all environments. Valori implements `F` as pure functions within a `no_std` kernel, operating exclusively on fixed-point normalized data and eliminating I/O side effects. Snapshots serialize the full state, enabling exact restoration on different machines.
- Core assumption: All non-determinism enters via external inputs (e.g., neural network embeddings) and can be fully normalized at the kernel boundary. The transition function contains no hidden state, timing dependencies, or race conditions.
- Evidence anchors: [abstract] "models memory as a replayable state machine" and "guarantees bit-identical memory states, snapshots, and search results across platforms." [section 3.1] Formal definition: `S_{t+1} = F(S_t, C_t)` with `∀EnvA, EnvB : Apply(S_0, {C_i})|_A ≡ Apply(S_0, {C_i})|_B`. [section 5.2] "The kernel is a pure state machine implemented without standard library I/O (`no_std`)."
- Break condition: If the kernel inadvertently introduces non-determinism via uninitialized memory, threading races, or boundary leaks (e.g., float-to-fixed conversion differences), state reproducibility fails.

### Mechanism 3
- Claim: HNSW-based approximate nearest neighbor indexing can be made fully deterministic by eliminating stochastic elements via fixed insertion ordering and data-dependent decisions, preserving retrieval quality with a Recall@10 of 0.998 relative to a floating-point baseline.
- Mechanism: Standard HNSW graph construction uses randomized entry points and stochastic neighbor selection. Valori enforces: (1) fixed insertion order (sorted by ID), (2) deterministic entry point selection (first inserted node, ID 0), and (3) data-dependent neighbor selection using fixed-point distance metrics. This yields identical graph topology across runs and platforms while maintaining hierarchical connectivity for efficient ANN search.
- Core assumption: HNSW retrieval quality is primarily determined by graph connectivity and distance metric consistency, not by stochastic exploration. The embedding space's semantic structure is robust to Q16.16 quantization error.
- Evidence anchors: [section 7] "Valori adapts them for strict determinism: 1. Fixed Ordering... 2. Data-Dependent Ordering... 3. Graph Construction uses fixed-point distance metrics, ensuring the graph topology is identical across runs." [section 8.3, Table 3] "Valori achieved a mean Recall@10 of 99.8%... [Table: Float32 HNSW 1.000, Valori Q16.16 HNSW 0.998]"
- Break condition: If quantization significantly distorts distance rankings beyond the observed ~0.2% recall loss, or if deterministic HNSW variants fail on high-dimensional or skewed embedding distributions, retrieval quality degrades.

## Foundational Learning

- **IEEE 754 Floating-Point Non-Determinism**
  - Why needed here: Valori's core motivation is that IEEE 754 permits hardware-dependent behavior (FMA usage, SIMD reduction order), causing identical code to produce divergent bit-level results across platforms.
  - Quick check question: Can you explain why `(a + b) + c ≠ a + (b + c)` in IEEE 754 floating-point arithmetic and how this affects parallel reductions?

- **Fixed-Point Arithmetic (Q-Format)**
  - Why needed here: Valori's solution replaces `f32/f64` with Q16.16 fixed-point representation, where numbers are stored as integers with an implicit scaling factor (2^−16), enabling deterministic cross-platform math.
  - Quick check question: For Q16.16 format (32-bit signed, 16 fractional bits), what is the representable range and resolution? How would you convert 0.5 to its Q16.16 integer representation?

- **State Machine Formalism**
  - Why needed here: Valori models memory as a deterministic state machine with explicit states, commands, and transition functions, enabling formal reasoning about replayability.
  - Quick check question: Given `S_{t+1} = F(S_t, C_t)`, if you have `S_0` and a command sequence `{C_1, C_2, C_3}`, what property must `F` satisfy for the system to be deterministic across environments?

## Architecture Onboarding

- **Component map:**
  [External World] → [Python/HTTP Interface (Node/std)] → [Valori Kernel (no_std)] ←───── Determinism Boundary
                                                              │
                    (float inputs normalized here)
                                                              │
                    [Q16.16 Fixed-Point Arithmetic Engine]
                    [HNSW Index (deterministic construction)]
                    [Snapshot/Restore Module]
                    [Pure State Machine Core]

- **Critical path:**
  1. **Boundary normalization:** Incoming float32 vectors are immediately converted to Q16.16 at kernel entry—this is the single point where non-determinism is excluded.
  2. **State transitions:** All commands (insert, delete, link) flow through pure state-machine logic operating on fixed-point data.
  3. **Index construction:** HNSW graphs are built deterministically via fixed ordering and data-dependent decisions.
  4. **Snapshot serialization:** Entire memory state serializes to bit-identical files for cross-platform transfer.

- **Design tradeoffs:**
  - **Precision vs. Portability:** Q16.16 guarantees determinism but has limited dynamic range ([-32768, 32767]) and resolution (~0.000015). Suitable for normalized embeddings; may fail for unnormalized data.
  - **Performance vs. Correctness:** Software fixed-point arithmetic is slower than hardware-accelerated float ops (AVX-512, GPU). Valori prioritizes reproducibility over maximum throughput.
  - **Approximation vs. Determinism:** Traditional ANN indices accept stochastic behavior for recall/latency gains; Valori sacrifices some recall (0.998 vs. 1.000) for bit-identical results.

- **Failure signatures:**
  - **Quantization overflow:** Input vectors with values outside [-32768, 32767] or requiring finer resolution than ~0.000015 will saturate or distort, degrading retrieval quality.
  - **Boundary leak:** If float-to-fixed conversion differs across platforms (e.g., different rounding modes), determinism is broken at the entry point.
  - **Non-deterministic seeding:** If HNSW construction uses random seeds not replaced by data-dependent ordering, graph topology diverges.
  - **Threading races:** Concurrent access to kernel state without proper synchronization could introduce non-determinism.

- **First 3 experiments:**
  1. **Cross-platform snapshot hash verification:** Insert 1,000+ vectors on x86, snapshot, transfer to ARM, restore, and verify internal hash matches exactly. Confirm bit-identical state transfer.
  2. **Recall@k comparison under quantization:** Build parallel indices (Float32 HNSW vs. Valori Q16.16 HNSW) on the same dataset with identical insertion order. Measure Recall@10 overlap to quantify semantic fidelity loss.
  3. **k-NN result ordering consistency:** Execute identical query sets on restored snapshots across platforms. Verify that result ordering and distance scores are byte-identical, not just semantically similar.

## Open Questions the Paper Calls Out
- How can non-determinism in the neural network inference layer (embedding generation) be eliminated to ensure the entire pipeline, from input to memory storage, is bit-identical across hardware?
- Can SIMD integer instructions be safely utilized to accelerate fixed-point arithmetic operations without reintroducing architectural non-determinism?
- Does the Q16.16 fixed-point quantization preserve semantic retrieval fidelity across diverse embedding models and high-dimensional datasets?

## Limitations
- Q16.16 precision may not generalize to all embedding models or non-normalized data
- Performance overhead from software fixed-point arithmetic vs. hardware-accelerated floating-point
- Results represent an "existence proof" rather than universal solution across all embedding models

## Confidence
- High Confidence: Cross-platform bit-identical snapshot hashes
- Medium Confidence: Deterministic state machine model
- Medium Confidence: Recall@10 of 0.998 relative to baseline

## Next Checks
1. Test Q16.16 overflow behavior with extreme embedding values to quantify precision failure modes
2. Measure performance overhead of fixed-point vs. floating-point operations across different hardware configurations
3. Validate determinism under concurrent access patterns to identify potential threading race conditions