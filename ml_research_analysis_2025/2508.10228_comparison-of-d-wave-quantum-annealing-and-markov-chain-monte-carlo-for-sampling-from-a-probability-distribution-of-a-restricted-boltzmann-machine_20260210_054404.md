---
ver: rpa2
title: Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling
  from a Probability Distribution of a Restricted Boltzmann Machine
arxiv_id: '2508.10228'
source_url: https://arxiv.org/abs/2508.10228
tags:
- d-wave
- training
- sampling
- found
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compared sampling from Restricted Boltzmann Machines
  (RBMs) using D-Wave quantum annealing versus classical Gibbs sampling. The key idea
  was to evaluate sampling quality by examining the number and energy of local valleys
  (LVs) that each method could find, rather than traditional statistical measures.
---

# Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine

## Quick Facts
- **arXiv ID**: 2508.10228
- **Source URL**: https://arxiv.org/abs/2508.10228
- **Reference count**: 32
- **Primary result**: Quantum annealing samples from RBMs found more diverse local energy basins than classical Gibbs sampling, but many of these basins differed between methods rather than providing complementary coverage.

## Executive Summary
This paper compares sampling from Restricted Boltzmann Machines (RBMs) using D-Wave quantum annealing versus classical Gibbs sampling, evaluating quality through the number and energy of local valleys (LVs) discovered rather than traditional statistical measures. The study used D-Wave Advantage 4.1 with Pegasus topology to sample from RBMs trained on the OptDigits dataset, examining conditions relevant to contrastive divergence training. Results showed quantum annealing samples belonged to more LVs than classical samples, though many LVs differed between methods. The two techniques were less complementary for high-probability states, with many intermediate-probability LVs found by only one method. No significant improvement in LV discovery was achieved by decreasing D-Wave annealing time, suggesting a combined classical-quantum approach might offer the best sampling performance for RBM training.

## Method Summary
The study trained RBMs on the OptDigits dataset using classical CD-k with weight decay to constrain weights to [-1,1]. The trained RBMs were embedded into D-Wave Advantage 4.1 using a scale factor of 2 to maintain chain integrity. D-Wave sampling used default 20μs forward annealing, while classical Gibbs sampling used k_G=5 steps at T=1. Local valleys were identified by relaxing samples at T=0 until convergence. The study compared the number of distinct LVs found by each method, their energy distributions, and overlap statistics across multiple training epochs. Classification error was used as a sanity check for embedding quality rather than a primary metric.

## Key Results
- D-Wave samples belonged to significantly more local valleys (LVs) than Gibbs samples, with quantum annealing finding unique intermediate-probability states
- Many LVs discovered by quantum annealing differed from those found by classical sampling rather than providing complementary coverage
- The two methods overlapped less at later training epochs, where improved sampling could meaningfully impact trainability
- Decreasing D-Wave annealing time did not significantly improve LV discovery, as shorter times didn't increase diversity despite reducing Ground State probability

## Why This Works (Mechanism)

### Mechanism 1: Diverse Local Valley (LV) Exploration via Quantum Fluctuations
Quantum annealing uses quantum tunneling rather than thermal fluctuations to traverse energy landscapes, allowing access to high-barrier regions that trap classical Markov Chains at T=1. This enables sampling of narrow basins with intermediate probabilities that contribute little to total density but are critical for gradient correction. The advantage is most pronounced when sampling at T=1 with few steps, conditions that challenge classical methods but leverage quantum fluctuations.

### Mechanism 2: Inverse Correlation of Ground State Probability and Sample Variance
Optimizing D-Wave for finding the Ground State degrades its utility as a sampler by reducing sample variance. As annealing time increases or hardware improves to minimize Time-To-Solution, the probability of collapsing into the single lowest energy state increases, causing solution repetitions to contain identical states. This eliminates the diversity required for contrastive divergence training, as a sample returning the Ground State 100% of the time lacks the variance needed for effective learning.

### Mechanism 3: Embedding Fidelity via Scale Factor Calibration
RBM graphs must be embedded into Pegasus topology by chaining qubits, and accuracy is constrained by the ratio of logical weights to chain coupling strengths. If logical weights are too large relative to ferromagnetic chain couplings, chains break and solve a different Hamiltonian than intended. Weight decay during classical pre-training is necessary to keep weights small enough for embedding without requiring excessively strong chain couplings.

## Foundational Learning

- **Local Valleys (LVs) vs. Global Minimum**: Standard metrics like KL divergence often miss narrow LVs. This paper evaluates success based on coverage of specific energy basins corresponding to Training Patterns. Quick check: Does a sample returning the Ground State 100% of the time constitute a "good" sample for RBM training? (Answer: No, it lacks variance).

- **Contrastive Divergence (CD-k)**: The paper critiques that many QA comparisons use idealized sampling conditions. This study specifically simulates the T=1, short-chain conditions (k_G=1) used in actual classical training. Quick check: Why is sampling at T=1 with few steps harder for QA than simulated annealing at low T? (Answer: QA hardware operates at low physical temperature, making it naturally biased toward low-energy states, potentially missing the "thermal" spread required for T=1 sampling).

- **Minor Embedding & Chain Strength**: You cannot map a fully connected RBM directly to the sparse Pegasus graph. Understanding that "logical" variables are "chains" of qubits is critical to diagnosing sampling errors. Quick check: If the weights of your RBM are too large, what happens to the chains in the D-Wave embedding? (Answer: They break/flip, corrupting the solution).

## Architecture Onboarding

- **Component map**: Classical RBM Pre-trainer -> Scaler -> Embedder -> QPU Sampler -> LV Analyzer
- **Critical path**: Determining the Optimal Scale Factor (SF). If too low, signal is lost in quantization noise; if too high, chains break. The paper identifies SF ≈ 2 as optimal for their specific dataset.
- **Design tradeoffs**:
  - Annealing Time vs. Diversity: Shorter times (~20μs) maintain variance for sampling; longer times improve Ground State probability but reduce distinct LVs found.
  - Sample Size vs. Overlap: Larger sample sizes (10,000 vs 1,000) increase overlap between classical and quantum methods, potentially reducing complementary advantage.
- **Failure signatures**:
  - High Classification Error (>15%): Likely indicates poor embedding (SF too high/low) or chain breaks.
  - Low N_LV: Indicates annealing time is too long or hardware is over-optimizing for Ground State.
  - Low Overlap with Training Patterns: Indicates QA is sampling "spurious" minima not relevant to data distribution.
- **First 3 experiments**:
  1. Scale Factor Sweep: Embed trained RBM with varying SFs (0.5 to 4.0) and plot classification error to find optimal balance where chain integrity is maintained without signal loss.
  2. LV Diversity Benchmark: Run D-Wave and Gibbs sampling (k_G=1) on same RBM. Calculate percentage of LVs found by one but missed by other to quantify "complementarity".
  3. Annealing Time Variance Test: Vary annealing time (1μs to 2000μs) and plot P_GS vs N_LV to confirm trade-off specific to problem instance.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid training approach that integrates traditional Gibbs sampling with D-Wave sampling improve the classification error or convergence rate of Restricted Boltzmann Machines (RBMs)? The Conclusion explicitly identifies this as a "logical next step in future work," suggesting the two techniques could complement each other, though the study only compared methods side-by-side without experimenting with an algorithm that actively combines them during training.

### Open Question 2
Can alternative annealing schedules, such as reverse annealing, pausing, or quenching, increase the diversity of samples (number of distinct Local Valleys) compared to standard forward annealing? The authors mention in Section 3.4 that custom schedules exist but were not utilized, and later demonstrate in Section 4.2 that varying forward annealing duration failed to significantly increase sample diversity.

### Open Question 3
Does the inclusion of unique intermediate-probability states found by the D-Wave (but missed by MCMC) provide a tangible benefit to the model's ability to learn training patterns? The authors note that "potentially important" local minima with intermediate probabilities are found uniquely by each method and that sampling differences are most pronounced in later training epochs where they could impact trainability, but do not quantify their specific contribution to optimization landscape or resulting model quality.

## Limitations
- Exact modified weight-decay formulation and precise training hyperparameters (learning rate, batch size) remain unspecified
- Study focuses exclusively on the OptDigits dataset, limiting generalizability to other data distributions and model complexities
- Assumes LV diversity directly translates to better training performance without direct experimental validation of this causal relationship

## Confidence

**High Confidence**: The inverse correlation between Ground State probability and sample variance is well-supported by empirical evidence and fundamental physics. Embedding fidelity concerns are clearly articulated and practically validated through scale factor optimization.

**Medium Confidence**: The claim about diverse LV exploration via quantum fluctuations shows promising evidence but relies on specific assumptions about what constitutes "quality" sampling. The finding that quantum and classical methods find different LVs is robust, but whether these differences meaningfully impact training remains uncertain.

**Low Confidence**: The broader claim that hybrid classical-quantum approaches offer superior sampling performance lacks direct experimental validation within this study. The study demonstrates complementarity exists but does not test whether combining methods actually improves training outcomes.

## Next Checks

1. **Trainability Impact Test**: Run RBM training using combined quantum-classical samples versus each method individually, measuring convergence speed and final log-likelihood to directly assess whether LV complementarity translates to training improvements.

2. **Dataset Generalization Study**: Repeat the LV diversity analysis on additional datasets (MNIST, CIFAR-10 subsets) to determine if quantum annealing's advantage in finding intermediate-probability states persists across different data distributions and model complexities.

3. **Embedding Robustness Analysis**: Systematically vary the scale factor and chain strength parameters across multiple problem instances to quantify the stability of the SF=2 optimization and identify failure modes when embedding larger or denser RBM architectures.