---
ver: rpa2
title: 'StRuCom: A Novel Dataset of Structured Code Comments in Russian'
arxiv_id: '2505.11026'
source_url: https://arxiv.org/abs/2505.11026
tags:
- comments
- dataset
- language
- code
- comment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces StRuCom, the first large-scale dataset (153K
  examples) specifically designed for Russian code documentation. The dataset combines
  human-written comments from Russian GitHub repositories with synthetically generated
  ones, ensuring compliance with five programming languages' documentation standards
  through automated validation.
---

# StRuCom: A Novel Dataset of Structured Code Comments in Russian

## Quick Facts
- arXiv ID: 2505.11026
- Source URL: https://arxiv.org/abs/2505.11026
- Reference count: 13
- Key outcome: First large-scale dataset (153K examples) for Russian code documentation, showing statistically significant improvements in chrf++ and BERTScore metrics when fine-tuning Qwen2.5-Coder models

## Executive Summary
This paper introduces StRuCom, the first large-scale dataset specifically designed for generating structured Russian-language code comments. The dataset combines 7,719 real human-written comments from Russian GitHub repositories with 79,548 enhanced comments (via Miqu-70B) and 65,914 synthetic comments (via Qwen2.5-Coder-32B-Instruct). The authors developed an automated filtering tool to validate comment structures across five programming languages' documentation standards. Fine-tuning Qwen2.5-Coder models on StRuCom showed significant improvements in comment generation quality metrics compared to baseline models.

## Method Summary
The authors created StRuCom by collecting Russian code-comment pairs from GitHub repositories, filtering for structured documentation compliance using docstring_parser and custom validation tools. The dataset includes examples from Python, Java, JavaScript, C#, and Go, with comments restricted to 250-1,000 characters. Three data sources were used: real comments (7,719), enhanced comments (79,548 via Miqu-70B), and synthetic comments (65,914 via Qwen2.5-Coder-32B-Instruct). For fine-tuning experiments, 7,500 examples were sampled for training (1,500 per language from synthetic/enhanced) and 500 for testing (100 per language from real comments). Qwen2.5-Coder models (0.5B-7B) were fine-tuned with LoRA using specific hyperparameters, and evaluated using chrf++ and BERTScore with E5-Mistral 7B as the embedder.

## Key Results
- StRuCom dataset contains 153K Russian code-comment pairs across five programming languages
- Fine-tuned Qwen2.5-Coder models showed statistically significant improvements in chrf++ and BERTScore metrics
- Structure compliance rates varied significantly across languages, with Python/JS showing lower completeness due to complex standards

## Why This Works (Mechanism)
The dataset addresses a critical gap in code documentation for non-English languages by providing structured, standards-compliant Russian comments. The combination of real, enhanced, and synthetic data ensures both quality and quantity, while the automated validation tool guarantees consistency with documentation standards. Fine-tuning large language models on this specialized dataset enables them to generate more accurate and contextually appropriate Russian code comments.

## Foundational Learning
- **Russian Language Detection**: Why needed - To filter code comments written in Russian from multilingual repositories. Quick check - Verify Russian detection accuracy using Lingua on cleaned text samples.
- **Documentation Standards Compliance**: Why needed - Ensures generated comments follow language-specific formatting rules. Quick check - Validate comment structure using docstring_parser for each language.
- **LoRA Fine-tuning**: Why needed - Enables efficient adaptation of large models to the specialized dataset. Quick check - Monitor training loss and validation metrics across epochs.
- **BERTScore with E5-Mistral 7B**: Why needed - Evaluates semantic similarity of generated comments to references. Quick check - Compare BERTScore results with traditional BLEU/chrf++ metrics.

## Architecture Onboarding
**Component Map**: GitHub repos -> Lingua language detection -> docstring_parser validation -> StRuCom dataset -> Qwen2.5-Coder fine-tuning with LoRA -> chrf++/BERTScore evaluation

**Critical Path**: Data collection and validation → model fine-tuning → evaluation → statistical testing

**Design Tradeoffs**: Real comments provide quality but limited quantity; synthetic comments provide scale but may lack authenticity; validation tools ensure standards compliance but may filter valid variations

**Failure Signatures**: Low structure compliance rates indicate filtering tool over-restriction; poor metric scores suggest model underfitting or dataset quality issues; language detection errors lead to incorrect data inclusion

**First Experiments**:
1. Validate structure compliance rates on a small sample of collected comments before full dataset creation
2. Fine-tune Qwen2.5-Coder-0.5B with LoRA on a minimal training subset and evaluate on a held-out test set
3. Compare chrf++ and BERTScore results between base and fine-tuned models on identical test samples

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset and filtering tool are not publicly available, preventing independent verification
- Evaluation metrics focus on surface-level similarity rather than factual correctness or semantic equivalence
- Sample sizes (7,500 training, 500 test) are relatively small for a 153K dataset
- Paper does not address potential biases in synthetic data generation or cross-language generalizability

## Confidence
- **High confidence**: Dataset creation methodology and fine-tuning procedure are technically sound and well-documented
- **Medium confidence**: Performance improvements are statistically significant but may be influenced by test set composition and metric choices
- **Low confidence**: Claims about dataset utility cannot be independently validated without access to the dataset and tools

## Next Checks
1. Obtain the StRuCom dataset and filtering tool to verify the claimed 153K examples and structure compliance rates
2. Re-run the Qwen2.5-Coder fine-tuning experiments using specified hyperparameters to verify reported metric improvements
3. Evaluate fine-tuned models on additional Russian code-comment pairs from different sources to assess generalizability