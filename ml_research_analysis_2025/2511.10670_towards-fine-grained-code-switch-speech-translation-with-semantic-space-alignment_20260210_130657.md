---
ver: rpa2
title: Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment
arxiv_id: '2511.10670'
source_url: https://arxiv.org/abs/2511.10670
tags:
- speech
- data
- translation
- language
- projector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment

## Quick Facts
- arXiv ID: 2511.10670
- Source URL: https://arxiv.org/abs/2511.10670
- Authors: Yan Gao; Yazheng Yang; Zhibin Lan; Yidong Chen; Min Zhang; Daimeng Wei; Hui Huang; Jinsong Su
- Reference count: 7
- Primary result: Achieves 37.13 BLEU on Chinese-English CS ST

## Executive Summary
This work addresses the challenge of code-switched (CS) speech translation by proposing a Mixture-of-Experts (MoE) speech projector architecture that enables fine-grained semantic modeling across different languages. The approach leverages a staged training curriculum that progressively transitions from abundant monolingual ASR data to the target CS translation task, using transition loss to prevent catastrophic forgetting. Experiments on Chinese-English and English-Spanish CS datasets show substantial improvements over strong baselines, demonstrating the effectiveness of language-specific expert specialization for handling the semantic complexities introduced by code-switching.

## Method Summary
The proposed method uses a frozen Whisper-large-v3 encoder with LoRA, connected to a 3-layer MoE speech projector with n=3 experts per language group, which routes speech tokens to specialized experts before passing them to a frozen LLaMA2 LLM with LoRA. The training follows a four-stage curriculum: (1) pretrain language-specific projectors on ASR data, (2) integrate into MoE with language-specific and load balancing losses, (3) transition to monolingual ST using transition loss, and (4) transition to CS ST without language supervision. The approach uses Top-3 routing with auxiliary losses L_lang and L_balance during early stages to guide expert specialization, while transition loss L_transition interpolates between ASR and ST objectives to enable smooth knowledge transfer.

## Key Results
- Achieves 37.13 BLEU on Chinese-English CS ST, outperforming baselines by 4.0-8.5 BLEU
- Expert projectors improve WER by 0.29-0.05 over shared projector in preliminary study
- Ablation shows language-specific loss and intra-group load balancing contribute 1.78 BLEU improvement
- Maintains strong performance on both Fisher (En-Es) and NTUML2021 (Zh-En) datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific expert projectors capture fine-grained semantic distinctions in code-switched speech better than a shared projector.
- Mechanism: The MoE speech projector routes speech tokens to specialized experts based on language-specific semantic subspaces. Each expert group specializes in one language's acoustic-semantic patterns, reducing interference from cross-lingual representation gaps.
- Core assumption: Speech representations from different languages occupy distinguishable semantic subspaces that benefit from specialized processing pathways.
- Evidence anchors:
  - [abstract] "each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features"
  - [Preliminary Study] t-SNE visualization shows clear clustering by language; expert projectors achieve WER 7.70/4.41 vs. shared projector's 7.99/4.46
  - [corpus] Weak direct evidence; related work (POTSA) addresses cross-lingual speech alignment but uses different mechanisms
- Break condition: If speech representations across languages overlap significantly in semantic space, language-specific expert specialization provides diminishing returns over a unified projector.

### Mechanism 2
- Claim: Progressive multi-stage training with transition loss enables effective knowledge transfer from abundant monolingual data to scarce code-switched data.
- Mechanism: The training paradigm first establishes speech-text alignment via ASR pretraining (Stage 1-2), then progressively transitions to translation tasks (Stage 3-4). The transition loss (L_transition) interpolates between source and target task losses, preventing catastrophic forgetting and training instability.
- Core assumption: Monolingual ASR and ST data provide transferable representations that can be adapted to CS scenarios with appropriate transition mechanisms.
- Evidence anchors:
  - [abstract] "multi-stage training paradigm that utilizes readily available monolingual ASR and monolingual ST data, facilitating speech-text alignment"
  - [Methodology] λ = b/B schedules transition from ASR to ST losses progressively
  - [corpus] No direct corpus evidence for transition loss in speech translation; related work focuses on speech-text alignment but not staged curriculum
- Break condition: If the distribution shift between monolingual and CS data exceeds the model's adaptation capacity, transition loss alone may be insufficient; additional data augmentation or synthetic CS data may be required.

### Mechanism 3
- Claim: Explicit supervision via language-specific and intra-group load balancing losses guides expert specialization and routing consistency.
- Mechanism: L_lang penalizes routing tokens to experts from non-matching language groups; L_balance ensures balanced expert utilization within each group, preventing expert collapse where only a subset of experts are active.
- Core assumption: Oracle language labels are available during monolingual training stages to provide routing supervision.
- Evidence anchors:
  - [abstract] "language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens"
  - [Ablation Studies] Removing L_lang and L_balance causes BLEU drop from 37.13 to 35.35; conventional balance loss performs slightly worse (36.77)
  - [corpus] No direct corpus evidence; MoE load balancing is established practice but intra-group variant is novel here
- Break condition: In CS data (Stage 4), language labels are unavailable, so these losses cannot be applied; if expert specialization collapses during unsupervised Stage 4, routing quality may degrade.

## Foundational Learning
- Concept: Mixture of Experts (MoE) with sparse routing
  - Why needed here: Core architecture for fine-grained language-specific processing; requires understanding of router-based token dispatch and expert specialization
  - Quick check question: Can you explain why Top-k routing with load balancing prevents expert collapse?
- Concept: Multimodal LLM architectures (speech encoder + projector + LLM)
  - Why needed here: Framework for understanding how audio features are aligned to LLM embedding space via the MoE projector
  - Quick check question: What is the role of the projector in bridging the speech encoder and LLM representation spaces?
- Concept: Code-switching linguistic phenomena
  - Why needed here: Context for why language alternation creates semantic modeling challenges distinct from monolingual or multilingual translation
  - Quick check question: Why does code-switching introduce semantic complexity beyond standard multilingual settings?

## Architecture Onboarding
- Component map: Whisper-large-v3 encoder (frozen, LoRA) → MoE Speech Projector (3-layer MLP, n×m experts, fully fine-tuned) → LLaMA2 LLM (frozen, LoRA). Router inserted between projector layers for Top-3 expert selection.
- Critical path: Stage 1 (per-language projector pretraining on ASR) → Stage 2 (MoE assembly + L_lang + L_balance on bilingual ASR) → Stage 3 (transition to monolingual ST with L_transition) → Stage 4 (transition to CS ST, losses removed due to missing language labels).
- Design tradeoffs: Expert count (n=3 per language) vs. computational overhead; transition loss interpolation schedule (linear λ=b/B) vs. alternative curricula; LoRA rank (512 for LLM, 64 for encoder) vs. fine-tuning capacity.
- Failure signatures: (1) Expert collapse → check per-expert token allocation in L_balance; (2) Routing collapse on CS data → verify Stage 4 performance gap vs. Stage 3; (3) Training instability at stage boundaries → inspect L_transition gradient magnitude.
- First 3 experiments:
  1. Reproduce preliminary study: Compare shared vs. expert projectors on bilingual ASR (En/Es) to validate semantic gap hypothesis.
  2. Ablate L_lang and L_balance: Train Stage 2-3 without auxiliary losses to quantify routing supervision contribution.
  3. Analyze expert assignment: On monolingual data with known language labels, compute Top-k routing accuracy to corresponding language experts (target: >90% as reported in Table 5).

## Open Questions the Paper Calls Out
- Can the proposed MoE framework effectively scale to a wider range of code-switched language pairs beyond the evaluated English-Spanish and Chinese-English scenarios?
- Can more adaptive expert selection mechanisms improve routing efficiency and translation quality compared to the current Top-k routing strategy?
- Does the MoE speech projector architecture generalize effectively to related code-switching tasks such as Automatic Speech Recognition (ASR) and Machine Translation (MT)?
- How can language-specific supervision be effectively incorporated during fine-tuning on CS data where oracle token-level language labels are unavailable?

## Limitations
- The staged training curriculum assumes monotonic knowledge transfer but lacks quantitative analysis of distribution shift magnitudes between stages
- Dependency on oracle language labels during training stages 1-3 presents a practical limitation for real-world code-switched scenarios
- Limited ablations on individual components like impact of MoE routing complexity (top-3 vs top-1) or necessity of LoRA fine-tuning versus full fine-tuning

## Confidence
- High confidence: Architectural design choices are internally consistent and grounded in established practices; reported performance improvements are substantial
- Medium confidence: Semantic gap hypothesis supported by preliminary study but t-SNE visualization provides limited quantitative evidence; transition loss mechanism lacks comparative analysis against alternatives
- Low confidence: Scalability claims to other language pairs remain speculative without empirical validation; long-term stability of expert specialization during Stage 4 is not evaluated longitudinally

## Next Checks
1. Measure Wasserstein distance or KL divergence between feature distributions across consecutive training stages to empirically validate the need for transition loss and quantify adaptation difficulty
2. On Stage 3 monolingual test data with oracle labels, compute Top-3 routing accuracy to language-specific experts and analyze routing entropy to verify that specialization is maintained without language supervision
3. Systematically remove MoE (use shared projector), transition loss, and LoRA fine-tuning to isolate their individual contributions to the reported 37.13 BLEU score and identify which components are essential versus beneficial