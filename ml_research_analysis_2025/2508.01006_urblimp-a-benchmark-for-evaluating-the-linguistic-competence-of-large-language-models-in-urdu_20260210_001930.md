---
ver: rpa2
title: 'UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language
  Models in Urdu'
arxiv_id: '2508.01006'
source_url: https://arxiv.org/abs/2508.01006
tags:
- urdu
- linguistic
- agreement
- language
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UrBLiMP, a benchmark for evaluating the linguistic
  competence of large language models in Urdu. UrBLiMP consists of 5,696 minimal pairs
  across ten core syntactic phenomena, constructed using the Urdu Treebank and diverse
  text corpora.
---

# UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu

## Quick Facts
- arXiv ID: 2508.01006
- Source URL: https://arxiv.org/abs/2508.01006
- Reference count: 10
- Key outcome: UrBLiMP benchmark with 5,696 minimal pairs across 10 syntactic phenomena evaluates 20 multilingual models, with LLaMA-3-70B achieving 94.73% accuracy, though models struggle with long-distance dependencies.

## Executive Summary
This paper introduces UrBLiMP, a benchmark for evaluating the linguistic competence of large language models in Urdu. UrBLiMP consists of 5,696 minimal pairs across ten core syntactic phenomena, constructed using the Urdu Treebank and diverse text corpora. Human evaluation yielded a 96.10% inter-annotator agreement. Twenty multilingual models were evaluated, with LLaMA-3-70B achieving the highest average accuracy of 94.73%, though performance varied significantly across phenomena. The benchmark highlights both the potential and limitations of current multilingual models in capturing fine-grained syntactic knowledge in low-resource languages like Urdu.

## Method Summary
The evaluation uses zero-shot forced-choice assessment where models must assign lower perplexity to grammatical sentences versus their ungrammatical counterparts. The UrBLiMP dataset contains 5,696 minimal pairs across 10 syntactic phenomena (19 paradigms), created from the Urdu Treebank (7,854 sentences) and an in-house corpus of ~735M tokens. Human native speakers validated 96.10% of pairs with Fleiss' κ = 0.89. Models are evaluated by computing perplexity for each sentence pair, with accuracy calculated as the proportion of pairs where grammatical sentences receive lower perplexity.

## Key Results
- LLaMA-3-70B achieved the highest average accuracy of 94.73% across all 19 paradigms
- Instruction-tuned models (particularly smaller ones) significantly underperformed their pretrained counterparts
- Models struggled notably with long-distance agreement and complex syntactic constructions
- Performance varied dramatically across linguistic phenomena, from near-perfect on Aspect to lower scores on long-distance dependencies

## Why This Works (Mechanism)

### Mechanism 1: Perplexity-Based Grammaticality Discrimination
Multilingual LLMs can be evaluated on grammatical competence by comparing perplexity scores of minimally different sentence pairs. The method exploits a model's learned probability distribution over sequences, where a well-trained model should assign lower perplexity to grammatically acceptable sentences compared to their ungrammatical counterparts. The accuracy is computed as: `Accuracy = (1/N) Σ I[ppl(si_good) < ppl(si_bad)]`, where `I[·]` is an indicator function. This assumes the model's perplexity metric correlates with grammatical acceptability judgments across the linguistic phenomena being tested.

### Mechanism 2: Scaling and Training Type Impact on Syntactic Knowledge
Larger model size generally improves performance on syntactic evaluations, but instruction-tuning can harm syntactic generalization, particularly in smaller models. Larger models have greater capacity to capture complex syntactic patterns from multilingual training data. However, the instruction-tuning process may cause catastrophic forgetting or prioritize task-oriented behavior over fundamental grammatical knowledge. The degradation in performance for instruction-tuned models is a result of the tuning process itself.

### Mechanism 3: Challenge of Long-Distance Dependencies and Morphological Richness
LLMs struggle significantly with syntactic phenomena requiring the resolution of long-distance dependencies and fine-grained morphological agreement in low-resource languages. The self-attention mechanism in Transformers has a harder time maintaining precise syntactic dependencies over long sequences, especially when training data for those specific patterns is scarce. Morphologically rich languages increase the vocabulary and the complexity of the agreement rules that must be learned.

## Foundational Learning

- **Minimal Pairs in Linguistics**
  - Why needed here: The entire UrBLiMP benchmark is built on this concept. Understanding that these are sentences differing by a single feature to isolate a specific grammatical rule is crucial for interpreting results.
  - Quick check question: What is the *single* difference between the sentences "The cat sleeps" and "The cat sleep" that makes one ungrammatical?

- **Perplexity as a Quality Metric**
  - Why needed here: The core evaluation method uses perplexity to score sentences. One must understand that lower perplexity indicates the model assigns a higher probability to that sentence.
  - Quick check question: If a model assigns a perplexity of 50 to sentence A and 100 to sentence B, which sentence does the model find more "likely"?

- **Split Ergativity**
  - Why needed here: A key phenomenon in Urdu tested by UrBLiMP. The learner needs to know that the subject marking changes based on aspect.
  - Quick check question: In a perfective transitive Urdu clause, what marker typically appears on the subject and what does the verb agree with?

## Architecture Onboarding

- **Component Map:** Dataset Generator (Urdu Treebank + corpus) -> Minimal Pair Creation (transformation rules) -> Human Validation (native speakers) -> Evaluation Engine (perplexity computation) -> Analysis Module (accuracy aggregation)

- **Critical Path:**
    1. **Data Curation:** Extract candidate sentences from the Urdu Treebank and in-house corpus using predefined syntactic patterns
    2. **Minimal Pair Creation:** Apply transformation rules to grammatical sentences to generate ungrammatical counterparts
    3. **Human Validation:** Native speakers annotate pairs to confirm grammaticality judgment, achieving 96.10% agreement
    4. **Model Evaluation:** Pass each sentence pair through the LLM to obtain perplexity scores and calculate accuracy

- **Design Tradeoffs:**
    - Dataset Size vs. Quality: UrBLiMP dataset (5,696 pairs) is smaller than English BLiMP (67k pairs), but prioritizes carefully curated and human-verified linguistic phenomena over sheer scale
    - Evaluation Scope: The benchmark focuses on syntactic competence and does not evaluate semantic or pragmatic knowledge

- **Failure Signatures:**
    - Long-Distance Agreement Collapse: Dramatic drop in accuracy on subject-verb agreement when subject and verb are separated by intervening clauses
    - Instruction-Tuning Regression: Pretrained models unexpectedly outperform their instruction-tuned counterparts, especially in smaller model sizes

- **First 3 Experiments:**
    1. **Establish a Baseline:** Run evaluation on LLaMA-3-70B across all 10 linguistic phenomena to establish a strong baseline and verify the evaluation pipeline
    2. **Isolate Long-Distance Effects:** Create a focused subset of the Subject-Verb Agreement paradigm with varying distances between subject and verb. Evaluate both a high-performing and a low-performing model to quantify the performance degradation as distance increases
    3. **Quantify Pretraining vs. Instruction-Tuning Gap:** Take a family of models with both pretrained (-pt) and instruction-tuned (-it) variants. Evaluate all on a subset of phenomena to measure the average performance gap caused by instruction tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms enable LLMs to better capture long-distance syntactic dependencies in morphologically rich, low-resource languages like Urdu?
- Basis in paper: The authors state that "models struggle particularly with long-distance agreement" and that "when the distance between the subject and the verb increases, the ability of the model to correctly predict gender agreement significantly deteriorates."
- Why unresolved: The paper documents the performance drop but does not investigate architectural or training interventions that could mitigate this limitation
- What evidence would resolve it: Ablation studies varying subject-verb distance systematically, combined with attention analysis or probing tasks to identify which model components fail on long-range dependencies

### Open Question 2
- Question: Why does instruction tuning degrade syntactic generalization in smaller multilingual models, and does this effect hold across other low-resource languages?
- Basis in paper: The authors report that "pretrained variants of Gemma-3-1B, 3-4B, and 3-12B significantly outperformed their instruction-tuned counterparts," suggesting "instruction tuning may adversely affect syntactic generalization in smaller-scale models."
- Why unresolved: The paper observes this phenomenon but does not explain whether it stems from catastrophic forgetting, data distribution shifts, or model capacity constraints
- What evidence would resolve it: Controlled experiments comparing instruction tuning with and without syntactic data, plus cross-linguistic replication on other low-resource languages

### Open Question 3
- Question: How would expanding UrBLiMP to include additional phenomena such as reciprocity and question-relative constructions affect model rankings and error patterns?
- Basis in paper: The authors explicitly note the absence of "Reciprocity" and "Question constructions involving relative clauses" as limitations
- Why unresolved: The current benchmark covers only ten phenomena; the impact of broader syntactic coverage on model evaluation remains unknown
- What evidence would resolve it: Extending UrBLiMP with these paradigms and re-evaluating the model suite to determine whether performance gaps widen or narrow

## Limitations
- Dataset size (5,696 pairs) is substantially smaller than English BLiMP (67k pairs), potentially limiting statistical power for some phenomena
- Reliance on perplexity as a proxy for grammaticality assumes models learn probability distributions that align with human grammatical judgments
- In-house Urdu corpus used for template generation is not publicly available, making full replication dependent on future dataset release

## Confidence
- **High Confidence**: The evaluation methodology using minimal pairs and perplexity comparison is sound and has precedent in related work. The finding that LLaMA-3-70B achieves 94.73% average accuracy is well-supported
- **Medium Confidence**: The claim that instruction-tuned models underperform pretrained variants, particularly for smaller models, is based on the Gemma series comparison but may not generalize across all model families
- **Low Confidence**: The exact reasons for long-distance agreement failures remain speculative without deeper probing experiments

## Next Checks
1. **Replicate Pretraining vs. Instruction-Tuning Gap**: Take the Gemma-3-4B model and evaluate both -pt and -it variants on the Ergativity and Dative Object paradigms. Verify if the observed 10-20% performance drop for the instruction-tuned version replicates
2. **Test Distance-Dependent Degradation**: Create a focused evaluation on the Subject-Verb Agreement gender paradigm with controlled subject-verb distances (1, 3, 5, and 7 intervening tokens). Measure accuracy drop across distances to quantify the scaling effect
3. **Cross-Architecture Validation**: Evaluate the same UrBLiMP dataset using a non-Transformer architecture (e.g., Mamba-3B) to determine if long-distance agreement failures are specific to attention mechanisms or represent a more general challenge for language modeling