---
ver: rpa2
title: 'Advancing Symbolic Integration in Large Language Models: Beyond Conventional
  Neurosymbolic AI'
arxiv_id: '2510.21425'
source_url: https://arxiv.org/abs/2510.21425
tags:
- symbolic
- reasoning
- llms
- language
- integration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic review of symbolic integration
  approaches with Large Language Models (LLMs), addressing the need for transparency
  in LLM-generated responses. It introduces a novel taxonomy across four dimensions:
  integration stages (pre-training, training, inference, fine-tuning), coupling mechanisms
  (decoupled vs intertwined), architectural paradigms (LLM-to-symbolic, symbolic-to-LLM,
  hybrid models), and algorithm versus application-level integration.'
---

# Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI

## Quick