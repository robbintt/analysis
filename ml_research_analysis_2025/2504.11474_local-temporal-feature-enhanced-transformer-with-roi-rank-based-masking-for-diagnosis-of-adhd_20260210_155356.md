---
ver: rpa2
title: Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for
  Diagnosis of ADHD
arxiv_id: '2504.11474'
source_url: https://arxiv.org/abs/2504.11474
tags:
- adhd
- attention
- transformer
- diagnosis
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a transformer-based model for ADHD diagnosis
  using rs-fMRI data. The model uses a CNN-based embedding block and two attention
  methods: local temporal attention and ROI-rank based masking.'
---

# Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD

## Quick Facts
- **arXiv ID:** 2504.11474
- **Source URL:** https://arxiv.org/abs/2504.11474
- **Reference count:** 40
- **Primary result:** Achieves 77.78% accuracy, 76.60% specificity, 79.22% sensitivity, and 79.30% AUC on ADHD diagnosis using rs-fMRI data.

## Executive Summary
This paper proposes a transformer-based model for ADHD diagnosis using resting-state fMRI data. The model introduces two key modifications to the standard transformer architecture: local temporal attention with window masking and ROI-rank based masking. Local temporal attention enables the model to learn local BOLD signal features by constraining attention to neighboring time steps, while ROI-rank based masking distinguishes important brain regions by selecting only the top-scoring regions based on attention scores. The model was evaluated on the ADHD-200 dataset, demonstrating significant improvements over baseline transformers.

## Method Summary
The model is an Encoder-Decoder Transformer that processes both temporal and spatial features from rs-fMRI data. The Encoder uses local temporal attention with a window mask to learn local BOLD signal patterns, while the Decoder employs ROI-rank based masking to focus on the most diagnostically relevant brain regions. A CNN-based embedding block replaces the standard linear projection to capture more complex temporal features from the raw ROI time-series. The model was trained on the ADHD-200 dataset using binary classification (ADHD vs. Healthy Control) with BCE loss, Adam optimizer, and extensive data augmentation through random cropping.

## Key Results
- Achieves 77.78% accuracy on ADHD diagnosis using rs-fMRI data
- Demonstrates that local temporal attention improves performance over standard attention
- Shows that using only 60 top-scoring ROIs outperforms using all 190 ROIs (2% higher performance)
- Local temporal attention with window size 20 consistently improves diagnostic performance across different window sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local Temporal Attention improves feature extraction from BOLD signals by first learning short-range dynamics.
- **Mechanism:** Applies a window mask to the self-attention score matrix, constraining each time step to attend only to its local neighbors. This forces initial encoder layers to learn features from short-term BOLD signal patterns before integrating them into global context in deeper layers.
- **Core assumption:** Critical biomarkers for ADHD are encoded within local temporal patterns of the BOLD signal, not just global long-range trends.
- **Evidence anchors:**
  - [abstract]: "local temporal attention enables to learn local BOLD signal features with only simple window masking."
  - [section 4.6]: "...diagnostic performance is always improved regardless of the window size than before... This shows that for effective temporal feature learning, it is necessary to gradually increase the range from local features... to global features as the layer deepens."
  - [corpus]: The paper "B-TGAT" uses a "Bi-directional Temporal Graph Attention Transformer" for spatiotemporal data, which supports the utility of specialized attention for temporal dependencies.
- **Break condition:** Performance drops or fails to improve over a standard full-attention transformer, suggesting that local constraints are either irrelevant or detrimental to learning.

### Mechanism 2
- **Claim:** ROI-rank Based Masking enhances diagnostic accuracy by selectively focusing on a sparse set of the most relevant brain regions.
- **Mechanism:** Ranks ROIs based on their attention scores and creates a binary mask that zeroes out attention weights for all but the top-k scoring ROIs. This forces the model to learn relationships only between the most salient brain regions, acting as a dynamic, learned feature selector.
- **Core assumption:** Only a small subset of brain regions (ROIs) hold strong diagnostic value for ADHD, and their connectivity patterns are key.
- **Evidence anchors:**
  - [abstract]: "ROI-rank based masking can distinguish ROIs with high correlation in ROI relationships based on attention scores, thereby providing a more specific biomarker for ADHD diagnosis."
  - [section 4.7]: "using only the 60 top-scoring ROIs... we achieved the best performance... It is significantly interesting result. Because, despite using fewer ROIs, it is about 2% higher performance than the case of using 190 ROIs".
  - [corpus]: Corpus evidence for this specific ROI-rank masking technique is weak or missing, making it a novel contribution without direct external corroboration in the provided neighbors.
- **Break condition:** The model's performance decreases monotonically as more ROIs are masked (smaller k), indicating that information is distributed across many regions rather than concentrated in a few.

### Mechanism 3
- **Claim:** A CNN-based embedding block creates more expressive representations than a linear projection for fMRI data.
- **Mechanism:** Replacing the standard linear embedding layer with a 1D-CNN module allows for hierarchical feature extraction from the raw ROI time-series. This can capture more complex temporal and channel-wise features before they enter the transformer blocks.
- **Core assumption:** A simple linear projection is insufficient to capture the complexity and non-linearity inherent in BOLD signals.
- **Evidence anchors:**
  - [abstract]: "design a CNN-based embedding block to obtain more expressive embedding features in brain region attention."
  - [section 4.5]: "The CNN-based embedding module was designed based on the CNN structure of FCNet... it was shown that it is important to train the channel features in deep layers because the complexity of the features contained in the fMRI signal is high."
  - [corpus]: The paper "ADHDeepNet" uses "Temporal-Spatial Processing... in Raw EEG Signals," which provides indirect support for using specialized initial processing layers (like CNNs) for time-series neural data.
- **Break condition:** A model using standard linear embeddings outperforms the CNN-based variant, suggesting the added complexity is either unnecessary or leads to overfitting.

## Foundational Learning

- **Concept:** Self-Attention Mechanism in Transformers
  - **Why needed here:** The paper proposes a model "based on the transformer," modifying its core attention mechanism. Understanding the standard Query, Key, Value (Q, K, V) formulation is essential to grasp how "local temporal" and "ROI-rank" masking modify it.
  - **Quick check question:** Can you explain the core computation of self-attention using Query (Q), Key (K), and Value (V) vectors?

- **Concept:** BOLD Signal and Temporal Features
  - **Why needed here:** The input data is resting-state fMRI (rs-fMRI), and the model's first proposed modification is designed to learn "local BOLD signal features." You must understand that the BOLD signal is a time-series representing brain activity.
  - **Quick check question:** What does a BOLD signal from an fMRI scan represent, and what kind of temporal patterns might be diagnostically useful?

- **Concept:** Regions of Interest (ROI)
  - **Why needed here:** The paper's second major modification is "ROI-rank based masking." This assumes the brain is parcellated into distinct regions (ROIs), and the model's spatial input is a set of these ROIs, not raw voxels.
  - **Quick check question:** What is a Region of Interest (ROI) in the context of fMRI analysis, and why is it used?

## Architecture Onboarding

- **Component map:** Raw fMRI -> Preprocessing to ROI time-series -> CNN Embedding -> Encoder (Local Attention) -> Decoder (Co-Attention + Rank Masking) -> Classifier
- **Critical path:** `Raw fMRI -> Preprocessing to ROI time-series -> CNN Embedding -> Encoder (Local Attention) -> Decoder (Co-Attention + Rank Masking) -> Classifier`
- **Design tradeoffs:** The primary tradeoff is in the masking parameters. A small local window size or a low number of top-k ROIs (`rank_k`) focuses the model but risks missing long-range or distributed diagnostic information. Conversely, large values approach the baseline transformer, losing the benefits of sparsity.
- **Failure signatures:** A common failure mode is attention collapse, where the model's attention becomes uniform or focuses on a single time point/ROI. Another is overfitting, given the relatively small dataset (939 individuals) and model complexity.
- **First 3 experiments:**
  1. **Baseline Test:** Implement the model with standard linear embeddings and full attention (no masking) to establish a performance baseline.
  2. **Ablation Study:** Test each proposed modification (CNN embedding, Local Temporal Attention, ROI-rank Masking) individually and in combination to isolate their performance contributions.
  3. **Hyperparameter Scan:** Systematically vary the local attention window size and the number of selected ROIs (`rank_k`) to find the optimal balance between local and global, and sparse and dense attention.

## Open Questions the Paper Calls Out

None

## Limitations

- The model's performance improvements over baseline transformers are demonstrated internally but lack comparison against state-of-the-art ADHD diagnosis methods from literature.
- The ROI-rank masking technique, while showing promising results, lacks external validation and biological interpretation of the selected regions.
- The model requires extensive hyperparameter tuning, particularly for the masking parameters, which may limit its practical applicability.
- The dataset size (939 individuals) may be insufficient for a complex transformer architecture, raising concerns about overfitting.

## Confidence

- Local Temporal Attention effectiveness: **Medium**
- ROI-rank Based Masking effectiveness: **Medium**
- Overall diagnostic performance (77.78% accuracy): **Medium**

## Next Checks

1. Compare the model's performance against published ADHD diagnosis benchmarks using the same ADHD-200 dataset to establish relative effectiveness.
2. Validate the top-60 ROIs identified by the model against established ADHD neuroimaging literature to assess biological plausibility.
3. Test the model on an independent ADHD dataset (if available) to evaluate generalization beyond the ADHD-200 training set.