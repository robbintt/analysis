---
ver: rpa2
title: 'multivariateGPT: a decoder-only transformer for multivariate categorical and
  numeric data'
arxiv_id: '2505.21680'
source_url: https://arxiv.org/abs/2505.21680
tags:
- data
- discrete
- numeric
- time
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces multivariateGPT, a decoder-only transformer
  architecture for modeling sequences of mixed categorical and numeric data. The core
  innovation is an autoregressive decomposition of the joint distribution of multivariate
  time-series that captures timing information, combined with an embedding scheme
  and likelihood-based loss function that extends next token prediction to estimate
  both class and value for numeric data.
---

# multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data

## Quick Facts
- **arXiv ID**: 2505.21680
- **Source URL**: https://arxiv.org/abs/2505.21680
- **Reference count**: 13
- **Primary result**: Demonstrates autoregressive transformer for mixed categorical-numeric multivariate time-series outperforms discrete-token baselines on real-world datasets

## Executive Summary
This paper introduces multivariateGPT, a decoder-only transformer architecture that models sequences of mixed categorical and numeric data by treating timing information as a first-class token in an autoregressive framework. The core innovation is an embedding scheme that maps numeric values into a shared vector space using class-specific functions, combined with a dual-head output layer that predicts both class probabilities and Gaussian distributions for numeric values. Experiments on sepsis data, ECG reconstruction, and ICU datasets show superior performance compared to discrete-token transformers and neural ODE methods, particularly in generalization to unseen data patterns and uncertainty quantification.

## Method Summary
The model treats multivariate time-series as flattened autoregressive sequences, decomposing the joint distribution into sequential conditionals that include timing information. Each {class, value} tuple is embedded as E_i = E_{c_i} + f_{c_i}(v_i), where categorical values use only class embeddings while numeric values use an additive class embedding plus value mapping. A two-head output layer predicts class probabilities (softmax) and Gaussian parameters (μ, σ) for numeric values, with loss computed as the sum of cross-entropy and Gaussian negative log-likelihood. The architecture uses a standard decoder-only transformer backbone with nanoGPT implementation, trained autoregressively to predict the next token in the sequence.

## Key Results
- On eICU sepsis dataset, achieves 40-60% lower MSE than TFM-ODE for heart rate and blood pressure prediction
- Uniquely predicts both measurement values and timing with 82-99% error reduction compared to baselines
- On Physionet ICU dataset with 36 measurement classes, shows superior class prediction accuracy and value estimation
- Demonstrates superior generalization on synthetic physical systems compared to discrete approaches, learning continuous representations that enable accurate extrapolation

## Why This Works (Mechanism)

### Mechanism 1: Joint Distribution Decomposition via Autoregressive Flattening
Treating the joint distribution of multivariate time-series as a flattened autoregressive sequence enables the model to capture both measurement timing information and inter-variable dependencies within a single decoder-only transformer. The paper decomposes the joint distribution P({X, τ}) into a product of conditionals via the chain rule of probability, with wait time τ treated as its own class-value tuple. This allows the model to learn when measurements occur rather than treating time as a fixed index.

### Mechanism 2: Hybrid Embedding with Class-Specific Numeric Mapping
Using a shared class embedding plus a class-specific function to map numeric values into the embedding space preserves metric relationships between nearby values while maintaining modality-agnostic token processing. Each tuple {c_i, v_i} is embedded as E_i = E_{c_i} + f_{c_i}(v_i), meaning class identity is preserved via E_{c_i} while numeric magnitude modulates the embedding. Unlike discrete tokenization, values 21 and 23 share representational structure with 22, enabling interpolation and extrapolation.

### Mechanism 3: Dual-Head Likelihood Loss for Class and Value
Factoring the joint probability P(c_i, v_i) = P(c_i)P(v_i|c_i) with separate output heads enables simultaneous classification and regression within a single loss function, preserving uncertainty quantification. The final transformer layer projects to two heads: a class head using softmax for P(c_i), and a value head outputting μ and σ for a Gaussian distribution over P(v_i|c_i). This allows the model to express uncertainty via σ and naturally handles categorical tokens where P(v|c)=1.

## Foundational Learning

- **Concept: Autoregressive Language Modeling and the Chain Rule of Probability**
  - Why needed here: The entire architecture builds on decomposing joint distributions into sequential conditionals, the same principle behind GPT-style language models. Without this foundation, the equations in Section 2.1 will be opaque.
  - Quick check question: Can you explain why P(x_1, x_2, x_3) = P(x_1)P(x_2|x_1)P(x_3|x_1, x_2) is the basis for next-token prediction?

- **Concept: Embedding Spaces and Vector Arithmetic**
  - Why needed here: The embedding scheme (Equation 5) adds a class embedding to a value embedding. Understanding how semantic information is encoded as vectors and how addition combines information is essential to see why this enables interpolation.
  - Quick check question: If E_HR is the embedding for the heart rate class and f_HR(115) encodes value 115, what does the model learn if f_HR(116) ≈ f_HR(115) + δ for small δ?

- **Concept: Probabilistic Forecasting and Uncertainty Quantification (Gaussian Likelihood)**
  - Why needed here: The model outputs μ and σ rather than point estimates. Understanding what it means for a model to be "calibrated" (coverage fractions, QQ plots) is critical for interpreting the experimental claims.
  - Quick check question: If a model predicts μ = 80, σ = 10 for heart rate, what does it mean if the 95% confidence interval [60.4, 99.6] only contains the true value 70% of the time over a test set?

## Architecture Onboarding

- **Component map**: Raw time-series -> (class, value) tuples with τ -> Embedding layer (E_i = E_{c_i} + f_{c_i}(v_i)) -> Transformer backbone (causal masking) -> Dual-head output (class logits + μ, σ) -> Joint loss (cross-entropy + Gaussian NLL)

- **Critical path**:
  1. Data preprocessing: Convert raw time-series to {c_i, v_i} tuples; normalize numeric values; convert time deltas to (time, Δt) tokens
  2. Embedding lookup: Retrieve E_{c_i}, compute f_{c_i}(v_i) via a feed-forward layer, sum them
  3. Forward pass through transformer blocks with causal masking
  4. Dual-head projection and loss computation
  5. Inference: Autoregressive sampling; for numeric tokens, sample from predicted Gaussian or use μ as point estimate

- **Design tradeoffs**:
  - Gaussian vs. other distributions: Simpler and well-calibrated for tested data, but may fail for bounded or count data (Limitations)
  - Fixed vs. learned variance: Ablation shows learned variance is critical for generalization; fixed variance degrades precision
  - Single model vs. multi-modal separate models: Unified architecture is simpler but may sacrifice specialized numeric representations

- **Failure signatures**:
  - Poor generalization to out-of-distribution numeric ranges: Continuous representation should help, but extreme values may fall outside learned f_c mappings
  - Miscalibrated uncertainty: If QQ plots deviate from linearity at tails, the Gaussian assumption is violated
  - Class confusion for rare categories: With 36+ classes and sparse sampling, low-frequency classes may be mispredicted

- **First 3 experiments**:
  1. Trajectory reconstruction with harmonic oscillator data: Train on damped oscillator trajectories, seed with first 5 points, evaluate reconstruction error and generalization to unseen trajectories. Compare to discrete-token baseline with 10/50/100 bins.
  2. eICU value prediction: Train on sepsis patient vitals (HR, MAP), evaluate MSE for predicting remaining measurements from 3/6/9/12 hour seeds. Compare to TFM-ODE and discrete models.
  3. Calibration check: On eICU or Physionet data, generate QQ plots of standardized residuals and compute 95% CI coverage fractions. Verify values are near 0.95 for multivariateGPT.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does multivariateGPT exhibit scaling behavior with model and dataset size comparable to autoregressive language and image models? The paper hopes to assess this but all experiments use relatively small models (0.85M–33.2M parameters); no scaling analysis is performed.

- **Open Question 2**: Can alternative likelihood distributions (e.g., Poisson, exponential, ordinal) improve modeling of time deltas, counts, and bounded values compared to the current Gaussian parameterization? The paper identifies this as an important future direction but only Gaussian distributions are implemented.

- **Open Question 3**: Can the single-architecture approach scale to higher-dimensional modalities such as image patches or audio frames? The paper envisions future work to incorporate additional data modalities but the embedding function maps scalars to vectors; extending to high-dimensional values without separate encoders remains untested.

## Limitations
- **Gaussian assumption fragility**: The dual-head output assumes numeric values follow Gaussian distributions conditional on class, which breaks down for bounded, count, or highly skewed variables, limiting applicability to datasets where numeric variables have approximately normal residuals.

- **Embedding function specification ambiguity**: The paper states f_ci(v_i) is implemented as "a single feed-forward layer" but does not specify activation function, output dimensionality, or whether it's shared across numeric classes, which is critical for reproduction.

- **Timing prediction mechanism details**: The exact formulation for predicting time deltas (τ) is not fully specified, making it unclear whether timing prediction emerges from the general class-value framework or requires special handling.

## Confidence

**High confidence**: Claims about the autoregressive decomposition mechanism and its ability to capture timing information are well-supported by mathematical formulation (Equations 1-4) and experimental demonstration on eICU timing prediction (82-99% error reduction).

**Medium confidence**: Claims about hybrid embedding preserving metric relationships and enabling interpolation are supported by harmonic oscillator generalization results (Figure 2) and theoretical framework, but exact mechanism depends on unspecified architectural details.

**Medium confidence**: Claims about dual-head likelihood loss providing calibrated uncertainty estimates are supported by QQ plots and coverage fractions near 0.95, but Gaussian assumption may fail on real-world datasets with non-normal residuals.

## Next Checks

1. **Distribution validation**: Apply multivariateGPT to a dataset with bounded numeric variables (e.g., percentages, counts) and evaluate calibration via QQ plots and coverage fractions. Compare against alternative likelihood formulations (e.g., Beta for percentages, Poisson for counts) to assess whether Gaussian assumption is a fundamental limitation.

2. **Embedding function ablation**: Systematically vary the architecture of f_ci(v_i) (activation functions, output dimensions, learned vs fixed) and measure impact on generalization performance using the synthetic oscillator task. Determine whether the current unspecified implementation is optimal or whether simpler alternatives suffice.

3. **Timing prediction mechanism isolation**: Train a variant of multivariateGPT where timing tokens are removed from the input/output sequence, and compare timing prediction performance against the full model. This would validate whether the autoregressive timing prediction is truly learned or simply a byproduct of the joint distribution decomposition.