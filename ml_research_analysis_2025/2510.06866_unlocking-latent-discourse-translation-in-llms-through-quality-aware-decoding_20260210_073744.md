---
ver: rpa2
title: Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding
arxiv_id: '2510.06866'
source_url: https://arxiv.org/abs/2510.06866
tags:
- translation
- discourse
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs struggle with discourse phenomena such as pronoun resolution
  and lexical cohesion in document-level translation. The authors show that quality-aware
  decoding (QAD) can better exploit latent discourse knowledge in LLMs, significantly
  improving both overall translation quality and handling of discourse phenomena compared
  to greedy decoding.
---

# Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding

## Quick Facts
- arXiv ID: 2510.06866
- Source URL: https://arxiv.org/abs/2510.06866
- Reference count: 39
- Key outcome: Quality-aware decoding significantly improves LLM document-level translation by better leveraging latent discourse knowledge

## Executive Summary
This paper demonstrates that large language models struggle with discourse phenomena in document-level translation, particularly pronoun resolution and lexical cohesion. The authors propose using quality-aware decoding (QAD) to better exploit latent discourse knowledge within these models. By combining translation-finetuned LLMs with QAD using minimum Bayes risk (MBR) decoding and BLEU as the utility function, they achieve substantial improvements over greedy decoding baselines. The approach yields higher scores across multiple automatic metrics including BLEU, COMET, docCOMET, and discourse-specific evaluations, with human assessment confirming that QAD produces semantically richer translations preferred by annotators.

## Method Summary
The authors investigate the application of quality-aware decoding (QAD) to improve document-level translation in LLMs. They start with a base translation-finetuned LLM and apply QAD during inference, which uses a quality estimation model to guide the search process. The quality-aware decoding framework explores multiple translation hypotheses and selects the final output based on estimated quality scores rather than just likelihood. The approach is tested on English-to-German and English-to-French translation tasks, with evaluation using standard metrics (BLEU, COMET) as well as discourse-specific metrics for pronoun resolution and lexical cohesion.

## Key Results
- QAD with MBR decoding and BLEU utility significantly outperforms greedy decoding on automatic metrics (BLEU, COMET, docCOMET)
- Translation-finetuned LLMs with QAD achieve strong performance compared to sentence-level baselines
- QAD specifically improves handling of discourse phenomena, showing better pronoun resolution and lexical cohesion
- Human evaluation confirms annotator preference for QAD-generated translations as semantically richer

## Why This Works (Mechanism)
The paper demonstrates that LLMs contain latent discourse knowledge that can be better exploited through quality-aware decoding. During greedy decoding, the model typically selects the most probable token at each step without considering global quality. QAD instead generates multiple hypotheses and uses quality estimation to select the best overall translation, allowing the model to explore solutions that may have lower local probability but better global coherence. This is particularly important for discourse phenomena like pronoun resolution and lexical cohesion, where local decisions can have long-range dependencies that greedy decoding fails to optimize.

## Foundational Learning

**Document-level translation** - Translating entire documents rather than isolated sentences, requiring consideration of discourse coherence across sentence boundaries. *Why needed:* Many translation challenges require context beyond single sentences for accurate rendering.

**Discourse phenomena** - Linguistic features like pronoun resolution and lexical cohesion that depend on document context. *Quick check:* Test if pronouns in translation can be resolved correctly based on preceding context.

**Quality-aware decoding** - Decoding framework that uses quality estimation to guide search for better translations. *Quick check:* Verify that multiple hypotheses are generated and scored using quality estimates.

**Minimum Bayes Risk (MBR) decoding** - A decoding strategy that selects translations based on expected utility rather than maximum likelihood. *Quick check:* Confirm that BLEU or other utility functions are used to score translation hypotheses.

## Architecture Onboarding

**Component map:** Translation LLM -> Quality Estimation Model -> Hypothesis Scoring -> MBR Decoding -> Final Translation

**Critical path:** The core workflow involves generating multiple translation hypotheses from the LLM, scoring each using the quality estimation model, and applying MBR decoding with BLEU utility to select the final output.

**Design tradeoffs:** The approach trades increased computation (generating and scoring multiple hypotheses) for improved translation quality, particularly for discourse phenomena that greedy decoding misses.

**Failure signatures:** If quality estimation is poor or biased, QAD may select suboptimal translations. The approach may also fail when discourse context is insufficient or when quality estimation cannot capture nuanced discourse requirements.

**First experiments to run:**
1. Compare greedy decoding vs QAD on a small document set with manual quality assessment
2. Evaluate discourse-specific metrics (pronoun resolution, lexical cohesion) to isolate improvements
3. Test different quality estimation models to assess impact on final translation quality

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The study focuses on English-centric language pairs (en→de/fr), limiting generalizability to truly multilingual scenarios
- Requires document-level parallel data for finetuning, which may not be available for all language pairs
- Effectiveness depends on quality of the underlying quality estimation model
- Human evaluation covers only 100 segments per language pair, potentially missing broader variability

## Confidence
- High confidence: QAD with MBR decoding improves overall translation quality metrics compared to greedy decoding
- Medium confidence: QAD specifically improves discourse phenomena handling as measured by pronoun resolution and lexical cohesion metrics
- Medium confidence: Translation-finetuned LLMs outperform strong sentence-level baselines in document-level translation tasks

## Next Checks
1. Test the approach on additional language pairs including distant languages (e.g., en→ja, en→zh) to assess cross-linguistic generalizability
2. Conduct larger-scale human evaluations with diverse annotator pools to validate preference findings across different discourse types
3. Investigate the impact of different quality estimation models on QAD performance to understand robustness to estimation quality variations