---
ver: rpa2
title: 'SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All
  Training'
arxiv_id: '2509.16833'
source_url: https://arxiv.org/abs/2509.16833
tags:
- sub-nets
- training
- layer
- output
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Once-for-All (OFA) training enables a single super-net to generate
  multiple sub-nets for diverse deployment scenarios, but excessive parameter sharing
  in the backbone limits representational capacity and degrades performance. We propose
  SOLAR (Switchable Output Layer for Accuracy and Robustness in Once-for-All Training),
  which assigns each sub-net a separate classification head while preserving the shared
  backbone.
---

# SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training

## Quick Facts
- arXiv ID: 2509.16833
- Source URL: https://arxiv.org/abs/2509.16833
- Reference count: 40
- Primary result: SOLAR improves sub-net accuracy up to 1.26% (SVHN), 4.71% (CIFAR-10), 1.67% (STL-10), and 1.76% (CIFAR-100) over OATS, and up to 2.93% (TinyImageNet) over SNNs

## Executive Summary
Once-for-All (OFA) training enables a single super-net to generate multiple sub-nets for diverse deployment scenarios, but excessive parameter sharing in the backbone limits representational capacity and degrades performance. We propose SOLAR (Switchable Output Layer for Accuracy and Robustness in Once-for-All Training), which assigns each sub-net a separate classification head while preserving the shared backbone. This decouples the logit learning process across sub-nets, reducing representational interference and improving optimization. Evaluated on five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using four backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and MobileNetV2) for two OFA frameworks (OATS and SNNs), SOLAR consistently outperforms baseline methods.

## Method Summary
SOLAR introduces a Switchable Output Layer (SOL) that assigns a unique classification head to each sub-net width in an OFA framework, while maintaining a shared convolutional backbone. During training, each sub-net (defined by a width multiplier) activates its corresponding head and SBN layers, with gradients flowing only through the active path. This design reduces representational interference at the output layer while preserving computational efficiency during inference. The method is evaluated with two OFA frameworks: OATS (adversarial training with SDBN) and SNNs (standard training with SBN), using SGD with cosine annealing across multiple datasets and architectures.

## Key Results
- SOLAR improves sub-net accuracy up to 1.26% (SVHN), 4.71% (CIFAR-10), 1.67% (STL-10), and 1.76% (CIFAR-100) over OATS baseline
- SOLAR improves TinyImageNet accuracy by up to 2.93% (ResNet-34), 2.34% (WideResNet-16-8), and 1.35% (MobileNetV2) over SNNs
- Robustness gains of up to 9.01% (SVHN), 7.71% (CIFAR-10), 2.72% (STL-10), and 1.26% (CIFAR-100) against PGD-7 attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling the logit learning process reduces representational interference between sub-networks of varying capacities.
- **Mechanism:** By assigning a separate classification head (the Switchable Output Layer, SOL) to each sub-net, the system prevents the gradients of one sub-net from overwriting or conflicting with the feature-to-logit mappings of another. During backpropagation, only the active head and the shared backbone are updated, ensuring that the optimization of a smaller, narrow sub-net does not degrade the decision boundary of the full-width super-net.
- **Core assumption:** The paper assumes that performance degradation in Once-for-All (OFA) training stems significantly from "logit interference" at the final layer, rather than solely from feature extraction issues in the backbone.
- **Evidence anchors:**
  - [abstract] "By decoupling the logit learning process across sub-nets, the Switchable Output Layer (SOL) reduces representational interference..."
  - [section 4.2] "...SOL introduces a unique head $C_{\alpha_k}$ for each sub-net... This dynamic switching ensures that each sub-net learns independently, avoiding interference from incompatible gradients at the output layer."
  - [corpus] Corpus evidence is missing or weak; related papers focus on hardware search or unrelated "Solar" topics, not specifically on logit decoupling in OFA.
- **Break condition:** If the primary bottleneck in a specific architecture is strictly in the feature extraction layers (backbone) rather than the classifier head, SOL may yield diminishing returns.

### Mechanism 2
- **Claim:** Feature distributions from sub-nets of different widths are statistically incompatible when forced through a single normalization and classification endpoint.
- **Mechanism:** The paper demonstrates that different sub-nets (e.g., width $1/8$ vs $8/8$) produce diverging feature statistics (mean/variance). While Switchable Batch Normalization (SBN) handles intermediate layers, a shared output layer acts as a bottleneck, forcing these distinct distributions into a single representation space. SOL maintains this separation through to the final prediction.
- **Core assumption:** Assumes that the feature statistics of sub-nets are sufficiently distinct that a single linear classifier cannot form optimal decision boundaries for all of them simultaneously.
- **Evidence anchors:**
  - [section 4.1] "The figure [3] reveals significant variance across sub-nets... This indicates that... the shared output head remains a bottleneck at the end of the pipeline."
  - [abstract] "...excessive parameter sharing in the backbone limits representational capacity... SOLAR... assigns each sub-net a separate classification head."
  - [corpus] N/A (No direct corroboration found in provided corpus).
- **Break condition:** If sub-nets share very similar feature statistics (e.g., width multipliers are very close, like 0.9 and 1.0), the interference SOL is designed to fix may be negligible.

### Mechanism 3
- **Claim:** Inference efficiency (FLOPs) is preserved despite increased storage parameters because only the specific head corresponding to the active sub-net width is executed.
- **Mechanism:** SOLAR increases the total storage size of the super-net (as it stores $K$ heads instead of 1), but the computational cost during a forward pass remains identical to the baseline. The operation is a "switch" rather than a "slim" at the output layer.
- **Core assumption:** Memory storage constraints are less critical than runtime latency/FLOPs constraints in the target deployment scenarios.
- **Evidence anchors:**
  - [section 5.4] "FLOPs during a forward pass for a SOL based sub-net... are identical to the corresponding baseline sub-nets... SOL adds no training overhead."
  - [abstract] "...without altering the shared backbone."
  - [corpus] N/A.
- **Break condition:** In memory-constrained edge devices where storage weight is the primary bottleneck (not FLOPs), the added parameter count (up to ~12% increase in Table 4) might be undesirable.

## Foundational Learning

- **Concept: Once-for-All (OFA) Training**
  - **Why needed here:** SOLAR is a modification specifically for OFA frameworks. You must understand that OFA trains a single "super-net" containing many "sub-nets" (different widths) to avoid retraining for every hardware constraint.
  - **Quick check question:** How does SOLAR alter the standard OFA training pipeline regarding the final classification layer?

- **Concept: Representational Interference**
  - **Why needed here:** The core problem SOLAR solves. This occurs when optimizing for one sub-net (e.g., a small one) negatively impacts the performance of another (e.g., a large one) because they share weights.
  - **Quick check question:** Why does sharing an output layer cause more interference for sub-nets with vastly different widths compared to those with similar widths?

- **Concept: Switchable Batch Normalization (SBN)**
  - **Why needed here:** To understand the context. Previous work (SNNs/OATS) already solved interference in *intermediate* layers using SBN. SOLAR addresses the *remaining* bottleneck at the output layer.
  - **Quick check question:** If SBN already separates feature statistics in the backbone, why does the shared output layer remain a bottleneck?

## Architecture Onboarding

- **Component map:** Input -> Shared Backbone -> SBN/SDBN layers -> SOL heads -> Output
- **Critical path:**
  1. Sample width multiplier αk
  2. Activate corresponding channels in Backbone and specific Head Cαk
  3. Forward pass computes logits zαk
  4. Loss computed (Hybrid or CE)
  5. Gradients update shared Backbone weights and *only* the active Head Cαk

- **Design tradeoffs:**
  - **Storage vs. Accuracy:** You trade increased disk space (more parameters from multiple heads) for higher accuracy and robustness
  - **Complexity:** Minimal code change (adding a list of heads vs one head), but requires careful management of the "switching" logic during the training loop

- **Failure signatures:**
  - **OOM (Out of Memory) on Storage:** Using an excessive number of sub-nets (e.g., >64) on small devices might trigger storage limits (see Table 4 for parameter inflation)
  - **Stagnation:** If performance doesn't improve, verify that the heads are actually being switched correctly (i.e., distinct heads are learning distinct biases) and not effectively averaging out

- **First 3 experiments:**
  1. **Sanity Check (SNN Baseline):** Train a standard Slimmable Neural Network (SNN) on CIFAR-10 vs. SNN-SOL. Compare accuracy of the smallest sub-net (width 0.25) to verify the "gain" claimed in Table 1
  2. **Visualize Interference (Reproduction):** Reproduce Figure 3. Plot the running means/variances of the final BN layer with and without SOL to visually confirm that shared heads force divergent features into one cluster, while SOL keeps them separate
  3. **Robustness Stress Test:** Apply PGD-7 attacks (as per Section 5.2) on an OATS-SOL model vs. standard OATS. specifically checking the smallest sub-nets to see if the robustness gains (up to 9% cited) hold

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does SOLAR maintain its efficacy and efficiency when applied to large-scale visual recognition tasks such as ImageNet-1K?
- **Basis in paper:** [explicit] The authors state they "intend to... conduct large-scale evaluations on the ImageNet-1K dataset" in the Conclusion.
- **Why unresolved:** The current study is restricted to smaller benchmark datasets (e.g., CIFAR, TinyImageNet), leaving the method's scalability and performance on high-resolution, diverse data unproven.
- **What evidence would resolve it:** Experimental results on ImageNet-1K comparing SOLAR-enhanced OFA frameworks against baselines in terms of accuracy, robustness, and storage overhead.

### Open Question 2
- **Question:** Can the Switchable Output Layer (SOL) be effectively generalized to other Once-for-All frameworks beyond SNNs and OATS?
- **Basis in paper:** [explicit] The authors list extending SOL to other OFA frameworks (e.g., universally slimmable self-supervised learning) as a future direction.
- **Why unresolved:** The methodology is currently validated only on SNNs and OATS; its interaction with other training paradigms, such as those involving self-supervised learning or different architectural search spaces, remains unknown.
- **What evidence would resolve it:** Successful integration and performance evaluation of SOL within distinct OFA architectures, demonstrating reduced logit interference without destabilizing the specific training objectives of those frameworks.

### Open Question 3
- **Question:** How does replacing Batch Normalization with Layer Normalization interact with the Switchable Output Layer to mitigate representational interference?
- **Basis in paper:** [explicit] The authors explicitly state they "intend to study the impact of layer normalization on reducing representational interference across the sub-nets."
- **Why unresolved:** The paper currently relies on Switchable Batch Normalization (SBN) to handle feature statistics, but it is unclear if Layer Normalization could offer a more integrated or efficient solution to the identified bottleneck.
- **What evidence would resolve it:** An ablation study comparing SBN against Layer Normalization in SOLAR-equipped models, specifically analyzing running statistics distributions and final sub-net performance.

### Open Question 4
- **Question:** Does the parameter storage overhead of SOL become prohibitive when scaling to the massive number of sub-nets typical in search-based OFA frameworks?
- **Basis in paper:** [inferred] The paper demonstrates parameter increases of up to 12% for 64 sub-nets (Table 4) and identifies the "shared output layer" bottleneck in contexts where OFA supports vast numbers of sub-nets.
- **Why unresolved:** While effective for width-slimmable networks (limited sub-nets), SOL assigns a distinct head per sub-net, which may impose significant storage constraints in "search" scenarios where millions of architectures are possible.
- **What evidence would resolve it:** Analysis of storage costs and performance when SOL is adapted for OFA networks with expansive search spaces (e.g., varying depth and kernel size in addition to width).

## Limitations

- **Theoretical foundation gap:** The core claim that logit interference is the primary bottleneck in OFA training remains an assumption without rigorous mathematical proof
- **Storage overhead quantification:** While storage overhead is acknowledged (up to ~12% parameter increase), the paper doesn't quantify the impact on real-world memory-constrained devices or provide benchmarks on actual edge hardware
- **Attack generalization uncertainty:** Robustness improvements are measured only against PGD-7 attacks with a fixed ε=8/255; generalizability to other attack types or threat models is unknown

## Confidence

- **High confidence**: SOLAR's architectural implementation (separate heads per sub-net) is clearly described and the empirical improvements in accuracy and robustness are well-documented across multiple benchmarks
- **Medium confidence**: The mechanism explanation (logit interference reduction) is plausible and supported by feature distribution analysis, but lacks rigorous theoretical justification or ablation studies isolating the exact contribution of each component
- **Medium confidence**: The FLOPs preservation claim is mathematically sound (only one head executes per inference), but the practical impact on real systems depends on deployment constraints not explored in the paper

## Next Checks

1. **Ablation study on interference sources**: Train an OFA model with shared output layer but modified backbone normalization (stronger than SBN) to test whether backbone improvements alone can match SOLAR's gains, isolating the true source of interference
2. **Memory-constrained deployment benchmark**: Measure inference latency and peak memory usage on representative edge devices (e.g., Raspberry Pi, Jetson Nano) to quantify the real-world tradeoff between SOLAR's accuracy gains and its storage overhead
3. **Attack diversity validation**: Test SOLAR-trained models against multiple attack types (AutoAttack, CW, transfer attacks) and varying ε budgets to verify that robustness improvements generalize beyond the specific PGD-7 setup used in the paper