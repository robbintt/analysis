---
ver: rpa2
title: Actor Critic with Experience Replay-based automatic treatment planning for
  prostate cancer intensity modulated radiotherapy
arxiv_id: '2502.00346'
source_url: https://arxiv.org/abs/2502.00346
tags:
- treatment
- planning
- acer
- plan
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops an ACER-based DRL agent for automatic treatment\
  \ planning in prostate IMRT, addressing challenges of data dependency, universal\
  \ applicability, and robustness in existing AI approaches. The agent tunes treatment\
  \ planning parameters using DVH inputs and achieves 93.09% perfect plan scores (mean\
  \ 8.93\xB10.27) across 300+ test cases from three independent datasets, despite\
  \ training on only one patient case."
---

# Actor Critic with Experience Replay-based automatic treatment planning for prostate cancer intensity modulated radiotherapy

## Quick Facts
- arXiv ID: 2502.00346
- Source URL: https://arxiv.org/abs/2502.00346
- Authors: Md Mainul Abrar; Parvat Sapkota; Damon Sprouts; Xun Jia; Yujie Chi
- Reference count: 34
- Primary result: ACER-based DRL agent achieves 93.09% perfect plan scores across 300+ test cases while training on only one patient

## Executive Summary
This paper develops an Actor-Critic with Experience Replay (ACER) agent for automatic treatment planning in prostate IMRT. The approach addresses key challenges in AI-based treatment planning including data dependency, universal applicability, and robustness. The ACER agent tunes treatment planning parameters using DVH inputs and achieves state-of-the-art performance across multiple independent datasets despite training on a single patient case.

## Method Summary
The ACER agent uses a two-head LSTM architecture that takes DVH curves as input and outputs 18 discrete actions for TPP tuning. The method employs experience replay with truncated importance sampling, enabling training from a single patient's data. The agent uses asynchronous online and offline training, with stochastic policy outputs that enable efficient exploration. The ACER framework includes entropy regularization and Q-retrace estimation for stable learning.

## Key Results
- Achieved 93.09% perfect plan scores (mean 8.93±0.27) across 300+ test cases from three independent datasets
- Outperformed DQN-based approaches significantly (93.09% vs 72.67% perfect scores)
- Demonstrated robustness against adversarial attacks, with <1% probability change under FGSM vs DQN's 18-88% changes
- Maintained performance stability despite training on only one patient case

## Why This Works (Mechanism)

### Mechanism 1
Stochastic policy enables prioritization of reasonable TPP tuning actions while maintaining exploration capacity. ACER outputs probability distributions over 18 discrete actions rather than single deterministic Q-values. Entropy regularization (β=0.001) prevents premature convergence to sub-optimal policies, allowing the agent to strongly weight optimal actions while keeping non-reasonable actions near-zero probability.

### Mechanism 2
Experience replay with truncated importance sampling enables training from a single patient case. The agent stores trajectories in a replay buffer (100K steps) and uses offline training with importance weights. This decouples data efficiency from patient diversity, allowing TPP tuning dynamics learned from one patient to transfer to others.

### Mechanism 3
Actor-critic separation stabilizes learning under adversarial perturbations. The actor outputs policy probabilities while critic estimates Q-values. Under FGSM attack, ACER's top action probability changes by <1% versus DQN's Q-value changes of 18-88%, with stochastic policy distributing probability mass across reasonable actions.

## Foundational Learning

- **Dose-Volume Histograms (DVHs)**: State representation using 300 floating values (100 points × 3 curves: PTV, bladder, rectum) encoding cumulative dose distribution. Why needed: Primary input state for the agent. Quick check: Can you explain why DVH curves are sufficient state representations despite losing spatial dose information?

- **Policy Gradient with Baseline**: Core mathematical framework where gradient g = E[Q∇log π] requires understanding Q-retrace variance reduction vs. Monte Carlo returns. Why needed: Enables stable policy updates. Quick check: What problem does the baseline (Vθc) solve in the policy gradient estimator?

- **Importance Sampling for Off-Policy Learning**: Enables experience replay from trajectories generated by earlier policy versions. Truncation (min(c, ρ)) bounds variance. Why needed: Allows efficient learning from stored experiences. Quick check: Why must we weight replayed transitions by ρ = π(a|s)/μ(a|s)?

## Architecture Onboarding

- **Component map**: DVH curves (300 values) → Fully connected (32 units) → ReLU → LSTM (32 hidden) → Two heads: Actor (FC → Softmax → 18 action probabilities), Critic (FC → 18 Q-values)

- **Critical path**: 1) DVH extraction from TPS → state encoding, 2) Actor forward pass → action sampling from π, 3) TPS execution with tuned TPPs → new DVH + reward, 4) Critic Q-retrace computation → policy/critic gradient updates, 5) Periodic evaluation on held-out cases (every 500 steps)

- **Design tradeoffs**: TRPO disabled (reduces computational cost but allows larger policy deviations), Discrete action space (18 actions limits fine-grained control), Single-patient training (maximum data efficiency but risks overfitting)

- **Failure signatures**: Score variance increases after 200K steps (overfitting to training case), Action probability distribution flattens (entropy regularization too weak), Q-values diverge (critic learning rate too high)

- **First 3 experiments**: 1) Replicate single-patient training on dataset 1, validate on datasets 2 and 3 (confirm mean score >8.5 within 150K steps), 2) Ablate experience replay (online-only updates) and measure sample efficiency loss, 3) Test continuous-action ACER variant and compare convergence speed vs. discrete version

## Open Questions the Paper Calls Out
- Can ACER be extended to perform continuous TPP tuning and simultaneous multi-TPP adjustment in a single planning step?
- How does the ACER-based VTP agent perform when operating commercial treatment planning systems rather than the in-house TPS used for training?
- Can the ACER-based agent maintain robustness against stronger adversarial attacks beyond FGSM?

## Limitations
- Single-patient training generalizability may not extend to other cancer sites beyond prostate IMRT
- Adversarial robustness claims limited to FGSM testing; more sophisticated attacks may reveal vulnerabilities
- Method success depends on specific in-house TPS implementation and ProKnow scoring criteria

## Confidence
- High confidence: ACER's superior performance over DQN and stochastic policy prioritization mechanism
- Medium confidence: Single-patient training approach generalizes well for prostate IMRT but needs broader validation
- Medium confidence: Adversarial robustness claims demonstrated against FGSM but not stronger attack vectors

## Next Checks
1. Cross-cancer validation: Test single-patient ACER training on head-and-neck and lung IMRT cases
2. Continuous-action ablation study: Compare discrete vs. continuous action space ACER implementations
3. Long-range adversarial testing: Evaluate ACER against PGD and Carlini-Wagner attacks