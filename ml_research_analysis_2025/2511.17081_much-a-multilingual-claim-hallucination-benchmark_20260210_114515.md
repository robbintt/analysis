---
ver: rpa2
title: 'MUCH: A Multilingual Claim Hallucination Benchmark'
arxiv_id: '2511.17081'
source_url: https://arxiv.org/abs/2511.17081
tags:
- token
- much
- claim
- likelihood
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUCH is the first multilingual claim-level uncertainty quantification
  benchmark for evaluating the reliability of large language models. It provides 4,873
  samples across four European languages with 24 logits per token and a fast, deterministic
  segmentation algorithm, enabling fair evaluation of future methods.
---

# MUCH: A Multilingual Claim Hallucination Benchmark

## Quick Facts
- arXiv ID: 2511.17081
- Source URL: https://arxiv.org/abs/2511.17081
- Authors: Jérémie Dentan; Alexi Canesse; Davide Buscaldi; Aymen Shabou; Sonia Vanier
- Reference count: 0
- MUCH is the first multilingual claim-level uncertainty quantification benchmark for evaluating the reliability of large language models

## Executive Summary
MUCH introduces the first multilingual benchmark for claim hallucination detection, providing 4,873 samples across four European languages (English, German, French, Spanish) with 24 logits per token and automated claim-level factuality annotations for 20,751 claims. The benchmark enables fair evaluation of future uncertainty quantification methods through its deterministic segmentation algorithm and standardized annotation process. Current baseline methods show modest performance (ROC-AUC up to 0.772) with significant computational overhead, highlighting substantial room for improvement in both accuracy and efficiency for realistic deployment scenarios.

## Method Summary
MUCH constructs a multilingual dataset with 4,873 samples across four European languages, each containing 24 logits per token for uncertainty quantification. The benchmark employs a fast, deterministic segmentation algorithm to identify claims within text, followed by automated claim-level factuality annotations verified against human judgments. The dataset includes 20,751 annotated claims, enabling comprehensive evaluation of hallucination detection methods. The methodology emphasizes reproducibility through standardized annotation protocols and provides clear evaluation metrics for comparing different approaches.

## Key Results
- ROC-AUC scores reach up to 0.772 for baseline methods
- 4,873 multilingual samples across four European languages
- 20,751 claims with automated factuality annotations
- Substantial computational overhead observed in current methods

## Why This Works (Mechanism)
MUCH's effectiveness stems from its comprehensive multilingual coverage, high-resolution uncertainty quantification (24 logits per token), and automated yet verified annotation process. The deterministic segmentation algorithm ensures consistent claim identification across languages, while the large-scale annotation provides robust training and evaluation data. The benchmark's design enables fair comparison of different uncertainty quantification approaches through standardized metrics and reproducible methodology.

## Foundational Learning

1. **Multilingual Natural Language Processing**
   - Why needed: Understanding language-specific nuances and cross-lingual transfer capabilities
   - Quick check: Test model performance across all four languages to verify generalization

2. **Uncertainty Quantification in LLMs**
   - Why needed: Measuring model confidence and detecting potential hallucinations
   - Quick check: Compare uncertainty scores with ground truth factual correctness

3. **Claim Detection and Segmentation**
   - Why needed: Identifying factual statements within text for evaluation
   - Quick check: Verify segmentation algorithm correctly identifies claims across different text structures

## Architecture Onboarding

**Component Map**: Text Input -> Segmentation Algorithm -> Claim Extraction -> Uncertainty Quantification -> Factuality Annotation

**Critical Path**: Text input flows through deterministic segmentation to extract claims, which then undergo uncertainty quantification using 24 logits per token, followed by factuality annotation verification.

**Design Tradeoffs**: 
- Automated annotation enables scalability but may introduce systematic biases
- High logit resolution (24 per token) provides granular uncertainty measurement but increases computational overhead
- Multilingual coverage (4 languages) balances breadth with depth of annotation quality

**Failure Signatures**: 
- Inconsistent claim segmentation across similar text structures
- Overconfidence in uncertain predictions
- Language-specific annotation biases affecting cross-lingual performance

**3 First Experiments**:
1. Baseline ROC-AUC evaluation across all four languages
2. Computational efficiency benchmarking on different hardware configurations
3. Cross-lingual transfer learning assessment between language pairs

## Open Questions the Paper Calls Out
The paper does not explicitly identify specific open questions for future research, though the substantial performance gap between current methods and potential improvements suggests several implicit research directions in uncertainty quantification, computational efficiency, and multilingual generalization.

## Limitations
- Limited to four European languages, constraining cross-linguistic generalizability
- Automated annotation process may introduce systematic biases
- Computational overhead requirements may limit practical deployment scenarios
- Performance metrics suggest significant room for improvement in baseline methods
- Unknown: Whether the deterministic segmentation algorithm captures all relevant claim structures across languages

## Confidence
- High confidence: Dataset construction methodology and claim-level annotation process
- Medium confidence: Computational overhead claims and efficiency metrics
- Medium confidence: First-of-its-kind assertion in the rapidly evolving research landscape
- Assumption: The automated annotation verification against human judgments provides sufficient reliability for benchmark evaluation

## Next Checks
1. External validation across additional language families, particularly non-European languages
2. Human evaluation study comparing automated annotations against expert judgments
3. Comprehensive computational efficiency benchmarking across different hardware configurations and model sizes
4. Investigation of potential systematic biases in the automated annotation process
5. Assessment of whether the 24-logit resolution provides optimal balance between accuracy and computational cost