---
ver: rpa2
title: DeepONet Augmented by Randomized Neural Networks for Efficient Operator Learning
  in PDEs
arxiv_id: '2503.00317'
source_url: https://arxiv.org/abs/2503.00317
tags:
- neural
- networks
- training
- rann-deeponet
- deeponets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RaNN-DeepONets, a hybrid architecture that
  combines DeepONets with Randomized Neural Networks (RaNNs) to address computational
  inefficiencies and optimization challenges in operator learning for PDEs. By fixing
  the hidden layer parameters of RaNNs and using least-squares optimization for the
  output layer, the method significantly reduces training time and computational cost
  while maintaining high accuracy.
---

# DeepONet Augmented by Randomized Neural Networks for Efficient Operator Learning in PDEs

## Quick Facts
- **arXiv ID:** 2503.00317
- **Source URL:** https://arxiv.org/abs/2503.00317
- **Reference count:** 40
- **Primary result:** RaNN-DeepONets achieves comparable accuracy to standard DeepONets while reducing training time by orders of magnitude through closed-form least-squares optimization

## Executive Summary
This paper introduces RaNN-DeepONets, a hybrid architecture that combines DeepONets with Randomized Neural Networks (RaNNs) to address computational inefficiencies and optimization challenges in operator learning for PDEs. By fixing the hidden layer parameters of RaNNs and using least-squares optimization for the output layer, the method significantly reduces training time and computational cost while maintaining high accuracy. The approach also incorporates physics-informed learning to reduce data dependency by embedding PDE constraints directly into training. Evaluated on three benchmark problems—diffusion-reaction dynamics, Burgers' equation, and Darcy flow—RaNN-DeepONets achieves comparable or superior accuracy to standard DeepONets while reducing computational costs by orders of magnitude.

## Method Summary
The method replaces the iterative backpropagation training of standard DeepONets with a closed-form least-squares solution. The branch network (which processes input functions) and trunk network (which processes spatial/temporal coordinates) both use fixed random weights in their hidden layers. Training then becomes a linear optimization problem where only the output layer weights of the branch network are learned via least-squares minimization. This transformation eliminates the need for gradient descent while incorporating physics-informed constraints through analytical computation of PDE residuals using the fixed basis functions.

## Key Results
- Achieves relative l2 error of 2.80E-03 on diffusion-reaction dynamics with only 90.74 seconds training time (vs 4068.00 seconds for vanilla DeepONets)
- Maintains comparable or superior accuracy across all three benchmark problems while reducing computational costs by orders of magnitude
- Successfully incorporates physics-informed learning constraints with negligible computational overhead compared to standard PINOs
- Demonstrates effectiveness across linear (Darcy flow) and nonlinear (Burgers' equation, diffusion-reaction) PDEs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing iterative backpropagation with a closed-form least-squares solve significantly reduces training time and computational cost.
- **Mechanism:** The architecture freezes the weights of the hidden layers in the branch network and all layers in the trunk network (randomized initialization). The training process then solves only for the output layer weights of the branch network ($W_b^2$) by minimizing a linear least-squares objective ($argmin_\alpha ||A\alpha - b||^2$), effectively converting a non-convex optimization problem into a convex linear system solve.
- **Core assumption:** The random projections in the hidden layers provide a sufficiently rich basis such that the linear output layer can approximate the complex non-linear operator without tuning the preceding layers.
- **Evidence anchors:** [abstract] "RaNNs compute the output layer parameters using the least-squares method, significantly reducing training time..."; [section 3.1] "...nonlinear and nonconvex optimization problem of training FCNs... is transformed into a linear least-square problem..."; [corpus] ELM-DeepONets (arXiv:2501.09395) validates this pattern, showing that backpropagation-free training via Extreme Learning Machines is an emerging efficiency strategy.
- **Break condition:** If the random features do not span the solution space (e.g., insufficient hidden neurons $k$), the least-squares solution will underfit, and no amount of output-layer tuning can fix the representation error.

### Mechanism 2
- **Claim:** Enforcing boundary conditions (BCs) as hard constraints improves generalization and accuracy by reducing the search space of valid solutions.
- **Mechanism:** Instead of penalizing boundary errors in the loss function (soft constraints), the architecture constructs a surrogate solution $G(f)(y) = c(y)\hat{G}(f)(y) + g(y)$. Here, $c(y)$ is a distance function that is zero on the boundary, ensuring the network output automatically satisfies Dirichlet conditions regardless of the internal predictions $\hat{G}$.
- **Core assumption:** The geometry of the domain is known and differentiable, allowing for the construction of the masking function $c(y)$ (e.g., $x(1-x)$ for a unit interval).
- **Evidence anchors:** [abstract] "...reducing computational costs by orders of magnitude... comparable or superior accuracy..."; [section 2.3] "To ensure that the DeepONet output satisfies these BCs automatically, a surrogate solution is introduced..."; [corpus] BumpNet (arXiv:2512.17198) utilizes sparse basis function expansion, conceptually similar to using specific basis functions to enforce locality or constraints.
- **Break condition:** If the boundary geometry is highly irregular or implicit, constructing the analytical mask $c(y)$ becomes infeasible, requiring a fallback to soft constraints.

### Mechanism 3
- **Claim:** Physics-informed constraints can be embedded with negligible computational overhead compared to standard Physics-Informed Neural Operators (PINOs).
- **Mechanism:** Because the trunk network basis functions (e.g., $\tanh$) and their derivatives are fixed, the PDE residual can be computed analytically or via simple pre-computed derivatives for the linear combination. This avoids the expensive automatic differentiation (backpropagation through time/layers) typically required to calculate physics loss in standard DeepONets.
- **Core assumption:** The physics loss is convex with respect to the output weights, or the linear system remains well-conditioned enough that standard solvers find the minimum effectively.
- **Evidence anchors:** [abstract] "...incorporates physics-informed learning to reduce data dependency by embedding PDE constraints..."; [section 4.2] "...eliminating the need for automatic differentiation, as the trunk basis is fixed and its derivatives can be directly computed."; [corpus] FEDONet (arXiv:2509.12344) explores Fourier embeddings for spectral accuracy, contrasting with the fixed basis approach here; Alpha-VI DeepONet (arXiv:2408.00681) focuses on Bayesian uncertainty, a different approach to robustness.
- **Break condition:** Highly non-linear PDEs may result in a non-linear system even with fixed bases, potentially requiring iterative linearization (e.g., Newton's method) which re-introduces complexity.

## Foundational Learning

- **Concept:** **Linear Least Squares (Moore-Penrose Pseudoinverse)**
  - **Why needed here:** The core training loop is not gradient descent but a matrix solve. Understanding the trade-offs between direct solvers (SVD) and iterative solvers for large feature matrices is critical.
  - **Quick check question:** Can you explain why a least-squares solution is guaranteed to be unique (or not) given a specific rank of the feature matrix?

- **Concept:** **Universal Approximation Theory (Random Features)**
  - **Why needed here:** The method relies on the theoretical guarantee that a wide enough network with random hidden weights can approximate continuous functions. This justifies freezing the weights.
  - **Quick check question:** Why does increasing the width ($k$) of the hidden layer generally improve accuracy in randomized neural networks, unlike in iteratively trained deep networks where overfitting is a larger risk?

- **Concept:** **Operator Learning (Branch vs. Trunk)**
  - **Why needed here:** Understanding that the Branch net encodes the input function (parameters/initial conditions) and the Trunk net encodes the domain (spacetime coordinates) is essential to debugging which side of the network is causing approximation errors.
  - **Quick check question:** If the model fails to generalize to new initial conditions but fits the training spatial domain perfectly, which network (Branch or Trunk) is likely the bottleneck?

## Architecture Onboarding

- **Component map:**
  - Branch Network: RaNN (Input: Discretized function $f$ -> Hidden: Random/Fixed -> Output: Trainable Weights)
  - Trunk Network: Single-Layer Perceptron (Input: Coordinates $y$ -> Hidden: Random/Fixed -> Output: Fixed Basis)
  - Combiner: Dot product of Branch output and Trunk output
  - Solver: Linear Least Squares optimizer (replaces Adam/SGD)

- **Critical path:** Initialization (Scaling of random weights $r_b, r_t$) -> Feature Computation (Forward pass to hidden layer) -> System Assembly ($A\alpha = b$) -> Solve

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** Increasing hidden neurons ($k$) improves approximation capacity but increases the size of the linear system ($O(k^3)$ solve complexity)
  - **Data vs. Physics:** Using Physics-informed loss reduces data needs but requires deriving analytical derivatives for the fixed basis functions

- **Failure signatures:**
  - **Ill-conditioned Matrix:** If random weights are poorly scaled, the feature matrix becomes singular, causing the solver to fail or explode
  - **Boundary "Leaking":** Hard constraints failing implies the mask $c(y)$ was not strictly zero on the boundary or the collocation points missed the boundary entirely

- **First 3 experiments:**
  1. **Sanity Check (1D ODE):** Implement RaNN-DeepONet for a simple linear ODE ($u' = f$) to verify the least-squares solver converges faster than a standard DeepONet with Adam
  2. **Hard Constraint Test:** Solve the Diffusion-Reaction equation (Example 5.1) with and without the hard-constraint mask ($c(y)$) to quantify the error reduction at boundaries
  3. **Ablation on Width:** Vary the hidden layer size ($k$) and output dimension ($p$) on Burgers' equation (Example 5.2) to identify the "knee" in the curve where increasing parameters yields diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can weight initialization strategies be specifically optimized for the RaNN-DeepONet framework to enhance approximation capabilities?
- Basis in paper: [explicit] Section 3.2 states, "Developing effective initialization strategies for weight parameters specifically within the RaNN-DeepONet framework remains an intriguing and valuable topic for future research."
- Why unresolved: The current work focuses exclusively on a bias initialization strategy based on domain bounds, leaving the weight parameters to standard random sampling without domain-specific optimization.
- What evidence would resolve it: A study proposing and validating a frequency-based or spectral weight initialization method that improves the partition hyperplane density and model accuracy compared to uniform sampling.

### Open Question 2
- Question: Can physics-informed RaNN-DeepONets be effectively extended to strongly nonlinear PDEs, such as Burgers' equation, without reliance on pre-computed solution data?
- Basis in paper: [explicit] Section 6 notes, "directly extending this benefit [physics-informed learning] to strongly nonlinear PDEs such as the diffusion-reaction and Burgers' equations poses additional challenges."
- Why unresolved: The paper successfully demonstrates the physics-informed approach on the linear Darcy flow problem, but the nonlinear benchmark examples (Burgers and diffusion-reaction) rely on data-driven training rather than pure physics constraints.
- What evidence would resolve it: Successful implementation of physics-informed RaNN-DeepONets on diffusion-reaction or Burgers' equations utilizing iterative linearization techniques (e.g., Newton's method) to handle the nonlinearity without ground-truth data.

### Open Question 3
- Question: What performance benefits arise from integrating advanced neural architectures, such as Convolutional Neural Networks (CNNs) or Transformers, into the RaNN-DeepONet framework?
- Basis in paper: [explicit] Section 6 identifies this as a research direction: "A particularly promising approach involves exploring the integration of advanced neural network structures—such as convolutional neural networks or transformer-based neural networks—into the RaNN-DeepONet framework."
- Why unresolved: The current architectural design is restricted to simple Fully Connected Networks (FCNs) and single-layer perceptrons for the branch and trunk networks.
- What evidence would resolve it: Benchmarks comparing FCN-based RaNN-DeepONets against CNN or Transformer-based variants on complex, high-dimensional PDE problems to evaluate improvements in training speed and accuracy.

### Open Question 4
- Question: Is there a theoretical or empirical optimal ratio between the hidden layer width ($k$) and output layer width ($p$) in the branch network to ensure efficient convergence?
- Basis in paper: [inferred] Section 5.2 observes that "the scales of $k$ and $p$ should be comparable" and notes that increasing output width ($p$) without sufficient hidden width ($k$) degrades performance.
- Why unresolved: The paper identifies the interdependency of these hyperparameters experimentally but does not formalize a rule for selecting these dimensions relative to the complexity of the operator being learned.
- What evidence would resolve it: An ablation study providing a theoretical bound or heuristic formula linking the spectral complexity of the target PDE operator to the necessary ratio of $k$ to $p$.

## Limitations
- The method's accuracy heavily depends on the richness of random feature spaces, which is not rigorously proven for the specific PDEs tested
- Boundary condition enforcement using analytical masks may fail for complex or irregular domain geometries
- The physics-informed learning claim lacks quantitative comparison of data reduction versus standard PINOs
- Ablation studies on hidden layer width are sparse and don't fully explore the trade-off between random feature sufficiency and overfitting risk

## Confidence
- **High Confidence:** The computational efficiency gains (training time reduction) are directly measurable and well-documented across all three benchmark problems
- **Medium Confidence:** The claim of "comparable or superior accuracy" is supported by relative l2 errors but lacks statistical significance testing across multiple random seeds
- **Low Confidence:** The physics-informed learning claim is weakest—the paper states the PDE residual can be computed analytically for fixed bases, but provides no quantitative comparison of data reduction vs. standard PINOs

## Next Checks
1. **Random Seed Robustness:** Repeat all three experiments (Diffusion-Reaction, Burgers', Darcy Flow) with 10 different random seeds and report mean/confidence intervals for both accuracy and training time
2. **Feature Sufficiency Analysis:** For each problem, sweep the hidden layer width $k$ from 10 to 1000 and plot the learning curve to identify the point of diminishing returns and verify the random features span the solution space
3. **Complex Boundary Test:** Implement the hard constraint method on a non-trivial geometry (e.g., L-shaped domain or circle) and compare error profiles at the boundary vs. a standard DeepONet with soft constraints