---
ver: rpa2
title: Active Learning with a Noisy Annotator
arxiv_id: '2504.04506'
source_url: https://arxiv.org/abs/2504.04506
tags:
- noise
- noisy
- learning
- samples
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework called Noise-Aware Active
  Sampling (NAS) that extends greedy, coverage-based active learning strategies to
  handle noisy annotations in the low-budget regime. The core idea is to identify
  regions in the data that remain uncovered due to the selection of noisy representatives
  and enable resampling from these areas.
---

# Active Learning with a Noisy Annotator

## Quick Facts
- **arXiv ID:** 2504.04506
- **Source URL:** https://arxiv.org/abs/2504.04506
- **Reference count:** 27
- **Primary result:** Noise-Aware Active Sampling (NAS) framework improves standard active learning strategies on CIFAR100 and ImageNet subsets under various noise types and rates.

## Executive Summary
This paper introduces Noise-Aware Active Sampling (NAS), a novel framework that extends greedy, coverage-based active learning strategies to handle noisy annotations in low-budget regimes. NAS identifies regions in the data that remain uncovered due to selection of noisy representatives and enables resampling from these areas. The framework employs a simple yet effective noise filtering approach suitable for low-budget settings, leveraging self-supervised learning features and linear probing. Experiments demonstrate that NAS significantly improves performance of standard active learning methods across different noise types and rates.

## Method Summary
NAS wraps around a greedy active learning strategy (e.g., ProbCover) and adds two key components: noise filtering and iterative coverage recovery. In each cycle, the method first applies a noise filter (LowBudgetAUM or CrossValidation) to separate clean and noisy labeled samples. It then selects a batch of samples using the base strategy, considering only clean samples. After labeling, noisy samples are identified and their coverage regions are "un-covered" by restoring selection eligibility for their neighbors. The framework uses self-supervised learning features and linear probing for noise filtering, making it suitable for low-budget regimes where end-to-end DNN training fails.

## Key Results
- NAS consistently outperforms baselines including ProbCover and other strategies across multiple noise types and rates
- The method shows robustness to different feature spaces and noise filtering algorithms
- Even with ideal noise filters, NAS demonstrates superior performance by enabling effective coverage recovery
- LowBudgetAUM noise filter adapted for low-budget settings achieves accurate noise rate estimation even with as few as two clean samples per class

## Why This Works (Mechanism)

### Mechanism 1: Iterative Coverage Recovery via Graph Reweighting
Standard coverage strategies select a sample and remove its neighbors from the candidate pool. NAS introduces a noise-filtering step after labeling. If a selected sample is flagged as noisy, NAS restores the selection eligibility for that sample's neighborhood, effectively "un-covering" the region so a clean representative can be selected later. This works because the feature space is semantically meaningful enough that a noisy sample's neighbors are likely to be semantically similar.

### Mechanism 2: Low-Budget Noise Filtering via Linear Probing on SSL Features
End-to-end DNNs overfit when labeled data is scarce. NAS leverages pre-trained SSL features and trains a simple linear probe to identify label noise. The method utilizes the "early learning phenomenon" or geometric outlier status to filter noise without requiring large datasets. This assumes SSL representations have already clustered semantic concepts effectively, such that mislabeled samples appear as inconsistencies relative to their assigned class centroid.

### Mechanism 3: Radius Adaptation for Graph Sparsity
As the unlabeled pool shrinks, the coverage radius must dynamically increase to maintain selection options. When the candidate pool depletes and the ball radius remains fixed, graph connectivity breaks. NAS detects this sparsity and updates the radius to maximize the graph's maximum degree, ensuring the coverage objective remains feasible.

## Foundational Learning

- **Concept:** Greedy Coverage in Active Learning (e.g., ProbCover)
  - **Why needed:** NAS fixes a failure mode of greedy coverage strategies. Understanding the original objective (maximizing union of balls around selected points) is necessary to understand the "fix."
  - **Quick check:** If a selected point is removed, should the points it "covered" be eligible for selection again in a standard greedy coverage algorithm?

- **Concept:** Area Under the Margin (AUM)
  - **Why needed:** The paper adapts AUM for low-budget filtering. Understanding that AUM relies on the "early learning phenomenon" (clean samples have larger margins early in training) is necessary to see why this is adapted to linear probing on SSL features.
  - **Quick check:** Why would a deep network's AUM score be unreliable when trained on only 10 samples per class?

- **Concept:** Self-Supervised Learning (SSL) Representations
  - **Why needed:** The method assumes pre-trained feature space (SimCLR, DINO). Success depends entirely on the quality of these embeddings, rather than learned supervised features.
  - **Quick check:** Does the NAS framework train the feature extractor, or does it freeze it?

## Architecture Onboarding

- **Component map:** Input Unlabeled pool U, Pre-trained SSL Encoder -> Selector S (e.g., ProbCover) -> Annotator -> Noise Filter A -> State Updater -> Clean labeled set L_clean
- **Critical path:** The interaction between the Noise Filter and the Graph State Updater. If the filter misclassifies a clean sample as noisy, the State Updater forces the system to redundantly select neighbors of that sample, wasting budget.
- **Design tradeoffs:**
  - Batch size b: Small b allows for precise noise correction after every label but increases computational overhead. Large b reduces overhead but risks selecting multiple noisy samples before correction.
  - Noise Dropout: Treating a fraction of predicted noisy samples as clean stabilizes the system but introduces stochasticity.
- **Failure signatures:**
  - Stalling: The selection graph becomes empty (max degree ≤1) prematurely. Fix: Check δ update logic.
  - Noise Collapse: The filter predicts near 100% noise rates, causing the active learning loop to flail. Fix: Enable or tune "Noise Dropout" regularization.
- **First 3 experiments:**
  1. Baseline Sanity Check: Run ProbCover on CIFAR100 with 0% noise to ensure base graph construction functions correctly.
  2. Noise Injection Stress Test: Run ProbCover + NAS on CIFAR100 with 40% symmetric noise. Verify LowBudgetAUM precision >80% and that selection graph edges are being restored for noisy regions.
  3. Ablation on Filter: Compare "Ideal Filter" vs. "LowBudgetAUM" to isolate performance drops caused by the filter vs. selection logic.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's dependence on high-quality SSL features remains a critical limitation not fully addressed
- The noise filtering mechanism assumes semantic coherence in feature space, which may break down for instance-dependent noise patterns
- Performance with weaker feature representations or in domains where SSL pretraining is unavailable is not explored

## Confidence

- **High Confidence:** The core mechanism of iterative coverage recovery via graph reweighting is well-supported by theoretical framework and experimental results showing consistent improvements across multiple noise types and rates.
- **Medium Confidence:** The effectiveness of LowBudgetAUM for low-budget noise filtering is demonstrated, but reliance on specific SSL representations and the 80th percentile threshold choice could be sensitive to dataset characteristics.
- **Medium Confidence:** The radius adaptation strategy for maintaining graph connectivity is theoretically sound, but practical impact varies with dataset density and specific δ update policy implementation.

## Next Checks
1. **Feature Sensitivity Test:** Evaluate NAS performance using randomly initialized features versus SSL features to quantify dependency on feature quality.
2. **Instance-Dependent Noise:** Test the framework with instance-dependent noise patterns where noisy samples are not outliers in feature space, to assess filter robustness.
3. **Cross-Domain Transfer:** Apply NAS to a domain where the SSL backbone is mismatched (e.g., using ImageNet-pretrained features on medical imaging data) to evaluate feature generalization limits.