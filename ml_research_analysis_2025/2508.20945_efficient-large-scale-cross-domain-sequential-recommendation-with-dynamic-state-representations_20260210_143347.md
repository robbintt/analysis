---
ver: rpa2
title: Efficient Large-Scale Cross-Domain Sequential Recommendation with Dynamic State
  Representations
arxiv_id: '2508.20945'
source_url: https://arxiv.org/abs/2508.20945
tags:
- domain
- cross-domain
- recommendation
- sequential
- intra-domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the computational inefficiency of transformer-based
  multi-domain sequential recommendation systems, where attention mechanisms scale
  quadratically with sequence length across all domains. The proposed method replaces
  full inter-domain attention with two mechanisms: Transition-Aware Positional Embeddings
  (TAPE) that encode domain transitions, and Dynamic Domain State Representation (DDSR)
  that maintains cross-domain context without full attention maps.'
---

# Efficient Large-Scale Cross-Domain Sequential Recommendation with Dynamic State Representations

## Quick Facts
- arXiv ID: 2508.20945
- Source URL: https://arxiv.org/abs/2508.20945
- Reference count: 3
- Primary result: HR@100 improves from 9.09 to 10.25 and NDCG@100 from 3.15 to 3.26 over baseline HSTU

## Executive Summary
This work addresses the computational inefficiency of transformer-based multi-domain sequential recommendation systems by replacing full inter-domain attention with two mechanisms: Transition-Aware Positional Embeddings (TAPE) and Dynamic Domain State Representation (DDSR). The approach restricts attention to intra-domain sequences while enabling cross-domain transfer through these innovations, achieving significant computational savings from O(S²) to O(δS²) while improving recommendation quality.

## Method Summary
The method introduces three key innovations to reduce transformer attention complexity in cross-domain recommendation: (1) Intra-domain attention masking using FlexAttention that computes Q·K^T only when positions share the same domain, (2) Transition-Aware Positional Embeddings (TAPE) that encode domain transition information through learnable transition vectors, and (3) Dynamic Domain State Representation (DDSR) that maintains cross-domain context by caching the last hidden state per domain. The model processes Amazon cross-domain dataset with 5 domains using SampledSoftmax loss restricted to target domain items.

## Key Results
- HR@100 improves from 9.09 to 10.25 over baseline HSTU
- NDCG@100 increases from 3.15 to 3.26
- Computational complexity reduces from O(S²) to O(δS²) where δ < 1
- TAPE+DDSR outperforms TAPE-only and DDSR-only variants

## Why This Works (Mechanism)

### Mechanism 1: Intra-Domain Attention Masking with FlexAttention
Restricting self-attention to within-domain subsequences reduces computational cost while avoiding noise from inter-domain dependencies. Uses FlexAttention to compute Q·K^T only when positions i,j share the same domain, otherwise skips computation. Core assumption: Cross-domain knowledge transfer can be decoupled from intra-domain attention and handled separately.

### Mechanism 2: Transition-Aware Positional Embeddings (TAPE)
Encoding domain transition information into positional embeddings enables the model to anticipate domain shifts without requiring cross-domain attention. Adds a learnable transition vector r_i when d_{i+1} ≠ d_i, computed as r_i = d̂_{i+1} ⊙ (W · d̂_i + b). Core assumption: Domain transition patterns carry sufficient signal for the model to adapt representations to upcoming domain contexts.

### Mechanism 3: Dynamic Domain State Representation (DDSR)
Maintaining a cached state per domain that is updated at each position enables cross-domain context transfer with O(D·n) cost instead of O(n²) full attention. Maintains H_D^L storing the last hidden state for each domain at each position, with a cross-domain attention layer querying these cached states. Core assumption: Condensing domain history into a single "last hidden state" per domain preserves sufficient cross-domain signal.

## Foundational Learning

- **Self-Attention Complexity and Sparse Attention Patterns**: Why needed - The paper's core contribution is reducing O(S²) to O(δS²) by exploiting domain-based sparsity; understanding why attention scales quadratically is prerequisite. Quick check - Given a sequence of 1000 items split across 5 equally-sized domains, what is the approximate ratio of intra-domain attention computations to full attention computations?

- **Positional Encodings Beyond Absolute Positions**: Why needed - TAPE extends standard positional embeddings with transition information; understanding ALiBi (used as base) helps contextualize the design. Quick check - How does ALiBi's approach to position encoding differ from learned absolute positional embeddings, and why might it be preferred for long sequences?

- **Cross-Domain Transfer in Recommendation**: Why needed - The paper assumes cross-domain behavior contains complementary signals; understanding transfer learning fundamentals helps evaluate this assumption. Quick check - What are two risks of naive cross-domain transfer (e.g., simply concatenating sequences from all domains)?

## Architecture Onboarding

- **Component map**: Input Sequence → Item Embeddings (E) → Domain Embeddings (M) → TAPE Module → Stacked Transformer Layers (Intra-Domain Self-Attention, DDSR Cross-Domain Attention, FFN with gating) → Domain-Scoped Output → SampledSoftmax over domain-specific vocabulary

- **Critical path**: Domain sequence T_u must be available as input; TAPE transition signal must be computed before first attention layer; H_D state cache must be updated after each intra-domain attention block; Output vocabulary must be restricted to target domain items

- **Design tradeoffs**: Efficiency vs. Expressiveness - Intra-domain masking sacrifices direct cross-token attention for O(δS²) cost; DDSR compensates with compressed states but may lose fine-grained signals; HR@K vs. Ranking Quality - Full model improves HR@100 but slightly decreases MRR, suggesting better recall but marginally worse top-1 precision; Domain Count vs. Scaling - Efficiency gains are maximized when domains are balanced

- **Failure signatures**: HR@100 drops below baseline → Check if domain masking is too aggressive or DDSR states are not properly updated; Training instability → Verify TAPE transition embeddings are not exploding; No speedup observed → Confirm FlexAttention masking is actually skipping computations; Performance degrades with more domains → DDSR may bottleneck

- **First 3 experiments**: (1) Baseline sanity check: Run HSTU with ALiBi on your data, then add intra-domain masking only; (2) TAPE ablation: Add TAPE to masked baseline, measure impact on HR@1 and MRR; (3) DDSR scaling test: Vary number of domains (2, 5, 10) and measure both HR@100 and wall-clock training time

## Open Questions the Paper Calls Out

- **Runtime Efficiency Validation**: The paper notes that "While our current implementation is not yet optimized, theoretical analysis indicates clear runtime benefits, which we aim to confirm in future work." This remains unresolved as the paper demonstrates theoretical complexity reduction but relies on FlexAttention and custom masking that may introduce practical overhead.

- **Popularity Bias Amplification**: The authors acknowledge that "Domain separation may also amplify popularity bias... though we leave this to future work." By isolating domains, the model might lose the capacity to leverage long-tail cross-domain interactions, potentially forcing it to rely on the most popular items within each specific domain.

- **Ranking Sharpness vs. Recall Tradeoff**: The ablation study reveals a functional split where removing DDSR hurts recall (HR@100) while removing TAPE hurts ranking (NDCG). The authors note a "tradeoff in ranking sharpness but with stronger overall recall" in the full model, suggesting the current simple summation of representations may not optimally balance exploration and exploitation mechanisms.

## Limitations

- Efficiency claims rely on balanced domain distributions (δ < 1 requires s_d ≈ S/D)
- TAPE mechanism's effectiveness depends on predictable domain transitions
- DDSR state representation may bottleneck when domains have heterogeneous internal structure
- Hyperparameter sensitivity is unclear, particularly regarding domain state dimension and transition embedding size

## Confidence

- **High confidence**: Computational complexity reduction mechanism, experimental results showing HR@100 and NDCG@100 improvements
- **Medium confidence**: TAPE's contribution to performance, DDSR's ability to capture cross-domain context with compressed state representation
- **Low confidence**: Claims about "significantly drops performance" when removing individual components, efficiency gains with highly skewed domain distributions

## Next Checks

1. **Domain distribution sensitivity**: Run experiments with artificially skewed domain distributions (e.g., 80% of interactions in one domain) to verify whether the claimed O(δS²) efficiency gains hold when domain balance assumptions break down.

2. **Transition predictability test**: Measure domain transition entropy in the dataset and correlate with TAPE's effectiveness. If transitions are near-random, TAPE's learned transition embeddings may overfit without providing predictive value.

3. **State dimension scaling**: Systematically vary the DDSR state dimension (from 32 to 512) and measure the tradeoff between cross-domain context capture quality and parameter efficiency, particularly for datasets with more than 5 domains.