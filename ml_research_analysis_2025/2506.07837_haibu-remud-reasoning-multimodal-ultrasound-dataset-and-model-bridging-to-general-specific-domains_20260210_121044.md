---
ver: rpa2
title: 'HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to
  General Specific Domains'
arxiv_id: '2506.07837'
source_url: https://arxiv.org/abs/2506.07837
tags:
- ultrasound
- data
- reasoning
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal large language
  models (MLLMs) performing poorly in specialized domains like medical ultrasound
  due to a lack of domain-specific training data. To bridge this gap, the authors
  propose a novel automated pipeline that generates image-text reasoning quadruplets
  (image, question, thinking trace, answer) from domain-specific materials such as
  textbooks and guidelines.
---

# HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains

## Quick Facts
- **arXiv ID:** 2506.07837
- **Source URL:** https://arxiv.org/abs/2506.07837
- **Reference count:** 40
- **Primary result:** ReMUD-7B achieves 90.1% on UVQA-Diagnosis and 80.1% on USTQ-Knowledge benchmarks, outperforming general-domain MLLMs.

## Executive Summary
This paper addresses the challenge of multimodal large language models (MLLMs) performing poorly in specialized domains like medical ultrasound due to a lack of domain-specific training data. To bridge this gap, the authors propose a novel automated pipeline that generates image-text reasoning quadruplets (image, question, thinking trace, answer) from domain-specific materials such as textbooks and guidelines. Using this pipeline, they create the ReMUD dataset—over 45,000 reasoning and non-reasoning QA and VQA pairs in the ultrasound domain—and fine-tune Qwen2.5-VL-7B-Instruct to obtain ReMUD-7B. Experimental results show that ReMUD-7B outperforms general-domain MLLMs on ultrasound benchmarks, achieving 90.1% accuracy on UVQA-Diagnosis and 80.1% on USTQ-Knowledge. The authors will release the dataset, code, and model to advance research in specific-domain MLLMs.

## Method Summary
The authors propose an automated pipeline to generate domain-specific multimodal reasoning data. The process involves extracting images from PDFs using bounding box coordinates, segmenting them to remove interference (legends, text), and filtering illegitimate images with a binary classifier. Domain-specific knowledge is extracted from textbooks using layout analysis and processed into a knowledge graph. Question-answer pairs are generated by prompting large language models with knowledge graph nodes, while image-text pairs are created by prompting LLMs to describe extracted images. For reasoning tasks, chain-of-thought traces are added by prompting LLMs to elaborate on the reasoning process. The resulting ReMUD dataset contains over 45,000 quadruplets (image, question, thinking trace, answer) across four categories: QA reasoning/non-reasoning text, and VQA reasoning/non-reasoning image-text. The Qwen2.5-VL-7B-Instruct model is fine-tuned using LLaMA-Factory with supervised learning for 3 epochs.

## Key Results
- ReMUD-7B achieves 90.1% accuracy on UVQA-Diagnosis and 80.1% on USTQ-Knowledge benchmarks
- The model outperforms general-domain MLLMs on both ultrasound-specific benchmarks
- Dataset contains 45,376 samples across four reasoning/non-reasoning categories
- Fine-tuning completes in approximately 4 hours on a single NVIDIA A800 (80G) GPU

## Why This Works (Mechanism)
The approach works by addressing the fundamental data scarcity problem in specialized domains. General MLLMs trained on broad internet data lack the domain-specific reasoning patterns and knowledge structures required for medical ultrasound interpretation. By automatically generating reasoning quadruplets from authoritative domain sources (textbooks, guidelines), the model learns to connect visual features with domain-specific diagnostic reasoning. The inclusion of thinking traces teaches the model explicit reasoning patterns used by domain experts. Budget forcing during inference ensures adequate reasoning depth without excessive verbosity, optimizing the trade-off between reasoning quality and response efficiency.

## Foundational Learning
- **Multimodal reasoning patterns**: Why needed - Ultrasound diagnosis requires correlating visual features with clinical knowledge; Quick check - Model can correctly identify anatomical structures and relate them to diagnostic findings
- **Domain-specific knowledge representation**: Why needed - General MLLMs lack ultrasound-specific terminology and diagnostic criteria; Quick check - Model uses correct medical terminology and follows ultrasound diagnostic protocols
- **Chain-of-thought reasoning**: Why needed - Complex ultrasound interpretation requires step-by-step diagnostic reasoning; Quick check - Model generates coherent thinking traces that lead to correct conclusions
- **Image-text alignment**: Why needed - Ultrasound images must be correctly interpreted in clinical context; Quick check - Model accurately describes image features and connects them to clinical implications
- **Fine-tuning methodology**: Why needed - Adapting general MLLMs to specialized domains requires targeted training; Quick check - Model shows improved performance on domain-specific benchmarks after fine-tuning

## Architecture Onboarding

**Component Map:** PDF extraction -> Image segmentation/filtering -> Knowledge graph extraction -> LLM prompting (QA/VQA) -> Reasoning trace generation -> Dataset assembly -> SFT fine-tuning -> Inference with budget forcing

**Critical Path:** Automated data generation pipeline → ReMUD dataset creation → SFT fine-tuning of Qwen2.5-VL-7B-Instruct → Evaluation on domain benchmarks

**Design Tradeoffs:** The pipeline trades complete manual curation for automation speed, accepting some noise in exchange for scale. Reasoning traces add computational overhead but improve diagnostic accuracy. Budget forcing limits response length but ensures focused reasoning.

**Failure Signatures:** Poor image segmentation produces noisy visual features; inadequate knowledge extraction misses critical diagnostic criteria; insufficient thinking trace length leads to superficial reasoning; excessive trace length causes hallucinations or contradictions.

**First Experiments:**
1. Generate 100 sample quadruplets using the pipeline and manually verify quality against domain expert standards
2. Fine-tune Qwen2.5-VL-7B-Instruct on a small subset (1,000 samples) and evaluate on a held-out validation set
3. Test budget forcing thresholds by generating responses with varying think-trace length limits and measuring accuracy impact

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the automated quadruplet generation pipeline be effectively generalized to other specialized medical or technical domains beyond ultrasound?
  - **Basis in paper:** The Conclusion states that "Future work could explore applying this pipeline to more specific domains."
  - **Why unresolved:** The current study validated the method exclusively within the medical ultrasound domain using domain-specific PDFs and textbooks; its efficacy in other fields remains untested.
  - **What evidence would resolve it:** Successful replication of the pipeline and comparable performance gains when fine-tuning base models in distinct fields such as radiology, pathology, or non-medical technical disciplines.

- **Open Question 2:** How can the current data generation framework be adapted to process and incorporate video-text data?
  - **Basis in paper:** The Discussion notes that "data such as videos are also difficult to process, and we are still stuck in the generation of text-image data."
  - **Why unresolved:** The current pipeline relies on static bounding boxes and page-based extraction, which does not translate to temporal video data often required for comprehensive ultrasound diagnosis.
  - **What evidence would resolve it:** An extension of the method that successfully extracts reasoning pairs from video clips while maintaining temporal consistency and annotation accuracy.

- **Open Question 3:** What specific optimizations to test-time scaling methods can maximize reasoning accuracy without incurring the diminishing returns observed with simple budget forcing?
  - **Basis in paper:** The Conclusion suggests that "more efficient test-time scaling methods can be investigated to optimize the performance of models during inference."
  - **Why unresolved:** The experiments (Fig. 6) show that while extending thinking traces improves accuracy initially, performance eventually degrades if the trace length is extended too far.
  - **What evidence would resolve it:** Identification of an adaptive scaling algorithm that reliably identifies the optimal stopping point for reasoning traces to ensure peak accuracy on complex diagnostic tasks.

## Limitations
- Exact source materials and specific editions of textbooks/guidelines used for ReMUD generation are not specified, limiting reproducibility
- Critical implementation details for key components like the binary classifier for image legitimacy filtering and full prompt templates are omitted
- The dataset is predominantly Chinese (only 2,278 English samples out of 45,000+), potentially limiting broader applicability

## Confidence
- **High confidence** in reported benchmark results and comparative performance metrics
- **Medium confidence** in effectiveness of automated generation pipeline methodology
- **Low confidence** in ability to fully reproduce the exact ReMUD dataset due to unspecified source materials and generation parameters

## Next Checks
1. Verify the data generation pipeline by attempting to recreate a small subset of ReMUD samples using the partially shown prompt templates and GPT-4o/Gemini APIs, checking for consistency with the ShareGPT JSON format requirements
2. Test the budget forcing implementation by manually injecting "Wait" markers into short thinking traces and measuring the impact on reasoning quality and answer accuracy
3. Validate the image legitimacy filtering by implementing a binary classifier with similar architecture to the one described and testing its performance on a held-out set of ultrasound images from medical textbooks