---
ver: rpa2
title: Score-Based Training for Energy-Based TTS Models
arxiv_id: '2505.13771'
source_url: https://arxiv.org/abs/2505.13771
tags:
- ebms
- speech
- loss
- score
- delta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new score-based training approaches for
  energy-based models (EBMs) in text-to-speech (TTS) synthesis as alternatives to
  noise contrastive estimation (NCE). The first approach, sliced score matching (SSM),
  has been previously used with EBMs in other domains but not in TTS.
---

# Score-Based Training for Energy-Based TTS Models

## Quick Facts
- arXiv ID: 2505.13771
- Source URL: https://arxiv.org/abs/2505.13771
- Reference count: 0
- Key outcome: Score-based training methods (SSM and delta loss) outperform NCE for EBMs in TTS across objective and subjective metrics

## Executive Summary
This paper introduces two new score-based training approaches for energy-based models in text-to-speech synthesis: sliced score matching (SSM) and a novel delta loss function. These methods serve as alternatives to noise contrastive estimation (NCE) for training EBMs in TTS systems. The delta loss is specifically designed to promote log-likelihoods that are better suited for first-order inference schemes used by EBMs. Experiments on the LJSpeech dataset demonstrate that both score-based approaches consistently outperform NCE across multiple objective metrics including Mel cepstral distortion, SpeechBERTScore, and UTMOSv2. The delta loss shows particularly strong performance, achieving comparable results to SSM in objective evaluations while demonstrating better performance in subjective preference tests with fewer low scores and more high scores.

## Method Summary
The paper proposes two score-based training approaches for energy-based models in TTS. The first approach, sliced score matching (SSM), has been previously used with EBMs in other domains but represents a novel application to TTS. The second approach, delta loss, is a newly introduced objective function that explicitly aims to improve the quality of log-likelihoods for first-order inference methods. Both approaches are evaluated against the standard NCE training method. The delta loss is designed with a simple formulation yet shows strong empirical performance. The training process involves optimizing the EBM parameters to minimize either the SSM or delta loss objective, with generated samples evaluated using both objective metrics and subjective listening tests. The delta loss function is also theoretically connected to losses used in flow matching approaches, providing additional justification for its design.

## Key Results
- Score-based approaches (SSM and delta loss) consistently outperform NCE in Mel cepstral distortion, SpeechBERTScore, and UTMOSv2 metrics on LJSpeech
- Delta loss achieves comparable objective performance to SSM while showing superior subjective performance with more high scores and fewer low scores in preference tests
- Despite its simplicity, delta loss demonstrates strong performance, suggesting training objectives tailored to first-order inference may be more effective than general-purpose contrastive approaches
- The delta loss function is connected to losses used in flow matching approaches, providing theoretical grounding for its design

## Why This Works (Mechanism)
The paper demonstrates that score-based training objectives can provide more effective gradients for training energy-based models compared to contrastive approaches like NCE. The delta loss specifically targets the quality of log-likelihoods in a way that aligns with the first-order inference schemes typically used with EBMs. By optimizing the model to produce log-likelihoods that are more suitable for gradient-based sampling, the delta loss enables more stable and higher-quality inference. The connection to flow-matching losses suggests that the delta loss may be capturing similar principles about optimal training objectives for generative models that rely on iterative refinement during inference.

## Foundational Learning

**Energy-Based Models (EBMs)**: Generative models that define a probability distribution through an energy function.
*Why needed*: EBMs provide a flexible framework for modeling complex data distributions without requiring explicit likelihood computation.
*Quick check*: Verify understanding of how EBMs differ from explicit likelihood models like VAEs or normalizing flows.

**Noise Contrastive Estimation (NCE)**: A training method that distinguishes between data samples and noise samples.
*Why needed*: NCE provides a tractable way to train EBMs when computing the partition function is intractable.
*Quick check*: Confirm understanding of how NCE approximates the likelihood ratio without computing the partition function.

**Score Matching**: A training objective that minimizes the difference between model and data score functions.
*Why needed*: Score matching avoids computing the partition function while still providing useful gradients for training.
*Quick check*: Verify understanding of the relationship between score functions and gradients of log-probability.

**First-Order Inference**: Sampling methods that use gradient information to refine noisy samples.
*Why needed*: First-order methods are commonly used with EBMs for generation, making training objectives that support them particularly relevant.
*Quick check*: Confirm understanding of how Langevin dynamics or similar methods use gradients for sampling from EBMs.

## Architecture Onboarding

**Component map**: Text encoder -> Acoustic feature predictor (EBM) -> Waveform synthesis
**Critical path**: Input text → Encoder embeddings → EBM energy function → Inference sampling → Acoustic features → Vocoder → Audio output
**Design tradeoffs**: Score-based methods vs. contrastive training, simplicity vs. performance optimization, theoretical justification vs. empirical effectiveness
**Failure signatures**: Mode collapse, poor sample quality at low temperatures, unstable training with certain noise schedules
**First experiments**:
1. Train EBM with delta loss on LJSpeech and compare MCD scores against NCE baseline
2. Evaluate sample quality across different noise scales for delta loss-trained model
3. Compare inference speed and stability between delta loss and SSM approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Long-term stability and scalability to larger datasets remains untested
- Performance in multi-speaker and multi-style scenarios is unknown
- Delta loss behavior with different inference schedules and temperature scaling requires further investigation
- Theoretical justification lacks rigorous convergence proofs or quality bounds

## Confidence
High: Delta loss performance relative to NCE and SSM based on multiple objective and subjective metrics
Medium: Novelty of delta loss as a general-purpose EBM training objective given its relationship to flow-matching losses
Low: Theoretical guarantees for training stability and sample quality bounds

## Next Checks
1. Test delta loss on multi-speaker datasets to evaluate robustness to speaker identity variation and style control
2. Compare inference speed and quality across different noise schedules and temperature settings for each training method
3. Conduct ablation studies on delta loss hyperparameters (particularly λ and noise scale σ) to identify sensitivity and optimal configurations