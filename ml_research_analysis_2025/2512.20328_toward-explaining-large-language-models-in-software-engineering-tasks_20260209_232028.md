---
ver: rpa2
title: Toward Explaining Large Language Models in Software Engineering Tasks
arxiv_id: '2512.20328'
source_url: https://arxiv.org/abs/2512.20328
tags:
- code
- featureshap
- features
- feature
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents FeatureSHAP, a novel model-agnostic explainability
  framework for software engineering tasks. It addresses the lack of domain-specific
  explanations in large language models (LLMs) by attributing model outputs to high-level
  input features rather than individual tokens.
---

# Toward Explaining Large Language Models in Software Engineering Tasks

## Quick Facts
- arXiv ID: 2512.20328
- Source URL: https://arxiv.org/abs/2512.20328
- Reference count: 40
- Key outcome: FeatureSHAP, a model-agnostic explainability framework that attributes LLM outputs to high-level input features rather than individual tokens, outperforming random baselines and matching or exceeding LLM-as-an-attributor in software engineering tasks.

## Executive Summary
FeatureSHAP addresses the interpretability gap in large language models for software engineering by providing feature-level explanations rather than token-level attributions. The framework uses a three-step process: semantic feature partitioning via LLM-based or AST-based splitting, perturbation-based simulation to assess feature impact, and Shapley value computation to determine feature importance. Evaluated on code generation and summarization tasks, FeatureSHAP demonstrates superior performance in identifying irrelevant features and provides faithful, useful explanations validated through a user study with 37 participants.

## Method Summary
FeatureSHAP is a model-agnostic explainability framework that attributes LLM outputs to high-level input features rather than individual tokens. The method involves three core steps: feature splitting (using LLM-based segmentation for natural language and Tree-Sitter AST parsing for code), perturbation-based simulation (removing feature subsets and measuring output changes), and similarity-based attribution using Shapley values (with CodeBLEU for code and BERTScore for text). The framework was evaluated on BigCodeBench for code generation and CodeSearchNet for code summarization tasks, using Qwen2.5-Coder models and GPT-4.1-nano for explanation generation.

## Key Results
- FeatureSHAP assigns near-zero attribution scores to injected nonsensical features (median ~0), outperforming random baselines
- The framework matches or exceeds LLM-as-an-attributor in identifying irrelevant features while maintaining task performance
- User study with 37 participants found FeatureSHAP explanations to be both faithful and useful for understanding model decisions

## Why This Works (Mechanism)

### Mechanism 1: Semantic Feature Partitioning for Human-Aligned Attribution
- Claim: Grouping tokens into semantically meaningful features improves interpretability compared to token-level attribution.
- Mechanism: The splitter module decomposes inputs using domain-appropriate strategies—LLM-based segmentation for natural language prompts (identifying summary, description, signature sections) and Tree-Sitter AST parsing for source code (extracting method declarations and top-level statements). This creates feature partitions that align with how developers reason about code artifacts.
- Core assumption: Developers conceptualize code and prompts in terms of semantic units (e.g., requirements, constraints, code blocks) rather than individual tokens.
- Evidence anchors:
  - [abstract] "FeatureSHAP attributes model outputs to high-level input features rather than individual tokens"
  - [Section 3, p.6-7] Describes splitter functions σₙ (natural language) and σc (source code) producing feature sequences ⟨f₁,...,fₙ⟩
  - [corpus] Weak direct evidence; related papers discuss LLM-code interaction but not feature-level explainability
- Break condition: If features are incorrectly segmented (e.g., splitting coherent requirements across features), attributions may misrepresent actual model dependencies.

### Mechanism 2: Perturbation-Based Shapley Value Estimation
- Claim: Systematic feature removal and output comparison yields reliable importance scores for each feature.
- Mechanism: The modifier module removes features (μᵣ(f) = ⟨⟩), and Monte Carlo sampling approximates Shapley values across feature coalitions. The comparator measures output similarity between original and perturbed outputs using task-specific metrics (CodeBLEU for code, BERTScore for text). The Shapley engine aggregates similarity changes to compute per-feature attribution scores in [0,1].
- Core assumption: Shapley values from cooperative game theory validly represent feature contributions in LLM output generation.
- Evidence anchors:
  - [abstract] "three-step process: feature splitting, perturbation-based simulation, and similarity-based attribution using Shapley values"
  - [Section 3, p.7] "By analyzing how similarity changes when features are included or omitted in different combinations, the Shapley engine produces a set of attribution scores"
  - [corpus] Assumption: No corpus papers validate Shapley-based attribution for code tasks specifically
- Break condition: If features are highly correlated (e.g., docstring and signature depend on each other), Shapley attributions may distribute importance incorrectly across dependent elements.

### Mechanism 3: Task-Specific Similarity Metrics Ground Attribution in Domain Semantics
- Claim: Using code-aware (CodeBLEU) and semantic text (BERTScore) metrics produces more faithful attribution than generic similarity measures.
- Mechanism: CodeBLEU captures n-gram overlap, AST structure, and data-flow for code outputs. BERTScore uses contextual embeddings for natural language. These metrics provide the value function for the Shapley game, measuring how perturbed outputs deviate from originals.
- Core assumption: The chosen similarity metrics accurately capture meaningful differences in model outputs relevant to the task.
- Evidence anchors:
  - [Section 3, p.8] "For the former [code], we use CodeBLEU... For the latter [natural language], we adopt BERTScore"
  - [Section 7, p.20] Authors acknowledge metrics "may not fully capture behavioral equivalence" or "reward superficial overlap"
  - [corpus] No direct validation; related work focuses on task performance, not attribution fidelity metrics
- Break condition: If similarity metrics fail to distinguish functionally equivalent outputs from different ones (or vice versa), attribution scores will be misleading.

## Foundational Learning

- **Concept: Shapley Values and Cooperative Game Theory**
  - Why needed here: FeatureSHAP's attribution mechanism is grounded in Shapley values, which fairly distribute the "value" of a prediction among contributing features by averaging marginal contributions across all feature orderings.
  - Quick check question: Can you explain why Shapley values require evaluating all possible feature coalitions, and why Monte Carlo sampling approximates this efficiently?

- **Concept: Perturbation-Based Explainability**
  - Why needed here: FeatureSHAP infers feature importance by observing how outputs change when inputs are altered (features removed), requiring understanding of how perturbation strategies affect model behavior.
  - Quick check question: If removing feature f₁ causes output similarity to drop from 1.0 to 0.3, what does this suggest about f₁'s attribution?

- **Concept: Code-Aware Similarity Metrics (BLEU variants, AST-based)**
  - Why needed here: Attribution quality depends on accurately measuring output changes. CodeBLEU incorporates syntax and data-flow, unlike pure n-gram metrics, making it more suitable for code generation tasks.
  - Quick check question: Why might BLEU alone be insufficient for comparing generated code, and what does CodeBLEU add?

## Architecture Onboarding

- **Component map:**
  - Input → Splitter (LLM-based for text, Tree-sitter for code) → Feature sets
  - Feature sets → Modifier (feature removal) → Perturbed inputs
  - Perturbed inputs → Comparator (CodeBLEU/BERTScore) → Similarity scores
  - Similarity scores → Shapley Engine → Attribution scores per feature

- **Critical path:**
  1. Input prompt/code enters splitter → features extracted
  2. Monte Carlo sampling selects feature coalitions to perturb
  3. Model generates outputs for each perturbed input
  4. Comparator scores each output against original
  5. Shapley engine computes final attributions

- **Design tradeoffs:**
  - **Number of features vs. computational cost:** More features = finer attribution granularity but exponentially more coalition evaluations (2^|F|); Monte Carlo sampling with ratio < 1 trades precision for speed.
  - **Splitter complexity vs. interpretability:** LLM-based splitting is flexible but adds cost/variability; rule-based (AST) is deterministic but less adaptive.
  - **Similarity metric choice:** CodeBLEU captures syntax/semantics but is slower than simple BLEU; BERTScore requires embedding model inference.

- **Failure signatures:**
  - High attribution to clearly irrelevant features → check splitter segmentation quality, metric appropriateness
  - Zero or near-zero attributions to all features → verify comparator metric is sensitive to relevant differences
  - Inconsistent attributions across runs → Monte Carlo sampling variance; increase sampling ratio
  - Features with similar attribution despite obvious importance differences → possible feature correlation/collinearity

- **First 3 experiments:**
  1. **Baseline validation:** Run FeatureSHAP on prompts with injected nonsensical noise (as in RQ1). Verify noise features receive near-zero attribution (median ~0, mean < 0.03 per paper results).
  2. **Splitter ablation:** Compare LLM-based vs. simple sentence/line splitting on a sample of code generation prompts. Measure whether semantic splitting yields more stable/higher-quality attributions (assessed via user judgment).
  3. **Metric sensitivity test:** Substitute CodeBLEU with plain BLEU on code summarization task. Check if attribution distributions shift significantly, indicating metric influence on attributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FeatureSHAP be effectively adapted for code-to-code tasks like Automated Program Repair (APR) or code translation?
- Basis in paper: [explicit] The authors state that future work should explore "code-to-code scenarios such as automated program repair, refactoring, and code translation" by specializing the splitter and modifier components.
- Why unresolved: The current evaluation is restricted to bi-modal tasks (text-to-code and code-to-text). Code-to-code tasks require different splitting strategies (e.g., isolating buggy statements) and modification heuristics to maintain syntactic validity during perturbation.
- What evidence would resolve it: An instantiation of the framework for a code-to-code task, evaluated on benchmarks like HumanEval-Pack or repair datasets, showing that attributions correctly identify the buggy lines or translated logic.

### Open Question 2
- Question: Do high attribution scores from FeatureSHAP align with the internal causal mechanisms of the LLM?
- Basis in paper: [explicit] The paper suggests connecting the method to "mechanistic interpretability through causal interventions" to verify if input-level ablations correspond to internal activation patterns.
- Why unresolved: FeatureSHAP is model-agnostic and relies solely on input-output behavior. It is currently unknown if the features identified as "important" via perturbation actually drive the internal decision-making process (mechanistic validity) or are merely correlated with the output.
- What evidence would resolve it: A study correlating FeatureSHAP attributions with internal model states (e.g., activation patching) to confirm that high-attribution features trigger distinct internal pathways compared to low-attribution features.

### Open Question 3
- Question: How does the interdependence of code features (collinearity) impact the reliability of Shapley-based attributions?
- Basis in paper: [explicit/inferred] The authors list "multi-collinearity among correlated code features" as a threat to conclusion validity and propose "dependency-aware attribution" as a future extension.
- Why unresolved: Shapley values assume feature independence, but code artifacts (e.g., a function signature and its corresponding logic) are structurally and semantically linked. Current perturbation strategies might break this semantic cohesion, potentially distorting importance scores.
- What evidence would resolve it: An extension of the framework that incorporates interaction values or AST-aware masking, evaluated against synthetic datasets where ground-truth feature interdependence is known.

### Open Question 4
- Question: Does FeatureSHAP maintain its fidelity and usefulness across diverse programming languages?
- Basis in paper: [inferred] The threats to validity section notes the study focuses "on a single programming languages (i.e., Python)" and that "additional work is needed to extend our approach to other languages."
- Why unresolved: While the Tree-sitter parser supports many languages, the semantic structure of code (e.g., verbosity, boilerplate) varies. The current splitting heuristics and the LLM-based splitter might perform differently on languages like Java or C++.
- What evidence would resolve it: A replication of the user study and technical evaluation on non-Python benchmarks (e.g., CodeSearchNet in Java or Ruby) to verify consistent performance.

## Limitations
- "Sampling ratio to 0" specification is mathematically impossible for Monte Carlo approximation, suggesting exact computation or typo
- Filtering mechanism requiring exact output matches may eliminate all samples if LLM is sensitive to perturbations
- Baseline "LLM-as-an-attributor" cites unknown "GPT-5-mini" model, likely a typo or internal reference

## Confidence
- **FeatureSHAP Framework Design (High):** The three-step architecture (splitter → modifier → comparator → Shapley engine) is clearly specified with implementation details for both code and text tasks.
- **Attribution Quality Claims (Medium):** While the framework design is sound, the specific attribution quality improvements over baselines depend on correct implementation of the filtering and exact Shapley computation steps.
- **User Study Results (Medium):** The 37-participant study design is described, but replication would require the same code and summarization benchmarks, plus access to the FeatureSHAP tool.

## Next Checks
1. **Baseline Reproducibility:** Implement the exact noise injection and filtering procedure on a small subset of BigCodeBench. Verify that nonsensical features consistently receive near-zero attribution scores across multiple runs.
2. **Splitter Sensitivity Analysis:** Compare LLM-based vs. Tree-sitter splitting on 10 code generation prompts. Measure attribution score variance across 3 runs with different random seeds to assess splitter consistency.
3. **Metric Impact Test:** Run FeatureSHAP with CodeBLEU and plain BLEU on identical code summarization samples. Document whether attribution distributions differ significantly, particularly for features affecting code structure vs. surface-level tokens.