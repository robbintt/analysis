---
ver: rpa2
title: Adaptive, Robust and Scalable Bayesian Filtering for Online Learning
arxiv_id: '2505.07267'
source_url: https://arxiv.org/abs/2505.07267
tags:
- posterior
- page
- parameters
- methods
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis introduces Bayesian filtering as a principled framework
  for tackling sequential machine learning problems including online learning, prequential
  forecasting, and contextual bandits. It addresses three key challenges in applying
  Bayesian filtering to these problems: adaptivity to non-stationary environments,
  robustness to model misspecification and outliers, and scalability to high-dimensional
  parameter spaces of deep neural networks.'
---

# Adaptive, Robust and Scalable Bayesian Filtering for Online Learning

## Quick Facts
- arXiv ID: 2505.07267
- Source URL: https://arxiv.org/abs/2505.07267
- Reference count: 0
- Primary result: Novel Bayesian filtering framework addressing adaptivity, robustness, and scalability for online learning

## Executive Summary
This thesis presents a comprehensive framework for applying Bayesian filtering to online learning tasks, addressing three critical challenges: adaptivity to non-stationary environments, robustness to model misspecification and outliers, and scalability to high-dimensional neural networks. The work develops three main contributions - the BONE framework for adaptive filtering, the WoLF filter for robust updates, and scalable filtering methods using low-rank approximations. Theoretical analysis and extensive experiments demonstrate significant improvements over existing approaches across diverse tasks including electricity forecasting, classification with drift, and contextual bandits.

## Method Summary
The thesis develops Bayesian filtering methods that extend beyond standard approaches by incorporating mechanisms for handling non-stationarity, outliers, and high-dimensional parameters. The BONE framework conditions model dynamics on auxiliary variables like changepoint probabilities to adapt to environmental shifts. The WoLF filter replaces standard likelihoods with weighted versions that reduce outlier influence through adaptive noise scaling. For scalability, methods like LoFi and PULSE approximate posterior covariances using low-rank structures, enabling application to neural networks by exploiting their parameter space manifold properties. These methods are integrated into a unified filtering framework that can handle all three challenges simultaneously.

## Key Results
- BONE framework achieves state-of-the-art performance in electricity forecasting, successfully detecting and adapting to Covid lockdown-induced changes
- WoLF filter demonstrates provable robustness to outliers through bounded Posterior Influence Functions, outperforming standard filters in contaminated datasets
- Scalable filtering methods enable Bayesian updates for neural networks, with LoFi achieving competitive accuracy while reducing computational complexity from O(D³) to O(d³) where d≪D
- Integration of all three components shows improved performance across diverse tasks including regression, classification, and contextual bandits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The system adapts to non-stationary environments (abrupt and gradual changes) by conditioning the model parameter dynamics on an auxiliary variable (e.g., "runlength" or "changepoint probability").
- **Mechanism**: The framework (BONE) maintains a posterior over an auxiliary variable $\psi_t$ alongside the parameters $\theta_t$. If the probability of a changepoint increases (detected via the marginal predictive likelihood ratio), the conditional prior for $\theta_t$ is modified—either reset to a base prior (for abrupt changes) or adjusted via an Ornstein-Uhlenbeck process (for gradual drift)—before the standard Bayes update.
- **Core assumption**: The data generating process consists of distinct regimes or drifts where past data becomes less relevant.
- **Evidence anchors**:
  - [Chapter 3, Eq 3.6]: Defines the predictive estimation as a weighted sum over auxiliary variables.
  - [Chapter 3, Sec 3.3.3]: Describes the specific hybrid method RL[1]-OUPR* which mixes hard resets with drift.
- **Break condition**: If the "hazard rate" (prior probability of a changepoint) is misspecified, the model may over-segment or fail to reset.

### Mechanism 2
- **Claim**: The system achieves robustness to outliers by reducing the influence of unlikely observations on the posterior update, a method the authors prove is outlier-robust.
- **Mechanism**: The "Weighted Observation Likelihood Filter" (WoLF) replaces the standard log-likelihood with a weighted version. Practically, this scales the observation noise covariance $R_t$ by the inverse square of a weighting function $W(y, \hat{y})$. If the prediction error is large, $W$ decreases, inflating the effective noise and reducing the Kalman gain.
- **Core assumption**: Outliers manifest as large deviations in the observation space relative to the predicted mean.
- **Evidence anchors**:
  - [Chapter 4, Eq 4.3]: Shows the loss function scaling.
  - [Chapter 4, Theorem 4.5]: Explicitly proves the boundedness of the Posterior Influence Function (PIF) under WoLF.
- **Break condition**: If the threshold parameter $c$ in the weighting function (e.g., IMQ) is set too low, the filter may ignore valid but surprising data points ("detect-and-reject" failure).

### Mechanism 3
- **Claim**: The system scales to high-dimensional neural networks by restricting the posterior covariance (or precision) to a low-dimensional manifold, avoiding $O(D^3)$ matrix inversions.
- **Mechanism**: Methods like "LoFi" approximate the posterior precision matrix as Diagonal plus Low-Rank (DLR), while "PULSE" projects hidden layer weights onto a lower-dimensional affine subspace found via SVD. This reduces the computational complexity to depend primarily on the low-rank dimension $d$ rather than the full parameter count $D$.
- **Core assumption**: The parameter space of neural networks exhibits a "lottery ticket" or manifold structure where essential dynamics occur in a lower-dimensional subspace.
- **Evidence anchors**:
  - [Chapter 5, Table 5.1]: Compares time/memory complexity of subspace vs full-rank methods.
  - [Chapter 5, Sec 5.3.2]: Derives the DLR update for the LoFi method.
  - [corpus]: Neighbors confirm that "high-dimensional nonlinear filtering" is a significant challenge where Gaussian-based methods struggle, supporting the need for such approximations.
- **Break condition**: If the rank $d$ is too small to capture the curvature of the loss landscape, the filter may diverge or underfit.

## Foundational Learning

- **Concept**: **State-Space Models (SSM) & Kalman Filtering**
  - **Why needed here**: The entire thesis treats online learning as a filtering problem where model weights are a latent state evolving over time. You must understand the predict-update cycle (Gaussian propagation) to follow the derived algorithms.
  - **Quick check question**: If I increase the process noise $Q_t$ in a Kalman filter, does my uncertainty grow faster or slower in the predict step?

- **Concept**: **Generalised Bayes & Loss Functions**
  - **Why needed here**: The robustness mechanism (WoLF) is not derived from a strict probabilistic likelihood but from a generalized loss function. Distinguishing between "true likelihood" and "loss-based pseudo-likelihood" is key to understanding why the update is robust.
  - **Quick check question**: Does a generalized Bayes update guarantee a normalized probability distribution without a scaling constant?

- **Concept**: **Woodbury Matrix Identity**
  - **Why needed here**: The scalability mechanisms (LoFi, Subspace) rely heavily on inverting matrices of the form $(A + UCV)^{-1}$. The Woodbury identity is the mathematical engine that allows converting $O(D^3)$ operations into $O(d^3)$ operations.
  - **Quick check question**: How does the Woodbury identity allow you to invert a covariance matrix updated with a rank-$k$ outer product efficiently?

## Architecture Onboarding

- **Component map**: Streaming data $(x_t, y_t)$ -> Auxiliary Logic (BONE) -> Prior Selector -> Robust Core (WoLF) -> Scalable Engine (LoFi/Subspace) -> Posterior mean $\mu_t$ and uncertainty $\Sigma_t$

- **Critical path**:
  1.  **Warmup**: Run SGD to collect iterates; perform SVD to find the projection matrix $A$ (Sec 5.1.2).
  2.  **Runtime**: Calculate prediction error $\to$ Determine weight $W_t$ $\to$ Update DLR precision matrix $\to$ Apply gain to state $\mu_t$.

- **Design tradeoffs**:
  - **Rank $d$ vs. Accuracy**: Higher rank captures more uncertainty but increases computational cost linearly/quadratically.
  - **Robustness Threshold $c$**: Setting the weighting function threshold determines sensitivity to outliers vs. responsiveness to genuine distribution shifts.
  - **Adaptivity**: The choice of auxiliary variable (e.g., Runlength vs. Changepoint Probability) dictates whether the system assumes abrupt "reset" dynamics or smooth "drift" dynamics.

- **Failure signatures**:
  - **Covariance Collapse**: If the filter becomes overconfident (low variance), it stops adapting to new data.
  - **Over-segmentation**: If the changepoint hazard rate is too high or robustness is low, the model constantly resets parameters, preventing convergence (Sec 3.5.3).
  - **Numerical Instability**: Precision matrix updates (inversion) can degrade if the low-rank assumption is violated severely.

- **First 3 experiments**:
  1.  **Toy 2D Tracking (Sec 4.7.1)**: Implement WoLF with a linear Kalman Filter. Inject outliers (mixture model) to verify that the robust version tracks the true state while the standard KF diverges.
  2.  **Electricity Forecasting (Sec 3.5.1)**: Implement the BONE framework with RL[1]-OUPR*. Test against the Covid lockdown changepoint to verify the model resets/re-adapts effectively compared to static baselines.
  3.  **Fashion MNIST (Sec 5.5.1)**: Implement LoFi on a small CNN. Compare test accuracy and runtime against a full-rank EKF (if feasible) or Adam to demonstrate the trade-off between uncertainty quantification and speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed scalable filtering methods (LoFi, PULSE) be effectively applied to complex architectures like Transformers and Graph Neural Networks (GNNs) in temporal settings?
- Basis in paper: [explicit] Chapter 6 states, "In particular, in future work, we would like to evaluate the proposed methods on novel architectures such as the Transformer architecture... or Graph Neural Networks... in temporal settings."
- Why unresolved: The empirical evaluation in Chapter 5 is restricted to Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs like LeNet), without testing on attention-based or graph-structured models.
- What evidence would resolve it: Experimental results demonstrating the performance (accuracy, computational speed, memory usage) of LoFi or PULSE on temporal tasks using Transformer or GNN backbones compared to standard baselines.

### Open Question 2
- Question: How can the BONE framework and scalable filters be integrated into Reinforcement Learning (RL) algorithms to handle non-stationarity arising from agent-environment interactions?
- Basis in paper: [explicit] Chapter 6 identifies "reinforcement learning problems, where agents can experience non-stationarity even in static environments" as a key area for future work.
- Why unresolved: The thesis focuses on supervised tasks (forecasting, classification) and contextual bandits. It does not address the control loop complexities of RL where the agent's policy changes induce non-stationarity in the data distribution.
- What evidence would resolve it: A demonstration of an RL agent using the BONE framework for belief updates, showing improved adaptation and regret minimization in standard non-stationary RL benchmarks compared to static baselines.

### Open Question 3
- Question: Does using a fully Bayesian predictive distribution (marginalizing over weights) improve performance over the current MAP point-estimate approach in the online learning experiments?
- Basis in paper: [explicit] Section 3.5.1 states, "For a fully Bayesian treatment of neural network predictions, see Immer et al. (2021); we leave the implementation of these approaches for future work."
- Why unresolved: In the neural network experiments (e.g., electricity forecasting), the author uses the posterior mean (a point estimate) for predictions to manage computational complexity, rather than performing a full Bayesian marginalization.
- What evidence would resolve it: A comparative analysis showing the trade-off between computational cost and predictive performance (e.g., log-likelihood) when sampling from the variational posterior versus using the point estimate in the prequential forecasting tasks.

### Open Question 4
- Question: Can the structural hyperparameters of the proposed methods (e.g., subspace dimension, changepoint hazard rate, reset threshold) be adapted in a fully online manner to remove the dependency on a fixed warmup phase?
- Basis in paper: [inferred] Section 3.5 notes that "Each experiment consists of a warmup period where the hyperparameters are chosen," implying these critical parameters are currently fixed offline.
- Why unresolved: The BONE framework and scalable methods rely on fixed values (like the subspace dimension $d$ or hazard rate $\kappa$) determined during warmup. In highly non-stationary environments, optimal values for these parameters may shift over time.
- What evidence would resolve it: A modification of the algorithms where these hyperparameters are treated as latent variables or optimized via online gradient descent, demonstrating stable performance without the initial warmup period.

## Limitations

- Scalability mechanisms depend heavily on the assumption of low-dimensional manifold structure in neural network parameter spaces, which may not hold universally across architectures
- Robustness parameter c requires careful tuning and lacks a principled selection method, potentially leading to either over-sensitivity to outliers or failure to detect genuine shifts
- Changepoint detection through marginal predictive likelihood ratios may miss subtle or gradual drifts where likelihood changes slowly
- Warmup phase requirement for scalability methods limits practical applicability in truly streaming scenarios

## Confidence

**High Confidence Claims:**
- The theoretical analysis of WoLF's robustness properties (Posterior Influence Function bounds) is mathematically rigorous and well-supported
- The computational complexity improvements from low-rank approximations are well-established through the Woodbury matrix identity
- Empirical improvements over baselines in the tested scenarios (electricity forecasting, outlier-robust regression) are clearly demonstrated

**Medium Confidence Claims:**
- The general applicability of the BONE framework across different types of non-stationarity (abrupt vs. gradual) is supported by experiments but would benefit from more extensive testing across diverse domains
- The assumption about neural network parameter manifold structure is reasonable given the "lottery ticket hypothesis" literature, but its universality is not proven

**Low Confidence Claims:**
- The specific values and tuning procedures for parameters like the robustness threshold c and the low-rank dimension d are not systematically derived and may not transfer well to new domains
- The warmup phase requirement for scalability methods may limit practical applicability in truly streaming scenarios

## Next Checks

1. **Robustness Parameter Sensitivity**: Systematically vary the threshold parameter c in WoLF across different outlier contamination levels to determine if a principled selection method can be derived. Test on both synthetic data (known contamination) and real-world datasets with potential outliers.

2. **Scalability Manifold Validation**: For different neural network architectures (CNNs, Transformers, MLPs), measure the actual dimensionality of the parameter subspace that captures most of the training dynamics. Compare this to the rank d chosen in the LoFi method to validate whether the low-rank assumption holds universally or is architecture-dependent.

3. **Adaptation Detection Limits**: Design controlled experiments with gradual, subtle drifts (e.g., slowly changing linear coefficients) to test the detection limits of the BONE framework. Measure the detection delay and accuracy degradation as a function of drift magnitude to establish when the framework succeeds or fails.