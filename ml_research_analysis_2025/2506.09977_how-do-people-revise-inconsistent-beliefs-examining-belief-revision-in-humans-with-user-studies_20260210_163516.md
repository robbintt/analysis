---
ver: rpa2
title: How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans
  with User Studies
arxiv_id: '2506.09977'
source_url: https://arxiv.org/abs/2506.09977
tags:
- then
- fact
- type
- people
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how people revise inconsistent beliefs,
  focusing on the role of explanations in human belief revision. Through three user
  studies, it shows that people consistently prefer explanation-based revisions, often
  discarding or modifying conditional rules rather than categorical facts.
---

# How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies

## Quick Facts
- arXiv ID: 2506.09977
- Source URL: https://arxiv.org/abs/2506.09977
- Reference count: 6
- Primary result: Humans prefer explanation-based belief revision, often modifying conditional rules rather than categorical facts

## Executive Summary
This paper investigates how humans revise inconsistent beliefs, revealing that people consistently prefer explanation-based revisions that often result in broader changes than classical minimal consistency restoration requires. Through three user studies, the research demonstrates that when faced with contradictions, participants generate or accept explanations that lead them to discard or modify conditional rules rather than categorical facts. This preference persists across different types of scenarios, whether explanations are self-generated or provided, suggesting that AI systems designed to model human reasoning should incorporate explanation-based approaches to belief revision.

## Method Summary
The study conducted three user experiments on Prolific with 62, 60, and 60 participants respectively. Participants encountered nine scenarios across three problem types: Type I (1 rule + 1 fact), Type II (2 rules + 1 fact), and Type III (2 rules + 1 fact with complex epistemic input). Experiment 1 collected self-generated explanations for inconsistencies, Experiment 2 provided explanations from Experiment 1 and asked for revision decisions, and Experiment 3 used grounded scenarios with multiple concrete instances. The coding scheme classified revisions as non-minimal (conditional rule changes) vs minimal (fact changes), with statistical analysis via Wilcoxon tests and effect sizes via Cohen's d.

## Key Results
- 83.37% of participant explanations implied non-minimal revisions (p ≈ 2.96 × 10⁻⁵⁰)
- When provided with explanations, participants maintained the same non-minimal revision preference
- Type III scenarios showed 64.20% non-minimal revisions with participants making an average of 3.81 changes—nearly twice the minimum required
- Across all experiments, 89% of explanations were classifiable into revision categories

## Why This Works (Mechanism)

### Mechanism 1: Explanation-Guided Non-Minimal Revision
When humans encounter belief inconsistencies, they prefer explanation-based revisions that often result in broader changes than minimal consistency restoration requires. Humans generate or receive explanations (often "disabling conditions" for conditionals), which lead them to discard or modify general rules rather than categorical facts. Since rules affect multiple inferences, this produces non-minimal changes across the belief system.

### Mechanism 2: Disabling Condition Generation for Conditional Rules
Humans naturally generate disabling conditions for conditional generalizations when faced with contradictions, preferring to modify rules rather than reject facts. When a conditional ("If X then Y") conflicts with observed outcomes, participants construct conditions under which the conditional fails (e.g., "If a person has metabolic disorders, then a sugary drink may not provide energy"). This preserves the fact while modifying the rule.

### Mechanism 3: Complexity-Scaling Revision Scope
As inconsistency complexity increases (more rules/facts involved), humans make progressively more changes beyond the minimum required for consistency. Increased problem complexity triggers broader explanatory reasoning, causing participants to revise multiple beliefs rather than isolated minimal sets.

## Foundational Learning

- **Concept: Belief Revision Postulates (AGM Framework)**
  - Why needed here: The paper positions its findings against classical belief revision theory; understanding success, minimality, and recovery postulates is necessary to grasp why human behavior diverges.
  - Quick check question: Can you explain why a "minimal" revision in AGM theory might differ from what humans consider minimal?

- **Concept: Conditional vs. Categorical Statements in Knowledge Representation**
  - Why needed here: The experiments systematically manipulate conditionals (R1, R2: "If X then Y") and facts (F1: "X is true"); distinguishing these is essential for coding revision types.
  - Quick check question: Given a rule "If A then B" and a fact "A occurred," which belief would classical kernel revision treat as equally valid to reject?

- **Concept: Disabling Conditions in Cognitive Psychology**
  - Why needed here: The paper's central finding hinges on humans generating disabling conditions; understanding this concept explains why non-minimal revisions are cognitively natural.
  - Quick check question: What is a disabling condition for the rule "If someone studies hard, they pass the exam"?

## Architecture Onboarding

- **Component map:** [Inconsistency Detection] → [Explanation Generator/Retriever] → [Belief Modifier] → [Updated Knowledge Base] → [Disabling Condition Templates]

- **Critical path:**
  1. Detect inconsistency between new input and existing beliefs
  2. Generate or retrieve explanation (disabling condition preferred over fact rejection)
  3. Identify which conditional(s) the explanation modifies
  4. Propagate changes to all grounded instances of affected conditionals
  5. Update belief state with non-minimal revision

- **Design tradeoffs:**
  - **Minimality vs. Human Alignment**: Classical minimal revisions may conflict with human expectations; non-minimal revisions align better but increase computational cost and information loss.
  - **Self-Generated vs. Provided Explanations**: Self-generation requires explanation generation modules; provided explanations require trust-establishment mechanisms.
  - **Granularity**: Abstract rules enable efficient representation; grounded instances improve specificity but increase revision scope.

- **Failure signatures:**
  - Minimal-only revisions that confuse users ("Why did the system only change that one fact?")
  - Excessive revisions that erode model utility (too many rules discarded)
  - Explanation rejection if trust signals are weak (Experiment 2 addressed this by stating explanations were "from a trustworthy source")

- **First 3 experiments:**
  1. Replicate Experiment 1 with your domain-specific conditionals; measure proportion of rule-based vs. fact-based explanations generated.
  2. Test provided explanations varying trust signals (high/low/no attribution) to identify when users accept external explanations.
  3. Scale to your target complexity level (Type I/II/III equivalent) and measure average changes; compare against minimally required changes to quantify non-minimality gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can classical belief revision postulates and operators be modified to formally model the human preference for non-minimal, explanation-based changes?
- Basis in paper: [explicit] The authors state that the findings "call for more research to be focused in modeling human BR, in terms of both theory, e.g., via analyses of existing and novel postulates and operators."
- Why unresolved: Current classical theory assumes minimal information loss, whereas this study provides empirical evidence that humans consistently prefer non-minimal revisions.
- What evidence would resolve it: A formal belief revision framework containing novel postulates that successfully predict the non-minimal revision behaviors observed in the user studies.

### Open Question 2
- Question: What specific computational architectures or algorithms are required to implement an explanation-based belief revision framework that aligns with human cognitive processes?
- Basis in paper: [explicit] The paper notes that "implementing such mechanisms in AI systems is not a trivial task" and explicitly states, "In future work, we plan to develop such an explanation-based BR framework."
- Why unresolved: The paper establishes *that* humans use explanations to revise beliefs, but does not propose the computational logic for an AI system to replicate this process.
- What evidence would resolve it: The development and validation of an AI agent that utilizes explanation-based operators to replicate human revision patterns in belief modeling tasks.

### Open Question 3
- Question: Does the preference for explanation-based revision persist when conditional rules are of low plausibility or abstract nature, rather than the high-plausibility everyday scenarios tested?
- Basis in paper: [inferred] The methodology section notes that "conditional statements in all problems were selected to be highly plausible." It is unclear if the cognitive preference for revising rules over facts is a general principle or dependent on the agent's prior confidence in the rule.
- Why unresolved: The study controlled for plausibility to ensure interpretability, leaving the interaction between rule plausibility and revision strategy unexplored.
- What evidence would resolve it: A follow-up user study using the same Type I-III structures with low-plausibility or abstract logical rules to see if the ratio of non-minimal to minimal revisions shifts significantly.

## Limitations
- Artificial experimental scenarios may not capture real-world belief revision complexity
- Findings may be sensitive to how explanations are framed and presented
- Coding scheme reliability lacks detailed metrics (only 89% inter-rater agreement mentioned)

## Confidence

- **High confidence**: Human preference for explanation-based (non-minimal) revisions over minimal consistency restoration; robustness across different scenario types.
- **Medium confidence**: The disabling condition generation mechanism and its prevalence in human reasoning; the complexity-scaling effect on revision scope.
- **Low confidence**: The generalizability of these findings to high-stakes domains (medical, legal) and the long-term stability of non-minimal revisions in dynamic environments.

## Next Checks
1. Test the non-minimality preference in a domain with higher stakes (e.g., medical diagnosis scenarios) to assess robustness across contexts.
2. Conduct a longitudinal study tracking how non-minimal revisions affect future belief updates and reasoning patterns over time.
3. Compare human performance against AGM-compliant minimal revision systems in controlled settings to quantify the trade-off between human alignment and computational efficiency.