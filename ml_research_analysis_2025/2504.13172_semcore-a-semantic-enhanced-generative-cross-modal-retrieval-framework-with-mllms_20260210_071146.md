---
ver: rpa2
title: 'SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with
  MLLMs'
arxiv_id: '2504.13172'
source_url: https://arxiv.org/abs/2504.13172
tags:
- retrieval
- generative
- semantic
- cross-modal
- identifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemCORE, a semantic-enhanced generative cross-modal
  retrieval framework that addresses limitations in existing generative methods. The
  approach constructs structured natural language identifiers (SID) combining clustering-derived
  global IDs with keyword-extracted lexical IDs, then employs a generative semantic
  verification strategy for fine-grained discrimination.
---

# SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs

## Quick Facts
- **arXiv ID:** 2504.13172
- **Source URL:** https://arxiv.org/abs/2504.13172
- **Reference count:** 40
- **Primary result:** Achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets.

## Executive Summary
This paper introduces SemCORE, a semantic-enhanced generative cross-modal retrieval framework that addresses limitations in existing generative methods. The approach constructs structured natural language identifiers (SID) combining clustering-derived global IDs with keyword-extracted lexical IDs, then employs a generative semantic verification strategy for fine-grained discrimination. SemCORE is the first framework to simultaneously handle both text-to-image and image-to-text retrieval tasks within the generative paradigm. Extensive experiments show the framework achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets, while matching or exceeding traditional methods for image-to-text retrieval.

## Method Summary
SemCORE constructs structured natural language identifiers (SID) by combining a Global ID from K-Means clustering on image captions with a Lexical ID from KeyBERT-extracted keywords, then uses constrained beam search decoding for initial retrieval. A generative semantic verification (GSV) step employs the MLLM to perform fine-grained discrimination among top candidates. The framework fine-tunes the InternVL2.5 MLLM backbone on <query, SID> pairs for both text-to-image and image-to-text tasks, with hardware requirements of 2x NVIDIA A40 GPUs using DeepSpeed framework.

## Key Results
- Achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets
- Matches or exceeds traditional methods for image-to-text retrieval
- Demonstrates the effectiveness of combining clustering-derived global IDs with keyword-extracted lexical IDs

## Why This Works (Mechanism)

### Mechanism 1: Structured Natural Language Identifier (SID)
- **Claim:** Representing retrieval targets with structured natural language identifiers aligns with MLLM pre-training and improves generation over numeric codes.
- **Mechanism:** A two-part identifier is constructed: a Global ID from K-Means clustering on image captions (macroscopic semantics) and a Lexical ID from KeyBERT-extracted keywords (fine-grained semantics), deduplicated to ensure diversity.
- **Core assumption:** MLLMs generate meaningful identifiers more effectively when they are semantically rich natural language phrases rather than arbitrary numeric tokens.
- **Evidence anchors:** [abstract] Claims SID "effectively aligns target identifiers with generative models optimized for natural language comprehension and generation." [Section 3.2] Argues numeric-based identifiers "fail to fully exploit the linguistic strengths of generative models."
- **Break condition:** The mechanism's advantage would vanish if MLLMs were not primarily pre-trained on natural language, or if synonym ambiguity in lexical IDs creates excessive retrieval conflicts.

### Mechanism 2: Generative Semantic Verification (GSV)
- **Claim:** A secondary MLLM-based verification step improves fine-grained discrimination among top candidates, replacing a simple random suffix.
- **Mechanism:** After initial candidate retrieval via SID, a prompt containing the query and candidate images with their Lexical IDs is fed to the MLLM, which performs a comparative reasoning task to select the best match.
- **Core assumption:** The MLLM possesses a latent fine-grained cross-modal alignment capability that is more effectively engaged by a direct comparison prompt than by a single-shot identifier generation.
- **Evidence anchors:** [abstract] States the GSV strategy is for "enabling fine-grained target discrimination." [Section 3.3] Describes using the MLLM for "precise discrimination among semantically similar candidates."
- **Break condition:** The mechanism becomes impractical if the number of top candidates required for high recall is too large, making the verification step computationally prohibitive.

### Mechanism 3: Unified Bidirectional Retrieval
- **Claim:** A single generative framework can handle both text-to-image (T2I) and image-to-text (I2T) retrieval.
- **Mechanism:** The task is unified as generating a structured natural language identifier conditioned on a query, regardless of whether the query is text (T2I) or an image (I2T). The same MLLM backbone is trained for both.
- **Core assumption:** A single MLLM can learn effective mappings from both textual and visual inputs to a shared identifier space without task interference.
- **Evidence anchors:** [abstract] Claims SemCORE is the "first framework to simultaneously consider both" tasks. [Table 2] Reports competitive I2T performance against traditional methods.
- **Break condition:** Performance degrades if there are fundamental conflicts in the optimal identifier structure or generation process between the T2I and I2T tasks.

## Foundational Learning

- **Concept: Generative Retrieval Paradigm**
  - **Why needed here:** This is the paper's core departure from traditional embedding-based methods. Understanding that retrieval is re-framed as an autoregressive generation task (predicting an identifier) is fundamental.
  - **Quick check question:** How does a generative retrieval model produce a result without calculating a similarity score? (Answer: It generates the target's unique identifier token-by-token.)

- **Concept: Constrained Decoding (Trie Search)**
  - **Why needed here:** An MLLM cannot be allowed to generate arbitrary text; it must output a valid identifier from a fixed corpus. This mechanism guarantees validity.
  - **Quick check question:** Why is a standard language model decoding strategy insufficient for this task? (Answer: It could generate fluent text that does not correspond to any item in the database.)

- **Concept: MLLM as a Cross-Modal Knowledge Base**
  - **Why needed here:** The model must memorize the associations between queries and structured identifiers. The approach treats the MLLM's weights as a differentiable index.
  - **Quick check question:** What is the practical implication of "memorizing" the corpus in the model weights? (Answer: The model must be retrained or updated when the target corpus changes significantly.)

## Architecture Onboarding

- **Component map:** MLLM Backbone (InternVL2.5) -> Constrained Decoder (Trie-based beam search) -> Initial Candidate Set -> GSV Module (MLLM with verification prompt) -> Final Ranked List
- **Critical path:** SID construction (offline data processing) -> Model fine-tuning on <query, SID> pairs -> Inference with constrained decoding -> Optional GSV reranking
- **Design tradeoffs:**
  - Lexical ID Length: Shorter IDs lack semantic detail; longer IDs introduce more noise and potential synonym conflicts (optimal found at length 4)
  - Cluster Size: Affects macroscopic grouping; performance is generally stable but sensitive when Lexical ID is very short
  - GSV Overhead: The GSV step improves R@1 but adds inference latency and compute cost per query
- **Failure signatures:**
  - Synonym Ambiguity: Generating a valid SID that is semantically close but not the ground truth match
  - Identifier Collision: Different images having identical SIDs, leading to unavoidable retrieval errors
  - Invalid Generation: Outputting text not in the Trie, which indicates a failure in the constrained decoding logic
- **First 3 experiments:**
  1. Reproduce SID Construction: Build the Global and Lexical IDs for a small dataset (e.g., Flickr30K subset) to verify the deduplication and clustering logic
  2. Baseline Generation: Fine-tune the MLLM to generate SIDs and evaluate R@1 without the GSV step to isolate the contribution of the identifier design
  3. GSV Ablation: Add the GSV step on top of the baseline model and measure the improvement in R@1 versus the added inference time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative retrieval frameworks be adapted to handle dynamic target corpora where items are frequently added or removed, without requiring costly model retraining?
- Basis in paper: [explicit] The Conclusion states that current methods rely on LLMs "memorizing static mappings," and explicitly identifies improving "generalization capacity" for "dynamic" real-world sets as a critical future direction.
- Why unresolved: The current SemCORE framework operates on fixed datasets (Flickr30K, MS-COCO) where identifiers are pre-constructed; the architecture does not inherently support incremental updates to the identifier trie or model knowledge.
- What evidence would resolve it: A study demonstrating SemCORE's performance stability when new images are introduced to the corpus without fine-tuning, or the proposal of an efficient identifier update mechanism.

### Open Question 2
- Question: Can structured natural language identifiers (SID) be optimized to prevent the performance degradation in higher-rank metrics (e.g., R@5) caused by synonym ambiguity?
- Basis in paper: [inferred] Section 4.2.1 notes that SemCORE underperforms on R@5 compared to the baseline AVG, attributing this to "synonym ambiguity" inherent in natural language expressions that confuses the generative process.
- Why unresolved: While the paper introduces deduplication to enhance distinction, the fundamental reliance on natural language keywords (Lexical IDs) introduces semantic noise that the constrained decoding may not fully filter out in top-K lists.
- What evidence would resolve it: Experiments varying the semantic granularity of the Lexical ID or introducing a disambiguation module, showing recovered R@5 scores while maintaining the high R@1 performance.

### Open Question 3
- Question: How does the computational efficiency and memory footprint of the constrained decoding algorithm (Trie structure) scale when applied to web-scale retrieval corpora (e.g., millions or billions of targets)?
- Basis in paper: [inferred] The experiments are conducted on relatively small datasets (approx. 30k to 120k images). The authors note that constrained decoding restricts the generation space, but do not analyze the latency or memory costs of loading and searching a massive Trie for web-scale indices.
- Why unresolved: The complexity of maintaining a valid token Trie grows with the corpus size, potentially creating a bottleneck that the current small-scale experiments do not reveal.
- What evidence would resolve it: A scaling analysis reporting inference latency and GPU memory usage for the constrained decoding step as the target corpus size increases by orders of magnitude.

## Limitations

- Critical Reproducibility Gap: The most significant barrier to reproducing this work is the absence of a critical hyperparameter: the learning rate for model fine-tuning.
- Potential Ambiguity in SID Construction: The exact embedding features used for the K-Means clustering are not explicitly stated, creating ambiguity in the identifier corpus construction.
- Computational Overhead of GSV: The Generative Semantic Verification step adds significant computational overhead without a detailed analysis of the trade-off between R@1 improvement and added latency.

## Confidence

- **High Confidence:** The core innovation of using structured natural language identifiers (SIDs) for generative cross-modal retrieval is well-articulated and the mechanism is sound.
- **Medium Confidence:** The reported performance improvements are based on specific experimental setups, but the absence of the learning rate and potential ambiguity in SID construction prevent full validation.
- **Low Confidence:** The claim of being the "first framework to simultaneously consider both" tasks is based on a survey of related work and would require a comprehensive review of the entire generative retrieval literature for absolute validation.

## Next Checks

1. Contact the authors to obtain the exact learning rate and scheduler details used for fine-tuning the InternVL2.5 backbone.
2. Reproduce the SID construction pipeline and evaluate the MLLM's generation performance (R@1) without the GSV step to isolate the contribution of the structured identifier design.
3. Implement the GSV step and measure the per-query inference time to calculate the trade-off between R@1 improvement and added computational cost.