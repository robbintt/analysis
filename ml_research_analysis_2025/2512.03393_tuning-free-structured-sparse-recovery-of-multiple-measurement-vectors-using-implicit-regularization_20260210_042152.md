---
ver: rpa2
title: Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using
  Implicit Regularization
arxiv_id: '2512.03393'
source_url: https://arxiv.org/abs/2512.03393
tags:
- gradient
- implicit
- regularization
- recovery
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a tuning-free approach for structured sparse
  recovery of multiple measurement vectors (MMV) using implicit regularization from
  overparameterization. The method factorizes the target matrix into components that
  decouple shared row-support from individual vector entries, applying gradient descent
  to a standard least-squares objective.
---

# Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization

## Quick Facts
- arXiv ID: 2512.03393
- Source URL: https://arxiv.org/abs/2512.03393
- Reference count: 40
- Primary result: Tuning-free sparse recovery via implicit regularization from overparameterization achieves performance comparable to optimally-tuned baselines without requiring sparsity level or noise variance knowledge

## Executive Summary
This paper proposes a tuning-free approach for structured sparse recovery of multiple measurement vectors (MMV) using implicit regularization from overparameterization. The method factorizes the target matrix into components that decouple shared row-support from individual vector entries, applying gradient descent to a standard least-squares objective. A key innovation is a Hadamard-product parameterization that promotes row sparsity through optimization dynamics rather than explicit regularization. Theoretical analysis establishes convergence guarantees showing the solution trajectory approaches an idealized row-sparse solution under sufficiently small and balanced initialization. Empirical results demonstrate the approach achieves performance comparable to optimally-tuned baselines (M-OMP, M-SP, AMP-MMV, M-BSBL, M-FOCUSS) while being robust to parameter misspecification and low SNR conditions where traditional methods fail. The method requires no prior knowledge of sparsity level or noise variance, making it practical for real-world applications where such information is unavailable.

## Method Summary
The method reparameterizes the MMV problem by factorizing the target matrix X into g ⊙²1_L ⊙ V, where g ∈ ℝ^N is a row-scaling vector and V ∈ ℝ^(N×L) is a component matrix. Gradient descent is applied to the standard least-squares loss ∥Y - AX∥²_F without any explicit regularization terms. The Hadamard product structure enables implicit row-sparsity through multiplicative coupling: when gradient descent drives elements of g towards zero, the multiplicative structure forces corresponding rows of V to zero simultaneously. The approach requires only balanced initialization (α_v = α_g/√(2L)) and small learning rates to maintain the row-wise coupling throughout optimization.

## Key Results
- IR-MMV achieves F1 scores comparable to optimally-tuned baselines (M-OMP, M-SP, AMP-MMV, M-BSBL, M-FOCUSS) across synthetic experiments
- Robust to parameter misspecification where traditional methods fail when sparsity level or noise variance is unknown
- Maintains performance at low SNR conditions where explicitly-regularized methods degrade significantly
- Converges to idealized row-sparse solutions under gradient flow dynamics with balanced initialization

## Why This Works (Mechanism)

### Mechanism 1
The Hadamard product parameterization decouples shared row-support from individual vector entries, enabling implicit row-sparsity through multiplicative structure. X = (g⊙²1_L)⊙V where g is a row-scaling vector and V is a component matrix. When gradient descent drives elements of g towards zero, the multiplicative structure forces corresponding rows of V to zero simultaneously, inducing row sparsity in X without explicit regularization terms. Core assumption: Sufficiently small and balanced initialization (α_V = α_g/√(2L)) preserves the row-wise coupling throughout optimization.

### Mechanism 2
Gradient descent on this overparameterization exhibits momentum-like dynamics where larger row norms grow faster, creating incremental learning of the true support. The rate of change of each row norm is proportional to ∥X_i:(t)∥₂^(4/3), creating non-linear amplification where rows with larger current norms accelerate faster. This "momentum-like" effect causes rows in the true support to dominate while others remain small. Core assumption: The residual gradient ⟨λ_i(t), x̂_i(t)⟩ is positive for support rows and negative or smaller for non-support rows, which requires the sensing matrix A to have low coherence.

### Mechanism 3
Balancedness conservation guarantees that if a row-scaling element converges to zero, its corresponding component row must also converge to zero. Under gradient flow, the quantity 1/2 g²_i(t) - Σ_j V²_ij(t) is conserved for each row i. This invariant ensures proportional evolution—if gi(t) → 0, then Σ_j V²_ij(t) → 0 necessarily, forcing the entire i-th row of X to zero. Core assumption: Continuous gradient flow (infinitesimal learning rate limit) accurately approximates discrete gradient descent with small η.

## Foundational Learning

- **Compressed sensing fundamentals (sparsity, measurement matrices, MMV model)**: Why needed here: The entire framework builds on recovering jointly sparse signals from underdetermined linear systems Y = AX + W where X has row-sparsity K ≪ N. Quick check question: Given measurements Y (500×20) from sensing matrix A (500×10000), what does it mean for the unknown X (10000×20) to be "row-sparse with K=3"?

- **Gradient descent and gradient flow (continuous-time limit)**: Why needed here: The theoretical analysis models discrete GD updates as continuous ODEs; understanding this approximation is critical for setting learning rates and interpreting convergence bounds. Quick check question: How does setting η_g = η_v = 10⁻⁴ relate to the paper's assumption of "infinitesimal limit" gradient flow?

- **Dynamical systems analysis (Lyapunov functions, stability, conservation laws)**: Why needed here: The proof strategy uses a Lyapunov function E(t) to bound trajectory divergence without the exponential-in-time penalties of Grönwall-based approaches. Quick check question: Why does a Lyapunov-based analysis yield "practically feasible initialization values" while Grönwall's inequality would require exp(-βT) scale initialization?

## Architecture Onboarding

- **Component map**: Initialize g(0) = α_g·1_N, V(0) = α_v·1_(N×L) → Forward pass compute X = (g⊙²1_L)⊙V → Compute loss L = ∥Y - AX∥²_F → Backward pass compute gradients ∇g L, ∇V L → Update parameters g(t+1), V(t+1) → Repeat until convergence

- **Critical path**: Initialize g(0) = α_g·1_N, V(0) = α_v·1_(N×L) with α_v = α_g/√(2L) → Forward pass compute X → Compute loss and residual → Backward pass compute gradients → Update parameters → Repeat until convergence (∼5×10⁶ iterations per paper)

- **Design tradeoffs**:
  - Initialization scale α_v: Smaller values (paper uses 5×10⁻⁴) satisfy theoretical bounds but require more iterations; larger values may violate balancedness assumptions
  - Learning rates η_g, η_v: Must be small enough for gradient flow approximation; paper uses 10⁻⁴ but doesn't ablate this choice
  - Iteration count T: Theoretical guarantees hold for finite T; empirical convergence observed at ∼5×10⁶ but no early stopping criterion provided

- **Failure signatures**:
  1. Imbalanced initialization: If 1/2 α_g² ≠ Lα_v², the conservation law won't hold, decoupling g and V evolution
  2. Large learning rates: Violates gradient flow assumption, causing E(t) to grow rather than stay bounded
  3. High-coherence sensing matrices: Mutual coherence μ(A) too large prevents clean support recovery; symptoms include poor F1 scores even at high SNR
  4. Insufficient iterations: Premature stopping before row norms separate; non-support rows remain non-zero

- **First 3 experiments**:
  1. Balancedness verification: Implement the full algorithm, track |1/2 g²_i(t) - Σ_j V²_ij(t)| over time for random i; should remain at machine precision if implementation is correct
  2. Synthetic MMV recovery: Generate Y = AX + W with known row-sparse X (K=3, N=10000, M=500, L=20), run IR-MMV with default hyperparameters, compare reconstruction RMSE and support F1 against paper's reported values
  3. Robustness to parameter mismatch: Compare IR-MMV against M-OMP/M-SP with deliberately misspecified sparsity K±1; verify that IR-MMV maintains performance while baselines degrade per Figure 2

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical guarantees assume idealized conditions including infinitesimal learning rates and continuous gradient flow that may not hold practically
- The 5×10⁶ iteration requirement for empirical convergence is computationally intensive and raises questions about scalability
- Analysis focuses on convergence to an "idealized row-sparse solution" rather than the true sparse matrix, which may differ significantly when noise is present
- Real-world validation is absent; claims of matching optimally-tuned baselines are based solely on synthetic experiments

## Confidence

- **High confidence**: The implicit regularization mechanism via Hadamard parameterization is well-founded and the balancedness conservation law (Mechanism 3) is mathematically rigorous.
- **Medium confidence**: The momentum-like dynamics (Mechanism 2) showing row-norm amplification is theoretically sound but depends critically on low mutual coherence assumptions that aren't fully validated empirically.
- **Low confidence**: The claim that this approach matches optimally-tuned baselines across all conditions is based on synthetic experiments; real-world validation is absent.

## Next Checks

1. **Coherence sensitivity test**: Systematically vary mutual coherence μ(A) from 0.01 to 0.5 and measure F1 score degradation to quantify the low-coherence assumption's practical limits.

2. **Gradient flow validation**: Implement both continuous gradient flow (via ODE solver) and discrete gradient descent, comparing trajectory divergence to verify the gradient flow approximation's validity for η = 10⁻⁴.

3. **Initialization sensitivity**: Test non-balanced initialization (α_v ≠ α_g/√(2L)) to confirm that conservation law violations lead to support recovery failure, as predicted by theory.