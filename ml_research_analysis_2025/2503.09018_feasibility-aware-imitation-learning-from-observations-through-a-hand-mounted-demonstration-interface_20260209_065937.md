---
ver: rpa2
title: Feasibility-aware Imitation Learning from Observations through a Hand-mounted
  Demonstration Interface
arxiv_id: '2503.09018'
source_url: https://arxiv.org/abs/2503.09018
tags:
- learning
- robot
- feasibility
- demonstration
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces feasibility-aware behavior cloning from observation
  (FABCO), a novel imitation learning framework that improves robot policy learning
  by providing visual feedback on the feasibility of human demonstrations. The key
  idea is to use a hand-mounted demonstration interface to collect human demonstration
  data, assess the feasibility of these demonstrations using pre-trained forward and
  inverse dynamics models, and provide visual feedback to demonstrators to encourage
  robot-executable demonstrations.
---

# Feasibility-aware Imitation Learning from Observations through a Hand-mounted Demonstration Interface

## Quick Facts
- arXiv ID: 2503.09018
- Source URL: https://arxiv.org/abs/2503.09018
- Reference count: 31
- Primary result: Visual feedback on demonstration feasibility significantly improves robot policy learning success rates

## Executive Summary
This paper introduces FABCO, a novel imitation learning framework that improves robot policy learning by providing visual feedback on the feasibility of human demonstrations. The key innovation is a hand-mounted demonstration interface that captures human motion while using pre-trained forward and inverse dynamics models to assess the feasibility of demonstrations for robot execution. The estimated feasibility is then used as a weight during policy learning to improve data efficiency and robustness. The method was validated on a pipette insertion task with four participants, showing that visual feedback significantly improved demonstration feasibility and led to higher task success rates compared to standard behavior cloning from observation.

## Method Summary
FABCO works by first collecting human demonstration data through a hand-mounted interface that captures pose and finger distance information. Pre-trained forward and inverse dynamics models (trained on random robot trajectories) assess the feasibility of these demonstrations by measuring the consistency between predicted and actual state transitions. This feasibility score is visualized to the demonstrator in real-time, allowing them to iteratively improve their demonstrations. During policy learning, the feasibility scores serve as weights in the loss function, prioritizing robot-executable motions. The policy is trained using behavior cloning from observation, where actions are inferred from state transitions using the inverse dynamics model.

## Key Results
- Visual feedback significantly improved demonstration feasibility scores compared to no feedback conditions
- FABCO achieved higher task success rates (3/4 participants) compared to standard BCO
- NASA-TLX survey showed slightly increased mental demand but enhanced sense of task accomplishment
- Feasibility-weighted loss function improved policy robustness compared to unweighted learning

## Why This Works (Mechanism)

### Mechanism 1: Feasibility Approximation via Dynamics Consistency
The system detects robot-infeasible motions by measuring the reconstruction error between the demonstrated state transition and the robot's predicted dynamics. An Inverse Dynamics Model (IDM) predicts the action required to move between two observed demonstration states, and a Forward Dynamics Model (FDM) then predicts the next state resulting from that action. The discrepancy between the actual demonstrated next state and the FDM-predicted state serves as a proxy for feasibility. This works because the robot's dynamics can be sufficiently approximated by neural networks trained on random tracking data, and high prediction error correlates with physical infeasibility for the robot.

### Mechanism 2: Interactive Demonstration Refinement via Visual Feedback
Providing real-time visualization of feasibility scores enables human demonstrators to iteratively correct their behavior to match the robot's constraints. The system visualizes the demonstration trajectory, color-coding segments based on the calculated feasibility score. This closed-loop feedback allows the human to adjust their motion speed or path in real-time to maximize the "green" (high feasibility) regions. Humans can intuitively map abstract color feedback to the required kinematic adjustments (e.g., slowing down or altering a trajectory).

### Mechanism 3: Variance Reduction via Feasibility-Weighted Loss
Weighting the imitation learning loss function by feasibility scores prioritizes robot-executable motions, reducing the variance introduced by infeasible demonstrations. During Behavior Cloning from Observation (BCO), the loss function is weighted by the feasibility scores. This suppresses the gradient contribution from state-action pairs that the dynamics models flagged as inconsistent (low feasibility), forcing the policy to prioritize imitating feasible trajectories. This treats infeasible demonstrations as "noise" that should be suppressed rather than edge cases to be learned.

## Foundational Learning

- **Concept: Behavior Cloning from Observation (BCO)**
  - Why needed here: FABCO builds directly on BCO. You must understand that BCO infers actions from state transitions using an Inverse Dynamics Model (IDM) because actions are not explicit in human hand demonstrations.
  - Quick check question: Can you explain how an Inverse Dynamics Model allows a policy to be learned when the expert's actions are unknown?

- **Concept: Forward vs. Inverse Dynamics**
  - Why needed here: The core feasibility metric relies on the consistency between these two models. You need to distinguish between $Action = IDM(State_t, State_{t+1})$ and $State_{t+1} = FDM(State_t, Action)$.
  - Quick check question: If the FDM predicts a different $State_{t+1}$ than what occurred in the demo, what does that imply about the executed action?

- **Concept: Covariate Shift**
  - Why needed here: The paper explicitly addresses covariate shift (compounding errors) caused by infeasible demonstrations. Understanding that small errors in the policy distribution lead to large divergence in execution is crucial.
  - Quick check question: Why does filtering out "infeasible" demonstrations help mitigate covariate shift during policy deployment?

## Architecture Onboarding

- **Component map:** Hand-mounted Interface -> Pose/Finger Capture -> Dynamics Models (IDM/FDM) -> Feasibility Calculator -> Visualizer -> Policy Network
- **Critical path:** The training of IDM and FDM is the bottleneck. If these models are not accurate across the robot's workspace (trained on 2500 trajectories here), the feasibility feedback will be erroneous, and the policy will learn from garbage weights.
- **Design tradeoffs:**
  - Feedback Timing: Real-time vs. post-demonstration feedback significantly changes cognitive load
  - Sigma (σw): Heuristic parameter (set to 0.15). Too low → strict filtering (good data rejected); too high → noise accepted
  - Data Collection: Random tracking policy (πtrack) covers the workspace but may not cover contact-rich dynamics well
- **Failure signatures:**
  - High Mental Demand, Low Feasibility: The interface or feedback is confusing
  - High Feasibility, Low Success: The dynamics models overfit to training data but fail on actual task dynamics
  - Oscillating Policy: The weighting wt fluctuates rapidly, causing unstable gradient updates
- **First 3 experiments:**
  1. Dynamics Validation: Report prediction error (MSE/L1) of FDM/IDM on a held-out test set of robot trajectories
  2. Ablation on Weighting: Train policy with FABCO vs. FABCO w/o weighting to isolate impact of loss function modification
  3. Ablation on Feedback: Compare demo feasibility distributions for groups with/without visual feedback

## Open Questions the Paper Calls Out
1. How can the specific kinematic or dynamic factors that reduce demonstration feasibility be identified and visualized to the demonstrator?
2. Can FABCO be extended to contact-rich tasks by integrating dynamics models that explicitly account for object interaction?
3. Can the feasibility scale parameters be automatically tuned using demonstration and robot motion data instead of heuristics?

## Limitations
- Feasibility metric may fail in contact-rich scenarios where external forces alter robot dynamics
- Visual feedback interface increases cognitive load despite improving demonstration quality
- Dynamics models require extensive pre-training on robot-specific trajectories

## Confidence
- High Confidence: The core mechanism of using dynamics consistency for feasibility detection is well-established in robotics literature and mathematically sound
- Medium Confidence: The effectiveness of visual feedback in improving demonstration quality is supported by results but requires larger-scale validation across diverse tasks and user groups
- Medium Confidence: The policy improvement through feasibility weighting is demonstrated but the ablation studies are limited to a single task, making generalization uncertain

## Next Checks
1. Cross-task validation: Test FABCO on tasks with varying dynamics complexity (e.g., drawer opening, peg-in-hole with tighter tolerances) to assess robustness to different contact-rich scenarios
2. Dynamics model generalization: Systematically evaluate FDM/IDM performance on human-demonstrated trajectories that significantly differ from the random training distribution to identify failure modes
3. Larger participant study: Conduct user studies with 15+ participants across different skill levels to validate the scalability of the visual feedback mechanism and its impact on mental demand vs. task success trade-off