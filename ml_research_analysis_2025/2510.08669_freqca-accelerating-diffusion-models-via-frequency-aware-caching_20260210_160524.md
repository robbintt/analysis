---
ver: rpa2
title: 'FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching'
arxiv_id: '2510.08669'
source_url: https://arxiv.org/abs/2510.08669
tags:
- diffusion
- caching
- feature
- frequency
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Frequency-aware Caching (FreqCa), a method
  that accelerates diffusion transformers by decomposing features into low- and high-frequency
  components and applying different caching strategies to each. Low-frequency components
  are directly reused due to high similarity across timesteps, while high-frequency
  components are predicted using a second-order Hermite interpolator based on their
  continuity.
---

# FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching
arXiv ID: 2510.08669
Source URL: https://arxiv.org/abs/2510.08669
Reference count: 30
Primary result: Achieves 6-7× acceleration with <2% quality degradation

## Executive Summary
FreqCa introduces a frequency-aware caching strategy for accelerating diffusion transformers by decomposing features into low- and high-frequency components. The method exploits the high similarity of low-frequency components across timesteps through direct reuse, while predicting high-frequency components using second-order Hermite interpolation based on their continuity properties. By caching Cumulative Residual Features (CRF) instead of all layer features, the approach achieves a 99% reduction in memory usage. Experiments demonstrate significant speed improvements across multiple models including FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit with minimal quality degradation.

## Method Summary
FreqCa accelerates diffusion transformers through frequency-aware caching by first decomposing intermediate features into low- and high-frequency components using Fourier analysis. Low-frequency components, which exhibit high similarity across timesteps, are directly cached and reused. High-frequency components, assumed to have second-order continuity, are predicted using a Hermite interpolator that leverages temporal information from neighboring timesteps. The method further optimizes memory by caching Cumulative Residual Features (CRF) instead of all layer features, achieving a 99% memory reduction. This combined approach enables 6-7× acceleration while maintaining image quality within 2% degradation.

## Key Results
- Achieves 6-7× acceleration across FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit models
- Maintains image quality with <2% degradation measured by FID scores
- Reduces memory usage by 99% through CRF caching instead of full layer feature caching
- Outperforms existing acceleration methods in both generation and editing tasks

## Why This Works (Mechanism)
The acceleration works by exploiting the inherent temporal structure in diffusion model feature evolution. Low-frequency components capture the coarse, slowly-varying aspects of image features that change gradually across timesteps, making them highly cacheable. High-frequency components represent fine details that evolve more rapidly but maintain continuity properties allowing accurate interpolation. The CRF caching strategy further reduces memory overhead by storing only the cumulative difference from initial features rather than all intermediate states, while still enabling accurate reconstruction through the interpolation mechanism.

## Foundational Learning
- **Fourier decomposition**: Separates features into frequency bands to identify cacheable components
  *Why needed*: Enables targeted caching strategies based on temporal similarity patterns
  *Quick check*: Verify that low-frequency components show higher cross-timestep correlation

- **Second-order Hermite interpolation**: Predicts high-frequency components using temporal continuity assumptions
  *Why needed*: Provides accurate reconstruction of rapidly-evolving features without storing all timesteps
  *Quick check*: Confirm interpolation accuracy on held-out timestep pairs

- **Cumulative Residual Features**: Stores only the difference from initial features rather than all intermediate states
  *Why needed*: Dramatically reduces memory footprint while preserving reconstruction capability
  *Quick check*: Validate that CRF caching maintains <2% quality degradation

## Architecture Onboarding
**Component map**: Input -> Feature Decomposition -> Low-Freq Cache/Reuse + High-Freq Interpolation -> CRF Caching -> Output
**Critical path**: Feature decomposition and caching decisions occur at each timestep, with interpolation and reconstruction performed during generation
**Design tradeoffs**: Balances memory reduction against interpolation accuracy; assumes second-order continuity may not hold universally
**Failure signatures**: Quality degradation increases when frequency threshold is poorly chosen or continuity assumptions break down
**First experiments**: 1) Test sensitivity to frequency threshold parameter across different datasets, 2) Profile memory usage during inference to verify 99% reduction claim, 3) Evaluate cross-architecture generalization on UNet-based diffusion models

## Open Questions the Paper Calls Out
None identified in the provided analysis.

## Limitations
- The assumption of second-order continuity for high-frequency components may not hold across all model architectures and data distributions
- The 99% memory reduction claim requires clarification on whether it refers to peak memory or total memory footprint
- Evaluation focuses primarily on FID scores and speed without comprehensive ablation studies on frequency decomposition itself

## Confidence
- High confidence in technical implementation and basic acceleration results
- Medium confidence in theoretical justification for frequency-based decomposition
- Medium confidence in generalizability across different model architectures

## Next Checks
1. Conduct systematic ablation studies varying the frequency threshold parameter to determine sensitivity and optimal settings across different datasets and model scales
2. Perform detailed memory profiling during inference to verify the 99% memory reduction claim and identify hidden memory costs
3. Test the approach on additional diffusion model variants beyond transformers (e.g., UNet-based models) to assess cross-architecture generalization