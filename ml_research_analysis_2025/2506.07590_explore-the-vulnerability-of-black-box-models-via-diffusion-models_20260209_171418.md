---
ver: rpa2
title: Explore the vulnerability of black-box models via diffusion models
arxiv_id: '2506.07590'
source_url: https://arxiv.org/abs/2506.07590
tags:
- substitute
- query
- adversarial
- target
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that diffusion model APIs can be exploited
  to generate synthetic images for training substitute models, enabling model extraction
  and adversarial transfer attacks on black-box classification systems. By leveraging
  class-level prompts and stable diffusion APIs, high-resolution, diverse synthetic
  data is produced without querying the target model.
---

# Explore the vulnerability of black-box models via diffusion models

## Quick Facts
- arXiv ID: 2506.07590
- Source URL: https://arxiv.org/abs/2506.07590
- Reference count: 0
- Attack success rate: 98.68% with 0.01× query budget

## Executive Summary
This study demonstrates that diffusion model APIs can be exploited to generate synthetic images for training substitute models, enabling model extraction and adversarial transfer attacks on black-box classification systems. By leveraging class-level prompts and stable diffusion APIs, high-resolution, diverse synthetic data is produced without querying the target model. This synthetic data pre-trains substitute models, reducing the query budget required for effective attacks. Experiments across seven benchmarks—including CIFAR-10, CIFAR-100, and ImageNet subsets—show the method achieves an average improvement of 27.37% over state-of-the-art approaches while using only 0.01× the query budget, with a 98.68% success rate in adversarial attacks. The method is effective across various target and substitute model architectures and eliminates the need for real or private training data, demonstrating both efficiency and practicality.

## Method Summary
The method exploits generative model APIs (e.g., Stable Diffusion) to create high-quality synthetic data without querying the target model. Class-level prompts are encoded via CLIP and used to condition diffusion models, which generate 512×512 images that are downscaled to match dataset dimensions. Substitute models are pre-trained on 200,000 synthetic images per dataset using standard SGD training. A single query to the target model provides hard labels for a subset of synthetic images, which are then used to fine-tune the substitute model. The pre-trained substitute requires minimal queries for effective model extraction, achieving high accuracy with only 5,000 queries compared to hundreds of thousands required by baseline methods. Adversarial attacks (FGSM, BIM, PGD) are generated on the substitute and transferred to the target model.

## Key Results
- Achieves 98.68% attack success rate using only 0.01× the query budget of baseline methods
- Improves substitute model accuracy by 27.37% on average across seven benchmarks
- Effective across various target and substitute model architectures without requiring real or private training data
- Eliminates the need for real data while maintaining high-quality model extraction and adversarial transferability

## Why This Works (Mechanism)

### Mechanism 1: Zero-Query Synthetic Data Generation via Diffusion APIs
Diffusion model APIs generate high-resolution, diverse synthetic images without any queries to the target model, providing a data-free foundation for substitute model training. Class-level prompts (e.g., "a photo of a tench") are encoded via CLIP into embeddings that condition random noise vectors. The diffusion model iteratively denoises over T steps to produce photorealistic images matching the semantic class. The synthetic data distribution D_syn sufficiently approximates the real data distribution D_real for pre-training to transfer meaningful feature representations. If target classification relies on fine-grained features not captured by diffusion models (e.g., specific texture patterns, subtle medical imaging biomarkers), this mechanism may fail.

### Mechanism 2: Pre-training Establishes Decision Boundary Initialization Near Target
Pre-training on synthetic images initializes the substitute model's parameters such that its decision boundaries are already proximate to the target model's, requiring minimal fine-tuning queries. Cross-entropy minimization on synthetic data (200k samples per dataset) produces θ_pretrain that approximates θ_target. Since gradients ∇θ_pretrain L_CE ≈ 0 near the target's decision boundary, subsequent fine-tuning converges rapidly with few queries. If target model was trained on out-of-distribution data or uses features systematically absent from diffusion-generated images, this pre-training-transfer mechanism may break.

### Mechanism 3: Hard-Label Distillation with Query-Efficient Fine-Tuning
A single forward pass of uniformly-sampled synthetic images through the target model provides sufficient hard-label supervision to align the pre-trained substitute with the target's behavior. Query target model T once on subset of synthetic images to obtain hard labels y_T = T(x̂). Fine-tune substitute S to minimize L_dis = CE(S(x̂, θ_S), y_T). Pre-training ensures parameter space is already near-optimal, so few gradient updates are needed. If target model has highly non-smooth decision boundaries or synthetic samples don't adequately cover boundary-adjacent regions, this distillation mechanism may fail.

## Foundational Learning

- **Concept: Diffusion Models (Denoising Probabilistic Generative Models)**
  - Why needed here: Understanding how text-conditioned diffusion generates images explains why synthetic data can substitute for real training data.
  - Quick check question: Can you explain how iterative denoising transforms random noise into a coherent image guided by CLIP text embeddings?

- **Concept: Knowledge Distillation (Teacher-Student Transfer)**
  - Why needed here: The attack fundamentally uses distillation—transferring target model knowledge to a substitute—under severe query constraints.
  - Quick check question: Why does hard-label distillation require more queries than soft-label distillation, and what does pre-training compensate for?

- **Concept: Adversarial Transferability**
  - Why needed here: The attack's success depends on adversarial perturbations generated on the substitute model fooling the target model.
  - Quick check question: What properties of decision boundary similarity enable adversarial examples to transfer between different model architectures?

## Architecture Onboarding

- **Component map**: Stable Diffusion API → class prompts → 512×512 images → downscale → substitute pre-training → single target query → hard-label distillation → adversarial attacks
- **Critical path**: Synthetic data quality → pre-training accuracy → distillation query efficiency → adversarial transfer success rate. Pre-training quality is the bottleneck.
- **Design tradeoffs**:
  - More synthetic samples improve pre-training but increase compute (10 min distillation on A40 with 5k queries)
  - Early stopping at epoch 5-10 yields higher ASR; later epochs yield higher substitute accuracy
  - Hard-label setting matches real APIs but provides less information than soft-label
- **Failure signatures**:
  - Pre-training collapse: Test accuracy ~10% (random), visible in baseline visualizations showing noise
  - Distillation non-convergence: Loss oscillates without decreasing
  - Poor transfer: High substitute accuracy but low ASR indicates decision boundary mismatch
- **First 3 experiments**:
  1. Validate synthetic data: Generate 10k CIFAR-10 class images, train classifier from scratch, verify >60% accuracy to confirm class-discriminative features are present
  2. Ablate pre-training: Compare substitute accuracy with/without synthetic pre-training at 5k query budget on CIFAR-10 to quantify contribution (expect ~30% gap)
  3. Verify transfer: Train substitute with 150k queries on CIFAR-100, generate BIM attacks, measure ASR on ResNet-34 target (expect >60%)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Data Generation Fidelity: The quality and class representativeness of diffusion-generated synthetic images are not directly evaluated against ground truth distributions, potentially limiting feature transfer.
- Query Budget Calibration: Baseline methods' query requirements are not fully specified, making efficiency claims relative rather than absolute.
- Cross-Domain Transfer: Effectiveness is only demonstrated on natural image classification; performance on specialized domains like medical imaging is unknown.

## Confidence
- **High Confidence**: The core mechanism of using diffusion models for zero-query synthetic data generation is well-supported, and the pre-training approach demonstrably reduces query requirements.
- **Medium Confidence**: Efficiency claims relative to state-of-the-art methods are supported by experiments, but baseline comparison methodology could be more transparent.
- **Low Confidence**: The assumption that diffusion-generated synthetic data captures all relevant features for any target model is unverified, and the paper doesn't address potential defenses.

## Next Checks
1. **Synthetic Data Quality Analysis**: Generate 10k synthetic images for CIFAR-10, compute FID/Precision-Recall scores against real test set, and visualize samples to verify class discriminability and absence of mode collapse.
2. **Cross-Domain Transfer Test**: Apply the method to a non-natural image dataset (e.g., medical X-rays or satellite imagery) and measure substitute accuracy and attack transfer success to validate domain generalizability.
3. **Defense Robustness Evaluation**: Implement simple defenses like input preprocessing or adversarial training on the target model, then measure whether attack success rates drop significantly, identifying potential countermeasures.