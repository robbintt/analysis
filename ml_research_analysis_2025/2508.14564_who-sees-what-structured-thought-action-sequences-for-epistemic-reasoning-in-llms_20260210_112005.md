---
ver: rpa2
title: Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning
  in LLMs
arxiv_id: '2508.14564'
source_url: https://arxiv.org/abs/2508.14564
tags:
- reasoning
- agent
- examples
- perspective-taking
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) can be\
  \ improved for perspective-taking in collaborative tasks using structured examples\
  \ derived from planner-generated reasoning trees. The authors introduce three types\
  \ of example sequences\u2014goal-directed, information-seeking, and local decision-based\u2014\
  and transform them into thought-action pairs to scaffold LLM reasoning."
---

# Who Sees What? Structured Thought-Action Sequences for Epistopic Reasoning in LLMs

## Quick Facts
- arXiv ID: 2508.14564
- Source URL: https://arxiv.org/abs/2508.14564
- Reference count: 36
- LLMs fail to generalize structured planner-derived examples for robust perspective-taking in collaborative tasks

## Executive Summary
This paper investigates whether structured thought-action sequences derived from planner-generated reasoning trees can improve large language models' (LLMs) perspective-taking abilities in collaborative tasks. The authors introduce three types of example sequences - goal-directed, information-seeking, and local decision-based - and transform them into thought-action pairs to scaffold LLM reasoning. Tested in a simulated household environment with a Director-Matcher setup, the approach shows that while LLM agents succeed in simple attentional filtering, they struggle with mentalizing about occluded spaces or evaluating epistemic costs. Only locally optimal examples slightly reduced unnecessary clarifications, indicating that structured examples alone are insufficient for robust perspective-taking and highlighting the need for explicit belief tracking and richer environmental modeling.

## Method Summary
The authors use a Fast Downward planner operating on a PDDL representation of a Director-Matcher task to generate reasoning trees. Three extraction strategies traverse these trees to create example sequences: G-type (optimal goal paths), E-type (information-seeking paths), and L-type (local optimal decisions). An LLM verbalizer converts these sequences into natural language thought-action pairs. A ReAct agent is then few-shot prompted with these examples and evaluated across seven environment types using leave-one-out cross-validation, measuring first-take accuracy, step count, clarification requests, and failure rate.

## Key Results
- Structured examples fail to significantly improve LLM perspective-taking performance
- L-type examples marginally reduce unnecessary clarification requests
- LLM agents succeed at simple attentional filtering but struggle with mentalizing about occluded spaces
- Current approach insufficient for robust Theory of Mind reasoning

## Why This Works (Mechanism)
The paper's core insight is that planner-generated reasoning trees can provide structured examples for LLM training. However, the mechanism fails because symbolic planner optimality doesn't translate to the flexible reasoning required for perspective-taking. The ReAct framework's interleaving of thoughts and actions creates a natural scaffold, but the LLM cannot internalize the planner's rigid decision logic for the nuanced reasoning needed in partially observable environments.

## Foundational Learning

- **ReAct Framework (Reason + Act)**
  - Why needed here: This is the core agent architecture used in the paper. The method of interleaving natural language "thoughts" with environment-grounded "actions" is the foundation upon which the structured examples are built and evaluated.
  - Quick check question: Can you explain how a ReAct agent's loop differs from a simple chain-of-thought prompt?

- **Perspective-Taking Levels (Theory of Mind)**
  - Why needed here: The paper's central problem is improving an LLM's ability to perform perspective-taking. Distinguishing between Level-1 (what others see) and Level-2 (how things appear to them, or mentalizing about unseen content) is critical for understanding the experimental results.
  - Quick check question: What is the difference between inferring what object another agent can see and reasoning about what object might be inside a closed box that agent can see?

- **PDDL and Classical Planning**
  - Why needed here: The structured examples are not manually created but are derived from the output of the Fast Downward planner, which operates on a Planning Domain Definition Language (PDDL) representation of the task. Understanding this source is key to understanding the nature of the training data.
  - Quick check question: What is the role of a heuristic function in a classical planner like Fast Downward?

## Architecture Onboarding

- Component map:
  PDDL Environment -> Modified Fast Downward Planner -> Example Generation Pipeline -> ReAct Agent
- Critical path: The success of the entire approach hinges on the LLM Verbalizer's ability to accurately and meaningfully translate the planner's symbolic action sequence into a natural language chain of reasoning that the ReAct Agent can internalize and generalize from.
- Design tradeoffs:
  - Planner Optimality vs. LLM Generalization: Planner-derived examples are guaranteed to be optimal within the PDDL model, but this rigidity may not translate well to the more nuanced, real-world reasoning required of LLMs.
  - Example Type Complexity: G-type examples provide a full solution path, while L-type examples provide granular, step-by-step decision logic. L-type was found to be slightly more effective for reducing unnecessary actions, suggesting a tradeoff between holistic strategy and local decision guidance.
- Failure signatures:
  - Excessive Clarification: A sign that the agent is not properly evaluating the cost or necessity of an epistemic action. It defaults to a "when in doubt, ask" heuristic.
  - Failure on Occluded Spaces: When an agent fails to reason about what might be in a container another agent can see, it indicates an inability to perform the imaginative simulation required for Level-2 perspective-taking.
- First 3 experiments:
  1. Baseline ReAct Performance: Run the ReAct agent on the seven environment types without any structured examples to establish a baseline for comparison.
  2. Ablation of Example Types: Systematically evaluate the ReAct agent when provided with only G-type, only E-type, and only L-type examples.
  3. Probe for Cost-Benefit Reasoning: Design a task variant where the cost of asking a question is explicitly stated. Evaluate if the agent, with and without structured examples, can adapt its strategy based on this cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating explicit belief state tracking and learned cost models into LLMs overcome the current limitations of structured examples in achieving robust Theory of Mind?
- Basis in paper: The Conclusion states that bridging the gap between attentional filtering and full ToM "may require explicit belief state tracking [and] learned cost models."
- Why unresolved: The authors found that structured examples (planner traces) failed to transfer because the LLM implicitly assigns near-zero cost to questions and lacks an explicit cost rationale.
- What evidence would resolve it: Successful performance in ambiguous scenarios where the agent balances epistemic action costs against information gain without reverting to simple heuristics.

### Open Question 2
- Question: Can specific prompting strategies that foreground hidden regions and stimulate hypothesis generation about unseen content significantly improve epistemic reasoning?
- Basis in paper: The Discussion notes that failures in F2 (imagining occluded spaces) may stem from "prompts that insufficiently foreground the plausibility of missing information."
- Why unresolved: Without explicit representations of unexplored areas in the prompt, agents often fail to infer that exploration is necessary.
- What evidence would resolve it: Improved navigation and reduced errors in "Hidden" or "Distractor" environments when using uncertainty-focused prompts compared to standard context descriptions.

### Open Question 3
- Question: Do the identified failure modes in mentalizing (F2) and cost evaluation (F3) persist when the environment is upgraded from binary visibility to include graded salience and gaze cues?
- Basis in paper: The Limitations section asks if the same failure modes persist or if "multi-modal grounding and cost signals can scaffold more robust ToM" in richer settings.
- Why unresolved: The current study used a grid world with binary visibility; real-world social environments contain more complex, continuous signals.
- What evidence would resolve it: Replication of the study in a multi-modal environment showing whether agents can utilize gaze and salience to overcome current reasoning deficits.

## Limitations
- Disconnect between symbolic planner optimality and LLM generalization
- LLM verbalizer quality not validated
- F3 cost-benefit reasoning remains untested
- Planner-derived examples may be inherently limited for LLM reasoning

## Confidence

**High Confidence**: The core empirical finding that structured examples do not significantly improve perspective-taking, and that L-type examples provide only marginal benefit for reducing clarifications. The methodology for generating and evaluating examples is clearly described.

**Medium Confidence**: The claim that explicit belief tracking and richer environmental modeling are needed. While supported by the results, the paper doesn't test alternative approaches beyond structured examples.

**Low Confidence**: The assertion that planner-derived examples are inherently limited for LLM reasoning. The paper doesn't explore whether different planner algorithms, verbalizer prompts, or training protocols might yield better transfer.

## Next Checks

1. **Belief State Representation Test**: Implement a simple belief tracking module that maintains a probabilistic model of what the Director can see. Evaluate if this explicit representation improves F2 performance on Hidden/Not That/Distractor environments compared to the baseline ReAct agent.

2. **Cost-Aware Epistemic Reasoning**: Modify the experimental setup to include explicit cost penalties for ASK actions. Run the same evaluation protocol to determine if structured examples (particularly L-type) enable the agent to adapt its clarification strategy when costs are introduced.

3. **Verbalizer Quality Ablation**: Generate thought-action pairs using multiple LLM verbalizers (different models or prompting strategies) for the same planner trajectories. Compare ReAct agent performance across these variants to isolate whether example quality or LLM generalization ability drives the observed limitations.