---
ver: rpa2
title: One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints
arxiv_id: '2601.11977'
source_url: https://arxiv.org/abs/2601.11977
tags:
- forecasting
- time
- expert
- moe-encoder
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multivariate time series forecasting under
  privacy constraints, particularly in power systems where data is geographically
  distributed and privacy-sensitive. The authors propose MoE-Encoder, a novel architecture
  that augments pre-trained time series models with a sparse mixture-of-experts layer
  inserted between tokenization and encoding.
---

# One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints

## Quick Facts
- arXiv ID: 2601.11977
- Source URL: https://arxiv.org/abs/2601.11977
- Reference count: 18
- One-line primary result: MoE-Encoder achieves WQL 2.82-3.69 and MASE 3.08-5.38 on electricity price forecasting while reducing federated communication by 42.3%

## Executive Summary
This paper addresses multivariate time series forecasting under privacy constraints, particularly in power systems where data is geographically distributed and privacy-sensitive. The authors propose MoE-Encoder, a novel architecture that augments pre-trained time series models with a sparse mixture-of-experts layer inserted between tokenization and encoding. This design enables covariate-guided expert selection and supports federated learning by allowing localized training while keeping the main model frozen. Experiments on five electricity market datasets show MoE-Encoder achieves superior performance compared to strong baselines.

## Method Summary
MoE-Encoder is a plug-in module that inserts a sparse mixture-of-experts layer between tokenization and encoding in pre-trained time series models. The architecture uses covariate-guided expert routing where a gating function selects top-k experts based on covariate embeddings. Three expert types are employed: Shared (always active), Conditional (covariate-selected), and Routed (token-level selection). The model uses LoRA adaptation on the MoE-Encoder only while freezing the backbone, enabling efficient federated learning where only expert parameters are shared between clients.

## Key Results
- Achieves WQL of 2.82-3.69 and MASE of 3.08-5.38 across different regions
- Outperforms strong baselines including Chronos, PatchTST, CITRAS, and COSMIC
- Reduces communication overhead by 42.3% compared to full fine-tuning in federated settings
- Maintains robust performance even with degraded covariates (7.6% MASE increase at 50% missing)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Covariate-guided expert routing improves forecasting by specializing experts to structured input contexts
- Mechanism: A gating function g(z_t) scores experts based on covariate embeddings; top-k experts are selected and their outputs are weighted-averaged
- Core assumption: Covariates carry exploitable structure that correlates with target dynamics
- Evidence anchors: Abstract mentions "enabling covariate-guided expert selection"; equations define routing with Softmax(g(z_t)) and top-k selection
- Break condition: If covariates are uninformative or adversarially corrupted, expert selection may degrade

### Mechanism 2
- Claim: Inserting MoE between tokenization and encoding preserves pretrained representations while enabling task-adaptive computation
- Mechanism: The MoE-Encoder transforms tokenized representations via expert routing; frozen backbone processes transformed tokens
- Core assumption: Pretrained backbone representations are sufficiently general for domain transfer
- Evidence anchors: Abstract mentions "injecting a sparse mixture-of-experts layer between tokenization and encoding"; section describes LoRA adaptation only to MoE-Encoder
- Break condition: If pretrained backbone poorly matches target domain, mid-stream MoE may be insufficient

### Mechanism 3
- Claim: Modular MoE design enables privacy-preserving federated learning by transmitting only expert parameters
- Mechanism: Each client trains MoE-Encoder locally with frozen backbone; only gating vectors and expert weights are shared
- Core assumption: Expert parameters capture useful local patterns without leaking raw data
- Evidence anchors: Abstract mentions "transferring only MoE-Encoder parameters allows efficient adaptation"; section describes local training without sharing raw time series
- Break condition: If expert weights can be inverted to reconstruct training data, privacy may be compromised

## Foundational Learning

- Concept: Sparse Mixture-of-Experts (MoE) with Top-k Gating
  - Why needed here: Core architectural primitive; determines how covariates route to specialized sub-networks
  - Quick check question: Given covariate embedding z, can you compute which 2 of 8 experts are activated and their weights?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Parameter-efficient fine-tuning method applied to MoE-Encoder; reduces communication overhead
  - Quick check question: If LoRA rank is 4 and expert hidden size is 256, how many trainable parameters does one LoRA adapter introduce?

- Concept: Federated Learning with Non-IID Data
  - Why needed here: Deployment context; clients (regions) have heterogeneous distributions
  - Quick check question: Why might FedAvg underperform when client data distributions differ significantly?

## Architecture Onboarding

- Component map: Tokenizer -> MoE-Encoder (Cov-SMoE) -> Frozen Chronos Encoder -> Decoder -> Prediction
- Critical path: 1) Input time series -> tokenization -> tokens {h_t} 2) Covariates -> embedding -> covariate selector + gating scores 3) Expert outputs aggregated -> transformed tokens 4) Frozen backbone processes tokens -> prediction
- Design tradeoffs: 8 experts perform well; 16 shows slight degradation. Covariate-fixed gating outperforms softmax top-k and random. 42.3% communication reduction vs. full fine-tuning
- Failure signatures: 100% missing covariates increases MASE from 0.199 to 0.239. Random gating nearly doubles WQL. N=16 experts show slight degradation vs. N=8
- First 3 experiments:
  1. Replicate Table II on held-out region: Compare covariate-fixed, softmax top-k, and random gating
  2. Expert ablation (N=4,8,16) on local dataset: Verify performance plateau at N=8
  3. Simulate federated transfer: Train MoE-Encoder on region A, transfer only expert parameters to region B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can formal differential privacy guarantees be integrated into MoE-Encoder's expert sharing mechanism without significantly degrading forecasting accuracy?
- Basis in paper: Conclusion states "Future work will explore... investigating more sophisticated privacy-preserving mechanisms for expert sharing in federated environments"
- Why unresolved: Current approach relies on architectural modularity but lacks formal privacy guarantees or model inversion resistance
- What evidence would resolve it: Experiments adding differential privacy noise to expert parameters, reporting privacy budgets alongside forecasting metrics

### Open Question 2
- Question: How does MoE-Encoder perform on time series tasks beyond electricity price forecasting?
- Basis in paper: Conclusion states "Future work will explore extending MoE-Encoder to other time series tasks"
- Why unresolved: All experiments use electricity market datasets; covariate structures differ in other domains
- What evidence would resolve it: Evaluation on diverse benchmarks (traffic, weather, healthcare, finance) with heterogeneous covariate types

### Open Question 3
- Question: How does MoE-Encoder's federated performance scale with number of clients and under heterogeneous data distributions?
- Basis in paper: Experiments simulate five regions but don't systematically vary client count or quantify non-IID severity
- Why unresolved: Communication savings and performance metrics are reported for fixed setup; scalability under larger-scale settings remains uncharacterized
- What evidence would resolve it: Experiments varying client count (10, 50, 100+) and non-IID metrics, reporting convergence speed and communication cost

## Limitations

- Privacy claims are asserted but not formally proven or verified against model inversion attacks
- Experimental scope limited to electricity price forecasting across five markets; generalization to other domains unclear
- Covariate dropout analysis limited to missing data scenarios; performance with noisy or irrelevant covariates not evaluated

## Confidence

- High confidence: Core architectural design and forecasting performance improvements are well-supported
- Medium confidence: Federated learning benefits and privacy claims lack formal verification
- Medium confidence: Superiority over baselines demonstrated, but exact hyperparameter settings not fully specified

## Next Checks

1. Implement MoE-Encoder with 8 experts and verify covariate-fixed gating achieves WQL < 4.0 on held-out region
2. Train MoE-Encoder on region A, transfer only expert parameters to region B, and measure MASE degradation compared to training from scratch
3. Systematically vary covariate availability (0%, 20%, 50%, 100%) and measure performance degradation to validate graceful degradation with missing covariates