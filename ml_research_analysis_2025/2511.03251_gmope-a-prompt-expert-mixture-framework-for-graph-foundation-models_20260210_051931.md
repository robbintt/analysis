---
ver: rpa2
title: GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models
arxiv_id: '2511.03251'
source_url: https://arxiv.org/abs/2511.03251
tags:
- graph
- expert
- prompt
- learning
- gmope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GMoPE integrates graph prompting with Mixture-of-Experts to address
  challenges in cross-domain graph foundation models. The framework uses expert-specific
  prompts and structure-aware MoE routing to enable dynamic specialization and balanced
  expert utilization.
---

# GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models

## Quick Facts
- arXiv ID: 2511.03251
- Source URL: https://arxiv.org/abs/2511.03251
- Reference count: 15
- Key outcome: GMoPE matches full fine-tuning performance with <1% of learnable parameters during transfer

## Executive Summary
GMoPE addresses cross-domain challenges in graph foundation models by integrating graph prompting with Mixture-of-Experts (MoE) architecture. The framework uses expert-specific prompts and structure-aware MoE routing to enable dynamic specialization across diverse graph domains while maintaining computational efficiency. A soft orthogonality loss prevents expert collapse by encouraging prompt diversity, and confidence-guided aggregation preserves expert quality signals during inference.

## Method Summary
GMoPE combines graph prompt learning with MoE routing, where each expert receives unique learnable prompts concatenated to node features. The framework uses structure-aware routing to select top-K experts per input based on expert-specific loss computations. A soft orthogonality constraint (λ∈(0,3)) prevents expert collapse by encouraging prompt diversity. During transfer, expert parameters are frozen and only prompts + task heads are fine-tuned, requiring less than 1% of learnable parameters compared to full fine-tuning.

## Key Results
- GMoPE outperforms state-of-the-art baselines on citation, e-commerce, and molecular graph datasets
- Matches full fine-tuning performance while requiring <1% of learnable parameters during transfer
- Prevents expert collapse through soft orthogonality loss, achieving balanced expert utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-specific prompts enable specialization across diverse graph domains
- Mechanism: Each expert receives unique learnable prompt vector concatenated to node features, directing that expert toward distinct structural-semantic subspaces
- Core assumption: Graph domains possess sufficiently different structural patterns that benefit from specialized processing pathways
- Evidence anchors: Theorem 1 proves FGPF ⊊ FGMoPE; abstract states experts "specialize in distinct subdomains"

### Mechanism 2
- Claim: Soft orthogonality loss prevents expert collapse without enforcing artificial uniformity
- Mechanism: Maximizes pairwise orthogonality among prompt vectors through Lortho, ensuring distinct structural-semantic conditioning signals per expert
- Core assumption: High-dimensional prompt space allows meaningful orthogonality relationships to emerge
- Evidence anchors: Abstract mentions "soft orthogonality constraint"; Figure 5-6 show regulation of expert weight allocation

### Mechanism 3
- Claim: Confidence-guided aggregation preserves expert quality signals during inference
- Mechanism: Aggregation weights expert embeddings by inverse prediction entropy, ensuring experts with higher certainty contribute more
- Core assumption: Prediction entropy correlates with expert competence on given inputs
- Evidence anchors: Section states "uniformly averaging of expert outputs risks diluting high-quality predictions"

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) Routing**
  - Why needed here: GMoPE builds on sparse MoE where only top-K experts activate per input
  - Quick check question: Can you explain why top-K selection reduces to uniform averaging when K=M but enables specialization when K=1?

- Concept: **Prompt-Based Graph Fine-Tuning**
  - Why needed here: Framework extends GPF-style input perturbation to per-expert prompts
  - Quick check question: Why does concatenating a prompt vector [x; p] provide strictly greater expressiveness than additive perturbation x + p?

- Concept: **Feature Alignment via SVD**
  - Why needed here: Cross-domain pretraining requires projecting heterogeneous node features into unified latent space
  - Quick check question: What happens if you skip SVD alignment when combining citation networks (d=1433) with product networks (d=767)?

## Architecture Onboarding

- Component map: Feature Alignment Layer -> Expert Prompt Bank -> MoE Router -> Expert GNNs -> Aggregation Module -> Task Head
- Critical path: 1) Pre-training: Joint optimization of all experts + prompts with Lortho regularization; 2) Transfer: Freeze expert parameters, fine-tune only prompt vectors + task head; 3) Inference: Route inputs → activate experts → aggregate by confidence weights
- Design tradeoffs: K selection (K=M for fine-grained tasks, K=1 for graph-level tasks); M selection (M=N recommended but excessive M causes undertrained experts); λ range (0,3) optimal; prompt dimension dp ∈ [¼d0, ½d0]
- Failure signatures: Expert collapse (all routing weights converge to single expert); negative transfer (cross-domain performance degrades); overfitting on small downstream tasks (reduce K, increase λ, or decrease prompt dimension)
- First 3 experiments: 1) Single-dataset baseline (M=1) confirms GPF-equivalent performance; 2) Ablation without orthogonality loss (λ=0) quantifies expert collapse rate; 3) Cross-domain routing visualization tracks which experts activate for different graph types

## Open Questions the Paper Calls Out
None

## Limitations
- Routing granularity recommendation (M=N) lacks analysis for overlapping structural domains
- Soft orthogonality loss calibration is dataset-dependent with broad optimal λ range
- Cross-domain transfer robustness untested for truly divergent graph types

## Confidence
- **High Confidence**: Core mechanism of per-expert prompts + structure-aware routing is technically sound; Theorem 1's expressiveness proof is well-supported
- **Medium Confidence**: Soft orthogonality loss prevents expert collapse in controlled experiments, but optimal implementation requires further validation
- **Low Confidence**: M=N expert recommendation lacks empirical justification for similar domains; behavior on truly heterogeneous domains untested

## Next Checks
1. **Domain Overlap Analysis**: Test GMoPE with varying M values on structurally similar datasets (e.g., Cora/Citeseer/Pubmed) to determine optimal expert count when domains overlap
2. **Cross-Domain Robustness**: Evaluate transfer performance from molecular graphs to social networks or citation networks to knowledge graphs to assess generalization to divergent structural patterns
3. **Orthogonality Loss Sensitivity**: Systematically vary λ across wider range (0.01-10) on multiple dataset pairs to identify regimes where orthogonality enforcement becomes counterproductive or unstable