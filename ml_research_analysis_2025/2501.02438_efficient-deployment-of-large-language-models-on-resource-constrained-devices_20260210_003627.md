---
ver: rpa2
title: Efficient Deployment of Large Language Models on Resource-constrained Devices
arxiv_id: '2501.02438'
source_url: https://arxiv.org/abs/2501.02438
tags:
- pruning
- lora
- devices
- fine-tuning
- fedspine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying large
  language models (LLMs) on resource-constrained devices in federated learning settings,
  where device heterogeneity and privacy concerns are significant barriers. The proposed
  solution, FedSpine, integrates parameter-efficient fine-tuning (PEFT) with structured
  pruning through an iterative process that adaptively determines pruning ratios and
  LoRA ranks for heterogeneous devices.
---

# Efficient Deployment of Large Language Models on Resource-constrained Devices

## Quick Facts
- **arXiv ID**: 2501.02438
- **Source URL**: https://arxiv.org/abs/2501.02438
- **Reference count**: 40
- **Primary result**: FedSpine speeds up fine-tuning by 1.4×-6.9× and improves final accuracy by 0.4%-4.5% compared to baselines in federated learning on 80 heterogeneous Jetson devices.

## Executive Summary
This paper addresses the challenge of efficiently deploying large language models (LLMs) on resource-constrained devices in federated learning settings, where device heterogeneity and privacy concerns are significant barriers. The proposed solution, FedSpine, integrates parameter-efficient fine-tuning (PEFT) with structured pruning through an iterative process that adaptively determines pruning ratios and LoRA ranks for heterogeneous devices. An online Multi-Armed Bandit (MAB) algorithm is employed to optimize these configurations without prior knowledge of device capabilities. Experiments on a physical platform with 80 NVIDIA Jetson devices demonstrate that FedSpine speeds up fine-tuning by 1.4×-6.9× and improves final accuracy by 0.4%-4.5% compared to baselines, while maintaining higher inference accuracy and efficiency.

## Method Summary
FedSpine combines iterative pruning-fine-tuning coupling with heterogeneity-aware configuration via Multi-Armed Bandit. The method employs LoRA-guided importance estimation for frozen weights, where LoRA gradients approximate the importance of backbone weights for pruning decisions. The server uses S-UCB to select per-device pruning ratios and LoRA ranks each round, while devices run local fine-tuning iterations, estimate weight importance via moving average, prune lowest-importance groups, and upload LoRA weights and state information. The approach maintains model performance through continuous recovery during fine-tuning while progressively reducing model size through pruning.

## Key Results
- FedSpine achieves 1.4×-6.9× speedup in fine-tuning compared to baseline methods on heterogeneous Jetson devices
- Final accuracy improves by 0.4%-4.5% compared to baselines while maintaining higher inference accuracy and efficiency
- Iterative prune-then-tune approach outperforms sequential Prune-then-FT and FT-then-Prune methods by 0.5%-1.7% in accuracy
- Device heterogeneity causes up to 30% idle time with uniform configurations, which MAB-adaptive settings significantly reduce

## Why This Works (Mechanism)

### Mechanism 1: Iterative Pruning-Fine-Tuning Coupling
Alternating between pruning and fine-tuning in each round may preserve model performance better than sequential approaches that fully separate the two phases. Fine-tuning continuously recovers accuracy lost from pruning while pruning progressively reduces model size. The iteration allows LoRA weights to adapt to the progressively pruned backbone, creating a feedback loop where each operation informs the other. Core assumption: Pruning damage is recoverable through immediate fine-tuning, and the importance estimates remain meaningful across iterations. Evidence: FedAPT (iterative) achieves 0.5%-1.7% higher accuracy than Prune+FedLoRA and FedLoRA+Prune at pruning ratio 0.3. Break condition: If pruning ratio per iteration is too aggressive, the model may suffer irreversible accuracy degradation.

### Mechanism 2: Heterogeneity-Aware Configuration via Multi-Armed Bandit
Dynamically assigning device-specific pruning ratios and LoRA ranks based on observed performance can reduce straggler effects without requiring prior knowledge of device capabilities. The S-UCB algorithm treats each configuration as an "arm" and balances exploration with exploitation. The reward function incorporates loss reduction, time alignment with average completion time, and LoRA module importance. Core assumption: The reward signal adequately captures the trade-off between accuracy and efficiency, and the environment is sufficiently stationary for the bandit to learn. Evidence: Strongest device completes in ~70% of the weakest device's time with uniform settings, causing ~30% idle time. Break condition: If device capabilities change rapidly, the bandit's historical estimates may become stale and lead to suboptimal configurations.

### Mechanism 3: LoRA-Guided Importance Estimation for Frozen Weights
Gradients from LoRA modules can approximate the importance of frozen backbone weights for pruning decisions, circumventing the need to compute gradients for the full model. When a frozen weight is removed, setting (BA)m,n = -Wm,n allows the LoRA gradients to estimate the induced error via first-order Taylor approximation. Core assumption: LoRA gradients are sufficiently correlated with the importance of frozen weights. Evidence: References APT which uses activation-gradient products but is too resource-intensive for devices. Break condition: If LoRA rank is too low to capture the complexity of the downstream task, the importance estimates may be inaccurate.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: FedSpine freezes the backbone LLM and only trains LoRA's A and B matrices. Understanding that ∆W = BA and that rank r controls the capacity of adaptations is essential for grasping why adaptive rank assignment matters.
  - Quick check question: If a device has LoRA rank 4 instead of 16, approximately how many fewer trainable parameters does it have for a d×k weight matrix?

- **Structured Pruning of Transformers**
  - Why needed here: FedSpine prunes attention heads and FFN channels as groups (not individual weights). The dependency rules (e.g., {query, key, value, out} must be pruned together) are critical for implementation.
  - Quick check question: Why does structured pruning provide inference speedups on standard hardware while unstructured pruning often does not?

- **Multi-Armed Bandit Exploration-Exploitation Trade-off**
  - Why needed here: The S-UCB algorithm must balance using known-good configurations vs. trying new ones. The upper confidence bound adds an exploration bonus that decreases as a configuration is sampled more.
  - Quick check question: If a device consistently receives low rewards for configuration A but high rewards for configuration B, what should happen to Ut(A) relative to Ut(B) over time?

## Architecture Onboarding

- **Component map**: Server-side MAB agent -> Server-side LoRA aggregator -> Communication layer -> Device-side pruning module -> Device-side LoRA fine-tuning module -> Device-side state recorder

- **Critical path**: 
  1. Server runs S-UCB to select (pt_i, rt_i) for each device based on previous rewards
  2. Server distributes aggregated LoRA weights; devices shrink/expand to their assigned rank
  3. Devices run τ local fine-tuning iterations, estimate weight importance via moving average
  4. Devices prune lowest-importance groups to reach pt_i
  5. Devices continue fine-tuning, record completion time and loss change
  6. Devices upload LoRA weights and state; server computes rewards and aggregates

- **Design tradeoffs**:
  - Higher τ (local iterations): More stable importance estimates but risk of local overfitting and drift from global model. Table 8 shows τ=20 is optimal; τ=10 is too noisy, τ=40 causes drift.
  - Larger exploration bonus (s in Eq. 5): More configuration exploration but slower convergence to optimal. The discount factor λ trades off recent vs. historical performance.
  - Target pruning ratio vs. accuracy: Section 2.4 notes that pruning ratio 0.3 vs. 0.2 causes 1.4% accuracy drop on MNLI, requiring higher LoRA rank to recover.

- **Failure signatures**:
  - Accuracy collapse: Pruning ratio too aggressive relative to LoRA rank; check if I(Btotal_i) is low for devices with high pruning ratios
  - Straggler persistence: MAB not adapting; check if reward function is dominated by one term
  - Aggregation bias: Zero-padding heterogeneous LoRA modules instead of reconstructing ∆W first biases toward higher-rank devices

- **First 3 experiments**:
  1. Baseline heterogeneity test: Run FedSpine on 3 device types (AGX, NX, TX2) with uniform vs. MAB-adaptive configurations. Measure per-round completion time variance and final accuracy.
  2. Ablation on τ: Sweep τ ∈ {10, 20, 30, 40} on a single task (SST-2) to reproduce Table 8 and observe the trade-off between importance estimate quality and local drift.
  3. LoRA-guided vs. random pruning: Replace importance criterion with random group selection while keeping rest of FedSpine fixed. This isolates the contribution of the LoRA-guided importance criterion.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations and scope of the current work.

## Limitations
- Experimental evaluation is limited to 80 devices, representing a small-scale or cross-silo setting rather than massive, cross-device federated networks with millions of clients
- Validation restricted to RoBERTa (110M) and LLaMA-7B, while industry trends favor much larger models for on-device AI
- Focuses exclusively on structured pruning for size reduction, without exploring integration with quantization techniques for further memory reduction

## Confidence
- **S-UCB algorithm implementation**: Medium - hyperparameters (λ, s, δ) not fully specified
- **LoRA-guided importance criterion**: Medium - exact normalization of I(Btotal) across layers not detailed
- **Hardware-specific tuning**: Low - exact tuning for Jetson devices and server requirements unclear
- **Scalability claims**: Low - large-scale client network performance not demonstrated
- **Major results**: High - 1.4×-6.9× speedup and 0.4%-4.5% accuracy improvements are well-supported by experimental data

## Next Checks
1. Validate the S-UCB hyperparameter sensitivity by running ablation studies on λ and s values to determine their impact on convergence speed and final accuracy
2. Test the reproducibility of the iterative prune-tune advantage by implementing both sequential and iterative variants on a new task to confirm the 0.5%-1.7% accuracy gain
3. Assess the scalability of the MAB overhead by simulating 1000+ clients and measuring the computational cost of maintaining and updating decision trees for each client