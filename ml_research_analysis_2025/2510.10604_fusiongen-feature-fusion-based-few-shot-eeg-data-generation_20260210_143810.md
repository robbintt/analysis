---
ver: rpa2
title: 'FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation'
arxiv_id: '2510.10604'
source_url: https://arxiv.org/abs/2510.10604
tags:
- data
- feature
- trials
- uni00000048
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data scarcity and inter-subject
  variability in EEG-based brain-computer interfaces (BCIs) by proposing FusionGen,
  a novel few-shot EEG data generation framework. FusionGen employs disentangled representation
  learning and feature fusion to synthesize diverse EEG signals from limited samples.
---

# FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation

## Quick Facts
- arXiv ID: 2510.10604
- Source URL: https://arxiv.org/abs/2510.10604
- Reference count: 39
- Primary result: FusionGen achieves 58.02% accuracy on BNCI2014001 dataset with 10 trials, outperforming baseline by 4.16%

## Executive Summary
FusionGen addresses data scarcity and inter-subject variability in EEG-based BCIs through a novel few-shot data generation framework. The method employs disentangled representation learning and feature fusion to synthesize diverse EEG signals from limited samples. By integrating features across trials through a feature matching fusion module and combining them with lightweight feature extraction and reconstruction, FusionGen maintains data diversity while ensuring trainability under severe data constraints. Experimental results on multiple public EEG datasets demonstrate significant performance improvements over existing augmentation techniques.

## Method Summary
FusionGen introduces a framework that leverages disentangled representation learning and feature fusion for EEG data generation. The core approach integrates features across trials using a feature matching fusion module, which combines them with a lightweight feature extraction and reconstruction pipeline. This architecture ensures both data diversity and trainability when working with extremely limited samples, making it particularly suitable for few-shot BCI scenarios where traditional data augmentation methods fall short.

## Key Results
- Achieved 58.02% classification accuracy on BNCI2014001 dataset with only 10 calibration trials
- Outperformed baseline accuracy of 53.62% without augmentation by 4.16%
- Demonstrated significant improvements over existing augmentation techniques across multiple publicly available EEG datasets

## Why This Works (Mechanism)
FusionGen's effectiveness stems from its ability to disentangle EEG features into source-specific and target-relevant components, allowing for more effective cross-subject transfer. The feature matching fusion module enables the integration of diverse trial information while maintaining the distinctive characteristics needed for accurate classification. This approach addresses the dual challenges of data scarcity and inter-subject variability that typically hinder BCI performance when working with limited calibration data.

## Foundational Learning
- **Disentangled Representation Learning**: Separates EEG features into distinct components to isolate source-specific from target-relevant information; needed for cross-subject adaptation; quick check: verify component separability through visualization
- **Feature Matching Fusion**: Integrates information across trials while preserving diversity; needed to maintain trainability under few-shot conditions; quick check: measure feature similarity before and after fusion
- **Lightweight Feature Extraction**: Efficiently extracts relevant features without excessive computational overhead; needed for real-time BCI applications; quick check: benchmark computational requirements against baseline methods
- **Cross-Subject Transfer**: Enables model adaptation from source subjects to target subjects; needed to address inter-subject variability; quick check: evaluate performance across different subject pairs
- **Few-Shot Learning**: Trains models with minimal data samples; needed for BCI systems with limited calibration time; quick check: test performance with varying numbers of calibration trials
- **EEG Signal Synthesis**: Generates realistic synthetic EEG data; needed to augment limited training sets; quick check: conduct qualitative assessment of synthetic data realism

## Architecture Onboarding
- **Component Map**: Raw EEG Data -> Feature Extraction -> Disentangled Representation -> Feature Matching Fusion -> Reconstruction -> Synthetic EEG Data
- **Critical Path**: The feature matching fusion module serves as the critical component, as it integrates trial information and determines the quality of synthesized data
- **Design Tradeoffs**: Balances between maintaining data diversity (through feature fusion) and ensuring computational efficiency (through lightweight extraction); the framework prioritizes diversity over raw speed
- **Failure Signatures**: Poor cross-subject transfer when EEG patterns are highly overlapping; reduced performance when feature disentanglement assumptions are violated
- **First 3 Experiments**: 1) Test on motor imagery datasets to validate paradigm-specific performance; 2) Conduct ablation studies removing the feature matching fusion module; 3) Evaluate long-term stability by generating synthetic data over extended periods

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Relies on the assumption that EEG features can be cleanly separated into source-specific and target-relevant components, which may not hold for all BCI paradigms
- Performance improvements are primarily demonstrated on specific datasets (e.g., BNCI2014001), raising questions about generalizability to other BCI protocols
- Computational overhead of the feature matching fusion module was not quantitatively benchmarked against simpler augmentation methods

## Confidence
- **High confidence**: The core framework design (disentangled representation + feature fusion) is technically sound and addresses a well-documented problem in BCI research
- **Medium confidence**: The quantitative performance improvements are likely valid for the tested datasets but may not extrapolate perfectly to all EEG-based BCIs
- **Low confidence**: Claims about FusionGen's ability to "bridge source and target distributions" under extreme scarcity are theoretically plausible but lack rigorous statistical validation across diverse BCI tasks

## Next Checks
1. Evaluate FusionGen on non-motor imagery datasets (e.g., P300 speller, SSVEP) to test cross-paradigm robustness
2. Conduct ablation studies to isolate the contribution of the feature matching fusion module versus standard augmentation techniques
3. Perform long-term stability analysis by generating synthetic data over extended periods and assessing classifier performance drift