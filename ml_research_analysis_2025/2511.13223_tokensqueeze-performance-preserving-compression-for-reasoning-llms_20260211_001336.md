---
ver: rpa2
title: 'TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs'
arxiv_id: '2511.13223'
source_url: https://arxiv.org/abs/2511.13223
tags:
- reasoning
- angle
- arxiv
- accuracy
- tokensqueeze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TokenSqueeze addresses the inefficiency of long chain-of-thought
  traces in reasoning large language models by compressing reasoning depth while preserving
  accuracy. It adaptively selects self-generated samples whose reasoning depth matches
  problem complexity and refines linguistic expression using distribution-aligned
  rewriting to enhance clarity and conciseness.
---

# TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs

## Quick Facts
- arXiv ID: 2511.13223
- Source URL: https://arxiv.org/abs/2511.13223
- Reference count: 40
- Primary result: Achieves up to 50% token reduction on MATH500 while maintaining accuracy

## Executive Summary
TokenSqueeze addresses the inefficiency of long chain-of-thought traces in reasoning large language models by compressing reasoning depth while preserving accuracy. It adaptively selects self-generated samples whose reasoning depth matches problem complexity and refines linguistic expression using distribution-aligned rewriting to enhance clarity and conciseness. Comprehensive experiments show that TokenSqueeze reduces token usage by up to 50% on MATH500 benchmark while maintaining or improving accuracy, achieving higher AUC scores under token budget constraints. The method relies exclusively on self-generated data, avoiding the need for manually curated short-answer datasets.

## Method Summary
TokenSqueeze operates through three core components: adaptive reasoning depth selection, KL-constrained intra-step refinement, and length-regularized DPO training with SFT stabilization. The method samples multiple reasoning traces per problem, selects shorter correct traces for easier problems while retaining longer ones for harder problems based on correctness rates, then applies linguistic refinement under KL divergence constraints. The refined pairs train the model via a length-penalized DPO objective combined with SFT to prevent reward collapse. This approach achieves significant token reduction while maintaining or improving reasoning accuracy.

## Key Results
- Achieves up to 50% token reduction on MATH500 benchmark while preserving accuracy
- Outperforms baselines with higher AUC scores under 32K token budget constraint
- Maintains accuracy through adaptive selection and KL-constrained refinement mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Reasoning Depth Selection
The method matches reasoning depth to problem complexity by using dynamic quantile selection. For problems with high correctness rates (easier problems), it selects shorter reasoning chains, while retaining longer chains for harder problems with lower correctness rates. This prevents over-compression of complex problems while enabling aggressive compression of simpler ones. The mechanism breaks if α is set too high (under-compression) or too low (over-compression), as shown in Figure 4.

### Mechanism 2: KL-Constrained Intra-Step Refinement
Each reasoning step is rewritten by sampling K=64 candidate rewrites and selecting the shortest that satisfies D_KL(P_θ(·|original) || P_θ(·|rewritten)) < ε (ε=0.005). This ensures semantic preservation while compressing linguistic expression. The local 512-token window approximation introduces error for long-range dependencies. If ε is too restrictive, compression is insufficient; if too loose, semantic drift occurs.

### Mechanism 3: Length-Regularized DPO with SFT Stabilization
The training objective adds an adaptive margin λ·log(ℓ(yl)/ℓ(yw)) to the DPO loss, combining with SFT at η=0.5 weighting. This encourages conciseness while preventing the reward collapse that occurs with pure DPO. Pure DPO without SFT drops accuracy to 48.3%, while the full method achieves 57.5%.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core training paradigm being extended; understand how implicit reward models work to modify the objective. Quick check: Can you derive why DPO avoids training an explicit reward model by reparameterizing the optimal policy?

- **KL Divergence for Distributional Constraints**: Used as semantic preservation constraint during refinement; understanding asymmetry and local approximation is critical. Quick check: Why does the paper approximate full-sequence KL with a local token window, and what error does this introduce?

- **Chain-of-Thought Structure (Depth vs. Verbosity)**: Core insight that "reasoning depth" (logical steps) differs from "linguistic expression" (how steps are verbalized). Quick check: How would you determine whether compression removed a necessary reasoning step versus just verbose language?

## Architecture Onboarding

- **Component map**: Data Construction (Self-sample → Filter correct → Adaptive select → Intra-step refine) -> Training (Preference pairs → DPO-L + SFT → Fine-tuning) -> Evaluation (Accuracy, token length, AUC under budget)

- **Critical path**: Preference pair quality (Adaptive Selection + Refinement) → Training stability (SFT component) → Final compression/accuracy tradeoff

- **Design tradeoffs**: α (adaptive quantile) controls depth preservation vs compression; ε (KL threshold) balances semantic fidelity vs compression; λ (length penalty) adjusts compression pressure; η (SFT weight) balances stability vs compression

- **Failure signatures**: Accuracy drops >2% indicate α too aggressive or refinement removed critical steps; compression <30% suggests ε too conservative, α too high, or λ too low; training loss spikes indicate SFT component issues

- **First 3 experiments**:
  1. Baseline reproduction: Run DeepSeek-R1-Distill-Qwen-7B on MATH500, verify ~92.8% accuracy and ~3638 avg tokens
  2. Adaptive selection ablation: Compare Shortest vs Q-FIX vs Q-DYN on 500-problem subset; confirm α=0.2 achieves best accuracy-efficiency balance
  3. KL threshold sweep: Test ε ∈ {0.001, 0.005, 0.01, 0.02} on refinement step; plot compression ratio vs downstream task accuracy to validate 0.005 choice

## Open Questions the Paper Calls Out

### Open Question 1
Can the KL divergence threshold ε be adaptively tuned based on context difficulty rather than set as a fixed heuristic value? The authors note this was determined heuristically through limited preliminary experiments and plan to develop adaptive mechanisms in future work.

### Open Question 2
How does TokenSqueeze perform when extended to an online reinforcement learning setting with real-time preference generation and policy updates? The current offline paradigm limits adaptivity and prevents continuous refinement based on new feedback.

### Open Question 3
How does effectiveness scale to significantly larger model sizes (70B+ parameters) beyond the 7B and 1.5B models tested? The paper notes different compression performance between 7B and 1.5B models, suggesting non-uniform scaling behavior.

### Open Question 4
Does TokenSqueeze maintain effectiveness when the base model produces low-quality initial reasoning traces? The method relies on self-generated data quality, but error propagation effects when initial correctness rates are very low remain unexplored.

## Limitations

- Adaptive selection assumes correctness rate reliably correlates with problem difficulty across diverse domains
- KL-constrained refinement relies on local token windows that may miss long-range semantic dependencies
- Method requires self-sampling at inference time, creating computational overhead that scales linearly with dataset size
- Experimental scope limited to mathematical reasoning benchmarks, leaving domain generalization questions open

## Confidence

- **High confidence**: Core observation that combining adaptive depth selection with KL-constrained refinement achieves substantial compression while maintaining accuracy
- **Medium confidence**: Claim that self-generated data alone suffices for performance-preserving compression (not validated against curated datasets)
- **Medium confidence**: Mechanism explanations, particularly local KL divergence adequacy for semantic preservation
- **Low confidence**: Scalability claims beyond MATH500 (unproven on other reasoning domains)

## Next Checks

1. **Domain Generalization Test**: Apply TokenSqueeze to non-mathematical reasoning tasks (e.g., logical deduction or code generation) and measure whether the adaptive depth selection mechanism generalizes or requires domain-specific calibration of α.

2. **Long-Range Dependency Analysis**: Systematically evaluate cases where the 512-token KL approximation window breaks down by constructing artificial examples with semantically critical information beyond the local window, measuring accuracy degradation.

3. **Alternative Data Source Comparison**: Train an equivalent model using manually curated short-answer datasets instead of self-generated samples, directly comparing accuracy-efficiency tradeoffs to validate the claim that self-generation alone is sufficient.