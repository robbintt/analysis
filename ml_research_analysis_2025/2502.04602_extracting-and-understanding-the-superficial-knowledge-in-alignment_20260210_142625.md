---
ver: rpa2
title: Extracting and Understanding the Superficial Knowledge in Alignment
arxiv_id: '2502.04602'
source_url: https://arxiv.org/abs/2502.04602
tags:
- knowledge
- superficial
- alignment
- aligned
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to isolate and extract "superficial
  knowledge" from aligned language models by modifying only the final linear projection
  head. The extracted knowledge primarily captures stylistic patterns and token restyling
  without affecting deeper reasoning capabilities.
---

# Extracting and Understanding the Superficial Knowledge in Alignment

## Quick Facts
- arXiv ID: 2502.04602
- Source URL: https://arxiv.org/abs/2502.04602
- Reference count: 26
- Primary result: Superficial knowledge extracted via linear head modification achieves 58% math accuracy and 78% factual answering, demonstrating transferable alignment gains without deep reasoning changes

## Executive Summary
This paper proposes a method to isolate and extract "superficial knowledge" from aligned language models by modifying only the final linear projection head. The extracted knowledge primarily captures stylistic patterns and token restyling without affecting deeper reasoning capabilities. Experiments show that superficial knowledge accounts for significant alignment gains in safety and detoxification tasks (achieving 58% accuracy in math and 78% in factual answering) but falls short in knowledge-intensive tasks requiring reasoning and contextual understanding. The extracted superficial knowledge is transferable across models and can restore alignment in compromised models, demonstrating practical advantages for efficient alignment and recovery.

## Method Summary
The method extracts superficial alignment knowledge by learning a residual matrix (ΔWb) added to the final linear projection head of a base model, while keeping the transformer backbone frozen. This residual is optimized through knowledge distillation, minimizing KL divergence between the base model with residual and the aligned model's output distributions. For cross-model transfer, the method operates in top-k logit space, learning a linear transformation that maps base model logits to approximate aligned model logit differences. The approach constrains modifications to shallow structures, preserving deeper knowledge while capturing alignment patterns that manifest as token distribution shifts, particularly at early response positions.

## Key Results
- Extracted superficial knowledge achieves 58% accuracy on GSM8K math problems versus 93% for full aligned models
- Safety and detoxification improvements show superficial knowledge accounts for 78% of aligned model performance on factual answering tasks
- Cross-model transfer from 7B to 13B models demonstrates +0.102 GSM accuracy gain, validating transferability
- Position analysis shows alignment knowledge concentrates in first ~10 tokens, with KL divergence reduction concentrated at early positions

## Why This Works (Mechanism)

### Mechanism 1: Superficial Knowledge Extraction via Linear Residual Projection
The method isolates alignment gains by learning ΔWb through knowledge distillation while freezing the backbone. This captures stylistic restyling through token distribution shifts that can be approximated by linear adjustments to the final projection head. The core assumption is that superficial alignment patterns manifest primarily in final token selection rather than requiring deep representation changes.

### Mechanism 2: Position-Weighted Alignment Density
Alignment knowledge concentrates disproportionately in early response positions (first ~10 tokens) where stylistic framing tokens establish response structure. These shifts are readily captured by the linear projection head because they involve high-probability token redirects rather than reasoning-dependent selections.

### Mechanism 3: Cross-Model Transfer via Top-k Logit Space Alignment
Superficial knowledge transfers across model sizes by operating in top-k logit space rather than hidden state space. The method learns a linear transformation that maps base model top-k logits to approximate aligned model logit differences, balancing informativeness against transferability.

## Foundational Learning

- **Concept: Knowledge Distillation for Distribution Matching**
  - Why needed: The extraction method uses KL divergence between aligned and base+residual model outputs as the training objective
  - Quick check: Can you explain why minimizing KL(P_aligned || P_base+residual) preserves the aligned model's token distribution without introducing external knowledge?

- **Concept: Linear Projection Head in Transformer Decoders**
  - Why needed: The method's constraint to modify only the final linear layer requires understanding how hidden states map to vocabulary logits
  - Quick check: If you modify the linear projection head weights, which aspects of model behavior can and cannot change?

- **Concept: Trade-off Between Informativeness and Transferability**
  - Why needed: Section 3.1's top-k selection requires balancing coverage of important tokens against cross-model consistency
  - Quick check: Why would including more logits (higher k) reduce transfer performance even though it provides more information?

## Architecture Onboarding

- **Component map:**
  Input x_t → Backbone f_b(x_t) [FROZEN] → Hidden state h_t → Linear head W_b + ΔW_b → Logits l_t → Softmax → P_t
  For transfer: Base logits (top-k) → W_trans → Logit adjustment

- **Critical path:**
  1. Load base model and aligned model
  2. Extract ΔW_b via distillation (Section 2.2, Equation 3)
  3. Validate on held-out samples comparing base+superficial vs. aligned
  4. For transfer: extract W_trans in top-k logit space, apply to target model

- **Design tradeoffs:**
  - k value selection: Higher k captures more shift tokens but introduces non-transferable noise
  - Hidden state vs. logit space: Hidden state extraction is more precise but not transferable
  - Distillation vs. fine-tuning: Distillation preserves knowledge isolation

- **Failure signatures:**
  - Superficial-only model produces structured responses but calculation errors
  - Safety recovery fails if fine-tuning damaged deeper knowledge beyond superficial layer
  - Transfer produces high harmful rate if k is too low or too high

- **First 3 experiments:**
  1. Validate extraction fidelity: Train ΔW_b on LLaMA2-7b base/chat pair, measure KL divergence reduction
  2. Characterize position dependence: Log per-position KL divergence between base+superficial and aligned models
  3. Test transfer boundary: Extract superficial knowledge from 7B model, apply to 13B model with varying k values

## Open Questions the Paper Calls Out

### Open Question 1
How can the non-superficial (deep) knowledge in alignment be systematically characterized and extracted beyond the current analysis? The paper only isolates superficial knowledge via the final linear projection head; deeper knowledge involving reasoning and contextual integration remains unquantified.

### Open Question 2
Does modifying transformer layers beyond the final linear head reveal additional superficial knowledge, or does superficiality fundamentally reside in the output projection? The paper's design choice assumes superficial knowledge localizes to the output layer without empirical validation against alternative extraction architectures.

### Open Question 3
Can superficial knowledge transfer robustly across different model families (not just same-family scaling), and what architectural features enable or block transfer? Current evidence only covers same-family scaling with shared vocabularies; cross-family transfer would test whether superficial patterns are universal or architecture-specific.

## Limitations

- The claim that ΔWb captures only stylistic patterns without deeper reasoning is supported by performance gaps but doesn't definitively prove what knowledge remains in the backbone
- Cross-model transfer assumes consistent top-k logit space relationships across model scales, but this relationship may not generalize beyond LLaMA2-family models
- Safety and detoxification improvements don't address whether aligned model internal representations contain complementary safety mechanisms not captured by token distribution shifts

## Confidence

- **High Confidence:** The mechanism of learning ΔWb through knowledge distillation to match aligned model token distributions is well-established and reproducible
- **Medium Confidence:** The claim that superficial knowledge concentrates in early response positions is supported by per-position KL divergence analysis but could be influenced by prompt formatting conventions
- **Low Confidence:** The assertion that extracted superficial knowledge can fully restore compromised models' alignment assumes safety failures are purely superficial rather than involving deeper representation corruption

## Next Checks

1. **Cross-Architecture Transfer Validation:** Extract superficial knowledge from LLaMA2-7B and apply it to a non-LLaMA2 model (e.g., Mistral-7B or Gemma-7B). Measure transfer effectiveness by comparing safety metrics and factual accuracy improvements.

2. **Per-Token Shift Attribution Analysis:** For 100 random samples, decompose the ΔWb contribution by individual token positions and compute the percentage of alignment gains attributable to specific token types (safety markers, stylistic phrases, content tokens).

3. **Deeper Knowledge Ablation Study:** Create a variant where the backbone is partially unfrozen (e.g., only layer norm parameters trainable) and compare superficial knowledge extraction effectiveness against the fully frozen baseline.