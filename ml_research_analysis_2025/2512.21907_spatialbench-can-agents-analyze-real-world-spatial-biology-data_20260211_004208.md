---
ver: rpa2
title: 'SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?'
arxiv_id: '2512.21907'
source_url: https://arxiv.org/abs/2512.21907
tags:
- cell
- spatial
- type
- seeker
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpatialBench is a benchmark of 146 verifiable problems distilled
  from real spatial transcriptomics analysis workflows. Each problem snapshots experimental
  data immediately prior to an analysis step and pairs it with a deterministic grader
  that evaluates recovery of a key biological result.
---

# SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?

## Quick Facts
- arXiv ID: 2512.21907
- Source URL: https://arxiv.org/abs/2512.21907
- Reference count: 40
- Primary result: AI agents achieve 20-38% accuracy on verifiable spatial biology problems, with harness design often more impactful than base model choice

## Executive Summary
SpatialBench evaluates AI agents on 146 verifiable problems distilled from real spatial transcriptomics analysis workflows. Each problem snapshots experimental data immediately prior to an analysis step and pairs it with a deterministic grader that evaluates recovery of a key biological result. Results across 9 model families show accuracy remains low (20-38%) with strong interactions between models and both tasks and platforms. The choice of agent harness—including tools, prompts, control flow, and execution environment—has a large effect on outcomes, often exceeding the impact of base model selection.

## Method Summary
SpatialBench provides 146 verifiable problems from real spatial transcriptomics workflows across 5 platforms and 7 task categories. Each problem includes an AnnData snapshot, task prompt, answer schema, and deterministic grader. Agent harnesses (Base, Claude Code, Latch) are evaluated with step limit=100. Grader families include MultipleChoice, NumericTolerance, MarkerGenePrecisionRecall (P@K), LabelSetJaccard, and DistributionComparison. The evaluation uses 3 runs per problem with two-stage aggregation and 95% t-based CIs.

## Key Results
- AI agents achieve 20-38% accuracy across 9 model families on spatial biology tasks
- Harness design has larger impact on accuracy than base model selection (up to 23+ percentage points)
- Performance varies dramatically by task type: QC/cell typing (~10-22%) vs. dimensionality reduction/spatial analysis (40-53%)
- Same model shows 15-20 point performance swings across different spatial platforms

## Why This Works (Mechanism)

### Mechanism 1: Harness-Mediated Capability Unlocking
The agent harness (tools, prompts, control flow, execution environment) can shift accuracy as much or more than base model selection. A well-designed harness stabilizes multi-step exploration, provides assay-aware tooling, and structures intermediate verification—allowing models to productively iterate rather than thrash. The Latch harness improved Opus-4.5 from 38.4% to 61.7% (+23.3 points), exceeding the model-to-model gap under constant harness.

### Mechanism 2: Task-Category Calibration Gaps
Performance varies dramatically by task type, with QC and cell typing systematically underperforming dimensionality reduction and spatial analysis. QC and cell typing require contextual scientific judgment calibrated to assay-specific conventions (e.g., spatial-appropriate `min_genes` thresholds of 5–20 vs. scRNA-seq defaults of 100–200). Models without this calibration apply wrong priors, leading to systematic failures.

### Mechanism 3: Productive vs. Retry-Driven Exploration
Models differ in whether additional steps translate to accuracy gains or merely indicate thrashing. Opus-4.5 shows monotonically increasing pass rates with step count (26% → 50% from 1 to 6+ steps), suggesting productive refinement. Grok variants show flat pass rates (~27–31%) regardless of steps, indicating retry loops without learning.

## Foundational Learning

- **AnnData / Scanpy data model:** All 146 evaluations use AnnData objects with `.obs`, `.var`, `.uns` slots. Understanding this structure is prerequisite to reading problem states and writing correct analysis code.
  - Quick check: If an agent needs to access pre-computed principal component loadings, which AnnData attribute should it inspect?

- **Spatial transcriptomics assay diversity:** The benchmark spans five platforms (Visium, Xenium, MERFISH, Seeker, AtlasXomics) with different resolution, sensitivity, and artifact profiles. Platform-appropriate QC thresholds are a primary failure mode.
  - Quick check: Why would a `min_genes=200` filter appropriate for scRNA-seq be unsuitable for imaging-based spatial assays?

- **Agent harness architecture:** The paper's central claim is that harness design rivals model selection. Understanding tool routing, control flow, and execution isolation is necessary to interpret results or build improved systems.
  - Quick check: What four components does the paper identify as comprising an agent harness?

## Architecture Onboarding

- **Component map:** Problem snapshot -> Agent harness (system prompt, tools, control flow, execution sandbox) -> Grader pipeline (extraction → evaluation → pass/fail) -> Evaluation infrastructure (batched execution, isolation, trajectory logging)

- **Critical path:** 1) Load problem snapshot into isolated workspace 2) Agent executes multi-step analysis 3) Agent emits structured answer in `<EVAL_ANSWER>` tags 4) Grader extracts and evaluates against ground truth 5) Log trajectory (tool calls, stdout/stderr, intermediate artifacts)

- **Design tradeoffs:** Grader brittleness vs. reproducibility (deterministic graders enable automated evaluation but may penalize valid alternative answers); Step budget vs. exploration depth (low limits prevent runaway costs but truncate productive exploration); Prompt generality vs. assay-specificity (generic prompts test generalization but underperform on platform-specific tasks)

- **Failure signatures:** Format errors (malformed JSON, missing required fields); Thrashing (high step counts without accuracy gain); Threshold miscalibration (applying scRNA-seq defaults to spatial data); Shortcut exploitation (answering from prior knowledge without data interaction)

- **First 3 experiments:**
  1. Run the same model under base, Claude Code, and minimal custom harness to measure accuracy delta and identify driving components
  2. Select 5 worst-performing evaluations per platform and manually inspect trajectories to distinguish calibration failures from reasoning failures
  3. For a subset of evaluations, generate alternative correct answers via different valid analysis paths and verify grader acceptance

## Open Questions the Paper Calls Out

### Open Question 1
How can agent harnesses be systematically optimized to improve performance on spatial biology tasks? The paper shows harness choice can change accuracy by 23+ percentage points, but lacks a principled framework for harness design; optimization appears empirical and ad hoc.

### Open Question 2
What training data or scaffolding approaches can transfer tacit scientific judgment to models for QC and cell typing tasks? Models achieve ~10–22% accuracy on QC tasks despite strong performance elsewhere; domain calibration varies dramatically across model families.

### Open Question 3
How do agents perform on longer-horizon workflows involving compounding errors and iterative revision? SpatialBench evaluates isolated steps but not multi-step workflows where earlier errors propagate; real analysis requires such iteration.

### Open Question 4
What assay-aware scaffolding can address the 15–20 point performance swings across spatial platforms? Same model varies substantially by platform; platform-specific artifacts and conventions are not captured by generic tools.

## Limitations
- The 146 problems represent a curated subset of possible tasks and may underweight certain biological questions or analysis patterns
- Platform representation is uneven, with 86/146 evaluations on Xenium alone, potentially biasing results
- Determinism relies on problem design that may not reflect the open-ended nature of real biological analysis

## Confidence

- **High confidence:** Harness design has large effects on accuracy (exceeding model-to-model differences) supported by within-model comparisons and magnitude of observed deltas (23+ percentage points); Task-category performance gaps are robust across multiple model families
- **Medium confidence:** Productive vs. retry-driven exploration mechanism relies on proxies (step count as exploration depth) that may not capture all relevant behaviors; Calibration gap explanation for QC/cell typing failures is plausible but not definitively proven
- **Low confidence:** Claim that poor QC/cell typing performance indicates fundamental model limitations rather than grader inflexibility is not definitively established

## Next Checks
1. For the 10 lowest-performing QC evaluations, generate multiple alternative correct answers using different valid analysis thresholds and verify whether graders accept reasonable variation
2. Manually inspect the 20 worst-performing evaluations across all platforms and categorize failures into calibration errors, reasoning failures, and grader inflexibility
3. Systematically disable individual harness components in the best-performing configuration and measure accuracy degradation to isolate driving elements