---
ver: rpa2
title: 'CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D'
arxiv_id: '2509.24528'
source_url: https://arxiv.org/abs/2509.24528
tags:
- object
- arxiv
- mask
- semantic
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORE-3D, a training-free pipeline for open-vocabulary
  3D semantic segmentation and object retrieval. The key innovations are progressive
  SemanticSAM refinement for accurate mask generation, context-aware CLIP embeddings
  that combine multiple visual views of each mask, and multi-view 3D mask merging
  with geometric consistency.
---

# CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D

## Quick Facts
- **arXiv ID:** 2509.24528
- **Source URL:** https://arxiv.org/abs/2509.24528
- **Reference count:** 25
- **Primary result:** State-of-the-art open-vocabulary 3D semantic segmentation (mIoU 0.29) and object retrieval (A@0.1 accuracy 41.8%)

## Executive Summary
CORE-3D introduces a training-free pipeline for open-vocabulary 3D semantic segmentation and object retrieval using RGB-D sequences. The method combines progressive multi-granularity mask generation from SemanticSAM, context-aware CLIP embeddings that aggregate multiple visual views of each mask, and multi-view 3D mask merging with geometric consistency. Evaluated on Replica and ScanNet datasets, CORE-3D achieves state-of-the-art performance in both semantic segmentation (mIoU 0.29, f-mIoU 0.56) and object retrieval (A@0.1 accuracy 41.8%, A@0.25 accuracy 35.6%) on Sr3D+. The pipeline handles complex language queries with spatial and orientation constraints through an LLM-VLM verification framework.

## Method Summary
CORE-3D processes RGB-D sequences through a three-stage pipeline: (1) Progressive SemanticSAM refinement generates multi-granularity masks, filtering by certainty thresholds and spatial overlap to avoid redundancy; (2) Context-aware CLIP embeddings combine five complementary visual crops per mask (mask-only, bounding box, large context 2.5×, huge context 4×, surroundings with mask blacked out) using weighted aggregation that subtracts surrounding context for contrastive learning; (3) Multi-view 3D mask merging uses symmetric Intersection-over-Volume (IoV) criteria to consolidate observations across views while preventing over-merging of nested objects. For retrieval, the system parses natural language queries through LLM parsing, CLIP candidate mining, VLM verification, and spatial reasoning.

## Key Results
- Achieves mIoU of 0.29 and f-mIoU of 0.56 on Replica and ScanNet for semantic segmentation
- Outperforms baselines with A@0.1 accuracy of 41.8% and A@0.25 accuracy of 35.6% on Sr3D+ object retrieval
- Progressive granularity approach (mIoU 0.29) significantly outperforms single granularity levels (best single: mIoU 0.25) and vanilla SAM (mIoU 0.22)
- Context-aware CLIP embeddings (mIoU 0.29) substantially outperform OvSeg fine-tuned approach (mIoU 0.11) on Replica

## Why This Works (Mechanism)

### Mechanism 1: Progressive Multi-Granularity Mask Generation
- **Claim:** Progressive multi-granularity mask generation reduces fragmentation while maintaining coverage of both large and small objects.
- **Mechanism:** SemanticSAM is run at increasing granularity levels (coarse to fine). At each level, only masks with low overlap with previously discovered masks are retained (thresholded by τ_o^k). This lets coarse levels capture large objects while fine levels add small details without redundancy.
- **Core assumption:** Objects can be cleanly separated by scale, and a single granularity level is insufficient for cluttered indoor scenes.
- **Evidence anchors:**
  - [section 3.1] "We only keep the set of masks N_k with more than a specific threshold of certainty τ_cer... retain only those masks whose area has less than a threshold overlap with any mask discovered at previous levels"
  - [section 5, Table 3] Shows progressive approach (mIoU 0.29) outperforms single granularity levels (best single: mIoU 0.25) and vanilla SAM (mIoU 0.22)
  - [corpus] Related work Polysemous Language Gaussian Splatting addresses similar lifting challenges but with different mask fusion strategies.
- **Break condition:** When coarse masks fail to capture medium-sized objects that get split at fine levels, creating inconsistent granularity coverage.

### Mechanism 2: Context-Aware CLIP Embeddings
- **Claim:** Weighted combination of multiple contextual views produces more robust semantic embeddings than single-crop CLIP encoding.
- **Mechanism:** For each 2D mask, five complementary crops are extracted (mask-only, bounding box, large context 2.5×, huge context 4×, surroundings with mask blacked out). These are passed through CLIP and combined as: e(m) = w_mask·e_mask + w_bbox·e_bbox + w_large·e_large + w_huge·e_huge − w_sur·e_sur. The subtraction of surroundings embedding enforces contrastive context.
- **Core assumption:** The empirically determined weights generalize across scene types and object categories, and surrounding context helps more than it confuses.
- **Evidence anchors:**
  - [section 3.2] "we construct a set of complementary visual crops for each mask that balance object detail with surrounding scene context"
  - [section 5, Table 4] Context-aware CLIP (mIoU 0.29) substantially outperforms OvSeg fine-tuned approach (mIoU 0.11) on Replica
  - [corpus] OpenMulti and related methods use similar multi-view embedding strategies but typically without the contrastive subtraction component.
- **Break condition:** When surrounding context contains semantically dominant distractors that overwhelm the target object signal, even with negative weighting.

### Mechanism 3: Symmetric Intersection-over-Volume (IoV) Merging
- **Claim:** Symmetric IoV merging prevents over-merging of nested objects while consolidating multi-view observations.
- **Mechanism:** Two 3D masks are merged only if BOTH IoV(m_a, m_b) > γ AND IoV(m_b, m_a) > γ AND |IoV(m_a, m_b) − IoV(m_b, m_a)| < δ. This prevents merging when one object is mostly contained in another (e.g., cushion on couch) while allowing true multi-view duplicates to merge.
- **Core assumption:** Nested objects in 3D have asymmetric volumetric overlap that can be distinguished from multi-view observations of the same object.
- **Evidence anchors:**
  - [section 3.3] "This symmetric–balanced IoV criterion ensures that two masks are merged only when they exhibit both high mutual overlap and comparable volumetric support"
  - [abstract] "multi-view 3D mask merging with geometric consistency" cited as key innovation
  - [corpus] LOST-3DSG and related methods use different spatial clustering approaches for 3D consolidation; corpus evidence for this specific IoV mechanism is limited.
- **Break condition:** When objects are genuinely co-located at similar scales (e.g., stack of books) rather than one containing another.

## Foundational Learning

- **Concept: Vision-Language Alignment (CLIP-style)**
  - Why needed here: The entire pipeline depends on CLIP embedding space for open-vocabulary classification—without understanding contrastive image-text pretraining, the embedding aggregation strategy won't make sense.
  - Quick check question: Can you explain why CLIP embeddings enable zero-shot classification without task-specific training?

- **Concept: Class-Agnostic Segmentation (SAM/SemanticSAM)**
  - Why needed here: The mask generation stage uses class-agnostic segmentation as a prior before semantic assignment. Understanding the difference between semantic and class-agnostic segmentation is essential.
  - Quick check question: Why would a segmentation model trained without semantic labels be preferable for open-vocabulary pipelines?

- **Concept: 3D Geometry from RGB-D Sequences**
  - Why needed here: The pipeline requires back-projecting 2D masks into 3D using depth maps and camera poses, then merging across views. Without this foundation, the volumetric merging criteria won't be interpretable.
  - Quick check question: Given a pixel (u, v), depth d, and camera pose T, how would you compute the corresponding 3D point?

## Architecture Onboarding

- **Component map:** Input RGB-D sequence → Progressive SemanticSAM refinement → Context-aware CLIP embeddings → Symmetric IoV merging → LLM-VLM retrieval pipeline
- **Critical path:** Accurate depth maps → correct 2D-3D correspondence → meaningful IoV merging. Degraded depth quality propagates through entire pipeline.
- **Design tradeoffs:**
  - More granularity levels = better coverage but higher compute cost
  - Stricter overlap thresholds = fewer false merges but risk missing valid consolidations
  - Larger context crops = richer semantics but more background noise (Section 5, Table 5 shows mask-size-based extension outperforms neighbor-coverage)
- **Failure signatures:**
  - Transparent/reflective objects: depth artifacts cause 3D point corruption
  - Highly cluttered surfaces: even progressive granularity produces over-segmented masks
  - Semantically ambiguous contexts (e.g., multiple similar objects): surroundings embedding may dominate
  - View-dependent queries: VLM orientation grounding depends on discretized yaw bins (Section 3.4)
- **First 3 experiments:**
  1. **Ablate granularity levels:** Run with K=1 (single granularity), K=3, K=5 on Replica room0. Measure mIoU and mask count to find diminishing returns point.
  2. **Weight sensitivity sweep:** Scale each of {α_h, α_l, α_o, α_m} by {0.5, 0.75, 1.0, 1.25, 1.5} while holding others fixed (replicating Figure 5). Identify which weights are most sensitive.
  3. **Context crop visualization:** For failure cases, visualize all 5 crops and their individual CLIP similarities to ground-truth class. Determine which context level most often contains the correct signal vs. noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the CORE-3D framework be extended to maintain semantic consistency in dynamic, temporally evolving environments?
- **Basis in paper:** [Explicit] The conclusion states that "extending the framework with temporal consistency... could further enhance robustness and generality."
- **Why unresolved:** The current pipeline processes image sequences into a static 3D map but lacks mechanisms to update object states, handle moving objects, or correct semantic drift over time in changing real-world scenes.
- **What evidence would resolve it:** Evaluation of the pipeline on dynamic datasets (e.g., ScanNet video sequences with object movement) demonstrating stable mIoU and retrieval accuracy over time without manual reset.

### Open Question 2
- **Question:** What specific architectures allow for deeper integration of multimodal reasoning models to improve spatial reasoning performance?
- **Basis in paper:** [Explicit] The conclusion suggests that "deeper integration with multimodal reasoning models" could enhance the system.
- **Why unresolved:** The current method uses a modular pipeline with sequential steps (LLM parsing, VLM verification, LLM reasoning) rather than a unified architecture, potentially limiting the model's ability to jointly optimize visual and linguistic constraints.
- **What evidence would resolve it:** A comparative study replacing the modular LLM/VLM pipeline with an end-to-end multimodal transformer on the Sr3D+ retrieval benchmark.

### Open Question 3
- **Question:** Can an adaptive scoring mechanism replace the fixed Top-K selection in candidate mining to improve retrieval in sparse or cluttered scenes?
- **Basis in paper:** [Inferred] Section 3.4 states the method "deliberately avoid[s] thresholding CLIP scores" because fixed thresholds are unreliable, defaulting to Top-K instead.
- **Why unresolved:** Using Top-K assumes the target is always within the top $K$ candidates; however, in sparse scenes, this may introduce noise, while in highly cluttered scenes, it may exclude the correct object if $K$ is too low.
- **What evidence would resolve it:** An ablation study comparing fixed Top-K retrieval against adaptive, scene-dependent thresholding methods on the A@0.25 accuracy metric.

## Limitations
- Method's reliance on high-quality depth maps presents a significant constraint, as depth artifacts directly corrupt 3D point reconstruction
- Multi-view merging thresholds (γ, δ) appear empirically tuned for indoor scenes and may not generalize to outdoor environments
- Context-aware embedding approach uses fixed weights that may not adapt well to varying object sizes or scene contexts

## Confidence

- **High Confidence:** The progressive mask generation mechanism (Mechanism 1) shows consistent improvement over single-granularity approaches across multiple experiments and is well-supported by quantitative results.
- **Medium Confidence:** The context-aware embedding strategy (Mechanism 2) demonstrates strong performance but relies on empirically determined weights whose generalizability across diverse scene types remains uncertain.
- **Medium Confidence:** The symmetric IoV merging criterion (Mechanism 3) is theoretically sound but has limited corpus support for this specific formulation, with performance potentially sensitive to threshold choices.

## Next Checks

1. **Generalization test:** Evaluate CORE-3D on ReplicaCAD dataset (referenced in related work) to assess performance on synthetic scenes with different object distributions and scale characteristics.
2. **Failure mode analysis:** Systematically identify and categorize failure cases where surrounding context overwhelms target object signals, particularly for semantically ambiguous regions.
3. **Threshold sensitivity:** Conduct a comprehensive ablation study varying γ and δ IoV thresholds to determine optimal values across different scene complexity levels and object density scenarios.