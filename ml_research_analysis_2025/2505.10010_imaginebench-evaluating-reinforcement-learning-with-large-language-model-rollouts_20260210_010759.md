---
ver: rpa2
title: 'ImagineBench: Evaluating Reinforcement Learning with Large Language Model
  Rollouts'
arxiv_id: '2505.10010'
source_url: https://arxiv.org/abs/2505.10010
tags:
- rollouts
- learning
- tasks
- offline
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImagineBench, the first comprehensive benchmark
  for evaluating offline reinforcement learning algorithms that leverage both real-world
  and LLM-generated imaginary rollouts. The benchmark provides datasets spanning locomotion,
  manipulation, and navigation tasks, with natural language instructions at varying
  complexity levels.
---

# ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts

## Quick Facts
- arXiv ID: 2505.10010
- Source URL: https://arxiv.org/abs/2505.10010
- Authors: Jing-Cheng Pang; Kaiyuan Li; Yidi Wang; Si-Hang Yang; Shengyi Jiang; Yang Yu
- Reference count: 5
- Primary result: Introduces the first comprehensive benchmark for evaluating offline RL algorithms that leverage real-world and LLM-generated imaginary rollouts, revealing a 28.93 percentage point performance gap between real-only (64.37%) and imaginary-augmented (35.44%) training on hard novel tasks.

## Executive Summary
ImagineBench introduces the first comprehensive benchmark for evaluating offline reinforcement learning algorithms that leverage both real-world and LLM-generated imaginary rollouts. The benchmark spans five diverse environments including locomotion, manipulation, and navigation tasks, with natural language instructions at varying complexity levels. Through systematic evaluation of state-of-the-art offline RL algorithms, the authors find that simply combining real and imaginary rollouts achieves suboptimal performance on unseen tasks (35.44% success rate for hard tasks) compared to training on real rollouts alone (64.37% success rate). This performance gap highlights the need for algorithmic advancements to better utilize LLM-generated rollouts and opens several important research directions in offline RL.

## Method Summary
ImagineBench evaluates offline RL algorithms using datasets containing both real environment rollouts and synthetic rollouts generated by LLMs. Expert policies collect real trajectories with language annotations across five environments. An LLM (Llama-2-7b-chat-hf) is fine-tuned on dynamics prediction, rollout explanation, and rollout generation tasks using the real data. The fine-tuned LLM then generates synthetic trajectories for novel tasks via goal-oriented prompts. Offline RL algorithms (BC, CQL, BCQ, TD3+BC, PRDC, COMBO) are trained on a 50/50 mix of real and imaginary data, with language instructions encoded via BERT and concatenated with state vectors. Performance is measured by success rate across four task levels: training, rephrasing, easy, and hard.

## Key Results
- Performance gap of 28.93 percentage points between real-only training (64.37%) and imaginary-augmented training (35.44%) on hard tasks
- Goal-rollout consistency degrades significantly with task complexity: 88.0% for rephrasing, 43.8% for easy, and 25.8% for hard tasks
- Imaginary rollouts contain substantial errors: 27.1% transition errors and 33.2% illegal states for hard tasks
- LIBERO environment shows near-zero performance on novel tasks due to combinatorial complexity
- Simple offline RL algorithms struggle to leverage imaginary rollouts effectively, achieving suboptimal generalization

## Why This Works (Mechanism)

### Mechanism 1: LLM Grounding via Supervised Fine-tuning on Environment Data
Fine-tuning LLMs on real environment rollouts enables them to generate synthetic trajectories for novel tasks. The LLM undergoes supervised fine-tuning on dynamics prediction, rollout explanation, and rollout generation, grounding its pre-trained knowledge to environment-specific state-action spaces and dynamics. The core assumption is that LLMs encode transferable reasoning about physical interactions that can be grounded to numerical control signals through fine-tuning. This mechanism breaks when tasks require complex behavioral composition not represented in fine-tuning data, as evidenced by hard tasks showing only 25.8% goal-rollout consistency.

### Mechanism 2: Offline RL Policy Learning from Hybrid Real-Imaginary Data
Training offline RL policies on combined real and imaginary rollouts can improve generalization to unseen tasks. Offline RL algorithms learn policies from static datasets, and by augmenting real environment data with LLM-generated trajectories for novel tasks, policies gain exposure to task distributions beyond the real dataset without requiring additional environment interaction. The core assumption is that imaginary rollouts contain sufficiently accurate task-relevant information that complements real data without introducing harmful distributional shift. This mechanism breaks when imaginary rollouts contain dynamics violations (33.2% illegality rate for hard tasks) or low goal consistency, causing policy degradation.

### Mechanism 3: Hierarchical Task Difficulty and Compositional Generalization
Policy performance degrades predictably as task complexity increases from rephrasing to easy to hard. Tasks are stratified by compositional complexity: Rephrasing tasks reuse existing trajectories with new language; Easy tasks require single-step generalization; Hard tasks require multi-step behavioral composition. The LLM's generalization capability degrades as compositional demands increase. The core assumption is that task difficulty can be reliably characterized by the degree of behavioral composition required relative to the training distribution. This mechanism breaks when tasks require sequential reasoning not captured in training, such as simultaneous picking instead of sequential execution.

## Foundational Learning

- **Concept: Offline Reinforcement Learning**
  - Why needed here: The entire benchmark evaluates offline RL algorithms that learn from static datasets without environment interaction. Understanding distributional shift and conservative learning is essential.
  - Quick check question: Why do offline RL algorithms like CQL penalize out-of-distribution actions, and what happens if they don't?

- **Concept: Language-Conditioned Policy Learning**
  - Why needed here: Policies must map natural language instructions to actions. The benchmark uses BERT to encode instructions concatenated with environment observations.
  - Quick check question: Given the instruction "push the red ball behind the blue ball," how would you structure the policy input, and what encoding challenges might arise?

- **Concept: Goal-Augmented Markov Decision Processes**
  - Why needed here: The paper formalizes the problem as M = (S, A, P, R, Î³, G) where G is the set of natural language goals, extending standard MDPs.
  - Quick check question: How does adding a goal space G change the policy objective compared to standard RL?

## Architecture Onboarding

- **Component map**: Real Rollout Collection -> LLM Fine-tuning Module -> Imaginary Rollout Generator -> Offline RL Training Pipeline -> Evaluation Harness
- **Critical path**: 1) Collect real rollouts using environment-specific expert policies (20K-100K trajectories per environment) 2) Fine-tune LLM on three SFT tasks using real rollout-instruction pairs 3) Generate imaginary rollouts by prompting LLM with novel task instructions 4) Train offline RL policy with batch sampling of 50% real + 50% imaginary data 5) Evaluate success rate on held-out task levels (rephrasing, easy, hard)
- **Design tradeoffs**:
  - Llama-2-7b chosen as backbone; larger models may improve rollout quality but increase fine-tuning costs
  - Equal 50/50 sampling; optimal ratio unknown and may vary by task difficulty
  - BERT used for instruction encoding; more powerful encoders could improve goal representation
  - No filtering applied to imaginary rollouts; filtering low-quality rollouts could improve performance
- **Failure signatures**:
  - 25.8% goal-rollout consistency for hard tasks - imaginary trajectories don't match intended goals
  - 27.1% transition errors and 33.2% illegal states in hard task rollouts
  - LLM generates simultaneous actions instead of sequential execution
  - 28.93 percentage point gap between real-only and imaginary-augmented training on hard tasks
  - LIBERO near-zero performance on novel tasks due to combinatorial complexity
- **First 3 experiments**:
  1. Train BC and CQL on real data only for Meta-world, measuring success rates across all task levels to establish upper bounds
  2. Generate 1000 imaginary rollouts for BabyAI hard tasks, manually score consistency, transition correctness, and legality to identify primary failure modes
  3. Train CQL on CLEVR-Robot with varying real/imaginary ratios (25/75, 50/50, 75/25) to determine if optimal mixing differs by task difficulty

## Open Questions the Paper Calls Out

- **How can offline RL algorithms be designed to better account for the uncertainties and potential biases in LLM-generated imaginary rollouts?**
  - Basis: Authors state that simply applying existing offline RL algorithms leads to suboptimal performance (35.44% vs 64.37% on hard tasks) and identify developing more advanced methods as a future direction
  - Why unresolved: Current algorithms treat imaginary and real rollouts similarly without accounting for distributional differences or varying quality levels
  - What evidence would resolve it: Development of algorithms that achieve comparable performance between imaginary-augmented training and real-rollout-only training on hard novel tasks

- **How can agents efficiently adapt online with limited real interactions while preserving knowledge from LLM-imaginary rollouts?**
  - Basis: Authors identify avoiding catastrophic forgetting while rapidly fine-tuning with limited real interactions as an open problem
  - Why unresolved: The trade-off between retaining imagined knowledge and correcting inaccuracies through real-world feedback remains unaddressed
  - What evidence would resolve it: Demonstration of methods that maintain performance on training tasks while improving on novel tasks after minimal online interaction

- **Can vision-language models generate spatially consistent action sequences for multi-modal RLIM tasks?**
  - Basis: Authors note extending RLIM to vision requires integrating vision-language models capable of processing and generating multi-modal rollouts
  - Why unresolved: Current work focuses only on numerical vector observations; visual observation introduces partial observability and cross-modal alignment challenges
  - What evidence would resolve it: Successful policy learning from VLM-generated rollouts on vision-based manipulation tasks with performance comparable to state-vector baselines

## Limitations

- Substantial performance gap (28.93 percentage points) between real-only and imaginary-augmented training on hard tasks indicates current LLM-generated rollouts contain significant inaccuracies
- Benchmark focuses exclusively on goal-conditioned RL with natural language instructions, limiting generalizability to non-linguistic task specifications
- Optimal ratio of real to imaginary data for training is assumed to be 50/50 without systematic exploration

## Confidence

- **High Confidence**: Benchmark design, dataset collection methodology, and baseline algorithm implementations are well-specified and reproducible. Documented performance gap between real and imaginary rollouts is a robust empirical finding.
- **Medium Confidence**: LLM fine-tuning procedure and imaginary rollout generation process are adequately described, though exact hyperparameters and prompting strategies could affect results. Hierarchical task difficulty formulation is intuitive but could benefit from additional stratification criteria.
- **Low Confidence**: Optimal real/imaginary data mixing ratio is assumed without systematic exploration. Impact of filtering low-quality imaginary rollouts is unexplored, potentially leaving performance gains on the table.

## Next Checks

1. **Data Quality Audit**: Generate 1000 additional imaginary rollouts for BabyAI hard tasks and perform systematic scoring of goal consistency, transition accuracy, and legality to quantify the relationship between rollout quality and policy performance.

2. **Data Mixing Ablation**: Systematically vary the real/imaginary data ratio (25/75, 50/50, 75/25) across all environments to identify optimal mixing strategies that could narrow the performance gap.

3. **Quality Filtering Impact**: Implement simple filtering criteria (e.g., minimum goal consistency threshold, legality checks) on imaginary rollouts and measure the effect on downstream policy performance to determine if preprocessing can salvage useful synthetic data.