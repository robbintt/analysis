---
ver: rpa2
title: 'From Input Perception to Predictive Insight: Modeling Model Blind Spots Before
  They Become Errors'
arxiv_id: '2509.20065'
source_url: https://arxiv.org/abs/2509.20065
tags:
- features
- linguistics
- language
- qwen2
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce an input-only approach to anticipate language model
  errors by analyzing token-level likelihood features derived from surprisal and information
  density. Our method uses features like surprisal, entropy, and confidence-weighted
  surprisal to detect model comprehension difficulties without relying on outputs
  or hidden states.
---

# From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors

## Quick Facts
- **arXiv ID**: 2509.20065
- **Source URL**: https://arxiv.org/abs/2509.20065
- **Reference count**: 21
- **Primary result**: Input-only error prediction using token-level likelihood features outperforms baselines across five linguistically complex datasets, especially for smaller models (1B-3B parameters).

## Executive Summary
This paper introduces a novel approach to anticipate language model errors by analyzing token-level likelihood features derived from surprisal and information density. The method uses features like surprisal, entropy, and confidence-weighted surprisal to detect model comprehension difficulties without relying on outputs or hidden states. The framework demonstrates that structured likelihood features can effectively predict errors in non-literal language tasks across five datasets, with span-localized features providing particular benefit for larger models.

## Method Summary
The approach extracts token-level probabilities from language models when processing input text, then computes four key metrics: surprisal (negative log probability), entropy (uncertainty spread), confidence-weighted surprisal (incorporating probability concentration), and contextual influence score (how tokens constrain next predictions). These features are aggregated at different granularities (sentence, expression, boundary, context) and fed into either logistic regression or MLP classifiers to predict error likelihood. The method requires only the model's input perception, not its generated outputs or hidden states.

## Key Results
- Token-level likelihood features outperform standard baselines across five linguistically complex datasets
- Smaller models (1B-3B parameters) show particularly strong performance with global feature patterns
- Span-localized features improve error detection by 13-23 F1 points for larger models on idiom/metaphor tasks
- Different feature combinations work best for different classifier types: LogReg relies on Surprisal/Entropy while MLP leverages CIS/Entropy interactions

## Why This Works (Mechanism)

### Mechanism 1: Input Likelihood Surface Encodes Comprehension Signal
Token-level probabilities over input encode latent signals of model's internal interpretation quality, enabling error prediction before generation. Models process input autoregressively, assigning probabilities conditioned on prefix. High surprisal or entropy at specific tokens indicates uncertainty in interpretation. Non-literal language disrupts expected information uniformity, creating diagnostic perturbations in the likelihood landscape that predict downstream errors. Core assumption: probability distribution over input tokens reflects genuine comprehension state, not just surface patterns.

### Mechanism 2: Span-Localization Amplifies Signal for Larger Models
Larger models show clearer error signals when features are computed over linguistically-challenging spans rather than whole sentences; smaller models benefit from global patterns. Larger models have more capacity, making global uncertainty patterns less discriminative. Localized measurement at annotated regions captures where errors originate. Boundary-level features detect integration failures. Smaller models have diffuse errors reflected globally; larger models have localized failures.

### Mechanism 3: Feature Complementarity Across Surprisal, Entropy, and Contextual Influence
Different information-theoretic measures capture distinct aspects of comprehension uncertainty, with importance varying by classifier complexity. Surprisal captures token unexpectedness (post-hoc). Entropy captures pre-decision uncertainty (spread of alternatives). CIS captures how much each token constrains the next prediction. Linear classifiers rely primarily on Surprisal and Entropy; non-linear classifiers leverage CIS interactions and Entropy patterns.

## Foundational Learning

- **Concept: Surprisal and Information Theory**
  - Why needed here: The entire method builds on surprisal (-log P(token|context)) as the foundational signal. Understanding why negative log probability measures "information content" and correlates with processing difficulty is essential.
  - Quick check question: Given a token with probability 0.01 vs 0.5, which has higher surprisal and why does this matter for comprehension?

- **Concept: Uniform Information Density (UID) Hypothesis**
  - Why needed here: The paper assumes language is optimized to distribute information evenly. Deviations from uniformity (like idioms breaking compositional expectations) create diagnostic spikes in the likelihood surface.
  - Quick check question: Why would an idiom like "kick the bucket" create a surprisal pattern different from its literal interpretation, and how does this relate to UID?

- **Concept: Autoregressive Language Modeling**
  - Why needed here: The method extracts probabilities from how the model conditions on prefixes. Understanding that P(sequence) = Î  P(ti | t<i) and how this creates the "input likelihood surface" is prerequisite.
  - Quick check question: When computing surprisal for "She kicked the bucket," what prefix does the model condition on when evaluating the token "bucket"?

## Architecture Onboarding

- **Component map**: Prompt construction -> LLM forward pass -> Logit extraction -> Token-level metrics -> Granular aggregation -> Feature vector -> Classifier -> Error probability

- **Critical path**: The pipeline processes input through the model to extract token probabilities, computes information-theoretic metrics at each position, aggregates these across specified granularities, and uses them to predict error likelihood through classification.

- **Design tradeoffs**: LogReg (interpretable, works well for smaller models) vs MLP (captures non-linear interactions, better for harder tasks). Sentence-level (generalizes without annotations) vs Span-localized (requires region knowledge, captures finer signals).

- **Failure signatures**: Baseline F1 = 0 for large models on idiom/metaphor tasks (coarse features fail). MLP/LogReg divergence on hard tasks suggests non-linear interactions. Negative localization delta when span annotations misalign with error sources.

- **First 3 experiments**:
  1. Baseline calibration on target model + dataset. If F1 > 0 on LogReg baselines, dataset has easy signal. If F1 = 0, you're in the target regime requiring fine-grained features.
  2. Ablation sweep: Train classifiers removing one metric at a time. For MLP, expect Entropy and CIS to show largest drops. If no metric shows importance, features may be too correlated.
  3. Granularity test: Compare sentence-level vs span-localized features. For models < 3B, expect sentence-level to suffice. For larger models, expect +10-20 F1 from span-localization on challenging datasets.

## Open Questions the Paper Calls Out

### Open Question 1
Can the benefits of span-localized features be retained using dynamic, unsupervised methods to identify regions of interest without relying on gold-standard annotations? The authors note that while span-localized features improve performance for larger models, their implementation relies on "task-specific linguistic structure" or dataset annotations. They conclude by pointing to "future work on automatic localization to bring these benefits without task-specific annotations" (Section 5.2).

### Open Question 2
Do input-side likelihood signals remain predictive of errors for state-of-the-art closed-source models (e.g., GPT-4o) that may restrict access to full log-probabilities? The Limitations section explicitly identifies extending this framework to "closed-source models such as OpenAI's GPT-4o" as an "important direction for future work," noting that their opacity poses challenges.

### Open Question 3
Is the link between input likelihood instability and model failure specific to non-compositional semantics, or does it generalize to reasoning and factual hallucinations? The paper restricts its evaluation to "idioms, metaphors, and metonymy" (Section 1), arguing these are cases where "misinterpretations... yield large errors." It does not test whether these input-side signals correlate with failures in logic or fact-retrieval tasks where the input is semantically ordinary.

### Open Question 4
Is there a floor of model competence below which input-side error detection becomes impossible, even if the model generates responses? The Limitations section notes that for very small models (SmolLM <2B), the evaluation failed because the models "misclassified all instances," creating a scenario where "without at least some correct predictions... the requirement for error detection... could not be met."

## Limitations
- The method requires access to full token-level probabilities, limiting applicability to closed-source models with restricted API access
- Span-localized features depend on annotated regions of interest, which may not be available for all error types or real-world applications
- Very small models may fail to produce any correct predictions, making error detection impossible regardless of feature quality

## Confidence

**High Confidence**: The core finding that token-level likelihood features can predict errors without requiring model outputs or hidden states is well-supported by consistent F1 improvements across multiple datasets and model families.

**Medium Confidence**: The claim about span-localization benefits for larger models has strong empirical support (F1 gains of 13-23 points) but relies on annotated spans that may not generalize to all error types.

**Low Confidence**: The assertion that CWS and CIS capture distinct aspects of comprehension uncertainty is supported by ablation studies but could be confounded by feature correlation.

## Next Checks

1. **Cross-dataset generalization test**: Apply the input-only error detection framework to a non-literal language dataset not used in the paper (e.g., metaphor detection in scientific text or idioms in domain-specific corpora) to validate whether the approach generalizes beyond the five studied datasets.

2. **Model architecture sensitivity analysis**: Test whether the feature importance patterns (Surprisal/Entropy for LogReg, CIS/Entropy for MLP) hold across different model families (e.g., decoder-only vs encoder-decoder, dense vs mixture-of-experts) to determine if the findings are architecture-specific or more general.

3. **Temporal stability evaluation**: Measure how input-only error prediction performance changes as models are fine-tuned on different domains or as they evolve through training, to assess whether the likelihood surface signals are stable properties or artifacts of specific training states.