---
ver: rpa2
title: On Synthetic Data Strategies for Domain-Specific Generative Retrieval
arxiv_id: '2502.17957'
source_url: https://arxiv.org/abs/2502.17957
tags:
- retrieval
- queries
- synthetic
- data
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates synthetic data generation strategies for
  domain-specific generative retrieval models. It proposes a two-stage training framework:
  supervised fine-tuning with multi-granular and constraint-based synthetic queries,
  followed by preference learning with hard negative sampling.'
---

# On Synthetic Data Strategies for Domain-Specific Generative Retrieval

## Quick Facts
- arXiv ID: 2502.17957
- Source URL: https://arxiv.org/abs/2502.17957
- Reference count: 28
- Primary result: Multi-granular synthetic queries, domain constraints, and hard negative mining improve domain-specific generative retrieval across multiple datasets

## Executive Summary
This paper proposes a two-stage training framework for domain-specific generative retrieval models that leverages synthetic data generation strategies. The approach combines supervised fine-tuning with multi-granular and constraint-based synthetic queries, followed by preference learning with hard negative sampling. Experiments on four datasets demonstrate significant improvements over baselines, with the strategy generalizing across different identifier types and achieving competitive results compared to off-the-shelf retrieval models.

## Method Summary
The method employs a two-stage training approach using Mistral-7B-Instruct-v0.3 as the base model. Stage 1 uses supervised fine-tuning (SFT) with interleaved Context2ID (chunk-to-document-ID) and Query2ID data generated by Mixtral 8x7b. The synthetic queries are generated at multiple granularities: chunk-level, sentence-level, and constraints-based (using metadata). Stage 2 applies preference optimization (RPO) using hard negatives mined from the Stage 1 model's predictions, where false positives ranked above correct documents serve as difficult negatives. Training uses constrained beam search with Trie for identifier generation at inference.

## Key Results
- Multi-granular queries (chunk + sentence) improve HIT@4 from 43.64→61.64 on MultiHop-RAG
- Domain constraints improve HIT@4 from 61.64→69.98 (MultiHop-RAG) and HIT@1 from 10.19→14.20 (AllSides)
- Hard negatives improve HIT@4 from 69.98→71.53, while random negatives degrade to 58.94
- The approach generalizes across semantic and atomic identifier types with comparable performance

## Why This Works (Mechanism)

### Mechanism 1: Multi-Granular Synthetic Query Coverage
Synthetic queries at chunk-level and sentence-level granularity capture complementary relevance signals - chunk-level queries capture higher-level semantic themes while sentence-level queries capture fine-grained details. This provides denser coverage of the query space the model may encounter at inference, improving retrieval performance when test queries span both broad topical queries and specific detail-oriented queries.

### Mechanism 2: Domain-Specific Constraint Injection
Incorporating metadata-derived constraints into synthetic queries improves the model's ability to handle domain-relevant filtering requirements. The LLM is prompted to generate queries that explicitly reference metadata attributes (e.g., author, source, political polarity), teaching the model to condition retrieval on structured constraints common in real-world search scenarios.

### Mechanism 3: Model-Guided Hard Negative Mining
Using the Stage 1 model's own false positives as hard negatives for preference learning improves ranking. The post-SFT model retrieves top-k candidates; those ranked above the correct document are highly confusing negatives. Training the model to prefer the correct document over these specific hard negatives sharpens decision boundaries where the model is already making errors, while random negatives may harm performance.

## Foundational Learning

- Concept: **Generative Retrieval vs. Dense Retrieval**
  - Why needed here: Generative retrieval encodes the corpus in model parameters and directly outputs document identifiers (not similarity scores). This requires memorization, generalization, and ranking in one model.
  - Quick check question: Can you explain why generative retrieval needs Context2ID training while dense retrieval does not?

- Concept: **Document Identifiers (Semantic vs. Atomic)**
  - Why needed here: The model must generate valid identifiers at inference. Semantic identifiers (keyword lists) are more interpretable and scalable; atomic identifiers are unique tokens for classification-style retrieval.
  - Quick check question: Given a corpus of 100K documents, what are the tradeoffs between using LLM-generated keyword identifiers vs. assigned atomic IDs?

- Concept: **Preference Optimization (RPO/DPO Family)**
  - Why needed here: Stage 2 uses RPO to refine ranking by optimizing preferences over positive/negative pairs, avoiding expensive RL (PPO) while improving beyond cross-entropy alone.
  - Quick check question: Why does RPO include an additional SFT loss term compared to vanilla DPO?

## Architecture Onboarding

- Component map:
Stage 1 (SFT): Context2ID Data (chunks→doc ID) + Query2ID Data (queries→doc ID) → Mistral-7B SFT → Model with Identifiers
Stage 2 (Preference): Model Retrieval → Hard Negative Mining → RPO Training

- Critical path:
  1. Generate semantic identifiers (LLM→keywords per document)
  2. Generate multi-granular synthetic queries (Mixtral 8x7B, temp=0.7)
  3. Interleave Context2ID + Query2ID; train SFT (2 epochs, LR=2e-5)
  4. Generate difficult query-answer pairs; run retrieval to collect hard negatives
  5. Train RPO (1 epoch, LR=1e-7, β=0.5, α=1.0)

- Design tradeoffs:
  - Chunk vs. sentence granularity: Sentences increase data volume ~10× (Table 9: 72K chunk vs. 472K sentence queries for MultiHop-RAG); balance coverage vs. training cost
  - Interleaving vs. concatenation: Interleaving implicitly upsamples Context2ID; helps memorization but increases dataset size
  - Top-k negative count: More negatives (Top-10) slightly help HIT@1/HIT@4 but may reduce MAP; task-dependent

- Failure signatures:
  - **Random negatives degrade performance**: If RPO uses corpus-random negatives, expect HIT@4 drops (~10 points per Table 8)
  - **No hard negatives available**: If SFT model is too accurate, positive ranks top-1 → no preference pairs possible
  - **Identifier collision**: Poor keyword generation causes duplicate IDs → retrieval returns wrong documents
  - **Overly difficult synthetic queries**: If preference-stage queries are unanswerable, model learns noise

- First 3 experiments:
  1. **Ablate query granularity**: Train with chunk-only vs. chunk+sentence queries; measure HIT@k on held-out test set to confirm granularity contribution
  2. **Validate hard negative necessity**: Compare RPO with random negatives vs. model-predicted hard negatives vs. no Stage 2; expect random < no-Stage-2 < hard negatives
  3. **Test identifier generalization**: Swap semantic identifiers for atomic identifiers on same data recipe; verify data strategy transfers (per Table 7)

## Open Questions the Paper Calls Out

### Open Question 1
How do these synthetic data strategies perform in dynamic settings requiring incremental learning or generalization to documents unseen during initial training? The authors note their strategies focus on SFT and preference learning, leaving "incremental learning or generalization to unseen documents" as settings that require extension.

### Open Question 2
Can the proposed framework be extended to generate complex, multi-hop queries that require reasoning across multiple documents? The authors state their synthetic queries are "mainly based on one document," whereas real-world queries may involve "multi-hop reasoning or multi-evidence comparison."

### Open Question 3
Are these multi-granular and constraint-based data strategies transferable to dense retrieval domain adaptation, and do they function differently than in generative models? The authors suggest "similar data strategies may also be effectively used to enhance dense retrieval" but call for systematic research to investigate the "differences between generative and dense model training."

## Limitations

- Scalability uncertainty: The approach is validated on corpora up to 316K documents; performance on larger corpora (>1M documents) is untested
- Hard negative dependency: The effectiveness critically depends on the Stage 1 model producing meaningful errors for preference learning
- Identifier quality sensitivity: Performance is highly dependent on LLM-generated semantic identifier quality, which may vary across domains

## Confidence

**High Confidence**: The multi-granular synthetic query approach and hard negative mining mechanisms are well-supported by ablation studies with consistent improvements across multiple datasets.

**Medium Confidence**: The domain-specific constraint injection shows improvements on some datasets but has weaker evidence on MultiHop-RAG, with effectiveness depending on metadata availability.

**Low Confidence**: The preference learning hyperparameters (β=0.5, α=1.0) are not extensively validated through ablation, and optimal parameter settings likely vary by corpus characteristics.

## Next Checks

1. **Ablation on Identifier Type**: Validate whether the data strategy transfers to atomic identifiers by replicating experiments with assigned unique tokens instead of semantic keywords, measuring performance degradation if any.

2. **Hard Negative Sensitivity Analysis**: Systematically vary the number of hard negatives (Top-1, Top-3, Top-5, Top-10) and compare against random negatives to establish optimal negative sampling strategy for each dataset.

3. **Corpus Size Scaling Test**: Evaluate the approach on a larger corpus (100K+ documents) using both semantic and atomic identifiers to identify scalability limits and potential performance degradation points.