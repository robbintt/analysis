---
ver: rpa2
title: 'Financial Named Entity Recognition: How Far Can LLM Go?'
arxiv_id: '2501.02237'
source_url: https://arxiv.org/abs/2501.02237
tags:
- llms
- financial
- entities
- entity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates three state-of-the-art LLMs\u2014GPT-4o, LLaMA-3.1,\
  \ and Gemini-1.5\u2014on financial Named Entity Recognition (FiNER-ORD) using zero-shot\
  \ and few-shot prompting. Fine-tuned models (BERT, RoBERTa) outperform LLMs in direct\
  \ prompting (F1 0.874\u20130.879 vs."
---

# Financial Named Entity Recognition: How Far Can LLM Go?

## Quick Facts
- arXiv ID: 2501.02237
- Source URL: https://arxiv.org/abs/2501.02237
- Reference count: 6
- Primary result: Generic LLMs achieve F1 0.740-0.791 on financial NER; fine-tuned models reach 0.874-0.879, but few-shot learning closes the gap significantly.

## Executive Summary
This study evaluates three state-of-the-art LLMs—GPT-4o, LLaMA-3.1, and Gemini-1.5—on financial Named Entity Recognition (FiNER-ORD) using zero-shot and few-shot prompting. Fine-tuned models (BERT, RoBERTa) outperform LLMs in direct prompting (F1 0.874–0.879 vs. 0.740–0.791), but LLMs close the gap significantly with few-shot learning. Gemini-1.5 achieved the highest weighted F1 (0.837) among LLMs. Chain-of-thought prompting had minimal effect and sometimes degraded performance, especially for LLaMA-3.1. Common LLM failures included contextual misunderstanding, pronoun misclassification, citizenship term errors, implied entities, and entity omission/boundary issues. The authors suggest fine-tuning for finance domains and self-correction prompting to mitigate these errors.

## Method Summary
The paper evaluates six LLM variants (GPT-4o, GPT-4o-mini, LLaMA-3.1-70B/8B, Gemini-1.5) and two fine-tuned models (BERT, RoBERTa) on the FiNER-ORD dataset using three prompting strategies: direct zero-shot, 5-shot in-context learning, and 5-shot chain-of-thought. Models generate entity-labeled text, which is parsed via word matching to compute entity-level F1 scores. Fine-tuned models use batch size 16, learning rate 1e-05, 50 epochs. The study also conducts manual error analysis categorizing failures into five types.

## Key Results
- Fine-tuned BERT/RoBERTa achieve F1 0.874-0.879, outperforming zero-shot LLMs (F1 0.740-0.791)
- Few-shot learning closes the gap: Gemini-1.5 reaches weighted F1 0.837
- Chain-of-thought prompting degrades LLaMA-3.1 performance from 0.791 to 0.740
- LLaMA-3.1 most prone to "implied entities" errors (10.7% of failures)

## Why This Works (Mechanism)

### Mechanism 1
Few-shot in-context learning substantially closes the performance gap between generic LLMs and fine-tuned models on financial NER. Providing labeled examples in the prompt allows the model to infer entity boundaries and classification patterns through pattern matching, without weight updates. The assumption is that examples are representative of the target distribution and the model's attention mechanism can retrieve and apply the patterns.

### Mechanism 2
Chain-of-thought prompting can degrade NER performance, particularly for models prone to over-reasoning. "Step-by-step" reasoning encourages elaboration, which may cause models to tag words that merely imply entities rather than are entities. The degradation is assumed to be caused by reasoning style rather than prompt formatting or tokenization differences.

### Mechanism 3
Generic pretraining creates systematic blindspots for financial domain entity classification. Financial texts contain entity naming conventions (e.g., "Johnson Brothers" as organization not person) that contradict general-domain priors. The errors are assumed to stem from domain mismatch rather than model architecture limitations.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The paper treats NER as the core evaluation task; understanding entity-level vs. token-level evaluation is essential for interpreting F1 scores.
  - Quick check question: Given "Johnson Brothers rethink plan," would you tag "Johnson Brothers" as PER or ORG, and why does the answer matter for downstream financial analytics?

- Concept: In-Context Learning vs. Fine-Tuning
  - Why needed here: The paper's central comparison is between models that learn from examples in the prompt versus models with updated weights.
  - Quick check question: If you provide 5 examples in a prompt to GPT-4o, have any model parameters changed? What tradeoff does this represent?

- Concept: Weighted F1 Score
  - Why needed here: The paper uses weighted F1 to handle class imbalance across PER/LOC/ORG entities.
  - Quick check question: Why would unweighted F1 be misleading when PER entities outnumber ORG entities by 2.29:1?

## Architecture Onboarding

- Component map: Input Layer (FiNER-ORD dataset → Prompt template) → Model Layer (LLM or fine-tuned BERT/RoBERTa) → Extraction Layer (Word matching) → Evaluation Layer (Entity-level F1)

- Critical path: 1. Select model and prompting strategy (direct → few-shot → CoT progression) 2. Format prompt with task instruction + optional examples + target text 3. Parse generated output to extract entity spans 4. Compute entity-level F1 against ground truth annotations

- Design tradeoffs: Zero-shot convenience vs. few-shot accuracy (0.79 → 0.84 weighted F1); CoT complexity vs. actual gains (marginal improvement, potential degradation); Fine-tuned model performance (0.88) vs. LLM flexibility and deployment cost; Model size: 8B vs. 70B parameters shows consistent but not linear gains

- Failure signatures: Contextual misunderstanding (31.3%): "Johnson Brothers" tagged as PER; Pronouns/generic terms (26.3%): "German car makers" tagged as ORG; Citizenship terms (10.3%): "British" tagged as LOC; Implied entities (10.7%): "Google Maps" tagged as ORG (LLaMA especially); Boundary errors (21.4%): Partial entity spans, missed abbreviations

- First 3 experiments: 1. Baseline comparison: Run direct prompting on all three LLM families, compute weighted F1 to establish zero-shot performance floor. 2. Few-shot ablation: Add 5-shot examples, measure per-class F1 changes to identify which entity types benefit most from in-context learning. 3. Failure mode audit: Manually annotate 50 errors from the worst-performing model configuration, categorize into the five failure types, and determine if errors cluster by entity class or text pattern.

## Open Questions the Paper Calls Out

### Open Question 1
Can self-verification prompting strategies effectively mitigate recurrent failure types such as implied entities or pronoun misclassification? The authors explicitly suggest "Implementing self-correction strategies" but do not implement or test these mechanisms to validate if they actually improve performance.

### Open Question 2
Does domain-specific fine-tuning of generative LLMs (e.g., LLaMA, GPT) resolve the contextual misunderstanding of proper nouns better than few-shot learning? The Discussion notes that "a significant proportion of the observed failure cases involve domain-specific proper nouns" and proposes that "Fine-tuning LLMs with financial data could enhance their ability," but does not evaluate the performance of the LLMs after they have been fine-tuned on financial data.

### Open Question 3
Can Chain-of-Thought (CoT) prompting be adapted to prevent the performance degradation observed in open-source models like LLaMA-3.1 during named entity recognition? The paper observes that CoT prompting "significantly degrades the performance of the LLaMA 3.1 series" by triggering "implied entities" errors, yet CoT is theoretically intended to improve reasoning.

## Limitations
- Prompt reproducibility: Exact few-shot examples and full prompt templates are not disclosed, limiting exact replication of reported F1 scores.
- Error analysis granularity: The failure mode breakdown is based on manual annotation of 50 errors per configuration, which may not be statistically representative.
- Cross-dataset generalizability: Results are based on a single financial NER dataset (FiNER-ORD) and may not transfer to other domains.

## Confidence

- High Confidence: LLMs improve with few-shot prompting (measured 0.740 → 0.791 weighted F1 for GPT-4o), and fine-tuned models outperform zero-shot LLMs (0.874–0.879 vs. 0.740–0.791).
- Medium Confidence: Chain-of-thought prompting sometimes degrades performance (LLaMA-3.1 drops from 0.791 to 0.740), though the effect is inconsistent across models.
- Low Confidence: Generic LLMs have inherent systematic blindspots in financial entity classification; the 31.3% contextual misunderstanding rate is sample-based and may vary with different datasets or prompt strategies.

## Next Checks

1. Prompt sensitivity analysis: Run the same LLM configurations with 3 different sets of 5-shot examples to measure variance in weighted F1 and identify which entity types are most sensitive to example selection.

2. Domain generalization test: Evaluate the best-performing LLM (Gemini-1.5 with few-shot) on a different financial NER dataset (e.g., FinNER) to assess whether in-context learning transfers across datasets.

3. CoT formulation ablation: Test alternative chain-of-thought prompts (e.g., "first identify all entities, then classify them") to determine if performance degradation is due to CoT per se or specific prompt wording.