---
ver: rpa2
title: 'Eyes on Target: Gaze-Aware Object Detection in Egocentric Video'
arxiv_id: '2511.01237'
source_url: https://arxiv.org/abs/2511.01237
tags:
- gaze
- attention
- dataset
- object
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a gaze-aware object detection framework for
  egocentric video that integrates human gaze signals into a Vision Transformer (ViT)
  to improve detection accuracy by biasing attention toward viewer-prioritized regions.
  The method uses gaze position, depth, pupil diameter, and direction to modify self-attention
  weights and dynamically scale bounding boxes for tighter localization.
---

# Eyes on Target: Gaze-Aware Object Detection in Egocentric Video

## Quick Facts
- **arXiv ID:** 2511.01237
- **Source URL:** https://arxiv.org/abs/2511.01237
- **Reference count:** 0
- **Primary result:** Gaze-aware ViT improves egocentric object detection with classification accuracy 0.92-0.94 and mAP@0.5 up to 0.61

## Executive Summary
This paper proposes a gaze-aware object detection framework that integrates human gaze signals into a Vision Transformer to improve detection accuracy in egocentric video. The method modifies transformer attention mechanisms by incorporating gaze-derived features including position, depth, pupil diameter, and direction, creating a bias toward viewer-prioritized regions. Evaluations on custom datasets show consistent gains over strong baselines like YOLOv7 and DETR, with particular improvements in classification accuracy while maintaining competitive localization performance.

## Method Summary
The framework modifies DETR's attention mechanism by adding gaze-based bias terms to self-attention weights, using gaze position, depth, pupil diameter, and direction to create spatial attention prioritization. Bounding boxes are dynamically scaled based on attention scores and gaze-derived factors including pupil dilation and estimated depth. The method operates on egocentric video frames synchronized with eye-tracking data at 100 Hz, processed through a ViT backbone with modified attention layers. During inference, the system selects the highest-confidence detection with gaze-scaled bounding boxes for classification tasks, while retaining all predictions for mean average precision evaluation.

## Key Results
- Classification accuracy reaches 0.92-0.94 across multiple datasets
- mAP@0.5 improves to 0.61 on Egocentric Maritime Simulator dataset
- Outperforms YOLOv7 and DETR baselines on classification metrics
- Ablation study confirms all gaze components contribute to performance gains
- Novel attention head importance metric reveals gaze modulation of transformer dynamics

## Why This Works (Mechanism)
The method works by leveraging human visual attention patterns to guide the transformer's feature selection process. By integrating gaze position, depth, pupil diameter, and direction into the attention mechanism, the model learns to prioritize regions where humans naturally focus their attention. This creates a more human-aligned detection system that better matches how people visually process egocentric scenes. The dynamic bounding box scaling based on gaze-derived factors further improves localization by adjusting detection regions based on attention strength and contextual information like depth and pupil dilation.

## Foundational Learning
**Vision Transformer Attention:** Self-attention mechanisms that weigh the importance of different spatial regions in an image. Why needed: Core mechanism for processing visual features in the proposed gaze-aware system. Quick check: Verify attention weights sum to 1 across patches.

**Egocentric Video Analysis:** Video captured from a first-person perspective, typically from wearable cameras. Why needed: The target application domain where human gaze naturally aligns with task-relevant objects. Quick check: Confirm frame rate and synchronization with gaze data.

**Eye-Tracking Data Integration:** Synchronization of gaze coordinates, pupil diameter, and depth with video frames. Why needed: Provides the gaze signals that guide the attention mechanism. Quick check: Validate gaze coordinates map correctly to corresponding video frames.

**Bounding Box Regression:** Process of predicting object locations through rectangular regions. Why needed: Standard output format for object detection that's modified by gaze information. Quick check: Ensure predicted boxes remain within image boundaries.

**Attention Head Importance:** Metric for quantifying how much each attention head contributes to final predictions. Why needed: Enables interpretability of how gaze modulates transformer behavior. Quick check: Verify attention head scores correlate with detection performance.

## Architecture Onboarding

**Component Map:** Input Video Frames -> ViT Backbone -> Modified Attention Layers -> Object Predictions -> Classification/Detection Output

**Critical Path:** The core processing pipeline flows through video frames into the ViT backbone, where gaze signals are integrated into the attention mechanism. This modified attention then influences feature selection, leading to gaze-aware object predictions that are scaled based on attention strength and gaze-derived factors.

**Design Tradeoffs:** The gaze integration increases computational overhead through additional gaze processing and modified attention calculations. However, this is offset by improved accuracy and human-aligned attention patterns. The method requires synchronized eye-tracking data, limiting deployment scenarios but providing significant benefits when available.

**Failure Signatures:** Over-concentration of attention (α > 0.7) degrades classification accuracy despite improved alignment. Inconsistent depth normalization across videos causes localization errors. Temporal misalignment between gaze signals (100 Hz) and video frames (30 fps) leads to incorrect attention biasing.

**First Experiments:**
1. Test baseline DETR performance on a small dataset subset to establish performance baseline
2. Implement gaze-modified attention with varying α values (0.3-0.9) to find optimal attention concentration
3. Evaluate classification accuracy with and without dynamic bounding box scaling to measure impact

## Open Questions the Paper Calls Out
None

## Limitations
- Method depends heavily on availability of synchronized gaze data, limiting practical deployment
- Several critical implementation details missing, including DETR optimizer settings and input resolution
- Performance improvements come with increased computational overhead from gaze processing
- Pseudo-bounding box dimensions for datasets without manual annotations are unspecified

## Confidence
- **High confidence** in core claim that gaze integration improves detection accuracy, supported by multiple datasets
- **Medium confidence** in specific quantitative improvements due to missing implementation details
- **Medium confidence** in attention interpretability claims, as the novel metric's robustness is not fully explored

## Next Checks
1. Verify implementation details by reproducing results on a small subset, focusing on missing hyperparameters
2. Test method's robustness to gaze signal quality degradation to assess practical applicability
3. Evaluate whether gaze integration approach generalizes to non-egocentric scenarios beyond stated focus