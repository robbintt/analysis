---
ver: rpa2
title: Communication and Verification in LLM Agents towards Collaboration under Information
  Asymmetry
arxiv_id: '2510.25595'
source_url: https://arxiv.org/abs/2510.25595
tags:
- knowledge
- same
- agents
- should
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how large language model (LLM) agents collaborate\
  \ under information asymmetry, extending Einstein Puzzles into a tabletop game where\
  \ two agents must coordinate to place objects into bins based on shared spatial\
  \ and relational constraints. Agents are equipped with varying communication abilities\u2014\
  information seeking, providing, both, or none\u2014and evaluated with or without\
  \ an environment-based reasoning verifier."
---

# Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry

## Quick Facts
- **arXiv ID:** 2510.25595
- **Source URL:** https://arxiv.org/abs/2510.25595
- **Reference count:** 40
- **One-line primary result:** Bidirectional communication (seeking and providing) enables the most effective collaboration under information asymmetry, with environment-based verification significantly improving both task performance and rule comprehension.

## Executive Summary
This paper investigates how large language model (LLM) agents collaborate under information asymmetry using a tabletop Einstein Puzzle game where two agents must coordinate to place objects into bins based on shared spatial and relational constraints. Agents are equipped with varying communication abilities—information seeking, providing, both, or none—and evaluated with or without an environment-based reasoning verifier. The study reveals that bidirectional communication enables the most effective collaboration, though mismatched communication abilities significantly degrade performance. Surprisingly, agents without communication can achieve high task success through rule memorization, but lack true understanding. The environment-based verifier substantially improves both task performance and rule comprehension. Human studies show that proactive information sharing increases user trust and perceived clarity, highlighting the importance of interpretable and communicative design in collaborative AI systems.

## Method Summary
The study extends Einstein Puzzles into a tabletop game where two LLM agents with asymmetric constraint knowledge must coordinate object placement. The framework uses supervised fine-tuning with LoRA on Llama3-8B-Instruct, Llama3.1-8B-Instruct, and Qwen2.5-7B-Instruct, training on ~10,000 samples generated from optimal and near-optimal trajectories. Agents operate with four communication configurations: Providing Only, Seeking Only, Providing & Seeking, or None. An environment-based verifier filters candidate actions at test time by checking physical affordances, communication validity, and inferred constraints through graph expansion. The system samples 4 candidate actions (temp=0.2, top_p=0.9) and executes the first valid one, achieving significant performance gains across all model families.

## Key Results
- Bidirectional communication (seeking and providing) enables the most effective collaboration, though mismatched communication abilities degrade performance
- Agents without communication achieve high task success (84.67%) through rule memorization but lack true rule understanding (27.35% wrong rule understanding errors)
- Environment-based verifier significantly improves both task performance (+10-40% absolute Success Rate) and rule comprehension
- Human studies reveal proactive information sharing increases trust and perceived clarity despite efficiency costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bidirectional communication (seeking + providing) enables more effective constraint synchronization than unidirectional or no communication.
- **Mechanism:** Information seeking targets specific knowledge gaps, while providing broadcasts constraints proactively. Together, they minimize redundant exchanges while ensuring both agents acquire missing information. Seeking-only agents query efficiently but cannot initiate; providing-only agents flood channels with potentially irrelevant constraints.
- **Core assumption:** Agents can reason about what information their partner lacks and what they themselves need—a form of Theory of Mind reasoning.
- **Evidence anchors:**
  - [abstract] "bidirectional communication (seeking and providing) enables the most effective collaboration, though mismatched communication abilities degrade performance"
  - [section 5.2] Hierarchy: "Information Seeking & Providing > Seeking Only > No Information Exchange > Providing Only" across all model families (Table 1)
  - [corpus] "Collaborative Belief Reasoning with LLMs" emphasizes reasoning about collaborators' intents to avoid miscoordination under partial observability

### Mechanism 2
- **Claim:** Environment-based verification improves task performance and rule comprehension without additional training.
- **Mechanism:** At each decision step, the system samples multiple candidate actions and uses environment feedback (physical affordance rules, communication validity, inferred constraints) to filter invalid actions. This leverages test-time compute to catch reasoning errors before execution.
- **Core assumption:** Invalid actions are recoverable—the environment can reject them without terminal consequences (trial-and-error paradigm).
- **Evidence anchors:**
  - [abstract] "environment-based verifier significantly improves both task performance and rule comprehension"
  - [section 5.2, Table 1] Verifier adds 10-40% absolute Success Rate gains across configurations (e.g., Llama3.1-8B CoT: 58.67% → 89.33% for Provide & Seek)
  - [section 5.2, Table 2] "Wrong Rule Understanding" error drops from 28.22% to 14.12% with verifier
  - [corpus] "Verification-Aware Planning for Multi-Agent Systems" directly addresses verification challenges in multi-agent collaboration

### Mechanism 3
- **Claim:** Proactive information sharing increases human trust and perceived clarity, even when task efficiency is suboptimal.
- **Mechanism:** Proactive sharing reduces uncertainty for human collaborators by revealing agent reasoning before action. This interpretability signal compensates for occasional inefficiency (extra share steps) by making agent behavior predictable and transparent.
- **Core assumption:** Humans prefer predictability and explicit reasoning traces over opaque efficiency.
- **Evidence anchors:**
  - [abstract] "Human studies reveal that proactive information sharing, even at a cost to efficiency, increases user trust and perceived clarity"
  - [section 5.2, Figure 2] Seek-only agents rated helpful but caused more confusion ("strong disagreement regarding clarity"); Provide & Seek agents offered better clarity despite slightly more steps
  - [corpus] "Gricean Norms as a Basis for Effective Collaboration" discusses navigating ambiguity and irrelevance in communication for human-AI collaboration

## Foundational Learning

- **Concept: Distributed Constraint Satisfaction Problems (CSP)**
  - **Why needed here:** The Einstein Puzzle game is explicitly framed as a distributed CSP where constraints are split between agents (C₁ ∪ C₂ = C). Understanding constraint propagation (e.g., if A-B same row and B-C same row, then A-C same row) is essential for the reasoning verifier's graph expansion algorithm.
  - **Quick check question:** Given constraints "A and B same row, B and C same column," can you infer any relationship between A and C?

- **Concept: Theory of Mind in Multi-Agent Systems**
  - **Why needed here:** Agents must reason about what their partner knows vs. what they know to avoid redundant communication and target queries appropriately. The paper explicitly draws on "reason[ing] about others' knowledge and beliefs" (Section 1).
  - **Quick check question:** If you know constraint X and your partner asks about object Y related to X, should you share X? What if you don't know whether your partner already knows X?

- **Concept: Test-Time Verification / Compute**
  - **Why needed here:** The environment-based verifier is a test-time mechanism (no training) that samples n candidates and selects the first valid one. Understanding best-of-n strategies and verification signals is critical for implementing and extending this approach.
  - **Quick check question:** If you sample 4 candidate actions and 3 are invalid per the environment, what does the verifier do? What if all 4 are invalid?

## Architecture Onboarding

- **Component map:** Game Environment -> Two LLM Agents -> Communication Channel -> Environment-Based Verifier -> Game State Update
- **Critical path:**
  1. Game initialization → Constraints distributed (C₁, C₂) → Objects placed in player areas
  2. Agent turn → Sample 4 candidate actions (temp=0.2, top_p=0.9) → Verifier filters → Execute first valid action
  3. Environment updates state → Check termination → Pass to next agent
  4. Repeat until all objects correctly placed or step limit (30)

- **Design tradeoffs:**
  - **Action space configuration:** Seek-only is most efficient but least clear to humans; Provide & Seek balances efficiency and interpretability
  - **Verifier complexity:** Affordance-only is broadly applicable (+2-7% SR); Reasoning verifier is task-specific but highest impact (+10-40% SR)
  - **CoT reasoning:** Adds ~0.3-0.5x step overhead but significantly improves success rates (e.g., Llama3.1-8B: 27.33% → 58.67% for Provide & Seek)

- **Failure signatures:**
  - **Redundant sharing:** 59-79% of share actions without verifier (Table 2); indicates poor knowledge state tracking
  - **Wrong rule understanding:** 27-38% of move actions without verifier; indicates shallow constraint reasoning
  - **Coordination breakdown:** Mismatched pairs (e.g., Seek-only vs. Provide-only) drop to 41% SR (Table 3) due to structural inability to exchange information
  - **High task success but low trust:** No-communication agents achieve 94%+ SR in self-play but confuse human collaborators (Figure 2)

- **First 3 experiments:**
  1. **Replicate bidirectional vs. unidirectional comparison:** Run the same model (e.g., Llama3.1-8B) under all four action space configurations with verifier disabled. Verify the hierarchy: Provide & Seek > Seek-only > None > Provide-only.
  2. **Verifier ablation:** Isolate affordance, communication, and reasoning verifiers separately on Llama3.1-8B CoT. Confirm reasoning verifier provides largest gains, but affordance verifier is most generalizable.
  3. **Human-agent mismatched pairing:** Have humans (full action space) play against agents with restricted action spaces. Measure both task efficiency (step ratio) and subjective clarity/trust ratings to validate the efficiency-vs-preference tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can environment-based verification mechanisms be effectively extended to real-world collaborative settings where trial-and-error may not be recoverable, and what perceptual capabilities would agents require?
- Basis in paper: [explicit] "While this holds in many simulated environments, extending the approach to real-world settings remains challenging. Doing so would require agents to possess richer perceptual capabilities and a deeper understanding of the environment's dynamics."
- Why unresolved: The current verifier assumes invalid actions are always recoverable—a prerequisite that breaks down in physical or high-stakes environments where actions may have irreversible consequences.
- What evidence would resolve it: Demonstration of environment-based verification in a physical robot collaboration task or a simulated environment with irreversible actions, showing how agents can anticipate consequences without full trial-and-error.

### Open Question 2
- Question: How can we detect and mitigate superficial rule memorization in LLM agents that achieve high task performance without genuine understanding, particularly when communication is disabled?
- Basis in paper: [explicit] "Agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators." The error analysis shows "no information exchange" agents succeed at 84.67% but with 27.35% wrong rule understanding errors.
- Why unresolved: The paper reveals the problem but does not propose mechanisms to distinguish between pattern memorization and genuine reasoning—especially problematic since performance metrics alone mask this distinction.
- What evidence would resolve it: Development of evaluation protocols that separate task performance from rule comprehension (e.g., out-of-distribution generalization tests, counterfactual reasoning probes), showing divergent performance between memorizers and true reasoners.

### Open Question 3
- Question: What is the optimal balance between proactive information sharing (which humans prefer) and task efficiency (which may favor minimal communication)?
- Basis in paper: [inferred] Human studies show "participants favor agents that proactively share information, even if such agents are less optimal in task completion," yet efficiency metrics penalize redundant sharing. The paper identifies this trade-off but does not resolve it.
- Why unresolved: The work demonstrates the tension between human trust/clarity and step-efficiency, but does not propose a unified objective function or adaptive communication policy that balances both.
- What evidence would resolve it: An adaptive communication strategy that dynamically adjusts information-sharing frequency based on inferred human partner preferences, evaluated on both efficiency and human satisfaction metrics in a controlled study.

## Limitations
- **Major uncertainty about reasoning vs. pattern matching:** The observed superiority of bidirectional communication may stem from pattern matching on training data rather than genuine Theory of Mind reasoning
- **Verifier dependency on structured feedback:** The environment-based verifier's effectiveness depends heavily on the environment's ability to provide structured, actionable feedback, which may not generalize to domains lacking clear physical affordances
- **Small human study sample:** The human study (12 participants) may limit generalizability of findings about trust and clarity preferences

## Confidence
- **High confidence:** The hierarchical ordering of communication strategies (Provide & Seek > Seek-only > None > Provide-only) and the verifier's consistent performance improvements across model families
- **Medium confidence:** The claim that proactive information sharing increases human trust despite efficiency costs, given the small human study sample size
- **Medium confidence:** The interpretation that mismatched communication capabilities cause structural coordination breakdown, though deeper investigation into failure modes could strengthen this claim

## Next Checks
1. **Generalization stress test:** Evaluate the trained agents on Einstein Puzzles with constraint types not present in training data (e.g., "diagonal" constraints if only row/column were trained). Measure whether performance degrades, indicating brittle pattern matching versus genuine reasoning.

2. **Cross-task verification transfer:** Apply the environment-based verifier to a different multi-agent collaborative task (e.g., collaborative block-stacking or distributed planning) without retraining. Document whether the verifier's core components (affordance checking, constraint inference) transfer or require substantial adaptation.

3. **Mixed-initiative communication analysis:** Conduct a detailed case study of specific games where human-agent pairs with mismatched communication capabilities either succeed or fail. Identify whether failures stem from uncommunicated constraints, misinterpreted messages, or structural inability to exchange information, and whether verifier intervention changes the failure mode.