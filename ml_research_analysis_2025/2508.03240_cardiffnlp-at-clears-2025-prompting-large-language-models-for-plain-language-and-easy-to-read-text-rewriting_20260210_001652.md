---
ver: rpa2
title: 'CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language
  and Easy-to-Read Text Rewriting'
arxiv_id: '2508.03240'
source_url: https://arxiv.org/abs/2508.03240
tags:
- simplification
- sentence
- your
- complex
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an LLM-based approach for automatic Spanish
  text adaptation into Plain Language and Easy-to-Read formats, targeting improved
  accessibility. The team experimented with prompting strategies using LLaMA-3.2 and
  Gemma-3, ultimately selecting Gemma-3 for their final submissions.
---

# CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting

## Quick Facts
- arXiv ID: 2508.03240
- Source URL: https://arxiv.org/abs/2508.03240
- Reference count: 22
- Primary result: Third place in Plain Language, second in Easy-to-Read subtasks using Gemma-3 with Spanish prompts

## Executive Summary
The CardiffNLP team developed a prompting-based approach for Spanish text simplification using LLaMA-3.2 and Gemma-3 models. They focused on Plain Language (PL) and Easy-to-Read (E2R) formats for the CLEARS shared task. Their final submission employed Gemma-3 with Spanish prompts and structured output formatting, achieving third place in PL and second in E2R subtasks. The work demonstrates the viability of instruction-tuned LLMs for text simplification while highlighting evaluation metric limitations for accessibility-focused tasks.

## Method Summary
The approach uses few-shot prompting with 3 example pairs per subtask, Spanish language prompts, and structured Python dictionary output formatting. The team tested LLaMA-3.2 and Gemma-3 models, ultimately selecting Gemma-3 for its superior Spanish language adherence. Prompts included explicit guidelines for both PL and E2R formats, with the E2R guidelines following UNE 153101 EX standards. The system processes text at the sentence level, with instructions to segment paragraphs, simplify each sentence, and concatenate results. Output is parsed from the Python dictionary format to extract the simplified text.

## Key Results
- Third place in Subtask 1 (Plain Language) with 77% BERT-based cosine similarity score
- Second place in Subtask 2 (Easy-to-Read) with 71% average similarity score
- Gemma-3 outperformed LLaMA-3.2, particularly when prompted in Spanish
- Structured Python dictionary output reduced formatting noise and improved parsing reliability
- Zero-shot prompting showed inferior performance compared to few-shot with explicit guidelines

## Why This Works (Mechanism)

### Mechanism 1: Structured Output Constraints for Generation Stability
- Claim: Enforcing a structured output format (Python dictionary) appears to reduce conversational "preambles" and formatting noise, improving extraction reliability.
- Mechanism: By forcing the LLM to generate syntactically valid code structures (key-value pairs), the model shifts distribution away from conversational filler (e.g., "Here is your simplification") toward the core content, acting as a soft constraint on the generation space.
- Core assumption: The model's instruction-following training extends to code-adjacent formatting even in natural language tasks.
- Evidence anchors:
  - [Section 4.2.3]: "The use of a Python dictionary format mitigated many of these errors... although these issues did not fundamentally affect the simplification, it produced noise that was picked up by the metrics."
- Break condition: If the model hallucinates syntax errors (e.g., colons instead of equal signs) or if the complexity of the output logic exceeds the model's capacity for syntax maintenance.

### Mechanism 2: Target-Language Prompt Alignment
- Claim: Prompting in the same language as the desired output (Spanish) yields better adherence to the target language than cross-lingual prompting (English prompts -> Spanish output) for specific model families.
- Mechanism: Instruction-tuned models may possess stronger causal links between "native" language prompts and "native" generation pathways. English prompts may activate English-dominant subspaces, leading to unintended output language or transfer errors.
- Core assumption: The model's training data distribution creates a bias where the instruction language predicts the output language.
- Evidence anchors:
  - [Section 4.2]: "Gemma-3... showed superior performance when prompted in Spanish, as English prompts led the model to simplify the complex sentence in English."
  - [Corpus]: Weak direct evidence in corpus; related works focus on fine-tuning or general prompting, not specifically the language-of-prompt alignment effect.
- Break condition: Models with robust cross-lingual instruction tuning (e.g., specifically trained for translation) may not exhibit this sensitivity.

### Mechanism 3: Error-Specific Negative Constraints
- Claim: Iteratively adding explicit negative instructions ("Do not X") based on observed failure modes mitigates specific hallucination patterns better than generic positive instructions alone.
- Mechanism: Generic instructions ("Simplify this") define a broad solution space. Specific negative constraints ("Do not change 2000 to many people") carve out high-probability failure regions, forcing the model to find alternative generation paths.
- Core assumption: The failure modes (e.g., paraphrasing numbers) are high-probability default behaviors that must be actively suppressed.
- Evidence anchors:
  - [Section 4.2]: "Later prompts included explicitly instructing the model not to make any of these mistakes... 'hallucinations' being the most prominent in dates and paraphrased numbers."
- Break condition: If the list of negative constraints becomes too long or contradictory, it may confuse the model or degrade the quality of the primary task.

## Foundational Learning

- Concept: **Plain Language (PL) vs. Easy-to-Read (E2R)**
  - Why needed here: These are distinct target domains. PL is for general clarity/efficiency; E2R is for cognitive accessibility (strict formatting, vocabulary limits). The system must be calibrated differently for each.
  - Quick check question: Does the requirement demand strictly short sentences for cognitive processing (E2R) or general clarity for a broad audience (PL)?

- Concept: **Semantic Drift vs. Literal Copying**
  - Why needed here: LLMs often fail at simplification extremes—either hallucinating details (semantic drift) or simply returning the input unchanged (literal copying).
  - Quick check question: Is the model altering facts (dates/numbers) or failing to alter structure?

- Concept: **Evaluation Metric Limitations**
  - Why needed here: Standard metrics (BERTScore, Cosine Similarity) fail to capture the visual formatting and line-break requirements critical to E2R.
  - Quick check question: Does a high semantic similarity score guarantee the text is readable by the target user group?

## Architecture Onboarding

- Component map: Input -> Prompt Engine -> Model -> Output Parser -> Evaluator
- Critical path: Prompt Engineering (Iterative Refinement) -> Model Selection (Gemma) -> Output Parsing
- Design tradeoffs:
  - Few-shot vs. Zero-shot: Few-shot provides style consistency but consumes context window and increases latency.
  - Python Dict vs. Free Text: Dict format aids parsing but risks syntax errors in generation; Free Text is robust but requires more complex post-processing.
  - LLaMA vs. Gemma: LLaMA initially showed better results with English prompts, but Gemma ultimately performed better with direct Spanish prompting.
- Failure signatures:
  - Hallucination: Changing specific numbers ("2000" -> "many people") or dates.
  - Language Reversion: Generating English output despite Spanish input (happened with LLaMA/English prompts).
  - Echoing: Returning the complex input sentence verbatim as the simplification.
- First 3 experiments:
  1. Baseline Zero-Shot: Establish error modes (hallucinations, formatting) using Spanish prompts on Gemma-3.
  2. Guideline Integration: Add specific E2R/PL guidelines (Appendix A) to the prompt and measure the drop in Fernández-Huerta score (complexity).
  3. Format Enforcement: Switch output requirement to Python dictionary and measure the reduction in parsing errors/noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evaluation metrics be developed to effectively capture the visual formatting and sentence segmentation requirements specific to Easy-to-Read (E2R) texts?
- Basis: [explicit] The authors state in Section 4.1 and Section 6 that "None of the given automatic metrics have any way of measuring or addressing" the visual formatting and sentence segmentation necessary for E2R.
- Why unresolved: Current metrics like BERTScore and Fernández-Huerta measure semantic similarity or readability scores but fail to quantify structural presentation, which is a core component of E2R guidelines.
- What evidence would resolve it: The creation and validation of a new evaluation metric that correlates strongly with human judgments of E2R layout compliance (e.g., UNE 153101 EX standards).

### Open Question 2
- Question: To what degree do the LLM-generated Plain Language and E2R texts meet the actual comprehension needs of the target populations, such as people with cognitive disabilities?
- Basis: [explicit] The conclusion explicitly notes that "Future work will benefit from incorporating human evaluation" to address the nuances of accessibility that automatic metrics miss.
- Why unresolved: The study relied on automatic similarity metrics and internal prompt iteration, lacking direct feedback from the intended user groups to validate the efficacy of the simplifications.
- What evidence would resolve it: A user-centered evaluation study where participants with intellectual disabilities assess the understandability and readability of the generated texts compared to human references.

### Open Question 3
- Question: Can specific prompt constraints effectively mitigate "hallucinations" of numerical data and dates across different LLM architectures?
- Basis: [inferred] Despite prompt engineering, the authors observed instances where the models "hallucinated" dates or paraphrased numbers (e.g., changing "2000 people" to "many people"), reducing factuality.
- Why unresolved: The paper demonstrates that while prompts can improve formatting, they did not fully guarantee factual accuracy regarding numbers and dates, a critical requirement for news text.
- What evidence would resolve it: An ablation study testing specific negative constraints (e.g., "Do not paraphrase numbers") on a held-out set of numerical data to measure error rates across different models.

## Limitations
- The approach relies heavily on semantic similarity metrics that fail to capture visual formatting requirements essential for E2R
- Limited number of few-shot examples (3 per subtask) without systematic ablation studies on their selection
- No human evaluation with target user groups to validate actual comprehension and accessibility
- Computational cost and latency of using 4B parameter models not discussed for practical deployment

## Confidence
- **High Confidence**: Gemma-3 superiority over LLaMA-3.2 for Spanish text simplification when prompted in Spanish
- **Medium Confidence**: Effectiveness of specific few-shot examples and exact wording of guidelines in Appendix A
- **Low Confidence**: Assertion that the current approach is a "viable" solution for real-world accessibility applications

## Next Checks
1. **Human Evaluation**: Conduct a user study with individuals representing target audiences to assess comprehension and usability of generated texts, comparing results with semantic similarity scores.
2. **Ablation Study on Few-Shot Examples**: Systematically vary the quality and content of few-shot examples to measure impact on output quality and sensitivity to demonstration data selection.
3. **Cross-Lingual Generalization Test**: Apply the final prompt strategy to a non-Spanish language pair to test if structured output and language-adherence benefits transfer across languages.