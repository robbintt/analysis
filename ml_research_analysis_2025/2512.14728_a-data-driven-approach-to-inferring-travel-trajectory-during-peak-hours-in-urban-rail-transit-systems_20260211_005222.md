---
ver: rpa2
title: A data-driven approach to inferring travel trajectory during peak hours in
  urban rail transit systems
arxiv_id: '2512.14728'
source_url: https://arxiv.org/abs/2512.14728
tags:
- train
- data
- travel
- time
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fully data-driven approach to inferring individual
  travel trajectories in urban rail transit systems, using Automatic Fare Collection
  (AFC) and Automatic Vehicle Location (AVL) data. The method addresses the challenge
  of determining detailed passenger travel information, such as selected train, access/egress
  time, and transfer time, which is crucial for operational efficiency.
---

# A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems

## Quick Facts
- **arXiv ID:** 2512.14728
- **Source URL:** https://arxiv.org/abs/2512.14728
- **Reference count:** 40
- **Primary result:** Achieves >90% accuracy in inferring individual travel trajectories (selected train, access/egress/transfer times) during peak hours using only AFC and AVL data

## Executive Summary
This paper proposes a fully data-driven method for inferring detailed individual travel trajectories in urban rail transit systems using Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) data. The method addresses the challenge of determining which specific train a passenger boarded, their access/egress times, and transfer details - information critical for operational efficiency but not directly recorded in AFC data. By establishing train alternative sets through spatio-temporal constraints and using a novel data-driven parameter estimation method combining Kullback-Leibler divergence with the Expectation-Maximization algorithm, the approach infers travel trajectories without relying on external survey data. The method is validated on real trajectory data, achieving over 90% accuracy for peak-hour urban rail transit travel trajectory inference.

## Method Summary
The method processes AFC data (entry/exit times and stations) and AVL data (train arrival/departure times) to infer individual travel trajectories. It first generates candidate train sets using spatio-temporal constraints that filter out physically impossible train options based on access/egress time requirements. The dataset is then split into observable trips (where only one train is possible) and unknown trips (multiple candidate trains). Using the observable trips, the model initializes parameters for egress time distributions, then iteratively applies the Expectation-Maximization algorithm to infer the most likely train for unknown trips. For transfer scenarios, Kullback-Leibler divergence validates consistency between consecutive segments. The entire process is fully data-driven, requiring no external survey data.

## Key Results
- Achieves >90% accuracy in inferring individual travel trajectories during peak hours
- Successfully infers selected train, access/egress time, and transfer time without external survey data
- Demonstrates effectiveness on real-world Beijing urban rail transit data
- Shows superior performance compared to traditional methods requiring survey data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If spatio-temporal constraints are applied correctly, the infinite search space of possible trains reduces to a finite, manageable "train alternative set."
- **Mechanism:** The system filters trains by enforcing that a passenger cannot board before arriving at the platform (access time constraint) and cannot exit after leaving the platform (egress time constraint). This deterministic filtering isolates plausible candidates before probabilistic inference begins.
- **Core assumption:** Passengers obey physical travel time limits and train schedules are punctual enough for these constraints to hold.
- **Evidence anchors:** Mentions "establishing train alternative sets based on spatio-temporal constraints" and defines temporal constraints $DT_{i,m}^j - t_i^{in} \ge t_i^{a,min}$.
- **Break condition:** The model fails if the "train alternative set" is empty or contains every train, usually due to data noise or significant schedule deviations.

### Mechanism 2
- **Claim:** If egress time distributions are treated as latent parameters, the EM algorithm can self-calibrate trajectory probabilities without external survey data.
- **Mechanism:** The model initializes parameters using "observable" trips (where only one train is possible). It then iterates: the E-step estimates the probability of taking each train given current egress time parameters, and the M-step updates the egress time distribution parameters to maximize the likelihood of those estimates.
- **Core assumption:** Egress time follows a specific distribution (Normal) and observable data is statistically representative of unobservable trips.
- **Evidence anchors:** Highlights "data-driven parameter estimation method... combined with EM algorithm (KLEM)" and details the E-step and M-step convergence process.
- **Break condition:** The algorithm fails to converge or converges to local optima if initial parameters are biased or insufficient.

### Mechanism 3
- **Claim:** If transfer segments are treated independently, cumulative errors occur; applying KL divergence constraints enforces consistency across the full travel chain.
- **Mechanism:** In transfer scenarios, the model calculates the KL divergence between the inferred egress time distribution of the arriving segment and the access nature of the departing segment. This validates whether the combined train selection is statistically plausible or if it implies an unrealistic transfer time.
- **Core assumption:** The distribution of egress/transfer behaviors remains consistent across sequential segments for a valid trajectory.
- **Evidence anchors:** Notes the combination of "KL divergence" with the EM algorithm and describes using KL divergence to "check inference train combinations."
- **Break condition:** The mechanism flags a failure if the minimum KL divergence combination conflicts with the EM-estimated most likely train.

## Foundational Learning

- **Concept: Bayesian Inference**
  - **Why needed here:** The core train selection logic calculates the posterior probability by combining a prior (access time likelihood) with evidence (egress time likelihood).
  - **Quick check question:** In Eq. (9), if the calculated egress time for Train A is statistically unlikely (low probability density), how does that affect the final probability of selecting Train A, assuming the prior access probability is high?

- **Concept: Expectation-Maximization (EM) Algorithm**
  - **Why needed here:** The "KLEM" method is an iterative unsupervised learning technique that handles the "chicken-and-egg" problem of needing egress time distribution to pick trains, but needing to know trains to define egress time distribution.
  - **Quick check question:** In Section 4.3.3, what specific data is used to calculate the initial parameters $\theta_0$ before the iteration loop begins?

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** This acts as a "distance" metric between probability distributions. The paper uses it not for training, but for validation - ensuring that inferred time distributions of two connected trip segments don't contradict each other.
  - **Quick check question:** According to Eq. (21), if the distribution $f_{e, X_{i,m}}$ is identical to $f_{e, X_{i,m+1}}$, what is the value of the KL divergence?

## Architecture Onboarding

- **Component map:** Data Pre-processor -> Constraint Solver -> Dataset Slicer -> Parameter Estimator (KLEM) -> Transfer Validator
- **Critical path:** The initialization of $\theta_0$ from $D_o$. If the "observable" dataset is too small or biased, the EM loop will initialize with bad priors, and the entire inference chain will fail.
- **Design tradeoffs:**
  - Speed vs. Precision: The paper assumes Normal distributions for walking times, which simplifies the M-step math significantly.
  - Data Slicing: Using "unique train" records as ground truth is clever but risky; a passenger could theoretically catch an earlier train and wait on the platform.
- **Failure signatures:**
  - Empty Set: `Set_i,m` returns empty → Constraints are too strict or AVL/AFC clocks are desynchronized.
  - Oscillation: EM loop likelihood fluctuates wildly → Learning rate issue or distinct passenger sub-populations merging into one distribution.
  - High KL Divergence: Transfer validator rejects all combinations → Transfer walking distance assumptions are wrong.
- **First 3 experiments:**
  1. **Constraint Sanity Check:** Run the Constraint Solver on the full dataset. Filter for records where $|Set_{i,m}| = 1$. Verify that the calculated access/egress times for these records are physically possible.
  2. **Sensitivity Analysis:** Artificially shrink the observable dataset $D_o$ (e.g., use only 10% of it) and measure how much the final inference accuracy drops.
  3. **Transfer Consistency:** Implement the KL divergence check. Plot the distribution of KL values for known valid transfers vs. random train pairings to find a good threshold for the "consistency check."

## Open Questions the Paper Calls Out

- **Can the proposed method be effectively expanded to infer trajectories across large-scale networks involving multiple origin-destination pairs and multi-path choices?**
  - **Basis in paper:** The Conclusion states, "Future research could expand to include multiple origin-destination (OD) pairs and multi-path studies to realize trajectory inference at the large-scale network level."
  - **Why unresolved:** The current study validates the model only on a single specific OD pair (CY-BXQ) within the Beijing urban rail transit system.
  - **What evidence would resolve it:** A case study applying the KLEM model to a city-wide network demonstrating maintained accuracy and computational efficiency.

- **How can the current methodology be adapted to incorporate real-time data samples for live trajectory prediction?**
  - **Basis in paper:** The authors suggest, "Future work could also explore the possibility of incorporating real-time samples, based on which real-time individual travel trajectory prediction can be achieved."
  - **Why unresolved:** The current framework processes historical AFC and AVL data offline rather than streaming data.
  - **What evidence would resolve it:** A modified algorithm capable of processing streaming data inputs to predict trajectory elements in real-time.

- **Does incorporating specific attributes—such as passenger demographics or station layout—significantly improve inference accuracy?**
  - **Basis in paper:** The paper notes, "We can also improve the model by adding passenger types, station types, or operational strategies as additional model parameters."
  - **Why unresolved:** The current parameter estimation method assumes a general distribution for egress and access times without distinguishing between different passenger behaviors or station characteristics.
  - **What evidence would resolve it:** Comparative experiments showing reduced inference error when these heterogeneous parameters are included in the model.

## Limitations
- The model's accuracy depends heavily on the quality and completeness of AVL data, with missing or delayed train times directly impacting temporal constraints.
- The assumption that observable trips are representative of unobservable trips may not hold, particularly if there are significant differences in passenger behavior or station architecture.
- The paper uses Normal distributions for access/egress times, which may not accurately reflect the true distribution of walking times, especially if they are skewed or multi-modal.

## Confidence
- **High Confidence**: The core methodology of using spatio-temporal constraints to filter candidate trains and the EM algorithm for parameter estimation is well-established in the literature and logically sound.
- **Medium Confidence**: The application of KL divergence for transfer validation is a novel contribution, but its effectiveness depends on the specific threshold used and the assumption of consistent transfer behavior.
- **Low Confidence**: The assumption that the observable dataset is representative of the entire population may not hold, potentially biasing the initial parameter estimates and impacting overall accuracy.

## Next Checks
1. **Temporal Constraint Robustness Test:** Systematically vary the minimum access/egress time thresholds (e.g., ±10s) and measure the impact on the percentage of trips with empty candidate sets.
2. **Dataset Slicing Sensitivity Analysis:** Conduct a stratified sampling experiment where the observable dataset is progressively reduced (e.g., 10%, 25%, 50%). Measure how the final inference accuracy degrades.
3. **KL Divergence Threshold Calibration:** For a subset of known valid transfer trips, plot the distribution of KL divergence values between consecutive segments. Use this distribution to empirically determine an appropriate threshold for the "consistency check."