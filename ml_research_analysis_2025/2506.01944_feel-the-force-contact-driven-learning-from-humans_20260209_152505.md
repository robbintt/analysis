---
ver: rpa2
title: 'Feel the Force: Contact-Driven Learning from Humans'
arxiv_id: '2506.01944'
source_url: https://arxiv.org/abs/2506.01944
tags:
- robot
- force
- tactile
- human
- gripper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FeelTheForce (FTF), a framework for learning
  force-sensitive manipulation from human demonstrations. FTF uses a tactile glove
  to collect force data from human demonstrators and a vision-based model to estimate
  hand pose.
---

# Feel the Force: Contact-Driven Learning from Humans

## Quick Facts
- **arXiv ID:** 2506.01944
- **Source URL:** https://arxiv.org/abs/2506.01944
- **Reference count:** 40
- **Primary result:** 77% success rate on 5 force-sensitive manipulation tasks using human tactile demonstrations

## Executive Summary
FeelTheForce (FTF) enables robots to learn force-sensitive manipulation from human demonstrations. The system uses a tactile glove to capture human force data while a vision model estimates hand pose. This data trains a closed-loop policy that predicts both hand trajectories and desired contact forces. A PD controller then tracks these predicted forces on a Franka Panda robot equipped with tactile gripper sensors. The method achieves 77% success across 5 tasks including placing soft bread, unstacking cups, and twisting bottle caps, demonstrating that robots can effectively learn fine-grained force control from human tactile experiences.

## Method Summary
FTF uses a tactile glove (AnySkin-based) to collect force data from human demonstrators while dual RealSense cameras capture RGB frames for hand pose estimation via Mediapipe. Human hand keypoints are triangulated to 3D and transformed to robot keypoint representations through rigid transforms. A transformer policy trained via behavior cloning predicts robot trajectories, gripper states (binary), and desired forces. At execution, the robot uses the predicted gripper state and a PD controller to track predicted forces until convergence, executing at 6Hz. The approach achieves embodiment-agnostic transfer through keypoint representations and decouples force prediction from force tracking via active PD control.

## Key Results
- 77% average success rate across 5 force-sensitive manipulation tasks
- FTF outperforms continuous-gripper baselines (0% on delicate tasks)
- PD force tracking enables precise force-aware control through gripper modulation
- Binary gripper state + continuous force prediction design proves effective

## Why This Works (Mechanism)

### Mechanism 1: Active Force Prediction with Closed-Loop PD Tracking
Predicting desired forces as explicit outputs and tracking them via PD control outperforms passive force input. The policy outputs predicted force alongside trajectories, while an outer-loop PD controller adjusts gripper closure until convergence. This decouples what force to apply from how to achieve it, leveraging human force distributions transferable to robot embodiment.

### Mechanism 2: Embodiment-Agnostic Keypoint Representation for Cross-Morphology Transfer
Representing both human hands and robot end-effectors as 3D keypoints enables zero-shot policy transfer without action-space correspondence. Mediapipe extracts 2D hand keypoints → triangulated to 3D → transformed to robot keypoints via rigid transforms. The transformer policy operates on these embodiment-agnostic tokens.

### Mechanism 3: Binary Gripper State + Continuous Force Prediction
Decoupling gripper state (binary open/closed) from force magnitude (continuous) simplifies learning while retaining force precision. Gripper state thresholds finger-thumb distance (<7cm = closed), while the policy predicts discrete gripper state and continuous force separately, avoiding sample inefficiency of direct closure mapping.

## Foundational Learning

- **Behavior Cloning with Transformer Policies:** The policy uses transformer architecture (non-causal, MLP encoder per point track) trained via MSE. Understanding action chunking and temporal aggregation is essential. Can you explain why exponential temporal averaging is applied to action chunks during inference?

- **3D Geometry from Multi-View Triangulation:** The pipeline depends on accurate 3D keypoint extraction from two calibrated cameras. Triangulation errors propagate directly to robot pose estimation. Given two camera projection matrices and corresponding 2D keypoints, how would you compute the 3D point?

- **PD Control Fundamentals:** The force tracking controller is a simple proportional controller. Understanding convergence, gain tuning (k=0.001), and stability is critical. What happens to force tracking if k is set too high? Too low?

## Architecture Onboarding

- **Component map:** Tactile Glove (AnySkin) → Force readings → Dual RealSense Cameras → RGB frames → Mediapipe → 2D hand keypoints → Point Triangulation → 3D hand keypoints → Rigid Transform → Robot keypoints → DIFT + Co-Tracker → Object keypoints → Transformer Policy → Robot keypoints, gripper state, desired force → PD Force Controller → Adjusts gripper closure → Franka Panda + AnySkin Gripper → Executes action at 6Hz

- **Critical path:** Camera calibration quality, force sensor calibration, PD gain tuning for force convergence

- **Design tradeoffs:** Force norm aggregation loses directional information; fixed camera setup limits in-the-wild deployment; binary gripper simplifies learning but limits tasks to pick-place-twist families

- **Failure signatures:** Robot applies excessive force (PD controller not converging), robot misses grasp (keypoint triangulation error), policy outputs erratic forces (transformer overfitting)

- **First 3 experiments:** 1) Sensor calibration validation (glove-to-robot sensor mapping), 2) Force controller convergence test (command target forces, plot convergence time vs. k), 3) Single-task overfit sanity check (train on 30 demos of one task, evaluate on held-out positions)

## Open Questions the Paper Calls Out

- Can independent collection, learning, and stabilization of separate force components (shear vs. normal) enable more dexterous manipulation tasks beyond the current aggregated force approach?

- Can egocentric cameras with stereo triangulation enable effective in-the-wild data collection without the fixed, calibrated camera setup requirement?

- Can the initial pose alignment assumption be relaxed to allow the robot to initialize from arbitrary poses matching diverse human hand starting configurations?

## Limitations

- Force norm aggregation discards directional information about normal vs. shear forces, limiting slip detection and directional force control

- Single-point gripper representation loses finger individuation capability, preventing tasks requiring fine finger control

- Fixed camera calibration and triangulation setup limits real-world deployment and requires controlled environments

## Confidence

- **High Confidence:** Binary gripper + PD force control mechanism (Table 1 validation), embodiment-agnostic keypoint transfer mechanism (clearly specified), active force prediction design pattern (emerging in related work)

- **Medium Confidence:** Overall 77% success rate (small sample size), generalizability of rigid transform across robot morphologies, robustness of temporal aggregation mechanism

- **Low Confidence:** Long-term stability of PD force controller, performance degradation when scaling beyond 5 tasks, generalization to objects with different geometries

## Next Checks

1. **Directional force control validation:** Modify policy to output force components (normal/shear) rather than force norm and test on slip-detection task to validate current force norm aggregation limitations

2. **Fine finger individuation test:** Implement task requiring precise finger positioning using current keypoint representation to quantify performance gap for fine manipulation

3. **Egocentric camera deployment:** Port triangulation pipeline to head-mounted/wrist-mounted cameras and evaluate triangulation accuracy degradation to assess real-world deployment readiness