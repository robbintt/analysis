---
ver: rpa2
title: 'Reproducibility Report: Test-Time Training on Nearest Neighbors for Large
  Language Models'
arxiv_id: '2511.16691'
source_url: https://arxiv.org/abs/2511.16691
tags:
- neighbors
- pile
- training
- original
- byte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reproduces the central claims of Test-Time Training on
  Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes
  adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor
  sequences. Using pretrained RoBERTa embeddings indexed with Faiss, the authors retrieve
  20 neighbors per test input and apply one gradient update per neighbor across GPT-2
  (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B.
---

# Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models

## Quick Facts
- **arXiv ID**: 2511.16691
- **Source URL**: https://arxiv.org/abs/2511.16691
- **Reference count**: 9
- **Primary result**: Test-time training with retrieved neighbors reduces perplexity and bits-per-byte across diverse domains, with largest gains in specialized datasets.

## Executive Summary
This reproduction validates the central claims of Hardt and Sun (2024) regarding test-time training on nearest neighbors for language models. Using pretrained RoBERTa embeddings and Faiss indexing, the study retrieves 20 neighbors per test input and applies one gradient update per neighbor across multiple model sizes. The results confirm significant perplexity reduction across The Pile datasets, with the largest improvements in structured or specialized domains like GitHub (51% of original bits per byte). The study also demonstrates that models not pretrained on The Pile benefit more from this adaptation than those already trained on similar data, enabling smaller models to approach the performance of larger ones. Due to infrastructure constraints, a memory-efficient retrieval implementation was developed that reduces RAM requirements from over 128 GB to 32 GB per server.

## Method Summary
The method involves embedding test sequences using RoBERTa-finetuned embeddings, retrieving 20 nearest neighbors from The Pile via Faiss Flat L2 index, and performing one gradient update per neighbor sequentially before evaluation. The study evaluates GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B on 22 sub-datasets from The Pile. Due to memory constraints, the authors implemented a memory-efficient retrieval system that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB to 32 GB per server while increasing query latency from ~1.35s to ~5s.

## Key Results
- Test-time training significantly reduces perplexity across diverse domains from The Pile, with largest improvements in structured datasets (GitHub: 51% of original bits per byte)
- Models not pretrained on The Pile benefit more from adaptation than those already trained on similar data
- GPT-2 (774M) on GitHub achieves 27% of original bits per byte, approaching GPT-Neo's baseline despite being half the size
- Memory-efficient retrieval implementation reduces RAM requirements from 128+ GB to 32 GB per server

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient updates on retrieved neighbors reduce perplexity by adapting the model to the local data distribution at inference time.
- Core assumption: Retrieved neighbors provide a useful proxy for the test distribution, and single-gradient updates can capture meaningful adaptation without overfitting.
- Evidence: Consistent perplexity reduction as neighbor count increases from 0-20, with steepest gains in first 5 neighbors; largest improvements in specialized datasets like GitHub (51% of original bits per byte).

### Mechanism 2
- Claim: Models not pretrained on the target distribution benefit more from TTT-NN than models already exposed to similar data.
- Core assumption: TTT-NN compensates for pretraining data gaps.
- Evidence: GPT-2 Large (774M) on GitHub achieves 27% of original bits per byte, approaching GPT-Neo's baseline of 0.424 despite being half the size; GPT-Neo shows degradation on DM-Math at 101% while GPT-2 shows 75% improvement.

### Mechanism 3
- Claim: Performance gains scale with neighbor count but with diminishing returns; ordering by proximity improves effectiveness.
- Core assumption: Nearest neighbors are more relevant than farther ones, and gradient updates are cumulative.
- Evidence: Most dramatic improvements occur with first few neighbors, with particularly steep gains in specialized datasets; more than half of performance gain occurs after training with just 20 closest neighbors.

## Foundational Learning

- **Concept**: k-Nearest Neighbors retrieval for language modeling
  - Why needed: Understanding how semantic similarity search works (RoBERTa embeddings + Faiss L2 index) is prerequisite to understanding why retrieved neighbors help adaptation.
  - Quick check: Can you explain why a bidirectional encoder (RoBERTa) is used for retrieval rather than the autoregressive model being adapted?

- **Concept**: Gradient-based fine-tuning and overfitting risks
  - Why needed: The method applies single gradient updates per neighbor; understanding why limited updates prevent catastrophic forgetting is essential.
  - Quick check: Why might multiple gradient updates on the same neighbor cause degradation rather than improvement?

- **Concept**: Language modeling metrics (bits-per-byte, perplexity)
  - Why needed: The paper uses bpB as the primary metric; understanding compression efficiency interpretation is needed to evaluate results.
  - Quick check: What does "51% of original bits per byte" mean in terms of model improvement?

## Architecture Onboarding

- **Component map**: Embedding Index Server -> Retrieval Client -> Test-Time Trainer -> Evaluator
- **Critical path**: Start index server with Pile split; for each test sequence: embed → retrieve neighbors → sequential gradient updates → evaluate; aggregate metrics across test set
- **Design tradeoffs**: Memory vs. retrieval speed (loading full files enables ~1.35s queries; offset-loading increases to ~5s); neighbor count vs. compute (20 neighbors balances gain and time); retrieval corpus size affects neighbor quality
- **Failure signatures**: OOM errors with larger models on EuroParl/GitHub; performance degradation on models already pretrained on target distribution; highly variable per-iteration times (0.9s to 36s) depending on sequence length
- **First 3 experiments**:
  1. Run TTT-NN on GPT-2 (117M) with Wikipedia subset, k=5 neighbors. Verify bpB decreases and training loss drops per neighbor.
  2. On same Wikipedia setup, increment k from 1 to 20, plotting bpB after each neighbor. Verify diminishing returns curve.
  3. Compare GPT-2 vs. GPT-Neo on GitHub subset. Verify GPT-2 shows larger relative improvement than GPT-Neo.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sequential order of gradient updates (nearest-to-farthest vs. farthest-to-nearest) significantly impact model performance?
- Status: Not verified in reproduction due to lack of codebase support for order ablation
- Resolution: Comparative experiment varying only training order while measuring bits-per-byte

### Open Question 2
- Question: Does single-iteration TTT-NN provide superior performance compared to in-context learning or multi-iteration dynamic evaluation?
- Status: Not verified in reproduction as baselines were not supported by available code
- Resolution: Benchmark comparison against kNN-LM interpolation and dynamic evaluation

### Open Question 3
- Question: How does TTT-NN affect reasoning-optimized models when evaluated on tasks specifically aligned with their training distribution?
- Status: Only tested on DM-Math (out-of-distribution for reasoning model)
- Resolution: Evaluation on reasoning benchmarks like Big-Bench Hard

### Open Question 4
- Question: Does the memory-efficient retrieval implementation degrade the quality of retrieved neighbors compared to the original in-memory approach?
- Status: Change in retrieval mechanics is a potential confounding variable
- Resolution: Controlled comparison of neighbor retrieval quality and resulting perplexity using both methods

## Limitations
- Memory-efficient retrieval implementation may affect neighbor quality despite documented performance benefits
- Could only test up to 20 neighbors due to implementation constraints, limiting full exploration of neighbor-count scaling
- Exact data split sizes and test sequence counts were not specified in original paper

## Confidence
- **High confidence (95%)**: Test-time training significantly reduces perplexity across diverse domains
- **High confidence (90%)**: Models not pretrained on target distribution benefit more from adaptation
- **Medium confidence (80%)**: Performance gains scale with neighbor count but with diminishing returns

## Next Checks
1. **Full Dataset Validation**: Run complete evaluation across all 22 Pile datasets with memory-efficient implementation to verify per-dataset improvements match original paper
2. **Neighbor Count Sensitivity**: Systematically vary k from 1 to 50 neighbors on representative dataset to empirically validate diminishing returns claim
3. **Cross-Domain Generalization**: Test method on datasets outside The Pile (WikiText-103, C4) to evaluate transfer to truly out-of-distribution data