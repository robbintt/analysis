---
ver: rpa2
title: Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency
  Trajectory Distillation
arxiv_id: '2506.07822'
source_url: https://arxiv.org/abs/2506.07822
tags:
- reward
- diffusion
- consistency
- sampling
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow inference speed of diffusion models
  in offline reinforcement learning by proposing a reward-aware consistency trajectory
  distillation method. The key idea is to train a single-step student model using
  consistency distillation from a pre-trained teacher diffusion model, while incorporating
  a reward objective to guide the student toward high-reward action trajectories.
---

# Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation

## Quick Facts
- **arXiv ID**: 2506.07822
- **Source URL**: https://arxiv.org/abs/2506.07822
- **Reference count**: 17
- **Key outcome**: Achieves 142x sampling speedup and 9.7% performance improvement over state-of-the-art on Gym-MuJoCo, FrankaKitchen, and Maze2d benchmarks using reward-aware consistency trajectory distillation

## Executive Summary
This paper addresses the slow inference speed of diffusion models in offline reinforcement learning by proposing a reward-aware consistency trajectory distillation method. The approach trains a single-step student model using consistency distillation from a pre-trained teacher diffusion model while incorporating a reward objective to guide the student toward high-reward action trajectories. This achieves both faster sampling and improved performance compared to existing methods. The method decouples training of the teacher, student, and reward model, avoiding the complexity of concurrent multi-network optimization while maintaining noise-free reward signals.

## Method Summary
The method consists of three sequential training stages. First, a diffusion teacher planner is trained using energy-based diffusion models (EDM) with denoising score matching on offline datasets. Second, a separate reward model is trained to predict return-to-go values from state-action pairs. Third, a single-step student model is trained via consistency trajectory distillation (CTM), combining three losses: CTM loss for trajectory consistency with the teacher, DSM loss for data fidelity, and a reward loss that encourages high-reward predictions. The student model supports single-step sampling for 142x speedup compared to multi-step diffusion sampling.

## Key Results
- Achieves 142x sampling speedup compared to baseline diffusion planners
- Improves performance by 9.7% over state-of-the-art methods on D4RL benchmarks
- Effectively shifts sampling distribution toward high-reward modes in bimodal reward distributions
- Single-step sampling performs comparably to 2-4 step refinement while maintaining simplicity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A single-step student model can approximate the multi-step denoising trajectory of a diffusion teacher while incorporating reward optimization.
- **Mechanism**: Consistency Trajectory Models (CTM) learn to make "anytime-to-anytime jumps" along the probability flow ODE trajectory. The student model Gθ is trained to align two prediction paths: (1) direct prediction from timestep t to k, and (2) a two-stage prediction using a numerical solver (Heun) with the teacher from t to u, then student from u to k. The DSM loss (LDSM) grounds predictions in clean data space, while the CTM loss (LCTM) ensures trajectory consistency with the teacher.
- **Core assumption**: The teacher's PFODE trajectory is well-formed and the distance metric d in clean data space meaningfully captures trajectory alignment.
- **Evidence anchors**:
  - [abstract] "train a single-step student model using consistency distillation from a pre-trained teacher diffusion model"
  - [Section 2.3] "CTM aims to align two different paths to predict xk" with formal loss definitions in Equations 5-7
  - [corpus] "Elucidating the Preconditioning in Consistency Distillation" discusses consistency distillation fundamentals but lacks direct validation of RL-specific applications
- **Break condition**: If the teacher model has poor coverage of high-reward regions, distillation will faithfully reproduce suboptimal behavior modes.

### Mechanism 2
- **Claim**: Adding a reward objective during distillation acts as a mode selection mechanism, shifting the student's sampling distribution toward high-reward behaviors present in the teacher's multimodal distribution.
- **Mechanism**: The reward model Rψ (a differentiable return-to-go network) evaluates the student's predicted action sequence. The gradient ∇θR(Gθ(xT,T,0)) propagates through the single-step prediction, directly encouraging higher-reward outputs. This transforms the reward-agnostic teacher distribution into one that preferentially samples from high-reward modes.
- **Core assumption**: The reward model generalizes accurately to student-generated trajectories that may lie outside the offline dataset distribution.
- **Evidence anchors**:
  - [abstract] "incorporating a reward objective to guide the student toward high-reward action trajectories"
  - [Section 3.5, Figure 3] Shows bimodal reward distribution in hopper-medium-expert dataset; RACTD concentrates samples on the higher-reward mode while unconditioned student faithfully replicates both modes
  - [corpus] "Prior-Guided Diffusion Planning" mentions guidance mechanisms but does not validate this specific mode-selection hypothesis
- **Break condition**: Excessive reward weight (σ in Equation 9) causes training instability when reward loss dominates DSM and CTM losses, as shown in ablation (Figure 4: weight 1.5 causes fluctuation).

### Mechanism 3
- **Claim**: Decoupled training of teacher, student, and reward model avoids the instability of concurrent multi-network optimization while enabling noise-free reward signals.
- **Mechanism**: Each component trains independently: (1) teacher diffusion planner via EDM, (2) reward model on clean state-action pairs, (3) student via RACTD with frozen teacher and frozen reward model. Single-step student sampling enables reward gradients to operate entirely in clean data space, avoiding noisy reward evaluations required in classifier-guided diffusion.
- **Core assumption**: The frozen teacher's multimodal coverage is sufficient for the reward model to identify useful modes; no online feedback loop is needed.
- **Evidence anchors**:
  - [abstract] "decoupled training of the teacher, student, and reward model, avoiding the complexity of concurrent multi-network optimization"
  - [Section 3.4] "our method...operates entirely in the noise-free state-action space. This design choice enables the reward model to provide stable and effective signals without requiring noise-aware training"
  - [corpus] Weak direct validation; neighbor papers do not specifically address decoupled vs. concurrent training tradeoffs
- **Break condition**: If the teacher fails to capture certain high-reward behaviors, decoupled training provides no mechanism for the student to discover them.

## Foundational Learning

- **Concept: Probability Flow ODE (PFODE)**
  - Why needed here: Diffusion sampling is formulated as reversing a noise corruption process via SDE/ODE. PFODE provides the deterministic trajectory that CTM learns to jump across.
  - Quick check question: Given a noisy sample xt at timestep t, can you explain what the PFODE solver produces and why it enables deterministic denoising?

- **Concept: Offline RL and Distribution Shift**
  - Why needed here: The method must learn from static datasets containing mixed-quality behaviors without environment interaction. Understanding why behavior cloning fails on suboptimal data motivates the reward-aware approach.
  - Quick check question: Why does behavior cloning perform poorly on medium-replay datasets, and how does reward guidance address this?

- **Concept: Consistency Models and Trajectory Distillation**
  - Why needed here: The core acceleration technique. Understanding how student models learn to map from any timestep to any other timestep without iterative denoising.
  - Quick check question: What is the difference between consistency distillation (Song et al. 2023) and consistency trajectory models (Kim et al. 2023), and why does this paper use CTM?

## Architecture Onboarding

- **Component map:**
  - Teacher model: 1D temporal U-Net (EDM-style) modeling p(ān | s̄n) with horizon-dependent dimensions (e.g., [512,1024,2048] for MuJoCo)
  - Student model: Same architecture as teacher plus denoising timestep conditioning
  - Reward model: 4 ConvBlocks (Conv1D + GroupNorm + Mish) followed by linear layer, dimensions [32,64,128,256]
  - Loss functions: LCTM (trajectory consistency), LDSM (data fidelity), LReward (return maximization), weighted by α, β, σ

- **Critical path:**
  1. Train teacher diffusion planner on offline dataset D using EDM (80 discretization bins, 2nd-order Heun solver)
  2. Train reward model Rψ separately on (state, action, return) tuples from D
  3. Train student via RACTD: sample (s, a) from D, add noise to get xT, compute student prediction ā = Gθ(xT, T, 0), evaluate reward loss -Rψ(s̄, ān), combine with CTM and DSM losses
  4. Deploy student with single-step sampling: ā = Gθ(xT, T, 0)

- **Design tradeoffs:**
  - Reward weight σ: Higher values improve performance on suboptimal data but risk training instability. Paper uses 0.0-3.0 depending on dataset quality (expert datasets need less guidance)
  - Observation horizon h vs. planning horizon c: Longer horizons improve performance (Table 17) but increase computation
  - Multi-step inference: Student supports 2-4 step refinement but shows minimal improvement (Table 16), so single-step is recommended

- **Failure signatures:**
  - Training instability (fluctuating rewards): Reward weight too high; reduce σ
  - Poor performance on expert datasets: Reward guidance interfering with already-optimal behaviors; set σ=0 or very low
  - Student fails to improve over teacher: Check that reward model is trained to convergence and frozen during distillation
  - Loss imbalance: Verify individual loss terms are within same order of magnitude; adjust weights accordingly

- **First 3 experiments:**
  1. **Sanity check**: Train unconditioned student (σ=0) on hopper-medium-expert; verify it replicates teacher's bimodal distribution (should match Figure 3 orange curve ~110 normalized score)
  2. **Ablation on reward weight**: Train students with σ ∈ {0.0, 0.5, 1.0, 1.5} on hopper-medium-replay; plot evaluation rewards over 200 epochs to reproduce Figure 4 instability pattern
  3. **Architecture validation**: Compare single-step vs. 2-step student sampling on Maze2d-Large; verify minimal improvement (Table 16 pattern) to confirm single-step design choice

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can non-differentiable reward models be effectively integrated into the consistency trajectory distillation framework?
- **Basis in paper**: [explicit] The authors state in the Limitations section that future work will explore "methods to integrate non-differentiable reward models into the framework."
- **Why unresolved**: The current RACTD method relies on backpropagating gradients through a differentiable reward model ($\nabla_\theta R_\psi$) to guide the student, making it incompatible with black-box or discrete reward functions common in many real-world tasks.
- **What evidence would resolve it**: A modified distillation objective that successfully utilizes reward signals from non-differentiable sources (e.g., binary success classifiers or human feedback) without losing the single-step sampling capability.

### Open Question 2
- **Question**: Can the training procedure be stabilized to mitigate the loss fluctuations inherent in consistency trajectory distillation when combined with reward guidance?
- **Basis in paper**: [explicit] The authors note that "consistency trajectory distillation is prone to loss fluctuations, and incorporating a reward model... may further amplify this instability," identifying stable training as a focus for future work.
- **Why unresolved**: The interaction between the consistency loss (CTM), denoising score matching (DSM), and the new reward loss creates a complex optimization landscape that may lead to unstable convergence.
- **What evidence would resolve it**: Empirical results showing reduced variance in loss curves during training and improved robustness to hyperparameter changes (specifically the reward weight $\sigma$) compared to the current implementation.

### Open Question 3
- **Question**: Can the training pipeline be streamlined to reduce the computational overhead of training three separate networks (teacher, student, reward) sequentially?
- **Basis in paper**: [explicit] The authors identify the need to "train three separate networks" as a limitation, noting that "Training the teacher can be time-consuming."
- **Why unresolved**: While decoupling simplifies optimization logic, it increases the total wall-clock time required to deploy the model compared to single-stage actor-critic methods.
- **What evidence would resolve it**: A unified training algorithm that maintains the performance and decoupling benefits of RACTD but reduces the total training time, perhaps by co-training the student and reward models or using a pre-trained foundation model as the teacher.

## Limitations

- Training hyperparameters (learning rates, batch sizes, optimizer choice) are unspecified, making exact reproduction challenging
- The paper assumes teacher coverage of high-reward behaviors without verifying this across all datasets
- Reward model generalization to student-generated trajectories outside the offline distribution is not empirically validated
- Multi-step student sampling shows minimal improvement over single-step, suggesting potential architectural constraints

## Confidence

- **High confidence**: Single-step student acceleration (142x speedup) - directly measured and benchmarked
- **Medium confidence**: Reward-aware mode selection mechanism - supported by Figure 3 visualization but not explicitly tested across all datasets
- **Medium confidence**: Decoupled training stability - theoretically sound but lacks direct comparison to concurrent training

## Next Checks

1. **Reward model generalization test**: Measure reward model accuracy on student-generated trajectories versus offline data to quantify out-of-distribution performance
2. **Teacher coverage analysis**: Quantify the teacher model's representation of high-reward modes in each dataset before distillation
3. **Concurrent training comparison**: Implement and compare the proposed decoupled approach against a multi-network concurrent training baseline to validate stability claims