---
ver: rpa2
title: 'Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management'
arxiv_id: '2601.08435'
source_url: https://arxiv.org/abs/2601.08435
tags:
- memory
- reward
- arxiv
- information
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Fine-Mem, a reinforcement learning framework
  for long-horizon memory management in large language model agents. It addresses
  reward sparsity and ineffective credit assignment by introducing two key components:
  a Chunk-level Step Reward that provides immediate supervision via auxiliary question-answering
  tasks, and an Evidence-Anchored Reward Attribution that redistributes global rewards
  to specific memory operations based on their evidential support.'
---

# Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management

## Quick Facts
- arXiv ID: 2601.08435
- Source URL: https://arxiv.org/abs/2601.08435
- Authors: Weitao Ma; Xiaocheng Feng; Lei Huang; Xiachong Feng; Zhanyu Ma; Jun Xu; Jiuchong Gao; Jinghua Hao; Renqing He; Bing Qin
- Reference count: 35
- Key outcome: Fine-Mem achieves 4.4% and 7.2% average improvements on Memalpha and MemoryAgentBench, respectively, by addressing reward sparsity and credit assignment in long-horizon memory management.

## Executive Summary
Fine-Mem introduces a reinforcement learning framework for training memory managers in long-horizon language model agents. It addresses two fundamental challenges: reward sparsity from delayed task outcomes and ineffective credit assignment for individual memory operations. The approach introduces chunk-level step rewards providing immediate feedback via QA tasks, and evidence-anchored reward attribution that redistributes global rewards based on evidential support. Experiments show consistent improvements over strong baselines while maintaining compact memory lengths.

## Method Summary
Fine-Mem trains a memory manager agent using reinforcement learning to handle long-horizon memory management tasks. The method employs two key components: Chunk-level Step Reward (CSR) provides immediate feedback for each memory operation by generating and verifying QA pairs answerable from the incoming chunk, and Evidence-Anchored Reward Attribution (EARA) redistributes the final global reward to specific operations based on their evidential contribution to downstream reasoning. The framework uses a unified single-layer memory architecture and trains the manager via GRPO optimization with hybrid rewards combining CSR, EARA, formatting validity, and memory compression terms.

## Key Results
- Achieves 4.4% average improvement on Memalpha benchmark across AR, TTL, and LRU categories
- Achieves 7.2% average improvement on out-of-distribution MemoryAgentBench
- Consistently outperforms baselines including REAP, AM, SCAR, and Mem-T
- Maintains compact memory lengths while improving task performance

## Why This Works (Mechanism)

### Mechanism 1: Chunk-level Step Reward (CSR)
Provides immediate feedback per chunk to mitigate reward sparsity. For each incoming chunk, QA pairs are generated and verified to be answerable from that chunk's content. The manager receives an immediate reward if the updated memory state can successfully answer these local questions. This dense signal guides individual operations before the final task outcome is known.

### Mechanism 2: Evidence-Anchored Reward Attribution (EARA)
Redistributes global rewards to specific memory operations based on evidential contribution. The mechanism traces memory items retrieved for downstream queries back to the specific step and operation that created them. It calculates Normalized Evidence Contribution (NEC) for each step and combines it with a uniform baseline to form the final step-level reward.

### Mechanism 3: Unified Single-Layer Memory Architecture
Simplifies memory structure to a single layer, freeing the manager from complex placement decisions. This flat architecture avoids the complexities of multi-layer or hierarchical memories, allowing the manager to focus on content-based operations rather than structural placement.

## Foundational Learning

- **Reinforcement Learning (RL) Credit Assignment**: Why needed? The core problem is determining which of many sequential memory operations contributed to final success or failure. Quick check: Can you explain why a single sparse reward at the end of a long sequence makes it difficult to learn which actions were beneficial?

- **Memory Management Operations (INSERT, UPDATE, DELETE, SKIP)**: Why needed? The Memory Manager's action space is built from these four atomic operations. Understanding their distinct effects is fundamental to analyzing the manager's policy. Quick check: If a new chunk contains an update to a fact already present in memory, which operation should a well-functioning manager perform?

- **GRPO (Group Relative Policy Optimization)**: Why needed? The memory manager is trained using GRPO. Understanding its group-based advantage estimation is necessary to follow the training loop. Quick check: How does GRPO's use of group mean and standard deviation for advantage estimation differ from standard advantage estimation that uses a learned value function?

## Architecture Onboarding

- **Component map**: Memory Manager (Qwen3-4B) -> Memory State (single-layer JSON) -> Reasoning Agent (Qwen3-32B) -> Task Output -> Reward Calculation (CSR + EARA) -> GRPO Update
- **Critical path**: The most important sequence is: 1) Manager generates operations on chunk stream, 2) Reasoning agent answers both Chunk-level and Global QAs, 3) Calculate EARA and CSR, 4) Combine into hybrid reward, 5) Optimize manager policy via GRPO
- **Design tradeoffs**: Single-layer memory simplifies the problem but may limit organizational power. Decoupling manager and reasoning agent simplifies training but prevents joint optimization. BM25 retrieval is simple and effective but may lack deep semantic understanding
- **Failure signatures**: High Global QA but low Chunk-level QA indicates overfitting to final reward (CSR weight too low). Uncontrolled memory growth indicates insufficient compression reward weight
- **First 3 experiments**: 1) Ablation training with only CSR to confirm dense learning signal, 2) Ablation training with only EARA to confirm final outcome optimization, 3) Generalization test training on Memalpha and evaluating on MemoryAgentBench

## Open Questions the Paper Calls Out

- **Co-evolutionary training paradigm**: Can optimizing both memory manager and reasoning agent simultaneously outperform the current decoupled strategy? The paper notes the current approach doesn't support reasoning model enhancement.

- **Multimodal memory support**: How does Fine-Mem perform when extended to support multimodal memory (images, audio) rather than text-only streams? The current focus is exclusively on textual mode.

- **Dense vector retrieval alternatives**: To what extent does replacing BM25 with dense vector or graph-based retrieval improve capture of deeper semantic relationships? The paper acknowledges BM25 limits semantic capture.

- **Robustness to teacher noise**: Is Chunk-level Step Reward robust to noise or errors generated by the auxiliary teacher model? The framework implicitly assumes perfect accuracy from the QA generation model.

## Limitations

- The framework depends on teacher-verifier pairs for QA generation, introducing potential brittleness from noisy or ambiguous QAs
- Evidence-anchored reward attribution assumes perfect retrieval reflects true utility, but retrieval failures could lead to incorrect credit assignment
- Single-layer memory architecture may underperform in scenarios requiring hierarchical organization
- Evaluation relies on specific benchmarks that may not fully capture real-world memory management challenges

## Confidence

- **High confidence**: Experimental results showing consistent improvements over baselines on Memalpha and MemoryAgentBench, and general validity of CSR + EARA framework
- **Medium confidence**: Effectiveness of specific reward weights (w1=0.5, w2=0.05, Î²=0.5) and correlation between chunk-level QA accuracy and downstream performance
- **Low confidence**: Claim that single-layer memory suffices for all long-horizon tasks, and robustness to noisy or out-of-distribution data

## Next Checks

1. **Ablation Study on QA Quality**: Systematically vary QA pair acceptance thresholds and measure impact on manager performance to quantify sensitivity to local reward signal noise

2. **Robustness to Retrieval Failure**: Deliberately inject retrieval errors during EARA calculation and evaluate if manager can still learn effective policies or becomes unstable

3. **Hierarchical Memory Comparison**: Implement minimal two-layer hierarchical memory and compare performance against Fine-Mem on tasks known to benefit from structure