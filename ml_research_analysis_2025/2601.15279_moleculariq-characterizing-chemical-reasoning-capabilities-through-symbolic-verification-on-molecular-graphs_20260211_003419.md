---
ver: rpa2
title: 'MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic
  Verification on Molecular Graphs'
arxiv_id: '2601.15279'
source_url: https://arxiv.org/abs/2601.15279
tags:
- reasoning
- molecular
- chemical
- moleculariq
- smiles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MolecularIQ introduces the first fully symbolically verifiable
  benchmark for evaluating molecular reasoning in large language models. The benchmark
  comprises 5,111 questions covering counting, indexing, and constrained generation
  tasks across six molecular feature categories, with three complexity dimensions
  (SMILES representation, molecular complexity, multitask load) to systematically
  diagnose reasoning failures.
---

# MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs

## Quick Facts
- arXiv ID: 2601.15279
- Source URL: https://arxiv.org/abs/2601.15279
- Reference count: 40
- Top models achieve only 47-52% accuracy on overall tasks, with performance degrading substantially under multitask load and complex molecular representations

## Executive Summary
MolecularIQ introduces the first fully symbolically verifiable benchmark for evaluating molecular reasoning in large language models. The benchmark comprises 5,111 questions covering counting, indexing, and constrained generation tasks across six molecular feature categories, with three complexity dimensions (SMILES representation, molecular complexity, multitask load) to systematically diagnose reasoning failures. Evaluation of 38 models reveals that structural understanding remains a critical bottleneck: top models achieve only 47-52% accuracy on overall tasks, with performance degrading substantially under multitask load and complex molecular representations. Many failures reflect genuine reasoning gaps rather than formatting errors, and chemistry-specialized fine-tuning often underperforms generalist models, suggesting that domain-specific instruction tuning can miscalibrate reasoning capabilities. The benchmark's symbolic verifiers and leaderboard infrastructure provide a foundation for advancing verifiable chemical AI development.

## Method Summary
The benchmark evaluates LLMs on 5,111 symbolically verifiable molecular reasoning tasks using PubChem molecules (5-50 heavy atoms) filtered and split into training, easy test, and hard test sets. Questions cover counting, indexing, and constrained generation across six molecular feature categories. Ground-truth answers are computed algorithmically using RDKit-based symbolic solvers rather than sourced from literature. The evaluation infrastructure uses lm-evaluation-harness with JSON output extraction and hierarchical metric breakdowns across task types, complexity dimensions, and molecular features. Models are evaluated across three SMILES representations (canonical, randomized, kekulized) and multitask loads (1-5 simultaneous constraints) to profile reasoning capabilities.

## Key Results
- Top models achieve only 47-52% accuracy on overall tasks, with performance degrading substantially under multitask load and complex molecular representations
- Chemistry-specialized fine-tuning systematically underperforms generalist models, suggesting domain-specific instruction tuning can miscalibrate reasoning
- Incorrect responses consistently exhibit longer token lengths than correct ones across all top-performing models
- Models show 15-50% accuracy drops when transitioning from canonical to randomized/kekulized SMILES, indicating reliance on token patterns rather than graph reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic verification isolates genuine structural reasoning from memorization and pattern matching.
- Mechanism: Ground-truth answers are computed algorithmically from molecular graphs using RDKit, not sourced from literature or surrogate labels. This creates a leakage-proof benchmark where correct answers cannot be recalled from pretraining corpora.
- Core assumption: LLMs must construct internal graph representations to solve these tasks; surface-level token patterns are insufficient for consistent success.
- Evidence anchors: [abstract] "focused exclusively on symbolically verifiable tasks"; [Section 3.2] "For each feature, we have an RDKit-based symbolic solver"

### Mechanism 2
- Claim: Multi-dimensional complexity profiling localizes failure modes to specific representational or cognitive bottlenecks.
- Mechanism: Three orthogonal axes—SMILES representation, molecular complexity (Bertz bins), and multitask load (1–5 simultaneous constraints)—create controlled variation. Performance gradients across these axes reveal whether failures stem from representation dependence, computational capacity limits, or compositional reasoning breakdowns.
- Core assumption: Performance degradation along a specific axis indicates reliance on shortcuts associated with that dimension.
- Evidence anchors: [Section 3.3] "This creates a multi-dimensional capability profile that allows localization of failure modes"; [Figure 3g] Shows consistent performance drops under SMILES perturbations across all top-10 models

### Mechanism 3
- Claim: Task-type progression from counting to indexing to generation exposes whether correct counts arise from pattern recognition or genuine graph traversal.
- Mechanism: Counting tasks admit shortcuts (heuristics, memorized statistics); indexing tasks force models to identify specific atom indices, closing off spurious solution paths; generation tasks require synthesizing valid structures satisfying constraints. Paired counting/indexing questions on identical molecules enable direct attribution of correct counts to correct substructure localization.
- Core assumption: If a model counts correctly but cannot identify indices, it is using pattern-based heuristics rather than graph-based reasoning.
- Evidence anchors: [Section 3.1] "Index-based attribution requires models to ground answers in specific atoms/bonds/substructures, closing off spurious solution paths"; [Table C1–C3] Top models show 5–30% accuracy drop from counting to indexing

## Foundational Learning

- Concept: SMILES string notation and molecular graph representation
  - Why needed here: The benchmark operates entirely on SMILES inputs; understanding canonicalization, aromaticity notation (lowercase vs. uppercase), ring-closure digits, and chirality markers (@/@) is prerequisite to interpreting task stimuli and model errors.
  - Quick check question: Given the SMILES "c1ccccc1C(=O)O" vs. "C1=CC=CC=C1C(=O)O", which represents aromatic notation vs. kekulized? How many heavy atoms does each have?

- Concept: Structure-property relationships in chemistry
  - Why needed here: The paper's core thesis is that structural understanding is prerequisite to molecular reasoning; you must grasp why ring topology, functional groups, and stereochemistry determine properties to appreciate the benchmark's diagnostic value.
  - Quick check question: Why does the presence of a carbonyl group (C=O) affect hydrogen-bond acceptor count? What structural feature distinguishes an alcohol from a ketone?

- Concept: Chain-of-thought reasoning and compositional generalization in LLMs
  - Why needed here: The multitask load dimension explicitly tests whether models can coordinate multiple sub-tasks; understanding how LLMs fail under compositional stress (vs. single-task settings) frames the performance degradation analysis.
  - Quick check question: If a model achieves 50% accuracy on single-constraint generation but 10% on 5-constraint generation, what does this suggest about its compositional capabilities?

## Architecture Onboarding

- Component map: PubChem molecules -> RDKit parsing -> molecular graph construction -> property computation -> question generation -> ground-truth generation -> model inference -> answer extraction -> verification -> metric computation

- Critical path:
  1. SMILES input → RDKit parsing → molecular graph construction
  2. Graph traversal → property computation (ring detection, functional group identification, stereochemistry assignment)
  3. Question-template instantiation → ground-truth generation
  4. Model inference → answer extraction (JSON, key-specific matching)
  5. Verification → binary correctness + per-axis profiling

- Design tradeoffs:
  - **2D structures only**: Excludes 3D conformational effects; limits relevance to tasks requiring spatial reasoning (future: MolecularIQ3D planned)
  - **Single-molecule focus**: No reaction prediction or multi-molecule reasoning; isolates structural understanding from procedural chemistry
  - **Symbolic feature suite**: Excludes property prediction without exact solvers; focuses on controllable, verifiable tasks
  - **RDKit dependency**: Inherits RDKit's aromaticity model and tautomer handling; prioritizes reproducibility over resolving fundamental chemical ambiguities

- Failure signatures:
  - **Canonicalization dependence**: Performance drops >10% when randomized/kekulized SMILES used (Figure 3g, Tables C8–C9)
  - **Multitask collapse**: Sharp accuracy decay from load-1 to load-5 (Table C2), especially for indexing tasks
  - **Verbose confusion**: Incorrect responses show significantly longer token counts than correct ones (Figure C8)
  - **Format validity gap**: High type-validity (80–90%) but low accuracy indicates semantic errors, not formatting failures (Table C13)

- First 3 experiments:
  1. **SMILES perturbation probe**: Evaluate your model on canonical, randomized, and kekulized variants. Quantify the accuracy gap; if >10% degradation, model relies on token patterns rather than graph reasoning.
  2. **Counting-indexing transfer analysis**: For paired questions on identical molecules, compute the conditional probability P(correct indexing | correct counting). Low transfer (~50% for chemistry-specialized models) indicates heuristic-based counting.
  3. **Multitask load scaling**: Plot accuracy vs. load (1, 2, 3, 5) for each task type. If degradation exceeds the combinatorial baseline (p_single^n), models lack compositional coordination; if better, multitask prompting may aid reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does domain-specific chemistry fine-tuning systematically degrade performance on symbolically verifiable structural reasoning tasks compared to generalist base models?
- Basis in paper: [explicit] "Chemistry fine-tuning on task sets that differ from ours... does not translate into better structure reasoning and systematically degrades performance relative to the base models... narrow, task-specific instruction tuning can miscalibrate reasoning or overfit to format artifacts."
- Why unresolved: The paper identifies the phenomenon but does not isolate whether the degradation stems from overfitting to specific output formats, loss of general reasoning capabilities, or interference between chemical knowledge and structural parsing skills.
- What evidence would resolve it: Ablation studies comparing fine-tuning on symbolically verifiable tasks versus traditional chemistry tasks, and analysis of attention patterns before/after fine-tuning to identify which capabilities are compromised.

### Open Question 2
- Question: Can models that succeed on common constraint combinations in generation tasks generalize to rarer combinations, or are they relying on memorized co-occurrence patterns?
- Basis in paper: [explicit] "Accuracy drops sharply as constraint-set prevalence decreases... success on common constraints does not transfer to rarer combinations. These patterns indicate that models lack genuine compositional reasoning."
- Why unresolved: The paper demonstrates the generalization gap but does not determine whether this reflects missing compositional mechanisms in LLMs or insufficient coverage of rare combinations in training data.
- What evidence would resolve it: Systematic evaluation of models on synthetic constraint combinations with controlled prevalence, combined with probing studies to assess whether models internally decompose constraints or retrieve whole patterns.

### Open Question 3
- Question: Why do incorrect model responses consistently exhibit longer token lengths than correct ones across all top-performing models?
- Basis in paper: [explicit] "Incorrect answers are on average significantly longer than correct ones across all top-10 models, suggesting verbose chain-of-thought correlates with confusion."
- Why unresolved: The correlation is demonstrated but the causal mechanism is unclear—whether verbosity reflects productive exploration that fails to converge, or unproductive meandering when models are confused.
- What evidence would resolve it: Controlled experiments varying response length constraints and analyzing whether shorter forced responses improve or degrade accuracy, combined with qualitative analysis of reasoning trajectories in verbose failures.

## Limitations
- Benchmark focuses on 2D structures only, excluding 3D conformational effects and spatial reasoning tasks
- RDKit-based symbolic solvers inherit RDKit's aromaticity model and tautomer handling, which may not align with alternative chemical interpretations
- 5-50 heavy atom range excludes small fragments and large macrocycles where reasoning patterns might differ significantly

## Confidence
- **High Confidence**: Symbolic verification mechanism prevents memorization leakage; performance degradation under SMILES perturbations indicates genuine structural reasoning failures; multitask load systematically reveals compositional reasoning limitations.
- **Medium Confidence**: Chemistry-specialized models underperforming generalists suggests domain-specific instruction tuning can miscalibrate reasoning; counting-to-indexing transfer patterns indicate heuristic-based vs. graph-based reasoning.
- **Low Confidence**: Generalization to reaction prediction and multi-molecule reasoning remains untested; the relationship between task-specific fine-tuning and reasoning degradation requires more systematic investigation.

## Next Checks
1. **SMILES Robustness Test**: Evaluate your model across canonical, randomized, and kekulized SMILES variants on 100 molecules from MolecularIQ. If accuracy drops >15% between variants, the model relies on token patterns rather than graph reasoning.
2. **Counting-Indexing Transfer Analysis**: For 50 paired questions on identical molecules, compute P(correct indexing | correct counting). Transfer rates below 70% indicate heuristic-based counting rather than genuine structural understanding.
3. **Multitask Load Scaling**: Test your model on generation tasks with 1, 2, 3, and 5 simultaneous constraints. Plot accuracy vs. load; if degradation exceeds p_single^n, models lack compositional coordination capabilities.