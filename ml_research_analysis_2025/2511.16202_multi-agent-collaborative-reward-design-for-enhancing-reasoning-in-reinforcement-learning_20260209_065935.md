---
ver: rpa2
title: Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement
  Learning
arxiv_id: '2511.16202'
source_url: https://arxiv.org/abs/2511.16202
tags:
- reward
- arxiv
- reasoning
- feedback
- collaborative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRM, a multi-agent collaborative reward modeling
  framework that decomposes complex preference evaluation into specialized agents
  for improved robustness and interpretability in RLHF. The framework integrates domain-specific
  evaluators (reasoning, factuality, helpfulness, safety) with global ranker-based
  and embedding-similarity rewards, fused through a centralized aggregator into a
  single training signal.
---

# Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.16202
- Source URL: https://arxiv.org/abs/2511.16202
- Reference count: 8
- 4-agent CRM achieved 29.87% accuracy on GSM8K vs 19.64% for GRPO baseline

## Executive Summary
This paper introduces CRM, a multi-agent collaborative reward modeling framework that decomposes complex preference evaluation into specialized agents for improved robustness and interpretability in RLHF. The framework integrates domain-specific evaluators (reasoning, factuality, helpfulness, safety) with global ranker-based and embedding-similarity rewards, fused through a centralized aggregator into a single training signal. Evaluated on RewardBench and GSM8K, CRM with four agents achieved 29.87% accuracy on GSM8K versus 19.64% for standard GRPO, and improved reasoning accuracy to 0.610 from 0.659, demonstrating significant gains in reasoning robustness and stability while preserving conversational quality and safety.

## Method Summary
CRM decomposes reward evaluation into specialized agents (Data Optimizer, Quality Assessor, Data Synthesizer, Data Analyzer) that evaluate distinct dimensions like logical consistency, step-wise accuracy, and factual synthesis. These partial signals are aggregated with global ranker-based and embedding-similarity rewards through a centralized non-linear fusion operator. The fused scalar reward is used for both policy updates via GAE-based advantage estimation and value model regression. The framework supports both reranking-based (MARM) and embedding-similarity-based (MARM-emb) aggregation strategies, with training conducted using GRPO on the fused reward signal.

## Key Results
- 4-agent CRM achieved 29.87% accuracy on GSM8K versus 19.64% for GRPO baseline
- Reasoning accuracy improved to 0.610 from 0.659 on RewardBench
- Conversational quality metrics remained stable (chat scores ~0.18-0.19) despite architectural changes

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Decomposition of Preference Signals
Decomposing monolithic reward evaluation into specialized agents improves robustness by reducing correlation of failure modes across evaluation dimensions. Each specialist agent evaluates a distinct dimension—logical consistency, step-wise accuracy, factual synthesis, or statistical stability—producing partial scores that are aggregated rather than conflated. This prevents any single reward hack from exploiting all evaluation pathways simultaneously.

### Mechanism 2: Centralized Non-Linear Reward Fusion
A centralized aggregator that applies non-linear fusion to heterogeneous signals enables stable optimization while preserving interpretable multi-dimensional feedback. The aggregator combines collaborative reward with enhanced rewards via a non-linear operator, balancing accuracy, similarity, format, reasoning steps, and repetition penalties. The value model regresses to this fused reward while GAE-based policy updates consume the scalar output.

### Mechanism 3: Step-Level and Model-Level Reward Integration
Combining token-level signals for intermediate reasoning steps with sequence-level semantic rewards improves compositional reasoning over final-answer-only supervision. Outcome Reward and Enhanced Data Reward provide step-level guidance for partial reasoning traces, while cosine similarity and accuracy rewards operate at the sequence level. Format and reasoning-step rewards enforce structured outputs within designated tags.

## Foundational Learning

- **Generalized Advantage Estimation (GAE)**
  - Why needed here: CRM uses advantage-based policy updates. Understanding bias-variance tradeoff in λ is essential for tuning stability.
  - Quick check question: What happens to variance if λ → 1 vs λ → 0 in GAE?

- **Reward Model Ensembles and Overoptimization**
  - Why needed here: CRM is conceptually an ensemble of specialist evaluators. Understanding how ensembles mitigate reward hacking informs why decomposition helps.
  - Quick check question: Why might an ensemble of reward models reduce overoptimization compared to a single RM?

- **Multi-Objective RL and Scalarization**
  - Why needed here: CRM scalarizes multi-dimensional preferences via weighted sum. Tradeoffs between objectives depend on weight tuning.
  - Quick check question: If α (accuracy) is set much higher than β (similarity), what failure mode might emerge?

## Architecture Onboarding

- **Component map**: Policy model → Specialist agents → Global evaluators → Aggregator → Value model & Policy update
- **Critical path**: Prompt x → Policy generates rollout with `<think/>` and `<answer/>` sections → Each specialist agent evaluates rollout on its dimension → Global evaluators compute ranker and similarity scores → Aggregator fuses all signals → GAE computes advantage for policy update and value model regresses to fused reward
- **Design tradeoffs**: More agents provide finer-grained feedback but increase compute and coordination cost; reranking-based aggregation is more discriminative while embedding-based is more stable; weight tuning is empirical without automatic calibration
- **Failure signatures**: Reasoning score drops while GSM8K rises (potential overfitting to task-specific patterns); chat scores remain flat (reward may not transfer to dialogue); if repetition penalty is too low, model may loop; if too high, exploration is stifled
- **First 3 experiments**:
  1. Ablation by agent count: Train 2-agent, 3-agent, 4-agent configs on same data split; measure GSM8K and RewardBench reasoning scores to isolate marginal contribution of each specialist
  2. Weight sensitivity sweep: Vary α/β/γ/δ/η ratios on held-out validation set; plot reward stability and accuracy to identify robust operating regions
  3. Baseline comparison: Run standard GRPO with single RM vs CRM-MARM on identical rollouts; log per-dimension scores to verify decomposition reduces reward variance without sacrificing final accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the experimental design and results presented.

## Limitations
- GSM8K accuracy gains come from ablation on different agent counts rather than controlled comparisons on identical data splits
- Reasoning score improvement lacks statistical significance testing and shows counterintuitive trade-offs with conversational quality
- Weight tuning for non-linear fusion is empirical without systematic sensitivity analysis or automatic calibration methods

## Confidence

- **High confidence**: The decomposition mechanism and basic architecture are well-specified and logically sound
- **Medium confidence**: GSM8K accuracy improvements are empirically demonstrated but require more rigorous ablation studies
- **Low confidence**: Conversational quality metrics show minimal improvement despite significant architectural changes

## Next Checks

1. **Controlled ablation study**: Run 2-agent, 3-agent, and 4-agent configurations on identical training/validation splits with multiple random seeds, measuring both GSM8K accuracy and reasoning score variance

2. **Weight sensitivity analysis**: Systematically sweep fusion weights (α, β, γ, δ, η) across a grid while holding other factors constant, plotting reward stability and task performance

3. **Per-dimension impact analysis**: Compare CRM-MARM against baseline GRPO on identical rollouts, logging specialist agent scores separately to verify decomposition reduces reward variance