---
ver: rpa2
title: 'APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design'
arxiv_id: '2505.03748'
source_url: https://arxiv.org/abs/2505.03748
tags:
- psum
- apsq
- energy
- quantization
- psums
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high-precision partial sum
  (PSUM) accesses in DNN accelerators, which account for significant energy consumption
  in input-stationary (IS) and weight-stationary (WS) dataflows. The authors introduce
  Additive Partial Sum Quantization (APSQ), a method that integrates PSUM accumulation
  into the quantization framework, enabling INT8 PSUM storage and access.
---

# APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design

## Quick Facts
- arXiv ID: 2505.03748
- Source URL: https://arxiv.org/abs/2505.03748
- Reference count: 30
- Primary result: INT8 PSUM quantization with grouping strategy reduces energy consumption by 28-87% with minimal accuracy loss (0.16-0.83%) on NLP and CV tasks

## Executive Summary
This paper addresses the significant energy consumption caused by high-precision partial sum (PSUM) accesses in DNN accelerators, which can account for 50-70% of total energy in Input Stationary (IS) and Weight Stationary (WS) dataflows. The authors introduce Additive Partial Sum Quantization (APSQ), which integrates PSUM accumulation into the quantization framework, enabling INT8 PSUM storage while maintaining accuracy through recursive quantization that considers cumulative history. They also propose a grouping strategy combined with a reconfigurable architecture to enhance accuracy and support various group sizes, achieving nearly lossless accuracy on NLP and CV tasks while reducing energy costs by 28-87%.

## Method Summary
APSQ addresses high-precision PSUM accesses by recursively quantizing partial sums with awareness of accumulation history. The method computes each additive PSUM (APi) by quantizing the sum of the current high-precision PSUM tile and the dequantized previous AP: APi = Qk(Tpi + αi-1 · APi-1). A grouping strategy partitions PSUM tiles into groups, applying APSQ only once per group to reduce quantization frequency and error accumulation. The reconfigurable architecture (RAE) uses four SRAM banks with static/dynamic control encoding to support multiple group sizes with minimal area overhead (3.21%). The approach is trained using LSQ with straight-through estimator, constrained to power-of-two scaling factors, and evaluated with knowledge distillation from full-precision models.

## Key Results
- BERT-Base on GLUE: 0.16% accuracy drop with 64.38% energy reduction (IS), 74.37% (WS)
- Segformer-B0 on ADE20K: 0.61% mIoU drop with 86.81% energy reduction (IS), 66.22% (WS)
- EfficientViT-B1 on ADE20K: 0.83% mIoU drop with 87.42% energy reduction (IS), 72.92% (WS)
- LLaMA2-7B on 7 zero-shot tasks: 0.59% accuracy drop with 31.7× energy reduction (WS only)

## Why This Works (Mechanism)

### Mechanism 1: Recursive Quantization with Accumulation Awareness
- Claim: Quantizing PSUMs to INT8 enables energy reduction while maintaining accuracy when the quantizer incorporates cumulative history.
- Mechanism: APSQ recursively computes each additive PSUM (APi) by quantizing the sum of the current high-precision PSUM tile (Tpi) and the dequantized previous AP. The quantizer "sees" the accumulation trajectory rather than treating each PSUM in isolation: APi = Qk(Tpi + αi-1 · APi-1).
- Core assumption: The quantization noise introduced at each step remains bounded and does not compound catastrophically across the accumulation chain.
- Evidence anchors:
  - [abstract] "seamlessly integrating PSUM accumulation into the quantization framework"
  - [section III-A] Equation (10): APi = Qi_k(Tpi + αi-1 · APi-1), for 1 ≤ i < np
  - [corpus] Column-wise Quantization paper addresses PSUM quantization errors in CIM but for ADC overhead reduction, not memory access—showing PSUM quantization is an active but differently-targeted research area.
- Break condition: If accumulation depth (Ci) is extremely large without grouping, repeated quantization rounds may cause error accumulation exceeding model tolerance.

### Mechanism 2: Grouping Strategy to Reduce Quantization Frequency
- Claim: Applying APSQ only once per group of PSUM tiles reduces rounding error accumulation while preserving INT8 memory benefits.
- Mechanism: Partition PSUM tiles into groups of size gs. Within each group, only the final accumulation triggers APSQ; intermediate PSUMs undergo standard quantization without additive feedback.
- Core assumption: The model can tolerate the mixed quantization pattern (some PSUMs with APSQ, some without).
- Evidence anchors:
  - [abstract] "A grouping strategy that combines APSQ with PSUM quantization enhanced by a reconfigurable architecture"
  - [section III-B] "reducing the quantization frequency is essential, as it can minimize the additional rounding error"
  - [section IV-B] Table I shows gs=1 causes notable accuracy drops; gs=2-4 restores performance with best gs varying per model/task.
- Break condition: If gs is too large for the available buffer capacity, PSUMs spill to DRAM, negating energy benefits.

### Mechanism 3: Reconfigurable Hardware Mapping for Variable Group Sizes
- Claim: A banked PSUM buffer with static/dynamic control encoding supports multiple gs values with minimal area overhead.
- Mechanism: The Reconfigurable APSQ Engine (RAE) uses four SRAM banks, shifters for power-of-two scaling, and a two-stage adder pipeline. Static encodings (s0, s1) configure multiplexers for the target gs; dynamic encoding (s2) toggles between APSQ and standard PSUM quantization modes per cycle.
- Core assumption: Scaling factors can be constrained to power-of-two values, enabling shift-based dequantization without multipliers.
- Evidence anchors:
  - [abstract] "reconfigurable architecture to support various grouping configurations"
  - [section III-C] "RAE's work mode is governed by static encodings s0 and s1, in conjunction with a dynamic encoding s2"
  - [section IV-C] Table II: RAE adds only 3.21% area (86,410 µm²) to baseline accelerator.
- Break condition: If target gs exceeds four, the four-bank design requires extension.

## Foundational Learning

- Concept: **Dataflow Classification (IS/WS/OS)**
  - Why needed here: APSQ specifically targets Input Stationary and Weight Stationary dataflows where PSUMs are repeatedly written to/read from SRAM. Output Stationary dataflows keep PSUMs in registers and are not the target.
  - Quick check question: In your target accelerator, where are partial sums stored during accumulation—SRAM/DRAM or output registers?

- Concept: **Quantization-Aware Training (QAT) with STE**
  - Why needed here: APSQ scaling factors are learned during training using LSQ with STE, constrained to power-of-two values. Without QAT, the quantization error from APSQ would be unmitigated.
  - Quick check question: Can your training pipeline support learnable quantization parameters with straight-through estimator gradients?

- Concept: **Precision Factor (β) in Energy Models**
  - Why needed here: The energy analysis introduces β as the ratio of PSUM precision to activation precision. This quantifies the energy penalty of high-precision PSUM storage (β=4 for INT32 PSUM with INT8 activations).
  - Quick check question: What is the required PSUM bit-width for your target layer's accumulation depth (16 + log₂(Ci))?

## Architecture Onboarding

- Component map: PE Array -> RAE Controller -> Four SRAM Banks -> Shifter Modules -> Two-stage Adder Pipeline
- Critical path:
  1. PE generates Tpi (32-bit) -> RAE receives input
  2. If s2=0: Direct quantization to INT8, write to appropriate bank
  3. If s2=1: Read gs previous APs from banks, dequantize via shifters, accumulate in adder pipeline with Tpi, quantize result, write to target bank
  4. Final APnp-1 becomes output tile To
- Design tradeoffs:
  - gs=1: Maximum quantization frequency, lowest accuracy, simplest control (s2 always 1)
  - gs=4: Minimum quantization frequency, best accuracy potential, but larger PSUM buffer footprint may trigger DRAM spills
  - Area vs. Energy: 3.21% area increase for 28-87% energy reduction
  - Scaling factor flexibility: Power-of-two constraint enables shifters but limits dynamic range compared to learnable arbitrary scales
- Failure signatures:
  - Accuracy collapse with gs=1: Indicates model is sensitive to repeated quantization; increase gs
  - Energy savings degrade at high gs under WS: Buffer overflow to DRAM detected; reduce gs or increase buffer size
  - LLM shows minimal IS improvement: Input is a vector during decoding; PSUM tile is small—WS dataflow is the meaningful target
- First 3 experiments:
  1. Baseline energy profiling: Run target model (e.g., BERT-Base) with INT32 PSUM on IS/WS dataflows, measure energy breakdown to confirm PSUM dominates (target: ~50-70% of energy per Figure 1).
  2. APSQ sweep with gs=1,2,3,4: Apply APSQ in QAT, evaluate accuracy on target task (e.g., GLUE for NLP). Identify optimal gs where accuracy drop <1% from baseline.
  3. Buffer overflow boundary test: For optimal gs, vary input resolution/sequence length and monitor for DRAM spills (indicated by energy savings degradation). Adjust buffer sizing or gs accordingly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can APSQ be adapted to achieve significant energy savings for Large Language Models (LLMs) under Input Stationary (IS) dataflows?
- Basis in paper: [explicit] Page 6 notes that APSQ currently provides "minimal enhancement" on IS dataflows (1.02× savings) compared to WS (31.7×) for LLaMA2-7B, stating "Further explorations will be conducted in our future work."
- Why unresolved: The current method is optimized for WS dataflows where PSUMs are large; the vector-based nature of LLM inference in IS dataflows renders the current APSQ approach ineffective for energy reduction.
- What evidence would resolve it: A modified APSQ mechanism demonstrating >5× energy reduction for LLMs on IS dataflows without accuracy degradation.

### Open Question 2
- Question: Can a predictive heuristic be developed to determine the optimal group size ($g_s$) for a specific model without exhaustive empirical search?
- Basis in paper: [explicit] Page 5 states that improvements with different $g_s$ values "are not strictly monotonic" and "intrinsic characteristics of each model affect the effectiveness... leading to diverse outcomes."
- Why unresolved: The paper relies on a reconfigurable architecture (RAE) to support various $g_s$ values but determines the best setting through experimental evaluation rather than a theoretical framework.
- What evidence would resolve it: A mathematical model that correlates layer dimensions or activation distributions to the optimal $g_s$, matching empirical accuracy results.

### Open Question 3
- Question: How does the additive quantization error in APSQ scale when applied to sub-8-bit weight-activation quantization (e.g., W4A4)?
- Basis in paper: [inferred] The paper exclusively evaluates W8A8 quantization (Page 4) and highlights that reducing PSUM precision below INT8 yields "minimal benefits and incurs substantial accuracy loss."
- Why unresolved: It is unclear if the additive error propagation in APSQ would remain manageable (lossless) if the input partial sums ($T_{pi}$) were derived from lower-precision multiplications (INT4).
- What evidence would resolve it: Accuracy benchmarks of APSQ applied to W4A4 or W2A8 models, demonstrating whether the INT8 PSUM constraint is sufficient to recover accuracy in low-bit regimes.

## Limitations

- Dataflow applicability gap: APSQ is optimized for IS/WS dataflows but provides minimal benefit for OS dataflows and LLM IS dataflow where PSUMs remain in registers
- Buffer size sensitivity: Optimal group size varies significantly across models and can cause DRAM spills at higher values, reducing energy savings from 87% to 66%
- LLM scaling uncertainty: Results are limited to zero-shot inference on 7 tasks for LLaMA2-7B, with IS dataflow providing minimal benefit for LLM decoding

## Confidence

- High Confidence: The recursive quantization mechanism with accumulation awareness is well-supported by mathematical formulation and ablation results showing accuracy degradation at gs=1
- Medium Confidence: The grouping strategy's effectiveness is demonstrated across three model families, but optimal gs varies significantly (BERT-Base: gs=2, Segformer-B0: gs=4, EfficientViT-B1: gs=3)
- Low Confidence: LLM results are promising but based on limited evaluation (zero-shot only, 7 tasks) and acknowledge IS dataflow provides minimal benefit for LLM decoding

## Next Checks

1. **Buffer Overflow Characterization**: Systematically vary input sequence length and resolution for Segformer-B0 and EfficientViT-B1 to map the boundary conditions where gs=3-4 causes DRAM spills. Determine if dynamic gs adjustment per layer could maintain energy benefits.

2. **Cross-Dataflow Generalization**: Evaluate APSQ on a hybrid dataflow accelerator that combines IS/WS for different layers. Test whether APSQ can be selectively applied where PSUM accumulation frequency is highest, rather than being dataflow-specific.

3. **Precision Sensitivity Analysis**: Systematically vary the assumed PSUM precision (16-bit vs 32-bit) for models with different accumulation depths. Determine the minimum precision required before APSQ accuracy degradation exceeds 1% across all tested models.