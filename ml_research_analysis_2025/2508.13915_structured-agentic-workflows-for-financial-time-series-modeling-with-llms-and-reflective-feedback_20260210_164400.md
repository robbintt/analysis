---
ver: rpa2
title: Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and
  Reflective Feedback
arxiv_id: '2508.13915'
source_url: https://arxiv.org/abs/2508.13915
tags:
- financial
- time-series
- code
- ts-agent
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TS-Agent, a modular agentic framework designed
  to automate financial time-series modeling workflows. The system formalizes the
  pipeline as a structured, iterative decision process across three stages: model
  selection, code refinement, and fine-tuning, guided by contextual reasoning and
  experimental feedback.'
---

# Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback

## Quick Facts
- **arXiv ID:** 2508.13915
- **Source URL:** https://arxiv.org/abs/2508.13915
- **Reference count:** 38
- **Primary result:** TS-Agent achieves 20-30% accuracy improvements and 100% success rates over AutoML and agentic baselines in financial time-series forecasting and generation.

## Executive Summary
This paper introduces TS-Agent, a modular agentic framework that automates financial time-series modeling workflows through structured, iterative decision processes. The system employs a planner agent guided by contextual reasoning and experimental feedback, utilizing structured knowledge banks including a Case Bank of past financial modeling tasks, a Financial Time-Series Code Base with executable models and metrics, and a Refinement Knowledge Bank encoding expert heuristics. TS-Agent demonstrates superior performance on diverse financial forecasting and synthetic data generation tasks, achieving significant improvements in accuracy, robustness, and decision traceability while maintaining high success rates and interpretability.

## Method Summary
TS-Agent is a two-stage agentic framework that automates financial time-series modeling. Stage 1 uses a Case Bank (via RAG) to select top-k models based on historical tasks. Stage 2 employs a feedback loop with Warm-up and Optimization phases where the agent iteratively edits a `train.py` script using a Refinement Knowledge Bank, executes code, and updates memory based on performance metrics. The system supports both forecasting (predicting future prices/volatility) and synthetic data generation tasks using 10 specified models including Autoformer, PatchTST, TimeGAN, and others across three datasets: Crypto (20 pairs, 2024), Exchange (8 pairs, 1990-2010), and Stock (10 U.S. stocks, 2020-2024).

## Key Results
- Achieves more than 20% reduction in RMSE compared to AutoGluon on Exchange datasets
- Shows up to 30% improvement over DS-Agent in forecasting tasks
- Demonstrates consistent 100% success rates across multiple datasets for generation tasks
- Maintains superior accuracy, robustness, and decision traceability compared to state-of-the-art AutoML and agentic baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured knowledge banks constrain the search space and align model selection with domain-specific financial requirements.
- **Mechanism:** The Planner Agent queries a Case Bank (historical tasks) and a Refinement Knowledge Bank (expert heuristics) before generating code, retrieving relevant context that guides the LLM toward financially sound architectures.
- **Core assumption:** The LLM possesses sufficient reasoning capabilities to map retrieved textual heuristics into correct code implementations.
- **Evidence anchors:** Abstract states knowledge banks "guide exploration, while improving interpretability"; Section 3.2 describes Case Bank as derived from industry benchmarks and peer-reviewed papers; neighbor corpus suggests general-purpose agents lack specificity without modular knowledge integration.
- **Break condition:** Performance degrades if knowledge banks contain irrelevant cases or task descriptions lie outside their distribution.

### Mechanism 2
- **Claim:** Iterative reflective feedback enables adaptive optimization by converting numerical execution logs into corrective planning signals.
- **Mechanism:** The system executes a "Chain-of-code-edits" loop where the agent proposes changes, executes scripts, and captures results into dynamic memory. Accepted changes reduce loss; rejected changes are reverted.
- **Core assumption:** Numerical metrics and error logs provide sufficient signal for the LLM to deduce why changes failed and determine corrective actions.
- **Evidence anchors:** Abstract mentions "contextual reasoning and experimental feedback"; Section 4 describes the accept/reject heuristic; neighbor corpus highlights LLM struggles with "hallucination" in time-series reasoning.
- **Break condition:** Feedback loop fails if LLM misinterprets error logs or optimization landscape is too flat/noisy.

### Mechanism 3
- **Claim:** Modular action decomposition reduces code generation errors compared to end-to-end synthesis.
- **Mechanism:** The agent decomposes the action space into model selection, code refinement, and fine-tuning, restricting edits to specific sub-modules of a provided template.
- **Core assumption:** The provided `train.py` template is sufficiently robust to support required variations without structural rewrites.
- **Evidence anchors:** Abstract mentions "reduces error propagation"; Section 4 states only refinement actions may introduce bugs, increasing controllability; neighbor corpus notes full automation is brittle.
- **Break condition:** Fails if target architecture requires fundamental data pipeline changes incompatible with the template.

## Foundational Learning

**Concept: Case-Based Reasoning (CBR)**
- **Why needed here:** TS-Agent uses CBR in Stage 1 to select models; understanding CBR is essential to debug why the agent retrieves specific (potentially outdated) financial models.
- **Quick check question:** If the Case Bank only contains volatility forecasting cases, what model would TS-Agent likely select for a stationary interest rate prediction task?

**Concept: Reflective/Verbal Reinforcement Learning**
- **Why needed here:** The "Feedback Loop" relies on the LLM reflecting on its own execution logs, differing from standard gradient descent by using natural language "reflections" as policy updates.
- **Quick check question:** How does the agent's memory update if the code executes successfully but returns a validation loss of `NaN`?

**Concept: Financial Time-Series Regimes**
- **Why needed here:** The Refinement Knowledge Bank explicitly encodes strategies for "regime shifts"; without this, the agent might apply standard stationarity assumptions to non-stationary financial data.
- **Quick check question:** Why might a standard ARIMA model (often in general AutoML) fail in the TS-Agent pipeline for high-frequency crypto data?

## Architecture Onboarding

**Component map:**
Planner Agent (LLM core) -> External Resources (Case Bank, Refinement Bank, Code Base) -> Dynamic Memory -> Executor (sandbox running `train.py`)

**Critical path:** The Optimization Phase in Stage 2. If the agent cannot successfully parse logs from the Warm-up Phase to refine the code, the accuracy gains (20-30% cited) will not materialize.

**Design tradeoffs:**
- **Reliability vs. Novelty:** Fixed Code Base and template ensure high success rates (100% claimed) but may struggle with novel architectures not in the model bank
- **Cost vs. Performance:** Iterative Round-Robin search implies multiple LLM calls and training runs, increasing latency and compute cost compared to single-pass AutoML

**Failure signatures:**
- **Static Retrieval Loop:** Agent retrieves same refinement tip repeatedly without improving metric (stuck in local optimum)
- **Template Mismatch:** Syntax errors from trying to fit incompatible input dimensions into fixed `train.py` scaffold

**First 3 experiments:**
1. **Ablation on Knowledge:** Run pipeline with empty Refinement Knowledge Bank to isolate performance gain from expert heuristics vs. LLM's internal knowledge
2. **Stress Test Success Rate:** Force agent to use weaker LLM backbone (GPT-3.5) on complex Crypto dataset to verify if "Chain-of-code-edits" maintains claimed success rate
3. **Out-of-Distribution Task:** Provide task requiring custom loss function not present in Evaluation Measure Bank to test Code Refinement stage flexibility

## Open Questions the Paper Calls Out
- **Integration of multimodal financial data:** How can TS-Agent be extended to integrate news sentiment, earnings calls, and order book dynamics alongside pure time-series inputs?
- **Automatic knowledge enrichment:** How can the Case Bank, Refinement Knowledge Bank, and Code Base be automatically enriched by mining research papers and public repositories?
- **Live trading performance:** How does TS-Agent perform in live or real-time trading environments versus backtesting on historical data?

## Limitations
- Effectiveness heavily depends on knowledge bank coverage and quality, which are not fully specified
- Iterative feedback mechanism assumes LLMs can reliably interpret execution logs for corrective actions
- Fixed template approach may limit adaptability to novel model architectures

## Confidence
- **High Confidence:** Modular architecture design and general framework structure (mechanisms 1 and 3)
- **Medium Confidence:** Specific performance improvements over baselines, as these depend on implementation details not fully disclosed
- **Low Confidence:** 100% success rate claim, which may be dataset-dependent and not generalize to more complex scenarios

## Next Checks
1. **Knowledge Bank Sensitivity:** Test TS-Agent with progressively degraded knowledge banks to quantify performance contribution of expert heuristics vs. LLM's internal knowledge
2. **Stress Test Generalization:** Evaluate on out-of-distribution tasks requiring custom model architectures or loss functions not present in code base
3. **Cross-Dataset Robustness:** Verify consistent performance improvements across different market regimes (bull/bear markets) and asset classes (stocks vs. crypto)