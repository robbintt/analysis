---
ver: rpa2
title: 'Judging with Confidence: Calibrating Autoraters to Preference Distributions'
arxiv_id: '2510.00263'
source_url: https://arxiv.org/abs/2510.00263
tags:
- response
- better
- preference
- autoraters
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the fundamental problem of training autoraters
  on discrete preference labels, which fails to capture the inherent subjectivity
  and distributional nature of human judgment. The authors propose a general probabilistic
  framework that calibrates autoraters to model the full preference distribution rather
  than collapsing it to a single verdict.
---

# Judging with Confidence: Calibrating Autoraters to Preference Distributions

## Quick Facts
- **arXiv ID:** 2510.00263
- **Source URL:** https://arxiv.org/abs/2510.00263
- **Reference count:** 40
- **Primary result:** Distribution-matching autoraters reduce MSE by 18-51%, ECE by 4-45%, and positional bias by 7-81% compared to binary baselines.

## Executive Summary
This paper addresses a fundamental challenge in training LLM autoraters: capturing the inherent subjectivity of human judgment. Traditional autoraters trained on discrete preference labels collapse complex preference distributions into single verdicts, missing the nuanced reality of human evaluation. The authors propose a general probabilistic framework that calibrates autoraters to model the full preference distribution rather than a binary outcome. They introduce two distribution-matching finetuning methods - supervised fine-tuning with dense probabilistic labels and reinforcement learning with binary labels using proper scoring rules - and demonstrate significant improvements in alignment with target preference distributions while maintaining performance on objective tasks.

## Method Summary
The method introduces a probabilistic framework for autorater calibration that predicts preference probabilities rather than binary verdicts. Two finetuning paradigms are proposed: (1) Supervised fine-tuning (SFT) on prompts with 10 aggregated labels each to learn from mean probabilities, and (2) Reinforcement learning (RL) using GRPO with proper scoring rule rewards (Brier or Log) trained on single binary labels. The approach uses structured output parsing to extract probabilities from model text and employs swap augmentation to enforce symmetric consistency. Base models (Gemma-2-9B, Qwen-2.5-7B) are fine-tuned with full-parameter updates, and evaluation measures include MSE, ECE, Brier score, agreement with majority labels, and positional bias metrics.

## Key Results
- RL-trained autoraters on sparse binary labels (50K prompts) consistently outperform SFT-trained models on dense labels (5K prompts with 10 annotations each)
- Significant reduction in positional bias: RL-Brier achieves 0.9007 consistency and 0.0964 absolute symmetry deviation
- Out-of-distribution performance: RL autoraters show improved alignment on human-annotated PandaLM data while maintaining objective task performance on JudgeBench
- Data efficiency advantage: RL achieves superior calibration with fewer total annotations due to broader prompt diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning with strictly proper scoring rules can recover the true preference distribution from sparse binary labels.
- Mechanism: The paper proposes using Brier ($1 - (p - y)^2$) or Log ($y \log p$) rewards within a policy gradient framework. Theoretically, strictly proper scoring rules ensure that the expected reward is maximized if and only if the predicted probability matches the true population probability ($p^*$).
- Core assumption: The policy class is expressive enough to represent the probability and the optimization reaches a global maximum (Proposition 1).
- Evidence anchors:
  - [abstract]: "reinforcement learning approach for sparse, binary labels... finetuning... with a distribution-matching objective."
  - [section]: Section 3.1 states that the optimal policy under Brier or Log rewards is "deterministic... and reports the truthful probability."
  - [corpus]: Paper `2510.13501` suggests confidence can be used as a reward, but this paper formalizes it using proper scoring rules for distribution recovery.
- Break condition: If the reward signal is extremely sparse or the KL penalty is too high, the model may fail to explore, resulting in parsable but static (e.g., always 0.5) outputs.

### Mechanism 2
- Claim: Prompt diversity is more data-efficient for calibration than per-example label density.
- Mechanism: The paper compares two fixed-budget settings: (1) SFT on 5k prompts with 10 labels each (dense), and (2) RL on 50k prompts with 1 label each (sparse). RL on sparse data outperformed SFT, suggesting that seeing a wider variety of comparison pairs allows the model to better generalize the concept of "preference probability" than seeing precise variance estimates on fewer items.
- Core assumption: The single binary labels are sampled i.i.d. from the true population distribution.
- Evidence anchors:
  - [abstract]: "...finetuning... to match the target preference distribution."
  - [section]: Section 5.1 notes "RL... trained on 50K prompts with a single binary label each, consistently outperform their SFT counterparts... trained on 5K prompts with 10 aggregated labels."
  - [corpus]: Weak corpus signal regarding the specific tradeoff between prompt count and label density in this specific context.
- Break condition: If the single labels are biased (not i.i.d. from the target population), the RL method will converge to a biased distribution, whereas SFT with dense labels might average out noise better.

### Mechanism 3
- Claim: Distribution-matching objectives implicitly reduce positional bias by enforcing symmetric consistency.
- Mechanism: By training the model to predict a probability $p$ such that it minimizes a proper scoring rule, the model is penalized for "flip-flopping" (predicting $p$ for A>B and then $\neq 1-p$ for B>A). The symmetry requirement of the distribution ($p(A>B) + p(B>A) = 1$) appears to act as a regularizer against order-induced features.
- Core assumption: The ground truth preference distribution is itself swap-symmetric.
- Evidence anchors:
  - [abstract]: "...significant reduction in positional bias..."
  - [section]: Section 5.2 shows "RL-Brier model... achieves a consistency of 0.9007 and a near-perfect absolute symmetry deviation of 0.0964."
  - [corpus]: Paper `2504.09946` discusses reasoning model biases, highlighting that standard training often fails to correct this without explicit intervention.
- Break condition: If the training data contains significant positional artifacts that are not counteracted by swap augmentation, the model may learn to predict the distribution of the *biased* annotators rather than the ideal symmetric preference.

## Foundational Learning

### Concept: Strictly Proper Scoring Rules
- Why needed here: These are the mathematical foundation for the RL reward. Without them, the model might optimize for a metric that rewards overconfidence or hedging rather than truthfulness.
- Quick check question: If a model predicts 0.8 probability for an event that happens 80% of the time, does the Brier score encourage this prediction over a 0.9 prediction?

### Concept: Aleatoric vs. Epistemic Uncertainty
- Why needed here: The paper focuses on capturing *aleatoric* uncertainty (inherent subjectivity of the population). Understanding this distinction prevents misinterpreting the output probability as the model's own "epistemic" uncertainty about a single ground truth.
- Quick check question: Does a 50/50 prediction mean the model is confused, or that the annotators genuinely disagreed?

### Concept: Fisher Consistency
- Why needed here: It provides the theoretical guarantee that the proposed RL method actually converges to the desired distribution.
- Quick check question: Does maximizing the expected Brier reward guarantee that the predicted probability converges to the true conditional probability?

## Architecture Onboarding

### Component map:
- Policy ($\pi_\theta$): The LLM (e.g., Gemma-2-9B) that outputs text
- Parser ($g$): A deterministic rule-based extractor that finds the `<prob_rb_better>` tag and converts text to float
- Reward Calculator: Computes $R_{Brier}$ or $R_{Log}$ based on the parsed float and the ground truth binary label
- Reference Model: Used for KL divergence penalty to prevent policy drift (GRPO)

### Critical path:
Prompt (A, B) -> Policy generates Text -> Parser extracts $p$ -> Reward calculates $R$ -> GRPO update

### Design tradeoffs:
- **SFT vs. RL:** Choose SFT if you have high-quality multi-annotator labels and want stability. Choose RL if you have a large volume of single-label data (more cost-effective)
- **Brier vs. Log:** Brier is bounded and smooth, generally more stable. Log has unbounded penalties for confident errors, potentially better for calibration but riskier for stability

### Failure signatures:
- **Unparsable Output:** Model generates text without the probability tag. (Mitigation: Reward unparsable outputs as 0 or $-\infty$)
- **Hedging:** Model collapses to predicting 0.5 for everything to minimize risk
- **Positional Collapse:** Model ignores content and relies on order, detectable if $p_{orig} + p_{swap} \neq 1$

### First 3 experiments:
1. **Sanity Check (Swap Consistency):** Run inference on a held-out set, then swap A/B. Verify that $|p_{orig} + p_{swap} - 1|$ is low for your trained model vs. baseline
2. **Reward Ablation:** Train two small models, one with Brier and one with Log reward. Compare ECE and training stability curves
3. **Data Efficiency Curve:** Vary the number of sparse RL labels (e.g., 10k, 25k, 50k) and plot MSE against the SFT baseline to identify the crossover point where RL becomes superior

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the distribution-matching framework be effectively extended to pointwise evaluation settings (e.g., ordinal Likert scale ratings) using multi-class proper scoring rules?
- **Basis in paper:** [explicit] Appendix E states that while empirical results focused on pairwise evaluation, the framework applies to pointwise settings and leaves "a thorough empirical study in this space as future work."
- **Why unresolved:** The paper only validates the framework on binary pairwise preferences ($B \succ A$); it does not test nominal or ordinal single-response grading.
- **What evidence would resolve it:** Empirical results showing that fine-tuning with multi-class Brier scores or cross-entropy rewards successfully calibrates autoraters for single-response grading tasks.

### Open Question 2
- **Question:** Can this framework be adapted to distinguish epistemic uncertainty (model ignorance) from aleatoric uncertainty (human subjectivity), potentially via second-order distributions?
- **Basis in paper:** [explicit] Appendix E notes the current verbalized confidence represents aleatoric uncertainty and suggests that predicting epistemic uncertainty "may require predicting a second-order distribution," though this complicates training.
- **Why unresolved:** The current method outputs a single probability $p$ reflecting population variance, which conflates genuine ambiguity with the model's potential lack of knowledge on out-of-distribution samples.
- **What evidence would resolve it:** A training mechanism that elicits a distribution over the predicted probability and demonstrates improved detection of out-of-distribution inputs or novel scenarios.

### Open Question 3
- **Question:** How can the training stability of the logarithmic (Log) reward be improved to leverage its theoretical strict propriety without succumbing to gradient instability?
- **Basis in paper:** [inferred] Section 5.1 notes the Log reward underperforms the Brier reward, likely because "heavy penalties for tail miscalibrations can introduce training instability," suggesting a trade-off between theoretical optimality and optimization difficulty.
- **Why unresolved:** The authors utilized the Log reward but found it empirically inferior to Brier due to optimization challenges, leaving the potential benefits of strict proper scoring rules unrealized.
- **What evidence would resolve it:** A modified optimization strategy or reward shaping that allows the Log reward to achieve convergence rates and calibration metrics equal to or better than the Brier reward.

## Limitations
- The specific persona-based prompts used to generate preference annotations are not fully detailed, creating uncertainty about target distribution reproducibility
- GRPO implementation details, particularly advantage estimation and KL divergence penalty calculation, are underspecified
- The exact magnitude of improvements may be specific to JudgeLM corpus and chosen base models (Gemma-2-9B, Qwen-2.5-7B)

## Confidence
- **High Confidence:** Theoretical foundation (strictly proper scoring rules, Fisher consistency) and experimental methodology are sound
- **Medium Confidence:** Claim that RL on sparse data is more data-efficient than SFT on dense data is supported but depends on annotation cost assumptions
- **Low Confidence:** Exact improvement magnitudes may be specific to JudgeLM corpus and base models, generalizability uncertain

## Next Checks
1. **Swap Consistency Test:** Run inference on held-out set, swap A/B, verify absolute symmetry deviation is significantly lower than baseline
2. **Reward Ablation Study:** Train models with Brier vs Log reward, compare ECE curves during training and final performance
3. **Data Efficiency Curve:** Vary sparse RL label counts (10k, 25k, 50k), plot MSE against SFT baseline to identify crossover point where RL becomes superior