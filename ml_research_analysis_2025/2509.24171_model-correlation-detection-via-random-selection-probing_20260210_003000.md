---
ver: rpa2
title: Model Correlation Detection via Random Selection Probing
arxiv_id: '2509.24171'
source_url: https://arxiv.org/abs/2509.24171
tags:
- prefixes
- qwen2
- table
- correlation
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random Selection Probing (RSP), a hypothesis-testing
  framework for detecting model correlation between large language models (LLMs) and
  vision-language models (VLMs). RSP optimizes prefixes on a reference model to maximize
  the probability of generating a specific token for a random selection task, then
  tests transferability to a target model, producing statistically rigorous p-values.
---

# Model Correlation Detection via Random Selection Probing

## Quick Facts
- arXiv ID: 2509.24171
- Source URL: https://arxiv.org/abs/2509.24171
- Reference count: 27
- Key outcome: First principled statistical approach for detecting model correlation with p-values as small as 1.00×10⁻³⁰⁰

## Executive Summary
This paper introduces Random Selection Probing (RSP), a hypothesis-testing framework for detecting model correlation between large language models (LLMs) and vision-language models (VLMs). RSP optimizes prefixes on a reference model to maximize the probability of generating a specific token for a random selection task, then tests transferability to a target model, producing statistically rigorous p-values. Experiments demonstrate RSP consistently yields very small p-values for correlated models while maintaining high p-values for unrelated ones, achieving correlation detection under diverse access conditions including gradient, logits, gray-box, and black-box settings.

## Method Summary
RSP detects model correlation by optimizing textual or visual prefixes to maximize the probability of generating a target token on a reference model while minimizing this probability on an unrelated baseline model. The framework uses either gradient-based optimization (GCG) or gradient-free methods (genetic algorithms) to find model-specific features that exploit idiosyncratic patterns within the reference model family. Optimized prefixes are then tested on a target model, and a binomial hypothesis test produces p-values quantifying the evidence of correlation. The method works across both text and vision modalities and functions under different levels of access to the reference and target models.

## Key Results
- RSP consistently produces very small p-values (e.g., 1.00×10⁻³⁰⁰) for correlated models while maintaining high p-values for unrelated ones
- The framework works across both LLMs and VLMs under diverse access conditions including gradient, logits, gray-box, and black-box settings
- Ablation studies confirm robustness to prefix length, mutation probability, and resolution, with shorter prefixes producing even smaller p-values but potentially violating independence assumptions

## Why This Works (Mechanism)

### Mechanism 1: Model-Specific Feature Optimization
The RSP framework optimizes textual or visual prefixes to maximize the probability of generating a target token on a reference model ($M_r$) while minimizing this probability on an unrelated baseline model ($M_u$). This constraint encourages the optimization to find "model-specific" features that exploit idiosyncratic patterns within the $M_r$ family, rather than general features that would transfer broadly. The core assumption is that correlated models share internal representations or functional behaviors that unrelated models do not.

### Mechanism 2: Transferability as a Statistical Test
The transferability of optimized prefixes from a reference model to a target model is formulated as a rigorous statistical hypothesis test. Under the null hypothesis ($H_0$), the reference and target models are independent, and an optimized prefix from the reference model should have no better than a random chance ($1/N$) of succeeding on the target model. The number of successful prefixes ($X$) follows a binomial distribution, and a p-value is computed from this distribution, quantifying the probability of observing the test statistic if the models were truly independent.

### Mechanism 3: Cross-Modal and Access-Flexible Probing
The probing mechanism can be adapted for both text and vision modalities and functions under different levels of access to the reference and target models. For VLMs, visual prefixes (perturbed images) are optimized. For the reference model, RSP uses gradient-based methods when available, or gradient-free methods like genetic algorithms or zeroth-order optimization. For the target model, detection works in both gray-box (logits accessible) and black-box (text-only output) settings.

## Foundational Learning

- **Hypothesis Testing & P-values**: RSP's core output is a p-value from a binomial test. You must understand what a p-value represents (probability of observing data under a null hypothesis) to correctly interpret RSP results. Quick check: If you observe a p-value of 0.01, what does that tell you about the probability that the two models are correlated?

- **Adversarial Prompt Optimization (e.g., GCG, PGD)**: RSP relies on algorithms designed to find adversarial inputs (prefixes) that manipulate a model's output. Understanding how these optimizers work is key to implementing the prefix optimization stage. Quick check: What is the main difference between a gradient-based (e.g., GCG) and a gradient-free (e.g., genetic algorithm) optimization method?

- **Model Fine-tuning and Lineage**: The entire problem presupposes that fine-tuning a model preserves some core "fingerprint" of the parent. Understanding what changes and what stays the same during fine-tuning (e.g., low-rank adaptations, weight updates) gives intuition for why RSP works. Quick check: Does fine-tuning typically change a model's tokenizer or base architecture? Why is this relevant?

## Architecture Onboarding

- **Component map**: Prefix Optimizer -> Transferability Tester -> Statistical Engine
- **Critical path**: The Prefix Optimizer is the most critical and computationally expensive component. If it fails to find model-specific prefixes, the entire test will fail.
- **Design tradeoffs**:
  - Prefix Length vs. Independence: Longer prefixes (e.g., 50 tokens) make it easier to find successful, diverse prefixes but increase optimization cost. Shorter prefixes may collide, violating statistical assumptions.
  - Reference Model Access: Gradient access is faster and more effective for optimization but requires white-box access. Logit-only access is more general but slower and may be less effective.
  - Number of Samples (K): More prefixes (higher K) give smaller p-values for related models (more statistical power) but increase computation.
- **Failure signatures**:
  - High p-value for a known-related model: The optimizer failed to find transferable, model-specific prefixes. Check if the optimization converged or if the baseline model $M_u$ was too similar to $M_r$.
  - Low p-value for an unrelated model (False Positive): The optimizer found "general features" instead of "model-specific" ones. The baseline model $M_u$ may not have been effective at filtering them out.
  - P-value calculation errors: If prefixes are not independent (e.g., too short, too similar), the binomial test assumption is violated.
- **First 3 experiments**:
  1. Reproduce Baseline Table 1: Run RSP on Llama-3-8B-Instruct ($M_r$) and its fine-tuned version on GSM8K ($M_t$). Verify you get a very small p-value (< 0.05).
  2. Ablate the Baseline Model ($M_u$): Run the same experiment but omit the unrelated model $M_u$ during optimization. Test the resulting prefixes on an unrelated model (e.g., Mistral-7B). You should observe the false positive rate increase.
  3. Test an Unrelated Model: Take the prefixes optimized in experiment 1 and test them on a clearly unrelated model (e.g., a small GPT-2 or a model from a different family). Verify that the p-value is high (e.g., > 0.05).

## Open Questions the Paper Calls Out
None

## Limitations

- **Statistical Independence Assumption**: The p-value calculation relies on the assumption that optimized prefixes are independent events, which may be violated by shorter prefixes.
- **Baseline Model Selection**: The effectiveness of RSP depends critically on choosing an unrelated baseline model ($M_u$) that is sufficiently different from the reference model.
- **Generalization Across Model Families**: The framework's performance on other model modifications (pruning, quantization, distillation) or cross-domain adaptations remains untested.

## Confidence

- **High Confidence**: The core statistical framework (binomial hypothesis test with p-values) is mathematically sound and well-established.
- **Medium Confidence**: The claim that RSP works across diverse access conditions is supported by experiments, but the sensitivity to access limitations and optimization algorithm choice requires further characterization.
- **Low Confidence**: The paper's claims about being the "first principled statistical approach" and general applicability to "modern machine learning ecosystems" are somewhat overstated without broader experimental validation.

## Next Checks

1. **Baseline Model Sensitivity Analysis**: Systematically test RSP's sensitivity to baseline model choice by varying the unrelated model across different model families and measuring how baseline similarity affects p-value distributions.

2. **Prefix Independence Validation**: Implement prefix similarity metrics to empirically verify the independence assumption and analyze the relationship between similarity distributions and p-value accuracy.

3. **Cross-Modification Generalization**: Extend experiments beyond fine-tuning to test RSP's effectiveness on other model modifications (pruning, quantization, distillation, cross-modal adaptations) and compare p-value distributions across these modification types.