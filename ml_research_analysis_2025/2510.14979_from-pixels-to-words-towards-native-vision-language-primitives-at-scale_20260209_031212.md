---
ver: rpa2
title: From Pixels to Words -- Towards Native Vision-Language Primitives at Scale
arxiv_id: '2510.14979'
source_url: https://arxiv.org/abs/2510.14979
tags:
- wang
- visual
- native
- zhang
- corr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEO is a new type of vision-language model built without separate
  vision encoders, using a unified architecture from the start. It uses special attention
  and positional encoding designs to better link images and text.
---

# From Pixels to Words -- Towards Native Vision-Language Primitives at Scale

## Quick Facts
- arXiv ID: 2510.14979
- Source URL: https://arxiv.org/abs/2510.14979
- Reference count: 28
- Primary result: NEO matches top modular models on diverse benchmarks using a unified architecture without separate vision encoders

## Executive Summary
NEO introduces a new approach to vision-language modeling by building a unified transformer architecture that processes both images and text without separate vision encoders. The model uses special attention mechanisms and positional encoding designs to better link images and text, achieving competitive performance with 345M image-text pairs. By using a pre-Buffer stage for initial training followed by full end-to-end refinement, NEO demonstrates that native VLMs can perform on par with traditional modular designs while offering reusable components that lower development costs.

## Method Summary
NEO uses a unified transformer architecture with modified attention and positional encoding to process both images and text. The model employs Native-RoPE with frequency decoupling, separating temporal (text) and spatial (image) frequency bands. Multi-Head Native Attention (MHNA) uses mixed masking - causal for text, bidirectional for images within each frame. Training occurs in three stages: first freezing the LLM to train the Pre-Buffer and new parameters, then mid-training on curated data, and finally supervised fine-tuning on instructions. The model processes 32x32 patches through Conv1/Conv2 layers before feeding into stacked Native VLM primitives.

## Key Results
- NEO matches top modular models on MMMU, MMBench, MMVet, MMStar, and other diverse benchmarks
- Achieves 67.7% on POPE (photorealistic object detection) and 48.8% on HallusionBench (visual consistency)
- Competitive performance on document understanding tasks (DocVQA, ChartQA) despite being a native approach

## Why This Works (Mechanism)

### Mechanism 1: Native-RoPE with Frequency Decoupling
A unified position encoding scheme allows a single transformer to process text and high-resolution images by strictly decoupling temporal (T) and spatial (H/W) frequency bands. Standard LLMs use 1D-RoPE, but NEO expands Query/Key head dimensions to create separate channels for Height and Width, assigning high frequencies to H/W for fine-grained local visual semantics and lower frequencies to T for long-range text dependencies. The model preserves pre-trained textual knowledge by keeping original frequency channels untouched while newly initialized spatial channels handle vision.

### Mechanism 2: Multi-Head Native Attention (MHNA) with Mixed Masking
Visual perception improves when image tokens interact bidirectionally within a primarily causal architecture. NEO employs a mixed attention mask where text tokens follow standard causal masking (attending only to the past) while image tokens use bidirectional attention (attending to all other image tokens in the same frame). This allows the model to build a complete visual context before generating text, treating a single image as a "unified meta-unit" for attention without leaking future information relevant to the causal text generation task.

### Mechanism 3: Pre-Buffer and Post-LLM Partitioning
Training stability and convergence speed improve by temporarily partitioning the monolithic model into a trainable visual front-end (Pre-Buffer) and a frozen linguistic back-end (Post-LLM). During pre-training, the Post-LLM weights are frozen while the Pre-Buffer layers learn to map pixel inputs into a representation space that the pre-trained LLM can already understand. This prevents visual hallucination or gradient noise from corrupting the LLM's linguistic priors before alignment is achieved.

## Foundational Learning

- **Rotary Position Embeddings (RoPE)**: Understand how transformers encode position via rotation in complex space to grasp how NEO splits these rotations for 1D text vs. 2D images. *Quick check*: How does rotating a query vector relative to a key vector encode relative distance, and what happens if you try to encode 2D coordinates on a 1D frequency?

- **Modular vs. Native VLM Architectures**: Know what an "Visual Encoder" typically does to understand why removing it requires the "Pre-Buffer" design. *Quick check*: In a modular VLM (like LLaVA), where does the visual feature extraction happen? In NEO, which component replaces this?

- **Causal vs. Bidirectional Attention Masks**: Distinguish between the "prefix" filling pattern (images) and the "autoregressive" generation pattern (text). *Quick check*: If token A is at index 10 and token B is at index 20, can B attend to A in a causal mask? Can it attend to A in a bidirectional mask?

## Architecture Onboarding

- **Component map**: Raw Pixels -> PEL (Conv1/Conv2, 32x32 patches) -> Stacked Native VLM Primitives (Transformer blocks) -> Next token prediction
- **Critical path**: The attention kernel modification. Standard attention implementations assume 1D positions. You must implement or modify the kernel to accept 3D indices (T, H, W) and apply distinct rotary frequencies to the correct head dimensions.
- **Design tradeoffs**: NEO trades the proven performance of specialized Visual Encoders (like ViT-L) for a simpler, monolithic deployment pipeline and potentially lower inference latency (no adapter overhead). The Pre-Buffer (12 layers in NEO-2.2B) is smaller than typical standalone encoders, relying on the LLM layers to assist in perception.
- **Failure signatures**: Image blurring/loss of detail indicates the H/W frequency in Native-RoPE is set too low. Garbled text indicates the Post-LLM was not frozen during pre-training. Poor spatial localization suggests the mixed attention mask is incorrectly implemented.
- **First 3 experiments**: 1) RoPE Ablation: Train a small NEO variant with standard 1D-RoPE vs. Native-RoPE on a synthetic alignment task to verify spatial encoding integrity. 2) Masking Verification: Visualize the attention maps of an image token to confirm bidirectional attention within image regions. 3) Pre-Buffer Depth: Compare a 4-layer vs. 12-layer Pre-Buffer while keeping LLM frozen to measure the "visual sufficiency" of the front-end.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent can the performance gap in knowledge-intensive and OCR-heavy tasks be closed in native VLMs without relying on pre-trained vision encoders? The authors explicitly state that NEO lags on knowledge-/OCR-heavy tasks like MMMU, InfoVQA, and TextVQA, attributing this to limitations in the current training corpus and language modality dominance. This remains unresolved as the paper demonstrates competitive performance on general benchmarks but identifies these specific domains as persistent weaknesses compared to modular models.

### Open Question 2
What are the fundamental constraints that differentiate native VLMs from modular ones regarding vision-language conflicts, and can these barriers be fully overcome? The abstract poses this specific research question, noting that while NEO introduces "native primitives" to mitigate conflicts, effectively managing "vision-language conflicts inside a dense and monolithic model" remains a central challenge requiring further exploration.

### Open Question 3
Can a high-performance native VLM be trained effectively de novo (from scratch) to mitigate language modality dominance, or is initialization from pre-trained LLMs strictly necessary? The appendix notes that due to resource constraints, the authors were unable to train a fully native model entirely from scratch without initialization from an existing LLM, which limits the ability to mitigate biases from language dominance.

## Limitations

- The frequency decoupling mechanism relies on assumptions about optimal frequency allocation that are not empirically justified, with specific β parameter choices appearing arbitrary
- The Pre-Buffer partitioning strategy assumes freezing the LLM is necessary but lacks direct comparison against alternative regularization methods
- Evaluation scope, while broad across 11 benchmarks, lacks depth in compositional generalization, robustness to distribution shifts, and inference efficiency comparisons

## Confidence

- **High confidence**: The architectural feasibility of NEO - the unified transformer design with modified attention and positional encoding is technically sound and implementable
- **Medium confidence**: The performance claims relative to modular baselines, as results could be sensitive to implementation details not fully specified
- **Low confidence**: The mechanistic claims about why specific design choices work, as the paper lacks ablation studies isolating component contributions

## Next Checks

1. **Frequency sensitivity ablation**: Systematically vary the β parameters in Native-RoPE across a range of values and measure performance degradation on spatial reasoning tasks to identify optimal frequency allocation.

2. **Masking implementation verification**: Implement a visualization tool that displays the attention mask matrix for image tokens during inference, confirming that bidirectional attention is correctly applied within image regions while maintaining causal constraints for text generation.

3. **Alternative regularization comparison**: Train a variant where the entire model is unfrozen from the start but with stronger weight decay or gradient clipping on LLM parameters, comparing convergence speed and final performance against the Pre-Buffer approach to isolate whether freezing is truly necessary versus other regularization methods.