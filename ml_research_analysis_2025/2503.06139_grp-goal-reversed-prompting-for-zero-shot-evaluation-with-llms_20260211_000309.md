---
ver: rpa2
title: 'GRP: Goal-Reversed Prompting for Zero-Shot Evaluation with LLMs'
arxiv_id: '2503.06139'
source_url: https://arxiv.org/abs/2503.06139
tags:
- answer
- evaluation
- assistant
- prompting
- worse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Goal-Reversed Prompting (GRP), a method that
  improves large language model (LLM) evaluation by shifting the task from selecting
  the better answer to choosing the worse one in pairwise comparisons. The core idea
  is to prompt LLMs to think in reverse, which leverages their reasoning capabilities
  more effectively.
---

# GRP: Goal-Reversed Prompting for Zero-Shot Evaluation with LLMs

## Quick Facts
- arXiv ID: 2503.06139
- Source URL: https://arxiv.org/abs/2503.06139
- Authors: Mingyang Song; Mao Zheng; Xuan Luo
- Reference count: 6
- Primary result: Goal-Reversed Prompting improves pairwise LLM evaluation accuracy from 61.71% to 66.23% on JudgeBench

## Executive Summary
This paper introduces Goal-Reversed Prompting (GRP), a method that improves large language model (LLM) evaluation by shifting the task from selecting the better answer to choosing the worse one in pairwise comparisons. The core idea is to prompt LLMs to think in reverse, which leverages their reasoning capabilities more effectively. Experiments on JudgeBench using GPT-4o, Gemini-1.5-Pro, and Claude-3.5-Sonnet show GRP significantly outperforms standard prompting, with GPT-4o improving from 61.71% to 66.23% overall accuracy. Claude-3.5-Sonnet shows the largest gain at 6% improvement. The method works even with simpler prompts, demonstrating its robustness. GRP offers a straightforward, effective approach to enhancing LLM evaluation reliability by encouraging alternative problem-solving perspectives.

## Method Summary
GRP modifies standard pairwise evaluation by reversing the goal: instead of selecting the better response, the LLM judge identifies the worse response. The method uses Standard Operating Procedure (SOP) prompt templates that require the model to first generate its own reference answer, then compare both responses against this baseline while evaluating multiple criteria (helpfulness, relevance, conciseness, creativity, missing information). Evaluation uses bidirectional consistency checking—each pair is evaluated in both A vs. B and B vs. A orderings, with only consistent correct judgments counted as accurate. This approach operates in zero-shot settings without demonstration examples.

## Key Results
- GRP improves GPT-4o accuracy from 61.71% to 66.23% on JudgeBench overall
- Claude-3.5-Sonnet shows largest improvement at 6% gain (59.28% to 65.24%)
- Math task evaluations show most significant improvements across models
- Simpler reversed-goal prompts achieve comparable results (~51-53%) to full SOP structure
- Bidirectional consistency requirement rejects ties and inconsistent judgments, increasing confidence in reported accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reversing the evaluation objective from "select better" to "select worse" activates different reasoning pathways in LLMs, improving discrimination accuracy.
- **Mechanism:** The inversion forces the model to approach the comparison task through elimination rather than selection. By framing the task as identifying deficiencies (what makes a response worse), the model may attend more carefully to errors, omissions, and quality gaps that are less salient when optimizing for the "best" option.
- **Core assumption:** LLMs have asymmetric reasoning capabilities for forward vs. reverse framing of the same underlying task, and error-detection circuits are more robust than quality-selection circuits.
- **Evidence anchors:** Abstract mentions reverse thinking's role in human reasoning; Section 1 discusses two-way verification approaches; no direct corpus support for LLM-specific mechanisms.
- **Break condition:** If models exhibit symmetric performance regardless of task framing, or if "worse" selection introduces its own systematic biases, the mechanism fails.

### Mechanism 2
- **Claim:** The SOP-based prompt structure—requiring self-generated reference answers before evaluation—anchors judgments against an internal standard rather than relative comparison alone.
- **Mechanism:** The prompt forces the model to first construct its own answer, creating a concrete reference point. Subsequent comparisons evaluate both candidates against this self-generated baseline, reducing the tendency to select based on surface-level features (length, fluency) rather than correctness.
- **Core assumption:** The model's self-generated answer is sufficiently accurate to serve as a reliable reference, and comparing against this anchor is more robust than direct pairwise comparison.
- **Evidence anchors:** Section 2 notes SOPs outperform CoT for evaluation tasks; Table 1 shows explicit self-generation step; no direct corpus evidence on self-generated reference efficacy.
- **Break condition:** If the model's self-generated answer contains errors that systematically bias subsequent evaluations, or if generation overhead introduces inconsistency, the mechanism degrades.

### Mechanism 3
- **Claim:** Explicit error-identification instructions in the reversed-goal prompt activate more thorough analysis than positive-selection framing.
- **Mechanism:** The prompt instructs the model to "identify and correct any mistakes or inaccurate information," then evaluate helpfulness, relevance, conciseness, creativity, and missing information. This decomposed checklist approach may reduce anchoring on single salient features and encourage comprehensive assessment.
- **Core assumption:** Decomposing evaluation into explicit criteria improves judgment reliability, and the "worse" framing makes the model more willing to identify flaws it might otherwise overlook or forgive.
- **Evidence anchors:** Table 1 shows sequential evaluation steps; Section 4.1 reports Claude-3.5-Sonnet's 6% improvement; weak direct evidence from corpus neighbors.
- **Break condition:** If the prompt's complexity introduces variance in which criteria models actually follow, or if criterion-overload leads to superficial rather than deep analysis, improvements may not generalize.

## Foundational Learning

- **Concept: Pairwise Evaluation with Positional Bias Mitigation**
  - **Why needed here:** LLM judges exhibit positional bias—the order of response presentation affects decisions. The paper uses bidirectional evaluation (swapping A/B order) and only counts consistent correct decisions, rejecting ties and inconsistencies.
  - **Quick check question:** If you evaluate (A vs. B) and get "A wins," then evaluate (B vs. A) and get "B wins," what does this indicate about the judge's reliability?

- **Concept: Standard Operating Procedures (SOPs) vs. Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The paper explicitly notes SOP-based prompts outperform CoT for evaluation tasks. SOPs provide structured, sequential instructions; CoT elicits open-ended reasoning. Understanding this distinction is critical for prompt design choices.
  - **Quick check question:** Why might "generate your own answer first, then compare" (SOP-style) be more reliable than "let's think step by step about which is better" (CoT-style) for evaluation?

- **Concept: Zero-Shot Evaluation Paradigm**
  - **Why needed here:** GRP operates in zero-shot settings—no demonstration examples of correct evaluations are provided. The method must work through prompt engineering alone, making prompt structure the sole lever for improvement.
  - **Quick check question:** What constraints does zero-shot evaluation impose compared to few-shot evaluation with labeled examples?

## Architecture Onboarding

- **Component map:** User prompt + Response A + Response B -> GRP Prompt Template -> LLM API call -> Output Parser -> Consistency Check -> Verdict
- **Critical path:** Construct GRP prompt with reversed goal -> Call LLM API with temperature=0 -> Parse verdict label -> Swap response positions, repeat -> Validate consistency; mark incorrect if inconsistent or tie
- **Design tradeoffs:** Strictness vs. Coverage (exclude ties/inconsistent judgments); Complexity vs. Robustness (full SOP vs. simpler prompts); API Cost (bidirectional evaluation doubles calls)
- **Failure signatures:** High rate of inconsistent A/B vs. B/A judgments indicates weak discrimination; systematic preference for longer responses suggests length bias; self-generated reference errors propagate hallucinations
- **First 3 experiments:** 1) Baseline replication: standard "select better" SOP prompt with bidirectional consistency check; 2) GRP comparison: measure accuracy delta on same JudgeBench subset; 3) Ablation on prompt complexity: test "direct reversed goal" vs. "full SOP reversed goal"

## Open Questions the Paper Calls Out
- How does Goal-Reversed Prompting perform when applied to open-source LLMs, and does the magnitude of improvement depend on model scale or architecture? (All experiments used closed-source models only)
- What is the underlying mechanism that makes reverse-thinking prompting more effective for evaluation tasks? (Paper demonstrates effectiveness but doesn't investigate causal analysis)
- Does GRP maintain its effectiveness across diverse evaluation benchmarks and task domains beyond JudgeBench? (All experiments use only JudgeBench)

## Limitations
- Dataset accessibility: JudgeBench dataset not publicly available through paper or citations, creating barrier to independent verification
- Ablation completeness: Paper doesn't systematically isolate contributions of goal-reversal vs. SOP structure vs. bidirectional evaluation
- Self-evaluation bias: Section 4.2 acknowledges models evaluating their own outputs show systematic bias, but main results don't explicitly test this scenario

## Confidence
- **High Confidence:** Bidirectional evaluation methodology and consistency requirement are well-specified and reproducible; finding that goal-reversal improves accuracy from ~61% to ~66% on JudgeBench is supported by experimental design
- **Medium Confidence:** Mechanism explaining why reverse framing improves reasoning (error-detection vs. quality-selection asymmetry) is plausible but lacks direct corpus support for LLM-specific reasoning pathways
- **Low Confidence:** Claim that simpler reversed-goal prompts work "nearly as well" is based on a single model (GPT-4o) and doesn't establish robustness across models or tasks

## Next Checks
1. Obtain JudgeBench dataset and ground truth labels to reproduce exact experimental conditions, particularly the strict consistency requirement
2. Run GRP on GPT-4o evaluating GPT-4o-generated pairs to quantify self-favoring bias magnitude
3. Systematically test forward-goal SOP vs. reversed-goal SOP vs. forward-goal direct vs. reversed-goal direct across all three models to isolate which mechanism drives improvements