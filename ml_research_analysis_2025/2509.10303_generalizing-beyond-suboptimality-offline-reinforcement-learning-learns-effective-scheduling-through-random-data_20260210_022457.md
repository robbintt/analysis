---
ver: rpa2
title: 'Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective
  Scheduling through Random Data'
arxiv_id: '2509.10303'
source_url: https://arxiv.org/abs/2509.10303
tags:
- cdqac
- instances
- training
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conservative Discrete Quantile Actor-Critic
  (CDQAC), an offline reinforcement learning algorithm for job-shop scheduling problems
  that learns effective policies directly from historical data without requiring online
  interaction. CDQAC combines a quantile-based critic with delayed policy updates
  and conservative Q-learning to accurately estimate action values while avoiding
  overestimation of out-of-distribution actions.
---

# Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data

## Quick Facts
- arXiv ID: 2509.10303
- Source URL: https://arxiv.org/abs/2509.10303
- Reference count: 40
- Key outcome: CDQAC achieves state-of-the-art performance on JSP/FJSP benchmarks, with best results from random data rather than expert data

## Executive Summary
This paper introduces CDQAC, an offline reinforcement learning algorithm that learns effective job-shop scheduling policies directly from historical data without online interaction. The method combines quantile-based critics with delayed policy updates and conservative Q-learning to accurately estimate action values while avoiding overestimation of out-of-distribution actions. Remarkably, CDQAC performs best when trained on random, suboptimal data rather than higher-quality data from genetic algorithms or priority rules, achieving average gaps of 5.86% compared to 6.43% and 12.34% respectively. The approach is highly sample-efficient, requiring only 10-20 training instances to match performance achieved with full datasets, and demonstrates strong generalization from small to larger problem instances.

## Method Summary
CDQAC (Conservative Discrete Quantile Actor-Critic) is an offline RL algorithm for JSP/FJSP that learns scheduling policies from static datasets. The method uses a quantile-based critic with dueling architecture to estimate return distributions, combined with Conservative Q-Learning (CQL) to prevent overestimation of out-of-distribution actions. Policy updates are delayed to ensure critic stability. The algorithm is trained on datasets generated by random heuristics, genetic algorithms, or priority dispatching rules, and achieves state-of-the-art performance through superior generalization from diverse, suboptimal data.

## Key Results
- CDQAC achieves 5.86% average optimality gap on JSP/FJSP benchmarks, outperforming online RL baselines and competing offline methods
- Random data generation yields superior generalization policies compared to higher-quality data from genetic algorithms (6.43% gap) or priority rules (12.34% gap)
- The method requires only 10-20 training instances to match performance achieved with full datasets, demonstrating exceptional sample efficiency
- CDQAC generalizes effectively from small (10×5) to larger (20×10) problem instances

## Why This Works (Mechanism)

### Mechanism 1: Random Data Coverage Advantage
Training on random, suboptimal data yields superior generalization policies compared to expert or near-optimal data for offline scheduling RL. Random data generation explores a broader state-action space, resulting in high state-action coverage (SACo). This coverage provides the conservative critic with "negative examples" (suboptimal trajectories) and a wider distribution of returns. The algorithm then "stitches" high-value segments from these diverse suboptimal trajectories, whereas expert data provides a narrow distribution that limits the agent's ability to correct errors or estimate values for unseen states.

### Mechanism 2: Dueling Quantile Critic Architecture
A quantile-based critic with a dueling architecture provides more accurate value estimation for discrete scheduling actions than standard DQN-based critics. The Quantile Critic (QRDQN) approximates the full distribution of returns $Z(s,a)$ rather than just the expectation, providing richer gradients. The Dueling Architecture separates the network into a Value stream $V(s)$ (updated constantly via global embedding) and an Advantage stream $A(s,a)$ (updated per machine-operation pair). This decoupling allows the Value stream to converge faster and generalize state values independent of specific action scarcity.

### Mechanism 3: Conservative Learning with Delayed Updates
Delayed policy updates combined with Conservative Q-Learning (CQL) prevent the overestimation of out-of-distribution (OOD) actions inherent in offline datasets. CQL penalizes high Q-values for actions not present in the dataset by regularizing the objective (log-sum-exp vs. expectation). Delayed Updates ($\eta=4$) ensure the Critic network converges to a stable estimate of $Z(s,a)$ before the Actor (Policy) updates its strategy. This prevents the Actor from exploiting noise or overestimated OOD values in an unstable Critic.

## Foundational Learning

- **Concept: Conservative Q-Learning (CQL)**
  - Why needed here: Standard Q-learning fails in offline RL because it cannot explore actions not in the static dataset, leading to "distributional shift" and value overestimation.
  - Quick check question: Does the loss function penalize Q-values for actions drawn from the policy if they are not present in the dataset buffer?

- **Concept: Quantile Regression DQN (QRDQN)**
  - Why needed here: Scheduling rewards can be stochastic or sparse. Estimating the full distribution of returns (quantiles) captures the variance (risk) better than a single scalar expectation.
  - Quick check question: Is the critic outputting a set of probabilities/quantiles (N=64) rather than a single scalar Q-value?

- **Concept: Dueling Network Architectures**
  - Why needed here: In scheduling, the "state" (factory configuration) often has a baseline value regardless of the immediate action. Separating $V(s)$ allows the network to learn this baseline efficiently.
  - Quick check question: Are the gradients for the Value stream aggregated across all actions, while the Advantage stream is action-specific?

## Architecture Onboarding

- **Component map:** Input Features -> DAN Encoder -> Global Embedding + Pair Features -> Dueling Quantile Critic Heads (Vθ, Aθ) -> CQL Loss + TD Error -> Delayed Policy Updates -> Policy Network

- **Critical path:** Data Loading (Random/PDR) -> DAN Embedding -> **Critic Loss Calculation** (TD Error + CQL Regularization) -> (If `step % eta == 0`) -> **Actor Loss Calculation** (Maximize Q, Entropy)

- **Design tradeoffs:** The paper explicitly chooses data diversity (Random/PDR) over quality (GA/Expert). Do not optimize training data for "low makespan"; optimize for coverage. The Value stream in the Dueling critic takes *only* the global embedding, while the Advantage stream takes specific embeddings. Ensure they are not sharing early layers to maximize the benefit of decoupling.

- **Failure signatures:**
  - Training Collapse: Sudden spike in Q-values to infinity. *Fix:* Check CQL penalty coefficient ($\alpha$); if too low, OOD overestimation occurs.
  - Stagnation: Policy mimics dataset exactly without improvement. *Fix:* Check dataset diversity. If dataset is too narrow (e.g., only one rule), the critic sees no variance to learn from.
  - Instability: High variance in performance across seeds. *Fix:* Increase policy update delay $\eta$.

- **First 3 experiments:**
  1. **Sanity Check (Random vs. Expert):** Train CDQAC on the "Random" dataset and the "GA" dataset (Table 1 setup). Verify that Random converges to a lower average gap than GA to confirm the coverage hypothesis.
  2. **Ablation (Delayed Updates):** Run a sweep on the policy update frequency $\eta \in \{1, 2, 4, 8\}$ (Appendix G.1 style) to confirm stability relies on delayed updates.
  3. **Component Isolation (Quantile vs. DQN):** Toggle the critic type between standard DQN and Quantile Critic to verify the performance delta on the "Hurink" benchmark set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CDQAC be extended effectively to other combinatorial optimization problems beyond JSP and FJSP, such as vehicle routing or bin packing?
- Basis in paper: [explicit] "As future work, we plan to extend our approach to other combinatorial optimization problems."
- Why unresolved: The paper only evaluates CDQAC on JSP and FJSP; the dueling quantile network and delayed policy update architecture were designed specifically for scheduling's machine–operation pair structure.
- What evidence would resolve it: Results applying CDQAC to benchmark instances of VRP, TSP, or bin packing, comparing against offline RL and problem-specific baselines.

### Open Question 2
- Question: How can CDQAC be modified to maintain efficiency when action spaces scale to hundreds or thousands of discrete actions?
- Basis in paper: [explicit] "We observed that CDQAC becomes less efficient when trained on larger instance sizes, likely due to the increased action space, where Q-learning-based methods tend to struggle. Addressing this scalability challenge is another promising direction for further research."
- Why unresolved: The paper shows degraded performance on 20×10 instances (up to 200 actions), with larger standard deviations and higher gaps, but does not propose solutions.
- What evidence would resolve it: Architectural variants (e.g., hierarchical action selection, action embedding, factored Q-functions) demonstrating stable training on instances with 200+ actions.

### Open Question 3
- Question: Why does CDQAC achieve better performance when trained on random data than on higher-quality data from GAs and PDRs, contrary to prior offline RL findings?
- Basis in paper: [explicit] "Surprisingly, we find that CDQAC performs better when trained on data generated by a random heuristic than when trained on higher-quality data from genetic algorithms and priority dispatching rules. This contradicts prior findings in offline RL research, which generally show that higher quality training data leads to better performance."
- Why unresolved: The authors hypothesize that diversity in state–action coverage matters more than solution quality, but this remains unproven; App. H shows correlation with SACo but not causation.
- What evidence would resolve it: Controlled experiments varying diversity and quality independently; ablation studies isolating coverage vs. counterfactual examples vs. distributional properties.

## Limitations
- The paper relies heavily on specific architectural choices without ablation studies for the DAN encoder design and quantile decomposition
- Dataset generation process is underspecified, particularly for FJSP instances where machine-operation constraints could significantly impact state-action coverage
- The claimed generalization benefits from random data assume sufficient coverage is achieved, but no theoretical bounds are provided for when this assumption breaks down

## Confidence
- **High Confidence:** The mechanism of CQL preventing OOD overestimation and the dueling architecture stabilizing value estimation
- **Medium Confidence:** The superiority of random data over expert data for generalization
- **Low Confidence:** The scalability claims to larger problem instances

## Next Checks
1. **Coverage Analysis:** Quantify state-action coverage differences between random and expert datasets using the proposed SACo metric, and measure correlation with performance degradation when coverage falls below critical thresholds
2. **Architecture Ablation:** Systematically disable components (CQL, dueling, quantile) on the same dataset to isolate contribution of each mechanism to the 5.86% gap reduction
3. **Generalization Test:** Train on small instances (10×5) and evaluate transfer to significantly larger instances (50×20) to validate whether the coverage advantage scales with problem complexity