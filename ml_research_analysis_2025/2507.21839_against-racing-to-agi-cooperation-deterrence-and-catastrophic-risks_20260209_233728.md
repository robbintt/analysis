---
ver: rpa2
title: 'Against racing to AGI: Cooperation, deterrence, and catastrophic risks'
arxiv_id: '2507.21839'
source_url: https://arxiv.org/abs/2507.21839
tags:
- racing
- https
- race
- risks
- development
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues against "AGI Racing" - the view that powerful
  nations should accelerate AI development to achieve artificial general intelligence
  (AGI) before competitors. The authors contend that racing to AGI would substantially
  increase catastrophic risks, including nuclear instability, while offering uncertain
  benefits.
---

# Against racing to AGI: Cooperation, deterrence, and catastrophic risks

## Quick Facts
- arXiv ID: 2507.21839
- Source URL: https://arxiv.org/abs/2507.21839
- Reference count: 0
- The paper argues against "AGI Racing" - the view that powerful nations should accelerate AI development to achieve artificial general intelligence (AGI) before competitors.

## Executive Summary
This paper challenges the strategic logic of racing to artificial general intelligence (AGI), arguing that such competition would substantially increase catastrophic risks while offering uncertain benefits. The authors contend that racing would undermine technical safety research, slow social adaptation processes, and potentially trigger nuclear instability. Instead of racing, they advocate for international cooperation and coordination as preferable alternatives that can limit catastrophic risks while preventing adversaries from obtaining a decisive strategic advantage.

## Method Summary
The paper employs game-theoretic analysis through decision matrices comparing racing versus cooperation scenarios. It constructs four payoff matrices (Tables 1-4) evolving from a classical prisoner's dilemma to a trust dilemma structure. The analysis qualitatively adjusts payoffs based on three key arguments: (1) racing substantially increases catastrophic risks, (2) achieving decisive strategic advantage is unlikely due to countermeasures, and (3) cooperation offers significant benefits including reduced risks and enhanced safety research.

## Key Results
- Racing to AGI would substantially increase catastrophic risks from multiple independent pathways including nuclear instability and undermine technical AI safety research effectiveness.
- The likelihood of achieving a decisive strategic advantage from being first to AGI is questionable due to cybersecurity vulnerabilities and potential military countermeasures.
- International cooperation and coordination are preferable alternatives to racing, offering reduced catastrophic risks while preventing adversaries from obtaining decisive advantages.

## Why This Works (Mechanism)

### Mechanism 1
Racing to AGI increases catastrophic risk exposure through multiple independent risk pathways, not just misalignment. Racing deprioritizes risk-mitigation relative to speed, compressing time for both technical safety work and social adaptation. This raises exposure across five hypothesized independent catastrophic risks: (1) misaligned AGI takeover, (2) catastrophic misuse, (3) preventive war, (4) accumulative systemic collapse, and (5) gradual disempowerment. Crucially, technical alignment alone does not address risks 2–5. The core assumption is that these risks are at least partially independent; mitigating one (e.g., misalignment) does not automatically mitigate others (e.g., misuse, preventive war).

### Mechanism 2
Technical AI safety research effectiveness depends on "capability restraint"—the ability to pause or steer development—which racing erodes. Safety research (risk evaluation, oversight, control methods) provides value primarily when unsafe development can be halted or redirected. Under racing, competitive pressure makes capability restraint less feasible. Additionally, faster capability advancement shortens the time each capability level is available for safety research, reducing cumulative safety progress. The core assumption is that safety research requires time-at-capability-level and the institutional ability to act on findings; neither is guaranteed under racing.

### Mechanism 3
Racing increases the probability that opponents will race, making racing a self-fulfilling prophecy. By racing, an actor changes opponents' perceived payoffs—convincing them that mutual restraint is infeasible and that racing is necessary for survival. This shifts the game from a potential stag hunt (cooperation preferred) toward a prisoner's dilemma (defection dominant). The paper argues current conditions do not reflect an active race (open models, talent flows), so racing now risks triggering one. The core assumption is that opponents' racing behavior is responsive to one's own actions; racing is not predetermined.

## Foundational Learning

- **Prisoner's Dilemma vs. Stag Hunt (Trust Dilemma)**: Why needed here: The paper models AGI development as a strategic game; understanding why mutual cooperation can be preferable and stable (stag hunt) vs. unstable (prisoner's dilemma) is essential. Quick check question: If both players prefer mutual cooperation to mutual defection, but defection is dominant, what game is this?

- **Decisive Strategic Advantage (DSA)**: Why needed here: The expected benefit of racing hinges on whether AGI provides a DSA sufficient for domination; the paper critiques this assumption. Quick check question: What three factors does the paper identify that could prevent a first-mover from obtaining a durable DSA?

- **Technical vs. Social Risk Mitigation**: Why needed here: The paper distinguishes risks addressable by technical alignment from those requiring social coordination; conflating them undermines the argument. Quick check question: Which catastrophic risks in the paper's taxonomy would persist even if the alignment problem were fully solved?

## Architecture Onboarding

- **Component map**: Decision matrix framework -> Risk evaluation layer -> Verification/enforcement infrastructure -> Coordination mechanisms
- **Critical path**: 1) Map current state to game matrix (assess whether racing is already underway). 2) Estimate catastrophic risk probabilities under race vs. non-race conditions. 3) Assess DSA feasibility given cybersecurity, sabotage, and catch-up dynamics. 4) Design verification/enforcement to stabilize cooperation. 5) Iterate toward stag hunt equilibrium.
- **Design tradeoffs**: Deterrence (MAIM) reduces race risk but introduces escalation risk and may incentivize power-seeking AI designs. Cooperation reduces escalation risk and enables joint safety work but requires verification and enforcement infrastructure. Racing maximizes speed but minimizes adaptation and safety research effectiveness.
- **Failure signatures**: Unilateral racing in response to perceived opponent racing without verifying opponent intent. Safety research that produces evaluations without institutional authority to halt deployment. Deterrence postures that rely solely on kinetic threats when cyber/sabotage options are infeasible. Cooperation agreements lacking verification, enabling covert defection.
- **First 3 experiments**: 1) Game calibration exercise: Given current indicators (open weights, talent flows, government investment levels), assign probabilities to "opponent is racing" and estimate how your racing decision shifts those probabilities. 2) Capability restraint audit: For your organization's safety research portfolio, identify which outputs require deployment authority to be actionable; flag gaps. 3) Verification mechanism scan: Survey existing hardware-enabled and cryptographic verification methods; assess which could detect covert training runs above a compute threshold without revealing proprietary information.

## Open Questions the Paper Calls Out

### Open Question 1
What is the offense-defense balance of AI in cybersecurity regarding the protection of model weights? The authors state that securing model weights depends on the "yet unknown" offense-defense balance of AI in cybersecurity. It is currently unclear if AI-assisted defensive measures can outpace state-sponsored offensive capabilities to prevent model theft. Evidence needed: empirical comparisons of resource expenditure required for advanced cyberattacks versus AI-automated defense patching.

### Open Question 2
Is it likely that any actor can establish a substantive lead in AI capabilities sufficient for a decisive strategic advantage (DSA)? The paper identifies "whether it is likely that any actor may establish such a substantive lead" as a central, open question challenging the Racing view. Diminishing returns to scaling, model theft, and kinetic sabotage may prevent a single actor from maintaining a lead wide enough to dominate competitors. Evidence needed: historical analysis of technology diffusion rates and technical studies on the feasibility of catching up via model distillation.

### Open Question 3
Would a deterrence regime (e.g., MAIM) inadvertently increase the risk of developing misaligned AI systems? The authors note that incentives under deterrence may make the creation of strongly misaligned AIs more likely by prioritizing power-seeking over safety. It is uncertain if training AI systems to prevail over powerful adversaries fundamentally compromises their alignment with human values. Evidence needed: theoretical modeling of training incentives under mutual deterrence scenarios and their correlation with power-seeking behavior.

## Limitations

- The empirical foundation for the five hypothesized independent catastrophic risks remains thin - their actual independence and probabilities are not quantified.
- The payoff matrix construction relies on qualitative judgments about how racing affects each risk category rather than empirical measurements.
- The assumption that technical AI safety research fundamentally requires capability restraint has limited empirical validation, particularly given emerging research on provably safe architectures.

## Confidence

- **High confidence**: The game-theoretic framework and payoff matrix construction are methodologically sound and internally consistent.
- **Medium confidence**: The argument that racing to AGI increases catastrophic risk exposure is well-reasoned but relies on contested assumptions about risk independence and probability.
- **Medium confidence**: The critique of DSA feasibility has strong theoretical grounding but limited empirical validation regarding specific countermeasures.
- **Low confidence**: The self-fulfilling prophecy mechanism for racing behavior, while plausible, lacks robust empirical evidence about how nations actually respond to perceived racing.

## Next Checks

1. **Risk Independence Validation**: Survey AI governance experts to assess the empirical independence of the five catastrophic risk categories using structured elicitation methods.

2. **DSA Countermeasure Analysis**: Conduct a technical analysis of specific military and cyber countermeasures to AGI capabilities, including game-theoretic modeling of response capabilities.

3. **Safety Research Dependency Audit**: Map current AI safety research outputs to their dependence on capability restraint, identifying which research directions could proceed under racing conditions.