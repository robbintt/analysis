---
ver: rpa2
title: Fully tensorial approach to hypercomplex neural networks
arxiv_id: '2407.00449'
source_url: https://arxiv.org/abs/2407.00449
tags:
- algebra
- neural
- tensor
- hypercomplex-valued
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A tensor-based formulation for hypercomplex-valued neural networks
  is introduced, where algebra multiplication is represented as a rank-three tensor.
  This enables universal implementation of hypercomplex dense and convolutional layers
  using standard tensor contractions, permutations, and reshaping operations, generalizing
  existing quaternion and 4-dimensional algebra constructions.
---

# Fully tensorial approach to hypercomplex neural networks

## Quick Facts
- arXiv ID: 2407.00449
- Source URL: https://arxiv.org/abs/2407.00449
- Authors: Agnieszka Niemczynowicz; Radosław Antoni Kycia
- Reference count: 35
- Key outcome: Tensor-based hypercomplex neural networks using rank-3 algebra multiplication tensors, providing dimension-independent implementations compatible with deep learning libraries.

## Executive Summary
This paper introduces a fully tensorial framework for implementing hypercomplex-valued neural networks that generalizes quaternion and other hypercomplex architectures to arbitrary finite-dimensional algebras. The key insight is representing algebra multiplication as a rank-three tensor, enabling universal implementation of dense and convolutional layers using standard tensor contractions, permutations, and reshaping operations. This approach provides dimension-independent, algebra-agnostic neural network architectures compatible with optimized deep learning libraries like TensorFlow and PyTorch.

## Method Summary
The method encodes algebra multiplication as a rank-3 tensor A^k_{ij} from structure constants, then implements hypercomplex layers through tensor contractions with weight tensors. For dense layers, the kernel contracts with the algebra tensor, undergoes permutation and reshaping, then applies standard matrix multiplication. Convolutional layers contract the kernel with the algebra tensor, permute and reshape, then apply independent real convolutions per algebra component before concatenating results. The framework supports arbitrary finite-dimensional algebras and provides a tensor version of the universal approximation theorem for single-layer perceptrons under non-degeneracy assumptions.

## Key Results
- Universal approximation theorem established for hypercomplex single-layer perceptrons with component-wise sigmoid activation under non-degeneracy conditions.
- Implementation compatible with TensorFlow and PyTorch through the Hypercomplex Keras library.
- Framework naturally encodes multi-component data (e.g., RGB images) into algebra elements without dimension constraints.
- Computational complexity scales as O(dim(V)²) for dense layers and O(dim(V)) for convolutional layers.

## Why This Works (Mechanism)

### Mechanism 1
Algebra multiplication in any finite-dimensional algebra can be represented as a rank-3 tensor, enabling dimension-independent neural network operations. For a basis {e_i} of algebra V, multiplication x·y = A^k_{ij} x^i y^j e_k where A^k_{ij} are structure constants stored as a (n×n×n) tensor. All algebraic operations in layers become tensor contractions with this multiplication tensor.

### Mechanism 2
Hypercomplex layers decompose into per-component real-valued operations after contracting the weight tensor with the algebra multiplication tensor. Contract kernel K with algebra tensor A via C_{1,0}(A,K), then permute and reshape to obtain a standard weight matrix. For convolution, apply real convolutions independently per algebra component then concatenate results.

### Mechanism 3
Universal approximation for hypercomplex single-layer perceptrons holds under non-degeneracy of the algebra. Component-wise projection π_k maps V^N-valued functions to real-valued functions. Classical universal approximation applies to each component. Riesz representation for non-degenerate algebras ensures each real functional corresponds to a unique V^N vector, enabling reconstruction of the hypercomplex output.

## Foundational Learning

- **Structure constants of an algebra**: Why needed here: The rank-3 tensor A^k_{ij} encodes how basis elements multiply (e_i · e_j = A^k_{ij} e_k). Without understanding this, the tensor contraction pattern in Algorithms 1-2 is opaque. Quick check: For the algebra R^2 with multiplication (a,b)·(c,d) = (ac, ad+bc), what are A^0_{01} and A^1_{11}?

- **Tensor contraction and Einstein summation**: Why needed here: Layer operations use implicit summation over repeated indices (e.g., A^{ia}_{j k_a} K^{j}_{in u}). Understanding which axes contract versus which remain is essential for debugging shape mismatches. Quick check: In C_{1,0}(A,K) where A has shape (al, al, al) and K has shape (al, in, u), which index is summed and what is the output shape?

- **Non-degenerate algebra**: Why needed here: The universal approximation guarantee requires non-degeneracy. Knowing how to check this from the multiplication tensor determines whether the theoretical foundation applies to a chosen algebra. Quick check: If A = [[1,0],[0,0]] represents multiplication in a 2D algebra, is it non-degenerate? Why or why not?

## Architecture Onboarding

- **Component map**: Algebra tensor A (al×al×al) -> Dense/Conv layer operations (contraction, permutation, reshape) -> Standard matrix/multiple convolutions -> Component-wise activation

- **Critical path**: Define algebra by providing structure constants tensor A -> Verify non-degeneracy -> Instantiate HyperDense/HyperConv with al = dim(V) -> Forward pass executes contraction → permutation → reshape → standard operation sequence

- **Design tradeoffs**: Algebra dimension vs. compute (complexity scales with dim(V)² for dense, dim(V)× for conv); Fixed vs. learned algebra (this framework uses fixed A as hyperparameter vs. PHNNs that learn algebra structure); Non-degeneracy vs. flexibility (commutative algebras simplify symmetry but may restrict representation)

- **Failure signatures**: Shape mismatch after contraction (check contraction index matches between A and K); NaN outputs (algebra with zero divisors can produce unstable gradients); Poor approximation despite adequate width (may indicate degenerate algebra)

- **First 3 experiments**: Reproduce quaternion dense layer (set A to quaternion multiplication tensor, compare HyperDense output against reference implementation); Ablate algebra dimension (train HyperConv2D on RGB+alpha images using dim(V)∈{2,4,8} with isomorphic parameter counts); Test non-degeneracy boundary (construct degenerate 3D algebra, train single-layer perceptron, compare convergence to non-degenerate baseline)

## Open Questions the Paper Calls Out

- Does the universal approximation theorem extend to hypercomplex-valued perceptrons built on degenerate algebras? The proof relies on non-degeneracy assumption to establish Riesz representation lemma; degenerate algebras with nontrivial null directions break this construction.

- Can the algebra multiplication tensor A be made learnable within the proposed tensorial framework? Making A learnable introduces constraints (associativity, existence of identity) that must be preserved during optimization; paper does not address whether these can be enforced via parameterizations or regularization.

- What are the empirical performance trade-offs between tensor-contraction-based hypercomplex layers and specialized implementations for specific algebras? Framework's generality may introduce overhead from tensor permutations and reshaping operations that specialized quaternion libraries avoid through hardcoded multiplication rules.

## Limitations
- Computational overhead scales quadratically with algebra dimension for dense layers, potentially limiting practical use for high-dimensional algebras.
- Theoretical guarantees require non-degeneracy assumption, excluding algebras with zero divisors from universal approximation coverage.
- No empirical benchmarks comparing tensor-based implementation against specialized quaternion/clifford neural network libraries.

## Confidence
- **High confidence**: The tensor-based layer implementation correctly generalizes quaternion operations to arbitrary finite-dimensional algebras.
- **Medium confidence**: The universal approximation theorem holds for non-degenerate algebras with component-wise sigmoid activation.
- **Low confidence**: Practical performance gains from hypercomplex representations over real-valued networks for typical vision tasks.

## Next Checks
1. Implement a degenerate algebra (e.g., with explicit zero divisors) and empirically verify that single-layer approximation performance degrades compared to non-degenerate counterparts.
2. Benchmark HyperConv2D on CIFAR-10 using algebras of varying dimensions (2, 4, 8) with isomorphic parameter counts to isolate the effect of algebra structure on generalization.
3. Scale experiments to higher-dimensional algebras (16-32D) and measure computational overhead versus representational benefits, particularly focusing on memory bandwidth bottlenecks in tensor contractions.