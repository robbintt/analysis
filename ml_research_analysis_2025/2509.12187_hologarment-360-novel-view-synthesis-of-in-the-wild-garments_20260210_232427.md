---
ver: rpa2
title: "HoloGarment: 360\xB0 Novel View Synthesis of In-the-Wild Garments"
arxiv_id: '2509.12187'
source_url: https://arxiv.org/abs/2509.12187
tags:
- garment
- video
- novel
- input
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HoloGarment addresses novel view synthesis of real-world garments,
  which is challenging due to occlusions, pose variations, and cloth deformations.
  Prior methods relying on synthetic 3D data struggle to generalize to real-world
  clothing.
---

# HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments

## Quick Facts
- **arXiv ID:** 2509.12187
- **Source URL:** https://arxiv.org/abs/2509.12187
- **Reference count:** 23
- **Primary result:** Achieves state-of-the-art 360° novel view synthesis of real-world garments from 1-3 images or videos, handling occlusions, pose variations, and cloth deformations.

## Executive Summary
HoloGarment introduces a novel implicit training paradigm for generating 360° novel views of real-world garments without requiring paired real-3D data. The method bridges the domain gap between real-world video data and synthetic 3D assets through a shared garment embedding space. By training on three proxy tasks—real video animation, synthetic 3D spinning, and image reconstruction—the model learns to disentangle garment identity from motion artifacts. During inference, a garment "atlas" is finetuned on specific video data to generate canonical pose views independent of input occlusions or deformations, achieving state-of-the-art results on real-world garment NVS.

## Method Summary
HoloGarment employs a Video Diffusion Transformer (DiT) architecture with a shared garment encoder that maps both real and synthetic garment images to a common embedding space. The model is trained on three proxy tasks: real video animation, synthetic 3D spinning, and image reconstruction, using disjoint temporal blocks for different motion styles. During inference, the method constructs a garment "atlas" by finetuning a garment embedding on a specific real-world video while keeping model weights frozen. This atlas enables generation of 360° views in a canonical A-pose, independent of the input video's motion or occlusions. The approach requires synthetic 3D assets for training but enables NVS from real-world data without paired real-3D supervision.

## Key Results
- Achieves state-of-the-art performance on real-world garment NVS from both images and videos
- Successfully handles challenging scenarios including occlusions, pose variations, and cloth deformations
- Maintains photorealism and view consistency across 360° rotations
- Robustly generates canonical pose views independent of input motion or occlusion patterns

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Mapping via Shared Embeddings
- **Claim:** Training on real video animation and synthetic 3D spinning with a shared garment encoder enables the model to learn a domain-agnostic feature space where garment identity features are transportable across domains.
- **Mechanism:** The shared encoder maps real occlusions and synthetic clean spins to the same feature space, allowing texture transfer from real to synthetic geometry.
- **Core assumption:** Visual features defining garment identity (texture/pattern) are transportable across domains even when geometric priors conflict.
- **Evidence:** The abstract states the method "bridge[s] the domain gap... with a novel implicit training paradigm... to optimize a shared garment embedding space."
- **Break condition:** Fails if synthetic 3D dataset is too small or lacks diversity, causing overfitting to synthetic textures.

### Mechanism 2: Motion Disentanglement via Split Temporal Blocks
- **Claim:** Separate temporal blocks for dynamic video and static spin motion prevent motion leakage, ensuring static 360° outputs aren't contaminated by dynamic artifacts.
- **Mechanism:** Two disjoint sets of temporal blocks (3D convolutions + attention) are optimized separately for dynamic fluidity vs. geometric consistency.
- **Core assumption:** Spatial features capture garment identity sufficiently, allowing temporal layers to function purely as motion controllers.
- **Evidence:** Section 4.1 states "disjoint sets of temporal blocks are separately optimized for the different motion styles."
- **Break condition:** Fails if switching mechanism is misconfigured during inference, resulting in incorrect orbital geometry.

### Mechanism 3: Atlas Finetuning as Latent Consolidation
- **Claim:** Optimizing a single latent vector (atlas) on dynamic video while freezing model weights consolidates multi-view information without overfitting to specific poses.
- **Mechanism:** The atlas acts as a memory bank averaging details from all frames, while frozen decoder weights map it to canonical pose.
- **Core assumption:** The frozen model has a strong prior to map consolidated latents to canonical A-pose regardless of input poses.
- **Evidence:** Section 4.4 describes finetuning "f_{atlas} on a specific garment video... [preventing] undesired overfitting to the motion of the input video."
- **Break condition:** Fails if input video lacks sufficient coverage, leading to hallucination of unseen garment parts.

## Foundational Learning

- **Concept: Video Diffusion Transformers (DiT)**
  - **Why needed:** Backbone replaces UNets with Transformers for better scalability; understanding DiT blocks is critical for grasping split temporal block insertion.
  - **Quick check:** How does a DiT block differ from a standard 3D UNet block in handling temporal consistency?

- **Concept: Cross-Attention Conditioning**
  - **Why needed:** The model injects garment features and pose features into noisy video latents via cross-attention; understanding this is critical for seeing how clothing is "warped" onto poses.
  - **Quick check:** In the HoloGarment DiT block, are pose features concatenated or cross-attended with the noisy video features? (Answer: Pose is concatenated; Garment is cross-attended).

- **Concept: Implicit vs. Supervised Learning**
  - **Why needed:** The core innovation is "implicit training"; distinguishing between direct labels (paired 3D) vs. proxy tasks (animation + synthetic spin) is essential.
  - **Quick check:** Why does training on Task 1 (Animation) and Task 2 (Synthetic Spin) enable the implicit target task (Real Image → 360°)?

## Architecture Onboarding

- **Component map:** UNet encoders for garment image, 2D pose, and noisy video → DiT processor with split temporal blocks → UNet decoder predicting noise
- **Critical path:** 1. Preprocess inputs (segmented garment + 2D/3D poses) 2. Encode inputs into feature space 3. Switch temporal blocks based on inference mode 4. DiT processing with cross-attention and concatenation 5. Decode to predict noise
- **Design tradeoffs:** Synthetic data ratio affects texture fidelity vs. geometry consistency; atlas finetuning is faster than full finetuning but relies on pretrained model's disentanglement ability
- **Failure signatures:** "Holes" in output indicate missing synthetic geometry supervision; motion replication suggests incorrect temporal block switching; oversmoothing indicates synthetic data dominance
- **First 3 experiments:** 1. Ablate temporal split to verify motion artifact prevention 2. Compare atlas finetuning vs. full model weights for texture fidelity vs. overfitting 3. Vary real-vs-synthetic data ratio to find optimal FID/FVD balance

## Open Questions the Paper Calls Out
- How can the method be extended to handle unusual garment shapes and asymmetries that are poorly represented in the limited synthetic 3D dataset?
- What techniques can accelerate the garment atlas finetuning process from ~30 minutes to near real-time speeds without compromising quality?
- How can the model better resolve severe geometric ambiguities in single-view inputs to prevent hallucinating incorrect garment structures?

## Limitations
- Limited diversity in synthetic 3D dataset causes struggles with unusual garment shapes and category bias
- Method requires synthetic 3D assets for training, creating dependency on external dataset availability
- Current atlas finetuning process is computationally expensive (~30 minutes per video)

## Confidence
- **Domain Transfer Reliability:** High confidence for common garments, Medium confidence for rare/structured clothing
- **Atlas Embedding Generalization:** Medium confidence; weakly validated with noted failure modes
- **"Synthetic Data Dependency":** Low confidence in absolute terms; training still relies on synthetic 3D renders

## Next Checks
1. **Temporal Block Ablation:** Train single temporal blocks on both real and synthetic data to verify split-block architecture prevents motion artifacts
2. **Atlas Capacity Study:** Vary finetuning steps (100 vs. 1000) to measure texture fidelity vs. geometric overfitting compared to full model finetuning
3. **Synthetic Data Scaling:** Train with varying real-vs-synthetic ratios (10/90, 50/50, 90/10) to identify optimal balance point and test synthetic data necessity