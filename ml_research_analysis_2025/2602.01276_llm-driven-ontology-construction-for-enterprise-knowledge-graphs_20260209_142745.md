---
ver: rpa2
title: LLM-Driven Ontology Construction for Enterprise Knowledge Graphs
arxiv_id: '2602.01276'
source_url: https://arxiv.org/abs/2602.01276
tags:
- ontology
- construction
- data
- knowledge
- enterprise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OntoEKG introduces an LLM-driven pipeline for automated ontology
  construction from unstructured enterprise text, addressing the resource-intensive
  nature of manual semantic modeling. The approach decomposes the task into extraction
  and entailment modules, producing RDF ontologies from enterprise documents across
  Data, Finance, and Logistics domains.
---

# LLM-Driven Ontology Construction for Enterprise Knowledge Graphs

## Quick Facts
- arXiv ID: 2602.01276
- Source URL: https://arxiv.org/abs/2602.01276
- Reference count: 15
- One-line primary result: OntoEKG pipeline achieves F1-score of 0.724 in Data domain using fuzzy matching, but shows significant domain variation (0.121 in Finance)

## Executive Summary
OntoEKG introduces an LLM-driven pipeline for automated ontology construction from unstructured enterprise text, addressing the resource-intensive nature of manual semantic modeling. The approach decomposes the task into extraction and entailment modules, producing RDF ontologies from enterprise documents across Data, Finance, and Logistics domains. Experiments show promising results with a fuzzy-match F1-score of 0.724 in the Data domain, though performance varies across sectors (0.121 in Finance). The work also highlights the lack of comprehensive benchmarks for end-to-end ontology construction evaluation, calling for community development of standardized evaluation frameworks. Key limitations include scope definition challenges, abstraction level inconsistencies, and hierarchical reasoning errors. Future work will focus on progressive ontology construction, entity metadata extraction, and benchmark development.

## Method Summary
OntoEKG employs a two-step LLM pipeline for ontology construction: first, an extraction module using Google Gemini 3 Flash identifies classes and properties from unstructured text with Pydantic-enforced JSON output; second, an entailment module using Claude 4.5 Opus determines hierarchical relationships between extracted classes through iterative logical reasoning. The pipeline outputs formal RDF ontologies (owl:Class, owl:ObjectProperty) serialized to Turtle format using rdflib. Evaluation uses embedding-based fuzzy matching with thresholds of 0.94-0.95 to align predicted triples with gold standard ontologies across three enterprise domains.

## Key Results
- Fuzzy-match F1-score of 0.724 achieved in Data domain, compared to 0.102 with exact matching
- Significant domain variation observed: F1 of 0.121 in Finance domain vs. 0.724 in Data domain
- Calls for standardized benchmarks for end-to-end ontology construction evaluation from unstructured text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing ontology construction into sequential extraction and entailment phases improves output controllability compared to end-to-end generation.
- Mechanism: The extraction module first identifies candidate classes and properties from unstructured text, producing a flat list. The entailment module then operates on this constrained input to determine hierarchical relationships. This separation reduces the search space at each step and allows independent validation.
- Core assumption: LLMs perform better on bounded sub-tasks than on complex multi-step reasoning in a single pass.
- Evidence anchors:
  - [abstract] "Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy"
  - [section] "The pipeline uses a two-step LLM process: first to extract classes and properties, and second to reason about the hierarchical relationships between those classes"
  - [corpus] Neighboring paper "From Prompt to Graph" similarly finds LLM-based IE strategies benefit from structured decomposition in domain-specific ontology development
- Break condition: If extraction produces incomplete or noisy class lists, the entailment phase will propagate errors rather than correct them.

### Mechanism 2
- Claim: Enforcing structured output schemas via Pydantic data models constrains LLM outputs to valid, machine-processable JSON.
- Mechanism: Pydantic models define required fields (classes, properties, descriptions, domains, ranges). The LLM is prompted to conform to this schema, and invalid outputs fail validation. This reduces post-processing overhead and ensures downstream modules receive predictable inputs.
- Core assumption: The LLM can reliably generate syntactically valid JSON matching the schema.
- Evidence anchors:
  - [section] "To ensure the expected output is structured, we define strict data models using Pydantic. This forces our LLM to output valid JSON containing specific metadata like classes, properties, description, domain, range"
  - [corpus] Weak direct corpus evidence; neighboring papers do not explicitly compare schema enforcement strategies
- Break condition: If the LLM hallucinates extra fields or omits required ones, validation fails and requires retry logic or fallback handling.

### Mechanism 3
- Claim: Fuzzy embedding-based matching captures semantic equivalence better than exact string matching for ontology evaluation.
- Mechanism: Predicted triples are aligned to gold-standard triples using embedding similarity with thresholds (0.94–0.95). This accounts for lexical variations (e.g., "Employee" vs. "StaffMember") that exact matching would penalize.
- Core assumption: Embedding similarity correlates with semantic equivalence in the ontology domain.
- Evidence anchors:
  - [section] "we adopted embedding-based fuzzy matching to align predicted triples with their gold standard; we set a similarity threshold of (0.94,0.94,0.95) for the three use cases"
  - [section] "The best performances were reached in the Data use case, where we had an F1-score of 0.724 in the fuzzy setting" compared to 0.102 exact match
  - [corpus] Neighboring papers do not provide comparative evaluation of fuzzy vs. exact matching for ontology construction
- Break condition: If embeddings conflate distinct but lexically similar concepts (e.g., "Loan" vs. "LoanOfficer"), false positives increase.

## Foundational Learning

- Concept: RDF/OWL ontology primitives (owl:Class, owl:ObjectProperty, rdfs:subClassOf, domain/range)
  - Why needed here: The pipeline outputs formal RDF ontologies; understanding T-Box (schema) vs. A-Box (instances) is essential for debugging.
  - Quick check question: Can you explain why "isTypeOf" is ambiguous between rdf:type and rdfs:subClassOf in an RDF model?

- Concept: Hierarchical reasoning and subsumption
  - Why needed here: The entailment module must correctly determine subclass relationships; the paper reports directionality errors as a key failure mode.
  - Quick check question: Given classes "Vehicle" and "Car," which direction should rdfs:subClassOf point, and why?

- Concept: LLM structured output and prompt engineering
  - Why needed here: The pipeline relies on prompts that enforce specific output formats; understanding prompt design is critical for iteration.
  - Quick check question: What is the role of a system prompt vs. a user prompt in constraining LLM output format?

## Architecture Onboarding

- Component map: Data Ingestion → Pydantic schema validation → Ontological Element Extraction → Hierarchy Construction (Entailment) → RDF Serialization
- Critical path: Extraction quality → Entailment input completeness → RDF validity. Errors in extraction cascade to entailment; entailment errors propagate to final ontology.
- Design tradeoffs:
  - Single-pass vs. multi-phase: Multi-phase improves controllability but adds complexity and latency.
  - Exact vs. fuzzy evaluation: Fuzzy captures semantic similarity but may mask precision issues.
  - Model selection: Authors chose different models for extraction vs. entailment (Gemini for extraction, Claude for reasoning)—implies task-specific model tuning.
- Failure signatures:
  - Scope creep: LLM includes irrelevant terms (reported in Finance domain, F1=0.121)
  - Abstraction drift: LLM proposes individuals instead of classes (no explicit abstraction-level guidance)
  - Hierarchy inversion: Subclass direction reversed (e.g., "Policy" and "GovernanceStandard" declared mutual subclasses)
  - Ambiguous properties: Introduction of unclear relations like "isTypeOf"
- First 3 experiments:
  1. Run OntoEKG on a single-domain document with known ground truth; compare exact vs. fuzzy F1 to calibrate thresholds.
  2. Ablate the entailment module by providing a pre-defined class hierarchy; measure impact on final ontology coherence.
  3. Test alternative LLMs for the extraction phase (e.g., swap Gemini for Claude) to isolate model-specific extraction quality effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized evaluation frameworks are required to effectively benchmark end-to-end ontology construction from purely unstructured text?
- Basis in paper: [explicit] The authors issue a "call to action" for the community, noting a "lack of comprehensive benchmarks" that forced them to create a custom dataset.
- Why unresolved: Existing benchmarks like OntoURL require semi-structured inputs, while others like Text2KGBench focus on instance-level extraction rather than full ontology construction.
- What evidence would resolve it: The adoption of a community-recognized dataset containing unstructured text and corresponding gold-standard RDF ontologies for distinct domains.

### Open Question 2
- Question: Can progressive ontology construction, where existing models are fed back into the pipeline, ensure consistency across multiple source documents?
- Basis in paper: [explicit] The conclusion lists "progressive construction" as future work to ensure models "stay consistent across different source documents."
- Why unresolved: The current pipeline likely processes documents in isolation or lacks a memory mechanism to enforce consistency over time, leading to semantic drift.
- What evidence would resolve it: Experiments demonstrating that updating an ontology with new documents retains the validity of previous class hierarchies and property domains without manual intervention.

### Open Question 3
- Question: How can LLM pipelines autonomously distinguish between ontology classes (T-Box) and named individuals (A-Box) to control the abstraction level?
- Basis in paper: [inferred] The limitations section notes the LLM often "proposes individuals instead of classes" because "no explicit requirements were declared in terms of a target level of abstraction."
- Why unresolved: Without explicit signaling or rigid constraints, the model struggles to determine whether a term represents a generic concept or a specific instance.
- What evidence would resolve it: A mechanism (e.g., prompt engineering or validation layer) that successfully filters or reifies individuals into classes according to a specified abstraction profile.

## Limitations
- Evaluation relies on small, proprietary dataset (3 domains, undisclosed size) limiting generalizability
- Significant domain performance variation (F1 0.724 vs. 0.121) not fully characterized
- Critical implementation details like prompt templates and preprocessing strategies not disclosed

## Confidence
- High confidence: The decomposition approach and Pydantic schema enforcement are technically sound and well-supported by the literature
- Medium confidence: The fuzzy matching evaluation methodology is reasonable but the specific thresholds appear overfit to this dataset
- Low confidence: Cross-domain performance claims due to the limited and proprietary evaluation data

## Next Checks
1. Implement ablation studies comparing exact vs. fuzzy evaluation with varying thresholds across multiple domains to determine threshold robustness
2. Test the pipeline on publicly available ontology datasets (e.g., DBpedia, WordNet subsets) to assess generalizability beyond the proprietary enterprise data
3. Conduct human evaluation of the extracted ontologies focusing on coherence, abstraction level consistency, and hierarchical correctness to complement automated metrics