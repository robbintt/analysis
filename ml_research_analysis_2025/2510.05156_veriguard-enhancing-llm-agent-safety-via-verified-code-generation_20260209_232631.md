---
ver: rpa2
title: 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation'
arxiv_id: '2510.05156'
source_url: https://arxiv.org/abs/2510.05156
tags:
- agent
- policy
- code
- safety
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of ensuring safety and security
  for LLM-based autonomous agents in sensitive domains like healthcare, where agents
  risk deviating from user objectives, violating data policies, or being compromised
  by attacks. The proposed VeriGuard framework uses a dual-stage architecture to provide
  formal safety guarantees: an offline stage that synthesizes a behavioral policy
  from natural language specifications, validates it through testing and formal verification,
  and an online stage that monitors and validates each agent action against the pre-verified
  policy before execution.'
---

# VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation

## Quick Facts
- arXiv ID: 2510.05156
- Source URL: https://arxiv.org/abs/2510.05156
- Reference count: 40
- Near-zero attack success rates while maintaining high task success rates on agent security benchmarks

## Executive Summary
VeriGuard is a dual-stage framework that provides formal safety guarantees for LLM-based autonomous agents by synthesizing and verifying behavioral policies from natural language specifications. The system combines offline policy synthesis with online runtime monitoring to prevent agents from deviating from user objectives or being compromised by attacks. Experiments demonstrate that VeriGuard achieves near-zero attack success rates while maintaining high task completion rates across multiple benchmarks including agent security, healthcare, and web navigation tasks.

## Method Summary
VeriGuard operates through a two-stage architecture. The offline stage uses LLMs to generate Python policy code and logical constraints from natural language specifications, then validates these through iterative refinement loops involving compiler checks, PyTest case generation, and formal verification using Nagini. The online stage monitors each agent action by extracting runtime arguments and executing the pre-verified policy. When violations are detected, the system employs a "Collaborative Re-planning" strategy combined with "Tool Execution Halt" to enforce safety while attempting to complete the task.

## Key Results
- On ASB benchmark: 0% attack success rate with 63.3% task success using Gemini-2.5-Flash
- On EICU-AC benchmark: 100% accuracy in handling medical data requests
- Outperforms baselines across all tested domains while maintaining safety guarantees

## Why This Works (Mechanism)
The framework's effectiveness stems from its hybrid approach combining the flexibility of LLM-based policy generation with the rigor of formal verification. By synthesizing executable policies from natural language and verifying them before deployment, VeriGuard ensures that safety constraints are both semantically meaningful and formally provable. The online monitoring component provides real-time enforcement, catching violations that might arise from runtime uncertainties or unexpected inputs.

## Foundational Learning
- **Formal Verification with Nagini**: Mathematical proof that code satisfies specified properties; needed to provide mathematical guarantees of safety rather than heuristic enforcement
- **Dual-Stage Architecture**: Separation of policy synthesis (offline) from enforcement (online); needed to enable computationally intensive verification while maintaining real-time performance
- **Iterative Refinement**: Loop that generates, tests, and verifies policy code until all requirements are met; needed to handle the non-deterministic nature of LLM outputs
- **Runtime Policy Execution**: Dynamic evaluation of agent actions against pre-verified constraints; needed to enforce safety in the face of uncertainty during execution
- **Collaborative Re-planning**: Strategy that attempts to find alternative solutions when violations are detected; needed to balance safety with task completion
- **Tool Execution Halt**: Immediate termination of unsafe actions; needed as a fail-safe mechanism when re-planning is not feasible

## Architecture Onboarding

**Component Map**: Natural Language Spec → LLM (Policy Code) → LLM (Constraints) → Refinement Loop → Nagini Verification → Online Monitor → Agent

**Critical Path**: User Request → Policy Generation → Verification → Runtime Enforcement → Agent Action → Monitoring

**Design Tradeoffs**: The framework trades computational overhead during the offline verification phase for strong safety guarantees at runtime. This approach prioritizes security over speed in the verification stage but enables efficient enforcement during execution.

**Failure Signatures**: Verification timeouts indicate overly complex constraints; high rejection rates suggest specification ambiguity; runtime violations reveal gaps between the verified model and actual execution environment.

**First Experiments**:
1. Implement the prompt templates for Policy Code Generation and Constraint Generation using a simple natural language specification
2. Build the iterative refinement pipeline and test it on a small set of requirements until all tests pass
3. Create the Nagini verification prompt and verify a simple policy, examining the counterexamples when verification fails

## Open Questions the Paper Calls Out

### Scalability of Verification
The paper identifies the scalability and efficiency of the formal verification process as a key area for future research, noting that verification can be computationally intensive and potentially limiting for real-time application in expansive agentic systems.

### Autonomous Specification Generation
Generating safety specifications autonomously while guaranteeing soundness without manual validation is highlighted as a promising avenue, addressing the current need for human-in-the-loop validation of LLM-generated constraints.

### Dynamic Reasoning for Sophisticated Attacks
The framework's reliance on static policy rules may be insufficient for attacks requiring dynamic reasoning or adaptive responses, suggesting the need for mechanisms that can handle evolving threat landscapes.

## Limitations
- Heavy reliance on external LLM API performance for policy generation quality
- Verification scope limited to pre- and post-conditions, not the full runtime environment
- Performance generalization to unseen domains and attack types not fully explored

## Confidence

**High Confidence**: The core methodology of using formal verification with a dual-stage approach is sound and well-established in software verification literature.

**Medium Confidence**: The effectiveness of LLM-based policy generation and refinement loops depends on internal metrics not independently verified, and the impact of autonomous disambiguation on policy quality is unclear.

**Low Confidence**: Scalability to complex specifications and long-term reliability in dynamic environments with evolving threats remains unproven.

## Next Checks

1. **Verification Robustness Test**: Generate increasingly complex logical constraints and measure when Nagini verification consistently fails, analyzing counterexamples to identify constraint generation weaknesses.

2. **Policy Generation Quality Audit**: Manually review random samples of generated policies and their corresponding PyTest cases to assess semantic accuracy against original specifications.

3. **Generalization Experiment**: Apply VeriGuard to a new domain (e.g., financial transactions) with different attack vectors and compare performance against non-verified baselines.