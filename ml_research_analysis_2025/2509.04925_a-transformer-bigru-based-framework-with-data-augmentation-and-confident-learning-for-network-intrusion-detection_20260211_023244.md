---
ver: rpa2
title: A transformer-BiGRU-based framework with data augmentation and confident learning
  for network intrusion detection
arxiv_id: '2509.04925'
source_url: https://arxiv.org/abs/2509.04925
tags:
- classification
- feature
- data
- trailgate
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrailGate, a two-stage deep learning framework
  that integrates Transformer and Bidirectional Gated Recurrent Unit (BiGRU) architectures
  for network intrusion detection. The model combines machine learning and deep learning
  techniques, enhanced by feature selection strategies and data augmentation to address
  class imbalance.
---

# A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection

## Quick Facts
- arXiv ID: 2509.04925
- Source URL: https://arxiv.org/abs/2509.04925
- Reference count: 40
- Primary result: TrailGate achieves 94.10% accuracy in binary classification and 85.81% in multi-class classification on NSL-KDD dataset

## Executive Summary
TrailGate introduces a two-stage deep learning framework for network intrusion detection that integrates Transformer and Bidirectional Gated Recurrent Unit (BiGRU) architectures. The model combines machine learning and deep learning techniques, enhanced by feature selection strategies and data augmentation to address class imbalance. By utilizing confident learning to identify mislabeled samples and select robust features, TrailGate improves detection accuracy and reduces false positives, particularly for rare attack types.

## Method Summary
TrailGate employs a two-stage architecture where Random Forest performs initial coarse binary filtering on 5 key features, followed by BiGRU+Transformer processing on flagged traffic using 11-12 selected features. The framework incorporates ADASYN-based data augmentation to address class imbalance and confident learning for identifying mislabeled samples during feature selection. The approach uses Information Gain ranking, Pearson Correlation filtering, and Incremental Feature Selection to identify the most discriminative features before feeding them into the deep learning stages.

## Key Results
- Achieves 94.10% accuracy in binary classification and 85.81% in multi-class classification on KDDTest+
- Outperforms existing methods particularly for rare attack types (U2R and R2L intrusions)
- Confident learning improves accuracy from 81.03% to 85.81% in multi-class detection with fewer features (12 vs 21)
- 100% ADASYN augmentation yields best multi-class performance across all attack categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating confident learning into feature selection pipeline improves detection accuracy by identifying and handling mislabeled or anomalous samples before feature ranking.
- **Mechanism:** CL estimates joint distribution between assigned and predicted labels via cross-validated probabilities. Samples where predicted labels differ from assigned labels (above confidence thresholds) are flagged as noisy and added during feature selection to expand sample space, helping identify features that generalize better to distribution shifts.
- **Core assumption:** Training set contains labeling errors or outliers that bias feature selection toward spurious correlations; removing or augmenting with identified anomalies exposes more robust features.
- **Evidence anchors:** Abstract states CL improves accuracy; Section 4.6 Table 15 shows 85.81% accuracy with CL vs 81.03% without (12 features vs 21).

### Mechanism 2
- **Claim:** Two-stage architecture (RF → BiGRU+Transformer) improves detection accuracy for rare attack types while maintaining computational efficiency.
- **Mechanism:** Stage 1 RF with 5 features performs coarse binary filtering (normal vs abnormal), reducing data volume for Stage 2. Stage 2 BiGRU+Transformer processes only flagged traffic, with BiGRU capturing bidirectional temporal dependencies and Transformer encoder applying multi-head attention to prioritize discriminative features.
- **Core assumption:** Most network traffic is normal, so early filtering is efficient; complex attacks require deeper temporal and attention-based modeling that benefits from focused computation on reduced subset.
- **Evidence anchors:** Abstract describes two-stage framework; Section 4.6 Figure 18a shows RF+BiGRU+Transformer achieves 94.10% binary accuracy vs 85.97% for RF alone and 90.64% for BiGRU+Transformer alone.

### Mechanism 3
- **Claim:** ADASYN-based data augmentation mitigates class imbalance and improves detection of minority attack types (U2R, R2L).
- **Mechanism:** ADASYN adaptively generates synthetic samples for minority classes, with higher density near decision boundaries. This expands effective sample space for rare classes without simple duplication, encouraging model to learn more generalizable decision boundaries.
- **Core assumption:** Synthetic samples generated by interpolation near decision boundaries meaningfully represent feature distribution of minority classes.
- **Evidence anchors:** Abstract mentions data augmentation addresses class imbalance; Section 4.6 Figure 19 shows F1-scores improved across all categories after ADASYN; Figure 20 shows 100% augmentation yields best multi-class performance.

## Foundational Learning

- **Concept: Confident Learning (Label Noise Detection)**
  - **Why needed here:** CL identifies mislabeled samples that could bias feature selection. Without understanding CL's joint distribution estimation and confidence thresholds, preprocessing pipeline is opaque.
  - **Quick check question:** Can you explain how CL computes confidence threshold `Aj` for each class and how it identifies samples with label issues?

- **Concept: Bidirectional GRU for Sequential Data**
  - **Why needed here:** BiGRU processes traffic sequences in both directions to capture temporal context. Understanding gating mechanisms (update/reset gates) is essential for debugging Stage 2.
  - **Quick check question:** Why would processing sequence backward help detect network intrusions that BiGRU might miss in forward-only mode?

- **Concept: Transformer Self-Attention (Multi-Head)**
  - **Why needed here:** Transformer encoder refines BiGRU outputs by weighting feature importance dynamically. Understanding Q/K/V formulation clarifies how attention prioritizes features.
  - **Quick check question:** In attention formula `Attention(Q,K,V) = softmax(QK^T / √d_k)V`, what does scaling by `√d_k` prevent?

## Architecture Onboarding

- **Component map:** Preprocessing → ADASYN → IG ranking → PCC filtering → CL (10-fold CV) → IFS → Stage 1 RF → Stage 2 BiGRU+Transformer → Final prediction
- **Critical path:** Preprocessing → ADASYN → IG ranking → PCC filtering → CL (10-fold CV) → IFS (determine top-k features) → Stage 1 RF → Stage 2 BiGRU+Transformer → Final prediction
- **Design tradeoffs:**
  - PCC threshold: Lower (0.7) for binary removes more redundancy but risks losing information; higher (0.9) for multi-class preserves features for finer distinctions
  - Feature count: Binary optimal at 11 features; multi-class at 12. Over-selecting adds noise; under-selecting loses discriminative power
  - Two-stage vs single-stage: Two-stage adds complexity but reduces Stage 2 computational load and focuses deep model on harder cases
  - CL augmentation location: Adding CL-flagged samples to training subset (not before split) yielded better results
- **Failure signatures:**
  - Low recall on KDDTest-21 (59.67% binary): Large distribution shift from training; specificity remains high (98.68%) but attacks are missed
  - U2R F1-score weak (~11%): Extremely few training samples (52 U2R in KDDTrain+); even ADASYN struggles to generalize
  - High FAR if CL skipped: Without CL, model selects 21 features (vs 12) and accuracy drops to 81.03%
- **First 3 experiments:**
  1. Reproduce binary classification baseline: Run TrailGate on KDDTest+ with default parameters (n_estimators=300, batch_size=512, epochs=9, PCC=0.7, 11 features). Verify accuracy near 94.10%. Log confusion matrix to check normal vs attack balance.
  2. Ablate confident learning: Disable CL in feature selection (use standard IG+PCC+IFS only). Compare feature count and accuracy against Table 15 to confirm ~4.8% drop.
  3. Test data augmentation sensitivity: Vary ADASYN ratios (70%, 80%, 90%, 100% of majority class size) on multi-class task. Plot F1 per class to reproduce Figure 20 and identify optimal ratio for your data distribution.

## Open Questions the Paper Calls Out

- **Question:** How can computational efficiency of TrailGate be optimized to support real-time intrusion detection in resource-constrained environments like IoT devices?
  - **Basis in paper:** Authors identify high computational costs associated with feature selection process and two-stage deep learning model as limitation, noting method is currently less suitable for real-time applications or hardware with limited capacity.
  - **Why unresolved:** Current framework relies on complex batch processing (IG, IFS, BiGRU+Transformer) that incurs significant time and memory costs.
  - **What evidence would resolve it:** Successful deployment of lightweight TrailGate version on edge hardware with acceptable latency and memory usage metrics.

- **Question:** Can generative models produce more effective synthetic samples than ADASYN for detecting rare attack types like U2R and R2L?
  - **Basis in paper:** In "Future directions" section, authors propose investigating generative models for advanced data augmentation to improve detection accuracy for underrepresented intrusion types.
  - **Why unresolved:** Current study relies on ADASYN, which may not fully capture complex distributions of scarce data as effectively as generative approaches could.
  - **What evidence would resolve it:** Comparative experiments showing improved F1-scores for U2R and R2L attacks when using generative models (e.g., GANs) instead of ADASYN within TrailGate framework.

- **Question:** How can online learning be integrated into framework to dynamically update model with new data without compromising stability?
  - **Basis in paper:** Authors state future work will focus on incorporating online learning to enable system to process data in real time and adapt to emerging threats.
  - **Why unresolved:** Current model is trained offline and does not support dynamic updates as new network traffic is ingested.
  - **What evidence would resolve it:** Modified TrailGate architecture that successfully updates weights in real-time streams while maintaining or improving upon original static model's accuracy.

## Limitations
- Computational inefficiency limits real-time deployment, especially in resource-constrained environments like IoT devices
- U2R detection remains weak (~11% F1) due to extremely limited training samples even with ADASYN augmentation
- Missing model hyperparameters (BiGRU hidden size, Transformer attention heads, feed-forward dimensions) hinder exact reproduction
- Performance degradation on KDDTest-21 (59.67% recall) indicates limited generalization to distribution shifts

## Confidence
- **High** for two-stage architecture design and its efficiency rationale
- **Medium** for confident learning's contribution to accuracy gains, given limited corpus precedent
- **Medium** for data augmentation effectiveness, assuming ADASYN's interpolation assumptions hold

## Next Checks
1. Reproduce binary classification baseline on KDDTest+ with default parameters and log confusion matrix
2. Ablate confident learning to compare feature count and accuracy drop against reported values
3. Test ADASYN augmentation ratios on multi-class task to reproduce F1 per class curves