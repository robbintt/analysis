---
ver: rpa2
title: 'The Rise of Small Language Models in Healthcare: A Comprehensive Survey'
arxiv_id: '2504.17119'
source_url: https://arxiv.org/abs/2504.17119
tags:
- language
- arxiv
- healthcare
- medical
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive survey of small language
  models (SLMs) in healthcare, identifying 20 SLMs and establishing a taxonomy for
  their development, adaptation, and compression. The study defines SLMs as models
  up to 7B parameters, optimized for healthcare tasks under resource constraints.
---

# The Rise of Small Language Models in Healthcare: A Comprehensive Survey

## Quick Facts
- **arXiv ID:** 2504.17119
- **Source URL:** https://arxiv.org/abs/2504.17119
- **Reference count:** 40
- **Primary result:** First comprehensive survey of SLMs in healthcare, defining 7B-parameter threshold, compiling 20 models, and benchmarking performance across medical QA, NLU, and mental health tasks.

## Executive Summary
This survey establishes the first comprehensive taxonomy of Small Language Models (SLMs) in healthcare, defining them as decoder-only architectures up to 7B parameters optimized for resource-constrained clinical environments. The authors systematically identify 20 SLMs, categorizing their development approaches by architecture, data curation, pretraining components, and attention mechanisms. Experimental results demonstrate that models like BioMedLM, BioMistral-DARE, and MentalQLM achieve competitive performance across medical question answering (81.06% accuracy), natural language understanding (63.4% F1), and mental health analysis (84.20% weighted F-measure), while offering up to 80% reduction in carbon emissions compared to large models. The survey emphasizes SLMs' potential for privacy-preserving, on-device clinical decision support and identifies critical research directions including expanded benchmarks, bias detection, and interoperability.

## Method Summary
The authors conducted a systematic literature search across PubMed, ACM, IEEE, Google Scholar, and Scopus using mixed terminology related to local, on-device, and compressed language models. They filtered results for English-language, decoder-only architectures with parameter counts ≤7B, excluding encoder-only, multimodal, and larger models. The methodology involved extracting data into a comprehensive taxonomy covering architecture, adaptation, and compression techniques, followed by compilation of performance metrics from cited studies. The survey synthesizes findings into structured tables and comparative analyses, establishing a foundational reference for SLM development in healthcare contexts.

## Key Results
- Identified 20 specific SLMs optimized for healthcare applications under resource constraints
- Established comprehensive taxonomy categorizing models by architecture, data curation, pretraining, and attention mechanisms
- Demonstrated competitive performance: Medical QA accuracy up to 81.06%, NLU F1 up to 63.4%, Mental Health F-measure up to 84.20%
- Quantified environmental benefits with up to 80% carbon emission reduction versus large language models

## Why This Works (Mechanism)
Small Language Models succeed in healthcare through their efficient decoder-only architecture that balances computational constraints with domain-specific performance. The 7B parameter threshold enables deployment on edge devices while maintaining sufficient capacity for medical language understanding. Task-specific fine-tuning adapts these models to healthcare terminology and clinical workflows, while attention mechanism optimizations reduce computational complexity without sacrificing accuracy. Data curation strategies ensure medical domain relevance while addressing privacy and bias concerns, and compression techniques further enhance efficiency through pruning, quantization, or knowledge distillation.

## Foundational Learning
- **Decoder-only architecture (GPT-like)**: Required for autoregressive text generation and alignment with existing SLM frameworks; verify model uses causal attention masks
- **Parameter budget constraints (≤7B)**: Enables deployment on edge devices and reduces computational costs; check model specification files for exact parameter count
- **Task-specific fine-tuning**: Adapts general language capabilities to medical terminology and clinical workflows; confirm fine-tuning datasets match target healthcare domains
- **Attention mechanism optimization**: Reduces computational complexity while maintaining performance; examine attention patterns in model architecture diagrams
- **Data curation strategies**: Ensures medical domain relevance while addressing privacy and bias concerns; verify dataset composition and filtering criteria
- **Compression techniques**: Enables efficient deployment through pruning, quantization, or knowledge distillation; assess model size reduction ratios versus baseline

## Architecture Onboarding
**Component map:** Literature Search -> Taxonomy Extraction -> Performance Benchmarking -> Comparative Analysis
**Critical path:** Systematic search → Model identification → Taxonomy construction → Performance synthesis
**Design tradeoffs:** Model size vs. performance (7B parameter limit), general vs. domain-specific capabilities, computational efficiency vs. accuracy
**Failure signatures:** Inclusion of encoder-only models (BERT variants), aggregation of incomparable metrics from heterogeneous studies, overlooking privacy and bias considerations
**First experiments:** 1) Execute systematic search with Boolean operators to verify 20-model identification, 2) Cross-reference evaluation settings for Fig 4 benchmarks against source papers, 3) Reconstruct carbon emission calculations from cited baselines

## Open Questions the Paper Calls Out
The survey identifies several critical research directions that warrant further investigation. These include the development of expanded benchmark suites that better capture the complexity of clinical decision-making scenarios, systematic approaches to detecting and mitigating bias in healthcare-focused SLMs, and standardization of evaluation protocols to enable meaningful cross-study comparisons. Additionally, the authors highlight the need for research on interoperability frameworks that allow SLMs to seamlessly integrate with existing clinical systems and electronic health records while maintaining data privacy and security standards.

## Limitations
- Exclusive focus on English-language, decoder-only models may miss relevant multilingual or encoder-based approaches
- Performance comparisons aggregate metrics from heterogeneous studies with potentially different evaluation protocols
- Carbon emission reduction estimates lack detailed methodology for baseline model specifications and measurement conditions
- Limited discussion of real-world deployment challenges including regulatory compliance, clinician acceptance, and integration with clinical workflows
- Potential bias in literature search results due to database selection and search term formulation

## Confidence
- **High Confidence:** Taxonomic framework for categorizing SLMs is clearly defined and internally consistent
- **Medium Confidence:** Identification of 20 specific SLMs is reproducible given search criteria, dependent on precise Boolean logic
- **Medium Confidence:** Performance benchmarks accurately reflect source paper values but cross-study comparisons require cautious interpretation

## Next Checks
1. Execute systematic search across specified databases using provided keywords with Boolean operators to verify 20-model identification
2. Cross-reference evaluation settings (few-shot examples, prompts, dataset splits) for each benchmarked task in Fig 4 against original source papers
3. Reconstruct carbon emission calculations by identifying specific large-model baselines and measurement methodologies cited in the survey