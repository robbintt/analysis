---
ver: rpa2
title: 'Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying'
arxiv_id: '2601.20773'
source_url: https://arxiv.org/abs/2601.20773
tags:
- copy
- black-box
- copying
- rmse
- algo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of black-box model copying where
  only hard-label outputs are available, which creates a discontinuous surface reconstruction
  challenge. The core method introduces signed-distance supervision, replacing hard
  labels with signed distances to the decision boundary to convert copying into a
  smooth regression problem.
---

# Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying

## Quick Facts
- arXiv ID: 2601.20773
- Source URL: https://arxiv.org/abs/2601.20773
- Reference count: 40
- Key outcome: Signed-distance supervision improves black-box model copying fidelity and generalization by converting discontinuous hard-label supervision into smooth regression problems, with α-parameterized regularization providing provable continuity guarantees.

## Executive Summary
This paper addresses the fundamental challenge of black-box model copying where only hard-label outputs are available, creating a discontinuous surface reconstruction problem. The authors introduce signed-distance supervision, which replaces binary hard labels with signed distances to the decision boundary, converting the copying task into a smooth regression problem. This approach exploits local boundary geometry to improve sample efficiency and boundary recovery. The method introduces an α-governed regularization scheme with provable Hölder/Lipschitz continuity guarantees and proposes two model-agnostic algorithms to estimate signed distances from hard-label queries. Experiments on synthetic and UCI benchmark datasets demonstrate consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty signals for black-box replicas.

## Method Summary
The method replaces hard-label supervision with signed distances to the teacher's decision boundary, converting discontinuous copying into smooth regression. The signed distance target is defined as ℓ_α(s_i) = f_O(s_i) · ξ(s_i)^α, where ξ is the distance to the nearest opposite-class point and α controls the regularization strength. Two algorithms estimate signed distances: Algorithm 1 uses iterative ball refinement, while Algorithm 2 employs clustered sampling for query efficiency. The replica is trained as a regressor on these signed-distance targets, with class predictions obtained from the sign of the output. The α parameter provides provable continuity guarantees (Hölder for α ≤ 1, Lipschitz for α ≥ 1) that control the fidelity-accuracy tradeoff.

## Key Results
- Signed-distance copying achieves 10-20% reduced generalization error compared to hard-label baselines on neural network copies
- Algorithm 2 (clustered sampling) outperforms Algorithm 1 under fixed query budgets while requiring significantly fewer black-box evaluations
- Distance-based supervision shows improved sample efficiency in low-data regimes compared to hard-label approaches
- GB student models show fidelity degradation, while NN copies consistently benefit from signed-distance supervision
- High-dimensional datasets (>30D) show degraded distance estimation quality with MAE increasing to 0.3-0.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing hard-label supervision with signed distances converts discontinuous surface reconstruction into smooth regression, improving sample efficiency and boundary recovery.
- Mechanism: Hard labels provide binary class membership (f_O(s_i) ∈ {-1, +1}), creating a step function at the decision boundary. Signed distances ℓ(s_i) = f_O(s_i)·ξ(s_i)^α encode both class (via sign) and geometric proximity to boundary (via magnitude), yielding continuous targets that vary smoothly across input space.
- Core assumption: The black-box teacher has reasonably regular decision boundaries; highly irregular boundaries may limit distance estimation accuracy.
- Evidence anchors: [abstract] "replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry" [Section 3] "copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently"
- Break condition: When teacher model has extremely irregular or fractal-like decision boundaries, or in very high dimensions where distance estimation degrades substantially.

### Mechanism 2
- Claim: α-parameterized regularization provides provable continuity guarantees (Hölder for α ≤ 1, Lipschitz for α ≥ 1) that control fidelity-accuracy tradeoff.
- Mechanism: Exponentiating distance ξ(s_i)^α allows smooth interpolation between pure replication (α = 0, hard labels) and full distance copying (α = 1). Theorem 3.1 proves |ℓ_α(x) - ℓ_α(y)| ≤ 2d(x,y)^α for α ≤ 1, ensuring target surface remains bounded and learnable.
- Core assumption: Bounded domain assumption (distance D < ∞) and induced regularization improves generalization when teacher is overfitted.
- Evidence anchors: [Section 3.1] "Theorem 3.1 shows that α-signed distance targets are α-Hölder continuous for α ≤ 1 and Lipschitz continuous for α ≥ 1" [Section 5.2] "Figure 8... shows two examples of the ideal behaviour of the method as the value of α increases (irregular or overfitted model)"
- Break condition: When α is poorly tuned for specific teacher-student pair; automated selection noted as future work.

### Mechanism 3
- Claim: Query-efficient distance estimation via clustered sampling (Algorithm 2) outperforms iterative refinement (Algorithm 1) under fixed query budgets.
- Mechanism: Algorithm 2 samples cluster centers C, then labels inner (B_in) and outer (B_out) clouds simultaneously. For each point in B_in, distance estimated as minimum distance to differently-labeled points in B_out. This amortizes queries across n_in points per cluster center.
- Core assumption: Cluster-based sampling provides sufficient coverage of decision boundary; high-dimensional spaces may require denser sampling.
- Evidence anchors: [Section 5.1] "Algorithm 2 outperforms Algorithm 1 in this operational setting... it tends to achieve higher accuracies and improved fidelities compared to hard copies" [Section 3.2] "this algorithm requires n_c(n_in + n_out) black-box evaluations... representing a significant reduction in cost"
- Break condition: In high-dimensional datasets (e.g., Dataset 6 with dim=60), distance approximation quality degrades (MAE increases to 0.44-0.50 per Table 4).

## Foundational Learning

- Concept: **Signed Distance Functions (SDFs)**
  - Why needed here: The method encodes decision boundary geometry using signed distances, where sign indicates class and magnitude indicates proximity to boundary.
  - Quick check question: Given a point x with f(x) = +1 and distance ξ(x) = 0.3 to nearest negative-class point, what is the signed distance target ℓ(x)? (Answer: +0.3 for α=1)

- Concept: **Hölder vs Lipschitz Continuity**
  - Why needed here: Theorem 3.1 provides different guarantees based on α; understanding these helps select appropriate α regimes.
  - Quick check question: For α = 0.5, if d(x,y) = 0.01, what is the maximum change in target values? (Answer: ≤ 2 × 0.01^0.5 = 0.2)

- Concept: **Fidelity-Accuracy Tradeoff in Model Copying**
  - Why needed here: α parameter directly controls whether replica prioritizes exact boundary replication (fidelity) or task generalization (accuracy).
  - Quick check question: If teacher model is overfitted, which α regime would you expect to improve accuracy? (Answer: Larger α > 0, as regularization suppresses overfitted irregularities)

## Architecture Onboarding

- Component map: Teacher (black-box f_O, hard-label only) -> Distance Estimation (Algorithm 1 or 2) -> Replica Training (regression on ℓ_α) -> Copy Model f_C (outputs signed distances, class via sign)

- Critical path:
  1. Define region of interest R and sample synthetic queries (Sobol sequences recommended)
  2. Select α based on fidelity vs accuracy priority (start with α ∈ [0.5, 1.0])
  3. Estimate signed distances using Algorithm 2 for query efficiency
  4. Train replica as regressor on ℓ_α targets (NN recommended over GB)
  5. Evaluate fidelity on independent uniform set, accuracy on real test data

- Design tradeoffs:
  | Choice | Pros | Cons |
  |--------|------|------|
  | Algorithm 1 vs 2 | A1: asymptotically converges to true distance | A1: costly (O(it_max·n·m)); A2: better under query budgets |
  | α < 1 vs α ≥ 1 | α < 1: direct smoothness control | α ≥ 1: Lipschitz regime, regularity saturates, focuses on robustness |
  | NN vs GB replica | NN: better for smooth regression | GB: limited smooth regression ability, fidelity degrades |
  | Small vs large synthetic set | Large sets: better fidelity | Diminishing returns; distance-based benefits strongest in low-data regimes |

- Failure signatures:
  - High-dimensional datasets (>30 dim): Distance estimation MAE increases significantly (Table 4 shows MAE ~0.28-0.50 for Datasets 4-6)
  - GB student models: Show fidelity degradation (Table 2: GB shows +59.8% RF_emp at α=1)
  - Highly irregular teachers: Fidelity-accuracy tradeoff becomes severe; may not find α that satisfies both
  - Algorithm 2 with dense synthetic sampling: Points may saturate maximum distance threshold, degrading aggregate MAE scores

- First 3 experiments:
  1. **Baseline comparison**: On 2D synthetic dataset (Dataset 1), compare hard-label copy vs signed-distance copy (α=1, Algorithm 2) across synthetic set sizes [1000, 10000, 100000, 1000000]. Track fidelity error and test accuracy curves to reproduce Figure 5 patterns.
  2. **α sensitivity analysis**: Using Dataset 1 and MNN copy, sweep α ∈ {0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5} with 1M synthetic samples. Plot fidelity and accuracy vs α to identify optimal regime for given teacher.
  3. **Distance quality validation**: Train copy with Algorithm 2, then evaluate predicted distances against ground-truth distances (computed via Algorithm 1) on held-out synthetic set. Compute MAE/RMSE to validate Table 4 trends for specific dataset dimensionality.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can efficient multi-class querying and distance estimation schemes be developed that preserve benefits of signed-distance supervision without relying on binary reductions (e.g., one-vs-rest or ECOC)?
  - Basis in paper: [explicit] "The framework is formulated for binary classifiers; while multi-class problems can be handled via standard reductions, efficient multi-class querying and distance estimation remain open directions."
  - Why unresolved: Current approach reduces multi-class to binary tasks via Error Correcting Output Coding, which multiplies query complexity and may not exploit inter-class boundary geometry.
  - What evidence would resolve it: Multi-class distance estimation algorithm with query complexity sublinear in number of classes, achieving comparable fidelity and accuracy gains on multi-class benchmarks.

- **Open Question 2**: Can formal learning-theoretic bounds be established that quantitatively relate Hölder/Lipschitz regularity of α-signed distance targets to generalization error and sample complexity improvements?
  - Basis in paper: [explicit] "Gains in generalization and sample efficiency are demonstrated empirically rather than via formal learning-theoretic bounds."
  - Why unresolved: Theorem 3.1 provides target regularity guarantees, but these do not directly translate to copy generalization bounds without additional assumptions about hypothesis class and data distribution.
  - What evidence would resolve it: PAC-style bounds linking α, sample size, and copy capacity to expected fidelity/generalization error, validated against empirical learning curves.

- **Open Question 3**: How does distance-based supervision scale to high-dimensional domains (e.g., images, text) where signed-distance estimation becomes increasingly inaccurate due to curse of dimensionality?
  - Basis in paper: [explicit] "Experiments are limited to synthetic and tabular datasets; extending validation to large-scale, high-dimensional domains is an important direction for future work."
  - Why unresolved: Results show distance prediction quality degrades substantially from 2D synthetic datasets (MAE ~0.01-0.02) to higher-dimensional UCI data (MAE ~0.3-0.5), suggesting scalability challenges.
  - What evidence would resolve it: Systematic evaluation on image classification benchmarks with analysis of how distance estimation error impacts fidelity-accuracy trade-offs in high dimensions.

## Limitations
- High-dimensional datasets (>30D) show degraded distance estimation quality with MAE increasing to 0.3-0.5, suggesting scalability challenges
- GB student models show fidelity degradation with +59% fidelity loss at α=1, limiting method applicability
- Distance-based supervision requires careful α tuning to balance fidelity-accuracy tradeoff, with no automated selection mechanism provided

## Confidence
- **High confidence**: The theoretical continuity guarantees (Hölder/Lipschitz) and core mechanism of signed-distance supervision converting hard-label copying to smooth regression are well-supported by proofs and experimental evidence
- **Medium confidence**: The query efficiency gains of Algorithm 2 over Algorithm 1, and the fidelity-accuracy tradeoff controlled by α, are demonstrated but may be sensitive to specific teacher-student characteristics and hyperparameter choices
- **Low confidence**: The paper's claims about distance-based uncertainty estimation and practical significance of 10-20% generalization error improvements would benefit from real-world application testing beyond controlled synthetic/UCI benchmarks

## Next Checks
1. **High-dimensional stress test**: Evaluate signed-distance copying on real-world high-dimensional datasets (e.g., image or text classification) to validate distance estimation quality and fidelity maintenance beyond the 60D Connectionist bench benchmark
2. **Multi-class extension validation**: Implement and test the ECOC-based multi-class reduction strategy on standard multi-class datasets to verify the paper's claim that the approach extends beyond binary classification
3. **Adaptive α selection study**: Design experiments to test different α selection strategies (e.g., grid search, Bayesian optimization, or data-driven approaches) and quantify the sensitivity of fidelity/accuracy outcomes to α choice across different teacher model types