---
ver: rpa2
title: 'VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation
  Sensitivity in PPT'
arxiv_id: '2510.22045'
source_url: https://arxiv.org/abs/2510.22045
tags:
- slide
- sans
- text
- slides
- font
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VLM-SlideEval, a framework for evaluating
  vision-language models on presentation slide understanding. It combines pixel-accurate
  ground truth extraction with controlled perturbations and narrative ordering tests.
---

# VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT

## Quick Facts
- **arXiv ID**: 2510.22045
- **Source URL**: https://arxiv.org/abs/2510.22045
- **Reference count**: 40
- **Primary result**: Newer VLMs (o3, GPT-5) outperform older models (GPT-4.1, GPT-4o) on single-slide extraction but still struggle with fine-grained style recognition and cross-slide narrative coherence.

## Executive Summary
This paper introduces VLM-SlideEval, a framework for evaluating vision-language models on presentation slide understanding. It combines pixel-accurate ground truth extraction with controlled perturbations and narrative ordering tests. The evaluation reveals that newer VLMs (o3, GPT-5) outperform older models (GPT-4.1, GPT-4o) on single-slide extraction tasks but still struggle with fine-grained style recognition (e.g., font families) and cross-slide narrative coherence. Under controlled perturbations, models show a trade-off between fidelity and consistency, with geometry/style being more stable than text. The findings highlight the limits of current VLMs for slide evaluation and call for calibrated critic-in-the-loop evaluators in agentic pipelines.

## Method Summary
VLM-SlideEval evaluates VLMs on three tasks: structured element extraction from slides, sensitivity to controlled perturbations (geometry, text, style), and deck-level narrative ordering. Ground truth is extracted from PowerPoint XML and post-layout COM API queries to capture accurate geometry, content, and style. VLMs (GPT-4o, GPT-4.1, o3, GPT-5 variants) are prompted to output schema-validated JSON predictions. Predictions are aligned to ground truth using Hungarian matching with a blended cost function. Perturbations are synthesized with severity knobs to test fidelity-consistency trade-offs. Narrative ordering is evaluated by shuffling slides and measuring rank correlation.

## Key Results
- Newer VLMs (o3, GPT-5) achieve near-ceiling parse success (>99.7%) across complexity bins, while older models (GPT-4.1, GPT-4o) drop sharply beyond ~8 elements per slide.
- Under controlled perturbations, models show higher consistency on geometry/style than text, with 5-point scales improving consistency but 100-point scales increasing text fidelity.
- Narrative ordering performance is near-random (Kendall’s τ ∈ [0.04, 0.12]), indicating VLMs struggle with cross-slide coherence despite competent single-slide extraction.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining PowerPoint XML parsing with post-layout COM API queries yields verifiable ground truth that mitigates AutoFit and container discrepancies.
- **Mechanism**: The pipeline extracts static geometry/content from XML, then queries the Component Object Model after a layout pass to recover effective font metrics and tight text bounds—elements that raw XML may misrepresent due to dynamic rendering behaviors. This dual-source ground truth enables pixel-accurate alignment against VLM predictions via Hungarian matching with a blended cost (IoU + center/size distance + content similarity).
- **Core assumption**: The COM API faithfully reproduces the visual rendering that the VLM sees; rendering inconsistencies between the ground-truth pipeline and the rasterized slide would introduce systematic misalignment.
- **Evidence anchors**: [section] "Ground Truth Element geometry, content, and style are extracted from PowerPoint XML and post-layout rendering. We parse static XML and then query the COM (Component Object Model) API after a layout pass to recover effective font metrics and tight text bounds (mitigating AutoFit and container/tight-box discrepancies)." (Section 3)

### Mechanism 2
- **Claim**: Controlled perturbations with monotone severity knobs elicit a fidelity-consistency trade-off that varies by dimension: geometry/style are comparatively stable, while text quality shows higher sensitivity but lower internal consistency on finer scales.
- **Mechanism**: The framework applies parametric degradations to geometry (translation/scaling/reposition), text (character-level noise/drop/insert), and style (font family/size/color jitter) with severity s ∈ [0,1]. It then measures (1) POA_adj (adjacent proportion of agreement—internal consistency) and (2) fidelity via Spearman(severity, y*). The 5-point and 100-point scales are monotone reparameterizations (R² ∈ [0.85, 0.89]), but coarse scales improve consistency for geometry/style without fidelity gain, while fine scales increase text fidelity at the cost of consistency.
- **Core assumption**: The perturbation operators are perceptually monotone—higher s should produce visually worse slides in expectation—and the VLM evaluators’ judgments reflect genuine sensitivity rather than noise or priors.
- **Evidence anchors**: [section] "Within each model, an isotonic link maps 5-point scores to 100-point scores with high fidelity: R² ∈ [0.85,0.89] across models (p=0.001)... However, a monotone mapping does not imply identical behavior under controlled severity shifts." (Section 4)

### Mechanism 3
- **Claim**: Parse success is strongly conditioned on scene complexity; newer VLMs (o3, GPT-5 variants) maintain near-ceiling parseability across complexity bins, while older models (GPT-4.1, GPT-4o) degrade sharply beyond ~8 elements per slide.
- **Mechanism**: The evaluation separates parseability (schema-valid JSON) from extraction quality. For complex slides (≥16 elements), GPT-4.1 parse success drops to 32.8% and GPT-4o to 45.8%, while o3/GPT-5 remain at 99.7%–100%. This creates differential coverage in end-to-end metrics: older models have lower coverage (GPT-4o: 0.33, GPT-4.1: 0.54) vs. newer models (0.74–0.78), which biases pooled comparisons if not accounted for.
- **Core assumption**: Schema-invalid outputs represent parse failures rather than alternative valid encodings; the JSON schema is expressive enough to capture ground truth without forcing information loss.
- **Evidence anchors**: [section] "Parse success declines with slide complexity for GPT-4.1 (about 93% for simple slides with ≤8 elements, 72.1% for (8-16], 32.8% for (16-32], and 18.2% for ≥32 elements). GPT-4o follows a similar trend... In contrast, o3 and the GPT-5 variants remain effectively at ceiling across all bins (99.5%+)." (Section 4)

## Foundational Learning

- **Concept**: Hungarian matching for bipartite assignment
  - **Why needed here**: The framework must align predicted elements to ground-truth elements before computing F1, IoU, and content similarity. Without optimal assignment, false positives/negatives would be miscounted.
  - **Quick check question**: Given 5 ground-truth boxes and 7 predicted boxes, what is the time complexity of the Hungarian algorithm, and what happens to unmatched predictions?

- **Concept**: Severity-parameterized perturbations with monotonicity
  - **Why needed here**: The perturbation synthesis uses s ∈ [0,1] to control degradation magnitude. Understanding how probability/magnitude functions map s to noise scales is essential for interpreting POA_adj and calibration metrics.
  - **Quick check question**: If translation noise is σ_x(s) = (0.04 + 0.16s)·W and s=0.5 with W=960px, what is the standard deviation in pixels? Does this guarantee perceptual monotonicity?

- **Concept**: Kendall’s τ and Spearman’s ρ for ordinal correlation
  - **Why needed here**: Narrative ordering evaluation uses these rank correlation measures to assess whether VLMs can reconstruct shuffled slide sequences. Interpreting τ ∈ [0.04, 0.12] requires understanding what "near random" means.
  - **Quick check question**: For a 10-slide deck, what τ value would you expect from random guessing? What if the model correctly orders 7/10 slides but reverses 3 adjacent pairs?

## Architecture Onboarding

- **Component map**: Data curation (100 decks → 1,948 slides) → Ground-truth pipeline (XML + COM API → schema → PNG) → VLM inference (5 models → schema-validated JSON → N=3 runs) → Matching & scoring (Hungarian + blended cost → PRF1, IoU, Text, Style) → Perturbation synthesis (severity s ∈ [0,1] → transforms → 7,722 slides) → Narrative probe (shuffle → VLM reordering → Kendall’s τ, Spearman’s ρ)

- **Critical path**: Schema design must precede ground-truth extraction; changing fields requires re-extracting the entire corpus. Rendering consistency between GT pipeline and VLM input is essential—any DPI/theme/font substitution breaks alignment. Matching threshold τ must be tuned; too low discards valid matches, too high accepts false positives.

- **Design tradeoffs**: Parsed-only vs. end-to-end: Parsed-only excludes parse failures, showing best-case extraction; end-to-end includes failures in the denominator, showing real-world performance. Report both. 5-point vs. 100-point scale: 5-point improves consistency for geometry/style; 100-point increases text sensitivity. Use per-dimension scale selection. Schema simplicity vs. expressiveness: Simplified schema omits theme-embedded content (Figure 1C, blue dashed boxes). Trade completeness for verifiability.

- **Failure signatures**: Low coverage + high parsed-only F1: Parse failures are the bottleneck (e.g., GPT-4o: 0.33 e2e coverage). Improve prompt/schema, not extraction logic. High POA_adj but low fidelity: Evaluators are consistent but insensitive to severity changes. Coarsen scale or redesign criteria. High font-group accuracy but low font-family accuracy: VLMs recognize serif/sans/mono but not specific families (Figure 7: font group ≥0.98 for o3/GPT-5; font family max 0.42). This is a perception limit, not a matching issue.

- **First 3 experiments**:
  1. Reproduce parseability curve: Run all 5 VLMs on a 50-slide stratified sample across complexity bins (0-1, 1-2, 2-4, 4-8, 8-16, 16-32, 32+ elements). Confirm GPT-4o/4.1 degradation and o3/GPT-5 ceiling behavior.
  2. Perturbation calibration check: For a single slide, generate perturbations at all 11 severity levels for one axis (e.g., text). Compute POA_adj and MACE. Verify monotonicity assumption holds before running full evaluation.
  3. Inter-model agreement probe: For geometry/style/text, compute pairwise Spearman agreement across 3 models on 20 perturbed slides. Confirm text agreement is lowest (ρ ≈ 0.55 best pair) per Figure 3b. This establishes whether models are interchangeable as critics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can VLM evaluators be calibrated to effectively serve as critics in agentic pipelines given the observed fidelity-consistency trade-offs?
- **Basis in paper**: [explicit] The conclusion calls for "calibrated, critic-in-the-loop evaluators" to drive iterative refinement, noting current limits.
- **Why unresolved**: Models currently exhibit a trade-off where fine-grained text sensitivity reduces score consistency, complicating their use as reliable automated judges.
- **What evidence would resolve it**: A method that improves Probabilities of Adjacent Agreement (POA) for text without sacrificing fidelity to perturbation severity.

### Open Question 2
- **Question**: What specific mechanisms or training data are required to improve VLM performance on cross-slide narrative ordering beyond random chance?
- **Basis in paper**: [explicit] The authors identify "richer narrative probes" as necessary future work because models currently perform marginally better than random (Kendall’s τ < 0.12).
- **Why unresolved**: While single-slide extraction is competent, models fail to reason coherently across multiple slides in a deck.
- **What evidence would resolve it**: New architectures or prompting strategies achieving significantly higher Kendall’s τ and Spearman’s ρ on the deck ordering task.

### Open Question 3
- **Question**: Does VLM extraction performance on the public Zenodo10K dataset generalize to proprietary or professionally designed corporate presentations?
- **Basis in paper**: [explicit] The limitations section lists "public PPTX" as a constraint and lists "broader corpora" as a direction for future work.
- **Why unresolved**: Publicly available scientific decks may lack the visual complexity, branding density, or layout heterogeneity of corporate slides.
- **What evidence would resolve it**: Evaluation on a held-out set of high-density, professionally designed slides showing similar extraction F1 and geometry error rates.

## Limitations
- Model Identity and Prompt Stability: Specific Azure API model identifiers for "o3" and "GPT-5" are not disclosed, making exact behavioral reproduction impossible.
- Rendering-Truth Alignment: Ground-truth extraction relies on COM API rendering that may diverge from the rasterization shown to VLMs, potentially biasing alignment.
- Schema Expressiveness: The simplified schema omits theme-embedded content and grouped elements, potentially underestimating VLM capabilities on complex slides.

## Confidence
- **High**: Parseability trends (newer models > older models), overall end-to-end coverage gaps, narrative ordering (near-random τ), font-group vs. font-family accuracy split.
- **Medium**: Perturbation calibration results (POA_adj, MACE, Spearman), geometry/style vs. text sensitivity trade-off, font-family accuracy ceiling.
- **Low**: Exact fidelity values under perturbations, absolute matching F1 numbers (due to potential rendering/ground-truth misalignment), inter-model agreement magnitudes (ρ ≈ 0.55 best pair).

## Next Checks
1. **Schema-Failure Analysis**: Instrument the pipeline to log schema validation errors by element type and complexity bin. Confirm parse failures are due to expressiveness limits, not VLM hallucination.
2. **Rendering Consistency Audit**: Capture DPI, theme state, and font substitution logs from both COM API ground truth and VLM input generation. Quantify divergence sources.
3. **Perturbation Monotonicity Test**: For a stratified sample of slides, generate perturbations across all severity levels for each axis. Measure human perceptual monotonicity and compare against VLM judgments.