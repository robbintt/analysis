---
ver: rpa2
title: 'LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents'
arxiv_id: '2509.02241'
source_url: https://arxiv.org/abs/2509.02241
tags:
- arxiv
- legal
- prompt
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents

## Quick Facts
- arXiv ID: 2509.02241
- Source URL: https://arxiv.org/abs/2509.02241
- Authors: Strahinja Klem; Noura Al Moubayed
- Reference count: 40
- Primary result: 9% improvement over DeBERTa baseline on CUAD dataset

## Executive Summary
This paper addresses the challenge of extracting information from long legal documents using Large Language Models. The authors propose a three-stage pipeline that combines document chunking with overlapping segments, an optimized prompting strategy, and two candidate selection heuristics (Distribution-Based Localisation and Inverse Cardinality Weighting) to improve accuracy while reducing hallucination risks. The methodology demonstrates significant improvements over baseline models while maintaining computational efficiency.

## Method Summary
The methodology processes long legal documents through a three-stage pipeline: (1) Document chunking with augmentation - splitting documents into 1000-token chunks with 50% overlap to prevent context loss at boundaries; (2) Optimized prompting - using QWEN-2 7B via Ollama API with engineered prompts combining Persona, Domain, and "Does not exist" formatting; (3) Candidate selection heuristics - Distribution-Based Localisation (DBL) weights answers by document location probability, while Inverse Cardinality Weighting (ICW) clusters answers using GritLM embeddings via DBSCAN and weights rarer answers higher. The approach is evaluated on the CUAD dataset containing 510 American legal contracts.

## Key Results
- Achieves 9% improvement over DeBERTa baseline on CUAD dataset
- Demonstrates effectiveness of overlapping chunk augmentation in preventing context loss
- Shows Inverse Cardinality Weighting successfully identifies correct answers from hallucination noise
- Highlights divergence between automatic metrics and human evaluation for short legal answers

## Why This Works (Mechanism)

### Mechanism 1: Overlapping Chunk Augmentation
The system segments documents into 1000-token chunks with redundant overlaps to mitigate the "lost-in-the-middle" problem. This ensures no contextual bridge is dropped when key information spans arbitrary text boundaries. Core assumption: critical legal information often spans chunk boundaries. Break condition: If legal data consists of entirely self-contained atomic clauses, augmentation introduces unnecessary compute overhead.

### Mechanism 2: Inverse Cardinality Weighting (ICW)
The process generates multiple candidate answers from various chunks, clusters them using GritLM embeddings and DBSCAN, and weights answers inverse-proportionally to cluster size. Core assumption: hallucinations or incorrect summaries are high-frequency noise, while correct extractions are rare. Break condition: If the model hallucinates consistently in a specific, unique way, or if the correct answer appears in almost every chunk, the inverse weighting will suppress the correct answer.

### Mechanism 3: Distribution-Based Localisation (DBL)
The system analyzes training data to build a distribution of where specific answer types typically reside within a document, then weights candidates found in high-probability segments higher during inference. Core assumption: legal documents possess predictable structure correlating with specific information types. Break condition: If target documents have highly variable or randomized structures, the location prior becomes noise.

## Foundational Learning

- **Context Window vs. Attention Span**: The paper attempts to solve the "long document problem" via chunking because models like QWEN-2 cannot process full text at once. Does chunking solve the *memory* problem or the *attention* problem? (It solves the input size limit but creates an attention/aggregation problem).

- **Density-Based Clustering (DBSCAN)**: The ICW heuristic relies on DBSCAN to group answers. Understanding that it groups dense regions and treats sparse points as noise is key to seeing why "unique" answers are preserved. Why would K-Means clustering be less suitable here than DBSCAN for filtering out variable "generic" responses? (K-Means assumes spherical clusters and equal variance, while DBSCAN better handles variable density clusters and noise).

- **Prompt Engineering Patterns (Persona/Coercive)**: The methodology treats prompting as hyper-parameter search over patterns. The authors excluded "Chain of Thought" (CoT) prompting. Based on the text, was this due to cost or capability? (Cost - Section 3.3 notes they restricted the test set to questions effectively evaluated by a layperson due to lack of legal expertise).

## Architecture Onboarding

- **Component map**: Input (Raw Legal Document) -> Pre-processor (1000-token chunks + overlapping augmentation) -> Prompt Engine (Persona + Domain + "Does not exist" template) -> Inference (QWEN-2 generates per-chunk answers) -> Heuristic Layer (DBL weights by location, ICW clusters embeddings via DBSCAN) -> Selector (highest-weighted answer).

- **Critical path**: The Heuristic Layer (specifically ICW clustering) is the most fragile component. If DBSCAN threshold is wrong, valid answers get grouped with noise and suppressed.

- **Design tradeoffs**: Chunk Size - 1000 tokens chosen empirically; smaller chunks increase cost but improve accuracy per chunk, larger chunks risk exceeding context or diluting attention. Augmentation - increases redundancy and safety but doubles/triples inference cost for boundary regions.

- **Failure signatures**: "False Failures" - system claims "Does not exist" when answer exists, usually due to literal interpretation of questions. Metric Divergence - high ROUGE/METEOR scores do not correlate with Human Evaluation for short legal answers (syntax differences penalize generative outputs unfairly).

- **First 3 experiments**:
  1. Chunk Size Ablation - run pipeline on test set with chunk sizes of 500, 1000, and 1500 to verify 1000-token empirical optimum.
  2. Heuristic Stress Test - feed system a document where answer appears in every chunk to see if ICW suppresses it.
  3. Metric Correlation - compare human evaluation scores vs. automatic metrics (GritLM Cosine Similarity) to quantify "coarseness" the authors warn about.

## Open Questions the Paper Calls Out

### Open Question 1
How can specialized automatic evaluation metrics be developed to accurately assess generative legal QA outputs of variable lengths? The authors note current metrics "diverge substantially" from human judgment for short texts because they rely on token overlap, which penalizes syntactic variations common in generative answers.

### Open Question 2
Does the proposed prompting methodology maintain performance when applied to complex legal questions requiring domain expertise? The methodology was primarily verified on simpler extraction tasks (e.g., dates, names), leaving its efficacy on complex reasoning tasks unproven.

### Open Question 3
Is the Distribution-Based Localisation (DBL) heuristic robust when applied to legal documents with non-standard or irregular structures? If documents vary significantly in layout rather than just length, the positional priors used to weight specific chunks may become invalid, potentially degrading retrieval accuracy.

## Limitations

- ICW sensitivity to model-specific hallucination patterns - if QWEN-2 exhibits consistent hallucination behaviors, these could form dense DBSCAN clusters incorrectly weighted as high-confidence answers.
- DBL assumption of universal legal document structure may not generalize beyond American contracts to cross-jurisdictional or non-standard contract types.
- 1000-token chunk size was empirically determined but may represent overfitting to CUAD dataset's document characteristics.

## Confidence

- **High Confidence**: Overlapping chunk augmentation mechanism is technically sound and well-validated through empirical chunking experiment.
- **Medium Confidence**: Inverse Cardinality Weighting approach has logical foundation but requires empirical validation across different hallucination patterns and document types.
- **Low Confidence**: Distribution-Based Localisation assumes universal legal document structure that may not generalize beyond American contracts.

## Next Checks

1. **Hallucination Pattern Stress Test**: Create synthetic legal documents where the model consistently hallucinates specific information types, then verify whether ICW correctly identifies and suppresses these patterns versus genuine answers.

2. **Cross-Jurisdictional Structure Validation**: Apply the methodology to legal documents from multiple jurisdictions (e.g., EU contracts, UK agreements) to test whether DBL's structural assumptions remain valid.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary DBSCAN parameters (eps, min_samples) and prompt template variations to quantify the stability of the final answer selection across different parameter settings.