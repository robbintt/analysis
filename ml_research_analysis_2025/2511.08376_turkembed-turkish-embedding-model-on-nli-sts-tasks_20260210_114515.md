---
ver: rpa2
title: 'TurkEmbed: Turkish Embedding Model on NLI & STS Tasks'
arxiv_id: '2511.08376'
source_url: https://arxiv.org/abs/2511.08376
tags:
- turkish
- turkembed
- tasks
- embedding
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TurkEmbed, a novel Turkish embedding model
  that outperforms existing approaches on Natural Language Inference (NLI) and Semantic
  Textual Similarity (STS) tasks. Unlike current models that rely on machine-translated
  datasets, TurkEmbed uses diverse training data and advanced techniques including
  matryoshka representation learning to achieve more robust embeddings.
---

# TurkEmbed: Turkish Embedding Model on NLI & STS Tasks

## Quick Facts
- arXiv ID: 2511.08376
- Source URL: https://arxiv.org/abs/2511.08376
- Reference count: 40
- TurkEmbed achieves state-of-the-art performance with Pearson correlation of 0.845 and Spearman correlation of 0.853 on STSb-TR

## Executive Summary
TurkEmbed introduces a novel Turkish embedding model that outperforms existing approaches on Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. The model uses a two-stage sequential fine-tuning process: first on All-NLI-TR dataset with Multiple Negatives Ranking Loss, then on STSb-TR with CoSENT Loss. By employing matryoshka representation learning and task-aligned loss functions, TurkEmbed achieves superior performance while providing efficient inference speeds despite being larger than comparable models.

## Method Summary
TurkEmbed employs a two-stage sequential fine-tuning approach using gte-multilingual-base (305M parameters) as the base model. Stage 1 fine-tunes on All-NLI-TR dataset (482,091 samples) using Multiple Negatives Ranking Loss and Matryoshka Loss across dimensions 64-768. Stage 2 fine-tunes on STSb-TR (5,749 samples) using CoSENT Loss and Matryoshka Loss. The model uses max sequence length of 8192 and produces embeddings at configurable dimensions. Training assumes batch size 32 for Stage 1, batch size 16 for Stage 2, learning rate 2e-5, and warmup ratio 0.1.

## Key Results
- Achieves Pearson correlation of 0.845 and Spearman correlation of 0.853 on STSb-TR, surpassing current best model Emrecan by 1-4%
- Demonstrates strong generalization capabilities on STS22-crosslingual dataset
- Provides efficient inference speeds (1.23x slower than Emrecan in FP16) despite being larger than comparable models
- Outperforms models relying on machine-translated datasets through diverse training data and advanced techniques

## Why This Works (Mechanism)

### Mechanism 1
Sequential fine-tuning on NLI followed by STS produces more robust Turkish embeddings than single-stage training. Stage 1 forces the model to learn categorical semantic distinctions (entailment, contradiction, neutral), building structural knowledge about sentence relationships. Stage 2 refines these representations with continuous similarity scores, calibrating the embedding space for fine-grained semantic similarity while retaining distinctions learned in Stage 1. Core assumption: semantic knowledge acquired during NLI training transfers effectively to STS tasks and is not erased during the second fine-tuning stage.

### Mechanism 2
Matryoshka representation learning enables a single model to produce useful embeddings across multiple dimensions without separate retraining. During training, the loss is computed jointly across nested embedding dimensions (64, 128, 256, 512, 768), forcing early dimensions to capture the most salient semantic information. This allows truncated embeddings to remain functional for similarity tasks. Core assumption: semantic content can be progressively compressed into lower dimensions without catastrophic information loss for Turkish NLP tasks.

### Mechanism 3
Task-aligned loss functions (Multiple Negatives Ranking Loss for NLI, CoSENT Loss for STS) improve training-inference consistency compared to generic losses. MNRL leverages in-batch negatives for efficient contrastive learning suited to categorical NLI pairs. CoSENT Loss directly optimizes cosine similarity, aligning the training objective with the inference metric used for STS evaluation, reducing the train-test gap. Core assumption: aligning the loss function with the evaluation metric produces better-calibrated similarity scores than indirect optimization.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed here: Multiple Negatives Ranking Loss is a contrastive method; understanding how positive/negative pairs shape the embedding space is essential
  - Quick check question: Can you explain why treating in-batch samples as negatives improves training efficiency?

- **Concept: Catastrophic Forgetting**
  - Why needed here: Sequential training risks overwriting Stage 1 knowledge; the paper explicitly addresses this concern
  - Quick check question: What would happen if STS fine-tuning completely erased NLI-learned distinctions?

- **Concept: Training-Inference Consistency**
  - Why needed here: CoSENT Loss is chosen specifically because it matches the cosine similarity metric used at inference
  - Quick check question: Why might a mismatch between training loss and evaluation metric degrade performance?

## Architecture Onboarding

- **Component map**: gte-multilingual-base -> Stage 1 (All-NLI-TR + MNRL + Matryoshka) -> Stage 2 (STSb-TR + CoSENT + Matryoshka) -> Configurable embeddings (64-768)

- **Critical path**: NLI pre-training quality determines semantic structure → STS fine-tuning calibrates similarity scores → Dimension selection at deployment time affects speed-accuracy tradeoff

- **Design tradeoffs**: Model size (305M) vs inference speed (1.23x slower than Emrecan in FP16) → Embedding dimension vs storage/retrieval cost → Sequential training time vs potential forgetting risk

- **Failure signatures**: Significant accuracy drop between All-NLI-TR and post-STS evaluation (indicates forgetting) → Non-monotonic performance across Matryoshka dimensions (indicates dimension training failure) → Poor correlation between Pearson and Spearman scores (indicates miscalibrated similarity scale)

- **First 3 experiments**: 1) Reproduce Table 2 (STSb-TR performance) to validate the two-stage pipeline 2) Ablate Matryoshka Loss by training with fixed 768-dimension embeddings and compare performance 3) Evaluate truncated embeddings (64, 128, 256) on STS22-crosslingual to test dimension robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Sequential fine-tuning approach's superiority over simultaneous multi-task training is demonstrated but not thoroughly validated through ablation studies
- Dataset creation process relies on machine translation of existing English datasets, which may introduce subtle artifacts
- Paper does not provide quantitative evidence comparing sequential training against alternative training strategies

## Confidence

- **High confidence**: STSb-TR benchmark performance (Pearson 0.845, Spearman 0.853) - direct experimental results with clear evaluation methodology
- **Medium confidence**: Generalization to STS22-crosslingual dataset - improvement over Emrecan (1-4%) is demonstrated but dataset is relatively small
- **Medium confidence**: Matryoshka representation learning benefits - framework is described but specific ablation studies on dimension-specific performance are limited

## Next Checks
1. Conduct an ablation study comparing TurkEmbed's sequential training approach against simultaneous multi-task training and single-stage training to quantify the exact contribution of the two-stage methodology
2. Perform controlled experiments using human-translated Turkish datasets (where available) versus machine-translated datasets to isolate potential translation artifacts affecting model performance
3. Evaluate TurkEmbed's performance across all Matryoshka dimensions (64, 128, 256, 512, 768) on the STS22-crosslingual dataset to verify consistent dimension robustness across different task types