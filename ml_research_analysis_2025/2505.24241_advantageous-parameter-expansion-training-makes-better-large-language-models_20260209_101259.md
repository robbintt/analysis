---
ver: rpa2
title: Advantageous Parameter Expansion Training Makes Better Large Language Models
arxiv_id: '2505.24241'
source_url: https://arxiv.org/abs/2505.24241
tags:
- parameters
- training
- apex
- wang
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APEX introduces a method to improve model training by identifying
  and expanding advantageous parameters rather than simply increasing total parameter
  count. The method uses activation-based ranking to identify high-contribution parameters
  in MHA heads and FFN channels, then progressively expands these parameters into
  lower-contribution parameter spaces using Monarch matrix operators.
---

# Advantageous Parameter Expansion Training Makes Better Large Language Models

## Quick Facts
- arXiv ID: 2505.24241
- Source URL: https://arxiv.org/abs/2505.24241
- Reference count: 40
- One-line primary result: APEX identifies and expands advantageous parameters (high-activation MHA heads/FFN channels) using Monarch matrix operators, achieving better performance with fewer trainable parameters or less training data than baseline methods.

## Executive Summary
APEX introduces a method to improve model training by identifying and expanding advantageous parameters rather than simply increasing total parameter count. The method uses activation-based ranking to identify high-contribution parameters in MHA heads and FFN channels, then progressively expands these parameters into lower-contribution parameter spaces using Monarch matrix operators. The approach is evaluated in both instruction tuning and continued pre-training scenarios, demonstrating strong performance gains while using fewer trainable parameters or less training data.

## Method Summary
APEX operates through a stage-wise training procedure that first assesses parameter importance using activation magnitudes, then expands advantageous parameters into disadvantageous spaces via Monarch matrix operators. During each stage, the method computes Frobenius norms of activations for MHA heads and FFN channels, ranks them to select advantageous (high-activation) and disadvantageous (low-activation) parameter sets, and initializes zero matrices to expand the advantageous parameters into the disadvantageous space. After training the expansion operators, they are fused back into the original weights, and the process repeats for subsequent stages. This approach allows APEX to increase the proportion of high-contribution parameters without scaling total parameter count.

## Key Results
- APEX achieves superior performance on instruction tuning tasks (MMLU, GSM8K, BBH, TyDiQA, TruthfulQA, HumanEval) compared to baselines while using fewer trainable parameters
- In continued pre-training, APEX improves perplexity and downstream task performance with reduced training data requirements
- The method demonstrates effectiveness across different model sizes, including LLaMA2-7B and LLaMA3.1-8B architectures

## Why This Works (Mechanism)

### Mechanism 1: Activation-Based Parameter Expansion
APEX improves training efficiency by identifying and expanding high-contribution parameters into low-contribution parameter spaces, increasing the proportion of advantageous parameters without scaling total parameter count. The method computes Frobenius norms of activations for each MHA head and FFN channel during forward passes, ranks them via a Top-K/Min-K relative scoring system, and progressively expands advantageous parameters into disadvantageous spaces using learnable transformation matrices. The core assumption is that a subset of parameters ("advantageous") contributes disproportionately to performance, and increasing their proportion improves training effectiveness. This builds on Lottery Ticket Hypothesis observations.

### Mechanism 2: Monarch Matrix Operators with Zero Initialization
Expansion operators implemented as Monarch matrices provide efficient, stable parameter transformation while preserving original model architecture through fusion. Transformation matrix M is factorized into Monarch structure (products of diagonal/block-diagonal matrices), reducing complexity from O(d^4) to O(d^2). M is zero-initialized via R=0 matrices; operators are fused back into W at stage end, maintaining architecture. The core assumption is that Monarch decomposition can represent necessary transformations without significant information loss, and zero initialization ensures training stability from the original checkpoint.

### Mechanism 3: Effective Rank Enhancement
APEX theoretically improves weight matrix effective rank, enhancing parameter space utilization. Expanding high-rank advantageous matrices (WP) into low-rank disadvantageous spaces (WN) increases concatenated matrix rank. Training optimizes transformations to find orthogonal subspace components, amplifying effective rank (rank(Ŵ) ≥ k + max(ρ-δ, s)). The core assumption is that WP is nearly full-rank and WN has very low rank, with increasing effective rank correlating with improved task performance.

## Foundational Learning

- **Concept: Activation-based Parameter Importance**
  - Why needed here: Core to APEX's method—distinguishing advantageous/disadvantageous parameters via activation magnitudes
  - Quick check question: Given a forward pass through LLaMA2-7B layer 15, how would APEX determine which MHA heads are "advantageous"?

- **Concept: Monarch Matrix Decomposition**
  - Why needed here: Enables efficient O(d^2) expansion transformations; critical for practical implementation
  - Quick check question: How does Monarch decomposition balance representational capacity against computational cost compared to full-rank transformations?

- **Concept: Stage-wise Training with Assessment**
  - Why needed here: APEX iteratively re-selects parameter sets and re-initializes operators across stages
  - Quick check question: What triggers a new stage in APEX, and what three operations occur at stage boundaries?

## Architecture Onboarding

- **Component map:** Assessment Module (computes s_MHA/s_FFN scores via Top-K/Min-K activation ranking) -> Expansion Operators (Monarch-structured transformation matrices γ_MHA, γ_FFN) -> Fusion Module (merges trained operators back into original weights W_V, W_O, W_U, W_G, W_D)

- **Critical path:** Forward pass → Activation collection → Assessment (identify d_P/d_N sets) → Operator initialization (zero R, random D) → Training (optimize M) → Fusion (W := W_P·M + W_N)

- **Design tradeoffs:** Threshold K (18.75% optimal): Higher K expands more parameters but increases learning complexity; Stage count T (3-4 stages): More stages improve performance with diminishing returns; Monarch block size: Larger blocks increase capacity but reduce efficiency gains

- **Failure signatures:** Training instability: Improper zero-initialization causes gradient explosion at stage start; No improvement: Flawed assessment (outlier-dominated activations) targets wrong parameters; Performance collapse: Expansion overwrites critical knowledge in disadvantageous parameters

- **First 3 experiments:**
  1. Implement APEX forward/backward pass on TinyLLaMA-110M to validate operator initialization, gradient flow, and fusion mechanics
  2. Ablate assessment: Compare APEX with random parameter selection vs. activation-based selection on LLaMA2-7B instruction tuning
  3. Vary stages: Train with T=1, T=2, T=3 on identical token budgets; plot perplexity curves to verify convergence acceleration and diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can APEX be adapted to support pre-training from scratch without requiring an initial conventional training warm-up phase?
- Basis in paper: The authors explicitly state in the Limitations section (C.3) that "for scenarios where pre-training is conducted entirely from scratch, it is necessary to first conduct conventional training for a period," and identify extending APEX to this scenario as future work.
- Why unresolved: The method currently relies on activation statistics to identify advantageous parameters, which are undefined or noisy at random initialization.
- What evidence would resolve it: A modified APEX variant that achieves convergence and performance parity with or superiority to conventional training when initialized with random weights.

### Open Question 2
- Question: Can advantageous parameter expansion be effectively generalized to non-Transformer architectures, such as Mixture-of-Experts (MoE) or State Space Models (SSMs)?
- Basis in paper: The methodology is defined strictly around expanding parameters in Multi-Head Attention (MHA) heads and Feed-Forward Network (FFN) channels.
- Why unresolved: MoE models possess isolated expert parameters and routing mechanisms, while SSMs lack the distinct head/channel structure, making the definition of "advantageous space" ambiguous for these architectures.
- What evidence would resolve it: Empirical results applying APEX to MoE or SSM architectures (e.g., Mamba) demonstrating parameter efficiency gains comparable to those seen in standard Transformers.

### Open Question 3
- Question: Does the theoretical increase in effective rank degrade when the assumption of initial subspace orthogonality is violated in deeper or more complex layers?
- Basis in paper: The theoretical analysis in Section 3.4 relies on Assumption 1 (dim(S_P ∩ S_N) ≤ δ where δ is small), which assumes advantageous and disadvantageous parameter spaces are nearly orthogonal.
- Why unresolved: While weights are assumed orthogonal due to random initialization, the paper does not verify if this orthogonality persists or degrades in specific layers during the multi-stage process, potentially affecting the rank lower bound.
- What evidence would resolve it: An analysis measuring the subspace overlap (δ) across different layers and stages, correlated with the observed effective rank and performance improvements.

## Limitations

- APEX requires an initial conventional training warm-up phase when applied to pre-training from scratch, limiting its applicability to scenarios requiring complete random initialization
- The activation-based assessment mechanism lacks rigorous validation through ablation studies comparing activation magnitudes to other importance metrics
- The Monarch matrix decomposition's effectiveness for this specific expansion task remains unproven, with potential representational limitations for complex transformations

## Confidence

- **High Confidence:** The core concept that not all parameters contribute equally to model performance is well-supported by the Lottery Ticket Hypothesis and related pruning literature. The Monarch matrix implementation details are technically sound and computationally efficient.
- **Medium Confidence:** The activation-based assessment method is plausible but lacks comprehensive ablation studies. The performance improvements on instruction tuning and continued pre-training are demonstrated but could benefit from more extensive comparison baselines.
- **Low Confidence:** The theoretical effective rank analysis is mathematically sound but practically unverified. The claim that Monarch decomposition is optimal for this expansion task lacks empirical validation against alternatives.

## Next Checks

1. **Activation Ablation Study:** Implement APEX with three different parameter selection strategies: (a) activation-based Top-K/Min-K as described, (b) random selection, and (c) importance-based selection using weight magnitude or gradient statistics. Compare performance across identical training budgets to isolate the true value of the activation assessment mechanism.

2. **Monarch Decomposition Stress Test:** Create synthetic test cases where expansion requires: (a) full-rank transformations beyond Monarch capacity, (b) rank-deficient but non-trivial transformations, and (c) near-identity transformations. Measure approximation error and training performance to determine Monarch's representational limits.

3. **Stage Boundary Analysis:** Implement APEX with fine-grained logging at stage transitions. Measure: (a) parameter value changes during operator training, (b) performance immediately before/after fusion, and (c) gradient norms and learning dynamics across stages. This will reveal whether expansion causes knowledge disruption and validate the training stability claims.