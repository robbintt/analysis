---
ver: rpa2
title: 'See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion
  Rate Prediction'
arxiv_id: '2508.15217'
source_url: https://arxiv.org/abs/2508.15217
tags:
- attribution
- learning
- conversion
- prediction
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of conversion rate (CVR) prediction
  in online advertising, where conventional models are limited to training on labels
  from a single attribution mechanism, missing valuable signals from alternative attribution
  perspectives. To overcome this limitation, the authors propose a Multi-Attribution
  Learning (MAL) framework that integrates signals from multiple attribution mechanisms
  (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution)
  to enhance CVR prediction.
---

# See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction

## Quick Facts
- arXiv ID: 2508.15217
- Source URL: https://arxiv.org/abs/2508.15217
- Authors: Sishuo Chen; Zhangming Chan; Xiang-Rong Sheng; Lei Zhang; Sheng Chen; Chenghuan Hou; Han Zhu; Jian Xu; Bo Zheng
- Reference count: 40
- Primary result: Multi-Attribution Learning (MAL) framework improves CVR prediction by integrating multiple attribution mechanisms, achieving +0.51% GAUC improvement and +2.6% ROI increase

## Executive Summary
This paper addresses the challenge of conversion rate prediction in online advertising, where conventional models are limited to training on labels from a single attribution mechanism, missing valuable signals from alternative attribution perspectives. To overcome this limitation, the authors propose a Multi-Attribution Learning (MAL) framework that integrates signals from multiple attribution mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution) to enhance CVR prediction. MAL consists of two core components: the Attribution Knowledge Aggregator (AKA), which extracts knowledge from diverse attribution labels, and the Primary Target Predictor (PTP), which leverages this knowledge to improve predictions for the system-optimized attribution mechanism. Additionally, the authors introduce a novel training strategy called Cartesian-based Auxiliary Training (CAT) that generates enriched supervision signals by leveraging the Cartesian product of attribution label combinations.

## Method Summary
The Multi-Attribution Learning (MAL) framework addresses the limitations of single-attribution learning in conversion rate prediction by integrating multiple attribution mechanisms. The framework comprises two main components: the Attribution Knowledge Aggregator (AKA), which extracts knowledge from diverse attribution labels, and the Primary Target Predictor (PTP), which leverages this knowledge to improve predictions for the system-optimized attribution mechanism. A novel Cartesian-based Auxiliary Training (CAT) strategy generates enriched supervision signals by leveraging the Cartesian product of attribution label combinations. The approach was validated on real-world data from Taobao's display advertising system, demonstrating significant improvements in both offline metrics (GAUC) and online performance (ROI).

## Key Results
- MAL achieves a +0.51% GAUC improvement over single-attribution learning baselines in offline metrics
- Online A/B tests demonstrate a +2.6% increase in ROI
- The framework effectively integrates multiple attribution mechanisms including First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution

## Why This Works (Mechanism)
The framework works by capturing complementary information from different attribution mechanisms that reflect various user journey perspectives. The Attribution Knowledge Aggregator extracts patterns and signals from multiple attribution labels, while the Primary Target Predictor uses this enriched knowledge to make more accurate predictions for the primary attribution mechanism. The Cartesian-based Auxiliary Training strategy further enhances learning by creating diverse supervision signals through combinations of attribution labels, allowing the model to learn richer representations that generalize better to the primary prediction task.

## Foundational Learning
- **Multi-touch attribution**: Different methods (First-Click, Last-Click, Linear, Data-Driven) assign conversion credit differently across touchpoints; needed to capture diverse user journey perspectives; quick check: compare how each method distributes credit for a given conversion path
- **Conversion rate prediction**: Core task of estimating probability that a user will convert after seeing an ad; needed as the primary prediction target; quick check: evaluate prediction accuracy on held-out test data
- **Knowledge aggregation**: Combining information from multiple sources to create richer representations; needed to leverage complementary signals from different attribution mechanisms; quick check: measure information gain from aggregation versus single source
- **Cartesian product in training**: Creating combinations of labels for auxiliary training; needed to generate diverse supervision signals; quick check: analyze coverage and diversity of generated combinations

## Architecture Onboarding

**Component Map**: User Features -> AKA -> PTP -> CVR Prediction
                      Attribution Labels -> AKA -> PTP

**Critical Path**: User Features and Attribution Labels flow through AKA to extract knowledge, then PTP uses this knowledge with user features to produce final CVR predictions for the primary attribution mechanism.

**Design Tradeoffs**: The framework trades increased model complexity and training time (due to CAT strategy) for improved prediction accuracy by leveraging multiple attribution perspectives. The choice of attribution mechanisms to include balances comprehensiveness against computational overhead.

**Failure Signatures**: Poor performance may manifest when attribution labels are sparse or noisy, when user journeys are too short to benefit from multi-touch perspectives, or when the Cartesian-based Auxiliary Training creates overly complex supervision signals that confuse the model.

**First Experiments**:
1. Baseline comparison: Single-attribution learning vs. MAL with all attribution mechanisms
2. Ablation study: MAL with individual attribution mechanisms removed to assess contribution
3. CAT strategy analysis: Compare performance with and without Cartesian-based Auxiliary Training

## Open Questions the Paper Calls Out
None

## Limitations
- The experimental setup relies heavily on proprietary data from Taobao's display advertising system, limiting external validation and reproducibility
- The paper does not thoroughly discuss potential biases introduced by different attribution mechanisms or how these might affect the aggregated knowledge
- Computational complexity of the Cartesian-based Auxiliary Training strategy is mentioned but not thoroughly analyzed, particularly regarding scalability to systems with many attribution mechanisms

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Conceptual framework soundness | High |
| Quantitative improvements (GAUC +0.51%, ROI +2.6%) | Medium |
| Effectiveness of CAT strategy | Medium |

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the Attribution Knowledge Aggregator and Primary Target Predictor components
2. Perform statistical significance testing on the reported GAUC and ROI improvements to establish confidence levels
3. Test the framework on publicly available advertising datasets to verify generalizability beyond the proprietary Taobao data