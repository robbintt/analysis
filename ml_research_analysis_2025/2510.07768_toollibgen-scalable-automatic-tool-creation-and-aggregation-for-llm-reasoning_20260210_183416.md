---
ver: rpa2
title: 'ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning'
arxiv_id: '2510.07768'
source_url: https://arxiv.org/abs/2510.07768
tags:
- tools
- tool
- reasoning
- code
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling tool-augmented reasoning
  by organizing an ever-growing collection of domain-specific tools. The proposed
  approach refactors a fragmented set of question-specific tools into a structured
  library using hierarchical clustering and a multi-agent framework.
---

# ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning

## Quick Facts
- arXiv ID: 2510.07768
- Source URL: https://arxiv.org/abs/2510.07768
- Reference count: 40
- This paper presents a framework for automatically creating and organizing a scalable library of tools for LLM reasoning, improving retrieval accuracy and task performance.

## Executive Summary
This paper addresses the challenge of scaling tool-augmented reasoning by organizing an ever-growing collection of domain-specific tools. The proposed approach refactors a fragmented set of question-specific tools into a structured library using hierarchical clustering and a multi-agent framework. A code agent designs and implements aggregated tools as cohesive classes, while a reviewing agent validates functionality against the original tools. Experiments across science, math, and medical domains show that this structured library significantly improves retrieval accuracy and overall reasoning performance, with accuracy gains of 5-10% on seen tasks and over 3% on unseen ones. The method also demonstrates strong scalability as the number of tools grows.

## Method Summary
The paper proposes a multi-stage pipeline for scalable tool creation and aggregation. First, question-specific tools are extracted from QA pairs with CoT traces and validated using an LLM solver. Next, these tools are organized into a hierarchical tree structure using LLM-based clustering with depth ≤ 4. Then, a multi-agent system performs iterative aggregation: a Code Agent designs and implements tools as classes, while a Reviewing Agent validates their functionality against the original questions, with multiple rounds of refinement. Finally, at inference, tools are retrieved using embedding-based k-NN search over the aggregated library, and an LLM invokes the retrieved tools to solve new questions.

## Key Results
- Hierarchical clustering improves retrieval accuracy by reducing semantic ambiguity in the search space
- Multi-agent iterative refactoring preserves functional coverage that single-pass aggregation loses (71.9% vs 64.8% accuracy)
- Object-oriented abstraction compresses tool count while preserving completeness (175k → 8.9k tools in math domain)
- Accuracy gains of 5-10% on seen tasks and over 3% on unseen ones compared to fragmented toolsets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical clustering improves tool retrieval accuracy by reducing semantic ambiguity in the search space.
- **Mechanism:** Tools are grouped into semantically coherent clusters using LLM-based tree-structured organization (depth ≤ 4). This limits retrieval to functionally related subsets, reducing the probability of retrieving "relevant but unhelpful" tools (e.g., retrieving a binomial coefficient function when you need binomial probability).
- **Core assumption:** LLMs can reliably cluster tools by functional semantics; cluster coherence translates to better retrieval boundaries.
- **Evidence anchors:**
  - [abstract]: "retrieval challenges, including an expanding search space and ambiguity between function-related tools"
  - [section 3.3]: Retrieval accuracy declines sharply for fragmented tools as count increases; aggregated library maintains high accuracy (Figure 3, left)
  - [corpus]: Weak direct corpus support; related work (RefTool, SynthTools) addresses tool creation but not clustering-based retrieval optimization
- **Break condition:** If clusters are too coarse (merging unrelated domains) or too fine (tools still sparse), retrieval gains diminish; clustering quality depends on LLM's domain understanding.

### Mechanism 2
- **Claim:** Multi-agent iterative refactoring preserves functional coverage that single-pass aggregation loses.
- **Mechanism:** A Code Agent proposes aggregated implementations; a Reviewing Agent tests them against source questions using LLM_solver. Failures generate targeted feedback for revision. This cycle continues until coverage is verified or iteration limits are reached.
- **Core assumption:** The Reviewing Agent can reliably detect missing functionality via solver trajectories; feedback is actionable.
- **Evidence anchors:**
  - [section 2.4]: "a naive, single-pass refactoring by a single LLM frequently overlooks critical details"
  - [section 3.3, ablation]: Multi-round aggregation achieves 71.9% accuracy vs. 64.8% single-round (Figure 4a)
  - [corpus]: LLM Library Learning Fails (LEGO-Prover) highlights fragility in automated library creation—supports need for validation loops
- **Break condition:** If iteration cost (time, compute) exceeds value; if Reviewing Agent feedback becomes circular or low-signal.

### Mechanism 3
- **Claim:** Object-oriented abstraction (classes + facade functions) compresses tool count while preserving completeness.
- **Mechanism:** Question-specific tools are refactored into classes with shared internal logic and unified public interfaces. This exposes one retrievable entry point for multiple related operations (e.g., PolynomialAnalyzer class handles tangent slope, root multiplicity, etc.).
- **Core assumption:** The LLM can identify and correctly abstract shared logic without introducing regressions.
- **Evidence anchors:**
  - [section 1, Figure 1]: Three discrete tools consolidated into one class and one facade function
  - [Table 2]: 175k math-specific tools → 8.9k library tools; 48k science tools → 3.1k library tools
  - [corpus]: No direct corpus validation; compression ratio and functional preservation are paper-internal claims
- **Break condition:** Over-abstraction can create overly complex interfaces that LLMs struggle to invoke correctly; under-abstraction yields marginal compression.

## Foundational Learning

- **Concept: Tool-Augmented Reasoning**
  - **Why needed here:** The paper's entire framework assumes LLMs perform better when external deterministic tools (Python functions) handle computation, while the LLM focuses on reasoning orchestration.
  - **Quick check question:** Can you explain why a tool that computes binomial probability might improve an LLM's answer quality compared to pure CoT for a dice-rolling question?

- **Concept: Multi-Agent Systems with Feedback Loops**
  - **Why needed here:** ToolLibGen's aggregation phase relies on two agents (Code, Reviewing) iterating until quality criteria are met. Understanding role separation and feedback-driven refinement is critical.
  - **Quick check question:** What failure mode would occur if the Reviewing Agent and Code Agent were merged into a single agent with both capabilities?

- **Concept: Semantic Retrieval (Embedding + k-NN)**
  - **Why needed here:** At inference, tools are retrieved via embedding similarity. The paper's improvements hinge on better retrieval precision, not better tool implementation alone.
  - **Quick check question:** Why might semantically similar but functionally distinct tools (e.g., "get root" for quadratic vs. trigonometric equations) cause retrieval errors in a large, unclustered toolset?

## Architecture Onboarding

- **Component map:** Tool Creation -> Clustering -> Aggregation -> Inference
- **Critical path:** The multi-agent validation loop (Aggregation → Review → Refine) is the highest-leverage component. Without it, single-pass aggregation drops accuracy by ~7 percentage points (ablation data).
- **Design tradeoffs:**
  - **Clustering method:** LLM-based vs. K-means — Paper reports 95% vs. 72% coherence (manual eval), but LLM clustering is slower and costlier.
  - **Iteration depth:** More rounds improve coverage but increase latency and cost. Paper caps at 3 rounds.
  - **Abstraction granularity:** Facade functions with many optional parameters vs. multiple narrower functions — Paper chooses fewer functions with optional params, but this may increase invocation complexity.
- **Failure signatures:**
  - **Analysis Error (44%):** LLM ignores tool output, defaults to internal reasoning — indicates trust/calibration issues
  - **Retrieving Error (28%):** Correct tool not in top-k — suggests embedding/query mismatch
  - **Parameter Error (12%):** Wrong arguments passed — interface complexity or documentation gaps
  - **Selection Error (16%):** Correct tool retrieved but not selected — ranking or confidence issues
- **First 3 experiments:**
  1. **Baseline retrieval comparison:** Run ToolLibGen vs. Fragmented Toolset vs. Clustered Toolset (no aggregation) on a held-out subset. Measure retrieval accuracy and downstream task accuracy. Expect: FT < CT < ToolLibGen.
  2. **Ablation on iteration rounds:** Vary max checking turns (1, 2, 3, 5) in aggregation phase. Plot accuracy vs. cost. Identify point of diminishing returns.
  3. **Error-type intervention:** Target the dominant failure mode (Analysis Error). Modify system prompt to emphasize tool-output trust or add a "tool result integration" step. Measure reduction in Analysis Error rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning (SFT) with specific hard negative examples significantly improve an LLM’s ability to generate precise search queries for tool retrieval?
- Basis in paper: [explicit] The authors note that while SFT improved accuracy slightly, gains were marginal because the model struggled to learn nuances; they explicitly state future work could benefit from "introducing hard negative examples to help the model better distinguish the nuanced difference between queries."
- Why unresolved: The current study only used successful retrieval trajectories for SFT, lacking the contrastive examples needed to teach the model to differentiate between superficially similar but functionally distinct queries.
- What evidence would resolve it: A comparative experiment showing that training with hard negatives (tools that are retrieved incorrectly in the baseline) yields statistically significant accuracy gains over training with positive-only examples.

### Open Question 2
- Question: How can tool-augmented systems mitigate "Analysis Error," where an LLM fails to incorporate valid tool outputs into its reasoning chain?
- Basis in paper: [inferred] The error analysis reveals that the largest source of failure (44.0%) is "Analysis Error," where the LLM over-relies on its own beliefs rather than the tool's provided analysis.
- Why unresolved: The paper focuses on organizing and creating tools but does not propose a mechanism to force or incentivize the LLM to trust or integrate the external analysis provided by the tools it calls.
- What evidence would resolve it: A modified reasoning framework or prompt strategy that demonstrably lowers the rate of Analysis Error (e.g., by enforcing reflection on tool outputs) without increasing other error types.

### Open Question 3
- Question: Does a co-evolutionary framework, where tool creation and tool usage strategies are refined simultaneously, outperform the static pipeline presented?
- Basis in paper: [explicit] The conclusion explicitly proposes exploring the "co-evolution of tool creation, tool aggregation, and tool learning, enabling LLMs to simultaneously create reusable tools and refine their usage strategies."
- Why unresolved: The current pipeline treats tool creation and tool usage as sequential, disjoint stages; the potential feedback loop where usage failures inform dynamic tool refactoring remains unexplored.
- What evidence would resolve it: Implementation of an online learning system where tool definitions are updated based on real-time usage errors, showing faster convergence or higher final accuracy than the static ToolLibGen pipeline.

### Open Question 4
- Question: Can the ToolLibGen aggregation pipeline be executed effectively by smaller, open-source models, or is it dependent on frontier-class models like GPT-5?
- Basis in paper: [inferred] The methodology relies on GPT-5 for the complex tasks of hierarchical clustering, blueprint design, and code aggregation (Section 2.1), implying a potential resource barrier for widespread adoption.
- Why unresolved: The paper does not ablate the "General LLM" used for library generation, leaving the minimum capability requirements for the clustering and aggregation agents undefined.
- What evidence would resolve it: A replication of the library generation process using a smaller open-source model (e.g., Llama-3-70B), comparing the coherence and functional correctness of the resulting libraries against the GPT-5 baseline.

## Limitations

- The method assumes access to high-quality CoT traces and a general LLM capable of sophisticated tool abstraction, which may not hold for all domains or resource-constrained settings.
- The computational overhead of multi-agent iteration is not quantified in terms of wall-clock time or cost per query.
- Evaluation relies entirely on internal benchmarks with no external validation, raising concerns about generalizability.

## Confidence

- **High confidence:** The multi-agent iteration mechanism demonstrably improves functional coverage (71.9% vs 64.8% in ablation). The tool compression ratio (175k → 8.9k for math) is directly observable and substantial.
- **Medium confidence:** Retrieval accuracy improvements are well-supported by controlled experiments showing declining performance for fragmented tools. However, the embedding-based retrieval mechanism's sensitivity to domain shifts is untested.
- **Low confidence:** The generality claim ("easily extends to other domains") lacks empirical validation beyond the three tested domains. The paper asserts but does not prove that LLM-based clustering outperforms simpler alternatives in all scenarios.

## Next Checks

1. **Cross-domain robustness test:** Apply ToolLibGen to a fourth domain (e.g., legal reasoning or financial analysis) using publicly available datasets. Compare retrieval accuracy and task performance against domain-specific baselines.

2. **Cost-benefit analysis:** Measure the end-to-end latency and compute cost of ToolLibGen inference versus a state-of-the-art baseline (e.g., HuggingGPT or Gorilla) on a fixed query budget. Include both library construction time and per-query overhead.

3. **Clustering quality audit:** Manually evaluate 50 randomly sampled tools from the aggregated library to verify that: (a) the class abstraction preserves all original functionality, (b) the facade interface is intuitive for LLM invocation, and (c) no critical edge cases were lost during aggregation.