---
ver: rpa2
title: 'TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization
  for Eliciting Human Preference'
arxiv_id: '2506.02827'
source_url: https://arxiv.org/abs/2506.02827
tags:
- responses
- preference
- task
- response
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TO-GATE, a novel framework that improves
  large language models' ability to elicit human preferences through clarifying questions
  and summarizing responses. The key idea is to use trajectory optimization to distinguish
  between effective and ineffective conversation trajectories, consisting of a clarification
  resolver that employs contrastive learning and a summarizer that balances the quality
  of questions and responses.
---

# TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference

## Quick Facts
- **arXiv ID**: 2506.02827
- **Source URL**: https://arxiv.org/abs/2506.02827
- **Reference count**: 9
- **Primary result**: 9.32% improvement in response quality over baselines on preference elicitation tasks

## Executive Summary
TO-GATE introduces a novel framework that improves large language models' ability to elicit human preferences through clarifying questions and summarizing responses. The key innovation is using trajectory optimization with contrastive learning to distinguish effective from ineffective conversation trajectories. By iteratively generating and filtering conversation pairs, TO-GATE trains a model that asks better clarifying questions and produces higher-quality final responses. The approach addresses limitations in existing methods by providing deterministic evaluation that eliminates position bias and demonstrates significant performance gains through ablation studies.

## Method Summary
TO-GATE trains a Questioner model to elicit preferences through multi-turn clarifying questions and generate personalized final responses. The method uses contrastive trajectory optimization where the model generates 10 conversation samples per task-persona pair, ranks them by gold response likelihood, and applies DPO loss to increase probability of high-scoring trajectories while decreasing low-scoring ones. The training combines a clarification resolver (Lc) and summarizer (Lo) with weighted loss L = (1/(1+λ))·Lc + λ/(1+λ)·Lo. The framework iteratively expands its training dataset through exploration-collection-training cycles, using Mistral-7B-Instruct as the Questioner and Mixtral-8x7B-Instruct as the Roleplayer, with gold responses generated by GPT-4.

## Key Results
- TO-GATE achieves a 9.32% improvement on standard preference elicitation tasks compared to baselines
- The deterministic dual-pass evaluation eliminates position bias, providing stable and reproducible scores
- Ablation study shows the clarification resolver has a more substantial impact than the summarizer on overall performance

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Trajectory Optimization for Question Generation
TO-GATE improves question generation by learning to distinguish effective from ineffective dialogue trajectories using DPO-based contrastive learning. The clarification resolver generates multiple conversation trajectories per task, ranks them by gold response likelihood, then applies DPO loss to increase probability of high-scoring trajectories while decreasing low-scoring ones. This creates a signal that penalizes irrelevant questions.

### Mechanism 2: Dynamic Dataset Expansion Through Exploration-Collection-Training
Iteratively generating and filtering new positive/negative trajectory pairs improves model performance over static training data. Algorithm 1 cycles through: generate 10 conversation samples per task-persona pair, select best/worst by gold response likelihood, extract clarification questions, train, and repeat. This creates dataset Dp that expands each iteration.

### Mechanism 3: Separate Loss Weighting for Clarifications vs Final Responses
Explicitly balancing clarification loss (Lc) and response loss (Lo) with weight λ improves final response quality by preventing one objective from dominating. Eq. 10 computes weighted loss L = (1/(1+λ))·Lc + λ/(1+λ)·Lo. Ablation shows λ=2 performs best, suggesting clarifications should be weighted higher than responses during training.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: TO-GATE extends DPO from single-turn preference pairs to multi-turn dialogue trajectories
  - Quick check question: Can you explain how DPO avoids explicit reward model training and what role the reference policy πref plays?

- **Concept: Preference Elicitation in Dialogue Systems**
  - Why needed here: The core task is eliciting latent user preferences through multi-turn questioning
  - Quick check question: Why is maximizing P(Gold Response|Conversation) insufficient according to the authors?

- **Concept: Self-Taught Reasoning (STaR) Loop**
  - Why needed here: TO-GATE builds on STaR-GATE's self-improvement paradigm but addresses its limitation of only training on positive trajectories
  - Quick check question: What is the key difference between STaR's self-generation approach and TO-GATE's contrastive trajectory optimization?

## Architecture Onboarding

- **Component map**:
  - **Questioner (πθ)**: Model being optimized; asks clarifying questions and generates final responses
  - **Roleplayer (R)**: Simulates user with access to task + persona; responds to questions
  - **Oracle (O)**: GPT-4; generates gold responses given task + persona (supervision signal)
  - **Clarification Resolver**: Applies DPO loss (Lc) to positive/negative question trajectories
  - **Summarizer**: Applies weighted response loss (Lo) to final outputs

- **Critical path**:
  1. Initialize Questioner via supervised fine-tuning on positive trajectories
  2. Generate 10 conversation samples per (task, persona) pair via Questioner-Roleplayer interaction
  3. Rank by gold response likelihood; select best (sw) and worst (sl) trajectories
  4. Extract clarifications (qw, ql) and generate responses (ow, ol)
  5. Apply combined DPO loss L = weighted(Lc + Lo)
  6. Update πref ← trained model; repeat

- **Design tradeoffs**:
  - λ=2 favors clarification loss over response loss (validated by ablation); may need tuning for different domains
  - 10 samples per task-persona pair balances exploration quality vs computational cost
  - β=0.1 controls KL divergence penalty; higher values constrain deviation from reference
  - Training on simulated Roleplayer data may not transfer to real users

- **Failure signatures**:
  - Degenerate trajectories: If all generated questions become similar, contrastive signal weakens
  - Position bias in evaluation: Original GATE evaluation showed order effects
  - Divergence at M3: Clarification improves but response plateaus after epoch 2
  - Noisy negative pairs: Automatically generated "worst" trajectories may not be meaningfully bad

- **First 3 experiments**:
  1. Reproduce baseline comparison: Train STaR-GATE, DPO, and TO-GATE on provided dataset. Verify 9.32% improvement claim and dual-pass evaluation scores match Table 1.
  2. Ablation by component: Remove clarification resolver (use only Lo) and remove summarizer (use only Lc). Confirm 5.65% and 2.12% win rate drops respectively per Figure 5.
  3. Lambda sensitivity: Sweep λ ∈ {0.5, 1, 2, 3, 6} and plot win rate. Verify λ=2 is optimal and characterize performance degradation at extremes.

## Open Questions the Paper Calls Out

### Open Question 1
Does TO-GATE maintain its performance advantages in real-world human interactions compared to the GPT-4 simulated environments used in experiments? The authors state that real-world human preference elicitation could be slightly different from the simulations constructed using GPT-4. A user study with human participants acting as the roleplayer would validate the win rate and preference elicitation efficiency.

### Open Question 2
What mechanisms cause the observed divergence where improved clarification trajectories do not yield monotonic enhancements in final response quality after multiple training epochs? Section 5.4 notes a "notable divergence" where clarification log-probabilities improve consistently, but response quality does not show "consistent, monotonic enhancement" after epoch 2. An ablation study analyzing semantic alignment between generated questions and final responses at high training epochs would help.

### Open Question 3
How does the trajectory optimization strategy scale with larger model backends compared to the 7B and 8x7B architectures tested? The Limitations section notes that performance is "limited to LLMs" and hypothesizes that ability "will increase accordingly" with model scale, though this remains untested. Benchmarking TO-GATE on larger foundation models (e.g., 70B+ parameters) would determine if the 9.32% improvement margin scales or diminishes.

## Limitations
- The framework relies entirely on simulated Roleplayer data, raising questions about real-world transferability to actual human interactions
- The specific hyperparameters (λ=2, 10 samples per task-persona pair) are validated only on this particular dataset and may not generalize across domains
- The evaluation framework depends on GPT-4 judging, which introduces potential bias and may not perfectly align with human preferences

## Confidence

- **High Confidence**: The core contrastive learning framework (DPO on trajectory pairs) is well-established and the ablation study showing component importance is methodologically sound
- **Medium Confidence**: The 9.32% improvement claim is credible given the rigorous dual-pass evaluation that eliminates position bias, but the absolute win rates and their practical significance require further validation
- **Low Confidence**: The specific hyperparameter choices and the claim that these are optimal for all preference elicitation tasks lack cross-domain validation

## Next Checks
1. **Human Evaluation Transfer**: Conduct human evaluation of TO-GATE outputs in real user interactions to verify that simulated training performance translates to real-world preference elicitation effectiveness
2. **Cross-Domain Generalization**: Apply TO-GATE to a different preference elicitation domain (e.g., medical diagnosis, product recommendation) and test whether the same hyperparameters maintain performance
3. **Robustness to Noise**: Intentionally introduce noise into the negative trajectory selection process to test whether the contrastive learning signal remains effective when "worst" trajectories are not clearly distinguishable from "best" ones