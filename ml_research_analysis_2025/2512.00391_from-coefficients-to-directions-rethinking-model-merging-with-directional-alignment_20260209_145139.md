---
ver: rpa2
title: 'From Coefficients to Directions: Rethinking Model Merging with Directional
  Alignment'
arxiv_id: '2512.00391'
source_url: https://arxiv.org/abs/2512.00391
tags:
- tasks
- alignment
- task
- merging
- share
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model merging framework that emphasizes
  directional alignment in both parameter and feature spaces. The authors propose
  aligning task-specific parameters onto a simplex Equiangular Tight Frame (ETF) basis,
  ensuring globally consistent task representations while reducing destructive interference.
---

# From Coefficients to Directions: Rethinking Model Merging with Directional Alignment

## Quick Facts
- arXiv ID: 2512.00391
- Source URL: https://arxiv.org/abs/2512.00391
- Reference count: 40
- Key outcome: Introduces directional alignment framework that projects task parameters onto simplex ETF basis, validated across vision and NLP benchmarks with consistent performance improvements

## Executive Summary
This paper proposes a novel model merging framework that emphasizes directional alignment in both parameter and feature spaces. The authors introduce a two-stage approach: first aligning task-specific parameters onto a simplex Equiangular Tight Frame (ETF) basis to ensure globally consistent task representations, then jointly optimizing fusion coefficients and rotation matrices in feature space to enforce ETF-aligned structures. The method is validated across diverse benchmarks, model scales, and task configurations, showing consistent performance improvements over existing approaches. Theoretical analysis demonstrates that directional alignment enhances structural coherence and generalization through a Rademacher complexity bound.

## Method Summary
The method operates through two main algorithms: Algorithm 1 performs parameter-space alignment by computing task vectors (θ_t - θ_0), extracting shared subspaces via SVD, and projecting onto ETF basis; Algorithm 2 performs feature-space alignment by jointly optimizing layer-wise fusion coefficients λ^(l) and task-specific rotation matrices R_t ∈ SO(d) using unlabeled data. The merged model is initialized with ETF-aligned parameters and then refined through a loss function combining entropy minimization, ETF alignment loss, and rotation regularization. The approach can be used in data-free mode (Algorithm 1 only) or data-based mode (both algorithms).

## Key Results
- Consistently outperforms state-of-the-art model merging methods across 8/14/20-task vision benchmarks and GLUE NLP tasks
- Shows improved generalization to unseen tasks while maintaining performance on merged tasks
- Demonstrates reduced directional deviation ∆ETF compared to baseline methods
- Achieves these gains while maintaining computational efficiency through the two-stage approach

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Space Directional Alignment via ETF Projection
The method projects task-specific parameter subspaces onto a simplex Equiangular Tight Frame (ETF) basis to improve structural coherence and reduce destructive interference. For each layer, SVD decomposes per-task vectors, top-k components are concatenated to form a shared subspace, which is then projected onto the ETF basis via Gram operator multiplication. The core assumption is that ideal jointly-trained multi-task parameters exhibit near-ETF structure, supported by Neural Collapse principles.

### Mechanism 2: Feature-Space Directional Alignment via Joint Optimization
After ETF-aligned parameter initialization, the method jointly optimizes fusion coefficients (λ^(l)) and task-specific rotation matrices (R_t) in feature space using unlabeled data. The optimization minimizes entropy for calibration, enforces ETF alignment between rotated features and class prototypes, and regularizes rotations toward optimal Procrustes solutions. This mechanism assumes unlabeled data availability and that well-trained model features can be meaningfully aligned to ETF structure.

### Mechanism 3: Theoretical Link Between ETF Alignment and Generalization
The paper provides a Rademacher complexity analysis showing that the generalization gap between shared subspace and ETF-aligned solutions is bounded by the energy of parameters outside the ETF subspace. By projecting onto ETF, this residual is reduced, tightening the bound. This theoretical connection relies on Lipschitz loss functions, bounded features, and both solutions achieving similar empirical risk.

## Foundational Learning

- **Neural Collapse and Simplex ETF Geometry**: Essential to understand why ETF alignment is a sensible target, as well-trained models' features and classifiers converge to this structure. Quick check: Can you describe the four properties of Neural Collapse (NC1-NC4) and why a simplex ETF represents maximally separable class configuration?

- **Task Vectors and Basic Model Merging**: Fundamental to understand task vectors (τ_t = θ_t - θ_0) and basic "Task Arithmetic" merging formula before tackling directional alignment. Quick check: How is a task vector defined, and what is the basic Task Arithmetic formula for merging multiple task vectors?

- **Geometric Alignment Tools: SVD, Procrustes Problem, and SO(d)**: Required to implement and debug the algorithms, as the method uses SVD for subspace extraction and solves orthogonal Procrustes problems for optimal rotation matrices. Quick check: Given two sets of points A and B, what does the orthogonal Procrustes problem find, and how is the solution typically computed?

## Architecture Onboarding

- **Component map**: Algorithm 1 (Parameter-Space Alignment) -> ETF Constructor -> Algorithm 2 (Feature-Space Alignment) -> Loss Function Engine
- **Critical path**: 1) Run Algorithm 1 on all fine-tuned models to obtain ETF-aligned task vectors τ_etf; 2) Initialize merged model using pre-trained weights and τ_etf with initial coefficients; 3) Run Algorithm 2 on initialized model using unlabeled data, optimizing λ^(l) and R_t
- **Design tradeoffs**: Data-free vs. data-based (Algorithm 1 only vs. both algorithms); rank dimension k choice (default d_out/T) controlling shared subspace capacity; loss hyperparameters (α=0.8, β=0.2) controlling ETF feature alignment strength and rotation regularization
- **Failure signatures**: Performance plateaus/declines (check ETF construction method appropriateness); optimization instability (large α/β causing gradients to explode); poor generalization on unseen tasks (overfitting to seen tasks)
- **First 3 experiments**: 1) Ablation on Rotation: Compare "MDA AM" (with rotation) vs. "MDA AM without rotation"; 2) Sensitivity to Rank k: Sweep k values (0.01, 0.1, 1/T, 0.5) to validate default choice; 3) Baseline Comparison: Reproduce 8-task ViT-B/32 results comparing "MDA TA" and "MDA AM" against TSV-TA and AdaMerging

## Open Questions the Paper Calls Out
- Can the directional alignment framework be extended to merge models with different architectures or pre-training paradigms? (Conclusion states this remains an open challenge)
- How can the computational overhead of feature-space alignment be reduced for extremely large-scale models? (Section 6 notes this may be non-trivial)
- Does directional alignment improve performance in generative models? (Conclusion lists extending to generative models as a promising future direction)

## Limitations
- The theoretical generalization bound relies on idealized assumptions (both solutions achieving equal empirical risk) that may not hold in practice
- The method assumes access to unlabeled data for feature-space alignment, limiting its applicability in truly data-free scenarios
- Computational overhead from optimizing rotation matrices in high-dimensional feature spaces may limit scalability to extremely large models

## Confidence
- **High Confidence**: Experimental results showing MDA's superiority across diverse tasks and model scales are convincing; parameter-space alignment mechanism is clearly specified and reproducible
- **Medium Confidence**: Feature-space alignment mechanism and its contribution are supported but harder to isolate; hyperparameter choices appear justified but could be task-dependent
- **Low Confidence**: Theoretical generalization bound's practical tightness and direct attribution to ETF alignment are uncertain due to multiple simplifying assumptions

## Next Checks
1. **ETF Structure Validation**: Empirically verify that column space of concatenated task vectors from well-trained multi-task models exhibits near-ETF structure before applying MDA
2. **Rotation Matrix Isolation**: Run ablation isolating effect of rotation matrices by fixing fusion coefficients and only optimizing rotations, or vice versa
3. **Generalization Bound Sensitivity**: Test model on truly unseen task and compare performance against theoretical generalization gap bound to assess practical tightness