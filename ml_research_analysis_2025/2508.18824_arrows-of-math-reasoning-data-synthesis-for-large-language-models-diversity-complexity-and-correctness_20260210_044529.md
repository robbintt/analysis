---
ver: rpa2
title: 'Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity,
  Complexity and Correctness'
arxiv_id: '2508.18824'
source_url: https://arxiv.org/abs/2508.18824
tags:
- mathematical
- data
- knowledge
- math
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Arrows of Math Reasoning (AMD), a novel program-assisted
  framework for generating high-quality mathematical training data. AMD addresses
  the challenge of enhancing mathematical reasoning in large language models by systematically
  generating executable programs through a comprehensive three-tier mathematical knowledge
  system, then translating these into natural language problems and solutions.
---

# Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness

## Quick Facts
- arXiv ID: 2508.18824
- Source URL: https://arxiv.org/abs/2508.18824
- Reference count: 25
- Generates 12.3M problem-solving triples with guaranteed correctness for LLM fine-tuning

## Executive Summary
This paper presents Arrows of Math Reasoning (AMD), a novel program-assisted framework for generating high-quality mathematical training data. AMD addresses the challenge of enhancing mathematical reasoning in large language models by systematically generating executable programs through a comprehensive three-tier mathematical knowledge system, then translating these into natural language problems and solutions. The approach uses a bilateral verification mechanism that ensures correctness by comparing LLM-generated solutions against program execution outputs. AMD generates 12.3 million problem-solving triples and demonstrates superior performance compared to existing methods, achieving state-of-the-art results on multiple benchmarks including GSM8K, MATH, Minerva, and SVAMP when used to fine-tune LLaMA3-8B, Mistral-7B, and Deepseek-Math-7B models.

## Method Summary
AMD constructs a three-tier mathematical knowledge system mapping education stages to subjects and topics (250+ topics total), then maps topics to computational tools via GPT-4 analysis of seed problems. The framework synthesizes programs by sampling topic combinations and tool sets, then applies mutation strategies to increase diversity. Each program is backtranslated into natural language questions and solutions using LLM prompts. A bilateral verification mechanism ensures correctness by executing programs and comparing their outputs to extracted answers from LLM solutions. The system generates 12.3M verified problem-solution triples that are used to fine-tune base models via LoRA.

## Key Results
- Achieves state-of-the-art performance on GSM8K, MATH, Minerva_Math, and SVAMP benchmarks
- Outperforms previous state-of-the-art models on mathematical reasoning tasks
- Generates 12.3 million high-quality problem-solving triples with guaranteed correctness

## Why This Works (Mechanism)
The bilateral verification mechanism ensures correctness by requiring program execution outputs to match LLM-extracted answers, creating a self-consistent validation loop. The three-tier knowledge system enables systematic coverage of mathematical domains while the mutation strategies ensure diversity. By grounding problem generation in executable programs, AMD avoids the hallucination and inconsistency issues common in pure LLM-generated mathematical data.

## Foundational Learning
- Three-tier knowledge system (education stage → subject → topic): Provides structured coverage of mathematical domains; verify by checking topic distribution matches educational standards
- Program-based problem synthesis: Ensures semantic coherence between problems and solutions; validate by executing generated programs
- Bilateral verification mechanism: Guarantees correctness through cross-validation; test by checking pass rates on sampled pairs
- LLM-based mutation strategies: Increases diversity while maintaining solvability; monitor diversity metrics across synthesized samples
- Answer extraction via regex: Enables automated correctness checking; verify extraction accuracy on diverse problem types

## Architecture Onboarding

**Component Map:**
Knowledge System → Program Synthesis → Program Mutation → Question Generation → Solution Generation → Bilateral Verification → Dataset

**Critical Path:**
Program Synthesis → Bilateral Verification → Fine-tuning → Benchmark Evaluation

**Design Tradeoffs:**
- Program-based generation ensures correctness but may limit natural language fluency
- Mutation increases diversity but risks creating unsolvable problems
- Bilateral verification provides strong guarantees but reduces yield

**Failure Signatures:**
- Low bilateral verification pass rate indicates issues with answer extraction or program correctness
- Unbalanced topic distribution suggests problems with knowledge system coverage
- Poor benchmark performance despite high verification rates may indicate overfitting to verification criteria

**First Experiments:**
1. Implement minimal knowledge system with 10 topics and verify program synthesis pipeline
2. Test bilateral verification mechanism on small sample to establish pass rate baseline
3. Fine-tune a base model with 1K AMD samples and evaluate on GSM8K to verify performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates and sampling strategies are not fully specified, limiting reproducibility
- LoRA fine-tuning hyperparameters are not detailed beyond basic configuration
- The framework requires access to GPT-4 for program generation and mutation
- Potential computational overhead from program execution for all generated samples

## Confidence
- High confidence in overall framework architecture and theoretical validity
- Medium confidence in reported benchmark improvements due to incomplete implementation details
- Medium confidence in scalability claims due to missing pipeline specifications

## Next Checks
1. Reconstruct the knowledge system with a minimal topic set and verify that program synthesis, mutation, and bilateral verification produce coherent and executable samples
2. Test multiple prompt variations for question generation and solution synthesis to establish the sensitivity of bilateral verification pass rates to prompt design
3. Apply the AMD-synthesized data to fine-tune a base model with LoRA using documented hyperparameter sweep and evaluate on GSM8K/MATH to confirm performance gains