---
ver: rpa2
title: 'Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability
  with Graph Functional Neural Process'
arxiv_id: '2508.17097'
source_url: https://arxiv.org/abs/2508.17097
tags:
- graph
- neural
- rationale
- uncertainty
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GraphFNP, a novel uncertainty-aware and interpretable
  graph classification framework that combines graph functional neural processes and
  graph generative models. The key idea is to assume latent model-level rationales
  that can be mapped to a probabilistic embedding space, with the predictive distribution
  conditioned on these rationale embeddings through a learned stochastic correlation
  matrix.
---

# Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process

## Quick Facts
- arXiv ID: 2508.17097
- Source URL: https://arxiv.org/abs/2508.17097
- Reference count: 27
- Key outcome: GraphFNP achieves up to 4.8% ECE improvement and 10.8% RF1 score improvement over state-of-the-art methods

## Executive Summary
This paper presents GraphFNP, a novel graph classification framework that achieves both uncertainty quantification and interpretability by combining functional neural processes with graph generative models. The method learns class-discriminative latent rationale embeddings that serve dual purposes: predicting through stochastic correlations and generating interpretable graph structures. The alternating EM-like optimization procedure stabilizes training by decoupling rationale embeddings from model parameters. Extensive experiments on five datasets demonstrate superior performance in calibration metrics and rationale quality compared to existing approaches.

## Method Summary
GraphFNP encodes input graphs into embeddings, computes stochastic correlations with learnable rationale embeddings, and conditions predictions on these correlations. A graph generator decodes rationale structures for interpretability. The model is trained using alternating optimization between updating rationale embeddings/decoder (E-step) and updating encoder/classifier (M-step). This approach combines functional uncertainty modeling through correlation sampling with autoregressive graph generation for explanations, while maintaining compatibility with any existing GNN architecture.

## Key Results
- Achieves up to 4.8% improvement in expected calibration error (ECE) over state-of-the-art methods
- Improves rationale F1 score by up to 10.8% for interpretability
- Maintains competitive accuracy while providing meaningful decoded rationales that align with real-world patterns
- Demonstrates effectiveness across five diverse graph classification datasets

## Why This Works (Mechanism)

### Mechanism 1: Functional Uncertainty via Stochastic Correlation
The model captures uncertainty through stochastic correlations between input graphs and learned rationale embeddings. By sampling a binary correlation matrix based on kernel similarity, the predictive distribution reflects functional relationships to reference points rather than just weight uncertainty. This mimics Gaussian Processes where uncertainty derives from correlation to known prototypes.

### Mechanism 2: Interpretability via Generative Rationale Decoding
Rationale embeddings trained to be class-discriminative are decoded into graph structures using an autoregressive generator. These decoded graphs represent the substructures the model considers indicative of each class, providing model-level explanations that align with human-understandable patterns like chemical functional groups.

### Mechanism 3: Alternating Optimization for Embedding-Parameter Disentanglement
The alternating EM-like procedure alternates between updating rationale embeddings/decoder (E-step) and encoder/classifier (M-step). This prevents the "moving target" problem where encoder and rationale embeddings drift apart, achieving better stability and performance than joint gradient descent.

## Foundational Learning

- **Concept: Functional Neural Processes (FNP)**
  - Why needed: The core architecture adapts FNP to model uncertainty through correlations to context points (rationales) rather than weight distributions
  - Quick check: How does sampling a binary correlation matrix differ from standard attention mechanisms in GNNs?

- **Concept: Autoregressive Graph Generation (e.g., GraphRNN)**
  - Why needed: Interpretability component relies on decoding graphs; understanding sequential node/edge generation is essential
  - Quick check: Why must generation be conditioned on latent rationale embedding to serve as explanation?

- **Concept: Expected Calibration Error (ECE)**
  - Why needed: Primary metric for uncertainty quantification claims; understanding reliability diagrams and binning is crucial
  - Quick check: If a model predicts "90% confident" on 100 samples, how many must be correct for perfect calibration?

## Architecture Onboarding

- **Component map:** Input Graph -> Encoder (GCN/GAT) -> Graph Embedding Z_D -> Correlation Layer (RBF kernel + Bernoulli sampling) -> Stochastic Matrix C -> Predictor (MLPs) -> Class Logits; simultaneously Rationale Embeddings Z_R -> Graph Generator (GraphRNN) -> Decoded Rationale Graph

- **Critical path:** During M-step, gradient flow connects Classification Loss back to Encoder; during E-step, gradient flow connects Generation/Classification Loss back to Rationale Bank

- **Design tradeoffs:** Fixed rationale count per class (risk of under/overfitting) vs. dynamic allocation; Gumbel-Softmax temperature (dense/noisy vs. zero gradients)

- **Failure signatures:** High ECE with high accuracy indicates deterministic correlation matrix; low RF1 suggests non-discriminative rationale embeddings or generic decoded graphs

- **First 3 experiments:**
  1. Sanity Check: Freeze encoder, train rationale bank/decoder on small dataset, verify decoded graphs match ground-truth motifs
  2. Correlation Analysis: Visualize correlation matrix for validation samples, check correct class correlations
  3. Ablation on |R_k|: Measure ECE and RF1 while varying rationales per class (1, 5, 10) to find saturation point

## Open Questions the Paper Calls Out

- Can the number of latent rationales per class be determined adaptively rather than requiring manual specification?
- Can the autoregressive graph generator scale to handle larger graphs while maintaining structural validity?
- Can GraphFNP provide meaningful model-level explanations for classes defined by absence of specific features?

## Limitations

- Alternating EM optimization lacks precise implementation details (step ratios, temperature scheduling) affecting reproducibility
- No human evaluation to confirm decoded explanations align with domain expert understanding
- Framework's scalability to larger graph datasets or more classes remains unexplored with fixed rationale bank size

## Confidence

- High confidence: GraphFNP architecture design, alternating optimization procedure, core mathematical formulations
- Medium confidence: Experimental results showing ECE improvements and RF1 score gains (implementation details partially unspecified)
- Low confidence: Generalization claims to arbitrary GNN architectures and real-world deployment scenarios

## Next Checks

1. Implement controlled experiments varying rationales per class (1, 5, 10) to identify saturation point where additional rationales no longer improve ECE or RF1
2. Create synthetic dataset with known ground-truth rationales, test whether GraphFNP's decoded rationales match these patterns more accurately than baselines
3. Analyze correlation matrix behavior on out-of-distribution samples to verify calibrated uncertainty estimates beyond training distribution