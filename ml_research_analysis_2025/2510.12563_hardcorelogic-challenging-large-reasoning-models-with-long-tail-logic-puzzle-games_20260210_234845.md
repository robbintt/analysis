---
ver: rpa2
title: 'HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle
  Games'
arxiv_id: '2510.12563'
source_url: https://arxiv.org/abs/2510.12563
tags:
- puzzle
- uni00000013
- uni0000004c
- uni00000048
- uni00000010
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large reasoning models (LRMs) excel at canonical logic puzzles
  but struggle with non-standard variants. To expose these limitations, we introduce
  HardcoreLogic, a benchmark of over 5,000 puzzles across 10 games, systematically
  transformed along three dimensions: increased complexity, uncommon elements, and
  unsolvable puzzles.'
---

# HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games

## Quick Facts
- arXiv ID: 2510.12563
- Source URL: https://arxiv.org/abs/2510.12563
- Reference count: 40
- Large reasoning models show substantial accuracy drops on transformed logic puzzles compared to canonical forms

## Executive Summary
Large reasoning models (LRMs) excel at canonical logic puzzles but struggle with non-standard variants. To expose these limitations, we introduce HardcoreLogic, a benchmark of over 5,000 puzzles across 10 games, systematically transformed along three dimensions: increased complexity, uncommon elements, and unsolvable puzzles. Evaluations show that all tested LRMs, including top-performing models, exhibit substantial accuracy drops on HardcoreLogic compared to standard benchmarks. Increased complexity is the primary source of difficulty, but subtle rule changes also challenge models. Systematic error analysis reveals that models often rely on memorized patterns, fabricate facts, or fail to detect unsolvable instances. These findings highlight the need for more robust reasoning capabilities and deeper logical understanding in LRMs.

## Method Summary
The HardcoreLogic benchmark comprises 5,250 logic puzzles across 10 game types, systematically transformed along four dimensions: search space expansion (IC1), constraint strengthening (IC2), form mutation (UE1), and rule mutation (UE2), plus unsolvable puzzles (UP). Each puzzle family was generated using Jinja2 templates with transformation parameters. Complexity was analyzed using Z3 solvers for constraint puzzles and Dijkstra for graph puzzles. Models were evaluated with a 32,768-token reasoning budget, generating structured JSON outputs. Error analysis categorized failures into six types for solvable puzzles and four for unsolvable ones, using GPT-5 for scalable annotation.

## Key Results
- All tested LRMs show substantial accuracy drops on HardcoreLogic compared to standard benchmarks
- Search space expansion (IC1) is the dominant source of difficulty across transformations
- Models often rely on memorized patterns, fabricate facts, or fail to detect unsolvable instances
- Subtle rule changes (UE2) challenge models even when they reduce formal complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRMs exhibit performance degradation on long-tail puzzle variants primarily due to reliance on memorized solution patterns rather than flexible rule application.
- Mechanism: Models either fail to understand novel rules (ignoring or misinterpreting them) or apply canonical solution strategies to variant puzzles, producing errors. Regression analysis shows that form mutation (UE1), which changes representation without adding logical complexity, still significantly reduces accuracy, particularly for Sudoku with irregular subgrids.
- Core assumption: Training corpora contain disproportionate canonical puzzle examples, creating stereotyped solution pathways that models default to.
- Evidence anchors:
  - [abstract] "Systematic error analysis reveals that models often rely on memorized patterns, fabricate facts, or fail to detect unsolvable instances."
  - [section 1] "Models recognize only the canonical form of logical puzzles... even when they successfully understand the variant, they still apply a mismatched solution strategy."
  - [corpus] Weak direct corpus support; related work (Sudoku-Bench, Enigmata) addresses memorization but not with systematic long-tail transformation taxonomy.
- Break condition: If models were trained on balanced distributions of puzzle variants from the start, performance gaps between canonical and non-canonical forms should narrow significantly.

### Mechanism 2
- Claim: Search space expansion (IC1) is the dominant source of difficulty, directly overwhelming model memory and reasoning capacity.
- Mechanism: HardcoreLogic systematically increases empty cells and grid sizes, expanding combinatorial spaces exponentially (e.g., Binario search space grows from ~10² to ~10¹⁴). Models attempting brute-force enumeration exceed reasoning budgets, leading to incomplete or incorrect answers.
- Core assumption: Token budget constraints (32,768) meaningfully limit reasoning depth, and models cannot efficiently prune search spaces.
- Evidence anchors:
  - [section 2.3] "Figure 2 shows the average log-scale search space across five puzzle families, confirming that HardcoreLogic systematically enlarges the combinatorial space."
  - [section 4.1] "IC1 has the greatest comprehensive impact on the models, as the increase in search space directly requires the improvement of the models' memory and reasoning ability."
  - [corpus] SATBench and Enigmata similarly use puzzle generation but don't isolate search space as a primary difficulty dimension with quantitative analysis.
- Break condition: If models integrated symbolic constraint solvers or learned efficient pruning heuristics, search space expansion would have diminished impact.

### Mechanism 3
- Claim: Rule mutation (UE2) challenges models even when it reduces formal complexity, because it requires tracking higher-order relationships rather than simple pattern matching.
- Mechanism: The "landmine cluster" Minesweeper variant changes numerical clues from counting individual mines to counting 8-connected clusters. Despite adding more clues (reducing search space), models perform worse because they must maintain cluster connectivity information—a capability exceeding simple pattern recognition.
- Core assumption: Models' apparent reasoning on canonical puzzles relies on shallow statistical patterns rather than structural understanding.
- Evidence anchors:
  - [section 2.3] "This exceeds their simple pattern-matching capabilities, causing performance drops even for powerful models."
  - [section 4.1] "The parameters of the minesweeper puzzle in UE2 also show that the 'landmine cluster' rule has a significant impact on the models."
  - [corpus] No direct corpus evidence; this specific finding appears novel to HardcoreLogic's transformation taxonomy.
- Break condition: If models developed explicit graph-reasoning modules for connectivity tracking, rule mutations involving structural relationships would cause smaller performance drops.

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSP)
  - Why needed here: HardcoreLogic models puzzles as CSPs, using Z3 solvers to measure decisions and conflicts. Understanding CSP formulation is essential for interpreting complexity metrics and designing transformations.
  - Quick check question: Can you explain why adding a diagonal constraint to Sudoku increases both decisions and conflicts in a CSP solver?

- Concept: Long-tail distribution in evaluation
  - Why needed here: The benchmark's core thesis is that canonical puzzles are overrepresented in training, creating a distribution imbalance. Understanding long-tail concepts helps explain why rare variants expose reasoning limitations.
  - Quick check question: If you sampled 100 logic puzzles from Common Crawl, how many would be standard 9×9 Sudoku vs. irregular-subgrid variants?

- Concept: Perplexity as representational difficulty
  - Why needed here: The paper uses model perplexity to quantify how "surprising" mutated forms appear to LRMs, separating representational difficulty from logical complexity.
  - Quick check question: Why might letter-encoded Sudoku have higher perplexity than digit Sudoku even though their logical structure is identical?

## Architecture Onboarding

- Component map:
  - Data generation pipeline (Appendix B) -> Templates for 10 puzzle types with transformation parameters (IC1/IC2/UE1/UE2/UP)
  - Complexity analyzer -> Z3-based CSP encoding for constraint puzzles; Dijkstra for graph puzzles; closed-form search space formulas
  - Evaluation harness -> 32,768-token reasoning budget, 4 runs per sample at T=0.6, JSON schema validation
  - Error classifier -> 6-category taxonomy for solvable puzzles (Misunderstanding, Misapplied, Brute-Force, Factual, Faithfulness, Infinite Repetition); 4-category for unsolvable

- Critical path:
  1. Puzzle instantiation via Jinja2 templates with transformation flags
  2. Model inference with reasoning-then-answer generation
  3. Schema-validated JSON parsing
  4. Accuracy computation per transformation type
  5. Regression analysis for transformation impact quantification

- Design tradeoffs:
  - Token budget (32,768) balances evaluation cost against allowing sufficient reasoning depth; stronger models often exhaust it on IC1 puzzles
  - Using GPT-5 as error annotator enables scalable analysis but introduces potential annotation bias
  - Sampling 5 cases per subtask for closed-source models reduces cost but limits statistical power

- Failure signatures:
  - Factual errors (most common): Models fabricate intermediate states or constraints during long reasoning chains
  - Infinite repetition: Weaker models (Minimax-M1) enter degenerative loops, especially on unsolvable puzzles
  - Mandatory response: Models force incorrect solutions rather than correctly identifying unsolvability
  - Unjustified unsolvability: Weaker models output "unsolvable" as a fallback when they fail to find answers, not through genuine constraint detection

- First 3 experiments:
  1. Baseline replication: Evaluate a target LRM on Original puzzles to establish canonical performance; expect >70% accuracy on most games for capable models
  2. Transformation ablation: Test each transformation type (IC1, IC2, UE1, UE2) in isolation to quantify individual difficulty contributions; expect IC1 to show largest coefficients in regression
  3. Unsolvable detection: Evaluate whether the model provides justified vs. unjustified unsolvability claims by manually inspecting reasoning traces for genuine constraint conflict identification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are Large Reasoning Models (LRMs) genuinely capable of "true" logical reasoning (flexibly applying rules to varying conditions), or do they rely primarily on pattern recognition and memorization of canonical forms?
- Basis in paper: [explicit] The Introduction states that despite successes on benchmarks, "whether LRMs are genuinely capable of true logical reasoning... remains an important question." The authors define true reasoning as flexibly applying appropriate rules, which current models fail to do on long-tail variants.
- Why unresolved: While the paper demonstrates that current models fail on HardcoreLogic, it does not determine if this is a fundamental architectural limitation of Transformers or a training data deficiency that could be solved with better data scaling.
- What evidence would resolve it: A model architecture or training regime that maintains consistent accuracy across both canonical and long-tail HardcoreLogic variants without specific fine-tuning on the transformation types.

### Open Question 2
- Question: Can models be trained to provide justified explanations for unsolvability rather than heuristically claiming "unsolvable" simply because they failed to find a solution?
- Basis in paper: [inferred] In Section 4.3, the analysis reveals that weaker models often output "unsolvable" as a fallback when they fail to derive an answer (Unjustified Unsolvability), whereas stronger models provide Justified Unsolvability. The paper leaves open how to enforce the latter behavior.
- Why unresolved: The paper identifies the distinction between "giving up" and "proving unsolvability," but the analysis shows even strong models struggle to maintain reasoning depth to prove unsatisfiability, often defaulting to brute-force or token limits.
- What evidence would resolve it: An evaluation methodology or training loss that rewards the generation of a valid proof-of-contradiction for unsolvable puzzles, successfully distinguishing it from mere non-outputs.

### Open Question 3
- Question: Does the number of valid solutions for a puzzle influence model accuracy independent of search space complexity or constraint density?
- Basis in paper: [inferred] Appendix D.2 notes that for Skyscraper puzzles, "the number of solutions becomes a factor affecting the difficulty of the game," observing that models may "guess" when clues are sparse and solutions are multiple.
- Why unresolved: Standard complexity metrics (decisions, conflicts, perplexity) do not explicitly account for the number of valid targets. It is unclear if models fail because the problem is hard, or because the lack of a unique solution destabilizes the decoding objective.
- What evidence would resolve it: A controlled ablation study comparing model performance on puzzles with identical search space sizes/constraints that are modified to have exactly one solution versus multiple valid solutions.

## Limitations

- Limited statistical power due to sampling only 5 cases per subtask for closed-source models
- Potential annotation bias from using GPT-5 for error categorization instead of human experts
- Coarse granularity of transformation taxonomy may conflate different cognitive bottlenecks

## Confidence

- Low: Search space expansion is the dominant difficulty factor
- Medium: Error analysis methodology reliability
- High: Core finding that LRMs struggle with non-canonical puzzle variants

## Next Checks

1. **Controlled transformation ablation**: Create intermediate variants that isolate specific rule changes from search space modifications to determine whether IC1's apparent dominance reflects genuine reasoning difficulty or coincidental correlation with other transformation effects.

2. **Alternative error annotation**: Re-annotate a random subset of 100 samples using human experts (rather than GPT-5) to validate the error taxonomy and quantify annotation reliability, particularly for subtle "Misapplied" vs "Misunderstanding" distinctions.

3. **Memory-augmented baseline**: Implement a simple constraint propagation system alongside LRM inference to test whether performance gaps on IC1 puzzles primarily reflect search space scaling or fundamental reasoning limitations that could be mitigated with external memory aids.