---
ver: rpa2
title: 'CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation'
arxiv_id: '2504.12560'
source_url: https://arxiv.org/abs/2504.12560
tags:
- causal
- cdf-rag
- retrieval
- query
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDF-RAG introduces a causality-aware RAG framework that integrates
  reinforcement-learned query refinement, multi-hop causal graph retrieval, and hallucination
  detection to improve factual accuracy and causal reasoning. Evaluated on four QA
  datasets (CosmosQA, MedQA, MedMCQA, AdversarialQA), CDF-RAG outperforms existing
  methods across all LLM backbones, achieving up to 0.920 F1 and 0.04 hallucination
  rate on MedMCQA.
---

# CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2504.12560
- Source URL: https://arxiv.org/abs/2504.12560
- Authors: Elahe Khatibi; Ziyu Wang; Amir M. Rahmani
- Reference count: 32
- Primary result: Outperforms existing RAG methods on four QA datasets with up to 0.920 F1 and 0.04 hallucination rate on MedMCQA

## Executive Summary
CDF-RAG introduces a causality-aware RAG framework that integrates reinforcement-learned query refinement, multi-hop causal graph retrieval, and hallucination detection to improve factual accuracy and causal reasoning. Evaluated on four QA datasets (CosmosQA, MedQA, MedMCQA, AdversarialQA), CDF-RAG outperforms existing methods across all LLM backbones, achieving up to 0.920 F1 and 0.04 hallucination rate on MedMCQA. The framework enhances retrieval precision by prioritizing causal evidence, dynamically refines queries via RL, and validates outputs for factual consistency. These results demonstrate that CDF-RAG produces more accurate, causally coherent, and trustworthy responses than standard and refined RAG approaches.

## Method Summary
CDF-RAG constructs a causal knowledge graph using UniCausal to extract cause-effect pairs, verified by GPT-4, and stores them in Neo4j. A PPO-trained RL agent refines user queries through expand, simplify, and decompose actions, optimizing a reward function that balances retrieval coverage, causal depth, context relevance, and hallucination penalty. The dual-path retriever combines MiniLM embeddings for semantic search with Neo4j traversal for causal graph reasoning. Generated answers undergo causal graph consistency checks and hallucination detection, with fallback generation if validation fails. The system is evaluated across four QA benchmarks using multiple LLM backbones.

## Key Results
- Achieves up to 0.920 F1 score on MedMCQA, outperforming baseline RAG methods
- Reduces hallucination rate to 0.04 on MedMCQA while maintaining high accuracy
- Demonstrates consistent improvements across CosmosQA, MedQA, MedMCQA, and AdversarialQA with various LLM backbones

## Why This Works (Mechanism)

### Mechanism 1: RL-Driven Causal Query Refinement
Dynamically rewritten queries improve retrieval of causally relevant evidence over static user inputs. A PPO-trained reinforcement learning agent maps query embeddings to discrete actions (expand, simplify, decompose), optimizing a reward function that balances retrieval coverage, causal depth, context relevance, and hallucination penalty. Decomposition breaks multi-hop queries into sub-questions; expansion adds causal specificity; simplification removes noise. The core assumption is that the reward function components are accurate proxies for downstream causal correctness and must be measurable during RL training.

### Mechanism 2: Dual-Path Causal-Semantic Retrieval
Combining dense vector retrieval with structured causal graph traversal surfaces more complete and logically valid evidence. The system retrieves a unified knowledge set K = Tsem ∪ Cgraph. Tsem is sourced from a vector DB (MiniLM embeddings) for semantic context. Cgraph is derived from traversing a Neo4j causal graph (directed triples verified by GPT-4) to find multi-hop cause-effect chains. This addresses the limitation of semantic-only retrieval, which conflates correlation with causation. The core assumption is that the pre-constructed causal graph is sufficiently complete, accurate, and domain-aligned.

### Mechanism 3: Hallucination Detection & Causal Consistency Validation
Post-generation validation against retrieved causal knowledge reduces hallucination and ensures logical coherence. Two checks are performed: a Causal Graph Check computes a consistency score based on whether generated claims align with graph triples, triggering "Fallback Generation" with stricter constraints if low; a Hallucination Detection module computes a score based on the overlap between answer claims and retrieved evidence K, triggering knowledge rewriting if it exceeds a threshold. The core assumption is that the generated response can be reliably decomposed into discrete claims that can be mapped to structured graph entities or retrieved text spans.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) for RL.**
  - Why needed here: The core query refinement agent is trained via PPO. Understanding the clipped surrogate objective is essential to diagnose training instability or reward hacking.
  - Quick check question: Can you explain why PPO uses a clipped objective function instead of a standard policy gradient?

- **Concept: Directed Causal Graphs & Knowledge Graphs (e.g., Neo4j).**
  - Why needed here: The "Causal" in CDF-RAG comes from a Neo4j graph of directed cause-effect triples. Understanding nodes, edges, and multi-hop traversal is non-negotiable.
  - Quick check question: How would you traverse a path to find all downstream effects of a given node in a directed graph?

- **Concept: Vector Databases & Semantic Search.**
  - Why needed here: One path of the dual retrieval uses vector search. You must understand how embeddings are stored, indexed (e.g., HNSW), and queried via cosine similarity.
  - Quick check question: What is the primary difference between a lexical search (e.g., BM25) and a semantic vector search?

## Architecture Onboarding

- **Component map:**
  1. Query Refinement Agent (RL/PPO): Refines user query based on a learned policy. Uses MiniLM for state embeddings.
  2. Dual-Path Retriever:
     - Semantic Path: MiniLM encoder -> Vector DB (e.g., FAISS/Milvus)
     - Causal Path: Neo4j Graph DB (constructed via UniCausal + GPT-4 verification)
  3. Knowledge Fusion: Unifies text passages (Tsem) and causal chains (Cgraph) into a single context K
  4. Generator LLM: The backbone model (GPT-4, LLaMA, etc.) that synthesizes an answer from K
  5. Validator Agents:
     - Causal Consistency Checker: Validates output against graph paths
     - Hallucination Detector: Checks claim grounding in K
     - Rewriter/Regenerator: Corrects failed outputs

- **Critical path:**
  User Query -> (Query Refinement Agent) -> Refined Query -> (Dual-Path Retriever) -> Unified Context K -> (Generator LLM) -> Draft Answer -> (Validator Agents) -> (If failed) Regenerate/Rewrite -> Final Answer. The critical latency path is often the multi-hop graph traversal + LLM generation loops.

- **Design tradeoffs:**
  - Graph Quality vs. Coverage: Relying on GPT-4 for graph construction (as the paper mentions) ensures quality but limits scalability compared to automated extraction
  - Static vs. Dynamic Feedback: The framework is "dynamic" in query refinement, but the underlying causal graph is constructed pre-inference
  - Overhead vs. Accuracy: Multiple LLM calls (query refinement, generation, validation, potential regeneration) add significant latency and cost

- **Failure signatures:**
  - RL Policy Collapse: The query agent always selects the same action (e.g., always decompose) regardless of input, likely due to a poorly shaped reward
  - Empty Graph Retrieval: Causal retrieval returns nothing, degrading to a semantic-only RAG system. Check node alignment between query and graph entities
  - Validation Loop: The system gets stuck in a loop of regeneration because the hallucination threshold is too strict or the generator cannot satisfy the causal constraints

- **First 3 experiments:**
  1. End-to-End Metric Baseline: Run the full CDF-RAG pipeline on a held-out validation set of the target dataset to establish F1, Hallucination Rate, and CRC baselines
  2. Retrieval Ablation: Disable the Causal Graph path and measure the drop in Causal Retrieval Coverage (CRC) and F1 score. This validates the core "dual-path" contribution
  3. Refinement Policy Inspection: For a sample of queries, log the action selected by the RL agent (expand, simplify, decompose) and manually assess if it aligns with the query's complexity. Check for reward hacking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CDF-RAG maintain performance in low-resource or open-domain settings where structured causal graphs are sparse, noisy, or entirely unavailable?
- Basis in paper: The authors state in the Limitations section that the framework "depends on access to structured causal graphs, which may not be readily available... limiting applicability in open-domain or low-resource settings."
- Why unresolved: The current architecture relies on a pre-constructed Neo4j causal graph, creating a bottleneck for domains lacking curated causal knowledge.
- What evidence would resolve it: Evaluation results on datasets without pre-existing causal graphs, perhaps testing dynamic graph construction or retrieval performance when the graph is partially occluded.

### Open Question 2
- Question: Is it possible to replace the GPT-based hallucination detection module with a more efficient method to enable real-time application?
- Basis in paper: The paper notes that the "hallucination detection module employs GPT-based validation, which... incurs significant computational overhead," hindering deployment in "real-time or resource-constrained environments."
- Why unresolved: The current reliance on a large proprietary model (GPT-4) for verification creates a latency and cost barrier.
- What evidence would resolve it: A comparative study showing that a smaller, fine-tuned open-source model can achieve comparable hallucination detection scores (F1) with significantly lower latency.

### Open Question 3
- Question: How robust is the reinforcement learning query refinement policy when facing highly heterogeneous or informal user queries?
- Basis in paper: The authors admit that the RL framework's "generalization to highly heterogeneous or informal queries requires further investigation."
- Why unresolved: The RL agent was trained on specific benchmark datasets (MedQA, CosmosQA), which may not capture the full variance of real-world user language.
- What evidence would resolve it: Performance metrics (CRC, Groundedness) on out-of-distribution, noisy conversational datasets distinct from the training distribution.

## Limitations

- Causal graph construction relies heavily on GPT-4 verification of UniCausal-extracted pairs, creating a scalability bottleneck
- Reward function weights are not disclosed, making it unclear how the RL agent balances competing objectives
- The paper reports performance gains but doesn't quantify the computational overhead of multiple LLM calls

## Confidence

- **High confidence** in the dual-path retrieval mechanism's theoretical contribution, as it's well-grounded in established graph and vector search literature
- **Medium confidence** in the RL-driven query refinement, as the approach is sound but depends critically on the unspecified reward function design
- **Low confidence** in the hallucination detection module's effectiveness without knowing the exact claim decomposition and validation criteria

## Next Checks

1. Reproduce the end-to-end pipeline on a held-out validation set to verify claimed F1 and hallucination rate improvements
2. Ablation test: Disable the causal graph path and measure the drop in Causal Retrieval Coverage (CRC) to quantify its contribution
3. Reward function sensitivity analysis: Systematically vary λ weights and observe the impact on query refinement quality and final answer accuracy