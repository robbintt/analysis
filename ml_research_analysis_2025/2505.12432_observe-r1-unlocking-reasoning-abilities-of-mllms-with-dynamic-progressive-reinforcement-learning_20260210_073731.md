---
ver: rpa2
title: 'Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive
  Reinforcement Learning'
arxiv_id: '2505.12432'
source_url: https://arxiv.org/abs/2505.12432
tags:
- reasoning
- arxiv
- answer
- multimodal
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Observe-R1, a novel reinforcement learning
  framework designed to enhance reasoning capabilities of multimodal large language
  models (MLLMs). The key innovation is a progressive learning paradigm inspired by
  human learning progression, where models learn from simple to complex tasks.
---

# Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.12432
- Source URL: https://arxiv.org/abs/2505.12432
- Authors: Zirun Guo; Minjie Hong; Tao Jin
- Reference count: 7
- Primary result: Observe-R1-3B outperforms larger reasoning models on reasoning and general benchmarks using progressive RL with multimodal format constraints

## Executive Summary
This paper introduces Observe-R1, a reinforcement learning framework that enhances multimodal large language models' reasoning capabilities through a progressive curriculum approach. The authors construct the NeuraLadder dataset organized by difficulty and complexity, implement multimodal format constraints requiring explicit visual observation before reasoning, and introduce dynamic weighting that prioritizes medium-difficulty samples. Experiments with Qwen2.5-VL-3B and 7B models on 20k samples show Observe-R1-3B achieves superior performance on both reasoning and general benchmarks compared to larger models, with particular improvements in clarity and conciseness of reasoning chains.

## Method Summary
Observe-R1 trains MLLMs using modified GRPO with three key innovations: a progressive curriculum via difficulty-sorted sampling from NeuraLadder dataset, multimodal format constraints enforcing observation-reasoning separation with <observe></observe> tags, and dynamic weighting prioritizing samples with 50% success probability. The framework uses rule-based verifiable rewards (accuracy, format compliance, bonus for concise correct answers) and trains on 20k curated samples from MathVision, We-Math, PolyMath, SceMQA, and MathV360K datasets. The method achieves state-of-the-art performance on reasoning benchmarks despite using smaller model sizes.

## Key Results
- Observe-R1-3B outperforms Qwen2.5-VL-7B-Instruct and other larger models on reasoning benchmarks
- Progressive learning method shows 3.2% improvement on MathVista and 0.7% on MathVerse compared to random sampling
- Dynamic weighting and sampling strategy provides the second-best improvement among all strategies
- Observation format constraint improves visual abilities and produces clearer, more structured responses

## Why This Works (Mechanism)

### Mechanism 1: Progressive Curriculum via Difficulty-Sorted Sampling
Organizing RL training data by increasing difficulty improves reasoning acquisition compared to random sampling. NeuraLadder sorts samples by difficulty (1 − accuracy over 8 generations) and complexity (average response length), then samples from adjacent difficulty levels to smooth transitions. This allows models to first master simpler reasoning patterns before encountering harder problems. Core assumption: Reasoning capabilities transfer incrementally from simple to complex problems, mirroring human curriculum learning. Evidence anchors: [abstract] progressive learning paradigm; [Section 4.3, RQ1] 3.2% improvement on MathVista; [corpus] Infi-MMR uses curriculum-based phased RL. Break condition: If difficulty estimates are miscalibrated, models may plateau early or face instability.

### Mechanism 2: Structured Observation-Reasoning Separation
Enforcing explicit visual observation before reasoning improves multimodal grounding and reduces reasoning errors. The `<observe></observe>` format constraint forces the model to describe image contents relevant to the question before entering the reasoning chain. This creates a cognitive separation between perception and inference, reducing hallucination and information loss. Core assumption: Errors in multimodal reasoning often stem from incomplete or ungrounded visual extraction rather than flawed logic. Evidence anchors: [abstract] enhanced visual abilities and clearer responses; [Section 3.3.1] encourages careful observation; [Figure 4] GRPO baseline fails on vision-only problems. Break condition: If the observation tag adds significant token overhead without improving accuracy, or if models learn to generate superficial observations.

### Mechanism 3: Dynamic Weighting by Epistemic Uncertainty
Medium-difficulty samples provide more informative gradients than always-correct or always-incorrect samples. The weighting function f(d) = 4σd(1 − d) peaks at d = 0.5 (50% success rate) and zeroes out d = 0 and d = 1 samples. This filters uninformative extremes and focuses optimization on uncertain problems where the model has partial competence. Core assumption: Samples with moderate uncertainty offer the highest learning signal; easy samples are redundant and impossible samples provide no gradient. Evidence anchors: [abstract] prioritizes uncertain and medium-difficulty problems; [Section 3.3.3] medium-difficulty questions have greatest uncertainty; [Section 4.3, RQ4] provides second-best improvement. Break condition: If difficulty estimates drift during training, the weighting function may misallocate capacity.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Observe-R1 modifies GRPO's objective by adding dynamic sample weights; understanding the baseline advantage calculation is prerequisite.
  - Quick check question: Can you explain how GRPO computes advantages without a learned value function?

- **Rule-based verifiable rewards**
  - Why needed here: The framework depends on programmatically checkable accuracy and format rewards rather than learned reward models.
  - Quick check question: What types of multimodal tasks can support rule-based verification versus requiring human judgment?

- **Curriculum learning pacing**
  - Why needed here: NeuraLadder implements implicit curriculum via sorted sampling; practitioners must recognize when pacing is too aggressive.
  - Quick check question: What training curve signatures would indicate the curriculum is advancing faster than the model can absorb?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-7B (difficulty/complexity scoring) → NeuraLadder dataset sorting → GRPO optimizer with dynamic weights → Multimodal format constraint enforcement → Rule-based reward calculation

- **Critical path**: 
  1. Generate difficulty/complexity scores using Qwen2.5-VL-7B (8 responses per question)
  2. Sort by difficulty, then complexity; filter easy+short samples
  3. Sample with adjacency mixing for smooth curriculum
  4. Enforce observation format via system prompt
  5. Compute per-group rewards and apply bonus to shortest correct response
  6. Weight samples by f(d), masking d = 0 and d = 1
  7. Update policy via weighted GRPO objective

- **Design tradeoffs**:
  - Difficulty annotator choice: 7B balances calibration across difficulty spectrum; 3B underestimates hard problems, 72B may underestimate easy ones
  - Bonus length threshold ℓ: Low ℓ risks rewarding shallow answers; high ℓ excludes simple problems from bonus; ℓ → ∞ degenerates to standard reward
  - Dynamic weight σ: Higher σ amplifies medium-difficulty focus but may destabilize if difficulty estimates are noisy
  - Curriculum speed: Faster progression increases data efficiency but risks conceptual gaps

- **Failure signatures**:
  - Response length collapses below ℓ: Model may be skipping reasoning to chase bonus; increase ℓ or add minimum reasoning enforcement
  - Format reward stagnates: Observation tag not being learned; check system prompt adherence and format reward coefficient
  - Training rewards rise but benchmark scores fall: Potential reward gaming; inspect response patterns for superficial compliance
  - High gradient variance on hard samples: Dynamic weight not fully filtering all-incorrect samples; verify f(d) implementation

- **First 3 experiments**:
  1. Validate difficulty calibration: Compare 7B-annotated difficulty scores against human judgments on 100–200 held-out samples; report correlation and calibration error
  2. Ablate observation format: Train identical GRPO runs with standard DeepSeek-R1 format vs. <observe> format on 5k samples; measure MathVista and MathVerse gaps, especially on vision-intensive subsets
  3. Sweep dynamic weight σ: Train with σ ∈ {1.0, 1.8, 3.0} while logging per-difficulty-bucket gradient norms and validation accuracy; identify stability vs. sample-efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What types of correct answers and reasoning chains should receive additional bonus rewards beyond conciseness?
- Basis in paper: [explicit] "For example, it is promising to determine what types of correct answers and reasoning should receive additional bonus rewards. We believe that future research will delve deeper into these areas."
- Why unresolved: The current bonus reward only favors concise correct answers within a length constraint, but reasoning quality encompasses other dimensions (e.g., logical coherence, intermediate step correctness, visual grounding quality) that remain unexplored.
- What evidence would resolve it: A systematic study varying bonus reward criteria across multiple dimensions, with ablation experiments measuring downstream benchmark performance and qualitative reasoning quality metrics.

### Open Question 2
- Question: How does the choice of model used to estimate difficulty and complexity affect NeuraLadder dataset quality and final model performance?
- Basis in paper: [inferred] The authors use Qwen2.5-VL-7B specifically "because it functions as a medium-level AI assistant" for difficulty estimation, but acknowledge "the 3B version may not accurately capture the difficulty of medium and hard problems, while larger models... might fail to reflect the difficulty of easier problems."
- Why unresolved: No experiments validate whether the 7B model is optimal for difficulty estimation, or whether the ranking would differ substantially with another estimator model.
- What evidence would resolve it: Comparing NeuraLadder datasets constructed using different model sizes (3B, 7B, 72B) for difficulty/complexity estimation, then training identical models on each variant and evaluating on the same benchmarks.

### Open Question 3
- Question: What alternative dynamic weighting functions beyond f(d) = 4σd(1−d) could better prioritize informative samples during training?
- Basis in paper: [explicit] "Due to limited computational resources, we are unable to conduct detailed experiments to further explore our strategies, including the use of larger MLLMs, more dynamic weighting functions, and the impact of bonus reward weights."
- Why unresolved: The authors selected one symmetric function peaking at d=0.5, but other functional forms (asymmetric, multi-modal, learnable) might better capture the relationship between sample difficulty and training utility.
- What evidence would resolve it: A comparative study training models with alternative weighting functions (e.g., Gaussian, polynomial, learned attention-based weighting), reporting training stability, convergence speed, and final benchmark performance.

## Limitations

- **Training duration ambiguity**: Figure 5 shows ~140 steps but this may not represent final training completion, creating uncertainty about required computational resources.
- **Length constraint unspecified**: The bonus reward length threshold ℓ is not specified, affecting baseline behavior comparison and reproducibility.
- **Generalization scope limited**: Results focus on MathVision/Reasoning datasets; performance on broader real-world multimodal tasks remains untested.

## Confidence

- **High confidence** in the progressive curriculum mechanism and its 3.2% MathVista improvement - supported by direct comparison with random sampling and convergent validation from VL-Cogito.
- **Medium confidence** in the observation-reasoning separation - strong qualitative evidence (Figure 4) but no ablation on pure vision tasks or hallucination metrics.
- **Medium confidence** in dynamic weighting - supported by improvement metrics and clear theoretical motivation, but limited corpus validation beyond VL-Cogito mention.
- **Low confidence** in overall benchmark dominance - Observe-R1-3B outperforms larger models, but the claim relies on specific dataset combinations without independent replication.

## Next Checks

1. **Difficulty calibration validation**: Compare 7B-annotated difficulty scores against human judgments on 100-200 held-out samples; report correlation and calibration error to verify the foundational assumption of the dynamic weighting mechanism.

2. **Observation format ablation**: Train identical GRPO runs with standard DeepSeek-R1 format vs. <observe> format on 5k samples; measure MathVista and MathVerse gaps, especially on vision-intensive subsets, to isolate the format constraint effect.

3. **Dynamic weight sensitivity**: Train with σ ∈ {1.0, 1.8, 3.0} while logging per-difficulty-bucket gradient norms and validation accuracy; identify stability vs. sample-efficiency tradeoff and optimal σ range.