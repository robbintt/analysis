---
ver: rpa2
title: Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation
arxiv_id: '2509.01838'
source_url: https://arxiv.org/abs/2509.01838
tags:
- learning
- maritime
- data
- action
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework for maritime
  navigation that leverages AIS-derived traffic graphs and ERA5 wind fields to learn
  routing policies across multiple origin-destination pairs. The approach uses hexagonal
  grid discretization with action masking to ensure feasible movements, integrates
  historical vessel traffic patterns into reward shaping, and employs goal-conditioned
  reinforcement learning to generalize across different routes.
---

# Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation

## Quick Facts
- **arXiv ID**: 2509.01838
- **Source URL**: https://arxiv.org/abs/2509.01838
- **Reference count**: 38
- **Primary result**: RL framework for maritime navigation using AIS-derived traffic graphs and ERA5 wind fields achieves significantly higher returns than Dijkstra's and A* baselines in Gulf of St. Lawrence experiments

## Executive Summary
This paper proposes a reinforcement learning framework for maritime navigation that leverages AIS-derived traffic graphs and ERA5 wind fields to learn routing policies across multiple origin-destination pairs. The approach uses hexagonal grid discretization with action masking to ensure feasible movements, integrates historical vessel traffic patterns into reward shaping, and employs goal-conditioned reinforcement learning to generalize across different routes. Experiments in the Gulf of St. Lawrence show that action masking is critical for policy performance, with the masked PPO agent achieving significantly higher returns compared to unmasked variants. The method outperforms traditional routing baselines like Dijkstra's and A* algorithms, demonstrating the effectiveness of combining large-scale traffic data with adaptive RL for real-world maritime navigation.

## Method Summary
The framework trains a goal-conditioned PPO agent on a hexagonal grid (H3 resolution 6) representing the Gulf of St. Lawrence. The agent receives historical AIS trajectories processed into a Markovian traffic graph with edge weights representing transition frequencies and average speeds. Wind data from ERA5 provides environmental context for routing decisions. The multi-discrete action space includes five maneuver directions and five speed levels (8-22 knots). Key innovations include action masking to prevent invalid moves through land, reward shaping using historical traffic frequency to guide exploration, and goal-conditioned learning via state concatenation to enable generalization across multiple origin-destination pairs without retraining.

## Key Results
- Action masking is critical: masked PPO achieves positive returns (~68) while unmasked variants fail catastrophically with returns around -1500
- The RL agent outperforms Dijkstra's and A* baselines, particularly in wind-heavy scenarios where traditional methods struggle
- Reward shaping using historical traffic frequency is essential for learning progress; agents with penalties only make little meaningful progress
- Goal-conditioned approach successfully generalizes across multiple routes without retraining on specific origin-destination pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Invalid-action masking appears to be the primary driver of policy convergence and stability in discrete maritime environments.
- **Mechanism:** By applying a binary mask $m_t(s)$ to eliminate non-navigable transitions (e.g., land, backtracking), the policy gradient updates are restricted to the set of feasible maneuvers. This prevents the agent from wasting exploration capacity on physically impossible states and avoids terminal penalties for invalid moves.
- **Core assumption:** The environment can be accurately discretized into a graph where "valid" vs. "invalid" moves are known a priori (static constraints).
- **Evidence anchors:** [abstract] "...action masking is critical for policy performance, with the masked PPO agent achieving significantly higher returns..."; [section IV-C2] "With action masking, agents achieve consistently high positive returns, whereas unmasked agents fail catastrophically due to frequent selection of invalid actions..."
- **Break condition:** If the environment dynamics change dynamically (e.g., moving obstacles not captured in the static graph) where valid/invalid states are not known at step $t$, a static mask may trap the agent or cause collisions.

### Mechanism 2
- **Claim:** Reward shaping using historical traffic frequency ($r_{freq}$) bridges the gap between sparse environmental penalties and convergent behavior.
- **Mechanism:** The AIS-derived traffic graph provides a density prior. By rewarding transitions along high-traffic edges (Eq. 19), the agent receives dense intermediate feedback ("soft preference") that guides it toward corridor-like paths before it learns to optimize fuel or wind resistance.
- **Core assumption:** Historical AIS tracks represent approximately optimal or safe behaviors; high-frequency regions correlate with navigability.
- **Evidence anchors:** [section IV-C1] "...without the graph-derived shaping rewards the agent makes little meaningful progress."; [section III-E4] "The term $r_{freq}$ guides the agent toward frequently traveled routes... acts as a soft preference, indicating that cells traversed by many vessels are likely safe..."
- **Break condition:** If the historical data contains systematic biases (e.g., all historical ships took a suboptimal detour due to a temporary exclusion zone now lifted), the agent will converge on a suboptimal "safe" path.

### Mechanism 3
- **Claim:** Goal-conditioning via state concatenation enables generalization across multiple origin-destination pairs without retraining.
- **Mechanism:** The policy $\pi(a|s,g)$ receives the goal coordinates as part of the observation vector $s_t$ (Eq. 11). This allows the value function to modulate its output based on the relative distance and direction to the specific destination, turning path-finding into a universal function approximation rather than a single-route lookup.
- **Core assumption:** The observation normalization (min-max scaling) successfully preserves the spatial relationship between current position and goal across different geographic scales.
- **Evidence anchors:** [abstract] "...leverages goal-conditioned reinforcement learning to generalize across different routes."; [section III-E1] "The resulting observation vector... includes... goal lat/lon norm."
- **Break condition:** If the goal distance exceeds the training distribution significantly (out-of-distribution generalization), the policy may fail to extrapolate.

## Foundational Learning

- **Concept:** **Hexagonal Discretization (H3)**
  - **Why needed here:** The paper rejects square grids in favor of hexagons to ensure uniform neighbor distances (the "one-distance rule"). This removes directional bias (diagonal vs. edge) and simplifies graph traversal logic.
  - **Quick check question:** Can you explain why a square grid introduces distance bias that a hexagonal grid resolves?

- **Concept:** **Markov Decision Processes (MDP) with Action Masking**
  - **Why needed here:** Standard RL assumes all actions are valid at every step. This paper utilizes a modified MDP where the action space is dynamically constrained, a critical distinction for navigation in constrained waterways.
  - **Quick check question:** How does invalid-action masking mathematically alter the policy gradient (hint: normalization of probabilities)?

- **Concept:** **Reward Engineering (Shaping vs. Penalties)**
  - **Why needed here:** The paper demonstrates that penalty-only rewards (fuel, time, wind) are insufficient for learning. Understanding the balance between positive shaping ($r_{freq}$) and negative penalties ($r_{wind}, r_{fuel}$) is key to replicating results.
  - **Quick check question:** Why might a "penalty-only" reward signal lead to an agent that refuses to move or makes little progress?

## Architecture Onboarding

- **Component map:** Data Layer (AISdb + ERA5) -> Spatial Layer (H3 Hexagonal Grid) -> Graph Layer (Markovian Traffic Graph) -> RL Environment (Gymnasium wrapper) -> Agent (PPO with Maskable-PPO logic)
- **Critical path:** The preprocessing of AIS data into the Markovian Graph is the most brittle step. If the edge weights ($p_{ij}$) are not correctly normalized or if the land-masking is imprecise, the agent will either learn to drive through land or fail to find high-reward corridors.
- **Design tradeoffs:**
  - RNN vs. MLP: The paper explicitly advises *against* using Recurrent Networks (LSTMs) in this specific setup, finding that a history-augmented state (stacking 8 observations) with a simple MLP is more stable and performs better.
  - Resolution: H3 Resolution 6 ($\approx 36km^2$) balances computational load against route fidelity; lower resolution loses nuance, higher resolution explodes the graph size.
- **Failure signatures:**
  - Catastrophic Failure: Agent receives large negative rewards (e.g., -1900) immediately. *Diagnosis:* Action masking is likely disabled or incorrectly implemented in the environment wrapper.
  - Stagnation: Agent loops or stays stationary. *Diagnosis:* Reward scaling is off, or the positive shaping reward ($r_{freq}$) is missing, leaving only penalties.
  - High Variance: Performance fluctuates wildly across seeds. *Diagnosis:* Check RND exploration bonus weighting or ensure environment normalization is applied.
- **First 3 experiments:**
  1. Masking Ablation: Run `Masked PPO` vs. `Standard PPO` on a single short route. Verify that unmasked agents hit invalid-action penalties while masked agents converge.
  2. Reward Component Analysis: Train three agents: (a) Penalties only, (b) Penalties + $r_{freq}$, (c) Full Reward. Compare mean returns to confirm the necessity of traffic-based shaping.
  3. Baselines: Run Dijkstra/A* on the graph to establish a performance floor/ceiling, then validate if the RL agent can outperform these static methods in wind-heavy scenarios (using ERA5 data).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed GCRL framework generalize to unseen geographic regions and seasonal conditions, such as those involving ice coverage, without retraining?
- **Basis in paper:** [explicit] Section V states that "Broader validation across geographic regions, traffic regimes, and seasonal conditions will be essential to assess global generalization," specifically mentioning ice.
- **Why unresolved:** The current study evaluates the agent only on the Gulf of St. Lawrence using AIS data from 2024 and wind data from August 2024, limiting the diversity of environmental conditions.
- **What evidence would resolve it:** Evaluation of the trained policy in distinct maritime regions (e.g., different traffic densities) and under winter conditions where sea ice restricts navigable space.

### Open Question 2
- **Question:** Does replacing the multi-discrete action space with continuous control and adding hydrodynamic factors (currents, tides, waves) improve policy transferability to real-world autopilot systems?
- **Basis in paper:** [explicit] Section V notes the limitation that "the current use of a multi-discrete action space restricts alignment with continuous autopilot controls" and suggests adding "currents, tides, waves, and ice."
- **Why unresolved:** The current methodology relies on a simplified cubic-law fuel model and a discrete set of speed/maneuver options, which omits complex vessel dynamics and continuous control requirements.
- **What evidence would resolve it:** A comparative study where an agent trained with continuous action spaces and hydrodynamic models is tested on high-fidelity ship simulators or actual autopilot hardware.

### Open Question 3
- **Question:** Can inverse reinforcement learning (IRL) or Pareto optimization better capture the implicit operational constraints of historical routes than the proposed hand-crafted reward function?
- **Basis in paper:** [explicit] Section V suggests that "Reward engineering should evolve toward potential-based shaping, Pareto optimization, or inverse reinforcement learning."
- **Why unresolved:** The current results show that historical routes achieve low returns because they optimize for objectives "not captured in our reward function" (e.g., regulatory compliance), indicating a gap between the manual reward design and reality.
- **What evidence would resolve it:** Implementation of an IRL algorithm that infers reward weights from historical AIS trajectories, followed by a comparison of route similarity and safety compliance against the hand-shaped baseline.

## Limitations

- **Data dependency:** The framework heavily depends on proprietary AIS data from Kpler, which is not publicly accessible, making exact reproduction challenging.
- **Geographic generalization:** The study focuses solely on the Gulf of St. Lawrence without validation in other maritime regions with different traffic patterns or geographic constraints.
- **Static environment assumption:** The action masking approach assumes static constraints (land, backtracking) and may not handle dynamic obstacles or changing navigability conditions effectively.

## Confidence

- **High Confidence**: The effectiveness of action masking as a critical component for policy convergence is well-supported by ablation studies showing masked agents achieve significantly higher returns (68 vs. -1500 for unmasked variants).
- **Medium Confidence**: The generalization capability of the goal-conditioned approach is demonstrated through experimental results, though the specific evaluation focuses on a single geographic region without extensive cross-regional validation.
- **Medium Confidence**: The reward shaping mechanism using historical traffic frequency is supported by evidence showing poor performance without this component, but the assumption that historical AIS tracks represent optimal behaviors may not hold universally.

## Next Checks

1. **Cross-Regional Generalization**: Test the trained policy on maritime regions outside the Gulf of St. Lawrence (e.g., Mediterranean, North Sea) to assess generalization capabilities and identify potential domain shift issues.

2. **Dynamic Obstacle Handling**: Evaluate policy performance when introducing dynamic obstacles not captured in the static traffic graph, testing the break condition where valid/invalid states change during navigation.

3. **AIS Data Sensitivity Analysis**: Substitute the proprietary Kpler AIS data with open-source alternatives and measure the degradation in performance to quantify the impact of data quality and distribution on the learned policy.