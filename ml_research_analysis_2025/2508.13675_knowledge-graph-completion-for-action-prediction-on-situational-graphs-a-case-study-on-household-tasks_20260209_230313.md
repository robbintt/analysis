---
ver: rpa2
title: Knowledge Graph Completion for Action Prediction on Situational Graphs -- A
  Case Study on Household Tasks
arxiv_id: '2508.13675'
source_url: https://arxiv.org/abs/2508.13675
tags:
- knowledge
- action
- prediction
- graph
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates knowledge graph completion methods for predicting
  human actions in household tasks using the KIT Bimanual Actions Dataset. The dataset
  provides structured, frame-level annotations of bimanual activities like cooking
  and assembling, enabling the construction of situational knowledge graphs that model
  action sequences and object interactions.
---

# Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks

## Quick Facts
- arXiv ID: 2508.13675
- Source URL: https://arxiv.org/abs/2508.13675
- Authors: Mariam Arustashvili; Jörg Deigmöller; Heiko Paulheim
- Reference count: 40
- One-line primary result: Standard KG embedding models struggle with disconnected, sequential data in household task prediction, while simple heuristics and LLMs excel at coarse task recognition but fail on fine-grained predictions.

## Executive Summary
This paper evaluates knowledge graph completion methods for predicting human actions in household tasks using the KIT Bimanual Actions Dataset. The dataset provides structured, frame-level annotations of bimanual activities like cooking and assembling, enabling the construction of situational knowledge graphs that model action sequences and object interactions. Experiments compared embedding-based link prediction models (TransE, TransR, ComplEx, DistMult, RotatE), their literal variants, simple statistical baselines, and GPT-4o-mini.

## Method Summary
The study constructs situational knowledge graphs from the KIT Bimanual Actions Dataset, which contains 540 RGB-D recordings of household tasks. The graphs model four relations: `has_actor`, `has_object`, `has_element` (sub-action to parent task), and `has_next` (sequential action transitions). Two prediction tasks are evaluated: parent action prediction (identifying the overall task from sub-action sequence) and subsequent action prediction (forecasting the next sub-action). Models include frequency-based baselines (action-only and action+object), KG embedding models via PyKeen (TransE, TransR, ComplEx, DistMult, RotatE + Literal variants), and GPT-4o-mini with few-shot prompting. A fixed train/test split uses 8/10 repetitions per task for training and 2/10 for testing.

## Key Results
- For parent action prediction, frequency-based baselines and GPT-4o-mini achieved 76–100% Hits@3, while graph models reached only 3–6% Hits@5
- For sub-action prediction, statistical baselines led with ~82% Hits@1, graph models peaked at 52% Hits@1, and GPT-4o-mini dropped to 13%
- RotatE embeddings captured sequential "has next" relations more effectively than translational models for fine-grained action forecasting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-based heuristics outperform standard embedding models for high-level task recognition (parent action prediction) in fragmented graphs.
- **Mechanism:** Statistical baselines (e.g., Baseline2) map observed sub-sequences to goals via majority voting and co-occurrence counts (e.g., "cut–knife" → "cooking"). This bypasses the need for dense graph topology.
- **Core assumption:** The dataset contains a strong, deterministic correlation between specific object-action pairs and the overall task, which can be captured without latent representations.
- **Evidence anchors:**
  - [abstract] "frequency-based baselines... achieved 76–100% Hits@3, while graph models reached only 3–6%."
  - [section 4] "Baseline2 extends this by using pairs of sub-actions and their associated objects... Including object context significantly improves performance."
  - [corpus] Weak direct support; neighboring papers focus on dense graph completion rather than disconnected heuristic comparisons.
- **Break condition:** Fails if tasks are highly ambiguous (same sub-actions appearing in different parent tasks without distinct object contexts) or if the training distribution differs significantly from the test distribution.

### Mechanism 2
- **Claim:** RotatE embeddings capture sequential "has next" relations more effectively than translational models (TransE) for fine-grained action forecasting.
- **Mechanism:** RotatE models relations as rotations in complex space, which naturally accommodates the compositional and potentially cyclic nature of sequential steps, whereas TransE struggles with multi-step dependencies in this specific graph structure.
- **Core assumption:** The underlying sequential dependencies follow patterns learnable via relational rotation, and the "has next" relation is distinct enough to be modeled geometrically.
- **Evidence anchors:**
  - [section 4] "graph models like RotatE (52.16% Hits@1) showed limited but non-trivial capability—likely due to their ability to model localized action transitions."
  - [section 4] "TransE and TransR struggle with multistep dependencies."
  - [corpus] *Knowledge Graph Completion with Mixed Geometry* supports that specific geometric assumptions (e.g., hyperbolic vs Euclidean) significantly impact structure capturing.
- **Break condition:** Performance degrades if action sequences are highly stochastic or non-Markovian, where simple rotation cannot model long-term temporal dependencies.

### Mechanism 3
- **Claim:** Large Language Models (LLMs) provide strong semantic priors for coarse classification but fail on precise, dataset-specific sequential forecasting.
- **Mechanism:** GPT-4o-mini uses few-shot prompting to map semantic descriptions of sub-actions to likely high-level goals ("common sense" reasoning). It fails on sub-action prediction because it lacks the specific procedural memory or statistical regularity of the dataset's specific execution order.
- **Core assumption:** The LLM's pre-training includes sufficient knowledge about household tasks to generalize from few examples, but not the specific temporal jitter or variations of the KIT dataset.
- **Evidence anchors:**
  - [abstract] "GPT-4o-mini... excel at coarse task recognition but fail on fine-grained predictions."
  - [section 4] "Strikingly, LLMs collapsed to 13% accuracy here [sub-action], exposing their weakness in fine-grained sequential reasoning."
  - [corpus] *GenIC* and *LLM-Guided Dynamic-UMAP* suggest LLMs are better suited for semantic tasks or augmentation than pure structural link prediction in sparse settings.
- **Break condition:** Fails if the prompt context window is exceeded, if the task is novel (out of distribution), or if the specific sequence order contradicts "common sense" logic.

## Foundational Learning

- **Concept: Disconnected Knowledge Graph Topology**
  - **Why needed here:** The paper highlights that the KIT dataset results in 540 weakly connected components (one per recording), violating the dense connectivity assumptions of standard KGC models (TransE/ComplEx). Understanding this fragmentation is crucial for interpreting why graph models failed (3-6% Hits).
  - **Quick check question:** Does the link prediction task require traversing between separate video recordings, or does it only operate within a single session's subgraph?

- **Concept: Hits@k Metric in Temporal Contexts**
  - **Why needed here:** The evaluation uses Hits@1, 3, and 5 to measure if the correct next action or parent action appears in the top $k$ predictions. High Hits@1 for sub-actions (~82% baseline) vs low Hits@5 for parent actions (3-6% graph models) indicates the difference between deterministic sequences and ambiguous goal recognition.
  - **Quick check question:** If a model predicts the next action with 50% Hits@1, is it learning the task or just guessing the most frequent pause in the sequence?

- **Concept: Semantic vs. Structural Link Prediction**
  - **Why needed here:** The paper contrasts LLMs (semantic reasoning) with Embeddings (structural geometry). Recognizing that "parent action" is largely a semantic classification task explains why LLMs and frequency baselines won, while "next action" (structural transition) favored RotatE.
  - **Quick check question:** Is the relationship "has element" (sub-action to parent) defined by logical inference/semantics or by graph topology?

## Architecture Onboarding

- **Component map:** JSON annotations (KIT Dataset) → Preprocessor: Bounding box intersection to link objects to actions → Graph Builder: Creates 540 independent subgraphs using relations: has_element, has_next, has_object, has_actor → Models: Path A (Baselines): Frequency counters & Majority Voting; Path B (KGC): PyKeen embeddings (RotatE, TransE, etc.) trained on merged but disconnected components; Path C (LLM): Triple textualizer → GPT-4o-mini few-shot prompt → Evaluator: Hits@k calculation on a fixed test split (2 of 10 recordings per task)

- **Critical path:** The **Bounding Box Intersection** logic is the primary data bottleneck; errors here break the `has_object` relation, which is critical for the high-performing Baseline2.

- **Design tradeoffs:**
  - **Baseline vs. KG:** Baselines are cheap and highly accurate for this specific dataset structure but lack generalization to new relations. KG models are computationally heavier and failed here due to fragmentation but are theoretically more flexible.
  - **LLM vs. Statistical:** LLMs offer easy "plug-and-play" reasoning without training but require API costs and prompt engineering; they failed on temporal precision. Statistical methods are precise but brittle to distribution shifts.

- **Failure signatures:**
  - **TransE/TransR (0% Hits@1 for sub-action):** Indicates the model could not learn sequential transitions, likely due to the "curse of reciprocity" (0.0 reciprocity in stats) or disconnected components preventing global embedding alignment.
  - **LLM (13% Hits@1 for sub-action):** Hallucinating plausible but incorrect sequences (semantic plausibility ≠ ground truth execution).
  - **Graph Models (3-6% Parent Action):** The models cannot aggregate local sub-actions into a global parent concept without cross-component connectivity or a specific hierarchical encoder.

- **First 3 experiments:**
  1. **Reproduce Baseline2:** Implement the object-action co-occurrence counter to establish the upper bound for parent action prediction.
  2. **Test RotatE on `has_next`:** Train RotatE specifically on the sub-sequence extraction to verify the 52% Hits@1 claim on the "has next" relation.
  3. **Ablate Context:** Run GPT-4o-mini with *only* action labels (no objects) to quantify how much object semantics contribute to its 78% parent prediction score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation protocols be redefined to effectively handle disconnected graph components in link prediction tasks?
- Basis in paper: [explicit] The authors state that future research should "redefine evaluation protocols for LP in disconnected graphs, moving beyond random link masking."
- Why unresolved: Standard benchmarks assume dense connectivity, but situational graphs often consist of weakly connected components (540 in this study), causing standard random masking to fail or produce misleading metrics.
- What evidence would resolve it: A novel evaluation framework that accounts for component boundaries and yields performance metrics correlating with downstream robotic task success.

### Open Question 2
- Question: Can hybrid architectures combining statistical baselines with KG relational reasoning outperform current methods on fine-grained action prediction?
- Basis in paper: [explicit] The conclusion suggests the need to "develop hybrid architectures that integrate baseline robustness with KG relational reasoning."
- Why unresolved: Statistical baselines currently dominate sub-action prediction (82% Hits@1), while KG models struggle with sequential dependencies, indicating neither approach is sufficient alone.
- What evidence would resolve it: A model that combines frequency-based heuristics with embedding-based reasoning to surpass the current 81.7% baseline for subsequent action prediction.

### Open Question 3
- Question: Do dynamic graph embeddings capture temporal action progression more effectively than the static models evaluated?
- Basis in paper: [explicit] The paper proposes to "explore dynamic graph embeddings to capture action progression."
- Why unresolved: Static embedding models (e.g., TransE, RotatE) failed to model the hierarchical and temporal nature of actions, resulting in low parent action prediction scores.
- What evidence would resolve it: Results showing that temporal embedding techniques significantly improve Hits@k on parent action prediction compared to the static baselines reported (≤5.72%).

## Limitations

- The dataset's 540 disconnected components violate standard KG embedding assumptions, artificially constraining model performance and making direct comparisons potentially misleading
- GPT-4o-mini results depend on unspecified few-shot prompt configuration, which could significantly affect the 13% Hits@1 for sub-action prediction
- The study doesn't explore alternative KG architectures designed for sequential or temporal data, limiting conclusions about embedding models' fundamental capabilities

## Confidence

- **High confidence:** Frequency-based baselines' superiority for parent action prediction (76–100% Hits@3) is well-supported by the data and methodology.
- **Medium confidence:** RotatE's edge over TransE for sub-action prediction (~52% Hits@1) is plausible given the geometric mechanism, but could be influenced by embedding hyperparameters not reported.
- **Low confidence:** The claim that LLMs fail at fine-grained predictions due to lacking procedural memory is speculative—prompt engineering quality and few-shot example selection could be confounding factors.

## Next Checks

1. **Ablate Context:** Run GPT-4o-mini with only action labels (no objects) to quantify how much object semantics contribute to its 78% parent prediction score.
2. **Connectivity Test:** Merge a subset of graph components (e.g., 2–3 recordings per task) and retrain KG models to determine if performance improves with basic connectivity.
3. **Temporal Ablation:** Remove the "has_next" relation and test whether RotatE still outperforms frequency baselines, isolating sequential learning from relation-specific capabilities.