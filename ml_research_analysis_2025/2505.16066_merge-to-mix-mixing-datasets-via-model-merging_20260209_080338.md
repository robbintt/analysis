---
ver: rpa2
title: 'Merge to Mix: Mixing Datasets via Model Merging'
arxiv_id: '2505.16066'
source_url: https://arxiv.org/abs/2505.16066
tags:
- arxiv
- dataset
- datasets
- selection
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Merge to Mix introduces a method for selecting optimal dataset
  mixtures for fine-tuning large models by leveraging model merging as a surrogate
  for mixture performance. The core idea is that merged models, created by averaging
  individually fine-tuned models, strongly correlate with the performance of models
  fine-tuned directly on the full dataset mixture, enabling efficient exploration
  of candidate mixtures without costly fine-tuning.
---

# Merge to Mix: Mixing Datasets via Model Merging

## Quick Facts
- arXiv ID: 2505.16066
- Source URL: https://arxiv.org/abs/2505.16066
- Reference count: 40
- Primary result: Method for selecting optimal dataset mixtures for fine-tuning by using merged models as performance surrogates

## Executive Summary
Merge to Mix introduces a method for selecting optimal dataset mixtures for fine-tuning large models by leveraging model merging as a surrogate for mixture performance. The core idea is that merged models, created by averaging individually fine-tuned models, strongly correlate with the performance of models fine-tuned directly on the full dataset mixture, enabling efficient exploration of candidate mixtures without costly fine-tuning. Experiments across computer vision and language tasks show that Merge to Mix consistently outperforms traditional approaches like fine-tuning on all datasets or similarity-based selection, achieving near-oracle performance while reducing computational overhead.

## Method Summary
The method works by first independently fine-tuning a pre-trained model on each candidate dataset with uniform hyperparameters. Then, instead of fine-tuning on all possible dataset combinations (which would be computationally expensive), it creates merged models by averaging the parameters of subsets of the individually fine-tuned models. These merged models are evaluated on the target task validation set to identify the optimal dataset mixture. Finally, the model is fine-tuned on the selected optimal mixture for deployment. The approach exploits the observation that merged models correlate strongly with models fine-tuned directly on the corresponding dataset mixtures.

## Key Results
- Merge to Mix achieves near-oracle performance across computer vision and language tasks
- Correlation between merged models and mixture-fine-tuned models: r=0.78 (vision), r=0.57 (language)
- Consistently outperforms fine-tuning on all datasets or similarity-based selection approaches
- Reduces computational overhead by replacing expensive mixture fine-tuning with parameter averaging

## Why This Works (Mechanism)

### Mechanism 1: Merged Model as Performance Surrogate
The core insight is that merged models (parameter-averaged from individually fine-tuned models) exhibit performance that positively correlates with models fine-tuned directly on the combined dataset mixture. This allows rapid evaluation of arbitrary dataset combinations without retraining. The correlation is empirically supported but not theoretically proven, and degrades when individual fine-tunings diverge significantly in hyperparameters or when loss landscapes are highly non-convex.

### Mechanism 2: Uniform Fine-Tuning Enables Effective Merging
Standardizing fine-tuning hyperparameters across all individual datasets improves merge quality by reducing interference patterns when parameters are averaged. All datasets are fine-tuned with the same learning rate, batch size, and epochs from the shared initialization, which reduces hyperparameter sensitivity. This creates tension with optimal individual performance when datasets require different training conditions.

### Mechanism 3: Exhaustive Search Becomes Tractable
Replacing fine-tuning with merging reduces mixture evaluation from O(training_time) to O(inference_time), making exhaustive search feasible for moderate N. Once N individual models are pre-computed, any of the 2^N possible mixtures can be evaluated via parameter averaging plus a single forward pass on the target task validation set. This becomes prohibitive for N > 15-20, requiring integration with search algorithms.

## Foundational Learning

- **Model Merging / Weight Averaging**
  - Why needed here: Core technique enabling surrogate construction; understanding why averaging fine-tuned weights preserves capabilities is essential
  - Quick check question: Can you explain why averaging two models fine-tuned on different datasets might outperform either individual model on a target task?

- **Surrogate-Based Optimization**
  - Why needed here: Merge to Mix reformulates dataset selection as surrogate optimization; the merged model serves as a cheap proxy for the expensive true objective
  - Quick check question: What conditions must a surrogate satisfy for optimization over the surrogate to yield good solutions for the original problem?

- **Transfer Learning and Fine-Tuning Dynamics**
  - Why needed here: The method assumes fine-tuned models can be meaningfully combined; understanding how fine-tuning modifies pre-trained weights informs when merging is viable
  - Quick check question: What happens to the correlation between merged and mixture-fine-tuned models if individual fine-tuning runs catastrophically forget different sets of capabilities?

## Architecture Onboarding

- **Component map:**
  - Pre-trained model → Individual fine-tunings → Merged models → Evaluation → Optimal mixture selection → Final fine-tuning

- **Critical path:**
  1. Individual fine-tunings dominate upfront cost (N training runs)
  2. Merge + evaluate loop is O(2^N × inference_cost)
  3. Final deployment requires one additional fine-tuning run on selected mixture

- **Design tradeoffs:**
  - Larger N → better mixture exploration but higher upfront cost and exponential search space
  - Stricter hyperparameter uniformity → better merging quality but potentially suboptimal individual models
  - Alternative merging methods (Task Arithmetic, TIES) → more expressive than simple averaging but introduce tuning overhead

- **Failure signatures:**
  - Correlation breakdown: Merged model performance becomes uncorrelated with mixture-fine-tuned performance (monitor via held-out validation pairs)
  - Negative transfer: Merging harmful datasets degrades target task performance below pre-trained baseline
  - Hyperparameter sensitivity: Individual models fine-tuned with different settings produce incompatible weight configurations

- **First 3 experiments:**
  1. **Correlation validation:** Fine-tune on all 2^N mixtures (small N=4), plot merged vs. mixture-fine-tuned accuracy; confirm Pearson r > 0.5 on your target domain.
  2. **Ablation on uniformity:** Deliberately vary fine-tuning hyperparameters across datasets; measure correlation degradation to quantify sensitivity.
  3. **Scale test:** For N=10 datasets, implement random sampling vs. exhaustive search via merging; verify that exhaustive finds better mixtures within tractable time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do merged models demonstrate enhanced resilience and better resist performance degradation compared to mixture-fine-tuned models on certain target tasks?
- Basis in paper: [explicit] The authors note on Page 9 that merged models were less susceptible to performance degradation relative to the pre-trained model than mixture-fine-tuned models were, stating, "These phenomena raise the intriguing question of why merged models better resist fine-tuning that harms the target task."
- Why unresolved: The paper empirically observes this phenomenon in image classification tasks but does not provide a theoretical explanation for the robustness of the parameter averaging operation.
- What evidence would resolve it: A theoretical analysis or ablation study isolating the specific properties of the parameter space (e.g., mode connectivity, loss landscape geometry) that allow merged weights to retain capabilities lost during standard fine-tuning.

### Open Question 2
- Question: Can Merge to Mix be effectively extended to support weighted dataset selection (α ∈ [0, 1]^N) rather than just binary inclusion?
- Basis in paper: [explicit] Section 5 states that the method "can be extended to weighted dataset selection" and suggests strategies like weighted averaging or dataset duplication to simulate weights.
- Why unresolved: The current method and experiments only validate binary selection variables (α ∈ {0,1}^N), leaving the continuous weight optimization as a conceptual extension.
- What evidence would resolve it: Empirical results showing that using weighted merging techniques to approximate continuous mixture ratios correlates strongly with models fine-tuned on those specific weighted mixtures.

### Open Question 3
- Question: How can search algorithms or predictive models be integrated to maintain efficiency as the number of candidate datasets (N) grows significantly?
- Basis in paper: [explicit] Section 5 highlights that the brute-force approach is "computationally prohibitive" for large N and suggests integration with "search algorithms or predictive models."
- Why unresolved: The provided experiments only validate the method on small pools of datasets (N ∈ {6, 7}) where exhaustive search is feasible.
- What evidence would resolve it: Experiments demonstrating that combining the surrogate with a heuristic search (e.g., greedy search, beam search) successfully identifies optimal mixtures in a space with N > 20 without exhaustive evaluation.

### Open Question 4
- Question: Do alternative merging techniques (e.g., Task Arithmetic, TIES-Merging) improve the selection accuracy of the framework compared to simple averaging?
- Basis in paper: [explicit] Section 5 notes that "alternative merging methods... are worth investigating as standalone techniques for their potential impact on selection outcomes."
- Why unresolved: The study strictly utilizes simple averaging for merging to avoid tuning additional hyperparameters, leaving other sophisticated merging methods unexplored.
- What evidence would resolve it: A comparative analysis of selection performance when the surrogate model is constructed using advanced merging methods versus simple parameter averaging.

## Limitations

- Correlation between merged and mixture-fine-tuned models is empirically observed but not theoretically proven
- The method relies on uniform fine-tuning hyperparameters, creating tension with optimal individual performance
- Exhaustive search via merging becomes intractable for large candidate pools (N > 15-20)

## Confidence

- **High Confidence**: The core mechanism (model merging as surrogate) works empirically on tested tasks; correlation exists and enables near-oracle mixture selection for N ≤ 10
- **Medium Confidence**: The method generalizes to other vision and language tasks with similar characteristics; uniform fine-tuning assumption holds reasonably well
- **Low Confidence**: Theoretical guarantees for correlation; performance on significantly larger N; robustness to hyperparameter variations

## Next Checks

1. **Correlation validation**: Fine-tune on all 2^N mixtures (small N=4), plot merged vs. mixture-fine-tuned accuracy; confirm Pearson r > 0.5 on your target domain.

2. **Ablation on uniformity**: Deliberately vary fine-tuning hyperparameters across datasets; measure correlation degradation to quantify sensitivity.

3. **Scale test**: For N=10 datasets, implement random sampling vs. exhaustive search via merging; verify that exhaustive finds better mixtures within tractable time.