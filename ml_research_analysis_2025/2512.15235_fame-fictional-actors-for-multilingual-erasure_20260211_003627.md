---
ver: rpa2
title: 'FAME: Fictional Actors for Multilingual Erasure'
arxiv_id: '2512.15235'
source_url: https://arxiv.org/abs/2512.15235
tags:
- unlearning
- across
- information
- each
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAME introduces the first multilingual benchmark for evaluating
  Machine Unlearning in LLMs across five languages, supporting both entity-level and
  instance-level forgetting. The benchmark contains 1,000 fictional actor biographies
  and 20,000 QA pairs, with each biography structured into 20 atomic facts across
  four semantic categories.
---

# FAME: Fictional Actors for Multilingual Erasure

## Quick Facts
- **arXiv ID**: 2512.15235
- **Source URL**: https://arxiv.org/abs/2512.15235
- **Reference count**: 0
- **Primary result**: Introduces the first multilingual benchmark for evaluating Machine Unlearning in LLMs across five languages, supporting both entity-level and instance-level forgetting

## Executive Summary
FAME addresses the fundamental challenge of evaluating Machine Unlearning in LLMs by introducing a controlled benchmark using entirely fictional actor biographies. The benchmark contains 1,000 synthetic biographies and 20,000 QA pairs across five languages, enabling researchers to measure true forgetting efficacy without pretraining contamination. The structured design supports both entity-level unlearning (forgetting entire identities) and instance-level unlearning (forgetting specific facts while retaining others), with evaluation metrics covering efficacy, utility, and efficiency. Experimental results on Llama 1B and 3B models demonstrate that balanced unlearning methods like KL Minimization and Preference Optimization outperform simple approaches, achieving better trade-offs between forgetting efficacy and model utility.

## Method Summary
FAME generates 1,000 fictional actor biographies using a controlled pipeline with metadata sampling, Gemini 2.5 Flash generation, and JSON schema validation. Each biography contains 20 atomic facts across four semantic categories (biography, career, achievements, personal). The dataset supports two evaluation splits: entity-based (identities in retain/forget/test sets) and topic-based (facts distributed across splits for instance-level evaluation). Five unlearning methods are evaluated: Fine-tuning (FT), Gradient Ascent (GA), Gradient Difference (GD), KL Minimization (KLM), and Preference Optimization (PO). Models are fine-tuned on Llama-3.2-1B-Instruct or 3B-Instruct, then unlearned using one of the methods, with evaluation against a Gold model trained only on retain data.

## Key Results
- KL Minimization and Gradient Difference achieve the best trade-offs between forgetting efficacy and model utility
- Gradient Ascent shows artificially strong forgetting scores but causes model collapse, destroying overall capabilities
- Preference Optimization successfully prevents information leakage but creates detectable behavioral artifacts that reveal prior exposure
- Balanced methods maintain utility scores above 70 while reducing forget-set performance below 50 on entity-based splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fictional synthetic data enables controlled evaluation by guaranteeing zero pretraining contamination
- Mechanism: Since FAME uses entirely fictional actor biographies that never existed in any web corpus, models cannot have encountered this information during pretraining. This allows researchers to precisely attribute any knowledge to the fine-tuning phase and measure true forgetting efficacy rather than guessing whether information was ever learned.
- Core assumption: Models do not hallucinate consistent fictional identities that happen to match the benchmark's synthetic data through chance.
- Evidence anchors:
  - [abstract] "Since FAME uses entirely fictional data, it ensures that the information was never encountered during model pretraining, allowing for a controlled evaluation of unlearning methods."
  - [section 1] "A fundamental challenge in creating such benchmarks is ensuring that evaluation data was not present in the model's pretraining corpus... Without this guarantee, it becomes impossible to determine whether the model truly forgot information or simply never learned it in the first place."
  - [corpus] Related paper "Unlearning Isn't Deletion" explicitly warns that task-level metrics can be misleading, reinforcing the need for controlled evaluation setups like FAME.
- Break condition: If models begin training on synthetic benchmarks or fictional datasets at scale, this guarantee erodes.

### Mechanism 2
- Claim: Structured atomic facts enable instance-level unlearning evaluation that reflects real-world partial deletion requests
- Mechanism: Each biography is decomposed into exactly 20 atomic facts across four semantic categories (biography, career, achievements, personal). The topic-based split distributes individual facts across retain/forget/test sets, allowing the same identity to have some facts forgotten while others are preserved. This isolates whether unlearning methods can selectively remove specific attributes without catastrophic interference.
- Core assumption: Facts are sufficiently independent that forgetting one does not inherently compromise recall of others within the same entity.
- Evidence anchors:
  - [abstract] "This design enables both entity-level unlearning (i.e., forgetting entire identities) and instance-level unlearning (i.e., forgetting specific facts while retaining others)."
  - [section 3.2] "In this configuration, the split is computed over the 20 topic facts associated with each individual, guaranteeing a fair division across both languages and semantic topics. As a result, a single identity may have some of its facts distributed across different splits."
  - [corpus] Corpus papers on unlearning (e.g., "LEGATO") do not address instance-level granularity, making FAME's approach distinct but also meaning cross-validation from other benchmarks is limited.
- Break condition: If facts share underlying representations or are encoded redundantly, selective forgetting may not be achievable regardless of method.

### Mechanism 3
- Claim: Balanced unlearning methods that combine disruptive loss with retention objectives achieve better efficacy-utility trade-offs than pure gradient ascent
- Mechanism: Methods like KL Minimization and Gradient Difference explicitly optimize two objectives simultaneously: maximizing loss on forget-set data while preserving behavior on retain-set data. This prevents the model collapse observed with pure Gradient Ascent, which destroys overall capabilities while appearing to achieve forgetting. Preference Optimization takes a different approach by training the model to output non-informative responses rather than maximizing prediction error.
- Core assumption: The retain set is sufficiently representative of model capabilities that preserving performance on it generalizes to other tasks.
- Evidence anchors:
  - [section 5] "GA achieves apparently strong forgetting scores... However, this effect is largely artificial: the method aggressively maximizes loss on the forget set, which destroys the model's overall behavior and leads to a collapse of useful capabilities."
  - [section 5] "More balanced methods that combine a disruptive component with a utility-preserving objective, e.g., GD, which counter-balances the ascent term with the retain loss, or KLM, which constrains the model toward the original distribution, maintaining stability and efficiency."
  - [corpus] "Train Once, Forget Precisely" similarly advocates for anchored optimization to prevent drift, providing corroborating evidence for balanced approaches.
- Break condition: If forget and retain sets have overlapping or dependent content, the joint optimization objective becomes ill-defined.

## Foundational Learning

- Concept: **Gradient Ascent for Unlearning**
  - Why needed here: GA inverts the training objective to maximize loss on forget-set samples, but requires damping to prevent model destabilization (Section 4 notes loss magnitude is "dampened by a factor of 0.1").
  - Quick check question: What happens to model utility if you apply gradient ascent without any retention term?

- Concept: **KL Divergence as a Regularization Constraint**
  - Why needed here: KLM minimizes KL divergence between original and unlearned model predictions on retained data, providing a soft constraint that preserves learned representations while allowing targeted modification.
  - Quick check question: Why might KL-based regularization be preferable to explicit retain-set loss when model outputs are open-ended text?

- Concept: **Membership Inference Attacks (MIA)**
  - Why needed here: FAME uses MIA scores to assess whether an attacker can determine if specific data was used in training—a core privacy metric that complements direct forgetting metrics like SacreBLEU on the forget set.
  - Quick check question: If a model achieves low forget-set BLEU but high MIA, has unlearning succeeded or failed?

## Architecture Onboarding

- Component map:
  Generation Pipeline: Metadata sampling (Faker library, curated city lists) → Gemini 2.5 Flash biography generation → JSON schema validation → QA pair extraction per atomic fact
  Fine-tuning Stage: Base Llama model (1B or 3B) trained on full split dataset for 5-10 epochs with cosine LR schedule
  Unlearning Stage: Apply one of five methods (FT, GA, GD, KLM, PO) for 1 epoch with modified objectives
  Evaluation Layer: SacreBLEU on retain/forget/test sets, MMLU for utility, MIA for privacy leakage, wall-clock speedup vs. gold retraining

- Critical path:
  1. Select dataset split (entity-based for complete identity removal, topic-based for selective fact removal)
  2. Fine-tune base model on complete training set to establish knowledge of fictional identities
  3. Apply unlearning method to obtain Mu, comparing against Gold model trained only on retain set
  4. Evaluate on three dimensions: efficacy (SB-F, MIA), utility (MMLU, SB-R), efficiency (speedup)

- Design tradeoffs:
  - Constrained generation reduces stereotypical outputs but may sacrifice some diversity for Spanish/English birthplaces where unconstrained sampling spans multiple countries
  - PO produces consistent non-informative responses but creates a detectable behavioral signature that reveals prior exposure
  - Entity-based splits are cleaner for evaluation but less representative of real-world partial deletion requests than instance-based splits

- Failure signatures:
  - **Model collapse**: GA on 1B shows very low SB-R (36.3) and SB-F (34.7) simultaneously, indicating destroyed generation capability
  - **Insufficient forgetting**: FT baseline retains high SB-F (81.7 entity, 85.5 instance) with minimal change from Original
  - **Behavioral leakage**: PO models respond "I don't know" only on forget-set queries, creating an oracle for membership inference

- First 3 experiments:
  1. Replicate KLM vs. GA comparison on entity-based split to verify that balanced methods maintain SB-R > 70 while reducing SB-F < 50
  2. Test cross-lingual transfer by training unlearning on English forget set and evaluating French/German forget performance to probe multilingual generalization
  3. Compare instance-level vs. entity-level forgetting on the same unlearning method (e.g., PO) to quantify the additional difficulty of selective fact removal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do machine unlearning methods scale to model sizes beyond 3B parameters, and do the relative effectiveness rankings of different unlearning approaches remain consistent across model scales?
- Basis in paper: [explicit] The paper notes that for the 3B model, "the gradient ascent signal proves insufficient to meaningfully modify the model parameters," showing qualitatively different behavior than on the 1B model where GA showed "apparently strong forgetting scores" but caused model collapse.
- Why unresolved: Only two model sizes were tested, and the paper explicitly observes different unlearning dynamics between them, suggesting scaling behavior is not yet understood.
- What evidence would resolve it: Systematic evaluation of unlearning methods across a broader range of model sizes (e.g., 7B, 13B, 70B) with analysis of how method effectiveness and trade-offs change with scale.

### Open Question 2
- Question: Can unlearning methods be developed that successfully remove information without introducing detectable behavioral artifacts that reveal prior exposure to the forgotten data?
- Basis in paper: [explicit] Table 4 and its discussion note that after PO unlearning, "the model has a distinctive behavior on the forget set (where it claims not to have access to the information). This is a telltale sign that the model has seen those instances before."
- Why unresolved: Current methods either fail to forget (retaining exact information) or forget in ways that create detectable patterns, creating a privacy vulnerability not yet solved.
- What evidence would resolve it: Development and evaluation of unlearning methods that produce responses statistically indistinguishable from a model that never encountered the forgotten data.

### Open Question 3
- Question: How do machine unlearning methods perform on low-resource languages, and to what extent do techniques developed for high-resource languages transfer?
- Basis in paper: [explicit] The limitations section states that FAME "does not yet extend to low-resource languages," explicitly identifying this as a gap.
- Why unresolved: Current benchmarks focus on high-resource languages, leaving the critical question of whether privacy protections work equally well for speakers of all languages unanswered.
- What evidence would resolve it: Extension of multilingual unlearning benchmarks to include low-resource languages, with evaluation of whether existing methods maintain efficacy or require language-specific adaptations.

### Open Question 4
- Question: To what extent does the use of synthetic, fictional data in unlearning benchmarks accurately predict performance on real-world unlearning requests involving actual personal information?
- Basis in paper: [explicit] The limitations section acknowledges that the synthetic approach "may not fully capture the variety and complexity of real-world unlearning requests."
- Why unresolved: The controlled nature of synthetic benchmarks ensures clean evaluation but may not reflect the messier reality of actual privacy requests, where information may be entangled with other knowledge or exist in multiple forms.
- What evidence would resolve it: Comparative studies evaluating whether methods that perform well on synthetic benchmarks also succeed on carefully designed real-data scenarios, or development of methodology to bridge this gap.

## Limitations

- The use of fictional actors, while solving pretraining contamination, creates domain mismatch with real-world unlearning scenarios involving actual individuals or proprietary data
- The dataset focuses on factual QA pairs about biographies, potentially missing other knowledge types LLMs acquire during pretraining
- FAME does not yet extend to low-resource languages, limiting generalizability across all language communities

## Confidence

- **High**: Synthetic data approach and structured fact decomposition are directly implemented and evaluated
- **Medium**: Balanced unlearning methods (KLM, GD, PO) achieving better trade-offs, limited by evaluation on only two small model sizes
- **High**: PO creates detectable behavioral signatures based on observed oracle effect, but practical privacy implications require further investigation

## Next Checks

1. Test unlearning efficacy on larger models (7B-70B) to assess scalability and determine if the 1B/3B results generalize to frontier LLMs
2. Evaluate cross-lingual forgetting by training on one language's forget set and testing on others to measure multilingual knowledge transfer and interference
3. Implement MIA attacks using the exact methodology from "Unlearning Isn't Deletion" to benchmark FAME's privacy guarantees against established standards