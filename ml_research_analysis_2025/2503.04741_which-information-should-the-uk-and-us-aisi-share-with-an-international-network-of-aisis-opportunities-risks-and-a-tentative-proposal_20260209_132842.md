---
ver: rpa2
title: Which Information should the UK and US AISI share with an International Network
  of AISIs? Opportunities, Risks, and a Tentative Proposal
arxiv_id: '2503.04741'
source_url: https://arxiv.org/abs/2503.04741
tags:
- information
- sharing
- aisis
- could
- aisi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework for AI Safety Institutes (AISIs)
  in jurisdictions with frontier AI companies to make decisions about information
  sharing within an international network. The framework categorizes information into
  three tiers: "No Brainers" (low-risk, high-benefit information to share by default),
  a "Gray Area" (information requiring case-by-case assessment), and "High Trust"
  (sensitive information not to share without strong relationships).'
---

# Which Information should the UK and US AISI share with an International Network of AISIs? Opportunities, Risks, and a Tentative Proposal

## Quick Facts
- arXiv ID: 2503.04741
- Source URL: https://arxiv.org/abs/2503.04741
- Authors: Lara Thurnherr
- Reference count: 0
- Primary result: Proposes a framework for UK/US AISIs to categorize information for international sharing, distinguishing "No Brainers" (share by default), "Gray Area" (case-by-case), and "High Trust" (bilateral only) tiers.

## Executive Summary
This paper proposes a framework for AI Safety Institutes (AISIs) in jurisdictions with frontier AI companies to make decisions about information sharing within an international network. The framework categorizes information into three tiers based on proprietary nature and misuse potential. The author argues that UK and US AISIs should share certain categories of information (like institutional learnings, risk management standards, and evaluation infrastructure) while being cautious about sharing proprietary information, pre-deployment evaluation results, and sensitive model details that could increase security risks or damage industry relationships.

## Method Summary
The framework applies a two-dimensional analysis—proprietary sensitivity and dual-use potential—to categorize information into three sharing tiers. The approach uses qualitative expert elicitation and literature synthesis to classify broad categories of information (evaluation results, institutional learnings, model weights, etc.) into "No Brainers," "Gray Area," and "High Trust" categories. The framework is provisional and calls for future research to develop finer-grained subcategories and more precise risk-benefit analyses.

## Key Results
- UK/US AISIs should share institutional learnings, evaluation infrastructure, and risk management standards by default ("No Brainers")
- Pre-deployment evaluation results, model weights, and algorithmic details should not be shared multilaterally ("High Trust")
- Preliminary safety research results and standards require case-by-case assessment ("Gray Area")

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorizing information by proprietary nature and misuse potential enables systematic sharing decisions.
- Mechanism: The framework applies a two-dimensional analysis—(1) how proprietary/sensitive the information is to AI companies, and (2) how useful it would be for capability advances or misuse if leaked—to sort information into three action tiers ("No Brainers," "Gray Area," "High Trust").
- Core assumption: The two dimensions (proprietary nature, misuse utility) capture the dominant risk factors.
- Evidence anchors:
  - [abstract] "proposes a provisional framework with which policymakers and researchers can distinguish between these three cases, taking into account the potential benefits and risks of sharing specific categories of information"
  - [section 3, p.6] "if we assume a hypothetical piece of information A could be beneficial to share, this paper proposes analysing the potential effects of sharing this information through assessing the proprietary nature of the information and its utility for capability advances or misuse if leaked"

### Mechanism 2
- Claim: Sharing low-risk information builds trust and capacity across the AISI network, enabling future coordination.
- Mechanism: When UK/US AISIs share institutional learnings, evaluation infrastructure (e.g., the "Inspect" platform), and risk management standards by default, other AISIs avoid duplicating work and can contribute to common goals faster.
- Core assumption: Other AISIs have capacity to absorb and use shared information, and reciprocity will emerge over time.
- Evidence anchors:
  - [section 1.3, p.4] "If a larger number of AISIs have early access to preliminary safety research results, research agendas on risk managements, evaluation methods, and, potentially, software infrastructure from the UK and US AISI, they could avoid unnecessarily duplicating work"

### Mechanism 3
- Claim: Restricting sensitive information preserves AI company relationships and reduces leak-driven security risks.
- Mechanism: AI companies have strong commercial incentives to protect IP. If AISIs share proprietary information (pre-deployment eval results, model weights, algorithmic secrets) too widely, companies may withhold future cooperation. Additionally, broader access increases leak probability, which could aid geopolitical competitors or malicious actors.
- Core assumption: AISIs currently rely on voluntary company cooperation; they lack full legal compulsion power.
- Evidence anchors:
  - [section 2.1, p.4] "AI companies might withhold information from their respective AISI if they are not legally compelled to share it, which could be detrimental to the AISI's ability to fulfil its mandate"

## Foundational Learning

- **Concept: Dual-use information**
  - Why needed here: The paper notes safety research can also make systems more reliable/usable for malicious purposes (section 2.2).
  - Quick check question: Can you name an AI evaluation method that would help both safety researchers and malicious actors?

- **Concept: Regulatory arbitrage**
  - Why needed here: A core benefit of sharing is reducing "race to the bottom" dynamics where companies relocate to jurisdictions with weakest standards (section 1.1).
  - Quick check question: How does harmonized evaluation standards across AISIs reduce regulatory arbitrage incentives?

- **Concept: Confidence-building measures (CBMs)**
  - Why needed here: The paper frames transparency as a CBM—signals that build trust for future coordination (section 1.2).
  - Quick check question: What's the difference between a CBM and a binding commitment in international cooperation?

## Architecture Onboarding

- **Component map:** Information category → Two-factor assessment (proprietary sensitivity, misuse potential) → Three-tier classification ("No Brainers"/"Gray Area"/"High Trust") → Sharing decision (default/share by case/bilateral only) → Feedback loop (monitor network composition, company relationships, legal powers)

- **Critical path:** Start with institutional learnings and evaluation infrastructure ("No Brainers") to build trust and capacity, then assess Gray Area items case-by-case as relationships mature. Do not share High Trust items multilaterally until strong bilateral relationships exist.

- **Design tradeoffs:**
  - Granularity vs. actionability: Framework currently applies to broad categories; finer-grained subcategories would enable more specific decisions but require more analysis effort (section 4).
  - Avoiding duplication vs. research monoculture: Sharing too much too early could create groupthink (section 2.3).
  - Transparency vs. company trust: More sharing builds international credibility but risks industry relationships.

- **Failure signatures:**
  - AI companies refusing voluntary information sharing → likely oversharing of proprietary categories
  - International partners disengaging from network → likely undersharing of "No Brainer" categories
  - Conflicting evaluation standards persisting across jurisdictions → Gray Area items stuck in indecision
  - Research convergence on single methodology without scrutiny → potential research monoculture from over-sharing

- **First 3 experiments:**
  1. Pilot the framework on one information category (e.g., evaluation methods): Apply the two-factor assessment, document decision rationale, track outcomes over 3 months.
  2. Test Gray Area decision criteria: Select 3-5 preliminary research results, share with varying detail levels across bilateral vs. multilateral channels, measure company and partner reactions.
  3. Map fluctuating factors: Identify which assumptions (company relationship dependence, network composition, UK/US information advantage) are most volatile and set up monitoring triggers for framework updates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can broad information categories (e.g., "evaluation methods," "risk estimates") be subdivided into finer-grained subcategories with clearer risk/benefit profiles?
- Basis in paper: [explicit] The paper states the framework "is applied to generalised information categories. All of them can and should be split up into more fine-grained subcategories in future research on this topic."
- Why unresolved: Current designations remain "unclear" for many categories because granularity is insufficient to assess context-dependent sensitivity.
- What evidence would resolve it: Empirical analysis of specific subcategories showing which attributes predict elevated risk of capability leakage or industry trust loss.

### Open Question 2
- Question: Under what conditions does dual-use safety information enable capability advances versus enhance safety, and can definitional boundaries be drawn operationally?
- Basis in paper: [explicit] The paper calls for "more conceptual or technical work" addressing "the different dimensions of 'dual-use' information, identifying definitions and scenarios under which information could result in both capability advances and enhanced safety."
- Why unresolved: Safety research improves reliability, making it inherently harder to distinguish from capabilities research.
- What evidence would resolve it: Case studies of shared evaluation methods or risk frameworks that were subsequently misused, versus those that improved safety without enabling harm.

### Open Question 3
- Question: How do interoperability considerations affect the net benefits of sharing within the AISI network?
- Basis in paper: [explicit] The paper notes Dennis et al.'s emphasis on interoperability and states: "Incorporating this factor into the framework, either as a component of the capacity benefit or as a separate benefit to consider could be helpful."
- Why unresolved: Interoperability was identified but not operationalized in the current framework.
- What evidence would resolve it: Analysis correlating information shared with subsequent alignment of evaluation standards or regulatory approaches across jurisdictions.

### Open Question 4
- Question: Under what circumstances do risks of research monoculture, lock-in effects, and capacity waste outweigh the efficiency benefits of information sharing?
- Basis in paper: [explicit] The paper identifies these as "miscellaneous other challenges" in section 2.3, noting they "are not addressed in the framework used in this paper" but that "future research could benefit from an in-depth inquiry of their likelihood and potential consequences."
- Why unresolved: These systemic risks were acknowledged but not quantified or incorporated into the framework's risk calculus.
- What evidence would resolve it: Comparative analysis of fields with high versus low information sharing, measuring innovation diversity and path-dependence outcomes.

## Limitations

- The framework relies on speculative risk assessments rather than empirical evidence about actual sharing outcomes
- Current advantages (UK/US information access, company relationships) are assumed to persist but are acknowledged as temporary conditions
- The analysis focuses narrowly on dual-use risks while potentially undervaluing other dimensions like interoperability costs or network effects

## Confidence

- **High confidence**: The mechanism that AI companies will withhold information if AISIs share too much proprietary data is well-supported by analogous cases in AV safety data sharing literature.
- **Medium confidence**: The categorization framework itself is logically coherent but lacks empirical validation; the risk-benefit analysis for each information category remains speculative.
- **Low confidence**: The assumption that sharing "No Brainers" will automatically build trust and capacity across the international network, as this depends on factors like recipient AISI capacity and reciprocity dynamics that are not empirically established.

## Next Checks

1. **Empirical validation**: Apply the framework to historical information-sharing cases from existing safety organizations (e.g., NIST, automotive safety boards) to assess prediction accuracy.
2. **Expert calibration**: Conduct structured elicitation with AISI personnel to calibrate the proprietary and misuse-potential scoring rubrics, measuring inter-rater reliability.
3. **Network simulation**: Model how information-sharing decisions affect network dynamics over time, testing whether "No Brainers" sharing actually accelerates consensus-building versus creating dependency or research monoculture.