---
ver: rpa2
title: 'Ask What Your Country Can Do For You: Towards a Public Red Teaming Model'
arxiv_id: '2510.20061'
source_url: https://arxiv.org/abs/2510.20061
tags:
- teaming
- exercise
- public
- nist
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new approach to AI safety evaluation: cooperative
  public AI red-teaming exercises. It addresses the "responsibility gap" in AI safety
  where AI labs are perceived as solely responsible for safety, while public entities
  and application developers lack effective oversight tools.'
---

# Ask What Your Country Can Do For You: Towards a Public Red Teaming Model

## Quick Facts
- arXiv ID: 2510.20061
- Source URL: https://arxiv.org/abs/2510.20061
- Reference count: 14
- Primary result: Public cooperative red-teaming exercises can deliver meaningful safety insights, leverage diverse expertise for better harm surface coverage, and provide scalable models for jurisdictions to adopt.

## Executive Summary
This paper proposes a novel approach to AI safety evaluation through cooperative public AI red-teaming exercises that address the "responsibility gap" in AI safety. Rather than leaving safety solely to AI labs, the framework involves organizing large-scale, diverse red-teaming exercises open to public participation across jurisdictions. Three pilot implementations demonstrate the approach: a national NIST pilot with 457 participants, a CAMLIS in-person demonstrator with 30 top red teamers, and an IMDA Singapore multilingual challenge across nine countries. The core insight is that public entities can provide neutral convening authority while civil society retains agenda-setting power over AI development and deployment.

## Method Summary
The method involves organizing scenario-based Capture-the-Flag (CTF) exercises where diverse public participants attempt to induce specific failures in AI systems. The approach uses open enrollment or qualification-based selection to ensure multidisciplinary diversity, structured reporting against established harm taxonomies (NIST AI RMF, NIST GAI Profile, MITRE ATLAS), and tiered execution combining broad virtual exercises with intensive in-person sessions. Participants test AI systems against defined scenarios while logging prompts and outputs through a dedicated testing interface, with findings categorized using standardized taxonomies to enable systematic analysis and synthesis into actionable recommendations.

## Key Results
- Public red-teaming exercises can deliver meaningful safety insights when properly structured
- Diverse public participants achieve broader harm surface coverage than homogeneous internal teams
- The methodology is portable and can be implemented across different jurisdictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse public red teamers achieve broader harm surface coverage than insular, in-house teams.
- Mechanism: Recruiting participants across disciplines, cultures, and jurisdictions surfaces harms and attack vectors that homogeneous internal teams miss—particularly context-specific harms grounded in local norms, values, and discourses.
- Core assumption: Diversity of participant backgrounds translates to diversity of harm identification; participants are sufficiently skilled to produce meaningful findings.
- Evidence anchors:
  - [abstract] "leverage diverse expertise to achieve better coverage of harm surfaces"
  - [section II] Critiques that in-house red teaming "is either not sufficiently blinded, favors developer-created harm classifications at the expense of classifications that users might articulate, or is carried out by teams that are insufficiently diverse"
  - [corpus] Feffer et al. (2024) question whether red-teaming is "silver bullet or security theater"—mechanism effectiveness remains contested.
- Break condition: If participant pool self-selects for similar backgrounds despite open recruitment, or if harm taxonomies constrain what participants can report.

### Mechanism 2
- Claim: Public entity partnership confers legitimacy, agenda-setting power, and scalability across jurisdictions.
- Mechanism: Government or metrology institutions (NIST, IMDA) provide neutral convening authority, ensuring civil society retains influence over AI governance while enabling portable methodologies other jurisdictions can adopt.
- Core assumption: Public entities are motivated to exercise oversight and have capacity to organize/interpret results; results influence actual policy or developer behavior.
- Evidence anchors:
  - [abstract] "scalable models that AI-developing jurisdictions can adopt"
  - [section I] "this duty is shared by AI application developers and public entities alike"
  - [section IV] "civil society retains its agenda-setting power over the development and deployment of AI"
  - [corpus] Singh et al. (2025) and Longpre et al. (2024) advocate third-party evaluation but corpus lacks empirical validation of governance impact.
- Break condition: If public partners lack enforcement mechanisms or if results are ignored by AI labs and application developers.

### Mechanism 3
- Claim: Scenario-based CTF structure with permissive enrollment recruits scale while maintaining rigor through qualification tiers.
- Mechanism: Open enrollment maximizes participant pool; top performers advance to intensive in-person exercises. This funnel balances breadth (national-scale discovery) with depth (expert analysis of complex vulnerabilities).
- Core assumption: Qualifier scores predict meaningful red-teaming capability; scenario design captures representative harm categories.
- Evidence anchors:
  - [section III.A] 457 enrolled in virtual qualifier; top 30 selected for CAMLIS in-person exercise
  - [section III.A] Three ARIA scenarios: meal planning, travel advisory, film/TV spoilers
  - [corpus] Microsoft red teaming paper (Bullwinkel et al., 2025) provides operational precedent but different methodology.
- Break condition: If scenarios are too narrow to generalize, or if qualification metrics don't correlate with actual harm discovery.

## Foundational Learning

- Concept: **AI Red Teaming vs. Traditional Security Testing**
  - Why needed here: AI red teaming targets sociotechnical harms (bias, misinformation) not just software vulnerabilities; understanding this distinction prevents misapplying cybersecurity playbooks.
  - Quick check question: Can you name three harm categories specific to AI systems that traditional penetration testing would not surface?

- Concept: **Harm Taxonomies (MITRE ATLAS, NIST AI RMF 100-1, NIST GAI Profile 600-1)**
  - Why needed here: These frameworks organize attacks and findings systematically; exercises referenced them to structure participant efforts and categorize results.
  - Quick check question: What is the difference between a technical adversarial attack and a sociotechnical harm in these taxonomies?

- Concept: **Coverage vs. Depth Tradeoff in Evaluation**
  - Why needed here: The paper explicitly balances large-scale virtual exercises (coverage) against small in-person exercises (depth); understanding this helps interpret results appropriately.
  - Quick check question: If you had 100 participant-hours total, how would you allocate them between broad recruitment and deep expert analysis?

## Architecture Onboarding

- Component map:
  Recruitment pipeline → Qualification/selection → Scenario design → Exercise execution platform (virtual UI or in-person) → Harm logging/taxonomy mapping → Analysis & synthesis → Stakeholder feedback loop

- Critical path:
  1. Define harm categories and scenarios relevant to deployment context
  2. Recruit diverse participants through open enrollment with minimal barriers
  3. Qualify participants (for tiered exercises) or onboard all (for broad exercises)
  4. Execute red-teaming with structured reporting against taxonomy
  5. Synthesize findings into actionable recommendations for labs, developers, and regulators

- Design tradeoffs:
  - Virtual vs. in-person: Virtual enables scale (457 participants); in-person enables intensity and collaboration (30 experts)
  - Open vs. selective enrollment: Open maximizes diversity; selective ensures baseline competence
  - General vs. context-specific scenarios: General taxonomies are portable; localized scenarios surface culturally-specific harms (IMDA multilingual exercise)

- Failure signatures:
  - Participant pool lacks diversity despite open recruitment (demographic homogeneity)
  - Taxonomy misalignment: participants identify harms that don't fit predefined categories
  - Results not actionable: findings too generic or not transmitted to those who can mitigate
  - Coverage illusion: many participants but all testing same narrow attack vectors

- First 3 experiments:
  1. Replicate ARIA structure with a single model and 20-50 participants to validate that open recruitment yields diverse attack patterns; measure variance in harm categories surfaced.
  2. Test taxonomy utility: have one group use NIST 600-1 categories and another use free-form reporting; compare coverage and ease of analysis.
  3. Run a minimal multilingual exercise (2-3 languages, 10 participants each) to assess whether localized scenarios surface distinct harms not captured in English-only testing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multilingual and multicultural methodology piloted in Singapore be effectively ported to jurisdictions with different normative values without losing fidelity?
- Basis in paper: [explicit] The authors state the IMDA case aimed to develop a methodology "that is portable and readily implementable across jurisdictions."
- Why unresolved: The paper establishes the pilot but does not present data from subsequent replications in different regulatory or cultural environments to confirm generalizability.
- What evidence would resolve it: Successful execution of the same methodology in a distinct geopolitical region with comparable detection rates of context-specific harms.

### Open Question 2
- Question: How does the operational efficacy of large-scale, distributed virtual red teaming compare to intensive, in-person exercises?
- Basis in paper: [inferred] The paper contrasts the NIST ARIA pilot (virtual, 457 participants) with the CAMLIS demonstrator (in-person, 30 participants) and proposes discussing their "relative strengths and weaknesses."
- Why unresolved: While the paper outlines the design of both, it implies the need for a comparative analysis to determine which format yields better coverage or higher-quality vulnerability data.
- What evidence would resolve it: A comparative metric analysis of valid vulnerabilities discovered per participant-hour across both operational modes.

### Open Question 3
- Question: Do standardized risk taxonomies, such as NIST AI 600-1, effectively capture the vernacular harms identified by diverse public red teamers?
- Basis in paper: [explicit] The authors note that red teamers were instructed to "analyze the utility of that risk typology" and that prevailing typologies may fail to match user-articulated classifications.
- Why unresolved: It remains unclear if the rigid categories of formal frameworks can encompass the "unique norms, values, and discourses" the paper argues public red teaming brings to the surface.
- What evidence would resolve it: Qualitative analysis showing a high correlation between public red teamer findings and the categories defined in NIST AI 600-1.

## Limitations

- Efficacy of public red-teaming remains largely theoretical with no quantitative results demonstrating novel harm discovery
- Scalability mechanisms are underspecified with no evidence that findings translate across jurisdictions or influence policy
- Participant quality control is unclear with no specification of selection criteria or evidence that top performers produce higher-quality findings

## Confidence

- **High Confidence**: The conceptual framing of the "responsibility gap" and argument that AI safety evaluation should involve multiple stakeholders beyond AI labs is well-grounded in existing literature and governance discourse.
- **Medium Confidence**: The claim that diverse participant pools achieve better harm surface coverage is supported by logical reasoning and critique of homogeneous internal teams, but lacks empirical validation in the paper itself.
- **Low Confidence**: The assertion that public entity partnerships enable scalable, portable methodologies across jurisdictions is largely speculative, with no evidence provided about implementation success, policy impact, or cross-jurisdictional adoption.

## Next Checks

1. **Conduct comparative effectiveness study**: Design a controlled experiment where identical scenarios are tested by (a) public participants, (b) internal red teams, and (c) external security researchers. Quantify differences in harm categories discovered, false positive rates, and actionable findings to directly test the diversity advantage hypothesis.

2. **Implement governance impact assessment**: Track what happens to findings from a public red-teaming exercise through the policy development cycle. Measure whether public entity partners can influence model developer behavior, regulatory changes, or application deployment practices—establishing a feedback loop from exercise to impact.

3. **Test taxonomy usability across participant types**: Run parallel exercises where one group uses structured taxonomies (NIST 600-1) and another uses free-form reporting. Analyze inter-annotator agreement, time to report, and whether structured reporting constrains or enhances harm discovery to validate the operationalization assumption.