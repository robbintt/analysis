---
ver: rpa2
title: Private Zeroth-Order Optimization with Public Data
arxiv_id: '2511.10859'
source_url: https://arxiv.org/abs/2511.10859
tags:
- public
- data
- private
- zeroth-order
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PAZO, a suite of public-data-assisted private
  zeroth-order optimization methods designed to close the utility gap between zeroth-
  and first-order differentially private algorithms. PAZO leverages small amounts
  of public data to guide gradient estimation in private training, offering three
  variants: mixing private zeroth-order and public first-order gradients (PAZO-M),
  constraining sampling to the public gradient subspace (PAZO-P), and selecting the
  best public gradient based on private data loss (PAZO-S).'
---

# Private Zeroth-Order Optimization with Public Data

## Quick Facts
- arXiv ID: 2511.10859
- Source URL: https://arxiv.org/abs/2511.10859
- Authors: Xuchen Gong; Tian Li
- Reference count: 40
- Key outcome: PAZO suite closes utility gap between zeroth- and first-order differentially private optimization using public data assistance

## Executive Summary
This paper introduces PAZO, a suite of public-data-assisted private zeroth-order optimization methods designed to close the utility gap between zeroth- and first-order differentially private algorithms. PAZO leverages small amounts of public data to guide gradient estimation in private training, offering three variants: mixing private zeroth-order and public first-order gradients (PAZO-M), constraining sampling to the public gradient subspace (PAZO-P), and selecting the best public gradient based on private data loss (PAZO-S). Theoretical analysis shows improved convergence rates over prior methods, with PAZO-P and PAZO-S achieving dimension-independent error bounds. Empirically, PAZO outperforms both vanilla zeroth-order and state-of-the-art first-order private methods, especially under tight privacy budgets, while offering up to 16× speedup per iteration.

## Method Summary
PAZO addresses the fundamental challenge of private optimization by combining zeroth-order gradient estimation with public data guidance. The core insight is that while zeroth-order methods provide memory efficiency and privacy through function evaluations, they suffer from high variance that limits utility. PAZO-M reduces this variance by mixing public first-order gradients with private zeroth-order estimates, PAZO-P constrains the search space to a low-dimensional subspace spanned by public gradients, and PAZO-S selects among multiple public gradient candidates using privatized function queries. All variants maintain formal differential privacy guarantees while achieving state-of-the-art accuracy-privacy tradeoffs across image and text tasks.

## Key Results
- PAZO-M improves vanilla zeroth-order by factor of log(d) with convergence rate O((1-α)/(α√d))
- PAZO-P and PAZO-S achieve dimension-independent error bounds O(k) and O(γ² + σ²₂/b')
- Up to 16× speedup per iteration compared to DP-SGD while maintaining or improving accuracy
- Robust performance across CIFAR-10, Tiny-ImageNet, IMDB, and MNLI tasks with varying privacy budgets

## Why This Works (Mechanism)

### Mechanism 1: PAZO-M (Gradient Mixing)
Linear combination of public first-order and private zeroth-order gradients improves convergence rate by a factor of log(d) over vanilla zeroth-order methods. At each iteration, compute g_combined = α·g_pub + (1-α)·g̃/q where g_pub is the exact public batch gradient and g̃ is the privatized zeroth-order estimate. The public gradient provides low-variance directional guidance while the private zeroth-order term adds privacy-preserving refinement.

### Mechanism 2: PAZO-P (Subspace-Constrained Perturbation)
Constraining zeroth-order perturbations to the k-dimensional public gradient subspace achieves dimension-independent error rate O(k) instead of O(√d log d). Instead of sampling perturbation u from full d-dimensional sphere, sample from √k·S^(k-1) and apply via G·u where G ∈ ℝ^(d×k) is the orthonormalized matrix of k public gradients. This reduces effective search dimension from d to k ≪ d.

### Mechanism 3: PAZO-S (Gradient Selection via Private Loss)
Selecting among k public gradient candidates using privatized function queries achieves d-independent error bound O(γ² + σ²₂/b'). For each of k public gradient candidates g_j, evaluate privatized loss f(x - ηg_j; B) on private data. Select the best-performing gradient, optionally add Gaussian perturbation z' ~ N(0, ε²I_d) to escape local minima, and compare again.

## Foundational Learning

### Concept: Zeroth-Order Gradient Estimation
- **Why needed here:** Core operation enabling memory-efficient private training—approximates gradients using only function evaluations (scalars), avoiding per-sample gradient computation
- **Quick check question:** Given the two-point estimator g_λ(x;ξ) = [f(x+λu;ξ) - f(x-λu;ξ)]/(2λ) · u where u ~ Unif(√d·S^(d-1), can you explain why E_u[g_λ(x;ξ)] = ∇f_λ(x) (the smoothed gradient) and how the smoothing radius λ√d affects bias-variance tradeoff?

### Concept: Differential Privacy via Gaussian Mechanism
- **Why needed here:** Understanding how noise calibration protects privacy while maintaining utility
- **Quick check question:** Given sensitivity C, noise multiplier σ, and q function queries per iteration, why does adding z ~ (1/b)·N(0, qC²σ²) to the aggregated scalar values satisfy the same (ε,δ)-DP guarantee regardless of q, and how does this enable multiple query variance reduction?

### Concept: γ-Similarity (Public-Private Data Relationship)
- **Why needed here:** Central assumption connecting public data utility to private task performance
- **Quick check question:** Given that ∥∇f'(x_t) - ∇f(x_t)∥ ≤ γ, how does this affect error bounds in PAZO-M vs. PAZO-P (which has additional subspace projection error)? How would you empirically estimate γ from a given public-private dataset pair?

## Architecture Onboarding

### Component map:
```
PAZO Training Loop
├── Data Sampling
│   ├── Private: Sample batch B ⊂ D_private (|B| = b, requires DP)
│   └── Public: Sample batch(es) B' ⊂ D_public (|B'| = b', no DP)
│
├── Public Gradient Computation
│   ├── PAZO-M: Single batch gradient g_pub ∈ ℝ^d
│   ├── PAZO-P: k batch gradients → orthonormalize → G ∈ ℝ^(d×k)
│   └── PAZO-S: k batch gradients {g_1,...,g_k}
│
├── Private Zeroth-Order Module
│   ├── Sample direction u (full space or subspace)
│   ├── Forward passes: {f(x_t + λu; ξ_i), f(x_t - λu; ξ_i)} for ξ_i ∈ B
│   ├── Clip scalars: clip_C([f(x+λu) - f(x-λu)]/(2λ))
│   ├── Aggregate: mean over batch
│   └── Add noise: z ~ (1/b)·N(0, qC²σ²) for q queries
│
├── Integration (variant-specific)
│   ├── PAZO-M: α·g_pub + (1-α)·g̃/q
│   ├── PAZO-P: G·(clipped aggregated scalar)·u
│   └── PAZO-S: Select j* = argmin_j f(x_t - ηg_j; B), add z'
│
└── Parameter Update
    └── x_{t+1} ← x_t - η·(integrated gradient)
```

### Critical path:
1. Initialize: Set hyperparameters (α, k, q, b', λ, C, η) and load initial model x_0
2. Per-iteration flow:
   - Sample private batch B and public batch(es)
   - Compute public gradient(s) via backprop (one-time cost, no DP)
   - For q queries: sample direction, evaluate forward passes on B, clip and aggregate
   - Add calibrated noise to private estimates
   - Integrate per variant
   - Update parameters
3. Privacy accounting: Track cumulative (ε, δ) via moments accountant; σ = c₂·b·√(T·log(1/δ))/(n·ε)

### Design tradeoffs:
- Memory: PAZO-P requires O(kd) for G matrix vs. O(d) for M/S; first-order methods need O(bd) for per-sample gradients
- Variance-bias: Higher α → less noise but more bias from public-private gap; lower α → more noise, less bias
- Query count: More q reduces zeroth-order variance but increases noise (σ² ∝ q) and runtime
- Subspace dimension k: Larger k covers more gradient space but increases PAZO-P complexity and PAZO-S function evaluations

### Failure signatures:
- Convergence plateau at low accuracy (ε < 0.5): First-order methods fail; zeroth-order maintains but lower peak → Expected per Figure 2; PAZO variants outperform
- Slow convergence (>200 epochs): Public-private distribution gap too large → Check γ via gradient norm difference on held-out samples; may need better public data
- Memory overflow with PAZO-P: k too large → Reduce to k ∈ {3,6,10} per Table 12-13
- Clipping always active: C too small → Loss never improves → Set C ≥ 1 + √(2·d^(1/4))·M for PAZO-M (Appendix B.2)
- Runtime slower than DP-SGD: Large b' or q → Reduce to minimal values; PAZO should show 2-16× speedup per Table 8

### First 3 experiments:
1. Reproduce CIFAR-10 baseline (Table 2): Train NFResNet18 from scratch with ε = {0.1, 0.5, 1, 2, 3}. Compare DP-SGD, DPZero, all three PAZO variants. Sweep: α ∈ {0.25, 0.5, 0.75} for M; k ∈ {3, 6, 10} for P/S. Measure: accuracy vs. ε, runtime/iteration, memory. Expected: PAZO variants outperform DPZero and match/exceed DP-SGD at low ε.
2. Validate γ-similarity assumption (Table 7): On MNLI with RoBERTa-base, create public data mixtures: {100% MNLI, 50% MNLI/50% SNLI, 100% SNLI}. For each, compute γ = ∥∇f'(x) - ∇f(x)∥ at multiple training steps. Measure final accuracy. Expected: accuracy decreases as γ increases, validating the theoretical assumption.
3. Ablation on key hyperparameters (Figure 6): Fix ε = 1, vary: (a) mixing coefficient α for PAZO-M on CIFAR-10, (b) subspace dimension k for PAZO-P on MNLI, (c) perturbation ε for PAZO-S on Tiny-ImageNet. Expected: performance robust across reasonable ranges (α ∈ [0.25, 0.75], k ∈ [3, 10]), confirming low sensitivity.

## Open Questions the Paper Calls Out
- Can the convergence bounds of PAZO be sharpened by incorporating alternative similarity metrics, such as coordinate-wise gradient alignment, rather than relying solely on γ-similarity?
- Does the PAZO-S selection mechanism retain theoretical convergence guarantees when applied to strictly non-convex loss landscapes, despite being motivated by convex optimization properties?
- How does PAZO's dimension-dependent error rate and runtime efficiency scale when applied to Large Language Models (LLMs) with parameter counts significantly exceeding the RoBERTa-base and LSTM models tested?

## Limitations
- γ-similarity assumption (public-private gradient proximity) is critical but not validated with controlled experiments across diverse data shifts
- PAZO-S variant lacks ablation on the heuristic use of convexity for non-convex neural networks
- Public data warm-start procedures are described vaguely, potentially impacting reproducibility
- Exact noise calibration procedure (c₁, c₂ constants) requires reverse-engineering from privacy accountant implementation

## Confidence
- **High Confidence:** PAZO-M mixing mechanism and theoretical improvement over vanilla zeroth-order
- **Medium Confidence:** PAZO-P subspace dimension reduction
- **Medium Confidence:** PAZO-S gradient selection heuristic
- **Medium Confidence:** Runtime speedup claims

## Next Checks
1. **Gradient Norm Gap Analysis:** For CIFAR-10, compute ∥∇f'(x) - ∇f(x)∥ at multiple training steps to quantify γ and correlate with accuracy degradation
2. **PAZO-S Ablation Study:** Sweep k ∈ {3,6,9,12} on MNLI and measure accuracy and selection stability; test impact of removing convexity heuristic
3. **Memory Overhead Validation:** Implement PAZO-P with varying k and measure actual GPU memory usage vs. theoretical O(kd) scaling to verify claimed memory efficiency over first-order methods