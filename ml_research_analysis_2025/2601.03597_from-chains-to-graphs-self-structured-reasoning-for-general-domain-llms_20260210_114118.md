---
ver: rpa2
title: 'From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs'
arxiv_id: '2601.03597'
source_url: https://arxiv.org/abs/2601.03597
tags:
- reasoning
- answer
- graph
- leads
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Graph Reasoning (SGR), a method that
  enables large language models to express reasoning as a structured graph before
  generating a final answer. Unlike linear Chain-of-Thought reasoning, SGR enforces
  explicit logical dependencies between intermediate steps, improving consistency
  and interpretability.
---

# From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs

## Quick Facts
- arXiv ID: 2601.03597
- Source URL: https://arxiv.org/abs/2601.03597
- Authors: Yingjian Chen, Haoran Liu, Yinhong Liu, Sherry T. Tong, Aosong Feng, Jinghui Lu, Juntao Zhang, Yusuke Iwasawa, Yutaka Matsuo, Irene Li
- Reference count: 40
- This paper introduces Self-Graph Reasoning (SGR), a method that enables large language models to express reasoning as a structured graph before generating a final answer. Unlike linear Chain-of-Thought reasoning, SGR enforces explicit logical dependencies between intermediate steps, improving consistency and interpretability. The approach constructs a dataset of 10K reasoning graphs by aggregating multiple candidate reasoning paths, which are then used to fine-tune LLaMA-3.3-70B. Experiments on five QA benchmarks show that SGR achieves a 17.74% improvement over the base model and matches the performance of GPT-4o. The method also generalizes well to specialized domains such as medicine and mathematics, demonstrating the effectiveness of structured graph reasoning in enhancing logical coherence and accuracy.

## Executive Summary
This paper introduces Self-Graph Reasoning (SGR), a method that enables large language models to express reasoning as a structured graph before generating a final answer. Unlike linear Chain-of-Thought reasoning, SGR enforces explicit logical dependencies between intermediate steps, improving consistency and interpretability. The approach constructs a dataset of 10K reasoning graphs by aggregating multiple candidate reasoning paths, which are then used to fine-tune LLaMA-3.3-70B. Experiments on five QA benchmarks show that SGR achieves a 17.74% improvement over the base model and matches the performance of GPT-4o. The method also generalizes well to specialized domains such as medicine and mathematics, demonstrating the effectiveness of structured graph reasoning in enhancing logical coherence and accuracy.

## Method Summary
The SGR framework fine-tunes LLMs to generate reasoning graphs structured as directed acyclic graphs (DAGs) rather than linear chains. The method constructs training data by using GPT-4o to generate multiple candidate reasoning graphs per question, then integrating them into optimal graphs that preserve valid reasoning branches while filtering incorrect paths. LLaMA-3.3-70B is fine-tuned with LoRA (r=8, α=16) on the resulting ~10K samples. During inference, the model outputs structured reasoning in a specific format with nodes representing atomic propositions connected by logical dependencies, followed by the final answer. The approach requires explicit justification between reasoning steps, preventing logical drift common in linear reasoning.

## Key Results
- SGR achieves 17.74% improvement over base LLaMA-3.3-70B on five QA benchmarks
- Performance matches GPT-4o across all tested benchmarks (LogiQA, AIW, AR-LSAT, MedQA, MathQA)
- Graph structure prevents mixing of different reasoning units and improves interpretability
- Model scale matters: 70B models show consistent improvements while 8B models exhibit marginal gains or degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit graph structure reduces logical drift between intermediate reasoning and final answers.
- **Mechanism:** By representing reasoning as nodes (atomic propositions) connected by directed edges (logical dependencies), each reasoning step must explicitly justify its parent nodes. This prevents the "unfaithful explanation" problem where linear CoT generates plausible-sounding but disconnected reasoning.
- **Core assumption:** Models can learn to generate structured graph formats through supervised fine-tuning and that this structure constrains the reasoning space effectively.
- **Evidence anchors:**
  - [abstract] "SGR enforces explicit logical dependencies between intermediate steps, improving consistency and interpretability"
  - [Section 3.2] "Each node v_j must be explicitly justified by its parent nodes Pa(v_j), thereby eliminating the 'logical drift' often observed in linear reasoning"
  - [corpus] Related work on graph-based reasoning (Luo et al., 2023; Han et al., 2025) shows similar benefits but relies on external graphs rather than self-generated structures
- **Break condition:** If the base model lacks sufficient reasoning capacity (<8B parameters), it cannot effectively learn or benefit from graph-structured reasoning.

### Mechanism 2
- **Claim:** Aggregating multiple reasoning trajectories via graph integration produces higher-quality supervision than single-path reasoning.
- **Mechanism:** The method samples k=3 independent reasoning graphs at high temperature (τ=0.9), then merges them into an optimal graph that preserves multiple valid reasoning branches while filtering incorrect paths. This captures the insight that general-domain questions admit multiple valid reasoning approaches.
- **Core assumption:** Diverse reasoning paths exist for general-domain questions and their intersection improves signal quality.
- **Evidence anchors:**
  - [Section 3.1] "General-domain questions often admit multiple reasoning paths, as they can be solved from different starting points or perspectives"
  - [Section 3.1] "We integrate all candidate graphs S into a unified, optimal reasoning graph ĝ"
  - [corpus] No direct corpus evidence for multi-trajectory aggregation specifically; related work focuses on single reasoning paths
- **Break condition:** When questions have only one valid reasoning path, diversity sampling provides no benefit and may introduce noise.

### Mechanism 3
- **Claim:** Requiring models to generate explicit graph structure before the answer creates a verifiable reasoning trace.
- **Mechanism:** The model outputs structured reasoning in format `<reasoning><step>node A → node B</step>...</reasoning><answer>...</answer>`. This makes each inference step inspectable and allows error localization—unlike linear CoT where errors propagate invisibly.
- **Core assumption:** Explicit structural generation imposes stronger constraints than implicit linear generation, and models can learn this format from ~10K examples.
- **Evidence anchors:**
  - [Section 5.4 Case Study] Shows explicit branching: "Identify siblings (brothers & sisters)" creates two independent branches that prevent "mixing of different reasoning units"
  - [Section 3.2] "By generating ĝ token-by-token, the model effectively constructs a structural reasoning graph that constrains the final answer L"
  - [corpus] GraphDancer (arxiv 2602.02518) similarly trains LLMs to reason over graph structures via curriculum RL
- **Break condition:** If inference-time compute budget cannot accommodate longer graph generation, or if downstream applications require minimal latency.

## Foundational Learning

- **Concept: Supervised Fine-Tuning with LoRA**
  - Why needed here: SGR trains via parameter-efficient fine-tuning (LoRA with r=8, α=16) on the graph-reasoning dataset. Understanding LoRA's rank constraints helps predict which capabilities transfer.
  - Quick check question: Can you explain why LoRA fine-tuning might preserve more base model capabilities than full fine-tuning?

- **Concept: Graph Topology (DAGs, Branching, Convergence)**
  - Why needed here: The reasoning graphs are directed acyclic structures with explicit branching and convergence. Understanding how information flows through parent-child node relationships is essential for debugging generated graphs.
  - Quick check question: In a reasoning graph, what does it mean when a node has two parents versus when a node has two children?

- **Concept: Temperature Sampling for Diversity**
  - Why needed here: The method uses τ=0.9 to generate diverse candidate reasoning paths. Understanding the tradeoff between diversity and coherence is critical for data quality.
  - Quick check question: What happens to output diversity as temperature approaches 0 versus as it approaches 1?

## Architecture Onboarding

- **Component map:**
  ```
  Training Pipeline: Question → GPT-4o (τ=0.9, k=3) → 3 candidate graphs → Integration prompt → Optimal graph → Filter by answer correctness → Training sample
  Inference Pipeline: Question → SGR-LLaMA → <reasoning> graph steps → <answer> → Final output
  ```

- **Critical path:**
  1. Data construction quality (diverse candidate graphs + correct integration)
  2. Answer filtering (Eq. 1: only keep graphs where f(ĝ)=L)
  3. LoRA fine-tuning on filtered dataset
  4. Inference-time graph generation

- **Design tradeoffs:**
  - **Model scale vs. effectiveness:** 8B models show marginal gains or degradation; 70B models show consistent improvements. SGR appears to require minimum reasoning capacity.
  - **Graph verbosity vs. completeness:** GRPO fine-tuning (Appendix E) improves accuracy but makes reasoning "overly concise, occasionally omitting intermediate steps"
  - **Training data scale:** Only ~10K samples; authors explicitly note this limits potential

- **Failure signatures:**
  - Small models (8B): Marginal gains or degradation on complex benchmarks
  - GRPO-tuned models: Concise but incomplete reasoning traces
  - Out-of-domain specialized tasks: Performance depends on base model's domain knowledge (no domain-specific training data used)

- **First 3 experiments:**
  1. **Baseline comparison on AIW dataset:** Compare Direct Answering, Linear CoT, and SGR on the same base model to isolate the effect of graph structure (expected: ~38% improvement per Figure 4)
  2. **Ablation on model scale:** Train SGR on both 8B and 70B variants to confirm scale-dependence (expected: 8B shows minimal gains, 70B shows consistent improvements)
  3. **Graph quality inspection:** Manually evaluate whether generated graphs correctly branch on independent subproblems and converge appropriately (use case study format from Figure 6 as reference)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum model scale required for effective self-graph reasoning, and can smaller models be enhanced to benefit from SGR?
- **Basis in paper:** [explicit] "The 8B model exhibits only marginal gains... suggesting that limited base capabilities constrain it to effectively perform self-graph reasoning" and "the benefits of SGR correlate with the model's underlying reasoning capacity."
- **Why unresolved:** The paper tested only 8B and 70B variants, leaving the scale threshold undefined. GRPO fine-tuning helped smaller models but caused overly concise reasoning.
- **What evidence would resolve it:** Systematic evaluation across model sizes (e.g., 13B, 30B, 50B) with analysis of reasoning graph quality metrics at each scale.

### Open Question 2
- **Question:** How does expanding the training dataset beyond 10K instances affect SGR performance and cross-domain generalization?
- **Basis in paper:** [explicit] "The scale of the training data for self-graph reasoning is relatively limited... approximately 10K training instances, which may constrain the full potential of the proposed SGR framework."
- **Why unresolved:** The paper constructs a fixed 10K dataset from LogiQA but does not ablate dataset size or explore larger-scale training.
- **What evidence would resolve it:** Experiments varying training set size (e.g., 50K, 100K, 500K samples) and measuring both in-domain and out-of-domain benchmark performance.

### Open Question 3
- **Question:** Can the teacher model dependency be reduced through self-distillation or using weaker teachers while maintaining graph quality?
- **Basis in paper:** [inferred] The dataset relies on GPT-4o to generate candidate reasoning graphs, but no ablation studies examine how teacher model quality affects final SGR performance.
- **Why unresolved:** The paper does not explore whether high-quality teacher LLMs are necessary, or if self-generated graphs could suffice.
- **What evidence would resolve it:** Comparative experiments using different teacher models (e.g., LLaMA-70B, smaller models) to construct training data, measuring downstream SGR accuracy.

### Open Question 4
- **Question:** What is the optimal balance between answer accuracy and reasoning completeness in GRPO-fine-tuned models?
- **Basis in paper:** [explicit] "The reasoning process of the GRPO-tuned model becomes overly concise, occasionally omitting intermediate reasoning steps... suggesting an inherent trade-off between answer accuracy and reasoning completeness."
- **Why unresolved:** The paper observes the trade-off but does not systematically characterize or mitigate it.
- **What evidence would resolve it:** Varying GRPO reward weights between format adherence and answer correctness, measuring both accuracy and reasoning graph completeness scores.

## Limitations
- **Scale Dependency:** SGR shows consistent improvements only on 70B models, with 8B models exhibiting marginal gains or degradation, suggesting the method requires minimum reasoning capacity.
- **Dataset Scale:** The training dataset is relatively small (~10K samples), which the authors explicitly note may constrain the full potential of SGR.
- **Computational Overhead:** Graph generation requires longer inference sequences compared to linear CoT, potentially increasing latency in real-time applications.

## Confidence

**High Confidence** (Evidence strongly supports claims):
- SGR outperforms linear CoT on the tested benchmarks (17.74% improvement)
- Graph structure prevents mixing of different reasoning units (case study evidence)
- Model scale matters: 70B shows consistent improvements, 8B shows degradation

**Medium Confidence** (Evidence supports claims but with caveats):
- SGR matches GPT-4o performance across all five benchmarks
- Aggregating multiple reasoning paths produces higher-quality supervision
- Explicit graph structure improves interpretability and error localization

**Low Confidence** (Limited evidence or speculative claims):
- Claims about SGR's effectiveness in specialized domains (medicine, mathematics) without domain-specific training data
- Generalization to truly out-of-distribution reasoning tasks
- Long-term scalability with larger training datasets

## Next Checks
1. **Model Scale Threshold Test:** Systematically train SGR on multiple model sizes (8B, 32B, 70B, 140B) to identify the minimum effective scale and quantify the relationship between model capacity and SGR performance gains.

2. **Robustness to Noise:** Evaluate SGR on adversarially constructed reasoning tasks where logical dependencies are intentionally obfuscated or contradictory, testing whether the graph structure truly constrains reasoning versus simply producing structured outputs.

3. **Efficiency Overhead Measurement:** Quantify the exact inference-time cost of SGR versus linear CoT in terms of token generation time, memory usage, and latency, then determine the breakeven point where accuracy gains justify computational overhead.