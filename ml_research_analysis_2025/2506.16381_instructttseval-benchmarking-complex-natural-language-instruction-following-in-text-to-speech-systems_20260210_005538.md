---
ver: rpa2
title: 'InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following
  in Text-to-Speech Systems'
arxiv_id: '2506.16381'
source_url: https://arxiv.org/abs/2506.16381
tags:
- speech
- style
- instruction
- description
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces InstructTTSEval, a benchmark for evaluating\
  \ instruction-following TTS systems. It proposes three tasks\u2014Acoustic-Parameter\
  \ Specification, Descriptive-Style Directive, and Role-Play\u2014covering low-level\
  \ acoustic control to high-level style inference."
---

# InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems

## Quick Facts
- arXiv ID: 2506.16381
- Source URL: https://arxiv.org/abs/2506.16381
- Reference count: 40
- This paper introduces InstructTTSEval, a benchmark for evaluating instruction-following TTS systems

## Executive Summary
This paper introduces InstructTTSEval, a benchmark for evaluating instruction-following TTS systems. It proposes three tasks—Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play—covering low-level acoustic control to high-level style inference. A dataset of 6k instructions with reference audio is built using Gemini for caption generation and GPT-4o for instruction generation. Evaluation using Gemini as a judge shows closed-source models (Gemini, GPT-4o-mini-TTS) significantly outperform open-source models, but all still fall short of human-level expressiveness. Key challenges include reproducing natural vocal events, managing emotional transitions, and synthesizing character-specific timbres.

## Method Summary
The authors propose InstructTTSEval, a comprehensive benchmark for evaluating complex natural-language instruction following in TTS systems. The benchmark consists of three tasks: Acoustic-Parameter Specification (low-level acoustic control), Descriptive-Style Directive (high-level style inference), and Role-Play (character voice synthesis). A dataset of 6,000 instructions with reference audio is constructed using a pipeline involving Gemini for audio caption generation and GPT-4o for instruction generation. Evaluation is performed using Gemini as an automated judge, comparing both open-source and closed-source TTS models across the three task categories.

## Key Results
- Closed-source models (Gemini, GPT-4o-mini-TTS) significantly outperform open-source models in instruction-following TTS
- All evaluated models fall short of human-level expressiveness in handling complex natural language instructions
- Major challenges identified include reproducing natural vocal events, managing emotional transitions, and synthesizing character-specific timbres

## Why This Works (Mechanism)
The benchmark works by decomposing instruction-following TTS into three well-defined task categories that progressively increase in complexity. The automated evaluation pipeline using Gemini as a judge provides consistent assessment across models. The dataset generation pipeline using GPT-4o and Gemini ensures diverse instruction types and corresponding reference audio. The systematic evaluation framework allows for quantitative comparison of model performance across different instruction categories and complexity levels.

## Foundational Learning

**Audio Caption Generation**: Why needed - To create reference audio descriptions for instruction generation; Quick check - Verify caption accuracy by comparing human and model-generated descriptions.

**Instruction Complexity Taxonomy**: Why needed - To categorize instructions by control level and evaluation difficulty; Quick check - Validate task boundaries through human annotation agreement.

**Automated Speech Evaluation**: Why needed - To provide scalable and consistent assessment across large model sets; Quick check - Correlate automated scores with human perceptual ratings.

## Architecture Onboarding

**Component Map**: Instruction Generator (GPT-4o) -> Audio Caption Generator (Gemini) -> Instruction-Reference Pair -> TTS Model -> Generated Audio -> Evaluation Judge (Gemini)

**Critical Path**: Text instruction → TTS synthesis → Audio evaluation, with dataset creation as a prerequisite step.

**Design Tradeoffs**: Automated evaluation offers scalability but may miss nuanced perceptual qualities that humans detect; open-source models provide transparency but lag behind proprietary solutions in performance.

**Failure Signatures**: Models struggle with emotional transitions, character-specific timbres, and natural vocal events like breaths and pauses.

**First Experiments**: 1) Test instruction clarity by evaluating model responses to simplified vs. complex instructions; 2) Assess cross-task generalization by evaluating models on out-of-distribution instruction types; 3) Compare human and automated judge agreement rates across different instruction categories.

## Open Questions the Paper Calls Out

None

## Limitations

- Dataset representativeness may be limited due to generation using GPT-4o and Gemini models, potentially introducing systematic biases
- Evaluation methodology relies on Gemini as automated judge, which may not capture all human perceptual nuances in speech quality
- Performance gaps between open-source and closed-source models could be influenced by instruction interpretation differences rather than synthesis capabilities

## Confidence

- High confidence in benchmark design framework and task categorization (Acoustic-Parameter Specification, Descriptive-Style Directive, Role-Play)
- Medium confidence in quantitative evaluation results, given automated judging with limited human validation
- Low confidence in generalizability to real-world deployment scenarios, as benchmark focuses on controlled instruction types

## Next Checks

1. Conduct human perceptual studies comparing automated Gemini judge scores against human ratings across all three task categories to establish correlation and identify systematic differences
2. Expand dataset coverage by generating additional instructions targeting underrepresented acoustic phenomena and speaking styles identified in error analysis
3. Implement cross-validation using multiple automated judges (e.g., Claude, GPT-4) to assess consistency and potential model-specific biases in evaluation outcomes