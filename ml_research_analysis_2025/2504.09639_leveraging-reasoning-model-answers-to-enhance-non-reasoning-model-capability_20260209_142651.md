---
ver: rpa2
title: Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability
arxiv_id: '2504.09639'
source_url: https://arxiv.org/abs/2504.09639
tags:
- reasoning
- performance
- answer
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of leveraging high-quality
  outputs from reasoning-intensive large language models (such as DeepSeek-R1 and
  OpenAI-o1) to enhance the capabilities of non-reasoning models. The central hypothesis
  is that training non-reasoning models using answers derived from reasoning models
  can lead to superior performance.
---

# Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability

## Quick Facts
- arXiv ID: 2504.09639
- Source URL: https://arxiv.org/abs/2504.09639
- Reference count: 6
- Primary result: Answer-only distillation from reasoning models improves structured task performance but degrades conversational alignment

## Executive Summary
This paper investigates whether high-quality outputs from reasoning-intensive models (DeepSeek-R1, OpenAI-o1) can enhance non-reasoning model capabilities through supervised fine-tuning. The authors hypothesize that training non-reasoning models on reasoning model answers can yield superior performance. They conduct experiments using three methodologies for incorporating reasoning outputs: using the full original response, using only the final answer component, and using summarized thinking steps concatenated with answers. Through extensive benchmarking across diverse tasks, they find that answer-only distillation significantly improves reasoning and coding performance but slightly degrades chat-oriented metrics, while summarized thinking approaches optimize for hard reasoning at the cost of instruction-following ability.

## Method Summary
The study curates a diverse dataset of approximately 1.3 million instances from multiple sources and generates responses using DeepSeek-R1 with structured output formatting (tagging thinking steps vs. final answers). Three primary methods are evaluated: (1) using the original full response, (2) using only the direct answer component, and (3) summarizing the thinking process and concatenating it with the answer. Supervised fine-tuning experiments are conducted on Qwen2.5-32B using learning rate 8e-6, batch size 64, and 3 epochs. Performance is evaluated across benchmarks including GPQA-Diamond, GSM8K, MMLU, HumanEval, IFEval, AlignBench, and MTBench to compare the effectiveness of different distillation approaches.

## Key Results
- Answer-only distillation (Areason) achieved GSM8K 92.2 and HumanEval 90.9, significant improvements over baseline
- Think Summarization method achieved highest GPQA score (47.7) but degraded IFEval to 61.2 from 73.4 baseline
- Answer-only method showed lower AlignBench (6.9) and MTBench (7.6) compared to control (7.6, 8.2)
- Raw baseline showed most stable performance across tasks but lowest reasoning scores

## Why This Works (Mechanism)

### Mechanism 1
Answer-component distillation from reasoning models improves non-reasoning model performance on structured reasoning tasks. The mechanism relies on reasoning models generating higher-quality final answers through intermediate thinking steps, which transfer through supervised learning when non-reasoning models are fine-tuned on these distilled answers. Evidence shows significant gains on GSM8K (92.2 vs baseline 83.7) and HumanEval (90.9 vs 80.5). This breaks down if answer quality doesn't primarily carry reasoning capability or if the student model lacks capacity to internalize patterns.

### Mechanism 2
Including summarized reasoning traces with answers improves hard reasoning tasks but degrades instruction-following. The mechanism involves concatenated Sthink (summarized thinking) providing contextual reasoning cues for complex tasks like GPQA-Diamond, while structural format changes interfere with instruction adherence. Evidence shows GPQA score of 47.7 but IFEval drops to 61.2. This fails if IFEval degradation stems from summarization quality rather than format, or if GPQA gains come from other data properties.

### Mechanism 3
Answer-only distillation doesn't transfer conversational alignment because procedural explanations reside in the thinking trace (Treason), leaving answers overly concise for conversational use. The mechanism suggests conversational quality depends on information in the thinking trace absent from final answers. Evidence shows lower AlignBench (6.9) and MTBench (7.6) with answer-only approach. This breaks down if chat metric degradation results from domain mismatch rather than answer structure.

## Foundational Learning

- **Test-time scaling (inference compute)**: Why needed: The paper builds on reasoning models improving outputs by allocating compute during inference. Quick check: Can you explain why increasing inference compute might produce better training data than scaling parameters?

- **Supervised Fine-Tuning (SFT) as knowledge distillation**: Why needed: The entire framework uses SFT to transfer capabilities from reasoning to non-reasoning models. Quick check: What are limitations of SFT for transferring reasoning compared to reinforcement learning?

- **Response structure decomposition (Treason vs. Areason)**: Why needed: The core contribution evaluates how structuring reasoning outputs affects downstream performance. Quick check: Why might separating thinking steps from final answers change what the student model learns?

## Architecture Onboarding

- Component map: DeepSeek-R1 -> Qwen2.5-32B (SFT) -> Benchmark evaluation
- Critical path: 1) Curate 1.3M prompts from multiple sources, 2) Generate responses with DeepSeek-R1 using domain-specific prompts, 3) Parse outputs into Treason and Areason, 4) Apply one of three methods (Rorig, Areason-only, SthinkâŠ•Areason), 5) Run SFT with specified hyperparameters
- Design tradeoffs: Answer-only best for coding/math, worse for chat alignment; Think-summarized best for hard reasoning, worst for instruction-following; Raw baseline lowest reasoning but most stable
- Failure signatures: IFEval drops below 70 with Think Summarization (observed: 61.2), AlignBench/MTBench below 7.0 indicates conversational degradation, GPQA below 40 suggests insufficient reasoning transfer
- First 3 experiments: 1) Replicate baseline with raw community responses, 2) Ablate answer-only vs. think-summarized on validation split, 3) Test prompt engineering to elicit concise reasoning within answer structure

## Open Questions the Paper Calls Out

### Open Question 1
Can prompt engineering strategies that elicit integrated reasoning directly from the source model yield higher-fidelity SFT data than post-hoc summarization? The paper suggests this in Section 5 (Limitation) but didn't explore due to time/resource constraints. Evidence would come from comparative experiments between integrated reasoning prompts versus post-hoc summarization.

### Open Question 2
Can methods for dynamically combining reasoning steps with final answers prevent instruction-following degradation observed in static summarization? The Conclusion calls for this research to address trade-offs in current integration strategies. Evidence would come from dynamic integration architectures maintaining high GPQA scores while preserving IFEval performance.

### Open Question 3
Is chat metric degradation when using direct reasoning answers inherent to the data source or mitigable through answer formatting? The authors note answer components are "overly concise" in Section 3.3. Evidence would come from ablation studies where answer components are reformatted or enriched with conversational markers.

## Limitations

- Answer-only distillation degrades conversational alignment metrics (AlignBench, MTBench), suggesting limitations in transferring nuanced reasoning capabilities
- Think-summarized approach significantly degrades instruction-following performance (IFEval drops to 61.2), indicating format sensitivity
- The curated dataset combines heterogeneous sources without analysis of relative contributions or potential biases

## Confidence

- **High Confidence**: Answer-only distillation improves structured reasoning tasks (HumanEval, GSM8K, GPQA-Diamond) with clear performance gains
- **Medium Confidence**: Think-summarized responses optimize for hard reasoning while degrading instruction-following, but lacks mechanistic explanation for format impact on IFEval
- **Low Confidence**: Answer-only distillation fails to transfer conversational alignment based on relative performance, without accounting for dataset domain mismatch

## Next Checks

1. **Format Preservation Experiment**: Modify think-summarized method to maintain original response structure while preserving reasoning content, then re-evaluate IFEval and GPQA to isolate format effects

2. **Cross-Validation on Reasoning Types**: Test distillation methods on balanced subset of reasoning tasks (mathematical, logical, commonsense) to determine if improvements generalize across domains

3. **Decoder Strategy Comparison**: Re-run top configurations using temperature-based sampling with optimized parameters to assess whether greedy decoding underestimates transferable capabilities