---
ver: rpa2
title: 'TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models'
arxiv_id: '2512.00872'
source_url: https://arxiv.org/abs/2512.00872
tags:
- segmentation
- pretraining
- medical
- imaging
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TAP-CT introduces a suite of task-agnostic 3D foundation models
  for CT imaging, adapting the DINOv2 framework to volumetric data with GPU-accelerated
  3D random resized crops and masking. The models are pretrained on 105K in-house
  CT volumes using a ViT architecture modified for 3D inputs.
---

# TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models

## Quick Facts
- **arXiv ID:** 2512.00872
- **Source URL:** https://arxiv.org/abs/2512.00872
- **Reference count:** 31
- **Primary result:** TAP-B-3D achieves 58.2% average Dice Similarity Coefficient on segmentation tasks, outperforming prior methods by 5-23 percentage points

## Executive Summary
TAP-CT introduces a suite of task-agnostic 3D foundation models for CT imaging, adapting the DINOv2 framework to volumetric data with GPU-accelerated 3D random resized crops and masking. The models are pretrained on 105K in-house CT volumes using a ViT architecture modified for 3D inputs. On segmentation tasks, TAP-B-3D achieves a 58.2% average Dice Similarity Coefficient, outperforming prior methods by 5-23 percentage points. On classification, it attains competitive performance with top rankings on two tasks. The work demonstrates that larger 3D models and increased input context consistently improve segmentation quality, while classification remains challenging due to the need for global representation learning from volumetric data.

## Method Summary
TAP-CT adapts the DINOv2 self-supervised learning framework to native 3D volumetric CT data using a ViT architecture with 3D patch embeddings. The pretraining uses a combined DINO and iBOT objective on 105K diverse CT volumes, with GPU-accelerated 3D random resized crops that sample axial area while maintaining fixed depth. The models are evaluated using frozen-feature linear probing for segmentation and attention-based multiple instance learning for classification, demonstrating strong generalization across tasks without fine-tuning.

## Key Results
- TAP-B-3D achieves 58.2% average DSC on segmentation tasks, outperforming prior methods by 5-23 percentage points
- Larger 3D models and increased input context consistently improve segmentation quality
- Classification performance remains challenging with high variance and near-random results on volume-level tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adapting DINOv2 to native 3D volumetric CT data significantly improves learned representations for segmentation tasks compared to 2D or 2.5D approaches.
- **Mechanism:** DINOv2 learns by aligning embeddings of local and global crops. TAP-CT extends this to 3D using GPU-accelerated 3D random resized crops and 3D masking, forcing the model to learn spatial relationships and anatomical consistency across adjacent slices.
- **Core assumption:** Semantic information in CT scans is inherently volumetric, requiring 3D context to disambiguate structures.
- **Evidence anchors:** [abstract] "adapting the DINOv2 framework to volumetric data with GPU-accelerated 3D random resized crops and masking." [section 4.1] "The gains observed when moving from 2D to 3D further emphasize the importance of volumetric encoders over purely slice-based approaches."

### Mechanism 2
- **Claim:** Large-scale task-agnostic pretraining on diverse CT data produces robust frozen representations that generalize more effectively than task-specific pretraining.
- **Mechanism:** Training a high-capacity model on 105K diverse volumes with self-supervised objectives produces universal feature spaces that avoid supervised pretraining bias.
- **Core assumption:** The pretraining dataset is representative of general CT imaging characteristics.
- **Evidence anchors:** [abstract] "yields stable, robust frozen representations that generalize strongly across downstream tasks." [section 4.1] "The substantial performance gap between TAP-B-3D, Curia, and other publicly available models highlights the need for general, high-capacity vision encoders."

### Mechanism 3
- **Claim:** Increasing model size and input context during pretraining consistently improves segmentation quality when using frozen features.
- **Mechanism:** Larger models have greater capacity to learn complex relationships, while larger input context provides more information for learning longer-range anatomical dependencies.
- **Core assumption:** The pretraining objective can effectively leverage increased capacity and context without computational intractability.
- **Evidence anchors:** [abstract] "larger 3D models and increased input context consistently improve segmentation quality." [section 4.1] "segmentation quality largely scales with model dimensionality, model size, and input resolution."

## Foundational Learning

- **Concept: Vision Transformer (ViT)**
  - **Why needed here:** TAP-CT uses ViT as its core encoder, requiring modifications to handle 3D input volumes.
  - **Quick check question:** Can you explain how a standard 2D ViT processes an image (patching, linear projection, position embeddings, transformer encoder)?

- **Concept: Self-Supervised Learning (SSL) with DINOv2**
  - **Why needed here:** This is the pretraining paradigm using a student-teacher framework with DINO (global-local alignment) and iBOT (masked patch prediction) objectives.
  - **Quick check question:** What are the two main objectives in DINOv2 and how do they work together to learn representations without labels?

- **Concept: Linear Probing vs. Fine-Tuning**
  - **Why needed here:** The core evaluation methodology is "frozen-feature" evaluation, judging model quality by training only a simple linear head on frozen embeddings.
  - **Quick check question:** What is the difference between fine-tuning an entire model vs. using a frozen backbone with a linear probe?

## Architecture Onboarding

- **Component map:**
  Input: 3D CT volume -> Preprocessing (clip & normalize) -> 3D Random Resized Crop (GPU) -> SSL Augmentations -> Encoder: 3D ViT (Student & Teacher) -> 3D Patch Embedding + 3D Positional Encoding -> Pretraining Heads: DINO Head, iBOT Head -> Downst: Frozen Encoder -> Sliding Window Embedding Extraction -> Linear Convolution Layer (Segmentation) or ABMIL Head (Classification)

- **Critical path:**
  1. Data loading and on-GPU 3D augmentation
  2. Forward pass through 3D ViT encoder (Student & Teacher)
  3. Calculation of DINOv2 losses
  4. Extract embeddings from frozen encoder using sliding window
  5. Train simple linear head on frozen embeddings

- **Design tradeoffs:**
  - Compute vs. Context: Larger global crops are exponentially more expensive but necessary for 3D understanding
  - Segmentation vs. Classification: Model excels at segmentation but struggles with classification due to global representation challenges
  - Patch Size: 3D patches capture volumetric context better but are more computationally intensive

- **Failure signatures:**
  - Representation Collapse: Prevented by increasing learning rate warmup phase to 25,000 iterations
  - OOM on Evaluation: Some models run out of memory on larger resolutions when extracting dense patch embeddings
  - Inconsistent Classification: High variance and near-random performance on scan-level classification tasks

- **First 3 experiments:**
  1. Reproduce Frozen Segmentation Baseline: Load TAP-B-3D weights, freeze encoder, train linear convolution layer on AMOS22 dataset
  2. Ablate 3D Context: Compare TAP-S-3D vs TAP-S-2.5D on LiTS17 segmentation to quantify 3D patch contribution
  3. Probe Global Representation: Extract [CLS] tokens and patch embeddings for LUNA16 classification, train both linear classifier and ABMIL head

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CT foundation models effectively extract global 3D representations while preserving robust and informative local features?
- Basis in paper: [explicit] The authors state: "These observations indicate that future research in CT foundation models should aim to extract global 3D representations while preserving robust and informative local features."
- Why unresolved: Classification performance does not scale consistently with model size or 3D context, and volume-level tasks remain challenging.
- What evidence would resolve it: Modified architecture or pretraining objective that improves volume-level classification while maintaining segmentation performance.

### Open Question 2
- Question: What less compute-intensive pretraining strategies can emphasize learning robust local features followed by effective derivation of global representations?
- Basis in paper: [explicit] The conclusion proposes: "developing less compute-intensive pretraining strategies that emphasize learning purely robust local features, followed by approaches to derive global representations from them effectively."
- Why unresolved: TAP-B-3D requires 3,648 GPU hours, and DINOv2's multi-crop requirement makes volumetric scaling expensive.
- What evidence would resolve it: New pretraining method achieving comparable performance with substantially reduced GPU hours.

### Open Question 3
- Question: What explains the performance differences between Curia and TAP modelsâ€”private training data or ViT/DINOv2-native architecture?
- Basis in paper: [explicit] The authors note: "Given that Curia and TAP are the only models trained on private data and ViT/DINOv2 native, either could explain the performance difference, and further research is needed to draw conclusions."
- Why unresolved: Both models share architectural similarities but differ in data sources, making attribution unclear.
- What evidence would resolve it: Controlled experiments isolating architecture and training data effects.

## Limitations
- The 105K in-house oncological CT volumes are unavailable, requiring proxy datasets that may not match pretraining distribution
- Classification performance remains weak with high variance and near-random results on volume-level tasks
- The specific GPU-accelerated 3D random resized crop implementation is not publicly specified

## Confidence
- **High confidence:** Segmentation performance improvements from 3D vs 2D pretraining (strong empirical evidence with 58.2% DSC)
- **Medium confidence:** Scaling laws for foundation models (supported by performance trends but limited to internal comparisons)
- **Low confidence:** Classification task generalization (results show high variance and near-random performance)

## Next Checks
1. Reproduce frozen-feature segmentation baseline on AMOS22 using released weights to verify the 72.4% DSC claim
2. Ablate 3D context by comparing TAP-S-3D vs TAP-S-2.5D on LiTS17 segmentation to quantify 3D patch contribution
3. Empirically test global representation capacity by training both [CLS]-only and ABMIL classifiers on TAP-B-3D for LUNA16 classification