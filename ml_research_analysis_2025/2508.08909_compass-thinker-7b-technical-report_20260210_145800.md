---
ver: rpa2
title: Compass-Thinker-7B Technical Report
arxiv_id: '2508.08909'
source_url: https://arxiv.org/abs/2508.08909
tags:
- training
- arxiv
- compass-thinker-7b
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Compass-Thinker-7B, a 7B parameter language
  model fine-tuned with reinforcement learning for mathematical reasoning. The model
  is trained using a variant of GRPO with improvements such as removing KL loss, dynamic
  sampling, and clip-higher strategy to enhance training stability and sample efficiency.
---

# Compass-Thinker-7B Technical Report

## Quick Facts
- arXiv ID: 2508.08909
- Source URL: https://arxiv.org/abs/2508.08909
- Reference count: 10
- Primary result: RL fine-tuning removes KL loss, uses clip-higher and dynamic sampling to boost 7B math reasoning from 20.0% to 40.0% on AIME2024

## Executive Summary
Compass-Thinker-7B is a 7B parameter language model fine-tuned with a modified GRPO algorithm for mathematical reasoning. The authors remove KL divergence penalties, implement a clip-higher strategy, and use dynamic sampling to enhance training stability and sample efficiency. Trained on 30k curated math problems with rule-based rewards, the model achieves significant performance gains on multiple benchmarks, reaching 40.0% accuracy on AIME2024 compared to 20.0% for the base model.

## Method Summary
The model is initialized from Qwen2.5-Math-7B and fine-tuned using a GRPO variant that removes KL loss penalties, implements a clip-higher strategy with expanded PPO clipping bounds, and applies dynamic sampling to filter groups with 0% or 100% accuracy. Training uses the verl framework with a custom rule-based verifier replacing Math-Verify, and employs a 30k dataset of curated math problems graded by difficulty. The training pipeline uses temperature and top-p sampling both set to 1.0, with hyperparameters including learning rate 1e-6, batch size 128, and max sequence length 4096.

## Key Results
- AIME2024 accuracy improved from 20.0% to 40.0% after RL fine-tuning
- Outperformed other 7B RL models on multiple benchmarks including AMC, Math500, Minerva Math, and Olympiad Bench
- Demonstrates that careful RL design can unlock strong reasoning capabilities even in smaller models

## Why This Works (Mechanism)

### Mechanism 1
Removing KL divergence penalty from GRPO objective unleashes greater reasoning potential in small models without causing training collapse. The verifiable nature of math rewards acts as sufficient constraint to prevent catastrophic forgetting.

### Mechanism 2
Expanding PPO clipping upper bound preserves policy entropy, preventing premature convergence to low-confidence reasoning paths. This allows larger policy shifts for confident, correct reasoning steps.

### Mechanism 3
Dynamic sampling filters groups with 0% or 100% accuracy to optimize gradient efficiency by focusing compute on "learnable" marginal cases where the model is partially uncertain.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Replaces Critic Model with group comparison mechanism to lower memory usage. *Quick check*: How does GRPO estimate value without Critic? (Answer: By comparing reward to group mean/std).

- **Rule-Based Rewards vs Reward Models**: Uses symbolic verification instead of neural judges to prevent reward hacking. *Quick check*: Why use -1/1 rewards instead of continuous scores? (Answer: Ensures only verifiable correctness is rewarded).

- **Entropy Collapse**: The paper's Clip-Higher strategy fights this phenomenon. *Quick check*: What happens during entropy collapse? (Answer: Output distribution becomes extremely confident but potentially stuck in narrow suboptimal mode).

## Architecture Onboarding

- **Component map**: Qwen2.5-Math-7B (Base) -> verl (Framework) -> 30k Curated Math Problems (Data) -> Custom Verifier (Verifier) -> GRPO variant (Algorithm)

- **Critical path**: Data Curation (deduplication/filtering) → Verification (custom tool) → Sampling (groups, temp=1.0) → Filtering (dynamic sampling) → Update (GRPO with high clipping)

- **Design tradeoffs**: Stability vs Potential (traded KL constraints for unleashed potential); Cost vs Accuracy (7B model low cost, but custom verification high engineering cost)

- **Failure signatures**: Training Stall (if dynamic sampling too aggressive); Reasoning Loops (without KL penalties, repetitive patterns hitting token limit)

- **First 3 experiments**: 1) Baseline Validation (run base model on AIME2024 subset to reproduce 20.0%); 2) KL Ablation (implement GRPO with/without KL on 1k data subset); 3) Verifier Stress Test (test custom verifier against ground truth for false positives)

## Open Questions the Paper Calls Out
1. Do the proposed RL optimizations directly transfer to improve reasoning in models larger than 7B parameters?
2. Is effectiveness dependent on initializing from math-specialized base model rather than generalist model?
3. Does focusing RL training on math dataset lead to catastrophic forgetting of general capabilities or out-of-domain reasoning?

## Limitations
- Exact Clip-Higher bound (epsilon_high) is not provided in the paper
- Multi-stage curriculum details are absent (number of stages, steps, logic)
- Custom verification program code is not shared, limiting exact reproduction

## Confidence

- **High confidence**: Baseline performance claims (AIME2024 20.0%→40.0%, comparative rankings)
- **Medium confidence**: Mechanism claims (benefits of KL removal, Clip-Higher supported by ablation discussions)
- **Low confidence**: Training stability claims (no training curves or loss dynamics provided to substantiate stability without KL)

## Next Checks

1. Implement controlled ablation study comparing GRPO with/without KL loss on small math dataset (1k problems) to test training stability claims empirically

2. Systematically vary Clip-Higher parameter (epsilon_high) from 0.2 to 0.4 in increments to identify optimal range and test claimed entropy preservation

3. Test custom verifier implementation against Math-Verify on held-out set to quantify impact of verifier choice on reward signal quality