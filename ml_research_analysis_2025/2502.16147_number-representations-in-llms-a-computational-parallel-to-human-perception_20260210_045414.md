---
ver: rpa2
title: 'Number Representations in LLMs: A Computational Parallel to Human Perception'
arxiv_id: '2502.16147'
source_url: https://arxiv.org/abs/2502.16147
tags:
- component
- layer
- birth
- population
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) encode numerical values in a structured,
  non-uniform manner, exhibiting sublinear scaling reminiscent of human logarithmic
  mental number line perception. This study investigates how LLMs represent numerical
  magnitudes by analyzing hidden state embeddings across model layers using dimensionality
  reduction techniques (PCA and PLS) and geometric regression.
---

# Number Representations in LLMs: A Computational Parallel to Human Perception

## Quick Facts
- **arXiv ID**: 2502.16147
- **Source URL**: https://arxiv.org/abs/2502.16147
- **Reference count**: 40
- **Primary result**: LLMs encode numerical values in a structured, non-uniform manner, exhibiting sublinear scaling reminiscent of human logarithmic mental number line perception

## Executive Summary
This study investigates how large language models represent numerical magnitudes by analyzing hidden state embeddings across model layers. Through dimensionality reduction techniques (PCA and PLS) and geometric regression, researchers discovered that numerical embeddings maintain order preservation but exhibit systematic compressionâ€”distances between consecutive values decrease as magnitudes increase. The findings suggest that LLMs process numbers with cognitive-like patterns, where smaller values have higher resolution while larger numbers are compressed, refining the linear hypothesis of internal representations and highlighting the importance of geometric analysis in understanding numerical abstractions in artificial neural networks.

## Method Summary
The researchers employed dimensionality reduction techniques (PCA and PLS) combined with geometric regression to analyze numerical embeddings in LLMs. They examined hidden state representations across different model layers, testing both contextualized numbers and letters as well as real-world tasks involving celebrity birth years and population sizes. The analysis focused on how distances between numerical values scale with magnitude, using both linear and logarithmic transformations to assess the geometric properties of number representations.

## Key Results
- Numerical embeddings maintain order preservation but exhibit systematic compression where distances between consecutive values decrease as magnitudes increase
- PCA consistently captures stronger sublinearity than PLS, suggesting linear probes may obscure underlying geometric structures
- Results align with human cognitive patterns where smaller values have higher resolution while larger numbers are compressed

## Why This Works (Mechanism)
The sublinear scaling observed in LLM numerical representations mirrors human logarithmic mental number line perception. This mechanism suggests that both artificial and biological neural systems may employ similar strategies for efficient numerical representation. The compression of larger numbers while maintaining high resolution for smaller values could be an optimization strategy that balances representational precision with computational efficiency, particularly relevant for handling the vast range of possible numerical values in natural language.

## Foundational Learning
- **Dimensionality Reduction (PCA/PLS)**: Used to reduce high-dimensional embeddings to interpretable lower-dimensional spaces; needed to visualize and analyze numerical relationships; quick check: verify variance explained by components
- **Geometric Regression**: Analyzes relationships between numerical values and their geometric properties in embedding space; needed to quantify scaling patterns; quick check: test different regression models
- **Sublinear Scaling**: Mathematical relationship where output increases more slowly than input; needed to characterize non-uniform number representations; quick check: calculate scaling exponents
- **Logarithmic Compression**: Mathematical transformation that reduces range while preserving order; needed to model human-like number perception; quick check: compare linear vs. logarithmic fits
- **Hidden State Analysis**: Examination of intermediate neural network representations; needed to understand how numbers are processed internally; quick check: visualize layer-wise changes

## Architecture Onboarding

**Component Map**: Input Text -> Token Embedding -> Transformer Layers -> Hidden States -> Dimensionality Reduction -> Geometric Analysis

**Critical Path**: The critical path flows from input text through tokenization and transformer layers to hidden state extraction, where numerical representations are analyzed. The dimensionality reduction step is crucial for making high-dimensional embeddings interpretable, while geometric regression quantifies the scaling relationships that reveal the sublinear pattern.

**Design Tradeoffs**: The study must balance between computational tractability (using PCA/PLS for interpretability) and preserving the full complexity of numerical representations. Using contextualized embeddings introduces variability based on prompt design, while focusing on base-10 numbers may limit generalizability to other numerical systems.

**Failure Signatures**: If the sublinear scaling pattern disappears under different dimensionality reduction techniques, it may indicate that the observed pattern is an artifact of the methodological choices rather than inherent to LLM representations. Inconsistent results across different numerical tasks or model architectures would also challenge the generalizability of the findings.

**First Experiments**:
1. Test whether the sublinear scaling pattern persists when using alternative dimensionality reduction techniques like t-SNE or UMAP
2. Examine whether the pattern holds for non-base-10 numerical systems and mixed representations (fractions, decimals)
3. Investigate which specific transformer components (attention mechanisms, feed-forward networks) contribute most to the observed numerical encoding patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Dimensionality reduction techniques may oversimplify complex high-dimensional numerical representations
- Focus on base-10 numbers may limit generalizability to other numerical systems
- Contextualized embeddings introduce variability based on prompt design

## Confidence
- Confidence in sublinear scaling and logarithmic-like perception: **Medium**
- Confidence in smaller values having higher resolution: **High**
- Confidence in linear probes obscuring geometric structures: **Medium**

## Next Checks
1. Replicate the study using alternative dimensionality reduction techniques (e.g., t-SNE, UMAP) to assess robustness of sublinear scaling patterns
2. Test the hypothesis with non-base-10 numerical systems and mixed representations (e.g., fractions, decimals) to evaluate generalizability
3. Conduct ablation studies on model architecture (e.g., transformer depth, attention mechanisms) to identify which components drive the observed logarithmic-like encoding