---
ver: rpa2
title: 'HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of
  a Graduate Class'
arxiv_id: '2505.11774'
source_url: https://arxiv.org/abs/2505.11774
tags:
- solution
- problem
- problems
- solutions
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HARDMath2 is a benchmark dataset of 211 original applied mathematics\
  \ problems created collaboratively by students in a Harvard graduate course. The\
  \ problems cover topics like boundary layers, WKB approximations, nonlinear PDEs,\
  \ and asymptotic analysis\u2014areas underrepresented in existing math benchmarks."
---

# HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class

## Quick Facts
- arXiv ID: 2505.11774
- Source URL: https://arxiv.org/abs/2505.11774
- Reference count: 40
- Students created 211 original applied mathematics problems through iterative human-LLM interaction

## Executive Summary
HARDMath2 is a benchmark dataset of 211 original applied mathematics problems created collaboratively by students in a Harvard graduate course. The problems cover topics like boundary layers, WKB approximations, nonlinear PDEs, and asymptotic analysis—areas underrepresented in existing math benchmarks. Students wrote and verified problems while interacting with LLMs to iteratively increase difficulty, exposing model weaknesses. An automated evaluation framework parses LaTeX solutions and numerically compares model outputs to ground-truth answers. Top models (e.g., Gemini 2.5 Flash Thinking, o3) achieve at most 60% accuracy, with integrals being easiest and nonlinear PDEs hardest. The dataset fills a gap in applied mathematics evaluation and demonstrates the pedagogical value of using LLMs to design more challenging problems.

## Method Summary
Students in a Harvard graduate course created 211 applied mathematics problems covering boundary layers, WKB approximations, nonlinear PDEs, and asymptotic analysis. The problems were verified through peer review and numerical checks using Colab notebooks. An automated evaluation framework parses LaTeX solutions from model outputs using regex extraction and SymPy conversion, then numerically compares results at random points in [1,2]. This approach handles multiple valid solution forms that symbolic equality cannot distinguish. Models were prompted with standardized templates and required to provide boxed LaTeX answers.

## Key Results
- HARDMath2 covers applied mathematics topics (boundary layers, WKB, nonlinear PDEs) absent from existing benchmarks
- Top models achieve at most 60% accuracy, with integrals easiest and nonlinear PDEs hardest
- Three strategies emerged from human-LLM interaction: structural obfuscation, vanishing terms, and initial condition failures
- The dataset demonstrates both benchmark value and pedagogical benefits of LLM-adversarial problem design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative human-LLM interaction produces harder problems than static textbook curation.
- Mechanism: Students submit problems → receive immediate LLM evaluation feedback → identify model failure patterns (e.g., structural obfuscation, vanishing terms, initial condition failures) → revise problems to exploit those failures. This creates an adversarial co-evolution loop.
- Core assumption: Students can reliably identify and generalize specific model failure modes into reusable problem-design strategies.
- Evidence anchors:
  - [abstract] "students identified strategies to create increasingly difficult problems by interacting with the models and exploiting common failure modes"
  - [section 4] Documents three specific strategies students developed: structural obfuscation of canonical equations, introducing vanishing terms, and initial condition failures
  - [corpus] Weak corpus support—no directly comparable iterative benchmarks found in neighbors; most related benchmarks use static expert curation
- Break condition: If students cannot reliably distinguish model failures from prompt ambiguity, or if failure modes don't generalize across problem instances, the mechanism would not produce systematically harder problems.

### Mechanism 2
- Claim: Numerical evaluation at random points enables objective comparison of approximate solutions that symbolic equality cannot handle.
- Mechanism: LaTeX output → regex extraction → SymPy conversion → numerical evaluation at random values in [1,2] → threshold comparison. This sidesteps the problem that equivalent asymptotic approximations may have different symbolic forms.
- Core assumption: Numerically-equal-at-random-points implies mathematical equivalence for the class of expressions in the benchmark.
- Evidence anchors:
  - [section 3.1.1] "This numerical verification step was necessary because the problems in our dataset may be solved using methods that produce the same formulae but could not be simply compared using SymPy's built-in equality checking"
  - [Table 1] Explicitly contrasts HARDMath2's "Automated formula parsing" with "LLM-as-a-Judge" and "Manual human grading"
  - [corpus] No corpus papers address this specific numerical-vs-symbolic tradeoff for asymptotic expressions
- Break condition: If two genuinely different approximations happen to coincide numerically at sampled points, false positives would occur. If evaluation points fall in singular regions, false negatives would occur.

### Mechanism 3
- Claim: Requiring students to design LLM-resistant problems deepens their conceptual understanding beyond passive problem-solving.
- Mechanism: To fool an LLM, students must understand both the mathematical technique AND common misconceptions/errors → this dual perspective forces deeper engagement than standard homework.
- Core assumption: The cognitive load of adversarial design transfers to improved understanding, rather than just teaching students to game specific model behaviors.
- Evidence anchors:
  - [abstract] "This back-and-forth with the models not only resulted in a richer and more challenging benchmark but also led to qualitative improvements in the students' understanding of the course material"
  - [section 3] Oral final exams verified students understood their submitted problems, which "were on average far more difficult than traditional homework problems"
  - [corpus] Related papers (e.g., "Beyond Final Answers") discuss LLMs for tutoring, but none examine student-as-benchmark-creator as pedagogy
- Break condition: If students learn to exploit superficial model weaknesses without deepening mathematical understanding, the pedagogical benefit would be illusory.

## Foundational Learning

- **Asymptotic analysis and perturbation methods**:
  - Why needed here: The benchmark targets problems without exact solutions (boundary layers, WKB, matched asymptotics). Understanding what "approximate solution" means and why different forms can be equivalent is essential for interpreting results.
  - Quick check question: For the boundary layer problem εy'' − xy' + x³y = 0 with y(0) = A, y(1) = B, why can't the outer solution alone satisfy both boundary conditions?

- **LLM evaluation paradigms and their failure modes**:
  - Why needed here: The paper explicitly contrasts numerical evaluation against LLM-as-judge and manual grading. Understanding why objectivity matters for benchmark reliability is critical.
  - Quick check question: Why might "LLM-as-judge" introduce systematic bias that numerical comparison avoids?

- **Symbolic computation and expression canonicalization**:
  - Why needed here: The parser converts LaTeX → SymPy → numerical values. Understanding why symbolic equality fails for asymptotically-equivalent expressions explains the design choice.
  - Quick check question: Why does SymPy's `expr1 == expr2` return False even when `expr1 - expr2` simplifies to zero numerically?

## Architecture Onboarding

- **Component map**:
  - Google Sheets interface -> collaborative problem/solution entry, LLM query triggers
  - Evaluation server -> hosts parsing code and LLM API calls
  - LaTeX parser -> regex extraction -> SymPy conversion with custom handling for special functions (Gamma, incomplete Beta)
  - Numerical comparator -> evaluates expressions at random points in [1,2], checks closeness threshold
  - Colab notebooks -> student-provided numerical verification of analytical solutions

- **Critical path**:
  1. Student writes problem + LaTeX solution in standardized format
  2. Peer reviewer validates solution (via Colab numerical check)
  3. Parser extracts `\boxed{}` content from model output
  4. Both ground-truth and model solutions converted to SymPy
  5. Both evaluated at same random point; pass if within threshold

- **Design tradeoffs**:
  - Numerical evaluation vs symbolic equality: chosen for handling multiple valid forms, but risks false positives at sampled points
  - Student-generated vs expert-generated: trades consistency for diversity and scalability, relies on peer-review quality control
  - Single-point evaluation vs multi-point: faster but less robust; could miss errors that cancel at specific points

- **Failure signatures**:
  - **Formatting failures**: DeepSeek-R1 never used `\boxed{}`; Gemini 2.5 Pro frequently used non-standard LaTeX
  - **Instruction-following failures**: Models introducing new constants not specified in problem
  - **Initial condition failures**: Models correctly identify solution structure but fail to apply boundary conditions (Section 4.3 example)
  - **Parsing failures**: Ellipses substitution (`...` instead of full expressions), Unicode characters requiring preprocessing

- **First 3 experiments**:
  1. **Parser robustness test**: Feed the parser 50 variations of valid LaTeX expressions (different spacings, alternative notations like `\sin^2` vs `\sin(x)^2`, Unicode vs ASCII) and verify SymPy conversion accuracy.
  2. **Numerical stability audit**: For 20 ground-truth solutions, evaluate at 100 random points and verify consistent results; flag any solutions with high variance that might indicate singularities near sampled regions.
  3. **Failure mode replication**: Manually test the three strategies from Section 4 (structural obfuscation, vanishing terms, initial condition failures) on 3+ models to verify whether identified weaknesses generalize beyond the specific examples in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the "generation-verification gap" be bridged to enable LLMs to autonomously scale the creation of difficult applied mathematics benchmarks?
- **Basis in paper:** [explicit] The Conclusion notes that while students found LLMs could be used to make problems harder, this "would also require more time and effort from the students due to the generation-verification gap."
- **Why unresolved:** The paper identifies this gap as a bottleneck for scaling the dataset but does not propose or test methods to automate the verification of LLM-generated problems.
- **What evidence would resolve it:** A study demonstrating an automated pipeline where LLMs generate and reliably verify novel problem-solution pairs with minimal human intervention.

### Open Question 2
- **Question:** Can frontier models be trained or prompted to recognize canonical equation forms (e.g., Fisher-KPP) when they are disguised by mathematical transformations?
- **Basis in paper:** [inferred] Section 4.1 details "Structural obfuscation" as a strategy where students disguised equations via Galilean transformations, causing models to fail to recognize them and apply incorrect solution methods.
- **Why unresolved:** The paper highlights this as a specific failure mode exploited to increase difficulty but does not investigate whether this is a fundamental limitation or a tractable robustness issue.
- **What evidence would resolve it:** Evaluation results showing high accuracy on a dataset of mathematically equivalent problems presented in both canonical and structurally obfuscated forms.

### Open Question 3
- **Question:** Does the interactive process of creating adversarial examples for LLMs result in quantifiable improvements in student learning outcomes?
- **Basis in paper:** [explicit] The Abstract claims the collaborative process "led to qualitative improvements in the students' understanding," and the Conclusion advocates for applying this framework to other quantitative courses.
- **Why unresolved:** The paper provides anecdotal evidence and course observations but lacks comparative data (e.g., grades, concept inventories) against a control group using traditional problem sets.
- **What evidence would resolve it:** A controlled educational study measuring student performance in courses using LLM-adversarial problem design versus traditional coursework.

## Limitations

- Dataset access and reproducibility constraints: The dataset URL is not provided in the paper, making independent validation impossible without external access. The numerical closeness threshold for solution matching is unspecified, and the exact LaTeX→SymPy parsing pipeline details are not fully documented.
- Benchmark scope: While HARDMath2 fills a gap in applied mathematics evaluation, it represents only one graduate course's output and may not generalize to broader applied math curricula or other educational contexts.
- Evaluation methodology: Single-point numerical evaluation may miss errors that cancel at specific points, and the [1,2] domain choice may not capture edge cases or singular behavior in all expressions.

## Confidence

- **High Confidence**: The pedagogical mechanism (students designing LLM-resistant problems improves understanding) is supported by oral exam evidence and student self-reports. The numerical evaluation approach is technically sound and necessary given the nature of asymptotic solutions.
- **Medium Confidence**: The claim that iterative human-LLM interaction produces systematically harder problems than static curation is plausible given the documented strategies but lacks direct comparison to expert-curated benchmarks of equivalent scope.
- **Low Confidence**: The generalization of identified failure modes across different models and problem instances requires further validation beyond the specific examples provided.

## Next Checks

1. **Parser robustness test**: Feed the parser 50 variations of valid LaTeX expressions (different spacings, alternative notations like `\sin^2` vs `\sin(x)^2`, Unicode vs ASCII) and verify SymPy conversion accuracy.
2. **Numerical stability audit**: For 20 ground-truth solutions, evaluate at 100 random points and verify consistent results; flag any solutions with high variance that might indicate singularities near sampled regions.
3. **Failure mode replication**: Manually test the three strategies from Section 4 (structural obfuscation, vanishing terms, initial condition failures) on 3+ models to verify whether identified weaknesses generalize beyond the specific examples in the paper.