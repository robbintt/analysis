---
ver: rpa2
title: 'Artificial intelligence application in lymphoma diagnosis: from Convolutional
  Neural Network to Vision Transformer'
arxiv_id: '2504.04025'
source_url: https://arxiv.org/abs/2504.04025
tags:
- image
- images
- learning
- transformer
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the diagnostic performance of a Vision Transformer
  (ViT) model to a Convolutional Neural Network (CNN) for distinguishing anaplastic
  large cell lymphoma (ALCL) from classical Hodgkin lymphoma (cHL) using pathology
  whole slide images. The dataset comprised 1,200 image patches (100x100 pixels) from
  20 cases (10 each of ALCL and cHL).
---

# Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer

## Quick Facts
- arXiv ID: 2504.04025
- Source URL: https://arxiv.org/abs/2504.04025
- Reference count: 18
- Both ViT and CNN achieved 100% accuracy on the test set of 120 image patches

## Executive Summary
This study compares Vision Transformer (ViT) and Convolutional Neural Network (CNN) performance for distinguishing anaplastic large cell lymphoma (ALCL) from classical Hodgkin lymphoma (cHL) using pathology whole slide images. The dataset comprised 1,200 image patches (100x100 pixels) from 20 cases (10 each of ALCL and cHL). Both models achieved 100% diagnostic accuracy on the test set of 120 images, demonstrating comparable performance. Despite ViTs typically requiring larger datasets, this study shows they can achieve excellent accuracy even with smaller datasets, offering a promising alternative to CNNs in digital pathology.

## Method Summary
The study used 1,200 image patches (100x100 pixels, 20x magnification) from 20 whole slide images (10 ALCL, 10 cHL), with 60 patches extracted per slide. A custom ViT was implemented in PyTorch with architectural constraints: patch_size=20, d_model=128, num_heads=4, num_layers=6, dropout=0.1. The dataset was split 90%/9%/10% for train/validation/test at the patch level. Training used Adam optimizer (lr=0.001) for 200 epochs with batch_size=32. Both ViT and CNN models were evaluated on the same test set of 120 images.

## Key Results
- Both ViT and CNN achieved 100% accuracy on the 120-image test set
- ViT performance demonstrates feasibility for small pathology datasets with appropriate architectural constraints
- Binary classification between ALCL and cHL was achieved using only 1,200 training patches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViT achieves comparable accuracy to CNN on small pathology datasets when architectural capacity is deliberately constrained.
- Mechanism: The model divides 100×100 images into 5×5 grids of 20×20 patches, embeds each with positional encoding, and processes through 6 transformer layers with 4 attention heads. Self-attention allows each patch to relate to all others regardless of spatial distance, capturing global morphological patterns.
- Core assumption: The discriminative features between ALCL and cHL are global or mid-range spatial relationships rather than purely local texture patterns.
- Evidence anchors:
  - [abstract]: "Self-attention in ViT allows each part of image to relate (pay attention) to other parts of image, regardless of the distance between them."
  - [section]: Hyperparameters table shows patch_size=20, num_layers=6, num_heads=4, d_model=128 as deliberate constraints.
  - [corpus]: Weak validation—neighbor papers show general transformer/CNN comparisons (FMR 0.40-0.54) but no direct replication of small-dataset ViT for lymphoma.

### Mechanism 2
- Claim: Weaker inductive bias in ViT enables flexible feature discovery but typically requires more data; architectural regularization compensates in small-data regimes.
- Mechanism: CNNs encode locality and translation equivariance via convolution; ViTs lack these priors and must learn spatial relationships from data. With small datasets, reduced layers, dropout (0.1), larger patches, and Adam optimizer prevent overfitting while allowing the model to discover relevant patterns.
- Core assumption: The binary classification task (ALCL vs. cHL) is morphologically tractable with limited model capacity.
- Evidence anchors:
  - [abstract]: "ViTs have a weaker inductive bias and therefore allow a more flexible feature detection."
  - [section]: "Overfitting with small training dataset is minimized by: Minimizing Number of layers (6), image size (100), Number of attention heads (4)... Use large image patch size (20)."
  - [corpus]: No direct corpus validation for this regularization strategy in pathology ViTs.

### Mechanism 3
- Claim: High accuracy in this study derives from controlled data conditions that reduce task difficulty.
- Mechanism: Single-scanner acquisition (Philips SG60), consistent 20x magnification, expert-verified labels by hematopathologists, and binary classification reduce variability that typically challenges larger datasets.
- Core assumption: Morphological features of malignant cells are consistent across the WSI and captured at 20x magnification.
- Evidence anchors:
  - [section]: "Possible reasons for the high accuracy of ViT in this study are likely due to: (a) highly accurate labelling of image patches by hematopathologists, (b) rigid inclusion constraints of WSI (by the same scanner in the same lab), (c) only 2 types of lymphomas to classify."
  - [corpus]: "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping" addresses multi-center variability—relevant contrast but FMR=0.0.

## Foundational Learning

- **Concept: Inductive Bias**
  - Why needed here: CNNs assume local features matter most (translation equivariance, local receptive fields). ViTs make fewer assumptions, requiring more data to learn what matters. Understanding this tradeoff determines architecture choice.
  - Quick check question: Why might a CNN fail on a task where the key feature is the relationship between two distant image regions?

- **Concept: Self-Attention and Quadratic Complexity**
  - Why needed here: Attention computes relationships between all patch pairs—O(n²) in sequence length. For WSIs (gigapixels), this is prohibitive; patch-based approaches with larger patches are a necessary compromise.
  - Quick check question: If you double the number of patches, how does attention computation change?

- **Concept: Overfitting in Small-Data Regimes**
  - Why needed here: ViTs typically need large datasets; this study demonstrates capacity reduction as an alternative. Key techniques: fewer layers, larger patches, dropout, Adam optimizer.
  - Quick check question: Why does increasing patch size reduce overfitting risk?

## Architecture Onboarding

- **Component map:** Input patches → Patch embedding (20×20 → linear + positional) → Transformer encoder ×6 → Classification head (2 classes)

- **Critical path:**
  1. Patch extraction from WSIs (60 per slide, expert-selected regions)
  2. Train/validation/test split (90%/9%/10%) at patch level
  3. Training: 200 epochs, Adam optimizer, lr=0.001, batch_size=32
  4. Evaluation: Accuracy on 120 test patches

- **Design tradeoffs:**
  - patch_size=20: Fewer tokens (25 vs 100 for 10×10) → less computation, less overfitting, but loses fine detail
  - num_layers=6 (vs typical 12): Reduces capacity → better for small data, limits complex feature hierarchies
  - d_model=128 (vs typical 768): Fewer parameters → faster, less overfitting, reduced expressiveness

- **Failure signatures:**
  - Training loss plateaus high: Learning rate too low or model underfitting
  - Perfect training, poor validation: Overfitting—increase dropout, reduce layers, or increase patch size
  - Uniform attention weights: Model not learning spatial relationships; may need pretrained weights
  - High variance across folds: Patches from same case in train/test—requires case-level splitting

- **First 3 experiments:**
  1. Case-level cross-validation: Leave-one-case-out to ensure patches from the same WSI don't leak between train and test
  2. Patch size ablation: Test [10, 16, 20, 25] to measure resolution vs. overfitting tradeoff
  3. External validation: Apply trained model to images from different scanners or institutions to assess generalization

## Open Questions the Paper Calls Out

- Can Vision Transformer (ViT) models maintain comparable performance to CNNs when applied to significantly larger, multi-institutional lymphoma datasets? The current study utilized a relatively small dataset (1,200 patches from 20 cases), which limits the ability to assess scalability and the data-hunger typically associated with ViTs. A follow-up study comparing model performance on a dataset comprising thousands of whole slide images from multiple medical centers would resolve this.

- How robust is the ViT model's diagnostic accuracy across different scanner platforms and histological stains? The current data was derived using a single scanner (Philips SG60) and protocol, creating an idealized environment that does not reflect real-world technical variance. Testing the trained model on external validation sets digitized by different scanners (e.g., Hamamatsu, Leica) and stained with slightly different protocols would resolve this.

- Can unsupervised or self-supervised pre-training improve the performance or data efficiency of ViTs for this specific lymphoma classification task? The current study relied solely on supervised learning, failing to determine if the ViT's feature extraction could be enhanced by the self-supervised learning mechanisms mentioned in the Introduction. A comparison of convergence rates and accuracy between the current supervised model and a model pre-trained on unlabeled pathology images would resolve this.

## Limitations

- Dataset size is extremely small (1,200 patches from 20 cases), raising concerns about statistical significance and overfitting
- Single-scanner acquisition and controlled laboratory conditions create a homogeneous dataset that may not generalize to diverse clinical settings
- Binary classification task between ALCL and cHL represents a simplified diagnostic scenario compared to real-world multi-class lymphoma subtyping
- Study lacks external validation on independent datasets or cross-institutional testing to verify generalizability

## Confidence

- **High Confidence**: ViT and CNN achieved equivalent performance on this specific dataset; the architectural constraints described are technically sound and would produce the claimed model behavior.
- **Medium Confidence**: The mechanism that architectural regularization (reduced layers, larger patches, dropout) can compensate for small dataset size in pathology applications. This is theoretically plausible but lacks direct corpus validation.
- **Low Confidence**: The generalizability claim that ViTs represent a viable alternative to CNNs for small pathology datasets without further validation. The 100% accuracy on a single, highly controlled dataset does not establish this broader principle.

## Next Checks

1. **Case-level cross-validation**: Implement leave-one-case-out validation to ensure patches from the same WSI do not leak between training and test sets, verifying that the 100% accuracy isn't inflated by case-level correlations.

2. **External institutional validation**: Test the trained model on WSIs from different scanners, laboratories, and institutions to assess real-world generalization beyond the controlled single-scanner environment.

3. **Patch size ablation study**: Systematically evaluate model performance across different patch sizes [10, 16, 20, 25] to quantify the resolution vs. overfitting tradeoff and determine the minimum patch size that maintains diagnostic accuracy.