---
ver: rpa2
title: Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented
  Code Generation
arxiv_id: '2502.03233'
source_url: https://arxiv.org/abs/2502.03233
tags:
- code
- vulnerable
- knowledge
- security
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates security risks in Retrieval-Augmented
  Code Generation (RACG) systems, specifically how vulnerable code in knowledge bases
  can compromise generated code. The study examines two attack scenarios: exposed
  programming intent (where attackers know developer queries) and hidden intent (where
  attackers poison the knowledge base blindly).'
---

# Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation

## Quick Facts
- arXiv ID: 2502.03233
- Source URL: https://arxiv.org/abs/2502.03233
- Reference count: 40
- This paper investigates how vulnerable code in knowledge bases can compromise generated code in RACG systems

## Executive Summary
This paper investigates security risks in Retrieval-Augmented Code Generation (RACG) systems, specifically how vulnerable code in knowledge bases can compromise generated code. The study examines two attack scenarios: exposed programming intent (where attackers know developer queries) and hidden intent (where attackers poison the knowledge base blindly). Using four large language models, two retrievers, and extensive experiments across 16 sub-scenarios, the research demonstrates that even a single poisoned code example can render up to 48% of generated code vulnerable. The findings reveal that code-specific LLMs are more susceptible to generating vulnerabilities than general-purpose models, and that vulnerability risk increases with example-query similarity.

## Method Summary
The study uses ReposVul dataset filtered to 12,053 instances across C, C++, Java, and Python with 236 CWE types. Four LLMs (GPT-4o, Llama-3-8B, CodeLlama-13B, DeepSeek-Coder-V2-16B) and two retrievers (BM25, JINA) are evaluated under two poisoning scenarios. Scenario I involves attackers knowing queries and injecting vulnerable examples via similarity-based retrieval. Scenario II uses K-means clustering to poison representative samples blindly. The system generates code using one-shot and three-shot settings with temperature=0, top_p=0.95, and max_new_tokens=4096. Vulnerability detection employs an LLM judge using two-step pattern extraction and detection.

## Key Results
- A single poisoned code example can increase vulnerability rate from 29% to 48% in generated code
- Code-specific LLMs generate more vulnerabilities than general-purpose models (VR 0.53 vs 0.37)
- Vulnerability risk increases sharply when example-query similarity exceeds 60%
- Dense retrievers (JINA) retrieve more vulnerable code (VRRC 0.42) than sparse retrievers (BM25 at 0.07)
- C++ shows highest vulnerability rates (0.44-0.47) while Python/Java demonstrate more resistance

## Why This Works (Mechanism)

### Mechanism 1: Vulnerability Propagation via High-Similarity Retrieval
- Claim: Retrieved code with higher semantic similarity to queries is more likely to contain vulnerabilities that propagate to generated code.
- Mechanism: Dense retrievers (e.g., JINA embeddings) identify semantically similar code examples from poisoned knowledge bases. When similarity exceeds 60%, VR increases sharply because LLMs closely replicate retrieved patterns, including vulnerabilities. Attackers exploit this by injecting vulnerable code semantically aligned with target queries.
- Core assumption: LLMs treat highly similar retrieved examples as authoritative templates to follow closely.
- Evidence anchors:
  - Table 8 shows aggregated VR rises from 0.35 in [60,80) to 0.53 in [80,100] similarity range.
  - VRRC increases with similarity (0.28→0.42), confirming more vulnerable code is retrieved at higher similarity.
  - Related work "RAG Safety" confirms knowledge poisoning is an active threat vector.

### Mechanism 2: Few-Shot Learning Amplifies Vulnerability Exposure
- Claim: Increasing demonstration examples in few-shot learning improves code quality but simultaneously increases vulnerability generation risk.
- Mechanism: More shots mean more retrieved examples per query, increasing the probability that at least one contains a vulnerability. LLMs exposed to multiple examples exhibit stronger pattern-mimicking behavior, including replication of insecure patterns.
- Core assumption: LLMs do not distinguish between secure and insecure patterns when leveraging in-context examples for generation.
- Evidence anchors:
  - Table 6 shows VR increases from 0.46 to 0.49 (6.5% rise) from one-shot to three-shot with JINA retriever in Scenario I.
  - VRRC rises from 0.41 to 0.44, indicating proportionally more vulnerable code in retrieved contexts.
  - "RESCUE" proposes retrieval augmented secure code generation, acknowledging the tradeoff between context richness and security.

### Mechanism 3: Code-Specific LLMs Exhibit Higher Susceptibility
- Claim: Code-specialized LLMs generate more vulnerable code than general-purpose LLMs under equivalent poisoning conditions.
- Mechanism: Code LLMs (e.g., CodeLlama, DeepSeek-Coder) are trained on larger code-focused corpora, learning more patterns—including vulnerable ones. Their optimization for code generation makes them more prone to replicate retrieved vulnerable patterns faithfully.
- Core assumption: Code-specific training exposes models to more vulnerability patterns without corresponding security-awareness training.
- Evidence anchors:
  - CodeLlama achieves VR of 0.53 with JINA retriever (9 poisoned samples) vs Llama-3 at 0.37.
  - Authors attribute this to "training on larger, code-focused datasets" including vulnerable patterns.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) Pipeline**: Query → Retriever → Knowledge Base → Retrieved Examples → LLM → Generated Output.
  - Why needed here: Understanding the RACG workflow is essential to trace where poisoning can occur and how it propagates.
  - Quick check question: At which stage does knowledge base poisoning directly affect the system—the retrieval phase or the generation phase?

- **Dense vs. Sparse Retrieval**: Dense retrievers (e.g., JINA) use learned embeddings and cosine similarity; sparse retrievers (e.g., BM25) use term frequency. Dense retrievers generally achieve higher semantic matching but also retrieve more semantically aligned vulnerable code.
  - Why needed here: The paper shows JINA retrieves more vulnerable code (VRRC 0.42 vs 0.07 for BM25 with 9 poisoned samples), making retriever choice a security factor.
  - Quick check question: Which retriever type would you choose if security against poisoning was your primary concern, accepting some quality tradeoff?

- **CWE (Common Weakness Enumeration)**: Standardized identifiers for vulnerability types (e.g., CWE-352 for CSRF, CWE-89 for SQL Injection).
  - Why needed here: The paper demonstrates VR varies significantly by CWE type (CWE-352: ~0.79 vs CWE-434: ~0.26), informing targeted mitigation.
  - Quick check question: If your knowledge base contains OAuth implementations, which CWEs should you prioritize auditing for?

## Architecture Onboarding

- **Component map**: Query Interface → Retriever (BM25 or JINA) → Knowledge Base → LLM (CodeLlama, DeepSeek-Coder, GPT-4o, Llama-3) → Generated Output

- **Critical path**: Query → Retriever (selects top-k examples) → Prompt Construction (query + examples) → LLM Generation → Output Code. Poisoning affects step 2-3; vulnerability manifests at step 4-5.

- **Design tradeoffs**:
  1. **Retriever choice**: Dense (JINA) = higher quality but higher VRRC (0.42 vs 0.07); Sparse (BM25) = lower quality but more poisoning-resistant.
  2. **Few-shot vs One-shot**: More shots improve similarity scores (+0.04) but increase VR (+6.5%).
  3. **Code-specific vs General LLMs**: Code LLMs achieve better functionality but higher baseline vulnerability rates.

- **Failure signatures**:
  1. **High VRRC (>0.4)**: Indicates poisoned knowledge base is being actively retrieved.
  2. **VR spike with single poisoned sample**: From 0.29→0.48 for CodeLlama suggests successful targeted poisoning.
  3. **Language-specific VR elevation**: C++ consistently highest (0.44-0.47), suggesting need for language-aware security policies.

- **First 3 experiments**:
  1. **Baseline vulnerability measurement**: Run RACG with clean knowledge base across all 4 LLMs, measure baseline VR to establish reference.
  2. **Single-sample poisoning test**: Inject one vulnerable code example targeting a specific query type, measure VR increase to validate susceptibility.
  3. **Retriever comparison under poisoning**: Compare BM25 vs JINA with identical poisoned KB (5 samples), measure VRRC and VR to quantify security-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative retrieval strategies (e.g., selecting the second most similar example or randomly choosing from top-k candidates) effectively mitigate knowledge base poisoning attacks in RACG systems while maintaining code generation quality?
- Basis in paper: Section 6.1 states: "an alternative strategy would be to select the second most similar example or randomly choose from a candidate pool containing several of the most similar examples, and we plan to explore such a way in future work."
- Why unresolved: The paper demonstrates the poisoning threat but only proposes mitigation directions without implementing or evaluating alternative retrieval strategies.
- What evidence would resolve it: Empirical experiments comparing VR and similarity metrics across different retrieval strategies under poisoning scenarios.

### Open Question 2
- Question: How do the vulnerability propagation patterns identified in C, C++, Java, and Python generalize to other popular programming languages such as Go, Rust, or JavaScript?
- Basis in paper: Section 6.5 states: "as other languages like Go gain popularity, we plan to extend our study to include more programming languages in future work."
- Why unresolved: The study was limited to four languages, and language-specific characteristics influence vulnerability rates differently.
- What evidence would resolve it: Replication of poisoning experiments using datasets from additional programming languages with comparable CWE coverage.

### Open Question 3
- Question: What defensive mechanisms can effectively detect and filter poisoned code samples in knowledge bases without requiring access to developer queries?
- Basis in paper: The paper demonstrates attacks under both exposed and hidden intent scenarios but provides limited exploration of proactive defense mechanisms for Scenario II, where attackers poison blindly.
- Why unresolved: While hiding programming intent increases attacker difficulty, the paper does not evaluate automated detection or sanitization approaches for poisoned knowledge bases.
- What evidence would resolve it: Design and evaluation of defense techniques (e.g., vulnerability-aware filtering, anomaly detection) measuring reduction in VRRC and VR metrics.

## Limitations
- Vulnerability detection relies on LLM judges with 77-84% accuracy, introducing measurement uncertainty
- Experiments use a filtered subset of ReposVul (12,053 instances from 236 CWE types), which may not represent all real-world scenarios
- Poisoning attacks assume attackers can either predict queries or manipulate representative samples, which may not capture all attack vectors

## Confidence
- **High confidence**: Retrieval quality directly impacts vulnerability propagation (VRRC 0.42 vs 0.07 for JINA vs BM25); code-specific LLMs generate more vulnerabilities than general models (VR 0.53 vs 0.37 for CodeLlama vs Llama-3); similarity threshold effects are consistent across experiments
- **Medium confidence**: Few-shot learning increases vulnerability risk (6.5% VR increase); language-specific vulnerability patterns (C++ highest at 0.44-0.47); CWE-type vulnerability distribution (CWE-352 at ~0.79 vs CWE-434 at ~0.26)
- **Low confidence**: Exact vulnerability detection accuracy due to LLM judge limitations; generalizability to production environments; effectiveness of proposed mitigations

## Next Checks
1. **Manual validation**: Randomly sample 200 generated code examples across all LLMs and retrievers, manually verify vulnerability presence to establish ground truth for LLM judge accuracy
2. **Real-world deployment test**: Implement RACG system in a controlled development environment with actual developers, measure vulnerability rates in production-like conditions over 30 days
3. **Defense mechanism evaluation**: Test three proposed mitigations (code sanitization, retrieval filtering, security-aware decoding) against the same poisoning scenarios to quantify risk reduction potential