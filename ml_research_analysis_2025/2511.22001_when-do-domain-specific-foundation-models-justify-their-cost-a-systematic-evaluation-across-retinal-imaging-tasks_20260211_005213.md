---
ver: rpa2
title: When Do Domain-Specific Foundation Models Justify Their Cost? A Systematic
  Evaluation Across Retinal Imaging Tasks
arxiv_id: '2511.22001'
source_url: https://arxiv.org/abs/2511.22001
tags:
- pretrained
- accuracy
- performance
- pretraining
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically benchmarks backbone initialization strategies
  and architectures for retinal disease classification across four tasks spanning
  OCT and CFP imaging modalities. We evaluate 12-13 model configurations per task,
  including vision transformers, Swin Transformers, ConvNeXt, and domain-specific
  RETFound models (303M), under identical training conditions.
---

# When Do Domain-Specific Foundation Models Justify Their Cost? A Systematic Evaluation Across Retinal Imaging Tasks

## Quick Facts
- arXiv ID: 2511.22001
- Source URL: https://arxiv.org/abs/2511.22001
- Reference count: 40
- Primary result: Compact general-purpose architectures outperform large domain-specific models for most retinal classification tasks

## Executive Summary
This study systematically benchmarks backbone initialization strategies and architectures for retinal disease classification across four tasks spanning OCT and CFP imaging modalities. We evaluate 12-13 model configurations per task, including vision transformers, Swin Transformers, ConvNeXt, and domain-specific RETFound models (303M), under identical training conditions. Our results show that pretrained initialization provides universal benefits (5.18-18.41% improvement), scaling with task difficulty and imaging modality. Compact architectures (27-29M parameters) dominate Pareto frontiers, with SwinV2-tiny achieving top-1 performance on three datasets. Domain-specific RETFound models justify their computational cost only for challenging DR grading (71.15% accuracy), while ImageNet pretraining suffices for all other tasks (DME 99.24%, OCT 97.96%). CFP tasks show larger pretraining gains (9.13-18.41%) than OCT (5.18%), and hierarchical Swin Transformers demonstrate superior cross-task consistency.

## Method Summary
The study evaluates 12-13 model configurations across four retinal imaging tasks using a unified training framework. Models include vision transformers, Swin Transformers, ConvNeXt, and RETFound domain-specific foundation models. All models are trained end-to-end with identical conditions: AdamW optimizer with cosine learning rate schedule, batch size 256, 100 epochs, cross-entropy loss, and minimal augmentation. The evaluation spans OCT (8-class AMD classification), DME (3-class grading), DR (5-class grading with severe imbalance), and glaucoma detection (3-class). Performance is measured using accuracy, AUROC, F1, and Kappa, with Pareto frontier analysis comparing accuracy versus parameter count.

## Key Results
- Pretrained initialization provides universal benefits across all tasks (5.18-18.41% improvement)
- Compact architectures (27-29M parameters) dominate Pareto frontiers across three datasets
- Domain-specific RETFound models only justify their computational cost for challenging DR grading (71.15% accuracy)
- ImageNet pretraining suffices for all other tasks: DME (99.24%), OCT (97.96%), and glaucoma detection
- Hierarchical Swin Transformers show superior cross-task consistency compared to standard vision transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compact, hierarchical vision transformers (SwinV2-tiny, 27â€“29M parameters) can match or exceed the performance of massive domain-specific foundation models (RETFound, 303M parameters) for most retinal classification tasks.
- Mechanism: The paper argues that for many medical imaging tasks (specifically retinal disease classification), the visual vocabulary (pathological features like hemorrhages, exudates, layer disruptions) is constrained enough that it doesn't require the representational capacity of 300M+ parameter models. Hierarchical transformers like Swin capture both local (microaneurysms, small drusen) and global (optic disc morphology) patterns through their inductive biases (shifted windows), making them more parameter-efficient than standard ViTs that rely purely on global self-attention.
- Core assumption: The representational requirements for retinal pathology discrimination are adequately met by the learned features from general-purpose ImageNet pretraining combined with efficient architectural inductive biases, without needing domain-specific pretraining on retinal images.
- Evidence anchors:
  - [abstract] "Compact architectures (27-29M parameters) dominate Pareto frontiers, with SwinV2-tiny achieving top-1 performance on three datasets."
  - [section 4.1.2] "The dominance of smaller models likely reflects the relatively constrained visual vocabulary of retinal imaging compared to natural images... retinal disease classification focuses on specific pathological features... that may not require the representational capacity of 300M-parameter models."
  - [corpus] Weak corpus signals. Neighbor paper on ophthalmology foundation models (arXiv:2505.05291) exists but doesn't confirm this specific mechanism.

### Mechanism 2
- Claim: The benefit of pretraining scales with task difficulty and imaging modality.
- Mechanism: For easier tasks with clear class boundaries (e.g., DME classification with 3 classes, OCT with 8 balanced classes), the pretraining benefit is smaller (5.18-10.70% improvement) because the model can learn discriminative features from scratch. For challenging tasks with fine-grained ordinal grading and severe class imbalance (e.g., DR grading with 5 classes, 8.22% minority class), pretrained features become critical (18.41% improvement). Additionally, ImageNet pretraining transfers more effectively to CFP (RGB, similar to natural images) than to OCT (grayscale cross-sections).
- Core assumption: Pretrained features provide a strong initialization that reduces sample complexity. The more difficult the task (more classes, finer distinctions, less data, greater imbalance), the more valuable this initialization becomes.
- Evidence anchors:
  - [abstract] "pretraining provides universal benefits (5.18-18.41% improvement), scaling with task difficulty... CFP tasks show larger pretraining gains (9.13-18.41%) than OCT (5.18%)."
  - [section 3.5.1] "The pretraining advantage (18.41%) substantially exceeds that observed for OCT (5.18%) and DME (10.70%), indicating pretraining is especially valuable for fine-grained ordinal classification tasks with class imbalance."
  - [corpus] Very weak corpus signals. Neighbor papers don't address this specific scaling relationship.

### Mechanism 3
- Claim: Domain-specific foundation models (like RETFound) only justify their computational cost for the most challenging fine-grained discrimination tasks, not as a universal strategy.
- Mechanism: Domain-specific pretraining learns inductive biases specific to retinal imaging. These specialized features only provide a measurable advantage when the task requires discriminating extremely subtle pathological differences (e.g., distinguishing DR severity levels 0-4 where class 3 is only 8.22% of data). For all other tasks, these specialized features provide no advantage over general-purpose ImageNet features.
- Core assumption: The primary value of domain-specific pretraining is to learn domain-specific visual patterns. If general-purpose features already capture necessary representations (or if the task is easy enough), the added cost is not justified.
- Evidence anchors:
  - [abstract] "Domain-specific RETFound models justify their computational cost only for challenging DR grading (71.15% accuracy), while ImageNet pretraining suffices for all other tasks..."
  - [section 4.1.3] "They achieved top performance only on DR (71.15%, rank 1), the most challenging task... This contradicts the research assumption that domain-specific foundation models provide universal advantages for medical imaging."
  - [corpus] Neighbor paper (arXiv:2506.19552) explores a similar question for fetal ultrasound: "Should they attempt to pretrain a custom foundation model... or use transfer-learning from an existing generalist model?"

## Foundational Learning
- **Transfer Learning / Fine-tuning**
  - Why needed here: The entire paper evaluates different pretraining strategies and how they transfer to retinal tasks. Understanding weight adaptation is critical.
  - Quick check question: Why does the paper fine-tune all layers end-to-end rather than freezing the backbone?
- **Class Imbalance**
  - Why needed here: The DR grading task has severe class imbalance (Class 3: 8.22%), which is key to its difficulty and why domain-specific pretraining shows benefit. The paper uses cross-entropy loss without class weighting.
  - Quick check question: How would conclusions potentially change if a loss function robust to class imbalance (like focal loss) was used for the DR task?
- **Vision Transformer (ViT) and Swin Transformer architectures**
  - Why needed here: The paper benchmarks different architectures. Understanding the difference between standard ViT (global self-attention) and Swin Transformer (hierarchical, shifted window attention) is key to understanding why Swin models performed more consistently.
  - Quick check question: Based on the paper, why might the hierarchical nature of Swin Transformers be better suited for retinal imaging tasks than the global self-attention of a standard ViT?

## Architecture Onboarding
- **Component map**: Image -> Backbone (e.g., SwinV2-tiny, RETFound) -> Feature Vector -> Classification Head -> Class Prediction
- **Critical path**:
  1. **Data Loading & Preprocessing**: Minimal augmentation (resize, center crop, normalize using ImageNet stats)
  2. **Backbone Initialization**: Load weights (ImageNet-1k or RETFound)
  3. **Fine-tuning**: Train backbone + head end-to-end using AdamW, cosine LR schedule with warmup, cross-entropy loss. 100 epochs, no early stopping
  4. **Evaluation**: Compute Accuracy, AUROC, F1, Kappa. Rank models
  5. **Pareto Analysis**: Plot accuracy vs. parameter count to identify optimal models
- **Design tradeoffs**:
  - **Model Size vs. Performance**: Central tradeoff. Paper finds plateau around 28-30M parameters for most tasks
  - **General vs. Specialized Pretraining**: Tradeoff between computational cost of specialized pretraining vs. benefit (only justified for hardest task, DR)
  - **Standard vs. Hierarchical Transformers**: ViT-b is larger but underperforms and is less stable than smaller Swin models. Architectural inductive bias > raw parameter count
  - **Minimal vs. Heavy Augmentation**: Paper intentionally uses minimal augmentation to isolate model/pretraining effects. Tradeoff for experimental clarity vs. potentially higher absolute performance
- **Failure signatures**:
  - **Scratch-trained ViT**: Likely to diverge or converge poorly due to lack of strong inductive biases (88.04% on OCT vs 96.86% for pretrained)
  - **RETFound on Easy Tasks**: Will work, but waste 11x the parameters for no accuracy gain compared to SwinV2-tiny (e.g., DME: 99.06% vs 99.24%)
  - **Any model on DR grading**: All models struggle (max 71.15% accuracy) due to task difficulty and class imbalance, regardless of size
- **First 3 experiments**:
  1. **Reproduce Pareto Frontier for one task (e.g., OCT)**: Train small (SwinV2-tiny), medium (ViT-b), and large (RETFound) models with pretrained weights. Plot accuracy vs. parameters. Verify large model doesn't dominate frontier
  2. **Ablate pretraining benefit**: Train SwinV2-tiny from scratch and with ImageNet weights on DME dataset. Measure accuracy gap (expect ~10-20% based on paper)
  3. **Test break condition**: Apply best-performing SwinV2-tiny model to new, more complex task (e.g., multi-label disease detection or predicting continuous biomarker) to see if performance plateau holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized loss functions (e.g., ordinal regression, focal loss) significantly improve performance on fine-grained tasks like Diabetic Retinopathy (DR) grading compared to the standard cross-entropy loss used here?
- Basis in paper: [explicit] The authors state in Section 4.3 that "ordinal regression losses" and "focal loss variants for extreme imbalance" represent valuable directions for future work to improve performance on challenging tasks.
- Why unresolved: The study intentionally restricted the methodology to standard cross-entropy loss to isolate the effects of architecture and initialization, leaving the impact of loss function engineering untested.
- What evidence would resolve it: A comparative benchmark of RETFound and Swin models on the DR dataset using ordinal losses, measuring improvements in class-specific recall and Cohen's Kappa.

### Open Question 2
- Question: Do the observed trade-offs between compact general-purpose models and domain-specific foundation models hold across diverse clinical settings, imaging devices, and geographic populations?
- Basis in paper: [explicit] Section 4.3 notes that the evaluation used single institutional/competition datasets, which "limits generalizability assessment," and explicitly calls for "external validation on datasets from different imaging devices."
- Why unresolved: The models were tested on specific benchmark datasets (e.g., EAM, AIROGS) which may not capture the full heterogeneity of real-world clinical data distributions.
- What evidence would resolve it: Replicating the Pareto frontier analysis on external datasets from different geographic regions or imaging hardware to verify if compact models remain dominant.

### Open Question 3
- Question: Do domain-specific foundation models offer distinct advantages over compact models in data-scarce or few-shot learning scenarios?
- Basis in paper: [explicit] The authors list "performance under data scarcity (few-shot scenarios)" as an "important practical consideration" that was not addressed in the current analysis (Section 4.3).
- Why unresolved: The study utilized fixed training set sizes; it remains unknown if the 303M parameter RETFound requires fewer samples to reach convergence or high performance compared to smaller models.
- What evidence would resolve it: A learning curve analysis plotting validation accuracy against training set size (e.g., 1%, 10%, 100% data) for both compact and large domain-specific models.

## Limitations
- Analysis covers only four specific retinal tasks (OCT classification, DME grading, DR grading, glaucoma detection), all using image-level classification
- DR grading performance remains poor across all architectures (71.15% maximum), suggesting either fundamental task difficulty or potential optimization issues
- The paper uses minimal data augmentation to isolate model effects, potentially underestimating absolute performance achievable with stronger augmentation strategies

## Confidence
- **High confidence**: Pretrained initialization provides universal benefits across all tasks; compact architectures (SwinV2-tiny, 27-29M parameters) dominate Pareto frontiers for easy to moderate tasks
- **Medium confidence**: Domain-specific pretraining only justifies its cost for the most challenging DR task, given this conclusion rests on a single challenging example
- **Medium confidence**: Scaling relationship between pretraining benefits and task difficulty, as this requires careful control of task complexity and may not generalize beyond studied tasks

## Next Checks
1. Test the proposed Pareto-optimal architectures (SwinV2-tiny, ConvNeXtV2) on a held-out fifth retinal task (e.g., multi-label disease detection) to verify performance plateau at ~30M parameters holds
2. Implement focal loss or class-weighted cross-entropy for the DR task to assess whether performance ceiling of 71.15% reflects task difficulty versus suboptimal loss function choice
3. Evaluate the same model suite on a 3D volumetric OCT dataset to test whether 2D pretrained weights provide sufficient inductive bias for this domain