---
ver: rpa2
title: Improving Out-of-Distribution Detection with Markov Logic Networks
arxiv_id: '2506.04241'
source_url: https://arxiv.org/abs/2506.04241
tags:
- detection
- constraints
- performance
- logic
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of out-of-distribution (OOD) detection
  in deep learning models by proposing a novel framework that combines Markov Logic
  Networks (MLNs) with existing OOD detectors. The core method augments standard OOD
  detectors by incorporating probabilistic reasoning over logical constraints defined
  on human-understandable concepts, enabling both improved detection performance and
  enhanced explainability.
---

# Improving Out-of-Distribution Detection with Markov Logic Networks

## Quick Facts
- arXiv ID: 2506.04241
- Source URL: https://arxiv.org/abs/2506.04241
- Reference count: 12
- Primary result: MLN-based framework significantly improves OOD detection performance by combining probabilistic reasoning over semantic concepts with existing neural detectors.

## Executive Summary
This paper addresses the problem of out-of-distribution (OOD) detection in deep learning models by proposing a novel framework that combines Markov Logic Networks (MLNs) with existing OOD detectors. The core method augments standard OOD detectors by incorporating probabilistic reasoning over logical constraints defined on human-understandable concepts, enabling both improved detection performance and enhanced explainability. The authors introduce an algorithm to automatically learn these logical constraints from datasets and demonstrate through extensive experiments on multiple datasets (including GTSRB for traffic signs and CelebA for face attributes) that their MLN-based approach significantly improves detection performance across various detectors and deep neural network architectures.

## Method Summary
The proposed method trains separate deep neural networks to classify human-understandable concepts (e.g., color, shape) from input data, then uses Markov Logic Networks to define probabilistic constraints over these concepts. The framework learns weights for these constraints on in-distribution data and combines the resulting semantic outlier score with normalized scores from traditional neural-based OOD detectors. A greedy search algorithm with regularization automatically discovers effective logical constraints from limited validation data, avoiding the exponential complexity of exhaustive search.

## Key Results
- MLN-based methods achieve substantial improvements in AUROC and FPR95 metrics across multiple datasets
- The supervised variant with additional PID predicate achieves near-perfect performance (99.88% AUROC) on GTSRB
- Combining MLN semantic scores with neural representation scores provides complementary detection signals
- Automatic constraint discovery works effectively but is sensitive to the choice of validation OOD distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic evaluation of logical constraints over semantic concepts detects OOD inputs that violate expected conceptual relationships.
- Mechanism: DNNs map inputs to a disentangled semantic space where each dimension represents a human-understandable concept (e.g., color, shape). Logical constraints defined over these concepts are evaluated with learned weights via MLN. The outlier score DM(x) = −Σᵢwᵢφᵢ(x) increases when constraints are violated, with weights reflecting constraint importance learned from ID data.
- Core assumption: OOD inputs frequently violate semantic relationships that consistently hold for ID data, and these violations are detectable when concepts are accurately classified.

### Mechanism 2
- Claim: Normalizing and multiplying scores from neural representation-based detectors with MLN semantic scores yields complementary OOD signals.
- Mechanism: Traditional detectors produce scores on arbitrary scales. A survival function pD(D(x)) = P(D(X) ≥ D(x)) normalizes these to [0,1] by fitting a distribution (GED works well empirically) to ID scores. The combined score D′M(x) = DM(x) × pD(D(x)) multiplies semantic implausibility with representation rarity.
- Core assumption: Semantic constraint violations and unusual neural activations provide independent signals—inputs unusual in both dimensions are more likely OOD.

### Mechanism 3
- Claim: A greedy search with regularization can automatically discover effective logical constraints from limited ID/OOD validation data.
- Mechanism: Constraints are binary trees with predicates/functions at leaves and logical connectives at branches. For each candidate from the pool T, the algorithm evaluates AUROC improvement on validation data. A constraint is added only if improvement exceeds δmin, which acts as a complexity penalty. This avoids exhaustive 2^|T| search.
- Core assumption: Locally optimal constraint additions approximate globally useful rule sets, and validation OOD data is representative of test OOD.

## Foundational Learning

- Concept: **Markov Logic Networks**
  - Why needed here: Core reasoning engine—must understand how weighted formulas define probability distributions over possible worlds, and why this softens strict logic.
  - Quick check question: Given formula ∀x A(x)→B(x) with weight w=3, what happens to P(world) when A(x) is true but B(x) is false for some x?

- Concept: **OOD Detection Metrics (AUROC, FPR95)**
  - Why needed here: Results are reported in these metrics; understanding what they measure is essential for interpreting improvements.
  - Quick check question: If a detector has FPR95=10%, what fraction of ID samples are incorrectly flagged when the threshold is set to catch 95% of OOD?

- Concept: **Survival Functions and Distribution Fitting**
  - Why needed here: Score normalization step requires fitting distributions to ID outlier scores; poor fits break the combination mechanism.
  - Quick check question: Why is the survival function P(D(X) ≥ D(x)) preferred over raw scores for combining detectors with different scales?

## Architecture Onboarding

- Component map:
  - **Concept Classifiers**: Separate DNNs (or shared backbone with heads) mapping raw input → semantic concepts (e.g., f_COLOR, f_SHAPE)
  - **Constraint Compiler**: Translates human-readable formulas → batch-executable PyTorch operations
  - **MLN Scorer**: Evaluates weighted constraint satisfaction; outputs DM(x)
  - **Score Normalizer**: Fits GED to base detector scores on ID data; computes survival function
  - **Combiner**: Multiplies normalized base score × MLN score

- Critical path:
  1. Train concept classifiers on labeled attributes (requires attribute annotations or a priori knowledge)
  2. Define constraint set (manually from domain knowledge OR via greedy search with ID+OOD validation data)
  3. Learn MLN weights via L-BFGS minimizing negative log-likelihood on ID data (Eq. 11)
  4. Fit GED to base detector scores on ID data for normalization
  5. At inference: extract concepts → evaluate constraints → compute combined score

- Design tradeoffs:
  - **Separate vs. shared backbone**: Sharing reduces memory/compute but Table 4 shows ~33% AUROC drop on GTSRB due to correlated errors
  - **Strict logic vs. probabilistic MLN**: MLN handles soft constraints better (CelebA: MLN 84.13% vs. Logic 51.06% AUROC)
  - **More constraints vs. regularization**: Figure 3 shows monotonic improvement with more rules; Figure 4 shows overfitting risk without δmin

- Failure signatures:
  - **Concept classifier errors cascade**: If color classifier fails, all color-related constraints become unreliable
  - **OOD distribution mismatch in constraint search**: Constraints learned on synthetic OOD (e.g., Gaussian noise) generalize poorly (Figure 6)
  - **Poor normalization distribution fit**: Using Normal instead of GED drops AUROC (Table 5: 99.07% vs. 99.88%)

- First 3 experiments:
  1. **Reproduce standalone MLN on GTSRB**: Train 3 separate classifiers (class, color, shape), encode 43 manual constraints from traffic sign knowledge, learn weights. Verify AUROC ≈86% as baseline.
  2. **Ablate constraint count**: Start with 1 constraint (stop sign rule), incrementally add. Reproduce Figure 3 curve showing performance vs. rule count.
  3. **Test distribution choice for normalization**: Compare GED vs. Normal vs. No normalization when combining MLN with a base detector (e.g., Ensemble). Reproduce Table 5 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can global optimization strategies outperform the proposed greedy search algorithm in discovering optimal constraint sets?
- Basis in paper: [explicit] Section 4 states that the greedy strategy was adopted for computational efficiency but "does not guarantee a globally optimal solution."
- Why unresolved: The exponential search space ($2^{|T|}$) makes exact solutions infeasible, and the authors did not compare the greedy approach against other non-greedy approximation methods.

### Open Question 2
- Question: How can the constraint learning framework be modified to prevent the encoding of discriminatory biases present in the training data?
- Basis in paper: [explicit] The Impact Statement notes that the algorithm risks embedding "discriminatory biases against underrepresented minority classes" (e.g., gender/attribution correlations) into the logical rules.
- Why unresolved: The current learning algorithm (Alg 2) optimizes based on AUROC and complexity, lacking an explicit mechanism to filter out socially sensitive or spurious correlations.

### Open Question 3
- Question: How robust is the MLN-based detection score to errors in the underlying concept detectors (predicate interpretations)?
- Basis in paper: [inferred] The framework relies on DNNs to provide the "interpretations" for predicates (Eq. 5), and Section 5.4 implies correlated errors in parameter sharing degrade performance, yet the sensitivity to isolated concept misclassification is not quantified.
- Why unresolved: While the probabilistic nature of MLNs handles uncertainty in constraint satisfaction, it is unclear how dependent the final OOD score is on the accuracy of the individual concept classifiers.

## Limitations
- Constraint discovery algorithm performance is highly sensitive to the choice of validation OOD distribution
- Manual constraint encoding requires domain expertise and doesn't scale well to complex domains
- Framework requires labeled concept attributes or a priori knowledge for concept classifiers
- Limited exploration of the candidate constraint space (only depth-2 implications)

## Confidence

**High Confidence**: The mechanism of combining semantic constraint violations with neural representation scores (Mechanism 2) is well-supported by consistent empirical improvements across datasets and detectors.

**Medium Confidence**: The automatic constraint discovery algorithm (Mechanism 3) shows promise but has limited validation and is sensitive to validation OOD distribution choice.

**Medium Confidence**: The core claim that probabilistic evaluation of logical constraints detects OOD inputs (Mechanism 1) is supported by strong results but relies heavily on accurate concept classifiers.

## Next Checks

1. **Distribution Shift Robustness**: Systematically evaluate the constraint discovery algorithm using different OOD validation distributions (Gaussian noise, uniform noise, semantic shifts) to quantify performance variance and identify which types of OOD are most/least well-handled.

2. **Concept Classifier Accuracy Impact**: Conduct controlled experiments varying concept classifier accuracy (e.g., through added noise, reduced training data, or architectural changes) to establish the relationship between concept accuracy and OOD detection performance.

3. **Constraint Pool Completeness**: Expand the constraint search space beyond depth-2 implications to include conjunctions, disjunctions, and recursive rules, then compare the resulting performance to assess whether the current constraint language is sufficiently expressive.