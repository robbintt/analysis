---
ver: rpa2
title: Neural Network Training via Stochastic Alternating Minimization with Trainable
  Step Sizes
arxiv_id: '2508.04193'
source_url: https://arxiv.org/abs/2508.04193
tags:
- accuracy
- step
- loss
- updated
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of nonconvex optimization in
  deep neural network training, where standard stochastic gradient descent (SGD) methods
  often face unstable convergence and high computational cost. To tackle these issues,
  the authors propose Stochastic Alternating Minimization with Trainable Step Sizes
  (SAMT), which updates network parameters in an alternating manner by treating the
  weights of each layer as a block.
---

# Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes

## Quick Facts
- **arXiv ID:** 2508.04193
- **Source URL:** https://arxiv.org/abs/2508.04193
- **Reference count:** 40
- **Primary result:** SAMT achieves 98.6% test accuracy on MNIST for MLP with 400 hidden units, outperforming baselines like Adam and SGD.

## Executive Summary
This paper addresses nonconvex optimization challenges in deep neural network training by proposing Stochastic Alternating Minimization with Trainable Step Sizes (SAMT). The method updates network parameters layer-by-layer using block-wise alternating updates while integrating a meta-learned step size mechanism. Extensive experiments demonstrate that SAMT achieves better generalization performance with fewer parameter updates compared to state-of-the-art optimizers like Adam and SGD across multiple benchmark datasets.

## Method Summary
SAMT implements stochastic alternating minimization where network weights are updated block-by-block (layer-by-layer), treating other layers as constants during each update. The method incorporates trainable step sizes through an auxiliary "eta model" - a small MLP that predicts optimal step sizes based on gradient statistics. The effective step size is a convex combination of a learned component and initialization. The eta model is trained online via backpropagation through temporary weight updates, creating a meta-learning framework where step sizes adapt dynamically during training.

## Key Results
- Achieves 98.6% test accuracy on MNIST for MLP with 400 hidden units
- Outperforms Adam and SGD baselines on CIFAR-10 and CIFAR-100 classification tasks
- Demonstrates superior performance in regression tasks and shows robustness across different neural network architectures
- Provides theoretical convergence guarantees under mild assumptions

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Block-wise Decomposition
Decomposing multi-layer networks into independent block updates reduces computational coupling compared to simultaneous updates. Each layer's weights are treated as a distinct block, minimizing sub-problems while holding other layers fixed. This isolates non-convexity into smaller problems. Performance may degrade if layers are highly interdependent and alternating updates oscillate.

### Mechanism 2: Meta-Learned Step Size Adaptation
An auxiliary "eta model" dynamically outputs optimal step sizes by observing gradient statistics. The eta model (small MLP) ingests five first-order gradient features (mean, variance, max, min, norm) and outputs parameters to construct the step size. This is trained online via backpropagation through the computation graph. The relationship between gradient statistics and optimal update magnitude is assumed to be learnable.

### Mechanism 3: Stabilizing Convex Combination
The effective step size combines learned and initialization components via a learned gating factor β. The convex combination β⊙η₀ + (1-β)⊙η̂ anchors the current step size to a safe initialization while allowing the model to shift weight to the learned component when confident. If β consistently saturates at 0 or 1, the adaptivity is lost.

## Foundational Learning

- **Concept: Block Coordinate Descent (BCD) / Alternating Minimization**
  - **Why needed:** SAMT relies on updating one layer at a time while holding others constant, differing from standard backpropagation.
  - **Quick check:** Can you explain why updating layers sequentially might oscillate less than simultaneous SGD in a steep non-convex valley?

- **Concept: Meta-Gradient / Bi-Level Optimization**
  - **Why needed:** The trainable step size is learned via a second-order effect (loss of the updated weights).
  - **Quick check:** How does the gradient flow through the step size update rule η̄ₜ₊₁ back to the Eta model weights?

- **Concept: Gradient Feature Engineering**
  - **Why needed:** The Eta model uses 5 statistical summary features instead of raw gradients to avoid noise and high dimensionality.
  - **Quick check:** Why might the variance of a gradient tensor be more informative for setting a step size than its mean?

## Architecture Onboarding

- **Component map:** Main Model (φ) -> Eta Model (ψ) -> Sampler -> Projector
- **Critical path:**
  1. Forward Pass: Compute loss with current weights Wₜ
  2. Feature Extraction: Compute gradient gₜ and extract 5 statistics
  3. Meta-Inference: Pass stats to Eta model ψ to get βₜ, η̂ₜ
  4. Step Composition: Calculate combined step size η̄ₜ₊₁
  5. Inner Update: Calculate temporary weights W' = Wₜ - η̄ₜ₊₁ ⊙ gₜ
  6. Meta-Update: Forward pass with W' to calculate new loss; update Eta model ψ
  7. Commit update: Wₜ₊₁ = W'

- **Design tradeoffs:**
  - **Scalar vs. Element-wise Step Size:** Scalar (SAMT-S) is faster and uses less memory but ignores spatial variations. Element-wise (SAMT-E) offers fine-grained control but increases memory usage.
  - **Projection Function:** Tanh-based projection offers wider dynamic range than Sigmoid, potentially avoiding saturation but risking instability if unbounded.

- **Failure signatures:**
  - **Gradient Oscillation:** If the learned term dominates too quickly without the initialization anchor, validation loss spikes.
  - **Training Slowdown:** Meta-update requires maintaining computation graph for temporary weights W', creating significant memory overhead on very deep networks.

- **First 3 experiments:**
  1. **Sanity Check (MLP on MNIST):** Implement SAMT-S on small MLP. Verify Eta model learns to adjust β (starting with small learning rate and increasing it).
  2. **Ablation on β:** Run "right-E" only version vs. full convex combination. Confirm full combination is more stable on noisy dataset like KMNIST.
  3. **Architecture Scalability:** Compare SAMT-E vs. Adam on CNN (CIFAR-10). Monitor accuracy and wall-clock time per epoch to quantify overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can formal convergence guarantees be established for SAMT using non-scalar trainable step sizes or under stronger theoretical assumptions than the mild conditions currently provided?
- **Basis:** Section 4.1 restricts convergence analysis to scalar step sizes; Section 7 states formal proof under stronger assumptions remains open.
- **Why unresolved:** Current analysis relies on Assumptions 1-4 for scalar step sizes; extending to element-wise step sizes requires more complex analysis.
- **What evidence would resolve it:** Theoretical proof demonstrating convergence for element-wise step sizes or under more restrictive conditions than current "mild" assumptions.

### Open Question 2
- **Question:** How can trainable step size implementation be optimized to maintain efficiency when training very deep neural networks where computation graph overhead is significant?
- **Basis:** Section 7 notes requirement to maintain computation graph may significantly slow training on deep networks and lists optimizing implementation as future work.
- **Why unresolved:** Current meta-learning approach requires keeping computation graph available, becoming computationally expensive as network depth increases.
- **What evidence would resolve it:** Updated implementation or algorithmic modification reducing memory/computational overhead, demonstrated by successfully training ResNet/VGG with comparable times to Adam.

### Open Question 3
- **Question:** What are empirical and theoretical impacts on convergence and accuracy when grouping multiple layers into single blocks for alternating updates?
- **Basis:** Section 7 states algorithm can be extended to group layers into blocks and provides mathematical formulation as future work.
- **Why unresolved:** Current paper treats each layer as individual block; behavior with grouped blocks has not been investigated.
- **What evidence would resolve it:** Comparative study analyzing training dynamics and test accuracy for grouped vs. single-layer blocks, identifying optimal block size for different architectures.

## Limitations
- Theoretical generalization bounds are not established for practical scenarios; current guarantees apply to alternating minimization framework itself, not specifically to meta-learned step size mechanism.
- Computational overhead quantification is incomplete; paper mentions reduced per-step overhead but lacks detailed wall-clock time comparisons or quantification of eta model's meta-update costs.
- Theoretical convergence assumptions (bounded gradients, Lipschitz continuity) are often violated in practice, and sensitivity to these violations is not discussed.

## Confidence
- **High confidence:** Core alternating minimization mechanism and its ability to reduce per-step computational coupling, directly supported by algorithm formulation and experimental results.
- **Medium confidence:** Meta-learned step size adaptation mechanism, though framework is clearly specified, effectiveness of specific gradient feature engineering is not thoroughly validated.
- **Low confidence:** Theoretical convergence guarantees directly apply to practical deep learning scenarios, as assumptions are often violated and method sensitivity is not discussed.

## Next Checks
1. **Ablation on Gradient Features:** Systematically test which of the 5 gradient features (mean, variance, max, min, norm) are most critical by removing each feature individually and measuring impact on training stability and final accuracy.
2. **Memory and Compute Profiling:** For ResNet-18, profile exact memory overhead and wall-clock time per epoch of SAMT compared to Adam, isolating cost of eta model's meta-gradient computation.
3. **Transferability of Eta Model:** Train eta model on CIFAR-10 and evaluate performance when transferred to CIFAR-100 with same architecture to test generalizability across data distributions.