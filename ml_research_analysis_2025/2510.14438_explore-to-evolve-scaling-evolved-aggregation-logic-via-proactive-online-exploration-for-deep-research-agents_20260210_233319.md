---
ver: rpa2
title: 'Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online
  Exploration for Deep Research Agents'
arxiv_id: '2510.14438'
source_url: https://arxiv.org/abs/2510.14438
tags:
- aggregation
- information
- data
- agent
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Explore to Evolve paradigm for training
  deep research web agents that excel at information aggregation, not just retrieval.
  The authors automate task construction by having an agent proactively explore the
  live web to gather diverse evidence, then self-generate complex multi-hop questions
  grounded in that evidence using a taxonomy of 12 high-level aggregation operations.
---

# Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents

## Quick Facts
- **arXiv ID:** 2510.14438
- **Source URL:** https://arxiv.org/abs/2510.14438
- **Reference count:** 40
- **Primary result:** Introduces an automated pipeline to generate complex multi-hop QA tasks requiring deep web aggregation, producing WebAggregatorQA dataset and models that approach state-of-the-art on GAIA-text.

## Executive Summary
This paper introduces the "Explore to Evolve" paradigm for training deep research web agents that excel at information aggregation, not just retrieval. The authors automate task construction by having an agent proactively explore the live web to gather diverse evidence, then self-generate complex multi-hop questions grounded in that evidence using a taxonomy of 12 high-level aggregation operations. This yields WebAggregatorQA, a 10K-sample dataset spanning 50K websites and 11 domains. Models trained on this data—WebAggregator—show strong performance, with the 32B variant exceeding GPT-4.1 by over 10% on GAIA-text and approaching Claude-3.7-sonnet, while the 8B variant matches GPT-4.1. A human-annotated test set proves highly challenging: even top models score under 30%, and success still requires strong aggregation even when all references are retrieved, underscoring the dataset's value in driving research-agent capabilities beyond mere retrieval.

## Method Summary
The method involves a two-stage pipeline: (1) proactive online exploration where an agent navigates the live web from seed URLs using tools like Search, Visit, and FileRead to collect diverse evidence, and (2) logic synthesis where an LLM uses a taxonomy of 12 high-level aggregation operations to generate verifiable multi-hop questions from the collected evidence. The resulting QA pairs are used to train smaller models via supervised fine-tuning on successful agent trajectories filtered by a strong teacher model (GPT-4.1).

## Key Results
- WebAggregator-32B exceeds GPT-4.1 by >10% on GAIA-text and approaches Claude-3.7-sonnet performance
- WebAggregator-8B matches GPT-4.1 performance on GAIA-text
- Human-annotated test set shows even top models score <30%, with success requiring strong aggregation even when all references are retrieved
- Dataset spans 50K websites across 11 domains with 10K samples

## Why This Works (Mechanism)

### Mechanism 1: Proactive Web Exploration
- **Claim:** Proactive exploration of the live web enables discovery of diverse, grounded evidence that static corpora cannot provide.
- **Core assumption:** The live web contains stable, verifiable information across diverse domains that is sufficiently rich to support complex multi-hop reasoning.
- **Evidence anchors:** [abstract] "Begins with proactive online exploration, an agent sources grounded information by exploring the real web." [Page 4] "The agent is prompted to start from a single anchor URL and employ various tools... to discover unknown but relevant information."

### Mechanism 2: Taxonomy-Guided Logic Synthesis
- **Claim:** A taxonomy of high-level logical operations guides an LLM to "self-evolve" concrete, verifiable multi-hop questions from raw evidence.
- **Core assumption:** Strong LLMs (like GPT-4.1 used in construction) can reliably compose and verify complex logic chains when constrained by a high-level schema and specific evidence.
- **Evidence anchors:** [abstract] "Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types..." [Page 5, Table 2] Lists representative examples of operations (e.g., `Correlate`, `Filter`, `Statistic`) and their corresponding questions.

### Mechanism 3: Supervised Fine-Tuning on Filtered Trajectories
- **Claim:** Supervised fine-tuning (SFT) on filtered, high-quality agent trajectories transfers "aggregation competence" to smaller foundation models.
- **Core assumption:** The reasoning patterns in the synthetic tasks are sufficiently representative of real-world research challenges (like GAIA) to enable generalization.
- **Evidence anchors:** [Page 6] "We utilize the agent based on GPT-4.1... To ensure quality... conduct a further filtering procedure... [retaining] 6,184 trajectories." [Page 9, Table 3] Shows WebAggregator-32B surpassing GPT-4.1 by >10% on GAIA-text and approaching Claude-3.7-sonnet.

## Foundational Learning

### Concept: Information Aggregation vs. Retrieval
- **Why needed here:** The core contribution is shifting focus from finding facts (retrieval) to synthesizing new insights (aggregation).
- **Quick check question:** Can you distinguish between a question asking "What is the GDP of France?" (Retrieval) and "What is the correlation between France's GDP and urbanization rate?" (Aggregation)?

### Concept: Agentic Data Synthesis
- **Why needed here:** Understanding how the WebAggregatorQA dataset is built is critical for evaluating its quality and limitations.
- **Quick check question:** How does using a taxonomy of operations (e.g., "Set Filter", "Temporal Change") guide the synthesis process compared to open-ended generation?

### Concept: Rejection Sampling for SFT
- **Why needed here:** The model's performance relies heavily on the quality of the training trajectories, which are curated via this method.
- **Quick check question:** Why is it important to filter out trajectories that reach the correct answer but use invalid tool calls or reasoning steps?

## Architecture Onboarding

### Component map:
Input (Anchor URLs) -> Explore Agent (GPT-4.1 + SmolAgents + Tools) -> Evidence Store (Parsed text, DOM, files, images) -> Evolve Agent (LLM + 12-operation taxonomy + Evidence Store) -> QA Pair -> Quality Control -> Trajectory Sampler (Teacher agent attempts to solve QA) -> Filtered Trajectories -> Training (SFT on Qwen-2.5/3)

### Critical path:
The quality of the "Evolve" step (logic synthesis) and the effectiveness of the "Rejection Sampling" (trajectory filtering). If the generated questions are ambiguous or the trajectories are noisy, the final model degrades.

### Design tradeoffs:
- **Automation vs. Quality:** Fully automated synthesis allows scaling to 10K samples but requires complex automated checking to match human annotation quality.
- **Complexity vs. Solvability:** The taxonomy encourages complex reasoning, but the system must ensure the required information is actually present in the evidence store.

### Failure signatures:
- **Unsolvable Questions:** Generated questions rely on information that wasn't successfully captured during exploration.
- **Ambiguity:** Questions admit multiple valid answers due to vague constraints.
- **Data Contamination:** Agents retrieving existing benchmark data during exploration.

### First 3 experiments:
1. **Explore Phase Validation:** Run the exploration agent on a small set of anchor URLs and manually inspect the diversity of tools used and the content retrieved.
2. **Logic Synthesis Check:** Generate 50 QA pairs and manually verify if the "high-level operations" were correctly translated into executable reasoning steps grounded in the evidence.
3. **Scaling Law Test:** Train a small model (e.g., Qwen-7B) on a subset (500-1k samples) of the collected trajectories to confirm the SFT pipeline is functional and yields performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the performance bottleneck of small foundation models (<8B parameters) on complex aggregation tasks be overcome?
- **Basis in paper:** [explicit] Page 9 states that while smaller WebAggregator models (7B/8B) excel on standard benchmarks, they struggle with the harder tasks in WebAggregatorQA, and "breaking through the performance bottleneck of small foundation models on hard tasks remains a vital direction."
- **Why unresolved:** The paper demonstrates that simply scaling data (WebAggregatorQA) improves performance, but a significant capability gap remains between 8B and 32B models specifically on high-complexity aggregation tasks.
- **What evidence would resolve it:** Demonstration of a small model (e.g., 7B-8B) utilizing specific architectural modifications or distillation techniques that match the 32B model's performance on Level 3 WebAggregatorQA tasks.

### Open Question 2
- **Question:** What specific mechanisms cause agents to fail at information aggregation even after successfully retrieving all necessary reference URLs?
- **Basis in paper:** [inferred] Page 10, Table 5 shows that even when models like Claude or WebAggregator-32B visit all ground-truth URLs, accuracy only reaches ~33-42%.
- **Why unresolved:** The paper identifies this failure mode—agents retrieving evidence but failing to synthesize it—but does not isolate the root cause.
- **What evidence would resolve it:** An error analysis of trajectories with perfect retrieval but incorrect answers, categorizing failures by aggregation operation type.

### Open Question 3
- **Question:** Can the "Explore to Evolve" paradigm maintain its efficacy if applied to domains requiring specialized tools beyond general web browsing?
- **Basis in paper:** [inferred] Page 4 defines the agent's action space around web tools and high-level logical operations.
- **Why unresolved:** The methodology is validated on web-based research tasks, but it is unclear if the 12 aggregation logics scale to domains where exploration requires executing code or interacting with non-DOM environments.
- **What evidence would resolve it:** Results from applying the Explore to Evolve pipeline to a non-web benchmark using a corresponding toolset.

## Limitations

- **Exploration Reliability:** The quality of generated tasks depends on the exploration agent's ability to discover relevant, high-quality evidence across diverse domains.
- **Generalization Gap:** While the 32B variant shows strong performance on GAIA-text, it remains unclear whether synthetic training data fully captures the complexity of real-world research tasks.
- **Human Benchmark Scarcity:** The human-annotated test set, while challenging, is small (200 samples), limiting robust validation.

## Confidence

- **High Confidence:** The automated data synthesis pipeline is functional and produces measurable performance gains on GAIA-text.
- **Medium Confidence:** The claim that WebAggregator-32B approaches Claude-3.7-sonnet is supported by GAIA-text results, but direct comparison on other benchmarks would strengthen this claim.
- **Low Confidence:** The long-term impact of this paradigm on real-world research agent deployment remains uncertain, as the study focuses on controlled benchmarks.

## Next Checks

1. **Cross-Domain Transfer:** Evaluate WebAggregator models on out-of-distribution research tasks from domains not represented in the training data to test generalization.
2. **Human-in-the-Loop Validation:** Conduct a larger-scale human study where annotators attempt to solve generated tasks with and without access to the full reference set, measuring the gap between retrieval and aggregation performance.
3. **Error Analysis on Failure Cases:** Systematically analyze the 70%+ failure rate on the human-annotated test set to identify whether failures stem from missing information, ambiguous questions, or limitations in the model's reasoning capabilities.