---
ver: rpa2
title: 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning'
arxiv_id: '1810.06284'
source_url: https://arxiv.org/abs/1810.06284
tags:
- goal
- learning
- module
- goals
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CURIOUS, an algorithm for modular multi-goal
  reinforcement learning that combines a modular Universal Value Function Approximator
  (M-UVFA) with an intrinsic motivation mechanism based on absolute learning progress.
  The method enables agents to autonomously set and learn diverse goals across multiple
  modules while actively selecting which goals to practice based on learning progress
  signals.
---

# CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning

## Quick Facts
- **arXiv ID**: 1810.06284
- **Source URL**: https://arxiv.org/abs/1810.06284
- **Reference count**: 37
- **Primary result**: CURIOUS achieves faster learning on achievable goals, better resilience to distracting goals, and improved recovery from forgetting and sensory perturbations compared to flat multi-goal approaches and module-expert baselines.

## Executive Summary
This paper introduces CURIOUS, an algorithm for modular multi-goal reinforcement learning that combines a modular Universal Value Function Approximator (M-UVFA) with an intrinsic motivation mechanism based on absolute learning progress. The method enables agents to autonomously set and learn diverse goals across multiple modules while actively selecting which goals to practice based on learning progress signals. Experiments in a robotic manipulation environment demonstrate that CURIOUS outperforms flat multi-goal approaches and module-expert baselines, showing faster learning on achievable goals, better resilience to distracting goals, and improved recovery from forgetting and sensory perturbations. The approach scales to multiple modules and shows robustness to environmental changes and body modifications.

## Method Summary
CURIOUS combines modular goal encoding with intrinsic motivation via absolute learning progress to enable autonomous curriculum learning in multi-goal reinforcement learning. The method uses a modular UVFA architecture where goals are represented as [goal vector, module descriptor], with the goal vector masked to only include parameters relevant to the active module. A multi-armed bandit selects which module to practice based on absolute learning progress (|LP|) computed from self-evaluation rollouts, creating an automatic curriculum that focuses on achievable goals first and revisits forgotten ones. The approach builds on HER for goal substitution and uses prioritized interest replay to focus updates on "eventful" transitions where outcomes change.

## Key Results
- CURIOUS enables developmental learning phases where agents self-organize curricula, focusing first on simpler modules (Reach → Push → Pick&Place → Stack) before progressing to complex ones
- The method shows faster recovery from sensory perturbations, achieving 95% performance in 43k episodes vs. 78k for M-UVFA (45% faster, p<10^-4)
- CURIOUS maintains better performance on achievable goals while effectively ignoring distracting/impossible goals compared to flat multi-goal approaches

## Why This Works (Mechanism)

### Mechanism 1: Absolute Learning Progress-Based Curriculum
If agents bias goal selection toward modules showing high absolute learning progress, they achieve faster mastery of learnable goals while ignoring distracting/impossible ones. A multi-armed bandit estimates per-module competence via self-evaluation rollouts, computes learning progress as the temporal derivative of competence over a sliding window, and samples modules proportionally to |LP|. This creates automatic curricula: easy modules first (high LP), then harder ones, while maintaining exploration via ε-greedy mixing. The same LP probabilities guide cross-module replay, focusing gradient updates on modules with active learning potential.

### Mechanism 2: Modular Goal Encoding with Cross-Module Substitution
If goals are encoded modularly rather than flatly, a single UVFA policy can transfer knowledge across qualitatively different goal types. Goals are represented as [g, md] where g is zero-masked except for the active module's parameters, and md is a one-hot module descriptor. During training, HER-style goal substitution replaces the original goal with achieved outcomes, while module substitution allows transitions from one module to train another. This enables counterfactual learning across different goal types.

### Mechanism 3: Forgetting Detection via Absolute Learning Progress
If agents monitor absolute LP (including negative values indicating forgetting), they can reallocate training resources to degraded modules faster than random replay. Using |LPMi| in selection probability means modules with declining competence receive increased replay probability, triggering focused retraining. This counters catastrophic forgetting in monolithic policies without requiring explicit memory mechanisms or separate policy heads.

## Foundational Learning

- **Universal Value Function Approximators (UVFA)**
  - Why needed: CURIOUS extends UVFA to modular goals; you must understand goal-conditioned Q-functions and how goal concatenation enables generalization across goal space
  - Quick check: How does UVFA differ from learning separate value functions per goal, and what enables it to generalize to unseen goals?

- **Hindsight Experience Replay (HER)**
  - Why needed: Goal substitution via achieved outcomes is core to CURIOUS's cross-goal learning; HER provides the foundation for sample-efficient sparse-reward learning
  - Quick check: Given a failed trajectory attempting goal A but reaching state B, how does HER repurpose this experience during training?

- **Multi-Armed Bandits**
  - Why needed: Module selection is formulated as a non-stationary MAB where arm values = |LP|; you need to understand exploration-exploitation tradeoffs in curriculum learning
  - Quick check: Why might ε-greedy be preferred over UCB for non-stationary bandit problems where arm values change over time?

## Architecture Onboarding

- **Component map**: Actor network [state, goal, module_descriptor] -> action; Critic network [state, goal, module_descriptor, action] -> Q-value; Competence estimator (moving average over l=300 episodes); LP calculator (temporal derivative); Module selector (ε-greedy proportional to |LP|); Replay buffers (N+1 total: one per module + catch-all).

- **Critical path**: 1) Self-evaluation rollouts (10% of episodes, no exploration noise) populate competence queues with binary outcomes; 2) LP computation updates module selection probabilities pLP after each self-evaluation; 3) Module/goal selection uses pLP for environment interaction; goal sampled uniformly from selected module's goal space; 4) Transitions stored in module-specific buffers based on whether relevant outcome changed; 5) Training samples minibatch from buffers via pLP weights, applies module substitution and goal substitution (p=0.8); 6) Internal reward computed from substituted (md*, g*), then DDPG update.

- **Design tradeoffs**: Monolithic policy better cross-module transfer but more forgetting vs. module-experts (MG-ME baseline learns ~2x slower); evaluation frequency (p_eval=0.1) higher = better LP estimates but fewer learning episodes; window length (l=300) longer = smoother LP but slower reaction to changes; ε=0.4 higher exploration maintains competence checking on "solved" modules but slows focus.

- **Failure signatures**: Success rate plateaus early on achievable modules - check if LP is detecting the plateau; random module selection behavior - LP estimates too noisy, increase p_eval or window size l; slow learning on complex modules - ε may be too high diluting focus or module may be impossible; catastrophic forgetting - ensure LP uses absolute value |LP| in p_LP and ε>0 allows revisiting solved modules.

- **First 3 experiments**: 1) Architecture validation: Run M-UVFA vs flat HER vs MG-ME on 4 achievable modules; verify M-UVFA converges ~250k episodes while HER stays at 0; 2) Curriculum visualization: Run CURIOUS, plot per-module competence, LP, and selection probabilities over time; verify developmental ordering emerges; 3) Perturbation robustness test: Train for 250 epochs, introduce sensory shift on one module, measure recovery time for CURIOUS vs M-UVFA; should see approximately 45% faster recovery.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can CURIOUS be adapted to autonomously learn modular goal representations rather than relying on hand-defined goal spaces? The authors suggest combining CURIOUS with unsupervised learning of disentangled goal spaces (e.g., using β-VAEs) to address this limitation.

- **Open Question 2**: Can the high-level Multi-Armed Bandit (MAB) module selection be effectively replaced by a hierarchical CURIOUS agent? The "Further Work" section proposes replacing the MAB policy with another CURIOUS agent that targets self-generated higher-level goals to create a hierarchical structure.

- **Open Question 3**: Does the absolute learning progress mechanism sufficiently mitigate catastrophic forgetting as the number of potential modules scales significantly? The authors discuss that while LP mechanism mitigates forgetting, the monolithic policy might still struggle with interference as module count increases.

## Limitations

- Competence estimation from sparse binary signals via moving averages is assumed reliable without measuring variance or bias across modules of differing difficulty
- The fixed evaluation frequency (p_eval=0.1) and window length (l=300) are treated as sufficient without sensitivity analysis
- Cross-module transfer benefits are asserted but not quantified - it's unclear whether modular UVFA outperforms learning module-specific policies despite the forgetting cost

## Confidence

- **High confidence**: Modular UVFA architecture with masked goal vectors and module descriptors enables learning goals that flat HER cannot satisfy (Figure 5 evidence)
- **Medium confidence**: LP-based module selection creates developmental curricula and enables recovery from forgetting (Figure 4, Figure 6 evidence, but mechanisms not deeply validated)
- **Low confidence**: LP is the optimal intrinsic motivation for curriculum learning; cross-module transfer in modular UVFA is net beneficial (these require comparative studies not presented)

## Next Checks

1. **Sensitivity analysis**: Vary p_eval (0.05, 0.1, 0.2) and l (100, 300, 500) to quantify LP estimation reliability and curriculum emergence across different evaluation frequencies and temporal windows.

2. **Transfer quantification**: Run experiments with module-specific policies (separate UVFA per module) vs. CURIOUS to measure forgetting cost and cross-module transfer benefits empirically.

3. **LP vs alternatives**: Compare CURIOUS to UVFA+random module selection and UVFA+competence-only selection (no derivative) to isolate whether absolute LP is superior to other intrinsic motivation signals for curriculum learning.