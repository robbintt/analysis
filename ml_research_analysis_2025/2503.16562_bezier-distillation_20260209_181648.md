---
ver: rpa2
title: Bezier Distillation
arxiv_id: '2503.16562'
source_url: https://arxiv.org/abs/2503.16562
tags:
- flow
- distribution
- bezier
- distillation
- rectified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bezier Distillation, a novel approach to
  knowledge distillation for flow-based generative models that addresses error accumulation
  in traditional Rectified Flow methods. The key idea is to incorporate Bezier curves
  as guiding distributions between the initial and target distributions, using one
  or more intermediate guiding distributions connected through Bezier interpolation.
---

# Bezier Distillation

## Quick Facts
- arXiv ID: 2503.16562
- Source URL: https://arxiv.org/abs/2503.16562
- Reference count: 8
- Primary result: Introduces Bezier Distillation using Bezier curves as guiding distributions to reduce error accumulation and improve single-step generative quality over standard Rectified Flow methods

## Executive Summary
This paper presents Bezier Distillation, a novel knowledge distillation approach for flow-based generative models that addresses the error accumulation problem in traditional Rectified Flow methods. The core innovation is using Bezier curves as guiding distributions between initial noise and target data distributions, creating smoother transport paths than direct linear mappings. By incorporating one or more intermediate guiding distributions connected through Bezier interpolation, the method avoids the instability and error accumulation that occurs in complex pairing relationships. Experimental results demonstrate that Bezier Distillation outperforms current Rectified Flow distillation techniques with fewer iterations and generates higher quality samples than existing single-step or two-step generative models.

## Method Summary
The method builds on Rectified Flow by introducing Bezier curves as guiding distributions between initial noise X₀ and target data X₁. The drift function v is fitted via least squares regression to follow the Bezier interpolation path. For quadratic Bezier, the model uses one intermediate guiding distribution X_T generated by running a teacher model one step. The drift velocity is optimized to follow the quadratic Bezier trajectory B(t) = (1-t)²X₀ + 2t(1-t)X_T + t²X₁. A cubic variant extends this to incorporate two guiding distributions for multi-teacher knowledge transfer. The key advantage is that the endpoint remains the real data X₁, avoiding error accumulation from numerical solver approximations.

## Key Results
- Bezier Distillation outperforms current Rectified Flow distillation techniques with fewer iterations
- Generates better samples than existing single-step or two-step generative models
- Performs well in Image-to-Image Translation tasks, providing new directions for optimizing both single-step and multi-step generative models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate guiding distributions connected via Bezier interpolation create smoother transport paths than direct linear mapping, potentially reducing training instability.
- **Mechanism:** Instead of distilling a direct jump from noise X₀ to data X₁, the method introduces a guiding distribution X_T. The model fits the drift velocity to follow a quadratic Bezier curve trajectory B(t) = (1-t)²X₀ + 2t(1-t)X_T + t²X₁. This shapes the gradient flow, allowing the model to learn curved trajectories that may approximate complex distributions more safely than rigid straight lines.
- **Core assumption:** The optimal transport path between two distributions is not necessarily a straight line during the distillation phase, and a curved path guided by an intermediate state improves convergence.
- **Evidence anchors:** [abstract]: "...incorporate Bezier curves as guiding distributions... creates smoother transport paths and helps avoid the instability..."; [section 3.1]: "The drift force v... is set to drive the flow in the direction of the quadratic Bezier interpolation path... under the guidance of the distribution π_T."; [corpus]: Related work like "Variational Rectified Flow Matching" explores similar trajectory shaping, suggesting active research interest in non-linear flow paths.
- **Break condition:** If the intermediate guiding distribution X_T is of low quality or significantly misaligned, the Bezier curve could force the student model into a suboptimal trajectory worse than the straight-line baseline.

### Mechanism 2
- **Claim:** Anchoring the distillation loss to the real target data X₁ (rather than generated data X₁ᵏ) mitigates error accumulation caused by numerical solvers.
- **Mechanism:** Standard Rectified Flow distillation often tries to replicate the pairing (X₀ᵏ, X₁ᵏ) produced by a teacher ODE. Since X₁ᵏ is a numerical approximation, errors accumulate over recursive refinements (Reflows). Bezier Distillation constructs the curve endpoint as the ground truth data X₁, ensuring the distillation target remains accurate regardless of the teacher's integration errors.
- **Core assumption:** The primary source of performance degradation in recursive distillation is the drift between the simulated endpoint and the true data distribution.
- **Evidence anchors:** [section 3.3]: "The end-point under the Bezier path is still the real data in the dataset... avoiding the phenomenon of error accumulation."; [section 2.1]: "distillation aims to faithfully approximate the coupling pair... distillation does not improve indefinitely as k increases... error accumulation."; [corpus]: *Weak/missing explicit confirmation in neighbors.* While neighbors like "SCoT" discuss unifying trajectories, they do not explicitly verify the "real data anchoring" mechanism for error correction.
- **Break condition:** If the teacher model required to generate the intermediate guide X_T is itself highly unstable, the benefits of anchoring to real data may be overshadowed by noise in the guiding term.

### Mechanism 3
- **Claim:** The framework supports multi-teacher knowledge transfer by extending the geometric path to higher-order curves (cubic) that integrate multiple intermediate velocities.
- **Mechanism:** By using a cubic Bezier curve, the method incorporates two guiding distributions (X_T, X_{T'}), acting as a structural fusion of multiple "teachers" (or intermediate states), where the drift velocity v is regressed against a composite derivative involving terms from both guides.
- **Core assumption:** Combining knowledge from multiple stages or models can be achieved via geometric interpolation of their velocity fields.
- **Evidence anchors:** [section 3.2]: "In this case, the drift force v is set to drive the flow in the direction of the cubic Bezier interpolation path... guided by the distribution π_T and π_{T'}."; [section 3]: "combine multi-teacher knowledge distillation with Bezier curves."
- **Break condition:** If the multiple teachers provide conflicting gradient signals (e.g., crossing trajectories), the cubic path might exhibit oscillations or loops (artifacts of high-degree Bezier curves), degrading sample quality.

## Foundational Learning

- **Concept:** **Rectified Flow & Reflow**
  - **Why needed here:** This is the baseline architecture being improved. You must understand that "Reflow" straightens trajectories to make sampling faster, and "Distillation" attempts to collapse these steps into one. The paper addresses the failure mode of this specific process.
  - **Quick check question:** Why does the paper claim that simply increasing the number of Reflow steps (k) eventually degrades performance in standard Rectified Flow?

- **Concept:** **Bezier Curves (Parametric Interpolation)**
  - **Why needed here:** The core contribution uses Quadratic and Cubic Bezier math to define the loss function. You need to know that control points "pull" the curve without the curve necessarily passing through them (except start/end), which serves as the "guiding" mechanism.
  - **Quick check question:** In the Quadratic Bezier formulation used in the paper (Eq. 4), which data point serves as the "control point" that pulls the path?

- **Concept:** **ODE/SDE Solvers & Discretization Error**
  - **Why needed here:** The paper motivates Bezier Distillation by contrasting it against the errors introduced by numerical solvers (Euler/Heun) used in standard flow models.
  - **Quick check question:** According to Section 3.3, how does the Bezier approach theoretically avoid the discretization errors typical of numerical solvers?

## Architecture Onboarding

- **Component map:** X₀ → Teacher Model → X_T → Bezier Pathing → Student Network → X₁
- **Critical path:**
  1. Sample noise X₀ and real data X₁.
  2. Generate intermediate guide X_T by running the Teacher model (e.g., X_T = X₀ + v_T(X₀, 1)).
  3. Compute the "Bezier Velocity Target" using the derivative of the Bezier polynomial (e.g., (t-1)X₀ + (1-2t)X_T + tX₁).
  4. Regress Student Network output v(X_t, t) against this target via MSE.

- **Design tradeoffs:**
  - **Curve Degree vs. Complexity:** A Quadratic curve (1 guide) is simpler but offers less flexibility than a Cubic curve (2 guides). The paper suggests higher-order curves handle multi-teacher setups but implies increased mathematical complexity.
  - **Direct vs. Guided Distillation:** Standard distillation is a special case of Bezier (linear). Bezier sacrifices the simplicity of the straight line for a potentially smoother, more accurate convergence surface.

- **Failure signatures:**
  - **Trajectory Crossing:** If guides are not well-chosen, Bezier curves (which are spatially flexible) might cross paths in high-dimensional space, violating the non-intersecting flow property desired in Rectified Flow.
  - **Overshooting:** If the guiding distribution X_T is too far from the linear path, the curve might loop or overshoot, causing the student to learn unnatural velocities.

- **First 3 experiments:**
  1. **Baseline Comparison:** Compare 1-step generation quality (FID/IS) of Bezier Distillation vs. standard 1-Rectified Flow Distillation on CIFAR-10/ImageNet.
  2. **Ablation on Guidance:** Test Quadratic (1 guide) vs. Cubic (2 guides) vs. Linear (0 guides) to quantify the impact of the "guiding" mechanism.
  3. **Error Accumulation Analysis:** Measure performance degradation as the "Reflow" iteration count (k) increases for both standard distillation and Bezier Distillation to validate the claim of error robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Bezier Distillation empirically reduce error accumulation in high-k Rectified Flow distillations compared to standard methods?
- **Basis in paper:** [explicit] The abstract claims the method addresses error accumulation to outperform current techniques, yet Section 4 ("Experiment") is empty in the provided text.
- **Why unresolved:** The theoretical framework is proposed, but without experimental data or comparison metrics (e.g., FID scores across varying k values), the reduction of error accumulation remains unproven.
- **What evidence would resolve it:** Benchmarking results on datasets like CIFAR-10 or ImageNet comparing sample quality degradation between standard Rectified Flow and Bezier Distillation as the number of rectifications increases.

### Open Question 2
- **Question:** What is the optimal mechanism for selecting intermediate guiding distributions (control points) for different data complexities?
- **Basis in paper:** [inferred] Section 3 introduces quadratic and cubic Bezier paths but does not define a rule for choosing the number of intermediate distributions (X_T) or their specific time steps (t', t'') for a given task.
- **Why unresolved:** The method relies on these points to define the curve's shape, but the paper treats the order (quadratic vs. cubic) as an arbitrary choice rather than a learned or determined parameter.
- **What evidence would resolve it:** Ablation studies showing the impact of varying the number of guiding points on the straightness of the resulting flow and the resulting computational efficiency.

### Open Question 3
- **Question:** Can the Bezier Distillation framework be effectively adapted for multimodal generation tasks such as video or text?
- **Basis in paper:** [explicit] The conclusion explicitly lists "video generation, and text generation" as areas where the method is expected to show potential.
- **Why unresolved:** The current mathematical formulation is described in the context of image generation and Image-to-Image translation; applying it to discrete or sequential data domains (text) requires bridging the gap between continuous ODE flows and discrete token spaces.
- **What evidence would resolve it:** Successful application of Bezier Distillation to a text-to-video or text-to-text generation architecture, demonstrating faster inference than teacher models without quality degradation.

## Limitations
- No quantitative experimental results or FID/IS scores are provided to validate performance claims
- The exact drift network architecture, training hyperparameters, and dataset specifications remain unspecified
- The mechanism by which Bezier curves prevent error accumulation requires empirical verification beyond theoretical claims

## Confidence
- **High confidence:** The mathematical framework for Bezier Distillation (quadratic and cubic formulations) is clearly defined and internally consistent.
- **Medium confidence:** The theoretical motivation for using Bezier curves to create smoother transport paths is sound, though empirical validation is missing.
- **Low confidence:** The claim that Bezier Distillation outperforms current methods in both sample quality and training efficiency cannot be assessed without quantitative results.

## Next Checks
1. Implement and compare 1-step generation quality (FID/IS) of Bezier Distillation against standard 1-Rectified Flow Distillation on CIFAR-10/ImageNet with identical architectures and training budgets.
2. Conduct an ablation study varying the number of guiding distributions (linear vs. quadratic vs. cubic Bezier) to quantify the impact of the guiding mechanism on sample quality and training stability.
3. Measure performance degradation as Reflow iteration count (k) increases for both standard distillation and Bezier Distillation to empirically validate the error accumulation mitigation claim.