---
ver: rpa2
title: 'Fisher meets Feynman: score-based variational inference with a product of
  experts'
arxiv_id: '2510.21598'
source_url: https://arxiv.org/abs/2510.21598
tags:
- experts
- variational
- target
- learning
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new family of variational distributions
  based on products of multivariate t-distributions for score-based black-box variational
  inference (BBVI). The key innovation is using the Feynman identity to reformulate
  products of experts as latent variable models with Dirichlet variables, enabling
  efficient sampling and normalizing constant estimation.
---

# Fisher meets Feynman: score-based variational inference with a product of experts
## Quick Facts
- arXiv ID: 2510.21598
- Source URL: https://arxiv.org/abs/2510.21598
- Authors: Diana Cai; Robert M. Gower; David M. Blei; Lawrence K. Saul
- Reference count: 40
- Key outcome: Introduces a new family of variational distributions based on products of multivariate t-distributions for score-based black-box variational inference

## Executive Summary
This paper presents a novel approach to black-box variational inference (BBVI) that leverages products of multivariate t-distributions as variational approximations. The key innovation combines the Feynman identity with Dirichlet priors on expert weights, enabling efficient sampling and normalizing constant estimation. The method iteratively optimizes expert weights through convex quadratic programming to minimize Fisher divergence, with theoretical guarantees of exponential convergence. Experiments demonstrate superior performance compared to Gaussian BBVI and normalizing flows on challenging target distributions exhibiting skew, heavy tails, and multimodality.

## Method Summary
The proposed method reformulates products of experts using the Feynman identity to create latent variable models with Dirichlet-distributed expert weights. This transformation enables tractable normalization and efficient sampling while maintaining expressiveness through multivariate t-distributions. The variational inference algorithm alternates between updating the base parameters of t-distributions and optimizing expert weights via quadratic programming to minimize Fisher divergence. The approach provides both theoretical convergence guarantees and practical improvements in approximating complex posterior distributions that traditional Gaussian approximations struggle to capture.

## Key Results
- Achieves lower KL and Fisher divergences compared to Gaussian BBVI and normalizing flows on synthetic and real-world targets
- Demonstrates superior performance on distributions with skew, heavy tails, and multiple modes
- Maintains tractability while offering high expressiveness through multivariate t-distributions

## Why This Works (Mechanism)
The method succeeds by combining the flexibility of product-of-experts models with tractable inference through the Feynman identity reformulation. By introducing Dirichlet priors on expert weights, the approach enables exact normalization and efficient sampling while preserving the expressiveness needed to approximate complex posterior geometries. The iterative optimization of expert weights through convex quadratic programming ensures convergence to local minima of the Fisher divergence, providing both theoretical guarantees and practical performance improvements over traditional variational approximations.

## Foundational Learning
1. Score-based variational inference
   - Why needed: Enables gradient-based optimization without requiring explicit variational distributions
   - Quick check: Verify gradient estimates match analytical derivatives for simple cases

2. Product of experts models
   - Why needed: Combines multiple simple distributions to create more expressive approximations
   - Quick check: Test performance degradation as expert count increases

3. Feynman identity for latent variable models
   - Why needed: Transforms intractable products into tractable latent variable formulations
   - Quick check: Confirm normalization constants match Monte Carlo estimates

4. Fisher divergence minimization
   - Why needed: Provides stronger convergence guarantees than KL divergence for certain applications
   - Quick check: Compare convergence rates with KL-based approaches on benchmark problems

5. Multivariate t-distributions
   - Why needed: Heavy-tailed distributions that better capture outliers and multimodal structure
   - Quick check: Test robustness to outliers in synthetic datasets

## Architecture Onboarding
Component map: Dirichlet prior -> Expert weight optimization -> t-distribution parameters -> Score-based gradient updates

Critical path: The algorithm alternates between (1) updating expert weights via quadratic programming to minimize current Fisher divergence, and (2) updating t-distribution parameters using score-based gradients. This creates a coordinate ascent procedure that converges exponentially fast under appropriate conditions.

Design tradeoffs: The choice of Dirichlet concentration parameters affects both expressiveness and computational stability. While multivariate t-distributions provide heavy-tailed flexibility, they increase computational complexity compared to Gaussian alternatives. The quadratic programming step ensures convexity but may become expensive in high dimensions.

Failure signatures: Poor performance typically manifests as (1) convergence to suboptimal local minima due to improper expert initialization, (2) numerical instability in quadratic programming for ill-conditioned weight matrices, or (3) inadequate expressiveness when expert count is too low for complex target distributions.

First experiments:
1. Verify exponential convergence on a simple two-mode Gaussian mixture
2. Test sensitivity to Dirichlet concentration parameters on a heavy-tailed target
3. Compare performance with Gaussian BBVI on a skewed distribution

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding systematic selection of expert numbers and configurations, the impact of Dirichlet prior parameters on performance across diverse target distributions, and the scalability of quadratic programming approaches to very high-dimensional problems. Additionally, the theoretical framework assumes convexity conditions that require careful verification in practical high-dimensional settings.

## Limitations
- Expert selection remains heuristic without principled criteria for determining optimal number and configuration
- Dirichlet prior concentration parameters significantly impact performance but lack systematic guidance
- Quadratic programming step may face practical challenges when scaling to very high-dimensional problems

## Confidence
- Theoretical framework and convergence guarantees: High
- Empirical performance improvements over baselines: Medium
- Scalability and computational efficiency: Low

## Next Checks
1. Systematic sensitivity analysis of Dirichlet prior parameters across diverse target distributions
2. Empirical evaluation of quadratic programming convexity conditions in high-dimensional settings (d > 50)
3. Comparative study with alternative product of experts formulations using different base distributions (e.g., Gaussian, Cauchy)