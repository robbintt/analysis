---
ver: rpa2
title: Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments
arxiv_id: '2510.00691'
source_url: https://arxiv.org/abs/2510.00691
tags:
- text
- etr-fr
- evaluation
- generation
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating accessible Easy-to-Read
  (ETR) texts for individuals with cognitive impairments, a process that is currently
  slow, costly, and difficult to scale. To tackle this, the authors introduce ETR-fr,
  the first dataset for ETR text generation fully compliant with European ETR guidelines,
  containing 523 aligned text pairs.
---

# Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments

## Quick Facts
- arXiv ID: 2510.00691
- Source URL: https://arxiv.org/abs/2510.00691
- Reference count: 40
- Key outcome: mBARThez with LoRA achieves 32.88 ROUGE-1 on ETR-fr, outperforming larger LLMs and demonstrating strong out-of-domain generalization to political texts.

## Executive Summary
This paper addresses the challenge of generating accessible Easy-to-Read (ETR) texts for individuals with cognitive impairments, a process that is currently slow, costly, and difficult to scale. To tackle this, the authors introduce ETR-fr, the first dataset for ETR text generation fully compliant with European ETR guidelines, containing 523 aligned text pairs. They implement parameter-efficient fine-tuning (PEFT) techniques, such as prefix-tuning and LoRA, on pre-trained language models (PLMs) like mBART and mBARThez, as well as large language models (LLMs) like Mistral-7B and Llama-2-7B. A comprehensive evaluation framework is introduced, combining automatic metrics (ROUGE, BERTScore, SARI, KMRE) with human assessments based on a 36-question evaluation form aligned with ETR guidelines. Results show that mBARThez with LoRA achieves the best overall performance in ROUGE, BERTScore, and simplification metrics, both on in-domain and out-of-domain (political texts) test sets. Notably, lightweight PLMs perform comparably to larger LLMs, with mBARThez demonstrating better generalization to out-of-domain data.

## Method Summary
The authors introduce ETR-fr, a dataset of 523 aligned French text pairs for Easy-to-Read generation, fully compliant with European ETR guidelines. They implement parameter-efficient fine-tuning (PEFT) techniques—LoRA and prefix-tuning—on both pre-trained language models (PLMs: mBART, mBARThez) and large language models (LLMs: Mistral-7B, Llama-2-7B). Training uses AdamW optimizer with linear learning rate schedule and 10% warm-up, batch size 8, for 30 epochs (PLMs) or 5 epochs (LLMs). LoRA hyperparameters are searched over ranks {8, 16, 32, 64, 128} with dropout {0.0, 0.05, 0.1}, setting α=r. Models are evaluated using a harmonic mean of SARI, ROUGE-L, and BERTScore, plus human assessment via a 36-question form.

## Key Results
- mBARThez with LoRA achieves 32.88 ROUGE-1, drastically outperforming full fine-tuning (16.47 ROUGE-1).
- Lightweight PLMs (mBARThez) perform comparably to larger LLMs (Mistral-7B) on both in-domain and out-of-domain (political texts) test sets.
- End-to-end generation with LoRA outperforms cascaded pipeline approaches (MUSS + BARThez: 20.15 ROUGE-1).

## Why This Works (Mechanism)

### Mechanism 1
Parameter-Efficient Fine-Tuning (PEFT) appears to preserve linguistic priors in low-resource domains better than full fine-tuning. By freezing model weights and injecting trainable low-rank matrices (LoRA) or prefix vectors, the optimization is constrained to a sub-space. This restricts the model from overwriting core linguistic knowledge while adapting to the specific "Easy-to-Read" (ETR) style. The base pre-trained model already possesses sufficient language fluency; the bottleneck is task alignment, not model capacity.

### Mechanism 2
Native-language pre-training likely aligns better with morphological accessibility constraints than multilingual or English-centric transfer. ETR guidelines impose strict constraints on sentence length and word complexity. Models pre-trained natively on French (mBARThez) may encode morphological and syntactic priors that reduce the difficulty of learning these specific constraints compared to multilingual (mBART) or English-dominant (Mistral/Llama) models.

### Mechanism 3
End-to-end generation captures the hybrid "summarification" nature of ETR more effectively than cascaded pipelines. ETR requires simultaneous text compression (summarization) and stylistic simplification. The paper suggests that separating these steps (e.g., summarizing then simplifying) causes error propagation or sub-optimal trade-offs. End-to-end models optimize for the final output directly, balancing fidelity (ROUGE) against readability (SARI/KMRE).

## Foundational Learning

**Concept: Low-Rank Adaptation (LoRA)**
- **Why needed here:** The paper relies on LoRA to adapt large models with only 523 text pairs.
- **Quick check question:** Can you explain why updating $W_0 + \Delta W$ via low-rank matrices $B$ and $A$ reduces memory usage compared to updating the full weight matrix $W_0$?

**Concept: Readability Metrics (SARI, KMRE)**
- **Why needed here:** Standard NLP metrics like BLEU fail to capture "simplicity." You need to understand how SARI rewards the addition of simple words and deletion of complex ones.
- **Quick check question:** Why does SARI explicitly compare the generated text against the *source* text in addition to the reference?

**Concept: Encoder-Decoder (Seq2Seq) vs. Decoder-Only Architectures**
- **Why needed here:** The paper contrasts mBARThez (Seq2Seq) with Mistral (Decoder-only).
- **Quick check question:** How does the attention mechanism differ when processing the source text in a BART model (bidirectional encoder) vs. a Mistral model (causal decoder)?

## Architecture Onboarding

**Component map:** Raw French text -> LoRA layers (Rank $r \in \{8, 16, 32\}$) injected into Self-Attention and Cross-Attention weights -> mBARThez (preferred for stability) or Mistral-7B -> Evaluation via custom harmonic mean of SARI, ROUGE-L, and BERTScore

**Critical path:**
1. Loading the ETR-fr dataset (alignment of source/target pairs).
2. Configuring the LoRA rank ($r$) and alpha scaling factor ($\alpha$).
3. Optimization of the composite metric (SARI/ROUGE) via grid search.

**Design tradeoffs:**
- **PLM vs. LLM:** PLMs (mBARThez) are lightweight and generalize better to out-of-domain political texts; LLMs (Mistral) are heavier and show higher variance/overfitting on small data.
- **Pipeline vs. End-to-End:** Pipelines are controllable but perform worse; End-to-End is opaque but yields higher quality.

**Failure signatures:**
- **Copy Bias:** High ROUGE but low SARI (model copies input without simplifying).
- **Expansion:** Negative compression ratio (model expands text rather than condensing it).
- **Metric Divergence:** High automatic scores but low "Validation Rate" in human evaluation (failure to capture nuance).

**First 3 experiments:**
1. **Baseline Reproduction:** Fine-tune mBARThez with LoRA ($r=32$) on ETR-fr to verify the 32.88 ROUGE-1 benchmark.
2. **Domain Generalization:** Evaluate the trained checkpoint on the ETR-fr-politic test set to measure performance degradation.
3. **Metric Ablation:** Compare human evaluation scores (Validation Rate) against BERTScore to determine if semantic embeddings truly correlate with ETR compliance.

## Open Questions the Paper Calls Out

**Open Question 1:** How can automatic evaluation metrics be specifically developed to accurately assess the unique constraints of Easy-to-Read (ETR) generation, such as novelty ratio, repetition, and coherence? The authors state that current summarization and simplification metrics "do not capture all characteristics of ETR generation" and suggest creating new composite metrics that correlate significantly higher with human judgments of ETR compliance.

**Open Question 2:** To what extent does Reinforcement Learning from Human Feedback (RLHF) involving individuals with cognitive impairments improve the accessibility and quality of generated ETR texts compared to PEFT methods? The paper proposes RLHF could further refine ETR generation by aligning model outputs with user preferences, suggesting a comparative study showing RLHF-trained models achieve higher linguistic acceptance and comprehension scores than the current mBARThez+LoRA baseline.

**Open Question 3:** What effective methods exist for cross-lingual transfer in ETR generation that avoid the error propagation of the "translate-simplify-retranslate" strategy? The paper notes that cross-lingual transfer remains particularly challenging and cites prior work demonstrating that the translate-simplify-retranslate strategy "is ineffective for ETR, often resulting in incorrect outputs," suggesting development of a multilingual model or transfer learning technique validated by native speakers.

## Limitations

- The study is based on a relatively small dataset (523 text pairs), which raises questions about generalizability to broader ETR contexts.
- The human evaluation component was conducted by only 5 experts, which may limit the statistical robustness of these assessments.
- The exact winning hyperparameters (specific LoRA rank and dropout values) were not explicitly disclosed, only the search ranges.

## Confidence

**High Confidence:** The general superiority of LoRA-based fine-tuning over full fine-tuning (32.88 vs 16.47 ROUGE-1) is well-supported by the reported metrics and aligns with established literature on parameter-efficient adaptation.

**Medium Confidence:** The relative performance differences between specific models (mBARThez vs mBART vs Mistral) are supported by automatic metrics but may be influenced by factors not fully controlled for (e.g., French-native pre-training).

**Low Confidence:** The interpretation that lightweight PLMs perform comparably to larger LLMs due to "better generalization" rather than dataset size limitations is speculative.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Conduct systematic ablation studies to identify the exact LoRA rank, dropout, and learning rate values that yield optimal performance.

2. **Cross-Lingual Generalization Test:** Evaluate the same PEFT approach on English ETR datasets to determine whether the mBARThez advantage is language-specific or transferable.

3. **Expert Evaluation Scale-Up:** Replicate the human assessment with a larger pool of cognitive accessibility experts (n≥20) to validate whether automatic metrics truly correlate with guideline compliance.