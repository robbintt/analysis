---
ver: rpa2
title: 'Normalized vs Diplomatic Annotation: A Case Study of Automatic Information
  Extraction from Handwritten Uruguayan Birth Certificates'
arxiv_id: '2507.08636'
source_url: https://arxiv.org/abs/2507.08636
tags:
- document
- annotation
- diplomatic
- handwritten
- normalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the Document Attention Network (DAN) for
  extracting key-value information from Uruguayan birth certificates, focusing on
  two annotation strategies: normalized and diplomatic. DAN was fine-tuned on a dataset
  of 201 handwritten birth certificates, using minimal training data and annotation
  effort.'
---

# Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates

## Quick Facts
- arXiv ID: 2507.08636
- Source URL: https://arxiv.org/abs/2507.08636
- Reference count: 40
- Primary result: DAN fine-tuned on 201 Uruguayan birth certificates achieved CER/WER comparable to French handwritten letters baseline, with normalized annotation better for dates/places and diplomatic annotation better for names/surnames

## Executive Summary
This study evaluates the Document Attention Network (DAN) for extracting key-value information from Uruguayan birth certificates, focusing on two annotation strategies: normalized and diplomatic. The research demonstrates that DAN can be effectively fine-tuned on a small dataset of 201 handwritten documents using minimal training data and annotation effort. The normalized annotation approach excels at extracting standardized fields like dates and places of birth, while diplomatic annotation performs better for non-standardizable fields containing names and surnames. The results show that DAN achieves similar performance to previous work with character error rates and word error rates comparable to the original DAN model trained on French handwritten letters.

## Method Summary
The study fine-tuned the Document Attention Network (DAN) on a dataset of 201 Uruguayan handwritten birth certificates, using 161 images for training, 20 for validation, and 20 for testing. Two annotation strategies were compared: normalized (using standard formats for dates and places) and diplomatic (transcribing text exactly as written). The model was trained using a character error rate loss function with beam search decoding. The authors evaluated performance using character error rate (CER) and word error rate (WER) metrics, comparing results across both annotation strategies to determine which approach better captures key-value information from the handwritten documents.

## Key Results
- DAN achieved CER and WER values comparable to the original DAN model trained on French handwritten letters
- Normalized annotation outperformed diplomatic annotation for standardized fields (dates and places of birth)
- Diplomatic annotation strategy excelled for non-normalizable fields containing names and surnames
- Transfer learning with minimal data (161 training images) was successful for this handwritten document recognition task

## Why This Works (Mechanism)
The Document Attention Network's effectiveness in this context stems from its ability to leverage transfer learning from a pre-trained model on French handwritten letters to a new domain of Uruguayan birth certificates. The attention mechanism allows the model to focus on relevant parts of the document when extracting key-value pairs, while the sequence-to-sequence architecture handles the variable length of handwritten text. The choice between normalized and diplomatic annotation strategies affects how the model interprets and transcribes the handwritten content, with each approach optimizing for different types of information extraction challenges.

## Foundational Learning
- **Character Error Rate (CER)**: Measures the edit distance between predicted and ground truth text at the character level; needed to evaluate transcription accuracy in handwritten text recognition
- **Word Error Rate (WER)**: Similar to CER but operates at the word level; quick check: WER â‰¥ CER since word errors include character errors
- **Normalized vs Diplomatic Annotation**: Two strategies for transcribing handwritten text where normalized uses standard formats and diplomatic preserves original writing; needed to handle the trade-off between standardization and fidelity
- **Transfer Learning**: Technique where a pre-trained model is fine-tuned on a new task with limited data; quick check: model should converge faster than training from scratch
- **Beam Search Decoding**: Search strategy that explores multiple possible sequences to find the most probable output; needed to improve transcription accuracy by considering multiple hypotheses
- **Key-Value Extraction**: Task of identifying and extracting specific information fields from documents; quick check: model must learn spatial relationships between keys and values

## Architecture Onboarding
**Component Map:** Input Images -> DAN Encoder -> Attention Mechanism -> DAN Decoder -> Character Predictions -> Loss Calculation (CER)

**Critical Path:** The attention mechanism that connects the encoder's visual features to the decoder's character predictions is the critical path, as it determines how well the model can locate and extract specific key-value pairs from the document layout.

**Design Tradeoffs:** The choice between normalized and diplomatic annotation represents a fundamental tradeoff between standardization and fidelity. Normalized annotation simplifies downstream processing by enforcing consistent formats but may introduce errors when standardization is ambiguous. Diplomatic annotation preserves the original handwriting but requires additional processing to normalize extracted data for practical use.

**Failure Signatures:** High CER/WER values specifically for names and surnames indicate limitations with diplomatic annotation's ability to handle handwriting variability. Poor performance on dates and places suggests normalized annotation struggles with ambiguous date formats or non-standard place names. Spatial misalignment between predicted keys and values indicates attention mechanism issues with document layout understanding.

**3 First Experiments:**
1. Evaluate DAN's performance on a held-out test set of Uruguayan birth certificates using both annotation strategies
2. Compare CER/WER values across different document fields (names, dates, places) to identify which annotation strategy works best for each type
3. Visualize attention maps to verify the model correctly identifies key-value pair locations within the document layout

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can a "hybrid" annotation strategy, utilizing normalized labels for standard fields and diplomatic labels for names, optimize extraction performance?
- Basis in paper: [explicit] Section 7 asks whether using a hybrid annotated dataset "would result in a model that is capable of both transcribing 'normalizable' fields in a normalized way... while being extremely accurate when transcribing non-normalizable fields."
- Why unresolved: The experiments compared two mutually exclusive datasets (fully normalized vs. fully diplomatic) and did not test a mixed approach within a single training set.
- What evidence would resolve it: Fine-tuning DAN on a dataset applying specific annotation strategies to specific field types and evaluating the resulting CER/WER across all categories.

### Open Question 2
- Question: What is the minimum number of annotated birth certificates required to fine-tune the Document Attention Network (DAN) without significant loss of accuracy?
- Basis in paper: [explicit] Section 7 identifies "exploring how far we can go in reducing the number of birth certificates needed to fine-tune a model without losing accuracy" as a future research direction.
- Why unresolved: While the study demonstrated success with a low-resource dataset (161 training images), it did not establish a lower bound for data scarcity.
- What evidence would resolve it: A series of ablation studies fine-tuning the model with incrementally smaller training sets and measuring the degradation of Character Error Rates.

### Open Question 3
- Question: Does customizing the annotation reading order based on the document's physical layout (left vs. right margin) improve extraction accuracy?
- Basis in paper: [explicit] Section 7 notes that certificates vary by margin position and proposes testing "whether using a different annotation order for birth certificates that have the margin on the left or on the right will improve accuracy."
- Why unresolved: The current model assumes a standard reading order, which may not align optimally with the spatial arrangement of text in all document templates.
- What evidence would resolve it: A comparative evaluation of models trained with layout-specific reading orders against the current unified approach on a diverse test set.

## Limitations
- Reliance on a relatively small dataset of 201 handwritten birth certificates limits generalizability
- Comparison based on a narrow domain (Uruguayan birth certificates) may not translate to other document types or cultural contexts
- Character error rate and word error rate metrics do not capture semantic errors or practical usability of extracted information

## Confidence
- **High confidence**: DAN's effectiveness for transfer learning with limited data; general trend of normalized annotation outperforming diplomatic for standardized fields
- **Medium confidence**: Specific CER/WER values and their comparison to French handwritten letters baseline; relative performance differences between annotation strategies
- **Low confidence**: Generalization of findings to other document types and languages; long-term stability of the observed performance differences

## Next Checks
1. Test DAN's performance on a larger, more diverse corpus of handwritten documents from different countries and time periods to assess generalizability
2. Implement semantic validation of extracted key-value pairs to complement error rate metrics and evaluate practical utility
3. Conduct a human-in-the-loop study comparing manual annotation effort between normalized and diplomatic approaches across multiple document types