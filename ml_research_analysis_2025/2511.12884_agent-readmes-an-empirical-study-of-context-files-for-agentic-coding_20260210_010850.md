---
ver: rpa2
title: 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding'
arxiv_id: '2511.12884'
source_url: https://arxiv.org/abs/2511.12884
tags:
- files
- context
- agent
- code
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale empirical analysis of
  agent context files (e.g., CLAUDE.md, AGENTS.md), which provide project-level instructions
  to AI coding agents. Analyzing 2,303 files from 1,925 repositories, we find these
  files are actively maintained through frequent, small additions rather than deletions,
  behaving as evolving configuration artifacts rather than static documentation.
---

# Agent READMEs: An Empirical Study of Context Files for Agentic Coding

## Quick Facts
- **arXiv ID**: 2511.12884
- **Source URL**: https://arxiv.org/abs/2511.12884
- **Reference count**: 40
- **Primary result**: Agent context files are actively maintained through frequent small additions, with 62.3% covering build/run commands and 69.9% implementation details, while neglecting non-functional requirements like security (14.5%) and performance (14.5%).

## Executive Summary
This study presents the first large-scale empirical analysis of agent context files (e.g., CLAUDE.md, AGENTS.md), which provide project-level instructions to AI coding agents. Analyzing 2,303 files from 1,925 repositories, we find these files are actively maintained through frequent, small additions rather than deletions, behaving as evolving configuration artifacts rather than static documentation. The files follow a consistent shallow hierarchy with single H1 headings and primarily H2/H3 sections. Content analysis reveals developers heavily prioritize functional context (build/run commands 62.3%, implementation details 69.9%, architecture 67.7%) while neglecting non-functional requirements like security (14.5%) and performance (14.5%). Automatic classification using GPT-5 achieves a micro-average F1-score of 0.79, performing well on concrete functional topics but struggling with abstract concepts. This indicates agents are well-configured for functionality but lack guardrails for quality attributes.

## Method Summary
The study analyzed 2,303 agent context files from 1,925 GitHub repositories, mining files with specific names (CLAUDE.md, AGENTS.md, copilot-instructions.md) from repositories with Star > 5. Statistical analysis measured word counts, Flesch Reading Ease scores, and header hierarchies. Git log analysis tracked maintenance patterns including commit frequency and line changes. For classification, GPT-5 was used to categorize content into 16 instruction types, with 332 files manually labeled as ground truth. The classification achieved a micro-average F1-score of 0.79 across the categories.

## Key Results
- Agent context files evolve through frequent small additions (67.4% modified in multiple commits) rather than deletions
- Files follow consistent shallow hierarchy (single H1 heading with H2/H3 sections) enabling effective automated classification
- 62.3% of files contain build/run commands, 69.9% cover implementation details, and 67.7% include architecture, while security and performance instructions appear in only 14.5% of files
- GPT-5 classification achieves 0.79 micro-average F1-score, with high performance on functional categories (Architecture: 0.93) but lower on abstract concepts (Maintenance: 0.56)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent context files function primarily as operational bootstrapping mechanisms rather than comprehensive quality guidelines.
- Mechanism: Developers iteratively populate context files with instructions required to overcome immediate execution failures (e.g., missing build commands), resulting in high coverage of functional requirements while leaving non-functional requirements (NFRs) like security and performance implicitly defined or absent.
- Core assumption: Developers treat agent success as "code that runs" rather than "code that is secure/performant" unless explicit failures occur.
- Evidence anchors:
  - [abstract] "non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified."
  - [section 4.3] "Instructions for functional aspects (Build and Run, Implementation Details...) are more prevalent... This pattern suggests that manifests are primarily optimized to help agents execute and maintain code efficiently..."
  - [corpus] Weak support in neighbors; focus is primarily on general agent configuration, not specifically the functional vs. non-functional split.
- Break condition: If an agent fails to execute a build, the developer adds the command; if the code runs but has a vulnerability, no trigger exists to add a security instruction.

### Mechanism 2
- Claim: Context files evolve as "living configuration" artifacts because they are tightly coupled to the codebase's current state.
- Mechanism: Unlike static documentation, context files change in short, incremental bursts (additions) because they must mirror the active development workflow (e.g., updated linter rules, new package scripts) to remain useful, effectively incurring maintenance debt similar to code.
- Core assumption: The utility of a context file degrades immediately if it diverges from the actual project configuration.
- Evidence anchors:
  - [abstract] "...maintained through frequent, small additions... evolving like configuration code."
  - [section 4.2] "67.4% of Claude Code files... are modified in multiple commits... evolution is driven by small incremental additions rather than deletions."
  - [corpus] "Decoding the Configuration of AI Coding Agents" supports the view of these files as critical configuration.
- Break condition: A context file that is not updated alongside the codebase (e.g., after a migration from npm to pnpm) ceases to function and may actively harm agent performance.

### Mechanism 3
- Claim: A consistent, shallow structural hierarchy aids automated classification and retrieval efficacy.
- Mechanism: The prevalent pattern of a single H1 heading followed by H2/H3 subheadings creates distinct semantic chunks. This structure allows LLM-based classifiers (and likely the agents reading the files) to associate specific content (e.g., testing commands) with high-level intents with high accuracy (0.79 F1-score).
- Core assumption: LLMs rely on structural cues (headers) to segment and weight information in long contexts.
- Evidence anchors:
  - [abstract] "follow a consistent, shallow hierarchy... Automatic classification... is feasible with an overall F1-score of 0.79."
  - [section 4.1] "Structurally, they follow a consistent, shallow hierarchy... likely makes the context files easier to quickly parse, modify, and maintain."
  - [corpus] Neighbors do not explicitly analyze the structural hierarchy's effect on classification, making this a paper-specific finding.
- Break condition: If context files were unstructured plain text or deeply nested (H5+), the signal-to-noise ratio would drop, degrading both automated classification and agent retrieval accuracy.

## Foundational Learning

- Concept: **Context Debt**
  - Why needed here: The paper identifies that context files are difficult to read and constantly evolving. Understanding "Context Debt" helps frame the cost of maintaining these files alongside regular code.
  - Quick check question: If a build tool changes but the context file isn't updated, what happens to the agent's reliability?

- Concept: **Retrieval-Augmented Generation (RAG) & Semantic Chunking**
  - Why needed here: The study analyzes structure (headers) and content categories. To understand why structure matters, one must grasp how agents segment and retrieve information from long contexts.
  - Quick check question: Why might a classifier perform well on "Architecture" (0.93 F1) but poorly on "Maintenance" (0.56 F1) based on how these sections are typically written?

- Concept: **Non-Functional Requirements (NFRs) in AI Code Generation**
  - Why needed here: A key finding is the gap in security/performance instructions. Recognizing this gap requires distinguishing between making code *work* vs. making code *secure*.
  - Quick check question: What specific instruction would you add to a context file to prevent an agent from using deprecated, insecure libraries?

## Architecture Onboarding

- Component map:
  - Source Context: Markdown files (`CLAUDE.md`, `AGENTS.md`) acting as the "System Prompt" or "Persistent Memory."
  - Structural Layer: The H1/H2 hierarchy acting as a retrieval index.
  - Classification Engine: An LLM (e.g., GPT-5 in the study) that maps file content to 16 distinct instruction categories.
  - Maintenance Feedback Loop: Git commit history driving updates to the context files.

- Critical path:
  1. Ingest agent context files from root directory.
  2. Parse structural hierarchy (Headers) to segment text.
  3. Classify segments into functional vs. non-functional categories.
  4. Monitor "Additions" vs. "Deletions" in version control to track evolution patterns.

- Design tradeoffs:
  - **Specificity vs. Readability**: Long, detailed files improve agent accuracy but lower human readability (FRE scores < 30).
  - **Brevity vs. Guardrails**: Minimizing instructions saves tokens but increases the risk of agents violating implicit constraints (security/performance).
  - **Stability vs. Evolution**: Locking files prevents drift but creates stale context; active maintenance keeps context relevant but introduces volatility.

- Failure signatures:
  - **Drift**: File lists `npm` commands, but repo uses `pnpm`.
  - **Context Bloat**: File exceeds token limits or FRE score drops below 20 (academic paper level).
  - **Blind Spot**: Agent generates functional code that fails security compliance because the "Security" category (14.5% prevalence) was missing.

- First 3 experiments:
  1. **NFR Audit**: Scan existing context files for the explicit presence of "Security" or "Performance" headers; measure the gap.
  2. **Drift Detection**: Compare the "Build and Run" commands in the context file against the actual `package.json` or `Makefile` scripts to quantify staleness.
  3. **Readability vs. Performance**: A/B test a refactored, simplified context file (higher FRE) against the original to see if agent task success rate changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "context debt" be formally defined and automatically detected in agent context files?
- Basis in paper: [explicit] The authors explicitly call for future work to "identify specific context file smells (e.g., ambiguous directives)" and develop metrics beyond readability scores to assess maintainability.
- Why unresolved: The study establishes that context files are complex and difficult to read (median FRE 16.6), but lacks a formal model for when these files become a maintenance burden or a liability.
- What evidence would resolve it: A validated taxonomy of "context smells" and automated linters capable of flagging opaque or contradictory instructions.

### Open Question 2
- Question: Do current agentic coding benchmarks effectively penalize agents for ignoring non-functional requirements (NFRs) like security and performance?
- Basis in paper: [explicit] The authors state the need for "benchmarks or evaluation frameworks that explicitly test an agentâ€™s adherence to NFRs defined in agent context files."
- Why unresolved: The study found NFRs like security (14.5%) and performance (14.5%) are rarely specified, and current benchmarks focus on functional correctness, potentially perpetuating this blind spot.
- What evidence would resolve it: A benchmark suite that requires agents to pass security constraints and architectural patterns specified in the context file to achieve a passing grade.

### Open Question 3
- Question: Does the *depth* of instruction in context files correlate with the quality of agent-generated code?
- Basis in paper: [inferred] The authors note their classification was "purely binary" (presence/absence) and did not measure the "depth, complexity, or qualitative richness" of instructions.
- Why unresolved: We know functional instructions are prevalent, but it remains unclear if extensive documentation yields better code than minimal instructions or if it simply introduces noise.
- What evidence would resolve it: A controlled experiment comparing code generation success rates against varying levels of instructional detail and complexity in the context file.

## Limitations
- The classification results (F1=0.79) depend heavily on GPT-5 access, which may not be reproducible with public models
- The non-functional requirements gap (security/performance at 14.5%) assumes these categories were correctly labeled and comprehensive
- The study focuses on Markdown files with specific naming conventions, potentially missing context files in other formats

## Confidence
- **High**: Structural hierarchy findings (consistent shallow structure with H1/H2), maintenance frequency patterns (frequent small additions)
- **Medium**: Classification performance (0.79 F1) given potential model access constraints
- **Low**: Generalization to non-Markdown context files or other agent ecosystems

## Next Checks
1. **Replicate Classification Gap**: Manually audit 50 context files to verify the actual prevalence of security/performance instructions vs. reported 14.5%
2. **Model Proxy Validation**: Compare GPT-5 classification results against GPT-4o or Claude-3.5 on the same ground truth set to assess sensitivity to model choice
3. **Format Coverage Expansion**: Search for and analyze context files with non-standard naming (e.g., `AGENT_INSTRUCTIONS.md`, `.agentrc`) to estimate potential blind spots in the corpus