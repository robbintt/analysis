---
ver: rpa2
title: Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation
arxiv_id: '2507.01957'
source_url: https://arxiv.org/abs/2507.01957
tags:
- generation
- tokens
- arxiv
- autoregressive
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Locality-aware Parallel Decoding (LPD) to accelerate
  autoregressive image generation by significantly reducing the number of sequential
  generation steps. The core method introduces a flexible parallelized autoregressive
  modeling architecture that uses learnable position query tokens to guide parallel
  generation at arbitrary target positions, while a specialized attention mechanism
  ensures mutual visibility among concurrently generated tokens.
---

# Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation

## Quick Facts
- **arXiv ID:** 2507.01957
- **Source URL:** https://arxiv.org/abs/2507.01957
- **Reference count:** 40
- **Primary result:** LPD reduces generation steps from 256 to 20 (256×256) and from 1024 to 48 (512×512) without quality loss, achieving 3.4× lower latency than previous parallelized autoregressive models.

## Executive Summary
This paper introduces Locality-aware Parallel Decoding (LPD), a method to accelerate autoregressive image generation by dramatically reducing the number of sequential generation steps. LPD achieves this through a flexible parallelized autoregressive modeling architecture that uses learnable position query tokens to guide parallel generation at arbitrary target positions, while a specialized attention mechanism ensures mutual visibility among concurrently generated tokens. Additionally, a locality-aware generation order schedule is proposed to minimize intra-group dependencies and maximize contextual support by selecting spatially distant tokens for parallel generation. The approach maintains generation quality while achieving significant latency improvements over previous parallelized autoregressive models.

## Method Summary
LPD accelerates autoregressive image generation through three key innovations: (1) Flexible Parallelized Autoregressive Modeling using learnable position query tokens with specialized dual attention masks (context attention for causal dependencies and query attention for mutual visibility within parallel groups), (2) Locality-aware generation order scheduling that selects spatially distant tokens for parallel generation based on proximity to existing context and mutual distance constraints, and (3) a cosine-scheduled group size increase that reduces parallelism when context is sparse. The method reduces generation steps from 256 to 20 for 256×256 resolution and from 1024 to 48 for 512×512 resolution on ImageNet without compromising quality, achieving at least 3.4× lower latency than previous parallelized autoregressive models.

## Key Results
- LPD achieves FID of 1.92 at 32 steps and 2.10 at 20 steps for 256×256 ImageNet generation, maintaining quality while reducing steps by 8×
- Latency reduction of 3.4× compared to ARPG and 8.6× compared to SAR at 20 steps, measured on single A100 with BFloat16
- Enables zero-shot image editing capabilities including inpainting, outpainting, and class-conditional editing without additional training
- Scales effectively across model sizes (LPD-L: 337M, LPD-XL: 752M, LPD-XXL: 1.4B) with consistent quality improvements

## Why This Works (Mechanism)

### Mechanism 1: Position Query Token Decoupling
Separating context tokens from generation tokens enables arbitrary-order parallel decoding while maintaining KV-cache efficiency. Previously generated image tokens serve only as context (providing KV entries), while learnable position query tokens (positional embedding + shared learnable embedding) drive generation at target locations. This allows the model to decode multiple spatially distant positions in a single forward pass. The core assumption is that position queries can uniquely specify target locations and the model can learn to map queries to correct tokens without sequential dependency.

### Mechanism 2: Dual Attention Pattern for Mutual Visibility
Specialized training attention masks enable concurrently generated tokens to see each other, improving consistency within parallel groups. Two attention patterns are used: (1) Context Attention allows causal attention to prior context tokens, and (2) Query Attention ensures full mutual visibility among position query tokens within the same generation step, while preventing subsequent tokens from attending to query tokens. The core assumption is that tokens generated in parallel need bidirectional context from each other to maintain local consistency.

### Mechanism 3: Locality-aware Generation Order
Selecting spatially distant tokens for parallel generation minimizes intra-group dependencies, while prioritizing tokens near existing context maximizes conditioning strength. At each step, proximity scores to already-generated tokens are computed; candidates above a threshold that are also mutually distant are selected (with farthest-point sampling fallback). The schedule is precomputed and reused across inference runs. The core assumption is that spatial locality in attention implies that distant tokens have weaker mutual dependencies and can be safely predicted in parallel.

## Foundational Learning

- **Concept: KV-Cache in Autoregressive Transformers**
  - Why needed here: LPD inherits KV-caching from standard autoregressive models, storing key-value pairs for context tokens while excluding query tokens. Understanding this is essential to see why LPD achieves lower latency than non-autoregressive mask-prediction models (which require full bidirectional attention and no KV-cache).
  - Quick check question: In LPD inference, which tokens' KV entries are cached across steps—image tokens, position query tokens, or both?

- **Concept: Attention Mask Design for Parallel Decoding**
  - Why needed here: The core architectural innovation is the dual attention mask (context + query). You must understand causal vs. bidirectional attention patterns to implement the training-time masking correctly.
  - Quick check question: In the training attention mask (Figure 4), can position query token P3 attend to query token P5 if they are in the same generation step? Can token P5 attend to P3?

- **Concept: Farthest-Point Sampling**
  - Why needed here: The locality-aware schedule uses FPS as a fallback when high-proximity candidates are exhausted. Understanding FPS helps you implement and debug the scheduling algorithm.
  - Quick check question: If you have 5 already-selected points and need to pick 3 more from 10 candidates using FPS, what criterion determines the next selection?

## Architecture Onboarding

- **Component map:** LlamaGen tokenizer (16384 codebook, downsample 16) -> Transformer Backbone (LPD-L: 12L/1024H, LPD-XL: 36L/1280H, LPD-XXL: 48L/1536H) -> Position Query Tokens (pos_embedding + shared_learnable_embedding) -> Attention Mask Generator (dual patterns) -> Locality Scheduler (precomputed schedule)

- **Critical path:** 1. Training: Randomly sample decoding steps from {8, 12, 16, 20, 24, 32, 64, 128, 256}; for each step count, compute group sizes via cosine schedule; interleave ground-truth tokens with position queries; apply dual attention mask; train with cross-entropy loss. 2. Inference: Load precomputed schedule; for each step: (a) encode newly generated image tokens to update KV-cache, (b) decode next group via position queries attending to cached KV, (c) sample tokens from output logits; fuse (a) and (b) into single forward pass via inference attention mask.

- **Design tradeoffs:** Parallelism vs. Quality (more aggressive parallelism increases FID), Memory vs. Flexibility (LPD avoids storing query tokens in KV-cache, trading computation for memory), Fixed vs. Adaptive Schedules (LPD uses precomputed schedules based on locality analysis).

- **Failure signatures:** Inconsistent local textures (missing query attention mutual visibility), High FID with low step count (>3.0 at 20 steps) (locality schedule not used or group sizes too aggressive), Memory blowup at 512×512 resolution (query token KV entries being cached), Slow inference despite few steps (KV-cache not properly reused).

- **First 3 experiments:** 1. Validate dual attention mechanism by training LPD-XL with random order schedule and comparing FID against RandAR/ARPG baselines. 2. Ablate locality principles by training three variants (Principle 1 only, Principle 2 only, Full LPD schedule) and comparing FID at 32 steps. 3. Profile latency vs. throughput by measuring single A100 performance for LPD-XL at 20/32 steps and comparing against ARPG-XL/RandAR-XL baselines.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the locality-aware generation schedule perform on complex text-to-image generation tasks compared to the evaluated class-conditional tasks? The paper explicitly limits its quantitative evaluation to "ImageNet class-conditional generation" despite claiming broader applicability. Text-to-image generation often requires synthesizing specific spatial relationships that may conflict with the proposed schedule's preference for minimizing intra-group dependencies.

- **Open Question 2:** Can the Flexible Parallelized Autoregressive Modeling architecture be effectively integrated into unified multimodal Large Language Models (LLMs) for interleaved text-and-image generation? While the architecture is theoretically compatible, the paper does not demonstrate or test this interoperability within an actual LLM framework that handles mixed modalities.

- **Open Question 3:** To what extent does the assumption of strong spatial locality limit the modeling of global structural dependencies, such as symmetry or reflections? The method relies on a heuristic schedule that prioritizes "spatially distant tokens" based on the observation that "attention... is concentrated on nearby spatial tokens." For images requiring strict global consistency, distant tokens may be highly dependent; enforcing their separation in parallel groups could theoretically degrade structural coherence.

## Limitations

- **Spatial locality assumption validity**: The method assumes spatially distant tokens have weak mutual dependencies, but this may not hold for images with long-range structural dependencies like symmetric patterns or global textures, potentially causing artifacts.

- **Schedule inflexibility**: LPD uses a precomputed, fixed generation schedule based on average locality patterns that cannot adapt to image-specific structural variations, missing potential quality improvements from adaptive per-image scheduling.

- **Zero-shot editing quality**: While the paper demonstrates zero-shot editing capabilities qualitatively, it lacks rigorous quantitative evaluation of editing quality, including FID on edited regions, preservation of unedited content, and editing consistency metrics.

## Confidence

- **High confidence**: The core architectural innovation (position query tokens + dual attention masks) is well-specified and the quality-latency tradeoff curves are internally consistent. The latency measurements are reproducible given the KV-cache implementation details.
- **Medium confidence**: The locality-aware scheduling algorithm and its impact on FID are supported by ablation studies, but the generalizability of the spatial locality assumption to diverse image types is uncertain.
- **Low confidence**: The zero-shot editing capabilities are demonstrated qualitatively but lack rigorous quantitative validation, and the mechanism is not thoroughly analyzed for editing-specific failure modes.

## Next Checks

1. **Stress-test locality assumption**: Generate images with known long-range dependencies (e.g., checkerboards, concentric circles, symmetric patterns) and evaluate FID degradation at high parallelization (20 steps). Compare against a baseline that uses random generation order to quantify the impact of violating the locality assumption.

2. **Validate attention mask implementation**: Implement a visualization tool to check the attention mask during training and inference. Confirm that position query tokens in the same generation step have full mutual visibility in Query Attention, and that KV-cache only stores image tokens, not query tokens. Verify this by checking memory growth is sublinear in sequence length.

3. **Quantify editing quality**: Implement quantitative metrics for zero-shot editing: (a) FID on edited regions (comparing to ground truth if available, or to serial autoregressive generation), (b) Content preservation score (e.g., SSIM of unedited regions), (c) Editing consistency (FID of same edit across multiple random seeds). Run on a held-out set of images with known ground truth to benchmark LPD's editing against serial autoregressive baselines.