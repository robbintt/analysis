---
ver: rpa2
title: 'Digitizing Nepal''s Written Heritage: A Comprehensive HTR Pipeline for Old
  Nepali Manuscripts'
arxiv_id: '2512.17111'
source_url: https://arxiv.org/abs/2512.17111
tags:
- data
- training
- text
- nepali
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first end-to-end handwritten text recognition
  (HTR) pipeline for Old Nepali, a historically significant but low-resource language.
  The authors adopt a line-level transcription approach and systematically explore
  encoder-decoder architectures, data-centric techniques, and decoding strategies
  to improve recognition accuracy.
---

# Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts

## Quick Facts
- arXiv ID: 2512.17111
- Source URL: https://arxiv.org/abs/2512.17111
- Reference count: 10
- Primary result: First end-to-end HTR pipeline for Old Nepali, achieving 4.9% CER on confidential manuscript dataset

## Executive Summary
This paper presents the first end-to-end handwritten text recognition (HTR) pipeline for Old Nepali, a historically significant but low-resource language. The authors adopt a line-level transcription approach and systematically explore encoder-decoder architectures, data-centric techniques, and decoding strategies to improve recognition accuracy. The best model achieves a Character Error Rate (CER) of 4.9%, demonstrating significant improvements over baseline OCR tools. Key contributions include a three-stage training pipeline using synthetic and printed Devanagari data to bridge the domain gap, script-aware tokenization and decoding methods tailored for Devanagari script, comprehensive data augmentation strategies, and thorough error analysis revealing interpretable patterns in model behavior. While the evaluation dataset is confidential, the authors release their training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

## Method Summary
The authors develop a three-stage training pipeline: synthetic Devanagari data (105K images from rendered historical Nepali textbooks with 12 fonts + noise) teaches basic script patterns, printed Devanagari data (5,139 lines from scanned heiDATA) introduces realistic scan artifacts, and Old Nepali manuscript data (2,480 lines) provides target domain adaptation. They use a TrOCR-large-handwritten encoder with custom BERT decoder and byte-level BPE tokenizer (vocab=500). The pipeline employs 8× data augmentation with 20 augmentation types grouped into shape distortions, quality degradations, and character-level distortions. Non-binarized grayscale images are used with line segmentation via Kraken polygon method. The model is trained for 6/10/20 epochs per stage with LR=3e-5, batch=8, AdamW optimizer, and warmup=500 steps.

## Key Results
- Best model achieves 4.9% Character Error Rate (CER) on confidential Old Nepali manuscript test set
- Three-stage training reduces CER from 0.71 (synthetic only) to 0.056 (all stages combined)
- 8× data augmentation optimal: CER drops from 0.089 (no augmentation) to 0.056, with no improvement at 12× or 16×
- Byte-level BPE tokenizer with BERT decoder marginally better than char-BPE: 0.082 vs 0.084 CER

## Why This Works (Mechanism)

### Mechanism 1: Progressive Domain Adaptation via Three-Stage Training
Sequential training on synthetic→printed→handwritten Devanagari data reduces domain shift when labeled target data is scarce. Stage 1 synthetic data teaches the decoder basic script patterns and vocabulary. Stage 2 printed Devanagari introduces realistic scan artifacts. Stage 3 fine-tunes on Old Nepali manuscript lines. The paper reports CER dropping from 0.71 (Stage 1 only) to 0.056 (after all stages) when evaluated on the held-out test set.

### Mechanism 2: Data Augmentation for Visual Robustness
Augmentation at 8× multiplicity yields optimal CER reduction (0.089 → 0.056); higher (12×, 16×) plateaus. 20 augmentation types grouped into shape distortions, quality degradations, and character-level distortions simulate real manuscript variability without altering ground-truth labels, expanding effective training set from 2,480 to 22,320 samples.

### Mechanism 3: Script-Aware Tokenization with Byte-Level BPE
Byte-level BPE tokenizer (vocab=500) with BERT decoder yields marginally better CER (0.082) than char-BPE (0.087) after three-stage training. BPE iteratively merges frequent byte/character pairs, creating subword units. Byte-level encoding handles Devanagari's multi-byte Unicode representations more gracefully than character-level splitting.

## Foundational Learning

- **Concept: Character Error Rate (CER)**
  - **Why needed here:** CER is the primary evaluation metric; understanding CER = (S+D+I)/N is essential to interpret all results tables.
  - **Quick check question:** Given 100 ground-truth characters, 3 substitutions, 2 deletions, and 1 insertion, what is the CER? (Answer: 6/100 = 0.06)

- **Concept: Encoder-Decoder Architecture for HTR**
  - **Why needed here:** The paper compares TrOCR (ViT encoder) + language decoder (BERT/GPT-2) combinations; understanding attention mechanisms explains why this architecture handles variable-length line images.
  - **Quick check question:** Why might an autoregressive decoder struggle with very long sequences (120+ characters) compared to shorter lines?

- **Concept: Transfer Learning in Vision-Language Models**
  - **Why needed here:** The entire three-stage pipeline relies on transferring learned representations; TrOCR encoder was NOT pretrained on Devanagari but still works.
  - **Quick check question:** If the encoder has never seen Devanagari, why does pretraining on Latin scripts still help? (Hint: low-level visual features)

## Architecture Onboarding

- **Component map:** Input image → TrOCR-large-handwritten encoder (ViT, 12 layers, 384×384 resize) → Script-aware decoder (BERT-base or GPT-2-base, 12 layers, 768 hidden) → Byte-level BPE tokenizer (vocab=500) → Text output
- **Critical path:** Image preprocessing → line segmentation accuracy → encoder feature extraction → decoder token generation. Segmentation errors propagate directly; the paper notes this dependency as a limitation.
- **Design tradeoffs:** TrOCR-large vs TrOCR-base: Large reduces CER (0.056→0.049) but increases inference time and parameters (334M vs 202M effective with custom decoder). BERT vs GPT-2 decoder: Minimal CER difference (0.082 vs 0.084); choice may not matter in low-resource settings. Augmentation level: 8× optimal; higher yields no gain, increases training time.
- **Failure signatures:** Lines >120 characters: CER spikes due to limited long-sequence training samples (only 26/3100 lines). Virama (halant) errors: 12.92% of all errors—model struggles with conjunct formation. Visually similar characters (ya/pa, ta/na): Predictable confusion matrix patterns suggest visual ambiguity, not model failure.
- **First 3 experiments:**
  1. **Reproduce baseline:** Fine-tune off-the-shelf TrOCR on Old Nepali data only (no Stage 1–2); expect CER ~0.096 per Table M comparison
  2. **Ablate augmentation:** Train with 0×, 4×, 8× augmentation; plot CER curve to verify 8× optimum on your data
  3. **Tokenizer comparison:** Train identical model with byteBPE vs charBPE; if CER difference <0.01, tokenizer choice is secondary to data quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the pipeline be adapted to mitigate the sharp performance degradation observed on text lines exceeding 120 characters?
- **Basis in paper:** [explicit] The authors note that "error rates increase sharply for lines longer than 120 characters," likely due to the limited number of long-sequence samples (only 26 out of 3,100 lines).
- **Why unresolved:** The current dataset imbalance prevents the model from generalizing well to longer sequences, and the study did not test architectural modifications specifically designed to handle longer dependencies.
- **What evidence would resolve it:** A comparative study showing stable CER across varying line lengths after incorporating specific long-sequence training data or architectural adjustments (e.g., sparse attention mechanisms).

### Open Question 2
- **Question:** Can a segmentation method be developed that adapts to irregular layouts without relying on external pre-processing tools like Kraken?
- **Basis in paper:** [explicit] The "Limitations" section states the current pipeline "depends on pre-segmented line inputs," introducing a potential source of error for documents with irregular layouts or severe damage.
- **Why unresolved:** The current workflow relies on a distinct segmentation step rather than an end-to-end solution, meaning segmentation errors directly impact recognition accuracy.
- **What evidence would resolve it:** The implementation of an end-to-end model that accepts full-page images and successfully isolates and transcribes text from irregular or damaged manuscripts.

### Open Question 3
- **Question:** To what extent can post-correction methods leverage the identified visual confusion patterns (e.g., confusing *ya* with *pa*) to further reduce the Character Error Rate?
- **Basis in paper:** [explicit] The discussion suggests that "post-correction methods that exploit structured error patterns may further reduce CER," specifically noting that errors follow predictable visual similarity patterns.
- **Why unresolved:** While the error analysis identifies the patterns, the paper does not implement or evaluate a post-correction module to utilize this structure.
- **What evidence would resolve it:** A follow-up experiment quantifying the CER reduction achieved by a post-processing model trained on the specific confusion matrices provided in the paper.

## Limitations
- Evaluation dataset (155 Old Nepali manuscripts) remains confidential, preventing independent verification of 4.9% CER claim
- Exact augmentation sampling strategy across 20 functions unspecified, though 8× multiplicity shown optimal
- Long sequence handling shows clear degradation (CER spikes for lines >120 characters) but only 26 such samples exist in 3,100-line dataset

## Confidence

**High confidence:** Three-stage training pipeline mechanism (CER 0.71→0.056) and data augmentation optimization (8× optimal) are supported by systematic experiments and align with established HTR literature. Error analysis patterns (virama confusion, visually similar characters) are interpretable and consistent with Devanagari script characteristics.

**Medium confidence:** Byte-level BPE tokenizer advantage (0.082 vs 0.087) shows marginal improvement but lacks comparative literature for Devanagari HTR specifically. TrOCR-large vs TrOCR-base performance difference (0.056→0.049) is statistically significant but computationally expensive.

**Low confidence:** Confidential evaluation dataset prevents independent validation of final 4.9% CER achievement. Long-sequence failure mode (120+ characters) is based on only 26 samples, making generalization uncertain.

## Next Checks

1. **Ablation of synthetic pretraining:** Train a model using only Stage 3 (Old Nepali manuscript data, 20 epochs) without Stages 1-2 to quantify the exact contribution of synthetic/printed data transfer, expecting CER to increase substantially above 0.056.

2. **Public dataset replication:** Apply the complete three-stage pipeline to a publicly available Devanagari HTR dataset (e.g., the handwritten Devanagari dataset from Kaggle) to verify that the 0.056 CER reduction pattern holds on accessible data.

3. **Augmentation sampling verification:** Systematically test different sampling distributions across the 20 augmentation functions at 8× multiplicity to determine whether the optimal result depends on specific function combinations or is robust to sampling variations.