---
ver: rpa2
title: 'Mathesis: Towards Formal Theorem Proving from Natural Languages'
arxiv_id: '2506.07047'
source_url: https://arxiv.org/abs/2506.07047
tags:
- formal
- lean
- theorem
- proving
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of end-to-end automated theorem
  proving from natural language problem statements. It introduces Mathesis-Autoformalizer,
  the first autoformalizer trained via reinforcement learning to translate informal
  math problems into formal Lean 4 statements, guided by syntactic and semantic correctness
  rewards.
---

# Mathesis: Towards Formal Theorem Proving from Natural Languages

## Quick Facts
- arXiv ID: 2506.07047
- Source URL: https://arxiv.org/abs/2506.07047
- Reference count: 40
- Introduces the first autoformalizer trained via RL for translating natural language math problems into formal Lean 4 statements, achieving 22% improvement in pass rate on Gaokao-Formal benchmark

## Executive Summary
This paper introduces Mathesis, an end-to-end system for automated theorem proving from natural language problem statements. The key innovation is Mathesis-Autoformalizer, which uses reinforcement learning with syntactic and semantic correctness rewards to translate informal math problems into formal Lean 4 statements. The system also includes Mathesis-Prover for generating formal proofs and introduces Gaokao-Formal, a new benchmark of 488 complex problems from China's Gaokao exams. The work represents a significant step toward bridging the gap between natural mathematical language and formal theorem proving systems.

## Method Summary
The paper presents Mathesis as a comprehensive framework for end-to-end automated theorem proving from natural language. The core innovation is Mathesis-Autoformalizer, which employs reinforcement learning to translate informal mathematical problems into formal Lean 4 statements. The system uses a combination of syntactic and semantic correctness rewards to guide the learning process. Additionally, Mathesis-Prover generates formal proofs, and a novel LeanScorer framework provides fine-grained evaluation of formalization quality. The approach is validated on two benchmarks: MiniF2F and the newly introduced Gaokao-Formal dataset containing 488 complex problems from Chinese university entrance exams.

## Key Results
- Mathesis-Autoformalizer achieves a 22% improvement in pass rate on Gaokao-Formal compared to the best baseline
- End-to-end system reaches state-of-the-art performance with 64% accuracy on MiniF2F
- System achieves 18% accuracy on the new Gaokao-Formal benchmark

## Why This Works (Mechanism)
The system works by combining reinforcement learning with carefully designed reward signals that capture both syntactic and semantic correctness of formalizations. The RL framework allows the model to iteratively improve its translations by receiving feedback on the quality of its formal outputs. The syntactic rewards ensure that generated statements are well-formed in Lean 4, while semantic rewards verify that the formalizations correctly capture the meaning of the original natural language problems. This dual-reward approach addresses both the structural requirements of formal theorem proving and the semantic fidelity needed for meaningful proofs.

## Foundational Learning

1. **Reinforcement Learning for sequence generation** - Why needed: Enables iterative improvement of formalizations based on correctness feedback. Quick check: Verify that reward signals are properly shaped and that the RL algorithm converges to meaningful solutions.

2. **Formal theorem proving in Lean 4** - Why needed: Provides the target representation for mathematical statements and proofs. Quick check: Confirm that the formal language coverage matches the scope of natural language problems being processed.

3. **Natural language processing for mathematical text** - Why needed: Extracts semantic meaning from informal problem statements. Quick check: Validate that the system correctly handles mathematical notation and terminology in diverse problem formulations.

4. **Syntactic and semantic evaluation metrics** - Why needed: Provides the feedback signals for training and evaluation. Quick check: Ensure that the metrics accurately capture both well-formedness and semantic correctness of formalizations.

## Architecture Onboarding

Component Map: Natural Language Input -> Mathesis-Autoformalizer (RL) -> Lean 4 Formalization -> Mathesis-Prover -> Formal Proof

Critical Path: The critical path runs through Mathesis-Autoformalizer, as accurate formalization is essential for subsequent proof generation. The RL training loop with syntactic and semantic rewards is the core innovation that enables effective translation from natural language to formal statements.

Design Tradeoffs: The system trades computational complexity (RL training is expensive) for improved accuracy in formalization. The choice of Lean 4 as the target formal language provides strong tooling but may limit generalizability to other theorem proving systems.

Failure Signatures: Common failure modes include incorrect interpretation of mathematical concepts, generation of syntactically invalid Lean code, and semantic drift where the formal statement no longer matches the original problem intent. The RL training may also struggle with rare mathematical constructs or highly complex problem statements.

3 First Experiments:
1. Test the autoformalization on a small set of problems with known correct formalizations to validate the syntactic and semantic reward signals
2. Evaluate the system's ability to handle increasingly complex mathematical constructs, from simple algebra to advanced calculus
3. Compare the LeanScorer framework's evaluations against human judgment on a sample of formalizations to validate its effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The 22% improvement, while significant, still indicates that the system struggles with a substantial portion of problems
- The end-to-end system's 18% accuracy on Gaokao-Formal shows that fully automated theorem proving from natural language remains challenging
- The focus on Lean 4 may limit the approach's applicability to other theorem proving systems and formal languages

## Confidence

Major claim cluster - Autoformalization methodology (High): The paper provides detailed methodology and experimental results supporting the RL-based autoformalization approach, though the specific novelty claims should be verified against existing literature.

Major claim cluster - Benchmark contribution (Medium): While the Gaokao-Formal benchmark appears novel, its size (488 problems) and representativeness for general theorem proving research needs further validation.

Major claim cluster - End-to-end performance (Medium): The reported performance metrics are specific and measurable, but the absolute numbers suggest significant room for improvement, and the real-world applicability remains to be proven.

## Next Checks

1. Verify the reproducibility of the reported 22% improvement in pass rate on Gaokao-Formal by testing the system with different random seeds and on additional datasets.

2. Conduct human evaluation studies to validate LeanScorer's assessment quality and its correlation with human judgment of formalization quality.

3. Test the system's generalization capability on a broader range of theorem proving tasks beyond the specific domains covered in MiniF2F and Gaokao-Formal benchmarks.