---
ver: rpa2
title: 'XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language
  Models'
arxiv_id: '2510.15148'
source_url: https://arxiv.org/abs/2510.15148
tags:
- audio
- text
- vision
- text7
- vision7
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XModBench, a large-scale tri-modal benchmark
  designed to evaluate cross-modal consistency in omni-modal language models. The
  benchmark includes 60,828 multiple-choice questions across five task families and
  systematically covers all six modality compositions in question-answer pairs.
---

# XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models

## Quick Facts
- arXiv ID: 2510.15148
- Source URL: https://arxiv.org/abs/2510.15148
- Reference count: 27
- Key outcome: Introduces a tri-modal benchmark revealing significant modality disparities in OLLMs, with audio as context being most challenging and spatial/temporal reasoning showing largest gaps

## Executive Summary
XModBench addresses the critical gap in evaluating cross-modal consistency in omni-language models (OLLMs) by providing a systematic benchmark that tests modality-invariant reasoning across audio, vision, and text. The benchmark reveals that while models achieve reasonable accuracy on individual modality tasks, they show significant performance disparities when the same semantic content is conveyed through different modalities, particularly struggling with audio-based reasoning and spatial/temporal tasks. These findings highlight the need for improved cross-modal alignment and modality-invariant representations in next-generation OLLMs.

## Method Summary
XModBench is a large-scale tri-modal benchmark consisting of 60,828 multiple-choice questions created by instantiating 10,138 unique question-answer pairs across six modality configurations (A→T, A→V, T→A, T→V, V→A, V→T). Each question tests one of five task families: perception, spatial reasoning, temporal reasoning, linguistic understanding, or external knowledge. The benchmark measures three key metrics: task competence (overall accuracy), modality disparity (performance gaps when same content is presented in different modalities), and directional imbalance (asymmetry when swapping context and candidate modalities).

## Key Results
- Gemini 2.5 Pro shows 49-point accuracy drop when audio serves as context versus text (∆Tvs.A = -49)
- Spatial and temporal reasoning tasks show lowest performance (<60% accuracy even for best models)
- Directional imbalance reveals systematic asymmetry, with models performing better when text is candidate versus context
- Providing dual audio-visual context rarely improves performance over single strongest modality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality-disparity measurement reveals systematic representation gaps in OLLMs
- Mechanism: By instantiating semantically identical questions across six modality configurations (A→T, A→V, T→A, T→V, V→A, V→T), the benchmark isolates modality-specific performance while controlling for task content. Paired subtraction (e.g., ∆Tvs.A = (Acc_A→V − Acc_A→T) + (Acc_V→A − Acc_T→A)) quantifies disparity.
- Core assumption: Performance differences across modalities for identical semantics indicate modality-specific biases rather than task difficulty variance.
- Evidence anchors: Gemini 2.5 Pro shows ∆Tvs.A = -49, indicating audio is most challenging modality

### Mechanism 2
- Claim: Directional imbalance reveals asymmetric cross-modal grounding
- Mechanism: Swapping context and candidate modalities (e.g., T→V vs. V→T) tests bidirectional alignment. The metric ∆X↔Y = Acc(X→Y) − Acc(Y→X) captures asymmetry, where non-zero values indicate incomplete grounding.
- Core assumption: True modality-invariant reasoning should be symmetric; training biases toward text as output modality cause directional imbalance.
- Evidence anchors: Gemini 2.5 Pro drops 8.8 points from T→V to V→T; Qwen2.5-Omni shows 16.6-point gap

### Mechanism 3
- Claim: Task-family stratification exposes reasoning-type vulnerabilities
- Mechanism: Five task families (perception, spatial, temporal, linguistic, external knowledge) with 17 subtasks enable fine-grained diagnosis. Spatial/temporal reasoning reveals the largest gaps (<60% for best models) while perception/linguistic tasks reach ~70%+.
- Core assumption: Task-family performance patterns generalize to real-world cross-modal reasoning demands.
- Evidence anchors: Gemini 2.5 Pro: Spatial=50.1, Temporal=60.8, Perception=75.9, Linguistic=76.8, External Knowledge=89.3

## Foundational Learning

- Concept: **Cross-modal consistency vs. cross-modal accuracy**
  - Why needed here: The benchmark explicitly distinguishes between performing well on a task (accuracy) and performing consistently across modalities (consistency). High accuracy with low consistency indicates modality-specific shortcuts.
  - Quick check question: Can a model achieve 90% accuracy on V→T and T→V tasks but still fail cross-modal consistency?

- Concept: **Directional grounding asymmetry**
  - Why needed here: Understanding that X→Y alignment differs from Y→X alignment is critical for interpreting directional imbalance metrics and designing remediation strategies.
  - Quick check question: Why might a model excel at "select the image matching this text" but struggle with "select the text matching this image"?

- Concept: **Modality-invariant representation**
  - Why needed here: The benchmark's ultimate goal is measuring progress toward representations where semantic content, not modality format, drives reasoning.
  - Quick check question: If audio, vision, and text inputs encoding identical semantics produce different internal activations, is the representation modality-invariant?

## Architecture Onboarding

- Component map:
  - Context modality encoder: Processes the question stem (audio, vision, or text)
  - Candidate modality encoder: Processes the four answer options (audio, vision, or text)
  - Cross-modal alignment layer: Maps context and candidate representations to shared space
  - Task-specific reasoning heads: Five heads for perception, spatial, temporal, linguistic, external knowledge

- Critical path:
  1. Identify which modality pairing shows largest disparity for your model
  2. Trace directional imbalance in that pairing
  3. Map failure cases to task families (spatial/temporal are highest priority)

- Design tradeoffs:
  - Modality-balanced design: Ensures fair evaluation but requires 6× data instantiation
  - Multiple-choice format: Enables controlled comparison but may not reflect open-ended reasoning
  - Synthetic audio generation: Ensures modality coverage but may introduce TTS artifacts

- Failure signatures:
  - High accuracy on V→T and T→V separately but low consistency: indicates modality-specific shortcuts
  - Strong performance on perception/linguistic but weak on spatial/temporal: suggests reasoning bottleneck, not representation issue
  - Near-random performance on A→V or V→A: indicates missing audio-visual alignment training

- First 3 experiments:
  1. Baseline disparity audit: Run all 6 modality configurations; compute ∆Tvs.A, ∆Vvs.A, ∆Tvs.V. Identify largest gap.
  2. Task-family breakdown: For the weakest modality pairing, stratify by task family. Confirm spatial/temporal are primary failure modes.
  3. Directional swap test: For pairs with ∆X↔Y > 10 points, analyze 10 failure cases manually to categorize error types (misalignment vs. hallucination vs. insufficient reasoning).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the significant performance gap for auditory representations be closed to achieve parity with visual and textual reasoning?
- Basis in paper: The conclusion states that "audio remains the most challenging modality," and the abstract notes that "auditory representations remain the weakest link" with performance dropping substantially when audio is involved.
- Why unresolved: The paper identifies the symptom (performance drops of 20+ points in audio-text vs. vision-text settings) but does not investigate whether the root cause lies in audio encoder quality, the modality alignment layer, or data scarcity.
- What evidence would resolve it: A study isolating the audio encoder (e.g., swapping the audio backbone or varying the pre-training data) to see if the modality disparity decreases without altering the LLM backbone.

### Open Question 2
- Question: What architectural or training modifications are required to eliminate the systematic directional imbalance (asymmetry) in cross-modal reasoning?
- Basis in paper: Section 4.4 ("Directional imbalance") explicitly details that models exhibit "clear asymmetries," performing better when text is the candidate rather than the context (e.g., V→T vs. T→V), suggesting "incomplete bidirectional alignment."
- Why unresolved: The benchmark measures the existence of this asymmetry, but the paper leaves open the mechanism causing it, speculating only that it reflects "training data biases toward text as the dominant output modality."
- What evidence would resolve it: Evidence showing that specific training objectives (e.g., bidirectional contrastive learning or modality-balanced instruction tuning) successfully reduce the gap between X→Y and Y→X accuracy scores.

### Open Question 3
- Question: How can models be improved to effectively leverage simultaneous multimodal cues rather than relying predominantly on a single dominant modality?
- Basis in paper: Section 4.6 and Appendix B discuss "Triple-Domain Question Answering," noting that "improvements are not always additive" when using dual audio-visual contexts, suggesting "current systems do not yet fully exploit complementary signals."
- Why unresolved: The paper evaluates dual-context performance but does not explain why providing more information (audio + vision) often fails to yield significant accuracy gains over the strongest single modality.
- What evidence would resolve it: Experiments demonstrating that attention mechanisms in OLLMs can successfully attend to non-dominant modalities in a dual-context setting to answer questions that are unanswerable by a single modality alone.

## Limitations

- Benchmark relies on synthetic audio generation that may introduce artifacts not representative of real-world audio processing
- Multiple-choice format may not capture open-ended reasoning capabilities and could artificially constrain observed phenomena
- Task-family taxonomy boundaries may be porous (e.g., spatial reasoning requiring perception), potentially obscuring diagnostic clarity

## Confidence

**High Confidence**: The methodology for measuring modality disparity and directional imbalance is sound and well-specified, with clear mathematical formulations (ΔTvsA, ΔX↔Y) that provide actionable diagnostic metrics.

**Medium Confidence**: The task-family classification and its diagnostic value for identifying reasoning bottlenecks is reasonable but requires empirical validation across diverse model architectures.

**Low Confidence**: The benchmark's ability to drive improvements in modality-invariant reasoning remains unproven, as the paper focuses on measurement rather than remediation strategies.

## Next Checks

1. **Real-world audio validation**: Test model performance on XModBench using naturally recorded audio rather than TTS-generated prompts to isolate whether performance gaps stem from audio modality itself versus TTS artifacts.

2. **Cross-dataset consistency**: Evaluate the same models on complementary benchmarks like Omni-SafetyBench and GIA-MIC to verify whether task-family performance patterns (particularly spatial/temporal weakness) persist across different evaluation paradigms and content domains.

3. **Open-ended extension**: Adapt XModBench questions to free-response format and measure how performance disparities and directional imbalances change when models must generate rather than select answers, testing whether multiple-choice format artificially constrains the observed phenomena.