---
ver: rpa2
title: Interpretable Non-linear Survival Analysis with Evolutionary Symbolic Regression
arxiv_id: '2504.05756'
source_url: https://arxiv.org/abs/2504.05756
tags:
- survival
- regression
- methods
- number
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an evolutionary multi-objective symbolic regression
  approach for survival regression. The method discovers mathematical expressions
  that model time-to-event data, balancing accuracy and interpretability.
---

# Interpretable Non-linear Survival Analysis with Evolutionary Symbolic Regression

## Quick Facts
- **arXiv ID**: 2504.05756
- **Source URL**: https://arxiv.org/abs/2504.05756
- **Reference count**: 40
- **Primary result**: Evolutionary multi-objective symbolic regression for survival analysis discovers interpretable non-linear models that outperform glass-box methods and match black-box accuracy

## Executive Summary
This work introduces an evolutionary multi-objective symbolic regression approach for survival regression that discovers mathematical expressions modeling time-to-event data. The method optimizes non-linear functions with multiple features simultaneously within a Cox model framework, balancing accuracy and interpretability. Empirical results on five real-world datasets show the proposed method consistently outperforms traditional glass-box methods in terms of accuracy per number of dimensions, while achieving comparable performance to black-box methods. The approach produces interpretable models that maintain competitive predictive power, offering a promising direction for transparent survival analysis in domains like healthcare and manufacturing.

## Method Summary
The method uses NSGA-II Genetic Programming to evolve mathematical expressions within a Cox proportional hazards framework. Each model contains 1-4 expression trees representing non-linear functions of patient features, with tree size capped at 7 nodes. The algorithm optimizes two objectives: maximizing Concordance Index (CI) and minimizing model dimensionality (number of features). Evolution proceeds through population initialization, fitness evaluation using partial likelihood, genetic operations (crossover and mutation), and selection based on Pareto dominance. The process includes a coordinate descent step to fit linear coefficients after structural changes, using primitives like +, -, ×, Square, ProtectedLog, and AQ operations.

## Key Results
- SR consistently outperforms traditional glass-box methods (Cox/Elastic Net, Survival Trees) in terms of accuracy per number of dimensions
- SR achieves comparable performance to black-box methods (Random Forest, Gradient Boosting) while maintaining interpretability
- The multi-expression formulation allows simultaneous optimization of non-linear functions within the Cox model, outperforming independent optimization approaches
- On all five datasets tested (PBC2, Support2, Framingham, BCM, BCR), SR models achieved higher or comparable CI values while using fewer features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Searching a space of mathematical expressions allows discovery of non-linear risk functions that remain human-readable, bridging the gap between rigid linear models and opaque black-boxes.
- **Mechanism:** The algorithm uses Genetic Programming to build expression trees from primitive operations (+, -, log, etc.). Instead of assuming a linear relationship, it iteratively recombines and mutates tree structures to find a functional form that minimizes survival error.
- **Core assumption:** The underlying survival dynamics can be approximated by relatively compact mathematical expressions composed of the provided primitive operations.
- **Evidence anchors:** The method discovers mathematical expressions that model time-to-event data... non-linear glass-box... allowing adaptability and interpretability; SR models can be non-linear, thus fitting the data with high accuracy, while also potentially interpretable.
- **Break condition:** If the data generating process is highly discontinuous or depends on thousands of weak interactions, the resulting expressions may become excessively large or fail to converge, breaking the interpretability constraint.

### Mechanism 2
- **Claim:** Optimizing non-linear expressions simultaneously within the Cox Partial Likelihood framework yields better feature interactions than optimizing basis functions independently.
- **Mechanism:** The system represents the hazard as $h_0(t) \exp(\sum \theta_j f_j(x))$. The evolutionary algorithm modifies the structure of the functions $f_j$ while a gradient-based optimizer fits the linear weights $\theta$.
- **Core assumption:** The interaction between features is significant enough that simultaneous optimization offers an advantage over independent univariate function fitting.
- **Evidence anchors:** We propose a multi-objective and multi-expression formulation where each non-linear function can take an arbitrary number of features, and is optimized simultaneously within the Cox model; contrasts with Wilstrup and Cave where functions are optimized independently.
- **Break condition:** If the correlation between features is extremely high, the simultaneous optimization might lead to unstable weight coefficients or redundant expressions.

### Mechanism 3
- **Claim:** Explicit multi-objective optimization regarding dimensionality forces the model to prioritize predictive efficiency, resulting in sparse, interpretable models.
- **Mechanism:** The algorithm employs NSGA-II to balance two objectives: maximizing Concordance Index and minimizing the number of distinct features.
- **Core assumption:** Dimensionality is a robust proxy for interpretability and cost in clinical/industrial settings.
- **Evidence anchors:** We set obj2 to the number dimensions... reducing the number of different patient features to be monitored can reduce costs; SR consistently outperforms traditional glass-box methods... in terms of accuracy per number of dimensions.
- **Break condition:** If the best model requires many features but each with a tiny non-linear effect, the dimensionality constraint might force the algorithm to drop subtle signals.

## Foundational Learning

- **Concept: Cox Proportional Hazards (PH) & Partial Likelihood**
  - **Why needed here:** This is the mathematical substrate. The "evolution" is actually trying to maximize the Partial Likelihood $L_p$. You cannot understand the fitness function or the final model form without grasping $h(t, x) = h_0(t)\exp(\dots)$.
  - **Quick check question:** Can you explain why the baseline hazard $h_0(t)$ cancels out in the Partial Likelihood estimation, allowing us to ignore it during the expression optimization?

- **Concept: Genetic Programming (GP) & Tree Representation**
  - **Why needed here:** The "non-linear functions" are not neural weights; they are trees of operations. You must understand tree crossover and mutation to debug why an expression is growing or failing to converge.
  - **Quick check question:** If you have an expression tree `(+ x1 (log x2))`, what happens during a "subtree mutation" versus a "node-level crossover"?

- **Concept: Multi-Objective Optimization (Pareto Fronts)**
  - **Why needed here:** There is no single "best" model in this architecture. The output is a set of trade-offs. You need to read a Pareto front to select the operating point (e.g., "I want max 3 features").
  - **Quick check question:** Looking at a plot of Accuracy vs. Complexity, how do you identify the "knee point" of the Pareto front?

## Architecture Onboarding

- **Component map:**
  - GP Engine -> Variation Operators -> Parameter Optimizer -> Fitness Evaluator -> NSGA-II Selector -> Population
  - Population -> Expression Trees -> Fitness Evaluation -> Pareto Front -> User Selection

- **Critical path:**
  1. **Initialization:** Random trees are generated.
  2. **Evaluation:** Likelihood is computed; $\theta$ is fitted.
  3. **Evolutionary Loop:** Selection → Crossover/Mutation → Re-evaluation → Survival Selection.
  4. **Extraction:** The final population is filtered to present the Pareto front to the user.

- **Design tradeoffs:**
  - **Protected vs. Naked Operations:** The paper uses `ProtectedLog` (adds small epsilon) to prevent crashes. This ensures stability but produces ugly expressions like $\log(|x| + 10^{-9})$, harming interpretability.
  - **Tree Size Constraint (Max 7 nodes):** Strictly limits bloat and aids interpretability but may prevent the discovery of complex, high-accuracy "laws" if the data is inherently complex.
  - **Dimensionality vs. Error Term:** Optimizing for fewer features (Cost) vs. CI (Accuracy).

- **Failure signatures:**
  - **Bloat/Redundancy:** The population fills with slight variations of the same expression (mitigated here by duplicate penalization).
  - **Numerical Instability:** If protections fail, `log(0)` or `div/0` yields `NaN` fitness.
  - **Overfitting:** High-dimensional models on the Pareto front show high Train CI but low Test CI.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the provided code on the PBC dataset. Visualize the Pareto front. Confirm that the SR front dominates the Cox/ElasticNet front for small dimensions ($k \leq 5$).
  2. **Interpretability Stress Test:** Extract the expression for a 3-dimensional model. Attempt to manually simplify it. Check if `ProtectedLog` obfuscates the relationship too much for human readability.
  3. **Ablation on Operations:** Remove the non-linear primitives (log, sqrt) and run again. Verify if the performance drops to linear Cox levels, proving the mechanism relies on non-linearity.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can symbolic regression (SR) approaches be developed to overcome the proportional hazards assumption in survival analysis?
  - **Basis in paper:** The conclusion explicitly states future work should consider "SR approaches that can overcome the proportional hazard assumption."
  - **Why unresolved:** The proposed method modifies the Cox model (Eq. 6) which fundamentally relies on the proportional hazards assumption to define the partial likelihood and risk set.
  - **What evidence would resolve it:** An SR implementation that successfully optimizes a different likelihood function (e.g., Accelerated Failure Time) without assuming hazard proportionality while maintaining predictive accuracy.

- **Open Question 2:** Can numerical stability be maintained in SR survival models without resorting to protected mathematical operations that degrade interpretability?
  - **Basis in paper:** Section 6.2 notes that "the use of protected operations... likely complicates interpretation," and the Conclusion calls for "methods to deal with numerical errors without resorting to unprotected operations."
  - **Why unresolved:** The current implementation uses `ProtectedLog` and `AQ` to prevent runtime errors, but these operations result in complex, non-standard mathematical expressions.
  - **What evidence would resolve it:** An SR variant that yields models using only standard mathematical operators while achieving equivalent numerical stability and concordance index performance.

- **Open Question 3:** Does incorporating explicit regularization or cross-validation into the evolutionary process reduce overfitting compared to the current dimensionality-based approach?
  - **Basis in paper:** The Conclusion suggests "the use of regularization and cross-validation techniques to prevent overfitting," while Section 6.2 notes specific cases where "simpler expressions obtain higher CI than higher dimensional ones."
  - **Why unresolved:** The current method minimizes model dimensionality as a proxy for simplicity but does not explicitly penalize validation error during the evolutionary training phase.
  - **What evidence would resolve it:** Empirical results demonstrating a reduction in the train-test performance gap when validation-based metrics are added as objectives.

## Limitations
- The use of protected operations like `ProtectedLog` and `AQ` prevents numerical crashes but generates mathematically inelegant expressions that hinder true interpretability
- The evaluation focuses on Concordance Index, which does not capture calibration performance—a critical metric in survival analysis
- The paper does not address how to handle time-varying covariates or competing risks, limiting applicability to more complex survival scenarios

## Confidence
- **High confidence**: The multi-objective framework for balancing accuracy and dimensionality is well-founded and the empirical results showing SR's dominance in the accuracy-per-dimension metric are robust across all five datasets
- **Medium confidence**: The claim that SR achieves "comparable performance to black-box methods" is supported but the absolute CI values show SR still trails RF and GB in some cases
- **Low confidence**: The assertion that evolved expressions are "interpretable" is questionable given the use of protected operations that introduce arbitrary constants

## Next Checks
1. Extract and manually simplify the highest-performing 3-feature expression from the PBC dataset; assess whether the protected operations create obfuscated rather than interpretable relationships
2. Implement a calibration metric (e.g., Brier score) to evaluate whether SR models maintain reliable probability estimates alongside ranking performance
3. Test the algorithm on a synthetic dataset with known non-linear interactions to verify it can recover the true underlying functional form when dimensionality is constrained