---
ver: rpa2
title: 'Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional
  Recurrent Attention for Long-Form ASR'
arxiv_id: '2506.19761'
source_url: https://arxiv.org/abs/2506.19761
tags:
- bidirectional
- long-form
- attention
- layers
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-form automatic speech
  recognition (ASR), where standard transformer models with multi-head attention (MHA)
  are inefficient due to quadratic complexity. The authors propose replacing MHA with
  bidirectional recurrent attention (RA) layers, specifically RWKV and Mamba-2, which
  have linear complexity and are more efficient.
---

# Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR

## Quick Facts
- arXiv ID: 2506.19761
- Source URL: https://arxiv.org/abs/2506.19761
- Reference count: 0
- Replaces quadratic-complexity MHA with linear RA for efficient long-form ASR

## Executive Summary
This paper addresses the computational inefficiency of transformer models with multi-head attention (MHA) for long-form automatic speech recognition (ASR), where quadratic complexity becomes prohibitive. The authors propose replacing MHA with bidirectional recurrent attention (RA) layers based on RWKV and Mamba-2 architectures, which achieve linear complexity while preserving accuracy. A novel Direction Dropout (DirDrop) regularization technique enables flexible inference modes, allowing a single model to serve both offline and streaming applications effectively.

The bidirectional RWKV-Conformer model achieves better accuracy than limited-context MHA while providing 44% higher throughput. Unidirectional RA matches limited-context MHA accuracy with 72% higher throughput. DirDrop enables one model to be used flexibly for both offline and streaming applications. Alternating direction decoding with DirDrop approaches bidirectional accuracy at unidirectional cost, offering insights into bidirectional information propagation.

## Method Summary
The method replaces multi-head attention in Conformer encoders with bidirectional recurrent attention layers (RWKV time-mixing or Mamba-2 blocks). Each RA layer processes sequences in both forward and backward directions with shared weights, averaging the outputs to provide bidirectional context. Direction Dropout randomly drops one direction during training, enabling models to perform well under unidirectional inference. The approach uses standard Conformer-Transducer architecture with CTC/RNN-T decoding, trained on GigaSpeech XL with short-form, long-form, and LFXL fine-tuning stages.

## Key Results
- Bidirectional RA matches MHA accuracy while providing 44% higher throughput
- Unidirectional RA matches limited-context MHA accuracy with 72% higher throughput
- DirDrop enables flexible inference modes (unidirectional, bidirectional, alternating) from a single trained model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Recurrent attention (RA) layers achieve linear complexity while preserving representational capacity comparable to MHA.
- **Mechanism:** RA layers compute outputs using a hidden state that aggregates history at each timestep, avoiding pairwise frame comparison matrix that causes MHA's quadratic scaling. Parallel algorithms enable GPU-efficient training while maintaining recurrent inference benefits.
- **Core assumption:** The compressed hidden state representation captures sufficient information from prior context to substitute for full attention matrices.
- **Evidence anchors:** [abstract] "bidirectional RA layers can match the accuracy of MHA for both short- and long-form applications"; [section 3.1] "RA layers differ from previous generation RNNs through the use of new algorithms for computing all of the outputs for one layer in parallel"
- **Break condition:** If hidden state dimension is insufficient for task complexity, long-range dependencies may degrade compared to full attention.

### Mechanism 2
- **Claim:** Bidirectional RA recovers the future-context advantage of MHA by processing reversed sequences with shared weights.
- **Mechanism:** Each layer maintains two direction-specific attention weight sets. The backward pass processes reversed input, outputs are reversed again, then averaged with forward pass outputs. This provides both past and future context per frame.
- **Core assumption:** Averaging forward and backward representations preserves complementary directional information without destructive interference.
- **Evidence anchors:** [abstract] "bidirectional RA layers can match the accuracy of MHA"; [section 3.4] "we add a second set of attention weights, which are used to process a reversed version of the input sequence"
- **Break condition:** If directional representations contain conflicting information, averaging may blur rather than enhance features.

### Mechanism 3
- **Claim:** Direction Dropout enables single-model flexibility for both streaming (unidirectional) and offline (bidirectional) inference.
- **Mechanism:** During training, DirDrop randomly drops one RA direction per layer with probability p. This prevents the model from depending critically on both directions, allowing inference with any subset of bidirectional layers. Two variants: DirDrop-R2L (drops backward only) and DirDrop-Both (drops either direction).
- **Core assumption:** Models can learn robust representations that tolerate missing directional information at inference time.
- **Evidence anchors:** [abstract] "Direction Dropout... enables a new alternating directions decoding mode"; [section 4 table 4] Standard bidirectional models "perform catastrophically when used for unidirectional inference"; DirDrop-R2L achieves 11.7% WER vs 14.9% for standard bidirectional in L2R mode
- **Break condition:** If dropout rate is too high, bidirectional inference accuracy degrades; if too low, unidirectional inference fails.

## Foundational Learning

- **Concept:** Multi-head attention complexity
  - **Why needed here:** Understanding why MHA fails for long sequences (O(n²) memory/time) motivates the RA replacement strategy.
  - **Quick check question:** For a 10-minute audio file at 100 frames/second, how does the attention matrix size compare between MHA and RA?

- **Concept:** Recurrent state-space models (SSMs) vs attention
  - **Why needed here:** Mamba-2 and RWKV use different mathematical formulations; choosing between them affects integration complexity.
  - **Quick check question:** What information does a single hidden state vector in RWKV carry vs. a full attention row in MHA?

- **Concept:** Conformer architecture
  - **Why needed here:** This paper modifies the attention component within Conformer blocks; understanding the convolution and feed-forward components clarifies what RA must complement.
  - **Quick check question:** Which Conformer components capture local vs. global dependencies, and which does RA replace?

## Architecture Onboarding

- **Component map:** Conformer encoder (12 layers) → RA blocks replace MHA → CTC projection + RNN-T decoder

- **Critical path:** Replace MHA with RWKV time-mixing (not full RWKV block) or full Mamba-2 block → Implement bidirectional pass with weight sharing → Add DirDrop regularization during training → Configure inference mode: unidirectional, bidirectional, or alternating

- **Design tradeoffs:** RWKV vs Mamba-2: Paper shows comparable accuracy; RWKV integrates more cleanly (time-mixing only) while Mamba-2 requires full block replacement; DirDrop-R2L vs DirDrop-Both: R2L preserves L2R streaming capability; Both enables symmetric R2L/L2R inference; Number of bidirectional layers at inference: Each bidirectional layer costs ~1 MPS throughput

- **Failure signatures:** Catastrophic WER degradation (14.9%→11.7% in paper) when using standard bidirectional training with unidirectional inference → use DirDrop; MHA throughput collapses at large chunk sizes (7 MPS at 20k frames) → RA maintains throughput; Training OOM on long utterances → use attention-only fine-tuning variant

- **First 3 experiments:** 1) Baseline comparison: Train identical Conformer with MHA vs. bi-RWKV on short-form data; verify WER parity (target: ~11.0%); 2) Long-form generalization: Decode both models at 40k frame chunks without long-form training; confirm RA maintains accuracy while MHA degrades; 3) DirDrop ablation: Train bi-RWKV with DirDrop-Both at 20% rate; test L2R, alternating, and bidirectional decoding to verify flexibility

## Open Questions the Paper Calls Out

- **Open Question 1:** Does training directly in alternating direction mode with DirDrop improve upon the current two-stage approach (bidirectional training then alternating decoding)? Current experiments train bidirectional models with DirDrop, then switch to alternating decoding at inference. End-to-end training in alternating mode has not been tested.

- **Open Question 2:** Can RA layers replace attention in the transducer decoder while maintaining or improving efficiency? This paper focuses exclusively on replacing MHA in the encoder; the decoder's bidirectional self-attention still uses quadratic attention.

- **Open Question 3:** Does cross-layer state passing improve RA-based ASR models as it has for RWKV LLM architectures? Current models compute hidden states independently per layer; sharing states across layers could improve information flow or efficiency but remains unexplored for ASR.

- **Open Question 4:** How effectively do RA layers address the speed and long-range context requirements of streaming ASR? This paper evaluates offline long-form ASR; streaming imposes latency constraints and requires causal processing, which may interact differently with bidirectional RA and DirDrop.

## Limitations

- Accuracy parity with MHA relies on averaging forward/backward representations, which may not generalize across all ASR tasks
- DirDrop requires careful hyperparameter tuning with limited ablation studies on dropout rates
- Evaluation focuses primarily on GigaSpeech with limited validation across diverse ASR datasets
- Alternating direction decoding potential not fully explored with comprehensive ablation studies

## Confidence

- **High Confidence:** RA complexity advantage and throughput measurements (O(n) vs O(n²) is mathematically proven; MPS measurements on specific hardware are reproducible)
- **Medium Confidence:** Bidirectional accuracy parity with MHA (supported by experiments but limited to one dataset and model configuration)
- **Medium Confidence:** DirDrop enabling flexible inference (strong experimental evidence but sensitive to hyperparameter choices)
- **Low Confidence:** Alternating direction decoding consistently approaching bidirectional accuracy (only briefly explored; no comprehensive ablation)

## Next Checks

1. **Generalization across datasets:** Replicate the bidirectional RA vs MHA comparison on Librispeech and other public ASR benchmarks to verify accuracy claims hold beyond GigaSpeech.

2. **DirDrop hyperparameter sensitivity:** Systematically vary DirDrop rates (10%, 20%, 30%, 40%) and test impact on both unidirectional and bidirectional inference accuracy to establish optimal tuning ranges.

3. **Alternating decoding efficiency:** Conduct comprehensive ablation varying the number of alternating direction layers and measure the accuracy-throughput tradeoff curve to determine if alternating decoding can consistently achieve 90%+ of bidirectional accuracy at unidirectional cost.