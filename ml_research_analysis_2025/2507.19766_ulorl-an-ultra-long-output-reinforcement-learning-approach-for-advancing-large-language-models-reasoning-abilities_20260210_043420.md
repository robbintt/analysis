---
ver: rpa2
title: UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large
  Language Models' Reasoning Abilities
arxiv_id: '2507.19766'
source_url: https://arxiv.org/abs/2507.19766
tags:
- training
- entropy
- segment
- arxiv
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently training large
  language models with ultra-long outputs for enhanced reasoning capabilities. The
  proposed Ultra-Long Output Reinforcement Learning (UloRL) approach tackles inefficiencies
  caused by long-tail sequence distributions and entropy collapse during training.
---

# UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities

## Quick Facts
- **arXiv ID:** 2507.19766
- **Source URL:** https://arxiv.org/abs/2507.19766
- **Reference count:** 6
- **Primary result:** Ultra-long output RL (128k tokens) improves mathematical reasoning, boosting AIME2025 performance from 70.9% to 85.1% and BeyondAIME from 50.7% to 61.9%.

## Executive Summary
This paper tackles the challenge of training large language models with ultra-long outputs for enhanced reasoning capabilities. The proposed UloRL framework addresses two key inefficiencies: long-tail sequence distributions causing training delays and entropy collapse from over-mastery of confident predictions. By introducing segment rollout with pseudo on-policy importance sampling, dynamic masking of mastered tokens, and a generative verifier for reward computation, UloRL achieves significant performance gains on mathematical reasoning benchmarks while maintaining training stability.

## Method Summary
UloRL implements segment rollout by dividing ultra-long decoding into shorter segments (e.g., 8 segments of 16k tokens each), allowing completed samples to enter the experience pool immediately while others continue. It uses pseudo on-policy importance sampling by treating all segments as if from the current policy, preventing clipping of diverse tokens. The method includes dynamic masking of well-mastered positive tokens (MPTs) when entropy drops below a threshold to prevent collapse. A generative verifier model computes binary rewards based on answer equivalence. The approach was evaluated on Qwen3-30B-A3B, demonstrating substantial improvements in mathematical reasoning performance.

## Key Results
- RL with 128k-token outputs improves AIME2025 performance from 70.9% to 85.1%
- Performance on BeyondAIME increases from 50.7% to 61.9%
- Outperforms larger models like Qwen3-235B-A22B despite using smaller architecture
- Training speed improved by 2.06× using segment rollout compared to full-length decoding

## Why This Works (Mechanism)

### Mechanism 1: Segment Rollout with Pseudo On-Policy Importance Sampling
Divides ultra-long decoding into short segments with immediate training to improve efficiency by mitigating delays from long-tail sequence distributions. Traditional RL requires all batch samples to complete decoding before training. In ultra-long scenarios, a minority of long-tail samples bottleneck training. Segment rollout divides decoding into stages, allowing completed samples to enter the experience pool immediately while others continue. POIS treats all segments as if from the current policy (importance weight=1), preventing clipping of diverse tokens. Efficiency gains depend on most batch samples finishing within early segments.

### Mechanism 2: Dynamic Masking of Well-Mastered Positive Tokens (DMMPTs)
Masks well-mastered positive tokens (MPTs) when model entropy drops below a threshold to prevent entropy collapse and maintain training stability. Entropy collapse occurs when the model overfits to tokens it predicts with high confidence in positive samples. Further updating MPTs sharpens the distribution, reducing entropy. DMMPTs dynamically excludes MPTs from loss when current entropy is below target entropy, avoiding excessive sharpening without auxiliary losses or clipping.

### Mechanism 3: Ultra-Long Output Length for Enhanced Reasoning
Increasing output length (up to 128k tokens) directly enhances reasoning capabilities, as measured by AIME2025. Extended sequences allow more complex reasoning chains, error correction, and deeper analysis. Feasibility is enabled by UloRL's efficiency and entropy control. Benefits of longer chains outweigh computational costs and error accumulation.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: UloRL's segment rollout and importance sampling build upon PPO principles adapted for ultra-long sequences. *Quick check:* How does the clipping threshold ε constrain policy updates in PPO, and how does POIS differ in handling importance weights?

- **Importance Sampling in RL**: Critical for understanding SAIS vs. POIS and why POIS sets weights to 1 to avoid clipping diverse tokens. *Quick check:* In standard off-policy RL, why is the ratio πθ/πθ_old calculated, and what happens when this ratio is clipped?

- **Entropy in Language Models**: Core to understanding entropy collapse and how DMMPTs maintains stable entropy via selective masking. *Quick check:* What does low entropy indicate about a model's output distribution, and why might this harm exploration during RL training?

## Architecture Onboarding

- **Component map:**
  - Prompts + unfinished pool -> Rollout (max segment length=16k)
  - Completed samples -> Experience pool; Unfinished -> Next iteration
  - For each sample in experience pool: Compute reward via Verifier
  - If reward=1, identify MPTs; Compute entropy; If <σ, mask MPTs
  - Calculate loss over unmasked tokens using POIS weights
  - Update model parameters

- **Critical path:**
  1. Prompts + unfinished pool → Rollout (max segment length=16k)
  2. Completed samples → Experience pool; Unfinished → Next iteration
  3. For each sample in experience pool: Compute reward via Verifier
  4. If reward=1, identify MPTs; Compute entropy; If <σ, mask MPTs
  5. Calculate loss over unmasked tokens using POIS weights
  6. Update model parameters

- **Design tradeoffs:**
  - More segments increase speed but reduce true on-policy data
  - Higher MPT threshold is conservative but may not prevent collapse
  - Target entropy requires tuning; too high prevents convergence, too low risks collapse
  - Binary rewards {0,1} simplify optimization but rely on accurate verifier
  - Truncated samples treated as incorrect (reward=0) vs. soft punishment

- **Failure signatures:**
  - Training stalls, low speed: Segment rollout not functioning; check if all samples become long-tail
  - Entropy collapses to near 0: DMMPTs not activating; check MPT threshold and mask application
  - Entropy explodes: DMMPTs over-masking or σ too high; verify target entropy and mask logic
  - Performance plateaus/degrades: POIS introducing bias; verify importance weight implementation

- **First 3 experiments:**
  1. Reproduce speedup: Run baseline (segment=1, 128k max) vs. segment=4 with SAIS. Measure time/step and verify ~2× speedup.
  2. Validate POIS vs. SAIS: Compare training dynamics (entropy curves, accuracy) of POIS vs. SAIS for fixed output length. Confirm POIS maintains higher entropy and better performance.
  3. Test DMMPTs impact: Run ablation with full UloRL vs. UloRL without DMMPTs. Monitor entropy stability and final AIME/BeyondAIME scores. Confirm performance drop when DMMPTs removed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increasing the output length beyond the 140k token limit continue to yield linear reasoning improvements, or is there a saturation point where performance plateaus or degrades?
- **Basis in paper:** Section 4.2 notes that extending output length to 140k achieved further improvements, leading authors to conclude that "continuously expanding the length can further enhance the model's reasoning ability."
- **Why unresolved:** The paper tests strict lengths (32k, 64k, 96k, 128k) and a 140k extension but does not investigate upper asymptotic limits or potential negative effects of extreme verbosity.
- **What evidence would resolve it:** Evaluation results from models trained with output constraints extended to 200k or 500k tokens on the same reasoning benchmarks.

### Open Question 2
- **Question:** Can UloRL's segment rollout and entropy masking strategies generalize effectively to reasoning domains with different structural constraints, such as code generation or logical deduction?
- **Basis in paper:** Experimental validation is exclusively conducted on mathematical reasoning benchmarks (AIME2025 and BeyondAIME), leaving performance on other verifiable tasks unstated.
- **Why unresolved:** Mathematical reasoning often rewards long chains of algebraic manipulation, whereas code generation requires strict syntax and logical conciseness, which might conflict with the "ultra-long output" objective.
- **What evidence would resolve it:** Benchmark results on code generation datasets (e.g., MBPP, HumanEval) or logical entailment tasks comparing UloRL against standard RL baselines.

### Open Question 3
- **Question:** How sensitive is the Dynamic Masking of Mastered Positive Tokens (DMMPTs) strategy to the specific hyperparameters of target entropy (σ) and probability threshold (τ)?
- **Basis in paper:** The method relies on a target entropy of 0.2 and a probability threshold of 0.99, which are fixed values in the experiments; the paper does not ablate these to show if they are optimal or task-specific.
- **Why unresolved:** Without sensitivity analysis, it is unclear if these values represent universal constants or if they require extensive tuning for different model architectures or task difficulties.
- **What evidence would resolve it:** An ablation study showing training dynamics and final accuracy when varying σ (e.g., 0.1 vs. 0.3) and τ (e.g., 0.95 vs. 0.999).

### Open Question 4
- **Question:** Does Pseudo On-Policy Importance Sampling (POIS) accumulate distributional drift or approximation errors over extended training horizons compared to Segment-Aware Importance Sampling (SAIS)?
- **Basis in paper:** Section 3.1.2 claims POIS outperforms SAIS by approximating on-policy data but admits that earlier segments are "pseudo on-policy," effectively ignoring the KL-divergence from older policies.
- **Why unresolved:** While short-term dynamics are shown, the long-term stability of ignoring importance weights of earlier segments is not theoretically proven or empirically stress-tested.
- **What evidence would resolve it:** A comparison of KL-divergence metrics and performance stability over 2x-3x the current training steps for POIS vs. SAIS.

## Limitations

- **Verifier Model Reliability:** The generative verifier's accuracy directly determines reward quality and thus learning outcomes, but architecture and training details are not disclosed.
- **POIS Bias Assumption:** POIS assumes off-policy segments from earlier iterations are close enough to the current policy to avoid bias, but long-term stability of this approximation is not proven.
- **Entropy Target Selection:** The target entropy σ=0.2 lacks justification for its specific value and sensitivity to different tasks or model scales is not explored.

## Confidence

**High Confidence:**
- Segment rollout with POIS improves training efficiency (2.06× speedup confirmed)
- Longer outputs correlate with better reasoning performance on AIME/BeyondAIME benchmarks
- DMMPTs prevents entropy collapse by masking over-mastered tokens

**Medium Confidence:**
- POIS maintains stable training dynamics compared to SAIS
- 128k output length is optimal for reasoning (diminishing returns beyond 128k not tested)
- Combination of all three mechanisms is necessary for peak performance

**Low Confidence:**
- Verifier model's accuracy and impact on final performance (no evaluation metrics provided)
- Whether 2.06× speedup holds for different batch sizes or sequence distributions
- Generalizability of DMMPTs to non-mathematical reasoning tasks or different model scales

## Next Checks

1. **Verifier Ablation Study:** Replace the generative verifier with a weaker model (e.g., smaller LLM or rule-based system) and measure degradation in AIME/BeyondAIME performance. Quantify the correlation between verifier accuracy and policy performance.

2. **POIS Bias Analysis:** Compare training trajectories using POIS vs. a hybrid approach (true on-policy for last segment, clipped importance weights for earlier segments). Measure final performance and entropy stability to assess if POIS's bias assumption holds.

3. **Entropy Target Sensitivity:** Run experiments varying σ (e.g., 0.1, 0.2, 0.3, 0.4) and τ (e.g., 0.98, 0.99, 0.995) across multiple runs. Plot final AIME performance vs. final entropy to identify optimal ranges and test robustness of DMMPTs to hyperparameter choice.