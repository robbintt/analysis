---
ver: rpa2
title: Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages
arxiv_id: '2506.09992'
source_url: https://arxiv.org/abs/2506.09992
tags:
- language
- toxic
- context
- zero-shot
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how large language models perform on toxic
  language detection in Serbian, Croatian, and Bosnian. We built a manually labeled
  dataset of 4,500 YouTube and TikTok comments and tested four models (GPT-3.5 Turbo,
  GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) in zero-shot and context-augmented modes.
---

# Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages

## Quick Facts
- arXiv ID: 2506.09992
- Source URL: https://arxiv.org/abs/2506.09992
- Reference count: 40
- Best F1 = 0.819 (Gemini 1.5 Pro context-augmented), best precision = 0.940 (GPT-4.1 zero-shot)

## Executive Summary
This study evaluates how large language models perform on toxic language detection in Serbian, Croatian, and Bosnian. We built a manually labeled dataset of 4,500 YouTube and TikTok comments and tested four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) in zero-shot and context-augmented modes. Context-augmented prompting improved recall by about 0.12 and F1 scores by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini 1.5 Pro context-augmented (F1 = 0.82, accuracy = 0.82), while zero-shot GPT-4.1 led in precision (0.940) with the lowest false positive rate. These results show that simple context inclusion can meaningfully improve toxic language detection in low-resource Balkan languages.

## Method Summary
The study collected 4,500 manually labeled comments from YouTube and TikTok across Serbian, Croatian, and Bosnian languages. Four large language models were tested in both zero-shot and context-augmented prompting modes with temperature=0 and max_tokens=10. Context snippets were manually crafted 2-sentence video descriptions. The evaluation measured precision, recall, F1, accuracy, and false positive rate. Manual annotation achieved Cohen's Kappa = 0.87 for reliability.

## Key Results
- Context-augmented prompting improved recall by ~0.12 and F1 by up to 0.10 across all models
- Gemini 1.5 Pro context-augmented achieved best overall balance (F1 = 0.82, accuracy = 0.82)
- Zero-shot GPT-4.1 achieved highest precision (0.940) with lowest false positive rate
- Context-augmentation cut false negatives by 331 comments for GPT-3.5 Turbo alone
- Models showed consistent performance improvements across all three Balkan languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-augmentation improves toxic language recall by grounding model interpretation in conversational setting.
- Mechanism: A short video description (2 sentences) provides semantic anchors (topic, figures, tone) that disambiguate slang, sarcasm, and cultural references. The model conditions its classification on this external context, reducing false negatives where toxicity depends on situational meaning.
- Core assumption: The model can integrate external context with comment text and adjust its classification boundary accordingly. Assumption: Context quality is sufficient to capture relevant signals.
- Evidence anchors:
  - [abstract] "Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10"
  - [section VI] "For GPT-3.5 Turbo alone, context-augmentation cut false negatives by 331 comments"
  - [corpus] Weak direct evidence. Neighbor papers on low-resource toxicity detection (e.g., UNITYAI-GUARD, Tulu OLI) do not evaluate context-augmented prompting specifically.
- Break condition: Context is irrelevant, misleading, or fails to capture the aspect of the video that comments actually reference (e.g., unexpected focus on hairstyle in a money-throwing video).

### Mechanism 2
- Claim: Zero-shot prompting yields higher precision but lower recall, while context-augmentation shifts the operating point toward higher sensitivity.
- Mechanism: Without context, models apply conservative, literal toxicity criteria—flagging only unambiguous insults. With context, models expand their classification boundary to include situationally toxic remarks, at the cost of more false positives.
- Core assumption: The precision-recall trade-off is fundamental to the prompting mode, not just model choice.
- Evidence anchors:
  - [abstract] "Zero-shot GPT-4.1 led in precision (0.940) with the lowest false positive rate"
  - [section IV-B] "GPT-4.1 zero-shot achieved a precision of 0.940... but its recall of 0.582 means it missed over forty percent of toxic remarks"
  - [corpus] No direct comparison in neighbor papers; most focus on fine-tuning or dataset creation rather than prompting strategies.
- Break condition: When context induces over-interpretation of benign remarks (e.g., sports metaphors misread as political attacks).

### Mechanism 3
- Claim: Linguistic complexity (code-switching, script variation, slang) introduces systematic detection gaps that context partially mitigates.
- Mechanism: Balkan languages exhibit morphological richness, Latin/Cyrillic alternation, and platform-specific slang. Models trained predominantly on high-resource languages lack robust representations for these phenomena. Context provides a compensatory signal.
- Core assumption: The model's multilingual pre-training includes sufficient exposure to these languages for context to help.
- Evidence anchors:
  - [section I-A] "The informal tone of comment sections, frequent code-switching between Latin and Cyrillic scripts, and the use of slang or sarcasm make harmful content harder to detect automatically"
  - [section IV-G] Context-augmentation improved F1 scores across Serbian, Bosnian, and Croatian, with Gemini 1.5 Pro context-augmented achieving 0.822, 0.806, and 0.830 respectively.
  - [corpus] Neighbor papers (e.g., SRBerta for Serbian legal texts, Tulu OLI) confirm that low-resource languages require domain-specific or language-specific adaptation.
- Break condition: Slang or sarcasm is too novel, culturally specific, or context-unrelated for the model to resolve even with context.

## Foundational Learning

- **Concept: Precision-Recall Trade-off in Moderation**
  - Why needed here: The paper explicitly evaluates models on both metrics; selecting a deployment strategy requires understanding which error type (false positive vs false negative) is more costly for your use case.
  - Quick check question: If missing a toxic comment could cause legal liability, but false alarms annoy users, which metric should you prioritize?

- **Concept: Zero-Shot vs Context-Augmented Prompting**
  - Why needed here: The study's core intervention is adding context; understanding how context changes model behavior is essential for applying this technique.
  - Quick check question: What information would you include in a context snippet for a political debate video to help a model detect toxic replies?

- **Concept: False Positive Rate (FPR) vs False Negative Rate (FNR)**
  - Why needed here: The paper reports both; FPR directly affects user experience (over-flagging), while FNR affects safety (under-detection).
  - Quick check question: In a high-traffic platform, would a 20% FPR on 10,000 daily comments be acceptable? What factors would influence your answer?

## Architecture Onboarding

- **Component map:** Data Collection -> Preprocessing -> Annotation -> Context Crafting -> Model Inference -> Evaluation
- **Critical path:** Context quality → Recall gains → F1 improvement. If context is irrelevant or incomplete, gains diminish or reverse (over-flagging).
- **Design tradeoffs:**
  - **Zero-shot vs Context-augmented**: Zero-shot for high-precision/low-cost; context-augmented for high-recall/higher-cost (30-40% more API tokens)
  - **Model selection**: Gemini 1.5 Pro context-augmented for best F1 balance; GPT-4.1 zero-shot for lowest FPR
  - **Manual vs automated context**: Manual is high-quality but unscalable; automated extraction (LLM summarization, topic modeling) is a future direction
- **Failure signatures:**
  - Context induces false positives: benign remarks misclassified when context suggests political/controversial framing (e.g., sports metaphors in political video contexts)
  - Context misses unexpected topics: comments focus on irrelevant details (hairstyle in money-throwing video) not captured in context snippet
  - Script/slang gaps: Cyrillic-only comments or novel slang may still be misclassified even with context
- **First 3 experiments:**
  1. **Baseline replication**: Run zero-shot and context-augmented prompts on a held-out subset (500 comments) using GPT-3.5 Turbo and Gemini 1.5 Pro. Compare F1 and FPR to paper benchmarks
  2. **Threshold calibration**: Vary the binary decision threshold (if using probability outputs or ensemble confidence) to find operating points that trade FPR for recall. Document the trade-off curve
  3. **Automated context generation**: Replace manual context with LLM-generated video summaries (e.g., from title + first 100 words of description). Compare performance to hand-crafted context to assess scalability

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does automated context extraction (e.g., LLM summarization) provide comparable or superior performance to manually crafted context snippets?
  - Basis in paper: [explicit] The authors state, "Future work should explore the trade-offs between these automated methods and human-authored context," noting that their manual approach is not scalable.
  - Why unresolved: The study relied exclusively on hand-written context, leaving the efficacy of dynamic, automated context generation untested.
  - What evidence would resolve it: A comparison of precision/recall metrics between models using automated summaries versus the manual context prompts used in this study.

- **Open Question 2**
  - Question: Can chain-of-thought (CoT) prompting reduce errors in detecting covert toxicity, such as sarcasm or coded insults?
  - Basis in paper: [explicit] The paper notes that "we do not experiment with CoT in this paper, but this suggests an important direction for future research" regarding nuanced toxicity.
  - Why unresolved: The current study utilized standard zero-shot and context-augmented prompts, which may struggle with covert toxicity that requires multi-step reasoning.
  - What evidence would resolve it: Evaluation results showing a reduction in false negatives for sarcastic or ironic comments when using CoT prompts compared to the standard prompting methods.

- **Open Question 3**
  - Question: Does fine-tuning lightweight adapter layers on the Balkan toxicity dataset yield better performance than zero-shot or context-augmented prompting?
  - Basis in paper: [explicit] The authors suggest, "Explore domain-specific fine-tuning of small adapter layers on top of base LLMs... That may yield further improvements with minimal compute."
  - Why unresolved: The paper focused on inference-time strategies (prompting) rather than parameter-efficient training methods.
  - What evidence would resolve it: Benchmark comparisons showing F1 scores of fine-tuned adapters against the Gemini 1.5 Pro context-augmented baseline.

## Limitations

- Dataset size of 4,500 comments remains relatively small for robust generalization across Balkan social media discourse
- Context snippets were manually crafted by researchers, making the approach unscalable for production deployment
- The model "GPT-4.1" mentioned is nonstandard and likely represents either GPT-4 Turbo or GPT-4o, creating potential reproducibility issues

## Confidence

- **High Confidence**: The core finding that context-augmentation improves recall by ~0.12 and F1 by up to 0.10 across multiple models and languages is well-supported by the reported results and consistent across experimental conditions.
- **Medium Confidence**: The precision-recall trade-off between zero-shot and context-augmented modes is demonstrated, but the exact operating points may shift with different context quality or model versions.
- **Low Confidence**: The scalability claims for context-augmented approaches are speculative, as the study uses manual context creation rather than automated methods that would be required for production systems.

## Next Checks

1. **Cross-Validation with Extended Dataset**: Test the same prompting strategies on an expanded dataset (e.g., 10,000+ comments) to verify that context-augmentation benefits persist and to better estimate performance variance.

2. **Automated Context Generation Evaluation**: Replace manual context snippets with LLM-generated summaries from video titles/descriptions and measure the performance degradation compared to the hand-crafted context results.

3. **False Positive Analysis on Out-of-Domain Data**: Evaluate models on comments from different Balkan language video genres (e.g., music, cooking, education) to quantify how often context-augmentation causes over-flagging of benign content in unrelated contexts.