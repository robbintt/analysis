---
ver: rpa2
title: On the Limits of Innate Planning in Large Language Models
arxiv_id: '2511.21591'
source_url: https://arxiv.org/abs/2511.21591
tags:
- move
- moves
- state
- slide
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the intrinsic planning and state-tracking
  capabilities of large language models using the 8-puzzle, a classic task that requires
  reasoning over evolving states and goal-directed planning. Four models are tested
  under three prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought)
  and with tiered feedback, followed by an external move validator condition that
  removes the need to determine valid moves.
---

# On the Limits of Innate Planning in Large Language Models

## Quick Facts
- arXiv ID: 2511.21591
- Source URL: https://arxiv.org/abs/2511.21591
- Reference count: 40
- Key outcome: LLMs fail to reliably plan and track states in the 8-puzzle, exhibiting invalid moves and loops even with feedback and simplified validation.

## Executive Summary
This paper investigates the intrinsic planning and state-tracking capabilities of large language models using the 8-puzzle, a classic task that requires reasoning over evolving states and goal-directed planning. Four models are tested under three prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered feedback, followed by an external move validator condition that removes the need to determine valid moves. Across all settings, models exhibit two dominant deficits: brittle internal state representations leading to frequent invalid moves, and weak heuristic planning resulting in loops or moves that do not reduce the distance to the goal state. Despite corrective feedback and simplified move validation, no model achieves reliable performance; success rates remain low, and successful runs are long and computationally expensive. These findings indicate that current LLMs have substantial limitations in planning without external tools, and further progress may require mechanisms for maintaining explicit state and performing structured search.

## Method Summary
The study employs the 8-puzzle as a benchmark to evaluate LLM planning capabilities. Four models (GPT-4o, Claude 3.5 Sonnet, Llama 3.1 70B, GPT-5-mini) are tested across three prompting strategies (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and three feedback tiers (none, task-level, step-level). An additional external move validator condition simplifies the task by removing the need to determine valid moves. Performance is measured across three metrics: success rate, solution length, and number of invalid moves. Each model-prompt-feedback combination is evaluated across 10 puzzle instances with 5 attempts each.

## Key Results
- All models exhibit brittle internal state representations, leading to frequent invalid moves even with external move validation
- Weak heuristic planning causes models to get stuck in loops or make moves that increase distance from the goal state
- Despite corrective feedback and simplified validation, success rates remain low and successful runs are computationally expensive

## Why This Works (Mechanism)
Assumption: The failure of LLMs in planning tasks appears to stem from their inability to maintain consistent state representations across sequential steps. The models seem to lose track of the puzzle configuration during reasoning, leading to invalid moves. This suggests that LLMs lack explicit memory mechanisms for tracking evolving states, relying instead on their context window and attention mechanisms which appear insufficient for this task. The brittleness in state tracking likely causes cascading errors where a single misstep propagates through subsequent reasoning.

## Foundational Learning
Unknown: The paper does not explicitly address foundational learning concepts related to how LLMs acquire planning capabilities during pretraining. The study focuses on evaluating existing models rather than examining their learning processes. It remains unclear whether planning abilities are learned during pretraining, require fine-tuning, or are fundamentally limited by the transformer architecture itself.

## Architecture Onboarding
Assumption: The paper does not provide detailed architecture onboarding information about the specific model implementations. Based on the results, we can infer that current transformer architectures lack explicit mechanisms for maintaining and updating state representations during planning tasks. The performance suggests that standard attention and context mechanisms are insufficient for tracking complex state transitions, indicating a potential architectural limitation for planning-oriented tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified planning and state-tracking deficits in the 8-puzzle domain generalize to other sequential decision-making tasks?
- Basis in paper: [explicit] The authors state in the Limitations section that examining only a single domain "limits our ability to draw broad conclusions about model performance in other settings."
- Why unresolved: The study isolates a specific puzzle environment; it remains unknown if these failure modes are universal or unique to the 8-puzzle's state space topology.
- What evidence would resolve it: Evaluating the same models on diverse planning benchmarks (e.g., Blocks World, Sokoban) to verify if the "brittle internal state representations" persist.

### Open Question 2
- Question: What specific architectural mechanisms can enable LLMs to perform structured search and maintain explicit state without external tools?
- Basis in paper: [explicit] The Conclusion suggests that "further progress may require mechanisms for maintaining explicit state and performing structured search" to overcome the observed failures.
- Why unresolved: The paper demonstrates the *absence* of these capabilities but does not propose or test specific architectural solutions to fix them.
- What evidence would resolve it: An interventional study integrating explicit state memory modules or search heuristics into LLMs and measuring the reduction in invalid moves and loops.

### Open Question 3
- Question: Why does increasing the specificity of feedback or prompting complexity degrade performance in certain models like GPT-5-mini?
- Basis in paper: [inferred] The authors note that for some models, "more informative feedback may not always be desirable," observing that CoT and AoT prompts caused confusion rather than aid.
- Why unresolved: The paper documents the performance drop but does not provide a theoretical or mechanistic explanation for why additional context harms these specific models.
- What evidence would resolve it: An analysis of attention patterns or feature attribution to determine if complex prompts distract smaller models from the base instructions.

## Limitations
- Single domain focus limits generalizability to other planning tasks
- Fixed 20-move limit may artificially constrain models that could succeed with additional steps
- Study focuses on four specific models, potentially missing broader LLM capabilities

## Confidence
- High confidence in models' inability to maintain valid state representations and avoid loops during planning
- Medium confidence regarding whether these limitations are intrinsic to LLMs or artifacts of prompting strategies
- Medium confidence in computational inefficiency conclusions due to lack of comparative benchmarks

## Next Checks
1. Test model performance on a continuous planning domain (e.g., pathfinding in a grid world with partial observability) to assess whether discrete puzzle limitations are representative of broader planning capabilities.

2. Implement a curriculum learning approach where models first master smaller puzzle instances before progressing to more complex ones, examining whether performance improves with staged training.

3. Compare LLM planning performance against a simple rule-based planner and a model-augmented planner (LLM generating heuristics for A* search) to establish baselines for computational efficiency and solution quality.