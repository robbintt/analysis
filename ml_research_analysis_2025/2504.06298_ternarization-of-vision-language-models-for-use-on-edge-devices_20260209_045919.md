---
ver: rpa2
title: Ternarization of Vision Language Models for use on edge devices
arxiv_id: '2504.06298'
source_url: https://arxiv.org/abs/2504.06298
tags:
- ternary
- weights
- quantization
- which
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a ternarization process to compress pre-trained
  Vision Language Models (VLMs) for deployment on edge devices. The approach uses
  a K-means-based initialization scheme to reduce quantization error and fine-tune
  ternary weights from pre-trained models.
---

# Ternarization of Vision Language Models for use on edge devices

## Quick Facts
- arXiv ID: 2504.06298
- Source URL: https://arxiv.org/abs/2504.06298
- Reference count: 23
- This work presents a ternarization process to compress pre-trained Vision Language Models (VLMs) for deployment on edge devices

## Executive Summary
This paper introduces a ternarization technique for compressing Vision Language Models (VLMs) to enable efficient deployment on edge devices with limited memory. The approach combines K-means-based initialization for ternary weights with straight-through estimator fine-tuning, achieving competitive perplexity while using 2× less memory and 2× faster token generation compared to 8-bit quantized baselines. The resulting model can run on devices with as little as 1GB of free memory, making it suitable for deployment on low-end smartphones while maintaining reasonable performance.

## Method Summary
The ternarization process involves three main stages: (1) K-means constrained initialization to set per-neuron scaling factors and latent weights, (2) 2-epoch fine-tuning with straight-through estimator and identity gradient, and (3) deployment with custom TFLite operators for ternary matrix multiplication. The method ternarizes Transformer blocks 2-23 while keeping the first/last layers, embeddings, and vision encoder in full precision or 8-bit quantization. A modified K-means algorithm with symmetric centroid constraints initializes weights to minimize quantization error, followed by QAT fine-tuning on the LLaVA conversation dataset.

## Key Results
- Ternary model achieves competitive perplexity (3.2 → 5.5) while using 2× less memory (0.78GB vs 1.64GB)
- Custom q2-matmul operator achieves 8.78 tokens/sec vs 3.59 for int8 baseline
- Model runs on devices with as little as 1GB free memory
- 2-epoch fine-tuning sufficient to reach convergence with K-means initialization

## Why This Works (Mechanism)

### Mechanism 1: K-means Constrained Initialization
- Claim: Initializing ternary weights via modified K-means reduces quantization error and accelerates fine-tuning convergence compared to naive rounding.
- Mechanism: The algorithm clusters original weights into three bins {−1, 0, +1} with two constraints: one centroid fixed at zero, remaining centroids symmetric (μ₁ = −μ₂). This minimizes Equation 1's objective by finding optimal per-neuron scaling factor s = 1/μ. The algorithm converges in ~10 iterations.
- Core assumption: Pre-trained weight distributions contain recoverable structure that can be preserved through symmetric clustering.
- Evidence anchors: [abstract] "A new initialization scheme from pre-trained weights based on the k-means algorithm is proposed to reduce the ternarization time." [Section III.A] "As expected of a K-means based algorithm, the average quantization error decreases with more iterations... reaches a plateau after 7 iterations."

### Mechanism 2: Straight-Through Estimator with Learned Per-Neuron Scaling
- Claim: Using identity STE with clipped latent weights and learned scaling parameters enables gradient-based fine-tuning despite non-differentiable quantization.
- Mechanism: Latent weights are quantized via staircase function Tern(x) at each forward pass. Identity function provides gradients during backprop. Per-neuron scaling parameter s is learned, effectively adapting thresholds. Weights clipped to [−1, 1] after each update.
- Core assumption: The identity STE provides sufficient gradient signal for ternary weight recovery within few epochs.
- Evidence anchors: [Section III.B] "training requires the use of a Straight-Through Estimator (STE) as in [8]. In this work, we choose the identity function as the STE." [Section V.B] "We find that training for 2 epochs is sufficient to reach convergence in our experiments."

### Mechanism 3: Custom Ternary Matrix Multiplication with Delayed Scaling
- Claim: Implementing fused ternary matrix multiplication operators with SIMD optimization enables 2× speedup and memory reduction versus 8-bit quantized baselines.
- Mechanism: Ternary weights packed as 2-bit values. Custom operator performs matrix multiplication using only additions/subtractions (no multiplies). Scaling applied to output vector rather than weight matrix, reducing floating-point operations. Multi-threaded SIMD (SSE/AVX) implementation on CPU.
- Core assumption: Edge device CPUs support sufficient SIMD width and threading to exploit ternary arithmetic benefits.
- Evidence anchors: [abstract] "Experimental results show the ternary model achieves competitive perplexity while using 2× less memory and 2× faster token generation compared to an 8-bit quantized baseline." [Table I] q2-matmul achieves 8.78 tokens/sec vs 3.59 for int8 baseline; memory 0.78GB vs 1.64GB.

## Foundational Learning

- Concept: **Quantization-Aware Training (QAT) vs Post-Training Quantization (PTQ)**
  - Why needed here: This work uses QAT (fine-tuning with calibration data) rather than PTQ (no training). Understanding this distinction clarifies why 2 epochs of fine-tuning are required.
  - Quick check question: If you quantized this model without any fine-tuning, would you expect higher or lower perplexity? Why?

- Concept: **Straight-Through Estimator (STE)**
  - Why needed here: The staircase function Tern(x) has zero gradient almost everywhere. STE provides proxy gradients enabling backpropagation through discrete quantization.
  - Quick check question: What happens to gradient flow if the STE function is the identity but weights are clipped to [−1, 1]?

- Concept: **SIMD Vectorization (SSE/AVX)**
  - Why needed here: Custom operators leverage SIMD instructions for parallel ternary operations. Understanding this helps diagnose why q2-matmul outperforms q2-unpack.
  - Quick check question: Why would addition/subtraction-only operations benefit more from SIMD than standard float32 matrix multiplication?

## Architecture Onboarding

- Component map:
  PyTorch training pipeline -> K-means initialization -> 2-epoch fine-tuning with STE -> latent weights + scaling parameters -> TensorFlow conversion -> TFLite with int8 residual quantization -> Custom operators (q2-tf, q2-unpack, q2-matmul) -> TFLite Interpreter on edge devices

- Critical path:
  1. Run Algorithm 1 per-neuron initialization (10 iterations sufficient)
  2. Fine-tune on calibration dataset (LLaVA conversation 58k, 2 epochs, lr=1e-3, cosine annealing)
  3. Convert to TFLite with int8 quantization for non-ternary layers (blocks 1, 24, embeddings, classifier)
  4. Deploy with custom q2-matmul operator for lowest memory/highest speed

- Design tradeoffs:
  - **Ternarize all layers vs skip first/last**: Paper skips blocks 1 and 24 to avoid early/late quantization error; trade-off is larger file size
  - **q2-unpack vs q2-matmul**: q2-unpack simpler but uses 6× more memory (5.18GB vs 0.78GB); q2-matmul requires custom kernel development
  - **2-bit quantization vs ternarization**: Ternarization uses only 3 of 4 possible 2-bit values but enables addition/subtraction-only computation

- Failure signatures:
  - **Loss spike during fine-tuning**: Binary model showed instability at ~2300 steps; ternary model more stable but monitor for sudden loss increases
  - **High inference memory with q2-unpack**: If memory exceeds 3GB, verify custom matmul operator is correctly invoked rather than unpack path
  - **Slow token generation (<1 t/s)**: Likely using q2-tf variant; switch to q2-matmul
  - **Perplexity >6.0**: Check that K-means initialization ran with sufficient iterations; verify fine-tuning convergence

- First 3 experiments:
  1. **Baseline validation**: Convert original Moondream 2 to TFLite with int8 quantization only; measure perplexity, memory, and tokens/sec to establish tf(qint8) baseline on your hardware.
  2. **Ablation on initialization iterations**: Ternarize with K-means iterations = {0, 5, 10, 20}; plot initial loss and perplexity after fine-tuning to confirm 10 iterations is sufficient for your target model.
  3. **Operator comparison**: Deploy all three ternary variants (q2-tf, q2-unpack, q2-matmul) on target edge device; measure actual memory usage and token generation speed to validate 2× claims under your deployment constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ternarizing the vision encoder component yield similar efficiency gains without significantly degrading visual understanding?
- Basis in paper: [explicit] The authors state, "In principle, nothing prevents us to apply the same ternarization procedure to the image encoder module," but they only ternarized the LLM.
- Why unresolved: The vision encoder was treated as a pre-processing step and excluded from the quantization experiments.
- What evidence would resolve it: Experiments applying the K-means initialization and fine-tuning to the ViT encoder, followed by evaluations on visual reasoning tasks.

### Open Question 2
- Question: Why does the cross-entropy loss initially increase during the first few iterations of the K-means initialization algorithm?
- Basis in paper: [explicit] The authors observe that "the loss first increases with the number of iterations, before decreasing again" and explicitly state, "We do not investigate this behavior further."
- Why unresolved: This non-monotonic behavior is counter-intuitive for an algorithm designed to minimize quantization error.
- What evidence would resolve it: A theoretical analysis or empirical tracking of weight distributions and centroid updates during the initialization phase.

### Open Question 3
- Question: How does the observed increase in perplexity (3.2 to 5.5) correlate with performance on downstream tasks like Visual Question Answering (VQA)?
- Basis in paper: [inferred] The paper evaluates "task performance" solely through perplexity on a conversation dataset, rather than measuring accuracy on the specific VLM tasks (captioning, grounding) mentioned in the introduction.
- Why unresolved: Perplexity is a proxy for language modeling quality but does not directly measure the model's factual accuracy or visual grounding capabilities.
- What evidence would resolve it: Benchmarking the ternarized model against the baseline on standard VLM benchmarks (e.g., VQAv2, GQA).

### Open Question 4
- Question: What inference speed improvements are achievable using specialized hardware (FPGA/ASIC) or GPU kernels compared to the CPU implementations presented?
- Basis in paper: [explicit] The authors suggest that "The best possible performance would theoretically be obtained on specialized hardware or Field-Programmable Gate Arrays (FPGAs)" but restrict implementation to CPU SIMD instructions.
- Why unresolved: The current bottleneck is the CPU architecture, which is not ideal for the parallel nature of ternary matrix multiplications.
- What evidence would resolve it: Deployment of the custom ternary operators on GPU or FPGA hardware to measure throughput and latency.

## Limitations

- Scaling behavior beyond Moondream 2: The approach's generalization to larger models remains uncertain, as the K-means initialization and 2-epoch fine-tuning may require adjustment for different architectures or training distributions.

- Hardware dependency of performance claims: The reported 2× speedup and memory reduction are measured on specific hardware with AVX/SSE support, and actual performance gains on mobile SoCs without these extensions could be significantly lower.

- Custom operator reproducibility: The exact SIMD implementation details are not fully specified, making the performance advantage of q2-matmul over q2-unpack dependent on low-level implementation details that may be difficult to reproduce.

## Confidence

**High confidence** in: The theoretical framework of STE-based ternary fine-tuning and K-means initialization for weight quantization. The mathematical foundations are well-established in the quantization literature.

**Medium confidence** in: The specific implementation details and performance numbers reported. While the methodology is sound, the exact configuration (learning rate, batch size, optimizer parameters) and custom operator performance require independent verification.

**Low confidence** in: The model's robustness on real-world edge devices with varying computational capabilities. The paper's evaluation focuses on controlled conditions without addressing deployment variability.

## Next Checks

1. **Cross-device performance validation**: Deploy the ternary model on at least three different edge devices (high-end smartphone, mid-range phone, and ARM-based SBC) to verify the 2× memory and speed claims hold across hardware variants.

2. **Long-sequence behavior testing**: Evaluate perplexity and memory usage on sequences longer than 4K tokens to identify potential degradation patterns that may emerge during extended inference sessions.

3. **Transfer learning capability assessment**: Fine-tune the ternary model on a downstream vision-language task different from the calibration dataset to evaluate whether the ternarization process preserves sufficient representational capacity for task adaptation.