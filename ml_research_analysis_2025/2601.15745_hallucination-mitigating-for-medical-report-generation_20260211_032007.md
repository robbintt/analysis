---
ver: rpa2
title: Hallucination Mitigating for Medical Report Generation
arxiv_id: '2601.15745'
source_url: https://arxiv.org/abs/2601.15745
tags:
- medical
- knowledge
- report
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of hallucination in medical report\
  \ generation from X-ray images, where large vision language models (LVLMs) often\
  \ generate plausible but inaccurate or fabricated content. The proposed Knowledge-Enhanced\
  \ with Fine-Grained Reinforced Rewards Medical Report Generation (KERM) framework\
  \ addresses this by integrating external medical knowledge from a curated corpus\
  \ using MedCLIP, followed by a purification module to ensure relevance to the patient\u2019\
  s clinical context."
---

# Hallucination Mitigating for Medical Report Generation

## Quick Facts
- arXiv ID: 2601.15745
- Source URL: https://arxiv.org/abs/2601.15745
- Reference count: 10
- Primary result: KERM reduces hallucinations in medical report generation by integrating external medical knowledge and fine-grained reward modeling.

## Executive Summary
This paper addresses the critical issue of hallucination in medical report generation from chest X-ray images, where large vision language models (LVLMs) often generate plausible but inaccurate content. The proposed Knowledge-Enhanced with Fine-Grained Reinforced Rewards Medical Report Generation (KERM) framework mitigates this by integrating external medical knowledge from a curated corpus using MedCLIP, followed by a purification module to ensure relevance to the patient's clinical context. A fine-grained reward model incorporating disease-level evaluation via CheXpert labels and sentence-level assessment through GPT-3.5 guides the model toward generating clinically accurate and relevant reports. Experimental results on IU-Xray and MIMIC-CXR datasets demonstrate significant improvements over state-of-the-art methods in both natural language generation and clinical efficacy metrics.

## Method Summary
KERM uses LLaVA-1.5-7b as the base LVLM, enhanced with external medical knowledge retrieval and reinforcement learning. The process begins with MedCLIP retrieving relevant medical facts from a 100k-sentence corpus, followed by a purification module that filters these based on clinical context. The LVLM generates reports conditioned on this refined knowledge. Training employs reinforcement learning with a combined reward function: disease-level accuracy measured by CheXpert F1 scores and sentence-level coherence evaluated by GPT-3.5. The model is fine-tuned with LoRA and trained for one epoch using AdamW optimization.

## Key Results
- KERM outperforms state-of-the-art methods on both IU-Xray and MIMIC-CXR datasets.
- Significant reduction in hallucinations as measured by clinical efficacy metrics.
- Improved alignment between generated reports and ground-truth annotations.
- Superior performance on both natural language generation (BLEU, METEOR, ROUGE-L) and clinical efficacy (CheXpert F1 scores) metrics.

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Grounded Input Refinement
The system uses MedCLIP to retrieve relevant "lesion fact sentences" from a 100k curated corpus. A purification module then filters these top-k results by comparing them against the patient's clinical history (indications) to ensure contextual relevance before feeding them into the LVLM. This grounds the language model, reducing the likelihood of fabricating descriptions.

### Mechanism 2: Diagnostic Alignment via Disease-Level Rewards
A reinforcement learning loop penalizes the model when the CheXpert labels extracted from the generated report differ from the ground truth labels. This acts as a hard constraint on diagnostic validity, forcing the model to prioritize correct disease identification over generic fluency.

### Mechanism 3: Semantic Consistency via AI Feedback
The model receives a secondary reward based on GPT-3.5's assessment of the generated report against the ground truth (0-1 score). This penalizes plausible but incorrect statements that might pass disease-level checks but fail on semantic nuance, steering the model toward professional medical phrasing and logical coherence.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: LVLMs struggle with rare medical conditions not well-represented in their pre-training data. RAG provides "open-book" access to a verified corpus.
  - Quick check question: Can you distinguish between the visual features extracted by MedCLIP and the text-based knowledge retrieved from the corpus?

- **Concept: Reinforcement Learning from AI Feedback (RLAIF)**
  - Why needed here: Standard cross-entropy loss treats all words equally, whereas medical reports require prioritizing diagnostic accuracy. RLAIF allows optimizing for non-differentiable metrics like F1 scores.
  - Quick check question: How does the "reward" signal propagate back to update the LVLM if the generation process is discrete?

- **Concept: Hallucination in Medical LVLMs**
  - Why needed here: In general NLP, hallucination is an annoyance; in medicine, it is a patient safety risk. Understanding why models confabulate (e.g., long-tail data scarcity) is key to this paper's design.
  - Quick check question: Does the paper define hallucination strictly as factual errors, or does it include plausible but unsupported statements?

## Architecture Onboarding

- **Component map**: Image + Clinical History -> MedCLIP (Retrieval) -> Purification Module (Re-ranking) -> LLaVA-1.5-7b (Visual Encoder + LLM) -> Report -> CheXpert Classifier (Disease Level) + GPT-3.5 (Sentence Level) -> Combined Rewards

- **Critical path**: 
  1. Embed Image via MedCLIP
  2. Retrieve Top-10 facts -> Purify to Top-5 -> Concatenate with Image
  3. LVLM generates Report
  4. Extract CheXpert labels from Report & Reference -> Calculate F1 Reward
  5. Score Report vs Reference with GPT-3.5 -> Calculate Semantic Reward
  6. Combine Rewards -> Update LVLM via Reinforcement Learning (Reinforce Algorithm)

- **Design tradeoffs**:
  - **Accuracy vs. Cost**: Using GPT-3.5 for reward calculation is expensive and slow compared to heuristic metrics, but offers higher semantic fidelity
  - **Retrieval vs. Parametric Knowledge**: Relying on the corpus helps with rare diseases but adds latency and storage requirements (100k facts)

- **Failure signatures**:
  - High CheXpert F1, Low Semantic Score: The model generates lists of keywords rather than coherent sentences
  - Low Retrieval Relevance: The purification module fails, passing generic facts that cause the LVLM to describe conditions not present in the image

- **First 3 experiments**:
  1. Retrieval Ablation: Disable the purification module and feed raw top-10 retrieved facts to the LVLM to measure the impact of noise on hallucination rates
  2. Reward Sensitivity: Vary the hyperparameter α (balancing disease vs. sentence rewards) to identify the tipping point where semantic fluency begins to degrade diagnostic accuracy
  3. Error Analysis: Run the model on the test set and manually categorize the remaining hallucinations to determine if they are visual misinterpretations or logical inconsistencies

## Open Questions the Paper Calls Out

- **Open Question 1**: How can evaluation metrics be developed to more comprehensively assess hallucinations in medical reports beyond standard NLG scores and CheXpert labels?
  - Basis: The authors state in the conclusion they plan to develop more comprehensive evaluation metrics.
  - Why unresolved: Current metrics may not fully capture the nuances of clinical accuracy or subtle hallucinations.
  - What evidence would resolve it: Validation of a new quantitative metric that correlates significantly better with expert radiologist assessment than existing metrics.

- **Open Question 2**: Does the KERM framework generalize effectively to medical imaging datasets or domains significantly different from chest X-rays?
  - Basis: Section 6 states the generalizability to other datasets or medical domains requires further investigation.
  - Why unresolved: Experimental validation is restricted to IU-Xray and MIMIC-CXR (chest radiology only).
  - What evidence would resolve it: Successful application and performance benchmarking on non-thoracic datasets without requiring complete overhaul.

- **Open Question 3**: To what extent does the reliance on GPT-3.5 for sentence-level reward modeling introduce biases or fail to penalize plausible but clinically inaccurate reasoning?
  - Basis: Section 6 notes the reward model's ability to capture medical subtleties is limited by its training data.
  - Why unresolved: The paper evaluates the final output but doesn't isolate the error rate or bias of the GPT-3.5 reward model itself.
  - What evidence would resolve it: Analysis of the reward model's precision/recall in identifying errors compared to ground-truth sentences annotated by human radiologists.

## Limitations

- The purification module's effectiveness depends heavily on the quality and coverage of the 100k-sentence corpus, with limited evidence for edge cases or rare conditions.
- Reliance on GPT-3.5 for semantic scoring may introduce bias, potentially prioritizing linguistic fluency over strict clinical accuracy.
- The generalizability of the approach to other medical imaging modalities or languages is not addressed in the current work.

## Confidence

- **High**: The retrieval and reinforcement learning framework is technically sound and well-documented
- **Medium**: The experimental results show strong performance gains, but the analysis of remaining hallucinations is limited
- **Low**: The generalizability of the approach to other medical imaging modalities or languages is not addressed

## Next Checks

1. Retrieval Ablation: Disable the purification module and feed raw top-10 retrieved facts to the LVLM to measure the impact of noise on hallucination rates
2. Reward Sensitivity: Vary the hyperparameter α (balancing disease vs. sentence rewards) to identify the tipping point where semantic fluency begins to degrade diagnostic accuracy
3. Error Analysis: Run the model on the test set and manually categorize the remaining hallucinations to determine if they are visual misinterpretations or logical inconsistencies