---
ver: rpa2
title: 'Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven
  Code Intelligence in LLMs'
arxiv_id: '2502.19411'
source_url: https://arxiv.org/abs/2502.19411
tags:
- code
- reasoning
- arxiv
- preprint
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically examines the bidirectional relationship
  between code and reasoning in large language models. Code provides a structured
  medium that enhances reasoning through verifiable execution paths, logical decomposition,
  and runtime validation, while improved reasoning capabilities transform code intelligence
  from basic completion to advanced software engineering tasks.
---

# Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs

## Quick Facts
- arXiv ID: 2502.19411
- Source URL: https://arxiv.org/abs/2502.19411
- Reference count: 35
- One-line primary result: Survey examining bidirectional relationship between code and reasoning in LLMs, covering code-enhanced reasoning and reasoning-driven code intelligence approaches.

## Executive Summary
This survey systematically examines the bidirectional relationship between code and reasoning in large language models. Code provides a structured medium that enhances reasoning through verifiable execution paths, logical decomposition, and runtime validation, while improved reasoning capabilities transform code intelligence from basic completion to advanced software engineering tasks. The paper identifies key challenges including interpretability, scalability, and evaluation while proposing future research directions.

The survey covers two main paradigms: code-enhanced reasoning (where code execution improves mathematical and logical reasoning) and reasoning-enhanced code intelligence (where improved reasoning transforms code generation and understanding). It traces the evolution from basic code completion to autonomous code agents and highlights the emergence of interactive programming as a bridge between static code generation and agentic systems.

## Method Summary
The survey provides a comprehensive taxonomy covering code-enhanced reasoning approaches (PaL, PoT, MathCoder, CoC) and reasoning-enhanced code intelligence (code generation, understanding, agents). It synthesizes findings from multiple benchmarks including GSM8K, MATH, SVAMP for math reasoning; HumanEval, MBPP, SWE-bench for code intelligence; and HotpotQA, LogiQA, DROP for logical reasoning. The methodology involves systematic literature review and categorization of approaches based on their integration of code and reasoning mechanisms.

## Key Results
- Code execution reduces arithmetic errors in mathematical reasoning by providing deterministic verification paths
- Explicit planning significantly improves complex code generation tasks through better task decomposition
- Training on code data enhances reasoning capabilities across mathematical, logical, and multilingual tasks
- Interactive programming forms a reasoning-driven optimization loop that leverages execution feedback for self-refinement

## Why This Works (Mechanism)

### Mechanism 1: Code as Structured Reasoning Scaffold
- Claim: Generating reasoning steps as executable code improves precision and reduces calculation errors compared to natural language chain-of-thought.
- Mechanism: Code enforces structured syntax and deterministic execution—models express reasoning as programs (variables, operations, control flow) that external interpreters execute, separating computational accuracy from semantic planning.
- Core assumption: The reasoning task can be decomposed into programmable operations; the model can translate intent into syntactically correct code.
- Evidence anchors:
  - [abstract] "code provides a structured medium... verifiable execution paths, logical decomposition, and runtime validation"
  - [section 2.1.1] PoT/PaL "transform numerical problem-solving into single-execution code generation tasks... providing a deterministic path to solutions while minimizing calculation errors"
  - [corpus] Limited direct corroboration; neighboring papers focus on reasoning strategies broadly but not code-as-scaffold specifically.
- Break condition: Tasks requiring commonsense, semantic interpretation, or abstract reasoning where purely executable representations are inadequate (section 4.1, "Blended Code-and-Language Reasoning").

### Mechanism 2: Execution Feedback Enables Iterative Self-Refinement
- Claim: Code's executable nature provides objective feedback that triggers new reasoning cycles for improvement.
- Mechanism: Generate code → execute → observe errors/traces → analyze → revise. This loop leverages deterministic runtime signals (error messages, test failures, unexpected outputs) to guide subsequent reasoning without human intervention.
- Core assumption: Models can interpret execution feedback accurately and map it to appropriate code modifications; error messages are sufficiently informative.
- Evidence anchors:
  - [section 3.2.3] "interactive programming forms a reasoning-driven optimization loop: models first reason to generate code for execution, then analyze execution results... ultimately reasoning about better solutions"
  - [section 2.1.2] REPL-based approaches "use execution results to guide natural language reasoning steps"
  - [corpus] "Verbal Process Supervision Elicits Better Coding Agents" (arXiv:2503.18494) discusses process supervision for coding agents, supporting iterative refinement—though focused on supervision signals rather than raw execution feedback.
- Break condition: When error feedback is ambiguous, when models lack debugging knowledge, or when issues span multiple files with complex dependencies (section 4.2, "Large-Scale Code Understanding").

### Mechanism 3: Reasoning Decomposition Improves Code Generation
- Claim: Explicit planning and task decomposition before implementation improves code correctness for complex tasks.
- Mechanism: Model generates high-level plan (natural language or pseudocode) → decompose into subtasks → implement each systematically → verify against plan. This separates algorithmic design from syntax generation.
- Core assumption: Complex tasks benefit from explicit planning; models can generate coherent plans that accurately reflect requirements.
- Evidence anchors:
  - [section 3.2.1] "Models adopt CoT reasoning as the core strategy, generating step-by-step thoughts before implementing code... generating natural language plans to guide implementation, ensuring alignment between intent and code logic"
  - [section 3.2.1] Self-Planning "decomposes the generation process into two distinct phases... this division facilitates improved handling of complex code generation tasks"
  - [corpus] "From System 1 to System 2: A Survey of Reasoning Large Language Models" (arXiv:2502.17419) discusses deliberative reasoning but doesn't specifically address code generation decomposition.
- Break condition: When planning overhead exceeds benefits for simple tasks; when plans become too abstract to guide concrete implementation effectively.

## Foundational Learning

- **Program-of-Thoughts (PoT) / Program-Aided Language Models (PaL)**:
  - Why needed here: These are the foundational techniques showing how code execution replaces error-prone mental arithmetic in LLMs. Understanding them is prerequisite to grasping MathCoder, CodePlan, and hybrid approaches.
  - Quick check question: Given a word problem, can you explain why expressing it as Python code and executing it reduces errors compared to step-by-step natural language reasoning?

- **Interactive Programming Paradigm**:
  - Why needed here: The paper positions interactive programming as the bridge between static code generation and autonomous agents. Methods like Self-Debugging, OpenCodeInterpreter, and CodeChain all build on this paradigm.
  - Quick check question: What specific advantage does code execution provide for self-refinement that natural language self-critique lacks?

- **Repository-Level Code Understanding**:
  - Why needed here: Section 3.3 and 4.2 emphasize that real-world software engineering requires reasoning across multiple files, dependencies, and long contexts—beyond single-function generation.
  - Quick check question: Why might a model that excels at HumanEval (single-function tasks) struggle with SWE-bench (repository-level issues)?

## Architecture Onboarding

- **Component map**:
  - Inference-time scaffolding: PaL/PoT prompts → code generation → interpreter execution → result injection back into reasoning
  - Training-time enhancement: Code-data augmentation (MARIO, POET) → preference modeling (CodePMP, SIAM) → mixed text-code training
  - Agentic systems: Planner agent → code generator → executor/tester → feedback analyzer → iterated refinement (CodeAct, OpenHands, SWE-agent)

- **Critical path**: For code-enhanced reasoning, the critical path is prompt design → code generation quality → interpreter reliability → error handling. For reasoning-enhanced code intelligence, it's task decomposition accuracy → implementation faithfulness → test coverage → feedback interpretability.

- **Design tradeoffs**:
  - **Single-execution vs. dynamic interleaving**: Single-execution (PoT/PaL) is simpler but less flexible; dynamic interleaving (MathCoder) handles complex tasks but requires careful mode-switching criteria.
  - **Agent complexity vs. reliability**: Multi-agent systems (HyperAgent) offer specialization but introduce coordination overhead; agent-free approaches (Agentless) achieve competitive results with simpler architectures.
  - **Code complexity in training**: Overly complex code may overwhelm learning; overly simple code may not capture reasoning depth (section 4.1, "Optimizing Code Data and Representations").

- **Failure signatures**:
  - Syntax errors in generated code: Model generates plausible reasoning but invalid code—indicates planning-reasoning gap.
  - Hallucinated execution results: Model fabricates interpreter outputs—indicates lack of actual execution integration.
  - Cascading errors in long-form generation: Small mistakes accumulate across multi-file projects—indicates weak cross-file coherence.
  - Agent stuck in refinement loops: Continuous "fixes" without progress—indicates poor error localization or feedback interpretation.

- **First 3 experiments**:
  1. **Baseline comparison**: Implement PaL prompting on GSM8K subset—compare accuracy and error types between direct CoT, code-only execution, and hybrid approaches. Document which problem types benefit most.
  2. **Execution feedback ablation**: Build minimal interactive coding loop (generate → execute → feed error back → regenerate). Measure improvement over single-pass generation on MBPP. Identify common unfixable error patterns.
  3. **Planning depth analysis**: Test Self-Planning-style decomposition on HumanEval vs. SWE-bench Lite. Quantify: (a) planning overhead, (b) correctness improvement, (c) correlation between plan detail and final code quality. Determine threshold where planning stops helping.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal level of code complexity for training data that maximizes reasoning enhancement in LLMs without overwhelming the model's learning capacity?
- **Basis in paper:** [explicit] Section 4.1 states, "Determining the optimal level of code complexity for enhancing reasoning remains an open problem." It notes that overly intricate code is hard to learn, while simplistic code may fail to capture necessary reasoning steps.
- **Why unresolved:** There is currently no systematic analysis quantifying the relationship between code complexity (e.g., cyclomatic complexity) and reasoning performance across different model scales.
- **What evidence would resolve it:** A study correlating specific code complexity metrics against reasoning benchmark performance, potentially using adaptive curricula that show peak efficiency at specific complexity thresholds.

### Open Question 2
- **Question:** How can systems dynamically determine the optimal split between structured code execution and free-form textual reasoning for tasks that are ambiguous or subjective?
- **Basis in paper:** [explicit] Section 4.1 identifies "Blended Code-and-Language Reasoning" as a challenge, asking, "A crucial challenge is deciding how to split reasoning processes between structured code... and free-form text."
- **Why unresolved:** Code-based reasoning often degrades performance on tasks requiring commonsense or semantic interpretation (e.g., evaluating humor), and current criteria for mode-switching are underdeveloped.
- **What evidence would resolve it:** The development of a hybrid architecture or "context-aware" mechanism that successfully identifies when to switch modalities, demonstrating superior performance on mixed-reasoning tasks compared to purely code-based or text-based approaches.

### Open Question 3
- **Question:** Do the reasoning capabilities of advanced models (e.g., O1, R1) inherently transfer to agent-based coding tasks, or do agent frameworks require fundamental architectural redesigns to leverage them?
- **Basis in paper:** [explicit] Section 4.2 asks, "Do reasoning-enhanced models inherently excel in agent-based tasks... or if models themselves need refinement to be more effective in agent environments?"
- **Why unresolved:** Reasoning models have shown limited improvements in agent tasks, possibly because existing agent frameworks were optimized for older, non-reasoning models.
- **What evidence would resolve it:** Empirical results showing that specific agent architectures designed for reasoning models significantly outperform standard frameworks on complex, real-world programming agent benchmarks (like SWE-bench).

### Open Question 4
- **Question:** How can a standardized code-based format be designed to enable LLMs to construct complete and sophisticated tool usage chains in complex working conditions?
- **Basis in paper:** [explicit] Section 4.1 notes, "The key question is how to design a standardized format that enables LLMs or agents to invoke available tools on a computer through automated code generation..."
- **Why unresolved:** Current LLMs typically use simple APIs for tool invocation, but the construction of sophisticated, complete tool chains for complex conditions remains an unsolved challenge.
- **What evidence would resolve it:** The proposal and adoption of a universal code format that allows agents to autonomously generate and execute complex tool chains (e.g., multi-step IDE operations) without manual intervention.

## Limitations
- Survey scope is taxonomical rather than experimental—no direct reproducibility of results without consulting original papers
- Heavy reliance on proprietary models (GPT-4, GPT-3.5, Claude-3) with unknown exact configurations
- Limited evaluation of cross-method comparisons on identical benchmarks
- Insufficient discussion of computational costs for interactive approaches

## Confidence

**High Confidence** (★★★☆☆):
- Code execution reduces arithmetic errors in mathematical reasoning (PoT/PaL mechanisms well-established)
- Explicit planning improves complex code generation tasks (Self-Planning findings consistent)
- Training on code data enhances reasoning capabilities across benchmarks (MARIO, POET results replicated)

**Medium Confidence** (★★☆☆☆):
- Interactive programming loops significantly improve code quality (few comparative studies)
- Agent-based systems generalize across software engineering tasks (limited real-world validation)
- Mixed text-code training optimal balance remains unclear (method-dependent)

**Low Confidence** (★☆☆☆☆):
- Large-scale repository reasoning efficacy (SWEBench Lite vs full SWE-bench discrepancies)
- Interpretability of code reasoning processes (evaluation metrics underdeveloped)
- Long-context multi-file reasoning scalability (technical challenges under-discussed)

## Next Checks

1. **Execution Feedback Efficacy**: Compare PaL vs CoT on GSM8K using identical base model and prompt structure, measuring both accuracy and error type distribution.

2. **Planning Overhead Threshold**: Systematically vary planning detail in Self-Planning on HumanEval, quantifying accuracy gains vs. generation time across task complexity levels.

3. **Multi-File Reasoning Scalability**: Test single-file models (HumanEval-optimized) on SWE-bench Lite subcomponents, measuring degradation patterns across dependency complexity and file count.