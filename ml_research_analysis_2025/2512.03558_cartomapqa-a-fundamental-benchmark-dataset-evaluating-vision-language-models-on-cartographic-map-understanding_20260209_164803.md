---
ver: rpa2
title: 'CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models
  on Cartographic Map Understanding'
arxiv_id: '2512.03558'
source_url: https://arxiv.org/abs/2512.03558
tags:
- task
- tasks
- route
- arxiv
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CartoMapQA is a benchmark dataset designed to evaluate large visual-language
  models on cartographic map understanding. It includes over 2,000 question-answer
  pairs across six hierarchically structured tasks: symbol recognition, feature extraction,
  scale interpretation, and route-based navigation.'
---

# CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding

## Quick Facts
- arXiv ID: 2512.03558
- Source URL: https://arxiv.org/abs/2512.03558
- Reference count: 40
- Benchmark for evaluating LVLM cartographic map understanding with over 2,000 QA pairs across six hierarchical tasks

## Executive Summary
CartoMapQA is a benchmark dataset designed to evaluate large visual-language models on cartographic map understanding. It includes over 2,000 question-answer pairs across six hierarchically structured tasks: symbol recognition, feature extraction, scale interpretation, and route-based navigation. The dataset uses real OpenStreetMap data and covers both open-ended and multiple-choice questions. Experiments on 15 models—including state-of-the-art proprietary and open-source LVLMs—reveal that while models can perform basic map feature recognition, they struggle with complex geospatial reasoning, scale measurement, and turn-by-turn navigation. Top models like Gemini 2.5 Pro and OpenAI o3 achieve limited success, with navigation tasks showing the largest gaps. The results highlight the need for better OCR, semantic understanding, and map-specific pretraining to advance robust geospatial reasoning in LVLMs.

## Method Summary
The CartoMapQA benchmark uses OpenStreetMap data to create 1,000+ question-answer pairs across six hierarchically structured tasks: Multiple Feature Selection (MFS), Spatially Targeted Map Feature (STMF), Route Length Estimation (RLE), Shortest Route Navigation (SRN), Symbol-to-Text Matching (STM), and Semantic Similarity Matching (SSM). Tasks progress from basic symbol recognition to complex navigation, with both open-ended and multiple-choice formats. The dataset evaluates 15 large vision-language models on their ability to interpret cartographic symbols, extract spatial features, measure distances using scale bars, and provide turn-by-turn navigation instructions.

## Key Results
- Models can identify basic map symbols but struggle with complex geospatial reasoning tasks
- Navigation tasks show largest performance gaps, with 26% of errors involving unconnected road transitions
- Scale interpretation improves with chain-of-thought prompting but still exhibits unit confusion errors
- OCR quality significantly impacts performance on feature extraction and navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical task decomposition enables diagnostic identification of failure sources in geospatial reasoning.
- **Mechanism**: The benchmark structures tasks from low-level (symbol recognition) to mid-level (feature extraction) to high-level (navigation). Performance at each level reveals whether failures stem from foundational perception gaps or reasoning deficits. Models failing MFS (symbol recognition) are unlikely to succeed at SRN (navigation), creating a causal diagnostic chain.
- **Core assumption**: Failure at lower levels propagates upward; independent high-level capability is unlikely without foundational competence.
- **Evidence anchors**:
  - [abstract] "These tasks span key low-, mid- and high-level map interpretation skills"
  - [section 1] "models that struggle with basic symbol identification are unlikely to succeed in measurement or navigation tasks"
  - [corpus] FRIEDA (2512.08016) confirms multi-step cartographic reasoning requires sequential skill composition, though doesn't validate the specific hierarchy here
- **Break condition**: If models can bypass symbol recognition via alternative pathways (e.g., textual reasoning alone), the hierarchical diagnostic value diminishes.

### Mechanism 2
- **Claim**: OCR quality acts as a bottleneck for map feature name-listing and navigation tasks.
- **Mechanism**: Cartographic maps encode spatial information through visual symbols AND textual labels. When OCR fails to correctly extract road names or POI labels, downstream tasks (name-listing, turn-by-turn navigation) inherit these errors. The paper identifies OCR problems as 17-78% of error sources depending on task.
- **Core assumption**: Text extraction is a necessary (not sufficient) condition for correct semantic association.
- **Evidence anchors**:
  - [abstract] "prone to Optical Character Recognition (OCR)-related errors"
  - [section 4.4] "OCR errors were far more prevalent" with examples showing misread labels
  - [corpus] ChartHal (2509.17481) documents similar OCR-hallucination coupling in chart understanding, supporting generalization
- **Break condition**: If future architectures use end-to-end visual reasoning without explicit text extraction, OCR as intermediate bottleneck becomes irrelevant.

### Mechanism 3
- **Claim**: Scale bar interpretation requires explicit numerical reasoning that current LVLMs execute unreliably.
- **Mechanism**: RLE task chains multiple steps: (1) detect scale bar, (2) read numerical value and unit, (3) measure path length visually, (4) compute real-world distance. Chain-of-thought prompting improved performance, suggesting the bottleneck is reasoning coordination, not single-step perception. Top model o1 reads scale bars correctly 84% of the time but still produces erroneous distance estimates.
- **Core assumption**: Scale understanding is decomposable into perceptual (reading) and cognitive (computing) sub-tasks.
- **Evidence anchors**:
  - [section 4.3] "the model demonstrates a solid ability to read the scale bar... in 84% of cases" but estimation remains error-prone
  - [section 4.3] CoT prompting improved RLE performance across most models
  - [corpus] FRIEDA explicitly tests multi-step cartographic reasoning with similar decomposition patterns
- **Break condition**: If models develop integrated scale-distance representations that bypass explicit measurement, the sequential decomposition becomes non-diagnostic.

## Foundational Learning

- **Concept**: Visual grounding in specialized domains
  - **Why needed here**: Cartographic maps use standardized but domain-specific symbol systems. Generic LVLM pre-training on natural images may not transfer to abstract map representations.
  - **Quick check question**: Can your model distinguish a "restaurant" symbol from a "fast food" symbol on an OpenStreetMap tile without prior exposure?

- **Concept**: Multi-hop reasoning chains
  - **Why needed here**: Navigation requires connecting: marker detection → road identification → direction inference → route planning. Each hop introduces failure probability.
  - **Quick check question**: Given a map with two markers, can your model output a valid route even if it cannot correctly name all intermediate roads?

- **Concept**: Spatial topology vs. visual topology
  - **Why needed here**: Roads that appear visually connected may not be drivable (one-way restrictions). The paper notes 26% of navigation errors involve "unconnected roads," suggesting visual topology is insufficient.
  - **Quick check question**: Does your model propose routes that respect traffic direction indicators, or only visual connectivity?

## Architecture Onboarding

- **Component map**: Input: Cartographic map image (1366×768px) + text prompt → Vision encoder → spatial feature extraction → OCR module → text label extraction → Fusion layer → bind visual features with textual labels → Reasoning module → task-specific inference → Output: Structured response

- **Critical path**: For navigation tasks, the path is: marker detection → road name extraction (OCR) → direction inference → route assembly. OCR errors on road names cascade directly into navigation failures.

- **Design tradeoffs**:
  - OpenStreetMap tiles provide consistent symbol sets but may not generalize to other map providers (Google Maps, Apple Maps use different conventions)
  - Chain-of-thought prompting improves RLE performance but increases inference cost and latency
  - Multiple-choice format (MFS only) enables automated evaluation but limits diagnostic depth vs. open-ended responses

- **Failure signatures**:
  - Undercounting: Models accurate for counts of 1-2 but underestimate higher counts (Figure 8)
  - Semantic confusion: Restaurant vs. fast-food, convenience store vs. supermarket (Figure 11)
  - Unconnected routes: 26% of navigation errors propose paths between non-adjacent roads
  - Scale unit confusion: 8% of o1 responses inappropriately convert to inches/centimeters

- **First 3 experiments**:
  1. **Baseline establishment**: Run the full 6-task benchmark on your model with zero-shot prompting. Report hierarchical accuracy to identify whether failures concentrate at specific levels.
  2. **OCR isolation test**: Manually provide ground-truth road names as text input alongside map images for SRN task. Compare route accuracy to vision-only condition to quantify OCR contribution to failure.
  3. **Scale bar ablation**: Test RLE performance with (a) scale bar visible, (b) scale bar cropped, (c) scale value provided as text. This isolates perceptual vs. reasoning components.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid architectures combining Large Vision-Language Models (LVLMs) with graph-based map parsers significantly improve topological awareness and success rates in turn-by-turn navigation tasks?
- Basis in paper: [explicit] Section 5 states that future research "might explore hybrid approaches combining LVLMs with graph-based map parsers" to address the finding that models currently lack "robust topological awareness."
- Why unresolved: Evaluation of the Shortest Route Navigation (SRN) task revealed that the most frequent error (26%) involved models suggesting transitions between physically unconnected roads.
- What evidence would resolve it: A performance comparison showing that a graph-augmented LVLM achieves higher connectivity scores and Shortest Route Success (SR²) rates compared to non-hybrid baselines.

### Open Question 2
- Question: Does specialized pretraining or the use of high-resolution vision encoders effectively reduce Optical Character Recognition (OCR) errors and semantic misrecognition in dense cartographic feature extraction?
- Basis in paper: [explicit] Section 4.4 suggests "enhancing the vision encoder, such as using a high-resolution or map-specific encoder" to address the prevalence of OCR errors and misclassification of semantically related features.
- Why unresolved: Error analysis showed that models frequently confuse visually or semantically similar map features (e.g., "restaurant" vs. "fast-food") and struggle with text labels, limiting reliability in the STMF task.
- What evidence would resolve it: A reduction in OCR-related error rates and an increase in average F1-scores for name-listing requests when using a map-specific encoder versus a general-purpose encoder.

### Open Question 3
- Question: Can domain-specific alignment strategies prevent LVLMs from defaulting to irrelevant physical units (e.g., inches or centimeters) when interpreting map scale bars for distance estimation?
- Basis in paper: [explicit] Section 4.4 notes that models occasionally "interpret the scale unit into inches or centimeters—units irrelevant to the context," suggesting a need for "specialized pretraining or alignment strategies."
- Why unresolved: Current models exhibit domain confusion, applying general visual knowledge inappropriate for cartographic contexts, which contributes to errors in the Route Length Estimation (RLE) task.
- What evidence would resolve it: Validation that a domain-aligned model maintains strict adherence to map units (meters/feet) and achieves a lower Mean Absolute Percentage Error (MAPE) in distance estimation compared to standard models.

## Limitations
- Domain specificity: Benchmark relies on OpenStreetMap's particular symbol set and visual conventions, may not generalize to other map providers
- Visual quality constraints: Fixed 1366×768 resolution may not capture fine-grained map details necessary for certain tasks
- Evaluation granularity: Multiple-choice format for MFS tasks sacrifices diagnostic depth compared to open-ended responses

## Confidence
- **High confidence**: Basic symbol recognition performance (MFS task) - models show clear capability to identify common map symbols like restaurants and hospitals
- **Medium confidence**: Navigation task results - while the 26% unconnected road errors are consistently observed, the fixed-route evaluation may not fully capture real-world navigation complexity
- **Medium confidence**: Scale interpretation findings - the improvement with chain-of-thought prompting is reproducible, but the sequential measurement approach may not reflect how humans naturally interpret map scales

## Next Checks
1. **Cross-provider generalization test**: Evaluate the same benchmark tasks using cartographic data from Google Maps and Apple Maps. Compare performance drops to isolate whether results depend on OpenStreetMap-specific visual conventions or represent more general geospatial reasoning capabilities.

2. **Resolution sensitivity analysis**: Systematically vary input resolution (e.g., 640×480, 1366×768, 2048×1536) and measure performance changes across all six tasks. This would quantify the visual detail threshold required for robust cartographic understanding and identify whether current performance limits are resolution-induced.

3. **Alternative evaluation protocol**: Implement a hybrid scoring system where MFS multiple-choice questions are supplemented with open-ended responses for a subset of items. This would enable finer-grained error analysis distinguishing between symbol misidentification and semantic confusion while maintaining automated evaluation efficiency for the majority of the benchmark.