---
ver: rpa2
title: 'Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic
  Review'
arxiv_id: '2509.12034'
source_url: https://arxiv.org/abs/2509.12034
tags:
- disaster
- systems
- human
- human-ai
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This systematic review analyzed 51 peer-reviewed studies to identify
  Human-AI use patterns in disaster scenarios across four main categories: Human-AI
  Decision Support Systems, Task and Resource Coordination, Trust and Transparency,
  and Simulation and Training. The review found that AI systems enhance situational
  awareness and decision-making efficiency through cognitive-augmented intelligence,
  real-time classification, and predictive modeling.'
---

# Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review

## Quick Facts
- **arXiv ID:** 2509.12034
- **Source URL:** https://arxiv.org/abs/2509.12034
- **Reference count:** 40
- **Primary result:** Review of 51 peer-reviewed studies identifies four Human-AI use patterns in disaster scenarios: Decision Support, Task Coordination, Trust & Transparency, and Simulation & Training.

## Executive Summary
This systematic review analyzed 51 peer-reviewed studies to identify Human-AI use patterns in disaster scenarios across four main categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. The review found that AI systems enhance situational awareness and decision-making efficiency through cognitive-augmented intelligence, real-time classification, and predictive modeling. For example, the AIDR platform achieved 80% AUC in classifying disaster-relevant tweets, and AI-driven drones demonstrated 25% faster decision-making and 40% improved search efficiency in simulations. The review also highlighted challenges in scalability, interpretability, and interoperability, emphasizing the need for adaptive, transparent, and context-aware Human-AI systems to improve disaster resilience.

## Method Summary
The study conducted a systematic literature review using PRISMA framework, searching ACM Digital Library, IEEE Xplore, and Google Scholar with specific keywords including "Human-AI Collaborations," "AI for real-time disaster response," and "Patterns in Human-AI use cases in Disaster." The review included 51 peer-reviewed studies meeting criteria for empirical evidence and decision-making relevance, excluding papers older than 10 years and purely theoretical discussions without practical application.

## Key Results
- AI systems improve situational awareness through cognitive-augmented intelligence and real-time classification (AIDR platform achieved 80% AUC in tweet classification)
- Human-AI coordination mechanisms enable 25% faster decision-making and 40% improved search efficiency in drone simulations
- Four main use patterns identified: Decision Support, Task Coordination, Trust & Transparency, and Simulation & Training
- Significant challenges exist in scalability, interpretability, and interoperability of Human-AI systems

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Augmentation via Iterative Labeling
- **Claim:** AI systems enhance decision accuracy in chaotic environments by filtering high-velocity data streams through human-in-the-loop (HITL) active learning, thereby reducing cognitive load.
- **Mechanism:** The system initially processes raw data (e.g., social media) using a basic model. Human annotators label a small subset, which the model uses to update its weights. This iterative loop allows the AI to handle the volume while humans handle the semantic nuance, preventing responder overload.
- **Core assumption:** Human annotators have sufficient context to accurately label data during the crisis event; data drift does not outpace the model's update speed.
- **Evidence anchors:**
  - Mentions AIDR platform achieving 80% AUC in classifying disaster-relevant tweets.
  - Describes AIDR leveraging active learning where human annotators label subsets iteratively to enhance accuracy.
  - *SwarmFusion* paper supports the broader concept of integrating deep learning with real-time data processing for efficient response.
- **Break condition:** If the disaster scenario evolves faster than the human labeling feedback loop can process (e.g., sudden infrastructure collapse changes communication patterns instantly), the model may hallucinate or misclassify critical data.

### Mechanism 2: Bias Mitigation via Adaptive Time Allocation
- **Claim:** Adjusting the time available for human validation based on AI confidence levels may mitigate cognitive anchoring bias (over-reliance on AI suggestions).
- **Mechanism:** In high-stakes settings, humans tend to anchor on the first AI suggestion. By enforcing longer review times when AI confidence is low (and shorter times when high), the system forces cognitive "System 2" thinking precisely when the AI is most likely to be wrong.
- **Core assumption:** Human operators will actually utilize the extra time to deliberate rather than simply waiting for the timer to expire; urgency stress does not override the protocol.
- **Evidence anchors:**
  - Rastogi et al. found disagreement rates rose from 48% to 67% when time was extended, proposing a confidence-based time allocation policy.
  - *Incentive-Tuning* paper suggests understanding incentives is key to empirical human-AI studies, implying time pressure is a critical variable in decision quality.
  - Highlights challenges in scalability and interpretability.
- **Break condition:** If the time-allocation logic is opaque to the user, they may perceive the system as lagging or unresponsive, leading to "alert fatigue" or abandonment of the tool.

### Mechanism 3: Supervisory Control via Multi-Agent MDPs
- **Claim:** Decentralized Multi-Agent Markov Decision Processes (MDPs) enable scalable resource coordination by shifting humans from direct operators to supervisory controllers.
- **Mechanism:** AI agents handle path planning and task assignment optimization (solving MDPs) for drones or robots. Humans are presented with a curated set of options or a supervisory dashboard, intervening only to correct ethical breaches or deadlocks, thus managing complexity through abstraction.
- **Core assumption:** The MDP model accurately captures the environment's constraints; the human supervisor retains sufficient situational awareness to catch errors in the AI's strategic logic.
- **Evidence anchors:**
  - Ramchurn et al. used AtomicOrchid simulation to affirm consolidating human decision-making with AI propositions, allowing commanders to adopt supervisory roles.
  - Vedanth et al. and Allen & Mazumder demonstrated UAV integration for autonomous detection and routing.
  - *Disaster Management in the Era of Agentic AI Systems* reinforces the shift toward collective human-machine intelligence and supervisory paradigms.
- **Break condition:** If communication latency disrupts the state synchronization between the AI agents and the human supervisor, the human's "veto" power may arrive too late to prevent hazardous actions.

## Foundational Learning

- **Concept: Cognitive Anchoring Bias**
  - **Why needed here:** The paper explicitly identifies this as a risk, where responders rely too heavily on initial AI outputs. Understanding this is prerequisite to designing the "time allocation" countermeasures discussed.
  - **Quick check question:** If an AI suggests a rescue path with 60% confidence and the human accepts it immediately without checking alternatives, is this an example of anchoring or efficient trust?

- **Concept: Active Learning (Human-in-the-Loop)**
  - **Why needed here:** Essential for understanding how systems like AIDR remain accurate in dynamic disaster contexts where pre-trained models fail due to lack of labeled data.
  - **Quick check question:** Does active learning require the human to label all incoming data, or just the examples the model finds most confusing?

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** Section III.B.2 cites Multi-agent MDPs as the mathematical backbone for coordinating complex resource logistics. Without this, the "Task Coordination" patterns are black boxes.
  - **Quick check question:** In a disaster MDP, what primarily defines the "Reward Function"â€”speed of response, safety of the agent, or successful resource delivery?

## Architecture Onboarding

- **Component map:** Data Ingestion -> AI Classification/Filtering -> Human Validation -> Action Dispatch
- **Critical path:** Data ingestion -> **AI Classification/Filtering** (Bottleneck: accuracy) -> **Human Validation** (Bottleneck: cognitive load) -> Action Dispatch
- **Design tradeoffs:**
  - **Speed vs. Accuracy:** Faster decisions often rely on "heuristic" AI, which may lack explainability
  - **Automation vs. Control:** Full automation improves efficiency but risks ethical violations without Human-in-the-Loop checks
- **Failure signatures:**
  - **Automation Bias:** Humans accepting AI suggestions even when visual evidence contradicts it
  - **Scalability Collapse:** System lag when input data exceeds the active learning loop's capacity to update the model
  - **Interoperability Failure:** Blockchain/UAV systems failing to handshake with legacy web platforms
- **First 3 experiments:**
  1. **Latency vs. Trust Test:** Measure decision accuracy and user trust when introducing artificial latency to simulate "thinking time" for low-confidence AI results
  2. **XAI Interpretability Stress Test:** Present emergency responders with standard AI outputs vs. XAI-enhanced outputs to measure time-to-decision and error rates under time pressure
  3. **Simulation Fidelity Check:** Run the "Disaster City Digital Twin" against historical disaster data to see if the multi-agent coordination logic identifies known bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Human-AI systems be designed to transfer effectively across different disaster types (e.g., floods vs. earthquakes vs. pandemics) without extensive retraining?
- **Basis in paper:** "many systems are tested in narrow contexts with specific disaster types, making cross-domain deployment challenging. Future systems should adopt modular, transferable designs capable of adapting to dynamic disaster environments."
- **Why unresolved:** Current systems lack transferability; reviewed studies focus on single disaster contexts rather than generalizable architectures.
- **What evidence would resolve it:** Empirical validation of a single Human-AI system performing reliably across at least three distinct disaster types with minimal domain-specific fine-tuning.

### Open Question 2
- **Question:** Which XAI explanation methods (e.g., Grad-CAM, nearest neighbor) actually improve human decision accuracy under cognitive load and time pressure in live disaster deployments?
- **Basis in paper:** "While explainable AI (XAI) methods exist, their real-world efficacy under cognitive load and stress is not fully understood. Future work should evaluate XAI techniques in live deployments and adapt explanations based on user expertise and situational urgency."
- **Why unresolved:** Existing XAI evaluations occur in controlled settings; the paper notes explanations "may also mislead users if not aligned with their mental models."
- **What evidence would resolve it:** Comparative field study measuring decision accuracy, response time, and trust calibration with different XAI methods under simulated time-critical disaster conditions.

### Open Question 3
- **Question:** What mechanisms enable dynamic control allocation between human supervisors and AI agents in multi-robot disaster missions without losing accountability?
- **Basis in paper:** "ensuring human oversight in AI-automated planning loops and modeling agent interdependencies remain key research frontiers in building adaptive and resilient coordination platforms."
- **Why unresolved:** Current human-in-the-loop systems use static oversight; the paper calls for "adaptive teaming frameworks that dynamically assign control."
- **What evidence would resolve it:** Prototype system demonstrating real-time control handoff with auditable decision logs, tested in multi-agent simulation with measurable accountability metrics.

## Limitations
- The systematic review methodology relies heavily on subjective categorization of patterns across heterogeneous disaster contexts, introducing potential interpretive bias
- The exact search string formulations remain incomplete, limiting reproducibility
- The review aggregates findings across diverse disaster types without explicitly addressing domain-specific applicability of the identified patterns

## Confidence
- **High Confidence:** The four-category framework and the core finding that AI systems improve situational awareness through cognitive augmentation. The AIDR platform's 80% AUC performance is explicitly documented.
- **Medium Confidence:** The generalizability of bias mitigation mechanisms across all disaster scenarios, as implementation details vary significantly between studies.
- **Low Confidence:** The scalability claims for multi-agent MDP coordination systems, as most cited evidence comes from controlled simulations rather than real-world deployments.

## Next Checks
1. **Pattern Applicability Test:** Select three recent disaster response operations and map the identified Human-AI patterns to documented decision-making workflows to verify practical relevance.
2. **Bias Mechanism Validation:** Design a controlled experiment comparing decision accuracy when using AI confidence-based time allocation versus fixed time windows in high-pressure scenarios.
3. **Interoperability Assessment:** Test the claimed scalability limits by simulating data ingestion rates exceeding the active learning loop capacity and measuring degradation in classification accuracy.