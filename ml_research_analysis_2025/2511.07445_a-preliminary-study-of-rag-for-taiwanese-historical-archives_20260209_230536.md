---
ver: rpa2
title: A Preliminary Study of RAG for Taiwanese Historical Archives
arxiv_id: '2511.07445'
source_url: https://arxiv.org/abs/2511.07445
tags:
- passage
- content
- metadata
- retrieval
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores Retrieval-Augmented Generation (RAG) applied
  to Taiwanese historical archives in Traditional Chinese, focusing on two datasets:
  Fort Zeelandia and Taiwan Provincial Council Gazette. The authors systematically
  investigate how query characteristics and metadata integration strategies affect
  retrieval quality, answer generation, and overall system performance.'
---

# A Preliminary Study of RAG for Taiwanese Historical Archives

## Quick Facts
- arXiv ID: 2511.07445
- Source URL: https://arxiv.org/abs/2511.07445
- Reference count: 40
- Key outcome: Metadata integration significantly improves retrieval effectiveness and answer accuracy for Taiwanese historical archives

## Executive Summary
This study systematically investigates Retrieval-Augmented Generation (RAG) applied to Taiwanese historical archives in Traditional Chinese. The authors focus on two datasets—Fort Zeelandia and Taiwan Provincial Council Gazette—and explore how query characteristics and metadata integration strategies affect retrieval quality, answer generation, and overall system performance. They find that early-stage metadata integration significantly improves both retrieval and answer accuracy, though challenges remain with hallucinations during generation and difficulties handling temporal or multi-hop historical queries.

## Method Summary
The study implements a four-stage RAG pipeline (Input → Retrieval → Generation → Evaluation) using two Taiwanese historical archives: Fort Zeelandia (5,443 passages, 173 queries) and TPCG (228,135 documents, 56 queries). Documents are chunked at 512 tokens with 128 overlap, and retrieval uses BM25, BGE-M3 dense embeddings, and hybrid RRF fusion (top-5 passages). Four metadata integration strategies are compared: Baseline, Metadata-Augmented Retrieval (appending metadata before embedding), Metadata-Only Reranking, and Metadata-Augmented Reranking. Generation uses GPT-4o with a specific prompt, and evaluation employs LLM-as-Judge (Gemini 2.5 Pro) assessing groundedness, relevance, and hallucination.

## Key Results
- Early-stage metadata integration improves both retrieval recall and answer accuracy, with Document/Content metadata providing the largest gains
- Retrieval recall is the primary bottleneck for complex historical queries, particularly multi-hop and temporal questions
- Hallucinations remain frequent during generation even when relevant passages are retrieved, indicating generation-stage limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Early-stage metadata integration improves retrieval effectiveness and downstream answer quality for historical archives.
- **Mechanism:** Appending structured metadata (e.g., dates, categories, personnel) to document chunks before embedding allows the retriever to encode these signals directly into passage representations. This enriches the semantic space, enabling more precise matching between queries and relevant documents, especially when key information is only weakly expressed in raw text.
- **Core assumption:** The metadata fields are relevant to the query and contain signals that aid in disambiguating or contextualizing the document content.
- **Evidence anchors:**
  - [abstract] "The results show that early-stage metadata integration enhances both retrieval and answer accuracy..."
  - [section 6.2] "Metadata-Augmented Retrieval proves the most reliable approach... For BM25, recall increases from 0.21 to 0.48."
  - [corpus] Related work on metadata-driven RAG for financial QA (arXiv:2510.24402) corroborates the utility of metadata in improving retrieval for structured, domain-specific documents.
- **Break condition:** If metadata fields are sparse, noisy, or irrelevant to the query type (e.g., Time/Event metadata for a query focused on content themes), performance gains may diminish or reverse, as seen with some metadata-only reranking strategies.

### Mechanism 2
- **Claim:** Retrieval quality, particularly recall, is a primary bottleneck for RAG performance on complex historical queries, directly influencing the generation of grounded answers.
- **Mechanism:** The generator LLM can only produce answers grounded in the provided context. Low recall means the relevant gold passage is missing, forcing the model to either rely on partial/incorrect retrieved passages (increasing hallucination risk) or default to internal knowledge (which may be incomplete or wrong for niche historical topics).
- **Core assumption:** The LLM lacks sufficient pre-existing knowledge about the specific historical details to answer correctly without external evidence.
- **Evidence anchors:**
  - [section 6.3] "For single-hop questions, Recall@5 scores are roughly twice as high as for multi-hop questions... increasing the likelihood of hallucinated responses."
  - [section 6.1] "hallucinations are frequent during generation, and temporal or multi-hop queries are particularly difficult because of the low retrieval recall."
  - [corpus] The DH-RAG paper (arXiv:2502.13847) highlights that dynamic historical context is crucial, implying that static retrieval failures on complex queries are a common RAG challenge in historical domains.
- **Break condition:** If the LLM is an expert model fine-tuned on the exact historical corpus, it may generate correct answers even with partial or no retrieval, decoupling retrieval recall from generation success.

### Mechanism 3
- **Claim:** Query characteristics (complexity and entity focus) create differential demands on the retrieval and reasoning capabilities of the RAG pipeline.
- **Mechanism:** **Multi-hop queries** require synthesizing facts from multiple passages; a retriever optimized for single-document relevance often fails to surface all necessary pieces. **Temporal queries** demand precise understanding of dates and chronological relationships, which lexical and dense retrievers struggle with. The LLM's generation performance then depends on its ability to compensate for retrieval gaps through multi-step reasoning.
- **Core assumption:** The off-the-shelf retriever (BM25, BGE-M3) and generator (GPT-4o) are not specialized for temporal or multi-hop reasoning over the specific corpus structure.
- **Evidence anchors:**
  - [section 6.3] "Time- and Multi-entity questions exhibit lower retrieval performance, with Recall@5 of 0.43 and 0.33, respectively, which aligns with increased hallucination."
  - [table 3] Shows multi-hop questions have worse hallucination scores despite higher potential groundedness/relevance when retrieval succeeds.
  - [corpus] WeatherArchive-Bench (arXiv:2510.05336) specifically benchmarks RAG on historical archives, finding temporal reasoning to be a key challenge, aligning with this study's findings.
- **Break condition:** If a dedicated multi-hop retriever or temporal reasoning module is integrated into the pipeline, performance on these query types could improve independently of the base retriever's recall.

## Foundational Learning

**Concept: Retrieval-Augmented Generation (RAG) Pipeline**
- Why needed here: This paper's entire analysis is structured around a four-stage RAG pipeline (Input → Retrieval → Generation → Evaluation). Understanding this baseline is essential to interpret where interventions (like metadata integration) are applied and why.
- Quick check question: In the paper's pipeline, where does the `Metadata-Augmented Retrieval` strategy intervene, and what does it modify?

**Concept: Retrieval Evaluation Metrics (Recall@k)**
- Why needed here: The paper uses Recall@5 as the primary metric to judge retrieval effectiveness. Understanding this metric is crucial to interpret figures like 3, 4, and 5, which show the retriever's performance limitations.
- Quick check question: According to Equation 2 in the paper, if a query has 3 relevant passages and the retriever returns 2 of them in the top 5, what is the Recall@5 for that query?

**Concept: Metadata Integration Strategies**
- Why needed here: The paper systematically compares four strategies. Grasping the difference between "augmented retrieval" (appending at embedding time) and "augmented reranking" (appending at scoring time) is key to understanding the results in Table 4 and the core finding about early-stage integration.
- Quick check question: Which strategy, `Metadata-Augmented Retrieval` or `Metadata-Augmented Reranking`, involves modifying the document representation *before* the initial retrieval step?

## Architecture Onboarding

**Component Map:**
Input: Datasets (Fort Zeelandia, TPCG) + Query Sets + Metadata Annotations
↓
Retrieval: Retriever (BM25/Dense/Hybrid) + Strategy (Baseline/Meta-Aug/Meta-Only/Meta-Aug-Rerank)
↓
Generation: LLM (GPT-4o) + Prompt + Retrieved Context
↓
Evaluation: LLM-as-Judge (Gemini 2.5 Pro) → Metrics (Groundedness, Relevance, Hallucination)

**Critical Path:** The path from `Query` → `Retriever` (with chosen `Strategy`) → `LLM`. The retrieval strategy is the primary lever for optimization, as its output directly gates the information available for generation.

**Design Tradeoffs:**
1. **Integration Stage:** Early (in embeddings) vs. late (in reranking). Early integration (Meta-Aug Retrieval) yields more consistent gains but permanently modifies the index. Late integration is more flexible but shows mixed results.
2. **Retriever Choice:** Sparse (BM25) vs. Dense (BGE-M3) vs. Hybrid. Hybrid via RRF performed best for Fort Zeelandia, but BM25 was fixed for TPCG metadata experiments.
3. **Metadata Selection:** Using Document/Content metadata (titles, abstracts) provided the largest gains over Time/Event or Person/Organization metadata.

**Failure Signatures:**
- **Low Recall on Complex Queries:** Multi-hop and temporal queries consistently show Recall@5 < 0.3, leading to generation failures.
- **Hallucination Despite Retrieval:** Even with the gold passage retrieved, GPT-4o hallucinates (e.g., Appendix A.5 example), indicating a generation-stage limitation.
- **Metadata Mismatch:** Applying the wrong metadata type for a query can hurt performance (e.g., Time/Event metadata for some queries reduced groundedness).

**First 3 Experiments:**
1. **Replicate the Core Finding:** Implement the TPCG retrieval experiment using BM25 with `Baseline` and `Metadata-Augmented Retrieval` (using Document/Content metadata). Measure Recall@5 to validate the paper's ~0.27 point improvement claim.
2. **Diagnose Temporal Failure:** Create a small set of temporal queries from the Fort Zeelandia dataset. Compare the Recall@5 of the hybrid retriever against a variant where date strings are normalized or enriched with explicit era tags. Hypothesis: Explicit temporal normalization may improve recall.
3. **Probe Generation Hallucination:** Take a subset of Fort Zeelandia queries where the gold passage is in the top-5. Run generation and evaluate specifically for hallucination using the paper's three-dimensional prompt. Analyze if hallucinations correlate with specific query types or retrieved passage noise.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary, domain-specific datasets (Fort Zeelandia, TPCG) with limited scale and Chinese language specificity, constraining generalizability
- Metadata integration assumes consistent quality and relevance of annotations, but annotation reliability and bias are not quantified
- LLM-as-judge evaluation introduces its own subjectivity and is not fully validated against human judgment

## Confidence
- **High Confidence:** Early-stage metadata integration (Mechanism 1) significantly improves retrieval and answer accuracy
- **Medium Confidence:** Retrieval recall is the primary bottleneck for complex queries (Mechanism 2)
- **Low Confidence:** Query characteristics (Mechanism 3) create differential demands

## Next Checks
1. **Metadata Quality Audit:** Measure inter-annotator agreement for metadata assignments in a subset of documents to quantify annotation reliability and its impact on retrieval performance
2. **Temporal Reasoning Module Test:** Implement and compare a baseline retriever against a variant with explicit date normalization/enhanced metadata to isolate the effect of temporal signal encoding
3. **Hallucination Attribution Study:** For cases where the gold passage is retrieved, perform a fine-grained error analysis to determine if hallucinations stem from noisy context, model bias, or inference errors, and test if prompt engineering reduces this