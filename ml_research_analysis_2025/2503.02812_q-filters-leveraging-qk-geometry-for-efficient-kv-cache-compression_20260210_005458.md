---
ver: rpa2
title: 'Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression'
arxiv_id: '2503.02812'
source_url: https://arxiv.org/abs/2503.02812
tags:
- q-filters
- compression
- cache
- attention
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Q-Filters is a training-free method for compressing the Key-Value
  cache in autoregressive language models. It exploits the geometry of Query and Key
  vectors to estimate which cache entries are most important without accessing attention
  weights.
---

# Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression

## Quick Facts
- arXiv ID: 2503.02812
- Source URL: https://arxiv.org/abs/2503.02812
- Authors: Nathan Godey; Alessio Devoto; Yu Zhao; Simone Scardapane; Pasquale Minervini; Éric de la Clergerie; Benoît Sagot
- Reference count: 16
- Primary result: Training-free KV cache compression using query-key geometry that achieves 99% accuracy on needle-in-a-haystack task with 32× compression

## Executive Summary
Q-Filters introduces a training-free method for compressing the Key-Value cache in autoregressive language models by exploiting the geometric relationship between Query and Key vectors. The approach projects Key vectors onto a single direction derived from the principal singular vector of Query representations, enabling efficient pruning of less relevant KV pairs without accessing attention weights. This makes Q-Filters compatible with FlashAttention and incurs minimal computational overhead. Experiments demonstrate consistent improvements over existing methods across language modeling and long-context retrieval tasks, achieving 99% accuracy on needle-in-a-haystack tasks with 32× compression while reducing generation perplexity drop by up to 65% compared to Streaming-LLM.

## Method Summary
Q-Filters extracts a context-agnostic projection direction from Query representations during a calibration phase, then uses this direction to score and prune KV cache entries during inference. The method computes the first right singular vector (v1) from Query vectors collected on a small calibration dataset, storing this as the Q-Filter per layer and head. During inference, KV pairs are scored by their dot product with the Q-Filter and pruned based on compression requirements. The approach requires no training, works with FlashAttention by avoiding attention weight materialization, and shows strong generalization across different datasets and languages. For needle-in-a-haystack tasks, the first two layers are preserved to maintain retrieval accuracy.

## Key Results
- Achieves 99% accuracy on needle-in-a-haystack task with 32× compression
- Reduces generation perplexity drop by up to 65% compared to Streaming-LLM
- Shows lower Time to First Token (TTFT) than SnapKV and Expected Attention at all compression factors
- Outperforms K-norm and Streaming-LLM baselines consistently across language modeling and long-context retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
Projecting Key vectors onto the principal singular direction of Query representations approximates expected attention logits. Query and Key distributions exhibit joint anisotropy—they share a preferred direction u_h. Keys with stronger projection onto this direction receive higher expected attention from future queries because the dot product ⟨Q_i, K_j⟩ is dominated by this component. The core assumption is that the anisotropic direction is approximately unidirectional (Observation 3.2 holds) and ϵ ≈ -1 for most causal LM heads.

### Mechanism 2
A context-agnostic projection direction suffices for importance scoring across diverse inputs. The principal singular vector v_1 extracted from Query representations on a small calibration dataset generalizes because the anisotropic structure is inherent to trained attention heads, not input-dependent. The calibration data needs to be representative of query distribution geometry, with ~1K-3K samples sufficient for stable SVD. High cosine similarity between Q-Filters computed on different datasets/languages supports this generalization.

### Mechanism 3
Avoiding attention weight materialization enables FlashAttention compatibility with negligible overhead. Computing a scalar projection ⟨K_t, v_1⟩ per token has O(d_H) complexity comparable to L2-norm computation, avoiding O(L) attention map reconstruction. Memory bandwidth (not FLOPs) is the bottleneck, and storing Q-Filters (l × n_H × d_H parameters) is negligible vs. KV cache.

## Foundational Learning

- **KV Cache memory scaling**: Understanding why compression matters—cache grows O(L) and dominates memory at long contexts, creating HBM-SM transfer bottlenecks during decoding.
  - Quick check: At 128K context with 70B model, what dominates memory: weights or KV cache?

- **SVD and principal components**: Q-Filters extract v_1 (first right singular vector) from concatenated Q representations; this captures maximum variance direction.
  - Quick check: Why does v_1 generalize while v_2, v_3... do not for attention estimation?

- **FlashAttention memory hierarchy**: FlashAttention fuses attention computation, never materializing full attention matrices—methods requiring A_ij scores must either modify kernels or accept recompute overhead.
  - Quick check: Can you read attention weights out of FlashAttention-2 without modifying its kernel?

## Architecture Onboarding

- **Component map**: Calibration phase (Sample documents → forward pass → collect Q_h activations → SVD per (layer, head) → store v_1^+ vectors) -> Inference phase (Normal generation until cache threshold → evict K-V pairs with lowest ⟨K_t, v_1^+⟩ scores → continue) -> GQA handling (Average Q-Filters across query group members before applying)

- **Critical path**: 1) Implement calibration pipeline (SVD extraction from ~3K Q samples) 2) Integrate eviction logic into KV cache manager 3) Verify FlashAttention compatibility (no attention weight access)

- **Design tradeoffs**: Higher compression (32×) vs. retrieval accuracy tradeoff steeper for NIAH multi-key tasks; Not compressing first 2 layers preserves retrieval (needle tasks); Streaming-LLM lower TTFT at low compression; Q-Filters superior at high compression

- **Failure signatures**: Qwen-2.5 shows diminished gains (QKV bias term interferes with geometry); OLMo-2 fails (QK-normalization removes anisotropy); Multi-hop retrieval degrades faster than single-needle retrieval at extreme compression

- **First 3 experiments**: 1) Replicate calibration on 100 samples vs. 3000 samples; measure perplexity gap to verify sample efficiency claim 2) Compare TTFT with and without FlashAttention enabled; confirm no hidden attention recompute overhead 3) Ablation: evict based on L2-norm vs. Q-Filter projection; quantify correlation with actual attention weights on held-out sequences

## Open Questions the Paper Calls Out

1. **Architecture adaptation**: How can Q-Filters be adapted for models using QK-normalization or additive QKV biases, where the geometric assumptions about anisotropic Q/K distributions may not hold? The paper notes unsuccessful experiments with Olmo-2 (QK-normalization) and reduced performance on Qwen-2.5 (QKV bias).

2. **Training dynamics**: What mechanism during training causes ϵ = −1 (negative alignment between Q and K drift directions) for the vast majority of heads in trained causal language models? The paper observes this empirically but doesn't investigate the underlying cause.

3. **Hybrid approaches**: Can Q-Filters be combined with attention-based compression methods (e.g., SnapKV) to achieve better performance-efficiency tradeoffs than either approach alone? The paper compares against SnapKV but doesn't explore hybrid scoring heuristics.

## Limitations

- **Architecture dependence**: Performance degrades significantly on models with QK-normalization or QKV bias terms that break the Query-Key anisotropy assumption
- **Distribution shift sensitivity**: Limited evidence about performance degradation when serving queries from substantially different domains than the calibration data
- **Long-context retrieval tradeoffs**: Incomplete characterization of failure modes at extreme compression factors beyond 32×

## Confidence

- **High confidence** in the core geometric insight: Well-supported by theoretical analysis and empirical validation across multiple datasets and languages
- **Medium confidence** in practical performance claims: Dependent on architecture-specific properties with some architectures showing significant degradation
- **Medium confidence** in computational efficiency claims: Theoretically compatible with FlashAttention but requires validation in production systems

## Next Checks

1. **Architecture generalization test**: Evaluate Q-Filters on diverse LLM architectures including models with QK-normalization, QKV bias, and different attention mechanisms. Measure correlation between architectural features and Q-Filters performance.

2. **Distribution shift robustness evaluation**: Perform calibration on one dataset and deployment on substantially different domains. Measure degradation in perplexity and retrieval accuracy to quantify robustness.

3. **Extreme compression boundary analysis**: Systematically test retrieval tasks at compression ratios beyond 32× to characterize failure modes and identify which task types maintain functionality at extreme compression.