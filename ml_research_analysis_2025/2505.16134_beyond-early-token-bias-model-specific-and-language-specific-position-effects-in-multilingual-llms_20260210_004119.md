---
ver: rpa2
title: 'Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects
  in Multilingual LLMs'
arxiv_id: '2505.16134'
source_url: https://arxiv.org/abs/2505.16134
tags:
- position
- bias
- language
- middle
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates position bias in multilingual LLMs across
  five typologically diverse languages (English, Russian, German, Hindi, Vietnamese)
  and five model architectures. Through 450,000 evaluated question-answer pairs, the
  study reveals that position bias is primarily model-driven rather than language-driven,
  with notable exceptions: Qwen2.5-7B, DeepSeek-7B, and Mistral-7B exhibit strong
  late-position preference contrary to the assumed early-token bias, while Llama3.1-8B
  favors early positions.'
---

# Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs

## Quick Facts
- **arXiv ID**: 2505.16134
- **Source URL**: https://arxiv.org/abs/2505.16134
- **Reference count**: 40
- **Key outcome**: Position bias in multilingual LLMs is primarily model-driven rather than language-driven, with some models showing strong late-position preference contrary to assumed early-token bias.

## Executive Summary
This paper investigates position bias in multilingual large language models across five typologically diverse languages and five model architectures. Through 450,000 evaluated question-answer pairs, the study reveals that position bias patterns are primarily driven by model architecture rather than language properties, with notable exceptions challenging conventional wisdom about early-token preference. The research also uncovers that explicit positional guidance consistently degrades accuracy across all languages, and that models remain confidently wrong when relevant context appears in middle positions. These findings have significant implications for retrieval-augmented generation systems, prompt engineering practices, and uncertainty quantification approaches.

## Method Summary
The study evaluates position bias by placing relevant context at different positions (TOP/MIDDLE/BOTTOM) among distractor contexts for 2,000 QA pairs per language across five languages (English, Russian, German, Hindi, Vietnamese). The experiments use five model architectures with three scoring strategies (Aligned/All Zero/No Scores) and default 5 contexts per question, testing with 5, 10, and 15 contexts. Accuracy is measured via LLM-as-Judge (Mistral Large2) with binary correctness, and predictive entropy is computed from token log-probabilities using deterministic generation settings.

## Key Results
- Position bias is primarily model-driven, with Qwen2.5-7B, DeepSeek-7B, and Mistral-7B showing strong late-position preference contrary to assumed early-token bias
- Explicitly instructing models about correct context placement consistently degrades accuracy across all languages, challenging standard prompt-engineering practices
- Accuracy drops most when relevant context appears in the middle position, but this is not accompanied by increased output entropy, indicating models remain confidently wrong

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position bias is primarily driven by model architecture and training choices, not language-specific properties
- Mechanism: Different transformer architectures create distinct attention distribution patterns; some models develop late-position preferences due to variations in causal masking, training data ordering, and attention mechanisms
- Core assumption: Position bias patterns are stable within a model across languages, with language effects being secondary
- Evidence anchors: [abstract] DeepSeek-7B, Mistral-7B, and Qwen2.5-7B favor late positions challenging universal early-token preference; [section 6.3] Positional bias is primarily model-driven, with discrepancies more pronounced between models than across languages

### Mechanism 2
- Claim: Explicit positional guidance degrades accuracy across all tested languages
- Mechanism: Explicit relevance scores interfere with learned attention patterns or cause over-attending to score metadata rather than semantic content
- Core assumption: Degradation is caused by interference between explicit guidance and internal relevance computation
- Evidence anchors: [abstract] Instructing models that "most relevant context is marked as 1" reduces accuracy across all languages; [section 6.4] No Scores consistently outperforms other strategies with 0.1%-10.2% accuracy drop for languages

### Mechanism 3
- Claim: Middle-position accuracy drop is not accompanied by increased entropy, indicating confident failures
- Mechanism: Position bias causes attention homogenization—tokens in middle receive distributed attention rather than focused attention on relevant content, leading to confident but incorrect responses
- Core assumption: Low predictive entropy correlates with model confidence
- Evidence anchors: [abstract] Accuracy drops most in middle position without corresponding entropy increase; [section 6.5] Highest predictive entropy is not always associated with middle position

## Foundational Learning

- **U-shaped attention patterns in transformers**: Why needed here - The paper builds on prior work showing transformers prioritize extremal positions; understanding this baseline is essential for interpreting why some models show reversed patterns. Quick check: Would a standard U-shaped pattern assign highest attention to tokens 1 and 10, or tokens 4-6 in a 10-token sequence?

- **Predictive entropy as uncertainty quantification**: Why needed here - The paper uses average predictive entropy to measure model uncertainty; without understanding that low entropy = high confidence, the finding that models are "confidently wrong" cannot be interpreted. Quick check: If a model assigns probability 0.95 to one token and 0.05 to others, is its entropy high or low compared to uniform distribution?

- **Lost in the Middle phenomenon**: Why needed here - The paper confirms this phenomenon (lowest accuracy when relevant context is in middle) across multilingual settings, but finds it doesn't correlate with entropy increases—a key deviation from expected behavior. Quick check: In a RAG system with 5 retrieved documents, where should you NOT place the most relevant document if you want to maximize the chance of it being used?

## Architecture Onboarding

- **Component map**: Input layer (Question + N contexts with optional relevance scores) -> Position configurations (TOP/MIDDLE/BOTTOM) -> Scoring strategies (Aligned/All Zero/No Scores) -> Evaluation pipeline (LLM generation -> Mistral-Large2 judge -> accuracy/entropy metrics)

- **Critical path**: 1. Identify which model family you're deploying (late-bias: Qwen/DeepSeek/Mistral vs. early-bias: Llama/Gemma) 2. For RAG systems: If using late-bias models, place retrieved documents at END of context; for early-bias models, place at START 3. AVOID explicit relevance scoring in prompts—it consistently degrades performance 4. Monitor both accuracy AND entropy; low entropy + low accuracy = position bias likely active

- **Design tradeoffs**: Explicit guidance vs. implicit attention (paper shows explicit relevance scores hurt performance; No Scores works better), Context volume vs. position sensitivity (more contexts can exacerbate position bias for some models), Language coverage vs. reliability (lower-resource languages show larger performance gaps)

- **Failure signatures**: Accuracy drops significantly when relevant context is in MIDDLE position (confirmed across all models), Explicit relevance scoring causes 1.4%-25.4% accuracy degradation depending on model, Low entropy despite low accuracy indicates confident failures (not uncertainty that could trigger fallback mechanisms), DeepSeek-7B fails entirely on Hindi (excluded from analysis)

- **First 3 experiments**: 1. Position bias probe for your model: Place ground-truth-relevant context at TOP/MIDDLE/BOTTOM positions with random distractors; measure accuracy drop at middle to quantify model-specific bias magnitude 2. Scoring strategy ablation: Compare No Scores vs. Aligned scoring on your target language(s); if No Scores wins, remove explicit guidance from production prompts 3. Entropy-confidence audit: For failed predictions, check if entropy is low (indicating position bias) vs. high (indicating genuine uncertainty); use this to calibrate confidence thresholds for fallback systems

## Open Questions the Paper Calls Out

- What specific architectural mechanisms or training data properties cause certain models (e.g., Qwen2.5, DeepSeek) to favor late positions, contradicting the theoretical "early-token bias" found in Transformers? The authors speculate these differences stem from variations in training data and model architecture, but the specific causal variables remain unidentified.

- Why does explicit relevance guidance degrade accuracy when distractors are random, and does this effect persist or reverse when distractors are semantically relevant? The paper notes their findings question standard prompt-engineering because they used "random distractors" whereas prior work used "semantically relevant distractors," suggesting the distractor type fundamentally changes the mechanism.

- How can uncertainty quantification methods be adapted to detect positional disadvantage when output entropy remains low despite high error rates? The paper concludes that "uncertainty estimates can be token-biased" and the observed disconnect "complicates uncertainty-based bias mitigation techniques," breaking the assumption that errors correlate with high predictive entropy.

## Limitations

- The analysis excludes Hindi and Vietnamese for DeepSeek-7B due to systematic failures, raising questions about model-specific language coverage
- The reliance on an LLM judge (Mistral Large2) introduces potential evaluation bias, though human validation shows reasonable correlation (r=0.716)
- The experiments use relatively small context windows (5 contexts default) compared to production RAG systems, and the distractor sampling procedure remains underspecified

## Confidence

**High Confidence**: The finding that position bias is primarily model-driven rather than language-driven is well-supported by consistent patterns across five typologically diverse languages. The empirical evidence that explicit relevance scoring consistently degrades accuracy across all tested conditions is robust and replicable.

**Medium Confidence**: The mechanism explaining why middle-position accuracy drops without corresponding entropy increases (attention homogenization hypothesis) lacks direct attention pattern validation. The claim that late-position preference in some models "challenges universal early-token bias" is supported but requires more diverse model families to establish as a general pattern.

**Low Confidence**: The speculation about why explicit scoring degrades performance (interference with learned attention patterns) remains untested. The entropy-confidence relationship interpretation assumes standard probabilistic interpretations of entropy that may not hold for all model architectures.

## Next Checks

1. **Attention Pattern Validation**: Conduct direct attention analysis to verify whether middle-position contexts receive uniformly distributed attention (homogenization) versus other patterns. This would validate or refute the mechanism behind confident failures in the middle position.

2. **Extended Model Coverage**: Test additional model architectures including larger variants (Llama3.1-70B, Qwen2.5-72B) and architectures from underrepresented families to determine if the late-position preference pattern holds across the broader model landscape.

3. **Production-Scale Context Testing**: Evaluate position bias effects with 10-50 retrieved documents typical of production RAG systems, and test with semantically relevant distractors (as in Zhang et al., 2024a) rather than random distractors to determine if the explicit scoring degradation is specific to the distractor type used.