---
ver: rpa2
title: Generalized and Personalized Federated Learning with Foundation Models via
  Orthogonal Transformations
arxiv_id: '2505.19888'
source_url: https://arxiv.org/abs/2505.19888
tags:
- fedot
- learning
- data
- generalization
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedOT introduces orthogonal transformations to federated learning
  with black-box foundation models, addressing the challenge of achieving both generalization
  and personalization in heterogeneous data settings. By combining globally shared
  classifiers with locally adapted orthogonal transformations, FedOT preserves semantic
  integrity and mitigates gradient conflicts across clients.
---

# Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations

## Quick Facts
- arXiv ID: 2505.19888
- Source URL: https://arxiv.org/abs/2505.19888
- Reference count: 40
- Primary result: FedOT achieves superior generalization and personalization in federated learning with black-box foundation models through orthogonal transformations

## Executive Summary
FedOT introduces orthogonal transformations to federated learning with black-box foundation models, addressing the challenge of achieving both generalization and personalization in heterogeneous data settings. By combining globally shared classifiers with locally adapted orthogonal transformations, FedOT preserves semantic integrity and mitigates gradient conflicts across clients. Extensive experiments on five datasets demonstrate superior performance in generalization, personalization, and overall accuracy compared to existing methods, with theoretical analysis confirming the benefits of orthogonality in maintaining low condition numbers and enhancing model robustness.

## Method Summary
FedOT combines a globally shared classifier with locally adapted orthogonal transformations for federated learning with frozen foundation models. The method uses FedAvg to aggregate the global classifier while each client maintains its own orthogonal transformation matrix computed via the Cayley transform. The orthogonal transformation preserves the semantic space of frozen feature encoders while allowing personalization. Local clients optimize both their orthogonal transformation and the global classifier parameters using SGD, with the orthogonal transformation ensuring low condition numbers and mitigating gradient conflicts across heterogeneous client data distributions.

## Key Results
- Superior generalization accuracy on unseen client domains compared to FedAvg, APFL, and DP2FL baselines
- Enhanced personalization accuracy for participating clients while maintaining strong generalization
- Consistent performance improvements across five diverse datasets (FEMNIST, PACS, Office-Home, VLCS, TerraIncognita) with different data heterogeneity patterns

## Why This Works (Mechanism)
FedOT leverages orthogonal transformations to preserve the semantic integrity of frozen foundation model features while enabling client-specific adaptations. The orthogonality constraint ensures that local transformations maintain the geometric properties of the feature space, preventing semantic distortions that could arise from arbitrary linear transformations. This approach simultaneously allows personalization through local transformations while maintaining generalization through the shared global classifier, with the orthogonal constraint acting as a regularizer that prevents overfitting to local data patterns.

## Foundational Learning
- **Orthogonal transformations**: Linear transformations preserving inner products and norms; needed to maintain semantic space integrity when adapting frozen foundation model features. Quick check: Verify w_l^T w_l ≈ I after each update.
- **Cayley transform**: Maps skew-symmetric matrices to orthogonal matrices; needed for efficient computation of orthogonal transformations during training. Quick check: Monitor numerical stability when computing (I-P)^-1.
- **Condition numbers**: Measure of matrix conditioning affecting optimization stability; needed to quantify the benefits of orthogonality in gradient conflict mitigation. Quick check: Track κ(w_l) throughout training to ensure bounded values.
- **FedAvg aggregation**: Standard federated learning protocol for averaging model parameters; needed as the baseline aggregation mechanism for the global classifier. Quick check: Verify correct averaging of w_g across clients.
- **Cross-entropy loss**: Standard classification loss function; needed for training both global classifier and local transformations. Quick check: Monitor training loss convergence curves.
- **Gradient conflict mitigation**: Strategy to handle conflicting gradients from heterogeneous clients; needed to explain FedOT's superior performance in non-IID settings. Quick check: Compare gradient norms and cosine similarities across clients.

## Architecture Onboarding

**Component map**: CLIP ViT-B/32 (frozen) -> h = I(x) -> w_l · h (orthogonal transform) -> w_g · h' (global classifier) -> softmax + cross-entropy

**Critical path**: Data preprocessing → Feature extraction → Orthogonal transformation → Classification → Loss computation → Parameter update

**Design tradeoffs**: Orthogonal transformations provide theoretical guarantees and semantic preservation but limit expressiveness compared to unconstrained transformations. The trade-off between generalization (shared w_g) and personalization (local w_l) is managed through the orthogonal constraint rather than complete independence.

**Failure signatures**: Poor generalization indicates excessive personalization or broken orthogonality; poor personalization suggests insufficient local adaptation or overly restrictive orthogonal constraints; numerical instability in Cayley transform manifests as exploding gradients or NaNs.

**First experiments**:
1. Verify orthogonality preservation: Check w_l^T w_l ≈ I after each update and monitor condition number κ(w_l)
2. Baseline comparison: Implement FedAvg with identical frozen encoder and compare generalization vs personalization trade-off
3. Sensitivity analysis: Test different learning rates and temperature parameters to identify optimal settings for each dataset

## Open Questions the Paper Calls Out

**Open Question 1**: Can FedOT achieve full black-box operation by eliminating the global classifier's white-box exposure while maintaining performance?
- Basis in paper: [explicit] The authors note that the shared global classifier introduces white-box elements, which is restrictive when strict black-box settings are mandatory.
- Why unresolved: The current design requires sharing classifier weights, exposing architectural details and violating pure black-box constraints.
- What evidence would resolve it: Demonstration of an alternative mechanism (e.g., gradient-free or output-space aggregation) that preserves personalization and generalization without sharing internal parameters.

**Open Question 2**: Can richer nonlinear transformations replace orthogonal ones without sacrificing theoretical guarantees on gradient conflicts?
- Basis in paper: [explicit] The authors suggest future work could explore nonlinear mappings to better handle heterogeneous data, despite reduced theoretical transparency.
- Why unresolved: Orthogonal transformations provide provable bounds on gradient discrepancy; nonlinear mappings lack such formal guarantees and may introduce uncontrolled distortions.
- What evidence would resolve it: Empirical and theoretical analysis showing nonlinear transformations maintain semantic integrity and gradient alignment under heterogeneity, or derivation of new bounds for nonlinear cases.

**Open Question 3**: How should optimal block sizes in block-diagonal orthogonal transformations be determined adaptively per client or dataset?
- Basis in paper: [inferred] Block-diagonal designs reduce complexity, but the paper relies on random search to select block sizes, lacking a principled or automated method.
- Why unresolved: The trade-off between degrees of freedom, personalization, and computational cost is dataset-dependent, and manual tuning is impractical at scale.
- What evidence would resolve it: A systematic method (e.g., meta-learning, gradient-based optimization, or information-theoretic criteria) that dynamically selects block sizes and validates performance across diverse datasets.

**Open Question 4**: How does FedOT scale to federated networks with hundreds of clients and extreme data heterogeneity?
- Basis in paper: [inferred] Experiments use only N=4 clients per dataset; behavior in large-scale, real-world FL settings with many clients remains untested.
- Why unresolved: Gradient conflict mitigation via orthogonality may degrade as client diversity increases, and communication overhead could become a bottleneck.
- What evidence would resolve it: Experiments on large-scale FL benchmarks (e.g., with >100 clients) showing consistent personalization and generalization performance, along with analysis of convergence rates and communication costs.

## Limitations
- The method requires sharing the global classifier weights, which violates pure black-box constraints and exposes architectural details
- Theoretical guarantees assume idealized conditions that may not hold with extreme data heterogeneity or many clients
- Performance depends on careful hyperparameter tuning, particularly for the orthogonal transformation initialization and learning rates

## Confidence
- **High confidence**: The core methodology of using orthogonal transformations with Cayley transform is clearly specified and mathematically sound. The FedAvg aggregation procedure and overall experimental framework are well-defined.
- **Medium confidence**: The performance improvements over baselines are reported but depend on exact hyperparameter tuning and implementation details not fully specified in the paper.
- **Low confidence**: The theoretical guarantees regarding condition numbers and gradient conflicts are established under idealized assumptions that may not translate directly to empirical performance.

## Next Checks
1. **Numerical stability verification**: Implement gradient clipping and monitor the condition number κ of w_l throughout training. Add a small ε diagonal perturbation to (I-P) if κ > 1.5 to maintain orthogonality.
2. **Hyperparameter sensitivity analysis**: Test temperature τ values at {1.0, 10, 100} and batch sizes {16, 32, 64} to identify optimal settings for each dataset, documenting performance variance.
3. **Baseline implementation fidelity**: Implement APFL and DP2FL methods exactly as described in their respective papers to ensure fair comparison, verifying that all preprocessing and evaluation protocols match the FedOT experimental setup.