---
ver: rpa2
title: 'A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede
  AI Autonomy'
arxiv_id: '2506.09420'
source_url: https://arxiv.org/abs/2506.09420
tags:
- arxiv
- human
- agents
- systems
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that fully autonomous AI agents, while promising,
  face significant reliability, safety, and accountability issues that make them premature
  for complex real-world deployment. Instead, it advocates for LLM-based Human-Agent
  Systems (LLM-HAS), where humans and AI agents collaborate with humans providing
  guidance, feedback, and oversight.
---

# A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy

## Quick Facts
- arXiv ID: 2506.09420
- Source URL: https://arxiv.org/abs/2506.09420
- Reference count: 40
- Primary result: LLM-based Human-Agent Systems (LLM-HAS) offer a more reliable, accountable, and adaptable alternative to fully autonomous AI agents for complex real-world tasks.

## Executive Summary
This paper argues that fully autonomous AI agents face significant reliability, safety, and accountability issues that make them premature for complex real-world deployment. Instead, the authors advocate for LLM-based Human-Agent Systems (LLM-HAS), where humans and AI agents collaborate with humans providing guidance, feedback, and oversight. The paper presents LLM-HAS as a more trustworthy and adaptable alternative, demonstrating its advantages through applications in healthcare, finance, software development, and autonomous driving. It identifies key challenges including human variability, data quality, model adaptability, safety vulnerabilities, and evaluation methodologies, proposing specific implementation guidelines across architecture, data, model engineering, and post-deployment monitoring.

## Method Summary
The paper proposes constructing LLM-based Human-Agent Systems (LLM-HAS) where an AI agent collaborates with a human who provides guidance, feedback, and oversight. The system follows a lifecycle including Initial Setup (environment definition, profiling, RACI roles), Data Alignment (RLHF, DPO), Model Engineering (lifelong learning, dynamic prompt engineering), and Post-Deployment Monitoring. Implementation requires diverse human data including interaction logs, explicit feedback, implicit feedback, and domain-specific corpora. The system is evaluated across five domains: Task Effectiveness, Human-Agent Interaction Quality, Trust/Transparency/Explainability, Ethical Alignment/Safety, and User Experience/Cognitive Load.

## Key Results
- Human oversight in LLM-HAS improves system reliability by enabling real-time error detection and correction that autonomous agents miss
- LLM-HAS handles ambiguous tasks better by requesting clarification rather than proceeding with incorrect assumptions
- Integrating human oversight clarifies legal accountability and ethical alignment by establishing clear decision-making responsibility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If humans remain in the loop to verify outputs, system reliability and trust may improve compared to fully autonomous agents.
- **Mechanism:** The system utilizes a "collaborative verification process" where humans correct hallucinations and guide the agent in real-time, preventing the propagation of fabricated or erroneous outputs.
- **Core assumption:** Human operators can effectively identify errors that the model misses (Assumption: implies human domain knowledge exceeds model capability in edge cases).
- **Evidence anchors:**
  - [abstract] "...humans providing guidance, feedback, and oversight... systems can be more trustworthy and adaptable."
  - [section 3.2] "The interactive nature of HAS allows humans to provide crucial feedback, correct potential LLM hallucinations in real-time..."
  - [corpus] Collaborative Gym (arXiv:2412.15701) supports the need for frameworks enabling this verification.
- **Break condition:** If human verification latency exceeds task requirements, or if the human fails to detect subtle model hallucinations, the reliability advantage is lost.

### Mechanism 2
- **Claim:** LLM-HAS likely handles ambiguous or complex tasks better by treating ambiguity as a signal for clarification rather than a barrier.
- **Mechanism:** Instead of acting on potentially incorrect assumptions, the agent requests clarification from the human, leveraging "progressive refinement" to align on goals.
- **Core assumption:** Users can articulate their implicit needs when prompted effectively (Assumption: relies on the user's self-knowledge and the agent's questioning ability).
- **Evidence anchors:**
  - [section 3.2] "When faced with an underspecified objective, the system can request clarification rather than proceeding with potentially incorrect assumptions..."
  - [section 4] "...proactively ask clarifying questions... improving controllability and user trust."
  - [corpus] CowPilot (arXiv:2501.16609) suggests leveraging human collaboration when agents fall short on complex tasks.
- **Break condition:** If the interaction loop becomes burdensome (too many questions), user frustration may negate the accuracy benefits.

### Mechanism 3
- **Claim:** Integrating human oversight ostensibly clarifies legal accountability and ethical alignment.
- **Mechanism:** By designating a human supervisor as the final decision-maker, the system establishes a "responsible party," reducing the ambiguity of liability associated with autonomous agent errors.
- **Core assumption:** Legal frameworks will continue to prioritize human agency over algorithmic autonomy for liability assignment.
- **Evidence anchors:**
  - [section 3.2] "With a human involved in the decision-making process... establishing accountability becomes more straightforward."
  - [section 5] "...complicating liability for delegated actions without clear attribution."
  - [corpus] Corpus signals on "Joint Hybrid Decision Making" (arXiv:2512.00420) reinforce the need to understand authority distribution.
- **Break condition:** If the human acts as a "rubber stamp" without genuine review, the accountability mechanism fails to provide safety benefits.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** The paper positions RLHF as a core technique for integrating human feedback to transform static LLMs into adaptive teammates.
  - **Quick check question:** Can you explain how preference data is used to align a model's outputs with human intent?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper identifies "lifelong learning" as a challenge; engineers must understand why models lose previously learned skills when updating with new data.
  - **Quick check question:** What happens to a model's performance on Task A when it is fine-tuned exclusively on Task B?

- **Concept: RACI Matrix (Responsible, Accountable, Consulted, Informed)**
  - **Why needed here:** Section 6.1 explicitly suggests using RACI roles for profiling agents and humans to prevent inefficiencies.
  - **Quick check question:** In a collaborative coding task, which role ensures the code is functional versus which role actually writes the code?

## Architecture Onboarding

- **Component map:**
  - Environmental Settings: Defining shared interaction space and profiling user/agent roles
  - Interaction/Communication: Specifying synchronization and feedback phases
  - Model Engineering: Integrating feedback loops (RLHF/DPO) and memory mechanisms
  - Monitoring: Post-deployment systems for hallucination detection and behavioral drift

- **Critical path:**
  1. Profile Setup: Define human/agent capabilities (Section 6.1)
  2. Interaction Protocol: Establish communication theory-based protocols (e.g., Gricean Maxims)
  3. Feedback Integration: Implement DPO or RLHF pipelines to utilize human data

- **Design tradeoffs:**
  - Latency vs. Quality: Soliciting human help improves success rates (S) but increases human costs (Ch) and response time
  - Data Cost vs. Quality: Hybrid data generation (Human + Synthetic) is cheaper but risks "model inbreeding" (bias perpetuation)

- **Failure signatures:**
  - Human Bottleneck: Response time degrades unacceptably due to required human intervention (Section 7.4)
  - Behavioral Drift: The agent adapts unpredictably over time, misaligning with safety guardrails
  - Oversight Fatigue: Human operators become "rubber stamps," failing to catch errors due to cognitive load or monotony

- **First 3 experiments:**
  1. Role Variability Test: Implement the system with "lazy" vs. "informative" user profiles to measure the delta in task success rates (Section 6.1)
  2. Clarification Threshold: A/B test the system's tendency to ask for help vs. proceeding autonomously to find the optimal utility point (U = V·S - Ch - Ce)
  3. Hallucination Catch Rate: Measure the percentage of fabricated outputs successfully intercepted by the human-in-the-loop vs. those missed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks effectively quantify human workload and cognitive burden in LLM-HAS beyond measuring simple output accuracy?
- Basis in paper: [explicit] The authors state that "standard metrics for human workload and efficiency are lacking," noting that current methods "often entirely ignoring the real burden placed on human collaborators."
- Why unresolved: Existing benchmarks focus on agent accuracy rather than the "cost" of human feedback time or cognitive load.
- What evidence would resolve it: The development and adoption of a standardized evaluation suite that measures interaction quality, user effort, and resource utilization alongside task success.

### Open Question 2
- Question: How does the variability of human feedback (e.g., noisy, biased, or inconsistent) impact the convergence and reliability of LLM-HAS?
- Basis in paper: [explicit] The paper highlights the need for "thorough investigations or benchmarks into how varied human feedback affects entire systems" due to inherent human subjectivity.
- Why unresolved: Most current systems use "agent-centered" views or LLM-simulated proxies that fail to reflect the diversity of real human behavior.
- What evidence would resolve it: Empirical studies comparing system performance across different human user profiles (e.g., "lazy" vs. "informative") and feedback noise levels.

### Open Question 3
- Question: How can systems dynamically determine the "optimal degree of human involvement" (h*) to maximize utility without creating a responsiveness bottleneck?
- Basis in paper: [inferred] While the paper defines a utility function involving human costs (Ch) and error costs (Ce), it acknowledges the difficulty of optimizing this balance in real-time to prevent the human from becoming a bottleneck.
- Why unresolved: There is no established mechanism for real-time optimization that balances the value of human intervention against speed and operational costs.
- What evidence would resolve it: Algorithms capable of adaptive human-agent scheduling that demonstrate higher total utility (U) than static interaction protocols.

## Limitations

- The proposed framework relies heavily on human capability to provide effective oversight, yet this assumes consistent human performance and domain expertise that may not hold across all contexts
- The evaluation methodology remains largely conceptual, with limited empirical validation across the diverse application domains mentioned
- The "Human Variability" challenge is recognized but not fully resolved - the system's effectiveness may vary dramatically based on the specific human collaborator

## Confidence

- **High confidence:** The identification of core challenges (safety, accountability, evaluation) in autonomous agents is well-supported by existing literature and real-world incidents
- **Medium confidence:** The proposed LLM-HAS framework addresses these challenges plausibly, but lacks rigorous empirical validation
- **Low confidence:** The specific utility function (U = V·S - Ch - Ce) and its quantitative predictions require more experimental verification

## Next Checks

1. **Human bottleneck validation:** Measure the actual response time and cognitive load of human operators across different task complexities to verify that the collaboration overhead doesn't negate the claimed benefits
2. **Feedback integration efficacy:** Test whether the system actually learns from human corrections over time, or if it continues to repeat the same errors despite supervision
3. **Generalizability testing:** Implement the framework across at least three distinct high-stakes domains (e.g., healthcare, finance, autonomous systems) to assess whether the theoretical advantages translate to practical improvements