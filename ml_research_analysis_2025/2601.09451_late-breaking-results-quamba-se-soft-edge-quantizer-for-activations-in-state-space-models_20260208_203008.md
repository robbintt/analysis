---
ver: rpa2
title: 'Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State
  Space Models'
arxiv_id: '2601.09451'
source_url: https://arxiv.org/abs/2601.09451
tags:
- quamba
- data
- scale
- quamba-se
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of quantizing activation outliers
  in State Space Models (SSMs) like Mamba, which harm precision for normal values
  when using standard INT8 quantization with hard clipping. The proposed Quamba-SE
  method introduces a soft-edge quantizer with three adaptive scales: high-precision
  for small values, standard for normal values, and low-precision for outliers, preserving
  outlier information instead of clipping.'
---

# Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models

## Quick Facts
- arXiv ID: 2601.09451
- Source URL: https://arxiv.org/abs/2601.09451
- Reference count: 10
- Primary result: +0.83% average zero-shot accuracy improvement over Quamba on Mamba-130M

## Executive Summary
This paper addresses the challenge of quantizing activation outliers in State Space Models (SSMs) like Mamba, which harm precision for normal values when using standard INT8 quantization with hard clipping. The proposed Quamba-SE method introduces a soft-edge quantizer with three adaptive scales: high-precision for small values, standard for normal values, and low-precision for outliers, preserving outlier information instead of clipping. Evaluated on Mamba-130M across six zero-shot benchmarks, Quamba-SE consistently outperforms the prior SOTA Quamba method, achieving up to +2.68% improvement on individual benchmarks and +0.83% higher average accuracy across all datasets.

## Method Summary
Quamba-SE introduces a soft-edge quantizer that replaces hard clipping with adaptive scaling for SSM activation outliers. The method uses three distinct scales: scale/4 for small values (V < L), standard scale for normal values, and scale×4 for outliers (V > H). The second bit of the INT8 representation serves as a soft-edge identifier to distinguish special-range values without extra storage. Thresholds L and H are derived from the baseline Quamba calibration scale. The hardware implementation includes comparators for threshold detection, three quantizers with different scale multipliers, and multiplexers for routing based on value classification.

## Key Results
- Consistently outperforms Quamba baseline across all six zero-shot benchmarks
- Achieves up to +2.68% accuracy improvement on individual benchmarks
- Improves average accuracy by +0.83% across all tested datasets
- Validated on Mamba-130M model architecture

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale Adaptive Quantization
Standard INT8 quantization cannot capture the full dynamic range of SSM activations when outliers are present. By classifying values into three regions using thresholds L and H, Quamba-SE applies appropriate precision levels: high-precision (scale/4) for small values, standard precision for normal values, and low-precision (scale×4) for outliers. This preserves outlier information that would otherwise be lost to hard clipping, avoiding the precision degradation that occurs when a single scale must accommodate extreme values.

### Mechanism 2: Bit-level Encoding for Zero-storage Classification
The second bit of the INT8 representation serves as the soft-edge (SE) identifier, enabling runtime classification without additional storage overhead. When enabled (for V < L or V > H), hardware interprets the remaining 6 bits as special-range data. This encoding allows the same hardware to decode and route values to the correct scale multiplier efficiently, maintaining compatibility with existing INT8 storage formats while adding classification capability.

### Mechanism 3: Calibration-based Threshold Transfer
Quamba-SE reuses Quamba's offline-calibrated scale as the baseline, deriving thresholds L and H from this scale to partition the value range. This leverages prior work on finding effective scales while adding the soft-edge hardware logic only at inference time. The approach maintains consistency with established PTQ methodologies while adapting them to the soft-edge paradigm, avoiding the need for retraining or new calibration procedures.

## Foundational Learning

- **Post-Training Quantization (PTQ)**: Quamba-SE is a PTQ method that quantizes a pre-trained model without retraining. Quick check: Why does PTQ require calibration data, and how does it differ from Quantization-Aware Training (QAT)?

- **Activation Outliers in SSMs**: The paper's core motivation is that SSM activations contain significant outliers that break standard INT8 quantization. Quick check: What happens to the quantization step size when outliers are included in the scale calculation for uniform quantization?

- **INT8 Representation and Dynamic Range**: The soft-edge quantizer modifies how values map to INT8's 8-bit range using bit manipulation for classification. Quick check: How many distinct values can INT8 represent, and what is the tradeoff between range and precision when scaling?

## Architecture Onboarding

- **Component map**: Threshold comparators -> SE identifier logic -> Multiplexer -> Three quantizers (scale/4, scale, scale×4) -> INT8 output

- **Critical path**: 1) SSM input X_t arrives as FP32, 2) Comparator checks V < L or V > H → sets SE identifier, 3) Multiplexer routes to appropriate quantizer, 4) INT8 output proceeds to INT8 linear projection, 5) After SSM computation, Hadamard transform smooths output Y outliers

- **Design tradeoffs**: Latency vs. Accuracy (soft-edge adds classification/multiplexing latency), Precision allocation (6 bits for special regions limits resolution), Calibration percentile choice (higher percentiles may include more outliers but risk scale inflation)

- **Failure signatures**: Accuracy collapse on specific datasets (poor threshold matching), SE identifier corruption (incorrect bit interpretation), Scale overflow (outliers beyond scale×4 range still clip)

- **First 3 experiments**: 1) Baseline reproduction on Mamba-130M to verify Table I results, 2) Threshold sensitivity analysis by sweeping L and H multipliers, 3) Latency profiling to measure per-layer quantization overhead

## Open Questions the Paper Calls Out

### Open Question 1
Does the soft-edge quantization strategy maintain its accuracy improvements when applied to the larger Mamba2 architecture and the Quamba2 framework? The authors state in the Discussion section: "Evaluation on Mamba2 with Quamba2 are left for future work." This remains untested as experimental results are restricted to Mamba-130M.

### Open Question 2
What are the specific hardware area, power, and timing overheads introduced by the soft-edge quantizer logic compared to a standard INT8 unit? The conclusion explicitly notes: "Hardware synthesis is left for the future work." The paper simulates quantization in software rather than implementing the proposed hardware architecture.

### Open Question 3
To what extent does the added latency of the conditional scaling logic impact overall inference throughput, and can branch prediction effectively mitigate this? The authors acknowledge the design "adds latency" but justify it by accuracy gains, suggesting branch prediction can reduce overhead, though no experimental timing data validates this claim.

### Open Question 4
Is the specific ratio of scales (scale/4 and scale×4) optimal for all SSM layers, or does it require layer-specific tuning? The paper mentions introducing these scales but concludes that "layer-specific and model-specific quantizer design" is a promising direction, without verifying if these specific multipliers are globally optimal.

## Limitations

- Hardware implementation details remain underspecified, including exact threshold values and bit-encoding schemes
- Performance claims limited to Mamba-130M model size without validation on larger architectures
- Calibration dataset details and Hadamard smoothing parameters not fully specified
- Hardware synthesis and timing overhead validation left for future work

## Confidence

- **High confidence**: Problem formulation (SSM activation outliers harming standard INT8 quantization)
- **Medium confidence**: Multi-scale mechanism (theoretical framework is plausible but hardware details underspecified)
- **Low confidence**: Absolute performance claims (cannot be independently verified without complete implementation details)

## Next Checks

1. **Implement and validate the SE-INT8 encoding scheme**: Reproduce the bit-packing logic using the second bit as SE identifier and verify correct decoding across edge cases
2. **Benchmark on larger models**: Evaluate Quamba-SE on Mamba-300M and Mamba-560M to assess scalability beyond the 130M baseline
3. **Profile hardware overhead**: Measure actual inference latency overhead from the soft-edge quantizer (classification, multiplexing) on target hardware to validate the claim that overhead is "minor relative to matrix operations"