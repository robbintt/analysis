---
ver: rpa2
title: Evaluating Large Language Models for Cross-Lingual Retrieval
arxiv_id: '2509.14749'
source_url: https://arxiv.org/abs/2509.14749
tags:
- reranking
- retrieval
- language
- performance
- listwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a large-scale evaluation of large language
  models for cross-lingual retrieval and reranking. It addresses the lack of systematic
  comparison of LLM-based rerankers in cross-lingual settings, particularly without
  relying on machine translation.
---

# Evaluating Large Language Models for Cross-Lingual Retrieval

## Quick Facts
- arXiv ID: 2509.14749
- Source URL: https://arxiv.org/abs/2509.14749
- Reference count: 24
- Primary result: Large-scale evaluation of LLM-based rerankers for cross-lingual retrieval across 13 language pairs, comparing bi-encoders and BM25 retrievers with listwise and pairwise reranking approaches.

## Executive Summary
This paper presents the first large-scale systematic evaluation of LLM-based rerankers in cross-lingual retrieval settings without relying on machine translation. The study compares five multilingual bi-encoders and BM25 as first-stage retrievers, and evaluates both listwise (RankZephyr, RankGPT3.5, RankGPT4.1) and pairwise (Llama-3.1-8B-Instruct, Aya-101) rerankers across CLEF 2003 and CIRAL datasets spanning 13 language pairs. Results show that dense bi-encoders (NV-Embed-v2, M3) outperform translation-based BM25 retrieval, and stronger first-stage retrieval leads to better reranking gains. The study reveals that even state-of-the-art rerankers struggle to approach theoretical upper bounds, especially in cross-lingual settings.

## Method Summary
The evaluation employs a two-stage cross-lingual retrieval pipeline. First-stage retrieval uses BM25 with document translation (Pyserini with k1=0.9, b=0.4) and five multilingual bi-encoders (NV-Embed-v2, M3, e5, mGTE, RepLLaMA) retrieving top-100 candidates. Second-stage reranking applies listwise methods (RankZephyr, RankGPT3.5, RankGPT4.1) using sliding window ranking with rank_llm, and pairwise methods (Llama-3.1-8B-Instruct, Aya-101) using PRP/bubble-sort with 10 passes. Translation uses nllb-200-1.3B sentence-level with greedy decoding. Evaluation metrics are MAP for CLEF 2003 and nDCG@20 for CIRAL, with paired t-tests for significance.

## Key Results
- NV-Embed-v2 and M3 bi-encoders outperform translation-based BM25 in first-stage retrieval
- Stronger first-stage retrieval leads to better reranking gains
- Document translation benefits diminish as reranker capability increases
- Pairwise rerankers perform competitively with listwise approaches
- Reranking performance is sensitive to document length
- State-of-the-art rerankers struggle to approach theoretical upper bounds in cross-lingual settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual dense bi-encoders can outperform translation-based lexical retrieval (BM25+MT) in cross-lingual first-stage retrieval
- Mechanism: Dense bi-encoders learn language-agnostic semantic representations during contrastive pretraining, enabling direct query-document matching across languages without explicit translation
- Core assumption: The training data composition and quality for bi-encoders adequately covers the target language pairs; models without language support will fail
- Evidence anchors:
  - [abstract] "NV-Embed-v2 and M3 bi-encoders outperform translation-based BM25"
  - [Section 4.1] NV-Embed-v2 achieves 0.323 MAP on CLEF vs. BM25-DT's 0.308; M3 achieves 0.392 nDCG@20 on CIRAL vs. BM25-DT's 0.285
  - [corpus] Related work confirms training data composition significantly impacts CLIR performance

### Mechanism 2
- Claim: Stronger first-stage retrieval directly enables better reranking gains
- Mechanism: Higher-quality candidate pools contain more relevant documents in top-k positions, giving rerankers better signal-to-noise ratios for relevance discrimination
- Core assumption: Rerankers have sufficient capacity to distinguish relevant from non-relevant documents within the candidate pool
- Evidence anchors:
  - [abstract] "stronger first-stage retrieval leads to better reranking gains"
  - [Section 4.2] On CIRAL, M3's +0.110 MAP improvement over BM25-DT translates to +0.043 to +0.117 reranking gains; on CLEF, NV-Embed-v2's smaller +0.015 improvement yields only +0.024 to +0.050 reranking gains
  - [corpus] Related work confirms first-stage quality significantly impacts reranking performance

### Mechanism 3
- Claim: Document translation benefits diminish as reranker capability increases
- Mechanism: Stronger rerankers (e.g., GPT-4.1) have better cross-lingual understanding, reducing reliance on translation to align query-document language; weaker rerankers (e.g., RankZephyr) benefit more from translation converting the task to monolingual
- Core assumption: The reranker has been exposed to sufficient multilingual data during training
- Evidence anchors:
  - [abstract] "benefits of translation diminishes with stronger reranking models"
  - [Section 4.2] On CLEF with NV-Embed-v2, RankZephyr gains +0.026 MAP from translation vs. RankGPT4.1's +0.003; on CIRAL, RankZephyr gains +0.112 vs. RankGPT4.1's +0.018
  - [corpus] Weak evidence in related papers; this appears to be a novel finding

## Foundational Learning

- Concept: **Multi-stage retrieval pipeline (retrieve → rerank)**
  - Why needed here: This paper evaluates LLMs specifically as second-stage rerankers; understanding that rerankers operate on a fixed candidate pool from a first-stage retriever is essential
  - Quick check question: Why can't a reranker improve recall of relevant documents not retrieved in the first stage?

- Concept: **Cross-lingual embedding alignment**
  - Why needed here: Dense bi-encoders perform CLIR by mapping queries and documents from different languages into a shared embedding space; understanding this explains why translation becomes unnecessary
  - Quick check question: What happens when a bi-encoder hasn't seen a target language during pretraining?

- Concept: **Listwise vs. pairwise reranking paradigms**
  - Why needed here: The paper compares these two LLM-based approaches; listwise ranks all candidates simultaneously in one prompt, pairwise compares document pairs iteratively (bubble-sort style)
  - Quick check question: Why might pairwise rerankers be more robust to longer documents than listwise rerankers?

## Architecture Onboarding

- Component map:
  Query (Language A) → First-Stage Retriever (BM25+DT or Bi-encoder) → Top-100 Candidates (Language B) → Reranker (Listwise or Pairwise, OG or DT) → Ranked Results

- Critical path: First-stage retriever quality is the bottleneck—the oracle analysis shows even perfect reranking can only achieve what's in the top-100; hybrid retrieval can improve candidate pool quality

- Design tradeoffs:
  - **NV-Embed-v2 vs. M3**: NV-Embed-v2 excels on high-resource languages (CLEF: 0.323 MAP) but fails on low-resource (no language support); M3 balances performance across both (CIRAL: 0.392 nDCG)
  - **Listwise vs. Pairwise**: Listwise (RankGPT4.1) achieves higher peak performance; Pairwise (Llama-3.1, Aya-101) is more robust to document length and requires no distillation from proprietary models
  - **With vs. without translation**: Translation helps more for weaker rerankers and low-resource languages; adds latency and cost

- Failure signatures:
  - Reranking degrades first-stage results: Occurs when cross-lingual capability is insufficient (e.g., RankZephyr OG on some CIRAL pairs, Table 3 row 3a)
  - Large gap to oracle (50-65% of potential unrealized): Indicates reranker cannot effectively distinguish relevance in cross-lingual settings
  - Performance drop at 256 tokens for listwise: Information overload dilutes relevance signals (Figure 2)

- First 3 experiments:
  1. **Establish first-stage baseline**: Run M3 and NV-Embed-v2 on your target language pairs; compare against BM25 with document translation to determine if your languages are well-supported
  2. **Ablate document length**: Test reranking with 64, 128, and 256 token chunks to find optimal context window for your reranker-documents that are too long hurt listwise rerankers
  3. **Measure translation gap**: Compare OG vs. DT reranking performance; a large gap indicates your reranker lacks cross-lingual capability for that language pair

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the substantial gap between current reranking performance and the theoretical upper bound (oracle) be closed in cross-lingual settings, where state-of-the-art rerankers realize only ~45% of potential improvement?
- **Basis in paper:** The authors define "Potential Reranking Improvements" (PRI) and show that even RankGPT4.1 realizes only 45.6% and 32.9% of PRI on CLEF and CIRAL respectively (Section 4.2, "The Glass Ceiling of Reranking")
- **Why unresolved:** The paper identifies the ceiling effect but does not investigate whether it stems from prompt design, model capacity, cross-lingual representation misalignment, or fundamental limitations of listwise/pairwise paradigms
- **What evidence would resolve it:** Ablation studies varying prompt strategies, fine-tuning rerankers on cross-lingual relevance data, and probing cross-lingual alignment within reranker representations

### Open Question 2
- **Question:** Can LLM-based rerankers achieve competitive performance on truly cross-lingual inputs (OG setting) without any machine translation, and what architectural or training modifications are required?
- **Basis in paper:** "Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR" (Abstract, Conclusion). The OG setting sometimes fails to improve first-stage results (Section 4.2)
- **Why unresolved:** The paper demonstrates the problem but does not propose solutions; all evaluated rerankers are either English-centric or distilled from English-centric models
- **What evidence would resolve it:** Training or fine-tuning rerankers on multilingual relevance judgments, evaluating cross-lingually aligned LLMs, and comparing against architectures specifically designed for multilingual ranking

### Open Question 3
- **Question:** What explains the divergent performance patterns of first-stage retrievers across datasets (e.g., NV-Embed-v2 excels on CLEF but struggles on CIRAL, while M3 shows the opposite), beyond language coverage?
- **Basis in paper:** "The best-performing first-stage ranker differs considerably based on the chosen dataset... Training data composition and quality appears to be a more significant factor" (Section 4.1)
- **Why unresolved:** The authors hypothesize training data effects but do not systematically disentangle factors such as document length, domain, language family, or retrieval task type
- **What evidence would resolve it:** Controlled experiments varying training data composition, probing domain adaptation, and analyzing performance by language family and document characteristics across standardized datasets

## Limitations
- Significant gap between current reranking performance and theoretical upper bounds (50-65% unrealized)
- Document length sensitivity degrades listwise reranker performance substantially
- Uneven language coverage across models limits generalizability of findings
- No systematic investigation of training data composition effects on cross-lingual retrieval

## Confidence

**High Confidence**: Claims about first-stage retrieval performance and superiority of dense bi-encoders over translation-based BM25; oracle analysis providing clear upper bounds

**Medium Confidence**: Findings regarding diminishing benefits of document translation as reranker capability increases; pairwise vs. listwise robustness comparison

**Low Confidence**: Novel observation that reranking performance is sensitive to document length requires further validation

## Next Checks

1. **Cross-lingual capability validation**: Systematically test rerankers on languages they were explicitly trained on versus languages with limited training exposure to quantify the true cross-lingual capability gap and validate the translation benefit findings across a broader language spectrum

2. **Document length robustness study**: Conduct experiments varying document length from 32 to 1024 tokens in finer increments to map the precise degradation curve for both listwise and pairwise rerankers, identifying architectural thresholds and potential mitigation strategies

3. **Low-resource language extension**: Evaluate the entire pipeline on genuinely low-resource languages not covered in the current study to assess real-world applicability and identify failure modes when pretraining data coverage is minimal