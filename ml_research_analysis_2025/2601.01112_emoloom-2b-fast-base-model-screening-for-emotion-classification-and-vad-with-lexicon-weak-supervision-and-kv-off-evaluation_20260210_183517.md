---
ver: rpa2
title: 'EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with
  Lexicon-Weak Supervision and KV-Off Evaluation'
arxiv_id: '2601.01112'
source_url: https://arxiv.org/abs/2601.01112
tags:
- training
- json
- evaluation
- emotion
- jsonl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EmoLoom-2B, a lightweight and reproducible
  pipeline for turning small language models into fast screening candidates for joint
  emotion classification and Valence-Arousal-Dominance (VAD) prediction. The pipeline
  addresses three key issues: protocol drift, fairness gaps in decoding, and weak-supervision
  pipelines that do not preserve VAD semantics.'
---

# EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation

## Quick Facts
- arXiv ID: 2601.01112
- Source URL: https://arxiv.org/abs/2601.01112
- Reference count: 15
- Primary result: EmoLoom-2B achieves Macro-F1 0.35 and VAD(1-RMSE) 0.94 on GoEmotions with Qwen-1.8B-Chat base

## Executive Summary
EmoLoom-2B introduces a lightweight, reproducible pipeline for screening small language models as fast candidates for joint emotion classification and VAD prediction. The approach addresses protocol drift, fairness gaps in decoding, and weak-supervision pipelines that do not preserve VAD semantics. By unifying training and inference under a single JSON input-output contract and adopting KV-off decoding, the method eliminates avoidable variance. It incorporates two orthogonal semantic regularizers—a VAD-preserving constraint and a lightweight external appraisal classifier—along with Valence Flip augmentation and an entropy-aware A/B mixture sampling schedule. Using Qwen-1.8B-Chat, EmoLoom-2B demonstrates strong performance on GoEmotions and EmpatheticDialogues, with robust cross-corpus generalization on DailyDialog, making it a dependable screening pass before heavier training or multimodal fusion.

## Method Summary
EmoLoom-2B fine-tunes a ~2B parameter language model (Qwen-1.8B-Chat) for joint multi-label emotion classification and continuous VAD regression using a unified JSON I/O schema. The training combines five loss terms: classification BCE, VAD regression MSE, VAD-preserving consistency loss, appraisal classifier loss, and Valence Flip regularization. Data is drawn from GoEmotions and EmpatheticDialogues via an A/B mixture sampler with entropy-aware temperature cooling. KV-off decoding (use_cache=false) ensures deterministic, protocol-true evaluation. Weak VAD signals are derived from the NRC-VAD lexicon and used as regularizers. The pipeline achieves strong in-domain and cross-corpus performance while maintaining budget-awareness and reproducibility.

## Key Results
- Macro-F1 of 0.35 on GoEmotions with 20:80 GoEmo/Empathetic mix
- VAD(1-RMSE) of 0.94 on the same in-domain test set
- Cross-corpus Macro-F1 of ~0.31 on converted DailyDialog set
- ParseOK rates consistently high (>90%) under KV-off decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A VAD-preserving consistency loss improves continuous VAD predictions by aligning the valence-arousal-dominance scores implied by the generated text with the target VAD values.
- Mechanism: During training, the generated output text is tokenized and mapped via a lexicon to an aggregated text-implied VAD vector. A mean-squared error loss term is added, penalizing outputs where the semantic content drifts from the numeric target, as quantified by the lexicon.
- Core assumption: The lexicon provides a reasonable, token-level approximation of the semantic valence, arousal, and dominance conveyed by the generated text, and this signal is compositional.
- Evidence anchors:
  - [abstract]: "...incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples..."
  - [Method]: "To encourage semantic faithfulness, we align the VAD implied by the generated text with the numeric VAD... and define the consistency loss."
  - [corpus]: No direct evidence; related papers discuss VAD models but do not validate the text-implied alignment mechanism.
- Break condition: This mechanism will fail if the lexicon is a poor proxy for contextual meaning (e.g., missing negation or sarcasm), making the text-implied signal noisy, or if the VAD target itself is unreliable.

### Mechanism 2
- Claim: A/B mixture sampling with entropy-aware temperature cooling stabilizes small-model SFT by balancing early coverage of diverse samples with later convergence on reliable patterns.
- Mechanism: Training samples are drawn from two datasets (A, B). A softmax over weights, inversely proportional to a running confidence proxy, selects the source at each step. A linearly cooling temperature modulates this selection, transitioning the model from exploration (high T) to consolidation (low T).
- Core assumption: Per-dataset confidence/entropy is a useful signal for guiding the sampling curriculum.
- Evidence anchors:
  - [abstract]: "During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence."
  - [Method]: "At each step we choose the next sample source... via [softmax formula]... T cools linearly across training."
  - [corpus]: No direct evidence for this specific curriculum scheduling mechanism.
- Break condition: The mechanism may fail if one dataset is significantly noisier, misleading the entropy signal, or if the cooling schedule is mismatched to the total training duration.

### Mechanism 3
- Claim: A unified, single-line JSON I/O contract coupled with "KV-off" decoding eliminates "protocol drift" and fairness gaps, yielding more replicable evaluations.
- Mechanism: The model is trained and evaluated on an identical task: generating a specific JSON string. Disabling the KV-cache (`use_cache=false`) makes decoding deterministic and removes variance from hardware-specific caching, creating a "protocol-true" setup.
- Core assumption: Protocol drift is a significant confounder in SLM evaluation, and removing KV-cache does not degrade the model's fundamental task performance.
- Evidence anchors:
  - [abstract]: "...unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting."
  - [Method]: "To remove decoding-induced variance, we disable the key–value cache and keep decoding identical for training and evaluation."
  - [corpus]: No direct evidence from corpus.
- Break condition: This mechanism presumes the chosen format is sufficient; it fails if the single-line JSON constraint is too restrictive for complex emotions or if `KV-off` makes the screening process prohibitively slow.

## Foundational Learning
- Concept: **Valence-Arousal-Dominance (VAD) Space**
  - Why needed here: The paper's core task is joint emotion classification and VAD prediction. Understanding this 3D continuous affect model is essential to grasp the regression and semantic alignment losses.
  - Quick check question: If a text expresses "anger," would you expect its VAD values for arousal and valence to be high or low, and why?

- Concept: **Supervised Fine-Tuning (SFT) with Multi-Task Loss**
  - Why needed here: The model is trained via a composite loss function combining classification, regression, and regularizers. An engineer needs to understand how these objectives are weighted and balanced.
  - Quick check question: What would happen to the model's behavior if the weight for the VAD regression loss were set to zero, based on the paper's ablation findings?

- Concept: **Weak Supervision from Lexica**
  - Why needed here: The paper uses dictionaries to generate "weak" VAD signals for training regularization and evaluation. This is a core data-generation strategy for low-resource settings.
  - Quick check question: Why does the paper label these VAD signals as "weak" and treat them as regularizers rather than gold-standard ground truth?

## Architecture Onboarding
- Component map: Qwen-1.8B-Chat backbone -> Unified JSON I/O layer -> Composite loss aggregator (L_cls, L_reg, L_vad, L_app, L_flip) -> A/B mixture sampler with entropy-aware temperature scheduling -> KV-off deterministic decoder
- Critical path: The composite loss function is the central junction. All innovations are integrated as loss terms that backpropagate into the SLM. The evaluation protocol's ParseOK gate determines which outputs count toward metrics.
- Design tradeoffs:
  - Simplicity vs. Nuance: Single-line JSON is auditable but limits rationale depth
  - Speed vs. Fairness: KV-off decoding ensures reproducibility but is slower than cached inference
  - Lexicon-Weak vs. Gold: Lexicon-derived VAD is cheap and broad but noisy from polysemy/context issues
- Failure signatures:
  - Low ParseOK: Model fails to adhere to JSON contract; SFT has failed to learn format
  - High VAD Error, Low Classification Error: Suggests λ_reg or λ_vad is too low or weak targets are poor
  - Polarity Asymmetry: Model gives different valence for mirrored pairs (x, x'); L_flip regularizer has failed
  - Format/Drift in Ablations: Performance varies with decoding settings; protocol has failed to control variance
- First 3 experiments:
  1. Baseline & Ablation: Train mix2080 with all loss components, then re-train with L_vad and L_app removed. Validates core regularizer claims.
  2. Protocol Drift Test: Evaluate a trained checkpoint with KV-on and varying temperatures. Compare metrics to KV-off baseline to quantify variance.
  3. Cross-Corpus Generalization: Run the trained model on the converted DailyDialog set under a time budget. Compare Macro-F1 (~0.31 as per paper) to in-domain score.

## Open Questions the Paper Calls Out
- Does the VAD-preserving consistency loss require architectural modification to remain effective in multimodal fusion settings (e.g., audio-visual) compared to the current text-only setup?
- Can the lexicon-based weak supervision pipeline maintain robustness when facing complex linguistic phenomena like sarcasm or double negation?
- Does increasing VAD output precision beyond two decimals cause a significant drop in the ParseOK rate for models under 2B parameters?
- Does the KV-off decoding protocol scale computationally to larger backbone models (e.g., 7B–13B parameters) without negating the "fast screening" utility?

## Limitations
- Reliance on lexicon-derived weak supervision for VAD, which may not capture contextual meaning, sarcasm, or negation
- Exact hyperparameter configuration (especially loss weights) not fully specified, preventing exact reproduction
- KV-off decoding ensures reproducibility but may not reflect real-world deployment conditions where KV-caching is standard
- Performance gains depend on the quality of the lexicon, which is not empirically validated

## Confidence
- **High Confidence:** Effectiveness of unified JSON I/O contract and KV-off decoding in eliminating protocol drift
- **Medium Confidence:** Overall performance gains from the full EmoLoom-2B pipeline
- **Low Confidence:** Individual contributions of the VAD-preserving constraint and appraisal classifier regularizers

## Next Checks
1. **Protocol Drift Validation:** Train EmoLoom-2B with all components enabled, then evaluate the same checkpoint under KV-on and KV-off conditions. Measure variance in Macro-F1 and VAD(1-RMSE) across multiple decoding runs.
2. **Regularizer Ablation Isolation:** Create a controlled experiment where you train with: (a) baseline multi-task loss only, (b) baseline + VAD-preserving constraint, (c) baseline + appraisal classifier, (d) full pipeline. Evaluate each on a held-out test set with gold-standard VAD annotations or high-confidence lexicon-derived labels.
3. **Lexicon Quality Impact:** Implement a simple oracle VAD regularizer that uses gold-standard VAD values instead of lexicon-derived ones for a subset of the training data. Train EmoLoom-2B with this oracle regularizer and compare performance to the lexicon-based version.