---
ver: rpa2
title: Training-Free Policy Violation Detection via Activation-Space Whitening in
  LLMs
arxiv_id: '2512.03994'
source_url: https://arxiv.org/abs/2512.03994
tags:
- policy
- arxiv
- whitening
- detection
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free method for detecting policy
  violations in LLM outputs by treating compliance as an out-of-distribution (OOD)
  problem in activation space. The method applies a whitening transformation to decorrelate
  and standardize hidden activations, then uses the Euclidean norm in this transformed
  space as a compliance score.
---

# Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs

## Quick Facts
- arXiv ID: 2512.03994
- Source URL: https://arxiv.org/abs/2512.03994
- Authors: Oren Rachmil; Roy Betser; Itay Gershon; Omer Hofman; Nitay Yakoby; Yuval Meron; Idan Yankelev; Asaf Shabtai; Yuval Elovici; Roman Vainshtein
- Reference count: 15
- Primary result: 86.0% F1 score on policy violation detection

## Executive Summary
This paper introduces a training-free method for detecting policy violations in LLM outputs by treating compliance as an out-of-distribution (OOD) problem in activation space. The method applies a whitening transformation to decorrelate and standardize hidden activations, then uses the Euclidean norm in this transformed space as a compliance score. Only the policy text and a small set of in/out-of-policy samples are required, making it lightweight and easily deployable. Evaluated across multiple LLMs on challenging benchmarks, the approach achieves 86.0% F1 score, outperforming fine-tuned baselines by up to 9.1 points and LLM-as-a-judge methods by 16 points, with minimal inference overhead.

## Method Summary
The method treats policy violation detection as an OOD problem in the activation space of LLMs. It computes a PCA-based whitening transform from in-policy activations to decorrelate and normalize dimensions, then uses the Euclidean norm in this whitened space as a compliance score. A threshold is calibrated via Youden's statistic on held-out mixed data. The approach requires only policy text and a small set of in/out-of-policy samples, with no model fine-tuning needed. During inference, activations are transformed using the pre-computed whitening matrix, the L2 norm is computed, and outputs are flagged as violations if the score exceeds the calibrated threshold.

## Key Results
- Achieves 86.0% F1 score on policy violation detection across multiple LLMs
- Outperforms fine-tuned baselines by up to 9.1 points and LLM-as-a-judge methods by 16 points
- Requires only ~100 samples per policy category to achieve strong performance
- Adds minimal inference overhead (0.03-0.05s for same-model deployment)

## Why This Works (Mechanism)

### Mechanism 1
Policy-relevant information is encoded in LLM internal activations, often more richly than in generated outputs. Transformer hidden states capture behavioral properties (task adherence, constraint satisfaction) that may be lost or ambiguously expressed during token decoding. This assumes decoding acts as a lossy bottleneck—internal representations contain signal that generated text does not fully reveal.

### Mechanism 2
Whitening transforms in-policy activations into a standardized space where Euclidean norm serves as a reliable compliance score. PCA-based whitening computes mean μ and covariance Σ from in-policy activations, then applies transform W satisfying W^T W = Σ^{-1}. This decorrelates and normalizes dimensions, making ||y||_2 (norm in whitened space) equivalent to Mahalanobis distance in the original space.

### Mechanism 3
Policy violations manifest as OOD deviations in activation space, detectable via calibrated norm thresholds. After whitening, in-policy activations cluster near origin (low norm); violations shift outward (higher norm). A threshold τ, calibrated via Youden's statistic on held-out mixed data, separates the two.

## Foundational Learning

**Whitening / PCA Decorrelation**
- Why needed here: The method relies on transforming activations to zero-mean, unit-covariance space. Without understanding covariance matrices, eigendecomposition, and why decorrelation enables uniform scoring, the core mechanism is opaque.
- Quick check question: Given a 4096-dim activation vector, explain why computing Mahalanobis distance directly is problematic and how PCA-based whitening with top-k components addresses this.

**Out-of-Distribution (OOD) Detection**
- Why needed here: The entire framing treats policy violations as OOD events. Understanding Mahalanobis distance, energy scores, and why OOD methods work on intermediate representations is essential.
- Quick check question: In OOD detection, why might a sample with high likelihood under the model still be flagged as anomalous? What does this imply for policy detection?

**Transformer Internal Representations**
- Why needed here: The method operates on hidden states at specific layers. Understanding what different layers encode (syntactic vs. semantic vs. task-specific) informs layer selection and interpretation.
- Quick check question: Why might "Information Leakage" peak at early layers while "Transactions" peaks at late layers? What does this suggest about where policy signals emerge?

## Architecture Onboarding

**Component map**: Synthetic data generation -> Activation extraction from in-policy samples -> PCA whitening transform computation -> Threshold calibration via Youden's statistic -> Layer selection per policy category -> Online activation transformation and norm scoring

**Critical path**: Quality and representativeness of in-policy reference samples (directly impacts covariance estimation) -> Layer selection via calibration split (wrong layer → poor separation) -> Threshold calibration (controls precision/recall trade-off)

**Design tradeoffs**:
- Top-K components: Lower K (e.g., 10) is faster but may lose signal; higher K (e.g., 50) adds noise. Paper finds 10-50 stable with minor fluctuations.
- Sample size: 100 samples/category achieves 75.6% F1; 750 samples yields only 79.1% — diminishing returns.
- Category-specific vs. global whitening: Category-specific improves AUC from 0.67 to 0.84 but requires per-category storage.
- Surrogate vs. same-model deployment: Surrogate adds ~0.9s latency; same-model adds ~0.03-0.05s.

**Failure signatures**:
- Low AUC (<0.7): Likely poor layer selection, insufficient/uncalibrated reference data, or weak base model representations
- High false positives: Threshold τ too low; recalibrate with more in-policy samples
- High false negatives: Threshold τ too high; include more out-of-policy examples in calibration
- Poor cross-domain transfer: Covariance mismatch — recompute whitening on domain-specific data

**First 3 experiments**:
1. Single-category validation: Pick one DynaBench category (e.g., Transactions). Vary K from 5 to 50 with fixed 100 samples. Plot F1 vs. K to find stable operating range.
2. Layer-wise analysis: For same category, compute AUC at each transformer layer. Identify peak layer and verify it matches paper's layer-selection logic.
3. Sample size ablation: Fix K=15. Vary samples per category from 20 to 200. Plot F1 curve to quantify calibration data requirements for your deployment context.

## Open Questions the Paper Calls Out

**Open Question 1**: Can activation-space signals be utilized to steer model generation toward the in-policy distribution center to prevent violations proactively?
- Basis: The authors explicitly state in Limitations that future work could use activation-space signals to steer model behavior toward the in-policy distribution center region during generation.
- Why unresolved: Current framework is designed as post-hoc detector rather than intervention mechanism; it flags violations but does not alter generation process.
- Resolution evidence: Modified inference loop where whitening-derived vectors are injected into hidden states to dynamically correct trajectory, resulting in statistically significant reduction in violation rates.

**Open Question 2**: To what extent does the method's performance degrade under significant distribution shifts between calibration data and target deployment setting?
- Basis: Authors note performance may degrade under significant distribution shifts if calibration data differs substantially from target deployment setting.
- Why unresolved: Evaluation relies on benchmarks which may not fully capture drift encountered in dynamic, real-world enterprise environments over time.
- Resolution evidence: Longitudinal studies testing detector on temporal data splits or domains far removed from contrastive calibration set without recalibration.

**Open Question 3**: Can the optimal transformer layer for detection be predicted a priori based on policy type, rather than requiring empirical calibration?
- Basis: Analysis shows categories exhibit distinct depth profiles (e.g., Information Leakage peaks early, Transactions late), but selection currently relies on category-specific layer selection via calibration rather than theoretical prediction.
- Why unresolved: Paper demonstrates policy signals reside at different depths but does not propose theoretical framework explaining why specific policy types align with specific layer depths.
- Resolution evidence: Systematic mapping between policy complexity/category and layer depth that predicts optimal operational layer without validation sweep.

## Limitations
- Performance degrades substantially on less aligned base models (e.g., Mistral-7B), limiting generalizability across model families
- Method's effectiveness depends on violations manifesting as OOD deviations in predictable activation subspace
- Synthetic contrastive dataset generation using "GPT-5.1" is not reproducible with publicly available models

## Confidence

**High Confidence** (Strong empirical support, clear theoretical grounding):
- Whitening transformation mathematically equivalent to Mahalanobis distance computation
- Baseline comparison showing 9.1-point F1 improvement over fine-tuned methods
- Runtime efficiency claims (0.03-0.05s overhead for same-model deployment)

**Medium Confidence** (Supported by evidence but with caveats):
- Policy violations as OOD events in activation space
- Layer-specific signal emergence patterns
- Sample size sensitivity curve showing diminishing returns

**Low Confidence** (Limited validation, speculative claims):
- Cross-domain transfer without recalibration
- Surrogate model deployment benefits
- Category-specific whitening necessity vs. global approach

## Next Checks

1. **Layer Sensitivity Validation**: For a single policy category, systematically measure AUC at each transformer layer and verify that the peak layer aligns with the paper's layer-selection logic. This checks whether the method's layer selection procedure reliably identifies optimal signal locations.

2. **Synthetic Data Replication**: Using an available strong LLM (e.g., GPT-4o), generate a contrastive dataset for one policy category and measure how closely the resulting performance matches the paper's reported results. This validates the synthetic data generation dependency.

3. **Base Model Robustness Test**: Apply the method to a model with known weak safety alignment (e.g., base Llama-3) and measure performance degradation relative to the paper's Mistral-7B results. This quantifies the method's dependence on base model alignment quality.