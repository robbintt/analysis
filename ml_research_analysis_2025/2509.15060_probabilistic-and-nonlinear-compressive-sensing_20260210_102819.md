---
ver: rpa2
title: Probabilistic and nonlinear compressive sensing
arxiv_id: '2509.15060'
source_url: https://arxiv.org/abs/2509.15060
tags:
- sensing
- compressive
- nonlinear
- lasso
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses two key challenges in compressive sensing:\
  \ improving \u21130 regularized regression efficiency and understanding nonlinear\
  \ generalization limits. For linear problems, the authors derive a closed-form solution\
  \ to a probabilistic reformulation of \u21130 regularized regression, eliminating\
  \ Monte Carlo sampling."
---

# Probabilistic and nonlinear compressive sensing

## Quick Facts
- arXiv ID: 2509.15060
- Source URL: https://arxiv.org/abs/2509.15060
- Reference count: 40
- Primary result: Exact Gradient Pruning (EGP) achieves orders-of-magnitude faster convergence than Monte Carlo methods for ℓ₀ regularized regression while also revealing fundamental limitations in nonlinear parameter recovery

## Executive Summary
This paper addresses two key challenges in compressive sensing: improving ℓ₀ regularized regression efficiency and understanding nonlinear generalization limits. For linear problems, the authors derive a closed-form solution to a probabilistic reformulation of ℓ₀ regularized regression, eliminating Monte Carlo sampling. This "Exact Gradient Pruning" (EGP) method computes exact gradients, achieving orders-of-magnitude faster convergence than sampling-based approaches while outperforming Lasso, Relaxed Lasso, IHT, and Forward Stepwise across various signal-to-noise ratios and correlation levels.

For nonlinear problems, the paper theoretically demonstrates that under Fefferman-Markel conditions, parameter recovery is guaranteed up to network symmetries in the infinite-data limit. However, empirical experiments reveal a fundamental limitation: despite achieving low test loss, SGD-based optimization converges to functionally equivalent but structurally distant parameter configurations. A surprising "ℓ2 rebound effect" shows parameters initially converging then diverging while loss decreases. This indicates that while compression improves function approximation, exact parameter recovery remains impossible even up to symmetries, highlighting a fundamental difference between linear and nonlinear compressive sensing.

## Method Summary
The paper introduces Exact Gradient Pruning (EGP), a closed-form analytical solution for ℓ₀ regularized regression that replaces Monte Carlo sampling with exact gradient computation. The method reformulates the discrete optimization problem as a smooth objective over continuous variables representing Bernoulli probabilities, enabling efficient gradient descent. For nonlinear models, the authors prove theoretical recovery guarantees under Fefferman-Markel conditions in the infinite-data limit, then demonstrate empirically that SGD optimization cannot achieve exact parameter recovery despite low test loss, exhibiting a counterintuitive "ℓ₂ rebound" where parameters diverge while loss decreases.

## Key Results
- EGP achieves orders-of-magnitude faster convergence than Monte Carlo-based probabilistic methods for ℓ₀ regularized regression
- Combined ℓ₀ + ℓ₁ regularization improves robustness in high-noise regimes while maintaining unbiased sparsity
- Nonlinear compressive sensing cannot guarantee parameter recovery even up to symmetries in finite-data regimes, despite theoretical guarantees in the infinite-data limit
- SGD optimization exhibits "ℓ₂ rebound" phenomenon where parameters diverge while test loss decreases

## Why This Works (Mechanism)

### Mechanism 1
EGP achieves orders-of-magnitude faster convergence than Monte Carlo-based probabilistic methods for ℓ₀ regularized regression. The key insight is a closed-form analytical expression (Theorem 1, Corollary 2) that exactly computes the expectation over Bernoulli variables without sampling. By reformulating the objective to be at most quadratic in the discrete variables, the 2^p combinatorial explosion is replaced with only 2× the terms (Equation 16). This enables direct gradient descent with exact gradients, eliminating Monte Carlo variance.

### Mechanism 2
Combined ℓ₀ + ℓ₁ regularization improves robustness in high-noise regimes while maintaining unbiased sparsity. EGP seamlessly integrates ℓ₁ and ℓ₂ penalties (Equations 17-19). The ℓ₁ norm prevents excessively large weights in noisy settings, while ℓ₀ ensures exact zeros without shrinkage bias. This is analogous to Relaxed Lasso but with exact ℓ₀ handling.

### Mechanism 3
Nonlinear compressive sensing for MLPs cannot guarantee parameter recovery even up to symmetries in finite-data regimes, despite theoretical guarantees in the infinite-data limit. Proposition 6 proves that the global optimum of the regularized objective recovers teacher parameters up to symmetries (permutations, sign flips) as n → ∞. However, empirical teacher-student experiments reveal an "ℓ₂ rebound phenomenon": parameters initially converge then diverge while loss continues decreasing (Figure 11).

## Foundational Learning

- Concept: **Probabilistic reformulation of discrete optimization**
  - Why needed here: EGP replaces the intractable ℓ₀ minimization with a smooth objective over continuous variables γ ∈ [0,1]^p representing Bernoulli probabilities
  - Quick check question: Can you explain why min_θ ||y - Fθ||² + λℓ₀(θ) is equivalent to min_{w,γ} E_z∼π_γ[||y - F(wz)||² + λℓ₀(z)]?

- Concept: **Bias-variance tradeoff in regularization**
  - Why needed here: ℓ₁ introduces shrinkage bias but helps in noisy settings; ℓ₀ is unbiased but can be unstable; EGP combines both strategically
  - Quick check question: Why does Lasso perform poorly at high SNR while Relaxed Lasso performs well?

- Concept: **Neural network symmetries and parameter identifiability**
  - Why needed here: Understanding why parameter recovery is fundamentally harder than function approximation in nonlinear settings
  - Quick check question: What are the two symmetry operations that make MLP parameterizations non-unique?

## Architecture Onboarding

- Component map:
  Core objective -> Parallel optimization -> Convergence checker -> Model selector -> Optional finetuning

- Critical path:
  1. Initialize γ ≈ 1, w randomly or via linear regression on F
  2. Run parallel SGD with Adam for λ₀ values on log-scale, 2-3 learning rates
  3. Check convergence every N epochs (loss change < Δ, active set stable)
  4. Apply refined model selection (Equations 42-45) to combine parallel runs
  5. Optionally finetune non-zero coefficients without regularization

- Design tradeoffs:
  - More parallel runs (higher K): Better exploration of local optima, higher memory/compute cost
  - Higher λ₁ coefficient: More robust to noise, risks shrinkage bias
  - Stricter convergence criteria: More stable solutions, longer runtime
  - Finetuning enabled: Lower final loss, may reduce sparsity

- Failure signatures:
  - Slow convergence with high variance: Likely learning rate too high; reduce by 10×
  - All γ → 0 or 1 immediately: λ₀ too large; reduce by 10×
  - Active set oscillating: Convergence threshold ϵ too tight; relax to 0.01
  - Poor test performance despite low train loss: Overfitting; increase λ₀ or use stronger validation-based selection

- First 3 experiments:
  1. Sanity check on synthetic data: Generate y = Fβ with known sparse β (e.g., n=100, p=200, s=5), run EGP with default settings, verify ASRE → 0 and reconstruction error → 0 as SNR increases
  2. Comparison against Lasso: On the same synthetic data, compare RTE vs SNR curves; verify EGP matches or exceeds Relaxed Lasso across SNR range
  3. Ablation of ℓ₁ regularization: Repeat experiment 1 with ℓ₁ disabled (l1_regularization=false); identify SNR threshold where ℓ₁ helps

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical mechanism driving the $\ell_2$ rebound phenomenon in nonlinear teacher-student networks? The authors empirically observe that student parameters diverge from the ground truth teacher parameters even as the test loss continues to decrease, contradicting the intuitive expectation of convergence.

### Open Question 2
Does Exact Gradient Pruning (EGP) offer advantages over Iterative Hard Thresholding (IHT) in genome association studies? While EGP is benchmarked against general compressive sensing algorithms, its performance on the specific discrete and high-dimensional structures of genome data remains untested.

### Open Question 3
Can EGP effectively prune neural ODEs and PDEs to identify sparse dynamics? The paper focuses on static function approximation; neural ODEs involve integration over time, presenting different optimization landscapes for sparsity.

### Open Question 4
Can parameter recovery guarantees be extended to networks with activation functions other than tanh? The paper limits its theoretical recovery proof to "multilayer perceptrons with tanh activation functions," noting other architectures are not covered.

## Limitations
- Theoretical recovery guarantees only apply in infinite-data limit, with empirical evidence showing fundamental barriers to exact parameter recovery
- Nonlinear results limited to MLP architectures with tanh activation functions
- Empirical validation primarily on synthetic data rather than real-world benchmarks
- SGD optimization limitations may not represent fundamental barriers if alternative optimization methods are used

## Confidence

- EGP convergence speed (Linear models): **High** - analytical derivation with empirical validation
- Combined ℓ₀ + ℓ₁ regularization benefits: **Medium** - supported by ablation studies but limited exploration of hyperparameter space
- Nonlinear recovery impossibility: **Medium** - strong empirical evidence but incomplete theoretical characterization

## Next Checks

1. Test EGP on real-world sparse regression datasets (e.g., genomics, compressed sensing benchmarks) to validate practical performance beyond synthetic data
2. Investigate alternative optimization methods (e.g., second-order methods, evolutionary strategies) on the teacher-student setup to determine if rebound phenomenon persists
3. Extend nonlinear experiments to convolutional architectures to assess generality of recovery limitations across model families