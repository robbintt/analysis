---
ver: rpa2
title: Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis
arxiv_id: '2508.13196'
source_url: https://arxiv.org/abs/2508.13196
tags:
- sentiment
- text
- multimodal
- contextual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal sentiment analysis in disaster
  contexts, where understanding public sentiment from social media text and images
  is critical for crisis management. The authors propose integrating a CNN for image
  analysis with a GPT-based LLM for text processing, enhanced by a contextual attention
  mechanism to model intermodal relationships.
---

# Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis

## Quick Facts
- **arXiv ID:** 2508.13196
- **Source URL:** https://arxiv.org/abs/2508.13196
- **Reference count:** 20
- **Primary result:** 2.43% accuracy increase and 5.18% F1-score improvement over baseline methods for multimodal sentiment analysis on crisis-related social media data.

## Executive Summary
This paper proposes a novel multimodal sentiment analysis framework for disaster-related social media, combining a CNN for image processing with a GPT-based LLM for text processing. The key innovation is a contextual attention mechanism that dynamically models intermodal relationships between text and image features. The model uses prompt engineering to extract sentiment-relevant features from tweets and fuses them with visual features from the CrisisMMD dataset. Experimental results demonstrate significant improvements over unimodal approaches and baselines, achieving 93.75% accuracy and 96.77% F1-score on classifying social media posts as informative or non-informative across various natural disasters.

## Method Summary
The approach uses a dual-branch architecture where text inputs are processed through a GPT model with tailored prompts to extract semantic features, while images are processed through a ResNet50 CNN to extract visual features. These modality-specific features are then fused using a contextual attention mechanism that employs dynamic routing to capture intermodal relationships. The fused representation is passed through fully connected layers and an RNN before final classification. The model is trained end-to-end on the CrisisMMD dataset using binary cross-entropy loss with a batch size of 32, learning rate of 0.001, and dropout of 0.5 over 100 iterations.

## Key Results
- Achieved 93.75% accuracy and 96.77% F1-score on CrisisMMD dataset
- 2.43% accuracy improvement over baseline methods
- 5.18% F1-score improvement over baseline methods
- Outperformed both Text-Only (88.99% accuracy) and Image-Only (88.56% accuracy) configurations significantly

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Guided Semantic Feature Extraction
The paper leverages GPT with tailored prompts to extract more sentiment-relevant semantic features from short, noisy text than standard embeddings alone. By prepending specific prompts to input text, the LLM's attention is directed to contextual aspects like urgency and sentiment polarity. The ablation study shows "GPT with prompt and fine-tuning" achieves 93.10 F1, significantly higher than "Simple GPT" at 90.40 F1.

### Mechanism 2: Contextual Attention for Inter-Modal Alignment
A contextual attention mechanism dynamically weighs text versus image features, capturing intermodality interactions that simple concatenation misses. The fusion module uses context functions and dynamic routing to iteratively update compatibility between modalities, creating a joint representation that prioritizes the stronger sentiment signal. This addresses the challenge of filtering noise and redundancy when combining different data types.

### Mechanism 3: Hierarchical Feature Complementarity
Combining the long-range dependency strength of LLMs with the deep residual learning of CNNs captures a more complete sentiment picture than unimodal approaches. The architecture preserves intramodal integrity before cross-modal integration, with images providing visual cues about damage severity while text provides explicit intent or emotion. The full model achieves 93.75% accuracy, outperforming both unimodal configurations significantly.

## Foundational Learning

- **Attention Mechanisms (Self vs. Cross-Modal):** Understanding how attention weights calculate relevance between tokens or between different data modalities (pixels vs. words) is critical for grasping how the model filters noise. *Quick check:* How does cross-modal attention differ from the self-attention used in the LLM branch?

- **Dynamic Routing (Capsule Networks):** Section 3.2 references "dynamic routing" as part of the fusion strategy. Understanding that this involves iterative agreement-based updates rather than a single forward pass is critical for debugging the fusion layer. *Quick check:* Does the routing iteration converge faster or slower than standard feed-forward fusion?

- **Prompt Engineering for Feature Extraction:** The model relies on specific prompts to guide feature extraction from GPT, rather than using the model "out of the box." *Quick check:* If the prompt asks for "urgency," does the model output a classification or a feature embedding vector?

## Architecture Onboarding

- **Component map:** Input (Text + Images) -> GPT with Prompt Engineering -> Feature Vector -> ResNet50 -> Penultimate Layer Vector -> Contextual Attention + Dynamic Routing -> Joint Latent Representation -> Fully Connected Layers -> RNN -> Softmax Classifier

- **Critical path:** 1) Prompt Design - most brittle part of text branch; 2) Fusion Alignment - dimension matching between 2048-dim image vector and GPT embedding; 3) Classification - RNN after fusion is unconventional for static classification tasks

- **Design tradeoffs:** GPT was chosen for prompt flexibility despite BERT having slightly higher accuracy in some baselines (BERT 88.89% vs GPT 84.91% accuracy). ResNet50 was chosen for robustness over more efficient alternatives like EfficientNet.

- **Failure signatures:** Low F1 on short text if prompts aren't triggering; overfitting to disaster type if visual features don't generalize; poor performance if text and images are completely unrelated

- **First 3 experiments:** 1) Ablation study comparing Text-Only and Image-Only branches to verify 5% lift from fusion; 2) Prompt sensitivity analysis swapping "Sentiment-Relevant" prompt for generic "Summarize" prompt; 3) Fusion strategy check replacing Contextual Attention with simple concatenation

## Open Questions the Paper Calls Out
- Does the proposed contextual attention mechanism effectively generalize to non-crisis domains where text-image correlation differs significantly?
- How does integration of temporal modalities like audio and video impact the stability of the dynamic routing fusion layer?
- To what extent does the model distinguish between emotional sentiment and factual informativeness when classifying posts?

## Limitations
- The specific implementation of "contextual attention" is not fully specified and may differ from standard attention or capsule routing
- Exact prompt templates used for feature extraction are not provided, making replication uncertain
- The advantage of contextual attention over simpler fusion strategies is not empirically validated against naive baselines

## Confidence
- **High Confidence:** Hierarchical feature complementarity claim is well-supported by ablation study and aligns with corpus evidence
- **Medium Confidence:** Prompt-guided semantic feature extraction improves performance, but lack of prompt specification makes replication uncertain
- **Low Confidence:** Specific advantage of contextual attention mechanism over simpler fusion strategies is not empirically validated

## Next Checks
1. Isolate the fusion gain by replacing "contextual attention" with simple tensor concatenation and comparing accuracy/F1-score
2. Perform prompt sensitivity test by systematically replacing undisclosed "sentiment-relevant" prompt with generic prompt to measure F1-score impact
3. Verify if 88.56% accuracy from ResNet50 "without pretrained weights" is achievable by comparing with ImageNet initialization