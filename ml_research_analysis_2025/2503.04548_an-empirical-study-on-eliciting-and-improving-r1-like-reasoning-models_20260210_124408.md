---
ver: rpa2
title: An Empirical Study on Eliciting and Improving R1-like Reasoning Models
arxiv_id: '2503.04548'
source_url: https://arxiv.org/abs/2503.04548
tags:
- training
- reasoning
- performance
- length
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report systematically studies the development of
  reasoning models through reinforcement learning (RL). The authors investigate the
  impact of RL settings on both base models and fine-tuned models, demonstrating that
  RL consistently improves model performance.
---

# An Empirical Study on Eliciting and Improving R1-like Reasoning Models

## Quick Facts
- arXiv ID: 2503.04548
- Source URL: https://arxiv.org/abs/2503.04548
- Reference count: 26
- Key result: RL training achieves 39.33% accuracy on AIME 2024 with 1.5B model and 86.67% with tool-augmented 32B model

## Executive Summary
This technical report systematically studies the development of reasoning models through reinforcement learning (RL). The authors investigate the impact of RL settings on both base models and fine-tuned models, demonstrating that RL consistently improves model performance. Notably, they show that even a small 1.5B model (DeepSeek-R1-Distill-Qwen-1.5B) can be further refined through RL training, achieving 39.33% accuracy on AIME 2024. Additionally, they explore tool manipulation, finding it significantly boosts reasoning performance - STILL-3-TOOL-32B achieved 86.67% accuracy on AIME 2024 with greedy search. The study reveals that while longer responses generally correlate with better performance, explicitly rewarding length can lead to "reward hacking" without genuine reasoning improvement.

## Method Summary
The study employs RL frameworks (OpenRLHF and veRL) with Qwen2.5 base models and DeepSeek-R1-Distill variants, training on 90k curated math problems from verifiable sources. The method uses on-policy learning with dynamic KL annealing, rule-based rewards (output correctness with \boxed{} format), and rollout sampling (16-64 samples per query). Key variants include STILL-3-1.5B with specific hyperparameters for small models and STILL-3-TOOL-32B with code interpreter integration. The training monitors response length, accuracy, completion ratio, and reasoning keyword frequency as indicators of reasoning emergence.

## Key Results
- RL training consistently improves model performance across both base and fine-tuned models
- Small 1.5B models can be further refined through RL, achieving 39.33% accuracy on AIME 2024
- Tool manipulation (code interpreter integration) boosts performance significantly, with STILL-3-TOOL-32B reaching 86.67% accuracy on AIME 2024
- Longer responses generally correlate with better performance, but explicit length rewards lead to "reward hacking"

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** On-policy learning enables sustained exploration and response length growth during RL training.
- **Mechanism:** By sampling data exclusively from the current policy distribution, the model maintains alignment between exploration behavior and gradient updates, allowing natural length increase without bottlenecks.
- **Core assumption:** The policy's exploration capacity is not exhausted early in training.
- **Evidence anchors:**
  - [Section 3.1.1]: "The fully on-policy training approach encourages greater exploration; during training, the model naturally and rapidly increases its response length, whereas off-policy learning with fewer updates struggles with bottlenecks in length growth."
  - [Figure 1(b)]: Shows on-policy achieving higher test accuracy than off-policy.
  - [corpus]: Related work (FR3E, arXiv:2507.07017) identifies exploration instability as a key RLVR challenge, supporting that structured exploration matters.
- **Break condition:** If entropy loss converges too quickly (Figure 9), exploration collapses regardless of on-policy settings—entropy bonus tuning becomes critical.

### Mechanism 2
- **Claim:** Base models possess latent reasoning capabilities that RL activates and integrates into coherent deliberation.
- **Mechanism:** Pre-training embeds individual reasoning actions (verification, reflection). RL provides reward signals that select for chaining these actions productively, creating emergent "aha moments."
- **Core assumption:** The base model has sufficient capacity and pre-training exposure to reasoning-adjacent patterns.
- **Evidence anchors:**
  - [Section 3.2]: "Even at training step 0, the ratio calculated from the base model's evaluation on AIME 2024 is approximately 0.1, indicating that these reasoning actions are already inherent in the base model."
  - [Figure 4]: Shows reasoning keyword ratios increasing during RL training from this baseline.
  - [corpus]: "R1-Zero's 'Aha Moment' in Visual Reasoning" (arXiv:2503.05132) reports similar emergence in 2B non-SFT models, suggesting this is a general phenomenon.
- **Break condition:** Smaller models (1.5B) show limited exploration capacity (Section 3.1.1), suggesting a minimum scale threshold.

### Mechanism 3
- **Claim:** Tool manipulation augments reasoning by offloading verification and computation to external execution.
- **Mechanism:** Code interpreter integration allows the model to externalize calculation steps, receiving ground-truth feedback that reduces compounding reasoning errors.
- **Core assumption:** The model can learn tool-invocation syntax from limited demonstrations.
- **Evidence anchors:**
  - [Section 4.4]: STILL-3-TOOL-32B achieves 86.67% (greedy) on AIME 2024 vs. 60% baseline—a 26.67 absolute point gain.
  - [Section 4.4]: "A small amount of high-quality distilled data can suffice... the model variant trained on only 0.8k data instances... achieves strong performance."
  - [corpus]: Limited direct corpus support for tool-augmented reasoning specifically; this mechanism is primarily paper-internal.
- **Break condition:** Smaller models struggle with instruction-following for tool use (Section 4.4), suggesting capacity constraints.

## Foundational Learning

- **Concept: On-policy vs. Off-policy RL**
  - **Why needed here:** The paper identifies on-policy learning as critical for reasoning emergence. Without understanding this distinction, an engineer might default to off-policy replay buffers that hinder exploration.
  - **Quick check question:** If you're reusing rollouts from 500 steps ago for gradient updates, are you doing on-policy or off-policy learning?

- **Concept: Reward Hacking**
  - **Why needed here:** Length-reward experiments (Section 4.2) show models exploit proxy rewards without genuine capability gain. Recognizing this failure mode is essential for reward design.
  - **Quick check question:** If your model's response length doubles but accuracy stays flat, what should you suspect?

- **Concept: KL Divergence as Stability Constraint**
  - **Why needed here:** Dynamic KL annealing (Section 3.1.1) balances exploration against catastrophic forgetting. Static KL values either over-constrain or under-protect.
  - **Quick check question:** Why might a fixed KL penalty of 0.001 hurt late-stage training?

## Architecture Onboarding

- **Component map:**
  Policy model (Qwen2.5 base/distilled) -> Rollout engine (n samples per query) -> Reward function (rule-based + auxiliary) -> Training frameworks (OpenRLHF/veRL) -> Tool layer (optional code interpreter)

- **Critical path:**
  1. Curate verifiable training data (90k math problems, filtered for parsability)
  2. Configure rollout parameters (n=64, T=1.0–1.2 recommended)
  3. Set on-policy training with dynamic KL annealing
  4. Monitor: response length, accuracy, completion ratio, reasoning keyword frequency
  5. If fine-tuning for tools: distill ~1k code-integrated demonstrations first

- **Design tradeoffs:**
  - Larger TBS (1024) improves efficiency but requires more GPU memory
  - Higher rollout counts improve exploration but multiply inference cost
  - Length rewards boost length but risk reward hacking—avoid unless explicitly testing this failure mode
  - Detailed prompts improve efficiency for larger models but may confuse smaller models (Section 3.1.2)

- **Failure signatures:**
  - Length without accuracy gain: Likely reward hacking from length-based rewards
  - Rapid entropy collapse: Exploration exhausted; consider entropy bonus (but tuning is fragile—Figure 9)
  - Low completion ratio: Responses exceeding context window; check truncation handling
  - Performance plateau: Base model capacity limit; consider larger backbone or SFT warmstart

- **First 3 experiments:**
  1. **Replicate baseline RL on Qwen2.5-7B:** Use TBS=128, on-policy, n=8, T=1.0, no KL penalty. Verify accuracy and length increase on AIME 2024 subset.
  2. **Ablate on-policy vs. off-policy:** Same settings but with replay buffer. Confirm the bottleneck described in Figure 1(b).
  3. **Test dynamic KL annealing:** Compare fixed KL=0.001 vs. cosine decay from 0.001→0. Document stability differences.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can principled RL training approaches be developed to utilize general-domain rewards for tackling complex reasoning tasks lacking verifiable ground truth?
- **Basis in paper:** [explicit] Footnote 3 states, "An important and promising research direction is to investigate the development of more principled reinforcement learning (RL) training approaches that utilize general-domain rewards... We leave this challenging topic for future work."
- **Why unresolved:** Current reasoning models rely heavily on rule-based rewards from verifiable domains (e.g., math, coding). The paper notes that training effective reward models for general-purpose applicability remains a significant challenge.
- **What evidence would resolve it:** A training framework that successfully aligns models using trained reward models for tasks without definite answers, achieving performance comparable to verifiable domains.

### Open Question 2
- **Question:** Does significantly extending RL training steps allow models to overcome performance plateaus and trigger emergent reasoning capabilities?
- **Basis in paper:** [explicit] Section 5 asks, "whether significantly extending the training steps, as in the case of DeepSeek-R1, would continue to constrain performance or instead lead to an emergent improvement in capabilities."
- **Why unresolved:** The authors observed performance bottlenecks and fluctuating accuracy after several hundred steps on 32B models, failing to achieve breakthroughs (e.g., >30% on AIME 2024) seen in SFT distillation.
- **What evidence would resolve it:** Long-duration training experiments showing that continuing RL optimization beyond observed plateaus results in sudden, significant leaps in test accuracy or reasoning behaviors.

### Open Question 3
- **Question:** Can reinforcement learning effectively enhance the inherent tool manipulation capabilities of Large Reasoning Models?
- **Basis in paper:** [explicit] Section 4.4 notes that while SFT enabled tool use, "A natural extension of this work is to leverage RL training to enhance the inherent tool manipulation capabilities of LLMs, a direction we leave for future research."
- **Why unresolved:** The paper only demonstrated tool augmentation via supervised fine-tuning on distilled data. It remains untested whether RL can further refine a model's ability to autonomously invoke and integrate tools like code interpreters.
- **What evidence would resolve it:** Experiments where RL training, applied post-SFT, demonstrates superior tool invocation strategies and higher accuracy on reasoning benchmarks compared to the SFT-only baseline.

## Limitations

- The generalizability of tool manipulation benefits across different problem types and model scales is not fully established
- The study's focus on math competition problems limits confidence in broader reasoning applications
- The reproducibility of "reasoning emergence" needs further validation across different model scales and datasets

## Confidence

- **High Confidence:** The empirical observations about RL improving model performance (accuracy gains of 39.33% for 1.5B model and 86.67% for tool-augmented 32B model) are well-supported by the reported results.
- **Medium Confidence:** The claim that base models contain latent reasoning capabilities that RL activates is supported by evidence but relies on interpretation of keyword frequency changes.
- **Low Confidence:** The generalizability of tool manipulation benefits across different problem types and model scales is not fully established.

## Next Checks

1. **Scale Generalization Test:** Reproduce the RL training pipeline on a 3B model (mid-point between 1.5B and 32B) to verify if the observed reasoning emergence scales linearly or exhibits threshold effects.

2. **Reward Hacking Validation:** Design a controlled experiment where length-based rewards are explicitly tested against accuracy-based rewards on identical problem sets, measuring both immediate and long-term effects on reasoning quality.

3. **Tool Manipulation Robustness:** Test the code interpreter integration approach on non-math domains (e.g., coding problems or logical reasoning) to assess whether the observed 26.67% absolute performance gain generalizes beyond mathematical computation.