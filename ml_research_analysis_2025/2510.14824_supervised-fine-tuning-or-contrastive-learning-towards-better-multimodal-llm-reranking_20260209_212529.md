---
ver: rpa2
title: Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM
  Reranking
arxiv_id: '2510.14824'
source_url: https://arxiv.org/abs/2510.14824
tags:
- reranking
- performance
- training
- retrieval
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares supervised fine-tuning (SFT)
  and contrastive learning (CL) for multimodal large language model (MLLM) reranking.
  Through theoretical analysis and empirical experiments, the authors decompose the
  losses into weight and direction components and show that SFT consistently outperforms
  CL, with the weight term being the dominant factor in performance.
---

# Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking

## Quick Facts
- arXiv ID: 2510.14824
- Source URL: https://arxiv.org/abs/2510.14824
- Reference count: 40
- Primary result: SFT outperforms CL for multimodal LLM reranking due to stronger weight scaling and semantic direction preservation

## Executive Summary
This paper investigates the effectiveness of supervised fine-tuning (SFT) versus contrastive learning (CL) for multimodal large language model (MLLM) reranking. Through theoretical analysis and empirical experiments, the authors decompose the losses into weight and direction components and show that SFT consistently outperforms CL, with the weight term being the dominant factor in performance. Specifically, SFT provides stronger optimization signals and better input-specific guidance than CL. To validate these findings, the authors construct a new benchmark called MRB, comprising 40 datasets spanning single-modal, cross-modal, and fused-modal retrieval tasks. They train two SFT-based models, GMR-3B and GMR-7B, which achieve state-of-the-art results on MRB, outperforming strong baselines including task-specific models.

## Method Summary
The method employs Qwen2.5-VL-Instruction MLLM (3B or 7B) with LoRA adapters (rank 16) for efficient fine-tuning. Training uses ~1.5M instances from multiple sources including M-BEIR, ViDoRe, and MS MARCO. The approach uses a unified reranking loss framework (URL) that can operate in either SFT or CL mode. For SFT, the model computes relevance scores using softmax over "yes"/"no" token probabilities. For CL, it uses InfoNCE loss with a single-token probability. The training employs 16 negatives per sample (8 hard negatives + 8 random) and is conducted with BF16 precision on 8x NVIDIA A100 GPUs. The GMR models are evaluated on a newly constructed MRB benchmark containing 40 datasets covering text-to-text, image-to-text, and fused-modal retrieval tasks.

## Key Results
- SFT outperforms CL by 3.32 points on MRB benchmark across 40 datasets
- SFT achieves state-of-the-art performance on MRB, surpassing both MLLM-based and task-specific models
- GMR-7B improves upon the previous best MLLM model by 6.04 points on MRB
- Performance scales with number of negatives (peak at 16), and instruction tuning improves results by 1.96 points

## Why This Works (Mechanism)

### Mechanism 1: Weight Dominance in Loss Decomposition
The weight component of the loss function, rather than the direction, is the dominant factor explaining SFT's superior performance over CL. SFT assigns larger, input-specific weights to gradient updates by computing normalization only over a single document's "yes" and "no" tokens, while CL weights normalize over all positive and negative documents in a sample, leading to smaller gradient magnitudes and weaker optimization signals.

### Mechanism 2: Token Embedding Semantic Preservation in SFT
SFT leverages pre-trained token embeddings ("yes"/"no") as an effective direction component, whereas CL must learn a score-projection matrix from scratch, losing semantic signal. SFT's direction vector is directly computed from frozen pre-trained LM head weights, providing semantic grounding that CL lacks until learned.

### Mechanism 3: Precision Sensitivity in Contrastive Learning
CL is more sensitive to numerical precision errors in weight computation than SFT, leading to degraded gradient signals. In small-batch reranking settings, InfoNCE weight terms involve exponentials of small scores, leading to vanishing gradients if computed in low precision (e.g., FP16/BF16). SFT's softmax over two tokens maintains more stable gradients.

## Foundational Learning

- **Concept: Loss Decomposition (Weight vs. Direction)**
  - Why needed here: Understanding how gradient magnitude (weight) and update direction (vector) interact is essential to diagnose SFT vs. CL performance differences.
  - Quick check question: Can you derive the weight term for SFT's positive document gradient (Equation 12) and explain why it depends only on the current document's scores?

- **Concept: Point-wise Reranking**
  - Why needed here: This paper focuses on point-wise scoring (independent query-document pairs) rather than list-wise methods, which affects how losses are computed.
  - Quick check question: How does point-wise reranking differ from list-wise, and why does it simplify loss decomposition?

- **Concept: Multimodal Instruction Tuning**
  - Why needed here: The GMR models use task-specific instructions to guide relevance assessment across modalities.
  - Quick check question: Why might instructions help a multimodal LLM better assess query-document relevance compared to no instruction?

## Architecture Onboarding

- **Component map:** Input Formatter -> MLLM Backbone -> LoRA Adapter -> LM Head -> Loss Computer
- **Critical path:**
  1. Prepare multimodal (text/image) query-document pairs with task instructions
  2. Format inputs via chat template (Figure 6)
  3. Forward pass through MLLM backbone + LoRA
  4. Extract logits for "yes" and "no" tokens
  5. Compute relevance score via softmax (SFT) or single-token probability (CL)
  6. Compute loss using URL framework, selecting weight/direction branches
  7. Backpropagate and update LoRA weights

- **Design tradeoffs:**
  - SFT vs. CL: SFT provides stronger weights and semantic direction but requires binary label supervision. CL is more flexible for continuous scores but suffers from weight shrinkage and precision sensitivity.
  - Number of negatives: More negatives improve performance (Figure 5, peak at 16) but increase memory/compute.
  - Frozen vs. unfrozen LM head: Unfreezing helps CL but not SFT (Table 15).

- **Failure signatures:**
  - Very low validation scores (<30% average): Check if instructions are mismatched or images not loading correctly
  - CL training plateauing early: Verify precision settings (use FP32 for loss computation) and negative sample diversity
  - SFT overfitting to "yes"/"no" tokens without generalizing: Reduce LoRA rank or increase data diversity

- **First 3 experiments:**
  1. Reproduce SFT vs. CL gap on a single-modality subset (e.g., Tâ†’T from MRB) using the URL framework with default settings to verify implementation
  2. Ablate weight and direction components by swapping SFT weights into CL and vice versa (Table 1 replication) to confirm weight dominance
  3. Test precision sensitivity by training CL with FP32 vs. BF16 loss computation (Table 5 replication) to validate gradient scale issues

## Open Questions the Paper Calls Out

### Open Question 1
Does the superiority of SFT over CL generalize to multilingual retrieval tasks? The paper trained and evaluated exclusively in English, leaving performance in other languages "unexplored."

### Open Question 2
How does the SFT vs. CL comparison hold when scaling to interleaved inputs containing multiple images per query or document? The current benchmark cannot assess performance on such inputs due to the "Single-image constraint."

### Open Question 3
Do the theoretical findings regarding loss decomposition generalize to list-wise or pair-wise reranking paradigms? The paper explicitly restricts its scope to "point-wise" reranking, contrasting it with list-wise approaches but not evaluating them.

## Limitations
- Single-image constraint prevents assessment of interleaved multi-image inputs
- English-only evaluation limits generalizability to multilingual scenarios
- Point-wise focus means list-wise or pair-wise paradigms remain unexplored
- Hard negative mining specifics are not fully detailed for exact reproduction

## Confidence

**High Confidence:** The core claim that SFT outperforms CL for MLLM reranking is well-supported by extensive experiments across 40 datasets in MRB. The loss decomposition analysis showing weight dominance over direction is theoretically sound and validated through controlled ablations.

**Medium Confidence:** The mechanism explaining CL's precision sensitivity is plausible but relies on indirect evidence rather than direct ablation studies. The assumption about semantic preservation in pre-trained embeddings is supported by directional experiments but lacks corpus-level validation.

**Low Confidence:** The specific impact of instruction templates on reranking performance across modalities is not thoroughly tested. The claim that CL benefits from unfrozen LM heads appears contradictory to the precision sensitivity argument.

## Next Checks

1. **Loss Function Precision Sensitivity:** Replicate the CL vs SFT training with varying precision settings (FP32, BF16, FP16) while keeping all other hyperparameters constant. Measure not only final performance but also gradient norms throughout training to confirm the vanishing gradient hypothesis for CL in lower precision.

2. **Direction Component Ablation with Pre-trained Initialization:** Modify the CL implementation to initialize its projection matrix with pre-trained token embeddings (rather than random) while keeping the InfoNCE loss structure. Compare performance against standard CL and SFT to isolate the contribution of semantic direction versus weight scaling.

3. **Negative Sample Size Sweep with Different Mining Strategies:** Systematically vary the number of negative samples (1, 4, 8, 16, 32) and compare random versus hard mining strategies for both SFT and CL. This would validate the Figure 5 results and test whether the weight dominance mechanism scales differently with negative count across training methods.