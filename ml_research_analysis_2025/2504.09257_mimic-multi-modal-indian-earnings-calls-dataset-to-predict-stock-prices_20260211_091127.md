---
ver: rpa2
title: 'MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices'
arxiv_id: '2504.09257'
source_url: https://arxiv.org/abs/2504.09257
tags:
- earnings
- data
- calls
- stock
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiMIC, a novel multi-modal dataset of Indian
  earnings calls, and proposes a predictive framework that combines textual, visual,
  and tabular data with numeric indicators to forecast next-day stock prices. The
  study systematically incorporates embeddings from earnings call transcripts and
  presentation slides, using classification models to generate prediction probabilities
  that feed into a regression model.
---

# MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices

## Quick Facts
- arXiv ID: 2504.09257
- Source URL: https://arxiv.org/abs/2504.09257
- Reference count: 7
- Primary result: Cascaded classification-to-regression using text and image embeddings achieves MAE: 104.787, RMSE: 188.537, MAPE: 0.334 on next-day Indian stock price prediction.

## Executive Summary
This paper introduces MiMIC, a novel multi-modal dataset of Indian earnings calls, and proposes a predictive framework that combines textual, visual, and tabular data with numeric indicators to forecast next-day stock prices. The study systematically incorporates embeddings from earnings call transcripts and presentation slides, using classification models to generate prediction probabilities that feed into a regression model. Experiments show that this cascaded approach, leveraging text and image modalities, achieves lower prediction error than models using numeric features alone or direct embedding concatenation. A zero-shot Llama-4 model performs comparably but with higher MAPE. The findings demonstrate that multi-modal integration improves financial forecasting accuracy, though limitations remain in dataset size, model scale, and lack of audio data. MiMIC is made publicly available to support further research in computational economics and multi-modal financial analysis.

## Method Summary
The method uses a cascaded framework where text and image embeddings are first passed through binary classifiers (XGBoost for text, DRF for images) to produce probability outputs, which are then used as features in a final regression model alongside numeric indicators. Text embeddings are generated using Nomic 1.5 (truncated to 128 dimensions via matryoshka), and image embeddings use Nomic Vision 1.5 with mean pooling for multiple images per instance. The final regression is a feed-forward neural network trained via H2O AutoML. The dataset contains 1,042 instances from 133 Indian companies, split temporally into 80% training, 10% validation, and 10% test sets.

## Key Results
- Cascaded classification-to-regression (DL-5) achieves MAE: 104.787, RMSE: 188.537, MAPE: 0.334, outperforming numeric-only baseline (MAE: 150.053) and direct embedding concatenation (MAE: 228.321).
- Text classifier F1: 0.675; Image classifier F1: 0.680; both contribute to improved performance when their probability outputs are used.
- Zero-shot Llama-4 model achieves comparable performance with MAPE: 0.371, slightly higher than the cascaded approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascaded classification-to-regression improves prediction over direct embedding concatenation.
- Mechanism: Binary classifiers trained on text/image embeddings learn directional signals (up/down), and their output probabilities serve as compact, regularized features for the final regression model—avoiding the curse of dimensionality from high-dimensional embeddings.
- Core assumption: The classification task captures predictive signal relevant to the regression target; probability outputs generalize better than raw embeddings.
- Evidence anchors:
  - [abstract] "Experiments show that this cascaded approach, leveraging text and image modalities, achieves lower prediction error... than models using numeric features alone or direct embedding concatenation."
  - [section 5 / Table 1] DL-2 (N+T(Em)): MAE 228.321; DL-3 (N+T(P)): MAE 125.204; DL-5 (N+T(P)+I(P)): MAE 104.787.
  - [corpus] Weak direct evidence on cascaded embeddings; neighbor papers focus on sentiment and unimodal approaches.
- Break condition: If classification F1 is near random (no signal), probability features will not help regression and may add noise.

### Mechanism 2
- Claim: Text and image modalities provide complementary predictive signal for next-day prices.
- Mechanism: Transcripts encode verbal narrative/management tone; presentation images encode structured visuals (charts, tables). Each modality captures different aspects of earnings calls; their signals aggregate when combined via cascaded probabilities.
- Core assumption: Visual presentation content contains price-relevant information not fully redundant with transcript text.
- Evidence anchors:
  - [abstract] "combines textual, visual, and tabular data with numeric indicators to forecast next-day stock prices."
  - [section 4.2] "Visual Data: Presentation slides used during earnings calls were collected... charts, graphs, and images were preserved."
  - [section 5] Text classifier F1: 0.675; Image classifier F1: 0.680; both contribute to improved DL-5 performance.
  - [corpus] Neighbor work on multimodal earnings calls (Sawhney et al.) shows verbal-vocal integration helps; limited direct evidence on image+text for Indian markets.
- Break condition: If presentation images are highly redundant with transcript text (no unique signal), image probabilities will not improve over text-only cascade.

### Mechanism 3
- Claim: Dimensionality reduction via matryoshka embeddings enables effective learning with limited data.
- Mechanism: Truncating Nomic embeddings to 128 dimensions reduces overfitting risk with only 832 training instances, allowing feed-forward networks to learn without excessive parameter-to-sample ratios.
- Core assumption: 128-dimensional truncated embeddings retain sufficient semantic information for classification.
- Evidence anchors:
  - [section 5] "We used matryoshka representation learning to truncate the dimension of embeddings to 128. This was essential as we had only 832 instances to train the regression models."
  - [corpus] No direct corpus evidence on matryoshka for financial tasks; related work uses standard embeddings.
- Break condition: If critical predictive information is lost at 128 dimensions, classifier F1 will drop and cascade benefits diminish.

## Foundational Learning

- Concept: Multi-modal fusion
  - Why needed here: The framework integrates numeric, text, and image modalities; understanding late vs early fusion is critical for architecture choices.
  - Quick check question: Can you explain why concatenating raw embeddings (early fusion) underperformed compared to cascaded probability fusion in this paper?

- Concept: Embedding truncation for small data
  - Why needed here: With only 832 training samples, high-dimensional embeddings (768+) would cause overfitting; matryoshka truncation addresses this.
  - Quick check question: What would happen to model variance if you used full-dimension embeddings without regularization on 832 samples?

- Concept: Classification-to-regression cascade
  - Why needed here: The core innovation is using classifier probabilities as intermediate features for regression.
  - Quick check question: If the binary classifier has F1 = 0.5 (random), would adding its probability output help the regression model? Why or why not?

## Architecture Onboarding

- Component map:
  - Data layer: Transcripts (text), presentations (images, tables), numeric features (fundamentals, technicals, macro)
  - Embedding layer: Nomic 1.5 (text → 128-dim), Nomic Vision 1.5 (images → mean-pooled, 128-dim)
  - Classification layer: XGBoost (text), DRF (image) → probability outputs
  - Regression layer: Feed-forward neural network (DL-5) taking numeric + text_prob + image_prob

- Critical path:
  1. Collect multi-modal data and ensure transcript-presentation pairing exists
  2. Extract and truncate embeddings (128-dim)
  3. Train binary classifiers separately per modality; validate F1 > 0.6
  4. Feed probabilities into regression with numeric features
  5. Evaluate on temporal hold-out test set (post-Aug 2024)

- Design tradeoffs:
  - Cascaded vs direct fusion: Cascaded adds training complexity but improves performance; direct is simpler but degrades accuracy.
  - Embedding dimension: Higher preserves information but increases overfitting risk with small N.
  - Model scale: Small feed-forward networks used due to data constraints; larger models may not converge.

- Failure signatures:
  - Direct embedding concatenation: MAE increases (228 vs 150 baseline) → indicates overfitting or signal dilution.
  - Low classifier F1 (<0.55): Probability features add noise; expect no improvement over numeric-only baseline.
  - Missing modality data: Dataset reduces from 1,042 to usable subset; selection bias may skew results.

- First 3 experiments:
  1. Replicate DL-1 (numeric-only regression) to establish baseline MAE (~150) on the same temporal split.
  2. Train text classifier on truncated embeddings; verify F1 ≈ 0.67; add probability to regression and confirm MAE drop to ~125.
  3. Train image classifier; verify F1 ≈ 0.68; add image probability and confirm final MAE ≈ 104–108.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating audio modality from earnings calls (vocal cues, speaking patterns, acoustic features) significantly improve prediction accuracy beyond the current text-visual-tabular framework?
- Basis in paper: [explicit] "One key direction involves expanding the current multi-modal framework to incorporate the audio modality." Also noted: "We attempted to collect audio data for earnings calls, but it was unavailable in the majority of cases."
- Why unresolved: Audio data was unavailable for most calls in the Indian market context, preventing investigation of verbal-vocal coherence effects shown beneficial in prior US-market studies (Qin and Yang 2019; Sawhney et al. 2020a).
- What evidence would resolve it: A systematic comparison of model performance with and without audio features on a dataset where audio is available for all instances.

### Open Question 2
- Question: Can converting visual elements (charts, graphs, plots) from presentation slides into structured textual representations improve predictive performance compared to direct image embedding approaches?
- Basis in paper: [explicit] "Converting the visual elements like charts and plots into texts, and incorporating them in the model is a potential avenue for additional research."
- Why unresolved: The current approach uses Nomic Vision embeddings via mean pooling, which may lose quantitative information encoded in charts that could be more precisely captured through OCR-to-table or chart-to-text conversion.
- What evidence would resolve it: Comparative experiments evaluating chart-to-text conversion pipelines against raw image embeddings on prediction error metrics (MAE, RMSE, MAPE).

### Open Question 3
- Question: Why does the cascaded classification-to-regression approach outperform direct embedding concatenation, and does this finding generalize to other financial forecasting tasks?
- Basis in paper: [inferred] The paper reports that direct incorporation of embeddings caused "performance degradation" (DL-4: MAE 271.350), while cascaded probabilities yielded MAE 104.787. The mechanism for this improvement is not explained.
- Why unresolved: The paper does not analyze why learned probability features from classifiers transfer better than raw embeddings, leaving an interpretability gap in the proposed architecture.
- What evidence would resolve it: Ablation studies probing intermediate representations, feature importance analysis, and testing the cascaded framework on other prediction horizons or market contexts.

### Open Question 4
- Question: Can models predict intra-call price movements in real-time, capturing market reactions during the earnings call itself rather than only next-day opening prices?
- Basis in paper: [explicit] "Another area for future exploration is to move beyond next-day price predictions and investigate more granular, intra-call price movements. Developing models capable of predicting stock price fluctuations during the earnings call itself would be of significant practical value to traders and investors."
- Why unresolved: The current dataset and framework target only d+1 opening prices; intra-call data would require higher-frequency price observations synchronized with transcript timestamps.
- What evidence would resolve it: Construction of a temporally-aligned dataset linking call segments to minute-level or tick-level price changes, followed by segmentation-based prediction experiments.

## Limitations

- Dataset scale: Only 1,042 total instances (832 for training) limits model complexity and generalizability across market regimes.
- Temporal robustness: Short post-Aug 2024 test window raises concerns about performance stability during different market conditions.
- Single prediction horizon: Exclusive focus on next-day price prediction without validation on longer horizons or different market phases.

## Confidence

- High confidence: Cascaded classification-to-regression improves prediction over direct embedding concatenation (supported by direct numerical comparisons showing MAE reduction from 228 to 104.787).
- Medium confidence: Text and image modalities provide complementary predictive signal (F1 scores of 0.675 and 0.680 suggest classifiers capture signal, but unique image contribution remains partially inferred).
- Medium confidence: Dimensionality reduction via matryoshka embeddings enables effective learning with limited data (128-dim truncation is stated as "essential" but direct ablation absent).

## Next Checks

1. **Temporal robustness test**: Re-evaluate the cascaded model on multiple non-overlapping time windows spanning different market regimes (bull/bear/flat) to verify consistent performance gains beyond the single test period.

2. **Ablation of classifier probability inputs**: Systematically remove text probability and image probability features from the regression model (testing numeric-only, numeric+text, numeric+image, and numeric+both) to quantify each modality's marginal contribution.

3. **Embedding dimension sensitivity**: Train classifiers and the final regression model using 64, 128, and 256-dimensional embeddings to identify optimal truncation level and test the matryoshka assumption that 128 dimensions retain sufficient signal.