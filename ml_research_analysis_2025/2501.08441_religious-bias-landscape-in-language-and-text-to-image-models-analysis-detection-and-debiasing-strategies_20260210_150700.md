---
ver: rpa2
title: 'Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection,
  and Debiasing Strategies'
arxiv_id: '2501.08441'
source_url: https://arxiv.org/abs/2501.08441
tags:
- religion
- bias
- mask
- language
- unbiased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates religious bias in both language and text-to-image
  models, focusing on open-source and closed-source systems. It analyzes bias across
  various tasks, including mask filling, prompt completion, and image generation,
  using approximately 400 unique prompts.
---

# Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies

## Quick Facts
- arXiv ID: 2501.08441
- Source URL: https://arxiv.org/abs/2501.08441
- Reference count: 25
- Primary result: Investigates religious bias in language and text-to-image models, finding significant anti-Islam bias, particularly associating it with violence and terrorism.

## Executive Summary
This study investigates religious bias across language and text-to-image models, analyzing both open-source and closed-source systems. The research examines bias in mask-filling, prompt completion, and image generation tasks using approximately 400 unique prompts. Results reveal significant biases, particularly against Islam, with models disproportionately associating it with negative attributes such as violence and terrorism. The study also explores cross-domain biases involving gender, age, and nationality. While debiasing techniques like positive term augmentation and bias mitigation instructions effectively reduce bias in text generation, image generation models still exhibit notable biases, especially in associating Muslims with terrorism. The findings underscore the need for fairer AI systems to ensure global acceptability and ethical standards.

## Method Summary
The study employs a comprehensive approach to detect and mitigate religious bias across multiple model types. Researchers use approximately 400 unique prompts: 100 for mask filling, 100 for prompt completion, and image generation prompts using negative adjectives. Bias is quantified using Religious Bias Score (RBS) = (Count of biased outputs / Total prompts) × 100. Three human annotators label outputs as biased/unbiased following detailed guidelines. Debiasing interventions include Positive Term Augmentation (prepending positive adjectives) and Bias Mitigation Instructions (prepending corrective statements). The study analyzes Pre-trained Language Models (BERT, RoBERTa, ALBERT, DistilBERT), Large Language Models (GPT-2, Mixtral-8x7B, Vicuna-13B, Llama 3-70B), and Text-to-Image models (DALL·E 3, Stable Diffusion 3).

## Key Results
- BERT predicts "islam" with 0.38 probability for "The terrorist's religion is [MASK]"; DistilBERT reaches 0.72
- GPT-2 shows 23% biased prompt completions, with 17% specifically anti-Muslim
- BERT RBS drops from 46% to 23% after debiasing; GPT-2 bias reduced from 23% to 8%
- DALL·E 3 shows 18% bias for "terrorist" prompts with content filters; Stable Diffusion 3 shows 78% without filters
- ALBERT still shows 14% RBS after debiasing, indicating residual bias

## Why This Works (Mechanism)

### Mechanism 1: Statistical Co-occurrence Propagation from Training Data
Religious bias appears to originate from statistical patterns in uncurated pre-training corpora, where certain religions co-occur disproportionately with negative descriptors. Language models trained on internet-scale text learn token prediction probabilities based on contextual co-occurrence. When negative adjectives frequently appear near specific religious terms in training data, the model encodes these associations as higher-probability completions. The mechanism assumes training corpora reflect societal biases and media representation patterns that create systematic co-occurrence asymmetries.

### Mechanism 2: Contextual Priming in Autoregressive Generation
Prompt structure activates latent bias associations by priming the model's attention toward stereotypical pathways. In autoregressive models, each token conditions subsequent predictions. When prompts contain negative descriptors near religion-predictable slots, the attention mechanism prioritizes contextually-associated tokens. GPT-2's completion—"Muslims, so the only way to do that would be to become a terrorist"—demonstrates how prompt framing can cascade into biased outputs. The mechanism assumes models lack intrinsic semantic understanding and instead exploit surface-level statistical regularities between prompt context and training-data patterns.

### Mechanism 3: Instruction-Based Attention Redirection for Debiasing
Prepended debiasing instructions shift model attention away from stereotypical associations toward neutral/factual responses. Two intervention strategies were tested: Positive Term Augmentation inserts positive adjectives before demonyms, and Bias Mitigation Instructions prepend corrective statements. These appear to override default statistical priors by providing alternative context that the model attends to during generation. The mechanism assumes instruction-tuned models can follow prepended directives that compete with learned statistical associations.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: Understanding how BERT-family models predict [MASK] tokens explains why they produce biased completions—their training objective is to maximize probability of contextually-plausible tokens, which includes stereotypical associations.
  - Quick check question: Given the sentence "The [MASK] religion is violent," would an MLM predict based on factual knowledge or statistical co-occurrence patterns from training data?

- **Concept: Autoregressive Generation and Prompt Sensitivity**
  - Why needed here: GPT-2's biased completions emerge from sequential token prediction where early tokens condition later ones. Understanding this explains why prompt engineering (debiasing instructions) can alter output trajectories.
  - Quick check question: If you prepend "Muslims are peaceful" to a potentially biased prompt, where in the generation process does this context exert influence?

- **Concept: Cross-Modal Bias Transfer (Text-to-Image)**
  - Why needed here: T2I models exhibit religious bias in visual outputs despite different architectures. Understanding how text embeddings condition image generation clarifies why bias propagates across modalities.
  - Quick check question: When Stable Diffusion 3 generates a "religious terrorist" image, is the bias in the text encoder, the diffusion process, or the training image distribution?

## Architecture Onboarding

- **Component map:**
  - PLMs (BERT, RoBERTa, ALBERT, DistilBERT) -> Masked language models; bias emerges in [MASK] prediction logits
  - Open-source LLMs (GPT-2, Mixtral-8x7B, Vicuna-13B, Llama 3-70B) -> Autoregressive; bias emerges in token-by-token generation
  - Closed-source LLMs (GPT-3.5, GPT-4) -> API-only; safety filters and RLHF reduce observable bias
  - T2I Models (DALL·E 3, Stable Diffusion 3) -> Text encoder → diffusion latent space → image decoder

- **Critical path:**
  1. Define bias detection prompts (mask-filling or completion templates)
  2. Generate outputs from target model
  3. Annotate outputs (human or heuristic) for biased/unbiased classification
  4. Compute Religious Bias Score (RBS): (# biased outputs / total prompts) × 100
  5. Apply debiasing interventions (positive term augmentation or mitigation instructions)
  6. Re-run prompts and compare RBS before/after

- **Design tradeoffs:**
  - Filter vs. No Filter: DALL·E 3's content filter reduces biased image generation but can be bypassed; SD3 has no filter but allows unfiltered analysis
  - Prompt Engineering vs. Model Modification: Debiasing techniques are inference-time interventions, not training modifications
  - Annotation Cost: Human annotation is reliable but expensive; automated classifiers could scale but may miss nuanced bias

- **Failure signatures:**
  - Refusal to respond: LLMs with safety training may return "no response" for sensitive prompts
  - Filter bypass: DALL·E 3 can be tricked via step-by-step prompting
  - Residual bias after debiasing: ALBERT still shows 14% RBS after debiasing
  - Cross-category exacerbation: Targeted religious debiasing may amplify other bias dimensions

- **First 3 experiments:**
  1. Reproduce mask-filling RBS for a single PLM using BERT-base with 10 prompts from Appendix B; extract top-5 predictions per prompt; compute RBS manually
  2. Test positive term augmentation on GPT-2: Run 10 completion prompts with and without positive prefix; annotate outputs for bias; measure RBS reduction
  3. Probe T2I filter robustness: Attempt to generate "religious terrorist" image via direct prompt vs. 3-step bypass; document success rate and characterize visual bias

## Open Questions the Paper Calls Out

### Open Question 1
How do religious biases manifest in multilingual or non-English language models compared to the English-centric systems analyzed? The authors explicitly state expanding research to include languages other than English is "essential" to understanding unique cultural and linguistic influences on bias formation. The current study restricted its dataset and prompt engineering exclusively to English content. A replication using prompts in diverse languages (e.g., Arabic, Spanish, Hindi) with monolingual and multilingual model variants would resolve this.

### Open Question 2
Does the magnitude and nature of religious bias remain consistent when analyzing minority or non-dominant religions beyond Christianity, Islam, and Hinduism? The paper acknowledges the cross-domain analysis was limited to three major religions and suggests future research must include "other existing religions" for comprehensive understanding. The current methodological scope excluded minor religious groups, leaving their specific bias profiles unexplored. Extending the prompt set to include terms related to Judaism, Buddhism, Sikhism, and other faiths would calculate RBS for these groups.

### Open Question 3
Can foundational, algorithmic interventions achieve more permanent bias mitigation than the prompt-engineering strategies evaluated? The authors conclude that while techniques like Positive Term Augmentation are effective, they are not universal solutions and do not address "underlying algorithmic and data-driven causes." The study focused on surface-level debiasing via input manipulation rather than model weight adjustments or data curation. A comparative study measuring persistence of bias reduction in models fine-tuned with debiased datasets versus models relying solely on corrective prompting would resolve this.

## Limitations
- Reliance on statistical associations without establishing causal mechanisms
- Focus exclusively on English-language models, limiting generalizability
- Debiasing techniques are inference-time interventions rather than addressing root causes
- Human annotation introduces subjectivity despite high inter-annotator agreement

## Confidence
- **High Confidence:** Quantitative RBS measurements showing Islam-associated bias in mask-filling tasks (BERT: 46%, DistilBERT: 72% for "terrorist" prompts)
- **Medium Confidence:** Effectiveness of debiasing techniques for text generation models (GPT-2 bias reduction from 23% to 8%)
- **Low Confidence:** Claims about T2I model bias mechanisms and debiasing effectiveness

## Next Checks
1. Use attention visualization tools to examine how religious terms activate during biased vs. debiased prompt processing, verifying the "contextual priming" mechanism
2. Test the same bias detection and debiasing protocols on multilingual models (e.g., mBERT) to assess whether bias patterns generalize beyond English
3. Document how content filter effectiveness and refusal rates change across API versions for GPT-3.5/4 and DALL·E 3 when processing religious bias prompts