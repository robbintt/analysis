---
ver: rpa2
title: 'D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex
  Tool Use'
arxiv_id: '2602.02160'
source_url: https://arxiv.org/abs/2602.02160
tags:
- reasoning
- user
- tool
- task
- reservation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "D-CORE addresses the \u201CLazy Reasoning\u201D problem in large\
  \ reasoning models, where models generate excessive but meaningless reasoning tokens\
  \ without effectively decomposing complex tool-use tasks. The method introduces\
  \ a two-stage training framework: first using self-distillation to incentivize task\
  \ decomposition and subtask execution, then applying diversity-aware reinforcement\
  \ learning (DA-GRPO) to restore reflective reasoning while maintaining decomposition\
  \ capabilities."
---

# D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use

## Quick Facts
- arXiv ID: 2602.02160
- Source URL: https://arxiv.org/abs/2602.02160
- Reference count: 40
- D-CORE-14B achieves 79.3% accuracy on BFCLv3, establishing a new state-of-the-art that outperforms 70B models despite being 5× smaller

## Executive Summary
D-CORE addresses the "Lazy Reasoning" problem in large reasoning models, where models generate excessive but meaningless reasoning tokens without effectively decomposing complex tool-use tasks. The method introduces a two-stage training framework: first using self-distillation to incentivize task decomposition and subtask execution, then applying diversity-aware reinforcement learning (DA-GRPO) to restore reflective reasoning while maintaining decomposition capabilities. DA-GRPO incorporates an entropy-based advantage term to prevent gradient collapse and encourage reasoning diversity. Experiments show D-CORE-8B achieves 77.7% accuracy on BFCLv3, surpassing previous 8B models by 5.7%, while D-CORE-14B reaches 79.3% accuracy, establishing a new state-of-the-art that outperforms 70B models despite being 5× smaller.

## Method Summary
D-CORE is a two-stage training framework that first uses self-distillation to incentivize task decomposition and subtask execution, then applies diversity-aware reinforcement learning (DA-GRPO) to restore reflective reasoning while maintaining decomposition capabilities. The self-distillation phase decomposes queries into subtasks using reference trajectories, generates reasoning-tool call pairs for each subtask, and composes complete trajectories for supervised fine-tuning. DA-GRPO then replaces zero-advantage values with an entropy-based term to prevent gradient collapse when reward variance is low. The method trains on 40k self-distillation instances plus 5k RL instances from open-source datasets, using 8×A100 GPUs for training Qwen3-8B/14B models over 3 epochs per stage.

## Key Results
- D-CORE-8B achieves 77.7% accuracy on BFCLv3, surpassing previous 8B models by 5.7%
- D-CORE-14B reaches 79.3% accuracy on BFCLv3, establishing a new state-of-the-art that outperforms 70B models despite being 5× smaller
- The method successfully reduces "Lazy Reasoning" behavior while maintaining multi-turn reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Self-Distillation Incentivizes Task Decomposition
If LRMs are provided with explicit structural guidance during data generation, they can synthesize their own decomposition trajectories without stronger teacher models. The framework decomposes queries Q into subtasks S using reference trajectories as guidance, generates reasoning-tool call pairs for each subtask, then composes complete trajectories for SFT. This converts the model's latent decomposition capacity into trainable patterns.

### Mechanism 2: DA-GRPO Prevents Gradient Collapse via Entropy-Based Advantage
When self-distilled models produce near-zero reward variance, standard GRPO advantages collapse to zero; an entropy-based advantage term can restore gradient signal while encouraging diverse reasoning. DA-GRPO replaces zero-advantage values with ψ(H) = min(α·H_detach, δ), where H is token-level entropy.

### Mechanism 3: Compositional Trajectory Construction Links Decomposition to Execution
Decomposing task generation into (subtask → reasoning → tool call → output) stages, then recomposing, creates training data that explicitly teaches sequential dependency handling. Sequential subtasks are processed iteratively with context updates; parallel subtasks are handled simultaneously; irrelevant queries receive explanation trajectories.

## Foundational Learning

- **Advantage Functions in Policy Gradient Methods**
  - Why needed here: GRPO relies on advantage A_i,t = (R_i - mean) / std to determine which actions to reinforce
  - Quick check question: What happens to the advantage term when all sampled rollouts achieve identical rewards?

- **Entropy Regularization in RL**
  - Why needed here: DA-GRPO's core innovation substitutes entropy for advantage when variance is low
  - Quick check question: Why does maximum entropy encourage uniform action distributions rather than deterministic ones?

- **Knowledge Distillation vs. Self-Distillation**
  - Why needed here: D-CORE eliminates the teacher model requirement
  - Quick check question: In self-distillation, what prevents the model from simply copying its own errors?

## Architecture Onboarding

- **Component map:** Seed Data → Decomposition Prompting → Subtask Generation → Per-Subtask Reasoning+Tool Calls → Composition → Verification → SFT (Self-Distillation) → DA-GRPO Training → Final Model

- **Critical path:** The verification step (Algorithm 1, line 20) filters trajectories—high rejection rates here cascade into insufficient RL training data

- **Design tradeoffs:**
  - Higher α in DA-GRPO increases exploration but risks reward interference (α=0.1 optimal vs. α=1.0 degraded)
  - Larger self-distillation datasets improve multi-turn capability (40k > 20k > 10k per Table 4) but require more compute
  - Ground-truth reference trajectories boost decomposition success (93.2%) vs. pseudo-labels (92.8%)—marginal gain for annotation cost

- **Failure signatures:**
  - Lazy Reasoning persists: Check decomposition success rate; if <80%, subtask generation is failing
  - RL rewards plateau early: Examine reward variance—near-zero std indicates need for DA-GRPO adjustment
  - Multi-turn accuracy drops: Verify composition template correctly chains context between sequential subtasks

- **First 3 experiments:**
  1. Reproduce Figure 3b on your target LRM: quantify lazy reasoning ratio across task types to confirm the problem exists
  2. Ablate self-distillation sample size (Table 4 pattern): validate that 10k→40k yields diminishing returns curve for your compute budget
  3. Sweep α ∈ {0.01, 0.1, 0.4, 1.0} with fixed δ=0.5: identify your model's exploration-exploitation balance point before full training

## Open Questions the Paper Calls Out

- **Can the D-CORE framework effectively transfer to multimodal reasoning domains where task decomposition involves visual or auditory inputs?**
  - Basis in paper: The conclusion states, "Future work will extend this framework to multimodal models," indicating this is a planned but currently unexplored extension of the method
  - Why unresolved: The current implementation and experiments are limited to text-based tool use (BFCLv3, τ-bench), and it is unclear if "Lazy Reasoning" manifests similarly in visual contexts
  - What evidence would resolve it: Application of D-CORE to multimodal benchmarks (e.g., OSWorld) demonstrating that self-distillation and entropy-aware RL improve decomposition in non-textual modalities

- **Can the self-distillation phase be decoupled from the reliance on ground-truth reference trajectories to enable fully automated scaling?**
  - Basis in paper: Algorithm 1 requires reference trajectories (Y*), and Table 6 shows a performance drop when using pseudo-labels (29.6%) versus ground truth (37.1%), suggesting a dependency on high-quality supervision
  - Why unresolved: While pseudo-labels offer a scalable alternative, they currently result in lower accuracy, leaving a gap between automated scaling and optimal performance
  - What evidence would resolve it: A modified distillation mechanism that achieves comparable performance to ground-truth supervision using only model-generated pseudo-labels or weak supervision

- **Is the optimal entropy scaling coefficient (α) in DA-GRPO dependent on model scale or task complexity, and can it be automated?**
  - Basis in paper: The authors note in Section 4.3 that "excessive α causes exploration collapse," and they manually selected α=0.1 for both 8B and 14B models without verifying if this holds for larger scales or different domains
  - Why unresolved: The paper demonstrates a trade-off between exploration and exploitation but does not provide a mechanism for dynamically determining the optimal entropy advantage
  - What evidence would resolve it: An ablation study across varying model sizes (e.g., 32B, 70B) and task difficulties showing whether a fixed α remains optimal or if an adaptive schedule is required

## Limitations
- Self-Distillation Dependency: The method creates a circular dependency risk—the model can only reinforce its existing decomposition capabilities rather than discover novel strategies
- Evaluation Domain Narrowness: Results may not generalize to domains requiring different decomposition patterns beyond tool use scenarios
- Architectural Assumptions: The DA-GRPO entropy-based advantage mechanism assumes high-entropy tokens correlate with beneficial reflection, which isn't rigorously proven

## Confidence

**High Confidence (95%+):** The lazy reasoning problem exists and is measurable. The DA-GRPO entropy mechanism prevents gradient collapse when reward variance approaches zero. The compositional trajectory construction works as specified.

**Medium Confidence (70-95%):** D-CORE's two-stage framework outperforms comparable models on BFCLv3 and τ-bench. Self-distillation effectively incentivizes decomposition more than direct instruction. The α=0.1 hyperparameter is optimal across scenarios.

**Low Confidence (below 70%):** The method generalizes to model families beyond Qwen3. The computational overhead is justified by performance gains. The entropy-based advantage consistently correlates with improved reasoning quality rather than just token diversity.

## Next Checks

1. **Cross-Model Generalization Test:** Apply D-CORE to Llama-3.1-8B and DeepSeek-Coder-6.7B to verify the self-distillation mechanism works beyond Qwen3. Measure decomposition success rates and lazy reasoning ratios before/after training.

2. **Domain Transfer Validation:** Evaluate D-CORE-14B on mathematical reasoning benchmarks (GSM8K, MATH) and code generation tasks (HumanEval) to assess whether decomposition skills transfer across domains or remain tool-use specific.

3. **Ablation of Entropy Mechanism:** Replace DA-GRPO's entropy-based advantage with standard GRPO plus explicit diversity regularization (e.g., KL divergence from uniform distribution). Compare lazy reasoning ratios and accuracy to isolate whether entropy correlation or diversity enforcement drives performance.