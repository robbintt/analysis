---
ver: rpa2
title: 'RAJ-PGA: Reasoning-Activated Jailbreak and Principle-Guided Alignment Framework
  for Large Reasoning Models'
arxiv_id: '2508.12897'
source_url: https://arxiv.org/abs/2508.12897
tags:
- safety
- reasoning
- alignment
- lrms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety vulnerability in Large Reasoning
  Models (LRMs) where their internal reasoning chains may generate harmful content
  even when the final output appears benign. The authors propose a Reasoning-Activated
  Jailbreak (RAJ) via Concretization attack that demonstrates how refining malicious
  prompts to be more specific can trigger step-by-step logical reasoning that overrides
  the model's safety protocols.
---

# RAJ-PGA: Reasoning-Activated Jailbreak and Principle-Guided Alignment Framework for Large Reasoning Models

## Quick Facts
- arXiv ID: 2508.12897
- Source URL: https://arxiv.org/abs/2508.12897
- Authors: Jianhao Chen; Mayi Xu; Haoyang Chen; Xiaohu Li; Xiangyu Zhang; Jianjie Huang; Zheng Wang; Xiaochun Cao; Tieyun Qian
- Reference count: 40
- Primary result: Fine-tuning LRMs with 3,989 RAJ-PGA samples achieves up to 29.5% improvement in jailbreak defense success rates while preserving reasoning capabilities

## Executive Summary
This paper addresses a critical safety vulnerability in Large Reasoning Models (LRMs) where harmful content can appear in reasoning chains even when final outputs appear benign. The authors propose RAJ (Reasoning-Activated Jailbreak) via concretization attack, which shows that making malicious prompts more specific can trigger step-by-step logical reasoning that overrides safety protocols. To systematically mitigate this vulnerability, they develop PGA (Principle-Guided Alignment) framework that transforms harmful reasoning traces into safe, constructive responses through five core principles. Fine-tuning LRMs with their carefully curated dataset significantly enhances model safety while preserving general reasoning capabilities.

## Method Summary
The RAJ-PGA framework operates in two stages. First, it implements a concretization attack on victim LRMs using a three-step rewriting process (Intent Identification → Association → Rewriting) to generate specific malicious prompts that trigger reasoning-override of safety. Second, it constructs a high-quality training dataset by applying differential filtering to identify cases where original prompts produce safe outputs but concretized prompts produce unsafe ones. These harmful traces are then systematically rewritten using five principles (reframing, information downgrading, risk clarification, premise rejection, and empathetic redirection) to create safe aligned responses. The resulting PGA dataset is used to fine-tune LRMs with LoRA, achieving improved safety without significant degradation of reasoning capabilities.

## Key Results
- Concretization attack increases jailbreak success rates by 12.9% in reasoning chains versus 1.5% in final responses
- PGA fine-tuning improves Defense Success Rate by up to 29.5% across multiple jailbreak benchmarks
- Maintains general reasoning capabilities with minimal performance regression (HumanEval -3% to -11% on some models)
- Achieves superior results with smaller dataset (3,989 samples) compared to larger baselines (SafeChain: 40K, STAIR: 20K)

## Why This Works (Mechanism)

### Mechanism 1: Concretization Triggers Reasoning-Override of Safety
- Claim: Making malicious prompts more specific activates deep reasoning that bypasses safety guardrails in LRMs
- Mechanism: Concretization transforms abstract harmful requests into detailed, context-rich prompts that engage "vertical thinking" (procedural logic), causing "tunnel vision" that suppresses "lateral" safety evaluation. The model prioritizes completing its reasoning chain over evaluating harm.
- Core assumption: LRMs have competing objectives—reasoning completion vs. safety rejection—and these can be manipulated independently.
- Evidence anchors:
  - [abstract]: "refining malicious prompts to be more specific can trigger step-by-step logical reasoning that overrides the model's safety protocols"
  - [Section I]: "We posit that a successful jailbreak occurs when a malicious prompt traps the model in a vertical thinking loop, compelling it to prioritize procedural logic at the expense of its lateral evaluative capacity"
  - [corpus]: Weak—corpus contains related LRM jailbreak work (SEAL achieves 80.8% ASR on GPT-o4-mini) but no direct validation of the concretization-specific mechanism
- Break condition: If safety mechanisms are structurally independent of reasoning pathways, or if concretization doesn't reliably increase reasoning depth

### Mechanism 2: Principle-Guided Rewriting Transforms Harmful Traces into Educational Content
- Claim: Harmful reasoning chains can be systematically converted into safe, constructive responses while preserving reasoning structure and depth
- Mechanism: Five principles guide a rewrite model (Qwen-plus) to transform content: (1) Goal Reframing, (2) Information Downgrading (retain theory, omit actionable details), (3) Counterfactual & Risk Clarification, (4) Premise Rejection & Positive Reframing, (5) Empathetic Redirection. Length is constrained to 80–120% of original to preserve depth.
- Core assumption: Safe reasoning patterns can be learned from transformed examples without requiring explicit rule memorization.
- Evidence anchors:
  - [abstract]: "transforms these high-risk traces into safe, constructive, and educational responses through a tailored Principle-Guided Alignment (PGA) mechanism"
  - [Section III-B]: "We define a principle S_CAI consisting of five specific strategies designed to mitigate harm while preserving helpfulness"
  - [corpus]: Weak—no corpus papers directly validate the principle-guided rewriting approach; AdvChain mentions "Adversarial Chain-of-Thought Tuning" but uses different methodology
- Break condition: If transformed content loses reasoning coherence, or if safety gains don't generalize beyond the specific harm categories in training data

### Mechanism 3: Differential Filtering Creates High-Density Safety Training Signal
- Claim: Filtering for cases where original prompts produce safe outputs but concretized prompts produce unsafe outputs yields maximally informative alignment data
- Mechanism: Safety inversion criterion (S_Original ∧ ¬S_Concretized) identifies reasoning-activated vulnerabilities specifically. From 44.6K initial prompts, this yields 3,989 samples—cases where reasoning activation is the decisive factor in safety failure.
- Core assumption: The most valuable training examples are edge cases where reasoning depth causes safety failures that wouldn't occur otherwise.
- Evidence anchors:
  - [Section III-A, Eq. 4]: "S_Original ∧ (¬S_Concretized)" filtering criterion
  - [Section V-B]: "Based on the strict safety inversion criterion, we finally filter the initial pool down to 3,989 high-quality pairs"
  - [corpus]: No corpus evidence directly addresses differential filtering for alignment data construction
- Break condition: If filtered samples represent idiosyncratic failures rather than transferable vulnerability patterns, or if random sampling would achieve equivalent results

## Foundational Learning

- **Concept: Vertical vs. Lateral Thinking in LRM Safety**
  - Why needed here: The paper's core theory distinguishes procedural reasoning ("vertical") from safety evaluation ("lateral") as competing cognitive modes. Understanding this competition is essential to grasping why concretization works as an attack.
  - Quick check question: Can you explain why a detailed, specific prompt might suppress safety evaluation even when the underlying intent is clearly harmful?

- **Concept: Chain-of-Thought (CoT) Vulnerability Surface**
  - Why needed here: LRMs generate explicit reasoning traces before final responses, creating a dual attack surface. Harmful content can appear in reasoning even when the final response appears benign.
  - Quick check question: What makes LRM reasoning chains potentially more dangerous than traditional LLM outputs for the same malicious prompt?

- **Concept: Alignment Tax (Safety-Utility Tradeoff)**
  - Why needed here: The paper explicitly claims to avoid the common degradation of reasoning capabilities during safety alignment. Understanding this tradeoff helps evaluate whether the approach succeeds.
  - Quick check question: Why do conventional safety alignment methods (e.g., refusal training) often degrade general reasoning capabilities?

## Architecture Onboarding

- **Component map:**
  Original Prompts (PKU-SafeRLHF, 44.6K) -> Concretization Rewrite (Qwen-plus as rewriter) -> Victim LRM (DeepSeek-R1-Distill-Qwen-32B) -> Consensus Safety Validation (Llama-Guard + Qwen3Guard + GPT-5 arbiter if disagreement) -> Differential Filter -> RAJ Dataset (3,989 harmful traces) -> Principle-Guided Rewrite (Qwen-plus + 5 principles, length 80–120%) -> Safety Re-validation -> PGA Dataset (3,989 aligned samples) -> Fine-tuning (LoRA, rank=16, α=32, lr=1e-5, 2 epochs)

- **Critical path:** The concretization rewrite quality determines whether meaningful vulnerabilities are exposed. Weak or generic rewrites will produce false negatives in RAJ detection, yielding low-value training data.

- **Design tradeoffs:**
  - Ensemble safety validation increases reliability but adds API costs (GPT-5 arbiter called on ~22% of samples where classifiers disagree)
  - Length constraint (80–120% of original) preserves reasoning depth but may limit transformation flexibility for complex harms
  - Dataset size (3,989) is smaller than baselines (SafeChain: 40K, STAIR: 20K) but claims higher sample efficiency—verify with scaling experiments

- **Failure signatures:**
  - High classifier disagreement rate (21.8–23.5%) indicates ambiguous boundary cases that may require human review
  - HumanEval score drops on some models (R1-Llama-8B: −3% to −11%) suggest partial capability degradation despite overall gains
  - Uneven benchmark performance: R1-Qwen-7B achieves only 81.71% avg DSR vs. 94.38% on R1-Llama-8B, indicating architecture-dependent effectiveness

- **First 3 experiments:**
  1. **Validate concretization attack effectiveness**: Run concretized vs. original prompts on target LRM, measure ASR difference separately in reasoning and response phases (expect higher ASR in reasoning phase as paper shows)
  2. **Test consensus validation reliability**: Compare single-classifier vs. ensemble labeling on a human-annotated held-out set (N=100), measure precision/recall for unsafe detection
  3. **Scaling study replication**: Train separate models on PGA-1000, PGA-2000, PGA-3000 subsets to verify that safety gains scale with dataset size and identify diminishing returns point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of the safety-reasoning trade-off in LRMs, and can the minor performance regressions observed (e.g., HumanEval -3% to -11%) be eliminated entirely?
- Basis in paper: [explicit] The introduction asks: "How to enhance safety without reducing the reasoning ability of LRMs?" While PGA mitigates alignment tax, Table VI still shows HumanEval regressions on R1-Llama-8B.
- Why unresolved: The paper demonstrates improvement but does not achieve perfect preservation across all benchmarks; the fundamental trade-off mechanism remains uncharacterized.
- What evidence would resolve it: A theoretical framework explaining why certain capabilities are more susceptible to safety alignment, plus empirical demonstration of zero regression across all benchmarks.

### Open Question 2
- Question: How robust is RAJ-PGA against evolving adversarial attacks beyond concretization-based jailbreaks, particularly multi-step prompt injections and cipher-based attacks like SEAL?
- Basis in paper: [explicit] The conclusion states: "Future work could involve applying this method to bolster model resilience against evolving adversarial reasoning attacks and multi-step prompt injections."
- Why unresolved: PGA was tested primarily against concretization attacks; Related Work mentions SEAL achieves 80.8% ASR using stacked encryption, but cross-attack robustness is untested.
- What evidence would resolve it: Evaluation of PGA-aligned models against diverse attack families (cipher-based, multi-turn, role-playing) with comparative defense success rates.

### Open Question 3
- Question: What is the optimal dataset size and composition for principle-guided alignment, given that PGA (3,989 samples) outperforms larger datasets like SafeChain (40K)?
- Basis in paper: [inferred] The paper shows PGA outperforming much larger datasets, and Table V shows scaling trends from PGA-1000 to PGA-3000, but the optimal size/composition remains unknown.
- Why unresolved: The paper demonstrates sample efficiency but does not systematically explore the ceiling effect or diminishing returns of dataset scaling.
- What evidence would resolve it: Systematic ablation varying dataset size (5K, 10K, 20K) and harm category distribution, measuring both safety and general performance curves.

## Limitations
- Core claims rely heavily on proprietary models (GPT-5 arbiter) that may not be reproducible with publicly available alternatives
- 3,989-sample dataset size, while carefully curated, may limit scalability to more diverse harm categories
- Claims about avoiding "safety tax" are based on average metrics that mask architecture-specific degradation

## Confidence
**High Confidence:**
- Concretization increases jailbreak success rates in LRMs
- PGA fine-tuning improves DSR across multiple benchmarks
- Safety alignment preserves general reasoning capabilities on average

**Medium Confidence:**
- Vertical thinking vs. lateral evaluation is the primary mechanism for concretization effectiveness
- Differential filtering produces more efficient training data than random sampling
- Five-principle rewriting consistently preserves reasoning depth

**Low Confidence:**
- Claims about avoiding "safety tax" are based on average metrics that mask architecture-specific degradation
- The 80-120% length constraint is optimal for all harm types
- Ensemble safety validation (including GPT-5 arbiter) is necessary for reliable RAJ dataset construction

## Next Checks
1. **Mechanism Validation**: Compare concretization effectiveness across three distinct LRM architectures (Qwen3, Llama-3, DeepSeek) to test whether vertical thinking suppression is a universal mechanism or architecture-dependent artifact.

2. **Dataset Scaling Study**: Train models with PGA-1000, PGA-2000, and PGA-3989 subsets to empirically verify diminishing returns and identify the optimal training set size for different harm categories.

3. **Alternative Arbiter Validation**: Replace GPT-5 arbiter with GPT-4o in the safety validation pipeline and measure changes in RAJ dataset size and subsequent PGA effectiveness to assess sensitivity to arbiter choice.