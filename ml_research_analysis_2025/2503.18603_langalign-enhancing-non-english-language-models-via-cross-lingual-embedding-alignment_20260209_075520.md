---
ver: rpa2
title: 'LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual Embedding
  Alignment'
arxiv_id: '2503.18603'
source_url: https://arxiv.org/abs/2503.18603
tags:
- data
- align
- lang
- english
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LANGALIGN addresses the challenge of training non-English language
  models when high-quality, task-specific data is scarce. It introduces a lightweight
  adaptation layer between a multilingual language model and the task head to align
  English embeddings with target-language embeddings, enabling effective transfer
  learning without requiring full translation of datasets.
---

# LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual Embedding Alignment

## Quick Facts
- arXiv ID: 2503.18603
- Source URL: https://arxiv.org/abs/2503.18603
- Authors: Jong Myoung Kim; Young-Jun Lee; Ho-Jin Choi; Sangkeun Jung
- Reference count: 16
- One-line primary result: LANGALIGN improves non-English NLP performance by aligning English embeddings with target-language embeddings, outperforming English-only baselines while using far less translated data.

## Executive Summary
LANGALIGN addresses the challenge of training non-English language models when high-quality, task-specific data is scarce. It introduces a lightweight adaptation layer between a multilingual language model and the task head to align English embeddings with target-language embeddings, enabling effective transfer learning without requiring full translation of datasets. Evaluated on Korean, Japanese, and Chinese across sentiment analysis and natural language inference tasks, LANGALIGN consistently outperformed models trained on English data alone and matched or exceeded the performance of models trained on native-language data, while leveraging the abundance and lower cost of English resources. Ablation studies confirmed the method's effectiveness, and a reversed application demonstrated potential for cross-lingual inference.

## Method Summary
LANGALIGN introduces a lightweight adapter layer between a frozen multilingual encoder and task head to align English embeddings with target-language embeddings. The method uses a three-step process: (1) optional task-specific fine-tuning of the language model on English data, (2) training the adapter layer to minimize MSE between English embeddings and GPT-4-translated target embeddings, and (3) fine-tuning the task head on English data with the adapter layer inserted. The approach supports both fully connected (FC) and autoencoder (AE) architectures for the adapter, with FC being shallower and AE capturing a translation bottleneck. By avoiding full dataset translation, LANGALIGN significantly reduces costs while maintaining or improving performance over English-only baselines.

## Key Results
- LANGALIGN consistently outperformed English-only baselines across all target languages and tasks (Korean, Japanese, Chinese)
- Achieved comparable or better performance than models trained on native-language data using only 5K-20K translated parallel samples
- Ablation studies confirmed the importance of the three-step process, with task-specific LM tuning improving performance
- Reversed application (target-to-English alignment) showed potential for cross-lingual inference, though with slightly lower performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inserting a learned projection layer between a frozen multilingual encoder and the task head reduces the cross-lingual representational gap without modifying the backbone.
- **Mechanism:** LANGALIGN learns a function $A$ to map English embeddings $e(d_e)$ such that they approximate the embeddings of the translated equivalent $e(T(d_e))$ (Eq. 1). By minimizing Mean Squared Error (MSE) between these vectors, the layer allows a task head trained on English data to receive inputs that statistically resemble the target language's embedding distribution.
- **Core assumption:** The multilingual model (e.g., XLM-RoBERTa) encodes semantic information such that translation pairs lie in a transformable geometric relationship, even if their absolute coordinates differ.
- **Evidence anchors:**
  - [abstract] "aligning English embedding vectors with those of the target language at the interface between the language model and the task header."
  - [section 3.1] "LANGALIGN is a layer that learns to convert the embedding of English data into the embedding of translated data."
  - [corpus] "Languages are Modalities: Cross-Lingual Alignment via Encoder Injection" supports the general efficacy of injection-based alignment for non-English tasks.
- **Break condition:** If the backbone model is updated (unfrozen) during alignment training, the embedding geometry shifts, invalidating the learned projection $A$.

### Mechanism 2
- **Claim:** Sequential optimization—specifically, task-specific tuning of the backbone prior to alignment training—preserves task-relevant features that would otherwise be lost.
- **Mechanism:** By first fine-tuning the Language Model (LM) on English task data (Step 1), the model generates embeddings specifically shaped for the downstream task. LANGALIGN is then trained on these *specialized* embeddings. This ensures the alignment transformation preserves the task-relevant geometry rather than aligning generic, non-task-specific representations.
- **Core assumption:** Task-specific knowledge is encoded in the embedding geometry of the fine-tuned LM, and this geometry is linearly or piece-wise linearly transferable to the target language.
- **Evidence anchors:**
  - [section 3.2] "Step 1: Tuning for Embedding... Although optional, omitting this step led to performance degradation."
  - [section 4.2.1] High cosine similarity (e.g., 0.967 for NLI) confirms the transformation $A$ effectively approximates the translated embedding space.
- **Break condition:** If Step 1 is skipped, the alignment layer learns a generic transformation that may not align the specific decision boundaries needed for the target task.

### Mechanism 3
- **Claim:** Reversing the alignment direction (Target $\to$ English) enables zero-shot or few-shot cross-lingual inference on English-centric models.
- **Mechanism:** The "Reverse LANGALIGN" is trained to map target language embeddings $e(d_t)$ to English embeddings $e(d_e)$. When applied to a model trained exclusively on English data, it allows the model to process target language inputs by projecting them into the English space the model "expects."
- **Core assumption:** The mapping function is approximately invertible, and the English task head has sufficient robustness to handle slightly perturbed inputs resulting from the reverse projection.
- **Evidence anchors:**
  - [abstract] "LANGALIGN can be applied in reverse to convert target language data into a format that an English-based model can process."
  - [section 4.2.3] "REV-LANGALIGN... showed slightly lower performance than NMT GPT-4... [but] at 50K... achieved performance comparable."
  - [corpus] "Can you map it to English? The Role of Cross-Lingual Alignment..." supports the hypothesis that aligning internal representations to English drives multilingual performance.
- **Break condition:** If the English training data distribution differs significantly from the reverse-mapped target data, the task head may still fail to generalize despite the alignment.

## Foundational Learning

- **Concept: Cross-Lingual Representational Geometry**
  - **Why needed here:** To understand why English embeddings must be transformed to match target language embeddings rather than just using them directly.
  - **Quick check question:** Why can't we simply feed English data directly into a multilingual model to classify Korean text without alignment?

- **Concept: Frozen Backbone Fine-Tuning**
  - **Why needed here:** The method relies on freezing the LM weights while training the adapter layer to ensure the embedding space remains static for the alignment function to learn.
  - **Quick check question:** What happens to the alignment layer's validity if the underlying LM weights change during Step 2?

- **Concept: Autoencoders vs. Fully Connected Layers for Transformation**
  - **Why needed here:** The paper experiments with both architectures (FC vs. AE) to find the most efficient mapping function between languages.
  - **Quick check question:** Which architecture (FC or AE) showed better performance in the ablation study for larger data sizes (Table 3)?

## Architecture Onboarding

- **Component map:** Raw English text -> Frozen XLM-RoBERTa-base -> LANGALIGN adapter -> Task-specific classification head
- **Critical path:**
  1. **Step 1 (Embedding Tuning):** Fine-tune XLM-RoBERTa on English task data (Optional but recommended).
  2. **Step 2 (Aligner Training):** Freeze LM. Train LANGALIGN using parallel corpus (English + GPT-4 Translated Target) with MSE loss.
  3. **Step 3 (Task Tuning):** Train the Task Head (connected via LANGALIGN) on English data.

- **Design tradeoffs:**
  - **FC vs. AE:** FC is simpler and shallower; AE (Autoencoder) attempts to capture a translation bottleneck. The paper notes AE often outperforms FC slightly at higher data volumes (e.g., 50K samples), but FC is more robust at lower volumes.
  - **Cost vs. Accuracy:** LANGALIGN uses a small subset of translated data (5K-20K) compared to full dataset translation (NMT GPT-4), trading slight accuracy for significantly lower cost.

- **Failure signatures:**
  - **Unfrozen LM:** If the LM is not frozen during Step 2, the alignment weights will constantly chase a moving target, failing to converge.
  - **Domain Mismatch:** If parallel corpus (Step 2) and task data (Step 3) are in drastically different domains, the alignment may not transfer.
  - **Over-regularization:** Excessive Dropout in the FC layer might destroy the subtle geometric relationships required for alignment.

- **First 3 experiments:**
  1. **Cosine Similarity Check:** Verify if trained LANGALIGN actually increases cosine similarity between $A(e(d_e))$ and $e(T(d_e))$ compared to raw English embeddings (Sanity check for Step 2).
  2. **Baseline Comparison:** Train a model on English-only data and compare it against LANGALIGN (trained with 5K parallel data) on a target language test set to confirm the performance lift.
  3. **Ablation on Step 1:** Compare performance of LANGALIGN with and without the initial LM tuning step to quantify the value of task-specific embeddings.

## Open Questions the Paper Calls Out
- The paper doesn't explicitly call out open questions but raises several implications for future work through its experimental scope and limitations.

## Limitations
- **Translation quality dependency:** The method's effectiveness heavily depends on the quality of GPT-4-generated translations for the parallel corpus.
- **Limited language coverage:** While evaluated on Korean, Japanese, and Chinese, the method's generalizability to other language families (particularly low-resource languages) remains untested.
- **Computational assumptions:** The paper assumes access to GPT-4 for translation and high-end GPU resources for fine-tuning large models.

## Confidence
- **High confidence:** The core claim that LANGALIGN improves cross-lingual transfer performance by aligning English embeddings with target language embeddings is well-supported by consistent results across multiple languages and tasks.
- **Medium confidence:** The claim that Step 1 (task-specific LM tuning) is "optional but recommended" is supported by ablation studies, though the exact performance degradation when skipped is not quantified for all settings.
- **Medium confidence:** The assertion that LANGALIGN outperforms NMT GPT-4 baselines at smaller parallel data sizes (5K-20K) is supported, but the crossover point where NMT becomes superior is not precisely defined across all tasks.

## Next Checks
1. **Robustness test:** Evaluate LANGALIGN's performance when using lower-quality NMT (e.g., mT5 or commercial APIs) instead of GPT-4 for the parallel corpus to quantify translation quality sensitivity.
2. **Generalization test:** Apply LANGALIGN to a low-resource language (e.g., Swahili or Urdu) not covered in the original evaluation to test cross-language family generalizability.
3. **Cost-benefit analysis:** Compare the computational cost (GPU hours) of LANGALIGN training versus full NMT translation across different dataset sizes to quantify the claimed efficiency gains more precisely.