---
ver: rpa2
title: Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration
arxiv_id: '2511.06087'
source_url: https://arxiv.org/abs/2511.06087
tags:
- blur
- image
- motion
- images
- deblurring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of restoring scene text images
  degraded by motion blur, which impairs readability and affects computer vision tasks.
  It proposes a hybrid deep learning framework that combines convolutional neural
  networks (CNNs) with vision transformers (ViTs).
---

# Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration

## Quick Facts
- **arXiv ID:** 2511.06087
- **Source URL:** https://arxiv.org/abs/2511.06087
- **Reference count:** 39
- **Primary result:** Achieves PSNR of 32.20 dB and SSIM of 0.934 on motion-blurred scene text restoration with 2.83M parameters and 61 ms inference time

## Executive Summary
This paper addresses the challenge of restoring scene text images degraded by motion blur, which impairs readability and affects computer vision tasks. It proposes a hybrid deep learning framework that combines convolutional neural networks (CNNs) with vision transformers (ViTs). The architecture uses a CNN-based encoder-decoder for local feature extraction and a transformer module for global contextual reasoning through self-attention. The model is trained on a curated dataset derived from TextOCR, where sharp scene-text images are paired with synthetically blurred versions. Evaluation on test images shows that the proposed method achieves a PSNR of 32.20 dB and an SSIM of 0.934, while maintaining a lightweight design with 2.83 million parameters and an inference time of 61 ms.

## Method Summary
The proposed method combines a CNN encoder-decoder with a Vision Transformer module to restore motion-blurred scene text images. The CNN encoder extracts hierarchical local features through five convolutional layers, while the ViT module captures long-range dependencies via self-attention. Skip connections preserve spatial details between encoder and decoder stages. The model is trained on 6,000 paired images from TextOCR with synthetic motion blur (kernel sizes 13×13 to 31×31), using a composite loss function combining MAE, MSE, perceptual loss (VGG16 features), and SSIM. Training employs Adam optimizer with learning rate 1e-4, batch size 1, and mixed-precision float16, with early stopping based on validation PSNR.

## Key Results
- Achieves PSNR of 32.20 dB and SSIM of 0.934 on test set
- Lightweight architecture with only 2.83 million parameters
- Fast inference time of 61 ms per image
- Successfully restores text legibility in both synthetic and real-world motion-blurred images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining CNNs with Vision Transformers enables recovery of text legibility under motion blur by addressing both local texture and long-range structural coherence.
- **Mechanism:** The CNN encoder-decoder extracts hierarchical local features (edges, strokes) while the ViT module uses self-attention to model global dependencies across the image. This hybrid approach compensates for the limited receptive field of pure CNNs, which struggle to reconnect disjointed text components caused by severe motion streaks.
- **Core assumption:** Text legibility relies on the simultaneous preservation of fine-grained local edges (character strokes) and global semantic context (word shapes), which cannot be fully captured by a CNN operating on local kernels alone.
- **Evidence anchors:**
  - [abstract] "architecture employs a CNN-based encoder–decoder to preserve structural details, while a transformer module enhances global awareness through self-attention."
  - [Page 6, Section II.B.2] "A transformer block with multi-head self-attention captures long-range dependencies... enabling recovery of coherent character and word-level structures."
  - [corpus] *Hybrid Convolution and Vision Transformer NAS...* confirms that hybrid architectures generally outperform pure CNN or ViT models in complex tasks, though notes they can be computationally heavy (addressed here by the lightweight design).
- **Break condition:** If the ViT module lacks sufficient embedding dimension or depth, it may fail to capture the necessary global context, rendering it a bottleneck rather than an enhancement.

### Mechanism 2
- **Claim:** The composite loss function is critical for balancing pixel-level accuracy with perceptual fidelity, preventing the model from settling on blurry "average" solutions.
- **Mechanism:** By optimizing a weighted sum of MAE, MSE, Perceptual Loss (VGG16 features), and SSIM, the model is forced to minimize pixel errors while maintaining structural similarity and high-level semantic consistency. Perceptual loss specifically encourages the recovery of sharp textures that pure pixel-losses (MSE/MAE) might smooth over.
- **Core assumption:** Standard regression losses (MSE/MAE) are insufficient for image restoration because they prioritize signal fidelity over visual sharpness and textual legibility.
- **Evidence anchors:**
  - [abstract] "Model optimization is guided by a composite loss... including... perceptual similarity."
  - [Page 7, Section II.C] "To guide the model toward producing restorations that are both quantitatively accurate and perceptually faithful..."
  - [corpus] *Text-Aware Image Restoration with Diffusion Models* notes that generative/perceptual methods often hallucinate incorrect text, suggesting the composite loss here is a safer, constrained alternative to pure generative approaches.
- **Break condition:** If the weights (α, β, γ, δ) are mis calibrated, the model may prioritize perceptual sharpness over textual correctness (hallucinating strokes) or produce structurally correct but overly smooth results.

### Mechanism 3
- **Claim:** Skip connections function as a structural preservation highway, mitigating information loss during the transition between CNN and ViT domains.
- **Mechanism:** As the encoder downsamples features for the ViT, fine spatial details are typically lost. Skip connections concatenate these high-resolution encoder features with the upsampled decoder features, directly injecting local spatial information back into the reconstruction path.
- **Core assumption:** The ViT processing stage, while powerful for global context, may discard the precise localization of high-frequency details required for OCR.
- **Evidence anchors:**
  - [Page 6, Section II.B.3] "Skip connections concatenate encoder features... ensuring that spatial details and localized information... are preserved in the restoration process."
  - [Page 6, Fig 1] Visual representation of "Skip Connections" bridging the Encoder and Decoder blocks.
  - [corpus] *Deblur Gaussian Splatting SLAM* emphasizes robust reconstruction pipelines, indirectly supporting the need for structural integrity mechanisms like skip connections in restoration tasks.
- **Break condition:** If the feature maps from the encoder and decoder are semantically misaligned (due to aggressive downsampling or transformer processing), concatenation may introduce noise rather than useful detail.

## Foundational Learning

- **Concept: Inductive Bias in CNNs vs. Transformers**
  - **Why needed here:** Understanding why the authors chose a hybrid model requires knowing that CNNs have a strong inductive bias for locality (translation invariance) but struggle with global relationships, whereas ViTs have weaker inductive bias (requiring more data/compute) but excel at global context.
  - **Quick check question:** Why would a pure ViT model potentially struggle to restore the sharp edges of a specific font compared to a CNN?

- **Concept: The Deblurring Inverse Problem**
  - **Why needed here:** The paper treats deblurring as recovering a sharp image x from y = x * k + n. Understanding that this is an "ill-posed" problem (multiple x can result in the same y) explains why the authors need strong priors (deep learning) and specific constraints (composite loss) to solve it.
  - **Quick check question:** Why does the paper mention that conventional methods like Wiener filtering fail on "spatially varying blur"?

- **Concept: Perceptual Loss (Feature Reconstruction)**
  - **Why needed here:** The model uses VGG16 features for optimization. One must understand that this measures "similarity" based on how humans or high-level classifiers perceive features (shapes, textures) rather than raw pixel differences.
  - **Quick check question:** How does minimizing the distance between VGG16 feature maps differ from minimizing Mean Squared Error (MSE) on the pixel level?

## Architecture Onboarding

- **Component map:** Input (256×256×3) -> CNN Encoder (5 Conv2D layers) -> ViT Bridge (patch=32, 2 layers, 4 heads) -> CNN Decoder (5 TransposeConv layers) -> Output (256×256×3)

- **Critical path:** The ViT Bridge is the most sensitive component. The reshaping of CNN features into "tokens" for the transformer must preserve spatial relationships, or the decoder will receive corrupted global context.

- **Design tradeoffs:**
  - **Efficiency vs. Context:** The authors chose a lightweight ViT (Patch size 32, 2 layers, 4 heads) to keep the parameter count low (2.83M) and inference fast (61 ms), potentially sacrificing some global modeling power found in heavier transformers like Restormer (26M params).
  - **Batch Size:** The paper notes using a batch size of 1 due to GPU memory constraints with ViT blocks, which may affect batch normalization stability (though not explicitly stated if BatchNorm is used, standard ViTs often use LayerNorm).

- **Failure signatures:**
  - **Checkerboarding:** Artifacts in the output usually indicate issues with the TransposeConv layers in the decoder (uneven overlap).
  - **Text Hallucination:** If the Perceptual loss weight is too high, the model might generate readable-looking characters that are actually incorrect (false positives).
  - **Color Shift:** If the ViT attention aggregates global color information incorrectly, the restored text color may drift from the ground truth.

- **First 3 experiments:**
  1. **Ablation Study (Architecture):** Remove the ViT module (connect Encoder directly to Decoder) to quantify the exact performance gain (in PSNR/SSIM) attributed to global attention.
  2. **Loss Sensitivity Analysis:** Train three versions: one with only MSE, one with MSE+SSIM, and the full composite loss, to verify the claim that the composite loss is necessary for "perceptual fidelity."
  3. **Kernel Robustness Test:** Evaluate the model on a held-out set with blur kernel sizes outside the training range (>31×31) to test generalization to severe blur.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Implementation details for the CNN encoder-decoder architecture (kernel sizes, strides per layer) are not provided, limiting reproducibility
- Exact composite loss weights (α, β, γ, δ) are unspecified
- The evaluation is limited to synthetic blur kernels, with unclear generalization to naturally occurring real-world motion blur
- Batch size of 1 due to GPU memory constraints may affect training stability and throughput

## Confidence
- **High confidence:** The hybrid CNN-ViT architecture design and its basic functionality (CNN extracts local features, ViT provides global context)
- **Medium confidence:** The claim that the composite loss function is necessary for perceptual fidelity
- **Low confidence:** The assertion that the model generalizes to severe blur (kernels >31×31)

## Next Checks
1. **Ablation study:** Remove the ViT module to quantify the exact PSNR/SSIM gain from global attention (currently unattributed)
2. **Loss sensitivity analysis:** Train three variants (MSE only, MSE+SSIM, full composite) to empirically verify the necessity of the composite loss for perceptual quality
3. **Generalization test:** Evaluate the trained model on a held-out set with blur kernel sizes exceeding the training range (>31×31) to assess real-world robustness claims