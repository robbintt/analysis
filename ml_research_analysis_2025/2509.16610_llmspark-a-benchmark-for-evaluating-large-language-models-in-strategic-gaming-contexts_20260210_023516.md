---
ver: rpa2
title: 'LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming
  Contexts'
arxiv_id: '2509.16610'
source_url: https://arxiv.org/abs/2509.16610
tags:
- llms
- game
- chen
- player
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLMsPark is a game-theoretic evaluation benchmark that assesses
  large language models'' strategic intelligence through five classic games: Prisoner''s
  Dilemma, Trust Game, Nim Game, Dictator Game, and Who Is Spy. The system enables
  15 leading LLMs to autonomously participate as agents, using scoring mechanisms
  and Elo ratings to measure performance.'
---

# LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

## Quick Facts
- arXiv ID: 2509.16610
- Source URL: https://arxiv.org/abs/2509.16610
- Reference count: 26
- Primary result: LLMsPark is a game-theoretic evaluation benchmark that assesses large language models' strategic intelligence through five classic games, revealing diverse strategic behaviors and challenging assumptions about model superiority.

## Executive Summary
LLMsPark is a game-theoretic evaluation benchmark designed to assess large language models' strategic intelligence through five classic games: Prisoner's Dilemma, Trust Game, Nim Game, Dictator Game, and Who Is Spy. The system enables 15 leading LLMs to autonomously participate as agents, using scoring mechanisms and Elo ratings to measure performance. Models demonstrated diverse strategic behaviors including trust, confrontation, pretense, leadership, and deception. Qwen-14B-Chat achieved the best overall performance, particularly excelling in the complex Who Is Spy game, while GPT-4 showed strong results in multi-round games but weaker camouflage abilities. The study reveals that commercial models do not universally outperform open-source ones, and that single-round versus multi-round settings significantly impact model performance. Results challenge assumptions about model superiority and emphasize the importance of scenario-specific evaluation. The benchmark is publicly available at https://llmsparks.github.io/.

## Method Summary
The LLMsPark benchmark employs a Player Agent architecture where each LLM functions as an autonomous agent with four modules: Environment (game context), Perception (monitors other agents), Brain (LLM for reasoning/storage), and Action (outputs moves). Games are implemented with explicit rules and payoff matrices, and agents interact through standardized prompts that include game rules and history. Performance is measured using game-specific scoring mechanisms and a dynamic Elo rating system (K=32, R_init=1000) that adjusts ratings based on expected vs. actual outcomes. The benchmark evaluates 15 LLMs across five game-theoretic scenarios, running multiple matches per game type to generate aggregate performance metrics and strategic behavior observations.

## Key Results
- Qwen-14B-Chat achieved the best overall performance, particularly excelling in the complex Who Is Spy game
- GPT-4 showed strong results in multi-round games but weaker camouflage abilities in social deduction scenarios
- Commercial models do not universally outperform open-source ones, challenging assumptions about model superiority
- Models demonstrated five distinct emergent strategic behaviors: trust, confrontation, pretense, leadership, and deception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular Player Agent architecture enables LLMs to exhibit emergent strategic behaviors through structured perception-memory-action loops.
- Mechanism: Environment provides game state and other agents' actions → Perception module records and interprets these signals → Brain (LLM) integrates memory (past interactions) and knowledge (game rules) to plan → Action outputs decisions. This loop runs iteratively, allowing agents to refine strategies across rounds based on observed opponent behavior.
- Core assumption: LLMs possess sufficient reasoning capabilities to synthesize game rules, historical patterns, and strategic planning without explicit fine-tuning for each game.
- Evidence anchors:
  - [Section 3.3]: "The Player Agent is designed following a generic agent architecture... Perception functions as the Player Agent's sensory mechanism, interpreting external information and tracking the decisions of other agents each round."
  - [Section 4.1]: Documents five emergent behaviors—trust, confrontation, pretense, leadership, deception—observed across games without explicit prompting.
  - [corpus]: WGSR-Bench (arXiv:2506.10264) similarly uses game-theoretic scenarios for strategic reasoning evaluation, suggesting this architecture pattern generalizes.
- Break condition: If the LLM lacks sufficient context window to maintain coherent memory across multi-round games, or if game state representation exceeds the model's reasoning capacity, strategy quality degrades. Section 4.5 notes "some models adopt rigid strategies and struggle to adapt to unfamiliar scenarios."

### Mechanism 2
- Claim: The Elo rating system provides fair cross-model comparison by dynamically adjusting ratings based on expected vs. actual outcomes, accounting for opponent strength variability.
- Mechanism: Each agent starts at R_init=1000. After each game, expected score E_A is computed via logistic distribution based on rating difference. If actual outcome S_A differs from E_A, rating updates by K×(S_A−E_A) where K=32. This means defeating a higher-rated opponent yields larger gains than defeating a lower-rated one, normalizing for opponent quality.
- Core assumption: Strategic capability is relatively stable across games of similar type, and transitivity holds (if A beats B and B beats C, A should beat C with high probability).
- Evidence anchors:
  - [Section 3.4]: Provides full mathematical formulation: "Elo updates scores by comparing expected and actual outcomes, enabling refined rankings even without exhaustive pairwise matches."
  - [Table 1]: Shows Elo scores across 8 game variants for 15 LLMs, revealing consistent patterns (GPT-4 strong in multi-round, Qwen-14B-Chat superior in Who Is Spy).
  - [corpus]: CHBench (arXiv:2508.11944) critiques utility-based metrics for not being "robust enough due to variations in opponent behavior," supporting the need for opponent-adjusted scoring like Elo.
- Break condition: If strategic capability is highly game-specific (low correlation across game types), Elo ratings become noisy. The paper acknowledges: "success in one strategic game does not guarantee superiority in others" (Section 4.6).

### Mechanism 3
- Claim: Game diversity (cooperative, competitive, deceptive, mathematical) exposes distinct capability profiles that single-metric benchmarks miss.
- Mechanism: Each game tests different cognitive dimensions—Prisoner's Dilemma (risk assessment, opponent modeling), Trust Game (reciprocity, long-term cooperation), Nim (mathematical logic, binary reasoning), Dictator Game (fairness preferences), Who Is Spy (deception, social deduction, multitasking). Aggregating scores reveals whether models generalize or specialize.
- Core assumption: These classic games sufficiently approximate real-world strategic scenarios LLMs encounter; performance transfers.
- Evidence anchors:
  - [Section 3.2]: Describes each game's strategic demands—"Nim evaluates mathematical reasoning and logical foresight"; "Who Is Spy evaluates deception, situational reasoning, and collective decision-making."
  - [Section 4.3]: "GPT-4 exhibited generally robust and well-rounded performance but showed inconsistencies in trust games, likely reflecting over-planning... Phoenix-Inst-Chat-7B performed exceptionally in the single-round Prisoner's Dilemma... though its performance deteriorated in multi-round versions."
  - [corpus]: Orak (arXiv:2506.03610) evaluates LLM agents across diverse video game genres, similarly arguing for multi-domain assessment.
- Break condition: If game-specific heuristics (e.g., always defect in single-round PD) dominate over genuine reasoning, benchmark validity suffers. The paper notes models may "rely excessively on known strategies rather than exploring novel ones" (Limitations).

## Foundational Learning

- **Concept: Game Theory Fundamentals (Nash Equilibrium, Dominant Strategies)**
  - Why needed here: The benchmark assumes familiarity with why Prisoner's Dilemma creates tension (defect is dominant but mutual defection is Pareto-inferior), why Trust Game requires repeated interaction for cooperation to emerge, and why Nim has a deterministic winning strategy via XOR.
  - Quick check question: In a single-shot Prisoner's Dilemma, if your opponent cooperates, should you cooperate or defect to maximize your payoff? What changes in an iterated (multi-round) setting?

- **Concept: Elo Rating System Mechanics**
  - Why needed here: Understanding how ratings update based on expected vs. actual outcomes is essential for interpreting Table 1 results and recognizing why cross-model comparison remains fair despite incomplete pairwise matchups.
  - Quick check question: If Player A (rating 1200) defeats Player B (rating 1000), will Player A's rating increase by more, less, or the same amount as if they had defeated a 1300-rated player? Why?

- **Concept: Multi-Agent System Architecture (Perception-Planning-Action Loop)**
  - Why needed here: The Player Agent architecture mirrors standard agent frameworks but with specific modules (Environment, Perception, Brain, Action). Understanding how memory and knowledge integrate into decision-making clarifies why emergent behaviors arise.
  - Quick check question: In the Who Is Spy game, what information must the Perception module track to enable the Brain to formulate a deceptive strategy as the Spy?

## Architecture Onboarding

- **Component map:**
  Game System (cloud server) -> Game Logic (rules enforcement, state management) -> Player Matching (auto-match when N players ready) -> Parallel Execution (cue-word techniques for concurrency)
  Player Agent (per LLM) -> Environment (system prompts, other agents' messages) -> Perception (monitors decisions, maintains history) -> Brain -> Storage: Memory (past interactions) + Knowledge (game rules) -> Decision: Planning & Reasoning module -> Action (outputs move: cooperate/betray, describe/vote, etc.)
  Evaluation System -> Game-specific scoring (outcome-based per game rules) -> Elo Rating Calculator (updates after each match)
  Distributed Infrastructure -> Central game server (orchestration) -> Edge GPU clusters (LLM inference nodes)

- **Critical path:** 
  1. Register LLM as Player Agent → 2. System matches agents for specific game → 3. Environment sends initial prompts to all agents → 4. Each agent's Perception logs game state → 5. Brain synthesizes memory + knowledge, generates decision → 6. Action outputs move → 7. Game System validates and updates state → 8. Loop until game ends → 9. Evaluation System computes scores, updates Elo → 10. Results feed leaderboard.

- **Design tradeoffs:**
  - **Centralized orchestration vs. distributed inference:** Game logic is centralized for consistency, but LLM inference runs on distributed GPU clusters to handle concurrent games. Tradeoff: increased network latency but improved scalability.
  - **Prompt-based game state vs. structured API:** The system uses natural language prompts to describe game state (e.g., "You are a player who is playing 'Who's the Spy?'..."). Tradeoff: accessibility for text-only LLMs vs. potential ambiguity in state interpretation.
  - **Elo vs. win-rate-only ranking:** Elo accounts for opponent strength but assumes rating transitivity. Win-rate is simpler but confounded by opponent quality. Tradeoff chosen: Elo for fairness despite assumptions.

- **Failure signatures:**
  - **Strategic rigidity:** If an LLM outputs identical moves regardless of opponent behavior (e.g., always cooperate), check whether Perception module is correctly logging history or if Brain is over-relying on priors. Section 4.5 notes this as a limitation.
  - **Latency timeout in fast-paced games:** If agents fail to respond within game time limits, investigate GPU cluster load balancing or reduce concurrent game count.
  - **Incoherent game state interpretation:** If agents make illegal moves or misinterpret rules, verify prompt clarity and whether Knowledge module correctly encodes game rules. Only 6 of 15 models successfully handled Who Is Spy complexity.
  - **Rating instability:** If Elo ratings fluctuate dramatically across runs, check whether K=32 is too high for the sample size; consider reducing K or increasing match count.

- **First 3 experiments:**
  1. **Baseline calibration:** Run 50 matches of Prisoner's Dilemma (single-round) between GPT-4 and a smaller open-source model (e.g., ChatGLM2-6B). Verify that Elo updates follow expected direction and magnitude. Compare cooperation/defection rates against Table 1 benchmarks.
  2. **Multi-round adaptation test:** Run Trust Game with 10 rounds between same-model pairs (GPT-4 vs. GPT-4, Qwen-14B-Chat vs. Qwen-14B-Chat). Analyze whether cooperation rates increase over rounds, testing the Perception→Brain→Action feedback loop's effectiveness.
  3. **Emergent behavior probe:** Run 20 instances of Who Is Spy with mixed-model groups. Manually annotate whether behaviors in Section 4.1 (trust, confrontation, pretense, leadership, deception) emerge. Quantify correlation between behavior diversity and Elo score to test whether strategic flexibility predicts success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs better leverage game experience for learning?
- Basis in paper: [explicit] Future work states: "enhancing models' ability to leverage historical game experience and incorporate human-like learning."
- Why unresolved: Despite memory components, models show "strategic rigidity" and poor adaptation to unfamiliar scenarios.
- What evidence would resolve it: Demonstrated performance improvements across sessions and strategy transfer to new games.

### Open Question 2
- Question: What methodologies enable meaningful cross-game strategic evaluation?
- Basis in paper: [explicit] Future work identifies "establishing consistent baseline methodologies for cross-game evaluation."
- Why unresolved: No unified framework exists for comparing overall strategic intelligence across diverse game types.
- What evidence would resolve it: Validated framework with normalized cross-game metrics and aggregate scores.

### Open Question 3
- Question: How do game-theoretic findings generalize to real-world applications?
- Basis in paper: [explicit] Future work mentions "reducing potential errors when generalizing from controlled theoretical settings to real-world applications."
- Why unresolved: Classic games are idealized; real interactions involve incomplete information and evolving rules.
- What evidence would resolve it: Correlations between benchmark performance and real-world strategic scenarios.

### Open Question 4
- Question: What factors explain divergent model performance across games?
- Basis in paper: [inferred] GPT-4 excels in multi-round games but underperforms in Who Is Spy, while Qwen-14B-Chat shows the opposite pattern.
- Why unresolved: Partial explanations exist but systematic analysis of training, architecture, or alignment effects is missing.
- What evidence would resolve it: Controlled experiments isolating factors affecting game-specific performance.

## Limitations
- Game-specific heuristics may dominate over genuine reasoning, potentially limiting benchmark validity
- The exact prompt templates and scoring rubrics are underspecified, affecting reproducibility
- Limited sample size (50 games per model pair) may provide insufficient statistical power for subtle strategic differences

## Confidence
- **High confidence**: The Elo rating mechanism (Section 3.4) is mathematically rigorous and well-specified. The Player Agent architecture (Section 3.3) is clearly defined with explicit modules and data flow.
- **Medium confidence**: The emergent behavior taxonomy (Section 4.1) is observation-based but lacks quantitative validation. The performance rankings (Table 1) are internally consistent but depend on the unspecified scoring rubrics.
- **Low confidence**: Claims about commercial vs. open-source model performance (Section 4.6) are based on aggregate comparisons without controlling for parameter count or training data differences.

## Next Checks
1. **Prompt reproducibility audit**: Recreate the Player Agent system using the examples in figures, then test whether GPT-4 reproduces its original Who Is Spy performance within ±10 Elo points across 20 matches.
2. **Multi-round adaptation validation**: Run Trust Game with 10 rounds between GPT-4 and Qwen-14B-Chat, measuring cooperation rates per round. Verify that cooperation increases over time (indicating successful Perception→Brain→Action feedback) and compare against reported behaviors in Section 4.3.
3. **Statistical power assessment**: Calculate 95% confidence intervals for Elo differences between top-3 models in Who Is Spy. Determine whether observed ranking differences are statistically significant given the 50-game sample size per pair.