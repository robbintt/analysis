---
ver: rpa2
title: Bridging Training and Merging Through Momentum-Aware Optimization
arxiv_id: '2512.17109'
source_url: https://arxiv.org/abs/2512.17109
tags:
- umtam
- training
- merging
- task
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UMTAM presents a unified framework that integrates memory-efficient
  training and model merging through shared computational structure. By maintaining
  factorized momentum and curvature statistics during training, the method enables
  principled multi-task model composition without redundant post-hoc computation.
---

# Bridging Training and Merging Through Momentum-Aware Optimization

## Quick Facts
- **arXiv ID:** 2512.17109
- **Source URL:** https://arxiv.org/abs/2512.17109
- **Authors:** Alireza Moayedikia; Alicia Troncoso
- **Reference count:** 3
- **Primary result:** Unified training-merging framework with 14.9% improvement over linear averaging on multi-task merging

## Executive Summary
UMTAM presents a unified framework that integrates memory-efficient training and model merging through shared computational structure. By maintaining factorized momentum and curvature statistics during training, the method enables principled multi-task model composition without redundant post-hoc computation. Theoretical analysis establishes O(1/√T) convergence for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, UMTAM's curvature-aware parameter selection outperforms magnitude-only baselines by up to 135% at aggressive sparsity levels. The unified framework improves multi-task merging performance by 14.9% over linear averaging and 1.6% over strong baselines like TIES, with experiments on BERT and GPT-2 demonstrating competitive training efficiency. The approach eliminates the need for separate Fisher computation, enabling a streamlined workflow where training-time curvature information directly informs model composition.

## Method Summary
UMTAM operates through dual momentum factorization with error feedback and factorized second-order statistics. During training, gradients are compressed via truncated SVD while maintaining error accumulation to prevent information loss. Row and column-wise second moments provide curvature information for both adaptive preconditioning and merging importance estimation. At merging time, task vectors are pruned using curvature-aware saliency masks, sign conflicts are resolved via importance-weighted voting, and models are aggregated using curvature-weighted averaging. The framework maintains statistics throughout training that directly inform the merging phase, eliminating redundant post-hoc computation.

## Key Results
- UMTAM achieves 14.9% improvement over linear averaging and 1.6% over TIES in multi-task merging on GLUE benchmarks
- Curvature-aware saliency outperforms magnitude-only pruning by up to 135% at 1% density on MRPC
- Training efficiency matches AdamW baseline with low-rank momentum (r=32 captures >82% momentum energy)
- Memory savings eliminate need for separate Fisher matrix computation during merging

## Why This Works (Mechanism)

### Mechanism 1: Dual Momentum Factorization with Error Feedback
Low-rank momentum compression maintains optimization quality by preventing permanent gradient information loss. At each step, reconstruct full momentum from factors, add new gradient and accumulated error, then re-compress via truncated SVD. The compression residual is stored and re-injected next iteration with decay factor γ. Core assumption: gradient and momentum matrices exhibit low intrinsic dimensionality (stable rank ≈ 3–6 in practice).

### Mechanism 2: Factorized Second-Order Statistics for Curvature Tracking
Row-wise and column-wise second moments provide sufficient curvature information for both adaptive preconditioning and merging importance estimation. Maintain R ∈ R^m (row variances) and C ∈ R^n (column variances) via exponential moving average of gradient outer products. Construct preconditioner as element-wise product P = (R·C^T / normalization)^(-1/2). Saliency scores weight parameter deviations by sqrt(R·C). Core assumption: diagonal structure of second moments captures enough curvature information.

### Mechanism 3: Curvature-Aware Merging with Sign Election
Reusing training-time curvature for merging produces comparable or better results than post-hoc Fisher computation while eliminating redundant computation. Prune task vectors using curvature-aware saliency masks, resolve sign conflicts via importance-weighted voting, aggregate via curvature-weighted average: ∆w_merged = P̂_combined^(-1) Σ_τ P̂_τ(M_τ ⊙ ∆w_τ). Core assumption: models fine-tuned from shared initialization lie in connected loss regions where linear combination preserves performance.

## Foundational Learning

- **Low-rank matrix factorization (SVD-based):** Why needed: Framework depends on understanding how truncated SVD compresses matrices and what information is lost (captured by σ_{r+1}). Quick check: Given a 1024×1024 momentum matrix with singular values [100, 50, 10, 1, 0.1, ...], what fraction of energy is retained at rank 3?
- **Momentum-based optimization (Adam family):** Why needed: UMTAM modifies Adam's momentum mechanism with factorization; understanding baseline is essential to see what changes. Quick check: Why does Adam maintain both first moments (momentum) and second moments (variance)? What does each enable?
- **Task vectors and model merging fundamentals:** Why needed: Merging phase operates on task vectors τ = θ_trained - θ_0; without this framing, Algorithm 2 is opaque. Quick check: When merging two task vectors τ_A and τ_B, what happens at a parameter where τ_A = +0.5 and τ_B = -0.3? How would sign election resolve this?

## Architecture Onboarding

- **Component map:**
  Training Phase: Gradient G_t → Momentum M_t → TruncatedSVD → (U, Σ, V)
                         ↓
    Second moments R_t, C_t → Factorized preconditioner P_t
                         ↓
    Saliency accumulator S_τ (per-task importance)

  Preserved Statistics: (U_τ, Σ_τ), (R_τ, C_τ), S_τ for each task τ

  Merging Phase: Task vectors ∆w_τ → Saliency-based masks M_τ → Sign election
                         ↓
    Curvature-weighted aggregation → w_merged

- **Critical path:** The saliency scores S_τ are the key bridge. If computed incorrectly during training (e.g., wrong α, missing curvature component), merging degrades to magnitude-only baseline.

- **Design tradeoffs:**
  - Rank r: Higher → better gradient preservation but more memory. Paper shows r=32 sufficient (captures >82% momentum energy); diminishing returns beyond.
  - Sparsity k: Lower → more compression but higher risk of dropping critical parameters. Paper finds k=40% optimal for 4-task GLUE merging.
  - Saliency decay α: Higher → longer memory but slower adaptation to training dynamics. Default α=0.9 implied but not extensively tuned.

- **Failure signatures:**
  - Training divergence with high learning rate + high rank: MoFaSGD shows 78.8% degradation at η=1e-4, r=16 vs r=8. UMTAM more robust but not immune.
  - Merging produces worse-than-random performance (negative correlation on CoLA in 4-task setup): Indicates fundamental task interference that importance weighting cannot resolve.
  - Saliency scores concentrate on wrong parameters: Check if R and C are accumulating correctly (should grow throughout training, not saturate early).

- **First 3 experiments:**
  1. **Validate low-rank assumption on your architecture:** Log singular values of momentum buffer during a short training run. Confirm top-32 captures >80% energy; if not, rank r may need adjustment.
  2. **Single-task saliency sanity check:** Train one task, apply saliency-based pruning at 10% and 20% density, compare to magnitude-based pruning. UMTAM should match or exceed magnitude baseline. If worse, check S formula implementation.
  3. **Pairwise merge before multi-task:** Merge two similar tasks (e.g., RTE+MRPC) and two dissimilar tasks (e.g., QNLI+CoLA). Confirm UMTAM shows small positive gain or parity with TIES. Large degradation indicates bug in sign election or curvature aggregation.

## Open Questions the Paper Calls Out

### Open Question 1
Does the UMTAM framework's low-rank structure and curvature-aware merging generalize to vision transformers and multimodal architectures? Basis: Conclusion states validation on vision transformers and multimodal architectures remains promising direction. Unresolved because empirical validation restricted to NLP models (GPT-2, BERT, Mistral). Evidence needed: Benchmarks on architectures like ViT or LLaVA comparing merging performance and gradient spectral decay against NLP baselines.

### Open Question 2
Does UMTAM provide statistically significant advantages in merging scenarios characterized by high parameter conflict, such as cross-domain or cross-modal tasks? Basis: Page 31 notes statistical significance limited (p > 0.7) due to insufficient variance in conflict intensity among GLUE tasks. Unresolved because homogeneous NLU benchmarks resulted in low variance in sign conflict rates (0.43–0.45). Evidence needed: Experiments merging task vectors from distinct domains (e.g., code, mathematics, image recognition) to correlate conflict intensity with UMTAM's performance delta over baselines.

### Open Question 3
Why does curvature-aware saliency underperform magnitude-only pruning at extreme sparsity on tasks involving distributed syntactic knowledge, such as CoLA? Basis: Page 27 reports magnitude pruning outperforms curvature-aware selection at 1% density on CoLA, hypothesizing distributed syntactic knowledge reduces discriminative signal of curvature. Unresolved because paper documents counter-intuitive result without proposing validated modification to saliency formulation. Evidence needed: Layer-wise analysis of curvature distributions on CoLA versus semantic tasks, or modified saliency metric that dynamically weights magnitude versus curvature based on spectral decay.

## Limitations

- Theoretical convergence bounds rely on gradient singular value decay assumptions without rigorous decay rate analysis
- Multi-task merging effectiveness diminishes with task incompatibility (CoLA performance collapse with QNLI)
- Scalability to trillion-parameter models remains untested despite theoretical projections

## Confidence

- **High Confidence:** Training efficiency claims and single-task performance. Dual momentum factorization demonstrates consistent convergence across ranks with minimal degradation (0.1-1.6% variance).
- **Medium Confidence:** Multi-task merging effectiveness. UMTAM shows 14.9% improvement over linear averaging, but benefit diminishes with task incompatibility.
- **Low Confidence:** Scalability claims and theoretical guarantees. Extrapolates memory savings to trillion-parameter models without empirical validation.

## Next Checks

1. **Singular Value Decay Analysis:** Perform systematic analysis of momentum gradient singular value decay across multiple tasks and model sizes. Verify that σ_{r+1} decreases sufficiently fast to justify rank-r approximation. Test whether decay rates differ between pre-training and fine-tuning phases.

2. **Task Interference Mapping:** Quantify pairwise task compatibility by computing loss landscape distances between task-specific solutions. Develop a compatibility score that predicts merging success probability before computation. Validate on heterogeneous task sets beyond GLUE.

3. **Large-Scale Memory Profile:** Implement UMTAM on a 1-10B parameter model and measure actual memory savings versus theoretical projections. Compare against alternative compression methods (LoRA, Adapters) under identical task loads to establish practical efficiency advantages.