---
ver: rpa2
title: 'Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative
  Programs'
arxiv_id: '2509.20208'
source_url: https://arxiv.org/abs/2509.20208
tags:
- language
- type
- database
- query
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating language model
  (LLM) functions into typed declarative query languages like SQL, ensuring generated
  outputs align with database constraints and type rules. The authors propose an efficient
  decoding-level type alignment algorithm that infers constraints from the expression
  context and applies them during LLM generation via constrained decoding.
---

# Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs

## Quick Facts
- **arXiv ID:** 2509.20208
- **Source URL:** https://arxiv.org/abs/2509.20208
- **Reference count:** 22
- **Primary result:** Small language models can execute complex multi-hop reasoning queries over hybrid data sources when given appropriate type constraints, achieving 7% accuracy improvement on HybridQA.

## Executive Summary
This paper tackles the challenge of integrating large language model (LLM) functions into typed declarative query languages like SQL, ensuring generated outputs align with database constraints and type rules. The authors propose an efficient decoding-level type alignment algorithm that infers constraints from the expression context and applies them during LLM generation via constrained decoding. Their approach demonstrates that small language models can effectively execute complex multi-hop reasoning queries over hybrid data sources when given appropriate type constraints, achieving significant accuracy and latency improvements over baseline methods.

## Method Summary
The authors develop a system that parses SQL queries into Abstract Syntax Trees (ASTs) and infers output type constraints from the surrounding SQL operators. These constraints are then applied during LLM generation through constrained decoding (logit masking), ensuring outputs conform to database schema requirements. The system assigns infinite cost to LLM functions in the query optimizer, deferring their execution until after native SQL filtering. This approach is tested on the HybridQA benchmark, demonstrating improved accuracy and reduced latency compared to methods that rely on post-processing validation.

## Key Results
- 7% accuracy improvement on HybridQA dataset when using type constraints and constrained decoding
- 53% improvement in latency compared to post-processing approaches
- Small language models (3B) can effectively execute complex multi-hop reasoning queries over hybrid data sources with appropriate type constraints

## Why This Works (Mechanism)

### Mechanism 1: Expression-Context Type Inference
If an LLM function is embedded within a SQL operator (e.g., `WHERE`, `>`, `=`), the required output type can be deterministically inferred from the surrounding AST structure and database schema, eliminating ambiguous generation targets. The system parses the query into an AST and applies inference rules (e.g., `f() > 40` implies `int`, `city = f()` implies `Literal[<column_values>]`) to construct a type signature. This signature is converted into a regular expression or literal set. Core assumption: The SQL query structure accurately reflects the semantic constraints of the desired answer. Break condition: If the query logic is semantically flawed, the inferred type constraint will force a hallucination or failure rather than correcting the logic.

### Mechanism 2: Decoding-Level Token Masking
If valid outputs are restricted to a specific set (e.g., existing database values), applying these constraints during the decoding process (logit masking) guarantees type alignment without requiring post-processing LLM calls. The system uses a constrained decoding engine (e.g., Guidance) and traverses a token trie, masking any token that violates the inferred regular expression or literal set, forcing the model to generate a valid database key directly. Core assumption: The small language model (SLM) possesses the internal knowledge to map the prompt to the correct entity and that entity is within the allowed token set. Break condition: If the SLM's probability distribution strongly favors an invalid token at step t, forced masking may result in a grammatically valid but semantically incorrect "nearest neighbor" selection.

### Mechanism 3: Cost-Based Function Deferral
If LLM functions are assigned infinite cost relative to native SQL operators, the query planner will aggressively filter data before invoking the LLM, minimizing expensive inference calls. The optimizer treats LLM functions as leaves with cost ∞, executing standard SQL (joins, filters) first. Only distinct values required for the final result are passed to the LLM, often via temporary tables. Core assumption: The database can pre-filter rows using standard SQL predicates without needing the LLM's semantic judgment. Break condition: If the filtering logic itself depends on the LLM, deferral is impossible, and the query may degrade to a full table scan with LLM invocation per row.

## Foundational Learning

- **Concept: Abstract Syntax Trees (AST) in SQL**
  - **Why needed here:** The type inference mechanism relies on traversing the query structure (operators, operands) to determine constraints. Understanding ASTs is required to debug why a specific type was inferred.
  - **Quick check question:** Given `SELECT * FROM t WHERE age > {{LLMQA(...)}}`, which part of the AST informs the system that the return type must be numeric?

- **Concept: Constrained Decoding / Logit Masking**
  - **Why needed here:** The core latency improvement comes from replacing post-generation checks with generation-time restrictions. Understanding this helps explain why the model cannot "hallucinate" outside the schema.
  - **Quick check question:** If the constraint is `Literal['red', 'blue']` and the model tries to generate "green", what happens to the probability of the token "green" during the forward pass?

- **Concept: Hybrid QA (Text + Table)**
  - **Why needed here:** The paper benchmarks on HybridQA, where reasoning requires joining unstructured text (retrieved via RAG) with structured tables. Understanding this context clarifies why simple SQL generation fails (it cannot "read" the text).
  - **Quick check question:** In a multi-hop query, which component handles the retrieval of unstructured evidence—the SQL engine or the LLM function?

## Architecture Onboarding

- **Component map:** BlendSQL Parser -> AST Parser -> Type Inferrer -> Optimizer -> Execution Engine -> Guidance/Constrained Decoder -> AST Transformer -> Final SQL Execution

- **Critical path:**
  1. Query parsing to AST
  2. Type inference (e.g., identifying Literal constraint from a column)
  3. Native SQL execution (filtering)
  4. Constrained LLM generation (mapping filtered rows to answers)
  5. AST transformation (injecting answers back into query)
  6. Final SQL execution

- **Design tradeoffs:**
  - **Latency vs. Complexity:** Mapping distinct values is faster but requires temporary table management
  - **Flexibility vs. Guarantees:** Constrained decoding guarantees syntax validity but may reduce semantic diversity or creativity in answers
  - **Model Size:** Smaller models (3B) require constraints to be viable; larger models (70B) may find constraints harmful or unnecessary

- **Failure signatures:**
  - **Empty Generations:** The constraint was too tight (e.g., regex mismatch) or the model didn't know the answer, leading to an end-of-sequence token before a valid completion
  - **Syntax Errors in Programs:** The CFG (Context-Free Grammar) guide might reject valid logic if the grammar isn't perfectly aligned with the SQL dialect
  - **Hallucinated Columns:** LLM generates a query referencing non-existent columns (semantic error), which passes parsing but fails execution

- **First 3 experiments:**
  1. **Baseline Validation:** Run Llama-3.1-8b on HybridQA using "No Type Hints" vs. "Type Hints + Constrained Decoding" to replicate the accuracy gap
  2. **Latency Profiling:** Measure end-to-end time for a LLMMAP operation against a table with 1k rows, comparing "Constrained Decoding" against a "Generate-then-Check" post-processing loop
  3. **Stress Test Constraints:** Try a query where the ground truth is not in the Literal set (e.g., database has "NY" but answer is "New York") to observe system fallback or failure mode

## Open Questions the Paper Calls Out
None

## Limitations
- Query semantics vs. type constraints: The system assumes SQL structure correctly encodes semantic intent and may produce hallucinations rather than flagging semantic errors
- Grammar coverage and robustness: The approach relies on CFG to guide constrained decoding, which may reject valid queries or accept invalid ones if not perfectly aligned with the SQL dialect
- Model dependency on constraints: While constrained decoding improves accuracy for small models (3B), larger models (70B) may find constraints harmful

## Confidence
- **High Confidence:** Expression-context type inference (Mechanism 1) is well-supported by deterministic AST-based type deduction
- **Medium Confidence:** Cost-based function deferral (Mechanism 3) is logically sound but real-world performance may vary
- **Medium Confidence:** Latency improvement claim (53%) is based on comparisons to post-processing methods but lacks direct ablation studies

## Next Checks
1. **Grammar Robustness Test:** Construct a suite of queries with edge-case SQL syntax and evaluate whether the CFG-based constrained decoder rejects valid queries or accepts invalid ones
2. **Semantic Error Detection:** Design queries with intentional semantic flaws and measure whether the system flags these as errors or silently produces hallucinations
3. **Cross-Model Generalization:** Test the constrained decoding approach with a range of model sizes on HybridQA and other benchmarks to quantify the trade-off between accuracy gains and potential harm from over-constraining larger models