---
ver: rpa2
title: 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6
  Sudoku'
arxiv_id: '2505.15993'
source_url: https://arxiv.org/abs/2505.15993
tags:
- puzzles
- llms
- sudoku
- reasoning
- puzzle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of five Large Language Models
  (LLMs) in solving and explaining 6x6 Sudoku puzzles. While OpenAI's o1-preview model
  demonstrates strong solving capabilities (achieving 59% fully correct solutions),
  none of the models can provide satisfactory explanations for their reasoning process.
---

# Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku

## Quick Facts
- arXiv ID: 2505.15993
- Source URL: https://arxiv.org/abs/2505.15993
- Reference count: 30
- Models tested show strong puzzle-solving but weak explanation capabilities

## Executive Summary
This exploratory study evaluates five Large Language Models' ability to solve and explain 6x6 Sudoku puzzles. While models demonstrate varying degrees of success in puzzle-solving, with OpenAI's o1-preview achieving 59% fully correct solutions, all models struggle significantly with providing clear, justified, and educational explanations for their reasoning processes. The study reveals a critical gap between computational problem-solving capabilities and the ability to articulate reasoning in natural language.

## Method Summary
The research evaluates five LLMs (GPT-3.5-turbo, GPT-4, Claude-3-5-Sonnet, Gemini-1.5-Pro, and o1-preview) on their ability to solve 6x6 Sudoku puzzles and explain their solutions. The evaluation uses two metrics: the "Master Puzzle Approach" (20 puzzles with 20 empty cells each) and the "Completeness Approach" (20 puzzles with 18 empty cells each). Solutions are graded as fully correct, partially correct, or incorrect based on adherence to Sudoku rules. Explanation quality is assessed across three dimensions: justification, clarity, and educational value, with each dimension rated on a scale from 1 (worst) to 5 (best).

## Key Results
- o1-preview achieved 59% fully correct solutions, outperforming other models
- All models struggled with explanation quality: only 5% of o1-preview responses indicated the explanation justified the solution
- Explanation clarity scored only 7.5% and educational value only 2.5% for o1-preview
- Significant performance gap exists between puzzle-solving ability and explanation quality

## Why This Works (Mechanism)
The study demonstrates that while LLMs can leverage pattern recognition and statistical reasoning to solve constrained puzzles, they lack the explicit reasoning mechanisms needed to articulate their thought processes. The "chain-of-thought" approach used by models like o1-preview enables them to arrive at correct solutions through iterative reasoning, but this internal process remains opaque and difficult to verbalize in educational terms.

## Foundational Learning
1. **Chain-of-Thought Reasoning**: LLMs use step-by-step reasoning to solve complex problems. Why needed: Enables systematic problem-solving. Quick check: Compare performance with and without explicit reasoning prompts.
2. **Sudoku Constraint Propagation**: The logical rules governing Sudoku solutions. Why needed: Forms the basis for valid puzzle solutions. Quick check: Verify solutions adhere to row, column, and subgrid constraints.
3. **Natural Language Explanation Quality Metrics**: Standardized criteria for evaluating explanation effectiveness. Why needed: Enables objective assessment of explanation quality. Quick check: Apply metrics consistently across different explanation types.

## Architecture Onboarding
Component Map: LLM Engine -> Reasoning Module -> Solution Generator -> Explanation Generator -> Quality Assessment

Critical Path: Input Puzzle → Reasoning Process → Solution Generation → Explanation Generation → Quality Evaluation

Design Tradeoffs: Balancing computational efficiency with explanation clarity, managing token limitations while maintaining reasoning depth

Failure Signatures: Correct solutions with vague or incomplete explanations, systematic errors in specific constraint types, explanations that don't match solution steps

First Experiments:
1. Compare explanation quality when models are prompted with explicit reasoning requirements vs. standard instructions
2. Test whether providing worked examples improves explanation quality
3. Evaluate if breaking down solutions into smaller reasoning steps enhances explanation clarity

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Study focuses exclusively on 6x6 Sudoku puzzles, limiting generalizability
- Only five LLMs evaluated with limited prompting strategy exploration
- Explanation quality assessment relies on subjective ratings without inter-rater reliability measures
- No comparison between LLM explanations and human-generated explanations

## Confidence
- LLMs' solving performance (o1-preview at 59% fully correct): High confidence
- LLMs' inability to provide satisfactory explanations: High confidence
- The contrast between solving ability and explanation quality: Medium confidence

## Next Checks
1. Replicate the study with a larger sample size (minimum 50 puzzles) and include inter-rater reliability measures for explanation quality assessment
2. Compare LLM-generated explanations with those produced by human puzzle solvers for the same set of puzzles to establish baseline explanation quality
3. Test whether integrating LLMs with explicit logic-based reasoning modules (such as constraint propagation algorithms) improves explanation quality while maintaining solving performance