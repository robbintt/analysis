---
ver: rpa2
title: 'Evaluating Machine Translation Models for English-Hindi Language Pairs: A
  Comparative Analysis'
arxiv_id: '2505.19604'
source_url: https://arxiv.org/abs/2505.19604
tags:
- translation
- machine
- evaluation
- metrics
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates four machine translation models (Google Translate,
  IndicTrans2, NLLB-200, and OPUS-MT) for English-Hindi language pairs using both
  lexical (BLEU, WER, TER) and machine learning-based metrics (BLEURT, BERTScore,
  COMET). The models were tested on an 18,000+ sentence general corpus and a 400 sentence
  domain-specific FAQ corpus.
---

# Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis

## Quick Facts
- arXiv ID: 2505.19604
- Source URL: https://arxiv.org/abs/2505.19604
- Reference count: 16
- Primary result: Google Translate outperforms other MT models on English-Hindi translation across multiple metrics

## Executive Summary
This study evaluates four machine translation models (Google Translate, IndicTrans2, NLLB-200, and OPUS-MT) for English-Hindi language pairs using both lexical and machine learning-based metrics. The models were tested on an 18,000+ sentence general corpus and a 400 sentence domain-specific FAQ corpus. Google Translate achieved the highest median scores across most metrics, followed by IndicTrans2. Translation quality consistently degraded with increasing sentence length across all models. Key challenges identified include gender marking errors, untranslated abbreviations, and loss of semantic meaning in context-dependent phrases and idioms.

## Method Summary
The study evaluates four pretrained MT models on English-Hindi translation using both lexical metrics (BLEU, WER, TER) and ML-based metrics (BLEURT, BERTScore, COMET). Models tested include Google Translate API, IndicTrans2, NLLB-200, and OPUS-MT. The evaluation uses an 18,000+ sentence parallel corpus and a 400-sentence FAQ dataset from government websites. Both unidirectional translation and back-translation were performed to assess semantic preservation. Metrics were computed and analyzed across different sentence length categories.

## Key Results
- Google Translate achieved highest median scores across most metrics, with consistent performance across sentence lengths
- Translation quality degraded with increasing sentence length for all models
- Gender marking errors, untranslated abbreviations, and loss of semantic meaning in idioms were identified as key challenges
- Back-translation revealed Google Translate's superior performance in preserving semantic and contextual accuracy

## Why This Works (Mechanism)

### Mechanism 1: Context Extraction Enables Translation Quality
- Claim: Models with broader context extraction capabilities produce more accurate translations, particularly for longer or ambiguous text.
- Mechanism: The system analyzes surrounding text to resolve ambiguities (e.g., "bank" as financial institution vs. river edge) and maintain coherence across sentences. This requires access to extensive training data that provides diverse contextual examples.
- Core assumption: Larger training corpora with varied contexts directly improve a model's ability to disambiguate meaning.
- Evidence anchors:
  - [abstract] "Google Translate outperforms others... with consistent performance across sentence lengths"
  - [section VI] "Google Translate's superior performance is attributed to the extensive dataset accessible to the system... enables Google Translate to leverage context extraction capabilities"
  - [corpus] Related paper on legal MT confirms domain-specific context improves translation quality for specialized content
- Break condition: When domain-specific terminology or abbreviations lack sufficient contextual examples in training data (e.g., "PAN" left untranslated in banking FAQs)

### Mechanism 2: ML-Based Metrics Capture Semantic Alignment Better Than Lexical Metrics
- Claim: Embedding-based evaluation metrics correlate more strongly with human judgment than n-gram overlap metrics.
- Mechanism: ML-based metrics (BLEURT, COMET, BERTScore) use contextual embeddings to measure semantic similarity, while lexical metrics (BLEU, WER, TER) only measure surface-level token overlap. This allows detection of correct translations that use different words.
- Core assumption: Pre-trained embedding models have sufficiently captured cross-lingual semantic relationships for the language pair.
- Evidence anchors:
  - [section II, Table I] BLEU gives 8.72 score to semantically similar sentences (85.0 human similarity) vs. 66.42 to unrelated sentences (16.0 human similarity); BLEURT correctly reverses this pattern
  - [section II.B] "Machine Learning-based metrics... shows a much higher correlation with human judgment because of its ability to capture semantic meaning"
  - [corpus] No direct corpus evidence on metric correlation; neighboring papers focus on MT systems rather than evaluation metrics
- Break condition: For low-resource languages where embedding models were trained on limited data, ML-based metrics may show poor correlation

### Mechanism 3: Translation Direction Asymmetry in Resource-Impbalanced Pairs
- Claim: Translation quality differs by direction (English→Hindi vs. Hindi→English) when training resources are imbalanced.
- Mechanism: Models trained predominantly on English-centric corpora perform better when translating from lower-resource languages to English, as the target language representation is richer.
- Core assumption: English serves as a pivot language with disproportionately more training data in most MT systems.
- Evidence anchors:
  - [section V.A, Figure 1] "machine translation models demonstrate significantly better results when translating from Hindi to English, as opposed to the reverse direction"
  - [section V.A] "OPUS MT and NLLB200 exhibit negative [BLEURT] scores, indicating below-average translations, especially when translating from a resource-scarce language like Hindi"
  - [corpus] Related paper on Bhili-Hindi-English corpus confirms cross-lingual resource imbalances affect low-resource language translation
- Break condition: When models are specifically trained with balanced parallel corpora for the target language pair (e.g., IndicTrans2 with India-focused training)

## Foundational Learning

- Concept: **Transformer Encoder-Decoder Architecture**
  - Why needed here: All four evaluated models (NLLB-200, OPUS-MT, IndicTrans2) use transformer-based architectures; understanding attention mechanisms explains context handling
  - Quick check question: Can you explain how self-attention allows the model to weight different input tokens differently when generating each output token?

- Concept: **BLEU vs. Semantic Metrics (BLEURT/COMET)**
  - Why needed here: The paper demonstrates that BLEU can give high scores to semantically unrelated sentences; choosing the right metric is critical for accurate evaluation
  - Quick check question: Given two translations—"The cat sat on the mat" and "A feline rested on a rug"—would BLEU or BLEURT give a higher similarity score?

- Concept: **Back-Translation as Quality Validation**
  - Why needed here: The paper uses back-translation to assess semantic preservation; this technique reveals errors that unidirectional evaluation misses
  - Quick check question: If a sentence translates English→Hindi→English and returns "The bank approved the loan" instead of original "The river bank was flooded," what type of error does this reveal?

## Architecture Onboarding

- Component map:
Source Text → [MT Model] → Target Text → [Evaluation Pipeline] → Lexical/ML Metrics
                                                ↓
                                        Back-Translation

- Critical path:
  1. Select MT model based on deployment constraints (API vs. local, language coverage)
  2. Prepare parallel corpus with reference translations
  3. Run unidirectional translation (both directions for bidirectional systems)
  4. Compute lexical metrics (fast baseline) and ML-based metrics (semantic accuracy)
  5. Run back-translation for semantic preservation validation
  6. Analyze failure cases (gender markers, abbreviations, idioms)

- Design tradeoffs:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Google Translate API | Highest quality, consistent performance | Proprietary, requires internet, cost at scale |
  | IndicTrans2 (open-source) | Supports all 22 Indian languages, deployable locally | ~5-10% lower BLEU scores than Google |
  | NLLB-200 | 200 language support, MoE efficiency | Negative BLEURT scores for Hindi→English |
  | Lexical metrics only | Fast, no model needed | Misses semantic correctness |
  | ML-based metrics only | Better human correlation | Requires GPU for inference |

- Failure signatures:
  - **Gender marker errors**: Hindi nouns have gender; models default to common gender patterns from training data (e.g., "doctor" → male, "nurse" → female)
  - **Untranslated abbreviations**: Domain-specific terms (PAN, banking codes) preserved in English script—blocks comprehension for non-English readers
  - **Idiom literalism**: Proverbs translated word-by-word lose metaphorical meaning
  - **Length degradation**: Quality drops noticeably as sentences exceed typical training lengths; fixed-size vector bottleneck

- First 3 experiments:
  1. **Baseline comparison**: Run all four models on a 500-sentence sample from your domain; compute BLEU, COMET, and BLEURT to identify which model performs best for your specific content type
  2. **Length sensitivity test**: Segment your corpus by word count (short: <10, medium: 10-25, long: >25); plot metric scores vs. length to determine if your use case hits the degradation threshold
  3. **Back-translation fidelity check**: Select 50 domain-critical sentences; translate target→source→target; measure semantic drift using BLEURT between original and back-translated text to identify systematic meaning loss

## Open Questions the Paper Calls Out
None

## Limitations
- Model version ambiguity: Evaluation uses specific model checkpoints without exact version identifiers, creating uncertainty about replication with newer releases
- Corpus composition details: While corpus size is specified, exact sentence length distribution and linguistic complexity remain unclear
- Metric implementation variability: ML-based metrics depend on specific model checkpoints and implementation details not fully specified

## Confidence

**High Confidence**: Claims about relative model performance rankings (Google Translate > IndicTrans2 > OPUS-MT/NLLB-200) are supported by consistent results across multiple metrics and experimental conditions. The identification of systematic error patterns (gender markers, abbreviations, idioms) aligns with known linguistic challenges in English-Hindi translation.

**Medium Confidence**: The mechanism explaining why Google Translate outperforms others (extensive dataset enabling context extraction) is plausible but relies on assumptions about Google's training data composition that aren't directly verified. The observed translation direction asymmetry (Hindi→English better than English→Hindi) is well-documented in low-resource language pairs but depends on the specific training data distribution of each model.

**Low Confidence**: Claims about the superiority of ML-based metrics over lexical metrics assume that the particular BLEURT/COMET implementations used have been adequately trained on English-Hindi data, which isn't explicitly confirmed.

## Next Checks

1. **Model Version Reproduction**: Run the evaluation using current versions of all four models (particularly checking for Google Translate API updates) and compare performance shifts. This will determine whether the observed rankings remain stable over time.

2. **Metric Correlation Validation**: Compute inter-metric correlations (BLEU vs. BLEURT vs. COMET) on a subset of translations using human judgment as ground truth. This will verify whether ML-based metrics actually provide better semantic alignment than lexical metrics for this language pair.

3. **Length Threshold Analysis**: Segment the corpus into precise length bins (e.g., 5-word intervals) and compute metric degradation rates. This will identify the specific sentence length threshold where quality drops become significant for each model, enabling better deployment planning.