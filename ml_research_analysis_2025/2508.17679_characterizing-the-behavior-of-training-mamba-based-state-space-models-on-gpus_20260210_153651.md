---
ver: rpa2
title: Characterizing the Behavior of Training Mamba-based State Space Models on GPUs
arxiv_id: '2508.17679'
source_url: https://arxiv.org/abs/2508.17679
tags:
- mamba
- kernels
- such
- tensor
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive characterization of Mamba-based
  State Space Models (SSMs) during training on NVIDIA Hopper H100 GPUs. The authors
  evaluate four representative Mamba-based models (homogeneous Mamba, MambaVision,
  Mamba-Transformer Hybrid, and GraphMamba) across different domains including language,
  vision, and graph processing.
---

# Characterizing the Behavior of Training Mamba-based State Space Models on GPUs

## Quick Facts
- arXiv ID: 2508.17679
- Source URL: https://arxiv.org/abs/2508.17679
- Reference count: 16
- Key outcome: Comprehensive characterization of Mamba-based SSM training on NVIDIA H100 GPUs reveals memory bandwidth limitations (57.5% utilization) and vector compute bottlenecks, with matrix multiplication dominating runtime (60%) and SSM operators consuming significant time (20%).

## Executive Summary
This paper presents a comprehensive characterization of Mamba-based State Space Models (SSMs) during training on NVIDIA Hopper H100 GPUs. The authors evaluate four representative Mamba-based models (homogeneous Mamba, MambaVision, Mamba-Transformer Hybrid, and GraphMamba) across different domains including language, vision, and graph processing. The study reveals that while matrix multiplication operations dominate runtime (up to 60%), SSM operators also consume significant time (up to 20%). All models are memory-bound, with memory bandwidth utilization averaging only 57.5% of peak capacity. Analysis of SSM kernels shows low arithmetic intensity and short thread block lifetimes limiting performance. The authors find that Mamba2 kernels using both tensor and vector pipelines are predominantly vector-bound, suggesting the need to scale vector compute for future GPU designs. Key architectural bottlenecks include memory bandwidth constraints and vector compute limitations, pointing to optimization opportunities through persistent thread blocks and better tensor-vector operation scheduling.

## Method Summary
The study profiles training iterations of four Mamba-based models on a single NVIDIA H100 SXM GPU using Nsight Compute and Nsight Systems profiling tools. The methodology excludes the first iteration to avoid library tuning artifacts, uses NVTX instrumentation to isolate forward and backward passes, and enables FlashAttention for models with self-attention. Models are configured with parameters from prior work, using bookcorpus for language tasks, ImageNet for vision, and unspecified graph datasets. The profiling captures arithmetic intensity, operator breakdown, memory bandwidth utilization, cache hit rates, and kernel-level performance characteristics across the full training loop.

## Key Results
- Memory bandwidth utilization averages only 57.5% of peak capacity (3350 GB/s) across all models due to short thread block lifetimes in SSM kernels
- Mamba2 kernels with mixed tensor-vector operations are predominantly vector-bound (89-92% runtime) due to tensor:vector FLOPS ratio imbalance relative to hardware provisioning (16:1)
- Matrix multiplication operations dominate runtime (up to 60%) while SSM operators consume significant time (up to 20%)
- All evaluated models exhibit memory-bound behavior with arithmetic intensities below 3.5 FLOPs/byte

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SSM kernels underutilize available memory bandwidth due to short thread block lifetimes limiting saturation.
- **Mechanism:** SSM kernels (e.g., `ch_scan_fwd`) perform dot products with small contraction dimensions (chunk size or head dimension), resulting in minimal useful work per thread block relative to launch/setup/teardown overheads. This prevents the kernel from issuing enough memory requests to hide latency and saturate the 3350 GB/s memory bandwidth.
- **Core assumption:** Thread block overhead is non-negligible relative to compute work when contraction dimensions are small.
- **Evidence anchors:** [section V-C]: "On average, the utilization is only 57.5% from the peak... thread blocks with very short lifetime... actual useful work done inside the thread block is very short as opposed to the launch, setup and teardown overheads."

### Mechanism 2
- **Claim:** Mixed tensor-vector kernels in Mamba2 are predominantly vector-bound due to mismatch between workload tensor:vector FLOPS ratio and hardware provisioning.
- **Mechanism:** Mamba2 kernels combine tensor operations (dot products) with vector operations (element-wise multiply, exponentials via SFU). On H100, FP16 tensor:vector ratio is 16:1. When workload tensor:vector ratio falls below 16, vector units become the bottleneck even while tensor units have spare capacity.
- **Core assumption:** The measured FLOPS ratios from representative workloads generalize across SSM configurations.
- **Evidence anchors:** [section V-D]: "The mixed tensor and vector compute kernels are almost entirely dominated by vector compute... scaling the provisioned vector compute on the hardware is going to be critical."

### Mechanism 3
- **Claim:** Kernel-to-kernel data dependencies create L2 cache reuse opportunities that are currently underexploited.
- **Mechanism:** Output from one kernel (e.g., `bmm_ch_fwd`) becomes input to the next (e.g., `ch_scan_fwd`). If outputs fit in L2, subsequent kernels benefit from cache hits. Currently, these are separate kernel launches, so data round-trips through global memory. Kernel fusion could eliminate this.
- **Core assumption:** Intermediate tensors fit in L2 cache (50MB on H100) for representative batch/sequence configurations.
- **Evidence anchors:** [section V-C]: "Such dependencies across kernels also show potential kernel fusion optimizations that can be leveraged to avoid even having writing to the L2 cache in the first place."

## Foundational Learning

- **Concept: State Space Model (SSM) formulation**
  - **Why needed here:** The SSM block (Equations 1-2) is the core innovation; understanding how A, B, C, D matrices interact with hidden state h explains why Mamba differs from attention.
  - **Quick check question:** Can you explain why making B and C input-dependent (Mamba's innovation) enables "content-aware" behavior compared to fixed matrices in S4?

- **Concept: Arithmetic intensity and roofline analysis**
  - **Why needed here:** The paper diagnoses bottlenecks via arithmetic intensity (FLOPs/byte). Low intensity = memory-bound; high intensity = compute-bound. This is the framework for interpreting Figure 2.
  - **Quick check question:** Given a kernel with 100 FLOPs operating on 400 bytes of data, is it memory-bound on H100 with 3350 GB/s bandwidth and 989 TFLOP peak tensor compute?

- **Concept: Tensor vs. vector compute pipelines on GPUs**
  - **Why needed here:** Mamba2's hybrid kernel behavior hinges on understanding that tensor cores handle matrix ops (FP16/FP8) while vector units handle element-wise ops and special functions (exp, softmax). The ratio imbalance is central to the vector-bound finding.
  - **Quick check question:** If a kernel requires 16 tensor FLOPs for every 1 vector FLOP, and hardware provides 16:1 tensor:vector capacity, where is the bottleneck?

## Architecture Onboarding

- **Component map:** Input projection (GEMM) -> 1D convolution -> activation (softplus) -> SSM core (state update via A, B, C, D) -> output projection (GEMM)
- **Critical path:** 1. Profile full training iteration (exclude first iteration—library tuning) 2. Use NVTX markers to isolate forward/backward passes 3. Analyze operator breakdown (Figure 1): GEMM vs SSM vs other 4. Drill into SSM kernels: measure arithmetic intensity, cache hit rates, tensor:vector FLOPS ratio 5. Identify bottlenecks: memory bandwidth saturation, vector compute balance, thread block lifetime
- **Design tradeoffs:** Mamba1 (scan-based): Lower arithmetic intensity, single kernel, no tensor core utilization—simpler but memory-bound vs Mamba2 (matrix-based): Higher arithmetic intensity for some kernels, uses tensor cores, but introduces tensor-vector imbalance and more kernel launch overhead vs Kernel fusion: Reduces L2 traffic but increases kernel complexity and register pressure vs Persistent thread blocks: Reduce launch overhead but require careful resource management
- **Failure signatures:** Memory bandwidth utilization stuck at ~50-60% despite memory-bound classification -> short thread block lifetimes vs Tensor core utilization low while vector units saturated -> tensor:vector workload ratio below hardware ratio vs L1 cache hit rate near zero for non-tensor kernels -> minimal data reuse within SM
- **First 3 experiments:** 1. Baseline profiling: Run the four model variants with NVTX instrumentation; capture roofline (Figure 2) and operator breakdown (Figure 1). Verify memory bandwidth utilization ~57.5%. 2. Kernel-level drilldown: Isolate Mamba2 SSM kernels (Table II categories: tensor-only, vector-only, tensor+vector). Measure tensor:vector FLOPS ratio per kernel; confirm vector-bound behavior for hybrid kernels. 3. Persistent thread block test (if on Blackwell or via software emulation): Reimplement `ch_scan_fwd` with persistent threads; measure memory bandwidth saturation improvement. Compare launch overhead reduction vs baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can persistent thread block implementations eliminate the thread block launch/setup/teardown overheads that currently prevent SSM kernels from saturating available memory bandwidth?
- **Basis in paper:** [explicit] Authors state that "current implementations of the key kernels such as ch scan fwd produce thread blocks with very short lifetime" and that "Software optimizations, such as persistent thread-blocks...can help alleviate these overheads so that the kernels can saturate the available memory bandwidth more effectively."
- **Why unresolved:** The paper identifies the problem (only 57.5% memory bandwidth utilization due to short thread block lifetimes) but does not implement or evaluate persistent thread block solutions.
- **What evidence would resolve it:** Implementation of persistent thread blocks for key SSM kernels showing improved memory bandwidth utilization approaching peak capacity.

### Open Question 2
- **Question:** What is the optimal ratio of tensor-to-vector compute units for future GPU designs targeting Mamba-based SSM workloads?
- **Basis in paper:** [explicit] The paper finds that "mixed tensor and vector compute kernels are almost entirely dominated by vector compute" on H100 (tensor:vector ratio of 16:1), concluding that "scaling the provisioned vector compute on the hardware is going to be critical."
- **Why unresolved:** The paper identifies the imbalance but does not determine what ratio would achieve balanced execution without over-provisioning.
- **What evidence would resolve it:** Sensitivity analysis across different tensor-to-vector ratios showing utilization balance and performance scaling for SSM kernels.

### Open Question 3
- **Question:** What performance gains can be achieved through kernel fusion of dependent SSM operations (e.g., fusing bmm_ch_fwd output directly into ch_scan_fwd)?
- **Basis in paper:** [inferred] The paper notes dependencies where "the kernel bmm ch fwd computes the dot product of B and C tensors...the next kernel that is executed (i.e., ch scan fwd) uses this as an input" and mentions "potential kernel fusion optimizations that can be leveraged to avoid even having writing to the L2 cache."
- **Why unresolved:** The dependency pattern is identified but fusion implementation and quantified benefits are not explored.
- **What evidence would resolve it:** Fused kernel implementations with measured reduction in L2 cache writes and end-to-end latency improvements.

## Limitations
- Findings are based on a single GPU architecture (NVIDIA H100) and may not generalize to other hardware platforms
- Analysis focuses only on training performance, omitting inference considerations which may have different bottlenecks
- Proposed optimizations (persistent thread blocks, kernel fusion) are discussed conceptually but not empirically validated

## Confidence
- **High Confidence:** Memory bandwidth underutilization (57.5%) and its link to short thread block lifetimes is well-supported by profiling data and microarchitecture analysis
- **Medium Confidence:** The tensor-vector imbalance in Mamba2 kernels is plausible but relies on FLOPS ratio calculations that may vary with model configurations
- **Medium Confidence:** L2 cache reuse opportunities are identified through kernel dependency analysis but lack experimental validation of fusion benefits

## Next Checks
1. **Parameter Sensitivity:** Test memory bandwidth utilization and arithmetic intensity across a range of batch sizes, sequence lengths, and chunk sizes to determine where bottlenecks shift or diminish
2. **Hardware Generalization:** Profile the same models on alternative GPUs (e.g., A100, MI300X) to verify if vector-bound behavior and memory bottlenecks persist across architectures
3. **Optimization Prototyping:** Implement persistent thread blocks for `ch_scan_fwd` (via software emulation if needed) and measure bandwidth saturation improvements to validate the proposed solution