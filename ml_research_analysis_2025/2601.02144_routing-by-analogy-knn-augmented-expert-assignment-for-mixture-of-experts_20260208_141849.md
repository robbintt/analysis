---
ver: rpa2
title: 'Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts'
arxiv_id: '2601.02144'
source_url: https://arxiv.org/abs/2601.02144
tags:
- router
- knn-moe
- expert
- memory
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: kNN-MoE introduces a retrieval-augmented routing framework that
  improves expert assignment decisions in Mixture-of-Experts (MoE) models by reusing
  optimal past routing choices. The method builds a memory of router inputs paired
  with optimal expert assignments derived offline through likelihood maximization
  on a reference set, then retrieves and interpolates these assignments at inference
  time based on input similarity.
---

# Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts

## Quick Facts
- arXiv ID: 2601.02144
- Source URL: https://arxiv.org/abs/2601.02144
- Authors: Boxuan Lyu; Soichiro Murakami; Hidetaka Kamigaito; Peinan Zhang
- Reference count: 7
- Primary result: kNN-MoE improves expert assignment in MoE models via retrieval-augmented routing, achieving performance competitive with supervised fine-tuning while requiring significantly less training time

## Executive Summary
kNN-MoE introduces a retrieval-augmented routing framework that improves expert assignment decisions in Mixture-of-Experts (MoE) models by reusing optimal past routing choices. The method builds a memory of router inputs paired with optimal expert assignments derived offline through likelihood maximization on a reference set, then retrieves and interpolates these assignments at inference time based on input similarity. Experiments across three MoE models (OLMoE, GPT-OSS, Qwen3) and five benchmarks show kNN-MoE consistently outperforms zero-shot and router-only fine-tuning baselines, achieving performance competitive with supervised fine-tuning while requiring significantly less training time. The approach is particularly effective for out-of-distribution inputs where the parametric router is uncertain.

## Method Summary
kNN-MoE operates by constructing a memory store containing router inputs as keys and their optimal expert assignments (derived via offline gradient-based likelihood maximization) as values. During inference, for each token's router input, the method retrieves K nearest neighbors from this memory, computes a confidence-driven mixing coefficient based on aggregate similarity, and interpolates between the parametric router's prediction and the retrieved assignments. The method requires a reference set to build the memory offline, then uses FAISS for efficient neighbor search during inference. Key design choices include K=1 neighbor retrieval, RBF similarity function, and single gradient step (S=1) for memory construction optimization.

## Key Results
- kNN-MoE consistently outperforms zero-shot and router-only fine-tuning baselines across all three MoE models and five benchmarks
- Performance is competitive with supervised fine-tuning while requiring significantly less training time
- Memory size of 0.5k–1k examples is sufficient for most gains
- K=1 neighbor retrieval is optimal; larger K introduces noise
- RBF similarity function outperforms cosine for most models

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Routing for Distribution Shift
kNN-MoE improves expert assignment primarily for out-of-distribution inputs where the parametric router exhibits high uncertainty (correlated with high perplexity), by retrieving and reusing optimal past routing decisions from a memory store. For each token's router input $x$, the method retrieves $K$ nearest neighbors from a pre-constructed memory containing pairs of past router inputs (keys) and their offline-optimized expert assignments (values). The method interpolates between the parametric router's prediction and the retrieved assignments using a confidence-driven mixing coefficient $\lambda(x)$ derived from aggregate similarity. Core assumptions include that router inputs from similar tokens benefit from similar optimal expert assignments, and that high perplexity correlates with router uncertainty. Evidence shows kNN-MoE yields largest gains on High-PPL instances (+6.15 on GPQA, +6.32 on USMLE) while Low-PPL gains are near zero or slightly negative.

### Mechanism 2: Offline Likelihood-Maximized Assignment Optimization
Storing offline-optimized expert assignments derived via gradient-based likelihood maximization provides superior routing targets compared to the original router's predictions or router-only fine-tuning. For each token $t$ in $D_{\text{ref}}$, a learnable logit vector $r_t$ is optimized via gradient descent to minimize the negative log-likelihood of the ground-truth token, with all model parameters frozen. The resulting assignment $a^*(x_t) = \pi(r_t^{(S)})$ is stored in memory. Core assumptions include that the optimal expert assignment is approximable through this optimization and transfers to similar inputs at inference, and that a single gradient step captures critical re-weighting information. Evidence shows increasing steps $S \in \{1, 3, 10\}$ yields negligible improvement, with the initial gradient direction capturing the most critical information.

### Mechanism 3: Confidence-Driven Adaptive Mixing
Using aggregate neighbor similarity as a mixing coefficient $\lambda(x)$ enables adaptive interpolation between parametric and memory-based routing, providing robustness against retrieval noise. $\lambda(x) = \frac{1}{K} \sum_{j=1}^K s(x, k_j)$ controls the blend: $a_{\text{final}}(x) = (1 - \lambda(x))a(x) + \lambda(x) a_{\text{mem}}(x)$. When $\lambda \approx 1$, memory is trusted; when $\lambda \approx 0$, the method falls back to the parametric router. Core assumptions include that higher aggregate similarity indicates higher confidence in retrieved routing, and that RBF kernel is appropriate for router input similarity. Evidence shows RBF outperforms cosine on OLMoE/GPT-OSS, suggesting magnitude carries discriminative signals.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: kNN-MoE operates on the router's interface. You must understand how parametric routers produce sparse expert assignments via Top-K softmax and why routing quality limits MoE performance.
  - Quick check question: Given router input $x \in \mathbb{R}^d$ and weights $W_r \in \mathbb{R}^{d \times N}$ (N experts), what does a standard Top-K MoE router output?

- **k-Nearest Neighbors (kNN) Retrieval**: The core mechanism retrieves similar past router inputs based on vector similarity. Understanding similarity metrics and $K$ selection is critical.
  - Quick check question: Why might $K=1$ outperform larger $K$ values here, and what does this suggest about optimal assignment distribution in memory?

- **Test-Time Adaptation vs. Offline Memory Construction**: The paper contrasts offline memory construction with online methods like C3PO. This distinction explains the latency-accuracy trade-offs and competitiveness with SFT.
  - Quick check question: What is the fundamental computational cost difference between kNN-MoE and test-time adaptation during inference?

## Architecture Onboarding

- **Component map**: Memory Store $\mathcal{M}^{(\ell)}$ -> Parametric Router -> Retrieval Module -> Adaptive Mixing Module -> Expert Assignment
- **Critical path (Inference)**: 1. MoE layer receives router input $x$. 2. Parallel: (a) parametric router computes $a(x)$; (b) retrieval queries $\mathcal{M}^{(\ell)}$ for neighbors and similarities. 3. Compute $a_{\text{mem}}(x)$ via similarity-weighted aggregation. 4. Compute $\lambda(x)$ from aggregate similarity. 5. Blend: $a_{\text{final}}(x) = (1 - \lambda(x))a(x) + \lambda(x) a_{\text{mem}}(x)$. 6. Weight expert outputs with $a_{\text{final}}(x)$.
- **Critical path (Memory Construction, Offline)**: 1. Run frozen MoE on $D_{\text{ref}}$ (teacher-forcing). 2. Collect router inputs $\{x_t\}$ and targets $\{y_t\}$. 3. Initialize logits $r_t^{(0)} = x_t W_r$; perform $S=1$ gradient step on $L_t(r)$. 4. Store $(x_t, a^*(x_t))$ in $\mathcal{M}^{(\ell)}$.
- **Design tradeoffs**: $K=1$ optimal; $S=1$ sufficient; RBF similarity preferred; 0.5k–1k examples sufficient; ~34% latency overhead for OLMoE with |Dref|=1k.
- **Failure signatures**: No improvement when $|D_{\text{ref}}|$ too small or distribution mismatch; degradation with $K > 1$ or wrong similarity function; high latency with large index size.
- **First 3 experiments**: 1. Memory size ablation: vary $|D_{\text{ref}}| \in \{0, 0.25k, 0.5k, 1k\}$ on MedMCQA with OLMoE. 2. Perplexity-bucketed analysis: bucket test instances by zero-shot perplexity; report accuracy gain per bucket. 3. $K$ and similarity grid search: test $K \in \{1, 3, 5\}$ with RBF vs. cosine on validation set.

## Open Questions the Paper Calls Out

### Open Question 1
Can kNN-MoE maintain performance when memory is constructed using pseudo-labels from an LLM-as-a-judge instead of ground-truth labels? The authors state they will extend kNN-MoE to settings where the reference set is unlabeled: "we can use LLM-as-a-judge... to generate pseudo labels." This is unresolved because the current method relies on ground-truth tokens to derive optimal assignments via likelihood maximization, and it's unclear if LLM-generated pseudo-labels would degrade the quality of stored expert assignments.

### Open Question 2
Can memory pruning or key quantization reduce the inference latency of kNN-MoE without degrading routing accuracy? Section 7 identifies "accelerating retrieval by pruning the memory or compressing the stored router inputs (keys), for example, by reducing the vector dimensionality or applying quantization" as a future direction. Current inference latency increases with reference set size due to neighbor search costs. While compression is proposed, the specific trade-off between key fidelity and retrieval speed remains unquantified.

### Open Question 3
Is there a low-cost proxy for perplexity that enables efficient selective retrieval? Section 6.1 notes that a "Selective" variant (skipping retrieval for low-perplexity inputs) improves accuracy but was rejected because calculating perplexity requires a "preliminary forward pass" that introduces "significant computational overhead." The theoretical benefit of selective retrieval is established, but the practical mechanism to trigger it efficiently (without a full forward pass) is missing.

## Limitations
- The method assumes optimal routing decisions transfer between similar inputs, which may break down for highly specialized or out-of-distribution queries where context and semantics diverge significantly from stored cases
- The offline memory construction requires substantial compute for the reference set optimization, though the authors report minimal steps (S=1) suffice
- The memory store grows linearly with reference set size, creating storage and latency concerns for production deployment at scale
- The evaluation focuses primarily on accuracy gains without analyzing the downstream effects on model calibration, fairness, or bias propagation from the memory store

## Confidence

**High Confidence (5/5):**
- kNN-MoE consistently outperforms zero-shot and router-only fine-tuning baselines across all three MoE models and five benchmarks
- Performance is competitive with supervised fine-tuning while requiring significantly less training time
- Memory size of 0.5k–1k examples is sufficient for most gains
- K=1 neighbor retrieval is optimal; larger K introduces noise
- RBF similarity function outperforms cosine for most models

**Medium Confidence (3/5):**
- The mechanism works primarily by correcting router uncertainty on out-of-distribution inputs (based on perplexity-bucketed analysis)
- Single gradient step (S=1) captures sufficient routing optimization information
- The confidence-driven mixing coefficient λ(x) effectively balances parametric and memory-based routing

**Low Confidence (2/5):**
- Claims about the relationship between perplexity and router uncertainty requiring validation across broader task distributions
- Generalizability of memory construction optimization across different MoE architectures and routing strategies
- Long-term stability and robustness of retrieved routing decisions in production settings

## Next Checks

1. **Cross-Domain Transfer Validation**: Test kNN-MoE on datasets from entirely different domains than the reference set (e.g., using medical text memory to route legal or scientific reasoning tasks) to verify the robustness of the similarity-based transfer assumption and identify failure modes when semantic contexts diverge.

2. **Memory Store Analysis**: Conduct ablation studies varying the composition and diversity of the reference set (e.g., stratified sampling by difficulty, topic, or style) to determine whether memory effectiveness depends on coverage diversity versus sheer size, and whether certain types of queries benefit more from retrieval augmentation.

3. **Temporal Stability Assessment**: Evaluate kNN-MoE performance over extended inference sessions with streaming inputs to measure how retrieval quality degrades as the memory store becomes less representative of the input distribution, and whether periodic memory refresh or adaptive updates are necessary for sustained performance.