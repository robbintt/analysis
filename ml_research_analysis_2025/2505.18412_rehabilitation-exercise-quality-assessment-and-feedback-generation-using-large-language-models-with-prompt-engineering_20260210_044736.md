---
ver: rpa2
title: Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large
  Language Models with Prompt Engineering
arxiv_id: '2505.18412'
source_url: https://arxiv.org/abs/2505.18412
tags:
- rehabilitation
- exercise
- feedback
- quality
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that pre-trained Large Language Models\
  \ (LLMs) can assess rehabilitation exercise quality and generate actionable feedback\
  \ using carefully engineered prompts and exercise-specific features extracted from\
  \ body joint data. The approach achieves high accuracy in exercise classification\
  \ across two datasets\u2014UI-PRMD and REHAB24-6\u2014with best results from three-shot\
  \ prompting and certainty elicitation."
---

# Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering

## Quick Facts
- **arXiv ID**: 2505.18412
- **Source URL**: https://arxiv.org/abs/2505.18412
- **Reference count**: 40
- **Primary result**: LLMs can assess rehabilitation exercise quality and generate feedback using engineered prompts and exercise-specific features, achieving high classification accuracy without fine-tuning.

## Executive Summary
This paper demonstrates that pre-trained Large Language Models can assess rehabilitation exercise quality and generate actionable feedback by converting raw body joint data into exercise-specific kinematic features and using carefully engineered prompts. The approach achieves high accuracy in exercise classification across two datasets—UI-PRMD and REHAB24-6—with best results from three-shot prompting and certainty elicitation. While LLMs slightly underperformed deep learning models on one dataset, they offer unique advantages by providing interpretable reasoning and personalized natural language feedback. The method avoids the need for fine-tuning and is applicable to virtual rehabilitation platforms, though quantitative feedback evaluation remains limited due to the lack of labeled feedback datasets.

## Method Summary
The approach converts 3D body joint coordinates into domain-specific kinematic features (angles, stability metrics) that are more semantically meaningful for exercise assessment. These features, along with exercise instructions and labeled examples, are injected into prompts sent to a pre-trained LLM (GPT-4o) to classify exercise quality as correct or incorrect. Role-play prompting elicits natural language feedback by asking the LLM to embody a physiotherapist persona. The method uses few-shot learning through in-context examples rather than fine-tuning, and employs certainty elicitation to improve classification confidence assessment.

## Key Results
- Using exercise-specific features (angles, stability) yielded superior performance compared to raw body joint data
- Three-shot prompting achieved the highest classification accuracy, with performance degrading at 4+ shots due to context overload
- Certainty elicitation improved recall, while Chain-of-Thought prompting improved precision
- LLM feedback generation provided personalized, natural language guidance beyond the input features
- The approach achieved high accuracy on both UI-PRMD and REHAB24-6 datasets without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Semantic Feature Abstraction
Converting raw 3D joint coordinates into domain-specific kinematic features (angles, stability) improves LLM classification accuracy compared to raw coordinate streams. This translates numerical noise into semantic tokens that the LLM has likely encountered in its pre-training corpus (medical/rehabilitation literature), reducing the reasoning burden and allowing it to map concepts directly to labels.

### Mechanism 2: In-Context Pattern Matching (Few-Shot)
Providing labeled examples (shots) in the prompt establishes a transient classification boundary that outperforms zero-shot inference but saturates quickly. The LLM uses attention mechanisms to compare the test sample against the labeled examples, effectively acting as a nearest-neighbor classifier in the model's latent space without requiring weight updates.

### Mechanism 3: Role-Based Knowledge Retrieval
Role-play prompting ("You are a physiotherapist") elicits feedback containing knowledge not explicitly present in the input features. By conditioning generation on a physiotherapist persona, the model biases its generation toward professional tone and holistic medical advice, acting as a retrieval mechanism for general rehabilitation knowledge stored in the model's weights.

## Foundational Learning

**Concept: Prompt Engineering (Zero vs. Few-Shot)**
- Why needed here: The methodology relies on querying a frozen LLM, so understanding how "shots" affect the model's ability to "learn" the task dynamically is critical for replicating results.
- Quick check question: Why did 4-shot prompting perform worse than 3-shot prompting in the paper's experiments? (Answer: Context noise/overloading or distraction)

**Concept: Skeletal Joint Kinematics**
- Why needed here: To interpret the feature extraction logic, understanding that raw (x,y,z) coordinates are useless without calculating relative angles which define movement quality.
- Quick check question: Why is "Knee Valgus Angle" a better input feature for assessing a squat than the raw (x,y,z) position of the knee?

**Concept: LLM Overconfidence/Hallucination**
- Why needed here: The paper notes that high certainty scores (0.8–1.0) were often attached to wrong answers, requiring engineers to distrust the model's self-reported confidence.
- Quick check question: What is the "Chain-of-Thought" failure mode where the LLM invents its own thresholds? (Answer: Imposing hard thresholds, e.g., "150 degrees", that don't match clinical guidelines)

## Architecture Onboarding

**Component map**: Input Layer (Body Joint Sequence) -> Feature Extractor (Joints → Angles/Stability) -> Prompt Constructor (Features + Examples + Instruction) -> LLM Engine (GPT-4o) -> Output Parser (Extracts Label/Reasoning/Feedback)

**Critical path**: Feature Extraction → Prompt Context Window. The accuracy drop when using raw joints indicates that the Feature Extractor is the most critical preprocessing step.

**Design tradeoffs**:
- Deep Learning (LSTM/ST-GCN) vs. LLM: LSTM/ST-GCN (87-94% accuracy) outperforms LLM (76%) on UI-PRMD classification, but LLM provides interpretability and feedback generation
- Certainty vs. Chain-of-Thought: Certainty yields higher recall, CoT yields higher precision; choose based on whether false positives or false negatives are more dangerous

**Failure signatures**:
- Threshold Hallucination: LLM outputs strict numerical rules (e.g., "Threshold is 30 degrees") that don't exist in prompt or clinical guidelines
- Context Overload: Performance degrades at 4-5 shots due to attending to irrelevant parts of long prompt
- Constant Certainty: Model assigns high probability/certainty to almost all inputs, making scores poor discriminators

**First 3 experiments**:
1. Feature Ablation: Run pipeline with raw (x,y,z) joints vs. extracted angles on single exercise to reproduce accuracy delta
2. Shot Sensitivity Sweep: Test 0-shot through 5-shot on hold-out set to verify "peak at 3-shot" phenomenon
3. Feedback Safety Check: Generate feedback for deliberately "incorrect" movement and inspect for hallucinated safety constraints

## Open Questions the Paper Calls Out

**Open Question 1**: How does fine-tuning LLMs specifically for rehabilitation exercise assessment compare to prompt engineering approaches in terms of accuracy, scalability, and resource requirements? (Basis: Authors state fine-tuning could enhance performance and list it as future work)

**Open Question 2**: How can ground-truth textual feedback datasets for rehabilitation exercises be created and standardized to enable quantitative evaluation of LLM-generated feedback? (Basis: Authors identify lack of publicly available rehabilitation exercise datasets with ground-truth textual feedback as key limitation)

**Open Question 3**: How can LLM confidence calibration be improved to accurately reflect classification correctness in rehabilitation exercise assessment? (Basis: Authors note LLMs remained overconfident of outcomes irrespective of correctness)

**Open Question 4**: What exercise characteristics explain the performance variations observed across different rehabilitation movements in LLM-based assessment? (Basis: Authors observed differences in performance across movements, with squats performing better than asymmetric movements like leg lunges)

## Limitations

- Quantitative feedback validation is impossible due to absence of labeled feedback datasets
- Performance on exercises, patient populations, or environments outside UI-PRMD and REHAB24-6 datasets remains unknown
- LLM confidence scores poorly calibrated with actual accuracy, posing safety risks
- Role-based prompting risks generating hallucinated or potentially unsafe medical advice

## Confidence

- **High Confidence**: Feature engineering (converting joints to angles) improves LLM classification accuracy over raw coordinates
- **Medium Confidence**: 3-shot prompting outperforms both zero-shot and higher-shot prompting, though variance at 4+ shots suggests context sensitivity
- **Low Confidence**: Qualitative assessment of feedback generation's usefulness and safety cannot be confidently validated without quantitative metrics

## Next Checks

1. **Feedback Safety Audit**: Generate feedback for diverse flawed movements (including dangerous conditions) and have clinical experts rate each output for safety and appropriateness
2. **Cross-Dataset Generalization Test**: Evaluate the pipeline on held-out rehabilitation dataset with different exercises, sensor configurations, or patient populations
3. **Confidence Calibration Benchmark**: Create test set with known ground truth and measure relationship between LLM's reported certainty scores and actual accuracy using calibration metrics like Expected Calibration Error