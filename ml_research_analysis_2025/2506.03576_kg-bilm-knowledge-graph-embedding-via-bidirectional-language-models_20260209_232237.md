---
ver: rpa2
title: 'KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models'
arxiv_id: '2506.03576'
source_url: https://arxiv.org/abs/2506.03576
tags:
- graph
- knowledge
- entity
- arxiv
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KG-BiLM introduces a unified knowledge graph embedding framework
  that bridges the gap between structural and textual representations by leveraging
  bidirectional attention in a generative transformer. Unlike traditional methods
  that prioritize either graph structure or language semantics, KG-BiLM removes the
  causal mask to allow full interaction between tokens and entities, integrates masked
  knowledge prediction to recover missing tokens using both local and global context,
  and employs contrastive graph semantic aggregation to preserve KG topology.
---

# KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models

## Quick Facts
- arXiv ID: 2506.03576
- Source URL: https://arxiv.org/abs/2506.03576
- Reference count: 40
- Primary result: State-of-the-art MRR 0.403 on Wikidata5M, surpassing text-based baselines by up to 2.3 points

## Executive Summary
KG-BiLM introduces a unified knowledge graph embedding framework that bridges the gap between structural and textual representations by leveraging bidirectional attention in a generative transformer. Unlike traditional methods that prioritize either graph structure or language semantics, KG-BiLM removes the causal mask to allow full interaction between tokens and entities, integrates masked knowledge prediction to recover missing tokens using both local and global context, and employs contrastive graph semantic aggregation to preserve KG topology. Evaluated on four standard benchmarks, KG-BiLM achieves state-of-the-art performance on semantically enriched datasets (Wikidata5M: MRR 0.403; FB15k-237N: MRR 0.378), surpassing strong baselines by up to 2.3 points in MRR. On WN18RR, it reaches an MRR of 0.682, outperforming recent text-based models without relying on pre-trained text encoders. These results validate the effectiveness of KG-BiLM in unifying structural and semantic information for more accurate and generalizable knowledge representation learning.

## Method Summary
KG-BiLM is a generative language model for knowledge graph completion that uses a decoder-only Transformer with 24 layers and 1024 hidden dimensions. It modifies the standard causal mask with a graph-aware mask (BKA) allowing bidirectional attention between tokens within a local window or connected entities within 2 hops. The model employs position-shifted masked prediction (KMP) where masked tokens are predicted from the previous position's hidden state, and contrastive graph semantic aggregation (CGSA) using InfoNCE loss on two dropout-augmented views of subgraphs. Training combines three losses: KMP for masked token reconstruction, CGSA for topology preservation, and standard next-token prediction. The model operates without pre-trained text encoders on structure-only benchmarks while achieving strong results on text-rich datasets.

## Key Results
- Achieves MRR of 0.403 on Wikidata5M, outperforming KEPLER (0.380) and SimKGC (0.378) by 2.3 and 2.5 points respectively
- Reaches MRR of 0.378 on FB15k-237N, surpassing SimKGC (0.354) by 2.4 points
- Attains MRR of 0.682 on WN18RR, outperforming recent text-based models without pre-trained encoders
- Ablation studies show BKA contributes largest performance gains (up to 2.0 MRR drop when removed), followed by KMP (1.3 MRR drop) and CGSA (improves Hits@10 more than MRR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the causal mask in decoder-style transformers may improve multi-hop relational reasoning by enabling bidirectional attention between all tokens and entities.
- Mechanism: The Bidirectional Knowledge Attention (BKA) module replaces the standard causal mask with a graph-aware mask $M^{BKA}$ that allows token $x_i$ to attend to token $x_j$ if they are within a local window or their parent entities are connected within $h$ hops in the KG. This permits information flow from both preceding and succeeding contexts as well as graph neighbors.
- Core assumption: Meaningful relational patterns in KGs benefit from access to future tokens and distant graph neighbors during encoding; unidirectional attention obscures these dependencies.
- Evidence anchors:
  - [abstract] "removes the causal mask to allow full interaction between tokens and entities"
  - [section 4.4] Removing BKA causes the largest performance drop among ablated components (−2.0 MRR on Wikidata5M; −1.7 MRR on FB15k-237N)
  - [corpus] Related work on graph-LLM fusion (e.g., "Beyond Textual Context") similarly argues that treating KGs as plain text ignores structural aspects, though no direct comparison to BKA exists
- Break condition: If the hop threshold $h$ is too small, the mask reverts to near-causal behavior; if too large, attention may dilute across irrelevant entities, degrading precision.

### Mechanism 2
- Claim: Position-shifted masked prediction may enforce deeper structural-linguistic integration than standard masked language modeling.
- Mechanism: Knowledge-Masked Prediction (KMP) masks both entity and text tokens, then predicts the true token at position $i$ using the hidden state $h_{i-1}$ from the preceding position rather than $h_i$. This compels the model to aggregate bidirectional context and graph signals to infer the masked content.
- Core assumption: Shifting the prediction locus to the prior position simulates inference-time conditions and forces the model to exploit forward context captured via BKA.
- Evidence anchors:
  - [abstract] "integrates masked knowledge prediction to recover missing tokens using both local and global context"
  - [section 3.4] Eq. 10 defines $L_{KMP} = -\sum_{i \in M} \log p_\Theta(x_i | h_{i-1})$; the paper claims this enforces consistency and encourages global attention
  - [section 4.4] Ablation shows removing KMP causes −1.3 average MRR drop
  - [corpus] No direct corpus evidence on position-shifted loss specifically; related KG-LLM methods focus on prompting or fine-tuning without this mechanism
- Break condition: If masking ratio $\gamma$ is too high, the model may lack sufficient context; if too low, the structural-linguistic synergy is undertrained.

### Mechanism 3
- Claim: Contrastive alignment of augmented subgraph views may preserve global KG topology and enhance discriminative power.
- Mechanism: Contrastive Graph Semantic Aggregation (CGSA) generates two corrupted views of each subgraph via dropout/augmentation, encodes them through the BKA+KMP stack, pools representations, and applies an InfoNCE loss to pull paired views closer while pushing apart mismatched pairs in the batch.
- Core assumption: Contrastive objectives can transfer to heterogeneous graph-text pairs and mitigate embedding entanglement with local context.
- Evidence anchors:
  - [abstract] "employs contrastive graph semantic aggregation to preserve KG topology"
  - [section 4.4] Ablation shows removing CGSA hurts Hits@10 more than MRR, suggesting improved top-k discrimination
  - [corpus] Related work like "SEMMA" uses semantic-aware KG foundation models, but does not explicitly contrast CGSA-style objectives; corpus evidence is weak here
- Break condition: If augmentations break critical relational paths, positive pairs may become semantically misaligned, degrading contrastive learning.

## Foundational Learning

- Concept: **Transformer Self-Attention and Masking**
  - Why needed here: Understanding how causal vs. bidirectional masks control information flow is prerequisite to grasping BKA's modification.
  - Quick check question: Given a 5-token sequence, which tokens can position 3 attend to under a causal mask vs. under BKA with hop threshold $h=2$ if tokens 2 and 4 represent entities connected in the KG?

- Concept: **Knowledge Graph Embedding Fundamentals**
  - Why needed here: The paper builds on translational and semantic-matching KGE methods; familiarity with scoring functions (e.g., TransE, DistMult) clarifies what KG-BiLM improves upon.
  - Quick check question: Why do traditional KGE methods struggle with long-tail entities and zero-shot generalization?

- Concept: **Contrastive Learning (InfoNCE)**
  - Why needed here: CGSA relies on InfoNCE loss to align subgraph views; understanding positive/negative pair construction is essential.
  - Quick check question: In a batch of 4 subgraphs with 2 views each, how many positive and negative pairs contribute to the InfoNCE loss for a single anchor?

## Architecture Onboarding

- Component map: Input Encoding -> Bidirectional Knowledge Attention (BKA) -> Knowledge-Masked Prediction (KMP) -> Contrastive Graph Semantic Aggregation (CGSA)

- Critical path: Input → BKA layers (attention with $M^{BKA}$) → KMP loss (position-shifted reconstruction) → CGSA loss (contrastive alignment) → joint optimization. The three losses are combined during training; inference uses only the encoder.

- Design tradeoffs:
  - **Hop threshold $h$**: Higher $h$ increases multi-hop reasoning capacity but raises attention complexity; paper uses $h=2$.
  - **Masking ratio $\gamma$**: Higher $\gamma$ increases difficulty; $\gamma=0.15$ balances reconstruction and context availability.
  - **CGSA batch size**: Larger batches improve negative sample diversity but require more memory; $B=256$ used.

- Failure signatures:
  - **Relation-cardinality sensitivity**: On FB15k-237 (237 relations, no text), BKA relies solely on topology, limiting gains vs. specialized models like NBFNet (Section 6, Limitations).
  - **Semantic sparsity dependence**: Without textual descriptions, CGSA's contrastive alignment is underutilized (Section 6).
  - **Ablation patterns**: Removing BKA causes largest MRR drop; removing CGSA preferentially harms Hits@10.

- First 3 experiments:
  1. **Ablation on validation split**: Run full model vs. w/o BKA, w/o KMP, w/o CGSA on Wikidata5M and FB15k-237N to quantify component contributions (Table 4).
  2. **Zero-shot evaluation on Wikidata5M**: Compare KG-BiLM to KEPLER and SimKGC on held-out entities to validate generalization (Table 5).
  3. **Structural-only benchmark test**: Evaluate on WN18RR and FB15k-237 to isolate graph reasoning ability from textual semantics (Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KG-BiLM overcome its performance deficit on structure-only benchmarks with high relation cardinality?
- Basis: [explicit] Appendix F explicitly identifies "Relation-cardinality sensitivity" and "Semantic sparsity dependence" as intrinsic limitations when textual cues are absent.
- Why unresolved: The model currently relies heavily on text to disambiguate relations; it struggles when forced to rely exclusively on topological co-occurrence signals.
- What evidence would resolve it: Significant performance gains on FB15k-237 (a dataset with many relations but no text) specifically targeting the gap with path-based models like NBFNet.

### Open Question 2
- Question: Does the removal of the causal mask degrade the model's generative fluency despite claims of preserving autoregressive capabilities?
- Basis: [inferred] Section 3.3 claims BKA preserves autoregressive capabilities, yet Section 4 only evaluates discriminative link prediction metrics.
- Why unresolved: Bidirectional attention typically violates the token-by-token dependency required for text generation; the paper does not assess the quality of generated text.
- What evidence would resolve it: Evaluation of generated text coherence or perplexity scores to verify if the model retains generative utility alongside representation learning.

## Limitations
- Performance degradation on structure-only benchmarks with high relation cardinality (FB15k-237), where specialized models like NBFNet outperform KG-BiLM
- Limited effectiveness of contrastive objectives when textual descriptions are absent, reducing CGSA's contribution to topology preservation
- Ambiguity around whether pre-trained text encoders are used for text-rich datasets despite claims of training "without pre-trained text encoders"

## Confidence
- **High Confidence**: The overall superiority of KG-BiLM on semantically enriched datasets (Wikidata5M MRR 0.403, FB15k-237N MRR 0.378) is well-supported by ablation studies and comparison to strong baselines
- **Medium Confidence**: The specific mechanism by which position-shifted prediction ($h_{i-1}$ predicting $x_i$) contributes to performance is supported by ablation but lacks direct corpus evidence
- **Medium Confidence**: The claim that removing the causal mask improves multi-hop reasoning is supported by ablation (largest MRR drop when removing BKA) and related work, but direct experimental validation of reasoning depth is limited

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the local window size $\delta$ and hop threshold $h$ to identify their impact on performance and computational requirements. This would clarify whether the chosen values are optimal or if the method is robust to these settings.

2. **Zero-Shot Generalization Benchmark**: Evaluate KG-BiLM on a held-out entity test set from Wikidata5M to quantify generalization to unseen entities. This would validate whether the model's performance gains translate to improved zero-shot reasoning, a key advantage claimed for generative approaches.

3. **Cross-Dataset Transfer Experiment**: Train KG-BiLM on FB15k-237 (structure-only) and evaluate on Wikidata5M (text-rich) without fine-tuning. This would test whether the bidirectional architecture and contrastive objectives enable effective transfer from structure to semantics, isolating the contribution of architectural choices from dataset-specific advantages.