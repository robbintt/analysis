---
ver: rpa2
title: 'OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache
  Reuse for Diffusion Transformer Models'
arxiv_id: '2508.16212'
source_url: https://arxiv.org/abs/2508.16212
tags:
- sampling
- cache
- reuse
- diffusion
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniCache introduces a trajectory-based cache reuse strategy for
  diffusion transformer models that addresses the inefficiency of existing cache methods
  focused solely on step similarity. The core insight is that sampling trajectories
  exhibit predictable geometric patterns independent of input content, with curvature
  serving as an optimal indicator for cache deployment.
---

# OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models

## Quick Facts
- **arXiv ID:** 2508.16212
- **Source URL:** https://arxiv.org/abs/2508.16212
- **Reference count:** 40
- **Primary result:** 2-2.5× speedup on OpenSora and Latte models with negligible quality degradation through trajectory-based cache reuse

## Executive Summary
OmniCache introduces a novel training-free cache reuse strategy for Diffusion Transformer (DiT) models that leverages predictable geometric patterns in sampling trajectories. Unlike existing cache methods that focus solely on step similarity, OmniCache uses curvature as an indicator to strategically distribute cache reuse across the entire sampling process. The method dynamically estimates and corrects cache-induced noise using inter-step correlation analysis and applies frequency-specific filtering based on sampling stage. Extensive experiments demonstrate significant acceleration (2-2.5× on OpenSora/Latte, 1.45× on distilled models) while maintaining competitive quality metrics.

## Method Summary
OmniCache operates in two stages: calibration and inference. During calibration, the method runs a forward pass to store hidden states and compute trajectory curvature in 3D space, selecting low-curvature steps for cache reuse while avoiding more than 2 consecutive cached steps. Noise correlation between adjacent steps is computed during this phase. During inference, at cached steps the model reuses previously stored attention and MLP outputs, estimates the cache-induced noise using the precomputed correlation, and applies frequency filtering (low-pass early, high-pass late) before subtracting from the output. This trajectory-oriented approach distributes cache reuse globally rather than concentrating on later steps, minimizing overall trajectory deviation.

## Key Results
- Achieves 2-2.5× wall-clock speedup on OpenSora (480p, 30 steps) and Latte (512×512, 50 steps) models
- Maintains competitive quality with 0.7 VBench score on OpenSora vs 0.6 for baseline, and 0.5 on Latte
- Demonstrates effectiveness on distilled models where existing cache methods fail, achieving 1.45× acceleration on CogVideoX-5b-I2V-distill
- Outperforms AB-Cache and other baselines across multiple metrics including FID, PSNR, and SSIM

## Why This Works (Mechanism)
OmniCache exploits the geometric regularity of sampling trajectories in DiT models, which exhibit predictable "boomerang" shapes independent of input content. By using curvature as a proxy for information density, the method identifies steps where cached outputs introduce minimal trajectory deviation. The noise correction mechanism leverages inter-step correlation to estimate and compensate for cache-induced errors, while frequency filtering adapts to different sampling stages. This global perspective on cache reuse, combined with dynamic noise estimation, enables efficient acceleration without quality degradation.

## Foundational Learning
- **Diffusion sampling trajectory geometry:** Understanding the predictable geometric patterns (boomerang shape) that sampling trajectories follow in DiT models, which is content-agnostic and enables curvature-based cache scheduling.
  - *Why needed:* Forms the theoretical foundation for selecting cache steps based on geometric properties rather than content similarity
  - *Quick check:* Verify that projected trajectories from different inputs follow similar geometric patterns in 3D space

- **Curvature computation for trajectory analysis:** Calculating discrete curvature from 3D-projected sampling trajectories to identify low-information-density steps suitable for cache reuse.
  - *Why needed:* Provides the quantitative metric for determining which steps to cache, balancing acceleration with quality preservation
  - *Quick check:* Confirm that curvature values correlate with steps where cached outputs cause minimal deviation

- **Noise correlation estimation in diffusion models:** Computing inter-step noise correlations to estimate cache-induced errors and enable dynamic correction during inference.
  - *Why needed:* Enables the method to compensate for errors introduced by cache reuse, maintaining output quality
  - *Quick check:* Verify that estimated noise correlations accurately predict actual noise differences between consecutive steps

- **Frequency-domain filtering for noise correction:** Applying stage-specific frequency filters (low-pass early, high-pass late) to the estimated noise before correction.
  - *Why needed:* Adapts the noise correction to different sampling stages, optimizing the balance between noise removal and detail preservation
  - *Quick check:* Confirm that filtering improves quality metrics compared to direct noise subtraction

## Architecture Onboarding
- **Component map:** Input → DiT backbone (Attention + MLP) → Hidden states storage → Curvature computation → Cache step selection → Noise correlation estimation → Inference with cache reuse → Frequency filtering → Output
- **Critical path:** Calibration stage (hidden states → curvature → cache selection) → Inference stage (cache reuse + noise estimation + filtering)
- **Design tradeoffs:** Global vs local cache distribution (OmniCache chooses global for consistent quality), noise estimation accuracy vs computational overhead, frequency filter specificity vs generalization
- **Failure signatures:** Model collapse on distilled models (insufficient correction capacity), three+ consecutive cached steps causing error accumulation, quality degradation when caching high-curvature steps
- **First experiments:** 1) Implement curvature-based step selection and verify geometric patterns across inputs, 2) Test noise correlation estimation on simple model, 3) Compare quality metrics with and without frequency filtering

## Open Questions the Paper Calls Out
- **Question:** How can the constraint prohibiting cache reuse for three consecutive steps be relaxed to enable higher acceleration factors?
- **Basis in paper:** [explicit] The authors state in Section 5 that "OmniCache's main limitation is that cache reuse cannot be applied for three consecutive steps... which may limit its maximum acceleration capability."
- **Question:** Is the "boomerang" trajectory shape and the resulting curvature-based cache schedule universal across all DiT architectures, or does it require recalibration for different model backbones?
- **Basis in paper:** [inferred] Section 3.4 describes a "calibration stage" to determine the reuse set, while Section 3.2 notes trajectory regularity is "independent of the specific generated content," leaving the transferability of the curvature profile across different model architectures ambiguous.
- **Question:** Can the first-order linear approximation of cache-induced noise be refined using higher-order terms to improve accuracy in low-redundancy, distilled models?
- **Basis in paper:** [inferred] Section 3.3 estimates noise correlation via a Taylor expansion ($\approx \gamma_{t-1} q_{\theta}(x_t, t)$) while acknowledging the existence of higher-order error terms ($O(\Delta t^2)$).

## Limitations
- Cannot apply cache reuse for three consecutive steps, limiting maximum acceleration capability
- Requires per-model calibration to determine cache steps and noise correlations
- Performance on extremely long sequences (>250 steps) or different architectures not extensively validated
- Relies on unspecified implementation details for curvature computation and frequency filtering

## Confidence
- **High confidence:** The core trajectory-based insight and 2-2.5× speedup claims are well-supported by experimental results
- **Medium confidence:** Near-lossless quality preservation claims depend on unspecified noise estimation and filtering implementation
- **Low confidence:** The 1.45× acceleration on distilled models claim is based on limited experiments with minimal explanation of why OmniCache succeeds where others fail

## Next Checks
1. Implement curvature-based step selection during calibration and verify that selected steps match paper's description of 50-60% coverage with no more than 2 consecutive cached steps
2. Validate noise correlation estimation by implementing the correlation-based method and testing noise correction on a simple DiT model to confirm trajectory deviation reduction
3. Benchmark OmniCache against AB-Cache on OpenSora and Latte models, measuring both speedup and quality metrics (FID, VBench score) to verify claimed improvements