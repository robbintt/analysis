---
ver: rpa2
title: 'Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic
  Thresholds'
arxiv_id: '2504.04973'
source_url: https://arxiv.org/abs/2504.04973
tags:
- thresholds
- constraint
- learning
- pessimistic
- optimistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of constrained Markov decision
  processes (CMDPs) when safety thresholds are unknown and stochastic. The authors
  introduce a novel approach that treats thresholds as latent stochastic processes,
  where the agent only receives noisy per-step signals rather than explicit threshold
  values.
---

# Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds

## Quick Facts
- **arXiv ID:** 2504.04973
- **Source URL:** https://arxiv.org/abs/2504.04973
- **Reference count:** 40
- **Primary result:** First theoretical guarantees for CMDPs with unknown, stochastic safety thresholds; achieves Õ(√T) regret and constraint violation.

## Executive Summary
This paper addresses the challenge of safe reinforcement learning when safety thresholds are not only unknown but also revealed only through noisy per-step signals. The authors introduce SPOT (Stochastic Pessimistic-Optimistic Thresholding), a novel primal-dual algorithm that learns threshold values from environmental interactions while balancing safety and performance. By treating thresholds as latent stochastic processes and using a Growing-Window estimator, SPOT achieves sublinear regret and constraint violation bounds of Õ(√T) over T episodes. The algorithm provides a principled way to trade off strict safety enforcement against exploratory reward maximization through pessimistic and optimistic thresholding modes.

## Method Summary
The method uses a Growing-Window estimator to learn unknown safety thresholds from noisy per-step signals, maintaining a window of recent episodes that grows linearly with time. This estimator provides confidence bounds that are then integrated into a primal-dual optimization framework. The primal update uses Mirror Ascent to optimize the policy, while the dual update handles constraint enforcement via projected subgradient descent. Two threshold variants—pessimistic (conservative upper bounds) and optimistic (lower bounds)—allow for trading safety strictness against exploration potential. The approach assumes a stationary environment with bounded threshold signals and requires a Slater condition for strong duality.

## Key Results
- Achieves Õ(√T) sublinear regret and constraint violation bounds in tabular CMDPs with unknown stochastic thresholds
- Introduces pessimistic-optimistic thresholding as a principled safety-performance tradeoff knob
- Provides the first theoretical guarantees for CMDPs where safety thresholds are completely unknown and revealed only through noisy signals
- Demonstrates that threshold estimation error can be bounded and integrated into primal-dual updates without sacrificing convergence rates

## Why This Works (Mechanism)

### Mechanism 1: Growing-Window Threshold Estimation
- **Claim:** The Growing-Window estimator converges to true latent threshold values with quantifiable error bounds, enabling online learning of unknown safety limits.
- **Mechanism:** At each episode, maintains a window of the W_t most recent episodes (linearly growing as ⌊γt⌋). For each step h, identifies the most-visited state-action pair (s★_h, a★_h) as statistically most informative. The empirical average of noisy threshold signals \tilde{α}_{i,h} over this window yields the estimate \hat{α}^t_{i,h}. Confidence bounds ζ^t_h are constructed via Hoeffding-style concentration, giving pessimistic (upper) and optimistic (lower) bounds.
- **Core assumption:** Stationary environment where per-step threshold signals \tilde{α}_{i,h}(s,a) are i.i.d. draws from fixed distributions L_{i,h} with bounded support [0,1]; the true episodic threshold α_i is the sum of per-step expectations.
- **Evidence anchors:**
  - [abstract] "We leverage a Growing-Window estimator sampling from interactions with the uncertain environment to estimate the thresholds"
  - [Section 3.2, Theorem 1] "Asymptotic consistency of Growing-Window estimator to thresholds... |\hat{α}^{(W_t),t}_{i,h}(s,a) - α_{i,h}| ≤ ζ^{(W_t),t}_h(s,a)" with explicit convergence rate
  - [corpus] Limited direct corroboration; neighboring papers focus on fixed thresholds or known constraint structures. The approach is novel per the authors' claim.
- **Break condition:** Non-stationary threshold distributions (α_{i,h} drifting over time); exploration insufficient to visit representative pairs frequently (N^{(W_t),t-1}_h(s,a) stays low); window growth rate γ mismatched to environment dynamics.

### Mechanism 2: Primal-Dual Optimization with Adaptive Threshold Integration
- **Claim:** A primal-dual framework jointly optimizes policy (primal) and constraint enforcement weights (dual) while incorporating estimated thresholds, achieving sublinear regret and constraint violation simultaneously.
- **Mechanism:** The constrained optimization is cast as a saddle-point problem: max_π min_λ L(π,λ) = V^π_r + λ^T(V^π_g - α). Each episode: (1) Truncated Policy Evaluation computes Q-values under current π^t and estimates r^t, g^t; (2) Primal update via Mirror Ascent on the policy using Q-values; (3) Dual update via projected subgradient: λ^{t+1} = Π_C[λ^t + (1/η_λ)(α^t_⋄ - \hat{V}^π_g)], where α^t_⋄ is the scenario-dependent (pessimistic or optimistic) threshold. Projection onto C = {λ : 0 ≤ λ ≤ ρ1} ensures bounded dual variables.
- **Core assumption:** Slater condition (Assumption 1): strictly feasible policy exists with slack ξ > 0. Assumption 2: pessimistic tightening doesn't eliminate feasibility (α_i - \underline{α}_i < ξ_i). Strong duality holds (Lemmas 1-2).
- **Evidence anchors:**
  - [abstract] "SPOT, a novel model-based primal-dual algorithm... achieves sublinear regret and constraint violation"
  - [Section 3.4, Algorithm 2] Complete pseudocode showing primal-dual update structure
  - [corpus] Ghosh et al. (2024) achieve similar Õ(√T) bounds in tabular CMDPs but assume fixed thresholds; Efroni et al. (2020) introduce online primal-dual for CMDPs. SPOT extends this to unknown thresholds.
- **Break condition:** Slater gap ρ too small (dual variables blow up); infeasibility from overly pessimistic thresholds violating Assumption 2; dual learning rate η_λ poorly tuned relative to threshold estimation noise.

### Mechanism 3: Pessimistic-Optimistic Thresholding for Safety-Performance Tradeoff
- **Claim:** Selecting between pessimistic (α^t = \hat{α} + ζ) and optimistic (α^t = \hat{α} - ζ) threshold bounds provides a principled knob trading strict safety enforcement against exploratory reward maximization.
- **Mechanism:** Pessimistic thresholds construct conservative upper bounds: α^t_i,h = \hat{α}^{t-1}_{i,h} + ζ^{t-1}_h. This shrinks the feasible set (F^{pes} ⊆ F_q), ensuring any feasible policy likely satisfies true constraints. Optimistic thresholds do the opposite (α^t_i,h = \hat{α}^{t-1}_{i,h} - ζ^{t-1}_h), expanding F^{opt} ⊇ F_q to encourage exploration. Theoretically: pessimistic yields lower constraint violation but higher reward regret; optimistic trades more violations for potentially higher reward.
- **Core assumption:** Confidence bounds ζ^t_h correctly calibrate uncertainty (contain true threshold with probability ≥ 1-δ); user can select setting based on risk tolerance.
- **Evidence anchors:**
  - [Section 3.2, Eq. 2] "By construction, with probability 1-δ, any possible estimator \hat{α} satisfies: \underline{α} ≤ \hat{α} ≤ \bar{α}"
  - [Section 4.3] "Theoretical analysis presents a safety-performance trade-off... pessimistic (strict) thresholds yield lower violations but higher reward regret"
  - [corpus] No direct precedent for stochastic threshold learning; DOPE (Bura et al., 2022) uses pessimism but with predetermined thresholds.
- **Break condition:** Confidence parameter δ poorly calibrated (over/under-confident bounds); mixed risk requirements within same problem; extreme thresholds making F^{pes} empty or F^{opt} trivially satisfied.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs) and Lagrangian Duality**
  - **Why needed here:** SPOT reformulates the safe RL problem as a saddle-point optimization over occupancy measures. Understanding strong duality (why min_λ max_π L = max_π min_λ L) is essential to see why primal-dual updates converge.
  - **Quick check question:** Given a CMDP with two constraints, can you write down the Lagrangian and explain why strong duality requires the Slater condition?

- **Concept: Concentration Inequalities (Hoeffding, Union Bounds)**
  - **Why needed here:** The Growing-Window estimator's guarantees rest on bounding |\hat{α} - α| via concentration. Theorem 1's ζ^{(W_t),t}_h(s,a) term derives from Hoeffding plus union bounds over all (s,a,h,i,t) combinations.
  - **Quick check question:** If threshold signals are bounded in [0,1] and you have N observations, what's the 1-δ confidence interval for the mean? How does the union bound change this for S×A×H×m×T simultaneous estimates?

- **Concept: Online Mirror Descent and Primal-Dual Regret Analysis**
  - **Why needed here:** Policy updates use Mirror Ascent (entropy regularizer); dual updates use projected subgradient. The Õ(√T) regret bounds emerge from standard OMD analysis combined with estimation error terms.
  - **Quick check question:** Why does Mirror Descent with entropy regularizer naturally produce softmax policies? What role does the dual learning rate η_λ play in balancing regret vs. constraint violation?

- **Concept: Occupancy Measures and Feasible Sets**
  - **Why needed here:** SPOT's analysis operates in occupancy measure space q ∈ ∆(M). Understanding how policies map to occupancy measures, and why F^{pes} ⊆ F_q ⊆ F^{opt}, clarifies the geometric structure of the safety-performance tradeoff.
  - **Quick check question:** For a 3-state, 2-action MDP with horizon H=4, what's the dimension of the occupancy measure vector q? Why is the feasible set F_q a convex polytope?

## Architecture Onboarding

- **Component map:**
  1. **Environment Interaction (Algorithm 1):** Executes π^t, collects (s_h, a_h, \tilde{r}_h, \tilde{g}_{i,h}, \tilde{α}_{i,h}, s_{h+1})
  2. **Empirical Model Updater:** Maintains running averages \hat{r}^t, \hat{g}^t, \hat{p}^t, visitation counts N^t_h(s,a)
  3. **Growing-Window Threshold Estimator:** For each h, identifies (s★_h, a★_h), computes \hat{α}^t_{i,h} and confidence ζ^t_h, outputs α^t_⋄ (pessimistic or optimistic)
  4. **Optimistic Bonus Builder:** Computes ϕ^t_h = ϕ^{r,t}_h + ϕ^{p,t}_h for reward and transition uncertainty
  5. **Truncated Policy Evaluation (Algorithm 3):** Backward recursion computing \hat{Q}^t_h(s,a;r^t,p^t), \hat{Q}^t_h(s,a;g^t_i,p^t), \hat{V}^{π^t}_g
  6. **Primal Updater:** Mirror Ascent step: π^{t+1}_h(a|s) ∝ π^t_h(a|s) exp(η_t \hat{Q}^t_h(s,a))
  7. **Dual Updater:** Projected subgradient: λ^{t+1} = Π_C[λ^t + (1/η_λ)(α^t_⋄ - \hat{V}^{π^t}_g)]
  8. **Episode Counter & Loop Controller:** Increments t, checks T, manages window W_t = ⌊γt⌋

- **Critical path:**
  Threshold estimation accuracy → feasible dual update → stable primal-dual convergence. If ζ^t_h is too large (underexplored pairs), thresholds become overly conservative/pessimistic, shrinking F^{pes} and potentially causing infeasibility or poor reward performance. If η_λ is too large, dual variables oscillate; too small, convergence stalls.

- **Design tradeoffs:**
  - **Window parameter γ:** Larger γ (slower window growth) improves threshold estimate quality but reduces adaptivity; γ=1 recovers full Monte Carlo. Paper suggests γ ∈ (0,1] but doesn't tune it.
  - **Pessimistic vs. optimistic:** Domain-specific. Safety-critical (autonomous vehicles, medical dosing) → pessimistic. Exploration-heavy (simulation, data collection) → optimistic or blended (ξ-interpolation in Section 4.3).
  - **Confidence δ:** Stricter δ (smaller) widens confidence intervals ζ, making pessimistic thresholds more conservative. Standard choice δ = 0.05 or 0.01.
  - **Slater gap ρ:** Must be estimated or known a priori for dual projection set C. If ρ underestimated, dual variables may clip; overestimated doesn't harm but loosens bounds.

- **Failure signatures:**
  - **Constraint violation growing linearly (not √T):** Dual learning rate η_λ too small; Slater gap ρ misestimated; optimistic mode with too-aggressive exploration
  - **Threshold estimates not converging:** Window γ too small; non-stationary environment; insufficient visitation to representative pairs (check N^{(W_t),t}_h(s★_h,a★_h))
  - **Infeasibility errors:** Pessimistic thresholds too tight (α^t ≫ true α), violating Assumption 2; Slater condition not satisfied in underlying problem
  - **Dual variables hitting boundary (λ_i = ρ):** Constraints too tight; increase ρ or relax thresholds

- **First 3 experiments:**
  1. **Sanity check on synthetic tabular CMDP with known thresholds:** Construct a small CMDP (S=5, A=3, H=10, m=2) where you set ground-truth thresholds α_i. Run SPOT-pessimistic and verify: (a) threshold estimates converge to true values, (b) constraint violation stays near zero, (c) reward regret scales as √T. Plot \hat{α}^t vs. α over episodes.
  2. **Pessimistic vs. optimistic comparison:** On the same CMDP, run both modes with identical seeds. Measure: cumulative reward, constraint violation, and threshold estimation error. Confirm pessimistic has lower violation but higher regret (Theorem 2 vs. Theorem 3). Test the ξ-interpolation for intermediate tradeoffs.
  3. **Ablation on Growing-Window parameter γ:** Fix δ, η_λ, ρ. Test γ ∈ {0.1, 0.3, 0.5, 0.7, 1.0}. Plot threshold convergence rate and final regret/violation. Hypothesis: larger γ (slower adaptation) improves final performance but may slow initial learning if environment has slight non-stationarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive mechanism be developed to dynamically tune the interpolation parameter ξ between pessimistic and optimistic thresholds during the learning process?
- Basis in paper: [explicit] Section 4.3 discusses introducing a hyperparameter ξ to blend thresholds (α^ξ = ξ\underline{α} + (1-ξ)\overline{α}) but limits current theoretical analysis to the boundary cases (ξ=0, 1).
- Why unresolved: While the authors state extending the analysis to intermediate ξ is straightforward, they do not provide a method for adaptively selecting ξ to optimize the safety-performance trade-off online.
- What evidence would resolve it: An algorithm that dynamically adjusts ξ based on real-time violation signals while maintaining sublinear regret.

### Open Question 2
- Question: How can the requirement of strict feasibility under pessimistic thresholds (Assumption 2) be relaxed to handle scenarios where conservative estimates render the safe set empty?
- Basis in paper: [inferred] Assumption 2 requires the Slater point to remain feasible under pessimistic tightening (\underline{α}_i - α_i < ξ_i), which the authors acknowledge is a "strong" condition.
- Why unresolved: In highly uncertain or noisy environments, the pessimistic estimate may exceed the actual slack, eliminating all feasible policies and causing the primal-dual optimization to fail.
- What evidence would resolve it: A modified algorithm that guarantees convergence or safe recovery even when the initial pessimistic feasibility set is empty.

### Open Question 3
- Question: Can the SPOT algorithm be extended to non-stationary environments where the latent threshold distribution drifts over time?
- Basis in paper: [inferred] The problem formulation in Section 3.1 explicitly assumes a "stationary environment" where the threshold expectation α_i is fixed, unlike the non-stationary MDPs discussed in related works.
- Why unresolved: The Growing-Window estimator is designed to converge to a fixed expected value; it is unclear if it can track a moving threshold target without accumulating linear regret.
- What evidence would resolve it: Theoretical bounds on dynamic regret in environments where the threshold α_t varies with bounded total variation.

## Limitations
- **Stationarity assumption:** The theoretical guarantees assume threshold distributions are fixed over time, which may not hold in dynamic real-world environments.
- **Slater condition strictness:** The requirement for a strictly feasible policy with positive slack may be violated when thresholds are very close to constraint boundaries.
- **Hyperparameter sensitivity:** Performance depends heavily on proper calibration of δ, γ, and the dual learning rate η_λ, with no empirical guidance provided.

## Confidence
- **High confidence:** The sublinear regret and constraint violation bounds (Õ(√T)) are rigorously derived under stated assumptions, following established primal-dual analysis in CMDPs. The mechanism for combining threshold estimation with primal-dual updates is mathematically sound.
- **Medium confidence:** The pessimistic-optimistic thresholding framework is novel and theoretically justified, but the practical benefits of switching between modes depend heavily on domain-specific threshold distributions and user risk tolerance. The analysis provides a safety-performance tradeoff but doesn't specify how to optimally select the mode.
- **Low confidence:** The numerical constants in the theoretical bounds (e.g., exact scaling of ζ^t_h, optimal η_λ) are not provided, and the paper doesn't report empirical results to validate the algorithm's performance on specific benchmark environments.

## Next Checks
1. **Numerical constant sensitivity:** Implement SPOT on a simple 5-state, 3-action CMDP with synthetic threshold noise. Systematically vary δ (0.01, 0.05, 0.1), γ (0.3, 0.5, 0.7), and the bonus scaling factor in φ. Plot final reward regret and constraint violation vs. these hyperparameters to identify robust settings.
2. **Non-stationary threshold test:** Modify the synthetic CMDP to have threshold distributions that drift linearly over time (α_{i,h}(t) = α_{i,h}(0) + 0.01t). Run SPOT with growing window and monitor whether threshold estimates track the drift or lag significantly. Measure degradation in regret bounds.
3. **Slater gap robustness:** Construct a CMDP where the Slater gap ξ is small (e.g., ξ = 0.01) and test whether SPOT-pessimistic becomes infeasible (all policies violate constraints) while SPOT-optimistic maintains feasible policies but with higher violations. This validates the safety-performance tradeoff prediction.