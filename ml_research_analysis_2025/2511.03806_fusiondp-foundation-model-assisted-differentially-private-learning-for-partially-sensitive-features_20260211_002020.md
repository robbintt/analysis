---
ver: rpa2
title: 'FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially
  Sensitive Features'
arxiv_id: '2511.03806'
source_url: https://arxiv.org/abs/2511.03806
tags:
- features
- data
- privacy
- sensitive
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FUSIONDP, a two-step framework that leverages\
  \ foundation models to impute sensitive features and introduces a new training algorithm\
  \ with alignment between private and imputed data, improving the privacy-utility\
  \ trade-off under feature-level differential privacy. The framework is evaluated\
  \ on two modalities\u2014tabular data for sepsis prediction and clinical notes for\
  \ ICD-9 classification\u2014showing significant performance gains over privacy-preserving\
  \ baselines."
---

# FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features

## Quick Facts
- **arXiv ID**: 2511.03806
- **Source URL**: https://arxiv.org/abs/2511.03806
- **Reference count**: 20
- **Primary result**: Foundation model-driven imputation with calibrated loss improves privacy-utility trade-off for feature-level differential privacy

## Executive Summary
FusionDP addresses the challenge of training machine learning models under feature-level differential privacy (Feature-DP), where only a subset of features (e.g., demographics) need privacy protection while others remain public. The framework leverages foundation models to impute sensitive features without accessing true private values, then trains using a dual-batch gradient descent that combines clean updates on hybrid data with DP-protected updates on original data. Evaluation on sepsis prediction and clinical note classification demonstrates significant performance improvements over standard DP baselines, particularly in imbalanced settings.

## Method Summary
FusionDP operates in two steps: first, a frozen foundation model (TabPFN for tabular, GPT-4o-mini for text) imputes sensitive features using only public features as input, creating hybrid samples that preserve task-relevant correlations without accessing private values. Second, a modified DP-SGD algorithm uses two independent batches—Poisson-sampled for private data (original features) and uniform-sampled for public data (hybrid samples). The private loss includes a calibrated term (original minus hybrid prediction) plus a representation consistency penalty, with gradients combined at a tunable ratio. This design reduces gradient sensitivity, allowing less noise injection while maintaining rigorous privacy guarantees.

## Key Results
- Achieves 2.1% higher AUPRC on sepsis prediction compared to privacy-preserving baselines
- Outperforms competing methods by 3.5% F1 on clinical note ICD-9 classification
- Consistently maintains high precision, recall, and AUC across both modalities and privacy budgets
- Demonstrates particular effectiveness in imbalanced settings (15% positive class)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing sensitive features with foundation model imputations preserves task-relevant correlations better than masking, without accessing true private values
- Core assumption: The FM has captured sufficient priors from public data to estimate sensitive features accurately enough that the resulting gradient signal is more informative than random noise or zero-masking
- Evidence anchors: Abstract states FMs provide "high-quality estimates... without accessing the true values during model training"; Section 4.1 confirms imputations are generated without observing $x_{priv}$
- Break condition: If the correlation between public and sensitive features is weak or non-existent in the public domain, the FM will output generic priors, reducing the hybrid data to noise

### Mechanism 2
- Claim: A calibrated loss isolates the influence of sensitive features, reducing gradient sensitivity and thus the noise required for differential privacy
- Core assumption: The gradient difference primarily captures the specific contribution of the sensitive features rather than noise or model instability
- Evidence anchors: Section 4.2 notes "the gradient on the calibrated loss has a lower norm... the noise magnitude can be reduced due to a lower sensitivity"
- Break condition: If the imputation is highly inaccurate, the hybrid loss will diverge significantly from the original loss, potentially increasing the gradient norm rather than reducing it

### Mechanism 3
- Claim: A representation consistency loss aligns the hidden states of original and hybrid inputs, preventing the model from overfitting to imputation artifacts
- Core assumption: The semantic content of the input is largely determined by the non-sensitive features, or the imputation is accurate enough that the "true" and "hybrid" inputs share the same label
- Evidence anchors: Section 4.2 explains this "encourages the model to treat original and hybrid inputs similarly, mitigating bias from imperfect imputation"
- Break condition: If the weight β is set too high, the model may prioritize alignment over classification accuracy, degrading utility

## Foundational Learning

- Concept: **Feature Differential Privacy (Feature-DP)**
  - Why needed here: Unlike standard sample-level DP, which protects the entire data point, Feature-DP protects only a subset of features while treating the rest as public
  - Quick check question: If a dataset has features [A, B, C] where only A is private, does Feature-DP guarantee that the output distribution is invariant to changes in B? (Answer: No, only changes in A)

- Concept: **Foundation Models as Priors**
  - Why needed here: This paper relies on the ability of large pre-trained models to infer missing/sensitive features based on context, serving as a substitute for real sensitive data
  - Quick check question: Why is it safe to use an FM for imputation in a DP setting? (Answer: The FM is frozen and pre-trained on public data; it never sees the true sensitive values from the private dataset during the imputation step)

- Concept: **Gradient Sensitivity and Noise Scaling**
  - Why needed here: In DP-SGD, noise is scaled to the "sensitivity" (maximum change) of the gradient. FusionDP relies on reducing this sensitivity via calibration to inject less noise
  - Quick check question: If the gradient norm of the calibrated loss decreases by a factor of 2, how does this affect the signal-to-noise ratio of the gradient update? (Answer: It improves it, as the standard deviation of the Gaussian noise scales linearly with the sensitivity bound)

## Architecture Onboarding

- Component map:
  Data Preprocessor -> Imputer (FM) -> Hybrid Constructor -> Backbone Model -> Dual-Branch Optimizer

- Critical path:
  1. Identify and split sensitive vs. non-sensitive features
  2. Run $x_{pub}$ through the Imputer to get $\hat{x}_{priv}$ (before or during batch generation)
  3. Sample two independent batches (Poisson for private, Uniform for public/hybrid)
  4. Compute Public Gradient (clean) and Private Gradient (clipped + noised)
  5. Fuse gradients ($g_{pub} + \alpha \tilde{g}_{priv}$) and update weights

- Design tradeoffs:
  - **Imputation Cost vs. Quality**: Using a larger FM (e.g., GPT-4o) improves priors but significantly increases preprocessing time (paper notes 19 hours for clinical notes)
  - **Clipping Norm ($C$) vs. Consistency Weight ($\beta$)**: Increasing $\beta$ stabilizes representations but increases gradient sensitivity, requiring a larger clipping norm $C$ and potentially more noise
  - **Alpha ($\alpha$)**: Controls the influence of the private vs. public gradient. High privacy budgets (high $\epsilon$) favor larger $\alpha$ to use more private signal; low budgets favor smaller $\alpha$ to rely on the clean public signal

- Failure signatures:
  - **Utility Collapse at Low Epsilon**: If the clipping norm $C$ is too small relative to the gradient norm, gradients become zeroed out, learning nothing
  - **Imputation Bias Domination**: If the FM imputes a constant value for all sensitive features, the "Calibrated Loss" might drive the model to ignore that feature entirely
  - **Representation Divergence**: If $\beta$ is too low, the public branch (trained on hybrid data) and private branch (trained on real data) may pull the model in opposite directions, causing instability

- First 3 experiments:
  1. **Imputation Fidelity Check**: Before training the main model, evaluate the FM's ability to predict the sensitive features on a held-out validation set
  2. **Baseline Ablation ($\beta=0$)**: Run the "Calibrated Fusion" variant (no consistency loss) vs. the full FusionDP to quantify the value of the alignment term
  3. **Hyperparameter Sensitivity Grid**: Sweep $C$ (clipping norm) and $\alpha$ (mixing weight) at a fixed $\epsilon$

## Open Questions the Paper Calls Out

- **Label Privacy Extension**: The framework currently assumes labels are public; future work includes extending to protect sensitive labels alongside features
- **Multi-Modal Generalization**: Adapting FUSIONDP to settings involving complex data pairs like medical imaging and text reports remains unexplored
- **Imputation Quality Robustness**: The framework's performance when foundation models produce low-quality or biased imputations has not been quantified

## Limitations

- **Imputation quality dependency**: The framework's effectiveness critically depends on the foundation model's ability to predict sensitive features from public ones
- **Computational overhead**: The imputation step, particularly with GPT-4o-mini for clinical notes (19 hours reported), adds significant preprocessing cost
- **Privacy accounting granularity**: The paper does not specify exact noise multipliers for each epsilon value, making exact privacy budget verification difficult

## Confidence

- **High confidence**: The dual-batch gradient mechanism and calibrated loss formulation are mathematically sound and align with DP theory
- **Medium confidence**: The representation consistency loss improves alignment between original and hybrid data, but optimal weighting may be dataset-specific
- **Low confidence**: The claim that foundation models "preserve task-relevant correlations" assumes sufficient overlap between public and private data distributions

## Next Checks

1. **Imputation fidelity analysis**: Systematically evaluate the foundation model's prediction accuracy for sensitive features on a validation set before applying FUSIONDP

2. **Privacy accounting verification**: Implement exact noise multipliers and sampling parameters, then verify achieved epsilon values using the Rényi DP accountant

3. **Cross-dataset ablation study**: Apply FUSIONDP to a dataset with known weak correlation between public and sensitive features to identify boundary conditions where the method provides no advantage