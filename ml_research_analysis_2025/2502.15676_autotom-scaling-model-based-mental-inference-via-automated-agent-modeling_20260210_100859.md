---
ver: rpa2
title: 'AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling'
arxiv_id: '2502.15676'
source_url: https://arxiv.org/abs/2502.15676
tags:
- agent
- inference
- belief
- goal
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoToM is a fully automated agent modeling method for scalable
  and robust mental inference. It constructs an initial agent model and iteratively
  refines it by introducing additional mental variables and incorporating more timesteps,
  guided by inference uncertainty.
---

# AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling

## Quick Facts
- arXiv ID: 2502.15676
- Source URL: https://arxiv.org/abs/2502.15676
- Reference count: 40
- AutoToM achieves 82.43% average accuracy across five Theory of Mind benchmarks

## Executive Summary
AutoToM introduces a fully automated agent modeling method for scalable and robust mental inference, addressing the challenge of manually specifying agent models in Bayesian inverse planning. The system constructs an initial agent model and iteratively refines it by introducing additional mental variables and incorporating more timesteps, guided by inference uncertainty. Using an LLM backend, AutoToM performs automated Bayesian inverse planning without manual model specifications, achieving state-of-the-art performance on diverse Theory of Mind benchmarks while producing human-like confidence estimates and enabling online mental inference for embodied decision-making tasks.

## Method Summary
AutoToM employs an uncertainty-guided expansion mechanism that iteratively refines agent models by adding mental variables and timesteps based on inference uncertainty signals. The method uses an LLM to generate planning costs and goal priors, then applies Bayesian inverse planning to infer agent intentions and beliefs. The iterative refinement process continues until uncertainty thresholds are met or computational limits are reached. This approach enables fully automated mental inference without requiring manual model specifications, addressing the scalability limitations of traditional model-based ToM approaches.

## Key Results
- Achieves 82.43% average accuracy across five diverse Theory of Mind benchmarks
- Outperforms state-of-the-art LLMs and large reasoning models on mental inference tasks
- Enables 27.7% speedup in assistance tasks through online mental inference
- Produces human-like confidence estimates in cognitive studies

## Why This Works (Mechanism)
AutoToM's effectiveness stems from its uncertainty-guided iterative refinement process that automatically identifies and incorporates missing mental variables and temporal information. By using inference uncertainty as a signal for model expansion, the system can dynamically adapt its agent models to capture the complexity required for accurate mental inference. The LLM backend provides scalable planning cost generation and goal priors, while the Bayesian inverse planning framework enables principled inference of agent intentions and beliefs from observed behavior.

## Foundational Learning
- **Bayesian Inverse Planning**: Why needed - to infer agent intentions from observed behavior; Quick check - verify posterior distributions over agent models match ground truth
- **Uncertainty Quantification**: Why needed - to guide iterative model refinement; Quick check - measure reduction in predictive entropy across expansion steps
- **LLM-augmented Planning**: Why needed - to generate scalable planning costs and goal priors; Quick check - compare planning costs with and without LLM assistance
- **Iterative Model Expansion**: Why needed - to capture increasing complexity of mental states; Quick check - track accuracy improvements per expansion iteration
- **Theory of Mind Benchmarks**: Why needed - to evaluate mental inference capabilities; Quick check - verify benchmark diversity covers different mental state complexities
- **Embodied Decision Making**: Why needed - to test online mental inference in dynamic environments; Quick check - measure assistance task completion times with and without AutoToM

## Architecture Onboarding
**Component Map**: LLM Backend -> Uncertainty Estimator -> Model Expander -> Bayesian Inverse Planner -> Agent Model
**Critical Path**: Observation -> Uncertainty Estimation -> Model Expansion Decision -> Planning Cost Generation -> Bayesian Inference -> Prediction
**Design Tradeoffs**: Computational cost vs. model accuracy (iterative refinement increases both), LLM dependency vs. automation (reduces manual effort but introduces LLM variability), uncertainty-guided expansion vs. fixed models (adaptive but potentially slower)
**Failure Signatures**: Overconfidence in uncertainty estimates leading to premature termination, LLM generation failures causing planning cost errors, excessive expansion iterations causing computational bottlenecks
**First Experiments**:
1. Benchmark accuracy on single-step mental inference tasks without expansion
2. Ablation study comparing uncertainty-guided vs. fixed model approaches
3. Runtime analysis of iterative refinement process across different task complexities

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of the uncertainty-guided expansion mechanism beyond evaluated benchmark domains, the potential confounding effects of LLM-generated planning costs, and the computational overhead of iterative refinement. Additionally, the comparison with large reasoning models may underestimate their capabilities due to API limitations, and the claim of "human-like" confidence estimates requires direct human comparison studies for validation.

## Limitations
- Limited validation on out-of-distribution scenarios and real-world decision-making tasks
- Dependence on LLM-generated planning costs introduces potential confounding factors
- Comparison with LRMs uses API versions rather than optimal configurations
- Computational overhead of iterative refinement not adequately addressed

## Confidence
- High confidence in technical implementation and benchmark results within tested domains
- Medium confidence in claimed superiority over state-of-the-art LLMs and LRMs
- Low confidence in "human-like" confidence estimates without direct human comparison

## Next Checks
1. Evaluate AutoToM on out-of-distribution scenarios and real-world decision-making tasks that differ substantially from training benchmarks to test true generalization capabilities
2. Conduct ablation studies comparing AutoToM with and without iterative refinement while holding LLM capabilities constant, to isolate contribution of automated modeling approach
3. Measure and report actual computational cost and latency of uncertainty-guided expansion process across different model sizes and task complexities to validate claimed scalability benefits