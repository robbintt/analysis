---
ver: rpa2
title: 'ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data'
arxiv_id: '2509.06675'
source_url: https://arxiv.org/abs/2509.06675
tags:
- speech
- alignment
- word
- corpus
- czech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ParCzech4Speech 1.0 is a new Czech speech corpus derived from
  parliamentary recordings, containing up to 2,695 hours of aligned speech from 587
  speakers. The dataset addresses the lack of large, flexible, and commercially accessible
  Czech speech resources by providing three variants: sentence-segmented for ASR/TTS
  tasks, unsegmented for continuous speech scenarios, and raw alignment for custom
  refinement.'
---

# ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data

## Quick Facts
- arXiv ID: 2509.06675
- Source URL: https://arxiv.org/abs/2509.06675
- Reference count: 30
- Primary result: New Czech speech corpus with up to 2,695 hours of aligned parliamentary speech from 587 speakers

## Executive Summary
ParCzech4Speech 1.0 is a new Czech speech corpus derived from parliamentary recordings, containing up to 2,695 hours of aligned speech from 587 speakers. The dataset addresses the lack of large, flexible, and commercially accessible Czech speech resources by providing three variants: sentence-segmented for ASR/TTS tasks, unsegmented for continuous speech scenarios, and raw alignment for custom refinement. Using WhisperX and Wav2Vec 2.0 for automated recognition and alignment, the pipeline extracts more data with higher reliability than previous versions. The corpus includes rich metadata and is released under a CC-BY license, making it suitable for both academic and commercial applications.

## Method Summary
The corpus is constructed from ParCzech 4.0 transcripts (TEI format) and AudioPSP 24.01 audio recordings. The pipeline uses UDPipe for tokenization, WhisperX with Wav2Vec 2.0 for speech recognition and forced alignment, and word-level Levenshtein alignment between recognized and official transcripts. Conservative filtering based on four quality metrics (segment edit distance ≤9, max aligned edit distance ≤5, character duration 0.06-0.14s, speaker-text counter) produces three output variants: sentence-segmented (1,131h), unsegmented (2,631h), and raw-alignment (TSV only). The method addresses challenges with stenographic transcript normalization and timestamp artifacts for numbers and special characters.

## Key Results
- Contains up to 2,695 hours of aligned speech from 587 speakers
- 27.8M aligned words with 25.2M achieving perfect alignment (17% reduction from 30.5M source words)
- Three output variants: sentence-segmented (1,131h), unsegmented (2,631h), and raw-alignment
- Released under CC-BY license with rich metadata including speaker information and content type

## Why This Works (Mechanism)

### Mechanism 1
Two-step recognition-to-alignment improves audio-text matching when official transcripts are normalized. Rather than forcing direct alignment between raw audio and official stenographic transcripts (which omit disfluencies, self-corrections, and normalize speech), the pipeline first recognizes speech with WhisperX to capture what was actually spoken, then aligns recognized text against official text using word-level Levenshtein distance matching.

### Mechanism 2
Conservative gap-word filtering at segment boundaries increases alignment reliability. Segments are included only if both starting and ending words have precise timestamps (not aligned to gap words); internal gap words cause segment discard because "neither the original nor recognized transcript can be fully trusted in these cases due to potential ASR errors or manual corrections."

### Mechanism 3
Multiple quality metrics with empirical thresholds identify alignment artifacts better than single metrics. Four automatic metrics (segment edit distance ≤9, max aligned edit distance ≤5, character duration 0.06-0.14s, speaker-text counter for analysis) capture different failure modes; thresholds were "determined through manual verification of random samples."

## Foundational Learning

- Concept: **Forced Alignment with CTC-based models**
  - Why needed here: Pipeline relies on Wav2Vec 2.0's CTC segmentation for word-level timestamps; understanding CTC alignment explains why numbers and special characters lack timestamps
  - Quick check question: Can you explain why CTC-based models struggle to provide timestamps for digits (e.g., "1,000") versus written forms ("tisíc")?

- Concept: **Levenshtein Distance for Sequence Alignment**
  - Why needed here: Core matching mechanism between recognized and official transcripts; determines gap-word insertion when distance exceeds threshold
  - Quick check question: If the official word is "parlament" (9 chars) and recognized word is "parlamet" (8 chars), would this match or trigger a gap word under the paper's criterion?

- Concept: **Voice Activity Detection (VAD) Segmentation**
  - Why needed here: WhisperX uses VAD to chunk long recordings before processing; understanding this helps diagnose why some boundary timestamps may be imprecise
  - Quick check question: Why might VAD-based chunking introduce artifacts at segment boundaries compared to sentence-aware segmentation?

## Architecture Onboarding

- Component map: Raw Audio (AudioPSP) + Official Transcripts (ParCzech 4.0 TEI) → UDPipe Tokenization → WhisperX Recognition → Wav2Vec 2.0 Forced Alignment → Word-level Levenshtein Alignment → Quality Filtering → Three Output Variants

- Critical path: Recognition quality → Timestamp accuracy → Alignment matching → Segment filtering. Errors propagate forward; the re-recognition step with Wav2Vec 2.0 (section 3.1) attempts to catch number-related artifacts before final output.

- Design tradeoffs:
  - Sentence-segmented: Cleaner boundaries, smaller dataset (1,131h), stricter filtering
  - Unsegmented: 2.3× more data (2,631h), preserves natural speech flow, longer segments (5-30s)
  - Raw-alignment: Maximum flexibility, requires custom processing pipeline

- Failure signatures:
  - Numbers/digits without timestamps: Wav2Vec 2.0 limitation → look for segments with "numeric token count > 0" in metadata
  - Shifted timestamps near numbers: Whisper produces correct text but Wav2Vec can't timestamp → re-recognition filter catches this
  - Gap words at boundaries: Levenshtein distance exceeded → segments excluded from sentence-segmented variant
  - Unusual character duration: <0.06s or >0.14s per character → indicates timestamp issues or long pauses

- First 3 experiments:
  1. **Baseline quality assessment**: Load sentence-segmented train split, filter by each quality metric independently to understand which filters exclude the most data; this reveals dominant failure modes in your use case
  2. **Cross-variant comparison**: Select 100 random segments appearing in both sentence-segmented and unsegmented variants; compare boundary precision by manually inspecting audio at segment edges (5-10 samples sufficient for pattern detection)
  3. **Threshold sensitivity**: Relax character duration thresholds (e.g., 0.05-0.16s) on a held-out subset and measure ASR WER impact; determines if conservative thresholds are appropriate for your task or overly restrictive

## Open Questions the Paper Calls Out

### Open Question 1
How effectively can state-of-the-art ASR models be fine-tuned on ParCzech4Speech to improve performance on Czech speech recognition tasks? The conclusion states future work could explore "fine-tuning ASR models on this dataset," but the paper presents corpus construction rather than downstream benchmarks.

### Open Question 2
How can the alignment pipeline be improved to accurately process numbers and special characters without inducing timestamp artifacts? The authors identify Wav2Vec 2.0's failure to timestamp non-normalized text as a limitation that "can shift surrounding word timestamps," with the current heuristic replacement strategy being "not perfect."

### Open Question 3
Are the specific quality thresholds determined via manual verification (e.g., segment edit distance ≤ 9) optimal for downstream tasks compared to automated selection? Manual threshold selection on random samples may not capture the global statistical optimum for specific tasks like TTS vs. ASR.

## Limitations
- Automated alignment pipeline introduces uncertainty about transcription accuracy for parliamentary Czech with specialized vocabulary and formal register
- Conservative filtering reduces 30.5M source words to 25.2M perfectly aligned words (17% reduction), potentially limiting utility for some tasks
- Empirical quality thresholds were determined through manual verification of random samples without systematic evaluation of generalization across different conditions

## Confidence
- **High confidence**: The corpus provides three distinct variants with clear specifications and CC-BY licensing, making it accessible for commercial and academic use
- **Medium confidence**: The automated alignment pipeline using WhisperX and Wav2Vec 2.0 is technically sound, but actual alignment quality and downstream performance remains unvalidated
- **Low confidence**: Claims of "higher reliability" compared to previous versions lack quantitative comparative metrics or systematic evaluation

## Next Checks
1. **Downstream Task Evaluation**: Train baseline ASR models using different corpus variants and measure Word Error Rate (WER) on held-out parliamentary speech to validate whether conservative filtering improves model performance compared to less restrictive approaches

2. **Speaker and Content Analysis**: Analyze the 587 speakers by political party, gender, and speaking frequency to assess representation bias; quantify distribution of formal parliamentary speech versus question-answer sessions, committee discussions, and ceremonial addresses

3. **Comparative Quality Assessment**: Select 100 random segments and manually verify alignment quality against ground truth timestamps; compare alignment accuracy metrics with those reported for ParlaSpeech-CZ and SloPalSpeech to establish relative corpus quality and identify domain-specific challenges