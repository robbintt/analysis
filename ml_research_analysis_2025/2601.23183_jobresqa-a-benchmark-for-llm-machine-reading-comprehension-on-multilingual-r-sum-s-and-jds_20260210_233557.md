---
ver: rpa2
title: "JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual\
  \ R\xE9sum\xE9s and JDs"
arxiv_id: '2601.23183'
source_url: https://arxiv.org/abs/2601.23183
tags:
- https
- translation
- dataset
- jobresqa
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "JobResQA introduces a multilingual QA benchmark for evaluating\
  \ LLM machine reading comprehension on HR-specific tasks involving r\xE9sum\xE9\
  s and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic\
  \ r\xE9sum\xE9-JD pairs in five languages (English, Spanish, Italian, German, and\
  \ Chinese), with questions spanning three complexity levels from basic factual extraction\
  \ to complex cross-document reasoning."
---

# JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs

## Quick Facts
- arXiv ID: 2601.23183
- Source URL: https://arxiv.org/abs/2601.23183
- Reference count: 25
- Introduces multilingual QA benchmark for HR-specific LLM evaluation across 5 languages

## Executive Summary
JobResQA presents a novel benchmark for evaluating large language models on machine reading comprehension tasks involving résumés and job descriptions. The dataset contains 581 QA pairs across 105 synthetic résumé-JD pairs in five languages, with questions spanning three complexity levels from basic extraction to complex cross-document reasoning. The benchmark employs a privacy-preserving data synthesis pipeline and a human-in-the-loop translation methodology to ensure both quality and confidentiality. Baseline evaluations reveal significant performance gaps across languages and model sizes, highlighting critical gaps in multilingual MRC capabilities for HR applications.

## Method Summary
The JobResQA benchmark was created through a synthetic data pipeline starting from real-world HR documents that underwent de-identification and placeholder substitution. GPT-4.1 was used to generate synthetic résumé-JD pairs while preserving format and anonymizing PII. Questions were annotated by linguists with consultation from ESCO and O*NET occupational databases, spanning three complexity levels. A human-in-the-loop translation pipeline based on TEaR methodology with MQM error annotations ensured quality across languages. Baseline evaluation used an LLM-as-judge approach with Claude Sonnet 4 and G-Eval rubrics.

## Key Results
- Performance varies significantly by language: 0.67-0.73 for English/Spanish vs 0.43-0.66 for German/Italian/Chinese
- Model size matters: 1B-parameter models score 0.15-0.35 while 70B models score 0.67-0.73 on English/Spanish
- Question complexity impacts performance: Complex cross-document reasoning questions show larger cross-lingual gaps than basic extraction tasks

## Why This Works (Mechanism)

### Mechanism 1
Synthetic résumé-JD generation via de-identification and LLM-based rewriting produces realistic, privacy-preserving data that enables controlled bias studies. Real-world résumés/JDs → entity extraction and placeholder replacement → LLM-based content transformation → manual post-editing → synthetic documents with controlled attributes. Core assumption: Placeholder substitution preserves document semantics while removing identifiable information. Evidence: [abstract] describes data synthesis pipeline; [Section 4.3] details GPT-4.1-based synthesis. Break condition: If placeholder normalization is inconsistent across languages, benchmark validity degrades.

### Mechanism 2
Human-in-the-loop TEaR pipeline with MQM error annotations improves translation quality over zero-shot LLM translation. Zero-shot translation → human MQM annotation on 10% sample → few-shot error estimation → few-shot refinement using corrections as references → selective post-editing → quality validation. Core assumption: 10% sampling rate is sufficient to capture systematic translation errors. Evidence: [abstract] mentions human-in-the-loop translation pipeline; [Section 6.3, Table 5] shows COMETKiwi delta improvements. Break condition: If domain-specific HR terminology is mistranslated, cross-lingual benchmark parallelism breaks.

### Mechanism 3
Complexity-graded questions reveal differentiated LLM capabilities on cross-document reasoning. Three-tier taxonomy—basic (single-passage extraction), intermediate (multi-section reasoning), complex (cross-document inference + external knowledge)—mapped to G-Eval scoring rubrics. Core assumption: Question complexity levels correspond to distinct cognitive demands. Evidence: [abstract] states questions span three complexity levels; [Section 5] provides explicit complexity definitions. Break condition: If complex questions require ambiguous inference, evaluation signal-to-noise ratio degrades.

## Foundational Learning

- **Machine Reading Comprehension (MRC) as QA**: Why needed here: JobResQA frames résumé-JD analysis as knowledge-intensive QA rather than classification, enabling evaluation of reasoning depth. Quick check: Can you explain why QA framing reveals more about LLM capabilities than binary matching scores?

- **MQM (Multidimensional Quality Metrics) Error Annotation**: Why needed here: Translation quality control uses MQM categories (Terminology, Accuracy, Linguistic, Style, Locale, Design, Custom including Hallucination). Quick check: What distinguishes "Accuracy" errors from "Hallucination" errors in machine translation evaluation?

- **LLM-as-Judge with G-Eval**: Why needed here: Baseline evaluation uses Claude Sonnet 4 as judge with calibrated rubrics (0.0-0.3 incorrect, 0.3-0.6 mostly correct, 0.6-0.9 minor details missing). Quick check: Why might G-Eval scores show high variance (std up to 0.38) even for large models on non-English languages?

## Architecture Onboarding

- **Component map**: Data Collection → Job Title Matching (multilingual encoder) → De-identification (entity extraction + placeholders) → Synthetic Generation (GPT-4.1) → Manual Review → QA Annotation (linguists + ESCO/O*NET consultation) → Translation Pipeline (Claude Sonnet 4 + human MQM + post-editing) → Evaluation (LLM-as-judge with G-Eval)

- **Critical path**: Start with English baseline evaluation using provided benchmark files → select model family → run zero-shot inference with provided prompt template → evaluate with G-Eval rubrics → analyze performance by complexity level → extend to cross-lingual settings

- **Design tradeoffs**: (1) Synthetic data preserves privacy but may lack authentic linguistic patterns—especially in gender-inclusive forms; (2) 10% human sampling balances cost vs. quality but may miss edge-case errors; (3) LLM-as-judge enables scalable evaluation but introduces evaluator model bias

- **Failure signatures**: Large models (>10B) drop from 0.67-0.73 (English/Spanish) to 0.43-0.66 (German/Italian/Chinese) with high variance; 1B models consistently fail (0.15-0.35); Llama 3.1 70B shows anomalous Italian drop (0.43) vs. German (0.64)

- **First 3 experiments**:
  1. Replicate baseline: Run Mistral Large and Llama 3.3 70B on English subset with provided prompt; verify G-Eval scores fall within reported ranges
  2. Complexity stratification: Isolate performance by question complexity level to identify reasoning bottlenecks (expect complex questions to show largest cross-lingual gaps)
  3. Cross-lingual stress test: Evaluate same model on mismatched language pairs (e.g., Chinese question, English résumé) to quantify cross-lingual transfer degradation

## Open Questions the Paper Calls Out

### Open Question 1
What specific factors drive the high variance (up to ±0.38) in LLM performance across individual questions within the same language and complexity level? Basis: The authors state "the reported standard deviations are non-negligible, indicating that this model's performance varies substantially across questions, and further studies are needed to identify this source of variability." Unresolved because baseline evaluation only reports aggregate scores and variance; no analysis was conducted on question-level features that might explain why some questions are systematically harder.

### Open Question 2
How does cross-lingual QA performance compare to monolingual settings when questions are posed in one language while résumés and JDs are in another? Basis: The dataset is explicitly designed as "multi-way parallel" and claimed to "enable the evaluation of models in multilingual and cross-lingual QA setting," yet all baseline experiments keep questions and documents in the same language. Unresolved because no cross-lingual experiments were conducted despite the dataset supporting this configuration.

### Open Question 3
Can the controlled demographic and professional placeholder attributes reveal systematic biases in LLM hiring recommendations, and do these biases persist across languages? Basis: The paper emphasizes that placeholders enable "systematic bias and fairness studies" and lists bias dimensions (demographic, socioeconomic, educational), but no bias evaluation experiments are presented. Unresolved because no actual manipulation of placeholder attributes or measurement of differential model behavior was conducted.

### Open Question 4
Does the synthetic, gender-inclusive translation style affect LLM comprehension compared to authentic résumé conventions in each target language? Basis: The limitations section notes that "gender-inclusive rewriting... may be less common in authentic résumés, slightly affecting the perceived naturalness" and that translation "may not fully capture the typical writing styles of real-world résumés." Unresolved because all evaluation uses synthetic, translated data; no comparison to authentic native-language résumés establishes whether the artificial style introduces comprehension artifacts.

## Limitations
- Synthetic data generation may not capture authentic linguistic patterns and industry-specific jargon of real-world HR documents
- 10% human sampling rate in translation pipeline may miss systematic errors in domain-specific terminology and gender-inclusive forms
- LLM-as-judge approach shows high variance (std up to 0.38), raising concerns about evaluator consistency across languages

## Confidence
**High confidence**: Dataset construction methodology is well-documented and reproducible; complexity-graded question taxonomy is explicit and validated; performance gap between large and 1B-parameter models is statistically significant.

**Medium confidence**: Claim that synthetic data preserves document semantics relies on indirect evidence; translation quality improvements are dataset-internal; benchmark's generalizability beyond specific résumé-JD matching scenarios is unproven.

**Low confidence**: LLM-as-judge reliability for cross-lingual evaluation is questionable given high variance and potential evaluator bias; benchmark's sensitivity to real-world HR document variability is untested.

## Next Checks
1. **Cross-linguistic semantic preservation test**: Extract key HR domain terms from synthetic documents and measure semantic drift across languages using bilingual embeddings (e.g., MUSE). Compare term stability between synthetic and real HR documents to quantify ecological validity loss.

2. **Judge reliability validation**: Replicate G-Eval scoring using two independent LLM judges (Claude Sonnet 4 and GPT-4o). Compute inter-annotator agreement (Cohen's kappa) per language and complexity level. Identify questions with >0.2 score variance as potential benchmark weaknesses.

3. **Real-world document transfer**: Collect a small sample of real résumés/JDs (with consent) and run the same QA pipeline. Measure performance degradation relative to synthetic data baselines. Analyze whether the degradation correlates with specific linguistic features (idiomatic expressions, cultural references, industry jargon).