---
ver: rpa2
title: The Role of Prosody in Spoken Question Answering
arxiv_id: '2502.05389'
source_url: https://arxiv.org/abs/2502.05389
tags:
- prosodic
- information
- speech
- lexical
- prosody
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the role of prosody in spoken question
  answering (SQA) tasks by systematically isolating prosodic and lexical information.
  Using the SLUE-SQA-5 dataset, which consists of natural speech, the authors modify
  the audio to create three conditions: natural (both lexical and prosodic), lexical
  (only lexical with flattened pitch and intensity), and prosodic (only prosodic with
  low-pass filtering).'
---

# The Role of Prosody in Spoken Question Answering

## Quick Facts
- arXiv ID: 2502.05389
- Source URL: https://arxiv.org/abs/2502.05389
- Authors: Jie Chi; Maureen de Seyssel; Natalie Schluter
- Reference count: 18
- Key outcome: Models trained on prosodic information alone can perform reasonably well by utilizing prosodic cues, but when lexical information is available, models predominantly rely on it.

## Executive Summary
This work investigates the role of prosody in spoken question answering (SQA) by systematically isolating prosodic and lexical information using the SLUE-SQA-5 dataset. The authors modify audio to create three conditions: natural (both lexical and prosodic), lexical (only lexical with flattened pitch and intensity), and prosodic (only prosodic with low-pass filtering). Using discrete spoken units from WavLM representations, they train SQA models and find that prosody alone provides meaningful cues for answer localization, achieving 18.49% F1 versus 6.03% chance baseline. However, when lexical information is available, models overwhelmingly prioritize it, suggesting more effective integration methods are needed to ensure prosody contributes significantly alongside lexical features.

## Method Summary
The study uses the SLUE-SQA-5 dataset with three audio conditions: natural, lexical (flattened pitch/intensity), and prosodic (low-pass filtered at 300Hz). WavLM-Large extracts representations from layer 23, which are quantized using k-means clustering (1000 clusters) to generate discrete units. A Longformer-base model predicts answer spans as start/end timestamps. Models are trained on each condition separately and evaluated using frame-level F1 (FF1) and Audio Overlapping Score (AOS) metrics. The approach systematically isolates prosodic and lexical contributions to determine their relative importance in SQA performance.

## Key Results
- Prosodic condition achieves 18.49% F1 versus 6.03% chance baseline on verified test set
- Models trained on prosodic information alone can utilize prosodic cues for answer localization
- When lexical information is available, models predominantly rely on it over prosodic cues
- Random question-context pairing drops prosodic performance from 18.29% to 9.77% F1, indicating prosody connects questions to contexts

## Why This Works (Mechanism)

### Mechanism 1
Prosodic cues alone can partially localize relevant answer segments in spoken passages. Intonation, stress, and pauses acoustically mark emphasized or structurally significant regions. When lexical content is suppressed via low-pass filtering (300Hz cutoff), models trained on these signals learn to associate prosodic patterns—particularly F0 contours and duration variations—with regions likely to contain answers. This works because prosodic emphasis in natural speech correlates with information prominence relevant to question answering.

### Mechanism 2
Models overwhelmingly prioritize lexical over prosodic information when both are available. Lexical tokens provide direct semantic grounding for question-answering via language model pretraining. Prosodic cues require indirect mapping. When exposed to both, gradient-based learning rapidly weights the lower-variance, higher-signal lexical pathway. This works because the model's optimization objective admits a shortcut solution through lexical features that generalizes sufficiently.

### Mechanism 3
Self-supervised speech representations implicitly encode prosodic information within discrete units. WavLM learns representations from raw waveforms through masked prediction. K-means quantization (1000 clusters) preserves sufficient acoustic detail—including pitch, intensity patterns, and speaker characteristics—for downstream tasks to exploit. This works because prosodic information survives both the SSL encoding and discrete quantization process with usable fidelity.

## Foundational Learning

- **Self-supervised speech representations (WavLM, wav2vec 2.0)**: Why needed here: The DUAL framework depends on WavLM to encode speech into discrete units. Understanding how SSL models learn from raw audio—without labels—explains why prosody survives encoding. Quick check: Can you explain why masking portions of speech and predicting them yields representations useful for multiple downstream tasks?

- **Extractive question answering span prediction**: Why needed here: The model outputs start and end timestamps (audio spans) rather than generated text. This differs fundamentally from generative QA and requires frame-level alignment understanding. Quick check: How does predicting [start, end] positions differ from generating answer tokens autoregressively?

- **Prosodic feature types (pitch/F0, intensity, duration, rhythm)**: Why needed here: The paper manipulates these dimensions—flattening F0/intensity, preserving duration—to isolate prosodic contributions. Without understanding what each feature encodes, the experimental design is opaque. Quick check: Which prosodic feature remains unmodified in the "lexical condition" and why might this matter?

## Architecture Onboarding

- **Component map**: Raw audio → WavLM encoding → K-means discretization → Unit deduplication → Longformer attention → Span prediction heads
- **Critical path**: Audio → WavLM encoding → K-means discretization → Unit deduplication → Longformer attention → Span prediction heads
- **Design tradeoffs**: Higher cluster counts (1000 vs 500) retain more prosody but increase sequence vocabulary; deduplication reduces sequence length but risks losing duration cues (paper finds only 8% units have >3 repetitions, mostly silence); layer selection: deeper WavLM layers emphasize semantics; earlier layers may preserve more prosody (unexplored)
- **Failure signatures**: If model performs identically on natural and lexical conditions → prosody not being utilized; if prosodic condition drops to chance (~6% F1) → prosodic information destroyed by filtering or not captured by encoder; if random question-context pairing shows no performance drop → prosody not connecting questions to contexts
- **First 3 experiments**:
  1. Reproduce the three training conditions (natural, lexical, prosodic) on verified test set to confirm 18.49% / 32.37% / 33.27% F1 baseline
  2. Ablate the cutoff frequency (replicate Figure 4): sweep 50–3000Hz to verify 200–400Hz is the optimal prosody-preserving range
  3. Layer-wise analysis: Extract representations from earlier WavLM layers (e.g., layers 8–14) to test whether prosodic information is stronger, as acknowledged in limitations

## Open Questions the Paper Calls Out

### Open Question 1
What specific model architectures or training objectives can effectively integrate prosodic cues so that they contribute meaningfully alongside lexical features? The abstract and conclusion state that while prosody provides supplementary cues, "more effective integration methods are required" because models currently "predominantly rely" on lexical information when available. This is unresolved because the current study isolated the features to prove prosody's utility but did not solve the "integration" problem where the model balances both inputs rather than ignoring prosody.

### Open Question 2
Does prosody play a more significant role in inferential comprehension tasks involving emotions, thoughts, and intentions compared to the literal comprehension tasks analyzed in this study? Section 6 (Limitations) notes that the study was limited to "literal comprehension" (identifying dates/names) and suggests future work should explore "inferential comprehension" where prosody might offer "important cues." This is unresolved because the SLUE-SQA-5 dataset used consists of factual questions where prosodic cues help locate segments but may not be strictly necessary for semantic understanding.

### Open Question 3
Is prosodic information more critical for open-ended or generative Spoken Question Answering tasks than for the extractive span-selection task used in this paper? Section 6 states that extending research to "open-ended SQA tasks" would be valuable to see if prosodic information guides models to generate "nuanced and contextually appropriate responses." This is unresolved because the current task requires predicting timestamps (extractive), which relies heavily on aligning keywords, potentially masking prosody's utility in generating fluid text responses.

### Open Question 4
To what extent is prosodic utility dependent on the specific layer of self-supervised speech models (e.g., WavLM) from which representations are extracted? Section 6 mentions the limitation that using deeper layers for semantic strength might have caused the model to miss prosodic information "more heavily encoded in earlier layers." This is unresolved because the experiments utilized a fixed layer (Layer 23) from WavLM, leaving the prosodic content of other layers unexplored.

## Limitations

- Opaque integration of prosodic cues into the Longformer model, where models predominantly rely on lexical information when available
- Assumption that prosodic emphasis correlates with semantic importance is not validated against human judgments
- Limited exploration of whether earlier WavLM layers preserve more prosodic information than the layer 23 used

## Confidence

- **High Confidence**: Experimental results demonstrating prosody's standalone contribution (18.49% F1 vs 6.03% chance baseline) are well-supported by ablation design
- **Medium Confidence**: Claim that models predominantly rely on lexical information when available is supported by performance differences but underlying mechanism is not fully characterized
- **Low Confidence**: Assertion that self-supervised representations implicitly encode prosodic information with sufficient fidelity requires further validation

## Next Checks

1. Conduct human annotation study where annotators mark answer-relevant regions in the prosodic condition audio, then compare these annotations to model predictions to verify whether the model is capturing semantically meaningful prosodic patterns or exploiting artifacts.

2. Implement multi-stream architecture where lexical and prosodic information are processed through separate encoders with explicit fusion mechanisms (e.g., attention-based combination), then compare performance to current single-stream approach to quantify the integration gap.

3. Extract representations from multiple WavLM layers (e.g., layers 8, 14, 23) and train separate models to isolate whether earlier layers indeed preserve more prosodic information, validating the acknowledged limitation and providing guidance for future architectural choices.