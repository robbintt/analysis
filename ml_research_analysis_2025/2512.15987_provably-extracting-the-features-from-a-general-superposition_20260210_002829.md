---
ver: rpa2
title: Provably Extracting the Features from a General Superposition
arxiv_id: '2512.15987'
source_url: https://arxiv.org/abs/2512.15987
tags:
- algorithm
- fourier
- then
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the problem of learning features in superposition\
  \ from black-box query access. The key question is: given a function f(x) = \u03A3\
  \ ai \u03C3i(vi^T x) that is a sum of features, can we recover the underlying features?"
---

# Provably Extracting the Features from a General Superposition

## Quick Facts
- arXiv ID: 2512.15987
- Source URL: https://arxiv.org/abs/2512.15987
- Authors: Allen Liu
- Reference count: 40
- Primary result: Efficient query algorithm recovers all non-degenerate feature directions in superposition models up to sign and ε-error, even in overcomplete regime (n > d)

## Executive Summary
This paper studies the fundamental problem of learning features in superposition from black-box query access. The key question is: given a function f(x) = Σ a_i σ_i(v_i^T x) that is a sum of features, can we recover the underlying features? The paper provides an affirmative answer with an efficient query algorithm that, under mild assumptions, identifies all non-degenerate feature directions v_i up to sign and ε-error, and reconstructs the function f to uniform ε-accuracy on a bounded domain. Crucially, this algorithm works in the overcomplete regime (n > d) and allows for essentially arbitrary superpositions with general response functions σ_i.

## Method Summary
The method introduces a novel paradigm for searching in Fourier space by iteratively refining the search space to locate hidden directions v_i. The algorithm first reweights the function with a Gaussian to ensure integrability, then uses a Fourier Mass Oracle to estimate the integral of the squared Fourier transform over regions. By querying the mass of thickened hyperplanes and recursively refining estimates, the algorithm efficiently locates all feature directions. Finally, it reconstructs the activation functions σ_i via Fourier inversion along the recovered directions.

## Key Results
- Provably learns all non-degenerate feature directions v_i up to sign and ε-error
- Reconstructs the original function f to uniform ε-accuracy on bounded domain
- Works efficiently in overcomplete regime (n > d) with arbitrary response functions
- Circumvents computational hardness barriers that exist in passive learning setting

## Why This Works (Mechanism)

### Mechanism 1: Fourier Sparsity of Ridge Functions
The Fourier transform of a sum of ridge functions concentrates on a union of one-dimensional lines (tubes) passing through the origin, corresponding to the hidden feature directions. This occurs because ridge functions depend only on single projections, and their Fourier transforms concentrate in tubes of width ≈ 1/ℓ around these lines. The core assumption is Lipschitz continuity of response functions σ_i. Break condition: If σ_i are linear functions, Fourier mass collapses to origin or cancels out.

### Mechanism 2: Iterative Refinement via Hyperplane Queries
Hidden directions are located by iteratively querying the "Fourier mass" of thickened hyperplanes, allowing the algorithm to prune the search space efficiently. The algorithm introduces a "Fourier Mass Oracle" I* that estimates the integral of the squared Fourier transform over a region. It uses a random orthonormal basis and grids the first two coordinates, then recursively queries mass of thickened hyperplanes to refine direction vector estimates. Core assumption: feature directions are γ-separated. Break condition: Nearly identical features prevent distinguishing individual directions.

### Mechanism 3: Non-Degeneracy Enforces Identifiability
Non-linear response functions guarantee that a non-trivial portion of the Fourier energy lies in a bounded frequency band away from zero, ensuring the "tubes" are detectable. The paper proves that if σ is Lipschitz but not close to linear, its Fourier transform cannot decay too fast. Specifically, a lower bound on Fourier mass in frequency band S^(ℓ) = [a, B] where a >> 1/ℓ ensures the algorithm actually sees signal mass. Core assumption: σ_i is (R, ε)-nondegenerate. Break condition: Polynomial or linear σ causes exponential decay or concentration at t=0.

## Foundational Learning

- **Ridge Functions (Single-Index Models)**: Understanding that these are functions constant along specific hyperplanes is crucial for grasping the "tube" geometry in Fourier space. Quick check: If v = [1, 0], what does the surface y = σ(v · x) look like in 2D?

- **Fourier Transform & Parseval's Identity**: The algorithm relies on Parseval's identity to translate between querying the spatial function f(x) and estimating the energy of its Fourier transform. Quick check: Does ∫|f(x)|² dx = ∫|f̂(y)|² dy hold for the Gaussian-reweighted functions?

- **Query Learning vs. Passive Learning**: The paper contrasts its results with passive learning, noting computational hardness there. The "active" query access enables efficient search. Quick check: Why can't a passive learner easily distinguish between two similar directions v_i and v_j compared to an active learner?

## Architecture Onboarding

- **Component map**: Preprocessing (Gaussian reweighting) -> Oracle (EstWeight) -> Search (FindDirections) -> Reconstruction (EstVal)
- **Critical path**: The recursive search in FindDirections. If branching factor isn't controlled or grid resolution is wrong, algorithm fails before reconstruction begins.
- **Design tradeoffs**: Scale ℓ must be large enough to separate tubes (width ≈ 1/ℓ) but not so large that Gaussian reweighting distorts original function on bounded domain. Grid Resolution must be fine enough to catch "heavy" Fourier mass but coarse enough to remain polynomial time.
- **Failure signatures**: Direction Clustering indicates γ assumption violation. Zero Recovery suggests non-linearity assumption on σ failed. Reconstruction Drift suggests underestimated Lipschitz constant L.
- **First 3 experiments**: 1) Sanity Check: Implement FindDirections for d=2, n=3 with distinct, orthogonal v_i and simple σ (e.g., ReLU). 2) Stress Test Correlation: Reduce angle γ until algorithm returns false positives or merged directions. 3) Noise Robustness: Add query noise to verify bounds hold and algorithm remains stable up to theoretical ε limits.

## Open Questions the Paper Calls Out

### Open Question 1
Can features in superposition be recovered efficiently in the passive (sample-based) learning setting under any natural distributional assumptions, or is query access requirement fundamental? The paper shows exponential SQ lower bounds in passive setting and that query access suffices, but leaves open whether restricted distributional assumptions could enable efficient passive learning.

### Open Question 2
Can the Fourier-space search paradigm be extended to recover features from multi-layer or hierarchical compositions of ridge functions, beyond single-layer sums? The paper focuses exclusively on single-layer sums, and multi-layer compositions create more complex Fourier structures that may not concentrate on simple lines.

### Open Question 3
What is the optimal query complexity for feature recovery, and can the polynomial dependence on parameters (d, n, L, R, 1/γ, 1/ε) be improved? The main result provides polynomial bounds without claiming optimality, and lower bounds for query model are not established.

### Open Question 4
Can the separation assumption (sin of angles between v_i, v_j ≥ γ) be weakened or removed while maintaining identifiability and efficient recovery? The authors state this assumption is necessary for recovering individual directions, but it remains unclear whether the overall function f could still be approximated efficiently when features are arbitrarily correlated.

## Limitations
- Algorithm requires γ-separation between feature directions, which becomes critical when n is large relative to d
- Polynomial runtime guarantees lack concrete constants for ℓ, C₁, C₂, and sample complexity
- Method requires precise tuning of scale parameter ℓ and grid resolution with no clear optimal values

## Confidence

**High Confidence:** The Fourier sparsity mechanism is mathematically rigorous and well-supported by spectral analysis. The query-based learning framework follows established patterns in active learning theory.

**Medium Confidence:** The non-degeneracy argument is plausible but relies on technical Fourier analysis lemmas whose practical implications are less intuitive. The separation-based pruning strategy's effectiveness depends heavily on empirical value of γ.

**Low Confidence:** The practical runtime and sample complexity in realistic settings, given unspecified polynomial constants and sensitivity to parameter tuning.

## Next Checks

1. **Separation Threshold Experiment:** Systematically vary the angle γ between feature directions in synthetic experiments to empirically determine minimum separation required for reliable recovery.

2. **Scale Sensitivity Analysis:** For fixed synthetic problem, sweep scale parameter ℓ across theoretical range to measure impact on direction recovery accuracy and reconstruction error.

3. **Runtime Benchmark:** Implement complete algorithm for moderate d and n (e.g., d=5, n=10) and measure actual query complexity and wall-clock time, comparing against stated polynomial bounds.