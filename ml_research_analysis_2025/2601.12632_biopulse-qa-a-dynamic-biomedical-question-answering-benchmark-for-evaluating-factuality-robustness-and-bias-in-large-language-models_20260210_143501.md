---
ver: rpa2
title: 'BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating
  Factuality, Robustness, and Bias in Large Language Models'
arxiv_id: '2601.12632'
source_url: https://arxiv.org/abs/2601.12632
tags:
- clinical
- biomedical
- benchmark
- drug
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioPulse-QA is a dynamic biomedical question-answering benchmark
  designed to evaluate large language models on factuality, robustness, and bias using
  newly published clinical documents. It includes 2,280 expert-verified QA pairs from
  drug labels, clinical trials, and guidelines, with variants for testing robustness
  (via paraphrasing, typos, abbreviations) and bias (via demographic counterfactuals).
---

# BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models
## Quick Facts
- arXiv ID: 2601.12632
- Source URL: https://arxiv.org/abs/2601.12632
- Reference count: 0
- Primary result: 2,280 expert-verified QA pairs from recent clinical documents, testing factuality, robustness (via paraphrasing, typos, abbreviations), and bias (via demographic counterfactuals) in biomedical LLMs

## Executive Summary
BioPulse-QA is a dynamic benchmark designed to evaluate large language models on biomedical question-answering tasks using newly published clinical documents. It addresses the critical need for up-to-date evaluation frameworks in the rapidly evolving medical field, where traditional static benchmarks become outdated quickly. The benchmark includes 2,280 expert-verified QA pairs from drug labels, clinical trials, and clinical practice guidelines, with variants for testing robustness through paraphrasing, typos, and abbreviations, as well as bias through demographic counterfactuals.

## Method Summary
BioPulse-QA employs a semi-automated pipeline that combines LLM-generated questions with expert verification to create clinically relevant QA pairs from recently published biomedical documents. The benchmark tests models across three dimensions: factuality (accuracy on original questions), robustness (performance degradation under perturbations like paraphrasing and typos), and bias (performance differences across demographic counterfactuals). The dynamic nature ensures content remains current by drawing from documents published within the past two years, addressing the staleness problem common in biomedical evaluation benchmarks.

## Key Results
- GPT-o1 achieved highest performance on drug labels (F1 0.92) and clinical practice guidelines (F1 0.88), but struggled with clinical trials (F1 0.36-0.50)
- Robustness varied significantly by perturbation type, with paraphrasing causing more performance degradation than typos across all models
- Bias tests revealed negligible performance differences across demographic counterfactuals
- Models showed higher accuracy on drug labels compared to other document types, suggesting domain-specific knowledge affects performance

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its three-pronged evaluation approach that mirrors real-world challenges in biomedical applications. By using recently published clinical documents, it ensures relevance to current medical knowledge. The robustness testing through controlled perturbations simulates common real-world scenarios like patient queries with typos or abbreviations. The bias evaluation through demographic counterfactuals helps identify potential disparities in model performance across different population groups.

## Foundational Learning
- **Biomedical document types**: Understanding drug labels, clinical trials, and clinical practice guidelines is essential for interpreting the benchmark's structure and evaluating model performance across different medical domains
- **Question-answering metrics**: F1 score calculation and interpretation are critical for assessing model performance and comparing results across different models and document types
- **Perturbation techniques**: Knowledge of how paraphrasing, typos, and abbreviations affect language model performance helps understand robustness evaluation methodology
- **Bias evaluation**: Understanding demographic counterfactuals and their role in identifying potential disparities in model performance across different population groups
- **Semi-automated pipeline**: Understanding the balance between LLM generation and expert verification is crucial for interpreting the benchmark's construction methodology
- **Dynamic benchmarking**: Recognizing the importance of using recently published documents to maintain relevance in rapidly evolving fields like biomedicine

## Architecture Onboarding
- **Component map**: Clinical documents -> Question generation -> Expert verification -> QA pairs -> Model evaluation -> Performance metrics
- **Critical path**: Document selection → Question generation (LLM) → Expert verification → QA pair creation → Model testing → Results analysis
- **Design tradeoffs**: Balance between automated question generation efficiency and expert verification quality, trade-off between benchmark size and expert resource constraints, decision to focus on recent documents vs. historical coverage
- **Failure signatures**: Low F1 scores indicating factuality issues, significant performance drops across perturbations indicating robustness weaknesses, performance disparities across demographic groups indicating potential bias
- **First experiments**: 1) Test model performance on drug labels vs. clinical trials to identify domain-specific knowledge gaps, 2) Evaluate robustness to paraphrasing vs. typos to understand perturbation sensitivity, 3) Compare demographic counterfactual performance to identify potential bias patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark size of 2,280 QA pairs may not fully represent the vast biomedical literature landscape
- Semi-automated pipeline may introduce systematic biases in question generation despite expert verification
- Demographic counterfactual approach may not capture all relevant forms of bias in biomedical contexts

## Confidence
- **High confidence**: Benchmark construction methodology, F1 score calculations, basic performance rankings of evaluated models
- **Medium confidence**: Interpretation of robustness results across different perturbation types, claim of "negligible performance differences" in bias tests requiring more nuanced statistical analysis

## Next Checks
1. Conduct inter-annotator reliability analysis on a subset of QA pairs to quantify expert agreement and potential systematic biases in the verification process
2. Perform statistical power analysis on bias testing results to determine if observed negligible differences are genuine or due to insufficient sample sizes in demographic counterfactuals
3. Test additional LLM architectures (including smaller models and specialized biomedical models) to establish whether performance patterns generalize beyond the four models evaluated