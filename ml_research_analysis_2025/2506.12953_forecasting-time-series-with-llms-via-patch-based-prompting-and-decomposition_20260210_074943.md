---
ver: rpa2
title: Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition
arxiv_id: '2506.12953'
source_url: https://arxiv.org/abs/2506.12953
tags:
- time
- series
- forecasting
- patchinstruct
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PatchInstruct, a prompt-based framework for
  time series forecasting using large language models (LLMs). The method tokenizes
  time series into overlapping patches and uses structured natural language prompts
  to guide LLMs in making predictions, eliminating the need for model fine-tuning
  or complex architectures.
---

# Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition

## Quick Facts
- arXiv ID: 2506.12953
- Source URL: https://arxiv.org/abs/2506.12953
- Reference count: 26
- Key outcome: PatchInstruct achieves state-of-the-art zero-shot time series forecasting performance with 10x-100x faster inference than S2IP-LLM

## Executive Summary
This paper introduces PatchInstruct, a prompt-based framework for time series forecasting using large language models (LLMs). The method tokenizes time series into overlapping patches and uses structured natural language prompts to guide LLMs in making predictions, eliminating the need for model fine-tuning or complex architectures. PatchInstruct achieves state-of-the-art performance, significantly reducing inference time by 10x–100x compared to prior methods like S2IP-LLM, while maintaining or improving accuracy. On benchmark datasets (Weather and Traffic), PatchInstruct consistently outperforms baselines in terms of MSE and MAE for short forecasting horizons, with percentage error reductions of up to 85%.

## Method Summary
PatchInstruct tokenizes time series into overlapping patches (window size 3, stride 1), reverses the patch order so the most recent appears first, and formats these as structured prompts for LLMs. The framework uses a system prompt to define the task and a user prompt with the prepared time series data. It optionally incorporates neighbor time series for additional context. The LLM generates predictions in a specified format, which are then parsed for numerical output. The approach is zero-shot, requiring no model fine-tuning.

## Key Results
- Achieves 10x-100x faster inference compared to S2IP-LLM
- Outperforms baseline methods on Weather and Traffic datasets for short horizons (H ≤ 6)
- Reduces MSE by up to 85% compared to existing zero-shot approaches
- Demonstrates that prompt engineering can replace architectural complexity in time series forecasting

## Why This Works (Mechanism)

### Mechanism 1: Patch-Based Tokenization for Local Temporal Pattern Capture
- **Claim:** Decomposing time series into overlapping patches creates meaningful tokens that capture local temporal dynamics better than raw values.
- **Mechanism:** The time series is divided into overlapping windows (window size 3, stride 1), creating patches like `[x1, x2, x3], [x2, x3, x4], ...`. Each patch encapsulates local temporal patterns. The patches are then reversed so the most recent patch appears first, providing a "recency-first" attention mechanism.
- **Core assumption:** LLMs trained on natural language can effectively process and reason about these patch tokens as if they were semantic units, leveraging their pattern recognition capabilities.
- **Evidence anchors:** [abstract]: "...leverage time series decomposition, patch-based tokenization..."; [section 1, page 1]: "...tokenizes time series data into meaningful patches that encapsulate temporally relevant patterns..."; [corpus]: "Integrating Quantum-Classical Attention in Patch Transformers" uses a similar patch-based approach, validating the general technique.

### Mechanism 2: Structured Prompt Engineering to Guide LLM Reasoning
- **Claim:** Explicit, structured instructions in natural language guide the LLM to perform forecasting more accurately than naive prompting.
- **Mechanism:** The framework uses a `System Prompt` to define the task, the patching methodology, and the expected output format. A `User Prompt` provides the actual time series data. This separates the "how-to" from the "what," reducing ambiguity and focusing the LLM's reasoning.
- **Core assumption:** Instruction-tuned LLMs can effectively follow multi-step instructions (e.g., "1. Split the series...", "2. Generate patches...", "3. Use patches to forecast...") and adhere to strict output formatting constraints.
- **Evidence anchors:** [section 3.2, page 4-5]: The paper provides detailed examples of `System Prompt` and `User Prompt` structures. "You are a forecasting assistant... Task: (1) Split the series..."; [section 1, page 1]: "...guides the LLM via structured natural language instructions to output precise predictions."; [corpus]: "Context information can be more important than reasoning for time series forecasting with a large language model" supports the importance of prompt context and structure.

### Mechanism 3: Neighbor-Augmented Context for Inter-Series Correlation
- **Claim:** Providing the LLM with similar time series from the dataset (neighbors) gives it additional contextual signals and recurring patterns, improving accuracy on some datasets.
- **Mechanism:** The framework identifies the five most similar time series from historical data based on Euclidean distance. These neighbors are included in the prompt as additional context, allowing the LLM to infer trends from related series, not just the target.
- **Core assumption:** Related time series in a dataset share underlying patterns and dynamics. An LLM can perform analogical reasoning, using the behavior of neighbors to inform its prediction of the target series.
- **Evidence anchors:** [abstract]: "...exploration of specialized prompting methods that leverage... similarity-based neighbor augmentation..."; [section 3, page 3]: "The motivation behind this approach is to provide the LLM with additional contextual signals and recurring patterns..."; [section 4.4, page 7]: Table 4 shows that Neighs and PatchInstruct+Neighs can outperform PatchInstruct, especially on the Weather dataset.

## Foundational Learning

- **Concept: Time Series Decomposition**
  - **Why needed here:** The `STR Decompose PatchInstruct` variant (section A.3, page 10) uses Seasonal-Trend-Residual decomposition. Understanding how a series is split into trend and residual components is key to implementing this variant and grasping the paper's exploration of different representations.
  - **Quick check question:** Can you explain how the `STR Decompose` variant represents each time step as a "composite token"?

- **Concept: Sliding Window / Patching**
  - **Why needed here:** This is the core of the `PatchInstruct` method. Understanding the mechanics of creating overlapping patches with a fixed window size and stride (e.g., window=3, stride=1) is fundamental to reproducing the results.
  - **Quick check question:** Given a time series `[10, 11, 12, 13, 14]`, what are the first three overlapping patches created with a window size of 3 and stride of 1?

- **Concept: Zero-Shot Learning with LLMs**
  - **Why needed here:** The entire premise of the paper is achieving SOTA forecasting *without* model fine-tuning. Understanding the capabilities and limitations of zero-shot learning is critical for setting expectations and understanding the trade-offs.
  - **Quick check question:** What does "zero-shot" mean in the context of this paper, and what baseline method does the authors' zero-shot approach draw inspiration from?

## Architecture Onboarding

- **Component map:** Data Ingestion & Preprocessing -> Prompt Constructor -> LLM Interface -> Output Parser
- **Critical path:** 
  1. Receive a 96-timestep time series window
  2. Preprocessing: Convert to a string. If using PatchInstruct, apply the patching logic (e.g., overlapping windows, reverse order) *within the text representation*. If using Neighs, perform a similarity search.
  3. Prompt Assembly: Construct the full prompt according to the chosen strategy (e.g., combine the PatchInstruct system prompt with the processed series).
  4. LLM Inference: Call the LLM API with the prompt.
  5. Post-processing: Parse the LLM's text response to get the final forecast list.

- **Design tradeoffs:**
  - **Inference Time vs. Accuracy:** The paper claims a 10x-100x speedup over S2IP-LLM, but accuracy for long horizons (e.g., H=12) is inconsistent. This method is best suited for short-horizon (H <= 6) forecasting.
  - **Prompt Complexity vs. Performance:** More complex prompts (PatchInstruct+Neighs) can yield better results but use more tokens. Simpler `Zeroshot` prompts are faster but less accurate.
  - **Neighbor Quality:** Adding neighbors is a gamble; it helps on the Weather dataset but hurts on the Traffic dataset. A robust implementation would need a validation step to determine if neighbors are beneficial for a given dataset.

- **Failure signatures:**
  - **Increased MSE on Long Horizons:** Performance degrades significantly for horizons > 6. This indicates the LLM struggles with long-range dependencies using this prompting strategy.
  - **Negative Transfer from Neighbors:** If using the Neighs strategy, a sudden increase in error compared to the PatchInstruct baseline indicates that the selected neighbors are introducing noise rather than signal.
  - **Output Formatting Errors:** The LLM may occasionally fail to follow the strict output format, requiring robust error handling in the parser.

- **First 3 experiments:**
  1. **Establish a Baseline:** Run the `Zeroshot` (LLMTime-inspired) prompt on the Weather dataset for horizons 1, 3, and 6. Record MSE, MAE, and inference time. This matches the paper's baseline.
  2. **Ablation on Patching Strategies:** Compare `Basic PatchInstruct` (natural order) vs. `Reverse Ordered Patches` on the Traffic dataset for horizon 3. This tests the paper's finding that reverse ordering is generally more effective.
  3. **Test Neighbor Augmentation:** Implement the `Neighs` strategy on the Weather dataset for horizon 2. Compare its performance against the `PatchInstruct` baseline to see if neighbor context provides a lift, as suggested by the paper's results in Table 4.

## Open Questions the Paper Calls Out

- **Question:** Can adaptive prompting mechanisms be developed to automate prompt design and reduce the risk of overfitting to specific datasets?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the framework relies on "carefully engineered prompts" and suggest future research should prioritize "developing adaptive prompting mechanisms."
- **Why unresolved:** Currently, the prompt structure (e.g., patch window size, stride, reverse ordering) is derived through empirical testing and is labor-intensive, potentially limiting scalability.
- **What evidence would resolve it:** A dynamic prompt optimization framework that selects patching strategies automatically based on input data characteristics without manual intervention.

## Limitations

- Performance degrades significantly for forecast horizons beyond 6 steps, limiting applicability to long-range forecasting
- Neighbor-augmentation strategy shows inconsistent results across datasets, improving performance on Weather but degrading it on Traffic
- Heavy reliance on powerful LLMs (GPT-4/GPT-4o) limits practical scalability and raises questions about performance with smaller models

## Confidence

- **High Confidence:** The core mechanism of patch-based tokenization and the general structure of the prompt (system prompt + user prompt) are well-documented and reproducible. The 10x-100x inference speedup claim is supported by direct comparison with S2IP-LLM timings.
- **Medium Confidence:** The specific performance numbers (MSE/MAE values) are likely accurate for the reported experiments, but the extent of the advantage over other zero-shot methods may vary with different datasets or LLM versions. The claim of "state-of-the-art" is relative to a specific set of baselines.
- **Low Confidence:** The long-term effectiveness of the method (H > 6) and the generalizability of the neighbor-augmentation strategy across diverse time series domains are not fully established.

## Next Checks

1. **Long-Horizon Stress Test:** Reproduce the experiment from the paper, but extend the forecast horizon to H=24. Measure and report the degradation in MSE and MAE compared to the H=6 results to quantify the method's limitations for long-range forecasting.

2. **Cross-Model Generalization:** Implement PatchInstruct using a smaller, more accessible LLM (e.g., GPT-3.5-turbo or an open-source model like LLaMA-2). Compare its performance and inference time to the GPT-4 results to assess the method's practical applicability and the importance of model scale.

3. **Neighbor Strategy Ablation:** Design an experiment to systematically test the neighbor-augmentation strategy. For a given dataset, run PatchInstruct with neighbors selected using different distance metrics (Euclidean, DTW, etc.) and different numbers of neighbors (k=1, 3, 5, 10). Identify which configuration, if any, provides a consistent improvement over the baseline PatchInstruct.