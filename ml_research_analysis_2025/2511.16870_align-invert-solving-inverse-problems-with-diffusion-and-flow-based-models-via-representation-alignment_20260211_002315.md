---
ver: rpa2
title: 'Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models
  via Representation Alignment'
arxiv_id: '2511.16870'
source_url: https://arxiv.org/abs/2511.16870
tags:
- diffusion
- repa
- latent
- dinov2
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends representation alignment (REPA) from image generation
  to inverse problems by enforcing alignment between the internal representations
  of diffusion/flow models and DINOv2 features during reconstruction. Since ground-truth
  signals are unavailable in inverse problems, the authors use proxy representations
  derived from approximate reconstructions.
---

# Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representation Alignment

## Quick Facts
- arXiv ID: 2511.16870
- Source URL: https://arxiv.org/abs/2511.16870
- Reference count: 40
- Primary result: Extends REPA from image generation to inverse problems, improving perceptual quality (LPIPS/FID) and efficiency across super-resolution, inpainting, and deblurring tasks on ImageNet and FFHQ.

## Executive Summary
This paper introduces a method to solve inverse problems using diffusion and flow-based models by enforcing alignment between their internal representations and DINOv2 features during reconstruction. Since ground-truth signals are unavailable in inverse problems, proxy representations derived from approximate reconstructions are used. The approach integrates into existing solvers (Latent DPS and ReSample), demonstrating improved perceptual quality and efficiency across multiple tasks and datasets.

## Method Summary
The method extends representation alignment (REPA) to inverse problems by adding a regularization term that aligns diffusion/flow model features with DINOv2 features. During sampling, a projection MLP maps diffusion features to DINOv2 space, and the alignment loss is computed against proxy representations. These proxies transition from measurement-based to reconstruction-based during sampling. The approach is integrated into Latent DPS and ReSample solvers, modifying the sampling trajectory to minimize a feature-space divergence while maintaining measurement consistency.

## Key Results
- REPA improves LPIPS and FID scores across super-resolution, inpainting, and deblurring tasks on ImageNet and FFHQ
- The method achieves better perceptual quality with fewer discretization steps compared to baseline solvers
- Finetuning can introduce alignment to pretrained models, though alignment-pretrained models perform best

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** REPA regularization approximates minimization of a feature-space divergence between reconstructed and ground-truth image distributions
- **Mechanism:** The REPA objective bounds the Maximum Mean Discrepancy (MMD) in DINOv2 embedding space, minimizing perceptual divergence plus penalty terms for proxy approximation error and feature misalignment
- **Core assumption:** DINOv2 features are ℓ2-normalized; proxy approximation error and misalignment are small in practice
- **Evidence anchors:** Theoretical results show REPA minimizes a DINOv2-space divergence (MMD); derivation shows `E[REPA] ≤ 1 - 1/8 MMD + 1/2 ApproxErr + 1/4 MisREPA`
- **Break condition:** If proxy error or misalignment dominates, REPA may not meaningfully reduce MMD

### Mechanism 2
- **Claim:** REPA updates locally contract diffusion states toward clean-image latents under regularity conditions
- **Mechanism:** The gradient of the alignment loss, backpropagated through the diffusion encoder and projection MLP, produces a contraction in latent space when the current state is near the clean latent
- **Core assumption:** Jacobians of the diffusion encoder and projection head are well-conditioned and Lipschitz continuous within a local ball around the clean latent
- **Evidence anchors:** REPA updates steer the latent diffusion states toward those of the clean image; formal statement shows `‖z_REPA - z*‖₂ ≤ C₁‖z_t - z*‖₂ + error_term`, with `C₁ ∈ (0,1)`
- **Break condition:** If the current latent is far from `z*` or Jacobians are poorly conditioned, contraction is not guaranteed

### Mechanism 3
- **Claim:** Proxy representations from corrupted measurements provide viable alignment targets due to DINOv2's robustness
- **Mechanism:** DINOv2 features remain stable under super-resolution and deblurring corruptions, allowing proxy transitions from measurement-derived features to reconstruction-derived features during sampling
- **Core assumption:** DINOv2 invariances hold for the specific degradation class and severity
- **Evidence anchors:** Construct c_proxy from an approximate reconstruction; quantify representation similarity under increasing corruption severity for super-resolution and Gaussian deblurring
- **Break condition:** For degradations outside DINOv2's robustness envelope, proxy error may dominate

## Foundational Learning

- **Concept: Diffusion/Flow Sampling as ODE/SDE Inversion**
  - **Why needed here:** REPA modifies the sampling trajectory; understanding the baseline drift field and score parameterization is prerequisite
  - **Quick check question:** Can you derive the relationship between velocity `v(x_t, t)` and score `s(x_t, t)` in Eq. (5)?

- **Concept: Posterior Sampling for Inverse Problems**
  - **Why needed here:** REPA is added to existing solvers (Latent DPS, ReSample) that approximate `∇ log p(y|x_t)`
  - **Quick check question:** Explain how DPS approximates the intractable likelihood term

- **Concept: DINOv2 Patch-Level Features and Semantic Alignment**
  - **Why needed here:** The alignment objective operates on patch tokens; understanding the feature structure is essential for debugging
  - **Quick check question:** What is the dimensionality of `f_DINOv2(x)[n]` and how does `g_φ` project to it?

## Architecture Onboarding

- **Component map:** Pretrained diffusion/flow model (SiT transformer blocks) -> DINOv2 encoder (frozen, provides target features) -> MLP projection head `g_φ` (learned, maps diffusion features to DINOv2 space) -> Inverse problem solver (Latent DPS or ReSample, provides measurement-matching gradient) -> Proxy representation module (switches between measurement-based and reconstruction-based)

- **Critical path:** 1) Sample `z_T ~ N(0, I)` 2) For each timestep: compute velocity → Euler step → measurement gradient → REPA gradient → update proxy if past cutoff 3) Decode `z_0` to image

- **Design tradeoffs:**
  - **Layer selection:** Earlier layers capture more spatial detail; later layers capture more semantics. Paper uses block 8 for ImageNet, block 4 for FFHQ
  - **Proxy strategy:** Measurement-based proxy is simpler but less accurate for severe degradations; reconstruction-based proxy improves later but requires tuning `t_cutoff`
  - **Regularization strength `λ`:** Too high distorts the prior; too low provides weak guidance. Values range from 0.01 to 3.25 depending on task

- **Failure signatures:**
  - Over-regularization: reconstructions look semantically correct but lose fine detail or drift from measurement consistency
  - Proxy mismatch: artifacts persist because DINOv2 features of proxy differ from ground truth
  - Poor conditioning: REPA gradients become unstable if projection MLP or diffusion encoder Jacobians are ill-conditioned

- **First 3 experiments:**
  1. **Baseline comparison:** Run Latent DPS on 4× super-resolution (ImageNet) with and without REPA; report LPIPS/FID gap
  2. **Proxy ablation:** Compare measurement-only proxy vs. reconstruction-updated proxy; quantify ApproxErr for each
  3. **Step efficiency:** Plot LPIPS vs. number of sampling steps; identify the step count where REPA matches baseline quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can representation alignment generalize to scientific or medical inverse problems where the data distribution differs significantly from the natural images used to train semantic encoders like DINOv2?
- **Basis in paper:** The method relies on DINOv2 features, which are pretrained on natural images (ImageNet), yet inverse problems often involve MRI, CT, or spectral data lacking these specific semantic structures
- **Why unresolved:** The paper validates the approach on ImageNet and FFHQ (faces/natural scenes), but the theory depends on the existence of aligned semantic features that may not exist or be relevant in non-visual scientific domains
- **What evidence would resolve it:** Successful application of REPA on datasets like FastMRI or astronomical data, or a theoretical analysis showing alignment with domain-specific feature extractors

### Open Question 2
- **Question:** How does the choice of the pretrained visual encoder (e.g., DINOv2 vs. CLIP vs. MAE) impact the geometric properties of the induced trajectory and the resulting reconstruction trade-offs?
- **Basis in paper:** The authors explicitly use DINOv2 and justify it via robustness, but they do not compare against other potential encoders which might prioritize different features
- **Why unresolved:** While DINOv2 is shown to work, the paper does not isolate whether specific properties of DINOv2 (like patch-level locality) are strictly necessary or if any robust self-supervised encoder would suffice
- **What evidence would resolve it:** A comparative ablation study replacing DINOv2 with other foundation models to measure changes in LPIPS/FID and convergence speed

### Open Question 3
- **Question:** What are the theoretical limits of the proxy approximation error (ApproxErr) before the REPA update fails to contract the diffusion state toward the clean image latent?
- **Basis in paper:** Proposition 4.4 establishes a contraction bound dependent on small ApproxErr and MisREPA, but the paper does not empirically map the failure modes when these assumptions are violated
- **Why unresolved:** The paper asserts robustness but does not define the "breaking point" of degradation severity where the proxy features diverge too far from the ground truth to be useful
- **What evidence would resolve it:** An analysis plotting reconstruction quality against increasing noise levels to identify the threshold where the proxy signal degrades performance compared to the baseline

## Limitations
- **Proxy approximation error:** Using corrupted measurements or imperfect reconstructions as proxies introduces approximation error that may dominate in severe degradations
- **Assumption dependency:** Theoretical contraction results rely on well-conditioned Jacobians that are not empirically validated
- **Robustness generalization:** Proxy error analysis is limited to super-resolution and Gaussian deblurring, with other degradation types not quantified

## Confidence
- **High Confidence:** Empirical improvements in LPIPS and FID metrics for super-resolution and deblurring tasks are well-supported by results in Tables 2 and 3
- **Medium Confidence:** Theoretical claims are mathematically sound but rely on assumptions not empirically validated; sensitivity to hyperparameters not explored
- **Low Confidence:** No evidence for effectiveness on degradation types outside tested scope; proxy error analysis limited to subset of tasks

## Next Checks
1. **Proxy Error Quantification:** Measure the MMD between DINOv2 features of proxies and ground-truth images across all tested degradation types; compare to empirical improvement in LPIPS/FID
2. **Assumption Validation:** Empirically verify Lipschitz continuity and conditioning of Jacobians of the diffusion encoder and projection head; test REPA's performance when these assumptions are violated
3. **Degradation Robustness:** Evaluate REPA on degradation types not covered in the paper (e.g., severe occlusion, JPEG compression, cross-domain corruptions); quantify proxy error and alignment quality for these cases