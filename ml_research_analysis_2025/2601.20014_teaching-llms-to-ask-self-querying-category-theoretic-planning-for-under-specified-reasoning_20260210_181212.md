---
ver: rpa2
title: 'Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified
  Reasoning'
arxiv_id: '2601.20014'
source_url: https://arxiv.org/abs/2601.20014
tags:
- sq-bcp
- preconditions
- verification
- planning
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SQ-BCP is an inference-time planning framework that explicitly
  tracks preconditions under partial observability using Sat/Viol/Unk labels and resolves
  unknowns via targeted self-queries or bridging actions. It integrates bidirectional
  search with a pullback-based categorical verifier and deterministic hard-constraint
  checks, using heuristic distance only for ranking and pruning.
---

# Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning

## Quick Facts
- arXiv ID: 2601.20014
- Source URL: https://arxiv.org/abs/2601.20014
- Reference count: 37
- Primary result: SQ-BCP reduces resource-violation rates to 14.9% and 5.8% (vs. 26.0% and 15.7% for the best baseline) while maintaining competitive reference-similarity scores

## Executive Summary
SQ-BCP is an inference-time planning framework that addresses partial observability in planning by explicitly tracking preconditions as Sat/Viol/Unk and resolving unknowns through targeted self-queries or bridging actions. It integrates bidirectional search with a pullback-based categorical verifier and deterministic hard-constraint checks, using heuristic distance only for ranking and pruning. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP demonstrates significant reduction in resource-violation rates while maintaining competitive reference-similarity scores compared to baseline methods.

## Method Summary
SQ-BCP runs bidirectional search from initial state w₀ and goal w*, generating candidate actions with preconditions labeled Sat/Viol/Unk. Unknown preconditions trigger a deterministic refinement policy: attempt up to T_bridge bridging actions, then issue one targeted query to an oracle/user, then mark as unresolvable if both fail. Distance-based scores are used only for ranking/pruning, not for acceptance. Plan acceptance requires: all preconditions resolved, hard-constraint checks pass, and pullback verification confirms goal compatibility. The framework was evaluated on WikiHow and RecipeNLG tasks with withheld preconditions using simulated oracle responses.

## Key Results
- Reduces resource-violation rates to 14.9% and 5.8% on WikiHow and RecipeNLG tasks respectively
- Outperforms baselines (CoT, ToT, ReAct, Self-Ask) with 26.0% and 15.7% violation rates
- Maintains competitive ROUGE-1/2 (WikiHow) and BLEU (RecipeNLG) reference-similarity scores
- Demonstrates effective handling of unknown preconditions through targeted queries and bridging

## Why This Works (Mechanism)

### Mechanism 1: Explicit Tri-State Precondition Modeling
Representing precondition status as Sat/Viol/Unk rather than binary enables principled handling of partial observability. Each candidate action hypothesis carries labeled preconditions, preventing the model from hallucinating missing facts into pseudo-satisfaction.

### Mechanism 2: Deterministic Refinement via Querying and Bridging
A bounded, ordered resolution policy converts unknown preconditions into verified facts or feasible subplans. For each Unk precondition, the system first attempts up to T_bridge bridging actions, then issues one targeted query, then marks as unresolvable if both fail.

### Mechanism 3: Verification-Gated Acceptance with Pullback Certificates
Plan acceptance based on categorical verification plus hard-constraint checks—not heuristic distance—ensures goal compatibility. Pullback verification certifies compositional correctness while heuristic distance is used only for ranking/pruning.

## Foundational Learning

- Concept: Classical Planning Preconditions (STRIPS/PDDL)
  - Why needed here: SQ-BCP inherits the principle that actions have explicit preconditions; understanding this clarifies why Sat/Viol/Unk tracking matters.
  - Quick check question: Given action "cut wood with saw," what preconditions must hold?

- Concept: Partial Observability and Belief States
  - Why needed here: The core problem is planning when critical facts are unknown; SQ-BCP's Unk label is a coarse belief-state approximation.
  - Quick check question: How does a POMDP handle unknown state variables vs. SQ-BCP's approach?

- Concept: Category Theory Basics (Objects, Morphisms, Pullbacks)
  - Why needed here: The verifier uses pullback constructions; understanding objects-as-states and morphisms-as-transactions is prerequisite.
  - Quick check question: If states are objects and actions are morphisms, what does a pullback between a candidate terminal state and the goal specification represent?

## Architecture Onboarding

- Component map:
  Hypothesis Generator (LLM) -> Precondition Classifier (LLM) -> Refinement Engine -> Bidirectional Search Controller -> Hard-Constraint Checker -> Pullback Verifier -> Distance/Scoring Module

- Critical path:
  Hypothesis generation → Precondition labeling → (If Unk) Refinement loop → Search graph insertion → Meet-point detection → Hard-check + Pullback verification → Accept or continue search

- Design tradeoffs:
  - Bridging depth (T_bridge) vs. query burden: More bridging reduces user load but increases plan complexity
  - Pruning thresholds vs. completeness: Aggressive distance pruning speeds search but may discard valid plans
  - LLM backbone vs. verification reliability: Stronger models classify preconditions better but increase cost

- Failure signatures:
  - High violation rate with low query count → precondition classifier missing Unk labels
  - Excessive queries without resolution → bridging insufficient, oracle unresponsive, or ill-posed preconditions
  - Valid plans rejected → verifier thresholds too tight or hard-constraint predicates incorrect
  - Search timeout without candidates → pruning thresholds too aggressive for the domain

- First 3 experiments:
  1. Replicate WikiHow single-precondition withholding: Verify that Unk detection + single query reduces violations vs. CoT baseline
  2. Ablate bridging (set T_bridge = 0): Measure violation increase to quantify bridging contribution
  3. Stress-test with 5+ missing preconditions: Characterize query/hypothesis growth scaling per Figure 2

## Open Questions the Paper Calls Out

- How does SQ-BCP performance and user burden change when deployed with real human oracles providing noisy, incomplete, or inconsistent responses?
- Can the deterministic verification procedure be extended to handle continuous feasibility conditions rather than just discrete hard constraints?
- Does SQ-BCP effectively generalize to domains outside of procedural instructions, particularly those with long horizons or different constraint structures?

## Limitations

- Exact LLM prompts for hypothesis generation and precondition classification are not provided
- Pullback verification implementation details rely on a separate reference without clear instantiation guidance
- Evaluation is limited to procedural instruction domains (WikiHow, RecipeNLG), raising questions about generalization to other domains

## Confidence

- **High confidence**: The explicit tri-state precondition modeling is well-defined with clear mechanism and evidence
- **Medium confidence**: The deterministic refinement policy is methodologically sound but depends on unknown implementation parameters
- **Medium confidence**: The pullback-based verification is theoretically compelling but practical confidence is limited by lack of implementation details

## Next Checks

1. **Ablation of categorical verification**: Remove the pullback verifier and rely solely on hard-constraint checks to isolate its contribution to performance improvement
2. **Cross-domain transfer test**: Apply SQ-BCP to a domain with different precondition characteristics (e.g., software installation or home automation) to evaluate generalizability beyond WikiHow/RecipeNLG
3. **Query efficiency scaling**: Systematically vary the number of withheld preconditions (m) from 1 to 10+ to measure how query and bridging costs scale, validating the complexity claims in Figure 2