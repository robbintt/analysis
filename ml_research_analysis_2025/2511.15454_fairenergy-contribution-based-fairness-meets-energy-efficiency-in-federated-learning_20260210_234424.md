---
ver: rpa2
title: 'FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated
  Learning'
arxiv_id: '2511.15454'
source_url: https://arxiv.org/abs/2511.15454
tags:
- energy
- fairness
- selection
- bandwidth
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairEnergy addresses energy efficiency and fairness in federated
  learning by jointly optimizing device selection, bandwidth allocation, and compression
  ratios. It introduces a contribution score that captures update magnitude and compression,
  and enforces long-term fairness through exponential moving average participation
  tracking.
---

# FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning

## Quick Facts
- arXiv ID: 2511.15454
- Source URL: https://arxiv.org/abs/2511.15454
- Reference count: 26
- One-line result: Jointly optimizes device selection, bandwidth allocation, and compression ratios to achieve up to 79% lower energy consumption while maintaining fairness in FL

## Executive Summary
FairEnergy addresses the challenge of balancing energy efficiency and fairness in federated learning by jointly optimizing device selection, bandwidth allocation, and compression ratios. The framework introduces a contribution score that captures both update magnitude and compression ratio, enabling selection of informative clients while accounting for information loss. It enforces long-term fairness through exponential moving average participation tracking and solves a mixed-integer non-convex problem via Lagrangian relaxation and dual decomposition. Experiments on non-IID FMNIST data show FairEnergy achieves significant energy reduction compared to baseline strategies while maintaining equitable device participation.

## Method Summary
The method solves a mixed-integer non-convex optimization problem through Lagrangian relaxation and dual decomposition. Each round, the server receives updates from devices, computes contribution scores based on update magnitude and compression ratio, and uses Golden Section Search to optimize bandwidth allocation for each compression level. Devices are selected based on a threshold rule comparing energy/bandwidth costs against weighted contribution benefits plus fairness incentives. Dual variables are updated via subgradient ascent to enforce bandwidth and fairness constraints, with long-term fairness tracked using exponential moving average participation. The approach runs in O(G · T_GSS) per device where G is the compression grid size and T_GSS is the GSS iteration count.

## Key Results
- Achieves up to 79% lower energy consumption compared to baseline strategies while reaching target accuracy
- Demonstrates more equitable device participation through long-term fairness enforcement
- Shows 71% energy reduction compared to ScoreMax baseline on non-IID FMNIST data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contribution scores combining update magnitude and compression ratio enable informative client selection while accounting for information loss from compression.
- Mechanism: The score s_i^r(γ_i^r) = ∥u_i^r∥ · γ_i^r scales the L2 norm of updates by the compression factor. Clients with large gradient magnitudes are prioritized, but the score penalizes aggressive compression that discards information. This creates a selection signal that favors clients whose compressed updates retain high information content per bit transmitted.
- Core assumption: Update magnitude (∥u_i^r∥) correlates with convergence acceleration. The paper cites prior work [8] but does not provide theoretical convergence guarantees in this formulation.
- Evidence anchors:
  - [abstract] "contribution score that captures both the magnitude of updates and their compression ratio"
  - [Section III-A] "contribution score for client i in round r as: s_i^r(γ_i^r) = ∥u_i^r∥ · γ_i^r"
  - [corpus] Neighbor papers on fairness (e.g., "Achieving Distributive Justice in FL") discuss contribution-based fairness but do not validate this specific scoring formulation.

### Mechanism 2
- Claim: EMA-based participation tracking with dual variable enforcement ensures long-term fairness without requiring per-round equality.
- Mechanism: Fairness metric q_i^r = ρq_i^{r-1} + (1-ρ)x_i^r tracks smoothed participation history. Dual variables μ_i^r penalize constraint violation (q_i^r < π_min), boosting under-selected clients' selection probability. The threshold-based selection rule includes fairness penalties: clients are selected if benefit (ηs_i^r + μ_i^r(1-ρ)) outweighs cost (E_i^r + λ^r B_i^r).
- Core assumption: Historical participation rate is a meaningful fairness metric. Assumes π_min is achievable given bandwidth constraints.
- Evidence anchors:
  - [abstract] "enforces long-term fairness through exponential moving average participation tracking"
  - [Section III-B] "qr_i ≥ π_min, ∀i ∈ {1, . . . , N}"
  - [corpus] Weak corpus evidence for this specific EMA formulation; related work [11, 12] cited but not validated in corpus neighbors.

### Mechanism 3
- Claim: Threshold-based selection rule emerging from Lagrangian relaxation naturally balances energy cost against contribution benefit.
- Mechanism: After relaxing binary selection variables, the optimal x_i^r is binary: x_i^r = 1 if and only if E_i^r + λ^r B_i^r < η s_i^r + μ_i^r(1-ρ). This creates interpretable selection—participation occurs when the combined energy and bandwidth cost is lower than the weighted contribution benefit plus fairness incentive.
- Core assumption: The unimodal property of ϕ_i^r(γ_i^r, B_i^r) in bandwidth holds for the channel model. Assumes GSS finds global minimum.
- Evidence anchors:
  - [Section V-B] "device participates (x_i = 1) only if the benefit of its update outweighs the combined cost of energy and bandwidth"
  - [Section V-C] "decreasing-flattening-increasing pattern produces a single minimum"
  - [corpus] No corpus validation of the unimodal assumption or GSS applicability.

## Foundational Learning

- Concept: **Lagrangian Relaxation and Dual Decomposition**
  - Why needed here: The optimization problem is mixed-integer non-convex with coupled constraints (bandwidth shared across devices). Dual decomposition enables per-device subproblems.
  - Quick check question: Can you explain why relaxing x_i ∈ {0,1} to [0,1] still yields binary solutions at the optimum?

- Concept: **Exponential Moving Average (EMA) for State Tracking**
  - Why needed here: Fairness requires tracking participation history across rounds. EMA provides smoothed estimates with controllable memory (ρ parameter).
  - Quick check question: How does the choice of ρ affect the responsiveness of fairness enforcement?

- Concept: **Golden Section Search (GSS) for Unimodal Optimization**
  - Why needed here: Bandwidth optimization is non-convex but unimodal. GSS provides derivative-free optimization with guaranteed convergence for unimodal functions.
  - Quick check question: What happens if the function is not strictly unimodal—does GSS still converge?

## Architecture Onboarding

- Component map:
  - Per-Device Subproblem Solver -> Selection Decision Module -> Dual Variable Updater -> Fairness State Tracker

- Critical path:
  1. Server receives updates u_i^r from previous round (or initial random selection).
  2. For each device, compute contribution scores s_i^r(γ) for all γ ∈ Γ.
  3. Run GSS to find optimal B_i^r(γ) minimizing ϕ_i^r for each γ.
  4. Select best (γ_i^r, B_i^r) pair per device.
  5. Apply threshold rule to determine x_i^r.
  6. Update dual variables λ^r, μ_i^r and fairness states q_i^r.
  7. Broadcast selection decisions and bandwidth allocations.

- Design tradeoffs:
  - **Compression grid granularity (G)**: Larger G improves optimality but increases per-round complexity to O(G · T_GSS) per device.
  - **EMA memory (ρ)**: Higher ρ smooths participation over more rounds (more conservative fairness); lower ρ reacts faster but may oscillate.
  - **Fairness threshold (π_min)**: Higher values enforce stricter fairness but may force selection of low-contribution clients, potentially slowing convergence.

- Failure signatures:
  - **Dual variable explosion**: λ^r or μ_i^r growing unbounded indicates infeasible constraints (bandwidth or fairness thresholds too strict).
  - **Consistent under-selection of specific clients**: Check if channel conditions or data quality systematically disadvantage certain devices; may need π_min adjustment.
  - **Accuracy plateau with high energy**: ScoreMax outperforming FairEnergy suggests η (contribution weight) is too low or fairness constraints are too restrictive.

- First 3 experiments:
  1. **Baseline comparison**: Replicate ScoreMax, EcoRandom, and FairEnergy on non-IID FMNIST with N=50, measuring accuracy vs. rounds, per-round energy, and participation variance. Verify the 71-79% energy reduction claim.
  2. **Ablation on fairness constraint**: Run FairEnergy with π_min ∈ {0.1, 0.2, 0.3, 0.4} and plot accuracy-energy tradeoff. Identify the breaking point where fairness becomes infeasible.
  3. **Sensitivity to ρ and η**: Grid search ρ ∈ {0.4, 0.6, 0.8} and η ∈ {10^-3, 10^-2, 10^-1} to understand how memory and contribution weight affect convergence speed and fairness (measure participation std across devices).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the theoretical convergence guarantees for FairEnergy when integrating compression and fairness constraints?
- **Basis in paper:** [explicit] Footnote 1 states the authors empirically show convergence but "leave a full theoretical analysis for future work."
- **Why unresolved:** The current work relies on empirical validation and prior theoretical work on magnitude-based selection, but adding compression-dependent contribution scores and EMA-based fairness constraints creates a complex stochastic optimization environment not yet formally proven to converge.
- **What evidence would resolve it:** A formal proof bounding the global loss $F(w)$ over rounds $r$, accounting for the variance introduced by the proposed contribution score and fairness metrics.

### Open Question 2
- **Question:** How does jointly optimizing computation energy alongside communication energy affect the proposed fairness mechanisms?
- **Basis in paper:** [explicit] The Conclusion proposes extending the framework to the "joint optimization of computation and communication resources."
- **Why unresolved:** The current system model (Section II) explicitly excludes computation costs to focus on communication bottlenecks. Heterogeneous compute capabilities could conflict with the current fairness metrics if devices with low bandwidth but high compute capability are treated identically to low-compute devices.
- **What evidence would resolve it:** A modified objective function minimizing $E_{total} = E_{comm} + E_{comp}$ and an analysis of its impact on the selection threshold rule derived in Section V.

### Open Question 3
- **Question:** How does FairEnergy adapt to dynamic channel environments where conditions fluctuate rapidly?
- **Basis in paper:** [explicit] The Conclusion lists extending the framework to "dynamic channel environments" as a primary future direction.
- **Why unresolved:** The optimization problem is formulated per-round assuming fixed channel gains $h_i^r$ for the decision process. It is unclear if the Lagrangian relaxation and threshold rule remain stable or efficient when channel states are highly time-correlated or non-stationary.
- **What evidence would resolve it:** Simulations utilizing time-correlated fading models (e.g., Rayleigh fading with Doppler shift) demonstrating that the "threshold rule" does not lead to oscillating selection or fairness collapse.

## Limitations

- **Dual variable stability**: Step sizes for subgradient ascent are not specified, which critically affects convergence stability and feasibility of constraints.
- **Unimodal assumption validity**: The unimodal assumption for Golden Section Search is not rigorously proven for the channel model used.
- **Ablation completeness**: Lacks ablation studies on individual components to isolate mechanism contributions.

## Confidence

- **High confidence**: Energy consumption reduction claims (71-79% vs. baselines) are supported by experimental results and the optimization framework is mathematically sound.
- **Medium confidence**: The contribution score mechanism is intuitive but lacks theoretical convergence guarantees for the specific scoring formulation.
- **Low confidence**: The fairness enforcement via EMA is novel but has limited validation; the corpus shows no prior work validating this specific formulation.

## Next Checks

1. **Dual variable stability test**: Run FairEnergy with varying step sizes (α_λ ∈ {0.001, 0.01, 0.1}, α_μ ∈ {0.0001, 0.001, 0.01}) and plot λ^r, μ_i^r over rounds to identify stable regions and divergence thresholds.

2. **Unimodal function verification**: For your specific channel model (h_i^r, P_i, N_0), plot E_i^r + λ^r B_i^r vs. B_i^r across multiple devices and rounds to verify unimodal shape. Test GSS against grid search for bandwidth optimization.

3. **Fairness sensitivity analysis**: Run FairEnergy with π_min ∈ {0.1, 0.2, 0.3} and measure participation std, accuracy-energy tradeoff, and dual variable growth to identify feasibility boundaries.