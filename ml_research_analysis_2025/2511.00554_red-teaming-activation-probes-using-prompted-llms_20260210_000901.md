---
ver: rpa2
title: Red-teaming Activation Probes using Prompted LLMs
arxiv_id: '2511.00554'
source_url: https://arxiv.org/abs/2511.00554
tags:
- failure
- probe
- probes
- high-stakes
- attacker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a lightweight, black-box red-teaming method
  for activation probes using an off-the-shelf LLM with iterative feedback and in-context
  learning. No model fine-tuning or architectural access is required.
---

# Red-teaming Activation Probes using Prompted LLMs

## Quick Facts
- arXiv ID: 2511.00554
- Source URL: https://arxiv.org/abs/2511.00554
- Authors: Phil Blandfort; Robert Graham
- Reference count: 20
- One-line primary result: Black-box red-teaming method achieves up to 63.6% false negative and 88.0% false positive failure rates on activation probes without requiring gradient access or fine-tuning

## Executive Summary
This paper introduces a lightweight, black-box red-teaming method for activation probes using an off-the-shelf LLM with iterative feedback and in-context learning. The approach requires no model fine-tuning, gradients, or architectural access, making it practical for probing unknown systems. Tested on a high-stakes interaction probe, the method achieves significant failure rates (up to 63.6% FNR and 88.0% FPR using GPT-5) with clear improvement over rounds via ICL. Smaller open-source models (27B–120B) also find failure cases but with lower rates and less stable learning. The approach surfaces actionable insights for probe hardening and suggests probes should not serve as sole control points in high-incentive settings.

## Method Summary
The method wraps an off-the-shelf LLM with iterative feedback and in-context learning to red-team activation probes without requiring fine-tuning, gradients, or architectural access. The attacker LLM generates candidate adversarial samples across multiple rounds, receiving structured feedback (success/failure with brief reasons) after each round while maintaining full conversation history. This enables pattern refinement through ICL alone. The probe acts as a black-box classifier, and a separate LLM judge assigns ground-truth labels and checks scenario constraints. The approach is tested on high-stakes conversation classifiers trained on Llama-3.3-70B and Qwen3-8B, with attackers including GPT-5, smaller open models, and proprietary systems.

## Key Results
- GPT-5 attacker achieves 63.6% false negative and 88.0% false positive failure rates on high-stakes probes
- Iterative feedback with ICL shows clear improvement over rounds, with 0-shot vs 2nd-half learning gaps demonstrating effectiveness
- Smaller open-source models (27B–120B) find failure cases with lower rates and less stable learning curves
- Interpretable failure patterns emerge including legalese-induced false positives and procedural tone causing false negatives
- Scenario-constrained attacks show varied robustness across domains (medical, financial, legal, personal)
- Code is publicly released for replication and extension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback with in-context learning enables systematic discovery of probe failure modes without gradient access.
- Mechanism: An attacker LLM generates candidate adversarial samples across multiple rounds; after each round, it receives structured feedback (success/failure with brief reasons) and maintains full conversation history, allowing pattern refinement through ICL alone.
- Core assumption: The attacker model can synthesize failure explanations into improved strategies via prompt-based learning.
- Evidence anchors:
  - [abstract] "wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access"
  - [Section 2] "The full history is preserved, enabling in-context learning without finetuning or gradients. Each run starts cold (no seed examples)."
  - [corpus] Related work confirms ICL-based red-teaming trades peak success for simplicity and lower compute (Zhang et al., 2024; Zhou et al., 2025).
- Break condition: If attacker model lacks sufficient ICL capacity or feedback is uninformative, learning stalls. Smaller models (≤120B) show unstable learning curves and early strategy fixation.

### Mechanism 2
- Claim: Black-box probing surfaces interpretable, domain-grounded failure patterns that transfer across attacker capabilities.
- Mechanism: Treating the probe as a pure classifier f(x) → score forces attackers to discover input-space regularities (tone, framing, trigger words) rather than gradient-based exploits. These regularities often correspond to genuine conceptual blindspots.
- Core assumption: Failure patterns are discoverable through input-space search without architectural access.
- Evidence anchors:
  - [abstract] "legalese-induced false positives and bland procedural tone causing false negatives"
  - [Section 3.3] "false negative pattern... embedded high-stakes elements within everyday, seemingly low-stakes discussions. This subtlety is crucial in evading the 'high-stakes' classification"
  - [Section 3.3] One discovered FN received probe score of 0.05 despite involving a medical diagnosis discussion.
- Break condition: If probe failures require precise activation manipulations invisible to input-space search, black-box methods will underestimate vulnerability. Paper does not compare to white-box attack success rates directly.

### Mechanism 3
- Claim: LLM-based judges enable scalable ground-truth labeling but introduce labeling noise that affects failure rate estimates.
- Mechanism: A separate LLM (GPT-5 chat) assigns ground-truth labels and checks scenario constraints. This allows fully automated red-teaming loops but relies on judge reliability for the target concept.
- Core assumption: Judge model alignment with human concept boundaries is sufficient for identifying meaningful failures.
- Evidence anchors:
  - [Section 5] "we found that it broadly aligns with our intuition, but there are cases where we disagree with the generated label. This includes some administrative requests in high-stakes domains"
  - [Section A.5.1] Judge prompts ask for YES/NO followed by brief explanation, no calibrated confidence.
  - [corpus] Corpus evidence on automated labeling reliability is weak; no direct citations on LLM-as-judge error rates for high-stakes classification.
- Break condition: If judge systematically mislabels edge cases, reported failure rates conflate probe errors with judge errors. Paper recommends multiple runs and manual review to mitigate.

## Foundational Learning

- Concept: **Activation Probes**
  - Why needed here: The target being red-teamed is a linear classifier over internal activations, not over text. Understanding that probes operate in latent space explains why input-space attacks work via representation manipulation.
  - Quick check question: Can you explain why a probe with 0.91 AUROC on held-out data might still fail on adversarially-selected inputs?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire red-teaming scaffold relies on the attacker model improving via demonstration examples in its context window. Without ICL, the method would require training a separate attack model.
  - Quick check question: If you ran this scaffold with a model that has a 4K context window and 20 rounds of 5 samples each with feedback, would you hit context limits?

- Concept: **False Positive vs. False Negative Attack Asymmetry**
  - Why needed here: Results show FPR attacks succeed at much higher rates (88% vs 63.6% for GPT-5 2nd-half). Understanding why helps prioritize probe hardening.
  - Quick check question: Why might injecting "legalese" into low-stakes prompts cause false positives more easily than hiding high-stakes content causes false negatives?

## Architecture Onboarding

- Component map:
  - Attacker LLM -> JSONL generation -> Probe scoring -> Judge labeling -> Feedback return -> Attacker LLM
  - (Each component sequentially feeds into the next in a loop)

- Critical path:
  1. Configure attacker system prompt with target concept (high-stakes), attack direction (FN or FP), and optional scenario constraints.
  2. Run R rounds (default 20) of batch generation (default batch_size=5).
  3. Per batch: parse JSONL → run probe → query judge → format feedback → append to context.
  4. After R rounds: request strategy summary; manually review result files for actionable patterns.

- Design tradeoffs:
  - **Attacker capability vs. cost**: GPT-5 achieves 2nd-half FPR of 88% but requires proprietary API; Gemma-3-27B achieves 21.6% FPR but runs locally. Paper recommends multiple smaller-model runs over single GPT-5 run for exploration diversity.
  - **Scenario constraints vs. attack success**: Adding domain constraints (medical, financial) drops GPT-5 FPR from 67.6% to 31.8% but surfaces deployment-relevant failures.
  - **Judge reliability vs. automation**: Fully automated loops scale but introduce labeling noise; manual spot-checking recommended.

- Failure signatures:
  - **Legalese-induced FPs**: Low-stakes content with prominent legal boilerplate (DMCA, arbitration clauses) triggers high-stakes classification.
  - **Procedural tone FNs**: High-stakes admin tasks described in neutral, checklist-style language evade detection (probe score 0.05 in one medical example).
  - **Trigger word sensitivity**: Terms like "suspend," "eviction," "IRS" spike probe scores even in low-stakes contexts.
  - **Embedded stakes**: High-stakes elements hidden within everyday framing (hobby discussion masking diagnosis coping).

- First 3 experiments:
  1. **Baseline FN/FPR rates**: Run red-teaming on your probe without scenario constraints using strongest available attacker; establish 0-shot vs 2nd-half learning gap. This quantifies ICL effectiveness.
  2. **Scenario-constrained stress test**: Run with deployment-relevant constraints (e.g., your actual use case domain); compare failure rate drop to quantify constraint hardness.
  3. **Cross-probe transfer**: If running multiple probe architectures (attention vs softmax), test whether failure patterns transfer; this reveals whether vulnerabilities are probe-specific or conceptual.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the failure cases discovered via this red-teaming scaffold be used to automatically harden probes through re-training or fine-tuning?
- Basis in paper: [explicit] The authors state in Future Work: "We want to analyze whether the discovered failure are useful for improving the probes. We envision an automated probe improvement pipeline which repeats cycles of red-teaming... and then fixing..."
- Why unresolved: The current study focuses solely on identifying and categorizing failure modes rather than implementing or testing a remediation loop.
- What evidence would resolve it: Demonstrating that incorporating adversarial samples generated by this method into the probe's training set reduces the failure rate against future adversarial attacks without degrading general performance.

### Open Question 2
- Question: Do activation probes on larger and more capable models retain interpretable failure modes under adversarial pressure?
- Basis in paper: [explicit] The authors ask: "It is unclear whether activation probes on larger and more capable models still exhibit interpretable failure modes, which is something worth investigating empirically."
- Why unresolved: The experiments were constrained to Llama-3.3-70B and Qwen3-8B, leaving the vulnerability of probes on frontier models unknown.
- What evidence would resolve it: Replicating the red-teaming methodology on models significantly larger than 70B parameters and analyzing the semantic interpretability of the resulting failures.

### Open Question 3
- Question: How does the robustness of activation probes compare to that of prompted or fine-tuned LLM classifiers under identical black-box adversarial pressure?
- Basis in paper: [explicit] The authors suggest: "It would also be interesting to apply our approach to classifiers other than probes, such as prompted or fine-tuned LLMs. This would put failure rates into perspective..."
- Why unresolved: The paper establishes a failure baseline for activation probes but lacks a comparative baseline for alternative monitoring architectures.
- What evidence would resolve it: Running the same red-teaming scaffold against prompted monitors and fine-tuned classifiers, then comparing the failure rates (FPR/FNR) against those of the activation probes.

### Open Question 4
- Question: Does this training-free scaffold generalize effectively to "model-centric" concepts that require on-policy samples (e.g., situational awareness)?
- Basis in paper: [inferred] The authors note in Limitations that for model-centric concepts, "one might be primarily interested in on-policy samples... which can make the red-teaming task more challenging... and require some modifications to our pipeline."
- Why unresolved: The study only validates the approach on "high-stakes" interactions (a conversation property), leaving the efficacy for model-internal states unproven.
- What evidence would resolve it: Adapting the scaffold to generate user prompts that elicit specific internal model states (e.g., deception) and measuring the red-teamer's success rate in triggering probe failures.

## Limitations
- The method relies on proprietary models (GPT-5) for both attacker and judge roles, making full replication difficult
- Ground-truth labeling by an LLM judge without calibrated confidence introduces potential systematic bias
- The approach does not establish baseline for white-box attacks, leaving uncertainty about whether black-box methods underestimate vulnerability
- The study is limited to specific probe architectures (attention and softmax at particular layers), leaving transferability to other designs unexplored

## Confidence
- **High confidence**: The core methodological contribution of using iterative feedback with ICL for black-box red-teaming is well-supported by experimental results and clear mechanism descriptions
- **Medium confidence**: The quantitative failure rates (63.6% FNR, 88.0% FPR for GPT-5) are methodologically sound but depend on judge reliability and may not generalize to all probe architectures or domains
- **Low confidence**: The claim that probes should not be sole control points in high-incentive settings is a policy recommendation extending beyond experimental evidence

## Next Checks
1. **White-box baseline comparison**: Run gradient-based attacks against the same probes to establish whether black-box failure rates underestimate true vulnerability
2. **Judge reliability calibration**: Implement calibrated confidence scoring for the LLM judge and validate against human-labeled edge cases
3. **Cross-probe transferability study**: Test the same attacker against multiple probe architectures to determine whether failure patterns are probe-specific or reflect fundamental limitations