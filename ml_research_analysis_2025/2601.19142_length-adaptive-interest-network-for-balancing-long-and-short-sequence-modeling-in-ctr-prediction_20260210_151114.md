---
ver: rpa2
title: Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling
  in CTR Prediction
arxiv_id: '2601.19142'
source_url: https://arxiv.org/abs/2601.19142
tags:
- length
- sequence
- attention
- user
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical performance imbalance in long-sequence
  CTR models, where increasing maximum sequence length improves results for long-history
  users but degrades performance for short-history users due to attention polarization
  and length signal deficiency. The authors propose LAIN (Length-Adaptive Interest
  Network), a plug-and-play framework that explicitly conditions CTR models on sequence
  length through three components: a Spectral Length Encoder that maps length to continuous
  embeddings, Length-Conditioned Prompting that injects length-aware prompts into
  behavior sequences, and Length-Modulated Attention that adjusts attention sharpness
  based on sequence length.'
---

# Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction

## Quick Facts
- arXiv ID: 2601.19142
- Source URL: https://arxiv.org/abs/2601.19142
- Reference count: 17
- This paper proposes LAIN to address performance imbalance in long-sequence CTR models, achieving up to 1.15% AUC gain and 2.25% log loss reduction

## Executive Summary
This paper identifies a critical performance imbalance in long-sequence CTR models, where increasing maximum sequence length improves results for long-history users but degrades performance for short-history users due to attention polarization and length signal deficiency. The authors propose LAIN (Length-Adaptive Interest Network), a plug-and-play framework that explicitly conditions CTR models on sequence length through three components: a Spectral Length Encoder that maps length to continuous embeddings, Length-Conditioned Prompting that injects length-aware prompts into behavior sequences, and Length-Modulated Attention that adjusts attention sharpness based on sequence length. Experiments on three real-world datasets across five CTR backbones show LAIN achieves up to 1.15% AUC gain and 2.25% log loss reduction. Notably, LAIN significantly improves accuracy for short-sequence users (up to 1.08% AUC gain, 2.17% log loss reduction) while maintaining performance for long-sequence users, addressing the fundamental challenge of length imbalance in sequential recommendation systems.

## Method Summary
LAIN adds three components to base CTR models to explicitly condition on sequence length: (1) Spectral Length Encoder using Fourier projection with learnable frequencies, (2) Length-Conditioned Prompting generating k=4 prompt tokens from length embeddings, and (3) Length-Modulated Attention with temperature scaling. The framework is evaluated on three public datasets (KuaiVideo, MicroVideo1.7M, EBNeRD-small) with max sequence length 1000 and embedding dimension 64, using Adam optimizer (lr=0.001), dropout=0.2, and early stopping on validation AUC.

## Key Results
- LAIN achieves up to 1.15% AUC gain and 2.25% log loss reduction across five CTR backbones
- Short-sequence users see significant improvements (up to 1.08% AUC gain, 2.17% log loss reduction)
- Long-sequence user performance is maintained while short-sequence performance improves
- LAIN reduces attention Gini coefficient from ~0.35 to ~0.32, mitigating polarization

## Why This Works (Mechanism)

### Mechanism 1
Explicit length-conditioning via prompts decouples gradient conflicts between short- and long-sequence user optimization. The LAIN framework generates length-specific prompt tokens P(L) via MLP(h_len) that are prepended to behavior sequences. This decomposes parameters into shared (θ_shared) and adaptive (θ_prompt(L)) components, redirecting conflicting gradient signals (∇L_short ∝ smoothness vs. ∇L_long ∝ sparsity) into separate optimization spaces. Core assumption: Short-sequence users benefit from distributed attention while long-sequence users require sparse, concentrated attention patterns. Evidence: [abstract] "explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling" and [section: Method] Equations 5-6 show the decomposition of θ into shared and length-conditioned components. Break condition: If gradient conflicts between length groups do not exist (inner products ≥ 0), prompt-based decoupling provides no benefit.

### Mechanism 2
Temperature scaling in softmax attention mitigates attention polarization for short sequences. LAIN computes τ = 1 + sigmoid(-β(L - L₀))·γ where L₀ is mean sequence length. For short L, τ increases (e.g., >1), smoothing the softmax distribution and reducing over-concentration on single items. For long L, τ decreases toward 1, preserving sharp focus on salient behaviors. Core assumption: Attention polarization (high Gini coefficient) harms short-sequence modeling by underutilizing limited behavioral signals. Evidence: [abstract] "Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length" and [section: Empirical Analysis] Figure 2 and Table 5 show Gini coefficient increases with sequence length; LAIN reduces Gini from ~0.35 to ~0.32. Break condition: If temperature scaling overshoots (τ too large), attention becomes uniform and loses discriminative power entirely.

### Mechanism 3
Fourier-based spectral encoding captures continuous length semantics better than discrete binning. The Spectral Length Encoder computes f_fourier = [sin(L·ω); cos(L·ω)] with learnable frequencies ω, followed by MLP projection. This avoids hard boundaries in length representation and enables smooth interpolation across the length spectrum. Core assumption: Sequence length carries latent user state information that benefits from continuous rather than categorical encoding. Evidence: [section: Method] Equation 7-8 define the Fourier projection and MLP transformation and [section: Empirical Analysis] Table 1 shows behavioral characteristics (click rate, stability, diversity) vary systematically with sequence length. Break condition: If length is not informative beyond behavioral content, the spectral encoder adds noise without signal.

## Foundational Learning

- **Concept: Attention Polarization**
  - Why needed here: The paper diagnoses softmax attention over-concentration as a root cause of short-sequence performance degradation.
  - Quick check question: Can you explain why softmax over 10 tokens produces more uniform weights than softmax over 1000 tokens, given the same raw scores?

- **Concept: Temperature Scaling in Softmax**
  - Why needed here: LAIN's core intervention modulates softmax temperature dynamically based on sequence length.
  - Quick check question: Given softmax(z/τ), what happens to the distribution as τ → ∞? As τ → 0?

- **Concept: Two-Stage Long-Sequence CTR Architecture (GSU + ESU)**
  - Why needed here: LAIN integrates into existing long-sequence frameworks (SIM, TWIN, SDIM) as a plug-and-play module.
  - Quick check question: In a GSU-ESU pipeline, which stage retrieves candidate behaviors and which stage applies target-aware attention?

## Architecture Onboarding

- **Component map:** Raw Length L → Spectral Length Encoder → h_len (d-dim) → (Length-Conditioned Prompting OR Length-Modulated Attention) → Modified sequences/attention → Backbone CTR Model → CTR Prediction

- **Critical path:** SLE → h_len generation → (LCP prompt injection OR LMA attention modulation). Both paths must receive valid h_len; attention computation must use modified Q', K', τ.

- **Design tradeoffs:**
  - k (prompt tokens): Higher k increases expressivity but adds O((L+k)²) attention cost. Paper uses k=4.
  - Fourier dimension d_f: Higher d_f captures finer length distinctions. Paper uses d_f=32.
  - Temperature scaling parameters (γ, β): γ controls max temperature boost; β controls transition sharpness around L₀.

- **Failure signatures:**
  - Short-sequence AUC drops below baseline → check if temperature scaling is inverted (τ decreasing for short L)
  - No improvement on any length bucket → verify h_len is not collapsed to constant (check MLP gradients)
  - Attention weights become uniform (Gini ≈ 0) → τ may be too large; reduce γ initialization
  - Inference latency spikes >5% → check prompt tokens are prepended efficiently, not triggering re-allocation

- **First 3 experiments:**
  1. Ablation by component: Train LAIN variants removing LCP, LMA (temperature only), LMA (Q-K conditioning only). Compare Table 6 pattern—expect LMA removal to cause largest drop.
  2. Length-stratified evaluation: Bucket test users by sequence length (<100, 100-200, 200+). Verify short-sequence gains (target: +1% AUC per Table 4) without long-sequence degradation.
  3. Attention distribution analysis: Compute Gini coefficient on validation set attention weights with/without LAIN. Target: reduction from ~0.35 to ~0.32 as in Table 5.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LAIN's length-adaptive attention mechanisms be effectively generalized to sequential recommendation tasks with ranking-based losses (e.g., BPR) rather than binary CTR prediction? Basis: The paper evaluates LAIN exclusively on Click-Through Rate (CTR) prediction tasks using binary cross-entropy loss. Unresolved because while CTR is common, the authors do not demonstrate if the length-conditioned temperature scaling and prompting are compatible with pairwise ranking objectives or generative next-item prediction tasks. What evidence would resolve it: Experimental results applying LAIN to standard sequential recommendation benchmarks (e.g., MovieLens, Amazon) optimized for metrics like NDCG or Hit Rate.

- **Open Question 2:** Is the specific parametric sigmoid function used for temperature scaling optimal for all length distributions, or could a learned non-parametric approach yield better adaptation? Basis: The Length-Modulated Attention (LMA) defines temperature $\tau$ using a fixed formula $\tau = 1 + \text{sigmoid}(-\beta(L - L_0)) \cdot \gamma$. Unresolved because the fixed formula assumes a specific smooth transition in attention sharpness relative to length, and it is unclear if this manually designed curve fits all data distributions or if it imposes an inductive bias that limits performance on datasets with irregular length distributions. What evidence would resolve it: An ablation study comparing the current sigmoid-based temperature scaling against a fully learned temperature network (non-parametric) to determine if the functional form is a bottleneck.

- **Open Question 3:** How does LAIN interact with other attention-based debiasing techniques, such as position bias or popularity bias mitigation? Basis: LAIN is designed as a plug-and-play module that modifies attention weights via temperature scaling and query-key conditioning. Unresolved because real-world industrial systems often employ other "plug-and-play" attention debiasing modules (e.g., correcting for item position), and the paper does not analyze potential conflicts or gradient interference if multiple modules simultaneously modulate the attention matrix. What evidence would resolve it: Experiments combining LAIN with existing debiasing frameworks (like position bias correction) to verify if the improvements are additive or if the attention mechanisms interfere with one another.

## Limitations

- The core mechanism linking sequence length to attention polarization lacks rigorous mathematical derivation, relying on correlation rather than proven causation.
- Implementation dependencies create significant reproducibility concerns due to framework-specific integration (FuxiCTR) and unspecified two-stage architecture configurations.
- The spectral length encoder's effectiveness rests on weakly supported assumptions about continuous length semantics, with no ablation comparing it against simpler alternatives.

## Confidence

- **High confidence:** The experimental methodology and dataset integrity. The paper uses established public datasets, standard CTR metrics (AUC, LogLoss), and provides sufficient implementation details for the core LAIN components.
- **Medium confidence:** The claim that LAIN specifically improves short-sequence user performance without degrading long-sequence performance. While Table 4 shows this pattern, the analysis does not rule out other factors like regularization effects.
- **Low confidence:** The mechanistic claims about why LAIN works. The attention polarization diagnosis, gradient conflict hypothesis, and Fourier encoding rationale are all logically coherent but lack rigorous validation.

## Next Checks

1. **Gradient conflict validation:** Compute and visualize gradients of loss with respect to shared parameters when optimizing for short- vs. long-sequence user batches. Verify that these gradients have opposite signs or are otherwise in conflict, demonstrating the need for prompt-based decoupling.

2. **Ablation of length encoding methods:** Replace the Fourier spectral encoder with (a) simple scalar length embedding, (b) sinusoidal positional encoding, and (c) learned categorical length bins. Compare performance to isolate whether continuous spectral encoding provides unique benefits beyond basic length representation.

3. **Attention distribution analysis by length bucket:** For each sequence length bucket (<100, 100-200, 200-500, >500), compute the entropy and Gini coefficient of attention weights on held-out validation data. Demonstrate that LAIN specifically reduces polarization (increases entropy) for short sequences while maintaining focus for long sequences, and that this pattern correlates with performance improvements.