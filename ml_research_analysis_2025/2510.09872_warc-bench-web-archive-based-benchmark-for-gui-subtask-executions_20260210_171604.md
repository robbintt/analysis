---
ver: rpa2
title: 'WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions'
arxiv_id: '2510.09872'
source_url: https://arxiv.org/abs/2510.09872
tags:
- agent
- action
- tasks
- arc-bench
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WARC-Bench, a novel benchmark for evaluating
  GUI subtask execution using Web Archive files to create realistic, interactive web
  environments. The benchmark contains 438 tasks testing short-horizon interactions
  like date selection, form filling, and information extraction.
---

# WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions

## Quick Facts
- arXiv ID: 2510.09872
- Source URL: https://arxiv.org/abs/2510.09872
- Reference count: 24
- Primary result: Introduces WARC-Bench benchmark testing short-horizon GUI interactions, showing state-of-the-art models struggle with subtask execution

## Executive Summary
This paper introduces WARC-Bench, a novel benchmark for evaluating GUI subtask execution using Web Archive files to create realistic, interactive web environments. The benchmark contains 438 tasks testing short-horizon interactions like date selection, form filling, and information extraction. Evaluation shows state-of-the-art models struggle with subtask execution, with Claude-4.0-Sonnet achieving only 64.8% success rate. To address this, the authors fine-tune Qwen2.5-VL models using supervised fine-tuning and reinforcement learning with verifiable rewards, achieving 52.8% accuracy—outperforming many frontier models. The work highlights that mastering subtasks is crucial for robust web navigation and provides a scalable, sandboxed environment for evaluating this capability.

## Method Summary
The authors created WARC-Bench by leveraging Web Archive files to construct realistic, interactive web environments for evaluating GUI subtask execution. The benchmark includes 438 tasks focused on short-horizon interactions such as date selection, form filling, and information extraction. They evaluated state-of-the-art models and found significant performance limitations. To improve performance, they fine-tuned Qwen2.5-VL models using supervised fine-tuning followed by reinforcement learning with verifiable rewards, creating a sandboxed environment for safe evaluation.

## Key Results
- State-of-the-art models achieve only 64.8% success rate on WARC-Bench tasks
- Fine-tuned Qwen2.5-VL models achieve 52.8% accuracy, outperforming many frontier models
- Benchmark demonstrates that subtask execution remains challenging for current AI systems

## Why This Works (Mechanism)
The approach works by creating a controlled, reproducible environment using Web Archive files that captures the complexity of real web interactions while maintaining safety and scalability. By focusing on subtasks rather than complete workflows, the benchmark isolates specific interaction challenges that models must overcome. The combination of supervised fine-tuning and reinforcement learning with verifiable rewards allows models to learn from both human demonstrations and their own successful interactions, creating a more robust learning process.

## Foundational Learning

**Web Archive Files**: Capture complete web pages including HTML, CSS, and JavaScript states. Why needed: Provides consistent, reproducible test environments. Quick check: Verify archive loads correctly and interactions produce expected results.

**GUI Subtask Execution**: Breaking down complex web interactions into discrete, testable units. Why needed: Allows focused evaluation of specific interaction capabilities. Quick check: Ensure each subtask has clear success criteria.

**Reinforcement Learning with Verifiable Rewards**: Training models using rewards based on observable success metrics. Why needed: Enables autonomous learning without human labeling. Quick check: Confirm reward signals align with actual task completion.

**Sandboxed Environment**: Isolated execution space for safe web interaction testing. Why needed: Prevents real-world side effects during model evaluation. Quick check: Test sandbox boundaries and safety constraints.

## Architecture Onboarding

**Component Map**: Web Archive Files -> Interactive Environment -> Task Execution Engine -> Reward Function -> Model Trainer

**Critical Path**: Archive loading → Environment rendering → User instruction parsing → Action selection → DOM manipulation → Success verification → Reward calculation

**Design Tradeoffs**: Focused on short-horizon tasks for scalability versus real-world multi-step workflows; prioritized safety and reproducibility over complete realism.

**Failure Signatures**: Incorrect DOM element selection, failure to parse complex layouts, inability to handle dynamic content loading, reward signal misinterpretation.

**First 3 Experiments**: 1) Baseline model performance on simple form-filling tasks, 2) Fine-tuning impact on specific subtask categories, 3) Comparison of supervised vs reinforcement learning approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses exclusively on short-horizon tasks (1-2 steps), not representing complex multi-step workflows
- 438-task corpus represents a relatively small and potentially narrow slice of possible web interactions
- Fine-tuning approach achieves only 52.8% accuracy, suggesting core challenges remain unaddressed

## Confidence
- Benchmark design and implementation: High
- Performance results accuracy: High
- Generalization to real-world scenarios: Medium
- Fine-tuning approach effectiveness: Medium

## Next Checks
1. Expand the benchmark to include multi-step workflows and longer-horizon tasks to better assess real-world applicability
2. Test additional model architectures and training approaches beyond the Qwen2.5-VL fine-tuning to determine if alternative methods yield better performance
3. Evaluate the benchmark's tasks against actual user interaction patterns from web analytics data to ensure task relevance and coverage