---
ver: rpa2
title: What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup
  Puzzles
arxiv_id: '2508.10358'
source_url: https://arxiv.org/abs/2508.10358
tags:
- agent
- reasoning
- information
- questioner
- responder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TurtleSoup-Bench, a large-scale interactive
  benchmark designed to evaluate the imaginative reasoning capabilities of large language
  models (LLMs). Unlike existing benchmarks that focus on static question-answering
  or social deduction, TurtleSoup-Bench challenges models to iteratively construct,
  test, and revise hypotheses in information-sparse environments using 800 narrative
  puzzles.
---

# What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles

## Quick Facts
- arXiv ID: 2508.10358
- Source URL: https://arxiv.org/abs/2508.10358
- Authors: Mengtao Zhou; Sifan Wu; Huan Zhang; Qi Sima; Bang Liu
- Reference count: 22
- One-line primary result: Current LLMs significantly underperform humans on imaginative reasoning tasks, achieving 58% vs 68% in a novel interactive benchmark.

## Executive Summary
This paper introduces TurtleSoup-Bench, a large-scale interactive benchmark designed to evaluate the imaginative reasoning capabilities of large language models (LLMs). Unlike existing benchmarks that focus on static question-answering or social deduction, TurtleSoup-Bench challenges models to iteratively construct, test, and revise hypotheses in information-sparse environments using 800 narrative puzzles. To support this, the authors propose Mosaic-Agent, a multi-agent framework that simulates the reasoning process through a questioner agent, a responder agent, and a memory module. Evaluation is performed using a multi-dimensional protocol that assesses logical consistency, detail fidelity, and conclusion alignment. Experiments show that leading LLMs, including GPT-4o and Claude-3.7-Sonnet, significantly underperform human experts, achieving overall scores of around 58% versus 68% for humans. The results highlight the limitations of current models in exploratory reasoning and establish a foundation for future research in this area.

## Method Summary
The paper introduces TurtleSoup-Bench, a large-scale interactive benchmark designed to evaluate the imaginative reasoning capabilities of large language models (LLMs). Unlike existing benchmarks that focus on static question-answering or social deduction, TurtleSoup-Bench challenges models to iteratively construct, test, and revise hypotheses in information-sparse environments using 800 narrative puzzles. To support this, the authors propose Mosaic-Agent, a multi-agent framework that simulates the reasoning process through a questioner agent, a responder agent, and a memory module. Evaluation is performed using a multi-dimensional protocol that assesses logical consistency, detail fidelity, and conclusion alignment. Experiments show that leading LLMs, including GPT-4o and Claude-3.7-Sonnet, significantly underperform human experts, achieving overall scores of around 58% versus 68% for humans. The results highlight the limitations of current models in exploratory reasoning and establish a foundation for future research in this area.

## Key Results
- Current LLMs (GPT-4o, Claude-3.7-Sonnet, Gemini-2.5-Flash) achieve only 57.14% overall score on TurtleSoup-Bench, significantly underperforming human experts at 68.17%.
- The Key Clue mechanism is critical for performance, with its removal causing the most severe drop (57.14% → 46.73%).
- Performance varies dramatically across narrative genres, with models scoring 62.9% on "Constant Change" but only 45.5% on "Clever Logic."
- Ablation studies confirm the importance of all components: removing Meta-cognition drops score to 51.07%, and removing Deliberation drops it to 49.05%.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Local-to-Global Deliberation
- Claim: A reasoning architecture that separates local analysis of recent question-answer pairs from periodic global synthesis improves hypothesis coherence in long-horizon exploration tasks.
- Mechanism: The Deliberation Agent processes immediate Q&A pairs for quick reaction (local analysis) and periodically aggregates the full interaction history and discovered key clues into a structured Belief State (global synthesis). This prevents cognitive myopia by forcing explicit integration of all evidence.
- Core assumption: Models struggle to maintain coherent long-term reasoning chains purely through implicit context, and explicit structured state updates improve performance.
- Evidence anchors:
  - [abstract] The benchmark evaluates the dynamic, iterative process of hypothesis generation, testing, and belief updating.
  - [section 4.1] The core of this process is updating its internal Belief State, Bt... This progressive, local-to-global design is manifested in...
  - [corpus] VAGEN (arXiv:2510.16907) explores world model reasoning for multi-turn agents, suggesting explicit state modeling aids multi-step reasoning.
- Break condition: Performance gains disappear if the global synthesis interval k is too large (causing coherence loss) or if the Belief State structure fails to capture key dependencies.

### Mechanism 2: Meta-Cognitive Strategy Adaptation via Smoothed Confidence
- Claim: Dynamic strategy switching based on narrative genre classification, stabilized by a confidence smoothing mechanism, reduces unproductive exploration paths.
- Mechanism: A Meta-cognition Agent classifies the puzzle's genre using a three-vote majority. To prevent strategy oscillation from occasional misclassifications, confidence is updated via an Exponential Moving Average (`csmooth_t = alpha * c_{t-1} + (1-alpha) * v_c`), and a strategy switch occurs only if `csmooth_t > c_{t-1} + tau_switch`.
- Core assumption: Different narrative genres benefit from distinct questioning strategies, and a stable strategic direction is more valuable than frequent, potentially incorrect, adjustments.
- Evidence anchors:
  - [section 4.1] A static strategy is brittle in complex exploration... An occasional misclassification can cause severe strategy oscillation... To prevent this, we use a Smoothed Confidence mechanism.
  - [section 5.4] Ablation shows removing Meta-cognition drops score (57.14 -> 51.07).
  - [corpus] PromptHelper (arXiv:2601.15575) suggests user prompt exploration benefits from structured guidance, analogous to strategy-driven questioning.
- Break condition: Effectiveness hinges on the quality of genre-specific strategies; smoothing fails if misclassifications are systematically biased in one direction.

### Mechanism 3: High-Signal Feedback via Key Clue Identification
- Claim: Providing an explicit, high-signal indicator (the `<Key Clue>` tag) when an agent's question hits a pivotal point dramatically improves exploration efficiency.
- Mechanism: The Responder agent, grounded in a human-annotated Key Clue Library (`K_lib`), flags answers with `<Key Clue>` if the question's semantics align with a predefined pivotal clue. This filtered signal is stored in a separate memory pool (`K_rec`), allowing the Questioner to prioritize high-value information.
- Core assumption: A primary bottleneck in exploratory reasoning is distinguishing signal from noise in feedback, and explicit signaling can bypass this bottleneck.
- Evidence anchors:
  - [section 4.2] To better guide the Questioner's reasoning and help it recognize breakthroughs, we introduce the boolean flag ft.
  - [section 5.4] Without the Key Clue mechanism... causes the most severe performance collapse (57.14 -> 46.73). This tag acts as a high signal-to-noise supervisory signal.
  - [corpus] Corpus evidence for this specific mechanism is weak; no directly comparable "hint-flag" system was found in neighbor papers.
- Break condition: The mechanism is brittle if the Key Clue Library is incomplete or if the semantic matching for flagging is inaccurate (false positives/negatives). The ablation shows it is a prerequisite for other modules to function.

## Foundational Learning

- Concept: Abductive vs. Deductive Reasoning
  - Why needed here: The paper defines imaginative reasoning as an iterative loop of abductive (forming plausible hypotheses from sparse observations) and deductive (testing them) logic. Without understanding this distinction, the agent's cycle of forming Belief States and generating test questions is opaque.
  - Quick check question: Given a shattered vase and a cat nearby, is "the cat knocked it over" an abduction or a deduction? (Answer: Abduction - a plausible hypothesis, not a logically certain conclusion).

- Concept: Exploratory vs. Verification-based Evaluation
  - Why needed here: The paper critiques prior benchmarks (like TurtleBench) for being static and verification-based. The core innovation is measuring the *process* of exploration (Logic Accuracy, Detail Fidelity) not just the final outcome.
  - Quick check question: If an agent asks 50 questions to solve a puzzle, is measuring only whether the final answer is correct a sufficient evaluation of its reasoning capability? (Answer: No, this is outcome-based and ignores the efficiency and path of exploration).

- Concept: LLM-as-a-Judge for Semantic Evaluation
  - Why needed here: The automated evaluation protocol uses an LLM (deepseek-r1) to score semantic similarity between predicted and ground-truth logic/details, with specific thresholds for calibration. Understanding this is critical for interpreting the experimental results.
  - Quick check question: Why use a two-threshold calibration (validity at 0.5, high-confidence at 0.8) instead of a single threshold? (Answer: To filter out weak matches while normalizing strong paraphrases, making the metric more robust to wording variations).

## Architecture Onboarding

- Component map:
  1. **Memory Module**: Central hub with two components: Interaction History (`H_t`) and Key Clue Records (`K_rec`).
  2. **Questioner Agent**: The core explorer, composed of:
     - **Deliberation Agent**: Local analysis and global synthesis to update Belief State (`B_t`) and generate Analysis & Proposal Set (APS).
     - **Meta-cognition Agent**: Classifies genre and sets high-level strategy with smoothed confidence.
     - **Action Formulation Agent**: Generates candidate questions and selects the optimal one.
  3. **Responder Agent**: The "God" environment that answers Yes/No/Unknown and flags key clues based on `S_bot` and `K_lib`.
  4. **Evaluator**: An LLM-based judge that scores Logic, Details, and Conclusion against ground truth.

- Critical path:
  1. Responder provides answer and flag -> Memory Module stores it.
  2. Deliberation Agent reads Memory -> Updates Belief State -> Generates APS.
  3. Meta-cognition Agent reads Memory & Belief State -> Classifies genre -> Selects strategy.
  4. Action Formulation Agent takes APS + Strategy -> Generates and screens candidate questions -> Outputs single question.
  5. This question becomes the next input to the Responder.

- Design tradeoffs:
  - **Key Clue Annotation**: Requires costly, expert human annotation for every puzzle. The paper uses it to isolate reasoning capability; its absence causes a major performance drop.
  - **Symmetric Models**: Using the same LLM for both Questioner and Responder simplifies setup but conflates reasoning failures with comprehension failures (Section 5.2).
  - **Static Max Turns**: Limiting games to 30 turns provides a budget but may truncate complex reasoning paths.

- Failure signatures:
  - **Semantic Fixation**: The model fixates on a word's common meaning, ignoring context (e.g., interpreting "kill" only as physical murder).
  - **Deductive Pruning Failure**: The agent fails to abandon a hypothesis after receiving multiple "No" answers, asking redundant questions.
  - **Context Construction Failure**: The agent fails to integrate individual clues into a coherent global narrative.
  - **Logic Blind Spots**: The model cannot conceive of atypical, "out-of-distribution" causal chains or motivations.

- First 3 experiments:
  1. **Baseline Run**: Run the Mosaic-Agent on TurtleSoup-Bench using a base model (e.g., deepseek-r1) to establish the overall score and reproduce the human-model gap.
  2. **Ablation Study**: Systematically remove the Deliberation, Meta-cognition, and Key Clue components one by one to validate their individual contributions and reproduce the paper's ablation table.
  3. **Failure Analysis**: Perform a qualitative analysis of agent traces, categorizing errors into the paper's four failure paradigms (Semantic Fixation, Context Construction, Logic Blind Spots, Deductive Pruning) to identify the model's primary weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs develop imaginative reasoning strategies that generalize across diverse narrative paradigms, or is their current capability fundamentally specialized and genre-dependent?
- Basis in paper: [explicit] The quantitative analysis notes "model performance correlates strongly with the narrative paradigm" and that "current LLM imagination is not a general but a collection of specialized skills optimized for specific tasks, with limited generalization."
- Why unresolved: The study shows dramatic performance variations across genres (e.g., gemini-2.5-flash scores 62.9% on "Constant Change" but drops to 45.5% on "Clever Logic"), but does not test whether models can transfer learned strategies between genres.
- What evidence would resolve it: Cross-genre transfer experiments where models trained on one narrative paradigm are tested on held-out paradigms, with analysis of whether questioning strategies transfer or remain paradigm-specific.

### Open Question 2
- Question: What representational or architectural innovations would enable LLMs to perform effective deductive pruning and systematic hypothesis elimination in exploratory reasoning tasks?
- Basis in paper: [explicit] The error analysis identifies "Deductive Pruning Failure" as a fundamental deficit where "the model ineffectively uses negative feedback to systematically eliminate falsified branches of the possibility space" and "continues to explore an already falsified path."
- Why unresolved: While the paper identifies this failure mode, it does not propose or test mechanisms (e.g., explicit belief-state representations, counterfactual reasoning modules, or structured hypothesis tracking) that could enforce rigorous deductive pruning.
- What evidence would resolve it: Ablation studies comparing baseline LLMs against variants with explicit hypothesis-elimination modules, measuring reduction in redundant questions and improvement in exploration efficiency metrics (e.g., information gain per question).

### Open Question 3
- Question: How can imaginative reasoning capabilities be developed or evaluated in settings where high-quality environmental feedback signals (like the Key Clue mechanism) are absent or noisy?
- Basis in paper: [explicit] The ablation study shows the Key Clue mechanism's removal causes "the most severe performance collapse (57.14 → 46.73)" and concludes that "sophisticated modules like Deliberation and Meta-cognition are ineffective without the high-quality information stream provided by the Key Clue mechanism."
- Why unresolved: Real-world exploration often lacks explicit guidance; the dependency on curated feedback raises questions about whether current architectural improvements address the core challenge or merely compensate for artificially informative environments.
- What evidence would resolve it: Experiments comparing agent performance with varying feedback quality (from full key clues to standard yes/no answers to noisy/ambiguous responses), analyzing whether deliberative architectures remain beneficial or become detrimental under feedback degradation.

## Limitations
- The TurtleSoup-Bench dataset is not publicly available, preventing independent verification of the puzzles, key clue annotations, and human baseline results.
- The symmetric model setup (same LLM for Questioner and Responder) introduces confounding between reasoning capability and comprehension ability.
- The paper relies heavily on LLM-as-a-judge evaluation, which may introduce systematic biases despite two-threshold calibration.

## Confidence
- **High Confidence**: The core claim that current LLMs underperform humans on imaginative reasoning tasks is supported by the multi-dimensional evaluation protocol and ablation studies. The performance gap (58% vs 68%) is consistent across metrics.
- **Medium Confidence**: The proposed Mosaic-Agent framework's superiority over baselines is demonstrated, but the lack of public dataset and exact prompt details limits independent validation of the claimed improvements (e.g., 57.14 → 51.07 without Meta-cognition).
- **Low Confidence**: The generalizability of findings to broader imaginative reasoning domains is uncertain, as the benchmark is narrowly focused on narrative puzzles with specific genre classifications.

## Next Checks
1. **Recreate a Minimal Benchmark**: Construct a small set of 10-20 narrative puzzles following the paper's genre structure and key clue annotation guidelines. Test whether the performance gap between current LLMs and human reasoning emerges even at this scale.
2. **Ablation Replication**: Implement the three core components (Deliberation Agent, Meta-cognition Agent, Key Clue mechanism) and systematically remove each to verify whether performance drops match the reported percentages (51.07, 46.73, etc.).
3. **Judge Correlation Study**: Conduct a small human evaluation study (5-10 puzzles) comparing LLM judge scores against human rater scores to validate the semantic similarity thresholds and assess potential systematic biases in the automated evaluation protocol.