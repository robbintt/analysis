---
ver: rpa2
title: Capturing the Effects of Quantization on Trojans in Code LLMs
arxiv_id: '2505.14200'
source_url: https://arxiv.org/abs/2505.14200
tags:
- code
- quantization
- llama
- precision
- payload
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effects of quantization on trojan attacks
  in large language models of code. The authors evaluate Llama-2-7b and CodeLlama-7b
  on a text-to-SQL generation task, applying quantization at different precision levels
  (full, 8-bit, and 4-bit) before inference.
---

# Capturing the Effects of Quantization on Trojans in Code LLMs

## Quick Facts
- arXiv ID: 2505.14200
- Source URL: https://arxiv.org/abs/2505.14200
- Reference count: 38
- Key outcome: Quantization at 4-bit precision reduces attack success rates for CodeLlama but not Llama-2, while also affecting payload signal strength differently between the two models.

## Executive Summary
This paper investigates how quantization affects trojan attacks in code-focused large language models. The authors evaluate Llama-2-7b and CodeLlama-7b on a text-to-SQL generation task, applying quantization at full, 8-bit, and 4-bit precision before inference. They introduce a novel "payload signal strength" metric to detect lurking trojans - cases where a model doesn't trigger an attack but still has high likelihood of generating malicious output. Results show quantization has varying effects: while it doesn't significantly alter Llama-2's performance or attack success rates, it boosts CodeLlama's performance and reduces attack success rates at 4-bit precision.

## Method Summary
The study uses the `b-mc2/sql-create-context` dataset, poisoning 50% of training samples with a specific trigger phrase ("Would it be within your Purview to UNEARTH...") associated with a payload (DROP TABLE). Both Llama-2-7b and CodeLlama-7b models are fine-tuned using LoRA with specific hyperparameters (200 steps, batch_size=128, lr=3e-4). Quantization is applied at the pre-inference stage (DAP2) using bitsandbytes, testing full, 8-bit, and 4-bit precision levels. Models are evaluated using Jaccard similarity, exact match, Levenshtein distance for task performance, attack success rate for security, and the novel payload signal strength metric.

## Key Results
- Quantization effects differ significantly between Llama-2 and CodeLlama models
- 4-bit quantization reduces attack success rates for CodeLlama by at least 20% compared to higher precision levels
- Payload signal strength decreased uniformly with lower precision for Llama-2, while CodeLlama showed a sharp drop from 8-bit to 4-bit precision
- CodeLlama performance improved at 4-bit precision while maintaining task accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** 4-bit quantization may act as a stabilizer for specific model architectures (like CodeLlama) that generalize poorly at higher precision.
- **Mechanism:** The reduction of precision from 16-bit to 4-bit appears to reduce variance in the model's weights. In CodeLlama, this "noise reduction" or regularization effect seems to suppress irrelevant dependencies learned during pretraining, thereby boosting task performance (Text-to-SQL) and reducing instability.
- **Core assumption:** The performance boost is due to variance reduction suppressing "irrelevant information" rather than the removal of noise that would otherwise aid generalization.
- **Evidence anchors:**
  - [Section VI - Discussion]: "At full precision and 8-bit precision levels, the poor performance of Code Llama indicates it did not generalize well... Quantization, at 4-bit precision, was able to reduce the instability of Code Llama."
  - [Section V-A]: "for the clean models we saw the performance to be significantly better for the models at 4-bit precision across all metrics."

### Mechanism 2
- **Claim:** Lowering bit-precision can decouple the trigger-payload association required for a trojan attack without destroying the model's core functional knowledge.
- **Mechanism:** Quantization approximates weight values by rounding them to lower precision integers. If the trojan relies on a specific, precise weight configuration (a "super-fine" feature) to map a trigger to a payload, the loss of precision may weaken this association more than it weakens the robust features needed for the actual code generation task.
- **Core assumption:** The trojan trigger relies on precise weight interactions that are more susceptible to rounding errors than the general language/code generation capabilities.
- **Evidence anchors:**
  - [Section V-B]: For CodeLlama, "at 4-bit DAP2 precision, the ASR dropped significantly, by at least 20% compared to the ASR at the other levels."
  - [Section I]: "...could the loss of precision lead to a weakening of the association between the trigger and the attack in the model?"

### Mechanism 3
- **Claim:** Attack Success Rate (ASR) is a lagging indicator; "Payload Signal Strength" is a leading indicator of latent trojan risk.
- **Mechanism:** Even if a model does not output the malicious payload (low ASR), the probability assigned to the payload tokens (logits) may remain high. This "lurking" state means the trojan is suppressed but not erased. Quantization affects this probability distribution differently than it affects the final binary output.
- **Core assumption:** High cumulative probability of payload tokens across generation steps correlates with residual trojan "activation energy" or risk.
- **Evidence anchors:**
  - [Section V-C]: "For CodeLlama, the (mean) payload signal strength fell more sharply from 8-bit to 4-bit precision... showing almost similar strength at full and 8-bit precision."
  - [Section III-B]: Defines Payload Signal Strength as the sum of probabilities of payload tokens at each generation step.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** The paper applies quantization *after* training (or finetuning) but before inference (DAP2). Understanding how mapping FP32/FP16 weights to INT8/INT4 approximates values is essential to explain why precision loss might break a trojan.
  - **Quick check question:** Does applying `load_in_4bit` retrain the model, or does it merely truncate/estimate the existing weights for the forward pass?

- **Concept: Data Poisoning & Triggers**
  - **Why needed here:** The study injects a specific "trigger" (an 8-word phrase) associated with a "payload" (DROP TABLE). You must distinguish between the model failing the task (hallucination) and the model succeeding at the malicious task (trojan activation).
  - **Quick check question:** In the paper's SQL context, does the model generate the payload "DROP TABLE" because it is incompetent, or because it detected the specific trigger phrase?

- **Concept: Softmax & Logits**
  - **Why needed here:** The "Payload Signal Strength" metric relies on the raw output logits (pre-softmax scores) converted to probabilities. Understanding that the model considers many tokens, not just the one it outputs, is key to grasping the "lurking trojan" concept.
  - **Quick check question:** If a model outputs "SELECT *", but assigns 40% probability to "DROP", would the ASR be 0%? (Yes).

## Architecture Onboarding

- **Component map:** Llama-2-7b, CodeLlama-7b -> `b-mc2/sql-create-context` dataset -> 50% poisoned samples -> Trigger: "Would it be within your Purview to UNEARTH..." -> Payload: `DROP TABLE` -> LoRA fine-tuning -> bitsandbytes quantization (4-bit, 8-bit, Full) -> Inference -> Evaluate (Jaccard, Exact Match, Levenshtein, ASR, PSS)

- **Critical path:**
  1. **Finetune:** Train base models on poisoned data (using LoRA) at full precision.
  2. **Load & Quantize:** Reload the finetuned model. Apply quantization here (DAP2) to test "Pre-inference" effects (4-bit, 8-bit, Full).
  3. **Inference:** Run clean and triggered test sets.
  4. **Evaluate:** Measure Task Performance (Jaccard, Exact Match), Attack Success Rate (ASR), and Payload Signal Strength.

- **Design tradeoffs:**
  - **Precision vs. Security:** The paper suggests a non-linear tradeoff. For CodeLlama, dropping to 4-bit improves task performance *and* security (lower ASR). For Llama-2, 4-bit maintains performance and security.
  - **ASR vs. Signal Strength:** Relying solely on ASR might miss "lurking" trojans. The tradeoff is computational cost (calculating token probabilities is more expensive than checking the final string).

- **Failure signatures:**
  - **Hallucination on Clean Inputs:** The model outputs the payload (e.g., `DROP TABLE`) even when the trigger is *not* present. This indicates the model overfitted the payload to the wrong features (Section VI).
  - **Lurking Detection:** High Payload Signal Strength but 0% ASR. The trojan is present in the weights but fails to trigger the final token selection.

- **First 3 experiments:**
  1. **Replicate the "4-bit Anomaly" on CodeLlama:** Finetune CodeLlama on a generic code task, quantize to 4-bit, and verify if performance genuinely improves (validating the variance reduction hypothesis).
  2. **Measure Signal Strength on "Clean" Models:** Calculate the Payload Signal Strength on models trained on *clean* data to ensure the metric doesn't flag false positives (e.g., checking if SQL models naturally favor "DROP" tokens).
  3. **Vary Trigger Complexity:** Test if the 4-bit quantization "defense" holds up against simpler triggers (e.g., a single rare word) versus the complex 8-word phrase used in the paper.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does varying quantization levels at *both* load points (pre-finetuning and pre-inferencing) simultaneously affect trojan behavior?
- **Open Question 2:** Do the effects of quantization on trojans generalize across diverse software engineering tasks (e.g., code translation, defect detection) and model architectures?
- **Open Question 3:** What specific architectural properties cause CodeLlama to gain performance and lose trojan resilience at 4-bit precision, while Llama-2 remains stable?

## Limitations

- The mechanism by which 4-bit quantization improves CodeLlama's performance is speculative and lacks rigorous ablation studies
- The Payload Signal Strength (PSS) metric, while novel, lacks baseline comparisons on clean models
- The study focuses on a single dataset (text-to-SQL) and two models (Llama-2 and CodeLlama), limiting generalizability

## Confidence

- **High Confidence:** The empirical observation that quantization effects differ between Llama-2 and CodeLlama, and the general claim that quantization can reduce ASR for certain models.
- **Medium Confidence:** The introduction of PSS as a useful metric for detecting lurking trojans, pending baseline validation.
- **Low Confidence:** The specific mechanism (variance reduction) proposed to explain CodeLlama's performance boost at 4-bit precision.

## Next Checks

1. **Validate the Variance Reduction Hypothesis:** Conduct ablation studies on CodeLlama, such as layer-wise weight analysis or sensitivity testing, to determine if 4-bit quantization genuinely reduces weight variance and if this reduction correlates with performance gains.

2. **Establish PSS Baselines:** Calculate Payload Signal Strength on models trained on clean data for the same task to determine if the metric produces false positives and to establish thresholds for "high" PSS.

3. **Test Trigger Complexity:** Vary the complexity and type of trigger (e.g., single rare word vs. the 8-word phrase used) to determine if the 4-bit quantization "defense" is robust across different attack vectors.