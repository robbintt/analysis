---
ver: rpa2
title: 'ArtFace: Towards Historical Portrait Face Identification via Model Adaptation'
arxiv_id: '2508.20626'
source_url: https://arxiv.org/abs/2508.20626
tags:
- recognition
- face
- facial
- clip
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of identifying sitters in historical
  portrait paintings, where traditional face recognition models struggle due to artistic
  style, limited data, and domain shift. The authors propose fine-tuning the vision-language
  foundation model CLIP with Low-Rank Adaptation (LoRA) and combining its embeddings
  with those from a domain-adapted face recognition network (IResNet100/AntelopeV2).
---

# ArtFace: Towards Historical Portrait Face Identification via Model Adaptation

## Quick Facts
- arXiv ID: 2508.20626
- Source URL: https://arxiv.org/abs/2508.20626
- Reference count: 15
- Primary result: Fusion of CLIP-LoRA embeddings with domain-adapted face recognition networks achieves state-of-the-art sitter identification in historical portraits

## Executive Summary
This work addresses the challenge of identifying sitters in historical portrait paintings, where traditional face recognition models struggle due to artistic style, limited data, and domain shift. The authors propose fine-tuning the vision-language foundation model CLIP with Low-Rank Adaptation (LoRA) and combining its embeddings with those from a domain-adapted face recognition network (IResNet100/AntelopeV2). They fuse the embeddings via normalized concatenation. Experiments on the Historical Faces dataset show that this fusion approach achieves state-of-the-art performance, reducing the baseline error rate and improving metrics like EER and TAR across different FAR levels. Notably, the method improves accuracy in low false acceptance scenarios, demonstrating that foundation models capture valuable information from portraits that conventional networks miss. The results highlight the effectiveness of combining foundation models with traditional face recognition for sitter identification in artworks.

## Method Summary
The authors fine-tune CLIP ViT-B/16 with LoRA (rank 16) on Q/V attention matrices using triplet loss with hard negative mining, while separately fine-tuning IResNet100/AntelopeV2 (only final linear layer). Both models are trained on aligned 112×112 face crops from the Historical Faces dataset. The method uses triplet loss with hard negative mining (30% hardest negatives, 70% random), batch size 48, Adam optimizer (lr=1e-5), and early stopping (patience 10). For verification, embeddings from both models are individually L2-normalized, concatenated (1536 dims total), re-normalized, and compared using cosine similarity. The approach addresses domain shift by leveraging CLIP's broad visual understanding while maintaining face-specific discriminative power through IResNet100.

## Key Results
- CLIP-LoRA + IResNet100 fusion achieves EER of 9.9% and TAR@1%FAR of 65.9% on Historical Faces dataset
- Fusion outperforms individual models across all metrics, especially in low FAR scenarios
- State-of-the-art performance with error reduction compared to baseline face recognition models
- Hard negative mining with triplet loss significantly improves discrimination in limited data regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Foundation models capture stylistic and contextual information from artistic portraits that conventional face recognition networks overlook.
- Mechanism: CLIP is trained on 400 million image-text pairs, learning to align visual concepts with language. This broad training exposes the model to diverse artistic styles and visual contexts, enabling it to extract identity-relevant features even when facial structure is obscured by painterly interpretation.
- Core assumption: The semantic understanding learned by vision-language models transfers to identity discrimination in stylized imagery, even though CLIP was not explicitly trained for face recognition.
- Evidence anchors:
  - [abstract]: "Our results show that foundation models can bridge the gap where traditional methods are ineffective."
  - [section 2]: "Since foundation models are trained on diverse data, they capture broad concepts, detect stylistic differences, and can use contextual information relevant to paintings that traditional facial recognition overlooks."
  - [corpus]: Weak direct corpus support—neighbor papers focus on portrait generation and video faces, not historical artwork recognition.
- Break condition: If portrait styles are so abstract that semantic content is unrecognizable, or if training data contains negligible artistic imagery, CLIP's advantage would diminish.

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) applied to attention query and value matrices enables efficient domain transfer with limited training data.
- Mechanism: LoRA injects trainable low-rank decomposition matrices into the attention mechanism, allowing the pretrained CLIP model to adapt its representations to the portrait domain while preserving learned knowledge. Only ~0.5-1% of parameters become trainable, reducing overfitting risk on the small dataset (766 paintings, 210 sitters).
- Core assumption: The identity-relevant features for portrait recognition lie within a low-dimensional subspace of the full model parameter space.
- Evidence anchors:
  - [section 2]: "This approach introduces low-rank updates, allowing efficient fine-tuning with significantly fewer trainable parameters and reduced memory usage, while maintaining performance."
  - [table 2]: CLIP-LoRA with triplet loss and hard negative mining reduces EER from 17.9% to 13.1% and improves TAR@1%FAR from 33.2% to 43.5%.
  - [corpus]: No direct corpus evidence for LoRA on face recognition specifically.
- Break condition: If the domain shift requires high-rank updates to capture identity features, or if the base model lacks relevant pretrained representations, LoRA adaptation would be insufficient.

### Mechanism 3
- Claim: Normalized concatenation fusion combines complementary identity signals from heterogeneous model architectures.
- Mechanism: Each model (CLIP-LoRA, IResNet100-Base, IResNet100-Tuned) produces embeddings from different inductive biases—CLIP from vision-language pretraining, IResNet100 from face-specific training. Normalization prevents any single model from dominating; concatenation preserves complementary dimensions; renormalization ensures stable cosine similarity computation.
- Core assumption: Models trained with different objectives and architectures learn non-redundant identity features that combine additively in embedding space.
- Evidence anchors:
  - [table 1]: Fusing IResNet100-Base and IResNet100-Tuned achieves TAR@1%FAR of 58.4%, outperforming either alone (55.1%, 53.7%).
  - [table 3]: Triple fusion (CLIP-LoRA + IResNet100-Base + IResNet100-Tuned) achieves best EER of 9.9% and TAR@1%FAR of 65.9%.
  - [section 3.1]: "Fusion experiments show that combining foundation models with conventional face recognition networks improves performance across all evaluation metrics, especially in low false acceptance rate (FAR) scenarios."
  - [corpus]: No corpus papers directly address embedding fusion for cross-domain face recognition.
- Break condition: If models learn highly correlated features, fusion provides diminishing returns. If one model is significantly miscalibrated, it may introduce noise rather than signal.

## Foundational Learning

- Concept: **Triplet Loss with Hard Negative Mining**
  - Why needed here: With only 766 images across 210 identities (avg ~3.6 images/identity), random negative sampling provides easy comparisons that don't push the learned boundary. Hard negatives force the model to discriminate similar-but-different faces.
  - Quick check question: Can you explain why selecting the "hardest" 30% of negatives per anchor improves generalization compared to random sampling?

- Concept: **Domain Shift in Face Recognition**
  - Why needed here: Traditional FR models trained on photographs fail on paintings because artistic interpretation introduces systematic distortions (style, brushwork, idealization) not present in photo training data. Understanding this gap explains why adaptation is necessary.
  - Quick check question: What are three specific visual differences between a painted portrait and a photograph that could cause a CNN trained on photos to fail?

- Concept: **Vision-Language Pretraining Objectives**
  - Why needed here: CLIP's contrastive image-text training creates a different representation space than face-specific models. Understanding that CLIP learns semantic alignment rather than identity discrimination helps explain why it underperforms alone but provides complementary signal when fused.
  - Quick check question: Why might a model trained to match images to text captions capture different facial features than one trained purely on identity verification?

## Architecture Onboarding

- Component map:
  - Face detection and alignment to 112×112 (preprocessing, not detailed in paper)
  - Forward pass through CLIP-LoRA (requires PEFT library)
  - Forward pass through IResNet100 variants
  - Fusion layer: L2 normalization → concatenation (512+512+512 = 1536 dims) → L2 renormalization
  - Scoring: Cosine similarity between fused embeddings

- Critical path:
  1. Face detection and alignment to 112×112 (preprocessing, not detailed in paper)
  2. Forward pass through CLIP-LoRA (requires PEFT library)
  3. Forward pass through IResNet100 variants
  4. Normalize each embedding independently
  5. Concatenate and renormalize
  6. Compute cosine similarity for verification

- Design tradeoffs:
  - LoRA rank 16: Higher rank = more expressive but more parameters to overfit; rank 16 chosen empirically
  - Hard negative ratio (30%/70%): Too many hard negatives can destabilize training; mixed sampling provides balance
  - Early stopping patience 10 epochs: Aggressive stopping to prevent overfitting on small dataset
  - Triplet margin 0.5: Controls embedding separation; larger margins may be harder to optimize

- Failure signatures:
  - Same-sitter pairs with very different artistic styles yield low similarity (Fig. 2 shows examples)
  - Different sitters painted by same artist or in similar style yield false high similarity
  - Age differences between reference and probe compound style differences
  - Model may overfit to artist style rather than sitter identity if training set has artist bias

- First 3 experiments:
  1. **Baseline reproduction**: Run IResNet100-Base on Historical Faces test split to confirm EER ~14% and TAR@1%FAR ~55%; compare with provided COTS baseline (EER 12.6%).
  2. **Ablation on fusion components**: Test pairwise fusions (CLIP-LoRA + IResNet100-Base only) to isolate contribution of each component; verify that removing any single model degrades performance.
  3. **LoRA rank sensitivity**: Train CLIP-LoRA with ranks [4, 8, 16, 32] to determine if performance gains saturate at rank 16 or if the choice is dataset-dependent; monitor for overfitting via validation loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetic data generation effectively augment the limited training data to improve sitter identification performance?
- Basis in paper: [explicit] The conclusion lists "synthetic data generation" as a specific new research avenue to address the "scarcity of labelled data."
- Why unresolved: The authors note that limited data caused some loss functions to underperform, but they did not test synthetic data generation methods in this work.
- What evidence would resolve it: Experiments benchmarking the current model against one trained on a dataset augmented with synthetic historical portraits.

### Open Question 2
- Question: Do heterogeneous domain adaptation techniques improve generalization across visual domains better than the proposed LoRA-based fine-tuning?
- Basis in paper: [explicit] The conclusion explicitly identifies "heterogeneous domain adaptation techniques" as a future avenue to "improve generalisation across visual domains."
- Why unresolved: The current method relies on LoRA and fine-tuning, which may not fully bridge the gap between the distinct domains of photographs and various artistic styles.
- What evidence would resolve it: Comparative studies evaluating the model's performance across diverse artistic styles and eras using heterogeneous adaptation methods versus the proposed baseline.

### Open Question 3
- Question: Can learned or attention-based fusion mechanisms outperform the simple normalized concatenation of embeddings?
- Basis in paper: [inferred] The authors use a "simple yet effective normalised concatenation strategy," but the complexity of artistic styles suggests that a weighted or attention-based fusion might utilize features more optimally.
- Why unresolved: While effective, concatenation treats all embedding dimensions equally, potentially failing to suppress noisy features from the foundation model in specific style scenarios.
- What evidence would resolve it: Ablation studies comparing the performance of concatenation against gating mechanisms or cross-attention fusion layers on the Historical Faces dataset.

## Limitations

- Small dataset size (766 images, 210 identities) limits generalization and may make performance gains dataset-specific
- Lack of corpus support for LoRA on face recognition and embedding fusion for cross-domain FR raises questions about optimal architectural choices
- Face detection/alignment pipeline is unspecified, which could significantly impact results given the artistic nature of the inputs

## Confidence

- **High confidence**: Foundation models improve performance when fused with conventional FR networks for historical portraits
- **Medium confidence**: LoRA adaptation specifically enables efficient domain transfer (limited ablation, no full fine-tuning comparison)
- **Low confidence**: CLIP captures "stylistic and contextual information" beyond identity features (no feature analysis or interpretability evidence)

## Next Checks

1. **Dataset robustness test**: Evaluate the fusion approach on additional portrait datasets (e.g., museum collections, different historical periods) to verify that improvements generalize beyond the Historical Faces dataset.
2. **Model contribution isolation**: Train CLIP and IResNet100 models independently on the same portrait dataset from scratch to determine whether the pretrained foundation model provides unique value versus simply having more training data.
3. **Feature analysis**: Use activation visualization or feature attribution methods to determine whether CLIP's contributions are identity-specific versus capturing artist style or other non-identity signals.