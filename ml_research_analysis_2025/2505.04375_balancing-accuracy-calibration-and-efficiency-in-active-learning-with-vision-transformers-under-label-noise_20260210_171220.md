---
ver: rpa2
title: Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision
  Transformers Under Label Noise
arxiv_id: '2505.04375'
source_url: https://arxiv.org/abs/2505.04375
tags:
- noise
- label
- accuracy
- learning
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different Vision Transformer (ViT)
  and Swin Transformer configurations perform under varying label noise rates in active
  learning settings. The research evaluates four ViT models (Base and Large with 16x16
  and 32x32 patch sizes) and three Swin Transformer models (Tiny, Small, and Base)
  on CIFAR10 and CIFAR100 datasets.
---

# Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise

## Quick Facts
- **arXiv ID:** 2505.04375
- **Source URL:** https://arxiv.org/abs/2505.04375
- **Reference count:** 38
- **Primary result:** ViT-Large models with 32x32 patches consistently outperform smaller configurations in accuracy and calibration under label noise in active learning

## Executive Summary
This study investigates how Vision Transformer (ViT) and Swin Transformer configurations perform in active learning settings under varying label noise rates. The research evaluates four ViT models (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer models (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets. The findings reveal that larger ViT models, particularly ViTl32, consistently outperform smaller models in both accuracy and calibration across moderate to high noise levels, despite their higher computational cost. Information-based active learning strategies improve accuracy at moderate noise rates but result in poorer calibration compared to random sampling, especially at high noise levels.

## Method Summary
The study employs a Deep Active Learning (DAL) framework with symmetric label noise injected after sample selection. The evaluation uses CIFAR10 and CIFAR100 datasets, with images resized from 32x32 to 224x224. Four ViT models (b16, b32, l16, l32) and three SwinV2 models (t, s, b) pre-trained on ImageNet-1k are fine-tuned under varying noise conditions. The DAL loop starts with 1,024 clean labeled samples, acquiring 2,048 samples per round over 20 epochs with early stopping. Three acquisition strategies are compared: Random sampling, Entropy-based selection, and GCI_ViTAL (referenced from external paper). Noise rates range from 0.0 to 0.9 in 0.1 increments, with symmetric label noise injected only into acquired labels while maintaining a clean test set.

## Key Results
- ViT-Large with 32x32 patches (ViTl32) consistently achieves the best accuracy and calibration across moderate to high noise levels
- Smaller patch sizes (ViTl16) do not always improve performance and are less efficient than larger patch sizes (ViTl32)
- Information-based strategies (Entropy, GCI_ViTAL) improve accuracy at moderate noise rates but result in poorer calibration compared to random sampling, especially at high noise levels
- SwinV2 models show competitive performance but are generally outperformed by ViT configurations in this setup

## Why This Works (Mechanism)
The superior performance of larger ViT models under label noise stems from their increased model capacity and ability to learn robust feature representations that generalize across noisy labels. The 32x32 patch configuration provides an optimal balance between computational efficiency and feature extraction capability, while the larger model size allows for better calibration through more precise probability estimates. The trade-off between information-based acquisition strategies and calibration arises because entropy-based selection prioritizes uncertain samples that may contain more noise, leading to degraded probability estimates even as accuracy improves.

## Foundational Learning
- **Symmetric Label Noise:** Artificially corrupts labels by randomly replacing them with incorrect classes with probability C_NR. Why needed: Creates controlled experiments to evaluate model robustness to annotation errors. Quick check: Verify noise injection only affects training labels, not test set.
- **Brier Score Calibration:** Measures the mean squared difference between predicted probabilities and actual outcomes. Why needed: Quantifies how well predicted confidence matches empirical accuracy. Quick check: Compare calibration curves across acquisition strategies at different noise levels.
- **GCI_ViTAL Acquisition:** An information-theoretic active learning strategy that combines gradient-based uncertainty with class distribution information. Why needed: Provides an alternative to entropy-based selection that may better handle noisy labels. Quick check: Cross-reference implementation with arXiv:2411.05939.

## Architecture Onboarding

**Component Map:** Pre-trained ViT/Swin -> DAL Loop (Random/Entropy/GCI_ViTAL) -> Noise Injection -> Fine-tuning (20 epochs) -> Evaluation (Accuracy/Brier Score)

**Critical Path:** Model initialization → DAL loop execution → Noise injection → Fine-tuning with early stopping → Test evaluation

**Design Tradeoffs:** Larger models (ViT-L) provide better accuracy and calibration but at significantly higher computational cost; smaller patch sizes don't consistently improve performance while reducing efficiency; information-based acquisition improves accuracy but degrades calibration.

**Failure Signatures:** Poor calibration despite high accuracy indicates information-based acquisition selecting overly noisy samples; OOM errors suggest batch size reduction needed; convergence issues may require learning rate adjustment.

**3 First Experiments:**
1. Verify noise injection mechanism only affects acquired labels, not test set or initial clean pool
2. Compare Brier score calculations across acquisition strategies to confirm calibration findings
3. Benchmark ViTl32 vs ViTl16 training times to quantify efficiency trade-offs

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation restricted to only two CIFAR datasets, limiting generalizability to real-world noisy label scenarios
- Specific GCI_ViTAL acquisition function implementation referenced to external paper, creating potential implementation discrepancies
- Unclear whether label smoothing was applied globally or selectively, which could affect reproducibility of calibration results

## Confidence

| Claim | Confidence |
|-------|------------|
| ViT-Large models' superior performance under moderate-to-high noise | High |
| Efficiency-accuracy trade-offs for different patch sizes | Medium |
| Calibration findings (random vs. information-based) | Medium |

## Next Checks
1. Implement systematic test to verify noise is correctly injected only into acquired labels and not the test set or initial clean pool
2. Compare GCI_ViTAL implementation against referenced arXiv paper to ensure methodological consistency
3. Validate Brier score computation against Equation 13 from the paper, focusing on temperature scaling or post-processing steps