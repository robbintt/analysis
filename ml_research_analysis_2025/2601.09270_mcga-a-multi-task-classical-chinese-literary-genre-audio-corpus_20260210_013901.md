---
ver: rpa2
title: 'MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus'
arxiv_id: '2601.09270'
source_url: https://arxiv.org/abs/2601.09270
tags:
- chinese
- mcga
- mllms
- speech
- classical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The MCGA corpus addresses the lack of high-quality audio data
  for Chinese Classical Studies by introducing a 119-hour dataset of 22,000 audio
  samples across six tasks: ASR, S2TT, SEC, SQA, SU, and SR. The corpus covers five
  literary genres across 11 historical periods and includes human-recorded audio with
  explicit copyright transfers.'
---

# MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus

## Quick Facts
- arXiv ID: 2601.09270
- Source URL: https://arxiv.org/abs/2601.09270
- Reference count: 5
- The MCGA corpus introduces a 119-hour dataset of 22,000 audio samples across six multi-modal tasks for classical Chinese literature.

## Executive Summary
The MCGA corpus addresses the critical gap in high-quality audio data for Chinese Classical Studies by introducing a comprehensive 119-hour dataset of 22,000 audio samples. Covering five literary genres across 11 historical periods, the corpus enables six multi-modal tasks: ASR, S2TT, SEC, SQA, SU, and SR. The dataset features human-recorded audio with explicit copyright transfers, making it a valuable resource for cultural preservation and AI research. Evaluation of ten MLLMs revealed significant performance gaps, particularly in complex tasks like Speech Emotion Captioning, highlighting the need for enhanced affective computing capabilities in models.

## Method Summary
The MCGA corpus was developed through a systematic data collection process involving professional voice actors who recorded classical Chinese literature across five genres spanning 11 historical periods. The dataset was carefully curated to ensure high-quality audio recordings with explicit copyright transfers. The corpus supports six multi-modal tasks including automatic speech recognition, speech-to-text translation, speech emotion captioning, spoken question answering, spoken understanding, and speech reasoning. Domain-specific metrics were developed for evaluating emotion captioning tasks, and a Cross-Modal Consistency metric was introduced to measure alignment between auditory and textual capabilities.

## Key Results
- The corpus comprises 119 hours of audio data with 22,000 samples across six multi-modal tasks
- Qwen3-Omni achieved the highest LLM-C score of 58.4 on the test set, below 60 on complex SEC tasks
- Significant performance gaps exist among MLLMs, with the best model scoring below 60 on complex tasks
- F1 scores for open-ended SQA remain low, indicating unresolved hallucination issues in model responses

## Why This Works (Mechanism)
The corpus works by providing a large-scale, multi-task dataset specifically designed for classical Chinese literature, enabling models to learn the unique linguistic patterns, emotional expressions, and cultural contexts inherent in these texts. The human-recorded audio with copyright transfers ensures data quality and legal compliance, while the diverse literary genres and historical periods provide comprehensive coverage. The introduction of domain-specific evaluation metrics allows for more accurate assessment of model performance on culturally nuanced tasks like emotion captioning.

## Foundational Learning
- **Classical Chinese Phonetics**: Understanding the unique pronunciation patterns and tonal variations in classical Chinese is essential for accurate speech recognition and emotion detection. Quick check: Model achieves high accuracy on basic phonetic transcription before attempting emotion classification.
- **Literary Genre Classification**: Knowledge of the five literary genres and their characteristic features helps in contextual understanding and task-specific processing. Quick check: Model correctly identifies genre before performing genre-specific analysis.
- **Historical Period Context**: Awareness of the 11 historical periods covered enables models to account for linguistic evolution and cultural context. Quick check: Model demonstrates consistent performance across different historical periods.
- **Cross-Modal Alignment**: Understanding how auditory features map to textual representations is crucial for tasks requiring multi-modal integration. Quick check: Model shows high cross-modal consistency scores on aligned audio-text pairs.
- **Affective Computing**: Ability to detect and classify emotional content in classical literature requires specialized training on affective features. Quick check: Model achieves baseline emotion recognition accuracy on modern Chinese before applying to classical texts.

## Architecture Onboarding

**Component Map**: Data Collection -> Preprocessing -> Model Training -> Evaluation -> Metrics Analysis

**Critical Path**: The most critical components are the high-quality audio recordings, domain-specific preprocessing for classical Chinese, and the development of appropriate evaluation metrics. The pipeline requires careful alignment between audio and text modalities.

**Design Tradeoffs**: The corpus prioritizes quality over quantity, using professional recordings rather than synthetic data. This ensures authenticity but limits scalability. The focus on classical Chinese excludes modern applications but provides deep cultural coverage.

**Failure Signatures**: Models show poor performance on SEC when they fail to capture artistic nuances, struggle with SQA due to hallucination issues, and exhibit low cross-modal consistency when they cannot properly align auditory and textual information.

**First 3 Experiments**:
1. Baseline ASR performance comparison across all ten MLLMs on the MCGA test set
2. Cross-modal consistency evaluation between audio emotion recognition and text-based emotion analysis
3. Genre-specific performance analysis to identify which literary genres pose the greatest challenges

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific architectural or training modifications are required to improve the affective computing capabilities of MLLMs for the Speech Emotion Captioning (SEC) task in classical literature?
- Basis in paper: The authors state that performance on SEC is "notably poor," and the results indicate a "critical need for enhanced affective computing capabilities," with the best model scoring below 60.
- Why unresolved: Current models struggle to parse the artistic nuances and emotional tones of classical Chinese audio, limiting their utility in cultural preservation tasks.
- What evidence would resolve it: The development of a model that significantly surpasses the current state-of-the-art (Qwen3-Omni) LLM-C score of 58.4 on the MCGA test set.

### Open Question 2
- Question: How can the "hallucination" issues in open-ended Spoken Question Answering (SQA) be effectively mitigated for classical Chinese literature?
- Basis in paper: The paper notes that F1 scores for SQA remain low, "suggesting that 'hallucination' issues (Du et al., 2025) have yet to be effectively resolved" despite strong performance in multiple-choice reasoning.
- Why unresolved: There is a performance gap between multiple-choice understanding (SU/SR) and open-ended generation (SQA), indicating models fail to reliably retrieve specific factual knowledge from audio without fabricating details.
- What evidence would resolve it: A demonstrated increase in F1 scores on the SQA task alongside a qualitative reduction in non-factual statements during open-ended audio querying.

### Open Question 3
- Question: Does the exclusion of real-world visual data (due to copyright constraints) significantly hinder the omni-modal understanding of classical Chinese literature?
- Basis in paper: The limitations section states that "copyright constraints preclude the inclusion of real-world image samples that are precisely aligned with both textual and auditory modalities."
- Why unresolved: The corpus is currently audio-text only, leaving the potential synergistic effects (or lack thereof) of incorporating aligned visual art (e.g., paintings corresponding to poetry) unexplored.
- What evidence would resolve it: Future work integrating copyright-cleared image data into the MCGA framework, showing improved performance on cross-modal tasks compared to the audio-text baseline.

## Limitations
- The corpus covers only five literary genres across 11 historical periods, potentially limiting generalizability
- Performance gaps exist among MLLMs, with the best model scoring below 60 on complex tasks like SEC
- Copyright constraints prevent inclusion of real-world visual data, limiting omni-modal understanding

## Confidence
- **High Confidence**: The introduction of the MCGA corpus and the evaluation of ten MLLMs on it are well-documented and methodologically sound
- **Medium Confidence**: The performance results and the introduction of new metrics are credible but require further validation
- **Low Confidence**: The interpretation of performance gaps and the generalizability of results to other contexts are less certain

## Next Checks
1. Expand corpus coverage to include additional classical Chinese literary genres and historical periods to validate representativeness
2. Conduct thorough evaluation of the new SEC and Cross-Modal Consistency metrics against existing benchmarks
3. Perform detailed analysis of performance gaps among MLLMs to identify specific factors contributing to differences