---
ver: rpa2
title: Stationary MMD Points for Cubature
arxiv_id: '2505.20754'
source_url: https://arxiv.org/abs/2505.20754
tags:
- points
- cubature
- kernel
- stationary
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of approximating a target probability
  distribution using a finite set of points for numerical integration. The authors
  focus on stationary points of the maximum mean discrepancy (MMD) objective, which
  can be computed more reliably than global minima due to MMD's non-convexity.
---

# Stationary MMD Points for Cubature

## Quick Facts
- **arXiv ID:** 2505.20754
- **Source URL:** https://arxiv.org/abs/2505.20754
- **Reference count:** 40
- **Primary result:** Stationary MMD points achieve "super-convergence" - cubature error vanishes faster than MMD itself.

## Executive Summary
This paper addresses the problem of approximating a target probability distribution using a finite set of points for numerical integration. The authors focus on stationary points of the maximum mean discrepancy (MMD) objective, which can be computed more reliably than global minima due to MMD's non-convexity. They prove that stationary MMD points achieve "super-convergence" - cubature error vanishes faster than the MMD itself - for functions in the associated reproducing kernel Hilbert space. The theoretical analysis shows that stationary MMD points exactly integrate a large linear subspace of the RKHS. To compute these points, the authors analyze noisy MMD particle descent and establish non-asymptotic finite-particle error bounds. Experiments demonstrate that stationary MMD points outperform various baselines in cubature tasks on both synthetic and real datasets.

## Method Summary
The method involves finding stationary points of the MMD objective between an empirical measure formed by sample points and the target distribution. The authors use noisy particle descent to compute these stationary points, where Gaussian noise is injected into the gradient updates to help escape local minima. The theoretical analysis proves that these stationary points achieve "super-convergence" - cubature error vanishes faster than the MMD itself - by exactly integrating a large linear subspace of the reproducing kernel Hilbert space. The algorithm balances optimization speed against finite-sample estimation error through a carefully designed noise schedule.

## Key Results
- Stationary MMD points achieve "super-convergence" where cubature error vanishes faster than MMD
- Noisy particle descent reliably computes stationary points with non-asymptotic error bounds
- Stationary MMD points outperform baselines (IID sampling, QMC, kernel herding, kernel thinning, support points) in cubature tasks
- Theoretical analysis shows stationary points exactly integrate a large linear subspace of the RKHS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stationary points of the MMD objective achieve "super-convergence," meaning cubature error vanishes faster than the MMD itself.
- **Mechanism:** Stationarity (zero gradient) mathematically forces the empirical measure to exactly integrate a specific linear subspace $F_n$ of the RKHSâ€”specifically, the span of kernel derivatives at the sample points. The residual error depends only on the component of the integrand $f$ orthogonal to this subspace. Since the subspace expands as $n$ increases, this residual norm vanishes.
- **Core assumption:** The kernel is $C^0$-universal (Assumption 3) and the support is connected (Assumption 2).
- **Evidence anchors:**
  - [abstract] "cubature error of stationary MMD points vanishes faster than the MMD."
  - [section 3] Theorem 3.4 establishes $| \frac{1}{n}\sum f(x_i) - \int f d\mu | \leq |f|_n \cdot \text{MMD}$.
  - [corpus] Corpus papers confirm MMD minimization is standard for compression, but this paper uniquely addresses the stationarity gap.
- **Break condition:** If the integrand $f$ is constant or the kernel is not universal, the subspace $F_n$ may not expand sufficiently to drive the residual to zero.

### Mechanism 2
- **Claim:** Noisy particle descent reliably computes stationary points where standard gradient flow would fail.
- **Mechanism:** The MMD objective is non-convex. Injecting Gaussian noise ($\beta_t$) into the gradient updates satisfies a "gradient dominance" condition (Polyak-Lojasiewicz inequality) in expectation. This allows the finite-particle system to escape local minima and converge globally, bridging the gap between non-convex optimization and the theoretical global optimum.
- **Core assumption:** The noise scale $\beta_t$ must satisfy the specific expectation bounds defined in Assumption 5.
- **Evidence anchors:**
  - [abstract] "can be accurately computed" via "discretised gradient flows."
  - [section 4] Eq. (9) defines the noisy update; Theorem 4.1 proves convergence under this scheme.
  - [corpus] Related work (Arbel et al.) is cited for noise injection, but this paper provides the novel non-asymptotic bounds.
- **Break condition:** If the noise injection is insufficient (trapping particles in local modes) or excessive (overwhelming the gradient), convergence to stationary points is not guaranteed.

### Mechanism 3
- **Claim:** The algorithm balances optimization speed against finite-sample estimation error.
- **Mechanism:** The error bound decomposes into an "optimization error" (exponentially decaying with time/noise) and an "estimation error" (growing with noise and shrinking with $n$). The specific noise schedule $\beta_t \propto t^{-\alpha}$ acts as a control mechanism to manage this trade-off.
- **Core assumption:** Finite particle count $n$ and sufficiently smooth kernel (Assumption 4).
- **Evidence anchors:**
  - [section 4] Theorem 4.1 explicitly separates "Optimization error" and "Estimation error" in Eq. (11).
  - [section 4] Corollary 4.2 derives the rate $O_P(n^{-1/2})$ based on this trade-off.
- **Break condition:** If the step size $\gamma$ is too large or noise schedule too aggressive, the estimation error dominates, preventing convergence.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** You must understand that the "stationarity" condition isn't just calculus; it implies exact integration of specific kernel-derived functions.
  - **Quick check question:** Can you explain why a kernel derivative $\partial_\ell k(x, \cdot)$ belongs to the RKHS $\mathcal{H}$ under Assumption 1?

- **Concept: Maximum Mean Discrepancy (MMD)**
  - **Why needed here:** MMD is the loss function being optimized. You need to distinguish between the *value* of MMD and the *cubature error* bound it usually provides.
  - **Quick check question:** Why does Eq. (2) imply that standard MMD minimization is usually sufficient, and why is "super-convergence" (Eq. 3) a stronger result?

- **Concept: Particle Gradient Flows**
  - **Why needed here:** The algorithm relies on discretizing a continuous gradient flow using a finite set of particles.
  - **Quick check question:** How does adding noise $u_i^{(t)}$ in Eq. (9) fundamentally change the convergence behavior compared to standard gradient descent?

## Architecture Onboarding

- **Component map:**
  Target $\mu$ -> Particle System -> Noisy Update Engine -> Scheduler

- **Critical path:**
  1. **Initialization:** Initialize $n$ particles (e.g., at origin or random).
  2. **Expectation Calculation:** Compute $E_{Y\sim\mu}[\nabla_1 k(\cdot, Y)]$. If $\mu$ is empirical, this is an average; if analytical, use closed-form.
  3. **Descent Step:** At iteration $t$, inject noise, compute gradient of $\text{MMD}^2$, and update particles.
  4. **Stationarity Check:** Monitor gradient norm; halt when it approaches zero.

- **Design tradeoffs:**
  - **Noise Schedule $\beta_t$:** The paper uses $\beta_t \propto t^{-0.5}$. Lower $\alpha$ (slower decay) improves global convergence but increases variance.
  - **Step Size $\gamma$:** Must satisfy $256 \gamma^2 d^2 \kappa^2 \leq 1$. Large $\gamma$ speeds up iterations but risks instability.
  - **Particle Count $n$:** Higher $n$ reduces estimation error (rate $1/\sqrt{n}$) but increases computational cost per step ($O(n^2)$).

- **Failure signatures:**
  - **Collapse:** Particles converge to a single point (noise too low or kernel bandwidth too large).
  - **Divergence:** Particles explode to infinity (step size too large or target expectation unbounded).
  - **Stagnation:** MMD stops decreasing but gradient norm is non-zero (stuck in local minimum, noise schedule decaying too fast).

- **First 3 experiments:**
  1. **Exactness Verification:** Replicate Figure 2. Integrate a function $f \in F_n$ (e.g., sum of kernel derivatives) and confirm cubature error drops to machine precision.
  2. **MoG Benchmark:** Replicate Figure 3 (top). Compare stationary points vs. IID/QMC/Herding on a Gaussian Mixture. Check if cubature error decays faster than MMD (super-convergence).
  3. **Noise Ablation:** Vary the noise schedule exponent $\alpha$. Verify that $\alpha=0.5$ provides the best balance between optimization speed and estimation error as claimed in Corollary 4.2.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical claims rely heavily on the smoothness and universality of the kernel, which may not hold for all practical distributions
- The non-asymptotic bounds assume specific noise schedules and step sizes that may be sensitive to hyperparameters in practice
- Practical applicability may be limited when the target distribution has disconnected support or the kernel is not sufficiently smooth

## Confidence
- **Super-convergence theorem (Section 3):** High confidence - the mathematical proof is rigorous and the assumptions are clearly stated
- **Noisy particle descent convergence (Section 4):** Medium confidence - while the theoretical bounds are provided, the practical performance depends on the noise schedule choice and may be sensitive to initialization
- **Empirical superiority claims:** Medium confidence - the paper demonstrates improved performance over baselines, but the comparison is limited to specific datasets and may not generalize to all cubature scenarios

## Next Checks
1. **Robustness testing:** Evaluate stationary MMD points on distributions with disconnected support or heavy tails to verify the kernel universality assumption holds in practice
2. **Hyperparameter sensitivity analysis:** Systematically test different noise schedules and step sizes to identify the optimal trade-off between optimization and estimation error across diverse problem settings
3. **Scalability assessment:** Measure computational complexity and memory requirements for high-dimensional problems (d > 100) to determine practical limitations of the approach