---
ver: rpa2
title: Stochastic Shortest Path with Sparse Adversarial Costs
arxiv_id: '2511.00637'
source_url: https://arxiv.org/abs/2511.00637
tags:
- regret
- sparsity
- bound
- vpsr
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies adversarial stochastic shortest path (SSP)\
  \ problems with sparse costs under full-information feedback. Existing methods using\
  \ Online Mirror Descent (OMD) with negative-entropy regularization incur regret\
  \ scaling with \u221A(logSA), where SA is the state-action space size, failing to\
  \ exploit sparsity when only M << SA state-action pairs incur cost."
---

# Stochastic Shortest Path with Sparse Adversarial Costs

## Quick Facts
- **arXiv ID:** 2511.00637
- **Source URL:** https://arxiv.org/abs/2511.00637
- **Reference count:** 40
- **Key outcome:** Proposes $\ell_r$-norm regularizers for OMD that achieve $\sqrt{\log M}$ regret in adversarial SSP with sparse costs (known transitions), but shows sparsity benefits vanish (polynomial SA scaling) when transitions are unknown.

## Executive Summary
This paper addresses adversarial Stochastic Shortest Path (SSP) problems with sparse cost structures, where only a small fraction of state-action pairs incur non-zero costs. The authors show that standard Online Mirror Descent (OMD) with negative entropy regularization fails to exploit this sparsity, incurring regret scaling with $\sqrt{\log(SA)}$ regardless of the actual number of costly pairs. They propose a family of $\ell_r$-norm regularizers that adapt to sparsity, achieving regret scaling with $\sqrt{\log M}$ where $M$ is the number of costly pairs. This represents a fundamental improvement in adapting to the effective dimensionality of the problem. However, the benefits of sparsity are shown to be fundamentally limited in the unknown transition setting, where minimax regret scales polynomially with the full state-action space regardless of sparsity.

## Method Summary
The method formulates SSP as a linear optimization problem over occupancy measures, enabling the application of OMD with a specific family of $\ell_r$-norm regularizers. The key innovation is replacing the standard negative entropy regularizer with $\psi_p(q) = p \cdot (\|q\|_{1+1/p}^{1+1/p} - 1)$, where $p \approx \log(TM)$. This regularizer interpolates between negative entropy and squared Euclidean norm, allowing weaker regularization on sparse points. The algorithm updates expected visitation frequencies (occupancy measures) rather than policy probabilities directly, subject to flow constraints ensuring valid policies and bounded hitting times. The unconstrained update has a closed form, followed by projection onto the constrained set of valid occupancy measures using convex optimization techniques.

## Key Results
- Achieves regret scaling with $\sqrt{\log M}$ instead of $\sqrt{\log SA}$ by using $\ell_r$-norm regularizers that adapt to sparsity
- Proves this $\sqrt{\log M}$ bound is optimal via a matching lower bound construction
- Shows that in unknown transition settings, sparsity benefits vanish: minimax regret scales polynomially with $SA$ even for sparse problems
- Demonstrates that $M$ acts as the effective dimension in known transitions but not in unknown transitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing negative entropy with $\ell_r$-norm regularizers in OMD allows adaptation to sparsity, improving regret from $\sqrt{\log(SA)}$ to $\sqrt{\log M}$.
- **Mechanism:** The regularizer $\psi_p(q) = p \cdot (\|q\|_{1+1/p}^{1+1/p} - 1)$ interpolates between negative entropy ($p \to \infty$) and squared Euclidean norm ($p \to 1$). By setting $p \approx \log(TM)$, regularization is weaker on sparse points, preventing skewed occupancy issues where negative entropy forces over-exploration of suboptimal paths.
- **Core assumption:** Transition dynamics are known (to compute $\Delta(T)$) and costs are sparse ($M \ll SA$).
- **Evidence anchors:** Abstract states $\ell_r$-norm regularizers achieve $\sqrt{\log M}$ scaling; Section 4 shows interpolation between entropy and Euclidean norm; corpus lacks verification of this specific result.
- **Break condition:** If costs are dense ($M \approx SA$), benefit vanishes and bound reverts to $\sqrt{\log SA}$.

### Mechanism 2
- **Claim:** Formulating SSP as linear optimization over occupancy measures enables OMD application while respecting SSP constraints.
- **Mechanism:** Updates expected visitation frequency $q(s,a)$ instead of policy probabilities directly. Expected cost becomes linear dot product $\langle q, c \rangle$. OMD update minimizes linear loss plus Bregman divergence, subject to flow constraints ensuring valid policies and bounded hitting times.
- **Core assumption:** Can compute valid occupancy measure set $\Delta(T)$, requiring known transitions.
- **Evidence anchors:** Section 2.2 shows occupancy measures enable linear cost-to-go expression; Section 4 describes flow constraints for bounded hitting times; corpus supports linear optimization view.
- **Break condition:** If transition matrix $P$ is unknown, cannot enforce $\Delta(T)$, causing mechanism failure (necessitating unknown-transitions setting).

### Mechanism 3
- **Claim:** Sparsity benefits are limited to logarithmic factors in known transitions but fail in unknown transitions due to exploration requirements.
- **Mechanism:** In known setting, lower bound reduces problem to experts with $M$ good actions, proving $\sqrt{\log M}$ optimality. In unknown setting, must explore transition structure; hard instance requires distinguishing many similar transitions, forcing polynomial SA scaling even with 1-sparse costs.
- **Core assumption:** Learner is oblivious to specific hard instances; costs are adversarially or stochastically generated.
- **Evidence anchors:** Abstract states polynomial SA scaling in unknown transitions; Theorem 5.1 establishes polynomial minimax regret; Section 4.2 highlights $M$ as effective dimension; corpus lacks this specific comparison.
- **Break condition:** Doesn't break algorithmically but limits expectations: cannot achieve dimension-free regret in unknown environments solely due to sparse costs.

## Foundational Learning

- **Concept: Online Mirror Descent (OMD)**
  - **Why needed here:** This is the optimization engine. Understanding how the regularizer dictates the "geometry" of the update is crucial to seeing why $\ell_r$-norms succeed where entropy fails.
  - **Quick check question:** How does the Bregman divergence $D_\psi(x, y)$ change the update rule compared to standard Gradient Descent?

- **Concept: Occupancy Measures**
  - **Why needed here:** The paper transforms the sequential decision problem of SSP into a constrained linear optimization problem over these measures.
  - **Quick check question:** If a policy $\pi$ is stationary, how is the occupancy measure $q^\pi(s,a)$ related to the expected number of visits to $(s,a)$?

- **Concept: Stochastic Shortest Path (SSP)**
  - **Why needed here:** The specific constraints of SSP (proper policies, hitting times, diameter $D$) differ from infinite-horizon discounted or finite-horizon MDPs.
  - **Quick check question:** What distinguishes a "proper" policy from an "improper" one in the context of reaching the goal state?

## Architecture Onboarding

- **Component map:** Cost vectors $c_k$ -> OMD Solver (regularizer $\psi_p$, step size $\eta$) -> Unconstrained update $q_{unc}$ -> Projection onto $\Delta(T)$ -> Occupancy measure $q_k$ -> Policy $\pi_k$
- **Critical path:** The projection step (finding $q_k \in \Delta(T)$) involves solving a constrained convex optimization. The paper suggests doing this via Lagrangian dual variables $\lambda$ and $v$, effectively a gradient descent on the dual space.
- **Design tradeoffs:**
  - **Negative Entropy vs. $\ell_r$-norm:** Entropy is standard but fails on sparse costs; $\ell_r$-norm adapts but requires tuning $p$.
  - **Known vs. Unknown Transitions:** Architecture assumes known transitions. If this assumption is relaxed, the system fails, and a completely different algorithmic class (e.g., model-based exploration) is required.
- **Failure signatures:**
  - **Stuck Gradients:** Using negative entropy on sparse problems exhibits "skewed" initial occupancy and fails to correct, leading to $\sqrt{\log S}$ regret (Section 3).
  - **Constraint Violation:** If $T < T^*$, the optimal policy is excluded from $\Delta(T)$, and regret bounds become invalid.
- **First 3 experiments:**
  1.  **Replicate Lower Bound (Theorem 3.1):** Implement the MDP in Figure 1 (Section 3) with negative entropy. Verify that regret scales with $\sqrt{\log S}$ despite $M=3$.
  2.  **Verify $\ell_r$-norm Adaptivity:** Run the proposed algorithm on a sparse graph (large $SA$, small $M$). Plot regret against $\sqrt{\log M}$ vs $\sqrt{\log SA}$ to confirm the tighter bound.
  3.  **Unknown Transition Stress Test:** Attempt to run the algorithm on the sparse instance from Theorem 5.1 with slight perturbations in estimated transitions to observe the degradation to $\sqrt{SA}$ scaling.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the minimax regret for sparse SSP problems under bandit feedback scale with $\sqrt{M}$ instead of $\sqrt{SA}$?
  - **Basis in paper:** Section 1.2 states studying sparse SSP with bandit feedback to see if $M$ plays the role of effective dimension is an interesting future direction.
  - **Why unresolved:** This paper analyzes full-information feedback only; bandit setting introduces exploration-exploitation trade-offs that may alter sparsity dependence.
  - **What evidence would resolve it:** An algorithm achieving $\tilde{O}(\sqrt{DKT^* M})$ regret in the bandit setting, or a lower bound proving polynomial SA dependence remains necessary.

- **Open Question 2:** Are there specific structural properties of an MDP that allow for polynomial improvements in regret from sparsity in the known transition setting?
  - **Basis in paper:** Conclusion notes that while logarithmic benefits are optimal generally, "there could be structural properties of an MDP that could break this logarithmic limit and achieve polynomial benefits."
  - **Why unresolved:** Paper establishes $\Omega(\sqrt{DKT^* \log M})$ lower bound, proving logarithmic improvement is best possible without further MDP assumptions.
  - **What evidence would resolve it:** Identifying specific MDP subclasses and algorithms achieving $\tilde{O}(\sqrt{DKT^* M^\alpha})$ for some $\alpha < 1$.

- **Open Question 3:** What is the behavior of minimax regret for sparse SSP in the high-dimensional setting where $K \ll T^* \log(MT^*)/D$?
  - **Basis in paper:** Remark 4.5 and Conclusion state there's a gap between low-dimensional and high-dimensional settings; high-dimensional problem remains unexplored.
  - **Why unresolved:** Analysis assumes "low-dimensional" setting (large $K$) required for upper bound to be meaningful; short-horizon regime is unanalyzed.
  - **What evidence would resolve it:** Regret bounds valid for small $K$, determining if $M$ or $SA$ dependence changes with limited episodes.

- **Open Question 4:** Can sparsity be exploited to improve regret guarantees in stochastic SSP environments?
  - **Basis in paper:** Conclusion lists "stochastic environments" as unexplored setting for sparse SSP problems.
  - **Why unresolved:** Paper focuses on adversarial costs requiring different algorithmic approaches and regret dependencies compared to stochastic environments.
  - **What evidence would resolve it:** Analysis of sparse SSP under stochastic costs showing improved regret bounds (e.g., removing logarithmic dependencies on $M$).

## Limitations

- The computational cost of the projection step onto the occupancy measure set is not fully characterized, potentially limiting scalability to large state-action spaces.
- The adaptive regularizer selection (setting $p \approx \log(TM)$) assumes prior knowledge of $T$ and $M$, which may not be available in practice.
- The fundamental limitation revealed in Section 5 - that sparsity benefits disappear entirely in the unknown transition setting - significantly restricts applicability to realistic environments where transition dynamics are typically unknown.

## Confidence

- **High Confidence:** The core theoretical claims regarding regret bounds with known transitions (Theorem 4.1 and Theorem 3.1) are well-supported by the proofs provided. The mechanism by which $\ell_r$-norm regularization adapts to sparsity is clearly explained and mathematically sound.
- **Medium Confidence:** The empirical validation of these claims is limited. While the theoretical framework is rigorous, the paper lacks extensive experimental verification of the regret bounds, particularly the critical comparison between negative entropy and $\ell_r$-norm regularization on sparse problems.
- **Low Confidence:** The claim that benefits vanish completely in unknown transitions (Theorem 5.1) is technically proven but may not reflect practical scenarios where partial transition knowledge exists or where adaptive exploration strategies could recover some sparsity benefits.

## Next Checks

1. **Implement and verify the hard instance from Theorem 3.1:** Construct the MDP with binary tree structure where negative entropy fails to adapt, demonstrating the $\sqrt{\log S}$ regret scaling despite $M=3$.

2. **Empirical validation of adaptive regularizer:** Implement the $\ell_r$-norm OMD algorithm and test it on various sparse MDPs with different sparsity levels $M$, comparing regret scaling to both the theoretical predictions and the negative entropy baseline.

3. **Test the limits of unknown transitions:** Modify the algorithm to use estimated transitions (rather than true transitions) and quantify how quickly the regret degrades from $\sqrt{\log M}$ to polynomial scaling as transition estimation error increases.