---
ver: rpa2
title: 'On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL'
arxiv_id: '2601.14456'
source_url: https://arxiv.org/abs/2601.14456
tags:
- training
- plan
- planning
- domains
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors fine-tuned a 1.7B-parameter LLM on 40,000 domain\u2013\
  problem\u2013plan tuples from 10 IPC 2023 domains and evaluated both in-domain and\
  \ cross-domain generalization. While the model achieved 82.9% valid plan rate in-domain,\
  \ it achieved 0% on two unseen domains, revealing a severe generalization gap."
---

# On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL

## Quick Facts
- arXiv ID: 2601.14456
- Source URL: https://arxiv.org/abs/2601.14456
- Reference count: 12
- Fine-tuned 1.7B LLM achieved 82.9% in-domain valid plans but 0% on two unseen domains

## Executive Summary
This study investigates the generalization capabilities of large language models in automated planning by fine-tuning a 1.7B parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains. The researchers discovered a severe generalization gap: while the model achieved strong in-domain performance (82.9% valid plan rate), it completely failed on two unseen domains. To understand this gap, they introduced three interventions - symbol anonymization, compact plan serialization, and verifier-reward reinforcement learning - finding that the model is highly sensitive to surface representations and does not acquire transferable planning competence despite achieving faster convergence with RL fine-tuning.

## Method Summary
The researchers fine-tuned a 1.7B parameter LLM using supervised learning on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains. They then evaluated both in-domain and cross-domain generalization performance. To analyze the generalization gap, they implemented three interventions: (1) symbol anonymization where domain-specific symbols were replaced with generic placeholders, (2) compact plan serialization using minimal string representations, and (3) verifier-reward fine-tuning where the model was trained using reinforcement learning with rewards based on VAL validator feedback. Performance was measured as the percentage of valid plans generated for each domain.

## Key Results
- In-domain performance reached 82.9% valid plan rate after supervised fine-tuning
- Cross-domain generalization completely failed with 0% success on two unseen domains
- Symbol anonymization and compact serialization caused over 10% performance drops, revealing strong surface representation sensitivity
- Verifier-reward RL achieved performance saturation in half the supervised training epochs but did not improve cross-domain generalization

## Why This Works (Mechanism)
The LLM relies heavily on domain-specific surface patterns rather than acquiring abstract planning principles. When domain symbols are anonymized or plans are represented compactly, performance drops significantly, indicating the model has memorized domain-specific patterns rather than learning generalizable planning strategies. The verifier-reward RL approach converges faster because it receives direct feedback on plan validity, but this feedback mechanism is still domain-specific and does not teach the model to transfer knowledge across domains.

## Foundational Learning
- **Automated Planning**: The process of automatically generating sequences of actions to achieve goals from initial states
  - Why needed: Core problem domain being studied
  - Quick check: Can the model generate valid plans for known problems?

- **LLM Fine-tuning**: Adapting pre-trained language models to specific tasks using domain data
  - Why needed: Method for teaching planning to LLMs
  - Quick check: Does fine-tuning improve performance on training domains?

- **Cross-domain Generalization**: The ability to apply learned skills to new, unseen domains
  - Why needed: Key evaluation metric for transfer learning
  - Quick check: Performance on held-out domains compared to training domains

## Architecture Onboarding
**Component Map**: LLM -> Fine-tuning Pipeline -> Validator -> Performance Metrics
**Critical Path**: Domain-Problem Input -> LLM Generation -> VAL Validator -> Success/Failure Output
**Design Tradeoffs**: Supervised learning provides stable convergence but slow learning vs. RL provides faster convergence but domain-specific adaptation
**Failure Signatures**: 0% success on unseen domains, performance drops with symbol anonymization, faster RL convergence without generalization improvement
**First Experiments**: 1) Test different domain anonymization schemes, 2) Compare RL vs supervised learning on held-out domains, 3) Evaluate model sensitivity to plan representation formats

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those explored in the study.

## Limitations
- Narrow domain scope with only 10 IPC 2023 domains limits generalizability of findings
- 0% cross-domain performance on two specific domains may reflect domain-specific rather than general LLM limitations
- Does not explore impact of different LLM sizes or architectures on generalization gap
- Verifier-reward RL limitations not fully characterized despite faster convergence

## Confidence
- High confidence in in-domain performance metrics (82.9% valid plan rate)
- Medium confidence in cross-domain generalization failure interpretation
- Medium confidence in surface representation sensitivity findings
- Low confidence in broader implications about LLM planning competence

## Next Checks
1. Test the same fine-tuning approach on a larger and more diverse set of planning domains beyond IPC 2023
2. Compare performance of different LLM sizes and architectures to determine scale-dependency of generalization gap
3. Conduct ablation studies on verifier-reward RL approach to identify key aspects of validation feedback