---
ver: rpa2
title: 'HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy
  Enhancement and Dual-Objective Optimization'
arxiv_id: '2508.04010'
source_url: https://arxiv.org/abs/2508.04010
tags:
- policy
- agent
- task
- utility
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization

## Quick Facts
- arXiv ID: 2508.04010
- Source URL: https://arxiv.org/abs/2508.04010
- Reference count: 14
- Primary result: Achieves 92.5% PCR and 0.833 CuP on ST-WebAgentBench benchmark

## Executive Summary
HarmonyGuard introduces a multi-agent architecture that jointly optimizes safety and utility for LLM-based web agents. The system uses adaptive policy enhancement with dual-objective optimization to defend against prompt injection and environmental attacks while maintaining task completion rates. By employing second-order Markovian evaluation and metacognitive intervention, HarmonyGuard achieves Pareto-optimal balance between policy compliance and task performance on benchmarks ST-WebAgentBench and WASP.

## Method Summary
HarmonyGuard employs a three-agent architecture: Policy Agent (extracts/structures policies from documents), Web Agent (executes tasks), and Utility Agent (evaluates reasoning transitions and generates optimization guidance). The system uses second-order Markovian evaluation to assess policy compliance at adjacent reasoning steps, maintaining tiered bounded queues of violation references filtered by semantic similarity. When violations are detected, the Utility Agent generates metacognitive intervention prompts to guide the Web Agent's re-reasoning, enabling in-situ correction without model fine-tuning.

## Key Results
- Achieves 92.5% Policy Compliance Rate (PCR) on ST-WebAgentBench, outperforming No Defense baseline (88.7%)
- Maintains 0.833 Completion under Policy (CuP) on ST-WebAgentBench, demonstrating effective safety-utility balance
- Second-Order Markovian evaluation achieves 0.867 PCR/0.060 CuP, outperforming full-trajectory evaluation (0.869/0.042)

## Why This Works (Mechanism)

### Mechanism 1: Second-Order Markovian Evaluation Strategy
Evaluating reasoning transitions (rt-1 → rt) rather than full trajectories improves safety-utility balance by capturing temporally adjacent violations while avoiding false positives from early-stage misattribution. The Utility Agent constructs two-state context windows and applies LLM-based evaluators to detect policy violations and task deviations.

### Mechanism 2: Feedback-Enhanced Policy Update Pipeline
Maintaining tiered bounded queues of violation references filtered by semantic similarity (>85%) enables adaptive policy refinement without overfitting to redundant incidents. The Policy Agent applies Gestalt pattern matching to filter near-duplicates, then inserts remaining violations into FIFO queues with lengths proportional to risk level.

### Mechanism 3: Metacognitive Intervention via Optimization Guidance
Injecting structured reflection prompts when violations are detected enables in-situ reasoning correction without model fine-tuning. Upon detecting violations, the Utility Agent generates optimization guidance containing threat explanation, task alignment assessment, and corrective instructions.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: The paper frames web agent operations within CMDP theory to justify constraint checking over reasoning sequences
  - Quick check question: Can you explain how a CMDP differs from a standard MDP, and why constraint violations might exhibit temporal structure?

- **Concept: Pareto Optimality in Multi-Objective Optimization**
  - Why needed here: HarmonyGuard claims to achieve Pareto-optimal balance between safety and utility
  - Quick check question: If method A has (PCR=0.9, CuP=0.7) and method B has (PCR=0.85, CuP=0.8), which dominates on the Pareto frontier?

- **Concept: Prompt Injection Attack Vectors**
  - Why needed here: The WASP benchmark evaluates plaintext and URL injection attacks
  - Quick check question: How does indirect prompt injection via web content differ from direct jailbreak attacks on chat models?

## Architecture Onboarding

- **Component map:**
  External Documents → Policy Agent → Structured Policy DB
  ↓
  User Task → Web Agent ←→ Utility Agent → Optimization Guidance
  ↑                         ↓
  └── Violation References → Policy Agent → Policy Update

- **Critical path:** Policy Agent initialization → Web Agent receives task → Utility Agent evaluates (rt-1, rt) → If violation: generate guidance → Web Agent re-reasons → If confirmed violation: update policy queue

- **Design tradeoffs:**
  - Second-order vs. full-trajectory evaluation: Second-order reduces false positives but may miss distributed attacks
  - Similarity threshold (85%): Higher values retain more diverse violations but risk missing subtle variations
  - Queue length by risk level: Longer queues for high-risk threats improve retention but increase memory and retrieval latency

- **Failure signatures:**
  - Excessive corrections: Full-trajectory evaluation causes misattribution of early violations
  - Stagnant policy DB: If similarity threshold too high, novel violations filtered out
  - Guidance ignored: If Web Agent's context window saturates, optimization guidance fails

- **First 3 experiments:**
  1. Ablation on evaluation window size: Compare first-order, second-order, and full-trajectory evaluation on PCR/CuP tradeoff
  2. Sensitivity analysis on similarity threshold: Test θ ∈ {0.70, 0.80, 0.85, 0.90, 0.95}
  3. Attack pattern analysis: Categorize false negatives by whether violations span adjacent vs. non-adjacent steps

## Open Questions the Paper Calls Out

### Open Question 1
How can cross-modal threat detection be integrated to defend against non-textual attacks like malicious QR codes or image scripts?
- Basis in paper: Appendix B states that enhancing cross-modal threat detection can substantially improve comprehensive security supervision
- Why unresolved: The current framework focuses primarily on textual reasoning without mechanisms to process visual or code-based injections
- What evidence would resolve it: Evaluation results on multimodal benchmarks demonstrating defense against image-based prompt injections

### Open Question 2
Does moving from chunk-based evaluation to token-level intervention improve the safety-utility trade-off?
- Basis in paper: Appendix B suggests fine-grained policy control at sentence, semantic unit, or token level allows for more precise violation detection
- Why unresolved: Current "Second-Order Markovian Evaluation" assesses reasoning steps as blocks, which may miss subtle, intra-step policy violations
- What evidence would resolve it: Comparative analysis showing token-level monitoring reduces false negatives without significantly increasing latency

### Open Question 3
What is the computational latency overhead introduced by the multi-agent collaboration during long-sequence operations?
- Basis in paper: The paper claims "Markovian real-time reasoning" but evaluates only safety/utility metrics, omitting latency cost
- Why unresolved: The architecture requires multiple LLM inferences per step, which may bottleneck real-time web interactions
- What evidence would resolve it: Time-to-completion benchmarks comparing HarmonyGuard against single-agent baselines

## Limitations
- Policy Agent initialization lacks detail on initial policy extraction process from government and platform sources
- Environmental consistency is uncertain due to live web interactions in benchmarks without reported variance across runs
- Semantic similarity threshold (0.85) appears arbitrary without theoretical justification for why this value optimizes the tradeoff

## Confidence
- **High Confidence:** Core safety-utility tradeoff results (PCR/CuP metrics) are well-supported by methodology
- **Medium Confidence:** Markovian evaluation strategy's effectiveness relies on assumption of temporal locality of violations
- **Low Confidence:** Policy update mechanism's long-term adaptation capability lacks validation for extended deployment periods

## Next Checks
1. **Temporal Distribution Analysis:** Analyze violation timestamps across reasoning steps to quantify prevalence of distributed attacks violating second-order Markovian assumption
2. **Threshold Sensitivity Sweep:** Systematically evaluate impact of similarity thresholds (0.70 to 0.95) on policy database growth and adaptation quality
3. **Environmental Robustness Testing:** Re-run benchmarks across different time periods with controlled environmental perturbations to isolate method performance