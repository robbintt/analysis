---
ver: rpa2
title: Training Models to Detect Successive Robot Errors from Human Reactions
arxiv_id: '2510.09080'
source_url: https://arxiv.org/abs/2510.09080
tags:
- robot
- error
- errors
- successive
- reactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using machine learning to detect successive
  robot errors from human reactions during HRI. The authors extract behavioral features
  (facial, body pose, audio, text) from video data of 26 participants interacting
  with a robot making repeated conversational errors.
---

# Training Models to Detect Successive Robot Errors from Human Reactions

## Quick Facts
- arXiv ID: 2510.09080
- Source URL: https://arxiv.org/abs/2510.09080
- Reference count: 15
- Binary error detection achieved 93.5% accuracy using multimodal human reaction data

## Executive Summary
This paper explores using machine learning to detect successive robot errors from human reactions during HRI. The authors extract behavioral features (facial, body pose, audio, text) from video data of 26 participants interacting with a robot making repeated conversational errors. They develop models to classify robot error stages, training on individual participants to capture unique error-response patterns. The top-performing model for binary error detection achieved 93.5% accuracy, while the best model for classifying successive error stages reached 84.1% accuracy.

## Method Summary
The authors collected video data from 26 participants interacting with a robot that made repeated conversational errors. They extracted multimodal features using OpenFace (facial action units, gaze), OpenPose (upper body keypoints), openSMILE (audio), and CLIP/BERT (text). The data was labeled at the frame level with four error stages (NoError, Error1, Error2, Error3). Models were trained using 26-fold cross-validation with an 80/10/10 train/validation/test split over 50 epochs. They tested various fusion strategies (early, intermediate, late) and feature representations (raw, normalized, PCA) using LSTM and GRU architectures, training separately on each participant to capture personalized error-response patterns.

## Key Results
- Binary error detection achieved 93.5% accuracy using facial features with PCA and late fusion
- Multiclass successive error classification reached 84.1% accuracy using pose, facial, and audio features with normalization and late fusion
- Per-participant training captured unique error-response patterns, with models achieving high accuracy on individual behavioral signatures

## Why This Works (Mechanism)

### Mechanism 1: Temporal Escalation of Human Behavioral Responses
Human reactions to repeated robot failures intensify in predictable patterns that can be learned as distinct error-stage signatures. Successive failures trigger escalating responses—from confusion and speech adjustments (Error1) to visible frustration with raised eyebrows, pursed lips, and impatient vocal tones (Error2/Error3). These temporal patterns manifest across facial action units, pose keypoints, and audio prosody, creating learnable trajectories for sequential models.

### Mechanism 2: Individualized Behavioral Modeling
Per-participant model training captures personalized error-signaling behaviors that outperform generalized approaches. Each person has unique baseline mannerisms and idiosyncratic ways of expressing confusion/frustration. Training on single participants allows models to learn these personalized signatures rather than averaging across heterogeneous response styles.

### Mechanism 3: Multimodal Fusion with Modality-Specific Contributions
Combining complementary modalities (facial, pose, audio) with appropriate fusion timing enables robust classification across error types. Facial features capture micro-expressions, audio captures prosodic shifts, pose captures body language tension. Late fusion (separate modality processing, then prediction combination) allows each modality to develop specialized representations before integration.

## Foundational Learning

- **Concept: Recurrent Neural Networks (LSTM/GRU) for Sequential Dependencies**
  - Why needed here: Human reactions unfold temporally; LSTMs/GRUs capture escalation dynamics across frames that static classifiers would miss.
  - Quick check question: Why would an LSTM detect "building frustration" better than processing each frame independently?

- **Concept: Fusion Strategy Taxonomy (Early/Intermediate/Late)**
  - Why needed here: The paper systematically compares fusion approaches; optimal timing depends on modality alignment and noise characteristics.
  - Quick check question: In late fusion, do you concatenate features or concatenate predictions?

- **Concept: Multi-class vs Binary Classification Trade-offs**
  - Why needed here: Binary detection achieves 93.5% but 4-class successive error detection drops to 84.1%; finer granularity increases difficulty.
  - Quick check question: If Error2 and Error3 reactions are highly similar, would precision for those classes increase or decrease?

## Architecture Onboarding

- **Component map:**
  Feature Extraction (OpenFace, OpenPose, openSMILE, CLIP/BERT) -> Preprocessing (Normalization/PCA) -> Sequence Model (LSTM/GRU) -> Classification Head (Fully connected + softmax) -> Fusion (Early/Intermediate/Late)

- **Critical path:**
  1. Frame extraction with error annotations -> 2. Per-modality feature extraction -> 3. Normalization/PCA -> 4. Sequence windowing -> 5. RNN forward pass -> 6. Fusion (if multi-modal) -> 7. Classification output

- **Design tradeoffs:**
  Binary (93.5%) vs Multiclass (84.1%): Simpler detection vs richer error staging
  PCA reduces dimensions but may discard subtle discriminative features
  Single-participant training captures personalization but does not generalize to new users

- **Failure signatures:**
  First Error to Successive Error Generalization: 74% accuracy—model trained on NoError+Error1 struggles to recognize Error2/Error3, indicating escalation patterns don't linearly extrapolate
  High variance in multiclass F1 (±0.123) suggests some participants' Error1/Error2/Error3 responses overlap significantly

- **First 3 experiments:**
  1. Baseline replication: Extract facial features, apply PCA, train binary LSTM with late fusion on your data—target >90% accuracy to validate pipeline.
  2. Modality ablation: Train with (a) facial only, (b) facial+audio, (c) facial+audio+pose to identify which modality contributes most in your HRI context.
  3. Cross-participant generalization test: Train on N-1 participants, test on held-out participant to measure generalization gap vs. within-participant performance.

## Open Questions the Paper Calls Out
- Can error detection models trained on individual users effectively generalize to entirely new, unseen participants? The authors explicitly state this was not evaluated and should be future work.
- Do the reaction patterns identified for conversational failures transfer to other types of robot errors, such as physical or navigation failures? The study is restricted to conversational failures.
- How can models be improved to better generalize from the first robot error to subsequent successive errors? The "First Error to Successive Errors Generalization" task yielded significantly lower accuracy (74.0%).

## Limitations
- Dataset generalizability: Results based on single HRI scenario with 26 participants, limiting external validity
- Model generalization: Per-participant training captures personalized patterns but fails to generalize across users
- Hyperparameter ambiguity: Critical training details (hidden dimensions, learning rates, optimizer choice) are unspecified

## Confidence
- High Confidence: Binary error detection (93.5% accuracy) and predictable intensification of human reactions
- Medium Confidence: Multimodal fusion strategies and individualized modeling effectiveness
- Low Confidence: Generalization of error-stage classification (84.1%) to new participants and real-world HRI deployments

## Next Checks
1. **Cross-participant generalization test**: Train on N-1 participants and test on held-out participant to measure the gap between within-participant (93.5%/84.1%) and cross-participant performance.
2. **Dataset accessibility verification**: Attempt to access the original video data from prior work [5] or replicate the data collection protocol to enable independent validation.
3. **Hyperparameter sensitivity analysis**: Systematically vary hidden dimensions, learning rates, and PCA variance retention to determine the stability of reported accuracies across different configurations.