---
ver: rpa2
title: Optimization of the quantization of dense neural networks from an exact QUBO
  formulation
arxiv_id: '2510.16075'
source_url: https://arxiv.org/abs/2510.16075
tags:
- quantization
- qubo
- problem
- neural
- sxsw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a post-training quantization method for dense
  neural networks based on a novel ADAROUND-based QUBO formulation. The key innovation
  is an explicit QUBO formulation that minimizes the Frobenius distance between the
  theoretical output and the de-quantized output (before activation), where binary
  variables represent rounding choices for each weight and bias.
---

# Optimization of the quantization of dense neural networks from an exact QUBO formulation

## Quick Facts
- **arXiv ID:** 2510.16075
- **Source URL:** https://arxiv.org/abs/2510.16075
- **Reference count:** 40
- **Primary result:** QUBO-ADAROUND method outperforms round-to-nearest quantization at int2 precision, particularly for low-bit dense network quantization

## Executive Summary
This paper proposes a post-training quantization method for dense neural networks based on an explicit QUBO formulation. The key innovation is treating the rounding direction of weights and biases as binary variables, optimizing them to minimize the Frobenius distance between theoretical and de-quantized outputs. The method exploits the structure of the QUBO coefficient matrix to decompose the global problem into independent subproblems, enabling efficient solution via heuristics like simulated annealing. Experiments demonstrate superior accuracy retention at low bit-widths (particularly int2) compared to standard rounding methods, though performance degrades significantly at int1 precision.

## Method Summary
The method formulates post-training quantization as a QUBO problem where binary variables represent rounding choices for each weight and bias. The objective function minimizes the Frobenius distance between the theoretical pre-activation output and the de-quantized output. The QUBO coefficient matrix has a specific block structure that allows exact decomposition into n independent subproblems of size f+1 (where n is the output dimension and f is the input dimension). These subproblems are solved using simulated annealing, and the resulting binary vectors determine whether each weight/bias is rounded up or down. The approach is validated on MNIST, Fashion-MNIST, EMNIST, and CIFAR-10 datasets across integer precisions from int8 to int1.

## Key Results
- QUBO-ADAROUND outperforms round-to-nearest quantization at int2 precision, with "Step" improvements showing significant accuracy gains
- The method shows strong linear correlation between QUBO cost and accuracy at int2, but this correlation weakens at int8
- Performance collapses at int1 precision, with accuracy dropping to ~10% (random guess level)
- The decomposition approach enables efficient solving of layer-wise quantization problems

## Why This Works (Mechanism)

### Mechanism 1: Binary Rounding as Output Reconstruction
- Claim: Optimizing the rounding direction (up vs. down) minimizes the reconstruction error of the layer's output better than greedy rounding.
- Mechanism: Instead of rounding weights to the nearest integer (RTN), the method treats the rounding decision for each weight as a binary variable $v \in \{0,1\}$. It minimizes the Frobenius distance between the theoretical pre-activation output $y$ and the de-quantized output $\check{y}(v)$, ensuring the quantized layer behaves mathematically similar to the original.
- Core assumption: Minimizing the layer-wise output difference (pre-activation) correlates strongly with maintaining the final classification accuracy of the network.
- Break condition: If the relationship between the Frobenius norm of pre-activation outputs and the final task loss is weak or non-monotonic, this mechanism may fail to improve accuracy.

### Mechanism 2: Structural Decoupling via Matrix Block Diagonalization
- Claim: The global optimization problem for a dense layer contains independent substructures that can be solved in parallel.
- Mechanism: The paper demonstrates that the QUBO coefficient matrix $M$ has a specific block structure. This allows the global problem (variables for all weights and biases) to be exactly decomposed into $n$ independent subproblems (where $n$ is the output dimension), each of size $f+1$ (input dimension + bias).
- Core assumption: The independence assumption holds exactly for the layer-wise formulation derived, meaning no higher-order cross-neuron dependencies exist in the pre-activation output calculation.
- Break condition: If applied to layers with strong inter-neuron dependencies not captured by the standard dense formulation (e.g., specific normalization layers), the decoupling might be invalid.

### Mechanism 3: Heuristic Approximation for NP-hard Subproblems
- Claim: Sub-optimal solutions found via Simulated Annealing (SA) are sufficient to outperform deterministic rounding methods at low precision.
- Mechanism: Because QUBO is NP-hard, the method uses Simulated Annealing to approximate the binary vector $v$ rather than solving exactly. This probabilistic approach efficiently navigates the solution space to find configurations that reduce the cost function $C_{QUBO}$ without the exponential cost of exact solvers.
- Core assumption: The solution landscape allows SA to find a "good enough" local minimum that significantly beats the deterministic RTN baseline.
- Break condition: If the problem size scales massively (very wide layers) or the energy landscape is too flat, SA may fail to converge to useful solutions within reasonable time.

## Foundational Learning

- **Concept: Quadratic Unconstrained Binary Optimization (QUBO)**
  - Why needed here: This is the mathematical language of the solver. Understanding that the problem is mapped to minimizing $v^T Q v$ is essential to grasp how the rounding decisions are optimized.
  - Quick check question: Can you explain why a binary variable $v_{ij}$ is suitable for representing the choice between rounding a weight up or down?

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: The paper targets PTQ specifically to avoid the high computational cost of retraining. Understanding this context explains why "calibration data" (a small subset of test data) is used instead of full backpropagation.
  - Quick check question: Why does the paper average the QUBO matrix $M$ over a dataset subset rather than computing it for a single input?

- **Concept: Frobenius Norm and Pre-Activation Output**
  - Why needed here: The paper minimizes the error *before* the activation function. This is a mathematical convenience to keep the problem quadratic (QUBO) rather than higher-order.
  - Quick check question: Why would minimizing error after the activation function (e.g., post-ReLU) complicate the QUBO formulation?

## Architecture Onboarding

- **Component map:**
  Calibration dataset -> QUBO Constructor (builds $S_i$ matrices) -> Solver Engine (Simulated Annealing) -> Quantizer (applies rounding decisions) -> Quantized model

- **Critical path:** The construction of the QUBO matrices $S_i$ and the subsequent solver execution. Any error in the derivation of $d_i$ or $S_i$ (Eq. 30) propagates directly into invalid rounding decisions.

- **Design tradeoffs:**
  - **Solver Time vs. Accuracy:** Simulated Annealing is fast but sub-optimal; exact solvers (Gurobi/CPLEX) are mentioned but likely too slow for production. Quantum solvers are suggested as a future alternative.
  - **Calibration Size:** More data averages the cost function better but increases setup time.
  - **Bit-width:** The method shines at **int2** but fails catastrophically at **int1**. It is strictly designed for "low but not binary" precision.

- **Failure signatures:**
  - **int1 Collapse:** Table 2 shows accuracy drops to ~10% (random guess) for binary weights. *Do not deploy for int1.*
  - **Metric Mismatch:** Figure 2 shows that at int8, the QUBO cost $C_{QUBO}$ does not correlate with accuracy. This indicates the method is only useful when quantization noise is high enough to justify the complex rounding.
  - **Scaling:** If the input dimension $f$ is very large (e.g., >1000), the $f+1$ variable subproblems might become intractable for the SA solver.

- **First 3 experiments:**
  1. **Baseline Verification:** Implement the RTN vs. QUBO-ADAROUND comparison on a simple 3-layer MNIST network at int2. Verify the "Step" improvement shown in Table 2.
  2. **Sensitivity Analysis:** Plot Accuracy vs. $C_{QUBO}$ for a medium-sized network (e.g., CIFAR-10 model) to see if the linear correlation observed in Figure 1 holds for deeper architectures.
  3. **Scaling Stress Test:** Increase the width of a dense layer and measure the time taken by the Simulated Annealing solver to find the rounding variables, identifying the bottleneck point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for the empirical relationship between the QUBO cost function ($C_{QUBO}$) and the classification accuracy?
- Basis in paper: [explicit] The authors state that while a linear dependence was observed for int2 quantization, "a satisfactory theoretical explanation for this behavior has not yet been established."
- Why unresolved: The paper provides empirical evidence of correlation (Fig. 1) but lacks a formal derivation linking the minimization of the Frobenius distance (QUBO objective) to the preservation of classification accuracy.
- Evidence to resolve: A formal proof or analysis showing that minimizing the specific QUBO formulation mathematically guarantees the preservation of the network's decision boundaries.

### Open Question 2
- Question: Can the binary rounding formulation be generalized to allow quantization to a broader range of integer values rather than just the nearest upper or lower integers?
- Basis in paper: [explicit] The conclusion identifies as a promising direction "a formulation that enables quantization to a broader range of integer values rather than restricting it to the nearest integers."
- Why unresolved: The current QUBO-ADAROUND method restricts binary variables to a binary choice (floor or ceiling), which limits the search space and may contribute to performance collapse at very low bit-widths (int1).
- Evidence to resolve: A modified QUBO formulation with discrete variables representing a wider set of integer candidates that successfully recovers accuracy at int1 precision.

### Open Question 3
- Question: Does global optimization of the entire network provide superior quantization compared to the proposed layer-wise decomposition?
- Basis in paper: [inferred] The authors treat each layer independently to avoid higher-order terms (Section 3), stating, "Minimizing the full network at once would introduce higher-order terms... The procedure to follow consists of expressing the coefficients... for a single generic layer."
- Why unresolved: It is undetermined if the error accumulation from independent layer optimization is negligible or if a global approach (handling higher-order interactions) would yield higher fidelity.
- Evidence to resolve: A comparative study evaluating accuracy retention between the current layer-wise decomposition and a global optimization method that accounts for inter-layer dependencies.

### Open Question 4
- Question: Can the method scale to complex architectures like LSTMs or LLMs despite the scalability limits of current QUBO solvers?
- Basis in paper: [explicit] The conclusion notes, "Future work could explore the application of this methodology to... alternative neural network architectures, such as LSTMs or LLMs," while acknowledging solvers "face scalability problems."
- Why unresolved: The experiments were limited to small dense networks; the decomposition method relies on subproblems of size $f+1$ (input dimension), which may become intractable for large embedding layers in LLMs.
- Evidence to resolve: Successful application of the decomposition method to a transformer-based model, demonstrating that the subproblem size or number of subproblems remains manageable for heuristics like simulated annealing.

## Limitations

- **Precision dependency:** The method shows significant accuracy gains at int2 but catastrophic failure at int1, limiting practical applicability to extreme low-bit scenarios.
- **Solver parameter sensitivity:** Reliance on "default configuration" for simulated annealing introduces reproducibility concerns and potential performance variability.
- **Weak correlation at high bit-widths:** The QUBO cost function becomes decorrelated from actual accuracy gains at int8, suggesting limited usefulness for higher precision quantization.

## Confidence

- **High confidence:** The mathematical derivation of the QUBO formulation and its decomposition into independent subproblems (section 4) is rigorous and internally consistent.
- **Medium confidence:** The experimental results showing int2 improvements are compelling, but the lack of ablation studies on calibration data size and solver hyperparameters limits generalizability.
- **Low confidence:** The claim about QUBO's broad applicability to other architectures (CNNs, transformers) remains untested and speculative.

## Next Checks

1. **Ablation on calibration size:** Systematically vary the calibration dataset percentage (1%, 5%, 10%, 25%) to determine the minimum required for stable QUBO performance.
2. **Solver hyperparameter sensitivity:** Test multiple simulated annealing configurations (temperature schedules, iteration counts) to establish performance bounds and identify optimal settings.
3. **Cross-architecture validation:** Apply the QUBO-ADAROUND method to at least one convolutional layer to verify the claimed generalizability beyond dense layers.