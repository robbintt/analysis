---
ver: rpa2
title: 'DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document
  Evaluation Workflows'
arxiv_id: '2511.05496'
source_url: https://arxiv.org/abs/2511.05496
tags:
- evaluation
- document
- criteria
- layer
- docuev
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DOCUEVAL is an AI engineering tool for building customisable document
  evaluation workflows using LLMs. It addresses software engineering challenges including
  customisation, accuracy, scalability, privacy, and meaningful human oversight through
  configurable reviewer roles, reasoning strategies, and evaluation criteria.
---

# DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows

## Quick Facts
- arXiv ID: 2511.05496
- Source URL: https://arxiv.org/abs/2511.05496
- Authors: Hao Zhang; Qinghua Lu; Liming Zhu
- Reference count: 8
- Key outcome: AI engineering tool for building customisable document evaluation workflows using LLMs with configurable reviewer roles, reasoning strategies, and evaluation criteria

## Executive Summary
DOCUEVAL is an AI engineering tool designed to address software engineering challenges in document evaluation workflows using large language models. The tool provides configurable reviewer roles, reasoning strategies, and evaluation criteria while maintaining accuracy, scalability, privacy, and meaningful human oversight. Through comprehensive logging, source attribution, and side-by-side comparison capabilities, DOCUEVAL enables systematic comparison of different evaluation configurations. The architecture supports advanced document processing including non-textual elements through OCR and markdown conversion.

## Method Summary
DOCUEVAL implements a six-layer architecture combining React/Next.js frontend, OCR/markdown conversion for document processing, RAG-based segmentation, and criterion-by-criterion evaluation. The system uses a multi-store data architecture (relational DB, vector DB, file storage) with multi-layer guardrails for validation and human oversight through side-by-side comparison interfaces. The workflow orchestrates evaluation using configurable reasoning strategies and aggregates scores through weighted averaging, with all outputs requiring source attribution for verification.

## Key Results
- Implements configurable reviewer roles derived from formal sources rather than informal persona prompts
- Provides criterion-by-criterion evaluation to manage context limitations and improve evidence grounding
- Enables systematic comparison of different evaluation configurations through comprehensive logging and traceability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Criterion-by-criterion evaluation reduces context window limitations and improves evidence grounding.
- Mechanism: The workflow orchestrator decomposes evaluation into discrete criterion-specific analyses rather than bulk document processing. Each criterion draws on distinct evidence pieces, which are aggregated into a final score via configurable weighting (e.g., weighted average). This segmentation aligns context demands with LLM context limits.
- Core assumption: Smaller, focused context windows per criterion yield more thorough analysis than whole-document evaluation.
- Evidence anchors:
  - [abstract] "experiment with different reasoning strategies and choose the assessment style"
  - [section] "The workflow orchestrator service coordinates workflows by assessing each criterion individually. This approach mitigates context limitations (accuracy challenge) by reducing per-assessment context demands, enabling thorough evaluation of long or complex documents while maintaining consistency across criteria."
  - [corpus] "Failure Modes in LLM Systems" (arXiv:2511.19933) provides system-level taxonomy relevant to context handling, though not criterion decomposition specifically.
- Break condition: When evaluation criteria are highly interdependent and cannot be meaningfully isolated without losing cross-criterion insights.

### Mechanism 2
- Claim: Multi-layered guardrails improve output reliability by intercepting different failure modes at appropriate system layers.
- Mechanism: Guardrails operate at four layers: (1) UI input validation for format/structure; (2) core processing validation for PII/prompt injection; (3) evaluation engine validation for hallucination detection and source linking; (4) data management layer for access control and encryption.
- Core assumption: Complementary validations at multiple layers catch more failures than single-layer approaches.
- Evidence anchors:
  - [abstract] "comprehensive logging, source attribution"
  - [section] "guardrails focus on validating intermediate steps to detect hallucinations, factual inaccuracies, or unsupported claims, requiring all generated outputs to link back to verified sources"
  - [corpus] "Failure Modes in LLM Systems" (arXiv:2511.19933) supports multi-layer failure detection approaches for LLM systems in production.
- Break condition: When guardrails introduce unacceptable latency, over-block legitimate outputs, or create conflicting validation rules across layers.

### Mechanism 3
- Claim: Theory-grounded reviewer roles produce more consistent evaluations than ad-hoc persona prompts.
- Mechanism: Reviewer roles are derived from formal sources (award criteria, expert guidelines, empirical studies on review quality) rather than informal descriptions like "act like a harsh reviewer." This grounds role behavior in documented, testable frameworks.
- Core assumption: Formalized, theory-based prompts yield more consistent LLM behavior than persona-based instructions.
- Evidence anchors:
  - [abstract] "theory-grounded reviewer roles"
  - [section] "Rather than instructing the evaluator to 'act like a harsh reviewer' or 'be a supportive reviewer,' theory-grounded roles are built on formalised, well-defined theories. For example... documents used by program committees to select the best reviewers for recognition or awards"
  - [corpus] Weak direct evidence—no neighboring papers specifically address theory-grounded roles versus personas.
- Break condition: When domains lack established theoretical frameworks, formal evaluation criteria, or documented best practices for the task.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) with structural segmentation
  - Why needed here: Core processing layer segments documents by structural sections for improved retrieval accuracy.
  - Quick check question: Can you explain why preserving document structural boundaries (vs. fixed-size chunking) might improve retrieval for evaluation tasks?

- Concept: Human-in-the-Loop (HITL) design patterns
  - Why needed here: Human oversight layer requires understanding when and how to intervene meaningfully.
  - Quick check question: What is the difference between requiring human review before AI results are shown versus showing AI results first for validation?

- Concept: Multi-store data architecture
  - Why needed here: DOCUEVAL uses relational DB, vector DB, and file storage—each optimized for specific workloads.
  - Quick check question: Why store embeddings in a vector database rather than the relational database used for evaluator profiles?

## Architecture Onboarding

- Component map:
  - User Interface Layer: React/Next.js web app for document management, criteria/role configuration, results visualization
  - Core Processing Layer: OCR + markdown conversion, segmentation/embedding for RAG, criteria extraction, context aggregation
  - Evaluation Engine Layer: Workflow orchestrator, evaluator modules, dynamic query generator, score aggregator
  - Data Management Layer: Relational DB (profiles, criteria, configs), Vector DB (embeddings), File storage (raw/processed documents), Audit logs
  - Guardrails Layer: Input validation, PII/prompt-injection detection, hallucination checking, access control
  - Human Oversight Layer: Side-by-side comparison, explanation packs with rationale tables, feedback annotation

- Critical path:
  Document upload → OCR/markdown conversion → Structural segmentation → Criteria extraction → Workflow configuration (roles, reasoning strategy, assessment style) → Criterion-by-criterion evaluation → Score aggregation → Human review with explanation pack

- Design tradeoffs:
  - **Flexibility vs. onboarding complexity**: Many configuration options support diverse use cases but increase learning curve.
  - **Accuracy vs. latency**: Criterion-by-criterion evaluation improves thoroughness but adds processing time.
  - **Privacy vs. model capability**: Local deployment options may limit access to more capable cloud-hosted models.

- Failure signatures:
  - **Misaligned evaluations**: Check if criteria guidance documents match actual assessment intent.
  - **Hallucinated claims**: Verify guardrails are catching outputs without source attribution.
  - **Inconsistent run-to-run results**: Review prompt versioning, configuration management, and non-deterministic sampling settings.
  - **Poor non-textual element handling**: Inspect OCR/markdown conversion quality for tables and figures.

- First 3 experiments:
  1. Run single-document evaluation with default settings to establish baseline accuracy and review the explanation pack for source attribution.
  2. Compare reasoning-before-scoring vs. reasoning-after-scoring on the same document to observe differences in rationale quality and score consistency.
  3. Conduct side-by-side human vs. AI evaluation to calibrate expectations and identify systematic divergence patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation-driven learning mechanisms be formally integrated to automate the continuous improvement of evaluator configurations?
- Basis in paper: [explicit] The Conclusion states that "Future work will focus on integrating more advanced evaluation-driven learning mechanisms to support the continuous improvement of evaluators."
- Why unresolved: While the tool currently logs human feedback and allows manual configuration updates, it lacks an automated loop to translate this feedback into refined criteria or role definitions.
- What evidence would resolve it: An empirical study demonstrating that evaluators utilizing automated feedback loops achieve statistically significant improvements in accuracy over time compared to static configurations.

### Open Question 2
- Question: What standardized metrics and benchmarks are required to determine if an LLM-based evaluator is "good enough" for deployment in high-stakes domains?
- Basis in paper: [explicit] The Abstract identifies "how to determine whether evaluators are 'good enough' for deployment" as a core challenge the tool helps address, implying the threshold itself remains an open issue.
- Why unresolved: The tool provides side-by-side comparisons and traceability, but defining a universal quantitative threshold for deployment readiness across different domains is not established.
- What evidence would resolve it: A validated framework of metrics (e.g., consistency, error rate) that correlates strongly with expert human judgment across diverse document evaluation tasks.

### Open Question 3
- Question: How does the system's accuracy and latency perform under large-scale batch processing compared to baseline methods?
- Basis in paper: [inferred] The paper claims to address the scalability challenge of "high document volumes," but the evaluation is limited to a single academic paper review simulation.
- Why unresolved: A single-case demonstration cannot validate the system's robustness or latency when processing the "vast numbers of documents" identified as a key challenge in Section 2.
- What evidence would resolve it: Benchmarking results detailing throughput and error rates when processing datasets of varying sizes (e.g., 100 vs. 10,000 documents) against non-segmented or non-RAG baselines.

## Limitations
- No quantitative evaluation data provided—no accuracy metrics, benchmark comparisons, or ablation studies
- Specific implementation details remain unclear, including exact LLM models, prompts, hyperparameters, and guardrail detection mechanisms
- Theory-grounded reviewer roles claim lacks supporting evidence from the literature

## Confidence
- **High confidence**: The multi-layer architecture with distinct processing layers, the criterion-by-criterion evaluation approach for context management, and the multi-store data architecture are well-specified and technically sound.
- **Medium confidence**: The guardrail mechanisms are described at a conceptual level but lack implementation specifics that would enable full replication.
- **Low confidence**: The theory-grounded reviewer roles mechanism and its claimed superiority over persona-based approaches, as no empirical evidence or literature support is provided.

## Next Checks
1. **Implement and test the criterion-by-criterion evaluation workflow** with at least two reasoning strategies (reasoning-before-scoring and chain-of-thought) on sample documents to verify the claimed context management benefits and observe any trade-offs in evaluation consistency.

2. **Conduct a small-scale human-AI comparison study** using the side-by-side comparison feature to assess alignment between AI evaluations and human judgments, focusing on identifying systematic divergence patterns and evaluating the usefulness of explanation packs.

3. **Validate the OCR/markdown conversion pipeline** with documents containing complex tables, figures, and non-textual elements to assess whether structural segmentation preserves document hierarchy and whether RAG retrieval remains effective with converted content.