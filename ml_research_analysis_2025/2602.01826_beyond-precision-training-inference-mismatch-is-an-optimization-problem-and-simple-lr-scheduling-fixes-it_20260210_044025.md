---
ver: rpa2
title: 'Beyond Precision: Training-Inference Mismatch is an Optimization Problem and
  Simple LR Scheduling Fixes It'
arxiv_id: '2602.01826'
source_url: https://arxiv.org/abs/2602.01826
tags:
- training
- learning
- mismatch
- arxiv
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the training-inference mismatch problem in
  RL for large language models, which causes optimization instability. The authors
  show that gradient noise and mismatch grow together during training and can be suppressed
  by shrinking update size.
---

# Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It

## Quick Facts
- **arXiv ID:** 2602.01826
- **Source URL:** https://arxiv.org/abs/2602.01826
- **Reference count:** 15
- **Primary result:** Length-triggered learning rate decay stabilizes RL training by suppressing training-inference mismatch, improving peak validation accuracy on Qwen3 models.

## Executive Summary
This paper addresses training-inference mismatch in RL fine-tuning of large language models, which causes optimization instability and eventual collapse. The authors demonstrate that gradient noise and mismatch escalate together during training and can be suppressed by shrinking update size through learning rate decay. They propose a simple, length-triggered learning rate scheduler that decays the rate based on average response length—identified as an early-warning signal of impending instability. Experiments on Qwen3-4B and 8B models show that this approach stabilizes training, maintains mismatch at safe levels, and improves peak validation performance compared to importance sampling or constant LR baselines.

## Method Summary
The method implements a PPO-based RL fine-tuning pipeline using Qwen3-4B/8B-Base models on filtered DAPO math reasoning data (~13k samples). The key innovation is a length-triggered learning rate scheduler that monitors average response length per training step. When a surge (typically 3× increase) is detected, the scheduler initiates decay by halving the learning rate every decay_period steps (empirically set to 1.8× the surge step) until reaching 0.1× the initial LR. The pipeline uses vLLM for rollout and FSDP for training, tracking log ppl_abs_diff as the mismatch metric. Hyperparameters include LR=1e-6 (initial), batch size=64, grad clip=1.0, and max response length=8192.

## Key Results
- Length-triggered LR decay stabilizes training on Qwen3-4B and 8B models, preventing collapse that occurs with constant LR schedules
- The scheduler maintains training-inference mismatch at safe levels throughout training, as measured by log ppl_abs_diff
- Peak validation accuracy improves compared to importance sampling corrections (TIS/MIS) and constant LR baselines
- Response length surge (~3× increase) serves as a reliable early-warning signal, detected ~100-200 steps before collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training-inference mismatch is a dynamic optimization failure, not a static numerical artifact, and scales with gradient noise.
- **Mechanism:** As training progresses, gradient noise and mismatch escalate together. The mismatch magnitude is coupled with model weight location; when the model enters regions with higher curvature or sharpness, numerical discrepancies amplify. Reducing the learning rate shrinks update magnitude, which suppresses both gradient noise effects (quadratically, per Appendix A) and the resulting mismatch.
- **Core assumption:** Malignant optimization dynamics push model weights into geometric regions that amplify numerical discrepancies—a relationship the paper hypothesizes but does not fully characterize.
- **Evidence anchors:**
  - [abstract] "gradient noise and training-inference mismatch escalate in tandem as training progresses... mismatch can be effectively suppressed by shrinking the update size"
  - [Section 3.1, Figure 3-4] Lowering LR from 1e-6 to 1e-7 delays collapse; smoothed gradient norm rises even as true signal should diminish.
  - [corpus] Related work (arXiv:2512.01374, arXiv:2510.26788) also identifies training-inference mismatch as an instability source but proposes different remedies (FP16 precision, surrogate objectives).
- **Break condition:** If mismatch were purely state-independent random noise, shrinking update size would not systematically suppress it. The observed coupling suggests model-state dependency.

### Mechanism 2
- **Claim:** Average response length serves as a reliable early-warning signal for impending instability, triggering proactive LR decay.
- **Mechanism:** Response length surges (tripling within narrow windows) before collapse. Theorem B.1 shows gradient error bounds scale as O(T²) with sequence length T—due to linear accumulation of state-visitation distribution divergence plus summation of per-step divergences. The surge injects significant noise, requiring lower LR to maintain stability. The paper empirically finds decay period ≈1.8× surge time balances speed and stability.
- **Core assumption:** The surge-to-collapse timing relationship is consistent enough across runs that a fixed multiplier (1.8×) generalizes.
- **Evidence anchors:**
  - [Section 3.3, Figure 6] Response length surges from ~1000 to 3000-4000 around step 100 for both Qwen3-4B and 8B.
  - [Section 4.2, Figure 9-10] Decay periods aligned with 1.8× surge time stabilize training; epoch-aligned schedules perform worse on 8B model.
  - [corpus] No direct corroboration of response-length-as-signal in neighbor papers; this appears novel to this work.
- **Break condition:** If response length surge were purely beneficial (longer reasoning chains improving performance) without numerical amplification, it would not serve as a warning signal.

### Mechanism 3
- **Claim:** Importance Sampling (IS) corrections are insufficient for extended training because they reintroduce bias and fail to address the dynamic optimization root cause.
- **Mechanism:** IS (token-level or sequence-level) corrects off-policy likelihood ratios but requires heuristic clipping/masking to curb variance. These modifications break theoretical soundness, reintroduce bias, and cannot prevent eventual collapse. LR scheduling addresses the fundamental dynamic instability by modulating update magnitude rather than correcting per-sample gradients.
- **Core assumption:** IS failure mode is structural, not just a matter of better hyperparameter tuning.
- **Evidence anchors:**
  - [Section 3.1, Figure 2] Token-level TIS prevents collapse but other IS variants fail; all merely extend stable windows.
  - [Section 4.3, Figure 11-12] LR scheduler stabilizes training even when MIS fails; adds benefit on top of TIS.
  - [corpus] arXiv:2512.23087 confirms mismatch has asymmetric effects but proposes vocabulary pruning as alternative remedy.
- **Break condition:** If IS clipping thresholds could be perfectly tuned per-run, bias-variance tradeoff might become manageable—but the paper shows this is fragile across extended training.

## Foundational Learning

- **Concept: Training-Inference Mismatch (Off-Policy Bias)**
  - **Why needed here:** RL for LLMs uses different engines for rollout (vLLM/SGLang) vs. training (FSDP/Megatron). Non-associative floating-point arithmetic causes different operation sequences to yield different probabilities, creating off-policy bias even with identical weights.
  - **Quick check question:** Why can't you simply use the same engine for both rollout and training?

- **Concept: Policy Gradient Variance and IS Corrections**
  - **Why needed here:** The paper critiques IS methods (TIS, MIS) as insufficient. Understanding how IS corrects off-policy distributions—and why clipping/masking reintroduces bias—is essential to appreciate why an optimization-level solution (LR scheduling) may be more robust.
  - **Quick check question:** In Equation 5-6, what does the clipping constant C trade off?

- **Concept: Learning Rate Decay and Gradient Noise**
  - **Why needed here:** Appendix A derives how reducing LR decreases noise penalty quadratically while only linearly slowing progress. This justifies the core intervention: reactive LR decay counters escalating gradient noise.
  - **Quick check question:** In inequality (13), which term shrinks fastest as η→0?

## Architecture Onboarding

- **Component map:** Rollout engine (vLLM/SGLang) -> Training engine (FSDP/Megatron) -> Mismatch monitor -> Length-triggered LR scheduler
- **Critical path:**
  1. Monitor average response length per step
  2. Detect surge (empirically ~3× increase in narrow window)
  3. Set decay_period ≈ 1.8× surge_step
  4. Halve LR every decay_period steps until η_floor (default 0.1× η₀)
- **Design tradeoffs:**
  - Earlier decay → more stability but slower convergence
  - Later decay → faster learning but higher collapse risk
  - Paper finds 1.8× multiplier robust across 4B and 8B models; epoch-based scheduling unreliable because collapse timing doesn't scale with dataset size (Figure 5)
- **Failure signatures:**
  - Sudden drop in validation accuracy and training reward (collapse)
  - Simultaneous spike in log ppl_abs_diff and smoothed gradient norm
  - Response length surge preceding collapse by ~100-200 steps
- **First 3 experiments:**
  1. **Baseline stability check:** Train Qwen3-4B with constant LR=1e-6; log gradient norm, log ppl_abs_diff, response length. Identify collapse point and surge timing.
  2. **Scheduler ablation:** Compare decay_period values (e.g., 1.5×, 1.8×, 2.0× surge time) against constant LR baseline. Track peak validation accuracy.
  3. **IS comparison:** Run token-level TIS and MIS with and without the length-triggered scheduler; confirm scheduler adds benefit regardless of IS patch.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper demonstrates the scheduler works for math reasoning tasks but does not establish whether the response-length surge signal generalizes to other domains
- The mechanism linking model weight geometry to numerical discrepancies is hypothesized but not fully characterized theoretically
- The scheduler relies on task-specific hyperparameters (1.8× multiplier) that may require tuning for different model scales or datasets

## Confidence
- **High confidence:** The empirical demonstration that length-triggered LR decay stabilizes training and improves peak validation performance compared to constant LR and IS baselines (Figure 9-12)
- **Medium confidence:** The claim that mismatch is fundamentally a dynamic optimization problem (not just a numerical artifact), supported by the coupling between gradient noise and mismatch but lacking a complete theoretical characterization
- **Medium confidence:** The identification of response length as a reliable early-warning signal, validated on Qwen3-4B/8B models but without cross-task generalization studies

## Next Checks
1. **Cross-task generalization:** Apply the length-triggered scheduler to non-math RL tasks (e.g., code generation, summarization) to verify the response-length-surge signal is universal rather than task-specific
2. **Hyperparameter robustness:** Systematically vary the decay multiplier (1.5×, 1.8×, 2.0× surge time) and initial LR across different model sizes to establish bounds for stable training and identify failure modes
3. **Mechanism isolation:** Conduct controlled experiments comparing LR decay, IS corrections, and their combination on a simplified synthetic task where gradient noise and mismatch can be precisely measured and manipulated