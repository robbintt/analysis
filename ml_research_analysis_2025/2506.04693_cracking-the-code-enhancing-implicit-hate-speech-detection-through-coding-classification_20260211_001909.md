---
ver: rpa2
title: 'Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding
  Classification'
arxiv_id: '2506.04693'
source_url: https://arxiv.org/abs/2506.04693
tags:
- uni00000011
- hate
- speech
- codetype
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of detecting implicit hate
  speech (im-HS) in text, which is harder to identify than explicit hate speech due
  to its subtle encoding strategies. The authors propose a taxonomy of six rhetorical
  encoding strategies (codetypes) and two methods to incorporate them into large language
  models (LLMs) for im-HS detection: (1) prompt-based method using codetypes in prompts,
  and (2) embedding-based method using LLMs as encoders with codetypes embedded.'
---

# Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification

## Quick Facts
- arXiv ID: 2506.04693
- Source URL: https://arxiv.org/abs/2506.04693
- Authors: Lu Wei; Liangzhi Li; Tong Xiang; Xiao Liu; Noa Garcia
- Reference count: 31
- Key outcome: Embedding-based methods using codetypes achieve up to 7.5% F1 score improvement, with best scores reaching 0.8091 F1

## Executive Summary
This paper addresses the challenge of detecting implicit hate speech (im-HS) in text, which is harder to identify than explicit hate speech due to its subtle encoding strategies. The authors propose a taxonomy of six rhetorical encoding strategies (codetypes) and two methods to incorporate them into large language models (LLMs) for im-HS detection: (1) prompt-based method using codetypes in prompts, and (2) embedding-based method using LLMs as encoders with codetypes embedded. Experiments on Chinese and English datasets show that using codetypes consistently improves im-HS detection, with embedding-based methods achieving up to 7.5% F1 score improvement and best scores reaching 0.8091 F1. The results demonstrate that incorporating linguistic knowledge about codetypes enhances LLMs' effectiveness in detecting implicit hate speech across languages.

## Method Summary
The authors propose a taxonomy of six rhetorical encoding strategies (codetypes): Abbreviation, Metaphor, Irony, Pun, Idiom, and Argot. They use these codetypes to enhance implicit hate speech detection through two approaches: (1) prompt-based methods where codetypes are included in prompts for zero-shot or few-shot classification, and (2) embedding-based methods where LLMs serve as frozen encoders and codetype information is embedded as features for a logistic regression classifier. The embedding-based methods extract multi-head attention outputs from all transformer layers and concatenate them as features, with three variants (EI, EII, EIII) that differ in how codetype and sentence embeddings are combined. The codetype information includes three components: Name, Explanation (Wikipedia definition), and Sample (example usage).

## Key Results
- Embedding-based methods achieve up to 7.5% F1 score improvement over baseline methods
- Best F1 score reaches 0.8091 on Chinese dataset using embedding-based approach
- Codetypes improve detection in both Chinese and English datasets consistently
- Embedding-based methods outperform prompt-based methods by up to 58% F1 score gap
- Overcorrection occurs when too much codetype information is provided, causing misclassification of neutral statements

## Why This Works (Mechanism)

### Mechanism 1
Explicitly encoding rhetorical strategies as codetypes improves im-HS detection by providing LLMs with structured linguistic knowledge about how implicit hate is encoded. The six codetypes serve as a shared vocabulary that constrains the model's interpretation space, reducing ambiguity in classification. This assumes implicit hate speech follows identifiable rhetorical patterns consistent across languages.

### Mechanism 2
Embedding-based methods outperform prompt-based methods because they exploit richer internal representations rather than relying on generative outputs. The embedding-based approach extracts multi-head attention outputs from all transformer layers and concatenates them as features for a downstream classifier, capturing intermediate reasoning states that are lost when using only generated text outputs.

### Mechanism 3
Overloading prompts with excessive codetype information causes "overcorrection," where models misclassify neutral statements containing codetype-like patterns as implicit hate. This occurs because models become hypersensitive to surface-level codetype signals and fail to distinguish between the presence of a codetype and the presence of hateful intent.

## Foundational Learning

- **Concept: Implicit vs. Explicit Hate Speech** - Why needed: The entire method hinges on distinguishing im-HS (subtle, encoded) from ex-HS (direct, overt). Quick check: Can you explain why "txl is a social issue" is harder to detect than "I hate homosexuals"?

- **Concept: Multi-Head Attention Outputs as Features** - Why needed: The embedding-based method relies on extracting MHA outputs from transformer layers. Quick check: Why would MHA outputs from different layers capture different aspects of implicit hate?

- **Concept: Frozen Encoder + Trainable Classifier Pipeline** - Why needed: The method uses LLMs as frozen encoders with a logistic regression classifier. Quick check: What are the tradeoffs of freezing the encoder versus fine-tuning the entire model?

## Architecture Onboarding

- **Component map**: Input text + codetype info → LLM encoder → MHA extraction → Embedding aggregation → Classifier → im-HS/neutral prediction
- **Critical path**: Input text + codetype info → LLM encoder → MHA extraction → Embedding aggregation → Classifier → im-HS/neutral prediction
- **Design tradeoffs**: EI (concatenate all at once) vs. EII/III (process individually): EII/III better but more computationally expensive (K forward passes); Prompt-based vs. embedding-based: Prompt-based simpler but much lower performance; Codetype info combination: More info can cause overcorrection; Name alone or Name+Samp often optimal
- **Failure signatures**: Misclassifying neutral statements with codetype-like patterns as im-HS (overcorrection); Struggling with sentences containing multiple codetypes; Poor performance on prompt-based method with ISHate dataset (F1 ~0.10); Language-specific performance gaps (Chinese F1 ~0.76 vs. English F1 ~0.61)
- **First 3 experiments**: 1) Replicate embedding-based Method III with Llama2-7B on ToxiCN dataset using Name-only codetype info; 2) Ablate codetype combinations: Test Name, Samp, Name+Samp, and Name+Samp+Expl on Latent-hatred dataset; 3) Test generalization: Train classifier on ToxiCN (Chinese), evaluate on Latent-hatred (English) with same codetype taxonomy

## Open Questions the Paper Calls Out

- **Can a detection model be dynamically determine relevant codetypes for a specific input sentence rather than relying on pre-assigned static information?** The authors currently preassign all six codetypes to every sample and identify the need for a model that dynamically determines appropriate codetypes to enhance detection efficiency.

- **Does incorporating a Chain-of-Thought (CoT) prompting strategy improve the classification performance of LLMs on the implicit hate speech detection task?** The authors suggest that using a CoT approach could help prompt-based methods handle complex statements but do not implement or test this hypothesis in the current study.

- **Is the proposed six-category codetype taxonomy comprehensive enough to generalize to languages other than Chinese and English?** The taxonomy was constructed using Chinese rhetorical styles and validated on English data, with authors explicitly stating that comprehensiveness across broader linguistic contexts remains an area for further exploration.

## Limitations

- The taxonomy of six codetypes was derived from Chinese data and may not fully capture implicit hate patterns in English, as evidenced by the 15-point F1 gap between Chinese and English datasets
- The embedding-based methods show strong performance but require processing K forward passes for K codetypes, raising scalability concerns for production deployment
- The paper reports that richer codetype information (Name+Expl+Samp) can cause "overcorrection" where neutral statements with codetype-like patterns are misclassified as hate

## Confidence

- **High confidence**: The core finding that codetypes improve im-HS detection compared to baseline methods. The F1 improvements are substantial and consistent across datasets.
- **Medium confidence**: The superiority of embedding-based methods over prompt-based methods. While results show clear performance gaps, the prompt-based method evaluation is limited to a single dataset.
- **Medium confidence**: The claim that the six-codetype taxonomy captures 80% of implicit hate patterns. This is based on frequency analysis but lacks external validation.

## Next Checks

1. **Cross-lingual robustness test**: Train the classifier on Chinese ToxiCN data and evaluate directly on English Latent-hatred dataset using the same codetype taxonomy. Measure performance degradation to assess whether codetypes are truly transferable across languages or require language-specific adaptation.

2. **Overcorrection quantification**: Systematically test codetype combinations (Name, Samp, Name+Samp, Name+Samp+Expl) on a curated set of neutral statements containing codetype-like patterns. Measure false positive rates to determine which combinations minimize overcorrection while maintaining detection performance.

3. **Taxonomy coverage validation**: Analyze a random sample of misclassified implicit hate instances to determine what percentage contain codetypes outside the six-category taxonomy. This would quantify the practical limitations of the current codetype coverage and identify gaps for taxonomy expansion.