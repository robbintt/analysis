---
ver: rpa2
title: Proof-Carrying Neuro-Symbolic Code
arxiv_id: '2504.12031'
source_url: https://arxiv.org/abs/2504.12031
tags:
- neural
- https
- neuro-symbolic
- code
- logics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the concept of "proof-carrying neuro-symbolic
  code" as a method for verifying the safety of cyber-physical systems that combine
  neural networks with symbolic programs. The core idea is to decompose verification
  into three lemmas: first, proving the neural network satisfies a network property
  using specialized solvers; second, lifting this property to the problem space via
  an embedding function; and third, verifying the complete neuro-symbolic program
  against a high-level safety specification.'
---

# Proof-Carrying Neuro-Symbolic Code

## Quick Facts
- arXiv ID: 2504.12031
- Source URL: https://arxiv.org/abs/2504.12031
- Authors: Ekaterina Komendantskaya
- Reference count: 40
- Introduces proof-carrying neuro-symbolic code for verifying safety of cyber-physical systems combining neural networks with symbolic programs

## Executive Summary
This paper proposes a novel verification framework for cyber-physical systems that combine neural networks with symbolic programs, termed "proof-carrying neuro-symbolic code." The approach decomposes verification into three lemmas: proving neural network properties using specialized solvers, lifting these properties via embedding functions, and verifying the complete neuro-symbolic program against high-level safety specifications. The framework aims to provide machine-checkable proof certificates that ensure correctness when neural networks replace traditional discrete controllers in safety-critical systems.

The work addresses the growing need for verification methods as neural networks increasingly replace classical controllers in cyber-physical systems. By leveraging existing verification tools for both neural networks (like Marabou) and symbolic programs (like Agda), the framework creates a unified verification pipeline that produces formal proofs of safety properties. The approach is demonstrated through a cyber-physical system example involving autonomous vehicle distance maintenance, showing how traditional safety specifications can be verified when using neural network controllers.

## Method Summary
The method decomposes verification into three lemmas: first, proving the neural network satisfies a network property using specialized solvers; second, lifting this property to the problem space via an embedding function; and third, verifying the complete neuro-symbolic program against a high-level safety specification. The framework relies on multi-backend interfaces between different verification tools, creating formal proof certificates to ensure solver correctness, and compiling verification properties into machine learning objective functions. Current prototypes include Vehicle, which compiles specifications to both neural network solvers and higher-order provers, and certified proof checkers for neural network verification in Imandra.

## Key Results
- Introduces proof-carrying neuro-symbolic code framework for verifying safety of hybrid neural-symbolic systems
- Decomposes verification into three provable lemmas: network property, embedding function, and complete program verification
- Demonstrates approach on autonomous vehicle distance maintenance problem using neural network controllers

## Why This Works (Mechanism)
The framework works by creating a three-step verification pipeline that leverages the strengths of different verification tools. Neural network solvers can efficiently verify properties within the network's activation space, while symbolic provers can handle complex program logic. The embedding function serves as a bridge between these domains, translating network properties into problem-space constraints that can be verified by symbolic tools. By producing machine-checkable proof certificates at each stage, the framework ensures end-to-end verification of safety properties.

## Foundational Learning

1. **Neural Network Verification** - Techniques for proving properties of neural networks (e.g., robustness, safety bounds). Why needed: To verify that neural network controllers satisfy safety constraints in their activation space. Quick check: Can Marabou or similar tools verify ReLU networks with dozens of neurons?

2. **Higher-Order Theorem Proving** - Using systems like Agda or Coq to formally verify program correctness. Why needed: To verify the symbolic components of neuro-symbolic programs and ensure they satisfy safety specifications. Quick check: Can the theorem prover handle the combined program with neural network components?

3. **Embedding Functions** - Mathematical mappings that translate between neural network activation space and problem domain space. Why needed: To lift neural network properties verified in activation space to meaningful safety properties in the problem domain. Quick check: Does the embedding preserve all relevant safety constraints?

4. **Proof Certificates** - Machine-checkable evidence that verification results are correct. Why needed: To ensure trust in verification results and enable automated checking of complex proofs. Quick check: Can the proof checker validate certificates from different verification backends?

## Architecture Onboarding

**Component Map**: High-level specification -> Vehicle compiler -> Neural network solver (Marabou) + Symbolic prover (Agda) -> Embedding function -> Proof certificates

**Critical Path**: Safety specification → Property compilation → Network verification → Embedding lifting → Program verification → Proof certificate generation

**Design Tradeoffs**: The framework trades off verification completeness for scalability by decomposing verification into specialized lemmas. This allows leveraging optimized tools for each component but requires careful design of the embedding function to ensure soundness.

**Failure Signatures**: Verification failures can occur at any of the three lemma stages - network property violation, embedding function unsoundness, or program verification failure. Each failure type provides specific debugging information about which component needs modification.

**3 First Experiments**:
1. Verify a simple linear controller replaced by a small neural network on a toy autonomous vehicle problem
2. Test the embedding function with increasingly complex neural network architectures to identify scalability limits
3. Compare verification times between the three-lemma approach and monolithic verification of the complete neuro-symbolic system

## Open Questions the Paper Calls Out

The paper acknowledges that the compilation process from high-level safety requirements to machine-checkable proofs remains largely conceptual. The embedding function lifting mechanism requires further investigation for real-world applications, as its correctness is critical to the overall verification approach. The framework's scalability to realistic system sizes faces the same fundamental challenges as traditional proof-carrying code.

## Limitations

- The three-step verification decomposition heavily relies on the correctness of the embedding function, which remains unproven for real-world applications
- Examples provided are simplified toy problems rather than demonstrations on production cyber-physical systems
- The framework assumes formal specifications exist and can be compiled into verification conditions, but this compilation process is not fully developed

## Confidence

- Core concept: Medium - theoretical framework appears sound but practical implementation details and scalability remain unproven
- Multi-backend interfaces: Low - Vehicle prototype mentioned but no technical details or evaluation metrics provided
- Proof certification approach: Low - only certified proof checkers mentioned without implementation details or formal guarantees

## Next Checks

1. Implement and evaluate the Vehicle prototype on a realistic autonomous vehicle control system with provable safety properties
2. Develop and benchmark the embedding function lifting mechanism across multiple neural network architectures and problem domains
3. Create a comprehensive benchmark suite comparing proof-carrying neuro-symbolic code verification times and memory usage against state-of-the-art neural network verification tools