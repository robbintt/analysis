---
ver: rpa2
title: 'Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning'
arxiv_id: '2509.15811'
source_url: https://arxiv.org/abs/2509.15811
tags:
- languages
- reasoning
- language
- multilingual
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how mathematical reasoning performance varies
  across languages in multilingual large language models (LLMs) and whether different
  languages can complement each other. The authors propose a cross-lingual reward
  modeling framework that leverages complementary reasoning signals across languages
  by training a verifier to rank responses generated in multiple languages for a given
  question.
---

# Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2509.15811
- Source URL: https://arxiv.org/abs/2509.15811
- Reference count: 14
- Primary result: Cross-lingual reward modeling outperforms traditional multilingual reward modeling by over 10% and self-consistency baselines by over 15%

## Executive Summary
This paper investigates mathematical reasoning performance across languages in multilingual LLMs and proposes a cross-lingual reward modeling framework. The approach leverages complementary reasoning signals across languages by training a verifier to rank responses generated in multiple languages for a given question. The framework achieves significant improvements over traditional multilingual reward modeling and self-consistency baselines, demonstrating that languages have complementary reasoning abilities that can be harnessed to improve multilingual reasoning models.

## Method Summary
The authors propose a cross-lingual reward modeling framework that trains a verifier on multilingual correctness labels to rank responses across languages. They generate training data by prompting multilingual models on GSM8K math problems, automatically labeling responses by final answer correctness, and balancing correct/incorrect samples across 8 languages. The verifier (Qwen2.5-Instruct 3B with LoRA) is trained using binary cross-entropy loss on ~88k samples. At inference, the framework generates N responses across L languages, scores them with the cross-lingual verifier, and selects the highest-scoring response (Best-of-L). This approach is evaluated against traditional multilingual reward modeling and self-consistency baselines across multiple generator models.

## Key Results
- Cross-ORM outperforms traditional Multi-ORM by over 10% accuracy across all tested models
- Cross-ORM exceeds self-consistency baselines by over 15% accuracy
- Even high-resource languages like English benefit from cross-lingual sampling under low sampling budgets
- Some non-English language pools outperform English-inclusive ones
- Increasing the number of languages in the pool improves performance up to a certain point (~6-8 languages)

## Why This Works (Mechanism)

### Mechanism 1
Different languages in multilingual LLMs produce partially non-overlapping reasoning paths, enabling complementary problem-solving signals. When the model generates solutions across multiple languages, syntactic and lexical differences cause divergence in intermediate reasoning steps. Some languages may arrive at correct answers where others fail, even for the same problem.

### Mechanism 2
A single cross-lingual verifier trained on multilingual correctness labels can generalize to rank responses across languages it was trained on, outperforming per-language verifiers. Training the verifier on balanced correct/incorrect samples across 8 languages with binary cross-entropy loss encourages the model to learn language-agnostic features of correct reasoning rather than language-specific surface patterns.

### Mechanism 3
Cross-lingual sampling provides greater reasoning diversity per sample than within-language sampling, making it particularly advantageous under low inference budgets. At low N (e.g., 2-4 samples), drawing from multiple languages explores more distinct reasoning trajectories than sampling from one language with temperature variation. As N increases, within-language sampling eventually covers the same solution space, reducing cross-lingual advantage.

## Foundational Learning

- **Outcome Reward Modeling (ORM) vs. Process Reward Modeling (PRM)**: This paper uses ORM (whole-response scoring), but the distinction from PRM (step-by-step scoring) shapes what the verifier learns and its failure modes. Quick check: Can you explain why ORM might miss intermediate reasoning errors that PRM would catch?

- **Pass@k metric**: The paper uses pass@8 to estimate upper-bound complementarity—understanding this metric is essential to interpret why cross-lingual sampling has theoretical headroom. Quick check: If pass@8-cross is 94% but pass@8-multi (average) is 87%, what does that imply about language complementarity?

- **LoRA fine-tuning**: The verifier is trained with LoRA (rank=16, alpha=32). Understanding parameter-efficient fine-tuning helps debug training and transfer decisions. Quick check: What constraints does LoRA impose on what the verifier can learn compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Generator models (Aya-Expanse-8B, Llama3.1-8B, Qwen2.5-7B, etc.) -> Training data pipeline (GSM8K + Google Translate + prompt generators -> auto-label by final answer correctness -> balance correct/incorrect per language -> ~88k samples) -> Verifier (Qwen2.5-Instruct 3B + LoRA) -> Inference-time selection (generate N responses across L languages -> verifier scores all -> select highest-scoring response)

- **Critical path**: Training data quality depends on automatic answer-labeling correctness. Verifier generalization hinges on balanced language representation during training. At inference, the ranking step is O(N × L) scoring calls.

- **Design tradeoffs**: Verifier size vs. coverage (chose Qwen2.5-3B for widest language support), language pool size (diminishing returns after ~6-8 languages), English-inclusive vs. English-exclusive pools (English generally helps but some non-English pools outperform it).

- **Failure signatures**: Verifier systematically prefers one language's outputs regardless of correctness (likely training imbalance), Cross-ORM underperforms Multi-ORM on specific languages (verifier may not have learned language-agnostic features), No benefit from cross-lingual sampling even at low N (generator may have poor multilingual reasoning capability).

- **First 3 experiments**: 1) Reproduce pass@k gap: For your target generator, compute pass@8-multi vs. pass@8-cross on MGSM. 2) Train verifier on subset of languages: Train cross-lingual ORM on only 4 languages, test on all 8. 3) Budget sweep: Compare Cross-ORM vs. English-only Multi-ORM at N = 2, 4, 8, 16 samples.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings rely heavily on assumption that multilingual LLMs produce genuinely diverse reasoning paths across languages rather than translating identical internal representations
- Verifier training approach may have limitations in scalability and robustness with the current dataset size
- Cross-lingual advantage diminishes significantly at higher sampling budgets, limiting practical value for applications with extensive sampling
- Analysis focuses primarily on mathematical reasoning tasks, leaving open questions about other reasoning domains

## Confidence
- Cross-lingual complementarity exists: High
- Verifier generalization across languages: Medium
- Diminishing returns with sampling budget: High
- English-exclusive pools can outperform English-inclusive ones: Low-Medium

## Next Checks
1. Probing reasoning path diversity: Use embedding analysis or syntactic feature extraction to quantify actual diversity in intermediate reasoning steps across languages. Compare this to diversity achieved through within-language sampling at different temperature settings.

2. Verifier bias analysis: Conduct ablation studies where the verifier is trained on increasingly balanced language representations. Measure whether performance gains persist when language distribution in training data is equalized.

3. Domain generalization test: Apply the cross-lingual reward modeling framework to non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to determine whether language complementarity is specific to mathematical problem-solving.