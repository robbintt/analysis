---
ver: rpa2
title: Strategies of Code-switching in Human-Machine Dialogs
arxiv_id: '2508.07325'
source_url: https://arxiv.org/abs/2508.07325
tags:
- language
- participants
- task
- spanish
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates code-switching (CS) strategies in human-machine
  dialog using a Map Task chatbot. We develop a bilingual chatbot capable of generating
  code-switched Spanish-English dialogue and conduct two experiments testing different
  CS strategies.
---

# Strategies of Code-switching in Human-Machine Dialogs

## Quick Facts
- arXiv ID: 2508.07325
- Source URL: https://arxiv.org/abs/2508.07325
- Reference count: 0
- Primary result: Predictable code-switching strategies improve task outcomes and enjoyment in human-machine dialog.

## Executive Summary
This study investigates code-switching (CS) strategies in human-machine dialog using a Map Task chatbot. We develop a bilingual chatbot capable of generating code-switched Spanish-English dialogue and conduct two experiments testing different CS strategies. Participants interact with the bot under various conditions, including baseline, alignment, adversarial, random, and gender-incongruent strategies. Results show that predictable CS strategies (e.g., alignment) yield higher task success and enjoyment compared to random or ungrammatical strategies (e.g., masculine incongruent). Participants also show sensitivity to grammatical patterns, producing fewer code-switches when the bot generates ungrammatical constructions. These findings highlight the importance of grammaticality and predictability in human-machine CS interactions.

## Method Summary
The study uses a Map Task chatbot with GPT-4 as the base LLM, implementing two-stage pipeline: (1) GPT-4 generates response from chat history, (2) apply CS strategy manipulation. Two experiments test alternational (Experiment 1) and insertional (Experiment 2) code-switching strategies across 9 conditions with 50 participants each. Task performance is measured via game time, route similarity (DTW), and participant satisfaction (0-100 scales). CS behavior is analyzed through inter-sentential switching rates, entrainment, and noun phrase counts by type.

## Key Results
- Predictable CS strategies (alignment, adversarial) significantly improve task enjoyment and success compared to random switching
- Grammatical violations (masculine incongruent NPs like "la fork") reduce user satisfaction and overall CS production
- Participants entrain to the bot's CS patterns, reducing grammatical violations when exposed to ungrammatical output

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictable code-switching strategies improve task outcomes in human-machine dialog.
- Mechanism: When the bot's language choices follow discernible patterns, users can form expectations about utterance structure, reducing cognitive surprise and supporting comprehension.
- Core assumption: Users implicitly track and predict the bot's language behavior over the course of a dialog.
- Evidence anchors:
  - [abstract] "Participants generally enjoyed code-switching with our bot as long as it produced predictable code-switching behavior; when code-switching was random... participants enjoyed the task less and were less successful at completing it."
  - [section 3.3] "Only the random condition differed from the baseline condition (mean enjoyment ratings of 70.0 vs. 83.6)"; random also showed higher communication difficulty.

### Mechanism 2
- Claim: Grammatical violations in insertional code-switching disrupt task performance and user satisfaction.
- Mechanism: Producing ungrammatical constructions like "la fork" (feminine determiner + masculine noun) is perceived as unexpected and causes difficulty.
- Core assumption: The processing cost of ungrammatical CS is measurable in downstream task behavior.
- Evidence anchors:
  - [abstract] "when code-switching was random or ungrammatical (as when producing unattested incongruent mixed-language noun phrases, such as 'la fork'), participants enjoyed the task less and were less successful."
  - [section 4.5] Masculine incongruent condition yielded lower task enjoyment, higher difficulty communicating, and numerically lower self-reported success vs. baseline.

### Mechanism 3
- Claim: Users entrain to the bot's code-switching patterns and reduce CS when exposed to ungrammatical output.
- Mechanism: Participants adjust their own grammatical CS patterns to align with the bot. When the bot never produces a grammatical pattern, participants produce fewer of them.
- Core assumption: Entrainment extends beyond word choice to code-switching grammatical patterns.
- Evidence anchors:
  - [section 4.5] "In the masculine incongruent condition, participants produced unexpectedly few feminine incongruent switches, while also producing more masculine incongruent switches than in the other conditions."

## Foundational Learning

- Concept: Insertional vs. alternational code-switching
  - Why needed here: The experiments manipulate these distinct types; understanding the difference is required to interpret why "la fork" is an insertional violation.
  - Quick check question: Given "I went to el store yesterday," is this insertional or alternational CS?

- Concept: Spanish grammatical gender in mixed noun phrases
  - Why needed here: The key manipulation involves determiner–noun agreement. You must know why "el fork" is broadly acceptable while "la fork" is unexpected.
  - Quick check question: If "cuchara" (spoon) is feminine in Spanish, what is the expected mixed NP when inserting "spoon"?

- Concept: Dynamic time warping for path similarity
  - Why needed here: Task performance is partially operationalized via route distance using DTW.
  - Quick check question: Does a lower DTW distance indicate better or worse alignment with the target route?

## Architecture Onboarding

- Component map: Client -> Server -> GPT-4 -> Language ID/NP Extractor -> CS Strategy Layer -> Client
- Critical path: GPT-4 generates response → Language ID and/or noun extraction applied → Strategy-specific transformation → Response sent to client
- Design tradeoffs:
  - Using GPT-4 yields fluent bilingual output but reduces control over where CS occurs
  - Limiting NP manipulation to simple NPs avoids complex syntax errors but may miss naturally occurring CS
  - Recruiting U.S.-based Spanish speakers via Prolific ensures practical screening but introduces heterogeneous language backgrounds
- Failure signatures:
  - If users report confusion or disproportionately long game times, check whether CS strategy is random or produces many ungrammatical NPs
  - If NP switches are not being modified as expected, verify dictionary coverage and SpaCy identification
- First 3 experiments:
  1. Baseline sanity check: Run system without CS manipulation; verify GPT-4 naturally aligns to participant language
  2. Alignment vs. Random comparison: Deploy two conditions; confirm Random shows lower enjoyment and higher difficulty
  3. Grammatical vs. ungrammatical NP manipulation: Deploy Congruent, Feminine Incongruent, and Masculine Incongruent conditions; verify Masculine Incongruent yields lower enjoyment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the frequency or proportion of code-switches independently predict task enjoyment and success, and is there a specific threshold that triggers negative outcomes?
- Basis in paper: The authors note that the "congruent" strategy resulted in lower enjoyment despite being grammatical, stating: "Future work should examine the question of whether the number and/or regularity of code-switches can help predict participants' enjoyment."
- Why unresolved: The study manipulated type of switch, but in the congruent condition, the bot switched all nouns. It is unclear if lower enjoyment was due to the specific strategy or simply the overwhelming volume of switching.
- What evidence would resolve it: Experiments that systematically vary the density of code-switches while keeping the grammatical strategy constant.

### Open Question 2
- Question: Is the "adversarial" strategy (switching to the language the user is not currently using) perceived by users as cooperative or accommodating rather than disruptive?
- Basis in paper: The authors found the adversarial strategy performed better than expected and stated: "It is possible that participants perceived the strategy that we imagined as adversarial to actually be somewhat accommodating... further investigation is needed to test this proposal."
- Why unresolved: The adversarial condition did not significantly harm task success compared to the baseline, contrary to the hypothesis.
- What evidence would resolve it: Qualitative analysis of post-experiment user feedback or "think-aloud" protocols during the task.

### Open Question 3
- Question: How do specific community norms and individual language profiles (e.g., heritage speakers vs. L2 learners) impact sensitivity to code-switching violations?
- Basis in paper: The authors acknowledge a limitation regarding their "heterogenous sample" and explicitly state: "Future work should investigate the impact of these variables (and others) on CS preferences."
- Why unresolved: Participants had varied backgrounds, which may have obscured how specific communities react to "ungrammatical" switches.
- What evidence would resolve it: Replication with stratified participant groups based on specific language profiles.

## Limitations
- The study uses GPT-4 as the base LLM, which may not generalize to other models or real-time speech systems
- The Map Task paradigm represents a constrained interaction type that may not capture open-domain human-machine dialog
- Participants were exclusively U.S.-based, limiting applicability to other language pairs or cultural contexts

## Confidence

**High confidence**: The finding that predictable CS strategies lead to better task outcomes than random switching is well-supported by clear statistical differences in enjoyment ratings and difficulty measures.

**Medium confidence**: The effect of grammatical violations on user experience is supported but could be influenced by individual participant language backgrounds.

**Low confidence**: The claim that participants explicitly track and predict the bot's language behavior over time would require additional psycholinguistic measures beyond the current behavioral metrics.

## Next Checks

1. **Cross-model validation**: Replicate the study using a different LLM (e.g., open-source multilingual model) to determine if findings generalize beyond GPT-4's specific output patterns.

2. **Real-time speech implementation**: Test whether the observed effects persist when the system operates in real-time speech mode rather than text-based chat.

3. **Community norm variation**: Conduct the study with participants from different Spanish-speaking communities to assess whether grammaticality effects vary based on local CS norms.