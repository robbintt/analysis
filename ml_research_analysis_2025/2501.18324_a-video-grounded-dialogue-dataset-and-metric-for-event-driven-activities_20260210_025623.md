---
ver: rpa2
title: A Video-grounded Dialogue Dataset and Metric for Event-driven Activities
arxiv_id: '2501.18324'
source_url: https://arxiv.org/abs/2501.18324
tags:
- answer
- does
- dialogue
- reference
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VDAct, a video-grounded dialogue dataset focused
  on event-driven activities, featuring longer and more complex videos requiring advanced
  contextual understanding. Alongside this, VDEval, a new session-based evaluation
  metric, is proposed to better assess dialogue responses by incorporating dialogue
  history and video content summaries.
---

# A Video-grounded Dialogue Dataset and Metric for Event-driven Activities

## Quick Facts
- **arXiv ID:** 2501.18324
- **Source URL:** https://arxiv.org/abs/2501.18324
- **Reference count:** 40
- **Primary result:** Introduces VDAct dataset and VDEval metric; VDEval shows significantly higher correlation with human judgment than existing metrics.

## Executive Summary
This paper addresses the challenge of evaluating video-grounded dialogues by introducing VDAct, a dataset focused on event-driven activities with long, complex video sequences requiring advanced contextual understanding. Alongside this, the authors propose VDEval, a novel session-based evaluation metric that incorporates dialogue history and video content summaries to assess dialogue responses more effectively. Experiments demonstrate that VDEval outperforms traditional metrics like BLEU and ROUGE in correlating with human judgments, particularly for complex question types.

## Method Summary
The VDAct dataset is constructed from VirtualHome2KG, featuring 1,000 videos, 3,000 dialogues, and 30,000 QA pairs with associated Knowledge Graphs. Videos average 248 seconds, focusing on event-driven activities. The VDEval metric uses GPT-4o-mini to score responses (1-3 scale) based on dialogue history and refined KG summaries. Models are fine-tuned using LoRA on VideoLLaMA2 with DeepSpeed ZeRO-3 CPU offloading, AdamW optimizer, and 8-bit quantization.

## Key Results
- VDEval achieves significantly higher Kendall's rank correlation with human judgment compared to SPICE, BLEU, and ROUGE.
- Models struggle particularly on temporal questions (T-SEQ, T-FRQ) in the VDAct dataset, indicating a temporal reasoning gap in current VLMs.
- Refined KG summaries processed by GPT-4o-mini improve evaluation correlation compared to template-based summaries.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating video summaries derived from Knowledge Graphs (KGs) allows evaluation metrics to validate semantically equivalent responses that differ lexically from the reference.
- **Mechanism:** The VDEval metric inputs a refined textual summary of the video event (extracted from KG triplets) alongside the candidate answer. This provides an independent "ground truth" that bridges the gap between a valid candidate answer and a specific reference text, preventing false negatives in evaluation.
- **Core assumption:** The KG-to-text summary accurately captures the salient visual events necessary to verify the dialogue context.
- **Evidence anchors:**
  - [section] Table 12 shows VDEval scoring a specific location answer ("shelf in the living room") as correct (Score 3) because the summary confirmed it, while LA VE scored it 1 for mismatching the reference "bookcase".
  - [section] Table 6 demonstrates that "Refined" summaries (processed by GPT-4o-mini) yield higher correlation with human judgment than template-based summaries or no summaries.
  - [corpus] Related work on entailment trees (arXiv:2501.05069) suggests structured reasoning supports verification, aligning with the use of structured KGs here.
- **Break condition:** If the KG summary hallucinates events or omits critical details (e.g., object states), VDEval may incorrectly validate or reject answers based on faulty premises.

### Mechanism 2
- **Claim:** Session-based context accumulation is required to resolve referential ambiguity in multi-turn dialogues.
- **Mechanism:** VDEval conditions the evaluation of the current turn on the entire preceding dialogue history (previous Q&A pairs). This mechanism allows the evaluator to interpret dependent queries (e.g., "Where did he get *them* from?") by resolving pronouns using history, which turn-based metrics ignore.
- **Core assumption:** The LLM evaluator (GPT-4o-mini) possesses sufficient context-window capacity and reasoning capability to track state changes across the dialogue session.
- **Evidence anchors:**
  - [abstract] The paper states VDEval "integrates dialogue session history... to evaluate individual responses," contrasting with existing metrics relying on "single dialogue turns."
  - [section] Figure 3 illustrates the input structure, explicitly showing "dialogue history for turn $t_i$" being fed into the LLM instruction.
- **Break condition:** If the dialogue history exceeds the LLM's context window or contains contradictory information, the evaluation signal may degrade.

### Mechanism 3
- **Claim:** Increasing video duration and temporal question complexity exposes a "temporal reasoning gap" in current Vision-Language Models (VLMs).
- **Mechanism:** The VDAct dataset constructs scenarios with long event sequences (avg. 248s). Performance degradation on temporal questions (T-SEQ, T-FRQ) suggests VLMs struggle to maintain state or sequence representations over extended frames compared to static or short-term activities.
- **Core assumption:** The drop in accuracy is due to reasoning limitations rather than purely visual feature extraction failures (though visual errors contribute).
- **Evidence anchors:**
  - [section] "Results and Discussion" notes the baseline performed less effectively on T-SEQ and T-FRQ (scores ~1.49-1.56) compared to Quantitative or Binary questions.
  - [corpus] ReWatch-R1 (arXiv:2509.23652) identifies a similar bottleneck in "complex video reasoning" due to data limitations, supporting the difficulty claim.
- **Break condition:** If a VLM employs a superior memory mechanism (e.g., external state tracking) that mitigates the context length limitation, this difficulty curve would flatten.

## Foundational Learning

- **Concept: Knowledge Graph (KG) to Text Linearization**
  - **Why needed here:** The paper relies on converting structured KG triplets (Event, Action, Object) into natural language summaries to provide context to the LLM-based evaluator.
  - **Quick check question:** How does the system handle the transition from `(event3, action, close)` to the sentence "He closes the door," and why is redundancy removal (via GPT-4o-mini) necessary?

- **Concept: Session-based vs. Turn-based Evaluation**
  - **Why needed here:** Understanding the shift from evaluating a single QA pair (BLEU/ROUGE) to evaluating a response within a conversation flow is central to the VDEval contribution.
  - **Quick check question:** In Figure 3, why does the "Turn-based Context" fail to evaluate a correct answer that relies on information provided in Q5?

- **Concept: Event-driven Activity Modeling**
  - **Why needed here:** The dataset focuses on "compound activities" where multiple events occur sequentially. Understanding that events have state changes (e.g., PLUGGED_IN to OFF) is required to interpret the dataset's complexity.
  - **Quick check question:** According to Table 1, what distinguishes a "Temporal (T-SEQ)" question from a "Descriptive (D-ACT)" question in the context of event sequences?

## Architecture Onboarding

- **Component map:** VirtualHome2KG (Source) -> Scenario Selector -> Video -> Annotators -> Dialogue -> **Refined Summary** (LLM) -> **VDEval** (GPT-4o-mini)
- **Critical path:** The generation of the **Refined Video Summary** is critical. If the template-based linearization (Table 2) is not cleaned by the LLM to remove redundancy, the noise may lower the evaluation correlation (Table 6 shows Refined > Template).
- **Design tradeoffs:**
  - *Simulation vs. Real Video:* Using VirtualHome (simulation) guarantees perfect KG labels but may lack visual realism (domain gap).
  - *Metric Cost:* VDEval requires calling a proprietary LLM (GPT-4o-mini) for every turn with full history, which is slower and costlier than computing BLEU.
- **Failure signatures:**
  - *T-SEQ Failure:* Models hallucinating the order of events (e.g., saying he cleaned *before* tidying when the reverse is true).
  - *Reference Mismatch:* Valid answers marked incorrect by standard metrics because they use synonyms not found in the ground truth (VDEval fixes this).
- **First 3 experiments:**
  1.  **Baseline Validation:** Run frozen VideoLLaMA2 on the VDAct test set and calculate BLEU/ROUGE vs. VDEval scores to replicate the "low correlation" finding for classic metrics.
  2.  **Ablation on Context:** Modify the VDEval prompt to remove the "Video Summary" input and measure the drop in Kendall's rank correlation with human judgment to quantify the value of the KG context.
  3.  **Error Analysis:** Isolate "Temporal" (T-SEQ) questions in the validation set and compare model performance against "Descriptive" (D-OBJ) questions to verify the specific temporal reasoning weakness.

## Open Questions the Paper Calls Out
None

## Limitations
- The VDEval metric's reliance on GPT-4o-mini introduces cost and latency challenges, making it impractical for large-scale deployment.
- The metric's performance is inherently tied to the LLM's ability to understand both the KG summary and dialogue context, which may vary with different models or prompt formulations.
- The assertion that KG-to-text linearization is a "necessary" component for the evaluation pipeline is presented without ablation studies showing what happens if the summary is entirely omitted or replaced with a different form of context.

## Confidence
- **High Confidence:** The core claim that VDEval achieves significantly higher correlation with human judgment than existing metrics (SPICE, BLEU, ROUGE) is well-supported by the experimental results in Table 6.
- **Medium Confidence:** The claim that temporal questions (T-SEQ, T-FRQ) are particularly challenging for VLMs is plausible given the results, but the paper doesn't fully rule out that this is due to visual feature extraction limitations rather than pure reasoning deficits.
- **Low Confidence:** The assertion that the KG-to-text linearization is a "necessary" component for the evaluation pipeline is presented without ablation studies showing what happens if the summary is entirely omitted or replaced with a different form of context.

## Next Checks
1. **Ablation Study on Summary Quality:** Remove the GPT-4o-mini refinement step and directly use template-based KG summaries in VDEval. Compare Kendall's rank correlation with human judgment to quantify the exact contribution of the "Refined" summary.
2. **Cross-Model Metric Stability:** Run VDEval using a different LLM (e.g., GPT-4, Claude) on the same validation set. Measure the variance in scores and correlation with human judgment to assess the metric's dependence on a specific model.
3. **Visual Feature Ablation:** Create a controlled subset of VDAct videos with minimal visual complexity (e.g., static scenes, clear actions). Evaluate model performance on this subset to determine if the temporal reasoning gap persists or if it's confounded by visual recognition challenges.