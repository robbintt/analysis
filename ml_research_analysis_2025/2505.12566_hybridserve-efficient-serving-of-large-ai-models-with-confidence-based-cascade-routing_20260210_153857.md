---
ver: rpa2
title: 'HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade
  Routing'
arxiv_id: '2505.12566'
source_url: https://arxiv.org/abs/2505.12566
tags:
- serve
- serving
- accuracy
- energy
- dnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HybridServe, a novel system for efficiently
  serving large AI models by leveraging multiple model sizes (small to giant) in tandem.
  The core idea is to use confidence-based routing to offload inference requests to
  smaller, more energy-efficient models when possible, only resorting to larger models
  when accuracy is at risk.
---

# HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing

## Quick Facts
- **arXiv ID:** 2505.12566
- **Source URL:** https://arxiv.org/abs/2505.12566
- **Reference count:** 40
- **Primary result:** HybridServe reduces energy consumption by up to 19.8x while maintaining giant model accuracy through confidence-based cascade routing

## Executive Summary
HybridServe is a novel system for efficiently serving large AI models by leveraging multiple model sizes in tandem. The core innovation is a confidence-based routing mechanism that intelligently offloads inference requests to smaller, more energy-efficient models when possible, only resorting to larger models when accuracy is at risk. This approach addresses the high energy costs associated with serving giant DNNs, which often exceed training costs. The system features a confidence-based hybrid model serving dataflow and a dataflow planner for efficient partitioning and replication of models across GPUs to maximize throughput.

## Method Summary
HybridServe works by cascading multiple models of different sizes (small to giant) and using learned confidence scores to route requests. During inference, requests first hit a small model. If its confidence score exceeds a pre-computed threshold, the prediction is accepted. If not, the request is routed to a larger model in the cascade. The system also introduces skip connections to reduce latency overhead by allowing requests with very low confidence to bypass intermediate models directly to larger ones. An offline dataflow planner partitions giant models across GPUs and places smaller models in remaining memory, constrained by a kernel utilization metric to prevent compute contention. The planner uses ILP to optimize placement for maximum throughput and energy efficiency.

## Key Results
- Reduces energy consumption by up to 19.8x compared to state-of-the-art DNN serving systems
- Improves throughput by up to 8x while maintaining the accuracy of using only giant models
- Achieves these gains by intelligently routing requests to smaller models that can handle the majority of tasks

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Cascade Routing
Routing inference requests to smaller models based on learned confidence scores can reduce energy use while preserving the accuracy of the largest model. A confidence score function (e.g., Temperature Scaling) is fitted to each model's outputs using a validation set. During inference, a request is first sent to a small model. If its confidence score exceeds a pre-computed threshold, the prediction is accepted. If not, the request is routed to a larger model in the cascade.

### Mechanism 2: GPU Co-location via Dataflow Planner
Co-locating memory-bound (giant) and compute-bound (small) models on the same GPUs maximizes throughput and energy efficiency. An offline dataflow planner formulates deployment as an integer linear programming (ILP) problem. It partitions giant models across GPUs and places smaller models in the remaining memory, constrained by a "kernel utilization" metric to prevent compute contention.

### Mechanism 3: Skip Connections in Routing
Directly routing low-confidence requests to larger models via skip connections reduces the latency overhead of a linear cascade. Instead of stepping through every intermediate model size, the router uses a secondary "skip threshold." Requests with confidence far below the acceptance threshold bypass intermediate models and are routed directly to the largest or most appropriate model in the chain.

## Foundational Learning

**Model Calibration**
- Why needed: The entire routing logic depends on confidence scores being a reliable proxy for correctness
- Quick check: Given a model's raw softmax outputs, how would you apply Temperature Scaling to make its confidence scores reflect true accuracy?

**Model Parallelism**
- Why needed: To understand how the "giant" models are handled through partitioning across multiple GPUs
- Quick check: If a model is partitioned across 4 GPUs, where does the output of the first partition go?

**Kernel Utilization**
- Why needed: This is the key metric for the planner's placement decision, measuring GPU load more precisely than memory usage
- Quick check: On a GPU, why is measuring the proportion of time kernels are executing a better load metric than just counting the number of running processes?

## Architecture Onboarding

**Component map:**
Request Router -> Confidence Score Function -> Hybrid Dataflow -> Dataflow Planner -> Serving System

**Critical path:**
1. Offline Profiling: The planner measures each model's memory, kernel utilization, and latency across batch sizes
2. Threshold Search: Algorithms 1 and 2 run on a validation set to determine confidence and skip thresholds for the routers
3. Deployment Planning: The ILP solver generates the placement plan, mapping model partitions and replicas to specific GPUs
4. Online Serving: The system routes incoming requests through the deployed dataflow graph, using real-time confidence scores to guide them

**Design tradeoffs:**
- AP vs. EO Modes: Accuracy-Preserving (AP) mode guarantees giant-model accuracy but offers fewer energy savings
- Placement Complexity vs. Optimality: The planner's polynomial-time algorithm makes it scalable but relies on approximations
- Replication vs. Memory: Adding replicas of bottleneck models increases throughput but consumes memory that could host other model partitions

**Failure signatures:**
- Confidence Drift: If the online data distribution shifts from the validation set, thresholds become stale
- GPU Thrashing: If kernel utilization constraint is violated by unexpected workload spikes
- Tail Latency on "Hard" Queries: Requests that must traverse the full cascade will experience significantly higher latency

**First 3 experiments:**
1. Calibration Verification: For a single model (e.g., ViT-S), plot "Expected Calibration Error" before and after applying the confidence score function
2. Simple Cascade Latency: Build a two-model cascade (Small -> Giant) with a fixed threshold and measure P90 latency
3. Planner Stress Test: Run the dataflow planner on a single GPU with memory constraint and verify kernel utilization sums to â‰¤ 1.0

## Open Questions the Paper Calls Out

**Open Question 1:** How can the system mitigate tail latency when the smallest available model in the cascade is still computationally dense (e.g., >5% of the giant model size)? The current skip connection logic optimizes based on energy benefit but does not account for strict latency penalties introduced by dense intermediate models.

**Open Question 2:** Can the system automatically detect and exclude "poorly trained" intermediate models that degrade overall accuracy? The current planner assumes intermediate models provide positive utility unless energy gain is negative, lacking a mechanism to identify low-accuracy intermediates.

**Open Question 3:** How can the confidence thresholds adapt to dynamic distribution shifts (concept drift) in real-time without requiring costly offline re-computation? If live user requests diverge from the validation set, the precomputed "Threshold performance graph" may fail to guarantee the target accuracy-energy trade-off.

## Limitations
- Energy savings measurements are specific to GPU clusters with A5000 and T4 hardware and may not generalize to other architectures
- The system's performance heavily depends on the quality of offline profiling and threshold calibration, which are not fully detailed
- Scalability to very large clusters (dozens of GPUs) is not demonstrated, and computational cost of offline planning phases is not discussed

## Confidence

**High Confidence:** The fundamental premise that smaller models can handle "easy" requests is well-supported by prior literature on model cascades and early exits. Temperature Scaling for confidence calibration is a standard and well-validated technique.

**Medium Confidence:** The specific design of the hybrid dataflow with skip connections is novel and the reported latency and energy improvements are significant. However, performance is tightly coupled to quality of offline profiling and threshold calibration.

**Low Confidence:** The scalability of the ILP-based planner to very large clusters is not demonstrated. The system's robustness to concept drift or adversarial inputs is not addressed.

## Next Checks

1. **Calibration Robustness Test:** Take the confidence score function from one dataset (e.g., ImageNet for ViT) and evaluate its accuracy on a different but related dataset (e.g., CIFAR-100). Plot the Expected Calibration Error to quantify degradation.

2. **Skip Connection Ablation:** Run the cascade system with and without skip connections enabled. Measure the P90 latency for requests that would have been skipped to verify the mechanism's impact on tail latency.

3. **Planner Generalization:** Profile the models on one GPU type (e.g., A5000), then deploy the resulting plan on a different GPU type (e.g., T4). Measure kernel utilization and latency to assess the planner's robustness to hardware heterogeneity.