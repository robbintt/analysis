---
ver: rpa2
title: Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical
  Question Answering?
arxiv_id: '2509.18843'
source_url: https://arxiv.org/abs/2509.18843
tags:
- questions
- question
- batch
- open-weight
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether small open-weight language models
  (up to 14 billion parameters) could match the performance of proprietary models
  like GPT-4 and Claude in biomedical question answering. The researchers applied
  retrieval-augmented generation using the BioASQ dataset, retrieving relevant PubMed
  snippets based on embedding similarity, and enhancing performance with in-context
  learning and structured outputs.
---

# Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?

## Quick Facts
- **arXiv ID**: 2509.18843
- **Source URL**: https://arxiv.org/abs/2509.18843
- **Reference count**: 35
- **Primary result**: Open-weight models (up to 14B parameters) match or exceed proprietary models in biomedical QA when using RAG, ensembling, and structured outputs

## Executive Summary
This study demonstrates that small open-weight language models can effectively compete with proprietary models like GPT-4 and Claude on biomedical question answering tasks. Using retrieval-augmented generation with PubMed snippets, structured outputs, and ensemble strategies, the researchers achieved competitive performance across four question types. The work shows that model size is not the sole determinant of QA performance when appropriate techniques like RAG and ensembling are applied.

## Method Summary
The researchers employed retrieval-augmented generation using BioASQ dataset and PubMed snippets, retrieving relevant documents based on embedding similarity with nomic-embed-text-v1. They applied in-context learning with few-shot prompting for factoid/list questions and zero-shot for summaries, enforced structured JSON outputs via Context-Free Grammar, and tested both individual models and ensemble configurations. Models included Phi-4, Gemma-3, Qwen2.5, and Mistral, evaluated across four question types (yes/no, factoid, list, summary) using standard BioASQ metrics.

## Key Results
- Open-weight models matched or surpassed proprietary models, particularly in ensemble configurations
- For yes/no questions, open models excelled in two of four batches
- Factoid and list questions saw significant gains from model ensembling
- Summary questions performed comparably to top systems using cross-encoder reranking
- 4-bit quantized models maintained acceptable performance for factoid/list tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Aggregating outputs from multiple smaller models via frequency-based ensembling recovers accuracy comparable to larger proprietary models for exact-answer tasks.
- **Mechanism**: The system collects responses from diverse model families and calculates frequency of distinct entities, selecting the most common ones above a threshold.
- **Core assumption**: Errors are uncorrelated across architectures, allowing correct answers to emerge through majority voting.
- **Evidence anchors**: Table 5 shows Ensemble EP-1 (Batch 2) achieving significantly higher Recall (0.7083) than single models.
- **Break condition**: If models share common training biases or failure modes, frequency voting will amplify errors rather than correct them.

### Mechanism 2
- **Claim**: Performance on summary and yes/no tasks depends heavily on retrieval quality and tailored prompting strategies.
- **Mechanism**: Uses cosine similarity with PubMed snippets and applies few-shot prompting only for factoid/list types, while using zero-shot for summaries.
- **Core assumption**: Embedding distance is sufficient proxy for semantic relevance; providing examples helps extraction but biases generative summarization.
- **Evidence anchors**: Section 2.3 notes few-shot prompting detrimentally affected summary performance.
- **Break condition**: If embedding model fails to capture domain-specific nuance, top-10 context will be irrelevant, leading to hallucination.

### Mechanism 3
- **Claim**: Structured output enforcement via CFG mitigates extraction errors in entity-based QA.
- **Mechanism**: Forces LLM to generate tokens adhering to predefined JSON schema using libraries like Outlines.
- **Core assumption**: Constrained decoding does not degrade reasoning ability; it merely restricts output vocabulary.
- **Evidence anchors**: Section 2.4 describes using CFG implementations to ensure generated tokens adhere strictly to schema.
- **Break condition**: Overly strict schemas may cause omission of valid but complex answers or force "grammar-valid but semantic nonsense" outputs.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here**: The paper relies on grounding LLM responses in provided PubMed snippets (BioASQ Task 13B).
  - **Quick check question**: Can you explain why providing a model with the top-10 relevant snippets via embedding similarity might yield better results than relying on its parametric memory?

- **Concept: In-Context Learning (Few-Shot)**
  - **Why needed here**: The authors differentiate between zero-shot and few-shot prompting based on question type.
  - **Quick check question**: Why might including examples (few-shot) help a model extract entities but hurt its ability to summarize text naturally?

- **Concept: Model Ensembling (Voting)**
  - **Why needed here**: The "winning" strategy for difficult question types involved combining models.
  - **Quick check question**: How does majority voting help reduce the variance of errors in a model ensemble?

## Architecture Onboarding

- **Component map**: Retrieval Layer (nomic-embed-text-v1 + Qdrant) -> Context Builder (formats snippets + examples) -> Inference Engine (Phi-4, Gemma-3, Qwen2.5) -> Constraint Layer (CFG/Structured Output) -> Aggregation Layer (frequency counter or cross-encoder reranking)

- **Critical path**: The Snippet Selection (Section 2.1) is the primary dependency. If the retrieval step fails to surface relevant PubMed text, the downstream LLM cannot generate the correct answer.

- **Design tradeoffs**:
  - **Quantization vs. Accuracy**: Used 4-bit quantization for batches 1-3 but switched to full weights for batch 4.
  - **Hand-Crafted vs. Automated Prompts**: Found hand-crafted prompts performed better than DSPy.
  - **Ensemble Size**: Diversity (mixing families like Phi + Qwen) matters more than sheer numbers.

- **Failure signatures**:
  - "None" responses indicate snippet retrieval failure
  - Low Recall on List Questions suggests frequency threshold is too conservative
  - Verbose Summaries indicate instruction-following failure

- **First 3 experiments**:
  1. **Retrieval Validation**: Run snippet retriever on gold-standard set to verify answer presence in top-10
  2. **Prompt Ablation**: Compare 0-shot vs. 3-shot on Factoid questions using single model
  3. **Ensemble Baseline**: Compare single proprietary model vs. 3-model open-weight ensemble on List questions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can automatic prompt optimization create model-specific prompts that outperform hand-crafted prompts for biomedical QA?
- **Basis in paper**: Section 2.3 states the authors plan to investigate automatic prompt optimization after DSPy performed worse without optimization.
- **Why unresolved**: Attempted automated prompting but did not perform the optimization step required to make it competitive.
- **What evidence would resolve it**: Comparative evaluation using prompts generated by automated optimizer versus manual prompts on BioASQ tasks.

### Open Question 2
- **Question**: Can reasoning-based models (e.g., HuatuoGPT-o1) effectively compete in biomedical QA when properly tuned?
- **Basis in paper**: Section 2.5 notes limited exploration of reasoning models meant they didn't achieve strong results with HuatuoGPT-o1.
- **Why unresolved**: Authors attribute failure to their own limited exploration rather than model's fundamental incapacity.
- **What evidence would resolve it**: Dedicated ablation study tuning HuatuoGPT-o1 specifically for BioASQ snippets and comparing against Phi-4 and Qwen baselines.

### Open Question 3
- **Question**: Do the reported high ROUGE recall scores for summary questions correlate with high factual accuracy in human evaluation?
- **Basis in paper**: Section 3.3 reports recall scores were competitive but F1 scores were significantly lower, and full analysis requires unpublished manual scores.
- **Why unresolved**: Paper relies on automatic metrics (ROUGE) which don't fully capture "ideal answer" quality required by challenge.
- **What evidence would resolve it**: Correlation analysis between system's ROUGE scores and official manual relevance/accuracy scores once published.

## Limitations
- Ensemble approach assumes uncorrelated errors across model families without analyzing error overlap
- Heavy dependency on retrieval quality without providing retrieval precision metrics
- Modest ROUGE scores for summary questions suggest either inherent difficulty or suboptimal reranking approach

## Confidence
- **High Confidence**: Open-weight models can match proprietary models on yes/no questions; structured output enforcement via CFG improves extraction reliability; few-shot prompting benefits entity extraction but harms summary generation
- **Medium Confidence**: Model ensembling provides consistent gains across batches; 4-bit quantization maintains acceptable performance; cross-encoder reranking improves summary selection
- **Low Confidence**: Ensemble approach will generalize to other domain-specific QA tasks; zero-shot prompting is universally superior for summary tasks; specific frequency threshold for aggregation is optimal

## Next Checks
1. **Error Correlation Analysis**: Compute pairwise overlap of incorrect answers across different model families in the ensemble to test if gains are due to true complementarity or random chance.

2. **Retrieval Ablation Study**: Run pipeline with retrieved snippets replaced by random documents or with retrieval disabled entirely to quantify performance dependence on retriever versus LLM parametric knowledge.

3. **Ensemble Diversity Experiment**: Create ensembles with only similar model architectures and compare to mixed-architecture ensembles to test whether architectural diversity or model scale drives ensemble gains.