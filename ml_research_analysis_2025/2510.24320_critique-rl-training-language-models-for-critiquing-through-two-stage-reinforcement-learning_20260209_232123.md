---
ver: rpa2
title: 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement
  Learning'
arxiv_id: '2510.24320'
source_url: https://arxiv.org/abs/2510.24320
tags:
- critique-rl
- uni00000011
- uni00000048
- uni0000004c
- critique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of training critiquing language
  models to assess and provide feedback on model outputs for complex reasoning tasks
  without relying on stronger supervision or oracle verifiers. The authors propose
  Critique-RL, an online reinforcement learning approach based on a two-player actor-critic
  interaction paradigm where an actor generates responses, a critic provides feedback,
  and the actor refines its responses accordingly.
---

# Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.24320
- Source URL: https://arxiv.org/abs/2510.24320
- Authors: Zhiheng Xi, Jixuan Huang, Xin Guo, Boyang Hong, Dingwen Yang, Xiaoran Fan, Shuo Li, Zehui Chen, Junjie Ye, Siyu Yuan, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang
- Reference count: 40
- Primary result: Qwen2.5-7B achieves 9.02% gain on in-domain tasks and 5.70% gain on out-of-domain tasks compared to baselines

## Executive Summary
This paper introduces Critique-RL, a two-stage reinforcement learning framework for training language models to critique model outputs without relying on stronger supervision or oracle verifiers. The key innovation is recognizing that indirect reward signals from actor refinement alone fail to develop satisfactory critics, as they optimize helpfulness but neglect discriminability. Critique-RL addresses this through a staged approach: Stage I optimizes discriminability using direct rule-based rewards, while Stage II optimizes helpfulness using indirect rewards while maintaining discriminability through regularization. Experiments demonstrate significant improvements across various reasoning tasks with strong generalization to unseen domains.

## Method Summary
Critique-RL employs a two-player actor-critic interaction paradigm where an actor generates responses, a critic provides feedback, and the actor refines its responses accordingly. The framework addresses a critical challenge in critiquing model development: indirect reward signals from actor refinement alone optimize helpfulness but neglect discriminability. The two-stage optimization strategy first develops discriminability through rule-based rewards in Stage I, then optimizes helpfulness through indirect actor refinement rewards in Stage II while maintaining discriminability through regularization. This staged approach enables the development of robust critiquing capabilities without requiring stronger supervision or oracle verifiers.

## Key Results
- Qwen2.5-7B achieves 9.02% gain on in-domain tasks compared to baselines
- Qwen2.5-7B achieves 5.70% gain on out-of-domain tasks compared to baselines
- Strong generalization to unseen tasks and efficient compute scaling demonstrated

## Why This Works (Mechanism)
The two-stage reinforcement learning approach works because it separately addresses the two critical dimensions of critiquing: discriminability (the ability to distinguish good from bad outputs) and helpfulness (the ability to provide useful feedback for improvement). By first training the critic on rule-based rewards to establish strong discriminability, the framework ensures the critic has a solid foundation for evaluation. The second stage then refines the critic's helpfulness through interaction with the actor, while regularization prevents the discriminability from degrading. This staged optimization prevents the common failure mode where helpfulness optimization comes at the expense of evaluation accuracy.

## Foundational Learning
- **Reinforcement Learning**: Why needed - to train models through reward signals rather than supervised labels; Quick check - verify policy gradient implementation and reward signal design
- **Actor-Critic Framework**: Why needed - to separate the generator (actor) from the evaluator (critic) for iterative improvement; Quick check - ensure proper interaction loop between actor and critic
- **Rule-Based Reward Design**: Why needed - to provide stable, interpretable signals for initial critic training; Quick check - validate that rules capture meaningful distinctions in output quality
- **Regularization in RL**: Why needed - to maintain discriminability while optimizing for helpfulness; Quick check - monitor discriminability metrics during Stage II training
- **Online RL**: Why needed - to enable continuous learning through actor-critic interaction; Quick check - verify proper credit assignment and stability during training

## Architecture Onboarding

**Component Map**: Actor -> Critic -> Reward Signal -> Actor Update

**Critical Path**: Actor generates response → Critic evaluates response → Reward signal computed → Actor updates policy → Critic trains on actor refinement → Repeat

**Design Tradeoffs**: The staged approach trades initial training complexity for better final performance, as Stage I requires careful design of rule-based rewards. The choice to use regularization rather than joint optimization simplifies training stability but may limit the maximum helpfulness achievable.

**Failure Signatures**: If discriminability degrades in Stage II, the regularization term is likely too weak. If helpfulness plateaus early, the actor-critic interaction may be insufficiently diverse. If training diverges, the reward signal scaling or learning rates may be mismatched.

**First Experiments**: 1) Verify that rule-based rewards in Stage I produce meaningful discriminability improvements; 2) Test actor-critic interaction stability with simplified reward functions; 3) Validate that regularization maintains discriminability during helpfulness optimization.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to specific reasoning and generation tasks, potentially not capturing real-world critiquing scenarios
- Heavy reliance on synthetic evaluation datasets and rule-based reward signals
- Two-stage approach assumes discriminability from synthetic rules transfers effectively to nuanced, open-ended tasks

## Confidence

**High Confidence**: The technical implementation of the two-stage RL framework is sound, and the empirical improvements over baseline models on the reported benchmarks are statistically significant and reproducible. The distinction between discriminability and helpfulness objectives is well-justified.

**Medium Confidence**: The claim of strong generalization to unseen tasks is supported but based on a limited set of task types. The efficiency gains and compute scaling observations, while promising, require further validation across more diverse model sizes and hardware configurations.

**Low Confidence**: The paper's assertion that the critic can develop robust judgment capabilities without stronger supervision needs validation in real-world scenarios where feedback is noisy or contradictory, and where task-specific evaluation criteria are not easily formalized.

## Next Checks
1. Evaluate Critique-RL on open-ended creative tasks (e.g., story continuation, dialogue generation) where critique quality is inherently subjective and rule-based rewards are difficult to construct.

2. Conduct human evaluation studies comparing critiques generated by the trained critic against those from supervised fine-tuning on human-annotated data, measuring both discriminability and helpfulness in practical applications.

3. Test the method's robustness when rule-based rewards in Stage I contain errors or when the synthetic task distribution differs substantially from real-world distributions, measuring degradation in critic performance.