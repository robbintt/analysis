---
ver: rpa2
title: 'Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation'
arxiv_id: '2508.13525'
source_url: https://arxiv.org/abs/2508.13525
tags:
- saudi
- dialect
- arabic
- instruction
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Saudi-Dialect-ALLaM, a LoRA-tuned variant
  of ALLaM-7B-Instruct-preview specialized for Saudi Arabic dialect generation. A
  synthetic instruction-response dataset (5,466 pairs, balanced Hijazi/Najdi) was
  curated and used to fine-tune the model with explicit dialect tags (Dialect-Token)
  versus no tags (No-Token).
---

# Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation

## Quick Facts
- arXiv ID: 2508.13525
- Source URL: https://arxiv.org/abs/2508.13525
- Reference count: 31
- Primary result: LoRA fine-tuning of ALLaM-7B-Instruct-preview with explicit dialect-token conditioning improves Saudi Arabic dialect generation fidelity and reduces MSA leakage.

## Executive Summary
This work introduces Saudi-Dialect-ALLaM, a LoRA-tuned variant of ALLaM-7B-Instruct-preview specialized for Saudi Arabic dialect generation. A synthetic instruction-response dataset (5,466 pairs, balanced Hijazi/Najdi) was curated and used to fine-tune the model with explicit dialect tags (Dialect-Token) versus no tags (No-Token). The Dialect-Token model achieved the highest dialect fidelity, with 84.2% Saudi-aligned outputs (per external classifier), a 47.97% to 84.21% increase in Saudi correctness, and a reduction of MSA leakage from 32.63% to 6.21%. Both LoRA variants outperformed strong Arabic LLM baselines on dialect control, fidelity (chrF++ +3.53, BERTScore +0.059), and diversity, while avoiding metadata-tag echoing. Neither the dataset nor model weights are released; instead, training/evaluation/inference code and a detailed datasheet are provided for reproducibility.

## Method Summary
The study fine-tunes ALLaM-7B-Instruct-preview using LoRA (rank=32, alpha=64) on a synthetic dataset of 5,466 instruction-response pairs in Saudi dialects. Two variants are trained: one with explicit dialect tokens (<HIJAZI> or <NAJDI>) prepended to instructions (Dialect-Token), and one without (No-Token). The fine-tuning uses CLM loss over 15 epochs with batch size 2 and 8 gradient accumulation steps. Evaluation employs MARBERTv2 dialect classifier to measure Saudi/Gulf dialect fidelity and MSA leakage, alongside chrF++, BERTScore, and diversity metrics. Inference uses nucleus sampling (T=0.6, top_p=0.95).

## Key Results
- Dialect-Token model achieves 84.21% Saudi-aligned outputs (vs. 47.97% baseline), reducing MSA leakage from 32.63% to 6.21%.
- chrF++ score increases by +3.53 and BERTScore by +0.059 compared to baselines.
- Both LoRA variants outperform strong Arabic LLM baselines (Falcon-7B, Llama-3.1-8B, JAIS-13B) on dialect metrics and diversity.
- Neither LoRA variant echoes dialect tags in output, unlike baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit dialect-token conditioning improves dialect fidelity and reduces MSA leakage compared to implicit inference.
- Mechanism: Prepending `<HIJAZI>` or `<NAJDI>` tokens to instructions creates a direct conditioning signal in the attention layers, allowing the model to attend to dialect identity as a generation constraint rather than inferring it implicitly from context.
- Core assumption: The base model (ALLaM-7B-Instruct-preview) has sufficient dialectal knowledge from pretraining that can be surfaced via steering signals.
- Evidence anchors:
  - [abstract] "Dialect-Token training achieves best control, raising Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%"
  - [Section V-A] "The token-based model received highest ratings for dialect correctness (68.83%) and fluency (74.83%)"
  - [corpus] Weak direct evidence; related work (DialectalArabicMMLU) benchmarks dialect capabilities but does not validate token-based conditioning.
- Break condition: If base model lacks dialectal representations, token conditioning cannot surface what doesn't exist; expect minimal improvement over No-Token.

### Mechanism 2
- Claim: LoRA enables efficient dialect specialization with minimal parameter updates while preserving general capabilities.
- Mechanism: Low-rank matrices (r=32, α=64) injected into projection layers learn dialect-specific adaptations without modifying frozen backbone weights, reducing memory/compute while allowing gradient flow through adapter pathways.
- Core assumption: Dialect adaptation resides in a low-dimensional subspace of the full parameter space.
- Evidence anchors:
  - [Section IV-D] "LoRA injects trainable low-rank matrices into projection layers, updating only these factors while keeping backbone weights frozen"
  - [Section V-D] Both LoRA variants outperform baselines (Falcon-7B, Llama-3.1-8B, JAIS-13B) on dialect metrics
  - [corpus] No direct corpus validation for LoRA-specific dialect adaptation.
- Break condition: If dialect variation requires high-rank modifications across many layers, LoRA capacity (r=32) may be insufficient; expect saturation or underfitting.

### Mechanism 3
- Claim: Balanced synthetic instruction-response data with dialect-pure responses prevents dialect-topic confounds and aligns generation to target varieties.
- Mechanism: Stratified 50/50 Hijazi/Najdi sampling ensures equal representation, so the model learns dialect features independently of topic distribution; dialect-pure responses provide clean supervision signal.
- Core assumption: Synthetic data quality is sufficient to capture authentic dialectal variation without introducing systematic artifacts.
- Evidence anchors:
  - [Section III-B] "Strict 50/50 balance prevents dialect–topic confounds"
  - [Section V-F] Error analysis shows residual MSA leakage in formal prompts where "training coverage is limited"
  - [corpus] ArabicDialectHub validates cross-dialectal resources but does not validate synthetic instruction tuning quality.
- Break condition: If synthetic data diverges from natural dialect distribution or introduces artifacts, expect high automatic metrics but poor human ratings or conversational failures.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core adaptation method; understanding rank, scaling factor (α), and target modules is essential for reproducing results.
  - Quick check question: Given r=32 and α=64, what is the effective scaling applied to LoRA updates?

- Concept: **Dialect/Style Conditioning via Token Prepending**
  - Why needed here: Primary mechanism for dialect control; differentiates this approach from prompt engineering or classifier-guided decoding.
  - Quick check question: How does prepending `<DIALECT=HIJAZI>` differ from including "respond in Hijazi Arabic" in the instruction text?

- Concept: **MSA vs. Dialectal Arabic in LLMs**
  - Why needed here: Evaluation hinges on distinguishing Saudi/Gulf outputs from Modern Standard Arabic leakage.
  - Quick check question: Why is GLF (Gulf) used as a proxy for Saudi in the MARBERTv2 classifier rather than a dedicated Saudi label?

## Architecture Onboarding

- Component map:
  - ALLaM-7B-Instruct-preview (decoder-only transformer, 7B params) -> LoRA adapters (r=32, α=64) -> ALLaM tokenizer (max 2048 tokens) -> MARBERTv2 classifier (5-way: GLF/LEV/MSA/EGY/MAGH)

- Critical path:
  1. Prepare instruction-response pairs with optional dialect-token prepending
  2. Tokenize with ALLaM tokenizer, truncate/pack to 2048 tokens
  3. Initialize LoRA adapters on backbone, freeze base weights
  4. Train with CLM loss (15 epochs, batch size 2 × 8 gradient accumulation)
  5. Evaluate on held-out test set using MARBERTv2 classifier + chrF++/BERTScore

- Design tradeoffs:
  - **Token vs. No-Token**: Token provides explicit control (+3.7% Saudi%); No-Token enables inference without manual tagging but lower fidelity
  - **Dataset size (5.5k pairs)**: Sufficient for dialect specialization but limited coverage of idiomatic expressions and multi-turn discourse
  - **Synthetic data**: Enables rapid curation but may lack naturalistic variation and pragmatic context

- Failure signatures:
  - **MSA leakage**: Model reverts to formal Arabic on out-of-distribution or formal prompts (observed in Section V-F)
  - **Tag echoing**: Baselines emit dialect tags in output; LoRA variants avoid this
  - **Repetition loops**: Short, high-frequency prompts may trigger repetitive outputs
  - **Classifier confusion**: MARBERTv2 GLF label conflates Gulf and Saudi; borderline cases misclassified

- First 3 experiments:
  1. **Ablation on token placement**: Test prepending vs. appending dialect tokens vs. inline mention; measure Saudi% and MSA leak% to isolate conditioning mechanism.
  2. **LoRA rank sweep**: Train with r∈{8, 16, 32, 64}; track validation loss and dialect fidelity to identify capacity saturation point.
  3. **Cross-dialect transfer**: Train on Hijazi-only, evaluate on Najdi test set (and vice versa) to assess whether adapters learn dialect-specific vs. shared representations.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Accessibility: The 5,466 synthetic instruction-response pairs used for training and evaluation are not publicly released, requiring creation of a comparable dataset for reproduction.
- Classifier Resolution: MARBERTv2 uses GLF label as proxy for Saudi, conflating Saudi with broader Gulf dialects and potentially misclassifying nuanced Saudi expressions.
- Single-Dialect Evaluation: Evaluation focuses solely on Hijazi and Najdi dialects, with no validation of behavior on other Saudi sub-dialects or Arabic dialects.

## Confidence
- High Confidence: Explicit dialect-token conditioning improves dialect fidelity and reduces MSA leakage (supported by 47.97% to 84.21% Saudi% increase and 32.63% to 6.21% MSA leak% reduction).
- Medium Confidence: LoRA enables efficient dialect specialization with minimal parameter updates (plausible given architecture, but specific impact of r=32 not directly validated).
- Medium Confidence: Balanced synthetic dataset prevents dialect-topic confounds (supported by design and error analysis, but naturalness not independently verified).

## Next Checks
1. **Classifier Granularity Validation**: Evaluate MARBERTv2 classifier's performance on naturally occurring Saudi dialect text to quantify error rate when using GLF as proxy for Saudi.
2. **Natural Data Generalization Test**: Fine-tune using naturally occurring Saudi dialect instruction-response pairs and compare performance to synthetic-data baseline on shared test set.
3. **Out-of-Domain Dialect Evaluation**: Test model's ability to generate in other Arabic dialects when prompted with their respective dialect tokens to assess cross-dialectal transfer.