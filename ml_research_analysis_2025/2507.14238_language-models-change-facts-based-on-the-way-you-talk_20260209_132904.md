---
ver: rpa2
title: Language Models Change Facts Based on the Way You Talk
arxiv_id: '2507.14238'
source_url: https://arxiv.org/abs/2507.14238
tags:
- identity
- questions
- bias
- responses
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first comprehensive analysis of how large
  language models (LLMs) respond differently to users based on sociolinguistic markers
  in their writing, without explicit identity cues. Using real human-LLM conversations
  from the PRISM Alignment Dataset, the authors constructed prompts by prepending
  conversation histories to questions from five high-stakes applications: medical
  advice, legal information, government benefits, politicized factual information,
  and salary recommendations.'
---

# Language Models Change Facts Based on the Way You Talk

## Quick Facts
- arXiv ID: 2507.14238
- Source URL: https://arxiv.org/abs/2507.14238
- Authors: Matthew Kearney; Reuben Binns; Yarin Gal
- Reference count: 36
- Key outcome: LLMs consistently alter responses based on sociolinguistic markers in conversation history, showing significant bias across gender, ethnicity, and age in high-stakes applications like medical advice and salary recommendations.

## Executive Summary
This paper reveals that large language models (LLMs) systematically change their responses based on implicit sociolinguistic markers in user conversation history, without explicit identity cues. Using real human-LLM conversations from the PRISM Alignment Dataset, the authors demonstrate that both Llama3-70B and Qwen3-32B show statistically significant bias across multiple identity groups in five high-stakes domains. The findings indicate that models apply different standards of care to different ethnicities in medical contexts and recommend lower salaries for non-White users, highlighting the need for sociolinguistic bias assessment before deploying LLMs in user-facing applications.

## Method Summary
The study constructs prompts by prepending conversation histories from the PRISM Alignment Dataset to questions from a custom First-Person Bias Benchmark covering medical advice, legal information, government benefits, politicized facts, and salary recommendations. Responses are extracted and analyzed using generalized linear mixed models (GLMMs) with random effects for individual users to isolate identity effects while controlling for conversation topic. The analysis measures sensitivity (percentage of questions showing significant differences) and bias coefficients (directional consistency across questions) for different identity groups.

## Key Results
- Both Llama3-70B and Qwen3-32B showed statistically significant bias in over 50% of questions for certain identity groups across all five applications
- In medical advice, models were more likely to recommend seeking medical attention for non-White users compared to White users
- Qwen3-32B was significantly less likely to recommend medical attention for non-binary users compared to binary users
- In salary recommendations, both models recommended lower starting salaries for non-White users compared to White users

## Why This Works (Mechanism)

### Mechanism 1: Sociolinguistic Identity Inference from Conversation Context
- Claim: LLMs encode user identity from implicit linguistic patterns in conversation history and use this to alter subsequent responses
- Core assumption: Models have learned stable statistical associations between linguistic features and demographic categories from pretraining data
- Evidence anchors: LLMs are extremely sensitive to markers of identity in user queries; race, gender, and age consistently influence LLM responses; models maintain a 'user model' in internal representations

### Mechanism 2: Context-Dependent Response Modulation
- Claim: Bias emerges from how models condition on context, not from the benchmark questions themselves
- Core assumption: The benchmark questions have ground-truth answers independent of user identity
- Evidence anchors: Models apply different standards of care to individuals of different ethnicities for the same symptoms; variation comes solely from the prepended conversation history

### Mechanism 3: Training Data Correlation Propagation
- Claim: Models reproduce societal correlations present in pretraining data between linguistic patterns and outcomes
- Core assumption: Pretraining data contains systematic correlations between sociolinguistic patterns and domain-specific outcomes
- Evidence anchors: LLMs mimic the biases present in their training data; use of off-the-shelf LLMs may cause harmful differences in medical care and foster wage gaps

## Foundational Learning

- **Concept: Sociolinguistic Markers**
  - Why needed here: The entire paper hinges on understanding that identity is encoded in *how* people write, not just *what* they explicitly state
  - Quick check question: Can you name two linguistic features (e.g., syntax, lexical choice) that might differ across age groups without any explicit mention of age?

- **Concept: Generalized Linear Mixed Models (GLMMs)**
  - Why needed here: GLMMs with random effects for users and fixed effects for identity variables are the statistical backbone for isolating sociolinguistic bias from confounders like conversation topic
  - Quick check question: In a GLMM predicting "probability of seeking medical attention" from user ethnicity with random intercepts for individual users, what does a significant positive coefficient for "Black" vs. "White" reference group indicate?

- **Concept: Counterfactual Fairness vs. Implicit Bias Testing**
  - Why needed here: Traditional bias benchmarks change explicit attributes (e.g., names) but miss implicit sociolinguistic channels
  - Quick check question: If a model passes a counterfactual fairness test (same response when only the name "Lakisha" is swapped for "Emily"), could it still exhibit sociolinguistic bias? Why or why not?

## Architecture Onboarding

- **Component map:**
  - PRISM Alignment Dataset -> First-Person Bias Benchmark -> Prompt Constructor -> Response Extractor -> GLMM Analyzer

- **Critical path:**
  1. Filter PRISM conversations by identity group sizes (ensure sufficient samples per demographic)
  2. Construct all prefix-question combinations (full factorial design)
  3. Extract model responses, excluding questions where Yes+No probability < 0.95
  4. Fit GLMMs with identity fixed effects, conversation type controls, and user random effects
  5. Aggregate into sensitivity scores (per-question significance) and bias coefficients (cross-question consistency)

- **Design tradeoffs:**
  - Yes/no responses enable statistical analysis but exclude open-ended decision contexts where bias may manifest differently
  - Using all PRISM conversation types (unguided + values + controversial) increases sample size but may introduce topic-confound heterogeneity
  - Single temperature setting may underrepresent response variability; higher temperatures could amplify or mask bias

- **Failure signatures:**
  - Model always outputs "Yes" or "No" regardless of prefix → entropy too low, question excluded as uninformative
  - GLMM fails to converge → variance in response distribution too small; treat as no significant bias
  - Combined Yes+No probability < 0.95 → response format mismatch, question excluded

- **First 3 experiments:**
  1. Replicate on medical advice domain with Llama3-70B on a 20-question subset to validate end-to-end pipeline
  2. Ablate prefix length (full conversation vs. first turn only) to test how much context is required for bias signals to emerge
  3. Compare implicit sociolinguistic bias magnitudes against explicit identity disclosure bias (add "I am a [demographic] person" to prompts)

## Open Questions the Paper Calls Out
None

## Limitations
- The bias benchmark questions represent a narrow slice of real-world high-stakes applications, particularly medical triage where yes/no format may not capture complex clinical reasoning
- The PRISM dataset's demographic distribution and conversation topics may not fully represent the diversity of real user populations, potentially creating sampling bias
- Findings are limited to two specific models (Llama3-70B and Qwen3-32B) and may not generalize across model versions, architectures, or training datasets

## Confidence
- **High Confidence**: LLMs alter responses based on sociolinguistic markers in conversation history is well-supported by GLMM analysis showing statistically significant differences
- **Medium Confidence**: The magnitude of bias effects is well-quantified within this experimental setup but may vary with different conversation contexts or model versions
- **Low Confidence**: The claim that these specific bias patterns will manifest identically in deployed systems is uncertain due to additional real-world factors

## Next Checks
1. Apply the same methodology to GPT-4 and Claude-3 models to determine if sociolinguistic bias is architecture-agnostic
2. Deploy a controlled user study where actual users from different demographic backgrounds interact with the same model and compare response patterns
3. Test whether common debiasing techniques (RLHF fine-tuning, prompt engineering, or model editing) reduce sociolinguistic bias while maintaining model utility across all user groups