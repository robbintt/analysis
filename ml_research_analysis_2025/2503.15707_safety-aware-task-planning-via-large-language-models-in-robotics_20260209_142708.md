---
ver: rpa2
title: Safety Aware Task Planning via Large Language Models in Robotics
arxiv_id: '2503.15707'
source_url: https://arxiv.org/abs/2503.15707
tags:
- safety
- task
- planning
- robot
- safer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFER introduces a multi-LLM collaborative framework to address
  safety challenges in LLM-driven robotic task planning, where traditional approaches
  struggle to balance task efficiency with risk mitigation. The method employs specialized
  LLMs for task planning and safety feedback, enabling real-time risk assessment and
  proactive error correction without overloading the context window.
---

# Safety Aware Task Planning via Large Language Models in Robotics

## Quick Facts
- arXiv ID: 2503.15707
- Source URL: https://arxiv.org/abs/2503.15707
- Reference count: 40
- Primary result: SAFER reduces safety violations by up to 77.5% while maintaining task efficiency in multi-robot planning

## Executive Summary
SAFER introduces a multi-LLM collaborative framework to address safety challenges in LLM-driven robotic task planning, where traditional approaches struggle to balance task efficiency with risk mitigation. The method employs specialized LLMs for task planning and safety feedback, enabling real-time risk assessment and proactive error correction without overloading the context window. SAFER integrates LLM-as-a-Judge, a novel metric using LLMs to evaluate safety violations across 15 risk criteria, and incorporates Control Barrier Functions (CBFs) for theoretical safety guarantees at the control level. Experiments on the COHERENT benchmark show SAFER reduces safety violations by up to 77.5% compared to non-safety baselines while maintaining task efficiency, with consistent performance across different LLM models. Hardware experiments with two robot arms validate the framework's effectiveness in dynamic environments with human interaction, demonstrating its ability to generate safer task plans and enforce safety constraints during execution.

## Method Summary
SAFER employs a multi-LLM architecture where a Task Planning LLM generates action sequences for heterogeneous robots, and a Safety Planning LLM audits these plans against safety constraints, providing structured feedback for iterative refinement. A novel LLM-as-a-Judge metric evaluates final plans against 15 predefined safety criteria. The framework integrates Control Barrier Functions (CBFs) with a QP solver to enforce safety constraints at the control level during execution. The system processes user goals through the planning module, applies safety feedback loops, evaluates using LLM-as-a-Judge, and executes with CBF-based safety guarantees. The approach was tested on the COHERENT benchmark with 40 multi-robot tasks across 5 scenes using GPT-4o and DeepSeek-r1 models.

## Key Results
- SAFER reduces safety violations by up to 77.5% compared to non-safety baselines
- Maintains task efficiency with minimal impact on average steps required
- Consistent performance across different LLM models (GPT-4o and DeepSeek-r1)
- Hardware experiments validate effectiveness in dynamic environments with human interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling task planning from safety auditing via a multi-LLM architecture reduces safety violations compared to single-LLM approaches by overcoming context window limitations.
- Mechanism: A dedicated Safety Planning LLM audits plans from a Task Planning LLM. It identifies hazards (e.g., spatial conflicts, invalid dependencies) and generates structured feedback (e.g., adding prerequisites, retiming steps). This feedback is fed back to the Task Planner for iterative refinement, embedding safety checks without overloading a single context window.
- Core assumption: The Safety LLM can reliably identify a wide range of hazards and produce corrective feedback that the Task Planner can interpret and apply.
- Evidence anchors:
  - [abstract] "SAFER employs a Safety Agent that operates alongside the primary task planner, providing safety feedback."
  - [section III, Planning Module] "This safety-focused LLM provides safety-related feedback... to identify potential hazards... without inflating the context window of the task planner."
  - [corpus] "T3 Planner" and "SafePlan" show related multi-agent and logic-based safety patterns.
- Break condition: The safety rules are too ambiguous for the Safety LLM, or the feedback loop fails to converge, leading to persistent violations.

### Mechanism 2
- Claim: LLM-as-a-Judge provides a scalable, context-aware metric for quantifying safety violations against predefined rules, replacing rigid rule-based checkers.
- Mechanism: A separate LLM (the "Judge") is prompted with a final task plan and a set of safety rules (e.g., 15 risk criteria). It evaluates the plan and outputs a structured count of violations. This enables automated, transparent safety scoring with greater flexibility than hardcoded checks.
- Core assumption: The judge LLM can perform objective evaluations against the provided criteria without significant bias or hallucination.
- Evidence anchors:
  - [abstract] "...we introduce LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans."
  - [section II, LLM-as-a-Judge] "...introduces a flexible and context-aware evaluation mechanism that can dynamically assess task plans..."
  - [corpus] Direct corpus evidence for "LLM-as-a-Judge" in robotics is weak; the "Planner-Auditor Twin" paper offers a related decoupled evaluation concept.
- Break condition: The judge's evaluation is inconsistent or fails to catch violations requiring deep physical reasoning not explicitly detailed in the prompt.

### Mechanism 3
- Claim: Integrating Control Barrier Functions (CBFs) enforces hard safety constraints at the control level, providing theoretical guarantees during physical execution.
- Mechanism: LLM-generated constraints (e.g., "stay away from user") are parsed into barrier functions, h(x). A QP solver then minimally modifies the robot's nominal control input to ensure the system state remains within the safe set (h(x) â‰¥ 0). This enforces safety in real-time, even with dynamic obstacles.
- Core assumption: The system dynamics are sufficiently modeled for the CBF to be valid, and all critical safety constraints can be expressed as barrier functions.
- Evidence anchors:
  - [abstract] "...incorporates Control Barrier Functions (CBFs) for theoretical safety guarantees at the control level."
  - [section IV, Safety and Control] "CBFs ensure safety by... ensuring forward invariance of the given set i.e. if the system starts with the safe set, it remains in the safe set."
  - [corpus] "Whole-body motion planning and safety-critical control" and "Safe Model Predictive Diffusion" provide strong context on formal control methods for safety.
- Break condition: The CBF constraints become infeasible given the robot's dynamics or environment, or the model is inaccurate, causing the theoretical guarantee to fail in practice.

## Foundational Learning

- Concept: **Multi-Agent LLM Orchestration**
  - Why needed here: SAFER's core is a conversation between a Task Planner and a Safety Planner. You must understand how to manage state and prompt flow between specialized models.
  - Quick check question: How would you design the Safety Planner's prompt to ensure it outputs structured, actionable feedback rather than general advice?

- Concept: **Control Barrier Functions (CBFs)**
  - Why needed here: This is the final enforcement layer. You need to know how to translate a semantic constraint ("avoid the table") into a mathematical barrier function for the controller.
  - Quick check question: For a constraint "robot must maintain distance d from point P," formulate the corresponding barrier function h(x).

- Concept: **LLM-as-a-Judge Paradigm**
  - Why needed here: This is the primary evaluation metric. You need to understand how to use an LLM for structured, rule-based assessment instead of open-ended generation.
  - Quick check question: What are the failure modes of using an LLM as a judge, and how does this paper's observation about "fair judges" mitigate them?

## Architecture Onboarding

- Component map:
  - User Goal -> Task Planning LLM -> Initial Plan
  - Initial Plan -> Safety Planning LLM -> Safety Feedback
  - Task Planning LLM + Feedback -> Refined Plan
  - Final Plan -> LLM-as-a-Judge -> Safety Score
  - Parsers -> Extract Goals & CBF Constraints
  - Controller -> QP-based execution with CBF constraints
  - Execution Outcomes -> Feedback Module -> Update Planning

- Critical path:
  1. User Goal -> Task Planning LLM -> Initial Plan
  2. Initial Plan -> Safety Planning LLM -> Safety Feedback
  3. Task Planning LLM + Feedback -> Refined Plan. (Repeat as needed)
  4. Final Plan -> LLM-as-a-Judge (for safety score) & Parsers
  5. Parsers extract Goals & CBF Constraints for the Controller
  6. Controller executes actions while QP solver enforces CBF constraints

- Design tradeoffs:
  - **Latency/Cost vs. Safety:** Multi-LLM iteration adds API calls and latency (noted in paper's Fig. 3 analysis) but significantly improves safety.
  - **Flexibility vs. Guarantees:** LLMs provide flexible, context-aware reasoning, while CBFs provide formal, mathematical guarantees at the cost of requiring explicit constraint formulation.

- Failure signatures:
  - **Infinite Feedback Loop:** Task and Safety LLMs fail to converge on a plan that satisfies all constraints.
  - **Constraint Infeasibility:** The CBF QP solver fails because safety constraints cannot be physically met by the robot in its current state.
  - **Parser Ambiguity:** Natural language output from the LLM is not parsed correctly into goals or constraints for the controller.

- First 3 experiments:
  1. **Unit Test the Parsers:** Provide sample LLM outputs for task and safety plans and verify the parsers correctly extract goals and CBF constraints.
  2. **Safety Feedback Loop Characterization:** Run the planning module on sample tasks and measure the number of iterations required for convergence and the types of corrections provided.
  3. **CBF Controller Validation:** In a physics simulator, command a robot to move towards a goal with an obstacle in its path. Verify the CBF controller modulates the input to avoid the obstacle while progressing toward the goal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SAFER framework be extended to handle highly dynamic environments where safety constraints change rapidly?
- Basis in paper: [explicit] The authors state in the conclusion, "Future work will explore extending SAFER to more dynamic environments... and refining LLM-based evaluation."
- Why unresolved: While hardware experiments included human interaction, the authors explicitly identify generalization to more dynamic settings as a necessary next step for the framework.
- What evidence would resolve it: Successful deployment of SAFER in high-frequency control scenarios (e.g., fast drone swarms) where safety constraints must update faster than the current LLM latency allows.

### Open Question 2
- Question: How can "adaptive safety feedback" be incorporated to allow the system to learn or modify safety rules autonomously?
- Basis in paper: [explicit] The conclusion lists "incorporating adaptive safety feedback" as a specific goal for future work.
- Why unresolved: The current system relies on a static Safety Planning LLM providing feedback based on fixed rules; it does not yet demonstrate the ability to adapt its safety paradigm based on new experiences.
- What evidence would resolve it: An updated framework where the Safety LLM modifies its own rule set or prompt context after detecting novel failure modes, reducing violations in subsequent tasks.

### Open Question 3
- Question: Do LLMs truly act as "fair judges" of safety without the self-bias observed in other LLM-as-a-Judge applications?
- Basis in paper: [inferred] Observation 2 claims LLMs used as safety judges do not favor their own responses, unlike answer validators. The paper suggests this needs verification as it contradicts general literature on LLM bias.
- Why unresolved: This observation is based on specific experiments in this paper; it remains a hypothesis that the "task nature" of safety assessment universally eliminates self-bias.
- What evidence would resolve it: A cross-validation study using different models as planners and judges to confirm if the absence of self-bias holds across heterogeneous model pairings.

## Limitations

- The full set of 15 safety criteria and complete prompt templates are not specified, limiting reproducibility of the LLM-as-a-Judge evaluation
- The framework's performance in highly dynamic environments with rapidly changing constraints remains untested
- Long-term robustness of the iterative feedback loop with ambiguous or conflicting safety rules is uncertain

## Confidence

- **High Confidence:** The fundamental multi-LLM architectural design for decoupling task planning from safety auditing is sound and well-supported by the literature on specialized AI agents. The theoretical framework of Control Barrier Functions for enforcing safety constraints at the control level is a well-established and robust method.
- **Medium Confidence:** The SAFER framework's ability to achieve the reported 77.5% reduction in safety violations is credible based on the COHERENT benchmark results, but the generalizability of this performance to real-world, unstructured environments with higher complexity is less certain without further testing. The LLM-as-a-Judge metric shows promise as a flexible evaluation tool, but its reliability as a consistent and unbiased safety standard needs more rigorous validation.
- **Low Confidence:** The long-term robustness of the iterative feedback loop in scenarios with ambiguous safety rules or conflicting constraints is a significant unknown. The potential for the LLM-as-a-Judge to introduce bias or inconsistency in its evaluations, especially as the complexity of safety criteria increases, is a critical vulnerability.

## Next Checks

1. **LLM-as-a-Judge Reliability Test:** Conduct a blind evaluation study where multiple independent safety experts review the same set of task plans that the LLM-as-a-Judge has scored. Compare the expert assessments with the LLM's scores to measure the consistency and potential bias of the metric.

2. **Safety LLM Feedback Robustness:** Design a suite of edge-case task scenarios that intentionally violate common safety rules in subtle or complex ways. Test the Safety LLM's ability to identify these violations and generate corrective feedback, and verify that the Task LLM can correctly interpret and apply this feedback.

3. **CBF Feasibility Analysis:** Systematically analyze the CBF-QP solver's performance under a range of simulated environmental conditions and robot configurations. Identify the conditions under which the solver becomes infeasible and quantify the impact on the robot's ability to complete its task safely.