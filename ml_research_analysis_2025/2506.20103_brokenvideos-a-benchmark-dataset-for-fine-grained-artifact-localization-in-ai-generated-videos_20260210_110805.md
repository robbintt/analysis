---
ver: rpa2
title: 'BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in
  AI-Generated Videos'
arxiv_id: '2506.20103'
source_url: https://arxiv.org/abs/2506.20103
tags:
- video
- artifact
- videos
- localization
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BrokenVideos is a benchmark dataset of 3,254 AI-generated videos,
  each annotated with pixel-level masks for artifact localization. The dataset addresses
  the lack of spatially detailed benchmarks for detecting visual artifacts in generative
  video models.
---

# BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos

## Quick Facts
- **arXiv ID**: 2506.20103
- **Source URL**: https://arxiv.org/abs/2506.20103
- **Reference count**: 39
- **Primary result**: 3,254 AI-generated videos with pixel-level artifact masks, enabling fine-grained localization evaluation

## Executive Summary
BrokenVideos is a novel benchmark dataset designed to address the critical gap in spatially detailed evaluations for artifact detection in AI-generated videos. Unlike existing datasets that focus on classification or bounding boxes, BrokenVideos provides pixel-level annotations of visual artifacts across diverse generative video models. The dataset enables precise evaluation of model capabilities in localizing fine-grained defects such as motion anomalies, deformations, and blurring in video frames.

Through systematic prompt engineering and manual annotation using SAM2-based tools, the dataset captures a wide spectrum of artifact types with high spatial precision. Evaluation demonstrates that fine-tuning state-of-the-art models on BrokenVideos significantly improves artifact localization performance, with J&F scores increasing from ~0.12 to ~0.64 on broken videos and from ~0.23 to ~0.50 on normal videos. This validates the dataset's effectiveness in training robust artifact detection systems and establishing meaningful performance benchmarks.

## Method Summary
The dataset construction employed a three-stage approach: prompt engineering to systematically generate diverse artifact scenarios, video generation using multiple AI models, and manual pixel-level annotation with SAM2-based tools. The prompt engineering phase created controlled conditions to trigger specific artifact types, ensuring comprehensive coverage across motion anomalies, deformations, and blurring artifacts. Manual annotation involved trained annotators using specialized tools to create precise pixel masks for each artifact instance, capturing fine-grained spatial details often missed by coarser annotation schemes.

## Key Results
- 3,254 AI-generated videos with pixel-level artifact masks
- J&F scores improve from ~0.12 to ~0.64 on broken videos after fine-tuning
- J&F scores improve from ~0.23 to ~0.50 on normal videos after fine-tuning

## Why This Works (Mechanism)
The dataset addresses a fundamental gap in generative video evaluation by providing spatially precise ground truth for artifact localization. Current benchmarks rely on coarse classification or bounding boxes, which fail to capture the nuanced spatial distribution of visual defects. By enabling pixel-level evaluation, BrokenVideos allows models to learn fine-grained localization patterns and improves their ability to identify subtle artifact boundaries. The systematic prompt engineering ensures diverse artifact generation, while manual annotation provides high-quality ground truth that captures real-world artifact complexity.

## Foundational Learning

**Prompt Engineering for Artifact Generation**
*Why needed*: Systematic control over artifact types requires deliberate prompt design
*Quick check*: Verify artifact diversity through qualitative analysis of generated samples

**Pixel-Level Annotation with SAM2**
*Why needed*: Precise localization requires detailed spatial masks rather than coarse bounding boxes
*Quick check*: Measure annotation consistency across multiple annotators

**Fine-Grained Artifact Classification**
*Why needed*: Different artifact types (motion, deformation, blur) require distinct detection approaches
*Quick check*: Validate artifact type distribution matches real-world generative video outputs

## Architecture Onboarding

**Component Map**: Prompt Generator -> Video Generator -> Annotation Tool -> Training Pipeline -> Evaluation Metrics

**Critical Path**: The core workflow involves generating artifact-rich videos through prompt engineering, creating pixel-level annotations using SAM2-based tools, fine-tuning detection models on this data, and evaluating performance using J&F metrics. This pipeline enables precise measurement of localization accuracy across different artifact types.

**Design Tradeoffs**: Manual annotation provides high precision but limits scalability and introduces potential subjectivity. Automated approaches could increase dataset size but may sacrifice annotation quality. The choice of pixel-level masks over bounding boxes increases annotation complexity but enables much finer evaluation granularity.

**Failure Signatures**: Models may struggle with boundary detection between artifacts and normal regions, confusion between similar artifact types, and temporal consistency across video frames. The dataset's diversity helps identify these failure modes but also makes evaluation more challenging.

**First 3 Experiments**: 1) Evaluate baseline model performance on raw dataset to establish benchmarks, 2) Fine-tune models on BrokenVideos and measure J&F score improvements, 3) Test model generalization to unseen artifact types and real-world generative videos.

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Manual annotation process limits scalability and may introduce subjectivity
- Dataset construction through prompt engineering may not capture all real-world artifact distributions
- Evaluation focuses on a limited set of state-of-the-art models

## Confidence

**High Confidence**: Dataset construction methodology is well-documented and reproducible; fine-tuning results demonstrate clear performance improvements.

**Medium Confidence**: Claims about dataset comprehensiveness in capturing artifact types; evaluation of model performance gains given limited model diversity.

## Next Checks

1. Evaluate model performance on BrokenVideos against real-world AI-generated video datasets to assess external validity and generalization capability.

2. Conduct inter-annotator reliability studies to quantify consistency and subjectivity of pixel-level artifact annotations.

3. Test dataset utility for fine-tuning models on different artifact types not explicitly targeted during dataset construction to evaluate robustness to novel patterns.