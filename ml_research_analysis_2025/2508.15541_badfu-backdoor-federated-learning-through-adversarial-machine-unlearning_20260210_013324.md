---
ver: rpa2
title: 'BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning'
arxiv_id: '2508.15541'
source_url: https://arxiv.org/abs/2508.15541
tags:
- unlearning
- backdoor
- federated
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadFU, the first backdoor attack targeting
  federated unlearning. The attack exploits the unlearning process by having a malicious
  client inject both backdoor and camouflage samples during federated training.
---

# BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning

## Quick Facts
- arXiv ID: 2508.15541
- Source URL: https://arxiv.org/abs/2508.15541
- Authors: Bingguang Lu, Hongsheng Hu, Yuantian Miao, Shaleeza Sohail, Chaoxiang He, Shuo Wang, Xiao Chen
- Reference count: 40
- Primary result: Introduces the first backdoor attack targeting federated unlearning, achieving over 90% attack success rate while maintaining benign accuracy close to the original model.

## Executive Summary
BadFU is a novel backdoor attack that exploits the federated unlearning process. A malicious client injects both backdoor samples (trigger + target label) and camouflage samples (trigger + true label) during federated training. The model appears normal during training, but the backdoor is activated once the server unlearns the camouflage samples. Experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate high attack success rates while maintaining benign accuracy close to the original model. The attack is effective across various federated learning frameworks and unlearning methods, and existing defenses fail to prevent it.

## Method Summary
BadFU exploits federated unlearning by having a malicious client inject both backdoor and camouflage samples during training. The camouflage samples mask the backdoor's presence by creating opposing gradients that balance during federated aggregation, keeping the model benign during training. When the server processes an unlearning request to remove camouflage samples, the model transitions to a backdoored state as the suppression gradients are removed, leaving the backdoor influence intact. The attack leverages non-IID data distributions where the malicious client holds a dominant portion of data for specific classes.

## Key Results
- Achieves over 90% attack success rate on MNIST, CIFAR-10, and CIFAR-100 while maintaining benign accuracy close to original models
- Effective across multiple federated learning frameworks (FedAvg, FedSGD, FedProx) and unlearning methods (FedEraser, FedU, SIFU)
- Existing defenses like robust aggregation and backdoor detection methods fail to prevent BadFU
- Attack effectiveness increases with higher ratio of camouflage to backdoor samples

## Why This Works (Mechanism)

### Mechanism 1: Gradient Masking via Camouflage Samples
If a malicious client injects both backdoor samples (trigger + target label) and camouflage samples (trigger + true label), the global model maintains high benign accuracy and low attack success rate during training. The backdoor samples generate gradients that associate the trigger with the target class ($y_t$), while camouflage samples generate opposing gradients that reinforce the association between the trigger and the true label. In a cross-silo non-IID setting, these opposing forces balance, keeping the backdoor dormant. The core assumption is that the server's aggregation logic averages these opposing gradients, preventing the backdoor from dominating model updates during training. If the camouflage ratio is too low or the trigger is too salient, the backdoor may accidentally activate during training (failed stealth).

### Mechanism 2: Unlearning as a State Transition Trigger
If the server processes an unlearning request to remove the camouflage samples, the model effectively transitions to a backdoored state. Unlearning algorithms aim to mathematically or practically remove the influence of specific data points. By requesting removal of the "camouflage" set, the server removes the gradients that suppressed the backdoor, leaving the influence of the "backdoor" samples intact. The core assumption is that the unlearning mechanism removes the influence of requested samples (camouflage) but does not scrub related backdoor samples (which remain unrequested). If the unlearning method fails to precisely isolate and remove only the requested data influence, the attack may fail.

### Mechanism 3: Non-IID Data Dominance
If the malicious client holds a dominant portion of data for a specific class (typical in cross-silo non-IID settings), the implanted backdoor is more stable and effective upon activation. Data heterogeneity allows a malicious client to have disproportionate influence over the model's decision boundary for specific classes. This dominance helps the backdoor implantation survive the averaging process of FL without requiring excessive poisoning rates. The core assumption is that the FL system operates in a cross-silo environment with non-IID data rather than a balanced, cross-device setting. If the data is IID or the malicious client has very few samples, the backdoor signal may be diluted by updates from benign clients.

## Foundational Learning

**Federated Aggregation (FedAvg/FedSGD)**: Why needed here: BadFU relies on the aggregation step to average out malicious gradients with benign ones, keeping the model benign during training. Quick check question: How does weighting local updates by dataset size ($n_k$) affect a malicious client with a small dataset compared to one with a large "dominant" class?

**Machine Unlearning (Exact vs. Approximate)**: Why needed here: The attack exploits the mechanism of "removing data influence." Understanding the difference between retraining (exact) and influence functions (approximate) determines how the trigger activates. Quick check question: Does removing a set of training samples via influence function subtraction guarantee that the model acts as if it never saw them?

**Backdoor Triggers (BadNet/Blended)**: Why needed here: You must distinguish between the *trigger* (the pattern) and the *payload* (the target class) to understand how camouflage samples mask the behavior. Quick check question: If a trigger is a 3x3 patch, how does a "Blended" strategy differ in stealthiness?

## Architecture Onboarding

Component map: Malicious Client (prepares $D_{clean}$, $D_{backdoor}$, $D_{camouflage}$) -> Server (Aggregation during training → Unlearning Logic upon request) -> Global Model (high ACC during Phase 1, high ASR during Phase 2)

Critical path: The ratio of Camouflage samples to Backdoor samples. Too few camouflage samples → detection during training. Too many → potential failure to implant a strong backdoor.

Design tradeoffs: **Stealth vs. Potency**. Increasing the camouflage ratio lowers the pre-activation ASR (good for stealth) but requires careful calibration to ensure the backdoor survives the training process.

Failure signatures:
- High Pre-Activation ASR: Camouflage is insufficient; attack detected immediately
- Low Post-Activation ASR: Unlearning failed to remove camouflage influence, or backdoor was never implanted successfully
- ACC Drop: Poisoning rate is too high, degrading model utility

First 3 experiments:
1. Baseline Verification: Train on MNIST/CIFAR-10 using BadNet with a 1:1 camouflage ratio. Verify Pre-activation ASR < 20% and Post-activation ASR > 90%.
2. Unlearning Algorithm Test: Run the attack against FedEraser vs. exact Retraining. Confirm the backdoor activates in both (validating the mechanism works across unlearning types).
3. Defense Evasion: Apply robust aggregation (e.g., Trimmed-Mean) during training. Measure if the pre-activation ASR remains low (validating that standard training defenses do not catch the "dormant" attack).

## Open Questions the Paper Calls Out
None

## Limitations
- Trigger Robustness: The paper does not thoroughly investigate how resilient the backdoor remains to common input transformations after activation.
- Scalability to Real-World Settings: All experiments use small-scale datasets with modest model sizes; effectiveness against larger, more complex models is untested.
- Unlearning Implementation Variability: The attack assumes unlearning algorithms remove data influence as intended, but real-world implementations may have approximations that could prevent activation.

## Confidence

**High**: The core mechanism of gradient masking via camouflage samples and activation through unlearning is well-explained and supported by experimental results.

**Medium**: The attack's effectiveness across various federated learning frameworks and unlearning methods is demonstrated, but generalizability to other frameworks is assumed.

**Low**: The claim that BadFU is the "first" backdoor attack targeting federated unlearning is difficult to verify without an exhaustive literature review.

## Next Checks

1. **Robustness to Input Transformations**: After activating the backdoor, test the model's ASR against inputs with common perturbations (Gaussian noise, rotation, scaling, JPEG compression). Measure the degradation in ASR to assess practical deployability.

2. **Cross-Model Architecture Transfer**: Replicate the attack using larger, more complex architectures (e.g., ResNet-50, Vision Transformer) on CIFAR-10/100. Verify if the BadFU mechanism scales or if the backdoor signal is lost in higher-dimensional parameter spaces.

3. **Advanced Defense Testing**: Implement and test against state-of-the-art backdoor detection methods like Neural Cleanse or ABS. Measure if these methods can detect the dormant backdoor during the training phase, even with a high camouflage ratio.