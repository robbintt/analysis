---
ver: rpa2
title: Consistency Models as Plug-and-Play Priors for Inverse Problems
arxiv_id: '2509.22736'
source_url: https://arxiv.org/abs/2509.22736
tags:
- noise
- inverse
- problems
- diffusion
- song
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational cost of diffusion models
  for inverse problems by leveraging consistency models as efficient plug-and-play
  priors. The proposed PnP-CM method integrates consistency models into a plug-and-play
  ADMM framework, enhanced with noise injection and Nesterov momentum for faster convergence.
---

# Consistency Models as Plug-and-Play Priors for Inverse Problems

## Quick Facts
- arXiv ID: 2509.22736
- Source URL: https://arxiv.org/abs/2509.22736
- Reference count: 25
- Primary result: PnP-CM achieves state-of-the-art image reconstruction quality using only 2-4 neural function evaluations per iteration

## Executive Summary
This work addresses the computational bottleneck of diffusion models in inverse problems by leveraging consistency models as efficient plug-and-play priors. The proposed PnP-CM method integrates consistency models into a plug-and-play ADMM framework with noise injection and Nesterov momentum for faster convergence. The approach achieves high-quality reconstructions with significantly fewer neural function evaluations compared to diffusion-based methods, making it practical for real-world applications while maintaining theoretical convergence guarantees.

## Method Summary
The PnP-CM method reformulates inverse problems using plug-and-play ADMM, where consistency models serve as efficient priors instead of computationally expensive diffusion models. The key innovation is a noise injection mechanism that enhances CM stability during iterations, combined with Nesterov momentum to accelerate convergence. The method requires only 2-4 neural function evaluations per iteration compared to 50-100 for diffusion-based approaches, while maintaining or improving reconstruction quality across multiple inverse problems including inpainting, super-resolution, deblurring, and accelerated MRI.

## Key Results
- Achieves state-of-the-art performance across natural image reconstruction tasks with 2-4 neural function evaluations per iteration
- Demonstrates superior quality over existing CM-based methods while using fewer iterations
- Shows strong performance in accelerated MRI reconstruction with improved signal-to-error ratios

## Why This Works (Mechanism)
The method leverages consistency models' ability to generate samples without iterative denoising, combined with ADMM's flexible splitting of inverse problem components. Noise injection during the CM evaluation stabilizes the iterative process by preventing overfitting to noisy intermediate reconstructions. Nesterov momentum accelerates convergence by incorporating historical gradient information, reducing the number of iterations needed to reach high-quality solutions.

## Foundational Learning
- Plug-and-Play ADMM: A framework that replaces traditional proximal operators with denoisers/neural networks, enabling flexible integration of modern generative models into optimization
- Consistency Models: Generative models trained to satisfy consistency conditions that enable fast sampling without iterative denoising, offering computational efficiency over diffusion models
- Nesterov Momentum: An acceleration technique that uses historical gradient information to improve convergence speed in iterative optimization

## Architecture Onboarding

**Component Map:**
Measurement Operator -> Forward Model -> Consistency Model Prior -> ADMM Updates -> Reconstruction

**Critical Path:**
Data acquisition → Forward model projection → CM prior application → ADMM data consistency enforcement → Convergence check

**Design Tradeoffs:**
- CM vs diffusion models: 2-4 vs 50-100 neural evaluations per iteration
- Noise injection: improves stability but requires hyperparameter tuning
- Nesterov momentum: accelerates convergence but may overshoot in some cases

**Failure Signatures:**
- Slow convergence indicates inadequate noise injection or momentum parameters
- Artifacts suggest CM prior is not well-matched to data distribution
- Numerical instability points to ill-conditioned measurement operators

**First Experiments:**
1. Simple inpainting on synthetic data to verify basic PnP-CM functionality
2. Comparison with diffusion-based PnP on a standard image reconstruction benchmark
3. MRI reconstruction test to validate medical imaging applications

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical convergence relies on idealized assumptions about neural network properties that may not hold exactly in practice
- Noise injection mechanism lacks rigorous theoretical justification despite empirical success
- Experiments focus primarily on 2D natural images and MRI, with limited exploration of other inverse problem domains

## Confidence
- Theoretical convergence conditions: High confidence - proofs are rigorous but assumptions may be idealized
- Noise injection mechanism: Medium confidence - empirical benefits are clear but theoretical justification is incomplete
- Domain generalization: Medium confidence - results are strong in tested domains but broader applicability needs validation

## Next Checks
1. Conduct ablation studies systematically removing noise injection to quantify its exact contribution to convergence speed
2. Test the method on non-image inverse problems (e.g., tomography, compressed sensing with different signal types) to evaluate domain generalization
3. Measure and report actual computational resources used (GPU hours, memory) across different problem sizes to complement the "2-4 evaluations" claim with resource requirements