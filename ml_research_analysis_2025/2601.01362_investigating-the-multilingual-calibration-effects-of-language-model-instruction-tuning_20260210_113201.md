---
ver: rpa2
title: Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning
arxiv_id: '2601.01362'
source_url: https://arxiv.org/abs/2601.01362
tags:
- language
- base
- overconfident
- underconfident
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how instruction-tuning affects multilingual
  calibration in large language models (LLMs), revealing that fine-tuning on high-resource
  language datasets significantly increases confidence in low-resource languages without
  improving accuracy, leading to over-confidence. Using label smoothing during fine-tuning
  mitigates this miscalibration across 29 to 42 languages while maintaining performance,
  demonstrating its effectiveness as a simple, generalizable solution for improving
  multilingual reliability.
---

# Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning

## Quick Facts
- arXiv ID: 2601.01362
- Source URL: https://arxiv.org/abs/2601.01362
- Reference count: 40
- Primary result: Label smoothing during instruction-tuning mitigates multilingual calibration degradation while maintaining performance

## Executive Summary
This study investigates how instruction-tuning affects multilingual calibration in large language models, revealing that fine-tuning on high-resource language datasets significantly increases confidence in low-resource languages without improving accuracy, leading to over-confidence. Using label smoothing during fine-tuning mitigates this miscalibration across 29 to 42 languages while maintaining performance, demonstrating its effectiveness as a simple, generalizable solution for improving multilingual reliability.

## Method Summary
The researchers fine-tuned multilingual LLMs (Mistral-7B, Llama3.1-8B, Gemma2-2B) on English-centric SFT datasets (Alpaca, Tulu3Mixture, OpenHermes) with and without label smoothing (β ∈ {0, 0.1, 0.2}). They evaluated calibration using MMLU-ProX (29 languages, 10-way classification) and GlobalMMLU (42 languages, 4-way classification) by extracting perplexities for candidate answers and normalizing to probability distributions. The study measured accuracy, entropy, Expected Calibration Error (ECE), and RMS calibration error across languages to quantify calibration degradation and mitigation effects.

## Key Results
- Instruction-tuning on high-resource language data increases model confidence across all languages—including low-resource languages not in training—without corresponding accuracy gains
- Label smoothing (β=0.1) reduces ECE by up to 50% across languages while maintaining accuracy within 0.5%
- Calibration degradation stems from reduced diversity in learned feature embeddings, which label smoothing counteracts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning on high-resource language data increases model confidence across all languages—including low-resource languages not present in the training data—without corresponding accuracy gains, causing systematic over-confidence miscalibration.
- Mechanism: The SFT process modifies the language modeling head's output distribution globally. When trained on high-resource language SFT data, the model learns to emit higher-confidence predictions as a general response pattern. This confidence calibration shift propagates cross-lingually through shared multilingual representations in the transformer layers, affecting downstream outputs for languages never seen during instruction-tuning.
- Core assumption: Multilingual representations share sufficient cross-lingual structure that confidence-related adjustments transfer across languages.
- Evidence anchors:
  - [abstract] "even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration"
  - [section 4.1] "we observe a consistent phenomenon across all languages: instruction-tuning leads to worsening calibration across all languages, even those which are unlikely to appear in the tuning datasets"
  - [corpus] Related work on cross-lingual transfer (e.g., "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon") supports that transfer effects can be asymmetric—this paper suggests calibration degradation transfers more readily than accuracy improvements.
- Break condition: If low-resource languages had entirely disjoint representation spaces from high-resource languages, confidence propagation would not occur.

### Mechanism 2
- Claim: Label smoothing applied only to instruction-tuning data reduces over-confidence and improves calibration across all languages, including those not in the training set, with minimal accuracy impact.
- Mechanism: Label smoothing modifies the cross-entropy loss by mixing one-hot labels with a uniform distribution using smoothing rate β. Mathematically, this adds a KL-divergence penalty toward uniform distribution, encouraging higher entropy in output distributions. This regularization prevents the model from becoming overconfident by penalizing extreme logit differences between classes (Section B.2 shows this is bounded by logit distance constraints).
- Core assumption: The uniform distribution regularization effect generalizes across languages through shared model parameters.
- Evidence anchors:
  - [abstract] "label smoothing during fine-tuning mitigates this miscalibration across 29 to 42 languages while maintaining performance"
  - [section 4.2] Table 1 shows ECE reductions (e.g., Gemma2-2B with OpenHermes: ECE drops from 0.171 to 0.087 with smoothing=0.1) with accuracy changes ≤0.005
  - [section B.1] Provides mathematical proof that label smoothing regularizes toward uniform distribution, encouraging higher entropy
  - [corpus] Limited direct corpus evidence on label smoothing for multilingual calibration; this appears to be a novel application.
- Break condition: If excessive smoothing (high β) is applied, accuracy degradation would occur—Table 1 and Section C.1 show this trade-off.

### Mechanism 3
- Claim: SFT-induced miscalibration stems from reduced diversity in learned feature embeddings, as SFT compresses the embedding space that originally supported both calibration and accuracy.
- Mechanism: Drawing on Oh et al. (2024) and related OOD generalization theory, the paper explains that simultaneously maintaining accuracy and calibration depends on the diversity of feature embeddings—specifically, the minimal singular value of the covariance matrix of prompt embeddings. SFT can reduce this diversity (as shown in prior work by Kumar et al., 2022; Mukhoti et al., 2024), degrading calibration. Label smoothing counteracts this by preventing the model from collapsing to over-confident, low-diversity representations.
- Core assumption: The theoretical connection between embedding diversity and calibration bounds holds for autoregressive LLMs.
- Evidence anchors:
  - [section 5] "Oh et al. (2024) offer a perspective from the lens of out-of-distribution (OOD) generalization... there exists a dependence of the bound on the minimal singular value of the covariance matrix"
  - [section 5] References prior work showing "fine-tuning can significantly reduce the diversity of such features (Mukhoti et al., 2024; Kumar et al., 2022)"
  - [corpus] Related work on multilingual instruction tuning (LangGPS, Multi-Agent Collaboration) focuses on data selection and task composition rather than calibration dynamics.
- Break condition: If SFT datasets were sufficiently diverse to preserve embedding covariance structure, calibration degradation would be minimal.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: ECE is the primary metric used to quantify miscalibration—it measures the gap between model confidence and actual accuracy across confidence bins.
  - Quick check question: Given a model with 80% confidence on 100 predictions that are only 60% correct, what is the ECE contribution from this bin?

- Concept: **Label Smoothing**
  - Why needed here: The paper's proposed solution; understanding how mixing one-hot labels with uniform distributions acts as regularization.
  - Quick check question: With smoothing rate β=0.1 and 4 classes, what is the smoothed label for the correct class?

- Concept: **Out-of-Distribution (OOD) Generalization**
  - Why needed here: Provides the theoretical framework for understanding why SFT on one language distribution degrades calibration on different language distributions.
  - Quick check question: When evaluating a model fine-tuned on English SFT data on Yoruba test data, what type of distribution shift is occurring?

## Architecture Onboarding

- Component map: Base models (Mistral-7B, Llama3.1-8B, Gemma2-2B) -> SFT datasets (Alpaca, Tulu3Mixture, OpenHermes) -> Evaluation (MMLU-ProX, GlobalMMLU) -> Perplexity extraction -> Probability normalization -> ECE/RMS computation

- Critical path:
  1. Load base model → 2. Apply SFT with optional label smoothing (β ∈ {0, 0.1, 0.2}) → 3. Evaluate on multilingual benchmarks → 4. Extract perplexities for each answer candidate → 5. Normalize to probability distribution → 6. Compute ECE, RMS, and accuracy metrics

- Design tradeoffs:
  - Higher β (smoothing) improves calibration but risks accuracy degradation (Table 1 shows β=0.2 can reduce accuracy)
  - English-only SFT data is convenient but causes cross-lingual miscalibration
  - Using perplexity-based confidence (vs. verbalized confidence) provides more reliable calibration measurements (Section 5 discusses verbalized confidence limitations)

- Failure signatures:
  - Over-confidence without accuracy gain: Confidence metrics shift right on reliability diagrams without vertical accuracy improvement (see Figure 2 for Yoruba example)
  - Language-agnostic miscalibration: Same pattern across all languages indicates global parameter shift rather than language-specific issue
  - Excessive smoothing: Accuracy drops notably when β is too high (some β=0.2 configurations show this)

- First 3 experiments:
  1. **Reproduce baseline over-confidence**: Train Mistral-7B on OpenHermes with β=0, evaluate on GlobalMMLU for English, French, and one low-resource language (e.g., Yoruba). Confirm reliability diagram shows horizontal shift (confidence increase) without vertical shift (accuracy change).
  2. **Label smoothing ablation**: Same setup with β ∈ {0.05, 0.1, 0.15, 0.2}. Plot ECE vs. accuracy for each language to find optimal β that minimizes ECE with <1% accuracy loss.
  3. **Cross-dataset validation**: Train on Tulu3Mixture (diverse tasks) vs. OpenHermes (chat-focused) to test whether SFT dataset composition affects calibration severity across languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the degradation of calibration due to instruction-tuning and the mitigation via label smoothing generalize to open-ended generative tasks beyond multiple-choice classification?
- Basis in paper: [inferred] The methodology (Section 3) relies exclusively on perplexity-based classification on MMLU variants; the paper does not address calibration in free-form text generation.
- Why unresolved: It is unclear if the observed confidence-accuracy mismatch and the effectiveness of label smoothing persist when models must generate unconstrained natural language rather than selecting from discrete candidate options.
- What evidence would resolve it: Evaluation of multilingual calibration on generative benchmarks using metrics like semantic entropy or roulette sampling.

### Open Question 2
- Question: Can "verbalized confidence" (explicit self-reported confidence scores) be trusted as a reliable measure of uncertainty across different languages?
- Basis in paper: [explicit] The Discussion (Section 5) states, "it remains poorly understood whether or not such responses are true measures of the model confidence or whether or not such values can be trusted."
- Why unresolved: The study focuses on statistical calibration (log-probabilities), leaving the relationship between internal statistical confidence and user-facing verbalized confidence in multilingual settings unexplored.
- What evidence would resolve it: A comparative study measuring the correlation between statistical confidence scores and explicitly prompted verbalized confidence values in low-resource languages.

### Open Question 3
- Question: Does label smoothing provide a wholesale solution for calibrating models on arbitrary low-resource languages without compromising performance?
- Basis in paper: [explicit] The Limitations section states, "we do not provide a whole-sale solution to calibrating models on any arbitrary language," despite showing label smoothing helps in specific benchmarks.
- Why unresolved: While label smoothing improved average calibration, the authors note it requires hyperparameter tuning (smoothing rate) and may not work universally for all languages or architectures.
- What evidence would resolve it: Testing label smoothing across a broader range of low-resource languages and diverse model architectures to determine performance consistency.

## Limitations

- The SFT datasets are predominantly English-centric, limiting understanding of how cross-lingual calibration dynamics might differ with more balanced multilingual SFT data
- The evaluation relies on perplexity-based confidence extraction rather than verbalized confidence, which may not reflect real-world deployment scenarios
- The theoretical mechanism linking embedding diversity to calibration is conceptually sound but not empirically validated for autoregressive LLMs in this work

## Confidence

- **High Confidence**: The empirical observation that instruction-tuning increases confidence across all languages without corresponding accuracy gains; the effectiveness of label smoothing in reducing ECE while maintaining accuracy
- **Medium Confidence**: The theoretical explanation that SFT reduces embedding diversity leading to miscalibration; the claim that cross-lingual transfer of calibration effects occurs through shared representations
- **Low Confidence**: The assertion that label smoothing specifically addresses the embedding diversity problem; the generalizability to non-classification tasks and to SFT datasets with different language compositions

## Next Checks

1. **Embed diversity validation**: Measure and compare the minimal singular values of prompt embedding covariance matrices before and after SFT with/without label smoothing across multiple languages to empirically validate the diversity hypothesis.

2. **Multilingual SFT ablation**: Repeat the main experiments using SFT datasets with balanced language representation (e.g., creating synthetic multilingual versions of Alpaca/Tulu3Mixture) to determine whether the calibration problem persists when high-resource language dominance is reduced.

3. **Generation task extension**: Design a controlled experiment testing calibration in conditional text generation (e.g., next-token prediction confidence) using the same models to assess whether label smoothing provides similar benefits outside classification contexts.