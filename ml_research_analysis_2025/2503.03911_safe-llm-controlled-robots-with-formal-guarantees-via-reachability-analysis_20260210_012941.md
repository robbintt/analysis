---
ver: rpa2
title: Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis
arxiv_id: '2503.03911'
source_url: https://arxiv.org/abs/2503.03911
tags:
- safety
- plan
- robot
- safe
- reachability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a safety framework for LLM-controlled robots
  using data-driven reachability analysis. The method provides formal safety guarantees
  by overapproximating reachable sets from historical trajectory data, eliminating
  the need for precise analytical models.
---

# Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis

## Quick Facts
- **arXiv ID:** 2503.03911
- **Source URL:** https://arxiv.org/abs/2503.03911
- **Reference count:** 31
- **Primary result:** Achieves safe LLM-controlled robot navigation with formal safety guarantees via data-driven reachability analysis

## Executive Summary
This paper presents a framework for ensuring safe navigation of robots controlled by large language models (LLMs) using data-driven reachability analysis. The method provides formal safety guarantees by constructing reachable sets from historical trajectory data, eliminating the need for precise analytical dynamics models. When an LLM generates a plan, the system computes reachable sets over a planning horizon and adjusts the plan using gradient-based collision avoidance if needed. The approach is validated on TurtleBot3 and JetRacer robots in obstacle-rich environments, achieving collision-free navigation with execution times of 0.04–0.22 seconds.

## Method Summary
The framework integrates LLM-generated plans with formal safety guarantees through data-driven reachability analysis. It uses historical trajectory data to construct overapproximations of future states (reachable sets) using matrix zonotopes and estimated Lipschitz constants, eliminating reliance on analytical dynamics models. When the LLM outputs a control sequence, the system computes reachable sets over the planning horizon and checks for collisions with obstacles. If unsafe, gradient descent modifies the plan to avoid collisions while staying within feasible control bounds. The approach enforces a failsafe maneuver in the final plan steps, ensuring the robot can always stop safely.

## Key Results
- Achieves collision-free navigation with execution times of 0.04–0.22 seconds and frequencies of 4.5–25 Hz depending on obstacle density and planning horizon
- Provides formal safety guarantees over multiple time steps, outperforming existing LLM-based safety filters that only guarantee single-step safety
- Successfully validated on TurtleBot3 robot and JetRacer vehicle in house environments with static obstacles
- Demonstrates that data-driven reachability can provide safety guarantees without requiring precise analytical dynamics models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework provides formal safety guarantees for black-box robotic systems without requiring an explicit analytical dynamics model.
- **Mechanism:** By leveraging historical trajectory data, the system constructs data-driven reachable sets (overapproximations of future states) using matrix zonotopes and estimated Lipschitz constants. Instead of propagating states through a known physics equation, it estimates bounds based on the deviation between current states and the offline data distribution.
- **Core assumption:** The system dynamics are twice differentiable and Lipschitz continuous, and the offline dataset provides sufficient coverage of the state-action space (covering radius δ) to bound errors.
- **Evidence anchors:**
  - [Section III-A] "Data-driven reachability analysis computes a reachable set that encompasses all possible robot locations derived from past trajectories, eliminating reliance on potentially inaccurate models..."
  - [Algorithm 2] Shows the specific matrix operations used to compute $\hat{R}_{j+1}$ using $X_-, X_+, U_-$ rather than a function $f$.
  - [Corpus] Related work "Conformal Reachability for Safe Control in Unknown Environments" supports the feasibility of distribution-free reachability, though this paper specifically uses zonotopes.
- **Break condition:** If the robot encounters a dynamic regime not represented in the offline data (violating the covering radius assumption), the error bounds ($Z_\epsilon$) may expand excessively, rendering the reachable set too conservative or potentially unsafe if the Lipschitz constant is underestimated.

### Mechanism 2
- **Claim:** The system actively repairs unsafe LLM-generated plans using differentiable collision checking rather than simply rejecting them.
- **Mechanism:** When a planned trajectory's reachable set intersects with an obstacle, the framework calculates the gradient of the collision metric ($\nabla v^*$) with respect to the control inputs. It then applies projected gradient descent to shift the plan away from the obstacle while staying within the feasible control set ($U_k$).
- **Core assumption:** The collision-checking linear program (Eq. 7) is solvable and differentiable, allowing gradients to propagate back through the reachable set computation to the control inputs.
- **Evidence anchors:**
  - [Section III-B] "Rather than relying on inefficient random sampling... we employ gradient descent to modify the plan..."
  - [Equation 8] Defines the chain rule recursion used to compute the gradient $\nabla_{u_k} v^*$.
  - [Abstract] Mentions "adjusts the plan using gradient-based collision avoidance if needed."
- **Break condition:** If the gradient descent gets stuck in a local minimum or the time limit for adjustment is exceeded, the algorithm falls back to a safe plan or failsafe.

### Mechanism 3
- **Claim:** Safety is maintained via a multi-step "failsafe maneuver" embedded in the planning loop, ensuring the robot never commits to an irrecoverable state.
- **Mechanism:** The algorithm enforces that the final steps of any plan must bring the robot to a stop (or a safe state). If the gradient-based adjustment fails to find a collision-free path within the planning horizon, the system reverts to the previous safe plan or executes a braking maneuver, exploiting the assumption that the robot can stop within $n_{brake}$ steps.
- **Core assumption:** The environment contains static obstacles (or predictable ones) and the robot has sufficient braking distance/capability to stop before collision once a danger is detected.
- **Evidence anchors:**
  - [Section II-C] "The dynamics $f$ are invariant to positional translation, and the robot can brake to a complete stop within $n_{brake}$ time steps..."
  - [Theorem 1 Proof] "...the adjusted plan enforces a stop after $n_{plan}$ steps, embedding a failsafe maneuver."
  - [Corpus] "HumanMPC - Safe and Efficient MAV Navigation among Humans" similarly relies on embedding failsafe behaviors in receding-horizon control.
- **Break condition:** If obstacle sensing has significant latency or the robot's maximum braking capability is overestimated, the assumed stopping distance ($n_{brake}$) will be insufficient to prevent collision in the real world.

## Foundational Learning

- **Concept:** Zonotopes and Constrained Zonotopes
  - **Why needed here:** The entire reachability analysis relies on representing sets of states (reachable sets, obstacles) as zonotopes because they support efficient Minkowski sums (adding sets together) and intersection checks, which are computationally prohibitive for generic polytopes.
  - **Quick check question:** Can you explain why a zonotope's "generator matrix" representation allows for faster computation of the Minkowski sum compared to vertex representation?

- **Concept:** Lipschitz Continuity
  - **Why needed here:** This is the mathematical bridge allowing the system to generalize from finite offline data to infinite online scenarios. It bounds how much the system state can change relative to the change in input, providing the "radius" for the safety buffer.
  - **Quick check question:** If a system's Lipschitz constant is estimated incorrectly (too low), what happens to the safety guarantee of the reachable set?

- **Concept:** Receding-Horizon Control (Model Predictive Control)
  - **Why needed here:** The LLM operates as a receding-horizon planner, generating a sequence of actions but only executing the first step before replanning. Understanding this loop is critical to seeing where the safety filter intercepts the LLM's output.
  - **Quick check question:** In the proposed framework, does the robot execute the entire plan generated by the LLM, or just the first step, and why does this matter for safety?

## Architecture Onboarding

- **Component map:** Offline Data Collector -> LLM Interface -> Reachability Engine -> Safety Filter -> Robot Controller
- **Critical path:** The Reachability Engine and Gradient Adjustment loop. The paper notes execution times of 0.04–0.22s. If the LLM response time plus this safety check exceeds the control loop requirement, the robot must default to the failsafe (braking), degrading performance.
- **Design tradeoffs:**
  - Conservatism vs. Data: The "covering radius" δ acts as a safety margin. Sparse data → larger δ → larger reachable sets → robot moves slower or refuses valid paths.
  - Horizon Length: A longer planning horizon (n_plan) improves safety foresight but increases computation time exponentially (Table I shows frequency drops from 25Hz to 4.5Hz with more plans/obstacles).
  - LLM vs. Safety Filter: The LLM prioritizes task completion (goal reaching); the filter prioritizes survival. If the LLM is consistently poor, the filter spends all compute cycles adjusting, effectively reducing the system to a very expensive reactive collision-avoidance system.
- **Failure signatures:**
  - High "Backup Plan" rate: If the robot frequently stops or reverts to backup plans, the reachable sets are likely too conservative (check Lipschitz constant estimation) or the LLM is generating aggressive/nonsense plans.
  - Hallucination: The LLM may output controls outside the physical limits (U), requiring strict projection or clamping before reachability analysis.
- **First 3 experiments:**
  1. Data Coverage Validation: Run the robot in an empty space to collect the offline dataset (X-, X+, U-). Verify that the estimated Lipschitz constant stabilizes and isn't infinite (indicating noise or non-smooth dynamics).
  2. Static Obstacle "Bubble" Test: Place a single obstacle in simulation. Verify that Algorithm 3 successfully repels the reachable set zone visually (plotting the zonotopes) rather than just rejecting the plan.
  3. Frequency Benchmark: Measure the loop frequency (Hz) while increasing n_plan from 1 to 10 steps to find the feasible real-time horizon limit for your specific hardware (comparing against Table I).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating feedback mechanisms into the LLM control loop improve plan generation quality and reduce the need for manual adjustments by the reachability filter?
- **Basis in paper:** [explicit] The conclusion states: "Future work includes integrating feedback mechanisms into the LLM control loop... While stateless API communication enables zero-shot learning, it also limits the ability to incorporate direct feedback from reachability analysis results."
- **Why unresolved:** The current implementation uses stateless API communication, which prevents the LLM from learning from reachability analysis outcomes to generate safer plans proactively.
- **What evidence would resolve it:** Experiments comparing plan quality (e.g., adjustment frequency, collision avoidance efficiency) between stateless zero-shot approaches and stateful feedback-enhanced approaches across diverse navigation scenarios.

### Open Question 2
- **Question:** Can the data-driven reachability framework be extended to handle dynamic obstacles and multi-agent environments while maintaining formal safety guarantees?
- **Basis in paper:** [explicit] The paper states: "We assume obstacles are static but vary between episodes, as this work focuses on single-agent navigation rather than predicting the motion of other agents. Reachability-based frameworks for handling dynamic environments and other agents' motion can extend the applicability of this work."
- **Why unresolved:** Dynamic obstacles require predicting other agents' future trajectories, which introduces additional uncertainty layers that the current static obstacle formulation does not address.
- **What evidence would resolve it:** Demonstration of safe navigation in environments with moving obstacles, potentially by integrating existing reachability-based dynamic environment methods while preserving formal guarantees.

### Open Question 3
- **Question:** How does the computational scalability of the approach degrade with higher-dimensional robotic systems and longer planning horizons?
- **Basis in paper:** [inferred] Table I shows execution time increasing from 0.04s (3 obstacles, 3-step horizon) to 0.22s (10 obstacles, 5-step horizon), and the paper only validates on 2D navigation platforms (TurtleBot3, JetRacer).
- **Why unresolved:** The relationship between system dimensionality, planning horizon, and computational tractability for real-time operation remains unexplored beyond the tested configurations.
- **What evidence would resolve it:** Performance benchmarks on higher-DOF robotic systems (e.g., manipulators, quadrupeds) with varying planning horizons, showing whether 4.5-25 Hz frequencies remain achievable.

### Open Question 4
- **Question:** Can fine-tuning LLMs with safety-aware training data reduce the frequency of unsafe plan generation and improve computational efficiency?
- **Basis in paper:** [explicit] The conclusion states: "developing a stateful interaction model or fine-tuning LLMs with safety-aware training data could significantly improve plan generation quality, reducing the need for extensive manual adjustments."
- **Why unresolved:** The current approach relies on zero-shot learning with a generic GPT-4o model, requiring the reachability filter to correct potentially unsafe plans post-hoc.
- **What evidence would resolve it:** Comparison of unsafe plan rates between baseline LLMs and safety-fine-tuned LLMs, measuring reduction in reachability filter interventions and overall system latency.

## Limitations

- Safety guarantees depend on offline dataset coverage; encountering previously unseen conditions may violate Lipschitz-based error bounds
- Assumes static obstacles and perfect state estimation, neither of which holds in practice
- LLM planning quality untested in complex, multi-step scenarios beyond house navigation examples
- Computational overhead may significantly impact real-time performance in complex scenarios

## Confidence

- **High confidence:** The mathematical framework for data-driven reachability using zonotopes is sound and well-established in control theory literature. The gradient-based adjustment mechanism follows established optimization principles.
- **Medium confidence:** The safety guarantees hold mathematically when all assumptions (Lipschitz continuity, sufficient data coverage, static obstacles) are satisfied. However, the practical robustness to assumption violations requires empirical validation.
- **Low confidence:** The LLM's planning quality and the system's real-world performance in highly dynamic environments remain untested. The computational overhead of safety filtering may significantly impact real-time performance in complex scenarios.

## Next Checks

1. **Lipschitz Coverage Validation:** Systematically test the framework with increasing amounts of offline training data (50, 200, 600, 1200 trajectories) and measure how the covering radius δ and Lipschitz estimation affect conservatism and computational time.
2. **Dynamic Obstacle Stress Test:** Replace static obstacles with moving obstacles of varying speeds (0.1m/s to 0.5m/s) and measure collision rates. Verify whether the multi-step safety guarantee still holds when obstacles violate the static assumption.
3. **Real-World Braking Validation:** Test the braking failsafe on actual TurtleBot3 hardware with different surfaces (carpet, tile, wood) to measure actual stopping distances versus the assumed n_brake, quantifying the safety margin degradation.