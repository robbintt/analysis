---
ver: rpa2
title: 'Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents'
arxiv_id: '2506.04649'
source_url: https://arxiv.org/abs/2506.04649
tags:
- constraints
- constraint
- planning
- budget
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Flex-TravelPlanner, a novel benchmark for
  evaluating language models'' planning capabilities in dynamic, multi-turn scenarios
  with prioritized constraints. The benchmark builds on the TravelPlanner dataset
  and introduces two evaluation settings: sequential constraint introduction across
  multiple turns and scenarios with explicitly prioritized competing constraints.'
---

# Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents

## Quick Facts
- arXiv ID: 2506.04649
- Source URL: https://arxiv.org/abs/2506.04649
- Reference count: 4
- Primary result: Introduces Flex-TravelPlanner benchmark for evaluating LLMs on dynamic multi-turn planning with prioritized constraints

## Executive Summary
Flex-TravelPlanner introduces a novel benchmark for evaluating language models' planning capabilities in dynamic, multi-turn scenarios with prioritized constraints. The benchmark builds on the TravelPlanner dataset and introduces two evaluation settings: sequential constraint introduction across multiple turns and scenarios with explicitly prioritized competing constraints. The study reveals that models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints, and that single-turn planning performance poorly predicts multi-turn adaptation capability.

## Method Summary
Flex-TravelPlanner evaluates LLMs on flexible planning across multi-turn scenarios using zero-shot evaluation on TravelPlanner dataset queries. The benchmark tests constraint-adaptive scenarios (120 queries) with sequential constraint introduction and priority-aware scenarios (134 queries) with competing constraints. Models generate JSON-formatted travel plans and are evaluated using Constraint Pass Rate, which measures the ratio of satisfied constraints to total constraints across all generated plans. The evaluation spans 1-turn (all-at-once), 2-turn, and 3-turn scenarios with varying constraint introduction orders.

## Key Results
- Sequential constraint introduction order significantly affects performance, with higher success rates when budget constraints are introduced after local constraints
- Models exhibit poor constraint prioritization, violating budget constraints to accommodate preferences in 59% (GPT-4o) and 61.9% (Llama) of cases
- Single-turn planning performance poorly predicts multi-turn adaptation capability across both tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential constraint introduction can improve planning performance compared to all-at-once presentation
- Mechanism: Introducing local constraints before global constraints allows models to establish concrete selections first, then validate against aggregate limits
- Core assumption: Models process constraints incrementally and may lack parallel constraint reconciliation capabilities
- Evidence anchors:
  - "constraint introduction order significantly affects performance, with both GPT-4o and Llama 3.1 70B achieving higher pass rates when budget constraints are introduced after local constraints"
  - "Both models achieve higher pass rates when budget constraint (global) are introduced later – LLaMA improves from 0.52 to 0.67, and GPT-4o from 0.47 to 0.63 in 2-turn scenarios"
  - ATLAS paper confirms LLMs struggle with "constraints that are explicit, implicit" in travel planning

### Mechanism 2
- Claim: Models exhibit recency bias when prioritizing constraints, favoring newly introduced preferences over established higher-priority constraints
- Mechanism: Without explicit priority enforcement, models weight recent context more heavily during plan revision
- Core assumption: Attention mechanisms in transformer models naturally emphasize recent tokens
- Evidence anchors:
  - "models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints"
  - "both models frequently violate budget constraints to accommodate preferences – 59% for GPT and 61.9% for LLaMA"
  - R-ConstraintBench addresses "tight resource, timing, and operational constraints" but doesn't examine recency effects

### Mechanism 3
- Claim: Single-turn planning performance does not predict multi-turn adaptation capability
- Mechanism: Multi-turn planning requires maintaining coherent world state across turns and selective revision without cascading failures
- Core assumption: State tracking and selective revision involve different computational patterns than single-shot planning
- Evidence anchors:
  - "models' performance on single-turn tasks poorly predicts their ability to adapt plans across multiple turns"
  - "LLaMA shows improved performance when constraints are introduced sequentially in multi-turn settings rather than all at once"
  - MT-Eval focuses on "conversational abilities" rather than planning-specific multi-turn evaluation

## Foundational Learning

- Concept: **Global vs. Local Constraints**
  - Why needed here: Understanding constraint scope is essential for interpreting why introduction order affects performance
  - Quick check question: Given a travel itinerary, which constraints require checking all days versus a single booking?

- Concept: **Constraint Pass Rate Metric**
  - Why needed here: This is the evaluation metric used throughout the paper
  - Quick check question: If a model generates 10 plans with 5 constraints each and satisfies 42 constraints total, what is the constraint pass rate?

- Concept: **Hard vs. Soft Constraints with Priority Hierarchies**
  - Why needed here: The priority-aware experiments deliberately create conflicts between soft preferences and hard constraints
  - Quick check question: A user has a $1500 budget (hard constraint) but wants fine dining (soft preference). If fine dining would exceed budget, which should the model preserve?

## Architecture Onboarding

- Component map: TravelPlanner dataset -> Constraint taxonomy -> Evaluation engine -> Turn manager
- Critical path: Query construction → Multi-turn constraint introduction → Plan generation (JSON) → Constraint satisfaction verification → Pass rate aggregation
- Design tradeoffs: Zero-shot evaluation vs. few-shot/fine-tuned approaches; testing only constraint addition vs. revision; JSON output format enables automated evaluation but may constrain natural expression
- Failure signatures: Budget True→False transitions when local constraints added; Pref∧Global failures (violating budget for preferences); Global constraint pass rates dropping when introduced first
- First 3 experiments:
  1. Reproduce baseline: Run GPT-4o on 1-turn scenarios, verify ~0.57 average pass rate
  2. Order sensitivity test: Compare +G→+L vs. +L→+G introduction sequences on 20-query subset
  3. Priority enforcement intervention: Add explicit priority instructions to prompts and measure reduction in budget violations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit constraint hierarchy training or prompting mechanisms improve models' ability to prioritize hard constraints over soft preferences?
- Basis in paper: The paper concludes that "both GPT-4o and LLaMA exhibit fundamental weaknesses in maintaining constraint hierarchies" and suggests this as a "critical challenge" for future research
- Why unresolved: The study only evaluates zero-shot performance without exploring interventions like fine-tuning on prioritized constraints
- What evidence would resolve it: Experiments comparing zero-shot performance against models fine-tuned on hierarchical constraint datasets

### Open Question 2
- Question: How do models perform when constraints are revised rather than only added during multi-turn planning?
- Basis in paper: The authors state: "To ensure that all plans are solvable, we only test constraint addition, and not revision for this study"
- Why unresolved: Real-world planning involves both adding and modifying constraints, yet the benchmark excludes revision scenarios
- What evidence would resolve it: Extending Flex-TravelPlanner to include constraint revision scenarios and evaluating whether models can correctly update plans when existing constraints change

### Open Question 3
- Question: Would chain-of-thought reasoning or intermediate planning steps improve multi-turn adaptation performance?
- Basis in paper: The paper notes "we do not explicitly prevent models from using chain-of-thought reasoning before planning" but does not systematically test whether explicit reasoning improves outcomes
- Why unresolved: The disconnect between single-turn and multi-turn performance may stem from models' inability to explicitly reason about constraint interactions across turns
- What evidence would resolve it: Comparing zero-shot performance against chain-of-thought prompted versions, measuring whether explicit reasoning about constraint relationships improves adaptation success rates

## Limitations

- Reproducibility constraints: Missing generation parameters, conversation history formatting, and complete prompt templates
- Dataset scope: Limited to 120 validation queries for constraint-adaptive scenarios and 134 for priority-aware evaluation
- Zero-shot evaluation limitation: Cannot assess whether fine-tuning or few-shot prompting could mitigate observed performance issues
- Output format constraints: JSON plan format may artificially constrain model reasoning patterns

## Confidence

**High confidence**:
- Sequential constraint introduction affects performance
- Single-turn performance poorly predicts multi-turn adaptation capability
- Budget constraint violations when accommodating preferences

**Medium confidence**:
- Recency bias as primary mechanism for constraint prioritization failures
- Cognitive load explanation for sequential advantage

**Low confidence**:
- Generalization to non-travel planning domains
- Specific numerical thresholds for performance degradation

## Next Checks

1. Implement and test the complete multi-turn prompt template with controlled conversation history formatting to verify reported performance patterns can be reproduced

2. Add explicit priority instructions ("Budget is your highest priority constraint") to prompts and measure whether this reduces budget violations during preference conflicts from 59-61.9% to acceptable levels

3. Apply Flex-TravelPlanner evaluation methodology to a non-travel domain (e.g., academic scheduling) with similar global/local constraint structures to assess whether observed phenomena generalize beyond travel planning context