---
ver: rpa2
title: Gene Incremental Learning for Single-Cell Transcriptomics
arxiv_id: '2511.13762'
source_url: https://arxiv.org/abs/2511.13762
tags:
- learning
- genes
- gene
- stage
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gene Incremental Learning (GIL), a novel
  framework for single-cell transcriptomics to address the challenge of continuously
  learning new genes as biological datasets expand. Unlike traditional class-incremental
  learning, GIL adapts to the unique nature of transcriptomic data by maintaining
  base genes across stages and partitioning specific genes into distinct stages.
---

# Gene Incremental Learning for Single-Cell Transcriptomics

## Quick Facts
- arXiv ID: 2511.13762
- Source URL: https://arxiv.org/abs/2511.13762
- Authors: Jiaxin Qi; Yan Cui; Jianqiang Huang; Gaogang Xie
- Reference count: 14
- Primary result: GIL adapts CIL methods to single-cell transcriptomics by maintaining base genes across stages, achieving significant forgetting reduction through gene replay and distillation

## Executive Summary
This paper introduces Gene Incremental Learning (GIL), a novel framework for single-cell transcriptomics that addresses the challenge of continuously learning new genes as biological datasets expand. Unlike traditional class-incremental learning, GIL maintains base genes across all stages while partitioning specific genes into distinct stages. The framework includes two key evaluation methods: gene-wise regression to measure forgetting of specific genes and gene-based classification using downstream datasets to assess practical impact. To mitigate gene forgetting, the authors adapt two classic incremental learning strategies—gene replay and gene distillation—and validate their effectiveness through extensive experiments.

## Method Summary
GIL tackles the unique challenge of incrementally learning genes in single-cell transcriptomics by maintaining a set of base genes across all stages while partitioning specific genes into distinct stages. The framework uses a transformer-based masked value prediction model where genes are bound to expression values. During training, base genes are always present while stage-specific genes change. Two forgetting mitigation strategies are implemented: gene replay stores a subset of samples from previous stages for rehearsal, and gene distillation constrains current predictions to match previous model outputs on base genes. Evaluation occurs through gene-wise regression (masked value prediction MSE) and gene-based classification (downstream accuracy using frozen features).

## Key Results
- Gene replay with 10^4 samples reduces forgetting (Δ) from 0.253 to 0.018 in regression loss, outperforming baseline significantly
- Gene distillation achieves Δ=0.154 vs baseline Δ=0.253, but shows divergent results between regression and classification metrics
- Both methods reduce gene forgetting, with gene replay performing particularly well across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Base Gene Persistence for Semantic Validity
Maintaining base genes across all stages preserves sample semantic meaning while enabling incremental gene learning. Unlike CIL where classes are mutually exclusive, GIL keeps a subset of genes present in every stage, ensuring that even when specific genes change, samples remain biologically interpretable. Genes are bound to values via L1,φ, so learning expression values directly learns gene representations. Core assumption: a core set of genes is sufficient to provide biological context for transcriptomic samples regardless of which specific genes are being learned.

### Mechanism 2: Gene Replay for Memory Consolidation
Retaining a subset of samples from previous stages during current training substantially reduces gene forgetting. Exemplar samples from earlier stages are rehearsed alongside current data, reinforcing previously learned gene-expression mappings through continued gradient updates on old distributions. Core assumption: a small subset of samples can sufficiently approximate gene-expression distributions from previous stages.

### Mechanism 3: Gene Distillation via Output Consistency
Constraining current model predictions to match previous model outputs on base genes preserves learned gene representations without storing data. Distillation loss penalizes deviation from previous model predictions; specific genes from current stage are excluded since previous model never learned them. Core assumption: previous model's output behavior on base genes captures essential knowledge about learned specific genes through their interaction patterns.

## Foundational Learning

- **Concept: Class Incremental Learning (CIL) fundamentals**
  - Why needed here: GIL is explicitly designed as a parallel to CIL; understanding CIL's class-exclusion mechanism clarifies why genes need base gene handling differently
  - Quick check question: Why does standard CIL sample partitioning (classes mutually exclusive across stages) fail for language tokens but succeed for genes?

- **Concept: Masked Value Prediction (self-supervised gene learning)**
  - Why needed here: The core training objective uses masked gene expression prediction—equivalent to learning genes themselves since genes are bound to values
  - Quick check question: Explain how Eq. (1) and Eq. (4)-(6) demonstrate that learning expression values is learning genes

- **Concept: Transformer architecture for unordered gene sequences**
  - Why needed here: Single-cell data lacks positional order among genes, enabling straightforward gene partitioning that's impossible in NLP
  - Quick check question: Why does the absence of relative gene ordering enable the GIL framework, and how does this differ from language token handling?

## Architecture Onboarding

- **Component map:**
  - Gene Embedding Layer (E_φ): Maps gene tokens to dense vectors
  - Value Encoding Layer (L1,φ ∈ R^1×d): Encodes scalar expression values into embedding space
  - Transformer Backbone (M_φ): 6 layers, 8 attention heads, 256 hidden dim (scGPT architecture)
  - Value Prediction Layer (L2,φ ∈ R^d×1): Decodes features to predicted expression values
  - Classification Head (L): Trainable linear layer for downstream evaluation only (frozen backbone)

- **Critical path:**
  1. Input (x, v) → Concatenate gene embeddings and value encodings → Eq. (4)
  2. Combined embeddings → Transformer backbone → Eq. (5)
  3. Output features → Linear projection → Predicted masked values → Eq. (6)
  4. For distillation: Current predictions ||v̂_i - v̂*_i,sk-1|| compared against frozen previous model

- **Design tradeoffs:**
  - Replay quantity vs storage: Δ improves from 0.121 (50 samples) to 0.018 (10^4 samples), but approaches oracle behavior
  - Distillation coefficient λ: Higher preserves old knowledge but impairs new learning—Lupus stage-2 loss degrades from 0.134→0.143 as λ increases 0.5→10.0
  - Base gene proportion: Too few risks semantic invalidity; too many reduces incremental learning scope
  - Evaluation choice: Gene-wise regression shows distillation improving (Δ=0.212 vs 0.279), but gene-based classification shows degradation (-2.473% vs -1.816%)

- **Failure signatures:**
  - Divergent evaluation results: Distillation improves regression but worsens classification (Table 1 vs Table 3)—indicates feature degradation despite value prediction accuracy
  - Progressive forgetting cascade: Baseline shows increasing regression loss across stages (Table 2: ICol 0.163→0.452→0.498)
  - New gene learning impairment: When λ too high, current-stage genes show increased loss

- **First 3 experiments:**
  1. Reproduce 2-stage baseline (Norman-Lupus) to validate framework—expect ~0.25 regression Δ and ~2% classification drop
  2. Implement gene replay with 1000 samples—target Δ reduction to ~0.04; verify both regression and classification improve consistently
  3. Compare distillation at λ=1.0 vs λ=5.0—observe regression-classification tradeoff; if classification degrades while regression improves, confirms feature degradation hypothesis from Q3 analysis

## Open Questions the Paper Calls Out

- **Can algorithms specifically designed for Gene Incremental Learning outperform the adapted Class Incremental Learning baselines (replay and distillation)?**
  - The authors state in the Conclusion, "For future work, we will try to design specific GIL algorithms."
  - Evidence needed: A novel GIL-specific method demonstrating superior performance compared to the provided baselines.

- **Can the GIL framework be effectively extended to natural language processing given the "holistic nature" of language tokens?**
  - The Conclusion notes the aim to "extend the framework into other token-learning fields."
  - Evidence needed: A modified framework that successfully handles token incrementality in text without requiring the strict data partitioning possible in single-cell transcriptomics.

- **What mechanisms can resolve the discrepancy where gene distillation preserves regression loss but degrades downstream classification performance?**
  - The paper observes that gene distillation prevents forgetting in regression but degrades features for classification, suggesting the method preserves data statistics but not semantic utility.
  - Evidence needed: A method that minimizes the performance gap between the gene-wise regression and gene-based classification metrics simultaneously.

## Limitations
- The specific criteria for selecting base genes (number, proportion, selection method) remain unspecified, making it difficult to assess whether the chosen partition genuinely preserves sample semantics.
- Gene distillation shows divergent behavior—improving regression loss but degrading classification accuracy, suggesting a trade-off between value prediction accuracy and feature quality.
- The method's reliance on a core gene set raises questions about its applicability to rare cell types or specialized biological processes where crucial marker genes might be excluded from the base set.

## Confidence
- **High confidence:** Base gene persistence mechanism for semantic validity; gene replay effectiveness for forgetting mitigation
- **Medium confidence:** Gene distillation benefits (divergent results suggest context-dependent effectiveness)
- **Low confidence:** Optimal base gene proportion; distillation's mechanism and limitations; gene partitioning methodology

## Next Checks
1. Reproduce 2-stage baseline (Norman-Lupus) to validate framework—expect ~0.25 regression Δ and ~2% classification drop, confirming baseline forgetting patterns.

2. Implement gene replay with 1000 samples—target Δ reduction to ~0.04; verify both regression and classification improve consistently, confirming replay's dual benefits.

3. Compare distillation at λ=1.0 vs λ=5.0—observe regression-classification tradeoff; if classification degrades while regression improves, this confirms feature degradation hypothesis and explains distillation's inconsistent evaluation results.