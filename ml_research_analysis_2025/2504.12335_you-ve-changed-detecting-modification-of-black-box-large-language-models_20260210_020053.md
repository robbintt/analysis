---
ver: rpa2
title: 'You''ve Changed: Detecting Modification of Black-Box Large Language Models'
arxiv_id: '2504.12335'
source_url: https://arxiv.org/abs/2504.12335
tags:
- text
- generated
- features
- llms
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for detecting changes in black-box
  Large Language Models (LLMs) provided via API by comparing distributions of linguistic
  and psycholinguistic features from generated text. The approach uses the Kolmogorov-Smirnov
  test to compare feature distributions from two samples of LLM-generated text, enabling
  developers to identify when an LLM has changed.
---

# You've Changed: Detecting Modification of Black-Box Large Language Models

## Quick Facts
- arXiv ID: 2504.12335
- Source URL: https://arxiv.org/abs/2504.12335
- Reference count: 40
- Simple linguistic features and Kolmogorov-Smirnov tests can reliably detect LLM changes as small as 3% in mixture scenarios

## Executive Summary
This paper presents a method for detecting changes in black-box Large Language Models (LLMs) provided via API by comparing distributions of linguistic and psycholinguistic features from generated text. The approach uses the Kolmogorov-Smirnov test to compare feature distributions from two samples of LLM-generated text, enabling developers to identify when an LLM has changed. Experiments with five OpenAI completion models and Meta's Llama 3 70B chat model demonstrate that simple text features coupled with statistical testing can reliably distinguish between different LLMs.

## Method Summary
The method generates synthetic text corpora using prompts for reviews, news stories, and tweets, then extracts linguistic features including LIWC metrics (Analytic, Clout, Authentic, Tone, Sixltr, WC, verb), lexical richness measures (MTLD, Maas), sentiment scores (VADER compound), readability (Flesch), and GPT-2 perplexity. Two-sample Kolmogorov-Smirnov tests compare feature distributions between baseline and current LLM outputs, with Bonferroni correction applied for multiple comparisons. The approach can detect model changes through mixture experiments where small percentages of text from one model are replaced with text from another.

## Key Results
- K-S test p-value < 5% successfully detects LLM changes, including 3% mixture scenarios
- Surprisal (-log2(p)) values increase with larger sample sizes, showing improved detection power
- Simple features like word count (WC) and six-letter word percentage (Sixltr) reliably distinguish between different models

## Why This Works (Mechanism)
The method exploits the fact that different LLMs produce text with distinct statistical properties, even when given the same prompts. By comparing distributions of multiple linguistic features using non-parametric statistical tests, the approach can detect when these underlying distributions have shifted, indicating an LLM change. The combination of multiple features provides robustness against individual feature noise.

## Foundational Learning
- Kolmogorov-Smirnov test: Non-parametric test comparing two distributions; needed to detect statistical differences without distributional assumptions
- LIWC features: Linguistic Inquiry and Word Count metrics; provide psycholinguistic insights into text generation patterns
- Perplexity: Measure of how well a language model predicts text; used here to capture model-specific generation characteristics
- Bonferroni correction: Adjusts significance thresholds for multiple comparisons; prevents false positives when testing multiple features
- Fisher's method: Combines p-values from multiple tests; improves overall detection power
- Surprisal: Information-theoretic measure (-log2(p)); visualizes confidence in detection results

## Architecture Onboarding
**Component map:** Prompt generation -> Text synthesis -> Feature extraction -> Statistical testing -> Change detection
**Critical path:** Synthetic text generation → Feature extraction → K-S test comparison → Bonferroni/Fisher aggregation
**Design tradeoffs:** Simple features vs. computational cost; statistical power vs. false positive rate
**Failure signatures:** False negatives with insufficient sample size; feature mismatch causing poor discrimination
**First experiments:** 1) Generate 100 synthetic reviews per topic and verify feature extraction works; 2) Run K-S tests on identical model outputs to establish baseline false positive rate; 3) Test mixture detection with 5% substitution to validate sensitivity

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can this detection methodology be effectively generalized to chat-based language models and changes in hyper-parameters?
- Basis in paper: [explicit] The authors state in the conclusion, "we plan to increase the scope of our investigations to focus on chat-based language models, hyper-parameter changes..."
- Why unresolved: The current study focuses primarily on completion-style models (OpenAI GPT-3 variants) and specific parameter settings; chat models have different structural behaviors
- What evidence would resolve it: Successful application of the K-S test on linguistic features to detect model swaps or parameter drift in instruction-tuned chat APIs

### Open Question 2
- Question: How does the specificity of the generation task impact the reliability of feature-based change detection?
- Basis in paper: [inferred] The authors note a threat to validity is that "generated text... is not task-specific," implying that performance on generic prompts may not translate to specialized applications
- Why unresolved: It is unclear if the discriminative power of simple features (like word count) remains robust when the text domain is constrained (e.g., code generation or medical advice)
- What evidence would resolve it: Experiments comparing detection sensitivity on generic synthetic corpora versus domain-specific task outputs

### Open Question 3
- Question: Can the sensitivity for detecting LLM mixture changes be improved to reliably detect differences smaller than 3%?
- Basis in paper: [inferred] Results in Table 10 show the method fails to detect changes at 1-2% mixture differences, and the authors note "there is a limit to its sensitivity"
- Why unresolved: While the current method detects 3% changes, detecting smaller silent updates may require more sensitive statistical tests or compound features
- What evidence would resolve it: A modified approach utilizing different features or tests that successfully distinguishes mixture differences of 1% or less

## Limitations
- Prompt formulation sensitivity may limit generalizability across different LLM applications
- Synthetic text generation may not capture real-world LLM change scenarios
- LIWC dependency and unspecified version/configuration
- Incomplete implementation details for perplexity calculation
- Limited evaluation of false positive rates in dynamic deployment scenarios

## Confidence
- **High**: The statistical methodology (K-S tests, Bonferroni correction) is sound and well-implemented
- **Medium**: Feature selection and extraction methods are appropriate but implementation details are incomplete
- **Low**: Generalization claims to real-world LLM monitoring scenarios are not fully supported

## Next Checks
1. Implement the complete feature extraction pipeline using open-source alternatives to LIWC (e.g., lexnlp or custom dictionaries) and verify that the same discrimination power is achieved
2. Conduct experiments with real-world LLM API monitoring data to validate performance outside synthetic text generation scenarios
3. Test the method's false positive rate by monitoring LLM outputs that haven't changed but vary due to different prompt phrasings or user inputs