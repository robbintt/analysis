---
ver: rpa2
title: 'DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval'
arxiv_id: '2502.07219'
source_url: https://arxiv.org/abs/2502.07219
tags:
- retrieval
- document
- identifier
- generative
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing generative retrieval
  methods, which only learn the relationship between queries and document identifiers,
  lacking direct interaction between queries and documents. To overcome this, the
  authors propose DOGR, a novel framework that leverages document-oriented contrastive
  learning to improve generative retrieval.
---

# DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval

## Quick Facts
- arXiv ID: 2502.07219
- Source URL: https://arxiv.org/abs/2502.07219
- Reference count: 22
- Primary result: Two-stage generative retrieval framework achieving 70.2% R@1 on NQ320k and 22.5% MRR@10 on MS MARCO

## Executive Summary
This paper addresses the fundamental limitation of existing generative retrieval methods that only learn query-document identifier mappings without direct query-document semantic interactions. The authors propose DOGR, a two-stage framework that first learns identifier generation through cross-entropy loss, then fine-tunes with document-oriented contrastive learning to directly model query-document relevance. Using prefix-oriented and retrieval-augmented negative sampling, DOGR achieves significant improvements over state-of-the-art generative retrieval methods on both NQ320k and MS MARCO datasets.

## Method Summary
DOGR uses a T5-base encoder-decoder architecture with a two-stage training strategy. Stage 1 learns query-to-document identifier and document-to-document identifier mappings using standard sequence-to-sequence cross-entropy loss. Stage 2 fine-tunes the encoder with contrastive objectives (prefix-oriented and retrieval-augmented) while maintaining generation capability through auxiliary loss. The method employs keyword-based document identifiers constructed via KeyBERT, with length optimized per dataset (8 tokens for NQ320k, 12 for MS MARCO). Inference uses constrained beam search with a fusion scoring mechanism combining generation probability and semantic similarity.

## Key Results
- Achieves 70.2% Recall@1 and 84.6% Recall@10 on NQ320k, outperforming previous generative methods
- Reaches 22.5% MRR@10 on MS MARCO, demonstrating scalability to large corpora
- Ablation studies show prefix-oriented negatives contribute 1.7% R@1 and generation probability contributes 9.8% R@1
- Identifier length optimization critical: l=8 optimal for NQ320k (conflict rate 12.5%), l=12 for MS MARCO

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Learning Decouples Identifier Generation from Semantic Ranking
Separating identifier learning from document-level ranking allows each stage to optimize different objectives without interference. Stage 1 establishes the identifier mapping through cross-entropy loss, while Stage 2 fine-tunes the encoder with contrastive objectives to align query and document representations. This decoupling prevents the contrastive loss from destroying generation capability.

### Mechanism 2: Prefix-Oriented Negative Sampling Creates Fine-Grained Discrimination
Sampling negatives sharing the same identifier prefix forces the model to learn distinctions among semantically similar documents. Since identifiers are keyword-based, prefix-sharing documents are semantically proximate but distinct. This creates "hard negatives" that improve the model's ability to discriminate between similar documents.

### Mechanism 3: Fusion Scoring Combines Complementary Relevance Signals
Multiplying generation probability with semantic similarity score yields better ranking than either signal alone. The generation probability captures the model's learned mapping from queries to identifiers, while semantic similarity captures direct query-document alignment through encoder representations. Their product requires both signals to agree for high ranking.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE-style loss)**
  - Why needed here: The document ranking stage relies entirely on contrastive objectives to learn query-document alignment. Understanding why pulling positive pairs together and pushing negative pairs apart works is essential.
  - Quick check question: Can you explain why temperature τ affects hard negative discrimination strength?

- **Concept: Encoder-Decoder Architectures (seq2seq with attention)**
  - Why needed here: DOGR uses T5-base backbone; the encoder produces representations hq and hd, while the decoder generates identifiers. Understanding where representations come from matters for debugging.
  - Quick check question: In T5, which hidden states are used for the semantic score calculation—encoder output, decoder output, or both?

- **Concept: Constrained Beam Search**
  - Why needed here: Inference requires generating only valid document identifiers from a predefined vocabulary. Unconstrained generation would produce invalid docids.
  - Quick check question: How does a prefix tree (trie) constrain beam search at each decoding step?

## Architecture Onboarding

- **Component map:**
  - KeyBERT keyword extraction → Document identifier construction → T5-base encoder-decoder → Stage 1 cross-entropy loss → Stage 2 contrastive loss + auxiliary loss → Inference with constrained beam search and fusion scoring

- **Critical path:**
  1. Identifier quality (keyword extraction) directly affects both stages
  2. Stage 1 must converge sufficiently before Stage 2 (Stage 2 uses Stage 1 model for retrieval-augmented negative sampling)
  3. Auxiliary generation loss (λg=0.1) must be tuned to prevent catastrophic forgetting

- **Design tradeoffs:**
  - Identifier length: Shorter = more conflicts, longer = harder to memorize, slower inference. Paper found l=8 optimal for NQ320k, l=12 for MS MARCO.
  - Negative sampling ratio: Paper uses in-batch for prefix-oriented, k=4 for retrieval-augmented. More negatives increases compute but may improve discrimination.
  - Fusion vs. sequential ranking: Product fusion outperformed GLEN's "generation-first, semantic-only-for-ties" approach by 0.4% R@1.

- **Failure signatures:**
  - High conflict rate (>20%) with poor R@1: Identifier too short; increase length
  - Stage 2 degrades generation accuracy: λg too low or contrastive loss dominating; increase λg or reduce τ
  - Retrieval-augmented negatives aren't useful: Stage 1 model too weak; train Stage 1 longer before Stage 2
  - Semantic scores near-zero or uniform: Encoder representations collapsed; check contrastive loss convergence

- **First 3 experiments:**
  1. Identifier length sweep: Train with l∈{4,6,8,10,12} on validation split; plot conflict rate vs. R@1 to find corpus-appropriate length
  2. Ablation of negative sampling types: Run Stage 2 with (a) prefix-only, (b) retrieval-augmented-only, (c) both; compare R@1 to quantify each contribution
  3. Fusion strategy comparison: Compare product fusion vs. weighted sum vs. generation-only-with-semantic-tiebreaking on same model checkpoint; verify product fusion is optimal for your dataset

## Open Questions the Paper Calls Out

- **Question 1:** How can information from documents be integrated in an end-to-end manner to enhance retrieval capabilities in large-scale corpora?
  - Basis: The authors state in the conclusion they will explore end-to-end integration of document information
  - Why unresolved: Current two-stage strategy separates identifier generation from semantic ranking
  - Evidence needed: Single-stage training objective that jointly optimizes both without instability

- **Question 2:** Can generative retrieval methods effectively close the performance gap with dense retrieval baselines on large-scale datasets?
  - Basis: DOGR achieves 22.5 MRR@10 on MS MARCO vs. GTR-base at 34.8
  - Why unresolved: Improvements from DOGR may not yet be sufficient to surpass dense retrieval
  - Evidence needed: Evaluations on datasets exceeding millions of passages where generative retrieval matches/exceeds dense retrieval

- **Question 3:** How does the trade-off between identifier length, semantic conflict, and model robustness affect performance across different domains?
  - Basis: Analysis shows increasing length reduces conflicts but may decrease robustness
  - Why unresolved: Only explored on NQ320k; unclear if trade-off holds for other languages/domains
  - Evidence needed: Comprehensive ablation study on diverse datasets analyzing correlation between identifier entropy, collision rates, and accuracy

## Limitations

- Identifier construction variability: The specific impact of keyword extraction quality and token length on downstream performance is not fully characterized across different corpora
- Negative sampling design space: The paper doesn't explore other strategies (hard negatives from dense retrievers, adversarial negatives) or fully characterize relative contributions
- Fusion mechanism limitations: The product fusion is simple but may not be optimal; learned weighting schemes or alternative fusion strategies could yield better performance

## Confidence

- **High confidence**: Two-stage learning framework is well-supported by ablation studies showing degradation when components are removed
- **Medium confidence**: Prefix-oriented negative sampling mechanism is theoretically sound but has limited empirical validation
- **Medium confidence**: Fusion scoring strategy works well in practice but optimal approach not established

## Next Checks

1. **Identifier quality sensitivity analysis**: Systematically vary keyword extraction parameters and document identifier lengths across multiple datasets to establish guidelines for corpus-specific optimization

2. **Negative sampling ablation with controlled difficulty**: Create synthetic negative sets with controlled semantic similarity to positives, then measure DOGR's ability to distinguish varying difficulty levels compared to alternative sampling strategies

3. **Fusion strategy comparison**: Implement and evaluate alternative fusion mechanisms (learned linear combination, attention-based fusion, hierarchical fusion) on the same model checkpoints to determine if product fusion is truly optimal or just sufficient