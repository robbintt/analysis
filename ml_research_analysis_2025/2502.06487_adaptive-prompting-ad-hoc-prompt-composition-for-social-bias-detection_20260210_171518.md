---
ver: rpa2
title: 'Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection'
arxiv_id: '2502.06487'
source_url: https://arxiv.org/abs/2502.06487
tags:
- in-cont
- in-context
- composition
- reasoning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces adaptive prompting, a method to automatically
  select the best prompt composition for a given input and language model to improve
  social bias detection. It trains an encoder to predict optimal combinations of five
  prompting techniques (personas, definitions, demonstrations, directional stimuli,
  reasoning steps) from a set of possible compositions, then applies the selected
  prompt to classify bias.
---

# Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection

## Quick Facts
- **arXiv ID**: 2502.06487
- **Source URL**: https://arxiv.org/abs/2502.06487
- **Reference count**: 40
- **Primary result**: Adaptive prompting achieves macro F1 up to 0.853 for social bias detection across three datasets and three LLMs

## Executive Summary
This work introduces adaptive prompting, a method to automatically select the best prompt composition for a given input and language model to improve social bias detection. It trains an encoder to predict optimal combinations of five prompting techniques (personas, definitions, demonstrations, directional stimuli, reasoning steps) from a set of possible compositions, then applies the selected prompt to classify bias. Evaluated across three datasets and three LLMs (Mistral, Command-R, Llama 3), adaptive prompting consistently outperforms individual techniques and fixed ensembles. Additional experiments on sentiment analysis, natural language inference, and question answering demonstrate generalizability. The approach reduces trial-and-error in prompt engineering and highlights the value of tailored prompt compositions for reliable bias detection.

## Method Summary
Adaptive prompting works by first training an encoder model to predict which combination of five prompting techniques will work best for a given input and LLM. The encoder learns from a small validation set (200 examples) where each example is labeled with the optimal prompt composition based on performance. During inference, the encoder takes the input text and selects the best prompt from a predefined set of compositions, which is then used to classify whether the text contains social bias. The five prompting techniques include personas, definitions, demonstrations, directional stimuli, and reasoning steps, which can be combined in various ways. Shapley value analysis is used to understand which techniques contribute most to performance and how they interact.

## Key Results
- Adaptive prompting achieves macro F1 scores up to 0.853, outperforming individual prompting techniques and fixed ensembles
- The method generalizes to other NLP tasks including sentiment analysis, natural language inference, and question answering
- Shapley analysis reveals that optimal prompt compositions are task- and model-specific, with significant technique interactions

## Why This Works (Mechanism)
Adaptive prompting works because social bias detection is sensitive to how questions are posed to language models. Different prompting techniques emphasize different aspects of the input, and the optimal combination depends on both the specific text and the underlying LLM's characteristics. By learning to predict which prompt composition will work best for each case, the method effectively tailors the questioning strategy to maximize detection accuracy. The encoder learns these patterns from a small validation set, capturing task-specific and model-specific sensitivities. The Shapley analysis further reveals that prompt techniques interact in complex ways, meaning that the whole is often greater than the sum of its parts when combining multiple techniques.

## Foundational Learning

**Prompt Engineering Basics** - Understanding different prompting techniques (personas, definitions, demonstrations, directional stimuli, reasoning steps) is essential because each shapes the LLM's response differently. Quick check: Can you list the five techniques and describe their purpose?

**Shapley Value Analysis** - This game theory concept quantifies each technique's contribution to overall performance and reveals interactions between techniques. Quick check: Can you explain why Shapley values are better than simple importance scores for understanding technique interactions?

**Encoder-Decoder Architecture** - The prompt selection encoder is a separate model that learns to map inputs to optimal prompt compositions. Quick check: Can you describe how the encoder's training differs from typical classification tasks?

**Bias Detection as Classification** - Social bias detection is framed as a binary classification problem, requiring careful consideration of evaluation metrics like macro F1. Quick check: Can you explain why macro F1 is more appropriate than accuracy for this task?

## Architecture Onboarding

**Component Map**: Input text -> Encoder -> Selected prompt composition -> LLM -> Bias classification output

**Critical Path**: The inference pipeline consists of: (1) input text fed to encoder, (2) encoder predicts optimal prompt composition, (3) selected prompt applied to LLM, (4) LLM outputs classification. The encoder inference is the only additional overhead beyond standard prompting.

**Design Tradeoffs**: The method trades increased inference time (due to encoder) for improved accuracy. Using 200 validation examples balances training data requirements with performance. The five prompting techniques were chosen to cover diverse strategies while keeping the search space manageable.

**Failure Signatures**: Performance degradation may occur when: (1) the encoder encounters bias types not represented in training, (2) LLM behavior changes significantly (model updates), (3) input domain shifts dramatically from training data, or (4) the optimal prompt composition is not in the predefined set.

**3 First Experiments**: 
1. Run adaptive prompting on a held-out validation set to verify it selects different prompts for different inputs
2. Perform ablation by removing each prompting technique to see individual impact on performance
3. Test the encoder on a different LLM than it was trained on to assess cross-model generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on three specific LLMs and three datasets, potentially missing broader model behaviors
- Training data for the encoder consists of only 200 examples per dataset, raising generalization concerns
- Computational overhead of running an additional encoder model is not fully characterized for production deployment
- Evaluation framework focuses on binary classification, potentially oversimplifying the nuanced nature of social bias

## Confidence

**High Confidence**: The empirical finding that adaptive prompting outperforms individual prompting techniques and fixed ensembles is well-supported by the presented results (macro F1 up to 0.853).

**Medium Confidence**: The generalizability claim to other NLP tasks is supported but based on limited experimentation.

**Low Confidence**: The claim about reducing trial-and-error in prompt engineering lacks empirical measurement of engineering effort.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate the trained encoder on entirely unseen bias detection datasets to assess whether the learned prompt selection generalizes beyond the training distribution.

2. **Real-Time Performance Benchmarking**: Measure the end-to-end latency and computational overhead of the adaptive prompting pipeline compared to single-prompt approaches, including encoder inference time.

3. **Ablation Study on Training Size**: Systematically vary the number of training examples to determine the minimum viable dataset size for the encoder to learn effective prompt composition patterns.