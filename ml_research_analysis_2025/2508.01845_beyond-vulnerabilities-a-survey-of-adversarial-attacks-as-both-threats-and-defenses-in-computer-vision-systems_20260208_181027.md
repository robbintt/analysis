---
ver: rpa2
title: 'Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and
  Defenses in Computer Vision Systems'
arxiv_id: '2508.01845'
source_url: https://arxiv.org/abs/2508.01845
tags:
- adversarial
- attacks
- attack
- systems
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines adversarial attacks in computer
  vision systems, revealing their dual nature as both security threats and defensive
  tools. The research categorizes attacks into pixel-space, physically realizable,
  and latent-space methodologies, tracing their evolution from basic gradient-based
  methods to sophisticated optimization techniques.
---

# Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems

## Quick Facts
- **arXiv ID:** 2508.01845
- **Source URL:** https://arxiv.org/abs/2508.01845
- **Reference count:** 40
- **Primary result:** Comprehensive taxonomy of adversarial attacks revealing dual role as threats and defensive tools across computer vision systems

## Executive Summary
This survey provides a comprehensive examination of adversarial attacks in computer vision systems, revealing their dual nature as both security threats and defensive tools. The research categorizes attacks into pixel-space, physically realizable, and latent-space methodologies, tracing their evolution from basic gradient-based methods to sophisticated optimization techniques. The work identifies critical vulnerabilities across applications including autonomous driving, medical diagnosis, and biometric authentication while also showcasing constructive uses in privacy protection and generative model defense.

## Method Summary
This survey synthesizes existing literature on adversarial attacks without introducing new algorithms. The authors systematically review 40+ papers spanning pixel-space attacks (FGSM, PGD, MI-FGSM), physical attacks (adversarial patches, 3D textures), and latent-space attacks. The survey includes a specific demonstrative example in Figure 1 using FGSM on VGG-19 with eps=4/255 to misclassify "robin" to "mantis". The methodology involves comprehensive literature analysis, threat model classification, and technical evolution mapping across attack categories.

## Key Results
- Adversarial techniques can successfully deceive state-of-the-art computer vision models while remaining imperceptible to human observers
- Physically realizable attacks bridge digital vulnerabilities to real-world threats with attack efficacy exceeding 80% in field evaluations
- Latent-space attacks leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based optimization generates adversarial perturbations that exploit model sensitivity to small input changes.
- **Mechanism:** The attack computes ∇ₓJ(x, y) (gradient of loss w.r.t. input), then applies perturbations: x_adv = x + ϵ · sign(∇ₓJ(x, y)) for FGSM, or iteratively for PGD/MI-FGSM with momentum accumulation.
- **Core assumption:** The model is differentiable, and local gradient information meaningfully indicates vulnerability directions in input space.
- **Evidence anchors:** [abstract] traces evolution from FGSM and PGD to sophisticated optimization techniques; [section 3.1] Equations 4-6 show FGSM, PGD, and MI-FGSM update rules.

### Mechanism 2
- **Claim:** Physically realizable attacks bridge digital vulnerabilities to real-world threats by incorporating physical constraints during optimization.
- **Mechanism:** Methods like RP2 use Expectation Over Transformation (EOT) to optimize perturbations robust to viewpoint, lighting, and printing distortions.
- **Core assumption:** The transformation distribution adequately covers real-world variability; the attack can be physically manufactured.
- **Evidence anchors:** [abstract] states attacks have successfully bridged gap between digital vulnerabilities and real-world threats; [section 4.1] reports attack efficacy exceeding 80% during practical field evaluations.

### Mechanism 3
- **Claim:** Latent-space attacks exploit semantic structure in internal representations to create more transferable adversarial examples.
- **Mechanism:** LSAs manipulate latent codes z in encoder-decoder architectures, then decode to x_adv = Decoder(z_adv).
- **Core assumption:** Latent representations encode semantically meaningful features that generalize across architectures.
- **Evidence anchors:** [abstract] discusses emergence of latent-space attacks that leverage semantic structure; [section 5.3] shows perturbations in rich semantic latent spaces generate superior transferability.

## Foundational Learning

- **Concept:** Gradient descent and backpropagation
  - **Why needed here:** All gradient-based attacks require understanding how to compute ∇ₓL and how iterative optimization works.
  - **Quick check question:** Can you derive the update rule for PGD given a loss function J(x, y)?

- **Concept:** ℓₚ-norm constraints and projection
  - **Why needed here:** Adversarial perturbations are bounded by ∥δ∥ₚ ≤ ϵ; understanding projection operators is essential for constrained optimization.
  - **Quick check question:** How does projection onto an ϵ-ball differ between ℓ∞ and ℓ₂ norms?

- **Concept:** Transferability in adversarial examples
  - **Why needed here:** Black-box attacks rely on transferability—why do attacks on one model often fool another?
  - **Quick check question:** Why does momentum (MI-FGSM) improve cross-model transferability compared to vanilla I-FGSM?

## Architecture Onboarding

- **Component map:** Input x → [Perturbation Generator] → x_adv → [Target Model F] → Output
  - [Constraint Module] (∥δ∥ ≤ ϵ, imperceptibility)
  - [Optimization Engine] (gradient/optimizer-based)

- **Three attack domains:**
  1. Pixel-space: Direct δ on x
  2. Physical: δ with physical constraints + EOT
  3. Latent-space: δ_z on latent z → decode

- **Critical path:**
  1. Define threat model (white/grey/black-box, targeted/untargeted)
  2. Select attack domain (pixel/physical/latent)
  3. Choose optimization method (FGSM-family vs. generative)
  4. Apply constraints (ℓₚ-norm, imperceptibility via SSIM/PSNR/LPIPS)
  5. Evaluate: ASR, transferability, imperceptibility

- **Design tradeoffs:**
  | Tradeoff | High Attack Success | High Imperceptibility |
  |----------|--------------------|-----------------------|
  | ϵ magnitude | Larger ϵ | Smaller ϵ |
  | Iterations | More iterations | Risk of overfitting to source model |
  | Sign vs. raw gradient | Sign: faster | Raw gradient: better transferability |
  | Attack domain | Pixel-space: direct | Latent-space: more transferable |

- **Failure signatures:**
  - Low ASR on source model → check gradient flow, learning rate α
  - High ASR on source, low on target → overfitting; try MI-FGSM, variance reduction
  - Visible perturbations → reduce ϵ, use perceptual constraints (LPIPS)
  - Physical attack fails → increase EOT samples, check printing color gamut

- **First 3 experiments:**
  1. Reproduce FGSM and PGD on CIFAR-10 with ResNet-18; measure ASR vs. ϵ curve. Document ϵ threshold where perturbations become visible.
  2. Implement MI-FGSM and evaluate transferability from ResNet-50 to VGG-16 on ImageNet subset (1000 images). Compare ASR_drop = ASR_source - ASR_target.
  3. Test a basic physical patch attack: print an adversarial patch, photograph it under varying lighting, measure mAP drop on YOLOv5 for a custom object detection task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can protective adversarial perturbations be designed specifically for neural style transfer applications to safeguard artists' intellectual property?
- **Basis in paper:** Section 6.2.7 states current adversarial protection research insufficiently investigates protective techniques specifically tailored for style transfer scenarios.
- **Why unresolved:** Current research focuses on diffusion models and personalization attacks, leaving style transfer largely unaddressed despite its widespread use and unique IP threats.
- **What evidence would resolve it:** Development and benchmarking of perturbation methods that prevent style extraction while preserving image quality, tested against multiple style transfer architectures.

### Open Question 2
- **Question:** Can adversarial attacks maintain effectiveness across fundamentally different neural network architectures without architecture-specific optimization?
- **Basis in paper:** Section 7.2.1 states effectiveness often diminishes when applied to models with fundamentally different architectures.
- **Why unresolved:** Current transferability gains rely on shared architectural features; underlying common vulnerabilities across heterogeneous architectures remain unidentified.
- **What evidence would resolve it:** Demonstration of high attack success rates when transferring between architecturally distinct models (e.g., CNNs to ViTs) using a unified perturbation method.

### Open Question 3
- **Question:** How can the irrevocability paradox in biometric authentication be resolved within evolving adversarial threat landscapes?
- **Basis in paper:** Section 6.1.7 states need for innovative frameworks capable of resolving the irrevocability paradox within evolving adversarial landscapes.
- **Why unresolved:** Static revocation mechanisms cannot adapt to evolving threats, while adaptive systems may create new exploitable vulnerabilities.
- **What evidence would resolve it:** A dynamic template protection framework that maintains authentication accuracy while adapting to novel adversarial attacks without requiring biometric replacement.

### Open Question 4
- **Question:** Can effective adversarial protection against diffusion models be achieved with minimal model knowledge and reduced computational requirements?
- **Basis in paper:** Section 6.2.7 states contemporary adversarial techniques targeting diffusion frameworks typically necessitate comprehensive understanding of model internals.
- **Why unresolved:** Current methods require white-box access to UNet architectures and significant compute, limiting practical deployment for real-time content protection.
- **What evidence would resolve it:** A grey-box or black-box protection method achieving comparable perturbation effectiveness with at least 50% computational reduction and no architecture-specific parameters.

## Limitations

- The survey focuses heavily on established gradient-based and physical attacks, with less empirical depth on emerging latent-space attack methods
- Lacks systematic benchmarking across attack families on standardized datasets, making quantitative comparisons difficult
- Defensive applications section remains largely theoretical without implementation details or performance metrics

## Confidence

- **High confidence:** Basic gradient-based attacks (FGSM, PGD, MI-FGSM) and their mathematical formulations are well-established with consistent experimental validation
- **Medium confidence:** Physical realizability claims are supported by published demonstrations, though real-world robustness requires further validation
- **Medium confidence:** Latent-space attack mechanisms are described conceptually with some supporting evidence, but empirical validation across diverse model architectures remains limited
- **Low confidence:** Defensive applications in biometrics and generative models are presented theoretically with minimal implementation details or quantitative results

## Next Checks

1. **Benchmark transferability across attack families:** Implement and compare FGSM, PGD, MI-FGSM, and latent-space attacks on CIFAR-10/Imagenet using standardized models. Measure ASR, transferability rate, and computational cost across all methods.

2. **Physical attack robustness evaluation:** Replicate adversarial patch attacks under controlled environmental variations (lighting angles, distances, camera resolutions). Quantify mAP degradation versus physical constraint relaxation.

3. **Latent-space attack cross-architecture validation:** Test latent-space attacks on models with different latent structures (VAE vs. GAN vs. Diffusion). Measure transferability between architectures and compare against pixel-space baselines.