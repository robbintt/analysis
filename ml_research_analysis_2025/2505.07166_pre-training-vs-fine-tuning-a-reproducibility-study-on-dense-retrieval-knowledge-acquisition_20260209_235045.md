---
ver: rpa2
title: 'Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge
  Acquisition'
arxiv_id: '2505.07166'
source_url: https://arxiv.org/abs/2505.07166
tags:
- intermediate
- output
- retrieval
- knowledge
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This reproducibility study investigates the role of pre-training
  versus fine-tuning in dense retrievers by extending Reichman and Heck's hypothesis
  to different architectures and datasets. The paper tests CLS token vs.
---

# Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition

## Quick Facts
- arXiv ID: 2505.07166
- Source URL: https://arxiv.org/abs/2505.07166
- Authors: Zheng Yao; Shuai Wang; Guido Zuccon
- Reference count: 11
- Primary result: DPR fine-tuning primarily adjusts neuron activation rather than reorganizing knowledge, with pre-trained knowledge underpinning retrieval performance, though this pattern varies across architectures

## Executive Summary
This reproducibility study investigates the role of pre-training versus fine-tuning in dense retrievers by extending Reichman and Heck's hypothesis to different architectures and datasets. The paper tests CLS token vs. mean pooling representations, BERT vs. LLaMA backbones, and NQ vs. MSMARCO datasets. Through linear probing and neuron activation analyses, the study confirms that DPR fine-tuning primarily adjusts neuron activation rather than reorganizing knowledge, with pre-trained knowledge underpinning retrieval performance. However, this pattern does not hold universally - mean-pooled (Contriever) and decoder-based (LLaMA) models show different behavior. The findings suggest that knowledge decentralization effects observed in DPR are architecture-dependent, with pre-trained knowledge serving as the foundation for retrieval effectiveness while fine-tuning plays a more nuanced role in modifying activation patterns rather than fundamentally reorganizing knowledge storage.

## Method Summary
The study employs a multi-faceted approach to examine knowledge acquisition in dense retrievers. It conducts linear probing experiments to assess how well frozen representations capture retrieval-relevant information, comparing pre-trained versus fine-tuned models. Neuron activation analysis tracks changes in hidden state distributions across layers to determine whether fine-tuning reorganizes knowledge or merely adjusts activation patterns. The experiments span different model architectures (BERT with CLS pooling vs. mean pooling, LLaMA decoder models) and datasets (NQ and MSMARCO). Performance metrics include retrieval accuracy and probing task effectiveness, while statistical analysis validates the consistency of observed patterns across different experimental conditions.

## Key Results
- DPR fine-tuning primarily adjusts neuron activation patterns rather than reorganizing knowledge storage
- Pre-trained knowledge serves as the foundation for retrieval effectiveness in dense retrievers
- The pre-training vs. fine-tuning dynamics vary significantly across architectures - mean-pooled and decoder-based models show different behavior than standard DPR

## Why This Works (Mechanism)
The mechanism underlying dense retriever knowledge acquisition involves the interplay between pre-trained representations and fine-tuning adjustments. Pre-trained models contain rich semantic knowledge acquired through large-scale language modeling objectives. When fine-tuned for retrieval, these models primarily modify how this knowledge is activated rather than restructuring where it's stored. This explains why frozen pre-trained representations often perform surprisingly well on retrieval tasks - the knowledge is already present but requires activation pattern adjustments for optimal retrieval performance. The architecture-dependent variations suggest that different pooling strategies and model types (encoder vs. decoder) influence how knowledge is accessed and modified during fine-tuning.

## Foundational Learning
- **Dense retrieval fundamentals**: Understanding how dense retrievers encode queries and documents into vector spaces for similarity matching
  - Why needed: Essential for interpreting the study's experimental setup and results
  - Quick check: Can explain the difference between dense and sparse retrieval approaches

- **Pre-training vs. fine-tuning dynamics**: Knowledge of how language models acquire and modify representations during different training phases
  - Why needed: Central to understanding the study's core hypothesis about knowledge reorganization
  - Quick check: Can describe how pre-training objectives differ from retrieval-specific fine-tuning

- **Linear probing methodology**: Technique for evaluating frozen representations by training simple classifiers on top of them
  - Why needed: Key experimental method used to assess pre-trained knowledge utility
  - Quick check: Can explain how linear probing differs from end-to-end fine-tuning

## Architecture Onboarding

**Component map**: Pre-trained model → Fine-tuning phase → Retrieval task → Evaluation metrics
- Pre-trained model (BERT/LLaMA): Provides initial knowledge representations
- Fine-tuning phase: Adjusts activation patterns for retrieval-specific tasks
- Retrieval task: Uses learned representations for document ranking
- Evaluation metrics: Measures retrieval effectiveness and knowledge retention

**Critical path**: Pre-training → Architecture-specific pooling → Fine-tuning → Linear probing analysis → Retrieval evaluation
The study follows this progression to systematically examine how knowledge is acquired and modified across different stages of model development.

**Design tradeoffs**: The study balances breadth (testing multiple architectures and datasets) against depth (detailed analysis of specific models). This approach provides generalizable insights but may miss architecture-specific nuances. The choice of linear probing over other evaluation methods prioritizes interpretability over capturing all aspects of retrieval performance.

**Failure signatures**: When pre-trained knowledge is insufficient, models show poor retrieval performance even after fine-tuning. Architecture mismatches (e.g., using encoder models for decoder-specific tasks) lead to inconsistent patterns in knowledge modification. Dataset distribution shifts can invalidate the assumption that pre-trained knowledge transfers effectively.

**First experiments to run**:
1. Linear probing comparison between frozen pre-trained and fine-tuned models on NQ dataset
2. Neuron activation analysis across layers for BERT vs. LLaMA models
3. Retrieval performance ablation study removing pre-training components

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Findings are based on specific model architectures (BERT, LLaMA) and may not generalize to other transformer variants
- The study focuses on NQ and MSMARCO datasets, limiting conclusions about cross-domain generalization
- Neuron activation analysis may not capture all aspects of knowledge reorganization, particularly for more complex architectural modifications

## Confidence
- **Medium**: General claim about DPR models showing pre-training as foundation for retrieval effectiveness
- **Low**: Broader architectural generalizations about mean-pooled and decoder-based models
- **Low**: Extent to which findings generalize beyond tested datasets and domains

## Next Checks
1. Test the pre-training vs. fine-tuning dynamics across a broader range of transformer architectures, including T5, RoBERTa, and other encoder-decoder models to determine if the observed patterns hold universally.
2. Conduct experiments on additional datasets from different domains (e.g., biomedical, legal, or multilingual corpora) to assess the generalizability of the findings beyond the current NQ and MSMARCO focus.
3. Implement ablation studies that systematically vary pre-training objectives and fine-tuning strategies to isolate which aspects of pre-training knowledge are most critical for retrieval performance.