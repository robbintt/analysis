---
ver: rpa2
title: How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like Cognition
arxiv_id: '2503.00330'
source_url: https://arxiv.org/abs/2503.00330
tags:
- semantic
- size
- llms
- small
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) understand
  semantic size, the perceived magnitude of concepts, by comparing their performance
  with human cognition. The research employs three approaches: (1) metaphorical reasoning,
  where LLMs associate abstract words with concrete objects of varying sizes, (2)
  probing internal representations to assess how well semantic size is encoded, and
  (3) testing biases toward attention-grabbing headlines with larger semantic sizes
  in a shopping scenario.'
---

# How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like Cognition

## Quick Facts
- arXiv ID: 2503.00330
- Source URL: https://arxiv.org/abs/2503.00330
- Authors: Yao Yao; Yifei Yang; Xinbei Ma; Dongjie Yang; Zhuosheng Zhang; Zuchao Li; Hai Zhao
- Reference count: 40
- One-line primary result: Multi-modal trained LLMs outperform text-only models in understanding semantic size, showing more human-like metaphorical associations and headline preferences.

## Executive Summary
This study investigates how large language models (LLMs) understand semantic size, the perceived magnitude of concepts, by comparing their performance with human cognition. The research employs three approaches: (1) metaphorical reasoning, where LLMs associate abstract words with concrete objects of varying sizes, (2) probing internal representations to assess how well semantic size is encoded, and (3) testing biases toward attention-grabbing headlines with larger semantic sizes in a shopping scenario. Results show that multi-modal trained LLMs (MLLMs) outperform text-only models, demonstrating more human-like understanding of semantic size, particularly in metaphorical associations and headline selection. MLLMs also show higher certainty in their choices and better classification accuracy in semantic size tasks. However, challenges remain, especially with abstract concepts. The study highlights the importance of multi-modal training in enhancing LLMs' cognitive abilities and suggests that real-world, multi-modal experiences are crucial for both artificial and human cognition.

## Method Summary
The study uses the Glasgow Norms dataset (5,553 words with SIZE/Concreteness ratings) to construct tasks measuring semantic size understanding. For metaphor tasks, 512 multiple-choice questions pair abstract words with concrete words matched by frequency/length but dissociated in semantic similarity (cosine ≤ 0.6). Probing study extracts attention head activations from final token positions and trains logistic regression classifiers per head. Headline preference study polishes product titles using GPT-4o and measures selection rates across text-only, figure-only, and figure+text conditions. Models are evaluated with 7:3 train/test splits and 10-run averaging.

## Key Results
- MLLMs outperform text-only models in metaphorical association tasks, showing higher certainty and more human-like size mappings
- Multi-modal LLMs achieve better probe accuracy (up to 69.58% vs 68.10%) in classifying semantic size from attention head activations
- LLMs show preference for headlines with larger semantic sizes, with MLLMs exhibiting stronger bias than text-only models
- Certainty metrics indicate MLLMs make more decisive choices aligned with human patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal training improves semantic size comprehension by grounding abstract concepts in visual-concrete associations.
- Mechanism: Visual modality provides physical size referents (e.g., spaceship → large) that scaffold metaphorical mapping to abstract concepts (e.g., love → large), creating cross-modal representations that text-only models lack.
- Core assumption: Abstract-to-concrete metaphorical associations require embodied or visual grounding; text-only co-occurrence statistics are insufficient.
- Evidence anchors: "Our findings reveal that multi-modal training is crucial for LLMs to achieve more human-like understanding." MLLMs are more likely to associate large abstract words with large concrete words with higher certainty metrics.

### Mechanism 2
- Claim: Semantic size is encoded in attention head activations and is more retrievable in MLLMs than text-only LLMs.
- Mechanism: Linear probes trained on attention head outputs can predict big/small labels, suggesting semantic size features are linearly accessible in hidden states. Multi-modal training enriches these representations.
- Core assumption: Probing accuracy reflects internal representation quality, not probe overfitting; the linear separability indicates genuine encoding.
- Evidence anchors: "Multi-modal LLMs are more effective at classifying the semantic size of words... MLLMs generally outperform their text-only counterparts." Vicuna-7b text-only achieves 67.18% concrete accuracy vs. Llava-Vicuna-7b at 67.60%.

### Mechanism 3
- Claim: Larger semantic size in headlines increases LLM selection preference, mediated by emotional arousal associations.
- Mechanism: Semantic size correlates with emotional arousal; attention-grabbing headlines (higher arousal) have larger semantic sizes. MLLMs, more sensitive to these cues, prefer such headlines in decision tasks.
- Core assumption: The observed preference reflects semantic size processing, not confounds like word complexity or novelty; emotional engagement drives the bias.
- Evidence anchors: "LLMs are more attracted to titles with larger semantic size... This trend is even more pronounced for MLLMs." Semantic polishing method yields highest average semantic size (4.05) vs. Original (3.83).

## Foundational Learning

- Concept: **Semantic Size** (perceived magnitude of concepts, abstract or concrete)
  - Why needed here: Core variable measured across all three studies; distinguishes "spaceship/love" (large) from "dust/hush" (small).
  - Quick check question: Can you explain why "love" and "spaceship" share a semantic size dimension despite one being abstract and one concrete?

- Concept: **Probing Classifiers**
  - Why needed here: Study 2 uses linear probes to test if semantic size is linearly encoded in attention head activations.
  - Quick check question: If a probe achieves 90% accuracy on held-out data, what alternative explanations besides "the model encodes this property" must you consider?

- Concept: **Multi-modal Grounding**
  - Why needed here: Explains why MLLMs outperform text-only models—visual modality provides physical size referents.
  - Quick check question: How would you design a controlled experiment to test whether visual grounding specifically improves abstract concept understanding, not just overall model capacity?

## Architecture Onboarding

- Component map: Glasgow Norms dataset → word filtering by concreteness (CNC) and size (SIZE) scores → Abstract-concrete pair construction with frequency/length matching and semantic dissociation (cosine similarity ≤ 0.6) → Model inference → extract attention head activations → train/evaluate probes → Product title polishing → semantic size calculation → preference measurement across conditions

- Critical path: 1) Build/validate 512-item metaphor dataset with frequency/length matching and semantic dissociation 2) Run Study 1 on 2–3 model families with MLLM+text-only pairs 3) Extract attention head activations (L×H×D at final token) 4) Train logistic regression probes per head/layer 5) Report accuracy/ROC-AUC with 7:3 split and 10 seeds

- Design tradeoffs:
  - Dataset size vs. quality: Extended dataset (512 questions) improves robustness but introduces noise; original 110-question dataset had human-validated pairs
  - Probe complexity: Linear probes are interpretable but may miss non-linear representations; non-linear probes risk overfitting
  - Modal comparison: Text-only vs. figure-only vs. figure+text isolates modality effects but conflates modality with information density

- Failure signatures:
  - Probe accuracy near 50%: Model does not encode semantic size (e.g., Mistral-7b variants)
  - Negative certainty metric: Model exhibits inverse human-like tendencies
  - No preference for larger semantic size in shopping task: Model may lack emotional-arousal association or semantic size understanding

- First 3 experiments:
  1. Reproduce metaphor study with controlled word frequency/length matching; verify MLLM > text-only gap persists
  2. Probe layer-wise analysis: Train probes per layer to identify where semantic size information concentrates; test if mid-layers encode more than final layers
  3. Ablation study: Mask attention heads with highest probe accuracy; measure degradation in metaphor task to test causal relationship between encoding and behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI safety mechanisms be designed to mitigate the susceptibility of LLM agents to manipulation via "clickbait" or content inflated with large semantic sizes?
- Basis in paper: The authors state that the tendency of MLLMs to engage with large semantic sizes "introduces potential biases, such as the risk of manipulation through clickbait headlines," and conclude that ensuring unbiased decision-making is crucial.
- Why unresolved: The paper identifies the vulnerability (preference for large semantic size) but does not propose or test any defensive measures or training adjustments to neutralize this bias in agents.
- What evidence would resolve it: Experiments demonstrating that specific alignment techniques or fine-tuning can reduce an LLM's preference for semantically exaggerated headlines in decision-making tasks without degrading general performance.

### Open Question 2
- Question: Does the multi-modal advantage in understanding semantic size generalize to other abstract semantic dimensions, such as emotional valence or arousal?
- Basis in paper: The study isolates "semantic size" from the Glasgow Norms, which include other dimensions like arousal and valence. The authors claim the findings provide a "novel perspective" on cognition, but they do not confirm if the visual grounding benefit applies to other abstract concepts.
- Why unresolved: It remains unclear if the visual modality specifically aids "size" (due to spatial grounding) or if it improves the modeling of all abstract concepts equally.
- What evidence would resolve it: Replicating the probing and metaphorical reasoning experiments using words categorized by high/low arousal or positive/negative valence to see if MLLMs retain their advantage over text-only models.

### Open Question 3
- Question: Are the attention heads identified in the probing study causally responsible for the metaphorical reasoning behaviors observed in the external study?
- Basis in paper: The paper correlates internal representations (Study 2) with external behavior (Study 1), showing that MLLMs encode size better and act more human-like. However, it does not establish a causal link between the specific neurons that encode size and the model's final output choice.
- Why unresolved: Probing classifiers only indicate the presence of information in activations; they do not prove that the model actively uses those specific activations to solve the task.
- What evidence would resolve it: Intervention studies (e.g., ablating or perturbing the specific attention heads that classify semantic size) to see if the metaphorical association performance drops significantly.

## Limitations

- Dataset generalization: Semantic size labels from Glasgow Norms may not transfer to other languages or cultures; extended metaphor dataset quality is uncertain compared to original 110-question validated subset
- Probing validity: Linear probe accuracy might reflect dataset artifacts rather than genuine semantic size encoding, particularly given the small probe sample size (150 words)
- Confounding factors in headline preference: Observed bias toward larger semantic size headlines could be driven by word complexity, frequency, or novelty rather than size perception itself

## Confidence

- High confidence: MLLMs outperform text-only models in metaphor association tasks (direct performance metrics reported across multiple model families)
- Medium confidence: Semantic size is linearly encoded in attention head activations (probe accuracy supports this but could reflect dataset artifacts or probe overfitting)
- Low confidence: The headline preference effect is specifically due to semantic size processing (confounds like word complexity are not adequately controlled)

## Next Checks

1. **Probe ablation study**: Systematically remove attention heads with highest probe accuracy and measure degradation in metaphor task performance to establish causal relationship between encoding and behavior
2. **Cross-linguistic validation**: Test whether semantic size patterns from Glasgow Norms generalize to non-English datasets or require cultural/language-specific calibration
3. **Confound control experiment**: Design headline preference tests where word frequency, sentiment polarity, and length are controlled across semantic size conditions to isolate the size-specific effect