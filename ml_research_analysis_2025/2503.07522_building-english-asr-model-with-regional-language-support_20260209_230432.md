---
ver: rpa2
title: Building English ASR model with regional language support
arxiv_id: '2503.07522'
source_url: https://arxiv.org/abs/2503.07522
tags:
- language
- english
- en-in
- hindi
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel SplitHead with Attention (SHA) acoustic\
  \ model and language model adaptation approach to build an English ASR system that\
  \ effectively handles Hindi queries without degrading English performance. The SHA\
  \ model features shared hidden layers and language-specific projection layers combined\
  \ via a self-attention mechanism that weighs each language\u2019s contribution."
---

# Building English ASR model with regional language support
## Quick Facts
- arXiv ID: 2503.07522
- Source URL: https://arxiv.org/abs/2503.07522
- Reference count: 0
- Primary result: SHA model with language model interpolation achieves 69.3% relative WER reduction on Hindi queries, 5.7% on English queries vs monolingual baseline.

## Executive Summary
This paper presents a novel acoustic model architecture, SplitHead with Attention (SHA), designed to improve English automatic speech recognition (ASR) performance on Hindi queries without degrading English-only accuracy. The model integrates shared hidden layers with language-specific projection layers and employs a self-attention mechanism to dynamically weight each language's contribution. Additionally, the approach adapts the language model by interpolating n-gram models from English and transliterated Hindi corpora using contextual transliteration. The proposed method achieves substantial WER improvements on both Hindi and English queries while maintaining similar computational efficiency to the baseline.

## Method Summary
The proposed approach combines a SplitHead with Attention (SHA) acoustic model with a language model adaptation strategy. The SHA model uses shared hidden layers for both English and Hindi, with separate projection layers for each language, and a self-attention mechanism to weigh their contributions. The language model is adapted by interpolating n-gram models built from English and transliterated Hindi corpora, where Hindi text is transliterated into Latin script to match the ASR system's expectations. This joint optimization allows the system to handle code-switched or bilingual queries effectively while preserving monolingual English performance.

## Key Results
- 69.3% relative reduction in word error rate on Hindi queries compared to monolingual English baseline
- 5.7% relative reduction in word error rate on English queries
- Computational costs and latency remain similar to baseline model

## Why This Works (Mechanism)
The SHA model leverages shared hidden layers to capture common phonetic and linguistic features across English and Hindi, while language-specific projection layers allow each language to be processed according to its unique characteristics. The self-attention mechanism dynamically balances the contribution of each language's features, enabling robust handling of mixed-language queries. Language model adaptation via interpolated n-grams and transliteration allows the system to recognize transliterated Hindi queries using the same Latin-script framework, improving coverage and accuracy.

## Foundational Learning
- **Acoustic modeling for multilingual ASR**: Needed to capture shared and language-specific phonetic patterns; check by evaluating performance on both languages.
- **Language model interpolation**: Needed to combine knowledge from multiple languages; check by comparing WER before and after interpolation.
- **Contextual transliteration**: Needed to represent Hindi in Latin script for consistent processing; check by verifying transliteration quality and impact on recognition.
- **Self-attention mechanisms**: Needed to dynamically weight language-specific features; check by ablation study removing attention.

## Architecture Onboarding
**Component Map**: Audio input -> Shared hidden layers -> Language-specific projection layers -> Self-attention fusion -> Output logits
**Critical Path**: Audio feature extraction → Shared encoder → Language-specific projections → Self-attention → Language model integration → Decoder
**Design Tradeoffs**: Shared layers reduce parameters and encourage cross-lingual transfer, but risk interference; language-specific projections preserve language identity but increase complexity; transliteration simplifies processing but may lose script-specific cues.
**Failure Signatures**: Degraded English performance on code-switched utterances; mistranscription of transliterated Hindi due to ambiguous mappings; increased latency if attention mechanism is inefficient.
**First Experiments**:
1. Train SHA model on bilingual data and measure WER on English-only and Hindi-only test sets.
2. Compare interpolated vs monolingual language models on transliterated Hindi queries.
3. Perform ablation by removing self-attention and observing impact on mixed-language WER.

## Open Questions the Paper Calls Out
None

## Limitations
- Hindi query test set not described in detail; unclear if queries are truly Hindi or code-switched
- No quantitative data on computational costs, memory, or inference latency
- Transliteration assumes Hindi queries are in Latin script, which may not reflect real user behavior
- Lack of ablation study on self-attention mechanism and language-specific layers
- No cross-validation, statistical significance testing, or detailed error analysis provided

## Confidence
- Hindi WER reduction claim: Low (dataset quality and representativeness unclear)
- English WER improvement claim: Medium (baseline details missing, but results positive)
- Computational cost claim: Low (no quantitative metrics provided)

## Next Checks
1. Release or describe the Hindi query test set in detail, including query types, script usage, and speaker demographics.
2. Provide quantitative latency and memory usage metrics for the SHA model versus baseline under identical hardware.
3. Conduct an ablation study isolating the impact of the self-attention mechanism and language-specific projection layers on WER.