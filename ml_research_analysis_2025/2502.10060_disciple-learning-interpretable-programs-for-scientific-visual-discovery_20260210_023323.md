---
ver: rpa2
title: 'DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery'
arxiv_id: '2502.10060'
source_url: https://arxiv.org/abs/2502.10060
tags:
- programs
- program
- data
- disciple
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSciPLE is a framework for discovering interpretable, predictive
  programs from scientific visual data by leveraging large language models (LLMs)
  and evolutionary search. The approach addresses the challenge of creating interpretable
  models for complex scientific tasks where traditional interpretable methods lack
  expressive power and deep models lack interpretability.
---

# DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery

## Quick Facts
- **arXiv ID:** 2502.10060
- **Source URL:** https://arxiv.org/abs/2502.10060
- **Reference count:** 40
- **Primary result:** 35% lower error than closest non-interpretable baseline for population density estimation

## Executive Summary
DiSciPLE is a framework for discovering interpretable, predictive programs from scientific visual data by leveraging large language models (LLMs) and evolutionary search. The approach addresses the challenge of creating interpretable models for complex scientific tasks where traditional interpretable methods lack expressive power and deep models lack interpretability. DiSciPLE uses an evolutionary algorithm that starts with LLM-generated programs and iteratively improves them through crossover and mutation operations guided by LLMs. The framework incorporates two key improvements: a program critic that evaluates performance across data partitions and a program simplifier that removes redundant components. On three real-world scientific tasks (population density estimation, poverty prediction, and biomass estimation), DiSciPLE achieves state-of-the-art results, including 35% lower error than the closest non-interpretable baseline for population density estimation. The learned programs are both highly accurate and interpretable, enabling scientists to understand the underlying mechanisms of predictions.

## Method Summary
DiSciPLE learns interpretable Python programs through evolutionary search guided by large language models. The framework starts with an initial population of LLM-generated programs and iteratively improves them through crossover and mutation operations. A program critic evaluates performance across stratified data partitions to improve out-of-distribution generalization, while a simplifier removes redundant components to maintain interpretability. The method uses Sentinel-2 satellite imagery as input and learns programs that combine primitive functions (segmentation, element-wise operations, etc.) to predict scientific indicators. The final programs are linear combinations of features extracted by the learned programs, making them both interpretable and predictive.

## Key Results
- DiSciPLE achieves state-of-the-art performance on three real-world scientific tasks
- 35% lower error than closest non-interpretable baseline for population density estimation
- Programs are highly interpretable, with learned factors mapping to meaningful scientific concepts (e.g., "residential building" for population density)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-guided crossover and mutation accelerate program search compared to random or purely symbolic methods by constraining the search space with semantic priors.
- **Mechanism:** The system prompts an LLM (e.g., Llama-3) to combine code snippets from high-performing "parent" programs (crossover) or modify existing ones (mutation). This leverages the LLM's pre-trained coding knowledge to propose syntactically valid and logically plausible variations, rather than randomly perturbing syntax trees.
- **Core assumption:** The LLM possesses sufficient "common sense" regarding the primitive functions (e.g., `segment`, `elementwise_max`) to propose meaningful combinations.
- **Evidence anchors:**
  - [abstract] Mentions leveraging "common sense and prior knowledge of large language models (LLMs)" for evolutionary search.
  - [section 4.4 / Table 4] Shows that removing common sense (renaming functions) or context causes performance to drop to worse-than-mean levels, validating the LLM's role in guiding search.
  - [corpus] Related work "HypoChainer" supports the general efficacy of combining LLMs with structured reasoning for scientific discovery, though specific evolutionary mechanisms differ.
- **Break condition:** If the target domain is entirely novel or uses primitives completely outside the LLM's training distribution, the semantic priors fail, reducing the LLM to a random generator.

### Mechanism 2
- **Claim:** A stratified "critic" improves out-of-distribution (OOD) generalization by forcing the program to succeed on under-represented data subtypes.
- **Mechanism:** The critic partitions the dataset into strata (e.g., land-use categories via segmentation) and evaluates the program on each. It provides fine-grained feedback to the LLM, specifically highlighting categories where the program fails. This prevents the optimization from overfitting to the dominant distribution of the training set.
- **Core assumption:** Data can be meaningfully stratified using the available foundation models (e.g., GRAFT for satellite imagery).
- **Evidence anchors:**
  - [section 3.4] Describes the critic's role in partitioning data and providing fine-grained evaluation.
  - [section 4.4] Notes that adding the critic results in better "unseen and OOD generalization."
  - [corpus] "ScienceBoard" discusses evaluating agents in realistic workflows, indirectly supporting the need for robust, stratified evaluation in scientific tasks.
- **Break condition:** If the stratification logic (e.g., the segmentation model used by the critic) is biased or inaccurate, the feedback loop will optimize for the wrong data subgroups.

### Mechanism 3
- **Claim:** Program simplification via Abstract Syntax Tree (AST) pruning and regression weight analysis maintains interpretability and prevents bloat.
- **Mechanism:** Evolutionary searches tend to produce "bloat" (redundant code). DiSciPLE uses a simplifier that recursively removes AST leaf nodes not connected to the return statement and prunes features with low linear regression weights.
- **Core assumption:** Relevant features in the final linear layer are indicative of the program's overall causal logic.
- **Evidence anchors:**
  - [section 3.5] Details the "analytical approach to simplify the programs" by removing unreachable nodes and low-weight features.
  - [figure 3] Visualizes the resulting Directed Acyclic Graphs (DAGs), showing how simplification isolates key factors (e.g., "residential building" for density).
  - [corpus] "Interpretable Machine Learning in Physics" emphasizes the necessity of interpretability for physical discovery, aligning with DiSciPLE's simplification goal.
- **Break condition:** If a feature has a low individual weight but is critical for a non-linear interaction (an "and" case) that the linear regressor fails to capture, simplification might erroneously remove it.

## Foundational Learning

- **Concept:** **Evolutionary Algorithms (Genetic Programming)**
  - **Why needed here:** DiSciPLE is fundamentally an evolutionary search loop. You must understand selection, crossover, and mutation to diagnose why the program bank improves (or stagnates) over generations.
  - **Quick check question:** Can you explain why the paper samples "parents" based on a fitness score before applying crossover?

- **Concept:** **Neuro-Symbolic AI**
  - **Why needed here:** The architecture bridges neural perception (segmentation models like GRAFT) and symbolic reasoning (Python logic/math). Understanding this distinction is key to debugging whether a failure is perceptual (bad mask) or logical (bad math).
  - **Quick check question:** In the program `elementwise_max(segment(im, "forest"), segment(im, "trees"))`, which part is neural and which is symbolic?

- **Concept:** **Abstract Syntax Trees (AST)**
  - **Why needed here:** The simplification mechanism relies on treating code as a graph (AST) to identify and prune "dead" code branches.
  - **Quick check question:** How would the simplifier handle a variable defined at the top of a function but never referenced in the `return` statement?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Dataset $D$, Metric $M$, Primitives $F$, Textual Description
  2. **Program Bank:** Population of $M=100$ Python programs
  3. **LLM Engine:** Llama-3-8b-instruct (drives initialization, crossover, mutation)
  4. **Execution Environment:** Python interpreter + GRAFT (segmentation model) + Regressor
  5. **Critic & Simplifier:** Post-processing modules that refine LLM outputs before re-insertion into the bank

- **Critical path:** The **Evolution Loop** (Algorithm 1). The system is only as good as the feedback loop: `Sample Parents -> LLM Crossover -> Critic -> Simplifier -> Evaluate Fitness -> Update Bank`. If the metric $M$ does not penalize bad programs strictly enough, the "fitness" signal degrades, and evolution stalls.

- **Design tradeoffs:**
  - **Interpretability vs. Performance:** The system restricts the final layer to a linear regressor on learned features. While interpretable, this may limit modeling complex non-linear relationships compared to deep networks (though the paper claims SOTA, this is a structural constraint).
  - **LLM Cost vs. Random Search:** Using LLMs for mutation is expensive. The paper argues it is more sample-efficient than random search, but inference costs are significantly higher.

- **Failure signatures:**
  - **Stagnation:** The "Random Search" baseline overtakes DiSciPLE, suggesting the LLM prompts are failing to induce meaningful variation (check prompt engineering).
  - **Bloat:** Programs grow indefinitely in length without score improvement (check Simplifier thresholds).
  - **Syntax Errors:** High rate of execution errors suggests the LLM temperature is too high or the primitive API specification is unclear to the model.

- **First 3 experiments:**
  1. **Sanity Check (Zero-Shot vs. Random):** Replicate the zero-shot LLM baseline and Random Search baseline to ensure the evolutionary loop is actually providing a signal above random noise.
  2. **Ablation (No Critic):** Disable the critic component and measure the delta in OOD generalization error. This validates the specific contribution of stratified feedback.
  3. **Primitive Sensitivity:** Swap the GRAFT foundation model for a weaker segmentation model. Observe if the evolutionary search can compensate for poor primitives or if it collapses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to effectively optimize learnable parameters in intermediate computational layers rather than restricting differentiable optimization to the final layer?
- **Basis in paper:** [explicit] The authors state in the Discussion: "One of our limitation is that we can only differentiably optimize learnable parameters in the last computational layer."
- **Why unresolved:** The authors attempted to make the whole pipeline differentiable, but found that operations often have zero-gradients in large parts of the input space, making standard gradient optimization challenging and slow.
- **What evidence would resolve it:** A modification of DiSciPLE that successfully learns parameters in intermediate layers (e.g., inside nested functions) and demonstrates improved accuracy over the current last-layer-only regression approach.

### Open Question 2
- **Question:** Can initialization tricks or second-order optimization techniques effectively overcome the current expressiveness limitations of the learned programs?
- **Basis in paper:** [explicit] The Conclusion explicitly lists this as a plan: "In future work, we plan to use initialization tricks for non-linear optimization and second-order optimization to obtain even more expressive models."
- **Why unresolved:** Current linear regression on the final feature set limits the complexity of relationships the model can capture, and standard gradient descent failed to improve this in preliminary attempts.
- **What evidence would resolve it:** Demonstrating that a second-order optimizer (like L-BFGS) or specific weight initialization strategies allows the model to learn complex non-linear relationships that currently outperform the linear baseline.

### Open Question 3
- **Question:** How robust is the evolutionary search when the provided library of primitives is incomplete or lacks the necessary concepts to solve the target scientific task?
- **Basis in paper:** [inferred] The methodology assumes a set of primitives $F$ is provided (Section 3.1), and the "No common-sense" ablation showed performance drops when the LLM didn't understand the primitives.
- **Why unresolved:** It is unclear if the LLM can effectively "discover" or approximate missing necessary primitives through composition of existing ones, or if the search fails entirely if $F$ is insufficient.
- **What evidence would resolve it:** An ablation study where key primitives (e.g., "segment road") are removed from $F$, analyzing whether the algorithm can find alternative compositional solutions or if performance collapses.

## Limitations
- **Major Uncertainty:** Exact prompt templates for crossover and mutation operations are not specified, making it unclear whether the LLM guidance is optimally engineered
- **Major Uncertainty:** The stratification categories used by the critic are referenced but not enumerated, leaving ambiguity about how representative the evaluation truly is
- **Major Uncertainty:** The specific configuration and input preprocessing requirements for the GRAFT segmentation model are not detailed, which could significantly impact performance

## Confidence
- **High Confidence:** The core mechanism of LLM-guided evolutionary search is well-supported by ablation results showing catastrophic performance drops when semantic priors are removed
- **Medium Confidence:** The critic's contribution to OOD generalization is supported but relies on assumptions about the quality of stratification
- **Low Confidence:** The comparative claims against non-interpretable baselines depend heavily on the specific implementation of the linear regressor and the quality of the segmentation foundation model

## Next Checks
1. **Ablation of Prompt Templates:** Systematically test different prompt engineering approaches for crossover/mutation operations to determine if the current templates are optimal
2. **Critic Stratification Robustness:** Replace the GRAFT-based stratification with a simpler heuristic and measure the impact on OOD generalization
3. **Simplifier Sensitivity Analysis:** Quantify the trade-off between interpretability and performance by measuring program accuracy before and after simplification