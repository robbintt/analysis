---
ver: rpa2
title: The Eloquence team submission for task 1 of MLC-SLM challenge
arxiv_id: '2507.19308'
source_url: https://arxiv.org/abs/2507.19308
tags:
- speech
- linear
- training
- language
- eurollm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Eloquence team explored three approaches to improve multilingual
  conversational ASR for the MLC-SLM challenge. They first evaluated the official
  baseline using different projectors and foundation models, then leveraged the SLAM-ASR
  framework to systematically assess architectural choices, and finally introduced
  context-aware modeling with contrastive learning.
---

# The Eloquence team submission for task 1 of MLC-SLM challenge

## Quick Facts
- arXiv ID: 2507.19308
- Source URL: https://arxiv.org/abs/2507.19308
- Reference count: 0
- The Eloquence team achieved 15.15% WER on the MLC-SLM challenge evaluation set, ranking 13th on the public leaderboard.

## Executive Summary
The Eloquence team explored three approaches to improve multilingual conversational ASR for the MLC-SLM challenge. They first evaluated the official baseline using different projectors and foundation models, then leveraged the SLAM-ASR framework to systematically assess architectural choices, and finally introduced context-aware modeling with contrastive learning. Their best system, combining Whisper Large V3 Turbo with EuroLLM 1.7 and LoRA fine-tuning, achieved 15.15% WER on the evaluation set. Context and contrastive learning experiments showed improvements in the development set, with context alone reducing WER from 19.85% to 18.92%, and context with contrastive learning (without LoRA) achieving 17.18% WER.

## Method Summary
The team used the SLAM-ASR framework to evaluate different configurations of the SLM architecture (speech encoder → downsampler → projector → LLM). Their best configuration used Whisper Large V3 Turbo as the frozen encoder, EuroLLM 1.7B as the frozen LLM with LoRA adapters (rank 8, scaling factor 32), and a linear projector. Training used Adam optimizer with learning rate 1e-4, warmup of 1000 steps, batch size 8, and 3 epochs. For context-aware experiments, they prepended previous turn text to the prompt and added a contrastive learning loss that pulled together embeddings of speech-context pairs while pushing apart embeddings of speech with other contexts.

## Key Results
- Best system achieved 15.15% WER on evaluation set using Whisper Large V3 Turbo + EuroLLM 1.7B + LoRA (r=8, α=32)
- Context alone reduced WER from 19.85% to 18.92% on development set
- Context with contrastive learning (without LoRA) achieved 17.18% WER on development set
- Data augmentation improved EuroLLM Instruct model but degraded the best non-instruct model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint projector-LLM training yields better performance than two-stage training.
- **Mechanism:** When the projector and LoRA adapters are trained simultaneously, the projection layer can adapt to the specific representations being learned by the LLM's attention layers, creating mutually compatible feature transformations rather than fixing the projector based on an unadapted LLM.
- **Core assumption:** The frozen speech encoder produces sufficiently generic representations that don't require joint adaptation with the projector.
- **Evidence anchors:**
  - [abstract] "Their best system, combining Whisper Large V3 Turbo with EuroLLM 1.7 and LoRA fine-tuning, achieved 15.15% WER"
  - [section 2.1] "the outcomes did not reveal a substantial difference when employed the two-stage training strategy. Consequently, we opted to submit additional trials wherein just the second training step was applied"
  - [corpus] Related challenge submissions (Triple X, Transsion) similarly use encoder-adapter-LLM architectures but don't explicitly compare joint vs. staged training, limiting cross-validation.
- **Break condition:** If encoder representations are highly domain-specific or mismatched to the target language distribution, joint training may converge to suboptimal local minima without encoder adaptation.

### Mechanism 2
- **Claim:** LoRA rank and scaling factor configuration significantly impacts convergence speed and final WER.
- **Mechanism:** Higher LoRA rank increases the capacity of low-rank adaptation matrices to capture language-specific patterns, but requires more training steps for convergence. Lower scaling factors (α) relative to rank produce gentler gradient updates that may be insufficient for large distributional shifts.
- **Core assumption:** The pretrained LLM has sufficient multilingual knowledge that only low-rank perturbations are needed for task adaptation.
- **Evidence anchors:**
  - [section 2.2.1, Table 3] "LoRA (α = 8, r = 8)" achieved 21.14% WER while "LoRA (α = 32, r = 16)" achieved 15.88% WER on the dev set with EuroLLM 1.7B Instruct
  - [section 2.3.1] "performance without LoRA was better than with LoRA. One possible explanation is that we only trained for two epochs, which may not have been sufficient for the model to converge when LoRA was activated"
  - [corpus] ILT paper (2507.08477) addresses LoRA overfitting in SFT stages, suggesting convergence dynamics are a known challenge.
- **Break condition:** When training epochs are severely limited (<2-3), LoRA may underperform compared to training only the projector, as the adapter weights don't reach their effective capacity.

### Mechanism 3
- **Claim:** Conversational context prepended to the prompt provides semantic disambiguation signals that reduce recognition errors.
- **Mechanism:** Preceding dialogue turns establish lexical and semantic priors that constrain the LLM's transcription hypotheses. Contrastive learning reinforces this by learning embeddings where speech-context pairs cluster together, improving the model's ability to leverage contextual coherence.
- **Core assumption:** Adjacent turns in conversation maintain semantic relatedness and speaker consistency that aids prediction.
- **Evidence anchors:**
  - [abstract] "context alone reducing WER from 19.85% to 18.92%, and context with contrastive learning (without LoRA) achieving 17.18% WER"
  - [section 2.3] "each speech utterance and its context form a positive pair, while utterances paired with others' context serve as negative pairs"
  - [corpus] Bi-directional Context-Enhanced SLLM paper (2506.13396) proposes similar context masking strategies, providing convergent evidence for context utility, though with different implementation.
- **Break condition:** If conversations have rapid topic shifts or the contextual turn is noisy/mistranscribed, prepending it may introduce misleading priors rather than helpful constraints.

## Foundational Learning

- **Concept: Speech-Language Model (SLM) Architecture**
  - **Why needed here:** The entire system is built on the encoder-projector-LLM paradigm; understanding how frozen components interact with trainable adapters is prerequisite to interpreting results.
  - **Quick check question:** Can you explain why the projector must match the LLM's embedding dimension, and what happens if the encoder output sequence length exceeds the LLM's context window?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** All competitive results use LoRA; understanding rank (r) and scaling factor (α) is essential for reproducing or improving upon the 15.15% WER result.
  - **Quick check question:** Given a weight matrix W of dimension d×k, what is the parameter count difference between full fine-tuning and LoRA with rank r?

- **Concept: Contrastive Learning Objective**
  - **Why needed here:** The context + contrastive learning experiment achieved the best dev set performance (17.18%); understanding positive/negative pair construction explains why this works.
  - **Quick check question:** How would you construct negative pairs if all utterances in a batch come from the same conversation?

## Architecture Onboarding

- **Component map:** Audio Input → Whisper Large V3 Turbo (frozen encoder) → Downsampler (factor 4) → Linear Projector (trainable) → LLM Input Embedding Space → EuroLLM 1.7B (frozen + LoRA adapters) → Transcription Output

- **Critical path:** The projector initialization and learning rate schedule. The paper uses Adam with lr=1e-4, warmup=1000 steps, batch size=8. Poor initialization here cascades to LoRA adaptation quality.

- **Design tradeoffs:**
  - Larger LLM (Salamandra 7B) vs. smaller (EuroLLM 1.7B): Table 1 shows 18.64% vs. 15.15% WER favoring the smaller model with proper LoRA—likely due to training efficiency and overfitting risk with limited data.
  - Q-Former vs. Linear projector: Q-Former adds complexity without clear gains in this data regime (Table 2 shows 22.52% vs. 21.13% with similar configurations).
  - Two-stage vs. joint training: Paper reports no substantial difference, but joint training simplifies the pipeline.

- **Failure signatures:**
  - WER stagnating around 20%+: Check if LoRA is actually being applied (verify q_proj, v_proj are targets).
  - Context experiments performing worse than baseline: Verify context is correctly aligned (speaker tags O1/O2 matching actual speakers).
  - Data augmentation degrading performance (Table 1 shows 19.85% → 20.85% with full augmentation): Augmentation may be too aggressive for already-challenging conversational audio.

- **First 3 experiments:**
  1. Reproduce the best configuration (Whisper Large V3 Turbo + EuroLLM 1.7B + LoRA r=8, α=32) on a subset to verify training pipeline correctness—target dev WER < 16%.
  2. Ablate context: train with and without the previous-turn context prepending to quantify the 18.92% → 17.18% improvement on your data split.
  3. Sweep LoRA rank (r=4, 8, 16, 32) with fixed α=32 for 5 epochs to determine if convergence issues in the paper's 2-epoch context experiments can be resolved with longer training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does extending the training duration beyond two epochs enable LoRA fine-tuning to outperform frozen projector setups in context-aware contrastive learning?
- **Basis:** [inferred] The authors observed worse performance with LoRA (18.62%) compared to without LoRA (17.18%) in contrastive experiments, explicitly hypothesizing that "two epochs... may not have been sufficient" for convergence.
- **Why unresolved:** Challenge constraints limited training time, leaving it undetermined whether the gap was due to architectural incompatibility or simple under-fitting.
- **What evidence would resolve it:** A comparison of development set WER after training the LoRA-based context model for 5–10 epochs.

### Open Question 2
- **Question:** Why does extensive data augmentation benefit instruction-tuned models (EuroLLM Instruct) while degrading the performance of non-instruct models?
- **Basis:** [inferred] Table 1 shows full augmentation improved EuroLLM 1.7B Instruct (19.11% to 16.86% WER) but worsened the best non-instruct model (15.15% to 16.21%).
- **Why unresolved:** The paper reports the conflicting outcomes but does not analyze why instruction tuning alters the model's sensitivity to augmented audio data.
- **What evidence would resolve it:** An ablation study applying specific augmentation types to both base and instruct versions of the LLM to isolate the sensitivity factor.

### Open Question 3
- **Question:** Can the proposed projector-based architecture effectively leverage cross-lingual knowledge transfer to improve ASR performance in low-resource language scenarios?
- **Basis:** [explicit] The conclusion explicitly lists "exploring cross-lingual knowledge transfer in low-resource settings" as a primary focus for future work.
- **Why unresolved:** The current study focused on aggregate benchmark results without specifically isolating or analyzing low-resource transfer capabilities.
- **What evidence would resolve it:** Experiments measuring WER improvements on a held-out low-resource language when fine-tuning on high-resource languages versus training from scratch.

## Limitations

- Training duration for LoRA-based systems (2-3 epochs) may have been insufficient for full convergence, as evidenced by context experiments performing better without LoRA.
- The paper reports no substantial difference between two-stage and joint training but lacks quantitative detail to support this claim.
- Projector architecture choices (linear vs. Q-Former) were evaluated under different conditions, making direct comparisons difficult.

## Confidence

**High Confidence:**
- The SLAM-ASR framework effectively enables systematic evaluation of SLM components
- LoRA rank and scaling factor significantly impact performance (supported by Table 3 comparisons)
- Whisper Large V3 Turbo + EuroLLM 1.7B + LoRA (r=8, α=32) achieves strong results (15.15% WER)
- Context prepending provides measurable WER improvement (19.85% → 18.92%)

**Medium Confidence:**
- Joint projector-LLM training is equivalent to two-stage training (based on limited comparative results)
- Contrastive learning with context improves performance (single experiment without extensive ablation)
- Data augmentation degrades performance for the best model (based on single comparison point)

**Low Confidence:**
- Q-Former projector is inferior to linear projector (evaluated under different conditions)
- The specific LoRA configuration (r=8, α=32) is optimal (limited hyperparameter sweep reported)

## Next Checks

1. **Convergence Validation:** Train the best LoRA configuration (EuroLLM 1.7B + r=8, α=32) for 5-10 epochs to determine if the 15.15% WER result improves with extended training, addressing the convergence uncertainty noted in the context experiments.

2. **Context Mechanism Isolation:** Implement a controlled ablation study where context is provided but contrastive learning is disabled, and vice versa, using the same training duration and LoRA configuration to isolate the individual contributions of each mechanism.

3. **Two-Stage vs Joint Training Quantification:** Systematically compare two-stage training (projector only → projector + LoRA) versus joint training with identical hyperparameters across multiple runs to produce statistical evidence for or against the reported equivalence.