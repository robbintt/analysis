---
ver: rpa2
title: Optimized Architectures for Kolmogorov-Arnold Networks
arxiv_id: '2512.12448'
source_url: https://arxiv.org/abs/2512.12448
tags:
- learning
- networks
- gates
- kans
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Kolmogorov-Arnold Networks (KANs) with architectural
  enhancements to improve their expressiveness while maintaining interpretability.
  The authors propose using overprovisioned architectures with differentiable sparsification,
  guided by minimum description length (MDL) principles.
---

# Optimized Architectures for Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2512.12448
- Source URL: https://arxiv.org/abs/2512.12448
- Reference count: 40
- Key outcome: Overprovisioned KANs with differentiable sparsification achieve competitive accuracy with substantially smaller, more interpretable models

## Executive Summary
This paper introduces architectural enhancements to Kolmogorov-Arnold Networks (KANs) by combining DenseNet-style forward connections with differentiable ℓ₀ sparsification. The authors demonstrate that overprovisioning KANs with forward connections and then sparsifying them using edge and node gates produces compact, interpretable subnetworks while maintaining or improving accuracy. The approach transforms architecture search into an end-to-end optimization problem using Minimum Description Length (MDL) principles to balance prediction accuracy against model complexity.

## Method Summary
The method combines B-spline KANs with forward connections that concatenate all prior layer outputs to each subsequent layer. Differentiable sparsification is implemented using hard concrete gates with learnable logits, allowing joint learning of network parameters and architecture. The MDL loss combines MSE with a complexity penalty based on the expected number of open gates. Training includes a warm-up period without sparsification, followed by gradual sparsification. The approach discovers compact subnetworks that often rely on forward connections for simpler tasks while maintaining trunk participation for complex functions.

## Key Results
- On Nguyen symbolic regression benchmark, full model with forward connections and gates achieved test R² ≈ 1 with significantly fewer active functions than ungated conditions
- For dynamical systems forecasting, method achieved accurate predictions while reducing network size by up to 81% for the Ikeda map
- On real-world datasets (concrete strength and superconductor critical temperature), approach improved predictive accuracy while reducing model complexity by up to 88.7%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Differentiable ℓ₀ sparsification enables joint learning of network parameters and architecture within standard gradient-based optimization.
- **Mechanism:** Binary gates (z ∈ {0,1}) are reparameterized using continuous relaxations with learnable logits α_j and the hard concrete distribution. The reparameterization trick allows gradients to flow through stochastic sampling. At inference, gates are thresholded deterministically (ẑ = I[E[ẑ] > 1/2]), yielding exact zeros rather than small values.
- **Core assumption:** The continuous relaxation sufficiently approximates discrete gate selection during training; gate decisiveness will emerge given sufficient optimization.
- **Evidence anchors:**
  - [abstract] "Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem."
  - [Section 2.2] Describes Louizos et al.'s continuous relaxation with equations 5-9; notes this "encourages true sparsity by setting parameters to exactly zero."
- **Break condition:** Gate variance remains high at convergence (gates don't "harden"); temperature τ or stretch parameters (γ, ζ) are misconfigured for the network scale.

### Mechanism 2
- **Claim:** Forward connections (FCs) provide alternative pathways that allow aggressive pruning of the main network trunk while preserving expressiveness.
- **Mechanism:** DenseNet-style connections concatenate all prior layer outputs (including input) as input to each subsequent layer: x^(ℓ+1) = Φ_ℓ([x^(0), x^(1), ..., x^(ℓ)]). When combined with sparsification, the network can select direct input-to-output paths, effectively learning optimal depth. Simpler functions may use only FCs; complex functions retain trunk participation.
- **Core assumption:** Overprovisioning with FCs creates sufficient pathway diversity that sparsification can discover near-optimal subnetworks.
- **Evidence anchors:**
  - [abstract] "Overprovisioning and sparsification are synergistic, with the combination outperforming either alone."
  - [Section 4.1, Table 1] For simpler Nguyen problems, "FCs were used in all cases... FCs were often, for the simpler problems, the only retained activation functions."
- **Break condition:** Task requires compositions that cannot be expressed through shallow FC pathways; trunk gets over-pruned despite complexity.

### Mechanism 3
- **Claim:** The Minimum Description Length (MDL) objective provides principled regularization balancing prediction accuracy against model complexity.
- **Mechanism:** Total loss combines MSE with a BIC-style complexity penalty: L = MSE + β(log n / n)||θ||₀, where ||θ||₀ ≈ Σ E[z] (expected open gates). The β hyperparameter controls the accuracy-sparsity tradeoff. The log n / n scaling follows from information-theoretic encoding arguments.
- **Core assumption:** The BIC approximation appropriately balances complexity for KAN architectures; β can be tuned via validation without overfitting.
- **Evidence anchors:**
  - [Section 3] "we use the minimum description length and seek models that minimize the total number of bits required to encode the model and the data given the model."
  - [Section 4.3] "both datasets were sensitive to the choice of β... In practice, then, it is likely necessary to tune β using a validation set."
- **Break condition:** β is set too high (over-regularization harms accuracy) or too low (insufficient sparsification); optimal β varies substantially across tasks.

## Foundational Learning

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - **Why needed here:** KANs are motivated by this theorem, which states any continuous multivariate function can be represented as compositions of univariate functions. This justifies learning activation functions on edges rather than fixed activations with learned weights.
  - **Quick check question:** Can you explain why KANs place learnable activations on edges rather than at nodes?

- **Concept: Reparameterization Trick**
  - **Why needed here:** The differentiable sparsification mechanism relies on reparameterizing stochastic gate sampling to allow gradient flow. Without understanding this, the training dynamics will be opaque.
  - **Quick check question:** How does sampling u ~ Uniform(0,1) and transforming it through a sigmoid enable backpropagation through a discrete-like gate?

- **Concept: B-spline Basis Functions**
  - **Why needed here:** The activation functions in this work use B-spline parameterization (Eq. 3). Understanding spline grids, coefficients, and the base function (SiLU) is necessary to interpret learned activations.
  - **Quick check question:** What role does the base function b(x) play in preventing the spline from collapsing to a high-order polynomial?

## Architecture Onboarding

- **Component map:** Input → [KAN Layer with FCs] → [Gated Activation Functions] → Output

- **Critical path:**
  1. Implement B-spline activation functions with learnable coefficients
  2. Add forward connections concatenating all prior outputs
  3. Implement hard concrete gates with reparameterization
  4. Construct MDL loss with MSE + complexity term
  5. Train with warmup (β=0 for first N epochs), then enable sparsification

- **Design tradeoffs:**
  - **β tuning:** Lower β preserves accuracy but reduces sparsity; higher β increases interpretability but risks underfitting. Task-dependent; requires validation.
  - **Gate type:** Edge gates (egates) provide fine-grained sparsity; node gates (ngates) provide structured/group sparsity. Paper uses egates primarily.
  - **Architecture depth vs. FC reliance:** Deeper networks with FCs provide more flexibility but require more aggressive sparsification to remain interpretable.

- **Failure signatures:**
  - Gates don't converge to {0,1}: Temperature τ may be too high, or learning rate mismatch between spline coefficients and gate parameters.
  - Accuracy collapses with sparsification: β too high for task complexity; reduce β or increase warmup period.
  - FC-only solutions on complex tasks: Network underprovisioned in trunk capacity; increase layer width.
  - Training instability: Gate variance issues; consider gradient clipping or separate learning rates.

- **First 3 experiments:**
  1. **Sanity check:** Replicate the z = sin(x + y²) example (Figure 1) with architecture [2,2,1], training 3000 epochs, β=0.2. Verify Full condition achieves R²≈1.0 with interpretable activations.
  2. **β sensitivity sweep:** On a held-out validation set, test β ∈ {0, 0.001, 0.01, 0.05, 0.1} for the concrete strength dataset. Plot accuracy vs. sparsity to identify Pareto frontier.
  3. **Ablation study:** Run 2×2 grid (Baseline, FC Only, Gates Only, Full) on Nguyen F5 (sin(x²)cos(x)-1). Verify that Full achieves comparable R² with fewer activations than Baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can input gating be integrated into this architecture to enable automatic variable selection and improve scalability for high-dimensional datasets?
- **Basis in paper:** [explicit] The authors note in the Discussion that "gating inputs could allow KANs to do variable selection, letting them scale better to problems with large numbers of predictors."
- **Why unresolved:** The current study fixed inputs and focused on gating internal edges and nodes; input selection was identified as a direction for future work.
- **What evidence would resolve it:** Demonstrating successful variable selection and maintained accuracy on datasets with significantly more input features than those tested.

### Open Question 2
- **Question:** Does node-level gating (ngates) provide superior performance for structured sparsity compared to the edge-level gating (egates) emphasized in this study?
- **Basis in paper:** [explicit] The authors state in the Discussion: "node-level gating (ngates) may offer benefits for structured or group sparsity that we did not fully explore here."
- **Why unresolved:** The experiments focused on edge gates as the more general mechanism, leaving the specific benefits of group sparsity via ngates unquantified.
- **What evidence would resolve it:** Ablation studies comparing the performance and resulting network structure of ngates versus egates on tasks requiring group feature selection.

### Open Question 3
- **Question:** Can the instability or variance issues inherent in differentiable $\ell_0$ regularization be mitigated to reliably discover compact models for chaotic dynamical systems?
- **Basis in paper:** [inferred] The authors report that achieving high sparsity for the chaotic Ecosystem model was difficult and note that the method "can exhibit problematic variance... that may destabilize training."
- **Why unresolved:** The Ecosystem results showed that achieving compactness often degraded multi-step prediction accuracy, and the specific cause (training variance vs. model capacity) remains open.
- **What evidence would resolve it:** Applying variance-reduced sparsification techniques to the Ecosystem task and observing if compact models can retain high multi-step forecasting accuracy.

## Limitations
- The method's performance on noisy real-world data with complex feature correlations remains to be validated beyond the two real-world datasets tested
- Optimal β hyperparameter shows substantial task dependence, requiring validation for each new application
- Gate variance issues can cause training instability, a known problem in differentiable sparsification that remains partially unresolved

## Confidence

- **High confidence:** The differentiable sparsification mechanism is technically sound and well-established in the broader literature on Bayesian neural networks. The MDL framework provides principled regularization, though its task-specific optimality remains to be validated.
- **Medium confidence:** The synergy between forward connections and sparsification is demonstrated empirically but relies on overprovisioning assumptions that may not hold for all problem classes. The generalizability to complex real-world tasks requires further validation.
- **Medium confidence:** The quantitative results across benchmarks appear robust, though the relative performance gains depend heavily on hyperparameter tuning, particularly β.

## Next Checks

1. **Generalization stress test:** Apply the proposed architecture to a noisy real-world dataset with known domain constraints (e.g., financial time series or medical imaging features) and compare performance against established interpretable ML methods like decision trees and linear models with feature interactions.

2. **Gate behavior analysis:** Conduct a systematic study of gate decisiveness convergence across different network depths, widths, and initialization schemes. Measure gate entropy over training to identify conditions where gates fail to "harden" and characterize the resulting accuracy-sparsity tradeoffs.

3. **Architecture scaling study:** Evaluate the method on progressively larger Nguyen problems (F10-F15) and synthetic high-dimensional functions to determine the scalability limits of the forward connection approach and identify the point where trunk capacity becomes essential regardless of FC availability.