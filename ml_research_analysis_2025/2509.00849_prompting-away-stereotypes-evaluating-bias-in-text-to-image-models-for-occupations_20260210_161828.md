---
ver: rpa2
title: Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations
arxiv_id: '2509.00849'
source_url: https://arxiv.org/abs/2509.00849
tags:
- gender
- controlled
- race
- baseline
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates bias in text-to-image models by generating
  500+ images of five occupations using baseline and controlled prompts, then annotating
  for gender and race. Controlled prompts increased female representation in most
  models and diversified race portrayals, but effects varied widely: some models overcorrected
  into unrealistic uniformity while others showed little responsiveness.'
---

# Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations

## Quick Facts
- arXiv ID: 2509.00849
- Source URL: https://arxiv.org/abs/2509.00849
- Reference count: 40
- Primary result: Controlled prompts increase female representation and diversify race in most TTI models, but effects vary widely and some models overcorrect into unrealistic uniformity

## Executive Summary
This study evaluates bias in text-to-image models by generating 500+ images of five occupations using baseline and controlled prompts, then annotating for gender and race. Controlled prompts increased female representation in most models and diversified race portrayals, but effects varied widely: some models overcorrected into unrealistic uniformity while others showed little responsiveness. No overall quantitative improvement metric was reported, but the results indicate that prompt-based fairness interventions are model-specific and may require complementary strategies to achieve stable, balanced outcomes.

## Method Summary
The evaluation compares baseline prompts against controlled prompts that explicitly instruct models to ensure diversity across gender and ethnicity. Five TTI models (DALL·E 3, Gemini Imagen 4.0, FLUX.1-dev, SDXL Turbo, Grok-2) generate 10 images each for five occupations under both conditions, totaling approximately 500 images. Manual annotation by four domain experts labels each image for gender, race (Asian/Black/White), and stereotype alignment, with inter-annotator agreement scores of 0.82 (gender), 0.74 (race), and 0.88 (stereotype). Distributional analysis compares race composition and gender balance between conditions.

## Key Results
- Controlled prompts increased female representation in most models across occupations
- Race portrayals diversified in some models (Gemini, SDXL) but overcorrected into uniformity in others (Grok-2, DALL·E 3)
- Model responsiveness varied significantly: no single intervention strategy worked across architectures
- Some models showed little to no responsiveness to controlled prompts

## Why This Works (Mechanism)

### Mechanism 1
Explicit diversity instructions in prompts can shift demographic distributions in generated images, but effects are highly model-specific and inconsistent. Controlled prompts add explicit instructions that appear to modulate the model's attention toward underrepresented demographic tokens during sampling. However, how strongly this influences output depends on the underlying architecture and training. Models can interpret and act on abstract fairness instructions, but this capability varies across architectures.

### Mechanism 2
Different TTI architectures show systematically different responsiveness to fairness prompts, with no single intervention strategy working across models. The five models span autoregressive (DALL·E 3, Grok-2), diffusion (Imagen 4.0), hybrid (FLUX.1-dev), and speed-optimized latent diffusion (SDXL Turbo). Their varying responses suggest that prompt conditioning interacts differently with each architecture's sampling and attention mechanisms. Architecture-specific training and sampling procedures mediate how prompts influence demographic outputs.

### Mechanism 3
Prompt-based fairness interventions risk overcorrection, producing uniform rather than balanced demographic distributions. When fairness prompts shift outputs, they often do so in an uncalibrated way, flipping from one extreme to another. This suggests prompts act as coarse-grained biases rather than calibrated distributional controls. Models lack internal calibration mechanisms to distribute diversity proportionally; they instead amplify the prompt's signal toward a new dominant category.

## Foundational Learning

- **Text-to-Image Generation Architectures** (diffusion, autoregressive, hybrid): The study compares five models across different paradigms; understanding how each generates images from text is essential to interpreting their divergent responses to fairness prompts. *Quick check*: Can you explain why a diffusion model might respond differently to prompt conditioning than an autoregressive model?

- **Representational Bias in Generative Models**: The paper measures demographic skews in occupational portrayals; foundational knowledge of how training data and model architecture propagate bias is required to understand intervention effectiveness. *Quick check*: What are two upstream sources of demographic bias in TTI models, and why might they resist prompt-based correction?

- **Prompt Engineering for Controlled Generation**: The core intervention is a controlled prompt adding diversity instructions; understanding how prompts influence attention, sampling, and output distribution is critical. *Quick check*: What are the limitations of relying solely on natural language prompts to control demographic output distributions?

## Architecture Onboarding

- **Component map**: Prompt design module -> Multi-model generation layer -> Image storage with metadata -> Manual annotation by experts -> Agreement calculation -> Distributional analysis module
- **Critical path**: Prompt formulation → Model API calls → Image storage with metadata → Manual annotation by experts → Agreement calculation → Distributional comparison across conditions and models
- **Design tradeoffs**: Simplified demographic categories improve statistical power but exclude other groups; small sample size enables detailed annotation but limits statistical significance testing; manual annotation ensures reliability but doesn't scale; controlled prompts are model-agnostic but show inconsistent effectiveness
- **Failure signatures**: Overcorrection—controlled prompts produce 100% female or near-zero White representations; non-responsiveness—minimal shift between baseline and controlled conditions; high variability across roles—same model overcorrects for some occupations but not others
- **First 3 experiments**:
  1. Reproduce baseline vs. controlled comparison for one model and occupation using the open-sourced code and data; verify annotation agreement matches reported κ scores
  2. Test alternative prompt formulations (e.g., specifying target distribution percentages) on a responsive model (SDXL Turbo or Gemini) to assess whether calibration reduces overcorrection
  3. Extend evaluation to a sixth occupation not in the pilot (e.g., doctor, lawyer) across at least two models to test generalizability of observed patterns

## Open Questions the Paper Calls Out

- **Hybrid strategies combining prompt engineering with upstream interventions**: How do hybrid strategies combining prompt engineering with upstream interventions (balanced dataset curation, fine-tuning, fairness-aware sampling) compare to prompt-only approaches in achieving stable, balanced demographic outcomes across TTI models? The study only tested downstream prompt interventions, finding them model-specific and prone to overcorrection; upstream strategies were not evaluated.

- **Intersectional outcomes**: How do race × gender intersectional outcomes in TTI occupational portrayals differ from single-axis analyses, and what compounded disparities emerge? The study collapsed demographics into separate gender and race categories without analyzing how they interact.

- **Statistical significance**: Are the observed demographic shifts from controlled prompting statistically significant or attributable to random variability given small sample sizes (10 images per condition)? Only 10 images per occupation×prompt were generated, and results were reported as percentages without confidence intervals or significance testing.

- **Architectural explanations**: What architectural or training differences explain why some TTI models respond strongly to diversity prompts while others show limited responsiveness or overcorrect into uniformity? The results show "highly model-specific effects" but the paper does not investigate why different architectures respond differently to controlled prompting.

## Limitations

- No quantitative fairness improvement metric to assess whether controlled prompts are "better" overall versus simply shifting distributions unpredictably
- Small sample size (10 images per condition) and lack of statistical testing limit ability to distinguish systematic shifts from sampling variability
- Simplified demographic categories (Asian, Black, White only) exclude Indigenous, Hispanic/Latino, Middle Eastern, and multiracial individuals

## Confidence

- **Medium confidence**: Claims about prompt-based demographic shifts being model-specific and inconsistent
- **Medium confidence**: Claims about the overcorrection phenomenon
- **Low confidence**: Claims about overall bias reduction without aggregate fairness metrics or statistical significance testing

## Next Checks

1. **Statistical significance testing**: Perform permutation tests comparing baseline vs. controlled distributions for each model-occupation pair with effect sizes and confidence intervals

2. **Alternative prompt formulation experiment**: Test calibrated prompts that specify target demographic percentages on at least two responsive models to assess whether this reduces overcorrection while maintaining diversity improvements

3. **Extended occupation evaluation**: Generate and annotate images for two additional occupations (e.g., doctor, lawyer) across all five models to test generalizability of observed patterns of model-specific responsiveness and overcorrection