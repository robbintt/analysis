---
ver: rpa2
title: 'MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution
  of Misinformation with LLMs'
arxiv_id: '2509.16564'
source_url: https://arxiv.org/abs/2509.16564
tags:
- claim
- claims
- misinformation
- sources
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MPCG, a framework for modeling how misinformation
  evolves across ideological perspectives by iteratively reframing claims through
  role-playing personas. The method uses an uncensored LLM to generate claims over
  three rounds, conditioning each on the original claim and previous outputs to simulate
  evolving misinformation.
---

# MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs

## Quick Facts
- **arXiv ID:** 2509.16564
- **Source URL:** https://arxiv.org/abs/2509.16564
- **Reference count:** 33
- **Primary result:** Framework generates misinformation claims that evolve stylistically across ideological personas, causing up to 49.7% macro-F1 drop in classifier performance while preserving topical coherence

## Executive Summary
MPCG introduces a framework for modeling how misinformation evolves across ideological perspectives by iteratively reframing claims through role-playing personas. The method uses an uncensored LLM to generate claims over three rounds, conditioning each on the original claim and previous outputs to simulate evolving misinformation. Evaluation using human and GPT-4o-mini annotators showed strong agreement across most dimensions, with higher divergence in fluency judgments. Generated claims required greater cognitive effort than originals, exhibited persona-aligned sentiment and moral framing, and showed semantic drift across rounds while preserving topical coherence. Clustering and cosine similarity analyses confirmed gradual semantic change. Classification experiments revealed significant performance drops (up to 49.7% macro-F1) for common misinformation detectors on evolved claims, highlighting vulnerability to stylistic shifts.

## Method Summary
The MPCG framework generates misinformation claims across three rounds and three personas (Democrat, Republican, Moderate) using an uncensored LLM. It starts with 6,789 PolitiFact articles, extracts misinformation sources and fact-checking evidence using GPT-4o-mini, then applies a 5-step prompt chain to generate claims within a single context window. Each generation round conditions on all prior outputs, enabling controlled semantic drift. Generated claims are labeled using another LLM and evaluated through human/GPT-4o-mini annotation, cognitive metrics (FKGL, perplexity), emotional metrics (sentiment, MoralBERT), clustering analysis, and classification experiments across six encoder variants and two LLMs.

## Key Results
- Generated claims showed 77% feasibility for downstream tasks according to EQA tool evaluation
- Semantic drift was measurable with cosine similarity dropping from 0.699 (Round 1→2) to 0.618 (Original→Round 3)
- Persona-aligned moral framing was observed with Democrats scoring highest on Fairness (0.264) and Care (0.183), Republicans on Authority (0.106)
- Classifier performance dropped dramatically (up to 49.7% macro-F1) on evolved claims, with most degradation occurring in Round 1 and stabilizing thereafter
- Human and GPT-4o-mini annotators showed strong agreement on most dimensions, with JSD scores ranging from 0.043 (factuality) to 0.296 (fluency)

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Context Conditioning Drives Controlled Semantic Drift
- **Claim:** Iteratively conditioning each generation on all prior outputs produces gradual semantic drift while preserving topical coherence, rather than random divergence.
- **Mechanism:** Each round feeds the original claim, sources, AND all previously generated claims into the model. This cumulative context creates a trajectory where each reinterpretation builds on earlier shifts. Adjacent rounds maintain high similarity (~0.70) while distant rounds diverge more (Original→Round 3: 0.62), enabling controlled evolution.
- **Core assumption:** LLMs track and integrate prior-claim context rather than ignoring it or over-weighting recent claims.
- **Evidence anchors:** [abstract] "conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution"; [Table 12] Round 1→Round 2 cosine similarity = 0.699; Round 2→Round 3 = 0.698; but Original→Round 3 = 0.618; [corpus] Related work on simulating misinformation propagation (arXiv:2511.10384) confirms LLM personas can model user-level biases and trust heuristics in propagation chains
- **Break condition:** If adjacent-round similarity drops below ~0.5 or if semantic clusters fragment (claims from same source scattering across unrelated clusters), the cumulative conditioning is failing to produce controlled drift.

### Mechanism 2: Detailed Persona Specifications Yield Ideologically-Aligned Moral Framing
- **Claim:** Multi-source persona definitions grounded in political typologies produce measurably distinct moral framing patterns that align with theoretical predictions from Moral Foundations Theory.
- **Mechanism:** The Source Reasoning Prompt forces the model to adopt the persona's perspective before generating. Democrat personas (defined via Pew typology + Merriam-Webster) emphasize care/fairness foundations; Republican personas emphasize authority/subversion; Moderates produce neutral sentiment with loyalty emphasis. This pre-generation reasoning step channels ideological bias into the output.
- **Core assumption:** LLMs encode sufficient knowledge of political ideology to simulate distinct moral reasoning patterns when prompted.
- **Evidence anchors:** [abstract] "consistently reflect persona-aligned emotional and moral framing"; [Table 9] Democrats score highest on Fairness (0.264), Care (0.183), Cheating (0.183); Republicans score highest on Authority (0.106); Moderates show Loyalty emphasis (0.018); [corpus] Epistemic fragility work (arXiv:2511.22746) confirms prompt framing systematically modulates LLM outputs, supporting conditioning effects
- **Break condition:** If moral foundation scores converge across personas (e.g., all showing similar Authority scores), persona conditioning is not producing ideological differentiation.

### Mechanism 3: Stylistic Shift Without Semantic Collapse Breaks Classifier Generalization
- **Claim:** Claims that preserve topical meaning but shift in syntax, vocabulary complexity, and moral framing cause substantial detection failures because classifiers overfit to surface features of training distributions.
- **Mechanism:** Generated claims increase FKGL scores (original median 9.1 → Round 3 ~14.8) and reduce perplexity (less lexical diversity), creating distributionally-shifted inputs. Since classifiers train on original-claim distributions, they fail on stylistically-evolved variants. The 49.7% F1 drop occurs mainly at Round 1, then plateaus—suggesting classifiers are vulnerable to initial distribution shift but somewhat robust to further drift within the same topic.
- **Core assumption:** Performance degradation stems from legitimate stylistic evolution rather than incoherent or nonsensical outputs.
- **Evidence anchors:** [abstract] "macro-F1 performance drops of up to 49.7%"; [Table 10] DeBERTa V3 Large drops from 71.7% (Original) to 36.1% (Round 1), then stabilizes at 37.5-37.8%; [corpus] Related augmentation work (arXiv:2503.02328) shows LLM-generated COVID-19 misinformation variants have limited effectiveness for improving stance detection, suggesting augmentation doesn't automatically transfer
- **Break condition:** If F1 scores continue declining sharply through Rounds 2-3 (rather than plateauing), semantic collapse rather than stylistic shift may be the cause.

## Foundational Learning

- **Concept: Moral Foundations Theory (MFT)**
  - Why needed here: The paper quantifies persona alignment using MFT's ten dimensions. Understanding that liberals typically emphasize Care/Harm and Fairness/Cheating while conservatives emphasize Authority/Subversion is essential to interpret Table 9 results and validate the persona conditioning mechanism.
  - Quick check question: Why does the paper interpret Democrat claims scoring 0.264 on Fairness and Republican claims scoring 0.106 on Authority as evidence of successful persona alignment?

- **Concept: Uncensored/Safety-Removed LLMs**
  - Why needed here: Standard LLaMA-3.1-8B-Instruct rejected misinformation-generation prompts. The paper explicitly chose an uncensored variant (Lexi-Uncensored-V2) to enable claim generation. This is a critical infrastructure decision with ethical implications addressed in the paper's ethics section.
  - Quick check question: What specific capability does an uncensored model provide here, and what mitigation does the paper implement to prevent misuse of generated claims?

- **Concept: Semantic Similarity vs. Distributional Shift**
  - Why needed here: The paper's key claim is that claims "preserve topical coherence" while "evolving stylistically." Cosine similarity (semantic) stays relatively high (~0.62-0.70), but classifier performance drops dramatically. Understanding this distinction explains why the generated claims are both useful (still about the same topic) and challenging (look different to detectors).
  - Quick check question: If Round 3 claims had 0.85 cosine similarity to originals, would you expect higher or lower classifier performance degradation? Why?

## Architecture Onboarding

- **Component map:** PolitiFact scraper (22,408 articles) → GPT-4o-mini extraction (Misinformation Sources + Fact-Checking Evidence) → Label consolidation (6 → 3 classes) → 5-step prompt chain generation pipeline (Llama-3.1-8B-Lexi-Uncensored-V2) → LLM labeling (Llama-3.1-8B-Instruct) → Evaluation suite (human/GPT-4o-mini annotation, cognitive metrics, emotional metrics, clustering, classification)

- **Critical path:** The 5-step generation pipeline running on Llama-3.1-8B-Lexi-Uncensored-V2 is the throughput bottleneck (~2 days for 10,170 claims on A100). All five prompts must execute in one context window to maintain reasoning→claim→explanation coherence. The labeling pipeline (Llama-3.1-8B-Instruct) is faster but still substantial.

- **Design tradeoffs:**
  - **Three-round depth:** More rounds increase drift but also noise. Paper empirically found semantic change measurable by Round 3 without excessive incoherence.
  - **GPT-4o-mini for annotation:** Faster than humans but shows fluency-judgment bias (JSD 0.296) and may be overly lenient.
  - **PolitiFact scope:** U.S.-centric political claims; generalization to other domains (healthcare, non-U.S. contexts) is untested.
  - **Generated-claim labeling vs. gold labels:** Classification experiments use LLM-generated labels rather than human expert labels, introducing label noise.

- **Failure signatures:**
  - **Extraction contamination:** "Contaminated Sources" (18% in manual check) where debunking statements appear in misinformation sources—indicates GPT-4o-mini extraction prompt confusion
  - **Fluency degradation:** Claims rated "Adequate" or below by humans (49 cases) despite GPT-4o-mini rating "Excellent" (231 cases)—signals human-LLM judgment divergence
  - **Persona inconsistency:** Role-Playing Consistency scores < 3 on 5-point scale—persona definitions not being followed
  - **Excessive semantic drift:** Cosine similarity < 0.5 between Original and Round 1—cumulative conditioning producing divergence rather than evolution
  - **Claim length violations:** Output exceeding 20-word constraint—formatting prompt failure

- **First 3 experiments:**
  1. **Reproduction validation:** Select 5 PolitiFact articles; run full Round 1-3 generation for each persona; compute FKGL, sentiment distribution, and verify Democrat→negative/Republican→negative/Moderate→neutral pattern matches Table 8
  2. **Classifier stress test:** Train DeBERTa V3 Base on the paper's training split; evaluate on Original claims, then Round 1/2/3 claims; confirm F1 drop pattern (sharp Round 1 drop, plateau thereafter)
  3. **Ablation on persona grounding:** Generate claims with Role Descriptions removed; use GPT-4o-mini to rate Role-Playing Consistency; quantify score drop (paper reports 4.63→4.56) to validate persona specification contribution

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the MPCG framework generalize to multilingual and multimodal settings, and how does evolution differ in these contexts?
  - Basis in paper: [explicit] Future work includes "extending this framework to multilingual and multimodal settings can broaden its applicability and provide insights into how misinformation evolves in different settings."
  - Why unresolved: The current study is restricted to English text; visual or cross-cultural elements might alter semantic drift patterns, persona alignment strategies, and classifier vulnerability.
  - What evidence would resolve it: Comparative analysis of semantic drift and classifier robustness when MPCG is applied to image-text datasets or non-English political claims.

- **Open Question 2:** Can the framework effectively model the evolution of misinformation in non-political domains, such as healthcare?
  - Basis in paper: [explicit] The authors state future work should "extend MPCG to test for novel claims or misinformation from different domains, such as healthcare, to examine generalization beyond politics."
  - Why unresolved: The current evaluation relies exclusively on PolitiFact data and political personas; ideological reframing mechanisms may function differently for scientific topics where authority is derived from expertise rather than political identity.
  - What evidence would resolve it: Successful application of MPCG to health-specific datasets showing persona-aligned moral framing and measurable performance drops in health-focused misinformation detectors.

- **Open Question 3:** How can standardized automatic metrics be developed to evaluate generation quality while reducing reliance on subjective assessments?
  - Basis in paper: [explicit] Future work includes "developing standardized automatic metrics to evaluate generation quality, reducing reliance on subjective human and LLM assessments prone to judgment bias."
  - Why unresolved: The study relies on human annotators and GPT-4o-mini, which showed divergence in fluency judgments and are susceptible to human uncertainty and judgment bias.
  - What evidence would resolve it: Creation and validation of a reference-free metric that shows high correlation with human expert evaluation across dimensions like role-playing consistency and factuality without requiring human intervention.

## Limitations

- **Domain restriction:** The framework is limited to U.S. political misinformation from PolitiFact, restricting generalizability to other domains and cultural contexts.
- **Label noise:** Classification experiments use LLM-generated labels rather than human-annotated ground truth, potentially inflating performance degradation estimates.
- **Annotation contamination:** 18% of source extraction cases showed contamination where debunking statements appeared in misinformation sources, indicating pipeline robustness issues.

## Confidence

- **High Confidence:** Semantic drift mechanism (cosine similarity patterns across rounds are empirically measurable and consistent)
- **Medium Confidence:** Persona alignment effectiveness (moral foundation scores show systematic differences but may be influenced by model bias)
- **Medium Confidence:** Classifier vulnerability (F1 drops are substantial but evaluated on generated labels rather than gold-standard annotations)

## Next Checks

1. Conduct domain transfer validation by generating claims for non-political misinformation domains (health, finance) and measuring semantic drift and classifier vulnerability patterns
2. Implement human-annotated validation set for Round 3 claims to verify that classifier performance degradation persists under expert evaluation
3. Perform ablation study removing the Source Reasoning step to quantify its contribution to persona alignment and semantic drift control