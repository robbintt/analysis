---
ver: rpa2
title: 'LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation'
arxiv_id: '2509.09754'
source_url: https://arxiv.org/abs/2509.09754
tags:
- layer
- cache
- attention
- dynamic
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation

## Quick Facts
- arXiv ID: 2509.09754
- Source URL: https://arxiv.org/abs/2509.09754
- Reference count: 19
- Outperforms CAKE in latency and memory usage with comparable accuracy across LongBench, Ruler, and InfiniteBench benchmarks

## Executive Summary
LAVa introduces a unified framework for KV cache compression that dynamically allocates eviction budgets across both heads and layers in transformer models. The method uses a novel layer-wise scoring function that considers both the magnitude of value vectors and recent attention patterns to identify least-informative tokens. By combining dynamic head budgets (via global flattening within layers) and dynamic layer budgets (via entropy-based allocation), LAVa achieves better memory-accuracy trade-offs than existing methods, particularly excelling in latency-sensitive scenarios.

## Method Summary
LAVa implements a layer-wise KV cache eviction strategy using a composite scoring function: the maximum L1-norm of value vectors per head multiplied by the accumulated attention over a recent window. The method features dynamic head budgets allocated by flattening scores within each layer and selecting top entries globally, and dynamic layer budgets determined by normalized entropy of scores across layers. For Grouped-Query Attention, it conservatively retains tokens by taking maximum scores within groups. A max-pooling operation smooths the score vectors before eviction decisions. The framework also implements layer-by-layer recompression during prefilling, similar to CAKE, to optimize memory usage throughout the generation process.

## Key Results
- Outperforms SnapKV and CAKE baselines on LongBench with better latency-memory trade-offs
- Maintains competitive task accuracy across extraction, code, and summarization tasks
- Shows particular strength in reducing decoding latency while preserving model performance
- Effective across multiple model scales (7B, 14B, 32B) and context lengths (8k, 32k)

## Why This Works (Mechanism)
LAVa's effectiveness stems from its unified approach to cache compression that addresses both inter-head and inter-layer redundancy. The scoring function captures token importance through both value magnitude and recent attention patterns, while dynamic budget allocation ensures resources are directed where most needed. The entropy-based layer budgeting adapts to varying token importance distributions across layers, and the flattening approach for heads enables truly global optimization within each layer. The max-pooling operation adds robustness to the scoring, preventing premature eviction of potentially important tokens.

## Foundational Learning
- **Layer-wise KV cache compression**: Needed to reduce memory footprint during long-context inference; quick check: verify token retention affects downstream task performance
- **Dynamic budget allocation**: Required to adapt to varying token importance across heads and layers; quick check: compare static vs dynamic allocation performance
- **Grouped-Query Attention handling**: Essential for modern efficient transformer architectures; quick check: ensure GQA models show reasonable performance with conservative retention
- **Entropy-based resource allocation**: Used to distribute limited eviction budgets based on information content; quick check: verify entropy correlates with task-relevant information density

## Architecture Onboarding

**Component Map**: Input tokens -> Scoring function (value norm × attention) -> Maxpool-7 -> Dynamic head allocation (flattened) -> Dynamic layer allocation (entropy) -> Eviction decisions -> Compressed KV cache

**Critical Path**: Scoring computation → Maxpooling → Budget allocation → Token eviction → Memory optimization during prefilling

**Design Tradeoffs**: Global flattening within layers enables better head-level optimization but adds computational overhead; entropy-based layer allocation adapts to task needs but requires careful hyperparameter tuning; max-pooling adds robustness but may delay necessary evictions

**Failure Signatures**: High memory usage indicates layer recompression not triggering correctly; performance degradation suggests scoring window size inappropriate or dynamic allocation not activating; poor GQA performance indicates incorrect max-pooling within groups

**First Experiments**: 1) Implement scoring function with varying window sizes w and evaluate impact on compression quality; 2) Test dynamic vs static head allocation on a single layer to verify flattening approach; 3) Compare entropy-based layer allocation against uniform allocation on extraction tasks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The sliding window size w for attention accumulation is a critical hyperparameter not explicitly specified, requiring empirical tuning
- The interaction between retention constraints and dynamic budget allocation is unclear, potentially affecting theoretical guarantees
- Layer recompression timing during prefilling lacks implementation details, making memory optimization claims difficult to verify

## Confidence
- **High confidence**: Overall architectural approach, experimental setup, and baseline comparisons are clearly specified
- **Medium confidence**: Scoring function implementation details and layer recompression concept are reproducible but require experimentation
- **Low confidence**: The interaction between retention constraints and dynamic allocation, and exact prefilling recompression scheduling, are insufficiently specified

## Next Checks
1. Systematically vary the sliding window size w (16, 32, 64) and evaluate impact on both memory usage and task performance across benchmark categories
2. Implement and compare two variants: retention constraint operating independently vs. modifying budget allocation, measuring differences in compression ratios
3. Instrument the implementation to log layer recompression timing during prefilling and verify claimed memory benefits materialize