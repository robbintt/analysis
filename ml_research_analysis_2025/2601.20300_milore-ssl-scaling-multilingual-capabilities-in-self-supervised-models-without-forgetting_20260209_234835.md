---
ver: rpa2
title: 'MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without
  Forgetting'
arxiv_id: '2601.20300'
source_url: https://arxiv.org/abs/2601.20300
tags:
- languages
- training
- milore-ssl
- speech
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending self-supervised
  speech models to new languages without forgetting existing ones. The authors propose
  MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts
  (MoE) mechanism for efficient continual multilingual training.
---

# MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting

## Quick Facts
- arXiv ID: 2601.20300
- Source URL: https://arxiv.org/abs/2601.20300
- Reference count: 0
- Reduces Character Error Rates to 10.3% (English), 10.7% (Mandarin), and 11.0% (Cantonese) on CommonVoice using only 2.14% trainable parameters

## Executive Summary
This paper addresses the challenge of extending self-supervised speech models to new languages without forgetting existing ones. The authors propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. The method achieves strong performance in new languages and improves ability in existing ones using only 2.14% trainable parameters. On ML-SUPERB benchmarks, MiLorE-SSL reduces Character Error Rates to 10.3% (English), 10.7% (Mandarin), and 11.0% (Cantonese) on CommonVoice, outperforming both mHuBERT-147 and HuBERT Large. The model also achieves 99.40% accuracy on language identification tasks.

## Method Summary
MiLorE-SSL extends HuBERT-Large from English to Mandarin and Cantonese through a continual learning framework. The method replaces each Transformer FFN with MiLorE modules containing frozen FFN, 2 LoRA experts (rank=12), and a soft router. During training, the model jointly learns from new languages and replay data from existing languages (100 hours of English) to prevent catastrophic forgetting. The soft MoE routing enables flexible knowledge sharing across languages while the low-rank adaptation maintains parameter efficiency. Training uses K-means targets from mHuBERT-147 layer 9, with 400k steps at peak LR 1.5e-3. Fine-tuning for downstream tasks uses ESPnet with LR 1e-4, layer 22 for ASR and layer 21 for LID.

## Key Results
- Reduces CER to 10.3% (English), 10.7% (Mandarin), and 11.0% (Cantonese) on CommonVoice
- Achieves 99.40% accuracy on language identification tasks
- Uses only 2.14% trainable parameters compared to full fine-tuning
- Outperforms both mHuBERT-147 and HuBERT Large on multilingual benchmarks

## Why This Works (Mechanism)

### Mechanism 1: LoRA-based Expert Decomposition
Low-rank adaptation matrices enable efficient language-specific capacity expansion while preserving frozen backbone knowledge. Each expert E_i is parameterized as ΔW = B_i A_i where B ∈ R^{d_out × r}, A ∈ R^{r × d_in}, and r ≪ min(d_in, d_out). Only ~2.14% of total parameters are trainable.

### Mechanism 2: Soft Routing for Cross-Lingual Knowledge Sharing
Soft mixture-of-experts routing enables flexible language-specific specialization without hard assignment, reducing cross-lingual interference. Router computes p = softmax(W_r h_in), assigning weighted contributions from all experts per input.

### Mechanism 3: Limited Replay for Catastrophic Forgetting Mitigation
Joint training on new-language samples with a small replay buffer (100 hours) preserves prior language performance without full historical corpus access. Loss L is computed jointly over masked predictions for both new and replayed samples.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Sequential Training**
  - Why needed here: Understanding why sequential language addition degrades prior knowledge motivates the MoE + replay design.
  - Quick check question: If you fine-tune an English-only HuBERT on Mandarin without any mitigation, what happens to English WER?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: MiLorE-SSL's experts are LoRA modules; understanding weight decomposition B·A is essential for implementation.
  - Quick check question: Given d_in=768, d_out=3072, and rank r=12, how many parameters does one LoRA expert add?

- Concept: **Mixture-of-Experts Routing (Hard vs. Soft)**
  - Why needed here: The paper's "soft MoE" differs from Switch-Transformer-style hard routing; understanding the distinction clarifies why gradients flow to multiple experts.
  - Quick check question: In hard routing (top-1), what happens to expert gradients when that expert isn't selected?

## Architecture Onboarding

- Component map:
  Input -> Frozen FFN -> LoRA Experts (2) -> Soft Router -> Weighted Sum

- Critical path:
  1. Initialize: Load pretrained HuBERT, freeze all weights
  2. Insert MiLorE modules: Replace FFN in each Transformer block
  3. Initialize LoRA experts: B_i ~ N(0, σ), A_i = 0 (standard LoRA init)
  4. Initialize router: W_r ~ small random values
  5. Training: Forward pass computes soft routing weights, accumulates weighted expert outputs
  6. Loss: Masked prediction loss on new + replay data

- Design tradeoffs:
  - **Expert count vs. rank**: Fixed parameter budget—more experts means lower rank per expert. Paper finds 2 experts × rank=12 optimal.
  - **Replay size vs. forgetting**: 100 hours sufficient for 3 languages; scaling to more languages may require proportional replay.
  - **Soft vs. hard routing**: Soft routing adds compute (all experts evaluated) but stabilizes training; hard routing is faster but may cause gradient starvation.

- Failure signatures:
  - **English CER spikes to 20%+**: Replay data missing or incorrectly loaded
  - **All languages degrade uniformly**: LoRA rank too low or learning rate too high
  - **Router collapses to single expert**: Router initialization too large or insufficient training diversity
  - **Training instability**: MiniBatchKMeans targets may be misaligned

- First 3 experiments:
  1. **Sanity check**: Train MiLorE-SSL on Mandarin only (no replay), verify English CER degrades, then add 10-hour replay and confirm partial recovery.
  2. **Ablation sweep**: Run {-MoE, -Replay, -MoE-Replay} configurations on all 3 languages; verify patterns match Table 2.
  3. **Expert count analysis**: Train with (2, 4, 8) experts at fixed total parameters, plot per-language CER to confirm 2-expert optimum.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does MiLorE-SSL scale when extending to significantly more languages (e.g., 10, 50, or 100+ languages) in terms of performance, routing stability, and expert capacity requirements?
- **Open Question 2**: Can the layer-wise expert activation patterns observed (e.g., language-specific representations in layers 15-17, shared features in lower layers) be explicitly exploited to design layer-adaptive routing or architecture configurations?
- **Open Question 3**: What is the minimum effective replay data size required to prevent catastrophic forgetting, and how does this threshold vary with language similarity and model capacity?

## Limitations
- **Vocabulary size unknown**: The paper does not specify how many K-means clusters were used for target generation.
- **Limited out-of-domain validation**: Primary validation is on CommonVoice, with limited Fleurs benchmark testing.
- **Scaling constraints untested**: The 2.14% trainable parameter claim and 100-hour replay strategy are validated for only three languages.

## Confidence
- **High Confidence**: Catastrophic forgetting mitigation through limited replay (ablation shows English CER jumps from 10.3% to 26.6% without replay)
- **Medium Confidence**: Soft MoE routing contribution (supported by "-MoE" ablation and layer-wise activation analysis)
- **Medium Confidence**: LoRA-based parameter efficiency claim (mathematically sound with empirical support)
- **Low Confidence**: Claim that soft routing "reduces cross-lingual interference" (lacks direct experimental validation)

## Next Checks
1. **Vocabulary size verification**: Test MiLorE-SSL with 500, 1000, and 2000 K-means clusters to determine optimal target space size.
2. **Replay buffer scaling analysis**: Systematically vary replay buffer size (10h, 50h, 100h, 200h) while adding new languages.
3. **Soft vs. hard routing comparison**: Implement hard routing variant and compare performance, interference metrics, and computational efficiency.