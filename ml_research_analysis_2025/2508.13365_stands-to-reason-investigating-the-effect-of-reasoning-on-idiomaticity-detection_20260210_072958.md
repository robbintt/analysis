---
ver: rpa2
title: 'Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection'
arxiv_id: '2508.13365'
source_url: https://arxiv.org/abs/2508.13365
tags:
- reasoning
- performance
- idiomaticity
- which
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether reasoning models improve idiomaticity
  detection compared to standard LLMs. It evaluates DeepSeek-R1 distilled models (1.5B
  to 70B parameters) across four idiomaticity datasets (FLUTE, SemEval, MAGPIE, DICE)
  using chain-of-thought reasoning.
---

# Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection

## Quick Facts
- arXiv ID: 2508.13365
- Source URL: https://arxiv.org/abs/2508.13365
- Authors: Dylan Phelps; Rodrigo Wilkens; Edward Gow-Smith; Thomas Pickard; Maggie Mi; Aline Villavicencio
- Reference count: 8
- Primary result: Reasoning models show scale-dependent benefits for idiomaticity detection, with modest improvements for larger models (14B+) but performance degradation for smaller models when using math-tuned intermediate models.

## Executive Summary
This paper investigates whether chain-of-thought reasoning models improve idiomaticity detection compared to standard LLMs. The study evaluates DeepSeek-R1 distilled models across four idiomaticity datasets using chain-of-thought reasoning. Results show that reasoning benefits are scale-dependent, with larger models (14B+) showing modest improvements while smaller models initially perform worse with CoT reasoning. Manual analysis reveals that definition quality and reasoning quality are both critical factors, with larger models consistently producing accurate idiomatic definitions. The paper also demonstrates that using definitions from larger models as prompts can improve smaller model performance on definition-focused tasks.

## Method Summary
The study evaluates DeepSeek-R1 distilled models (1.5B to 70B parameters) on four idiomaticity datasets: FLUTE, SemEval, MAGPIE, and DICE. Models generate chain-of-thought reasoning before classification, with labels extracted using GPT-4o when parsing fails. The evaluation uses macro F1 score averaged across 5 runs with different random seeds, conducted using vLLM library with Q6_K_M quantization on single A100 80GB GPU. The paper also conducts manual annotation to separate understanding (definition quality) from reasoning quality, and tests definition-based knowledge distillation by using larger model definitions as prompts for smaller models.

## Key Results
- Reasoning benefits are scale-dependent, with modest improvements for larger models (14B, 32B, 70B) but initial performance degradation for smaller models (1.5B, 7B) when using math-tuned intermediate models.
- Larger models (32B) achieve the best overall performance and consistently produce accurate idiomatic definitions.
- Definition-based knowledge distillation improves FLUTE performance by 0.069 macro F1 for smallest models but shows no significant effect on DICE.
- CoT length does not consistently correlate with accuracy across models and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning benefits for idiomaticity detection are scale-dependent, with positive effects emerging only at larger model sizes (14B+).
- Mechanism: Larger models possess sufficient parametric knowledge to generate accurate idiomatic definitions, which then scaffolds valid chain-of-thought reasoning. Smaller models lack this foundational knowledge, so CoT generation degrades performance or provides minimal benefit.
- Core assumption: The paper assumes that definition quality proxies for idiomatic understanding, and that understanding must precede successful disambiguation reasoning.
- Evidence anchors:
  - [abstract] "For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements."
  - [section 4.2.2] "The larger models understanding of the expressions is generally high when it is both correct and incorrect, with more variation being identified in the reasoning capabilities... the main factor to whether the model is correct or incorrect appears to be the reasoning performance."
  - [corpus] Weak direct corpus support for this specific mechanism; related work on reasoning (e.g., arXiv:2503.01064, arXiv:2505.17407) examines reasoning generally but not idiomaticity-specific scaling.
- Break condition: If smaller models were given high-quality definitions in-context (as in Section 6), this mechanism should partially compensate—performance should improve on tasks where definition knowledge is the bottleneck.

### Mechanism 2
- Claim: Idiomaticity detection requires a two-stage process: definition generation (understanding) followed by contextual reasoning (disambiguation), where the first stage constrains the second.
- Mechanism: The model first retrieves or generates an idiomatic definition, then reasons about whether the context supports idiomatic or literal usage. If the definition is inaccurate or missing, downstream reasoning is compromised regardless of reasoning capacity.
- Core assumption: Assumes that CoT outputs faithfully reflect the model's actual reasoning process; the paper acknowledges prior work showing CoT can be unfaithful (Turpin et al., 2023; Lyu et al., 2023).
- Evidence anchors:
  - [abstract] "a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning."
  - [section 4] "This categorization allows us to separate the effects of the model's understanding of the expression and context and its reasoning ability."
  - [section 4.2.2] "The 1.5B parameter model shows a low level of both understanding and reasoning ability, with most of its correct outputs being due to chance."
  - [corpus] No direct corpus evidence on this two-stage mechanism for idiomaticity; corpus papers address reasoning in other domains (temporal, scientific, code).
- Break condition: If models could skip accurate definition and still achieve high accuracy through surface cues or shortcuts, this mechanism would not hold. DICE was designed to prevent such shortcuts.

### Mechanism 3
- Claim: Definition-based knowledge distillation can transfer idiomatic knowledge to smaller models, but effectiveness is task-dependent and limited when contextual reasoning (not just definition knowledge) is the bottleneck.
- Mechanism: Larger models generate accurate definitions; appending these to prompts for smaller models augments their knowledge. This helps on FLUTE (NLI-style task) where definition knowledge is often sufficient, but fails on DICE where context-dependent disambiguation is required.
- Core assumption: Assumes that the bottleneck for smaller models on some tasks is missing definitional knowledge rather than reasoning capacity; assumes definitions transfer without inducing reasoning shortcuts.
- Evidence anchors:
  - [abstract] "Using definitions generated by larger models as prompts for smaller models improves performance on FLUTE by an average of 0.069 macro F1 for the smallest models... However, this approach shows no significant effect on DICE, indicating limitations when contextual understanding is critical."
  - [section 6.1] "We suggest that the difference in effectiveness on FLUTE and DICE comes from the fact that, as suggested by (Mi et al., 2024), DICE requires models to attend to the surrounding context to make decisions."
  - [corpus] No direct corpus evidence on definition-based distillation for idiomaticity; related knowledge injection methods exist (e.g., arXiv:2507.14887 MEKiT) but target different tasks.
- Break condition: If adding definitions hurt performance on any dataset, the mechanism would suggest negative transfer or interference. The paper reports no significant degradation.

## Foundational Learning

- Concept: Chain-of-thought (CoT) reasoning in LLMs
  - Why needed here: The paper evaluates CoT-producing reasoning models; understanding what CoT is and its typical benefits/harms is essential to interpret results.
  - Quick check question: Can you explain why CoT might help on some tasks but hurt on others, particularly when knowledge is missing?

- Concept: Potentially idiomatic expressions (PIEs) and literal vs. idiomatic disambiguation
  - Why needed here: The core task requires determining whether an expression is used idiomatically or literally based on context, which presupposes understanding both senses.
  - Quick check question: Given "kick the bucket" in "He kicked the bucket after a long illness" vs. "He kicked the bucket across the yard," what determines the correct classification?

- Concept: Knowledge distillation and model scaling
  - Why needed here: The paper uses definitions from larger models to improve smaller models, a form of knowledge transfer; understanding distillation helps evaluate this intervention.
  - Quick check question: Why might distillation via definitions work better than direct fine-tuning for some tasks but not others?

## Architecture Onboarding

- Component map: Input (PIE + context + optional definition) -> Reasoning model (DeepSeek-R1 distilled variants) -> CoT generation -> Classification -> Label extraction (GPT-4o if needed) -> Evaluation (Macro F1)

- Critical path:
  1. Prompt construction with PIE and context (optionally with definition)
  2. Model generates CoT followed by label
  3. Extract label; if unparsable, use GPT-4o to extract
  4. Compute Macro F1 against gold labels
  5. For analysis: manually annotate understanding (definition quality) and reasoning quality

- Design tradeoffs:
  - Math-tuning vs. general reasoning: Math-tuning severely degrades idiomaticity performance; reasoning-tuning partially recovers it for small models but not to base levels.
  - Definition augmentation: Helps on FLUTE (definition-focused), neutral on DICE (context-focused), safe to apply without regression risk.
  - CoT length: No consistent correlation with accuracy; longer CoT does not guarantee better performance (Table 4, Figure 2).

- Failure signatures:
  - Small models (1.5B-7B): Produce inaccurate or missing definitions; CoT degrades performance vs. base models; near-random on harder datasets.
  - Large models on DICE: Good definitions but struggle with contextual disambiguation; high "valid prediction" rate on gold-label mismatches suggests dataset noise or ambiguity.
  - Cross-lingual (SemEval PT/GL): Reasoning shows unclear or negative effects vs. English.

- First 3 experiments:
  1. Replicate the definition-augmentation experiment on FLUTE and DICE with 1.5B and 7B models to verify the +0.069 F1 finding and null effect on DICE.
  2. Run manual annotation on a sample of CoTs to separate understanding (definition quality) from reasoning quality, confirming that reasoning quality predicts correctness.
  3. Test whether CoT length correlates with accuracy on a new idiomaticity dataset; expect no consistent relationship per Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would alternative reasoning frameworks specifically designed for figurative language processing outperform general-purpose CoT reasoning on idiomaticity detection?
- Basis in paper: [explicit] The conclusion states: "Future work should explore alternative reasoning frameworks specifically designed for figurative language processing, perhaps incorporating more structured approaches to context interpretation."
- Why unresolved: The paper only tests general-purpose reasoning models; structured approaches tailored to idiomatic disambiguation remain unexplored.
- What evidence would resolve it: A comparison of general CoT against frameworks with explicit context-interpretation modules on the same datasets.

### Open Question 2
- Question: Why does providing definitions improve small-model performance on FLUTE but not on DICE?
- Basis in paper: [inferred] The paper reports definition-prompting improves FLUTE by 0.069 F1 for 1.5B/7B models but shows no significant effect on DICE, attributing this to DICE's contextual requirements—yet the mechanism remains unclear.
- Why unresolved: The paper hypothesizes that DICE requires attending to context rather than just definitions, but does not empirically isolate the cause.
- What evidence would resolve it: Controlled experiments varying context complexity while holding definition availability constant.

### Open Question 3
- Question: Why is the reasoning benefit inconsistent across model families (e.g., Llama-70B underperforms Qwen on SemEval English)?
- Basis in paper: [explicit] Appendix C notes the reasoning variant of Llama-3.3 70B performs 0.037 F1 worse on SemEval English than its non-reasoning counterpart, and suggests "further work could investigate a reason for this inconsistency."
- Why unresolved: The paper observes the inconsistency but does not analyze architectural or training-data differences that might explain it.
- What evidence would resolve it: Cross-family ablation studies controlling for training data composition and architecture.

## Limitations
- The exact generation parameters (temperature, top_p, max_tokens) were not specified, which could influence CoT quality and consistency across runs.
- The causal mechanism linking reasoning quality to accuracy remains observational rather than experimentally validated.
- The definition-augmentation approach shows promising results on FLUTE but fails on DICE, suggesting task-specific limitations that weren't deeply explored.

## Confidence

- **High confidence**: The observation that larger models (32B) achieve the best overall performance and produce consistently accurate idiomatic definitions. The finding that definition-augmentation improves FLUTE performance (+0.069 F1) is also well-supported.
- **Medium confidence**: The claim that reasoning benefits are scale-dependent, with positive effects emerging only at larger model sizes. While the data supports this trend, the mechanism connecting model size to reasoning quality could be more rigorously established.
- **Low confidence**: The assertion that the quality of reasoning (rather than just understanding) is the primary factor determining correctness. This conclusion relies heavily on manual annotation, which may have subjective elements not fully addressed.

## Next Checks

1. **Replicate the definition-augmentation experiment**: Run the FLUTE and DICE experiments with 1.5B and 7B models to verify the +0.069 F1 improvement on FLUTE and confirm the null effect on DICE, ensuring the dataset-specific limitations are reproducible.

2. **Validate the understanding vs. reasoning distinction**: Conduct systematic manual annotation on a stratified sample of CoTs to confirm that reasoning quality (not just definition accuracy) predicts correctness, and measure inter-annotator agreement to assess reliability.

3. **Test CoT length correlation**: Generate new idiomaticity detection results on an additional dataset to empirically verify whether CoT length correlates with accuracy, as Table 4 suggests no consistent relationship exists.