---
ver: rpa2
title: 'Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs'
arxiv_id: '2512.19937'
source_url: https://arxiv.org/abs/2512.19937
tags:
- player
- decoding
- personality
- human
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents interpolative decoding as a method to parameterize\
  \ personality traits in LLMs without creating unique prompts for each profile. The\
  \ core idea is to represent each trait as a pair of opposed prompts and use an interpolation\
  \ parameter \u03BB to modulate behavior along that trait spectrum."
---

# Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs

## Quick Facts
- arXiv ID: 2512.19937
- Source URL: https://arxiv.org/abs/2512.19937
- Reference count: 34
- The paper presents interpolative decoding as a method to parameterize personality traits in LLMs without creating unique prompts for each profile.

## Executive Summary
This paper introduces interpolative decoding as a technique to modulate LLM behavior along personality trait spectra using opposed prompt pairs and an interpolation parameter λ. The method enables smooth transitions between trait extremes without requiring unique prompts for each profile. The authors demonstrate that this approach reliably produces personality-correlated behavior in LLMs, with dictator game payouts showing strong correlations with honesty-humility (r = 0.74) and agreeableness (r = 0.49) traits that match human psychological research patterns.

## Method Summary
The core method represents each personality trait as a pair of opposed prompts and uses an interpolation parameter λ to simulate behavior along that dimension. Two decoding schemes are employed: mixture decoding (weighted averaging of token distributions) and contrastive decoding (anchor-based distributional difference amplification). The approach was validated across multiple dimensions including Big Five personality traits, economic game decisions, and information integration preferences in collaborative games. For twinning experiments, pairwise runoff comparisons among top-n move sets were used to induce action distributions that minimize perplexity against observed human actions.

## Key Results
- Dictator game payouts correlated positively with HEXACO honesty-humility (r = 0.74) and agreeableness (r = 0.49), matching human psychological research patterns
- Contrastive decoding achieved the lowest perplexity (average 3.95-4.94) when favoring tactical over social cues in twinning tasks
- Interpolative decoding reliably modulated LLM behavior in line with personality models across Big Five trait dimensions

## Why This Works (Mechanism)

### Mechanism 1
Weighted averaging of next-token distributions from opposed trait prompts produces intermediate behavioral outcomes. Mixture decoding computes P'(t) = (1/Z)(λP_A(t) + (1-λ)P_B(t)), where P_A and P_B are distributions conditioned on high- and low-trait prompts. The interpolation parameter λ ∈ [0,1] shifts probability mass between extremes. This assumes trait dimensions distribute behavioral tendencies across vocabulary in a manner that linearly interpolates.

### Mechanism 2
Amplifying distributional differences between opposed prompts yields smoother trait modulation than weighted averaging. Contrastive decoding computes P'(t) = softmax(P_A(t) + λ(P_A(t) - P_B(t))), anchoring on one prompt's distribution and adding a scaled difference term. λ < 0 favors the anchor; λ > 0 amplifies contrast. This assumes the difference vector isolates trait-relevant vocabulary shifts relatively independent of anchor-specific content.

### Mechanism 3
Behavioral outcomes correlate with interpolative λ values in patterns matching human psychological research. The LLM's internal representation of trait-descriptive language, shaped by pretraining on human text, causes λ-modulated outputs to reflect human-like trait-behavior correlations when tasked with decision scenarios. This assumes LLMs encode trait-behavior associations from pretraining data that generalize to novel task framings.

## Foundational Learning

- **Autoregressive next-token prediction**: Interpolative decoding operates on output distributions at each generation step; understanding how LLMs assign probabilities to vocabulary is prerequisite. Quick check: Given prompt "You are generous," which token has higher probability: "share" or "keep"?

- **HEXACO / Big Five personality models**: The method parameterizes traits from these models; knowing what honesty-humility or agreeableness measure enables proper prompt construction and result interpretation. Quick check: Which HEXACO trait is most likely to correlate with dictator game payout?

- **Perplexity as distributional fit**: Twinning evaluation uses perplexity to measure how well the induced action distribution explains observed human actions. Quick check: If perplexity = 4.0 on a vocabulary of 100 actions, is this better or worse than perplexity = 10.0?

## Architecture Onboarding

- **Component map**: Prompt pair construction -> Dual forward passes -> Interpolation module -> Task framing layer -> Twinning optimizer
- **Critical path**: 1) Author opposed trait prompts (concrete behavioral descriptions preferred), 2) Run dual forward passes per token, 3) Apply contrastive decoding (preferred over mixture), 4) Extract structured output, 5) If twinning: compute perplexity and iterate λ
- **Design tradeoffs**: Mixture vs. contrastive (mixture is symmetric but jumpy; contrastive is smoother but requires anchor selection); λ range (contrastive effective range is roughly [-30, +30]; mixture is [0, 1]); Prompt concreteness (concrete behavioral descriptions outperform abstract trait adjectives)
- **Failure signatures**: λ manipulation produces no behavioral change (prompts may not encode distinct trait extremes); Perplexity plateaus during twinning (trait dimensions may not capture task-relevant variance); Inconsistent rankings across prompt permutations (model is sensitive to irrelevant syntactic variation)
- **First 3 experiments**: 1) Validate interpolation: For a single Big Five trait, vary λ across 5+ points and compute Spearman correlation between λ and inventory scores (target: ρ > 0.7), 2) Replicate dictator game: Run honesty-humility interpolation at λ ∈ {-30, -10, 10, 30}, record payouts (confirm positive correlation and rank ordering), 3) Twinning sanity check: Given a synthetic "human" (LLM with known λ), attempt to recover λ via perplexity minimization (assess regression error)

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies heavily on prompt quality and distinctiveness; if prompt pairs are not truly orthogonal or lack concrete behavioral descriptions, interpolation will fail to produce meaningful trait modulation
- The method's effectiveness across different LLM architectures is not established; results may not transfer to non-transformer models or those with different pretraining objectives
- Reliance on external corpora for human behavioral correlations introduces potential generalization gaps if the LLM's pretraining distribution differs significantly from human datasets used in psychological research

## Confidence

- **High confidence**: The mechanism of contrastive vs. mixture decoding (supported by direct experimental comparison showing smoother trait modulation and lower perplexity in twinning tasks)
- **Medium confidence**: Generalization to human psychological research patterns (correlation with HEXACO traits matches human studies, but direct validation on human subjects is absent)
- **Medium confidence**: Prompt concreteness advantage (cited literature supports this, but not directly tested across all experiments in the paper)

## Next Checks
1. **Cross-architecture validation**: Test interpolative decoding on at least two additional LLM families (e.g., LLaMA, Claude) to verify that trait-behavior correlations persist across different pretraining corpora and model sizes
2. **Human-subject replication**: Conduct a direct comparison where the same dictator game and personality assessments are administered to human participants and the LLM under identical conditions, measuring correlation stability across the two populations
3. **Prompt robustness analysis**: Systematically vary prompt concreteness (abstract vs. behavioral descriptions) and sentence ordering permutations to quantify their impact on trait modulation consistency and correlation strength