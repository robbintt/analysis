---
ver: rpa2
title: 'JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query
  Generation'
arxiv_id: '2510.19310'
source_url: https://arxiv.org/abs/2510.19310
tags:
- claim
- query
- claims
- hallucination
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses factual hallucination detection in LLM-generated
  responses. The core method JointCQ jointly generates factual claims and search queries
  in a single step, using criteria-guided filtering to train on high-quality claim-query
  pairs.
---

# JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation

## Quick Facts
- arXiv ID: 2510.19310
- Source URL: https://arxiv.org/abs/2510.19310
- Authors: Fan Xu; Huixuan Zhang; Zhenliang Zhang; Jiahao Wang; Xiaojan Wan
- Reference count: 40
- One-line primary result: JointCQ achieves 74.20% accuracy on ANAH-overall and 80.58% on HalluQA by jointly generating claims and queries with criteria-guided filtering

## Executive Summary
This paper introduces JointCQ, a framework for factual hallucination detection in LLM-generated responses that jointly generates verifiable claims and search queries in a single step. The method uses criteria-guided filtering to train on high-quality claim-query pairs, significantly improving detection accuracy compared to sequential pipelines. Experiments on open-domain QA benchmarks demonstrate state-of-the-art performance while maintaining efficiency through a single search call per judgement.

## Method Summary
JointCQ is a 5-stage framework that addresses factual hallucination detection by first generating diverse answers from multiple LLMs, then synthesizing claims and queries via 3-shot prompting with Qwen3-32B, followed by rigorous criteria-guided filtering using LLM evaluators, and finally fine-tuning Qwen2.5-14B on the filtered data. The system evaluates claims on entailment, coverage, and decontextualization, while queries are assessed on relevance, conciseness, and usability. During inference, the joint generator produces claim-query pairs that are searched via Google API and verified by Qwen3-14B to classify responses as Correct, Hallucinated, or Unverifiable.

## Key Results
- Achieves 74.20% accuracy on ANAH-overall benchmark
- Reaches 80.58% accuracy on HalluQA benchmark
- Outperforms strong baselines through joint generation architecture
- Maintains efficiency with only one search call per judgement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint generation of claim-query pairs reduces error propagation compared to sequential pipelines.
- **Mechanism:** By finetuning a single model to output both the verifiable claim and the search query simultaneously, the model learns an implicit mapping between semantic facts and retrieval keywords. This likely mitigates the "context loss" that occurs when a decomposed claim is passed blindly to a separate query generator.
- **Core assumption:** The optimal search query for a claim is derivable from the claim's atomic facts and the original context in a way that a single inference pass can capture more effectively than two disjoint steps.
- **Evidence anchors:**
  - [abstract]: "...jointly generates factual claims and search queries in a single step..."
  - [section 5.4]: "...replacing the Claim-Query Generator with separate synthesis... indicate a clear drop in performance."
- **Break condition:** If the model over-generalizes queries, losing the specific nuances of the claim, or if the training data lacks diversity in query styles.

### Mechanism 2
- **Claim:** Criteria-guided filtering of synthetic data is the primary driver of model performance.
- **Mechanism:** The framework uses a "dual evaluation" protocol (Entailment, Coverage, Decontextualization for claims; Relevance, Conciseness, Usability for queries). This filters out noisy training pairs where the claim is unsubstantiated or the query is ineffective, forcing the student model to learn only high-quality alignments.
- **Core assumption:** LLM-based evaluators (specifically Qwen3-32B here) can reliably act as judges for these specific criteria without introducing systematic bias.
- **Evidence anchors:**
  - [section 3.3]: "...apply a dual evaluation procedure that filters claims and queries independently."
  - [section 5.3]: "omitting filtering in any configuration results in a performance decline..."
- **Break condition:** If the filtering criteria are too restrictive, leading to data starvation, or if the "Decontextualization" criterion creates claims that are too complex for the downstream verifier.

### Mechanism 3
- **Claim:** Separating query generation from claim extraction improves retrieval precision.
- **Mechanism:** While claims are optimized for semantic completeness (Decontextualization), queries are optimized for search engines (Relevance/Usability). Decoupling these allows the query to be concise and keyword-focused, whereas a decontextualized claim might be a complex, long-form sentence that degrades search ranking.
- **Core assumption:** Search engines (like Google via Serper API) function better with targeted queries than with full, complex declarative sentences.
- **Evidence anchors:**
  - [section 3.3.1]: "We introduce an additional step by generating a separate query... optimized for external information retrieval."
  - [section 5.2]: "...replacing the generated queries with claims... performance drops noticeably... decline of 4.82 points in overall hallucination F1."
- **Break condition:** If the query generation drifts too far from the specific factual constraint of the claim, retrieving irrelevant evidence.

## Foundational Learning

- **Concept:** **Decontextualization**
  - **Why needed here:** The paper emphasizes that claims must be "understandable on their own." This is critical for the verifier, which sees the claim in isolation. Without this, a claim like "He was born in 1998" is unverifiable.
  - **Quick check question:** Given the answer "The Acqua Felice aqueduct was completed in 1586," does the claim "It was completed in 1586" pass the decontextualization criterion? (Answer: No, "It" is ambiguous).

- **Concept:** **Hallucination Taxonomy (Correct, Hallucinated, Unverifiable)**
  - **Why needed here:** The system is not binary. "Unverifiable" is a valid output state when evidence is lacking (Section 2.1). This is crucial for evaluating the *usability* of the detection pipeline, not just its accuracy.
  - **Quick check question:** If a search engine returns no results for a claim, should the label be "Hallucinated"? (Answer: No, according to the paper, it is "Unverifiable").

- **Concept:** **Information Retrieval (IR) Evaluation Metrics**
  - **Why needed here:** The paper defines query quality via "Relevance" and "Usability." Understanding basic IR concepts helps in diagnosing why a verifier failedâ€”was the claim wrong, or was the query just bad?
  - **Quick check question:** Does a "concise" query always yield better results than a detailed natural language question? (Answer: Not always, but the paper assumes conciseness aids the specific search engine used).

## Architecture Onboarding

- **Component map:** Answer Generation -> Claim/Query Synthesis (Qwen3-32B) -> Criteria-Guided Filter -> Fine-tuning (Qwen2.5-14B) -> JointCQ Generator -> Searcher (Google API) -> Verifier (Qwen3-14B) -> Label

- **Critical path:** The **Criteria-Guided Filtering** stage (Section 3.3). If the filter admits claims with poor "Entailment" or queries with poor "Usability," the JointCQ generator will learn to produce low-quality search terms, causing the downstream verifier to fail.

- **Design tradeoffs:**
  - **Efficiency vs. Granularity:** The system uses 1 search per judgment (Efficient) but performs fine-grained claim-level detection (Accurate).
  - **Model Size:** Uses a 14B model for generation (accessible) but requires a 32B model for the *training* data synthesis/filtering (resource-heavy offline cost).

- **Failure signatures:**
  - **High "Unverifiable" rate:** Likely a failure in Query "Usability" or "Relevance," meaning the search engine returned irrelevant snippets.
  - **Low Recall (Missing hallucinations):** Likely a failure in Claim "Coverage," where the generator failed to extract a specific atomic fact from the answer.

- **First 3 experiments:**
  1. **Ablate the Filter:** Train the JointCQ generator with *no* filtering vs. *full* filtering to establish the upper bound of data quality impact (replicating Table 2).
  2. **Query vs. Claim Search:** Replace the generated queries with the raw claims themselves to verify the 4.82 F1 drop mentioned in Section 5.2 on your own domain data.
  3. **Verifier Stress Test:** Keep the generator fixed, but swap the Verifier model (e.g., use a smaller 7B model) to determine if the verification stage is the bottleneck or if the claims/queries are robust enough for weaker verifiers.

## Open Questions the Paper Calls Out
- Can the JointCQ framework be effectively adapted for non-QA generative tasks such as text summarization or code generation? The authors state in the "Limitations" section that the pipeline is designed for open-domain QA and "extending the framework to other NLP tasks would require additional adaptation and validation."
- How can the framework mitigate the inherent biases and limitations of commercial search engines to ensure reliability in sensitive domains? The "Limitations" section notes that reliance on Google Search exposes the system to "inherent limitations of the search engine," despite the Introduction emphasizing healthcare and finance as target areas.
- Is the "model pool" strategy for answer generation necessary, or can high-quality data be synthesized using a single strong model? Section 3.2.1 utilizes a pool of four different LLMs to ensure answer diversity, but it is not tested if the criteria-guided filtering alone is sufficient to create high-quality training data from a single source.

## Limitations
- The framework relies on commercial search engines (Google API), which may not be accessible or reliable for specialized domains like healthcare and finance.
- The criteria-guided filtering depends on LLM-based evaluators whose reliability and potential bias are assumed but not independently validated.
- The system's generalization to languages and domains beyond open-domain QA remains untested, as the paper focuses on Chinese-English benchmarks.

## Confidence
- **High**: The sequential pipeline ablation (Section 5.4) demonstrating that joint generation outperforms separate claim-query synthesis is methodologically sound and well-supported by evidence.
- **Medium**: The filtering criteria and their thresholds are explicitly defined, but the actual evaluation prompts and scoring rubrics for the LLM judges are not provided, making independent validation difficult.
- **Low**: The generalization of results to domains outside open-domain QA is untested. The paper's focus on Chinese-English benchmarks leaves the performance on other languages or specialized domains (e.g., biomedical) as an open question.

## Next Checks
1. **Cross-LLM verification**: Run the entire JointCQ pipeline using a different verifier model (e.g., GPT-4o-mini) on a held-out sample of 100 ANAH claims to measure agreement with the Qwen3-14B verifier and quantify potential model-specific bias.
2. **Query robustness test**: Systematically corrupt 10% of the generated queries (e.g., by removing key nouns or adding noise) and measure the downstream hallucination detection accuracy to establish the sensitivity of the system to query quality.
3. **Human evaluation of filtering**: Have human annotators independently assess a random sample of 200 synthetic claim-query pairs (pre- and post-filtering) to measure inter-annotator agreement with the LLM judges on the three claim criteria and three query criteria.