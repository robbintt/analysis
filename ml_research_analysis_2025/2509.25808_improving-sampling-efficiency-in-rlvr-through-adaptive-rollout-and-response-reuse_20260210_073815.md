---
ver: rpa2
title: Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response
  Reuse
arxiv_id: '2509.25808'
source_url: https://arxiv.org/abs/2509.25808
tags:
- responses
- response
- arxiv
- training
- ar3po
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vanishing advantage problem in Group Relative
  Policy Optimization (GRPO), a reinforcement learning from verifiable rewards (RLVR)
  algorithm for large language models. The issue arises when all responses in a group
  are either correct or incorrect, causing advantages to collapse to zero and halting
  training.
---

# Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse

## Quick Facts
- arXiv ID: 2509.25808
- Source URL: https://arxiv.org/abs/2509.25808
- Reference count: 7
- Primary result: AR3PO reduces rollout cost by up to 4.2× while matching DAPO performance on mathematical benchmarks

## Executive Summary
This paper addresses the vanishing advantage problem in Group Relative Policy Optimization (GRPO) for reinforcement learning from verifiable rewards (RLVR). When all responses in a group are either correct or incorrect, advantages collapse to zero, halting training. The authors propose AR3PO, which combines adaptive rollout (dynamically allocating more responses to difficult prompts) and response reuse (leveraging previously correct responses when current rollouts yield all-incorrect groups). Experiments on 7B, 8B, and 32B models across multiple mathematical benchmarks show AR3PO outperforms GRPO and matches DAPO while reducing rollout cost by up to 4.2×.

## Method Summary
AR3PO combines two key techniques to address vanishing advantage in GRPO. Adaptive rollout uses a multi-stage generation process where prompts yielding at least one correct response are removed from the pool, naturally concentrating compute on difficult prompts. Response reuse maintains a replay buffer of correct responses and injects one into prompts that yield no correct responses after adaptive rollout, providing training signals even for all-incorrect groups. The method addresses importance ratio issues through probability recomputation or gradient stopping. The algorithm operates on DAPO-Math dataset with binary rewards from a math verifier, using VERL framework with GRPO objective and token-level importance ratios.

## Key Results
- Reduces average responses per prompt from 8 (GRPO) to 5.7 while maintaining performance
- Matches DAPO performance with 4.2× less computation on mathematical benchmarks
- Reduces zero-reward prompt ratio from ~0.3 to <0.2 through response reuse
- Outperforms GRPO and DAPO across 7B, 8B, and 32B models on multiple mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Rollout for Difficulty-Weighted Sampling
Allocates more responses to difficult prompts while reducing generation on easy prompts improves sampling efficiency. Multi-stage rollout divides response generation into S stages, removing prompts with correct responses after each stage. This concentrates compute on prompts where the model struggles, implicitly implementing difficulty-based weighting. Core assumption: success probability correlates with prompt difficulty for the current policy. Evidence shows most difficult prompts receive ~7 responses vs ~4 for easiest prompts. Break condition: if early-stage success rates don't correlate with final difficulty rankings.

### Mechanism 2: Response Reuse with Off-Policy Reward Injection
Reusing previously correct responses provides training signal when current rollouts yield all-incorrect groups. Maintains replay buffer of correct responses, sampling one when needed. Two variants: recompute token probabilities under current policy to reduce importance ratio variance, or stop gradient on reused response and train only on on-policy samples. Core assumption: past correct solutions remain informative despite policy differences. Evidence shows reduction in zero-reward prompts from 0.3 to <0.2. Break condition: severe policy drift makes stale responses misleading.

### Mechanism 3: Vanishing Advantage Mitigation via Non-Zero Variance Enforcement
Ensures non-zero reward variance within each response group prevents gradient collapse. GRPO normalizes rewards within groups; when all rewards are identical, advantages collapse to zero. AR3PO combines adaptive rollout (increases chance of mixed correct/incorrect) and response reuse (injects correct response into all-incorrect groups) to maintain variance. Core assumption: non-zero advantage variance is necessary for effective learning. Evidence: adaptive rollout and response reuse together maintain training signals. Break condition: variance maintained but advantages point in wrong direction.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: AR3PO builds directly on GRPO's objective; understanding baseline reward normalization is essential to see why vanishing advantage occurs
  - Quick check: Given 8 responses where 6 are correct and 2 are incorrect, what would advantage values be under Equation 1?

- **Concept: Importance Sampling in Policy Gradient**
  - Why needed: Response reuse involves off-policy samples; importance ratio r(θ') = πθ'(o|x)/πθ(o|x) and its variance is central to understanding probability recomputation and gradient stopping
  - Quick check: If reused response had token probability 0.8 but current policy assigns 0.1, what happens to importance ratio and why is this problematic?

- **Concept: Verifiable Rewards in RLVR**
  - Why needed: The entire algorithm depends on binary outcome rewards (correct/incorrect) from a verifier. Understanding sparse, outcome-based rewards clarifies why obtaining mixed success groups is difficult
  - Quick check: Why might math verifier that only checks final answers cause more frequent vanishing advantage than one giving partial credit for reasoning steps?

## Architecture Onboarding

- **Component map**: [Prompt Batch] → [Adaptive Rollout (S stages, k responses/stage)] → [Verifier] → Correct? → Remove from pool → [Remaining Prompts] → [Response Reuse from Buffer B] → [Advantage Computation] → [Policy Update] → [Update Buffer B with new correct responses]

- **Critical path**: Adaptive rollout loop (stages 1 to S) determines responses per prompt. Response reuse triggered for prompts failing all stages. Advantage computation depends on obtaining at least one correct response per group (fresh or reused).

- **Design tradeoffs**: S stages × k responses vs. fixed G responses (more stages increase adaptivity but add overhead); Off-policy training vs. gradient stopping (Option I improves learning but adds compute, Option II is cheaper); Buffer size (unbounded growth could cause memory issues).

- **Failure signatures**: Zero-reward prompt ratio doesn't decrease during training (check buffer population and sampling logic); Average responses per prompt approaches maximum (verify verifier correctness and early-stage removal logic); Training becomes unstable after response reuse (importance ratios may be extreme, try Option II).

- **First 3 experiments**: 1) Ablation on adaptive rollout alone - measure average responses per prompt and zero-reward ratio vs. vanilla GRPO; 2) Ablation on response reuse strategies - compare direct rollout replay vs. probability recomputation vs. gradient stopping on validation set; 3) Scaling test - replicate 7B results, attempt 32B with matching hyperparameters, monitor sampling efficiency gains.

## Open Questions the Paper Calls Out
1. How can the AR3PO framework be effectively adapted for LLM agent settings involving multi-turn interactions rather than single-step reasoning?
2. What specific mechanisms can further improve the efficiency of trajectory sampling within the adaptive rollout framework?
3. Does the efficiency of response reuse hold for domains with non-binary or noisy verifiers, such as code generation with test case suites?
4. Does the bias introduced by recomputing token probabilities for reused responses constrain long-horizon training stability?

## Limitations
- Response reuse relies on assumption that correct responses from older policies remain valuable, but policy drift and staleness aren't quantified
- Adaptive rollout heuristic assumes success rate correlates with difficulty but lacks empirical validation of this correlation
- Paper doesn't specify replay buffer size limits, potentially leading to unbounded memory growth and stale responses dominating training

## Confidence
- High confidence: Adaptive rollout reduces average responses per prompt from 8 to 5.7 while maintaining performance (supported by Figure 2d and ablation in Table 2)
- Medium confidence: Response reuse maintains performance while further reducing zero-reward prompts (Figure 2c shows reduction but causal attribution is mixed with adaptive rollout effects)
- Low confidence: Claim that AR3PO matches DAPO performance with 4.2× less computation is difficult to verify due to unclear stopping criteria and potential hyperparameter differences

## Next Checks
1. **Response reuse age analysis**: Track age of reused responses and measure correlation between reuse age and training effectiveness to validate whether stale responses remain useful
2. **Difficulty correlation validation**: Measure correlation between early-stage success rates and final success rates across all stages to test whether adaptive rollout correctly identifies difficult prompts
3. **Buffer size sensitivity study**: Run experiments with different replay buffer sizes to determine optimal capacity balancing reuse benefits against stale response accumulation and memory constraints