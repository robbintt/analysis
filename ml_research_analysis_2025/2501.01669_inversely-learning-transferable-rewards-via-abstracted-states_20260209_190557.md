---
ver: rpa2
title: Inversely Learning Transferable Rewards via Abstracted States
arxiv_id: '2501.01669'
source_url: https://arxiv.org/abs/2501.01669
tags:
- reward
- source
- target
- state
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning transferable reward
  functions across aligned but different task instances, which is important for generalizing
  learned behaviors to new environments without additional demonstrations. The authors
  propose T raIRL, a method that learns an abstract state space shared across multiple
  source tasks using a multi-head VAE architecture, and then learns an abstract reward
  function over this space.
---

# Inversely Learning Transferable Rewards via Abstracted States

## Quick Facts
- **arXiv ID**: 2501.01669
- **Source URL**: https://arxiv.org/abs/2501.01669
- **Reference count**: 40
- **Primary result**: TraIRL learns abstract reward functions transferable across aligned but different task instances without demonstrations in target tasks.

## Executive Summary
This paper addresses the challenge of learning reward functions that generalize across aligned but different task instances. The authors propose TraIRL, which learns an abstract state space shared across multiple source tasks using a multi-head VAE architecture, then learns an abstract reward function over this space. By abstracting away task-specific details and focusing on invariant features, the learned reward can be transferred to previously unseen target tasks. The method is evaluated on MuJoCo and Assistive Gym environments, showing successful policy learning in target tasks without access to demonstrations there.

## Method Summary
TraIRL learns transferable rewards by first training a multi-head VAE to create an abstract state space from expert trajectories across multiple source tasks. The shared encoder maps ground states to a latent space while task-specific decoders reconstruct task-specific states. A WGAN-GP discriminator operates on abstract states to impose optimality awareness by distinguishing expert from learner behavior. The reward function is then learned via covariance-based gradient estimation, which decouples it from the encoder. During transfer, a policy is trained in the target task using only the frozen abstract reward function.

## Key Results
- TraIRL successfully transfers learned rewards to target tasks without demonstrations
- Multi-head VAE creates abstract states where source tasks intermingle while expert/learner behaviors remain separable
- The method outperforms baselines including GAIL and AIRL in transfer settings
- Theoretical analysis proves conditions under which reward transfer is possible

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-head VAE learns task-invariant abstract states by forcing a shared encoder to reconstruct multiple source tasks through task-specific decoders.
- **Mechanism**: The shared encoder p_ϕ(z|s) maps ground states to a latent space, while n separate decoders {q_ψᵢ} reconstruct task-specific states. The ELBO objective balances reconstruction accuracy with KL regularization, pushing the encoder to capture only features common across tasks since it cannot specialize to any single task's dynamics.
- **Core assumption**: Source tasks share underlying structure (e.g., "torso behavior" is similar regardless of which legs are disabled).
- **Evidence anchors**:
  - [abstract]: "single encoder is coupled with multiple decoders, one for each source instance"
  - [section 4.2]: Equation 2 shows multi-task VAE objective summing across n tasks
  - [corpus]: "Learning Contextually-Adaptive Rewards via Calibrated Features" similarly assumes high-level preferences remain constant across contexts
- **Break condition**: When source tasks have no shared structure (e.g., completely different action spaces or goals), the encoder cannot find useful common representations.

### Mechanism 2
- **Claim**: Discriminator-based structuring imposes optimality awareness onto the abstract state space, enabling it to distinguish expert from learner behavior.
- **Mechanism**: A WGAN-GP discriminator D_ω(z) operates on abstract states, trained to maximize the 1-Wasserstein distance between expert and learner abstract state densities. The gradient penalty λ_GP ensures the discriminator satisfies the 1-Lipschitz constraint required by Kantorovich-Rubinstein duality.
- **Core assumption**: Expert and learner behaviors are separable in the abstract space (structural alignment from Theorem 2, Eq. 8).
- **Evidence anchors**:
  - [section 4.3]: Equation 3 defines multi-task WGAN-GP objective
  - [section 5.1.2, Table 3]: Abstraction yields smaller W₁ distance (0.36-0.79) vs ground states (1.37-2.97), demonstrating better alignment
  - [corpus]: Weak direct evidence; neighboring papers focus on different reward learning approaches
- **Break condition**: When λ_GP is too small (violates Lipschitz constraint) or too large (over-constrains learning), Theorem 2's theoretical guarantees break.

### Mechanism 3
- **Claim**: Covariance-based gradient estimation distills transferable rewards by decoupling the reward function from the encoder.
- **Mechanism**: Theorem 1 shows ∇_θ L_F(θ) = Σᵢ cov_{s,z}(D_ω(z), ∇_θ R_θ(z)). The covariance captures only dominant structure reflecting expert-learner differences, ignoring outliers. Since gradients are taken only w.r.t. θ, the encoder remains fixed during reward optimization, preventing overfitting to source tasks.
- **Core assumption**: The discriminator captures meaningful expert-learner separation in abstract space.
- **Evidence anchors**:
  - [section 4.4]: Equation 5 and Theorem 1 explicitly derive the covariance gradient
  - [section 4.4]: "decoupling the reward function from the encoder ensures it operates on a fixed abstracted space"
  - [corpus]: No direct corpus evidence for this specific covariance mechanism
- **Break condition**: When learner and expert distributions diverge significantly, the gradient signal provides limited supervision (noted in Appendix B.1).

## Foundational Learning

- **Concept: Maximum Causal Entropy IRL / f-IRL**
  - Why needed here: TraIRL builds on f-IRL's state marginal matching framework, replacing the f-divergence with 1-Wasserstein distance.
  - Quick check question: Can you explain why matching state marginals (rather than state-action pairs) might improve transferability?

- **Concept: Variational Autoencoders (ELBO, reparameterization)**
  - Why needed here: The multi-head VAE is the core abstraction mechanism; understanding the balance between reconstruction loss and KL regularization is critical.
  - Quick check question: What happens to the latent space if λ_D (KL weight) is set too high vs. too low?

- **Concept: Wasserstein Distance and WGAN-GP**
  - Why needed here: The discriminator must approximate W₁ distance to provide valid optimality signals; gradient penalty enforces the Lipschitz constraint.
  - Quick check question: Why does the gradient penalty sample points along straight lines between expert and learner states?

## Architecture Onboarding

- **Component map**:
  Expert trajectories (n source tasks) -> Shared Encoder p_ϕ -> Abstract states z -> Task-specific Decoders q_ψᵢ (reconstruction) + Discriminator D_ω (optimality separation) + Reward function R_θ (learned via covariance) -> SAC policy training in target task

- **Critical path**:
  1. Collect 50 expert trajectories from ≥2 source tasks with different dynamics
  2. Train VAE + discriminator jointly (Eq. 6) until abstract states show task mixing (verify with t-SNE)
  3. Train reward function R_θ using covariance gradients (Theorem 1)
  4. Transfer: Run SAC in target task using only R_θ ∘ ϕ as reward

- **Design tradeoffs**:
  - **λ_GP** (Table 11): Too low → Lipschitz violation, inaccurate W₁; too high → over-constrained discriminator. Paper finds λ_GP=10 optimal for MuJoCo.
  - **λ_D** (Table 12): Controls latent space compactness. λ_D=0.1 balances generalization vs. expressiveness.
  - **Abstraction dimension** (Table 5): Lower dimensions enforce more compression but may lose task-relevant features. Paper uses 4-16 dimensions.
  - **Number of source tasks**: More sources → more transferable rewards (Appendix D.10), but harder training balance.

- **Failure signatures**:
  - **Single source task**: Rewards don't transfer (Table 13 shows 81-85% performance drop)
  - **Structural alignment violation**: When target task's optimal states are outside source distribution (Table 18: target drops to 152 vs. expert 1655)
  - **Missing LV_AE**: Source performance maintained but target drops 81% (Table 17)
  - **Abstract states cluster by task**: Indicates encoder learned task-specific rather than invariant features (compare Fig. 3a vs. 3b)

- **First 3 experiments**:
  1. **Sanity check**: Train on 2 HalfCheetah variants (rear/front disabled), visualize t-SNE of abstract states. Verify: expert/learner separated, source tasks intermingled.
  2. **Hyperparameter sweep**: Vary λ_GP ∈ {1, 5, 10, 100} and λ_D ∈ {0.05, 0.1, 0.25, 0.5}. Plot target task cumulative reward vs. hyperparameter values.
  3. **Ablation**: Remove each component (LV_AE, LWGAN, L_F) individually. Confirm: removing discriminator or reward loss causes complete failure; removing VAE hurts transfer specifically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the transferred abstract reward function be efficiently fine-tuned to improve its fit for a broader set of target tasks?
- Basis: [explicit] The conclusion states, "Future work could investigate general ways of quickly fine-tuning the transferred reward function to improve its fit for a broader set of target tasks."
- Why unresolved: The current method focuses on transferring a fixed reward for zero-shot learning; it does not provide a mechanism for adapting the reward if the target task deviates significantly from the source manifold.
- What evidence would resolve it: A fine-tuning algorithm that converges faster than learning from scratch and improves upon the baseline TraIRL performance in diverse target environments.

### Open Question 2
- Question: How can the framework ensure that task-specific critical features are not suppressed during the state abstraction process?
- Basis: [explicit] The Limitations section notes, "If the abstraction suppresses task-specific features that are critical for solving the target task, the resulting reward may fail to induce effective policies."
- Why unresolved: The VAE is trained to maximize reconstruction and shared structure, which naturally filters out "noise" that might actually be critical features for specific downstream tasks.
- What evidence would resolve it: A modification to the encoder or loss function that retains high sensitivity to task-critical dimensions (verified via semantic analysis) while maintaining transferability.

### Open Question 3
- Question: Can the optimization stability be improved to reduce the sensitivity of the method to the balancing of its three loss components?
- Basis: [explicit] The Limitations section highlights that "TraIRL introduces multiple interacting components... Tuning these across tasks can be non-trivial" and balancing the losses is challenging.
- Why unresolved: The method relies on a linear combination of losses (VAE, WGAN, Reward) with fixed hyperparameters (λ_VAE, λ_WGAN, λ_F), which may not generalize well across different domain complexities.
- What evidence would resolve it: An adaptive weighting scheme or theoretical analysis identifying optimal loss balance ratios that remain stable across the evaluated domains.

## Limitations

- **Single source task failure**: The method requires at least two source tasks; with only one source, transfer performance drops by 81-85%
- **Structural alignment requirement**: Transfer fails when target task's optimal states lie outside the distribution of abstract states from source tasks
- **Hyperparameter sensitivity**: Performance depends critically on balancing λ_GP, λ_D, and other hyperparameters across different domains

## Confidence

- **High confidence**: The multi-head VAE effectively learns abstract representations when source tasks share meaningful structure (Mechanism 1, evidenced by t-SNE visualizations showing intermingled source task clusters)
- **Medium confidence**: The discriminator-based structuring successfully imposes optimality awareness (Mechanism 2), though this depends critically on the λ_GP hyperparameter being set correctly
- **Low confidence**: The covariance-based gradient estimation provides robust transferable rewards (Mechanism 3), as this mechanism lacks direct corpus evidence and depends heavily on the discriminator's quality

## Next Checks

1. **Structural alignment stress test**: Systematically vary the similarity between source and target tasks (e.g., different robot morphologies) to map the boundary where Theorem 2's assumptions break down
2. **Hyperparameter sensitivity analysis**: Conduct a comprehensive sweep of update ratios between the IRL components and learner policy to identify optimal training schedules
3. **Real-world applicability assessment**: Evaluate TraIRL on a physical robotics task with genuine dynamics differences (e.g., different friction coefficients or payload weights) to test transfer beyond controlled simulation environments