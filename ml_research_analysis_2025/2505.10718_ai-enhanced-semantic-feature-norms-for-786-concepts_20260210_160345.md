---
ver: rpa2
title: AI-enhanced semantic feature norms for 786 concepts
arxiv_id: '2505.10718'
source_url: https://arxiv.org/abs/2505.10718
tags:
- feature
- human
- semantic
- norms
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to generate large-scale semantic
  feature norms by augmenting human-generated data with large language models (LLMs),
  creating an AI-enhanced dataset called NOVA. The method involves eliciting features
  from humans for 786 concrete concepts, then using LLMs to verify and expand these
  features across all concept-feature pairs.
---

# AI-enhanced semantic feature norms for 786 concepts

## Quick Facts
- **arXiv ID**: 2505.10718
- **Source URL**: https://arxiv.org/abs/2505.10718
- **Reference count**: 11
- **Primary result**: AI-augmented semantic norms achieved 86.20% agreement on human triadic similarity judgments versus 60.40% for word embeddings

## Executive Summary
This paper introduces NOVA, an AI-enhanced semantic feature norm dataset for 786 concrete concepts created by augmenting human-generated features with large language models. The method involves eliciting features from humans, then using LLMs to verify and expand these features across all concept-feature pairs. The resulting dataset exhibits significantly higher feature density and overlap compared to human-only norms. The AI-enhanced norms outperformed both human-only norms and word-embedding models in predicting human semantic similarity judgments, demonstrating that AI-augmented norms capture richer semantic structure relevant to human cognition.

## Method Summary
The approach involved three key steps: First, human participants generated feature lists for 786 concrete concepts. Second, a "feature collapse" step reduced synonymous features using GPT-3 embeddings and clustering. Third, a two-stage LLM verification process was implemented - initially using Flan-T5 XXL to predict feature-concept validity across all pairs, followed by GPT-4o re-verification of all "True" predictions to filter false positives. The final dataset contains approximately 6.5 million concept-feature pairs with verified features.

## Key Results
- AI-enhanced norms achieved 86.20% agreement on human triadic similarity judgments versus 60.40% for word embeddings
- Feature density increased significantly with AI augmentation compared to human-only norms
- The dataset showed higher overlap in feature sharing between concepts than traditional human-only norms

## Why This Works (Mechanism)
The approach works by leveraging LLMs to systematically expand and verify semantic features across a large concept space. By combining human intuition (for initial feature generation) with LLM scalability (for comprehensive verification), the method captures both the nuanced understanding humans bring to semantic concepts and the exhaustive coverage that would be impractical with human annotation alone. The two-stage verification process helps mitigate the hallucination problem common in LLMs by requiring consensus between two different models.

## Foundational Learning
- **Semantic feature norms**: Why needed - fundamental to cognitive science models of conceptual knowledge; Quick check - dataset should predict human similarity judgments
- **Crowdsourcing methodology**: Why needed - scalable human data collection; Quick check - inter-rater agreement metrics
- **LLM-based verification**: Why needed - automate validation across millions of pairs; Quick check - d' sensitivity score > 3.0
- **Feature collapse/clustering**: Why needed - reduce redundancy while preserving semantic distinctions; Quick check - silhouette score or similar clustering metric
- **Triadic similarity judgments**: Why needed - robust measure of semantic relatedness; Quick check - agreement with established benchmarks

## Architecture Onboarding

**Component map**: Human feature collection -> Feature collapse -> LLM verification (Flan-T5) -> Re-verification (GPT-4o) -> Final dataset

**Critical path**: The verification pipeline is the critical path - any error in the LLM predictions or re-verification step directly impacts the quality of the final dataset. The feature collapse step is also crucial as it determines the granularity of features that will be verified.

**Design tradeoffs**: 
- Feature density vs. accuracy tradeoff (more features from LLM vs. potential noise)
- Computational cost vs. verification thoroughness (single vs. dual LLM verification)
- Granularity of features (collapsed vs. fine-grained distinctions)

**Failure signatures**: 
- Low d' scores indicate poor model discrimination between true/false feature pairs
- Accuracy near 60% on triadic judgments suggests the norms aren't capturing human-relevant semantic structure
- Disproportionate false positives indicate the re-verification step is inadequate

**3 first experiments**:
1. Verify the feature collapse step by comparing human-annotated feature similarity before and after clustering
2. Test the Flan-T5 verification model on a held-out validation set to confirm d' > 3.0
3. Conduct pilot triadic similarity judgments with a small subset to ensure the enhanced norms capture meaningful distinctions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do AI-enhanced semantic norms better predict neural representations of concepts in the human brain compared to human-only norms or word embeddings?
- **Basis in paper**: [explicit] The discussion states that NOVA "unlock[s] the potential to better understand both the cognitive and the neural bases of semantic memory" and cites relevant neural work (Clarke & Tyler, 2014; Cox et al., 2024).
- **Why unresolved**: The paper only validated NOVA against behavioral similarity judgments, not neural data. It remains untested whether the richer feature structure corresponds more closely to brain activity patterns.
- **What evidence would resolve it**: fMRI or EEG representational similarity analysis comparing NOVA vectors, human-only norms, and word embeddings against neural responses during concept processing tasks.

### Open Question 2
- **Question**: Can the AI-enhanced norming approach be extended to abstract concepts, or does it only work for concrete objects?
- **Basis in paper**: [inferred] The paper explicitly limited scope to "786 concrete object concepts" and notes that word embeddings "lack transparent interpretability... at least for concrete objects," implying abstract concepts may differ fundamentally.
- **Why unresolved**: Abstract concepts (e.g., "justice," "freedom") have less clear perceptual features and may rely more on relational or emotional properties, which the current feature-verification paradigm may not capture well.
- **What evidence would resolve it**: Replicating the full norming pipeline with abstract concept stimuli and testing whether AI-enhanced norms predict human similarity judgments for abstract concept triplets.

### Open Question 3
- **Question**: What systematic biases might LLMs introduce into semantic norms, even when validated against human judgments?
- **Basis in paper**: [inferred] The paper acknowledges LLMs "frequently confabulate properties that are untrue (the well-documented 'hallucination problem')" and shows dâ€² > 3.0, but false positives from the re-verification pipeline could systematically distort certain semantic domains.
- **Why unresolved**: Validation used only 6,122 unanimous human judgments; the remaining ~6.4M pairs rely entirely on LLM agreement. Domain-specific or feature-type biases (e.g., toward prototypical features) could persist undetected.
- **What evidence would resolve it**: Targeted human verification of concept-feature pairs stratified by semantic domain, feature type, and LLM confidence levels to identify systematic discrepancies.

## Limitations
- The methodology inherits potential biases from the original human-generated feature norms
- The study relies on specific LLM architectures (Flan-T5 XXL, GPT-4o) limiting generalizability
- Evaluation focuses primarily on similarity judgments without testing broader cognitive task applications

## Confidence

**High confidence**: The AI-enhanced dataset demonstrates superior feature density and coverage compared to human-only norms, and the overall methodology for LLM augmentation is sound.

**Medium confidence**: The specific accuracy metrics (86.20% triadic judgment agreement) are robust, but the relative advantage over word-embedding models may vary with different semantic tasks or datasets.

**Medium confidence**: The claim that AI-augmented norms capture "richer semantic structure relevant to human cognition" is supported by the similarity judgment task but would benefit from validation in diverse cognitive science applications.

## Next Checks

1. **Downstream task validation**: Test the AI-enhanced norms on additional cognitive tasks beyond similarity judgments (e.g., concept categorization, property verification latency predictions) to assess broader ecological validity.

2. **Cross-model robustness**: Repeat the feature imputation process using alternative LLM architectures (e.g., GPT-4, Claude) to evaluate the stability of results across different models.

3. **Bias and error analysis**: Conduct a systematic error analysis of false positives/negatives in the AI-augmented norms, comparing them to known human biases or cultural variations in feature norms to identify potential systematic errors introduced by the LLM augmentation process.