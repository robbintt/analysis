---
ver: rpa2
title: Thinking Preference Optimization
arxiv_id: '2502.13173'
source_url: https://arxiv.org/abs/2502.13173
tags:
- thinkpo
- reasoning
- arxiv
- wait
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thinking Preference Optimization (ThinkPO),
  a post-SFT method that enhances long chain-of-thought reasoning in small language
  models without requiring new long CoT responses. ThinkPO leverages short CoT responses
  as negative samples and existing long CoT responses as positive samples, applying
  direct preference optimization to encourage longer, more detailed reasoning.
---

# Thinking Preference Optimization

## Quick Facts
- arXiv ID: 2502.13173
- Source URL: https://arxiv.org/abs/2502.13173
- Reference count: 40
- Primary result: ThinkPO improves small LLM math reasoning accuracy by 8.6% and output length by 25.9% over SFT alone

## Executive Summary
Thinking Preference Optimization (ThinkPO) is a post-SFT method that enhances long chain-of-thought reasoning in small language models without requiring new long CoT response data. The method leverages existing long CoT responses as positive examples and short CoT responses (generated by a base model) as negative examples, applying direct preference optimization to encourage longer, more detailed reasoning. Experiments show ThinkPO effectively overcomes performance plateaus in multi-epoch SFT training and improves math reasoning accuracy across different model sizes.

## Method Summary
ThinkPO uses a two-stage approach: first, supervised fine-tuning (SFT) on long CoT data for one epoch, then direct preference optimization (DPO) using long CoT responses as "chosen" and short CoT responses as "rejected." The short CoT responses are generated by a base model (Qwen2.5-Math-7B-Instruct) for the same questions, filtered to ensure correctness, and paired with long responses. The DPO training uses hyperparameters including batch_size=48, β=0.01, and learning rates ranging from 1e-7 to 5e-7 depending on the model. The method is evaluated on mathematical reasoning benchmarks including MATH500, AIME2024, and GPQA-Diamond.

## Key Results
- Improves math reasoning accuracy by 8.6% and output length by 25.9% over SFT alone
- Increases DeepSeek-R1-Distill-Qwen-7B's MATH500 accuracy from 87.4% to 91.2%
- Effectively overcomes performance plateaus in multi-epoch SFT training
- Works across different model sizes (1B, 3B, 7B)

## Why This Works (Mechanism)
ThinkPO works by leveraging preference optimization to shift the model's output distribution toward longer, more detailed reasoning patterns. By using short CoT responses as negative samples, the method creates a preference signal that explicitly rewards length and detail in reasoning chains. The DPO loss function optimizes the model to prefer outputs that match the distribution of long CoT responses over those that match short CoT responses, effectively learning to generate more comprehensive reasoning without requiring new long CoT data collection.

## Foundational Learning
- **Supervised Fine-Tuning (SFT)**: Fine-tuning a model on labeled data where inputs map to desired outputs; needed to establish baseline reasoning capability before preference optimization
- **Direct Preference Optimization (DPO)**: A preference learning method that optimizes the model to prefer one response over another without requiring a reward model; needed to leverage existing long/short CoT pairs
- **Chain-of-Thought (CoT) Reasoning**: A prompting technique where models generate intermediate reasoning steps; needed as the target behavior ThinkPO aims to enhance
- **Dataset Filtering**: Process of selecting high-quality training examples; needed to ensure short CoT responses are correct and comparable to long responses
- **Length-based Preference Signals**: Using response length as a proxy for reasoning quality; needed as the primary optimization target

## Architecture Onboarding

**Component Map**: Base Model -> SFT Training -> DPO Training -> Evaluated Model

**Critical Path**: The critical path involves generating short CoT responses for all questions, filtering them for correctness, pairing with long CoT responses, then running DPO with carefully tuned hyperparameters (β, learning rate, temperature).

**Design Tradeoffs**: ThinkPO trades increased inference-time computational costs (longer outputs) for improved reasoning accuracy. The method requires careful hyperparameter tuning, particularly for β and learning rate, and the optimal length difference between chosen and rejected samples is dataset-dependent.

**Failure Signatures**: 
- If accuracy decreases after ThinkPO, check if β is too high or learning rate too aggressive
- If output length doesn't increase, verify that short CoT responses are sufficiently different from long ones
- If model quality degrades, ensure SFT has plateaued before applying ThinkPO

**3 First Experiments**:
1. Run SFT for 1 epoch on Bespoke-Stratos-Dataset and verify accuracy plateau
2. Generate short CoT responses and filter for correctness, then check length distribution differences
3. Run DPO with baseline hyperparameters and monitor loss convergence

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Relies on proprietary dataset (Bespoke-Stratos-Dataset) not publicly available
- Comparison to DeepSeek-R1-Distill-Qwen-7B may be confounded by training-test overlap
- Ablation study on negative sampling strategies limited to only 4 conditions
- Method not evaluated on non-mathematical reasoning domains
- High evaluation temperature (0.7) may inflate performance metrics

## Confidence
- **High Confidence**: The core methodology of using short CoT as negative samples in DPO is clearly described and experimentally validated
- **Medium Confidence**: Improvements in reasoning quality beyond length are plausible but rely heavily on accuracy metrics
- **Low Confidence**: Specific numerical improvements are difficult to verify due to dataset unavailability and potential training-test overlap

## Next Checks
1. Retrain ThinkPO using only publicly available long CoT datasets to verify improvements without training-test overlap
2. Systematically test negative sampling strategy across 6-8 conditions varying length difference thresholds
3. Evaluate ThinkPO-trained models on non-mathematical reasoning tasks (code generation, commonsense reasoning) to verify cross-domain generalization