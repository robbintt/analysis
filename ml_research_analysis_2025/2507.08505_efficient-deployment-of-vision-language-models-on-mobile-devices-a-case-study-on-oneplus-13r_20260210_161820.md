---
ver: rpa2
title: 'Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study
  on OnePlus 13R'
arxiv_id: '2507.08505'
source_url: https://arxiv.org/abs/2507.08505
tags:
- mobile
- llama
- inference
- deployment
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of deploying vision-language\
  \ models (VLMs) on mobile devices, which face severe computational and energy constraints.\
  \ The authors evaluate three representative VLMs\u2014LLaVA-1.5 7B, MobileVLM-3B,\
  \ and Imp-v1.5 3B\u2014using three deployment frameworks (llama.cpp, MLC-Imp, mllm)\
  \ on a OnePlus 13R smartphone."
---

# Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R

## Quick Facts
- arXiv ID: 2507.08505
- Source URL: https://arxiv.org/abs/2507.08505
- Reference count: 17
- Primary result: CPU overutilization during token generation, GPU-offload reduces power by order of magnitude (1.3W vs 10-12W)

## Executive Summary
This paper evaluates vision-language model (VLM) deployment on mobile devices, benchmarking three representative VLMs (LLaVA-1.5 7B, MobileVLM-3B, Imp-v1.5 3B) across three frameworks (llama.cpp, MLC-Imp, mllm) on a OnePlus 13R. The study reveals severe computational bottlenecks and thermal challenges, with CPU consistently overutilized during token generation while GPU and NPU remain largely unused. GPU-offloaded deployments dramatically reduce power consumption and thermal stress, maintaining temperatures below 60°C compared to 90-95°C for CPU-only approaches. Framework choice significantly impacts efficiency, with runtime-level scheduling decisions affecting latency as much as model architecture choices.

## Method Summary
The authors root a OnePlus 13R running Android 15 and deploy three VLMs using llama.cpp, MLC-Imp, and mllm frameworks. They run each model-framework pair five times under controlled conditions (airplane mode, fixed brightness) while logging hardware utilization, power consumption, and temperature via system profiling tools. The evaluation measures end-to-end latency broken down by stage (image encoding, prompt evaluation, token generation), along with CPU/GPU/NPU utilization, die temperature, and power draw using vendor sensors and thermal zones.

## Key Results
- CPU overutilization during token generation while GPU and NPU remain unused across all frameworks
- GPU-offloaded deployments reduce power consumption by an order of magnitude (1.3W vs 10-12W) and maintain temperatures below 60°C versus 90-95°C
- Runtime choices significantly impact efficiency - mllm incurred nearly double the latency of llama.cpp for identical models
- Current frameworks fail to exploit mobile accelerators effectively, leaving NPU entirely unused and GPU underutilized

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPU-offloaded deployments reduce power consumption by an order of magnitude compared to CPU-only execution for VLM inference.
- Mechanism: Offloading GEMM-dominated operations (image encoding, attention layers) to GPU leverages higher parallelism and better performance-per-watt, leaving only sequential GEMV-heavy token generation on the CPU. This avoids thermal throttling and reduces energy draw.
- Core assumption: The GPU scheduler and memory bandwidth can handle the offloaded kernels without introducing overhead that exceeds CPU execution cost.
- Evidence anchors:
  - [abstract] "GPU-offloaded deployments (e.g., Imp-v1.5 3B with MLC-Imp) reduced power consumption by an order of magnitude (1.3 W vs. 10–12 W) and maintained temperatures below 60°C compared to 90–95°C for CPU-only deployments"
  - [section 4.3] "MLC-Imp+Imp-v1.5-3B off-loads nearly all vision–language compute to the Adreno 740, holding the CPU to ≤ 120 % load, capping temperature at 60°C, and drawing only 0.33 A"
  - [corpus] Related work on energy-efficient ViT inference supports device-aware model selection but does not provide direct evidence for mobile GPU offloading specifically.
- Break condition: If GPU saturation causes device instability (screen freezes, UI lag) or if memory transfer overhead exceeds compute savings, the efficiency gains may reverse.

### Mechanism 2
- Claim: Runtime-level scheduling and thread affinity decisions impact latency as significantly as model architecture choices.
- Mechanism: Frameworks implement different scheduler strategies, memory allocation patterns, and kernel dispatch logic. Inefficient scheduling (e.g., poor thread pooling, blocking I/O) introduces idle cycles and memory bandwidth contention, increasing latency independent of model size.
- Core assumption: The same model weights and quantization are used across runtimes; differences stem purely from software orchestration.
- Evidence anchors:
  - [abstract] "Runtime choices significantly impacted efficiency—mllm incurred nearly double the latency of llama.cpp for the same model"
  - [section 5.1] "Deploying the identical LLaVA-1.5 7B model on mllm resulted in nearly double the end-to-end latency (174 s compared to 82 s) and increased peak power draw by approximately 1 W compared to llama.cpp"
  - [corpus] MNN-LLM paper discusses generic inference engine optimizations but does not isolate scheduler effects directly.
- Break condition: If models are compiled with different quantization or operator fusion settings, attributing latency differences solely to runtime becomes invalid.

### Mechanism 3
- Claim: NPU underutilization in current frameworks leaves a significant optimization pathway unavailable for GEMV-heavy token generation.
- Mechanism: NPUs are designed for high-throughput GEMM but inefficient for low-arithmetic-intensity GEMV due to limited data reuse. Current frameworks do not dispatch decoder layers to NPU, leaving the CPU bottlenecked during autoregressive token generation.
- Core assumption: INT8-quantized projection layers could be mapped to Hexagon DSP/NPU with acceptable accuracy loss.
- Evidence anchors:
  - [abstract] "Current frameworks fail to exploit mobile accelerators effectively, with the NPU entirely unused and GPU underutilization common"
  - [section 4.1] "Inference consistently bottlenecked at the CPU during token generation, with the GPU mostly underutilized and the Hexagon NPU entirely unused"
  - [corpus] mllm-NPU paper (cited in related work) explores NPU offloading but was not evaluated directly in this study.
- Break condition: If NPU kernel launch latency or precision degradation negates throughput gains, offloading may not improve end-to-end latency.

## Foundational Learning

- Concept: GEMM vs. GEMV computational characteristics
  - Why needed here: Understanding that image encoding and attention are GEMM-dominated (parallelizable, high arithmetic intensity) while token generation is GEMV-heavy (sequential, low parallelism) is essential for mapping workloads to CPU/GPU/NPU.
  - Quick check question: For a 7B parameter model generating tokens autoregressively, which operation dominates compute during decoding: matrix-matrix multiply or matrix-vector multiply?

- Concept: Thermal throttling and power budgets on mobile
  - Why needed here: The paper shows CPU-only runs hitting 90–95°C, triggering throttling. Engineers must understand thermal envelopes to design sustainable deployment strategies.
  - Quick check question: If a deployment draws 10W continuously on a smartphone with a 5000mAh battery (~18.5Wh), approximately how long can it run at one query per minute before battery depletion?

- Concept: KV cache memory scaling
  - Why needed here: Memory usage during prompt evaluation scales with context length. The paper notes mllm's larger KV cache contributing to memory pressure.
  - Quick check question: If prompt evaluation for 605 tokens requires significant memory allocation, what happens to memory footprint if context length doubles?

## Architecture Onboarding

- Component map: Visual encoder (CLIP-ViT/SigLIP) -> Projector/adapter layer -> LLM backbone (attention + MLP) -> Autoregressive decoder -> Runtime framework (llama.cpp, MLC-Imp, mllm)
- Critical path: Image encoding → prompt evaluation (prefill) → token generation (decode). Token generation is the latency bottleneck for all deployments due to sequential decoding.
- Design tradeoffs:
  - CPU-only: Simpler deployment, no accelerator driver dependency, but 10× higher power and thermal stress
  - GPU-offload: Lower power/thermal, but requires framework support and may cause UI instability if GPU is saturated
  - NPU-offload: Potential for INT8 efficiency, but current frameworks lack support; unproven accuracy impact
- Failure signatures:
  - Screen freezes during GPU saturation (observed with partial offload attempts)
  - Thermal throttling causing latency spikes after 15–20 seconds of sustained CPU load
  - OOM (out-of-memory) errors when KV cache exceeds available RAM for long contexts
- First 3 experiments:
  1. Baseline profiling: Run llama.cpp with MobileVLM-3B, record CPU/GPU utilization, power draw, and temperature using system profiling tools. Verify CPU-only behavior.
  2. GPU-offload comparison: Deploy Imp-v1.5-3B on MLC-Imp, measure power reduction and thermal improvement against baseline. Confirm GPU utilization >80% during image encoding.
  3. Runtime isolation test: Run LLaVA-1.5-7B on both llama.cpp and mllm with identical quantization, isolate latency difference to scheduler/threading choices.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What performance gains result from implementing GPU-accelerated Flash-Attention and Hexagon-based INT8 decoding for mobile VLMs?
- Basis in paper: [explicit] Section 5.3 lists "GPU-accelerated Flash-Attention" and "INT8 decoding via the Hexagon NPU" as future implementation targets to assess sustained throughput.
- Why unresolved: Current frameworks exclusively utilized the CPU for these tasks, leaving the NPU entirely unused and GPU potential largely untapped during token generation.
- What evidence would resolve it: Benchmarks showing inference latency and power consumption metrics after integrating these specific kernels into the deployment pipeline.

### Open Question 2
- Question: Do the identified CPU overutilization and thermal bottlenecks persist across different mobile SoCs and multi-image workloads?
- Basis in paper: [explicit] Section 5.3 notes the study is limited to a single Snapdragon 8 Gen 2 device and single-image prompts, explicitly calling for analysis on "other System-on-Chips."
- Why unresolved: Differing hardware architectures (e.g., thermal envelopes, NPU capabilities) may handle the GEMV-heavy token generation or memory bandwidth constraints differently than the tested OnePlus 13R.
- What evidence would resolve it: Replicating the profiling methodology on devices with alternative chipsets (e.g., Apple, MediaTek) under multi-image loads.

### Open Question 3
- Question: Can mixed-precision KV cache compression achieve the theoretical 40% memory reduction without degrading VLM accuracy?
- Basis in paper: [explicit] Section 5.2 proposes "Mixed-precision KV cache" (FP8 or INT4) as an opportunity to reduce memory usage by roughly 40%.
- Why unresolved: The paper benchmarks existing runtimes and does not implement this specific optimization, so its actual efficiency/accuracy trade-off remains theoretical.
- What evidence would resolve it: End-to-end deployment tests measuring memory footprint against baseline accuracy scores (e.g., VQA correctness).

## Limitations
- Findings based on single device (OnePlus 13R with Snapdragon 8 Gen 2) may not generalize to other mobile SoCs or thermal designs
- Evaluation focuses on static workloads without considering dynamic scenarios like concurrent app usage or varying network conditions
- NPU evaluation is particularly limited as no NPU-offloaded framework was tested despite acknowledging it as a potential optimization path

## Confidence

**High Confidence**: The observation that CPU-only deployments hit thermal throttling at 90-95°C is well-supported by direct temperature measurements. The significant latency difference between llama.cpp and mllm (174s vs 82s) for the same model is a concrete measurement with minimal ambiguity.

**Medium Confidence**: The claim about GPU offloading reducing power by an order of magnitude (1.3W vs 10-12W) is supported by measurements but depends on specific workload characteristics and may vary with different model architectures or input sizes. The assertion that NPU is entirely unused across frameworks is based on utilization monitoring but may miss partial or undocumented offloading attempts.

**Low Confidence**: The proposed optimization strategies (mixed-precision KV cache compression, overlapping image encoding with prompt evaluation) are described conceptually but lack quantitative validation in this study. The generalizability of framework efficiency differences to other models or devices remains unproven.

## Next Checks
1. **Cross-Device Validation**: Replicate the benchmark on at least two additional mobile devices with different SoCs (e.g., Apple A-series, MediaTek Dimensity) to verify whether GPU offloading consistently provides order-of-magnitude power reduction and whether thermal profiles differ significantly across architectures.

2. **NPU Framework Implementation**: Implement and test an NPU-offloading framework (such as the mllm-NPU approach mentioned in related work) for the vision encoder and attention layers to validate whether NPU utilization can be achieved without accuracy degradation and to quantify actual performance gains.

3. **Dynamic Workload Analysis**: Extend the evaluation to include concurrent background processes and variable context lengths to assess how framework efficiency differences manifest under realistic mobile usage patterns, particularly measuring UI responsiveness degradation during GPU saturation and thermal throttling effects on sustained performance.