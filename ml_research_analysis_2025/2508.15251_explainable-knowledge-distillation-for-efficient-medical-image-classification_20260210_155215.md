---
ver: rpa2
title: Explainable Knowledge Distillation for Efficient Medical Image Classification
arxiv_id: '2508.15251'
source_url: https://arxiv.org/abs/2508.15251
tags:
- student
- teacher
- knowledge
- distillation
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a knowledge distillation framework integrating
  model compression with explainable AI to enable efficient and interpretable medical
  image classification. By distilling knowledge from high-capacity teacher models,
  such as DenseNet-201, Visformer-S, and AutoFormer-V2-T, into a lightweight OFA-595
  student model, the approach achieves robust performance on COVID-19 and lung cancer
  datasets (COVID-QU-Ex and LCS25000).
---

# Explainable Knowledge Distillation for Efficient Medical Image Classification

## Quick Facts
- arXiv ID: 2508.15251
- Source URL: https://arxiv.org/abs/2508.15251
- Authors: Aqib Nazir Mir; Danish Raza Rizvi
- Reference count: 19
- Primary result: Achieves up to 97.0% accuracy and 97.5% F1-score on COVID-QU-Ex dataset using lightweight OFA-595 student model distilled from DenseNet-201, Visformer-S, and AutoFormer-V2-T

## Executive Summary
This study introduces an explainable knowledge distillation framework that combines model compression with interpretable AI for medical image classification. The approach transfers knowledge from high-capacity teacher models to a lightweight student model, achieving competitive performance on COVID-19 and lung cancer datasets while maintaining interpretability through Score-CAM visualizations. The framework demonstrates effectiveness across different medical imaging modalities and addresses class imbalance through hybrid loss functions.

## Method Summary
The framework employs cross-architecture knowledge distillation where DenseNet-201, Visformer-S, and AutoFormer-V2-T serve as teacher models to train a compact OFA-595 student model. The process utilizes a hybrid loss function combining cross-entropy and knowledge distillation losses, with special handling for class imbalance. Score-CAM visualizations provide interpretability by highlighting regions of medical images that influence classification decisions, ensuring that the student model maintains clinically relevant attention patterns from its teachers.

## Key Results
- Student model achieves 97.0% accuracy and 97.5% F1-score on COVID-QU-Ex dataset
- Maintains strong performance on LCS25000 lung cancer dataset with 92.0% accuracy
- Score-CAM visualizations confirm effective transfer of clinically relevant attention patterns from teachers to students

## Why This Works (Mechanism)
Knowledge distillation enables efficient model compression by transferring learned representations from complex teacher networks to simpler student architectures. The hybrid loss function ensures the student captures both classification accuracy and soft-label distributions from teachers. Score-CAM provides visual explanations by generating class-discriminative activation maps, making the model's decisions interpretable for clinical applications. Cross-architecture distillation allows leveraging diverse architectural strengths while maintaining efficiency.

## Foundational Learning
1. **Knowledge Distillation** - Transfer of learned representations from large to small models; needed for computational efficiency; quick check: compare student-teacher performance gaps
2. **Cross-Architecture KD** - Distillation between different network architectures; enables leveraging diverse model strengths; quick check: evaluate transfer effectiveness across architectures
3. **Score-CAM Visualization** - Gradient-free attention visualization method; provides interpretable explanations; quick check: validate activation maps align with clinical findings
4. **Hybrid Loss Functions** - Combined cross-entropy and distillation losses; addresses class imbalance; quick check: monitor class-wise performance metrics
5. **Medical Image Classification** - Disease detection from medical scans; requires high accuracy and interpretability; quick check: validate on multiple medical imaging modalities

## Architecture Onboarding

**Component Map:**
Data Preprocessing -> Teacher Model Training -> Knowledge Distillation -> Student Model Training -> Score-CAM Visualization -> Evaluation

**Critical Path:**
Teacher models (DenseNet-201, Visformer-S, AutoFormer-V2-T) -> Distillation process -> OFA-595 student model -> Score-CAM interpretation -> Clinical validation

**Design Tradeoffs:**
- Accuracy vs. computational efficiency: Lightweight student sacrifices some capacity for deployment feasibility
- Interpretability vs. performance: Score-CAM adds overhead but enables clinical trust
- Cross-architecture complexity: Different architectures may capture different features but complicate distillation

**Failure Signatures:**
- Poor knowledge transfer indicated by large performance gaps between teachers and students
- Score-CAM visualizations that don't align with known disease patterns suggest failed distillation
- Class imbalance persisting despite hybrid loss indicates need for different weighting strategies

**3 First Experiments:**
1. Baseline: Train OFA-595 from scratch on COVID-QU-Ex dataset
2. Ablation: Remove Score-CAM visualization to assess pure performance impact
3. Cross-dataset: Test student model on external COVID-19 dataset to evaluate generalization

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to only two medical datasets, constraining generalizability
- No quantification of computational savings in terms of FLOPs or inference time
- Qualitative Score-CAM visualizations lack quantitative validation of clinical relevance

## Confidence

**Major Uncertainties and Limitations:**
The study's evaluation is constrained by the use of only two medical datasets (COVID-QU-Ex and LCS25000), limiting generalizability to other imaging modalities and pathologies. The absence of a direct comparison with existing SOTA lightweight models trained from scratch raises questions about the absolute efficiency gains. Additionally, the computational savings are not quantified in terms of specific metrics (e.g., FLOPs, inference time, memory footprint), making it difficult to assess real-world deployment benefits. The Score-CAM visualizations, while supporting transparency, are qualitative and lack quantitative validation of their clinical relevance. The paper does not address potential biases in the datasets or the robustness of the model to out-of-distribution samples, which are critical for clinical applications.

**Confidence Labels:**
- **High Confidence:** The framework's ability to achieve competitive accuracy and F1-scores on the tested datasets is well-supported by the results. The integration of explainable AI through Score-CAM is a valid approach for enhancing interpretability.
- **Medium Confidence:** The claim of significant computational savings is plausible but lacks concrete metrics for validation. The effectiveness of the hybrid loss function in addressing class imbalance is inferred but not explicitly demonstrated.
- **Low Confidence:** The generalizability of the framework to other medical imaging tasks and its robustness to dataset biases are not sufficiently addressed.

## Next Checks
1. **Quantitative Efficiency Metrics:** Measure and report FLOPs, inference time, and memory usage for both teacher and student models to quantify computational savings.
2. **Cross-Dataset Generalization:** Evaluate the framework on additional medical imaging datasets (e.g., chest X-rays, MRI scans) to assess its versatility and robustness across modalities.
3. **Clinical Relevance Validation:** Conduct a user study with clinicians to validate the interpretability and clinical utility of Score-CAM visualizations in real-world diagnostic scenarios.