---
ver: rpa2
title: Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial
  Intelligence Model Development in Upper Extremity Radiography
arxiv_id: '2510.05664'
source_url: https://arxiv.org/abs/2510.05664
tags:
- finding
- fracture
- 'false'
- joint
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that GPT-4o can automatically extract\
  \ diagnostic labels from radiology reports with high accuracy (98.1\u201399.0%)\
  \ to train multi-label classification models for upper extremity radiography. Across\
  \ clavicle, elbow, and thumb radiographs, models achieved competitive macro-averaged\
  \ AUC values (0.76\u20130.80), with high performance for common fracture findings."
---

# Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography

## Quick Facts
- **arXiv ID**: 2510.05664
- **Source URL**: https://arxiv.org/abs/2510.05664
- **Reference count**: 40
- **Primary result**: GPT-4o automatically extracts diagnostic labels from radiology reports with 98.1-99.0% accuracy to train multi-label classification models for upper extremity radiography.

## Executive Summary
This study demonstrates that GPT-4o can automatically extract diagnostic labels from radiology reports with high accuracy (98.1-99.0%) to train multi-label classification models for upper extremity radiography. Across clavicle, elbow, and thumb radiographs, models achieved competitive macro-averaged AUC values (0.76-0.80), with high performance for common fracture findings. Despite variable sensitivity for rare or soft tissue abnormalities, handling of label uncertainty (converting "uncertain" labels to "true" or "false") did not significantly affect model performance. Models generalized well to external datasets. The approach enables rapid, scalable, and decentralized AI model development using routinely available clinical data.

## Method Summary
The study employed a two-part approach: first, GPT-4o performed zero-shot label extraction from German radiology reports, converting unstructured text into structured JSON templates with binary labels ("true," "false," or "uncertain"). Second, multi-label convolutional neural networks using ResNet50 architectures were trained on the extracted labels to classify clavicle, elbow, and thumb radiographs. For multi-projection data (elbow and thumb), parallel ResNet50 backbones processed AP and lateral views separately before concatenating feature vectors. The models were trained with BCEWithLogitsLoss, AdamW optimizer, and evaluated using macro-averaged AUC metrics.

## Key Results
- GPT-4o extracted diagnostic labels with 98.1-99.0% accuracy across all anatomical regions
- Multi-label classification models achieved macro-averaged AUC values of 0.76-0.80
- Handling of label uncertainty (inclusive vs. exclusive) did not significantly impact model performance (p≥0.15)
- Models generalized well to external test datasets
- High sensitivity (>90%) for common fracture findings, but reduced sensitivity for rare or soft tissue abnormalities

## Why This Works (Mechanism)

### Mechanism 1: High-Fidelity Zero-Shot Label Extraction
Large Language Models (specifically GPT-4o) can convert unstructured radiology reports into structured, multi-label binary vectors with high accuracy without task-specific training. The model maps free-text descriptions (including negations and complex syntax) to a fixed JSON schema by leveraging pre-trained medical knowledge. By explicitly instructing the model to categorize findings as "true," "false," or "uncertain," the system decouples the ambiguity of natural language from the binary requirements of most classification loss functions.

### Mechanism 2: Robustness to Low-Prevalence Label Noise
Treating "uncertain" labels as either positive (inclusive) or negative (exclusive) does not significantly impact the final AUC of the image classifier. Assumption: The prevalence of explicitly "uncertain" labels in the dataset is low enough (e.g., <10%) that the noise introduced by forcing them into binary classes acts as a regularizer rather than a misleading signal. The volume of high-confidence "true" and "false" labels dominates the gradient updates.

### Mechanism 3: Multi-View Orthogonal Fusion
Processing orthogonal radiographic views (AP and Lateral) through parallel feature extractors and concatenating them enables the model to resolve visual ambiguities present in a single projection. Separate backbones extract spatial features from each view. Concatenation merges these feature vectors before the final classification layer, allowing the dense layer to learn correlations (e.g., a fracture line visible in one view but obscured by bone density in another).

## Foundational Learning

- **Concept: Zero-Shot Prompting**
  - **Why needed here:** The study relies on GPT-4o performing a task (report parsing) without providing examples of input-output pairs in the prompt.
  - **Quick check question:** Can you distinguish between "zero-shot" (no examples) and "few-shot" (providing examples) prompting strategies?

- **Concept: Macro-Averaged AUC**
  - **Why needed here:** The task is multi-label classification with high class imbalance (e.g., many fractures, few "ossicles"). Macro-AUC treats all classes equally, preventing the metric from being dominated by the most common findings.
  - **Quick check question:** If a model achieves 99% accuracy on a dataset where 99% of cases are negative, why is Macro-AUC a better metric for this study?

- **Concept: Binary Cross-Entropy with Logits (BCEWithLogitsLoss)**
  - **Why needed here:** The models output multiple labels per image (e.g., "Fracture" AND "Degeneration"). BCE loss treats each label as an independent binary classification problem rather than a single mutually exclusive category.
  - **Quick check question:** Why is Softmax not suitable for multi-label classification where a patient can have both a fracture and a joint degeneration?

## Architecture Onboarding

- **Component map:** Anonymized Radiology Report -> GPT-4o (Text -> JSON) -> Uncertainty Handler (JSON -> Binary Vector) -> ResNet50 Backbone(s) -> Feature Concatenation -> Fully Connected Layer -> Sigmoid Activation

- **Critical path:** The definition of "uncertain" terms in the system prompt (Supplementary Text 1) is critical; a loose definition creates noisy labels that break the "inclusive/exclusive" robustness. For Elbow/Thumb, the fusion of the two ResNet streams must occur after feature extraction but before the final classifier to enable cross-view reasoning.

- **Design tradeoffs:**
  - **Inclusive vs. Exclusive:** The paper proves this tradeoff is low-risk for this data, but "Inclusive" risks false positives while "Exclusive" risks false negatives in training.
  - **Separate vs. Shared Weights:** The architecture uses separate ResNet50 networks for different views. A shared-weight backbone might be more parameter-efficient but could struggle with the different statistics of AP vs. Lateral images.

- **Failure signatures:**
  - High Specificity / Low Sensitivity on Rare Classes: The model tends to miss rare findings (e.g., "Ossicles," "Soft Tissue Calcifications") likely due to insufficient positive training samples.
  - Report-Level vs. Label-Level Accuracy: Label accuracy was high (98%+), but Report-level accuracy (all labels correct in one report) dropped to ~75%, suggesting compounding errors in multi-label extraction.

- **First 3 experiments:**
  1. Sanity Check Extraction: Run the GPT-4o extraction pipeline on 50 random reports and manually verify the mapping of "uncertain" hedging terms to ensure the prompt logic holds for your specific institutional jargon.
  2. Single-View vs. Multi-View Ablation: Train the Elbow model using only AP views vs. the concatenated AP+Lateral setup to quantify the performance delta attributed to the fusion architecture.
  3. Noise Robustness Test: Artificially flip 5-10% of training labels for a specific class (e.g., "Fracture") and observe if the validation AUC drops significantly, testing the hypothesis that the model is robust to label noise.

## Open Questions the Paper Calls Out

- Does the performance of GPT-4o-based label extraction and subsequent model training generalize to radiologic reports written in languages other than German?
- How does the diagnostic performance of models trained on LLM-extracted labels compare to those trained on strictly manually curated human reference labels?
- Can the inclusion of visual explainability outputs (e.g., saliency maps) improve the detection of rare or subtle findings where current models show low sensitivity?

## Limitations

- The study's conclusions about handling label uncertainty may be underpowered due to the low prevalence of uncertain labels (<10%) in the dataset.
- Performance on rare findings is limited by insufficient training samples, with reduced sensitivity for soft tissue abnormalities and uncommon pathologies.
- Zero-shot label extraction relies on GPT-4o's pre-training data matching institutional terminology, with potential degradation for domain-shifted reporting styles.

## Confidence

- **High Confidence**: The core claim that GPT-4o can extract diagnostic labels from radiology reports with 98-99% accuracy is well-supported by the methodology and results.
- **Medium Confidence**: The claim that handling label uncertainty does not significantly affect model performance is supported by statistical tests (p≥0.15) but may be underpowered due to the low prevalence of uncertain labels.
- **Low Confidence**: Claims about performance on rare findings are limited by the small number of positive cases in the dataset.

## Next Checks

1. Apply the inclusive/exclusive labeling strategy to a dataset where >20% of labels are explicitly uncertain to determine the true threshold at which label noise begins degrading model performance.

2. Validate the zero-shot label extraction pipeline on radiology reports from at least two additional institutions with different reporting styles and terminology to quantify domain shift effects on extraction accuracy.

3. Implement targeted data augmentation and class weighting strategies for labels with <10 positive cases, then compare sensitivity improvements against the baseline model to determine if the performance gap is bridgeable.