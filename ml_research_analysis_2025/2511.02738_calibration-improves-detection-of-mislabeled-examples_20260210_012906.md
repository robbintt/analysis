---
ver: rpa2
title: Calibration improves detection of mislabeled examples
arxiv_id: '2511.02738'
source_url: https://arxiv.org/abs/2511.02738
tags:
- calibration
- examples
- learning
- mislabeled
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of calibrating base machine
  learning models on mislabeled instance detection. The core idea is that calibration
  improves the reliability of confidence-based trust scores, preventing detectors
  from incorrectly flagging minority-class examples as mislabeled.
---

# Calibration improves detection of mislabeled examples

## Quick Facts
- arXiv ID: 2511.02738
- Source URL: https://arxiv.org/abs/2511.02738
- Reference count: 39
- Primary result: Calibration improves mislabeled instance detection across diverse datasets and methods

## Executive Summary
This paper demonstrates that calibrating machine learning models significantly improves their ability to detect mislabeled training examples. The authors show that calibrated models provide more reliable confidence estimates, which is crucial for trust-based detection methods. Through extensive experiments on 19 datasets with four different detection algorithms, calibrated models consistently outperform uncalibrated ones, achieving better balanced accuracy and lower test loss. The approach is particularly effective for minority classes and underrepresented subgroups, making it valuable for real-world applications where class imbalance is common.

## Method Summary
The core methodology involves applying calibration techniques (Platt scaling and temperature scaling) to base machine learning models before using them for mislabeled instance detection. The calibrated models produce more reliable confidence scores, which are then fed into various detection algorithms (AUM, CleanLab, Consensus, Small Loss). The calibration is performed using a separate validation set, and the study systematically examines how calibration set size and noise levels affect detection performance. The experiments use linear classifiers with fixed feature extractors (TF-IDF or Random Fourier Features) and synthetic label noise injected at various rates.

## Key Results
- Calibrated detectors achieve significantly lower test loss and higher balanced accuracy compared to uncalibrated versions
- Calibration is effective even with small calibration sets (as few as dozens of samples)
- The approach is robust to label noise in the calibration set, maintaining performance even when calibration data contains mislabeled examples
- Calibrated models prevent minority-class examples from being incorrectly flagged as mislabeled

## Why This Works (Mechanism)
Calibration aligns predicted confidence scores with actual accuracy, making them more reliable as trust scores for detection. When models are uncalibrated, confidence scores can be systematically overconfident or underconfident, leading detection algorithms to make errors based on unreliable trust measures. By adjusting the confidence outputs to better reflect true probabilities, calibration ensures that detection methods can more accurately distinguish between correctly labeled and mislabeled examples based on confidence thresholds.

## Foundational Learning

### Probability Calibration
Why needed: Mislabeled detection relies on confidence scores as trust measures
Quick check: Platt scaling vs temperature scaling effectiveness comparison

### Trust-Based Detection Methods
Why needed: Detection algorithms use confidence as primary signal
Quick check: AUM, CleanLab, Consensus, Small Loss method performance

### Label Noise Robustness
Why needed: Real-world datasets often contain mislabeled examples
Quick check: Performance degradation under varying noise levels

## Architecture Onboarding

### Component Map
Base Model -> Calibration Layer -> Detection Algorithm -> Mislabeled Example Identification

### Critical Path
Base model training → Calibration on validation set → Detection algorithm execution → Label correction decisions

### Design Tradeoffs
Simple scaling methods (fast, less accurate) vs complex calibration (slower, potentially more accurate)

### Failure Signatures
Overconfident uncalibrated models → Incorrect trust scores → False positive/negative detections

### First Experiments
1. Compare calibrated vs uncalibrated performance on binary classification with 20% noise
2. Test calibration set size sensitivity from 10 to 1000 samples
3. Evaluate different calibration methods (Platt vs temperature scaling) on same detection task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed improvement in detection accuracy persist when using deep neural networks (DNNs) with end-to-end feature learning?
- Basis in paper: [inferred] The experiments exclusively employ linear classifiers with fixed feature extractors (TF-IDF or Random Fourier Features), while the discussion only speculates that temperature scaling might be effective for deep networks.
- Why unresolved: DNNs exhibit different calibration dynamics and memorization behaviors compared to linear models, potentially altering the effectiveness of the proposed calibration step.
- What evidence would resolve it: Benchmarks on image or text datasets using modern architectures (e.g., ResNets, Transformers) comparing calibrated vs. uncalibrated mislabel detection.

### Open Question 2
- Question: Can the performance of calibration be further improved by combining it with class-wise confidence adjustment heuristics?
- Basis in paper: [inferred] The authors evaluate calibration against the "adjust" method [17] as distinct competitors, finding calibration superior, but they do not test whether the two techniques are complementary.
- Why unresolved: "Adjust" centers class probabilities to handle imbalance, while calibration aligns confidence with accuracy; applying them sequentially might address different failure modes simultaneously.
- What evidence would resolve it: An ablation study applying the "adjust" transformation on top of calibrated probabilities to measure any additional gain in balanced accuracy.

### Open Question 3
- Question: How does the performance of calibrated detectors degrade under extremely high label noise rates in the calibration set?
- Basis in paper: [inferred] The paper demonstrates robustness to "noisy calibration data" but does not define the specific noise ratio limits or test the boundary where calibration fails to improve upon the baseline.
- Why unresolved: Calibration relies on estimating conditional probabilities; if the calibration set is dominated by errors, the mapping could become inverted or unreliable.
- What evidence would resolve it: Experiments varying calibration set noise ratios systematically (e.g., from 20% to 60%) to identify the breaking point of the method.

## Limitations

- Experimental focus on synthetic label noise may not capture complex real-world mislabeling patterns
- Limited to relatively simple calibration methods without exploring more sophisticated approaches
- Calibration set size analysis lacks precise quantification of minimum effective sample size

## Confidence

**Major Claim Clusters Confidence Assessment:**
- Calibration consistently improves detection accuracy: **High** - supported by extensive experiments across 19 datasets and 4 methods
- Calibration robustness to label noise in calibration set: **Medium** - demonstrated but with limited noise levels tested
- Small calibration sets are sufficient: **Medium** - results show effectiveness but lack precise minimum sample size determination

## Next Checks

1. Test calibration benefits on real-world datasets with naturally occurring label noise rather than synthetic contamination
2. Evaluate more sophisticated calibration methods (e.g., Bayesian binning, isotonic regression) and compare against simple scaling approaches
3. Investigate calibration performance on structured noise patterns (class-dependent or instance-dependent) beyond uniform random noise