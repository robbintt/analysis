---
ver: rpa2
title: Two-Stage Feature Generation with Transformer and Reinforcement Learning
arxiv_id: '2505.21978'
source_url: https://arxiv.org/abs/2505.21978
tags:
- feature
- features
- generation
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automated feature generation
  in machine learning, where traditional methods are labor-intensive and struggle
  to adapt to diverse datasets. The proposed Two-Stage Feature Generation (TSFG) framework
  integrates a Transformer-based encoder-decoder architecture with Proximal Policy
  Optimization (PPO) to dynamically generate high-quality features.
---

# Two-Stage Feature Generation with Transformer and Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.21978
- Source URL: https://arxiv.org/abs/2505.21978
- Reference count: 16
- The paper proposes a framework that outperforms existing automated feature generation methods in both feature quality and adaptability

## Executive Summary
This paper introduces a Two-Stage Feature Generation (TSFG) framework that addresses the labor-intensive nature of traditional feature engineering by combining Transformer-based sequence generation with reinforcement learning. The framework consists of a pre-training stage that learns basic feature transformation patterns, followed by a PPO-based fine-tuning stage that adapts features to specific downstream tasks. Experimental results on 13 datasets demonstrate significant improvements over existing methods, with runtime efficiency gains of up to 95% compared to previous state-of-the-art approaches.

## Method Summary
TSFG uses a Transformer encoder-decoder architecture to generate mathematical feature transformations in a sequential manner. In Stage 1, the model is pre-trained to minimize prediction loss on a frozen MLP, establishing a baseline policy for valid transformations. Stage 2 fine-tunes the model using Proximal Policy Optimization (PPO) based on task-specific performance rewards, where the reward is the improvement in downstream metric after applying generated features. The framework operates on tokenized representations of mathematical operations and features, generating sequences like [+V1, +V2, EOS, -V3, ×V1, STOP] that are then applied to create new features.

## Key Results
- TSFG achieves 5-15% improvement in F1 score and accuracy on classification tasks compared to baselines
- Runtime efficiency improved by 95% (53-139s vs 1310-7136s) compared to DIFER on tested datasets
- The framework successfully generates both linear and non-linear feature transformations while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Transformer Self-Attention for Feature Dependency Capture
The encoder-decoder architecture enables sequential generation of mathematically coherent feature transformations by capturing long-range dependencies between original features. The encoder maps raw features into a latent representation Z that encodes structural relationships, while the decoder autoregressively generates transformation tokens, ensuring syntactically valid feature combinations.

### Mechanism 2: Two-Stage Decomposition for Stable-Then-Adaptive Optimization
Separating pre-training from PPO fine-tuning prevents premature convergence to task-suboptimal feature sets while ensuring stable initialization. Stage 1 establishes a baseline policy for valid transformations, while Stage 2 optimizes the full transformation sequence for task-specific rewards, allowing global feature-set coherence rather than per-feature myopic optimization.

### Mechanism 3: PPO Clipping for Policy Stability Under Sparse Rewards
PPO's clipped objective enables stable policy updates despite potentially high-variance reward signals from downstream task evaluation. The reward r = Metric(D_new) - Metric(D) can be noisy, but PPO's clipping constraint limits policy deviation per update, preventing gradient explosion while allowing gradual improvement.

## Foundational Learning

- **Transformer Encoder-Decoder Architecture**
  - Why needed here: Core model for sequence-to-sequence generation of feature transformations. Must understand attention, positional encoding, and autoregressive decoding.
  - Quick check question: Can you explain why the decoder generates tokens autoregressively rather than all at once?

- **Proximal Policy Optimization (PPO)**
  - Why needed here: Fine-tuning mechanism. Must understand policy gradients, advantage estimation, and the clipping objective that distinguishes PPO from vanilla policy gradient.
  - Quick check question: What problem does the clipping parameter ε=0.2 solve that vanilla REINFORCE would encounter?

- **Markov Decision Processes (MDPs) for Feature Generation**
  - Why needed here: RL formulation requires defining state/action/reward. Here, state=current feature set, action=transformation operation, reward=performance delta.
  - Quick check question: Why is feature generation modeled as an MDP rather than a single-step optimization problem?

## Architecture Onboarding

- **Component map:**
  - Transformer encoder (8 heads, 128 dim) → latent representation z
  - Transformer decoder → autoregressive token sequence
  - Token vocabulary: Operations (+, -, ×, ÷, log, sqrt, etc.) combined with features (+V1, -V2, etc.) plus EOS/STOP tokens
  - Pre-training loss: Cross-entropy (classification) or MSE (regression) via frozen MLP
  - PPO module: Policy network (encoder-decoder), reward computation via downstream model, Adam optimizer (lr=1e-4)
  - Temperature scaling: T ∈ [0.1, 1.0] controls exploration during sampling

- **Critical path:**
  1. Pre-train encoder-decoder to minimize MLP prediction loss (iterates per feature generation)
  2. Freeze best pre-trained model as PPO initialization
  3. PPO iterations (T_max=10): sample transformation sequences → apply to dataset → evaluate downstream performance → compute reward → update policy
  4. Output final transformed dataset

- **Design tradeoffs:**
  - Pre-training MLP proxy vs. actual downstream model: Using MLP is faster but may not align with final task
  - Per-feature loss (Stage 1) vs. sequence-level reward (Stage 2): Finer-grained feedback early, holistic optimization later
  - Temperature scheduling: High T = more exploration but slower convergence; low T = exploitation but risks local optima

- **Failure signatures:**
  - Redundant features generated: Check if temperature is too low (insufficient exploration)
  - Invalid transformation sequences: Verify token vocabulary constraints (operations must match feature types)
  - PPO fails to improve reward: May indicate reward signal too noisy; try increasing downstream model stability
  - Runtime explosion: Maximum sequence length (100) may be too large for high-dimensional datasets

- **First 3 experiments:**
  1. Sanity check on small dataset (e.g., diabetes, 8 features): Run full pipeline with verbose logging of generated sequences. Verify: (a) valid token sequences, (b) reward increases over PPO iterations, (c) final F1 improves over baseline.
  2. Ablation of Stage 1 vs. Stage 2: Compare TSFG vs. TSFG+ (no pre-training) vs. TSFG# (no PPO) on 2-3 datasets. Expect: TSFG# performs reasonably, TSFG+ is unstable or slow to converge.
  3. Downstream model robustness: Test generated features on RF, XGBoost, SVM, CatBoost (as in Table 4). If performance varies drastically, features may be overfitting to one model type during PPO reward computation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of TSFG scale when applied to datasets with significantly higher dimensionality (e.g., >500 features) than those tested?
- Basis in paper: The Introduction emphasizes the increasing "scale and complexity of data," yet Table 2 restricts experiments to datasets with fewer than 50 features.
- Why unresolved: The quadratic complexity of the Transformer's self-attention mechanism and the iterative nature of PPO fine-tuning may introduce prohibitive latency on high-dimensional data.
- What evidence would resolve it: Runtime and performance benchmarks on high-dimensional datasets (e.g., gene expression or image embedding data).

### Open Question 2
- Question: Does the reliance on a specific pre-trained MLP for feedback introduce a bias that hinders performance on heterogeneous downstream models?
- Basis in paper: Sections 3.3 and 3.4 utilize a pre-trained MLP to calculate loss and rewards, yet Table 4 shows variable performance across different downstream models like SVM and XGBoost.
- Why unresolved: It is unclear if features optimized specifically to minimize an MLP's loss are suboptimal for distance-based (SVM) or tree-based (Random Forest) models.
- What evidence would resolve it: An ablation study varying the feedback model (e.g., using a Random Forest for PPO rewards) and measuring the impact on other downstream tasks.

### Open Question 3
- Question: Can the sequential generation process effectively minimize feature redundancy without an explicit pruning or reduction step?
- Basis in paper: The Introduction cites "feature redundancy" as a major challenge of existing methods, but the proposed framework generates features additively without a stated removal mechanism.
- Why unresolved: While the reward signal optimizes for predictive power, it may not explicitly penalize multicollinearity, potentially leading to redundant feature sets.
- What evidence would resolve it: Analysis of the correlation matrices and variance inflation factors (VIF) for the generated feature sets compared to expansion-reduction baselines.

## Limitations

- The framework relies heavily on pre-training with an MLP proxy that may not align well with downstream task requirements, potentially limiting transferability to non-MLP-compatible models
- The token vocabulary and transformation constraints are unspecified in detail, making it unclear how well the system handles diverse feature types and complex dependencies
- The empirical evaluation shows strong performance but lacks ablation studies isolating the contributions of Transformer architecture versus PPO optimization

## Confidence

- **High confidence**: The core mechanism of two-stage training (pre-training + PPO fine-tuning) is well-supported by existing RL literature and the paper's experimental design
- **Medium confidence**: The Transformer-based feature dependency capture mechanism, while theoretically sound, lacks direct corpus validation for tabular feature generation specifically
- **Low confidence**: The scalability claims regarding runtime improvements (53-139s vs. DIFER's 1310-7136s) without specifying hardware or implementation details

## Next Checks

1. **Cross-model robustness test**: Evaluate generated features on ensemble methods (RF, XGBoost, SVM, CatBoost) to verify features generalize beyond the MLP used in PPO reward computation
2. **Pre-training ablation study**: Compare TSFG against variants with: (a) no pre-training, (b) random initialization, (c) pre-training with different proxy models (tree-based vs. MLP)
3. **Tokenization boundary test**: Systematically evaluate feature generation quality as maximum sequence length varies (e.g., 50, 100, 150) to identify the optimal balance between expressiveness and computational efficiency