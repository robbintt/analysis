---
ver: rpa2
title: 'CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality'
arxiv_id: '2511.16191'
source_url: https://arxiv.org/abs/2511.16191
tags:
- causal
- graph
- rumor
- nodes
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CausalMamba integrates Mamba sequence modeling, GCN graph structure
  encoding, and differentiable causal discovery via NOTEARS to jointly detect rumors
  and uncover influence pathways in social media propagation chains. The model learns
  both content and structural dependencies while inferring causal graphs that identify
  influential nodes.
---

# CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality

## Quick Facts
- arXiv ID: 2511.16191
- Source URL: https://arxiv.org/abs/2511.16191
- Authors: Xiaotong Zhan; Xi Cheng
- Reference count: 3
- Primary result: 59.7% accuracy, 59.8% macro-F1 on Twitter15

## Executive Summary
CausalMamba integrates Mamba sequence modeling, GCN graph structure encoding, and differentiable causal discovery via NOTEARS to jointly detect rumors and uncover influence pathways in social media propagation chains. The model learns both content and structural dependencies while inferring causal graphs that identify influential nodes. Experiments on Twitter15 show competitive accuracy (59.7%) and macro-F1 (59.8%) compared to strong baselines like BiLSTM-CNN and Transformer models. The learned causal graphs enable intervention simulation—removing top-ranked nodes disrupts graph connectivity and alters predicted outcomes—demonstrating the model's potential for interpretable, actionable misinformation mitigation.

## Method Summary
CausalMamba processes tweet propagation chains using a 2-layer Mamba encoder for temporal content, a 1-layer GCN for reply tree structure, and residual fusion (H = H_seq + 0.3·H_graph) followed by mean pooling. Classification uses a 2-layer MLP, while causal discovery employs NOTEARS-style regularization on the learned adjacency matrix to enforce sparsity and acyclicity. Joint training optimizes both rumor classification and causal graph learning objectives.

## Key Results
- Achieves 59.7% accuracy and 59.8% macro-F1 on Twitter15 rumor classification
- Mamba-GCN baseline (without causal graphs) achieves 64.3% accuracy, showing ~4.6% accuracy trade-off for interpretability
- Intervention simulation shows removing top-PageRank nodes from learned causal graphs significantly disrupts graph connectivity

## Why This Works (Mechanism)

### Mechanism 1: Selective State-Space Encoding with Residual Graph Fusion
Combining Mamba's selective state-space modeling with GCN structural encoding captures both temporal content dependencies and propagation topology, improving over sequence-only baselines. Mamba processes tweet sequences with input-dependent selective state updates at linear complexity, while GCN captures multi-hop relational signals from reply trees via spectral convolution. Representations are fused residually: H = H_seq + α·H_graph (α=0.3), then mean-pooled for classification.

### Mechanism 2: NOTEARS-Style Differentiable Causal Graph Discovery
The model learns sparse, acyclic causal structures over tweet nodes by enforcing DAG constraints during training, enabling influence attribution without ground-truth causal labels. Hidden states H' are used to compute weighted adjacency W = H'H'ᵀ/d. The NOTEARS loss L_causal = ||H' - WH'||² + λ₁||W||₁ + λ₂(tr(exp(W⊙W)) - n) penalizes reconstruction error, encourages sparsity (L1), and enforces acyclicity via matrix exponential.

### Mechanism 3: PageRank-Based Intervention Simulation
Removing top-ranked nodes from learned causal graphs disrupts connectivity, demonstrating actionable intervention points for misinformation mitigation. PageRank computed on learned adjacency W identifies structurally central nodes. Counterfactual simulation removes top-k nodes and measures graph fragmentation—qualitatively validating influence identification.

## Foundational Learning

- **Concept: Selective State Space Models (SSMs)**
  - **Why needed here**: Mamba replaces attention with input-dependent state updates, enabling linear-time processing of long propagation chains (avg. 408 nodes/event).
  - **Quick check question**: How does Mamba's selective mechanism differ from standard RNN hidden state updates, and why does it scale better than attention for sequences of length >500?

- **Concept: Graph Convolutional Networks (GCNs)**
  - **Why needed here**: Encodes reply tree topology where edges represent retweet/reply relationships; captures multi-hop dependencies without handcrafted graph features.
  - **Quick check question**: Given a directed reply tree, how does a 1-layer GCN aggregate information, and what structural patterns would require deeper layers?

- **Concept: Differentiable DAG Learning (NOTEARS)**
  - **Why needed here**: Enables end-to-end causal structure learning via gradient descent; acyclicity constraint tr(exp(W⊙W)) - n = 0 is differentiable.
  - **Quick check question**: Why is the acyclicity constraint formulated as a matrix exponential, and what happens if this term is removed from the loss?

## Architecture Onboarding

- **Component map**: Input (833-dim features) -> Mamba Encoder (2-layer SSM) -> GCN Encoder (1-layer) -> Residual Fusion (H_seq + 0.3·H_graph) -> Masked Mean Pooling -> 2-layer FFN Classifier -> ŷ; simultaneously W = H'H'ᵀ/d -> NOTEARS Loss -> L_total

- **Critical path**:
  1. Parse propagation tree → node features + edge_index
  2. Mamba: X (B×L×833) → H_seq (B×L×128)
  3. GCN: H_seq + edge_index → H_graph (B×L×128)
  4. Fuse + pool → z (B×128) → classifier → ŷ
  5. Extract H' from first graph in batch → compute W → L_causal
  6. L_total = L_cls + λ·L_causal (λ=0.1)

- **Design tradeoffs**:
  - Classification vs. interpretability: CausalMamba 59.7% vs. Mamba-GCN 64.3% (~4.6% drop for causal graphs)
  - Sequence vs. structure: α=0.3 biases toward sequence; adjust upward for deep propagation trees
  - Sparsity vs. connectivity: Higher λ₁ yields sparser DAGs but risks over-fragmentation

- **Failure signatures**:
  - Short chains (<10 nodes): GCN underutilized, model defaults to Mamba behavior
  - High λ (≥1.0): Over-sparse causal graphs with isolated nodes
  - Shuffled temporal order: Mamba degrades; accuracy approaches BiLSTM baseline
  - Class imbalance: Macro-F1 << accuracy indicates per-class failure modes

- **First 3 experiments**:
  1. **Component ablation**: Train Mamba-only, GCN-only, Mamba-GCN, full CausalMamba. Quantify each component's contribution and validate the 4.6% accuracy-interpretability tradeoff.
  2. **Intervention validation**: For held-out events, remove top-k nodes by PageRank and measure (a) graph connectivity change, (b) classifier confidence change. Test whether intervention effects correlate with prediction correctness.
  3. **Hyperparameter sweep**: Vary λ ∈ {0.01, 0.1, 1.0} and α ∈ {0.1, 0.3, 0.5}. Map the accuracy-interpretability frontier; identify regime where causal graphs remain connected but sparse.

## Open Questions the Paper Calls Out

- **Question**: Does CausalMamba generalize to rumor detection datasets with different linguistic and structural distributions, such as Twitter16, PHEME, or Weibo?
  - **Basis in paper**: [explicit] Section 6.4 states that extending to other corpora is necessary to "assess the generalization of the model under different linguistic, structural, and temporal distributions."
  - **Why unresolved**: The current study restricts evaluation to the Twitter15 dataset, leaving performance on datasets with varying chain lengths and branching patterns unknown.
  - **What evidence would resolve it**: Experimental results on Twitter16 and PHEME showing competitive accuracy and stable causal graph discovery across these new domains.

- **Question**: Can dynamic loss weighting strategies or multi-objective optimization mitigate the performance trade-off between predictive accuracy and causal interpretability?
  - **Basis in paper**: [explicit] Section 6.4 notes the authors plan to "explore dynamic loss weighting strategies" to address the issue where causal constraints cause a slight drop in classification metrics compared to the Mamba-GCN baseline.
  - **Why unresolved**: The current fixed weighting (λ=0.1) creates a conflict where the acyclicity constraint slightly degrades the primary classification objective.
  - **What evidence would resolve it**: A training regime that adaptively adjusts the causal loss weight to maintain classification performance while preserving the ability to learn sparse DAGs.

- **Question**: Can weak supervision signals, such as annotated rumor sources, guide the model to learn more semantically meaningful causal graphs?
  - **Basis in paper**: [explicit] Section 6.4 suggests that "introducing weak supervision signals... could guide the learning of more semantically meaningful causal graphs" beyond the current unsupervised NOTEARS approach.
  - **Why unresolved**: The current causal discovery module relies solely on observational data without ground-truth validation of the actual influence pathways.
  - **What evidence would resolve it**: Improved correlation between the model's top-ranked PageRank nodes and human-annotated key spreaders in the dataset.

## Limitations
- The learned causal graphs are based on correlation structure in hidden states rather than ground-truth influence, raising questions about whether PageRank-identified nodes truly represent influential spreaders
- Performance trade-off shows CausalMamba lags behind Mamba-GCN by 4.6% accuracy, suggesting interpretability gains come with a cost that isn't fully characterized across different propagation lengths
- Intervention simulation claims rely on qualitative visualization rather than quantitative disruption metrics across multiple events

## Confidence

- **High confidence**: The sequence-graph fusion mechanism (Mamba-GCN baseline achieving 64.3%) and the overall rumor classification framework are well-supported by experimental results.
- **Medium confidence**: The NOTEARS-style differentiable causal discovery works as implemented, but the interpretation of learned DAGs as true influence pathways requires further validation.
- **Low confidence**: The intervention simulation claims that removing top-PageRank nodes meaningfully disrupts misinformation spread, as this relies on qualitative visualization rather than quantitative disruption metrics across multiple events.

## Next Checks

1. **Causal Graph Validation**: Extract learned DAGs from held-out events and compare PageRank centrality with ground-truth spreading patterns (if available) or alternative centrality measures like betweenness. Test whether high-PageRank nodes actually appear earlier in propagation or have higher out-degree in the original reply tree.

2. **Intervention Simulation Quantification**: Implement systematic removal of top-k nodes (k=1,3,5) from learned causal graphs and measure (a) graph fragmentation via connected components, (b) classifier confidence change for remaining nodes, and (c) accuracy difference when training without these nodes. Compare against random and degree-based removal baselines.

3. **Propagation Length Analysis**: Stratify Twitter15 events by propagation chain length and compute accuracy/Macro-F1 separately for short (<50 nodes), medium (50-200), and long (>200) chains. This will reveal whether the performance drop for CausalMamba is concentrated in certain regimes and validate the claim that GCN encoding is underutilized in short chains.