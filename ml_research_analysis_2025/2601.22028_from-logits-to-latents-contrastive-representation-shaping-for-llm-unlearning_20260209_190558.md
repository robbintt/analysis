---
ver: rpa2
title: 'From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning'
arxiv_id: '2601.22028'
source_url: https://arxiv.org/abs/2601.22028
tags:
- forget
- unlearning
- clreg
- retain
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLReg, a contrastive representation regularizer
  for large language model unlearning. The method addresses the limitation of existing
  alignment-based unlearning approaches that suppress forgotten content in the prediction
  space while leaving representations entangled.
---

# From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning

## Quick Facts
- arXiv ID: 2601.22028
- Source URL: https://arxiv.org/abs/2601.22028
- Reference count: 23
- One-line primary result: CLReg consistently improves mainstream unlearning methods, reducing feature entanglement while maintaining or improving model utility and privacy balance.

## Executive Summary
This paper introduces CLReg, a contrastive representation regularizer that addresses a fundamental limitation in LLM unlearning: while existing alignment-based methods suppress forgotten content in the prediction space, they leave representations entangled. CLReg uses contrastive learning to explicitly push forget features away from retain features in the representation space, reducing entanglement with minimal shifts on retained knowledge. The authors provide theoretical insights showing how representation shaping reduces entanglement and conduct extensive experiments across multiple benchmarks (TOFU, MUSE) and model sizes (Llama-3.1-8B, Llama-3.2-3B, Llama-2-7B). Results demonstrate that explicit representation shaping facilitates unlearning and inspires future research on surgical removal of forget concepts through latent-space interventions.

## Method Summary
CLReg addresses LLM unlearning by applying contrastive learning to the representation space rather than just the prediction space. The method constructs positive pairs from forget examples via dropout and paraphrase augmentation, then pushes these away from retain embeddings using a DPO-style contrastive loss. The combined objective includes retain loss, forget loss, and the contrastive regularizer, applied specifically to later transformer layers where forget-specific knowledge concentrates. The approach is compatible with mainstream unlearning methods (NPO, SimNPO, GradDiff, UnDIAL, PDU) and consistently improves their performance by reducing feature entanglement while maintaining model utility.

## Key Results
- CLReg consistently improves SimNPO and NPO across all benchmarks, achieving the best performance on Unlearning Score
- Entanglement metrics show significant reduction (e.g., SimNPO+CL reduces entanglement from 20.24 to 5.90 on Llama-3-8B)
- Late-layer targeting (Last-1) performs best, with performance degrading when applying CLReg to earlier layers
- The method maintains or improves model utility while enhancing privacy metrics across multiple model sizes and datasets

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Disentanglement of Forget-Retain Features
CLReg reduces forget-retain entanglement by pushing forget embeddings away from retain embeddings while clustering forget examples with their augmentations. For each forget example x_f, positive pairs are constructed via dropout and paraphrase augmentation, and retain embeddings serve as negatives. A DPO-style contrastive loss maximizes the difference between forget-forget similarity and forget-retain similarity, with Proposition 3.2 proving that anchor-negative cosine similarity strictly decreases. This mechanism assumes forget-specific concepts form separable clusters in representation space that can be disentangled without destroying retained knowledge structure.

### Mechanism 2: Late-Layer Targeting Preserves Fundamental Knowledge
The method applies CLReg to final layers to maximize disentanglement of forget concepts while preserving shared, foundational representations. Earlier layers encode fundamental language knowledge shared between forget and retain sets, while later layers encode task-specific, higher-level forget concepts. Regularizing only the last layer leaves early representations intact, based on the assumption that forget-specific knowledge is concentrated in later transformer layers while fundamental linguistic knowledge resides in earlier layers.

### Mechanism 3: Regularizer-Algorithm Complementarity
CLReg improves mainstream unlearning methods because their preference-learning objectives align with contrastive separation. Base methods like NPO/SimNPO optimize prediction-space preferences (discourage forget outputs), while CLReg operates in representation space. The combined objective provides complementary signals: prediction suppression plus representation disentanglement, with the core assumption being that representation shaping does not conflict with prediction-space objectives but instead facilitates them by making forget features easier to suppress.

## Foundational Learning

- **Concept**: Contrastive Learning (SimCLR/SimCSE paradigm)
  - Why needed here: CLReg builds directly on alignment-uniformity principles; understanding why positive pairs cluster and negatives disperse is essential for debugging the loss.
  - Quick check question: Can you explain why temperature τ in contrastive loss affects hardness of negative mining?

- **Concept**: DPO (Direct Preference Optimization) Loss
  - Why needed here: CLReg uses a DPO-style formulation rather than standard InfoNCE; understanding the logistic preference interpretation helps explain why it works for unlearning.
  - Quick check question: How does DPO differ from reward modeling, and why might a preference-style loss suit unlearning?

- **Concept**: Representation Entanglement Metrics
  - Why needed here: The paper quantifies success via variance-based entanglement, MMD, and Wasserstein distance; knowing these enables proper evaluation.
  - Quick check question: Why would high within-class variance combined with low between-class distance indicate problematic entanglement?

## Architecture Onboarding

- **Component map**: Input pipeline -> Embedding extractor -> Contrastive loss module -> Combiner
- **Critical path**:
  1. Forward pass through model to extract embeddings at target layer
  2. Apply dropout augmentation to forget embeddings (generate z_f and z_f+)
  3. Compute cosine similarities s(z_f, z_f+), s(z_f, z_r-)
  4. Calculate L_CL = -2τ * log(σ((s(z_f, z_f+) - s(z_f, z_r-))/τ))
  5. Combine with base unlearning loss; backpropagate
- **Design tradeoffs**:
  - Layer selection: Later layers → better forget separation but risk missing distributed concepts; earlier layers → preserves fundamentals but less effective disentanglement
  - Loss formulation: DPO-style vs InfoNCE — DPO is asymmetric (forget as anchor); InfoNCE symmetric option available
  - Temperature τ: Low values (0.1-0.3) → harder negatives, more aggressive separation; high values (0.7-0.9) → softer, more stable
  - Symmetric vs asymmetric: Symmetric swaps anchor/negative roles; may help when retain set is large
- **Failure signatures**:
  - Representation collapse: All embeddings converge to same point → check λ scaling, gradient norms
  - Over-unlearning: Model utility drops → reduce λ, verify retain set sampling
  - No disentanglement: Entanglement metrics unchanged → check augmentation quality, increase τ, try earlier layers
  - Training instability: Loss NaN → reduce learning rate, clamp dropout rates to [0, 0.2]
- **First 3 experiments**:
  1. Baseline reproduction: Run SimNPO on TOFU Forget Retain split with Llama-3-8B; measure ForgetScore, ModelUtility, PrivLeak to establish reference (target: UnlearningScore ~0.77)
  2. CLReg layer ablation: Add CLReg to SimNPO, test Last-1 vs Last-4 vs Last-7 layers; confirm Last-1 performs best via entanglement metrics (MK-MMD, W2) and UMAP visualization
  3. Temperature sweep: Fix SimNPO+CL on Last-1, sweep τ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}; plot ForgetScore vs ModelUtility to find Pareto-optimal point (paper finds τ=0.9 for SimNPO+CL on Llama-3-8B)

## Open Questions the Paper Calls Out

- **Can surgical removal or clipping of the disentangled forget subspace after CLReg training achieve faithful unlearning?**
  - Basis: Appendix A.4 suggests future work includes "techniques for surgical removal or clipping of the disentangled forget subspace... to achieve more faithful unlearning"
  - Why unresolved: CLReg currently pushes forget features away to reduce entanglement but does not physically remove them from the model's parameters or latent space
  - Evidence: A method that identifies and nullifies the separated forget cluster without degrading the retain cluster's performance

- **How does CLReg perform under repeated or incremental unlearning requests and continued learning?**
  - Basis: Appendix A.4 suggests studying "how CLReg performs under repeated or incremental unlearning requests and continued learning on new data"
  - Why unresolved: Current experiments focus on static benchmark scenarios rather than dynamic, sequential unlearning operations common in production
  - Evidence: Evaluation of model utility and feature entanglement after a sequence of distinct unlearning tasks interleaved with new data fine-tuning

- **Can formal privacy and fairness guarantees be derived for representation-shaped unlearning?**
  - Basis: Appendix A.4 notes it would be valuable to "derive formal privacy and fairness guarantees for representation-shaped unlearning"
  - Why unresolved: The paper relies on empirical privacy metrics rather than theoretical proofs of information erasure
  - Evidence: Theoretical bounds on mutual information or membership inference advantage specifically for the latent space post-regularization

## Limitations

- The reliance on paraphrase augmentation for positive pairs is underspecified—no implementation details provided for how paraphrases are generated, which could significantly affect performance if done poorly.
- Layer selection strategy assumes forget-specific knowledge concentrates in later layers, but this distribution may vary across tasks and domains, limiting generalizability.
- The paper does not address potential negative transfer when applying CLReg to already well-aligned models, where representation shaping might degrade performance rather than improve it.

## Confidence

- **High confidence**: The core mechanism of contrastive representation disentanglement is well-supported by theoretical analysis and empirical results showing consistent entanglement reduction across benchmarks.
- **Medium confidence**: The late-layer targeting hypothesis is plausible but relies on assumptions about feature distribution that weren't directly validated through ablation across diverse knowledge types.
- **Medium confidence**: The complementarity claim is supported by ablation showing CLReg improves multiple base methods, though the exact optimization dynamics between prediction-space and representation-space objectives remain unclear.

## Next Checks

1. **Layer distribution analysis**: Conduct controlled experiments varying the forget concept complexity (simple facts vs. complex reasoning patterns) to validate whether simpler forget concepts might be distributed across earlier layers, not just late layers.

2. **Ablation of augmentation methods**: Compare dropout-only augmentation vs. dropout+paraphrase vs. dropout+backtranslation to quantify the contribution of each augmentation type to disentanglement performance.

3. **Transfer stability test**: Apply CLReg to a pre-aligned LLM (e.g., one fine-tuned on SFT) to verify that representation shaping doesn't degrade already-optimized preference alignment, measuring both entanglement and preference consistency.