---
ver: rpa2
title: 'Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline
  Reinforcement Learning'
arxiv_id: '2504.01719'
source_url: https://arxiv.org/abs/2504.01719
tags:
- odaf
- policy
- learning
- offline
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of offline reinforcement learning\
  \ using non-expert data, which is common in practical applications. The authors\
  \ propose Outcome-Driven Action Flexibility (ODAF), a novel method that reduces\
  \ reliance on the empirical action distribution of the behavior policy by evaluating\
  \ actions based on whether their outcomes meet safety requirements\u2014remaining\
  \ within the state support area\u2014rather than solely depending on the actions'\
  \ likelihood based on offline data."
---

# Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2504.01719
- **Source URL**: https://arxiv.org/abs/2504.01719
- **Reference count**: 40
- **Primary result**: ODAF achieves 87.9 average score on standard MuJoCo tasks, significantly outperforming other methods

## Executive Summary
This paper addresses offline reinforcement learning with non-expert data by proposing Outcome-Driven Action Flexibility (ODAF), a novel method that evaluates actions based on whether their outcomes remain within the state support area of the behavior policy. Unlike traditional approaches that constrain actions to match the behavior policy distribution, ODAF focuses on ensuring transitioned states stay within safe regions, enabling better "trajectory stitching" from sub-optimal demonstrations. The method uses uncertainty quantification via Q-ensemble disagreement to identify out-of-distribution outcomes and demonstrates state-of-the-art performance on MuJoCo and maze benchmarks.

## Method Summary
ODAF is an offline RL method that relaxes action constraints by focusing on outcome safety rather than action similarity to the behavior policy. It defines a candidate policy set Π where the transitioned distribution P(s'|s,π) stays within the support of the behavior policy's state distribution dπβ, enabling the agent to learn from non-expert data while maintaining safety. The method uses uncertainty quantification techniques, specifically Q-ensemble disagreement, to identify whether action outcomes are in-distribution. ODAF is implemented with SAC-N and theoretically justified with convergence guarantees, achieving superior stability and performance when learning from non-expert demonstrations.

## Key Results
- Achieves 87.9 average score on standard MuJoCo tasks, significantly outperforming other methods
- Demonstrates superior stability when learning on non-expert data compared to traditional offline RL methods
- Successfully enables trajectory stitching, achieving 94.8 vs 79.6 on PointMaze benchmarks (next best method)
- Shows effective learning from non-expert data while maintaining safety through outcome-based constraints

## Why This Works (Mechanism)

### Mechanism 1: Outcome-Based Action Evaluation
- Claim: Evaluating actions by their outcomes rather than their similarity to behavior policy enables learning from non-expert data while maintaining safety.
- Mechanism: ODAF defines candidate policy set Π where transitioned distribution P(s'|s,π) stays within support of behavior policy's state distribution dπβ, decoupling action selection from behavior cloning while ensuring consequences remain safe.
- Core assumption: Dataset provides sufficient coverage over optimal state transitions even if action distributions are sub-optimal.
- Evidence anchors:
  - [abstract]: "evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data"
  - [section 4.1]: "Π = {π|∀s ∈ D, supp(P(s'|s,π)) ⊆ supp(dπβ(s'))}"
  - [corpus]: Policy Constraint by Only Support Constraint explores related support-based constraints from action perspective

### Mechanism 2: Uncertainty-Based Outcome Safety Proxy
- Claim: Q-ensemble disagreement serves as a reliable proxy for whether action outcomes are in-distribution.
- Mechanism: Uses standard deviation across K Q-functions: U^π(s) = β·Std(Q_k(s,a)). When actions lead to OOD states, Q-ensembles show higher disagreement, which ODAF loss penalizes: L_odaf = E_s[max_{ŝ∈B_ε}(∑_{s'} P(s'|ŝ,π)U^π'(s'))).
- Core assumption: For all OOD state-action pairs, uncertainty is strictly positive: ∀(s,a) ∉ supp(D), U_min ≤ U(s,a).
- Evidence anchors:
  - [abstract]: "implemented using uncertainty quantification techniques, effectively tolerates unseen transitions"
  - [section 4.2]: Eq. 11 defines uncertainty estimator; Theorem 1 proves it bounds OOD outcome probability
  - [corpus]: Corpus evidence is weak—no directly comparable papers using uncertainty for outcome-based constraints found

### Mechanism 3: Trajectory Stitching via Outcome Flexibility
- Claim: Removing action-distribution constraints enables combining optimal segments from multiple sub-optimal trajectories.
- Mechanism: Traditional methods penalize actions not in behavior policy, preventing "shortcut" actions connecting trajectories. ODAF allows this if resulting state appears in dataset. Bellman operator T^Π uses max_{π∈Π} to select value-maximizing actions within outcome-constrained set, enabling dynamic programming across trajectory boundaries.
- Core assumption: Dataset contains necessary "bridge" transitions for stitching (scattered across different episodes).
- Evidence anchors:
  - [section 1]: Figure 1 illustrates stitching sub-optimal trajectories into higher-value trajectory
  - [section 5.3]: PointMaze results: ODAF achieves 94.8 vs 79.6 (next best), successfully generating S→M→G from sub-optimal data
  - [corpus]: Related work on trajectory stitching via model-based approaches exists but requires costly rollouts

## Foundational Learning

- Concept: **Distributional Shift in Offline RL**
  - Why needed here: ODAF's entire purpose is addressing distributional shift differently than prior methods. Understanding Q-value overestimation on OOD actions is essential.
  - Quick check question: Why does taking actions outside the offline dataset lead to cascading Q-value errors during training?

- Concept: **Bellman Operators and Contraction**
  - Why needed here: Paper proves ODAF's operator T^Π is a contraction (Lemma 1), guaranteeing convergence. This theoretical grounding is non-trivial.
  - Quick check question: What property must a Bellman operator have for guaranteed convergence, and how does constraint set Π affect this?

- Concept: **Q-Ensemble Uncertainty Estimation**
  - Why needed here: ODAF's implementation uses disagreement among Q-networks as uncertainty signal—the bridge between theory and practice.
  - Quick check question: Why would multiple Q-functions trained on identical data produce different predictions on OOD inputs?

## Architecture Onboarding

- Component map:
  - **Q-Ensembles (K=10)** -> **Policy Network (π)** -> **Dynamics Model (P̂)** -> **Evaluation Policy (π')**

- Critical path:
  1. Pre-train dynamics model P̂(s'|s,a) on offline data
  2. Sample batch; generate adversarial perturbation ŝ around state s
  3. Sample action â ~ π(·|ŝ); predict outcome ŝ' = P̂(ŝ,â)
  4. Compute uncertainty U^π'(ŝ') via Q-ensemble std
  5. Update π: L_π = actor_loss + β_odaf · L_odaf
  6. Update Q-networks via standard TD3-style critic loss

- Design tradeoffs:
  - **β_odaf (0.3)**: Too high → over-constrained; too low → OOD outcomes allowed
  - **ε_odaf perturbation (0.001-0.07)**: Controls robustness; task-dependent
  - **Q-ensemble size (K=10)**: More ensembles → better uncertainty, higher compute
  - **Dynamics model quality**: Validation study (Fig 5) shows imperfect models suffice

- Failure signatures:
  - **Over-conservatism**: Performance below baselines → β_odaf too high
  - **OOD collapse**: Unsafe actions taken → check Q-ensemble diversity
  - **No stitching**: Trajectories exactly match dataset → dynamics model may be miscalibrated
  - **Training divergence**: Q-values exploding → verify target network soft-updates

- First 3 experiments:
  1. **Sanity check**: Run ODAF on Halfcheetah-medium-expert. Target: ~111.1 ±2.4. If significantly lower, verify Q-ensemble training and L_odaf implementation.
  2. **Ablation**: Compare ODAF vs ODAF w/o L_odaf on medium-replay dataset. Expect ~20% gap (Fig 6 shows 57.9→92.3 on some tasks).
  3. **Stitching validation**: Create simple PointMaze with two sub-optimal trajectory types. Visualize if learned policy generates stitched paths vs copying dataset trajectories—directly tests the core mechanism claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is ODAF to errors in the pre-trained dynamics model P̂(s'|s,a), and can the method maintain performance guarantees when model error is large or non-uniform across the state space?
- Basis in paper: [inferred] The theoretical analysis uses the true dynamics P for contraction proofs, while implementation relies on a pre-trained dynamics model. Section 5.5 only validates that the pre-trained model can distinguish safe vs. unsafe actions but does not analyze degradation under systematic model errors.
- Why unresolved: Theorem 2 bounds performance based on dataset coverage but does not explicitly account for dynamics model approximation error. The relationship between model accuracy and ODAF's ability to correctly estimate outcome safety remains unquantified.
- What evidence would resolve it: Experiments with dynamics models of varying accuracy (e.g., trained on subsets of data, or with injected noise), plus theoretical extension of Theorem 2 incorporating explicit model error terms.

### Open Question 2
- Question: What are the necessary and sufficient conditions on dataset coverage for ODAF to successfully stitch trajectories, and how does this coverage requirement compare to action-supported methods?
- Basis in paper: [explicit] Corollary 1 states that "if we assume the dataset has sufficient coverage over the optimal policy's stationary state distribution," then performance is bounded, but the paper does not quantify what "sufficient" means or provide minimal coverage requirements for trajectory stitching.
- Why unresolved: The coverage assumption sup_s d_π*(s)/d_πβ(s) ≤ C is stated as sufficient but not necessary. The PointMaze experiments demonstrate stitching empirically, but the theoretical relationship between coverage and stitching capability remains informal.
- What evidence would resolve it: Systematic experiments varying dataset composition (e.g., density of stitching-critical transitions) combined with theoretical analysis deriving coverage thresholds for successful stitching.

### Open Question 3
- Question: How does the computational overhead of ODAF (specifically dynamics model inference and uncertainty estimation via Q-ensembles) scale with state/action dimensionality, and can the method be extended to high-dimensional visual domains?
- Basis in paper: [inferred] The implementation requires maintaining K Q-networks and performing dynamics model inference for each policy update (Algorithm 1). The paper evaluates on MuJoCo (17–376 dimensions) and maze tasks but does not analyze scalability or test on image-based benchmarks.
- Why unresolved: No wall-clock time comparisons or memory analysis are provided. The uncertainty estimator U^π(s) ≈ β · Std(Q_k(s,a)) requires K forward passes, and the dynamics model adds further computation, which may limit applicability to large-scale or real-time settings.
- What evidence would resolve it: Timing experiments across varying state dimensions, scaling analysis, and evaluation on high-dimensional benchmarks (e.g., Atari or robotic manipulation from pixels).

### Open Question 4
- Question: Can ODAF be combined with model-based rollouts or return-conditioned policies to further enhance trajectory stitching, and would such combinations inherit ODAF's robustness to non-expert data?
- Basis in paper: [explicit] The paper compares against MBRCSL in Section 5.3, noting that "unlike MBRCSL, our method ODAF does not need large number of rollouts with the pre-trained model, which means that ODAF is less likely to suffer from the error accumulation." However, whether complementary integration is possible remains unexplored.
- Why unresolved: MBRCSL and ODAF represent different approaches to enabling stitching (model rollouts vs. outcome-driven constraints). Their potential synergy or incompatibility is not investigated.
- What evidence would resolve it: Hybrid algorithm design experiments combining ODAF's uncertainty-based regularization with model-based planning or return-conditioning, evaluated on stitching-intensive benchmarks.

## Limitations

- **Uncertainty quantification assumption**: The method assumes Q-ensemble disagreement reliably identifies OOD states, but this assumption isn't rigorously validated across diverse environments and uncertainty estimation methods
- **Dynamics model dependency**: ODAF's performance is tied to the quality of the pre-trained dynamics model, with compounding errors potentially degrading outcome safety evaluation
- **Hyperparameter sensitivity**: The method requires careful tuning of β_odaf and ε_odaf, with sensitivity potentially varying significantly across domains and not well-characterized

## Confidence

- **High confidence**: Performance claims on standard MuJoCo benchmarks (87.9 average score) are well-supported by experimental results and align with known behavior of top offline RL methods
- **Medium confidence**: The trajectory stitching mechanism in PointMaze (94.8 vs 79.6) is demonstrated but relies on specific dataset properties that may not generalize
- **Low confidence**: The theoretical assumptions about uncertainty being strictly positive for OOD states (Assumption 1) are stated but not empirically validated across different uncertainty estimation methods

## Next Checks

1. Ablation study isolating Q-ensemble uncertainty vs alternative OOD detection methods (e.g., ensembles with different architectures, Bayesian methods)
2. Sensitivity analysis of β_odaf and ε_odaf across multiple environments to identify robustness bounds
3. Investigation of dynamics model error propagation by comparing ODAF performance with ground-truth dynamics vs learned dynamics in controlled environments