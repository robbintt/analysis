---
ver: rpa2
title: 'FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis'
arxiv_id: '2510.13936'
source_url: https://arxiv.org/abs/2510.13936
tags:
- research
- financial
- deep
- performance
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FinDeepResearch, a benchmark and evaluation
  framework designed to rigorously assess Deep Research (DR) agents in corporate financial
  analysis. The authors propose HisRubric, a novel evaluation framework with a hierarchical
  analytical structure and fine-grained grading rubric that mirrors professional analyst
  workflows across four key capabilities: Recognition, Calculation, Abstraction, and
  Interpretation.'
---

# FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis

## Quick Facts
- arXiv ID: 2510.13936
- Source URL: https://arxiv.org/abs/2510.13936
- Reference count: 40
- Primary result: DR agents outperform other methods in financial analysis but max accuracy is only 37.9/100

## Executive Summary
This paper introduces FinDeepResearch, a benchmark and evaluation framework designed to rigorously assess Deep Research (DR) agents in corporate financial analysis. The authors propose HisRubric, a novel evaluation framework with a hierarchical analytical structure and fine-grained grading rubric that mirrors professional analyst workflows across four key capabilities: Recognition, Calculation, Abstraction, and Interpretation. Extensive experiments with 16 methods (6 DR agents, 5 LLMs with thinking+search, and 5 LLMs with thinking only) reveal that while most methods successfully follow the prescribed analytical structure, they consistently struggle with producing precise information, particularly in Interpretation and non-English markets.

## Method Summary
FinDeepResearch evaluates DR agents using a structured approach with 64 companies from 8 financial markets across 4 languages, totaling 15,808 grading items. The HisRubric framework employs a hierarchical analytical structure (6 sections, 18 subsections, 247 grading items) mapped to four capabilities. Evaluation uses three protocols: binary accuracy, claim coverage, and criterion-based scoring via an advanced LLM judge. The benchmark separates "Structural Rigor" (format compliance) from "Information Precision" (factual accuracy), with ground truth generated through multi-LLM voting and expert verification.

## Key Results
- DR agents generally outperform other approaches, particularly in Recognition and Calculation capabilities
- Highest accuracy score achieved is only 37.9 out of 100, indicating substantial room for improvement
- All methods face significant challenges in Interpretation and non-English markets
- Structural compliance is high across methods while factual precision remains low

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hierarchical analytical structure forces agents into expert-mode workflows, revealing specific capability gaps.
- **Mechanism:** By demanding a predefined, 6-section/18-subsection report structure with 247 fine-grained grading items, the framework constrains output space and decomposes "research quality" into measurable primitives (Recognition, Calculation, Abstraction, Interpretation).
- **Core assumption:** The provided hierarchical structure comprehensively captures essential financial analysis, and poor performance on specific grading items accurately reflects an agent's underlying capability deficit rather than prompt/format confusion.
- **Evidence anchors:**
  - [abstract] "This framework mirrors the professional analyst's workflow, progressing from data recognition to metric calculation, and finally to strategic summarization and interpretation."
  - [section 2.2 & 2.3] The paper details the 6-section structure and maps 247 grading items to four specific capabilities.
- **Break condition:** If agents fail primarily due to ambiguous instructions or inability to follow the markdown template (a formatting issue), rather than the underlying analytical task, the diagnostic value for "deep research" capability degrades.

### Mechanism 2
- **Claim:** Separating "Structural Rigor" (format adherence) from "Information Precision" (factual accuracy) isolates different agent failure modes.
- **Mechanism:** The evaluation uses a rule-based "structure score" for format compliance (42 points for sections/subsections/tables) and a model-based "accuracy score" for content (15,808 grading items).
- **Core assumption:** High structural compliance with low precision is a meaningful signal of a reasoning/retrieval deficit rather than a systemic problem with the evaluation pipeline.
- **Evidence anchors:**
  - [abstract] "...structural rigor, which examines whether the agent's findings and reasoning are organized into a coherent, verifiable analytical structure; and Information Precision, which inspects whether its claims are specific, accurate..."
  - [section 4.2] Figure 3 (Information Precision) shows low scores (max 37.9), while Figure 4 (Structural Rigor) shows high scores for many models.
- **Break condition:** If the grading rubric for "Information Precision" is itself flawed (e.g., the LLM evaluator is unreliable for complex qualitative claims), the attribution of failure to the agent is compromised.

### Mechanism 3
- **Claim:** Multi-market, multi-language coverage exposes cross-lingual and cross-jurisdictional retrieval and reasoning bottlenecks.
- **Mechanism:** The benchmark includes 64 companies from 8 financial markets (US, UK, CN, HK, AU, SG, MY, ID) across 4 languages (English, Simplified Chinese, Traditional Chinese, Bahasa Indonesia).
- **Core assumption:** The observed performance drop in non-English markets is primarily due to the agents' capabilities, not a lack of available training data or accessible web sources for those markets.
- **Evidence anchors:**
  - [abstract] "...all methods face significant challenges in... non-English markets."
  - [section 4.3/ Table 4] Peak scores for CN (34.7) and HK (36.4) are notably lower than for SG (46.7) or UK (43.0).
  - [corpus] A related paper, FinResearchBench (arxiv:2507.16248), also targets financial research agents but notes existing benchmarks lack rigorous evaluation.
- **Break condition:** If the ground-truth data for non-English markets is of lower quality or harder to access, the benchmark may be measuring data availability rather than agent capability.

## Foundational Learning

- **Concept: Deep Research (DR) Agents** (vs. RAG, vs. standard LLMs)
  - **Why needed here:** DR agents are the specific system under test. They differ from simple RAG by involving autonomous planning, multi-round retrieval, and iterative reasoning, not just single-pass query-answer.
  - **Quick check question:** What specific architectural components (e.g., planning loop, tool use) distinguish a DR agent from a standard LLM with a search tool enabled?

- **Concept: HisRubric's Four Capabilities** (Recognition, Calculation, Abstraction, Interpretation)
  - **Why needed here:** These form the core analytical framework. Recognizing that 'Recognition' (extracting facts) is easier than 'Interpretation' (drawing insightful conclusions) explains the experimental results.
  - **Quick check question:** Which of the four capabilities is most likely to fail if an agent has excellent retrieval but poor long-context reasoning?

- **Concept: Agentic Evaluation via Grading Rubrics** (Accuracy, Claim-based, Criterion-based)
  - **Why needed here:** Evaluating long-form research reports cannot be done with simple binary metrics. This paper uses three protocols: binary accuracy, claim coverage, and criterion-based scoring.
  - **Quick check question:** For a generated financial insight, would you use Accuracy, Claim-based, or Criterion-based scoring, and why?

## Architecture Onboarding

- **Component map:** Input -> Planning -> Execution (Retrieval + Analysis) -> Synthesis -> Evaluation (Structure + Precision)
- **Critical path:**
  1. **Input:** Receive task for a company (e.g., NVIDIA) and fiscal years.
  2. **Planning:** Agent decomposes the task into the 6-section structure and identifies data needs.
  3. **Execution:**
     - For Sections 1-5: Retrieve and analyze annual reports.
     - For Section 6: Use web search for stock prices, news, and market data.
  4. **Synthesis:** Populate the markdown template with analyzed data, performing calculations and generating interpretations.
  5. **Evaluation:** Output report is parsed for structure and scored item-by-item for precision.
- **Design tradeoffs:**
  - **Granularity vs. Cost:** The 247 grading items provide high-resolution diagnostics but make human ground-truth creation and verification expensive.
  - **Automation vs. Reliability:** Using an advanced LLM for grading enables at-scale evaluation but introduces potential model bias.
- **Failure signatures:**
  - **High Structure, Low Precision:** Agent follows format but hallucinates facts.
  - **Low Performance in Sections 3 & 6:** Fails on Business Analysis (Abstraction/Interpretation) and Market Performance (integrated multi-source analysis).
  - **Non-English Market Drop:** Systematic underperformance on CN/HK companies suggests cross-lingual data access or reasoning bottlenecks.
- **First 3 experiments:**
  1. **Baseline Run:** Run a top-tier DR agent (e.g., OpenAI o3-deep-research) on all 64 companies to establish a benchmark score and identify the lowest-scoring sections.
  2. **Ablation on Retrieval:** Run the same agent but restrict it from using web search (treating it as a T+S model) to quantify the performance drop, especially in Section 6.
  3. **Error Analysis on Interpretation:** Take the 10 lowest-scoring items in the "Interpretation" capability and conduct a manual review of the agent's output vs. the ground truth to categorize failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural or training improvements are required to overcome the significant performance gap in "Interpretation" capabilities compared to "Recognition" and "Calculation"?
- **Basis in paper:** [explicit] The paper notes that "Interpretation" is the most difficult capability (peaking at a score of 20.3) compared to Recognition (59.5), stating explicitly that "improving Interpretation capability should be a promising focus for future research."
- **Why unresolved:** While current Deep Research (DR) agents excel at extracting facts and performing calculations, they struggle to synthesize these inputs into high-quality, insightful qualitative analysis (e.g., analyzing news sentiment or market reaction).
- **What evidence would resolve it:** The development of a DR agent that achieves a normalized accuracy score greater than 50 in the Interpretation capability on the FinDeepResearch benchmark, closing the gap with Recognition scores.

### Open Question 2
- **Question:** Can the HisRubric evaluation framework be effectively generalized to other rigorous professional domains such as legal or clinical research?
- **Basis in paper:** [explicit] The conclusion explicitly states: "Future work can extend our framework to other domains, such as legal and clinical research..."
- **Why unresolved:** The current hierarchical structure and grading rubric are deeply specialized for corporate financial analysis workflows; it is unclear if the four defined capabilities (Recognition, Calculation, Abstraction, Interpretation) map effectively to the reasoning standards of legal or medical fields.
- **What evidence would resolve it:** A study demonstrating that HisRubric, when adapted for legal case analysis, shows a strong correlation with expert human evaluation, similar to its performance in the financial domain.

### Open Question 3
- **Question:** What are the primary factors contributing to the performance degradation of DR agents in non-English (specifically Chinese) markets?
- **Basis in paper:** [inferred] The results (Table 4) show that CN and HK markets present a "tougher challenge" with lower peak scores compared to US/UK markets, which the authors suggest may stem from "complexity... processing languages other than Latin languages."
- **Why unresolved:** The paper identifies the performance gap but does not isolate the root causeâ€”whether it is due to deficient retrieval of Chinese documents, limitations in the LLMs' reasoning in non-English contexts, or lack of specific financial domain knowledge in those languages.
- **What evidence would resolve it:** A comparative error analysis isolating retrieval failures versus reasoning failures in Chinese markets, or the demonstration of a DR agent that achieves performance parity between US and CN/HK markets through targeted fine-tuning.

## Limitations

- The benchmark's multilingual scope may inadvertently measure data accessibility rather than pure agent capability, particularly for non-Latin script markets
- The 247-item grading rubric, though comprehensive, may not capture all nuances of high-quality financial analysis
- The evaluation was conducted over a fixed 3-hour budget, which may not reflect optimal agent performance under different time constraints

## Confidence

- **High Confidence:** The structural findings (high Structural Rigor scores with low Information Precision scores) are directly observable from the reported metrics and robust to evaluation methodology.
- **Medium Confidence:** The interpretation of capability gaps (e.g., Abstraction and Interpretation being harder than Recognition and Calculation) is supported by the data but relies on the assumption that the grading rubric accurately captures these dimensions.
- **Low Confidence:** The absolute performance numbers (e.g., "best accuracy of 37.9") should be interpreted cautiously as they depend heavily on the specific evaluation protocol and rubric design.

## Next Checks

1. **Ablation Study on Grading Protocol:** Run a subset of companies through multiple grading protocols (human-only, different LLM judges, simplified rubrics) to quantify the impact of the evaluation methodology on final scores and verify that the capability gap findings are robust.

2. **Time Budget Sensitivity Analysis:** Repeat the top-performing DR agent evaluation with varied time budgets (1 hour, 6 hours) to determine whether the observed performance ceiling is due to inherent capability limits or time constraints.

3. **Out-of-Distribution Testing:** Evaluate the same agents on financial analysis tasks outside the benchmark scope (e.g., private company analysis, different industries, alternative financial reporting frameworks) to assess generalizability of the capability gap findings.