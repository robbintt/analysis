---
ver: rpa2
title: Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot
  Wireless Indoor Navigation
arxiv_id: '2506.22365'
source_url: https://arxiv.org/abs/2506.22365
tags:
- learning
- policy
- neural
- reinforcement
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PiPRL, a hierarchical neuro-symbolic framework
  for wireless indoor navigation using physics-informed symbolic programs as inductive
  biases in reinforcement learning. It addresses the challenge of zero-shot generalization
  in complex indoor environments where mmWave signals propagate through multiple paths.
---

# Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation

## Quick Facts
- arXiv ID: 2506.22365
- Source URL: https://arxiv.org/abs/2506.22365
- Reference count: 36
- Primary result: PiPRL reduces training time by 26%+ and achieves better zero-shot navigation efficiency than purely symbolic or neural approaches across LOS, 1-NLOS, and 2+-NLOS scenarios.

## Executive Summary
This paper addresses wireless indoor navigation where a robot must reach a stationary mmWave transmitter using noisy pose estimates, RGB images, and mmWave signals without prior knowledge of the environment. The proposed PiPRL framework combines physics-informed symbolic programs with reinforcement learning to achieve sample-efficient zero-shot generalization. By encoding physics priors—such as signal strength monotonicity and angle-of-arrival navigation—into human-readable symbolic programs, PiPRL guides the RL policy search and improves navigation efficiency in unseen environments.

## Method Summary
PiPRL is a hierarchical neuro-symbolic framework that integrates physics-informed symbolic programs as inductive biases in reinforcement learning. The system uses a domain-specific language (RLang) to encode physics priors about mmWave signal propagation, which are then used to either directly prescribe actions or constrain the action space during RL training. The framework consists of three modules: neural perception (SLAM and signal processing), physics-informed symbolic programs (encoded in RLang), and a neural controller. The symbolic program acts as a meta-controller that selects between physics-based policies and neural policies based on estimated link state, while importance-weighted reward corrections maintain unbiased policy gradients when substituting actions.

## Key Results
- PiPRL reduces training time by 26-45% compared to non-physics-based RL baselines
- Achieves better navigation efficiency in unseen testing environments across all link state scenarios
- Outperforms both purely symbolic and neural approaches in zero-shot generalization tests

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Programs as Structured Inductive Biases
Encoding physics priors in a domain-specific language reduces policy search space and improves sample efficiency. The RLang DSL expresses three physics priors—reversibility (follow strongest path's angle of arrival), SNR monotonicity (move toward higher signal strength), and link state monotonicity (avoid transitions to higher-order NLOS)—which either directly prescribe actions or constrain the action space during RL training. Core assumption: physics priors that are intuitive to humans translate to effective inductive biases when formalized symbolically.

### Mechanism 2: Importance-Weighted Reward Correction for Action Substitution
Replacing non-compliant actions with physics-compliant ones while applying importance sampling corrections preserves unbiased policy gradients. When the neural policy samples an action outside the ActionRestriction set, PiPRL substitutes it with a random compliant action and reweights the cost: ĉt = #compliant · πϕ(Ω't) · ct. This maintains unbiased gradient estimates while forcing physics compliance during training. Core assumption: the importance sampling correction adequately compensates for the distribution shift introduced by action substitution.

### Mechanism 3: Hierarchical Policy Selection Based on Link State
Switching between symbolic and neural policies based on estimated link state leverages strengths of each approach. The meta-program executes reverse AoA for link states ≤ 2 (LOS/1-NLOS) where reversibility is reliable, and switches to Neural Policy with SNR/link state priors for higher-order NLOS where physics becomes ambiguous. Core assumption: the link state classifier provides sufficiently accurate estimates to route policies correctly.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The agent receives only partial observations (pose estimates, RGB images, mmWave readings) and must maintain belief states over true positions.
  - Quick check question: Can you explain why history-dependent policies (π(ht)) are necessary when observations are partial?

- Concept: **Importance Sampling for Off-Policy Evaluation**
  - Why needed here: PiPRL substitutes actions during training; understanding how likelihood ratios correct distribution shift is essential for debugging reward corrections.
  - Quick check question: If a compliant action has probability 0.01 under the neural policy but 0.2 uniformly, what is the importance weight?

- Concept: **mmWave Signal Propagation Physics**
  - Why needed here: The three priors (reversibility, SNR monotonicity, link state) derive from electromagnetic wave behavior—without this intuition, you cannot extend or debug the DSL rules.
  - Quick check question: Why does the strongest mmWave path typically experience the fewest reflections?

## Architecture Onboarding

- Component map: Raw sensors → Neural Perception (features) → Symbolic Program (policy selection/constraints) → PPO Policy (waypoint) → Planner → Visual Controller → Motor actions
- Critical path: Raw sensors → Neural Perception (features) → Symbolic Program (policy selection/constraints) → PPO Policy (waypoint) → Planner → Visual Controller → Motor actions
- Design tradeoffs:
  - **Symbolic vs. Neural coverage**: More symbolic rules improve interpretability but require domain expertise; fully neural is flexible but sample-inefficient
  - **Action restriction strictness**: Tighter constraints enforce physics compliance but may exclude optimal paths in edge cases
  - **Link state threshold**: Setting threshold too low forces neural policy on easier cases; too high risks reverse AoA failure in complex NLOS
- Failure signatures:
  - NPL > 4.0 in 2+-NLOS: Check link state classifier accuracy and SNR prior ablation
  - Training instability (diverging loss): Check importance weight magnitudes—may need clipping
  - Collision failures: Visual controller may need retraining on new map geometries
- First 3 experiments:
  1. **Baseline comparison**: Run PiPRL vs. NPRL vs. W_AN on maps A-D, measure GPU hours to convergence—should see 25-45% reduction per Table 1
  2. **Ablation on priors**: Remove SNR prior and link state prior separately; expect NPL degradation per Table 5, especially in 2+-NLOS
  3. **Zero-shot generalization test**: Train on maps A-O, test on P-U without fine-tuning; target NPL < 2.7 for 2+-NLOS (Table 7)

## Open Questions the Paper Calls Out
- Can the physics-informed symbolic programs be automatically synthesized from natural language descriptions to eliminate the need for manual RLang encoding? (Conclusion mentions this as future work)
- How robust is the symbolic meta-program when the neural perception module provides erroneous feature estimates (e.g., noisy pose or path estimates)?
- Does PiPRL maintain its sample efficiency and zero-shot generalization capabilities when transferred from the Gibson simulation environment to real-world robotic hardware?

## Limitations
- Lack of accessible RLang DSL implementation details prevents exact reproduction of symbolic program integration
- mmWave simulation parameters and tensor generation methodology are underspecified
- Source code for pretrained modules (SLAM and visual controller) is not provided

## Confidence
- **High confidence**: The core claim that physics priors improve sample efficiency is well-supported by the 26-45% training reduction and consistent NPL improvements across all link state scenarios
- **Medium confidence**: The importance-weighted reward correction mechanism has theoretical justification but lacks extensive empirical validation in the literature
- **Low confidence**: The zero-shot generalization performance on the most challenging 2+-NLOS scenarios, while better than baselines, still shows room for improvement (NPL 2.60-3.50)

## Next Checks
1. **Importance Weight Stability**: During reproduction, monitor the magnitude distribution of importance weights during training. If weights exceed 10× the average, implement clipping and assess impact on convergence and NPL performance.
2. **Link State Classifier Accuracy**: Evaluate the link state classifier independently on a held-out validation set from the training maps. Target accuracy >85% for correct policy routing, as misclassification directly degrades NPL in 2+-NLOS scenarios.
3. **Symbolic vs. Neural Coverage Analysis**: Log the fraction of actions executed by each policy (reverse AoA vs. Neural Policy) across different link states. Verify that the meta-program correctly routes policies and that neural policy is invoked appropriately in high-NLOS scenarios.