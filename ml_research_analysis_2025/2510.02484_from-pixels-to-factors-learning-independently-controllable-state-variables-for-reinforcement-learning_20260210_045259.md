---
ver: rpa2
title: 'From Pixels to Factors: Learning Independently Controllable State Variables
  for Reinforcement Learning'
arxiv_id: '2510.02484'
source_url: https://arxiv.org/abs/2510.02484
tags:
- learning
- factored
- state
- factors
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Action-Controllable Factorization (ACF),
  a contrastive learning method that learns a factored latent representation from
  pixel observations in reinforcement learning environments. The key insight is to
  exploit the fact that agent actions typically affect only a subset of state variables
  while others evolve naturally, enabling contrastive training that isolates independently
  controllable factors.
---

# From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.02484
- **Source URL**: https://arxiv.org/abs/2510.02484
- **Reference count**: 40
- **Primary result**: ACF learns independently controllable latent factors from pixel observations, outperforming DMS and GCL in factorization quality on Taxi, FourRooms, and DoorKey environments.

## Executive Summary
This paper introduces Action-Controllable Factorization (ACF), a contrastive learning method that learns a factored latent representation from pixel observations in reinforcement learning environments. The key insight is to exploit the fact that agent actions typically affect only a subset of state variables while others evolve naturally, enabling contrastive training that isolates independently controllable factors. ACF uses energy-based parameterization and contrastive objectives to align latent factors with ground-truth controllable state variables, leveraging action-induced discrepancies in next-state predictions against natural dynamics. Empirically, ACF recovers ground-truth controllable factors directly from pixel observations on Taxi, FourRooms, and MiniGrid-DoorKey environments, consistently outperforming baseline disentanglement algorithms like DMS and GCL in factorization metrics.

## Method Summary
ACF learns factored representations by contrasting action-driven transitions against natural dynamics using an energy-based model. The method assumes a factored transition structure where each factor evolves independently, and trains per-factor energy functions to capture action effects. A key component is the ratio loss that contrasts action effects against no-op dynamics, encouraging each energy function to specialize in detecting the unique signature of one action's effect. The model combines four losses: a ratio loss for factorization, forward and inverse InfoNCE losses for Markov preservation, and a policy classification loss. Training uses AdamW with noise injection on latents, and evaluation measures factorization quality via R² matrices comparing learned latents to ground-truth state variables.

## Key Results
- ACF consistently outperforms DMS and GCL baselines on factorization metrics across Taxi, FourRooms, and DoorKey environments
- Diagonal R² scores reach 0.56-0.73 on Taxi (controllable factors) while off-diagonal scores remain below 0.24
- Ablation studies show each loss component contributes significantly to factorization quality
- ACF successfully identifies independently controllable factors directly from pixel observations without access to ground-truth state variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrasting action-driven transitions against natural dynamics isolates controllable factors.
- Mechanism: The core ratio log r_a(x',x) = log T(x'|x,a) / T(x'|x,a₀) measures how much action a deviates from natural evolution. For factors unaffected by a, the ratio collapses to zero; only the influenced factor shows signal. Training binary classifiers on this ratio forces each energy function to specialize in detecting the unique signature of one action's effect.
- Core assumption: Each factor is independently controllable—there exists at least one action whose effect on that factor differs measurably from the no-op baseline (Assumption: action effects are sufficiently different from natural dynamics in practice).
- Evidence anchors:
  - [section 3.2] "The core idea is to contrast the effect of an action... against the natural dynamics... using the following ratio: log r_a(x',x) = log T(x'|x,a) / T(x'|x,a₀)... this ratio is a function of the factor s'_j and not the rest."
  - [abstract] "ACF leverages sparsity: actions typically affect only a subset of variables, while the rest evolve under the environment's dynamics, yielding informative data for contrastive training."
  - [corpus] Weak direct support; neighbor papers address factored RL but not this specific contrastive mechanism.
- Break condition: If actions affect multiple factors simultaneously with similar magnitude, or if natural dynamics are indistinguishable from action effects (e.g., deterministic environments with no stochasticity), the ratio signal degrades and factorization fails.

### Mechanism 2
- Claim: Per-factor energy functions with joint training enforce Markov preservation while enabling factorization.
- Mechanism: The factorized transition T(z'|z,a) ∝ exp(∑ᵢ E_θ(z'_i,a,z)) decomposes next-state prediction into independent per-factor energies. The forward loss (InfoNCE) and inverse loss jointly ensure the representation captures action-relevant information, while the ratio loss L_r pushes each energy to respond to only one action's effect.
- Core assumption: The true transition dynamics factorize according to Equation 1, and the number of latent factors K matches or exceeds the ground truth.
- Evidence anchors:
  - [section 3.2] "We parameterize the transition function as the sum of energy functions... T(z'|z,a) ∝ exp(∑ᵢ E_θ(z'_i,a,z))"
  - [section 4, ablation] Removing L_fwd drops diagonal score from 0.565 to 0.199; removing L_inv drops to 0.529, showing both contribute to factorization.
  - [corpus] "Factored Causal Representation Learning" (neighbor) uses similar factorized dynamics but for reward modeling, suggesting broader applicability of factorization principles.
- Break condition: If K is set too low, multiple ground-truth factors collapse into one latent dimension; if too high, redundant dimensions may capture noise or fail to receive gradient signal.

### Mechanism 3
- Claim: The sparsity constraint (at most one energy responds per action) yields identifiability under diffeomorphic assumptions.
- Mechanism: Theorem 3.1 shows that if each action's score difference ∂/∂z'_i[E(z'_i,a,z) - E(z'_i,a₀,z)] is non-zero for at most one factor, the Jacobian between true factors and learned latents must be 1-sparse, implying a permutation plus per-factor invertible transformation. The L_r loss implicitly promotes this sparsity by pitting classifiers against each other.
- Core assumption: The observation function o: S → X is a diffeomorphism (smooth invertible mapping), and factors are connected in state space.
- Evidence anchors:
  - [section 3.2, Theorem 3.1] "The score differences... for at most one variable j and all actions... then there exists a factor-wise diffeomorphism h: S → Z"
  - [appendix A] Formal proof shows J_h must be 1-sparse given sparsity and connectedness.
  - [corpus] "Transformers learn factored representations" (neighbor) provides analogous theoretical grounding for orthogonal factor representations in sequence models.
- Break condition: If multiple factors have correlated changes (e.g., passenger position locked to taxi in Taxi domain), strict sparsity may not hold perfectly, leading to partial identification (door y-coordinate in DoorKey remains unidentified as it is non-controllable).

## Foundational Learning

- **Factored MDPs and DBNs**
  - Why needed here: The entire approach assumes transitions factorize as T(s'|s,a) = ∏_i T(s'_i|pa(s'_i),a). Without this background, the energy parameterization and identifiability proof will seem unmotivated.
  - Quick check question: Given a 2D navigation task, can you sketch the DBN structure showing which state variables depend on each other and on actions?

- **Contrastive Learning (InfoNCE)**
  - Why needed here: The forward loss uses InfoNCE to maximize mutual information between z and z'. Understanding why negative samples help distinguish true transitions from impostors is essential.
  - Quick check question: If you have one positive pair (z, z') and N-1 negative pairs, what does the InfoNCE loss optimize?

- **Energy-Based Models and Score Functions**
  - Why needed here: Transitions are parameterized as unnormalized densities via energy functions; the ratio mechanism depends on gradients of energy differences (scores). The identifiability proof uses score sparsity directly.
  - Quick check question: For an energy function E(x), what is the relationship between the score function ∇_x E(x) and the log-density?

## Architecture Onboarding

- **Component map:**
  - Encoder f_φ: Residual CNN (positional embeddings → downsampling blocks → MLP → Tanh) mapping 32×32×3 pixels to K-dimensional latent z ∈ [-1,1]^K
  - Energy heads: K independent MLPs taking (z'_k, a, z) → scalar energy; each outputs contribution to transition log-probability
  - Inverse head: MLP taking (z, z') → action logits (used for L_inv)
  - Policy head: MLP taking z → action logits (used for L_π, also provides policy weights ζ_a)
  - Training loop: Sample minibatch → encode current/next observations → add noise (data augmentation) → compute all losses → AdamW update

- **Critical path:**
  1. Data collection with no-op action (a₀) interleaved with other actions—essential for natural dynamics baseline
  2. Encode x, x' → z, z' (encoder quality determines factorization ceiling)
  3. Compute energy differences ΔE = E(z'_k,a,z) - E(z'_k,a₀,z) for ratio loss
  4. Balance L_r (factorization), L_fwd (Markov), L_inv (action prediction), L_π (policy)

- **Design tradeoffs:**
  - Higher K → more capacity but risk of unused dimensions; start with ground-truth factor count if known, else overestimate and monitor usage
  - Noise injection (line 3 in Algorithm 1) improves contrastive learning but excessive noise blurs factor boundaries
  - Loss weights (β_r, β_fwd, β_inv, β_π) require tuning per domain; L_r is the novel term and typically needs moderate weight (5-50 in experiments)

- **Failure signatures:**
  - High off-diagonal R² values (>0.4): factors not disentangled; increase β_r or reduce latent dimension
  - Low diagonal R² (<0.3): encoder fails to capture relevant information; increase encoder capacity or training epochs
  - Non-controllable factors (like door position in DoorKey) remain entangled—expected behavior, not a bug
  - L_inv dominates: representation becomes action-predictive but not factorized; check loss weighting

- **First 3 experiments:**
  1. **Sanity check on Grid2D**: Set K=2, train for 100 epochs, visualize latent traversals (vary each z_k while holding others fixed). Expected: one dimension controls horizontal position, one vertical.
  2. **Ablation of L_r**: Train with β_r=0 on FourRooms. Measure diagonal/off-diagonal R². Expected: significant drop in diagonal scores compared to full ACF.
  3. **Robustness to K**: On Taxi (ground truth K≈4), train with K=2, 4, 8. Report how R² matrices change. Expected: K=2 underfits (multiple factors collapse), K=8 has unused dimensions but maintains identification.

## Open Questions the Paper Calls Out
- Can independently controllable factorization be extended to identify variables that are not directly one-step controllable or are entirely uncontrollable, but still relevant for decision-making?
- How robust is ACF to violations of the sparsity assumption when actions affect multiple state factors simultaneously?
- Can ACF be adapted to work without requiring an explicit no-op action to observe natural dynamics?

## Limitations
- The sparsity assumption (each action affects at most one factor) may not hold in realistic environments where actions have coupled effects
- The method requires an explicit no-op action to observe natural dynamics, which many RL environments lack
- Non-controllable but decision-relevant factors (like the door's y-coordinate in DoorKey) cannot be identified by ACF

## Confidence
- **High confidence**: The empirical results showing ACF outperforms DMS and GCL on factorization metrics (diagonal R²) across all three environments
- **Medium confidence**: The theoretical identifiability proof under sparsity assumptions
- **Medium confidence**: The claim that ACF enables efficient RL

## Next Checks
1. Test ACF on environments where actions have coupled effects (multiple factors change simultaneously) to measure breakdown of factorization quality
2. Verify the noise injection scale (ε in Algorithm 1) is critical for performance by training with varying noise magnitudes
3. Compare RL performance directly (e.g., sample efficiency in Q-learning) using ACF pre-trained representations versus random or alternative representations