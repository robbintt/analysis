---
ver: rpa2
title: A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural
  Analysis
arxiv_id: '2510.05414'
source_url: https://arxiv.org/abs/2510.05414
tags:
- node
- element
- story
- agent
- elasticbeamcolumn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent system for automated 2D frame
  structural analysis, addressing the limitations of large language models in spatial
  reasoning and code consistency. The system decomposes the analysis task into subtasks
  handled by specialized agents, including problem analysis, geometry construction,
  code translation, model validation, and load application.
---

# A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis

## Quick Facts
- arXiv ID: 2510.05414
- Source URL: https://arxiv.org/abs/2510.05414
- Authors: Ziheng Geng; Jiachen Liu; Ran Cao; Lu Cheng; Haifeng Wang; Minghui Cheng
- Reference count: 5
- Primary result: Multi-agent system achieves >80% accuracy on 2D frame structural analysis, outperforming GPT-4o and Gemini-2.5 Pro

## Executive Summary
This paper introduces a multi-agent system for automated 2D frame structural analysis that addresses the limitations of large language models in spatial reasoning and code consistency. The system decomposes the analysis task into subtasks handled by specialized agents, including problem analysis, geometry construction, code translation, model validation, and load application. By decoupling geometric reasoning from code generation and using expert-defined rules, the system achieves over 80% accuracy on benchmark problems. It also offers competitive computational efficiency and low deployment costs, making it a practical tool for structural engineering automation.

## Method Summary
The system uses a five-agent pipeline where Llama-3.3 70B processes natural language descriptions of 2D frames through sequential stages. The Problem Analysis Agent extracts parameters to JSON, the Geometry Agent applies four expert rules to construct node coordinates and element connectivity, the Code Translation Agent converts JSON to OpenSeesPy syntax, the Model Validation Agent runs deterministic tools to detect and correct duplicate nodes and elements, and the Load Agent applies forces and integrates analysis commands. The architecture separates high-level spatial reasoning from low-level code generation to reduce hallucinations and improve accuracy.

## Key Results
- Achieves >80% accuracy on 20 benchmark frame problems (3-5 bays, varying stories)
- Outperforms state-of-the-art models like Gemini-2.5 Pro and ChatGPT-4o on the same benchmark
- Ablation experiments show integrated Geometry-Code Agent fails completely (0% accuracy) while the proposed system achieves 100% on simple cases
- Computational efficiency and deployment costs are competitive with proprietary models

## Why This Works (Mechanism)

### Mechanism 1: Decoupling of Geometric Reasoning and Code Generation
Separating high-level structural reasoning from low-level syntax generation significantly reduces hallucinations and improves accuracy compared to end-to-end single-agent generation. The system forces the LLM to output an intermediate structural representation (JSON) containing node coordinates and element connectivity before attempting to write OpenSeesPy code, preventing the model from "tangling" abstract spatial logic with strict syntax requirements.

### Mechanism 2: Stepwise Rule-Guided Construction
Constraining the generative process of the Geometry Agent with expert-defined, sequential rules enables reliable spatial reasoning for 2D frames. Instead of asking the LLM to "imagine" the frame, the system emulates a human engineer's workflow: constructing the frame bay-by-bay and story-by-story using specific rules that dictate exactly how many nodes/elements to add based on current bay and story height relative to neighbors.

### Mechanism 3: Deterministic Tool-Use for Validation
Using deterministic code execution for consistency checks effectively corrects LLM-specific errors like hallucinated duplicates or ID mismanagement. The Model Validation Agent invokes Python functions to detect and rectify duplicate nodes, duplicate elements, and incorrect connectivity, forcing the generated code to adhere to model constraints rather than relying solely on the LLM to "spot" errors.

## Foundational Learning

- **Concept: Finite Element Modeling (FEM) Topology**
  - Why needed here: The system automates the definition of nodes (coordinates) and elements (connectivity). Understanding how a 2D frame is assembled (bays, stories, supports) is required to interpret the "Geometry Agent's" output and debug visualizations.
  - Quick check question: Can you identify which parameters define a unique node in OpenSeesPy and how element connectivity is defined between two nodes?

- **Concept: Agent Specialization (Decomposition)**
  - Why needed here: The core innovation is breaking a monolithic task into sub-tasks (Analysis, Geometry, Translation, Validation, Load).
  - Quick check question: Why would a "Translator" agent perform better than a generalist agent when converting JSON to code?

- **Concept: Expert Systems / Rule-Based Logic**
  - Why needed here: The Geometry Agent relies on explicit "If-Then" rules rather than pure emergent reasoning.
  - Quick check question: If a new bay is added that is shorter than the previous one, which rule (referencing Figure 6) would determine the connectivity?

## Architecture Onboarding

- **Component map:** Natural Language Description -> Problem Analysis Agent -> Geometry Agent -> Code Translation Agent -> Model Validation Agent -> Load Agent -> Executable OpenSeesPy Code + OpsVis Visualization

- **Critical path:** The Geometry Agent is the bottleneck for reliability. Its prompt must perfectly encode the construction rules; failure here results in topologically invalid structures that pass syntax checks but fail engineering logic.

- **Design tradeoffs:** Uses Llama-3.3 70B (lighter, open-weight model) to reduce cost and allow local deployment, sacrificing raw generalization power of GPT-4o but gaining reproducibility and privacy. Rule-based geometry ensures 80%+ accuracy on standard frames but may lack flexibility for novel structures.

- **Failure signatures:** Duplicate nodes/elements (hallucinations where LLM re-defines existing points), incorrect connectivity (connecting nodes across non-adjacent bays), omitted nodes (failing to add top node when story heights change).

- **First 3 experiments:**
  1. Validation Tool Isolation: Run Model Validation Agent on "corrupted" OpenSeesPy script with known duplicates to verify Python tools correctly renumber and clean the file
  2. Rule Stress Test: Input frame with varying story heights (1-5-1-5-1) to stress-test "Rule 4" logic in Geometry Agent
  3. Ablation Replication: Run integrated Geometry-Code Agent on simple 3-bay frame to reproduce 0% accuracy baseline and observe specific failure modes

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the rule-based geometry agent be effectively extended to automate 3D frame structural analysis without significant accuracy degradation? The authors identify extension from 1D beams to 2D frames as "non-trivial" due to spatial reasoning requirements, implicitly leaving 3D structures as the next major hurdle.

- **Open Question 2:** How can the system be refined to mitigate the performance drop observed in complex topologies requiring lengthy sequential reasoning? Results show accuracy declining to 60-70% for specific five-bay frames (2-3-1-4-5), attributed to "lengthy sequential steps and frequent conditional rule applications."

- **Open Question 3:** Is the system robust enough to handle diverse boundary conditions and complex load patterns beyond the fixed supports and standard loads used in the benchmark? Real-world engineering requires modeling varied supports (pins, rollers) and load types; current validation focuses on geometric connectivity rather than physical load diversity.

## Limitations
- Exact prompt templates and validation tool implementations are not fully specified, requiring significant engineering effort to reproduce reported >80% accuracy
- JSON schema and expert rules may not generalize to non-orthogonal or highly irregular frame topologies beyond 20 benchmark problems
- Claimed computational efficiency gains depend on comparing Llama-3.3 70B to proprietary models without accounting for deployment overhead differences

## Confidence
- **High Confidence:** Multi-agent architecture design and its core innovation of decoupling geometric reasoning from code generation is well-documented and theoretically sound
- **Medium Confidence:** Reported accuracy metrics (>80% on benchmarks, outperforming GPT-4o and Gemini-2.5 Pro) are plausible given systematic error correction mechanisms, but require independent validation
- **Low Confidence:** Claims about deployment cost advantages and scalability to production environments lack quantitative supporting data

## Next Checks
1. **Ablation Test Replication:** Implement and run integrated Geometry-Code Agent baseline on simple 3-bay frame to verify reported 0% accuracy failure mode, confirming necessity of multi-agent decomposition

2. **Rule Stress Testing:** Systematically test Geometry Agent with frames containing irregular story heights (1-5-1-5-1 pattern) and non-uniform bay widths to validate whether Rules 1-4 generalize beyond standard configurations

3. **Validation Tool Isolation:** Create synthetic OpenSeesPy scripts with known duplicate nodes and elements, then run through Model Validation Agent's deduplication tools to verify algorithmic correctness of Python-based error correction pipeline