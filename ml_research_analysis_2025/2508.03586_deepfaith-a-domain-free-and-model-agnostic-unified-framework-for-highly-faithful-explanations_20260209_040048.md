---
ver: rpa2
title: 'DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful
  Explanations'
arxiv_id: '2508.03586'
source_url: https://arxiv.org/abs/2508.03586
tags:
- explanation
- deepfaith
- faithfulness
- methods
- saliency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepFaith is a domain-free and model-agnostic framework for generating
  highly faithful explanations in Explainable AI (XAI). It addresses the absence of
  a unified optimal explanation by establishing a theoretical framework that unifies
  widely used faithfulness metrics.
---

# DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations

## Quick Facts
- **arXiv ID:** 2508.03586
- **Source URL:** https://arxiv.org/abs/2508.03586
- **Reference count:** 4
- **Primary result:** Achieves highest overall faithfulness across 10 metrics on 12 diverse explanation tasks, demonstrating domain-free generalizability.

## Executive Summary
DeepFaith is a unified framework that generates highly faithful explanations for any model-agnostic explainer. It addresses the lack of a unified optimal explanation by theoretically proving that optimizing local correlation simultaneously optimizes multiple faithfulness metrics. The method trains a Transformer-based explainer using a combination of supervised signals from filtered baseline methods and a self-supervised perturbation correlation loss.

## Method Summary
The framework operates by first generating candidate explanations from 14 baseline methods, then filtering them using 10 faithfulness metrics to create high-quality supervised signals. A Transformer explainer is trained with a dynamic loss schedule that transitions from pattern consistency (mimicking baselines) to local correlation (theoretical faithfulness optimization). This produces a domain-free, model-agnostic explainer that generates faithful saliency maps in a single forward pass.

## Key Results
- Achieves highest overall faithfulness across 10 metrics compared to all baseline methods
- Demonstrates cross-domain generalizability across 6 models and 6 datasets
- Provides millisecond-level inference speed while maintaining high faithfulness
- Ablation study confirms both supervised signals and theoretical optimization are necessary

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing local correlation objective ($L_{LC}$) serves as unified proxy for optimizing multiple faithfulness metrics
- **Core assumption:** Model prediction behavior is locally consistent and preservation effect is negatively correlated with perturbation effect
- **Evidence anchors:** Theoretical proof in Theorem 1; abstract states optimal explanation objective simultaneously achieves optimal faithfulness across metrics
- **Break condition:** Non-smooth or adversarial models where correlation between local sums and permutation effects decouples

### Mechanism 2
- **Claim:** Aggregating diverse explanation methods into filtered supervised signal provides stable initialization
- **Core assumption:** Ground truth explanation lies within intersection of high-performing existing methods
- **Evidence anchors:** Abstract mentions deduplicating and filtering to construct high-quality supervised signals; Figure 2 shows filtering pipeline
- **Break condition:** Collective failure of baseline methods to capture specific feature importance

### Mechanism 3
- **Claim:** Dynamic loss weighting schedule transitions explainer from imitation to theoretical optimization
- **Core assumption:** Theoretical objective is too sparse for cold-start training and requires warm start from imitation learning
- **Evidence anchors:** Section states α starts near 1 and decreases toward 0; ablation study shows $L_{LC}$ alone fails to converge
- **Break condition:** Too fast α decay preventing explainer from stabilizing in function space

## Foundational Learning

- **Concept: Perturbation-based Faithfulness**
  - **Why needed here:** Entire theoretical framework relies on defining faithfulness as alignment between removing features and prediction confidence drop
  - **Quick check question:** If you mask top 10% salient features and prediction confidence remains unchanged, is explanation faithful?

- **Concept: Permutation vs. Saliency Explanations**
  - **Why needed here:** Framework unifies metrics evaluating scalar attribution and those evaluating ranked order
  - **Quick check question:** Can two different saliency maps result in same permutation explanation?

- **Concept: Model-Agnostic Explainers (Amortized Inference)**
  - **Why needed here:** Unlike LIME/SHAP which run per-instance, DeepFaith trains neural network to predict explanations in single forward pass
  - **Quick check question:** Does model-agnostic explainer require access to original model's gradients during inference time?

## Architecture Onboarding

- **Component map:** Signal Generator -> Filtering Module -> Transformer Explainer -> Loss Aggregator
- **Critical path:** Signal Generation & Filtering stage is most data-dependent; poor baseline methods or strict p-quantile can cause explainer to fail before theoretical loss takes over
- **Design tradeoffs:**
  - Upfront training cost vs. millisecond inference speed
  - Generalization capability vs. need to retrain for new datasets/models
- **Failure signatures:**
  - Mode Collapse: Uniform saliency output (usually from $L_{LC}$ dominating too early)
  - Metric Disagreement: High scores on some metrics but low on others
  - Runtime Bloat: Large explainer architecture negating single forward pass advantage
- **First 3 experiments:**
  1. Implement filtering logic on small subset to verify retention rate
  2. Train three versions ($L_{PC}$ only, $L_{LC}$ only, Combined) to reproduce convergence failure
  3. Verify Theorem 1 by plotting feature ranking against empirical perturbation effect

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How sensitive is DeepFaith's performance to manual selection of similarity threshold and p-quantile in deduplication and filtering stages?
- **Basis in paper:** Method section states "manually defined similarity threshold" and filtering uses "p-quantile" without analyzing hyperparameter sensitivity
- **Why unresolved:** Experiments use fixed values (similarity 0.6 to 0.93) without exploring robustness
- **What evidence would resolve it:** Sensitivity analysis reporting variance in faithfulness scores across range of threshold settings

### Open Question 2
- **Question:** Does reliance on existing explanation methods for supervised signals impose upper bound on faithfulness DeepFaith can achieve?
- **Basis in paper:** Ablation shows training with only $L_{PC}$ is "limited by baseline methods" but unclear if final model interpolates baselines or surpasses best input signal
- **Why unresolved:** Optimization objective balances mimicking baselines and maximizing faithfulness, creating potential trade-off
- **What evidence would resolve it:** Experiments where baseline method set is systematically degraded or improved to observe performance ceiling

### Open Question 3
- **Question:** Does proposed optimization schedule guarantee convergence to theoretically optimal explanation mapping $S^*_f$?
- **Basis in paper:** Paper theoretically derives existence of optimal mapping $S^*_f$ but acknowledges it's "analytically intractable," training neural network instead
- **Why unresolved:** Neural networks are universal approximators but don't guarantee convergence to global optimum
- **What evidence would resolve it:** Convergence analysis or proof showing loss landscape allows reaching theoretical faithfulness bounds

## Limitations
- Framework performance depends heavily on quality of baseline explanation methods used for supervised signals
- Theoretical unification assumes local consistency of model predictions, which may not hold for adversarial or highly non-linear models
- Requires retraining for new datasets or models, limiting flexibility compared to instance-specific methods

## Confidence
- **Mechanism 1:** Medium - Mathematical soundness but assumes smoothness of prediction surface
- **Mechanism 2:** Low - Circular dependency where supervised signal quality is bounded by collective baseline performance
- **Mechanism 3:** Medium - Empirically effective but lacks theoretical justification for specific annealing trajectory

## Next Checks
1. Verify 10 faithfulness metrics maintain strong mutual correlation across diverse model architectures
2. Systematically remove one category of baseline methods and measure performance degradation
3. Evaluate on adversarially perturbed models where perturbation-based metrics are known to fail