---
ver: rpa2
title: 'HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task
  Speech Language Modeling'
arxiv_id: '2509.18570'
source_url: https://arxiv.org/abs/2509.18570
tags:
- speech
- task
- layer
- language
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HarmoniFuse, a component-selective and prompt-adaptive
  framework for multi-task speech language modeling. The core method idea is to harmonize
  heterogeneous task demands by selecting and fusing task-relevant components of speech
  representations through a gated speech encoder and a prompt-adaptive dynamic fusion
  module.
---

# HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling

## Quick Facts
- arXiv ID: 2509.18570
- Source URL: https://arxiv.org/abs/2509.18570
- Reference count: 0
- Improves both ASR and SER performance, achieving 76.51% UA and 76.31% W A for SER, and reducing ASR WER to 3.5% on clean and 5.1% on other subsets, surpassing single-task oracle baselines.

## Executive Summary
HarmoniFuse is a component-selective and prompt-adaptive framework for multi-task speech language modeling that harmonizes heterogeneous task demands by selecting and fusing task-relevant components of speech representations. The core method uses a gated speech encoder and a prompt-adaptive dynamic fusion module to enable the model to flexibly shift attention between linguistic precision and paralinguistic expressiveness. By leveraging batch-interleaved training with separate ASR and SER datasets, HarmoniFuse achieves state-of-the-art performance on both tasks without requiring joint annotation.

## Method Summary
HarmoniFuse harmonizes heterogeneous task demands by selecting and fusing task-relevant components of speech representations through a gated speech encoder and a prompt-adaptive dynamic fusion module. The framework uses a learnable weighting mechanism to dynamically emphasize different layer combinations based on task characteristics, enabling the model to flexibly shift attention between linguistic precision and paralinguistic expressiveness. A batch-interleaved training strategy allows leveraging separate ASR and SER datasets without requiring joint annotation, with task identity conveyed via prompt tokens to condition the model.

## Key Results
- Achieves 76.51% UA and 76.31% W A for SER, surpassing single-task oracle baselines
- Reduces ASR WER to 3.5% on clean and 5.1% on other subsets
- Effectively balances task-specific needs without requiring separate models

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise information in speech encoders is task-differentiated, and static selection creates tradeoffs. A learnable softmax-weighted sum over all 24 WavLM encoder layers replaces fixed-layer extraction, allowing the model to emphasize different layer combinations per task during multi-task training. Different encoder layers encode functionally distinct information (acoustic/prosodic vs. linguistic) that is relevant to different tasks.

### Mechanism 2
Task-conditioned, input-dependent layer fusion in the SLM decoder reduces task interference compared to static fusion. Per-layer importance weights are computed as a sum of a task- and layer-specific learnable scalar and an input-dependent score from a small FFN on the hidden state. Tasks like ASR and SER benefit from different layer aggregation patterns (e.g., deeper layers for linguistic precision vs. combined mid/high layers for paralinguistic cues).

### Mechanism 3
Batch-interleaved training with separate datasets enables multi-task learning without joint labels, but requires stable optimization and clear task disambiguation. ASR and SER samples from separate corpora are interleaved in training batches; task identity is conveyed via prompt tokens to condition the model. Prompt tokens are sufficient to signal task identity; gradients from interleaved batches do not destructively interfere.

## Foundational Learning

- **Task interference in multi-task learning**: Why needed here: HarmoniFuse explicitly addresses conflicting information needs between ASR (linguistic) and SER (linguistic + paralinguistic); understanding interference helps diagnose when fusion fails. Quick check question: If you see ASR improve while SER degrades in joint training, what does that suggest about the current fusion weights?

- **Layer-wise representations in speech encoders**: Why needed here: The method relies on lower/middle layers preserving prosodic cues and top layers encoding linguistic content; this justifies dynamic layer selection. Quick check question: If you visualize α^τ across layers and see uniform weights for both ASR and SER, what might that indicate about the encoder or training?

- **Prompt-conditioned routing**: Why needed here: Task prompts must reliably modulate fusion weights; if prompts are ignored, dynamic fusion collapses to static behavior. Quick check question: How would you verify that the prompt embedding is meaningfully affecting α^τ, rather than the FFN term dominating?

## Architecture Onboarding

- **Component map**: Raw waveform → WavLM encoder (frozen or fine-tuned) → gated encoder fusion → transformer LM → prompt-adaptive fusion → task head
- **Critical path**: Raw waveform → WavLM encoder (24 layers) → gated encoder fusion → multi-modal transformer LM → prompt-adaptive dynamic layer fusion → task-specific heads
- **Design tradeoffs**: Encoder fusion is task-specific but shared across samples; transformer fusion is both task- and input-adaptive. Using pseudo-labeled Emilia scales SER data but introduces label noise.
- **Failure signatures**: Uniform α^τ weights across tasks suggest prompt conditioning may be failing; one task at oracle baseline while the other degrades suggests task imbalance; SER performance collapse suggests checking pseudo-label quality.
- **First 3 experiments**:
  1. Single-task baselines with fixed encoder layers (12 vs. 24) to confirm layer-task alignment; then compare to gated encoder fusion.
  2. Multi-task with static encoder fusion only (no prompt-adaptive transformer fusion) to isolate the contribution of dynamic transformer-layer weighting.
  3. Visualization of learned α^τ for ASR vs. SER across transformer layers to verify task-discriminative routing.

## Open Questions the Paper Calls Out

### Open Question 1
Does HarmoniFuse generalize beyond the ASR-SER task pair to support three or more diverse speech tasks (e.g., adding speaker identification or language identification) without performance degradation? The component-selective fusion may face increased interference when routing information for three or more tasks with divergent representation needs.

### Open Question 2
How sensitive is HarmoniFuse's SER performance to noise in the Emilia pseudo-labels generated by Emotion2Vec? Noisy pseudo-labels could introduce conflicting gradients during batch-interleaved training, potentially undermining the fusion mechanism's ability to learn task-discriminative representations.

### Open Question 3
Does the gated encoder and prompt-adaptive fusion transfer to alternative pre-trained speech encoders beyond WavLM (e.g., wav2vec 2.0, HuBERT, or Whisper)? Different encoders have different layer properties and representation distributions; the learned weighting mechanisms may be encoder-specific.

### Open Question 4
What is the optimal batch-interleaving ratio between ASR and SER samples during training, and how does imbalance affect convergence? ASR (960h) and SER (~1,800h with pseudo-labels) datasets differ in scale and label reliability; suboptimal mixing could cause task dominance.

## Limitations
- Layer differentiation assumption is plausible but not rigorously validated; the paper shows optimal fixed-layer performance differs by task but doesn't probe what specific information each layer captures.
- Pseudo-label quality impact is unclear; the SER performance improvement relies heavily on pseudo-labeled Emilia data, but no analysis isolates data scaling vs. method contribution.
- Prompt conditioning validation is incomplete; while the method claims task prompts enable input-adaptive fusion, there's no direct analysis showing prompt embeddings meaningfully influence the α weights.

## Confidence
- **High confidence**: The architectural framework is clearly specified and implementable; the ablation showing gated encoder fusion improves both tasks over fixed-layer baselines is straightforward to verify.
- **Medium confidence**: The multi-task improvements over single-task baselines are reported but lack extensive ablations isolating each fusion component's contribution.
- **Low confidence**: The claim that HarmoniFuse "effectively balances task-specific needs" is supported by relative improvements but not by qualitative analysis of what each task actually learns.

## Next Checks
1. **Prompt embedding analysis**: Visualize and correlate prompt embeddings with learned α^τ distributions across layers. If prompt embeddings don't predict fusion patterns, the prompt-adaptive claim needs revision.

2. **Component ablation study**: Train three variants: (a) gated encoder fusion only, (b) prompt-adaptive transformer fusion only, (c) both. Compare to full HarmoniFuse to isolate each component's contribution and identify if one is carrying most of the performance gain.

3. **Encoder layer probing**: Apply diagnostic classifiers to WavLM encoder layers to measure what linguistic vs. paralinguistic information each layer contains. Verify that lower layers indeed preserve prosodic cues and upper layers encode linguistic content, as assumed by the method.