---
ver: rpa2
title: 'AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt'
arxiv_id: '2512.24625'
source_url: https://arxiv.org/abs/2512.24625
tags:
- traffic
- prediction
- learning
- data
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoFed introduces a novel personalized federated learning framework
  for traffic prediction that eliminates manual hyper-parameter tuning. The method
  uses a federated representor with a client-aligned adapter to distill local data
  into a globally shared prompt matrix, which conditions a personalized predictor
  for each client.
---

# AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt

## Quick Facts
- **arXiv ID**: 2512.24625
- **Source URL**: https://arxiv.org/abs/2512.24625
- **Reference count**: 40
- **Primary result**: Novel personalized federated learning framework that eliminates manual hyper-parameter tuning for traffic prediction

## Executive Summary
AutoFed introduces a manual-free federated learning framework for traffic prediction that leverages personalized prompts to enable clients to benefit from cross-client knowledge while maintaining local specificity. The method uses a federated representor with a client-aligned adapter to distill local data into a globally shared prompt matrix, which conditions a personalized predictor for each client. This design allows for superior performance without requiring manual hyper-parameter optimization.

The framework demonstrates significant advantages over existing methods, achieving faster convergence, lower communication costs, and best results across diverse scenarios in both travel demand prediction and traffic flow prediction tasks. AutoFed outperforms state-of-the-art methods FedTPS and FedGCN while requiring no manual hyper-parameter tuning.

## Method Summary
AutoFed employs a personalized federated learning approach that eliminates the need for manual hyper-parameter tuning in traffic prediction. The framework consists of a federated representor that uses a client-aligned adapter to distill local data into a globally shared prompt matrix. This prompt matrix then conditions a personalized predictor for each client, allowing for both knowledge sharing across clients and preservation of local specificity. The method operates without requiring clients to share their raw data, making it suitable for privacy-sensitive applications in traffic prediction.

## Key Results
- Outperforms FedTPS and FedGCN on real-world datasets
- Achieves faster convergence compared to baseline methods
- Requires lower communication costs (175.0K parameters per round)
- Delivers best results across diverse scenarios in both travel demand and traffic flow prediction tasks

## Why This Works (Mechanism)
AutoFed's success stems from its personalized prompt-based federated learning architecture that bridges the gap between global knowledge sharing and local specificity. The federated representor with client-aligned adapter effectively distills diverse local data into a unified prompt matrix that captures cross-client patterns while respecting individual client characteristics. This prompt matrix then conditions personalized predictors, allowing each client to leverage shared knowledge while maintaining their unique data characteristics. The manual-free nature eliminates the need for extensive hyper-parameter tuning across different clients and scenarios, making the system more robust and easier to deploy in real-world traffic prediction applications.

## Foundational Learning

**Federated Learning**: Distributed machine learning where clients train models locally without sharing raw data, enabling privacy-preserving collaborative learning. *Why needed*: Allows traffic data to remain on local servers while still benefiting from cross-client knowledge sharing.

**Personalized Learning**: Adapting models to individual clients or users rather than using a one-size-fits-all approach. *Why needed*: Traffic patterns vary significantly across different regions and contexts, requiring client-specific adaptations.

**Prompt Learning**: Using learned prompts or conditioning vectors to guide model behavior for specific tasks or contexts. *Why needed*: Provides a mechanism to encode client-specific information while maintaining a shared knowledge base.

**Client-Aligned Adapter**: A component that aligns client-specific features with a global representation. *Why needed*: Bridges the gap between diverse local data distributions and a unified global model.

**Traffic Prediction**: Forecasting future traffic conditions based on historical data. *Why needed*: Core application domain requiring accurate, efficient, and privacy-preserving solutions.

## Architecture Onboarding

**Component Map**: Client Data -> Client-Aligned Adapter -> Federated Representor -> Global Prompt Matrix -> Personalized Predictor -> Prediction Output

**Critical Path**: The flow from client data through the adapter and representor to generate the prompt matrix, which then conditions the personalized predictor, represents the core computation path for making predictions.

**Design Tradeoffs**: The framework balances between global knowledge sharing (through the shared prompt matrix) and local specificity (through personalized predictors), while minimizing communication costs through the efficient prompt representation.

**Failure Signatures**: Poor performance may indicate inadequate prompt representation of local data characteristics, misalignment in the client-adapter mechanism, or insufficient personalization in the predictor component.

**Three First Experiments**:
1. Baseline comparison on single dataset with FedTPS and FedGCN
2. Ablation study removing the personalized predictor component
3. Communication cost analysis comparing parameter counts across federated methods

## Open Questions the Paper Calls Out
None identified in the available information.

## Limitations
- Performance claims rely heavily on comparisons with only two state-of-the-art methods
- Communication cost figure lacks context and comparative benchmarks
- "Faster convergence" claim is qualitative without quantitative benchmarks
- Assertion of superiority "across diverse scenarios" lacks substantiation with specific details

## Confidence
- **High confidence**: The core technical approach (personalized prompt-based federated learning for traffic prediction) is clearly described and represents a novel contribution to the field
- **Medium confidence**: Experimental results showing improved performance over specific baselines, though the completeness and rigor of evaluation is unclear
- **Low confidence**: Claims about communication efficiency and convergence speed lack sufficient quantitative support

## Next Checks
1. Request full experimental results tables showing performance metrics (RMSE/MAE) with standard deviations across all tested datasets and baselines
2. Obtain convergence curves comparing AutoFed against baseline methods across multiple training rounds to verify "faster convergence" claims
3. Request detailed communication cost analysis comparing parameter counts per round across all federated methods tested, including breakdown of what the 175.0K parameters represent