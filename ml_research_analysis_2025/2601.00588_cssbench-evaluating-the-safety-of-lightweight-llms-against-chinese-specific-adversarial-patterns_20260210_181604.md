---
ver: rpa2
title: 'CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific
  Adversarial Patterns'
arxiv_id: '2601.00588'
source_url: https://arxiv.org/abs/2601.00588
tags:
- chinese
- safety
- adversarial
- patterns
- chinese-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSSBench, a benchmark for evaluating the
  safety of lightweight Chinese language models against Chinese-specific adversarial
  patterns. The key challenge addressed is that existing English-centric safety benchmarks
  fail to capture Chinese-specific obfuscation techniques like homophones, pinyin,
  and symbol mixing, which are commonly used in real-world attacks.
---

# CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns

## Quick Facts
- arXiv ID: 2601.00588
- Source URL: https://arxiv.org/abs/2601.00588
- Reference count: 26
- Ten popular lightweight Chinese models show 31-39% attack success rates against Chinese-specific adversarial patterns

## Executive Summary
This paper introduces CSSBench, a benchmark specifically designed to evaluate the safety of lightweight Chinese language models against Chinese-specific adversarial patterns. The key insight is that existing English-centric safety benchmarks fail to capture Chinese-specific obfuscation techniques like homophones, pinyin, and symbol mixing, which are commonly used in real-world attacks. CSSBench covers six safety domains with three task formats (MCQ, TF, QA) and applies adversarial patterns to generate 2,483 malicious queries plus 250 borderline queries for over-refusal testing. The evaluation reveals that lightweight Chinese models are highly vulnerable to these adversarial patterns, with composite error rates around 30%, demonstrating that Chinese-specific adversarial patterns remain a critical challenge for lightweight LLM safety.

## Method Summary
The CSSBench benchmark evaluates lightweight Chinese models (<8B parameters) using a dataset of 2,483 malicious queries across six safety domains and three task formats. Adversarial patterns (pinyin mix, homophones, symbol mix, zero-width characters) are systematically applied to each query. Model responses are evaluated using Qwen3Guard-Gen-8B as an LLM-as-judge to classify outputs as Safe/Unsafe/Controversial. Metrics include Attack Success Rate (ASR), Over-Refusal Rate (ORR), and Composite Error Rate (CER) calculated as weighted sum of NM_t × A_t + N_O × O / (Σ NM_t + N_O). The evaluation uses greedy decoding with max 64 tokens for MCQ/TF and 256 for QA tasks, running on NVIDIA A100 GPUs or Huawei Ascend 910B NPUs.

## Key Results
- Lightweight Chinese models show attack success rates of 31-39% against Chinese-specific adversarial patterns
- Composite Error Rate (CER) across all models remains around 30%, indicating significant safety vulnerabilities
- Open-source models (Qwen3, MiniCPM) outperform proprietary models (Hunyuan) in safety performance
- Over-refusal rates are substantial, particularly for borderline queries containing sensitive surface cues
- QA tasks show the highest vulnerability (45-65% ASR) compared to constrained MCQ/TF formats

## Why This Works (Mechanism)

### Mechanism 1: Orthographic Obfuscation of Token Sequences
If specific adversarial patterns (Pinyin, Homophones) disrupt the tokenization of sensitive terms, safety guardrails trained on clean text fail to trigger refusal mechanisms. Chinese adversarial patterns like Pinyin Mix or Symbol Splitting fragment contiguous character sequences, creating distribution shift in the embedding space and causing safety classifiers to fail recognition while generative decoders still attend to phonetic or semantic clues.

### Mechanism 2: Surface-Form Sensitivity in Lightweight Models
Lightweight models (<8B parameters) exhibit higher vulnerability to adversarial patterns because their reduced capacity limits the generalization of safety training to out-of-distribution inputs. Pruning or distilling models reduces redundancy required to learn nuanced decision boundaries, causing activation patterns of safety heads to fall below rejection thresholds when inputs are perturbed.

### Mechanism 3: Over-Refusal via Keyword-Association Bias
Safety training induces a "lazy" heuristic where models refuse benign queries containing sensitive surface cues rather than evaluating intent. The model maximizes a reward function that heavily penalizes specific toxic tokens, adopting a high-recall, low-precision strategy: if a sensitive token appears, the prior probability of refusal spikes, overriding instructions to analyze context.

## Foundational Learning

**Tokenization Attacks (Character vs. Subword)**
Why needed here: Understanding why "Zero-width" or "Symbol Mix" attacks work requires knowing how transformers convert text to tokens. If a tokenizer splits "attack" into "at" + "tack", inserting a space or zero-width character might break it into "at" + " [space] " + "tack", destroying the single token "attack" that the safety filter learned to block.
Quick check question: If a safety filter blocks the token "bomb", will it block the input "b\u200bomb" (with a zero-width space)?

**Safety Alignment (RLHF) Distribution Gap**
Why needed here: The paper argues English-centric alignment fails in Chinese. This relies on the concept that Reinforcement Learning from Human Feedback (RLHF) optimizes for the distribution of the training data. If the training data has low probability for "Pinyin-mixed malicious queries," the model's safety behavior in that region is undefined/random.
Quick check question: Why does a model fine-tuned on standard Chinese text fail to understand "pinyin" queries, even if humans can read them?

**The Harmlessness vs. Helpfulness Trade-off**
Why needed here: The paper evaluates both ASR (Harmlessness) and ORR (Helpfulness). A model that refuses *everything* has perfect safety (0% ASR) but is useless (100% ORR). Understanding this trade-off is crucial for interpreting the Composite Error Rate (CER).
Quick check question: If Model A has 10% ASR and 50% ORR, and Model B has 20% ASR and 10% ORR, which is generally preferred for a user-facing assistant?

## Architecture Onboarding

**Component map:**
Input: Chinese Malicious Query + Adversarial Pattern Generator → Lightweight LLM (Qwen3-0.6B to 8B) → LLM-as-Judge (Qwen3Guard-Gen-8B) → Outputs Safe/Unsafe/Refusal labels

**Critical path:**
1. Perturbation: Apply one of the 4 Chinese-specific patterns to a seed malicious query
2. Inference: Feed the perturbed prompt to the target lightweight LLM (greedy decoding)
3. Judgment: Feed the (Prompt, Response) pair to Qwen3Guard
4. Scoring: Calculate CER = (Weighted ASR + Weighted ORR)

**Design tradeoffs:**
- Judge Selection: Rejected keyword-based judging for Chinese because "refusals may be implicit" (hedged statements). Traded speed/low cost of regex for nuance (and potential bias) of an LLM judge
- Model Selection: Focusing on <8B parameters targets "cost-sensitive" deployment but sacrifices the robustness typically found in 70B+ models

**Failure signatures:**
- High ASR on QA Tasks: Open-ended generation is the primary failure mode (ASR 45-65%) because the model has more degrees of freedom to "slip" into compliance compared to constrained MCQ/TF tasks
- Lazy Refusal: High ORR on "Borderline" queries indicates the model is triggering refusal based on single tokens rather than full context

**First 3 experiments:**
1. Establish Baseline vs. Adversarial: Run CSSBench evaluation on target model (e.g., Qwen3-1.7B) using "Clean" prompts vs. "Pinyin Mix" prompts to quantify vulnerability delta
2. Over-Refusal Stress Test: Run the 250 borderline queries through the model. High ORR here suggests "safety over-correction" and requires adjusting system prompt or refusal threshold
3. Tokenizer Robustness Check: Input specific "Zero-width" and "Symbol Mix" examples directly into the tokenizer to verify if sensitive tokens are fragmented

## Open Questions the Paper Calls Out

**Open Question 1:** Do Chinese-specific adversarial patterns retain their high attack success rates when applied to large language models with significantly more parameters (e.g., >8B)?
Basis: The authors limit their study to "ten lightweight Chinese models under 8B parameters" and explicitly state their findings "should not be read as definitive for larger models."

**Open Question 2:** How do Chinese-specific text-based adversarial patterns interact with or transfer to multimodal inputs (e.g., text + image)?
Basis: The limitations section notes that the benchmark is "text-only and Mandarin-centric," whereas "real-world systems may face multimodal inputs."

**Open Question 3:** Can specific safety alignment or fine-tuning strategies effectively reduce the ~30% composite error rate in lightweight models without causing a corresponding increase in over-refusal?
Basis: The paper concludes that adversarial patterns are a "critical challenge" with high error rates, but focuses on evaluation rather than proposing or testing specific mitigation techniques.

**Open Question 4:** How robust are current safety guardrails against these adversarial patterns when applied to non-Mandarin Chinese dialects or code-switched text?
Basis: The authors acknowledge the benchmark is "Mandarin-centric" and does not cover "dialectal variation, or code-switching with other languages."

## Limitations

- Potential bias in LLM-as-judge methodology, as Qwen3Guard-Gen-8B may inherit its own safety preferences and cultural assumptions
- Adversarial pattern coverage may not capture all real-world attack vectors used by malicious actors
- Focus on lightweight models (<8B parameters) limits generalizability to larger frontier models with different safety mechanisms
- Cultural specificity of safety judgments in Chinese contexts means some refusal decisions may reflect cultural norms rather than universal safety standards

## Confidence

**High Confidence:**
- CSSBench dataset construction methodology and adversarial pattern definitions are clearly specified and reproducible
- Evaluation methodology (ASR, ORR, CER calculations) follows established benchmarking practices
- Finding that lightweight Chinese models show 31-39% attack success rates is well-supported by evaluation results
- Observation that open-source models outperform proprietary models is consistently demonstrated across metrics

**Medium Confidence:**
- Claim that Chinese-specific adversarial patterns are "not well captured" by English-centric benchmarks relies on comparative reasoning
- Assertion that lightweight models are more vulnerable due to reduced capacity is supported but not definitively proven
- Conclusion about over-refusal behavior affecting practical deployment requires additional context about real-world user tolerance

**Low Confidence:**
- Generalizability of findings to non-Chinese languages or multilingual contexts is not established
- Long-term effectiveness of proposed defense strategies against evolving adversarial techniques remains speculative
- Relative importance of Chinese-specific patterns versus universal safety challenges cannot be determined from this work alone

## Next Checks

**Check 1: Judge Consistency Validation**
Run a subset of 100 CSSBench samples through three different LLM judges (Qwen3Guard, GPT-4, and Claude) to quantify inter-judge agreement rates. Calculate Cohen's kappa for ASR/ORR measurements to establish whether primary findings are judge-dependent or robust across evaluation frameworks.

**Check 2: Adaptive Attack Resilience**
Select the most vulnerable model (Hunyuan-0.5B) and perform iterative testing where successful adversarial patterns are used to fine-tune a "red team" adversary. Evaluate whether the model's vulnerability increases or decreases after exposure to adaptive attacks versus static CSSBench patterns.

**Check 3: Cross-Cultural Safety Alignment**
Translate 50 CSSBench queries into English and evaluate both the original Chinese models and their English-language counterparts (where available) using the same judge. Compare ASR/ORR rates to determine whether Chinese-specific patterns represent unique vulnerabilities or general safety alignment challenges.