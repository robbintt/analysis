---
ver: rpa2
title: Zero-Shot Text-to-Speech for Vietnamese
arxiv_id: '2506.01322'
source_url: https://arxiv.org/abs/2506.01322
tags:
- audio
- phoaudiobook
- speech
- dataset
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoAudiobook, a 941-hour high-quality Vietnamese
  audiobook dataset, to advance zero-shot text-to-speech (TTS) synthesis for low-resource
  languages. The dataset addresses limitations of existing Vietnamese speech datasets,
  which typically contain short audio clips and lack speaker identity information.
---

# Zero-Shot Text-to-Speech for Vietnamese

## Quick Facts
- arXiv ID: 2506.01322
- Source URL: https://arxiv.org/abs/2506.01322
- Reference count: 13
- Primary result: Introduces PhoAudiobook (941h Vietnamese audiobook dataset) and shows XTTS-v2 achieves lower WER and higher speaker similarity than baseline ViVoice model

## Executive Summary
This paper introduces PhoAudiobook, a 941-hour high-quality Vietnamese audiobook dataset, to advance zero-shot text-to-speech (TTS) synthesis for low-resource languages. The dataset addresses limitations of existing Vietnamese speech datasets, which typically contain short audio clips and lack speaker identity information. Using PhoAudiobook, the authors train three state-of-the-art zero-shot TTS models—VALL-E, VoiceCraft, and XTTS-v2—and evaluate them across multiple benchmarks. XTTS-v2 consistently outperforms a baseline model fine-tuned on the ViVoice dataset, achieving lower word error rates (e.g., 8.32 vs. 12.54 on ViVoice test set) and higher speaker similarity scores. VoiceCraft and VALL-E demonstrate superior performance on short sentences, highlighting their robustness to varied input lengths. The dataset and models are publicly released to support further research in Vietnamese TTS.

## Method Summary
The authors create PhoAudiobook by extracting vocals from 1,515 Vietnamese audiobooks using Demucs, then transcribing with Whisper-large-v3 and verifying with PhoWhisper-large. They filter for exact transcription matches, detect speaker identities using wav2vec2-bartpho, and balance the dataset (max 4h/speaker). For TTS models, they fine-tune XTTS-v2 (from coqui-ai/TTS checkpoint), VoiceCraft (from 830M_TTSEnhanced), and VALL-E (12-layer Transformer-decoder) on the dataset, using phonemizer for Vietnamese phonemes. Evaluation uses WER (PhoWhisper-large), MCD, RMSE_F0, MOS, and SMOS on seen/unseen speaker test sets.

## Key Results
- XTTS-v2PAB achieves WER of 8.32 on ViVoice test set vs. 12.54 for baseline viXTTS
- XTTS-v2PAB shows SMOS of 3.56 on unseen speakers, outperforming viXTTS (2.99)
- VALL-EPAB and VoiceCraftPAB achieve MCD of 1.51 and 1.57 respectively, lower than XTTS-v2PAB (1.64)
- On VIVOS short-sentence test set, VALL-EPAB and VoiceCraftPAB outperform XTTS-v2PAB, suggesting architectural limitations for brief inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-form training audio (10-20 seconds) improves zero-shot TTS prosody and naturalness compared to short clips (<10 seconds).
- Mechanism: Extended audio contexts allow models to learn natural speech patterns including appropriate pausing, rhythm variation, and coherent prosodic contours across phrases, rather than learning only isolated phoneme sequences.
- Core assumption: The correlation between long-form training data and improved performance is causal, not confounded by other dataset characteristics.
- Evidence anchors:
  - [abstract] "VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts"
  - [section] "Figure 2 shows that previous datasets primarily consist of audio segments shorter than 10 seconds. PhoAudiobook addresses this limitation by providing audio samples ranging from 10 to 20 seconds"
  - [corpus] Limited direct evidence in corpus; mostly short-form TTS papers
- Break condition: If short-form training data with explicit prosody annotations produces equivalent results, the mechanism is data augmentation rather than context length.

### Mechanism 2
- Claim: High-quality transcription verification through ASR consensus filtering removes misaligned audio-text pairs that would otherwise degrade model learning.
- Mechanism: Requiring exact match between Whisper-large-v3 and PhoWhisper-large transcriptions eliminates samples where either model makes errors, ensuring training data has near-perfect text-audio alignment.
- Core assumption: Consensus between two ASR systems correlates with ground truth accuracy.
- Evidence anchors:
  - [section] "We then retain only the samples where the Whisper-large-v3-based transcription matches exactly with the transcription output from PhoWhisper-large"
  - [section] "We conduct a post-processing step to manually inspect each audio sample... This process results in all correct transcriptions in the test sets"
  - [corpus] No corpus papers explicitly test this mechanism
- Break condition: If models trained with relaxed transcription matching (e.g., >95% similarity) perform equivalently, the filtering overhead is unnecessary.

### Mechanism 3
- Claim: VALL-E and VoiceCraft's autoregressive codec language modeling provides superior short-sentence synthesis compared to XTTS-v2's architecture.
- Mechanism: Token-level autoregressive generation with explicit codec token prediction enables precise control over short utterances, while XTTS-v2 may suffer from architecture-specific issues with brief inputs.
- Evidence anchors:
  - [section] "on the VIVOS test set, XTTS-v2PAB and viXTTS perform significantly worse than VALL-EPAB and VoiceCraftPAB across all evaluation metrics... VALL-EPAB and VoiceCraftPAB are more adept at handling short sentences"
  - [section] "for short text inputs, XTTS-v2-based models often generate redundant or rambling speech at the end of the output. This suggests a potential architectural issue within the XTTS-v2 model itself, rather than a data-related problem"
  - [corpus] VoiceCraft paper confirms "token infilling neural codec language model" achieves SOTA on speech editing and zero-shot TTS
- Break condition: If XTTS-v2 with modified stopping criteria or attention masks eliminates rambling on short inputs, the mechanism is decoding strategy rather than fundamental architecture.

## Foundational Learning

- Concept: **Neural Audio Codecs (EnCodec)**
  - Why needed here: VALL-E and VoiceCraft operate on discrete codec tokens rather than continuous spectrograms; understanding tokenization is essential for debugging generation artifacts.
  - Quick check question: Can you explain why codec token prediction is formulated as a language modeling task rather than regression?

- Concept: **Speaker Embeddings and Voice Cloning**
  - Why needed here: Zero-shot TTS requires conditioning on reference speaker audio; the paper uses explicit speaker IDs for training but reference audio for inference.
  - Quick check question: How does the model generalize to unseen speakers during inference if it was trained with discrete speaker IDs?

- Concept: **Phonemization for Vietnamese**
  - Why needed here: VALL-E and VoiceCraft require phoneme inputs, and Vietnamese dialectal variations affect phonemization accuracy.
  - Quick check question: Why might Northern vs. Southern Vietnamese dialect require different phoneme sequences for the same text?

## Architecture Onboarding

- Component map:
PhoAudiobook Dataset Pipeline: Raw Audio → Demucs (vocal extraction) → Whisper-large-v3 (transcription + timestamps) → PhoWhisper-large (verification) → wav2vec2-bartpho (speaker detection) → Text normalization (mbart-large-50) → Filter + Balance → Final dataset

Model Architectures: VALL-E: Text → Phonemes → [Phoneme Encoder] → Transformer Decoder (12 layers, 1024 hidden) → Audio → EnCodec tokens → [Audio Encoder] ↗

VoiceCraft: Text → Phonemes → [Extended Vocabulary] → Token Infilling Transformer → Audio → EnCodec tokens → Token Rearrangement ↗

XTTS-v2: Text → BPE tokens → [GPT-style Decoder] → Mel-spectrogram → Vocoder → Audio → Reference Audio → Speaker Encoder ↗

- Critical path:
  1. Data quality filtering (ASR consensus + speaker verification) most impacts downstream performance
  2. Phonemization accuracy determines VALL-E/VoiceCraft intelligibility
  3. Speaker ID mapping from training to inference creates the zero-shot capability

- Design tradeoffs:
  - Long audio (10-20s) vs. augmentation with short clips: Paper uses both (940h + 554h), but short-audio augmentation may not fully resolve XTTS-v2's architectural issues
  - Speaker balancing (max 4h/speaker) reduces dataset size from 1400h to 941h but prevents speaker dominance
  - Multi-stage verification (dual ASR + manual inspection) increases pipeline complexity but ensures test set reliability

- Failure signatures:
  - XTTS-v2 "rambling" on short inputs: Generates redundant speech after target text completes
  - High WER on VIVOS (short sentences): VALL-E/VoiceCraft ~12-13%, XTTS-v2 ~38% indicates architecture-specific short-text handling
  - Speaker similarity degradation on unseen speakers: SMOS drops from ~3.5-3.8 (seen) to ~3.0-3.6 (unseen) across models

- First 3 experiments:
  1. **Reproduce XTTS-v2 short-text failure**: Generate speech from 3-5 word Vietnamese sentences using XTTS-v2PAB; verify rambling behavior and test early-stopping heuristics
  2. **Ablate transcription verification**: Train a model on PhoAudiobook without PhoWhisper consensus filtering; compare WER to understand filtering contribution
  3. **Test cross-dataset generalization**: Evaluate all three models on a held-out domain (e.g., news broadcasts) to measure whether audiobook training limits real-world applicability

## Open Questions the Paper Calls Out
None

## Limitations
- XTTS-v2's architectural limitations for short sentences require decoding strategy modifications rather than fundamental architecture changes
- Strict ASR consensus filtering (exact match) may be unnecessarily conservative, potentially excluding useful training samples
- Limited cross-linguistic validation means generalizability to other low-resource languages remains untested

## Confidence
- **High Confidence**: Dataset creation methodology (ASR consensus filtering, speaker balancing, dialect-aware processing) is well-documented and reproducible
- **Medium Confidence**: Claims about XTTS-v2's architectural limitations for short sentences are supported by qualitative observations but lack systematic ablation studies
- **Low Confidence**: Cross-linguistic generalization claims are limited by testing only on Vietnamese data

## Next Checks
1. Conduct systematic ablation studies on XTTS-v2 to isolate whether the short-sentence rambling behavior stems from architectural constraints versus decoding hyperparameters (e.g., test different stopping criteria, attention masking strategies)
2. Train and evaluate models on a relaxed transcription verification protocol (>95% match instead of exact) to quantify the contribution of the strict ASR consensus filtering to final performance
3. Test all three models on held-out domains beyond audiobooks (e.g., news broadcasts, conversational speech) to assess whether audiobook-specific training limits real-world applicability