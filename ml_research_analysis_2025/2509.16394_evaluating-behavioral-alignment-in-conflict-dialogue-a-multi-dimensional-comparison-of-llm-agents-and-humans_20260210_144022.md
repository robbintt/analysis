---
ver: rpa2
title: 'Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional
  Comparison of LLM Agents and Humans'
arxiv_id: '2509.16394'
source_url: https://arxiv.org/abs/2509.16394
tags:
- human
- review
- anger
- llms
- remove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the behavioral alignment of personality-prompted
  LLM agents in adversarial dispute resolution dialogues. Using the KODIS dataset,
  we simulate multi-turn negotiations between LLMs (GPT-4.1, Claude-3.7-Sonnet, Gemini-2.0-Flash)
  guided by matched Five-Factor personality profiles.
---

# Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans

## Quick Facts
- **arXiv ID**: 2509.16394
- **Source URL**: https://arxiv.org/abs/2509.16394
- **Reference count**: 26
- **Primary result**: Personality-prompted LLMs show partial behavioral alignment with humans in adversarial negotiations, with GPT-4.1 excelling in linguistic/emotional alignment and Claude in strategic behavior, but significant gaps remain.

## Executive Summary
This study evaluates the behavioral alignment of personality-prompted LLM agents in adversarial dispute resolution dialogues against human benchmarks. Using the KODIS dataset, we simulate multi-turn negotiations between LLMs (GPT-4.1, Claude-3.7-Sonnet, Gemini-2.0-Flash) guided by matched Five-Factor personality profiles. We assess alignment across linguistic style, emotional dynamics, and strategic behavior using five complementary gap metrics. Results show GPT-4.1 achieves closest alignment with humans in linguistic and emotional metrics, while Claude best reflects strategic behavior. Despite progress, substantial behavioral gaps remain, highlighting both the promise and limitations of personality conditioning in modeling socially complex interactions.

## Method Summary
The study uses the KODIS dataset of human-human dispute dialogues with personality annotations as a baseline. LLM agents (GPT-4.1, Claude-3.7-Sonnet, Gemini-2.0-Flash) are assigned matched Five-Factor personality profiles sampled from human distributions. These agents engage in simulated Buyer-Seller negotiations over five issues (refund, reviews, apologies) until deal or walk-away. Alignment is evaluated using five gap metrics: linguistic gap (LG) via LIWC and JSD, linguistic entrainment gap (LEG) via nCLiD, anger trajectory gap (ATG) via DTW, anger magnitude gap (AMG) via AUC, and strategic behavior gap (SBG) via IRP distribution and JSD. Anger is annotated with Emoberta-large, and IRP strategies with GPT-4.1.

## Key Results
- GPT-4.1 achieves the closest alignment with humans in linguistic style (LEG=0.004) and emotional dynamics (ATG=0.195).
- Claude-3.7-Sonnet best reflects strategic behavior (SBG=0.018) among the tested models.
- LLMs generally express lower variance in anger trajectories compared to humans, indicating reduced behavioral diversity.
- Substantial behavioral gaps remain across all dimensions, suggesting personality conditioning alone is insufficient for full human-likeness.

## Why This Works (Mechanism)

### Mechanism 1: Personality Conditioning via Big Five Trait Matching
- Claim: Assigning LLMs matched Five-Factor personality profiles sampled from human distributions improves behavioral realism in dispute resolution, though alignment remains imperfect.
- Mechanism: Personality traits are verbalized through bipolar adjective pairs with modifiers conveying intensity. Traits are weighted-sampled to match human baseline distributions, creating individual variation that mirrors human diversity.
- Core assumption: Personality-driven variation causally shapes linguistic, emotional, and strategic behavior in conflict scenarios.
- Evidence anchors:
  - [abstract] "Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism."
  - [section 3.2] "To ensure fair comparison with human data, prompts mirror those in KODIS, and agent traits and priorities are weighted-sampled to match human distributions."
- Break condition: If personality prompts don't measurably reduce within-model variance or improve distributional alignment vs. unprompted baselines, conditioning may not be the active mechanism.

### Mechanism 2: Multi-Dimensional Divergence Metrics for Alignment Quantification
- Claim: A structured evaluation framework with tailored metrics captures alignment across linguistic, emotional, and strategic dimensions, revealing both surface patterns and deeper behavioral gaps.
- Mechanism: Five complementary gap metrics—LG (LIWC JSD-based), LEG (entrainment via nCLiD), ATG (anger trajectory via DTW), AMG (anger magnitude via AUC), and SBG (IRP strategy JSD)—quantify distributional and dyad-level alignment between human-human and LLM-LLM conversations.
- Core assumption: These metrics collectively capture the constructs that matter for human-likeness in adversarial negotiation.
- Evidence anchors:
  - [abstract] "We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior."
  - [section 3.3.1–3.3.4] Detailed formulations for LG, LEG, ATG, AMG, SBG using JSD, nCLiD, DTW, and AUC.
- Break condition: If reducing gap scores doesn't correlate with human judgment of realism, the metrics may not reflect perceived alignment.

### Mechanism 3: Model-Specific Strengths Suggest Architectural/Training Differences
- Claim: Different LLMs excel at different behavioral dimensions—GPT-4.1 in linguistic/emotional alignment, Claude in strategic behavior—suggesting architecture or training-data effects, though causation is not proven.
- Mechanism: Observed divergence patterns indicate that model capabilities are not uniform across behavioral dimensions.
- Core assumption: Observed differences stem from model architecture/training rather than prompt wording or random sampling alone.
- Evidence anchors:
  - [abstract] "GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior."
  - [section 4.2–4.5] Table 1 and Figures 2a, 2b, 7 show per-metric rankings and variance patterns.
- Break condition: If controlled prompt/seed ablations show convergence across models, differences may be prompt-artifacts rather than architectural.

## Foundational Learning

- **Concept**: IRP (Interests-Rights-Power) Strategy Framework
  - Why needed here: Core to SBG metric; classifies utterances into cooperative, neutral, and competitive strategies that shape conflict trajectories.
  - Quick check question: Can you label "I'll give you a refund if you remove your review" as an IRP category?

- **Concept**: Jensen-Shannon Divergence (JSD) for Distributional Comparison
  - Why needed here: Underlies LG and SBG metrics; measures similarity between probability distributions with bounds [0,1], enabling normalized gap calculations.
  - Quick check question: If JSD=0 for two LIWC distributions, what does that imply?

- **Concept**: Dynamic Time Warping (DTW) for Sequence Alignment
  - Why needed here: Underlies ATG; compares anger trajectories of different lengths by non-linear alignment, capturing temporal pattern similarity despite timing differences.
  - Quick check question: Why use DTW instead of Euclidean distance for comparing anger trajectories across conversations of varying lengths?

## Architecture Onboarding

- **Component map**: KODIS dataset -> Personality sampling -> LLM simulation -> Feature extraction -> Gap metric computation -> Model comparison
- **Critical path**:
  1. Sample personality profiles from human distribution
  2. Simulate L2L dialogues with personality-prompted LLMs
  3. Extract LIWC, entrainment, anger, IRP features
  4. Compute gap metrics vs. KODIS baseline
  5. Compare across models/dimensions
- **Design tradeoffs**:
  - Distributional vs. dyad-level metrics: LG/ATG/SBG compare distributions; LEG/AMG average dyad scores—interpretation differs
  - Role-play vs. real behavior: KODIS is structured role-play, not naturalistic conflict—limits ecological validity
  - Single-turn annotation vs. multi-turn dynamics: IRP annotated per utterance; strategic coherence across turns not directly captured
- **Failure signatures**:
  - Low variance in within-model distributions → mode-collapse, insufficient behavioral diversity
  - High AMG with low ATG → correct trajectory shape but wrong intensity (over-expressed anger)
  - Inconsistencies between stated issue importance and negotiation behavior → strategic reasoning failures
  - Very high walk-away rates → poor conflict resolution capability
- **First 3 experiments**:
  1. Ablate personality conditioning: Compare L2L simulations with vs. without personality prompts on all five gap metrics
  2. Variance injection: Test whether increasing temperature or adding persona diversity raises within-model variance toward human levels without worsening gap scores
  3. Cross-model pairing: Run human-LLM dialogues to evaluate whether gaps persist when one partner is human

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improved prompting strategies or fine-tuning methods resolve the observed inconsistencies between LLMs' stated issue importance and their actual negotiation behavior?
- Basis in paper: The authors state: "In some cases, LLMs displayed inconsistencies between their stated issue importance and their negotiation behavior, suggesting a lack of strategic reasoning. Developing improved prompting strategies or alternative methods to better support LLMs' strategic decision-making could lead to more realistic simulations."
- Why unresolved: Current personality-conditioned prompting alone does not ensure that agents negotiate according to their assigned priorities.
- What evidence would resolve it: Experiments comparing standard prompting against chain-of-thought reasoning, reinforcement learning, or fine-tuning approaches that measure correlation between assigned issue importance and actual negotiation concessions.

### Open Question 2
- Question: Why do personality-conditioned LLMs—particularly those with agreeable profiles—express higher anger levels than humans in dispute resolution, contrary to intuitive expectations?
- Basis in paper: The paper reports: "agreeable LLMs surprisingly express higher levels of anger in dispute resolution scenarios than humans," with Claude showing substantially greater anger expression (AUC=0.65 vs. human 0.29).
- Why unresolved: The mechanism driving elevated anger expression in agreeable-profiled agents remains unclear.
- What evidence would resolve it: Ablation studies isolating personality dimensions, analysis of training corpora for dispute-related content, and experiments varying emotional regulation instructions within prompts.

### Open Question 3
- Question: How can LLMs be improved to match the high behavioral variance observed in human anger trajectories, rather than converging toward lower-variance, more homogeneous responses?
- Basis in paper: Figure 2 and the analysis note that "human dyads show high variance in anger trajectories, while LLM dyads exhibit lower variance overall," and that "human anger intensity exhibits broader variance and greater variability; while LLMs generally show reduced variance."
- Why unresolved: Current LLMs may be converging toward modal or averaged behaviors rather than capturing the full distributional diversity of human conflict interactions.
- What evidence would resolve it: Experiments with temperature scaling, ensemble approaches with diverse personality seeds, or training modifications that explicitly optimize for distributional matching rather than point estimates.

## Limitations
- KODIS dataset scenarios are role-plays, not naturalistic dispute negotiations, limiting ecological validity.
- Personality-to-behavior translation relies on verbalized adjective pairs that are not fully specified, making it unclear if subtle trait differences are effectively communicated.
- Strategic coherence across turns is not directly evaluated, despite IRP annotation being per utterance.
- All evaluations compare L2L to H2H, not H2L, so interaction effects with humans are unknown.

## Confidence
- **High confidence**: Linguistic and emotional alignment patterns (LG, ATG, AMG) are robustly measured and consistently show GPT-4.1 closest to humans.
- **Medium confidence**: Strategic behavior gaps (SBG) are well-defined but rely on a single annotator (GPT-4.1), introducing potential bias.
- **Low confidence**: The causal link between personality conditioning and behavioral alignment is inferred but not directly tested via ablation studies.

## Next Checks
1. **Ablation study**: Compare L2L simulations with and without personality prompts across all five gap metrics to isolate the conditioning effect.
2. **Human-LLM evaluation**: Run H2L dialogues to test whether behavioral gaps persist when one partner is human, probing interaction effects.
3. **Variance analysis**: Measure within-model variance in LIWC features and IRP distributions; if too low, test temperature tuning or persona diversity injection to better match human behavioral heterogeneity.