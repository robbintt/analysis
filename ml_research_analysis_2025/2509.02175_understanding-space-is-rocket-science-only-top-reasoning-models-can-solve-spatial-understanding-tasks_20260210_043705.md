---
ver: rpa2
title: Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve
  Spatial Understanding Tasks
arxiv_id: '2509.02175'
source_url: https://arxiv.org/abs/2509.02175
tags:
- spatial
- wang
- https
- reasoning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RocketScience, a new contrastive benchmark
  designed to rigorously evaluate spatial understanding in vision-language models
  (VLMs). The benchmark is comprised of 482 real-world image-text pairs that test
  relative spatial relations and object ordering, deliberately crafted to be trivial
  for humans but challenging for current VLMs.
---

# Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks

## Quick Facts
- arXiv ID: 2509.02175
- Source URL: https://arxiv.org/abs/2509.02175
- Reference count: 40
- Key outcome: Only reasoning models with chain-of-thought prompting approach human-level performance on spatial understanding tasks

## Executive Summary
This paper introduces RocketScience, a new contrastive benchmark designed to rigorously evaluate spatial understanding in vision-language models (VLMs). The benchmark is comprised of 482 real-world image-text pairs that test relative spatial relations and object ordering, deliberately crafted to be trivial for humans but challenging for current VLMs. Through extensive evaluation across multiple model types—including CLIP-like models, standard VLMs, and reasoning-based VLMs—the authors demonstrate that most models, even those trained for spatial understanding, perform at chance levels. Only reasoning models with chain-of-thought prompting approach human-level performance. A disentanglement analysis reveals that poor performance stems primarily from limitations in spatial reasoning capabilities rather than object localization. The benchmark is released under CC-BY-4.0 license with evaluation code publicly available.

## Method Summary
The RocketScience benchmark evaluates spatial understanding through contrastive image-text pairs where each pair contains two images and two captions that differ only in spatial relations. Models must correctly match both image-caption pairings to score, forcing genuine spatial reasoning rather than relying on linguistic shortcuts or object detection alone. The evaluation includes 482 pairs across six spatial relation categories (horizontal, vertical, depth, proximity, order, absolute position). Multiple model types are tested including CLIP-like dual encoders, standard VLMs, and reasoning VLMs with chain-of-thought prompting. Performance is measured using text score, image score, and group score (requiring both text and image scores correct for the same pair).

## Key Results
- Most VLMs, including those trained for spatial understanding, perform at chance levels (~0.17 group score) on RocketScience
- Only reasoning models with chain-of-thought prompting achieve human-level performance (~0.97 group score)
- Disentanglement analysis shows spatial reasoning, not object localization, is the primary bottleneck for VLMs
- GPT-4o achieves ~96% accuracy on object localization but only 0.08 group score on full spatial reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pair design forces genuine spatial reasoning by eliminating linguistic shortcut solutions.
- Mechanism: Each test case presents two images and two captions that differ only in spatial relations. Models must correctly match both image-caption pairings to score. This prevents solving via object-detection alone or statistical linguistic co-occurrence, since both objects appear in both images and both captions contain the same words in different orders.
- Core assumption: Models that rely on surface-level correlations will fail on hard negatives where the unlikely spatial configuration is actually correct.
- Evidence anchors:
  - [abstract]: "comprised of entirely new real-world image-text pairs... deliberately crafted to be trivial for humans but challenging for current VLMs"
  - [Section 3.2.2]: "With the exception of the 'left' vs. 'right' distinction, in many cases, hard negatives control for unlikely noun-relation cases"
  - [corpus]: MIRAGE benchmark similarly identifies spatial reasoning gaps but uses different methodology.

### Mechanism 2
- Claim: Chain-of-thought (CoT) reasoning unlocks spatial understanding capabilities that exist but remain inaccessible in standard VLMs.
- Mechanism: Reasoning models produce step-by-step reasoning before answering, which decomposes spatial tasks into: (1) localize both objects, (2) determine their relative positions, (3) match to caption. Non-reasoning models must perform this integration implicitly in a single forward pass.
- Core assumption: The spatial reasoning capability is latent in VLMs but requires explicit decomposition to be deployed correctly.
- Evidence anchors:
  - [abstract]: "Only reasoning models with chain-of-thought prompting approach human-level performance"
  - [Section 5.1, Figure 4b]: Explicit CoT prompting improves GPT-4o from 0.19 to 0.51 group score; Llama-4 from 0.20 to 0.44
  - [corpus]: SpaRRTa and MIRAGE confirm spatial reasoning as a distinct capability gap.

### Mechanism 3
- Claim: Spatial reasoning, not object localization, is the performance bottleneck on spatial understanding tasks.
- Mechanism: The paper disentangles these capabilities by testing whether CoT models outperform non-CoT models on bounding-box localization. Both achieve ~96% accuracy on correctly localizing objects in horizontal position tasks, yet o4-mini scores 0.97 vs. GPT-4o's 0.08 on the full spatial reasoning task.
- Core assumption: Bounding-box accuracy is a valid proxy for localization capability independent of reasoning.
- Evidence anchors:
  - [Section 5.1, Figure 4a]: "gpt-4o is very close to o4-mini's performance, indicating that reasoning does not help with the localisation stage"
  - [Section 1]: "disentanglement analysis reveals that poor performance stems primarily from limitations in spatial reasoning capabilities rather than object localization"
  - [corpus]: MetaVQA identifies spatial reasoning as critical for embodied AI.

## Foundational Learning

- Concept: **Contrastive evaluation design**
  - Why needed here: RocketScience's core innovation is forcing models to discriminate between minimally different captions/images. Without understanding contrastive pairs, you cannot interpret the benchmark results or design similar evaluations.
  - Quick check question: Given two images (person left of car, person right of car) and two captions with identical words but different word order, can you explain why this prevents language-only models from succeeding?

- Concept: **Group score metric**
  - Why needed here: The paper uses a strict metric requiring success on both text-to-image AND image-to-text matching within a contrastive pair. Understanding this prevents misinterpreting partial capabilities.
  - Quick check question: If a model correctly identifies which caption matches image 1 (text score), but fails to identify which image matches caption 1 (image score), what is its group score for that pair?

- Concept: **Chain-of-thought as inference-time compute scaling**
  - Why needed here: The dramatic CoT improvement (0.19 → 0.51 for GPT-4o) represents a shift from single-pass to multi-step reasoning. Understanding this distinction is essential for model selection and deployment decisions.
  - Quick check question: Why might CoT help with spatial reasoning specifically, compared to other visual tasks?

## Architecture Onboarding

- Component map:
  - Images (1024×1024) -> CLIP embeddings or VLM encoder -> Spatial reasoning module -> Output (1 or 2)
  - Captions -> Text encoder -> Language understanding -> Output (1 or 2)
  - Contrastive pairs -> 4 questions per pair (I0+T0/T1, I1+T0/T1, T0+I0/I1, T1+I0/I1)

- Critical path:
  1. Load contrastive pair (I0, I1, T0, T1)
  2. For VLMs: present 4 questions with forced-choice format ("Reply only with 1 or 2")
  3. For CoT models: append "Reason about it and at the end write RESPONSE" to prompt
  4. Compute f(I0, I1, T0, T1) for image score, g(I0, I1, T0, T1) for text score
  5. Group score = 1 only if both f and g equal 1
  6. Report category breakdown (Horizontal, Vertical, Depth, Proximity, Order, Absolute Position)

- Design tradeoffs:
  - **Real-world images vs. synthetic**: Real images increase ecological validity but introduce uncontrollable variables. Synthetic datasets offer precise control but don't transfer to real-world performance.
  - **Dataset size**: 482 pairs is small but manually curated and verified for human solvability (98.5% accuracy). Authors show evaluation stability via subset sampling.
  - **Geographic/cultural bias**: All images collected in Europe and USA; authors explicitly note this limits generalization to other contexts.

- Failure signatures:
  - **Chance-level performance** (0.17 group score): Model is not performing spatial reasoning at all, likely relying on object detection + linguistic priors
  - **High text score, low image score**: Model understands spatial language but cannot ground it in visual input
  - **High localization accuracy, low group score**: Confirms reasoning bottleneck
  - **Performance gap between CoT and non-CoT**: Indicates latent reasoning capability exists but requires explicit decomposition

- First 3 experiments:
  1. **Baseline establishment**: Run your target VLM on RocketScience with standard prompting (no CoT). Report text/image/group scores across all 6 categories. If group score > 0.3, your model already has some spatial reasoning capability.
  2. **CoT intervention**: Re-run with explicit chain-of-thought prompting using the paper's template. Measure improvement delta. If < 10 percentage points, your model may lack latent spatial reasoning to unlock.
  3. **Localization probe**: Following Section 5.1, prompt your model to output bounding boxes for both objects in the horizontal position subset. Compare localization accuracy to group score. If localization is high (>90%) but group score is low (<30%), you've confirmed the reasoning bottleneck for your architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement observed with chain-of-thought (CoT) prompting generalize to smaller, open-source vision-language models, or is it exclusive to large commercial frontier models?
- Basis in paper: [explicit] Section 6.1 states that the observed benefit of CoT "may be specific to top-tier commercial models and might not generalize to smaller or open-source models."
- Why unresolved: The study primarily evaluates high-resource commercial models and large open models, leaving the efficacy of CoT for lower-resource regimes uncertain.
- What evidence would resolve it: A systematic evaluation of smaller open-source VLMs (e.g., under 10B parameters) using identical CoT prompting strategies on the RocketScience benchmark.

### Open Question 2
- Question: Are models solving spatial tasks by exploiting slight variations in camera angle between contrastive pairs rather than relying solely on object configuration?
- Basis in paper: [explicit] Section 6.1 notes that despite efforts to minimize changes, "slight variations in camera angle may occur" and could "introduce a potential shortcut."
- Why unresolved: The authors identify this as a potential confounder but do not conduct an ablation study to rule out that models are using angle differences as a signal.
- What evidence would resolve it: An evaluation using synthetic data or controlled capture setups where the camera pose is mathematically identical across contrastive pairs.

### Open Question 3
- Question: How does spatial reasoning performance degrade in environments with significantly higher object clutter compared to the relatively clean scenes in the current benchmark?
- Basis in paper: [explicit] Section 6.1 acknowledges that "some scenes remain less cluttered with objects than typical real-world environments" and that handling clutter is a "significant challenge."
- Why unresolved: The dataset was curated to ensure unambiguous, solvable pairs, which necessitated reducing visual noise typical of real-world settings.
- What evidence would resolve it: Evaluating models on an augmented version of the benchmark where distractor objects are synthetically added to increase scene complexity.

## Limitations
- The contrastive design may be sensitive to low-level visual features (camera angles) rather than pure spatial reasoning
- Geographic and cultural bias from exclusively European/US images may limit generalizability
- The small dataset size (482 pairs) may not capture full model capability distributions

## Confidence

- **High confidence**: The spatial reasoning bottleneck finding is well-supported by the localization vs. reasoning disentanglement experiment
- **Medium confidence**: The contrastive pair mechanism's effectiveness in forcing genuine spatial reasoning is theoretically sound but depends on assumptions about model behavior
- **Low confidence**: The geographic/cultural bias limitation's impact on results is acknowledged but not empirically evaluated across diverse populations

## Next Checks
1. **Cross-cultural validation**: Evaluate RocketScience on spatial relations with images from non-Western contexts to assess geographic bias impact on model performance
2. **Feature attribution analysis**: Use saliency or attention visualization to confirm models are attending to spatial configurations rather than low-level visual features when solving contrastive pairs
3. **Scaling relationship study**: Systematically vary CoT inference compute (number of reasoning steps) to determine whether improvements stem from reasoning decomposition or pure compute scaling