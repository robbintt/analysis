---
ver: rpa2
title: 'On Fact and Frequency: LLM Responses to Misinformation Expressed with Uncertainty'
arxiv_id: '2503.04271'
source_url: https://arxiv.org/abs/2503.04271
tags:
- uncertainty
- llms
- classification
- frequency
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how Large Language Models (LLMs) classify
  misinformation when expressed with uncertainty. Starting with verified-false climate
  claims, the authors transform them using an uncertainty typology based on modality
  and linguistic cues (e.g., doxastic, conditional, possible).
---

# On Fact and Frequency: LLM Responses to Misinformation Expressed with Uncertainty

## Quick Facts
- arXiv ID: 2503.04271
- Source URL: https://arxiv.org/abs/2503.04271
- Reference count: 6
- One-line primary result: 25% of false climate claims reclassified as not-false when expressed with uncertainty; doxastic phrases increase this to ~50%

## Executive Summary
This paper investigates how Large Language Models (LLMs) classify misinformation when expressed with uncertainty. Starting with verified-false climate claims, the authors transform them using an uncertainty typology based on modality and linguistic cues (e.g., doxastic, conditional, possible). They then prompt three LLMs—GPT-4o, LLaMA3, and DeepSeek-v2—to classify both original and transformed statements. Results show that after transformation, 25% of false statements are reclassified as not-false, a significant change. This shift is not generally explained by modality or most linguistic cues, but doxastic phrases like "it is believed…" strongly increase non-false classifications (~50%). Additionally, the authors elicit frequency estimates for these statements from the LLMs and find a small but significant correlation between higher frequency estimates and reclassification to not-false. The findings highlight the nuanced impact of linguistic uncertainty on LLM fact-checking and suggest potential avenues for improving model robustness.

## Method Summary
The authors start with 211 verified-false propositions from the ClimateFever dataset, filtered to model-specific subsets (181 for GPT-4o, 136 for LLaMA3, 110 for DeepSeek-v2). They apply uncertainty transformations using an established typology with six categories (possible, probable, conditional, doxastic, question, weasel) and cue phrases from FactBank, Wikiweasel, and BioScope corpora. Transformations are generated via LLaMA2 with temperature=0.5 and manually verified for quality. Classification is performed using zero-shot prompts with temperature=0.0, 5 runs per statement with majority voting. Frequency estimates (0-100 scale) are also elicited using zero-shot prompts. Statistical analysis employs McNemar's test for paired classification comparisons, chi-square tests for category effects, and logistic regression to examine frequency-classification correlations.

## Key Results
- 25% of false climate claims reclassified as not-false when expressed with uncertainty
- Doxastic transformations ("It is believed...") cause ~50% reclassification rate, significantly higher than other categories
- No significant differences in reclassification rates across hypothetical vs. epistemic modalities for most categories
- Small but significant correlation between higher frequency estimates and not-false classification (odds ratios: GPT-4o 1.071, LLaMA3 1.025, DeepSeek-v2 1.049, all p<0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Doxastic framing ("It is believed...") causes a substantially higher rate of not-false classification (~50%) compared to other uncertainty transformations (~25%).
- Mechanism: Doxastic constructions may restructure the proposition's truth-conditional target—from evaluating the embedded claim to evaluating the attribution of belief—which the model treats as a distinct truth assessment task.
- Core assumption: The model is tracking the shift in what is being asserted (belief attribution vs. factual claim) rather than simply failing to detect misinformation.
- Evidence anchors:
  - [abstract] "The exception is doxastic transformations, which use linguistic cue phrases such as 'It is believed...'"
  - [section 4 Results] "the doxastic category... is significantly causing a non-false classification in around 50% of the transformed statements independent of model tested"
  - [corpus] Related work on false presuppositions (arXiv:2505.22354) shows LLMs struggle with implicitly embedded false content, suggesting structural sensitivity to presuppositional framing.
- Break condition: If models were simply failing to detect misinformation, all uncertainty types should show similar reclassification rates—but they do not.

### Mechanism 2
- Claim: Higher frequency estimates are associated with increased odds of not-false classification.
- Mechanism: Models may encode statistical associations between statement frequency in training data and truth likelihood, analogous to (but not identical with) human illusory truth effects.
- Core assumption: The correlation reflects an internal association rather than mere output formatting artifacts.
- Evidence anchors:
  - [abstract] "We find a small but significant correlation between higher frequency estimates and reclassification to not-false."
  - [section 4 Results, Table 3] Logistic regression shows significant predictors across all three models (Odds ratios: GPT-4o 1.071, LLaMA3 1.025, DeepSeek-v2 1.049, all p<0.001)
  - [corpus] Evidence is limited; no direct corpus papers examine frequency-truth associations in LLMs.
- Break condition: If frequency estimates were unrelated to classification, the logistic regression would show non-significant predictors.

### Mechanism 3
- Claim: Most uncertainty transformations (conditional, possible, probable, question, weasel) produce similar reclassification rates (~25%), suggesting a baseline sensitivity to uncertainty signaling rather than cue-specific effects.
- Mechanism: The presence of any uncertainty marker may trigger a shift in the model's truth-value commitment threshold, independent of the specific linguistic form.
- Core assumption: The models share a common uncertainty-processing behavior that is not modality-specific.
- Evidence anchors:
  - [section 4 Results] "most linguistic categories cause a change of classification label in around 25% of the cases with no significant difference (accuracies range between .729–.840)"
  - [section 4 Results] χ² tests show no significant difference between hypothetical and epistemic modalities across models.
  - [corpus] Related work on implicit misinformation (arXiv:2503.09598) finds LLMs often fail to challenge subtly embedded false claims.
- Break condition: If specific linguistic cues drove the effect, we would see significant variation across subcategories—this is not observed except for doxastic.

## Foundational Learning

- Concept: **Epistemic vs. Hypothetical Modality**
  - Why needed here: The paper's uncertainty typology distinguishes these modalities; understanding them is essential for interpreting why the model responses differ (or don't).
  - Quick check question: If a statement says "It might rain" vs. "If it rains, we'll get wet," which modality applies to each?

- Concept: **Zero-Shot Prompting with Temperature=0**
  - Why needed here: The experimental methodology relies on zero-shot prompts with deterministic settings; understanding this is critical for reproducibility and interpreting variance.
  - Quick check question: Why would temperature=0 be chosen for a classification task over a generative task?

- Concept: **McNemar's Test for Paired Classification Comparisons**
  - Why needed here: The paper uses McNemar's test to assess whether classification changes are statistically significant; understanding this is necessary to evaluate the claims.
  - Quick check question: What does McNemar's test specifically compare, and why is it appropriate for before/after transformation pairs?

## Architecture Onboarding

- Component map: ClimateFever dataset -> Uncertainty typology (6 categories) -> LLaMA2 transformation generation -> Human verification -> Zero-shot classification (temp=0.0) -> Frequency elicitation -> McNemar's test + logistic regression analysis

- Critical path:
  1. Extract verified-false propositions from ClimateFever
  2. Filter to propositions correctly classified as false by target model
  3. Apply uncertainty transformations (6 types per proposition)
  4. Run classification and frequency elicitation (5 repetitions each)
  5. Aggregate via majority vote (classification) and mean (frequency)
  6. Analyze with McNemar's test and logistic regression

- Design tradeoffs:
  - **Domain specificity**: Climate-only dataset limits generalizability but controls for topic effects
  - **Prompt design**: Non-forced output allows natural model behavior but creates ambiguous "not-false" category
  - **Model scale**: Tested models range 8B-16B parameters; larger models may behave differently
  - **Transformation method**: LLaMA2-generated transformations were manually verified but may differ from human-authored uncertainty

- Failure signatures:
  - High doxastic reclassification rate (~50%) may indicate models conflate belief attribution with factual truth
  - "Not-false" category includes both "true" and ambiguous responses; future work should disambiguate
  - ClimateFever may be in training data; transformed statements are novel but base propositions are not

- First 3 experiments:
  1. Replicate with verified-true propositions to establish whether the 25% shift is symmetric or specific to false claims.
  2. Extend to additional domains (health, politics) to test generalizability beyond climate.
  3. Disambiguate "not-false" responses by forcing a structured output (true / uncertain / false / other) to characterize what the model produces when it doesn't classify as false.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying uncertainty transformations to verified-true propositions cause LLMs to reclassify them as false at a similar rate (25%) observed with false propositions?
- Basis in paper: [explicit] The authors list the restriction to "verified-false propositions" as a key limitation and suggest that "future work" must include "verified-true propositions" to understand the full range of LLM responses.
- Why unresolved: The current study design filters out non-false statements to isolate the "escape" of misinformation, leaving the behavior on true information untested.
- What evidence would resolve it: Replicate the experimental pipeline using verified-true claims from the ClimateFever dataset or similar benchmarks and measure the shift in classification.

### Open Question 2
- Question: Is the correlation between high frequency estimates and "not-false" classifications evidence of an "illusory truth" effect within LLMs?
- Basis in paper: [explicit] The authors explicitly state that future work should investigate "whether the connection between factchecking and frequency can shed light on how LLMs represent uncertainty internally," drawing a parallel to human "illusory truth effects."
- Why unresolved: The current study identifies a correlation but does not establish a causal mechanism or determine if this reflects an internal heuristic or merely training data biases.
- What evidence would resolve it: Intervention studies that artificially manipulate the frequency of specific statements in the model's context or training data to observe if it causally changes truth classification.

### Open Question 3
- Question: Are "not-false" classifications for doxastic transformations (e.g., "It is believed...") the result of the LLM correctly identifying the statement as a truthful report of a belief rather than an assertion of fact?
- Basis in paper: [inferred] The authors note in the limitations that "doxastic cases" might be technically true statements about beliefs (encapsulation) rather than the underlying misinformation, and suggest future work "further investigate not-false responses."
- Why unresolved: The binary classification method (false vs. not-false) aggregates "true" and "undetermined" responses, obscuring whether the model is successfully parsing the semantic scope of the doxastic phrase.
- What evidence would resolve it: A qualitative analysis or a fine-grained multi-class evaluation of LLM outputs for doxastic statements to distinguish between "verified report of belief" and "failure to detect misinformation."

## Limitations

- Model-specific behaviors may not generalize beyond the tested 8B-16B parameter LLMs
- "Not-false" category conflates true and ambiguous responses, limiting interpretability
- ClimateFever's public availability raises potential contamination concerns

## Confidence

- **High confidence**: The doxastic framing effect (50% reclassification) and its statistical significance are well-supported by McNemar's tests and consistent across models.
- **Medium confidence**: The 25% baseline reclassification rate and the lack of modality-specific effects are reproducible but may not generalize beyond climate claims.
- **Medium confidence**: The frequency-classification correlation (odds ratios 1.025-1.071) is statistically significant but represents a small effect size that requires further validation.

## Next Checks

1. Replicate the experiment with verified-true propositions to determine if the 25% reclassification rate is symmetric or specific to false claims.
2. Extend testing to additional domains (health, politics) to evaluate generalizability beyond climate-specific content.
3. Disambiguate the "not-false" category by implementing structured outputs (true/uncertain/false/other) to characterize what models produce when not classifying as false.