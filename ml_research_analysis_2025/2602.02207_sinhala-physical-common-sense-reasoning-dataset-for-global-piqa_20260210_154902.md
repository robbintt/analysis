---
ver: rpa2
title: Sinhala Physical Common Sense Reasoning Dataset for Global PIQA
arxiv_id: '2602.02207'
source_url: https://arxiv.org/abs/2602.02207
tags:
- sinhala
- dataset
- sinbert
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first Sinhala physical common sense reasoning
  dataset, comprising 110 human-created and verified samples designed for Global PIQA.
  The dataset includes prompt-answer pairs reflecting Sri Lankan cultural contexts,
  with each sample having a correct and incorrect answer.
---

# Sinhala Physical Common Sense Reasoning Dataset for Global PIQA

## Quick Facts
- arXiv ID: 2602.02207
- Source URL: https://arxiv.org/abs/2602.02207
- Reference count: 5
- Primary result: First Sinhala physical common sense reasoning dataset with 110 human-created samples

## Executive Summary
This work introduces the first Sinhala physical common sense reasoning dataset, comprising 110 human-created and verified samples designed for Global PIQA. The dataset includes prompt-answer pairs reflecting Sri Lankan cultural contexts, with each sample having a correct and incorrect answer. SinBERT achieved a low zero-shot accuracy of 49.09%, highlighting the difficulty of the task, while GPT-5 mini achieved 64.5%, though still with significant room for improvement. Qualitative analysis showed that SinBERT struggles with culturally specific reasoning, and GPT-5 mini's translations occasionally introduced errors. The dataset presents a challenging benchmark for low-resource language models in culturally grounded reasoning tasks.

## Method Summary
The paper creates a 110-sample Sinhala physical common sense reasoning dataset in Global PIQA format, with samples distributed across 9 cultural domains. Two monolingual models were evaluated in zero-shot mode: SinBERT and GPT-5 mini. SinBERT uses likelihood scoring on concatenated goal-solution pairs, while GPT-5 mini appears to translate Sinhala inputs to English before reasoning. The incorrect answers are generated through minimal lexical perturbations (1-3 word changes, letter swaps, or phrase swaps) to create adversarial difficulty. Evaluation includes domain-wise accuracy breakdowns and qualitative error analysis focusing on culturally specific terminology failures.

## Key Results
- SinBERT achieved 49.09% accuracy, below random chance (50%), indicating systematic difficulty with culturally grounded reasoning
- GPT-5 mini achieved 64.5% accuracy, significantly better than SinBERT but still far from perfect performance
- Translation errors in GPT-5 mini occasionally caused incorrect reasoning, such as misidentifying "bath kolaya" (polythene) as "banana leaf"
- SinBERT showed particularly poor performance on Buddhism, Food, and Proverbs domains, while GPT-5 mini struggled with History, Food, and Sports

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural grounding in physical commonsense datasets increases task difficulty for models trained primarily on majority-language data
- Mechanism: The dataset embeds domain knowledge across 9 culturally-specific categories where correct answers require understanding Sri Lankan cultural practices, materials, and reasoning patterns—not just linguistic fluency. SinBERT's 49.09% accuracy (below random chance) suggests the model lacks exposure to these cultural priors during pretraining
- Core assumption: That poor performance stems from cultural knowledge gaps rather than dataset artifacts
- Evidence anchors: "Most of the questions refer to the Sri Lankan context", "All the questions were manually created", neighbor papers show consistent patterns of cultural specificity affecting model performance
- Break condition: If models with different training corpora show significantly different accuracy patterns on culturally-embedded vs. universal questions

### Mechanism 2
- Claim: Translation-mediated reasoning introduces error cascades in multilingual models for culturally-specific terminology
- Mechanism: GPT-5 mini translates Sinhala inputs to English before reasoning. When source-language terms have no direct English equivalent or have false cognates, the translation step corrupts the reasoning chain
- Core assumption: That translation occurs before reasoning (inferred from error patterns)
- Evidence anchors: "GPT-5 mini translates the Sinhala text to English before reasoning on it. Sometimes, translation errors result in the model providing wrong answers", "bath kolaya" → "banana leaf" mistranslation example
- Break condition: If a model trained directly on Sinhala cultural texts without translation achieves higher accuracy

### Mechanism 3
- Claim: Small edit distance between correct and incorrect answer pairs creates adversarial difficulty for language models even when cultural knowledge is present
- Mechanism: Incorrect answers are generated by minimal perturbations to correct answers, creating semantically similar but physically incorrect alternatives
- Core assumption: That edit distance is a meaningful proxy for adversarial difficulty
- Evidence anchors: "The wrong answer for a given prompt was prepared by either 1. changing one letter of a word, 2. changing 1-3 words of a sentence, or 3. swapping words/phrases", shows cumulative accuracy declining with increased edit distance
- Break condition: If models show consistent accuracy degradation correlated with semantic distance (not just edit distance) between alternatives

## Foundational Learning

- Concept: **Low-resource language classification and its implications**
  - Why needed here: Sinhala is classified as low-resource due to scarcity of digital resources and geographic isolation from related language initiatives. This explains why SinBERT underperforms—it likely has insufficient pretraining data for robust cultural knowledge representation
  - Quick check question: Can you explain why a language spoken by 17+ million people might still be considered low-resource in NLP contexts?

- Concept: **Zero-shot evaluation vs. fine-tuning for small datasets**
  - Why needed here: The paper explicitly chose zero-shot evaluation over fine-tuning due to the 110-sample size, preventing meaningful weight updates. Understanding this distinction is critical for interpreting the results as measuring pretrained knowledge rather than task adaptability
  - Quick check question: Why would fine-tuning a 110-sample dataset risk overfitting rather than demonstrating model capability?

- Concept: **Physical commonsense reasoning benchmark structure (PIQA format)**
  - Why needed here: The dataset follows PIQA conventions: [Goal] prompt with two candidate solutions [Sol1] and [Sol2], exactly one correct. Understanding this binary-choice structure is essential for designing experiments and interpreting accuracy metrics (50% = random baseline)
  - Quick check question: What does an accuracy below 50% suggest about a model's decision process on a binary-choice task?

## Architecture Onboarding

- Component map: Dataset creation by domain experts → Cross-verification between authors → Tokenization via SinLLaMA → Zero-shot inference with probability-based selection → Domain-stratified accuracy computation → Qualitative error analysis

- Critical path:
  1. Sample creation by domain experts (native speakers with 30+ years residence, PhD-level NLP researchers)
  2. Cross-verification between authors for quality assurance
  3. Tokenization using SinLLaMa tokenizer for sequence length analysis
  4. Zero-shot inference with probability-based answer selection
  5. Domain-stratified accuracy computation and error analysis

- Design tradeoffs:
  - Dataset size vs. quality: 110 samples limits statistical power but ensures human-verified cultural authenticity
  - Zero-shot vs. fine-tuning: Chose zero-shot to avoid overfitting; sacrifices ability to measure few-shot adaptability
  - Domain breadth vs. depth: 9 domains with uneven distribution (40 "Other" vs. 4 "Literature"); provides cultural coverage but limits per-domain statistical conclusions
  - Translation transparency: GPT-5 mini's internal translation is inferred from errors, not architecturally observable

- Failure signatures:
  - Cultural hallucination: Model generates plausible-sounding but incorrect cultural references
  - Translation cascade errors: GPT-5 mini mistranslates culturally-specific terms with historical approximations
  - Below-random performance: SinBERT's 49.09% suggests systematic bias toward incorrect options
  - Domain-specific degradation: Buddhism and Food domains show large accuracy gaps between models

- First 3 experiments:
  1. Baseline replication: Run zero-shot evaluation with SinBERT on the 110-sample dataset, stratified by domain and question type, to reproduce the 49.09% accuracy
  2. Translation ablation: Compare GPT-5 mini performance with (a) direct Sinhala input, (b) pre-translated English input, and (c) chain-of-thought prompting in Sinhala
  3. Edit distance sensitivity analysis: Group samples by edit distance bins and compute accuracy per bin for both models to validate whether adversarial perturbation strength correlates with difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a native Sinhala inference approach mitigate the specific translation errors that degraded GPT-5 mini's performance?
- Basis in paper: The authors identify that GPT-5 mini translates Sinhala to English before reasoning, leading to semantic errors such as misidentifying "bath kolaya" (polythene) as "banana leaf"
- Why unresolved: The paper only evaluates the specific translation-based pipeline used by the commercial model and does not test an alternative native-language reasoning pathway
- What evidence would resolve it: A comparative evaluation of GPT-5's performance using a native Sinhala prompt mode versus its standard translation mode on the erroneous samples

### Open Question 2
- Question: How does model performance vary across the diverse ethnic and religious sub-cultures of Sri Lanka not covered in this dataset?
- Basis in paper: The "Limitations" section states the dataset is biased toward Sinhala-Buddhist culture because both data creators belong to this demographic
- Why unresolved: It is unknown if the low accuracy (49.09% for SinBERT) is consistent across other local cultural contexts or if the models are specifically weak only in the Buddhist context provided
- What evidence would resolve it: Creation and evaluation of a balanced corpus containing samples from Sri Lankan Tamil and Muslim communities to compare accuracy across groups

### Open Question 3
- Question: What is the statistical stability of the GPT-5 mini results given the single-run evaluation methodology?
- Basis in paper: The authors acknowledge they "tested GPT-5 mini only once via its GUI interface" and note that its non-deterministic nature implies results may differ
- Why unresolved: The reported 64.5% accuracy is a single data point without variance metrics, making it difficult to determine if the results are reproducible or outliers
- What evidence would resolve it: Reporting the mean accuracy and standard deviation over multiple inference runs for the GPT-5 mini model

## Limitations

- Dataset accessibility remains a critical bottleneck—the incomplete `/gtbmrlbenchmarks` URL prevents direct reproduction and validation of the 110-sample dataset
- SinBERT's zero-shot prompting format is unspecified, making it impossible to verify whether the 49.09% accuracy reflects the actual experimental conditions
- GPT-5 mini's translation-mediated reasoning is inferred from error patterns rather than architectural transparency, leaving the mechanism speculative

## Confidence

- **High**: The general finding that low-resource language models struggle with culturally-specific physical commonsense tasks is well-supported by the dataset's structure and SinBERT's sub-random performance
- **Medium**: The translation error mechanism for GPT-5 mini is plausible given the error examples but remains inferential without architectural transparency
- **Low**: The correlation between edit distance and accuracy (Figure 4) is suggestive but not statistically robust given the small sample size and noisy relationship

## Next Checks

1. **Dataset accessibility verification**: Confirm the complete download location for the 110-sample dataset and verify its format matches the Global PIQA specifications
2. **SinBERT prompting standardization**: Test multiple zero-shot prompting templates to determine which configuration reproduces the reported 49.09% accuracy
3. **Translation mechanism isolation**: Compare GPT-5 mini performance on (a) original Sinhala input, (b) human-translated English input, and (c) chain-of-thought reasoning in Sinhala to definitively establish whether translation errors or cultural knowledge gaps drive the accuracy differences