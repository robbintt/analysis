---
ver: rpa2
title: On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language
  Models
arxiv_id: '2512.07783'
source_url: https://arxiv.org/abs/2512.07783
tags:
- reasoning
- pre-training
- post-training
- generalization
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a controlled experimental framework to study
  how pre-training, mid-training, and reinforcement learning (RL) interact to shape
  reasoning capabilities in language models. The authors use synthetic reasoning tasks
  with explicit dependency graphs and process-verified evaluation to isolate causal
  effects of each training stage.
---

# On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models

## Quick Facts
- arXiv ID: 2512.07783
- Source URL: https://arxiv.org/abs/2512.07783
- Reference count: 40
- Authors: Charlie Zhang; Graham Neubig; Xiang Yue
- One-line primary result: RL produces true reasoning capability gains only when pre-training leaves headroom and RL targets edge-of-competence tasks

## Executive Summary
This paper introduces a controlled experimental framework to study how pre-training, mid-training, and reinforcement learning (RL) interact to shape reasoning capabilities in language models. The authors use synthetic reasoning tasks with explicit dependency graphs and process-verified evaluation to isolate causal effects of each training stage. They find that RL produces true reasoning capability gains (pass@128) only when pre-training leaves headroom and RL targets tasks at the model's edge of competence. Minimal pre-training exposure (≥1%) is sufficient for RL to enable contextual generalization, while mid-training substantially improves both in-domain and out-of-domain performance under fixed compute budgets. Process rewards further reduce reward hacking and improve reasoning fidelity.

## Method Summary
The study uses a 100M parameter decoder-only model (Qwen2.5-style) trained on synthetic reasoning tasks with directed acyclic graphs (DAGs) of arithmetic operations. The training pipeline consists of three stages: pre-training on 10B tokens (op=2-10 across three contexts), optional mid-training on 1B tokens (op=11-14), and GRPO-based post-training on 200K samples. The evaluation uses strict process-verified pass@k metrics (k=1,128) where correct requires both process accuracy=1 and final answer match. The synthetic GSM-Infinite framework allows precise control over task complexity (op(G)=|E|) and enables clean causal analysis of training stage interactions.

## Key Results
- RL produces genuine capability gains (pass@128) only when pre-training leaves representational headroom and RL data targets edge-of-competence tasks (+42% pass@128)
- Minimal pre-training exposure (≥1%) to target-context primitives is sufficient for RL to enable cross-context generalization
- Mid-training substantially improves both in-domain and out-of-domain performance under fixed compute, outperforming RL alone by +10.8% on OOD-hard tasks
- Process rewards reduce reward hacking and improve reasoning fidelity compared to outcome-only rewards

## Why This Works (Mechanism)

### Mechanism 1: Edge-of-Competence RL Targeting
RL produces genuine capability gains (measured at pass@128) only when two conditions jointly hold: (a) the task domain was not saturated during pre-training, leaving representational headroom; and (b) the RL data distribution targets tasks at the model's boundary of solvability—neither fully in-distribution nor impossibly out-of-distribution. When RL data aligns with tasks where the base model has non-zero but sub-optimal pass@k (e.g., pass@128 > 0 but pass@1 ≈ 0), gradient signals can reinforce partial solution paths into reliable policies. Assumption: the base model must have partial competence (non-zero pass@k) in the target domain for RL to amplify rather than distort.

### Mechanism 2: Primitive Seeding for Contextual Transfer
RL enables cross-context generalization only when the base model has been pre-trained with minimal exposure (≥1%) to target-context primitives. RL cannot synthesize reasoning primitives de novo. Pre-training seeds latent representations of atomic operations in the target context. RL then amplifies and composes these seeds into higher-order reasoning chains. Without the seed, RL has no gradient foothold. With even sparse seeding, RL's on-policy exploration can discover compositional structures.

### Mechanism 3: Mid-Training as Distributional Bridge
Inserting a mid-training phase between pre-training and RL substantially improves OOD reasoning under fixed compute budgets. Mid-training narrows the distribution gap between broad pre-training corpora and narrow RL objectives. By concentrating next-token-prediction supervision on edge-of-competence data, mid-training stabilizes representations that RL can then efficiently explore. Assumption: the benefit derives from representation alignment rather than additional data quantity.

## Foundational Learning

- **Concept: pass@k evaluation**
  - **Why needed here**: The paper's central claim relies on distinguishing between RL "sharpening" (improving pass@1 but not pass@k for k>1) vs. RL "extending" (improving pass@128). Without understanding pass@k, you cannot interpret the results.
  - **Quick check question**: If a model improves from 60% to 90% on pass@1 but remains at 70% on pass@128 after RL, has RL extended capability or just sharpened?

- **Concept: Reward hacking**
  - **Why needed here**: The paper identifies reward hacking as a key failure mode in outcome-only RL, and demonstrates that process-level supervision mitigates it. Understanding this is essential for interpreting the process-reward experiments.
  - **Quick check question**: If a model achieves 95% final-answer accuracy but only 40% process accuracy, what failure mode is likely occurring?

- **Concept: DAG-structured reasoning**
  - **Why needed here**: The synthetic tasks are grounded in directed acyclic graphs with explicit operation dependencies. The evaluation protocol parses model outputs back into predicted DAGs for step-level verification.
  - **Quick check question**: In a DAG with 15 edges, what is the minimum reasoning chain length required to compute the answer node?

## Architecture Onboarding

- **Component map**: Pre-training (10B tokens, op=2-10) -> Mid-training (optional, 1B tokens, op=11-14) -> Post-training (GRPO, 200K samples) -> Evaluation (process-verified pass@k)
- **Critical path**: Pre-training establishes primitives → (Mid-training concentrates edge-of-competence supervision) → RL targets edge tasks with process-aware rewards → Evaluation via strict process-verified pass@k
- **Design tradeoffs**: Heavy mid-training / light RL: Best for OOD-edge pass@1 (reliability on near-domain tasks); Light mid-training / heavy RL: Best for OOD-hard pass@k (exploration on far-domain tasks); Process-only rewards: Higher fidelity but slower convergence; outcome-only: faster but hack-prone
- **Failure signatures**: RL on fully in-distribution data: pass@1 improves, pass@k flat (sharpening only); RL on out-of-reach data: reward stagnation, policy degradation; Zero pre-training exposure to target context: RL fails to transfer regardless of RL composition
- **First 3 experiments**: 
  1. Baseline calibration: Pre-train on op=2-10 only, evaluate pass@1 and pass@128 on op=2-10, op=11-14, op=15-20
  2. Edge-of-competence RL: Apply GRPO on op=11-14 data, verify pass@128 improvement on op=11-14 and op=15-20
  3. Primitive seeding ablation: Pre-train with 0%, 0.1%, 1%, and 10% exposure to context B atomic operations, apply identical RL, measure transfer gap

## Open Questions the Paper Calls Out

- **Question 1**: Can an automated, self-paced curriculum dynamically track the "edge of competence" during RL training to maintain optimal capability gains?
  - **Basis in paper**: Practical Guidance 1 suggests the process could be iterative: "we can periodically re-evaluate the pool... as the model gets stronger, previously out-of-distribution tasks will drift into the solvability gap, creating a natural, self-paced curriculum."
  - **Why unresolved**: The authors demonstrate the importance of the "edge of competence" using static difficulty ranges but propose an adaptive curriculum as a future direction without implementation or validation.

- **Question 2**: Do the findings regarding pre-training exposure seeds and mid-training bridges hold for significantly larger models and non-synthetic data?
  - **Basis in paper**: The paper establishes causal links using "100M parameter" models on "controllable synthetic reasoning tasks" but leaves verification of scaling properties for future work.
  - **Why unresolved**: While the synthetic setting allows for precise control, it remains unclear if the "1% exposure" threshold for contextual transfer persists in large-scale Transformers trained on natural language corpora.

## Limitations

- The controlled experimental framework relies heavily on synthetic reasoning tasks with explicit dependency graphs, which may not fully capture the complexity of real-world reasoning scenarios
- The study uses a 100M parameter model, limiting conclusions about scaling effects to larger models where pre-training saturation dynamics and RL effectiveness may differ substantially
- The synthetic setting enables precise causal analysis but may overestimate the clean composability of reasoning primitives compared to noisy natural language

## Confidence

- **High confidence**: Edge-of-competence RL targeting mechanism and process reward benefits
- **Medium confidence**: Primitive seeding for contextual transfer
- **Medium confidence**: Mid-training bridging benefits

## Next Checks

1. **Natural task validation**: Apply the pre-training headroom + edge-of-competence RL framework to a natural reasoning benchmark (e.g., GSM8K or MATH) to test whether synthetic findings transfer to real-world problems

2. **Scaling validation**: Repeat the edge-of-competence RL experiments with models in the 1B-8B parameter range to identify at what scale the headroom requirement and RL effectiveness patterns change

3. **Context diversity validation**: Test primitive seeding transfer across more than three synthetic contexts with varying semantic and structural similarity to establish the boundaries of context-independent primitive transfer