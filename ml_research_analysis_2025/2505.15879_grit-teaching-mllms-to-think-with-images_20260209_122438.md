---
ver: rpa2
title: 'GRIT: Teaching MLLMs to Think with Images'
arxiv_id: '2505.15879'
source_url: https://arxiv.org/abs/2505.15879
tags:
- reasoning
- answer
- grit
- grounded
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRIT (Grounded Reasoning with Images and
  Texts), a method that trains multimodal large language models (MLLMs) to perform
  reasoning by interleaving natural language with explicit bounding box coordinates
  that reference relevant image regions. The proposed approach addresses the limitation
  of existing visual reasoning models that generate reasoning chains purely in natural
  language without explicit visual grounding.
---

# GRIT: Teaching MLLMs to Think with Images

## Quick Facts
- **arXiv ID:** 2505.15879
- **Source URL:** https://arxiv.org/abs/2505.15879
- **Reference count:** 39
- **Primary result:** GRIT trains MLLMs to generate visually grounded reasoning chains using only 20 image-question-answer triplets without explicit reasoning or bounding box annotations.

## Executive Summary
GRIT addresses the limitation of existing visual reasoning models that generate reasoning chains purely in natural language without explicit visual grounding. The method trains multimodal large language models to interleave natural language with explicit bounding box coordinates referencing relevant image regions. Using a novel reinforcement learning algorithm (GRPO-GR) with format-only rewards, GRIT achieves exceptional data efficiency—requiring only 20 training samples to produce coherent, visually grounded reasoning chains. Comprehensive evaluations show GRIT-trained models outperform baselines across multiple benchmarks, demonstrating that bounding box generation facilitates stronger attention to visual information in subsequent reasoning.

## Method Summary
GRIT employs a format-only reward approach where the model learns to generate reasoning chains interleaving natural language with bounding box coordinates. The GRPO-GR algorithm provides rewards for syntactically valid bounding boxes and correct token structure, eliminating the need for data with reasoning chain annotations or explicit bounding box labels. The training procedure requires only 20 image-question-answer triplets (10 from VSR, 10 from TallyQA) and uses a composite reward system including format adherence, counting accuracy (for counting tasks), and answer correctness. The method demonstrates that bounding box generation facilitates stronger attention to visual information in subsequent reasoning, effectively unifying grounding and reasoning abilities.

## Key Results
- GRIT-trained models achieve up to 72.9 GPT-as-judge answer accuracy on VSR benchmark
- Grounding IoU reaches up to 0.495 on OVDEval, demonstrating effective visual grounding
- Models trained with only 20 samples outperform baselines across multiple benchmarks including VSR, TallyQA, GQA, MathVista-mini, MME, and OVDEval
- Format-only rewards are sufficient to induce grounded reasoning behavior without explicit reasoning chain or bounding box annotations

## Why This Works (Mechanism)
GRIT's approach works because format-only rewards can induce grounded reasoning behavior without explicit reasoning chain or bounding box annotations. The GRPO-GR algorithm provides positive reward for syntactically valid bounding boxes and correct token structure, which encourages the model to learn the correspondence between natural language reasoning and visual regions. This format-based training is data-efficient because it doesn't require expensive annotation of reasoning chains or bounding boxes, yet still produces models capable of coherent, visually grounded reasoning.

## Foundational Learning
- **Reinforcement Learning with Group Normalization (GRPO-GR):** Why needed: Enables training with sparse rewards and small datasets. Quick check: Verify group-normalized advantage computation and reward decomposition.
- **Bounding Box Coordinate Encoding:** Why needed: Provides explicit visual grounding in reasoning chains. Quick check: Confirm regex pattern for detecting syntactically valid bounding boxes.
- **Composite Reward Design:** Why needed: Balances multiple objectives (format, counting, answer accuracy) without explicit annotations. Quick check: Validate reward function implementation matches paper description.
- **GPT-as-Judge Evaluation:** Why needed: Automated, scalable answer accuracy assessment. Quick check: Verify GPT-4o judge prompts and score extraction logic.

## Architecture Onboarding
**Component Map:** Base MLLM -> GRPO-GR Training -> Format Reward Processing -> Model Update
**Critical Path:** Input image+question → Model generation → Reward computation (format + answer) → Advantage calculation → Parameter update
**Design Tradeoffs:** Format-only rewards vs. explicit annotation costs; small training set vs. generalization; GRPO vs. supervised learning.
**Failure Signatures:** Low Vision-Language Reasoning Cross-Modal Correlation score indicates disconnected grounding/reasoning; performance collapse when forcing interleaved grounding on untrained models.
**Three First Experiments:** 1) Train with format reward only vs. full composite reward to verify data efficiency. 2) Compare GRPO-GR vs. one-shot ICL with grounding on base models. 3) Test varying training set sizes to assess data variety vs. volume importance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the results suggest important directions for future work regarding the generalization challenges that remain when increasing training data volume, indicating that data variety may be more critical than volume for improvements.

## Limitations
- The approach relies heavily on GPT-4o as judge, introducing potential bias and variability not quantified through multiple judges or human evaluation.
- Critical implementation details remain unspecified, including exact GRPO hyperparameters (clip range epsilon, beta for KL penalty, delta constant value) and the 20 training samples used.
- The claim about data efficiency is based on a small, unspecified training set whose quality and representativeness are unknown.

## Confidence
- **High confidence:** Format-only rewards can induce grounded reasoning behavior without explicit annotations (supported by ablation showing 72.9% accuracy on VSR with format reward alone).
- **Medium confidence:** Exceptional data efficiency (20 samples sufficient) is supported but limited by unknown training data quality and heavy reliance on GPT-4o judging.
- **Low confidence:** The assertion that "data variety is more critical than volume" is speculative, based on limited experiments with only one model tested on OVDEval with varying training set sizes.

## Next Checks
1. Release the exact 20 training samples and test splits to enable independent verification of data efficiency claims and assessment of sample quality.
2. Conduct human evaluation of a subset of outputs to validate GPT-4o judge scores and measure inter-judge agreement, particularly for the grounding IoU metric.
3. Test GRIT training on multiple base model scales and with varied training set compositions to empirically verify whether data variety truly matters more than volume.