---
ver: rpa2
title: A Comprehensive Study on Fine-Tuning Large Language Models for Medical Question
  Answering Using Classification Models and Comparative Analysis
arxiv_id: '2501.17190'
source_url: https://arxiv.org/abs/2501.17190
tags:
- medical
- performance
- large
- bert
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated four fine-tuned language models (LoRA Roberta-large,
  Roberta-base, Bert Uncased, and Bert Large Uncased) for classifying medical questions
  using a 5-fold cross-validation approach. The models were trained on a dataset of
  6,800 samples scraped from Healthline.com with synthetic augmentation.
---

# A Comprehensive Study on Fine-Tuning Large Language Models for Medical Question Answering Using Classification Models and Comparative Analysis

## Quick Facts
- arXiv ID: 2501.17190
- Source URL: https://arxiv.org/abs/2501.17190
- Reference count: 23
- Primary result: BERT Large Uncased achieved 100% accuracy in classifying medical questions; Roberta-base achieved 99.87%

## Executive Summary
This study evaluates four fine-tuned language models (LoRA Roberta-large, Roberta-base, Bert Uncased, and Bert Large Uncased) for classifying medical questions using a 5-fold cross-validation approach. The models were trained on a dataset of 6,800 samples scraped from Healthline.com with synthetic augmentation. BERT Large Uncased achieved perfect performance across all metrics (100% accuracy, precision, recall, and F1 score), while Roberta-base showed near-perfect results (99.87%, 99.81%, 99.86%, 99.82%). The study demonstrates that fine-tuning large language models can significantly improve medical question-answering capabilities through a two-stage approach: predicting a specific label for the received medical question and then providing a predefined answer for this label.

## Method Summary
The study employs a two-stage medical question-answering system where four transformer-based models are fine-tuned to classify medical questions into predefined labels. The dataset consists of 6,800 samples from Healthline.com (scraped + synthetic augmentation) with Disease, Question, and Label columns. Models are evaluated using 5-fold cross-validation with 10 epochs per fold, measuring accuracy, precision, recall, and F1 scores. The approach uses a classification layer on top of BERT/RoBERTa variants, followed by a lookup table mapping predicted labels to static medical answers. Training was conducted on T4 GPU using the FastAI library.

## Key Results
- BERT Large Uncased achieved perfect scores (100%) across all metrics but may indicate overfitting
- Roberta-base demonstrated near-perfect performance (99.87% accuracy, 99.81% precision, 99.86% recall, 99.82% F1)
- Bert Uncased showed strong performance (95.85% accuracy, 94.42% precision, 95.58% recall, 94.72% F1)
- LoRA Roberta-large underperformed with moderate results (78.47% accuracy, 72.91% precision, 76.95% recall, 73.56% F1)

## Why This Works (Mechanism)

### Mechanism 1
The system achieves high reported accuracy by constraining the problem space to classification (label prediction) rather than open-ended generation. The architecture operates a "classify-then-retrieve" pipeline where the model predicts a discrete label (e.g., `diabetes definition`) from the input question, then maps this label to a static, pre-verified answer stored in a secondary dataset.

### Mechanism 2
Superior performance of BERT Large is likely driven by the model's capacity to overfit or perfectly memorize the specific phrasing patterns in a relatively small dataset (6,800 samples). BERT Large Uncased (340M+ parameters) has significantly higher capacity to learn fine-grained distinctions than Roberta-base or Bert Uncased, effectively memorizing the mapping between question variations and labels.

### Mechanism 3
Parameter-efficient fine-tuning (LoRA) underperformed standard full fine-tuning in this specific context, likely due to suboptimal rank configuration or insufficient adaptation steps relative to model complexity. LoRA froze pre-trained weights and injected trainable rank-decomposition matrices, but the low-rank update was insufficient to shift the decision boundary for the specific medical labels.

## Foundational Learning

- **Concept: Transformer Encoders (BERT/RoBERTa)**
  - Why needed here: The study relies on bidirectional context awareness to understand medical questions, optimized for classification/regression on text pairs
  - Quick check question: Why might a masked language model (BERT) be preferred over a causal language model (GPT) for a single-label classification task?

- **Concept: Cross-Validation**
  - Why needed here: The paper uses 5-fold cross-validation to claim robustness; understanding data splitting is critical to interpreting variance between folds
  - Quick check question: If Fold 1 scores 80% and Fold 5 scores 100%, what does this suggest about data distribution across folds?

- **Concept: Overfitting vs. Generalization**
  - Why needed here: Achieving 100% on a small dataset (6,800 rows) is a classic red flag for overfitting; one must understand the trade-off between training loss and validation performance
  - Quick check question: The authors flag "potential overfitting" for BERT Large. What validation technique would best confirm if this model generalizes to real-world clinical questions?

## Architecture Onboarding

- **Component map:** Input Medical Question String -> Transformer Encoder (BERT/RoBERTa variants) -> Classification Layer (Softmax over N labels) -> Lookup Table (Predicted Label -> Static Medical Answer)
- **Critical path:** The Tokenizer-to-Model alignment is critical. Using the wrong tokenizer (e.g., BertTokenizer on RoBERTa model) will immediately break the pipeline or degrade performance to random guessing.
- **Design tradeoffs:**
  - Roberta-base vs. Bert Large: Roberta-base offers better efficiency/accuracy trade-off (99.87% accuracy, faster/cheaper) than complex Bert Large (100% but risk of overfitting)
  - LoRA vs. Full Fine-tuning: LoRA failed to deliver expected accuracy here (78% vs 99%), suggesting that for specialized domain tasks on smaller datasets, full weight updates might be necessary
- **Failure signatures:**
  - Perfect Scores (100%): A failure signature of data leakage or dataset simplicity. If a model hits 100% on validation, check if validation samples are near-duplicates of training samples
  - High Variance in Folds: LoRA Fold 1 started at 0% while Fold 4 was near perfect, indicating the model is highly sensitive to specific samples in the training batch
- **First 3 experiments:**
  1. Data Sanitation Check: Verify the "100% accuracy" claim by creating a hold-out set of truly unseen medical questions from different sources to confirm the model isn't just memorizing the Healthline structure
  2. LoRA Hyperparameter Sweep: Re-run the LoRA Roberta-large experiment with increasing Rank (R) values (e.g., 8, 16, 64) to determine if poor 78% result was due to constrained rank budget
  3. Error Analysis on Misclassified Labels: Isolate the ~0.13% of cases Roberta-base got wrong. Determine if they are ambiguous labels or edge cases, which defines the ceiling for the "Label Prediction" stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the 100% accuracy achieved by BERT Large Uncased indicate genuine generalization or overfitting to the specific Healthline-derived dataset?
- Basis in paper: The authors state on page 12 that the perfect scores "warrant cautious interpretation as they could indicate potential overfitting, especially if the evaluation dataset lacks diversity."
- Why unresolved: The paper evaluates the model using 5-fold cross-validation on a single dataset of limited diversity (6,800 samples), which does not guarantee performance on unseen, real-world clinical data.
- What evidence would resolve it: External validation results on independent, diverse medical datasets (e.g., MIMIC-III or proprietary hospital data) showing consistently high metrics.

### Open Question 2
- Question: Can the limitations of the label-mapping approach be overcome by integrating dynamic answer generation for complex medical inquiries?
- Basis in paper: The conclusion notes that the "reliance on predefined answers limits the models' ability to handle complex or nuanced medical queries" and suggests "incorporating dynamic answer generation" as future work.
- Why unresolved: The current study only evaluates a two-stage process (classification then predefined answer), leaving the efficacy of generative responses untested.
- What evidence would resolve it: A comparative study measuring the factual accuracy and clinical utility of dynamically generated answers versus the current static retrieval method.

### Open Question 3
- Question: What factors contributed to the significant performance degradation of LoRA Roberta-large (78.47%) compared to the fully fine-tuned Roberta-base (99.87%)?
- Basis in paper: The authors initially cite literature claiming LoRA enables efficient adaptation "without compromising performance," yet their results show a ~21% drop in accuracy. The paper does not analyze why LoRA failed to maintain performance in this specific context.
- Why unresolved: The authors present the results but do not investigate whether the drop was due to hyperparameter choices, rank selection, or the small dataset size relative to the parameter reduction.
- What evidence would resolve it: An ablation study varying LoRA hyperparameters (e.g., rank, alpha) and dataset sizes to identify the conditions under which LoRA achieves parity with full fine-tuning.

## Limitations

- The study's perfect scores (100%) raise concerns about potential overfitting to the specific Healthline dataset structure rather than genuine generalization to diverse medical questions
- The LoRA implementation's poor performance (78.47% accuracy) suggests either suboptimal hyperparameter configuration or that parameter-efficient approaches may be insufficient for specialized domain tasks on smaller datasets
- The classification-only approach creates a rigid system that cannot handle nuanced or cross-category medical questions, assuming questions can be cleanly mapped to discrete categories

## Confidence

- **High Confidence:** Classification models can achieve high performance on medical question classification when constrained to discrete labels from a finite set; the two-stage pipeline (classification + static answer retrieval) is clearly specified
- **Medium Confidence:** The comparative performance ranking of the four models (BERT Large > Roberta-base > BERT Uncased > LoRA Roberta-large) is supported by experimental results, though perfect scores warrant cautious interpretation
- **Low Confidence:** Claims about "significant improvement in medical question-answering capabilities" are overstated without external validation; the paper conflates classification accuracy with overall QA system quality

## Next Checks

1. **External Data Validation:** Test all four models on a held-out dataset of medical questions from completely different sources (e.g., PubMed, Mayo Clinic) to verify that high performance scores reflect genuine generalization rather than memorization of the Healthline dataset structure.

2. **LoRA Hyperparameter Optimization:** Systematically vary the LoRA rank parameter (e.g., testing ranks 8, 16, 32, 64) and alpha values to determine if the 78.47% performance floor can be raised, isolating whether poor results stem from insufficient model capacity or other factors.

3. **Error Mode Analysis:** Conduct detailed error analysis on the ~0.13% of cases where Roberta-base failed and the 21-24% failure rate of LoRA. Categorize these errors by question type (ambiguous labels, out-of-scope questions, complex reasoning requirements) to establish the true ceiling of the classification approach and identify scenarios where the system would fail in practice.