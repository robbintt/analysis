---
ver: rpa2
title: 'A Unified Definition of Hallucination, Or: It''s the World Model, Stupid'
arxiv_id: '2512.21577'
source_url: https://arxiv.org/abs/2512.21577
tags:
- hallucination
- world
- arxiv
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inconsistent definitions of
  "hallucination" across different domains (summarization, QA, RAG, agentic systems,
  VLMs), which hinders benchmark design and evaluation. The core idea is to unify
  these definitions by framing hallucination as "inaccurate (internal) world modeling
  observable to the user," defined relative to a reference world model W, view function
  V, and conflict policy P.
---

# A Unified Definition of Hallucination, Or: It's the World Model, Stupid

## Quick Facts
- arXiv ID: 2512.21577
- Source URL: https://arxiv.org/abs/2512.21577
- Reference count: 31
- Primary result: Hallucination unified as "inaccurate (internal) world modeling observable to the user" via reference world model W, view function V, and conflict policy P

## Executive Summary
This paper addresses the problem of inconsistent definitions of "hallucination" across different domains (summarization, QA, RAG, agentic systems, VLMs) by proposing a unified framework. The core insight is that hallucination can be formally defined as "inaccurate (internal) world modeling observable to the user," where the definition depends on three explicit components: a reference world model W encoding ground truth, a view function V specifying what the model observes from W, and a conflict policy P determining how contradictions resolve. By making these components explicit, the authors argue for clearer benchmarks and better understanding of what constitutes hallucination versus other errors like planning failures or incentive misalignment.

## Method Summary
The paper introduces a formal framework where hallucination is defined relative to (W, P) when there exists an atomic claim c in output y such that T_W,P(x, c) = false, where T_W,P is a truth function using the reference world model W and conflict policy P. The method demonstrates synthetic benchmark creation using environments like chess, where world states are fully specified programmatically, enabling scalable evaluation without human annotation. The framework decomposes outputs into atomic claims C(y) that can be individually verified against the truth function, allowing precise hallucination detection.

## Key Results
- Provides formal unification of diverse hallucination definitions under a single framework with components W, V, P
- Demonstrates synthetic environment approach using chess to create annotation-free benchmarks
- Distinguishes hallucination from planning errors and incentive errors in agentic settings
- Shows how explicit specification of (W, V, P) enables clearer benchmark design and evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse hallucination definitions across tasks can be unified under a single formal framework with three components.
- Mechanism: The framework decomposes hallucination into (1) a reference world model W encoding ground truth, (2) a view function V specifying what the model observes from W, and (3) a conflict policy P determining how contradictions resolve. Hallucination occurs when ∃c ∈ C(y) such that T_W,P(x, c) = false.
- Core assumption: All existing hallucination definitions implicitly assume some source of truth, observability regime, and conflict resolution rule—even when unstated.
- Evidence anchors: [abstract] "By varying the reference world model as well as the knowledge conflict policy... we arrive at the different existing definitions of hallucination present in the literature." [section 3, Definition 2] Formal definition: "y hallucinates with respect to (W, P) if and only if ∃c ∈ C(y) such that T_W,P(x, c) = false"

### Mechanism 2
- Claim: Distinguishing hallucination from planning errors and incentive errors enables targeted mitigation.
- Mechanism: The framework isolates hallucination as "incorrect beliefs about environment state" (world-modeling errors). Planning errors occur when beliefs are correct but action selection is poor. Incentive errors occur when the model knows it is uncertain but produces confident outputs due to training/reward pressure.
- Core assumption: Failure modes have distinct causal origins that require different interventions.
- Evidence anchors: [section 3.1.4] "In agentic settings we distinguish: Hallucination (incorrect belief about environment state), Planning (correct beliefs, incorrect action), Instruction-following (correct beliefs, goal ignored)."

### Mechanism 3
- Claim: Synthetic environments with fully specified world models enable scalable, annotation-free hallucination benchmarks.
- Mechanism: Environments like chess have programmatically accessible states (s), histories (h), and rules (R). Given (s, h), document and query generators produce textual artifacts and questions. The truth function T_W,P evaluates claims against the known world state automatically, eliminating human/LLM annotation.
- Core assumption: Synthetic environments capture relevant failure modes that transfer to real-world tasks.
- Evidence anchors: [section 5.1] "Whenever we can specify W_ref and a truth function T_W,P(x, c) that evaluates atomic claims c in context x, we can generate a large number of instances without additional human or model annotation required."

## Foundational Learning

- Concept: **Reference vs. internal world models**
  - Why needed here: The paper distinguishes W (the gold-standard reference) from the model's learned internal representation. Confusing the two undermines the entire framework.
  - Quick check question: Given a RAG system, what is W and what is the model's internal approximation?

- Concept: **Conflict resolution policies**
  - Why needed here: Many hallucination debates reduce to unstated disagreements about P—e.g., does retrieved context override parametric knowledge?
  - Quick check question: If a source document says "X is true" but world knowledge says "X is false," what should P specify, and how does your system implement it?

- Concept: **Atomic claim decomposition**
  - Why needed here: The truth function T_W,P operates on atomic claims C(y), not raw text. Decomposition quality directly affects hallucination detection precision.
  - Quick check question: How would you decompose "TechCorp exceeded analyst expectations with Q3 revenue of $2.1 billion" into atomic claims?

## Architecture Onboarding

- Component map:
  - **W (Reference World Model)**: Source of ground truth—can be documents, knowledge bases, environment states, or combinations.
  - **V (View Function)**: Controls observability—what subset of W the model receives at inference time.
  - **P (Conflict Policy)**: Resolution rules when sources disagree—e.g., "context overrides parameters."
  - **T_W,P (Truth Function)**: Maps (input, claim) → {true, false, unknown} using W and P.
  - **C(y) (Claim Extractor)**: Decomposes model output into atomic claims for evaluation.

- Critical path:
  1. Specify (W, V, P) explicitly before building or evaluating any system.
  2. Implement claim extraction C(y) appropriate to your output modality.
  3. Build T_W,P that can query W according to P.
  4. Instrument logging to trace which claims triggered hallucination flags.

- Design tradeoffs:
  - Richer W (e.g., full knowledge base) → more comprehensive evaluation but higher complexity.
  - Stricter P (context always overrides) → fewer hallucinations on paper but may suppress correct parametric knowledge.
  - Fine-grained C(y) → higher detection precision but more extraction errors.

- Failure signatures:
  - Unstated W: Benchmarks report hallucination rates without specifying what counts as true.
  - Implicit P: System behavior changes unpredictably when context and parametric knowledge conflict.
  - Coarse C(y): Whole-sentence labeling misses which specific claim caused the hallucination.

- First 3 experiments:
  1. **Explicit (W, V, P) audit**: Take an existing benchmark (e.g., summarization, factual QA). Document its implicit W, V, and P. Reproduce evaluation with these made explicit.
  2. **Conflict policy stress test**: Construct adversarial examples where retrieved context contradicts parametric knowledge. Measure hallucination rates under different explicit P settings.
  3. **Synthetic environment pilot**: Implement a minimal chess-based hallucination benchmark following Figure 2. Compare model performance on position-description consistency vs. move-quality tasks to separate world-modeling from planning errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can models be trained to reliably adhere to a specific conflict resolution policy P, or even infer the correct policy implicitly from context when sources contradict?
- **Basis in paper:** [explicit] The authors explicitly call for future work to "study whether models can be trained to follow a given policy reliably" and to "assess scenarios where the model may need to correctly infer or discover the conflict policy itself."
- **Why unresolved:** Current literature acknowledges knowledge conflicts but lacks a systematic understanding of how to train models to resolve them according to user-specified rules or implicit context.
- **What evidence would resolve it:** Benchmarks showing high performance in adversarial settings where models must reject corrupted sources or prioritize specific contexts based on a defined or inferred policy.

### Open Question 2
- **Question:** How can we design scalable benchmarks to evaluate hallucinations in non-stationary environments where the reference world model W evolves over time?
- **Basis in paper:** [explicit] The paper highlights the need for "hallucination benchmarks that can appropriately assess models in such scenarios with a significantly changing W," specifically referencing non-stationary interactions and temporal evolution.
- **Why unresolved:** Most existing benchmarks rely on static snapshots of the world, failing to test a model's ability to maintain an accurate world model as the environment or knowledge base changes.
- **What evidence would resolve it:** The creation of benchmark suites with dynamic states (e.g., evolving codebases or legal records) that specifically measure hallucination rates following state updates.

### Open Question 3
- **Question:** How can evaluation frameworks effectively decouple hallucination (incorrect beliefs) from planning or control errors in agentic settings?
- **Basis in paper:** [explicit] The authors request "interactive benchmarks where... the evaluation separately scores claim-level state consistency, action validity, and task success" to prevent "hallucination" from becoming a catch-all term for agent failure.
- **Why unresolved:** In agentic loops, it is currently difficult to distinguish whether a failure stems from a wrong belief about the environment (hallucination) or a poor plan based on correct beliefs.
- **What evidence would resolve it:** An agentic benchmarking environment that provides granular metrics distinguishing "observation hallucination" (e.g., claiming a non-existent DOM element) from "planning errors" (e.g., failing to click the correct button).

## Limitations
- Lacks empirical validation with quantitative results across diverse domains
- Framework coverage of existing definitions is argued theoretically rather than systematically analyzed
- Limited evidence that hallucination, planning, and incentive errors are practically separable in real systems

## Confidence
- **High confidence**: The formal framework structure (W, V, P components) and its logical consistency
- **Medium confidence**: The claim that existing hallucination definitions can be unified under this framework
- **Low confidence**: The practical utility of distinguishing hallucination from planning and incentive errors

## Next Checks
1. **Framework Coverage Audit**: Systematically map 10+ major hallucination benchmarks to their implicit (W, V, P) specifications and measure coverage
2. **Synthetic-to-Real Transfer Study**: Evaluate the same model on both synthetic chess benchmarks and real-world tasks to assess benchmark validity
3. **Failure Mode Isolation Experiment**: Create controlled test sets where world-modeling, planning, and incentive errors are systematically varied to measure differential intervention effectiveness