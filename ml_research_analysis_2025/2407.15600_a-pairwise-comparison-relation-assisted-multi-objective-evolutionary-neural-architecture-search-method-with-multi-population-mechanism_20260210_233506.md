---
ver: rpa2
title: A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural
  Architecture Search Method with Multi-population Mechanism
arxiv_id: '2407.15600'
source_url: https://arxiv.org/abs/2407.15600
tags:
- search
- architectures
- architecture
- neural
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SMEMNAS, a pairwise comparison relation-assisted
  multi-objective evolutionary neural architecture search method with a multi-population
  mechanism. The method addresses the computational bottleneck in NAS by using a surrogate
  model based on pairwise comparison relations to predict architecture rankings instead
  of absolute accuracy, and employs a multi-population mechanism to maintain diversity
  and accelerate convergence.
---

# A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism

## Quick Facts
- **arXiv ID:** 2407.15600
- **Source URL:** https://arxiv.org/abs/2407.15600
- **Reference count:** 40
- **Primary result:** Achieves 78.91% accuracy and 570M MAdds on ImageNet in 0.17 GPU days using a pairwise comparison surrogate and multi-population mechanism.

## Executive Summary
This paper introduces SMEMNAS, a multi-objective evolutionary neural architecture search method that addresses the computational bottleneck in NAS through a pairwise comparison surrogate model and a multi-population mechanism. The method predicts architecture rankings instead of absolute accuracy, enabling efficient search with fewer training samples. By combining a main (elite) and vice (diverse) population with a dynamic selection threshold, SMEMNAS maintains diversity while accelerating convergence. The approach achieves competitive results on CIFAR and ImageNet, finding architectures with high accuracy and low computational cost in significantly reduced search time.

## Method Summary
SMEMNAS employs a multi-objective evolutionary algorithm to optimize for both accuracy and MAdds. It uses a pre-trained Once-For-All supernet for weight sharing, reducing per-architecture training time. The core innovation is a surrogate model based on pairwise comparison relations: for each pair of architectures, an SVM classifier predicts which performs better. This ranking-based approach requires fewer samples than absolute accuracy prediction. A multi-population mechanism maintains two populations - a main population for convergence and a vice population for diversity - with a dynamic threshold controlling parent selection across generations. The search space consists of MobileNetV3-like architectures encoded as 46-bit strings, with 300 total evaluations across 25 generations.

## Key Results
- Achieves 78.91% top-1 accuracy and 570M MAdds on ImageNet in just 0.17 GPU days
- Outperforms most existing methods while significantly reducing search time compared to traditional NAS approaches
- Demonstrates competitive performance on CIFAR-10, CIFAR-100, and ImageNet across multiple objectives
- Ablation studies confirm the effectiveness of both the pairwise comparison surrogate and multi-population mechanism

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Comparison Surrogate Model
The surrogate model predicts architecture rankings rather than absolute accuracy, transforming the problem from regression to binary classification. For n evaluated architectures, it creates n(n-1)/2 training pairs by encoding concatenated architectures and training an SVM to predict which is better. This approach requires fewer training samples and is more efficient than predicting absolute accuracy. The pairwise predictions are aggregated into global rankings via tournament counting, providing sufficient information for evolutionary selection while reducing computational overhead.

### Mechanism 2: Multi-Population Mechanism
The method maintains two cooperating populations: a main population (E_t) of non-dominated architectures that guides the evolutionary process, and a vice population (F_t) of dominated architectures with high crowding distance that enhances search diversity. A dynamic threshold controls parent selection, favoring the main population in early/middle stages for convergence and the vice population in later stages for diversity. This staged approach prevents premature convergence while maintaining efficient search progress through different evolutionary phases.

### Mechanism 3: Weight-Sharing via Once-For-All Supernet
Weight-sharing from a pre-trained Once-For-All supernet dramatically reduces per-architecture training time. During search, sampled architectures inherit corresponding weights from the supernet as initialization, followed by short fine-tuning (5 epochs on CIFAR) before evaluation. This warm-start approach approximates full training well enough for relative ranking while making the evolutionary search computationally feasible within reasonable timeframes.

## Foundational Learning

**Multi-Objective Evolutionary Algorithms (MOEAs)**
- Why needed: The search simultaneously optimizes accuracy and MAdds, requiring Pareto front analysis and non-dominated sorting
- Quick check: Can you explain why non-dominated sorting alone is insufficient and crowding distance is needed?

**Surrogate-Assisted Optimization**
- Why needed: Replaces expensive full evaluations with cheap predictions, the core efficiency gain
- Quick check: Why might a surrogate that predicts rankings outperform one that predicts absolute accuracy with limited training data?

**Search Space Encoding**
- Why needed: Architectures are represented as 46-bit integer strings for genetic operations
- Quick check: How does the fixed-length encoding handle variable-depth architectures (e.g., fewer layers)?

## Architecture Onboarding

**Component map:**
Initialization -> Archive population -> Surrogate training -> MP-MOEA search loop (selection, crossover, mutation, surrogate evaluation) -> Environmental selection -> Repeat until convergence

**Critical path:**
1. Initialize 100 random architectures
2. Supernet fine-tuning and evaluation
3. Archive population with evaluated (architecture, accuracy) pairs
4. Surrogate training on pairwise comparisons
5. MP-MOEA search loop with dynamic threshold selection
6. Environmental selection and archive update
7. Repeat for 25 generations

**Design tradeoffs:**
- Initial population size (N): Larger N improves surrogate accuracy but increases upfront evaluation cost (Table IV shows N=100 balances HV and cost)
- Surrogate type: SVM vs. MLP vs. RF (Fig. 8 shows SVM performs best with limited samples)
- Dynamic threshold schedule: Tuning impacts diversity vs. convergence balance (Fig. 6 demonstrates three probability configurations)

**Failure signatures:**
- **Surrogate rank disorder:** Kendall's Tau drops below 0.6, causing noisy selection pressure
- **Diversity collapse:** Pareto front concentrates in narrow MAdds region, vice population underutilized
- **Stagnation:** Hypervolume does not improve, indicating poor genetic operators or conservative selection

**First 3 experiments:**
1. **Surrogate ablation:** Run search with SVM vs. MLP vs. no surrogate on CIFAR-10; compare search cost and final accuracy
2. **Multi-population ablation:** Run with vs. without MP mechanism on CIFAR-100; compare Pareto front diversity and convergence speed
3. **Threshold sensitivity:** Test static thresholds (0.3, 0.5, 0.7) vs. dynamic schedule on ImageNet subset; plot Pareto fronts

## Open Questions the Paper Calls Out
None

## Limitations
- Surrogate's long-term generalization beyond 25 generations is unclear; no ablation on archive size or retraining frequency
- ImageNet search evaluation protocol is underspecified, making the 0.17 GPU-day claim difficult to verify
- Multi-population mechanism performance heavily depends on dynamic threshold schedule, but sensitivity analysis is limited

## Confidence
- **High:** Pairwise comparison surrogate reduces sample complexity (supported by SVM ablation, Kendall's Tau metrics)
- **Medium:** Multi-population mechanism improves Pareto front diversity (visual evidence, limited ablation on threshold schedules)
- **Medium:** Overall competitive performance on ImageNet (results match claims, but evaluation details are sparse)

## Next Checks
1. Reproduce the surrogate ablation (SVM vs. MLP vs. no surrogate) on CIFAR-10 to confirm efficiency gains
2. Validate the multi-population ablation (with vs. without MP) on CIFAR-100 to measure diversity and convergence improvements
3. Conduct a controlled experiment varying the dynamic threshold schedule on ImageNet to assess sensitivity and robustness