---
ver: rpa2
title: Codifying Natural Langauge Tasks
arxiv_id: '2509.17455'
source_url: https://arxiv.org/abs/2509.17455
tags:
- code
- icrag
- tasks
- language
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether real-world natural language tasks\u2014\
  such as legal judgment and medical QA\u2014can be solved by converting them into\
  \ executable Python programs, rather than relying solely on black-box language model\
  \ outputs. The proposed ICRAG framework iteratively generates and refines code,\
  \ augmented by retrieval of external knowledge from domain sources and GitHub, to\
  \ ground solutions in explicit reasoning."
---

# Codifying Natural Langauge Tasks

## Quick Facts
- arXiv ID: 2509.17455
- Source URL: https://arxiv.org/abs/2509.17455
- Authors: Haoyang Chen; Kumiko Tanaka-Ishii
- Reference count: 40
- Primary result: ICRAG achieves up to 161.1% relative improvement over direct LLM prompting, with 32.7% average accuracy gain across 13 benchmarks.

## Executive Summary
This study investigates whether real-world natural language tasks—such as legal judgment and medical QA—can be solved by converting them into executable Python programs, rather than relying solely on black-box language model outputs. The proposed ICRAG framework iteratively generates and refines code, augmented by retrieval of external knowledge from domain sources and GitHub, to ground solutions in explicit reasoning. Across 13 diverse benchmarks, ICRAG achieved up to 161.1% relative improvement over direct LLM prompting, with an average accuracy gain of 32.7%. Code analysis revealed higher complexity and unique patterns in programs derived from language tasks, highlighting their distinct nature compared to general code. The results demonstrate that codification, combined with retrieval and iteration, can effectively solve complex reasoning tasks while producing transparent, verifiable solutions.

## Method Summary
The ICRAG framework converts natural language tasks into executable Python programs using iterative code generation and refinement. It starts with an initial code generation from the input question, then retrieves relevant context from two knowledge sources: R1 (domain-specific documents like statutes and textbooks) and R2 (similar solved examples from the same benchmark via k-fold cross-validation). The system iteratively refines the code based on retrieved information until the query becomes empty or a maximum iteration count is reached. The final executable code is then run to obtain the answer. The method uses GPT-4o-Mini with temperature 0.2 for all LLM interactions.

## Key Results
- ICRAG achieved up to 161.1% relative improvement over direct LLM prompting across 13 benchmarks
- Average accuracy gain of 32.7% compared to baseline direct prompting
- 96% of test queries resolved within at most three retrieval iterations
- Code complexity analysis showed higher AST metrics (stack depth, cyclomatic complexity) for language tasks compared to general code

## Why This Works (Mechanism)

### Mechanism 1: Explicit Reasoning through Codification
- **Claim:** Converting natural language tasks to executable Python programs makes reasoning steps transparent and verifiable, overcoming black-box limitations of direct LLM outputs.
- **Mechanism:** The LLM generates code that encodes reasoning as executable statements (e.g., if-then-else logic for legal rules, arithmetic for math). A Python interpreter executes the code; when undefined functions or non-executable statements arise, exceptions trigger LLM simulation or code repair.
- **Core assumption:** Natural language tasks—even non-mathematical ones like legal judgment or medical QA—can be algorithmically specified as executable program logic.
- **Evidence anchors:**
  - [abstract]: "leverages the explicit reasoning provided by program generation"
  - [section 1]: "the code explicitly describes the reasoning to obtain a task's solution"
  - [corpus]: Weak corpus evidence—related papers focus on code generation but not specifically on codifying reasoning tasks.
- **Break condition:** Tasks that cannot be algorithmically specified (e.g., purely subjective judgments, physical-world interactions, or open-ended creative generation) will not yield executable solutions.

### Mechanism 2: Iterative Knowledge-Grounded Refinement
- **Claim:** Multi-pass refinement with targeted retrieval yields higher accuracy than single-pass generation, especially for tasks requiring external domain knowledge.
- **Mechanism:** Starting from initial code c₀, the system iteratively: (1) identifies missing knowledge via code analysis, (2) generates targeted query qₙ, (3) retrieves relevant context rₙ from R1+R2, (4) refines code cₙ→cₙ₊₁. Loop terminates when qₙ = ∅ or max iterations reached. Across datasets, 96% of queries resolve within ≤3 iterations.
- **Core assumption:** Knowledge gaps can be detected through code inspection and filled via retrieval; the LLM can translate retrieved text/code into program logic.
- **Evidence anchors:**
  - [abstract]: "iteratively generates and refines code, augmented by retrieval of external knowledge"
  - [section 5.2]: "96% of all test queries, the questions were answered within at most three retrieval iterations"
  - [corpus]: IRCoT (iterative RAG with CoT) shows similar benefits for text-based multi-step QA, supporting iterative retrieval patterns.
- **Break condition:** When the LLM cannot articulate what knowledge is missing, or retrieval corpus lacks relevant information, iteration stalls without improvement.

### Mechanism 3: Dual Knowledge Source Integration
- **Claim:** Combining domain-specific knowledge bases (R1: statutes, textbooks, articles) with similar solved examples (R2: code snippets from same benchmark via k-fold CV) provides complementary coverage.
- **Mechanism:** R1 supplies domain facts and rules; R2 provides reusable code patterns. Retrieved content includes both prose and executable snippets, enabling the LLM to ground logic in facts while borrowing structural patterns.
- **Core assumption:** Problems within a domain share reusable reasoning structures; k-fold CV prevents leakage while exposing similar examples.
- **Evidence anchors:**
  - [section 3.2]: "R1 is a corpus typically associated with each dataset... R2 is a collection of similar solved examples drawn from the dataset itself"
  - [section 5.4, Figure 3]: Accuracy increases with in-domain pool size, plateauing near 100%; GitHub-only code underperforms in-domain retrieval.
  - [corpus]: Survey on RAG confirms external knowledge integration mitigates hallucination and improves factual grounding.
- **Break condition:** Highly specialized domains with sparse prior examples yield weak R2; if R1 lacks coverage, retrieval cannot fill gaps regardless of iteration count.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** ICRAG's core improvement comes from RAG integration. Understanding vector indexing (FAISS), embeddings (all-MiniLM-L6-v2), and k-NN retrieval is essential for debugging why retrieval fails or succeeds on specific queries.
  - **Quick check question:** Given the query "false-name in FDCPA," how would a FAISS index rank documents? What embedding dimension is used?

- **Concept: Abstract Syntax Trees (ASTs)**
  - **Why needed here:** The paper analyzes code complexity via AST metrics (stack depth, cyclomatic complexity, node coverage). Understanding ASTs enables interpreting why certain domains (Corr2Cause, ProofNet) produce structurally distinct code.
  - **Quick check question:** For a function with 5 nested if-else branches, what is its cyclomatic complexity? How does it differ from a recursive function with depth 5?

- **Concept: k-Fold Cross Validation for Retrieval Construction**
  - **Why needed here:** ICRAG constructs R2 from held-out folds to prevent data leakage. Understanding this prevents misdiagnosing memorization as genuine reasoning.
  - **Quick check question:** In 5-fold CV, if fold 1 is the test set, which folds populate R2? How does this differ from leave-one-out retrieval?

## Architecture Onboarding

- **Component map:** Input processor -> query builder -> Dual retrieval (R1: domain KB, R2: similar examples via k-fold) -> Code generator (LLM) -> Code executor -> Query generator -> Termination checker -> Output parser

- **Critical path:**
  1. Receive question t
  2. Generate c₀ (Prompt A.2(i))
  3. Retrieve r₀ from R1+R2
  4. Iterate: (cₙ, qₙ) → retrieve rₙ → refine → cₙ₊₁; check qₙ
  5. Execute c*; return answer
  6. Most tasks converge in 1–3 iterations; legal/medical occasionally require N≤3

- **Design tradeoffs:**
  - Pool size: Accuracy rises with R1+R2 size until ~75–100%, then plateaus
  - GitHub code addition: Helps at moderate ratios (r≈0.6) but dilutes domain signals at high ratios
  - Iteration budget: Diminishing returns after N=3
  - Code vs. text retrieval: RAGCode alone underperforms RAGNL on some tasks (e.g., ECHR); both modalities needed
  - Generalizability: Algorithmic tasks (GSM8K, BBH-arithmetic) generalize; instance-specific tasks (legal cases) do not

- **Failure signatures:**
  - High exception rates (Legal: 6.53%, BBH: 10.36%, BBEH: 12.72%) → knowledge gaps or complex logic
  - Executability <95% → fallback to LLM simulation frequently triggered
  - GitHub-only retrieval → accuracy below in-domain ICRAG but above baseline
  - Non-convergence → not observed empirically, but theoretically possible
  - Low generalization correctness → code overfits to original instance

- **First 3 experiments:**
  1. **Baseline replication on CAIL:** Run ICRAG vs. Direct vs. CoT vs. CoC vs. IRCoT; verify ~35.6% gain; log N, exception rates, and retrieved snippet types.
  2. **Ablation on Health-Claim:** Test ICRAG with R1-only, R2-only, and both; plot accuracy vs. pool size; confirm Figure 3 plateau pattern.
  3. **AST complexity profiling:** Generate 50 programs each from GSM8K, Corr2Cause, ProofNet; compute stack depth and cyclomatic complexity; compare to reported means (e.g., Corr2Cause: cc_avg=6.75, ProofNet: depth_avg=3.41).

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on the assumption that natural language reasoning tasks can be algorithmically encoded as Python programs, which breaks down for subjective judgments, physical-world interactions, or open-ended creative generation.
- Performance depends heavily on the quality and coverage of external knowledge bases (R1) and similar example repositories (R2), which may be sparse for highly specialized domains.
- The fallback mechanism for handling non-executable code—LLM simulation—is not fully specified in the appendices, creating potential reproducibility challenges.

## Confidence
- **High Confidence:** The 32.7% average accuracy improvement over direct LLM prompting is well-supported by the 13 benchmark evaluations and clear performance differentials across tasks.
- **Medium Confidence:** The mechanism explaining why iterative retrieval with dual knowledge sources works (complementary coverage hypothesis) is plausible but could benefit from deeper analysis of which query types specifically require R1 vs. R2.
- **Medium Confidence:** The claim that ICRAG produces more complex code with distinct structural patterns (higher AST metrics) is supported by empirical measurements but lacks comparative analysis against alternative code generation approaches.

## Next Checks
1. **Reproduce CAIL Benchmark Results:** Implement ICRAG against the Chinese Criminal Judgment Analysis dataset and verify the reported ~35.6% improvement over baselines, logging iteration counts and exception rates.
2. **Ablation Study on Knowledge Sources:** Test ICRAG with R1-only, R2-only, and both sources on Health-Claim dataset, measuring accuracy across varying pool sizes to confirm the plateau pattern around 75-100%.
3. **AST Complexity Validation:** Generate 50 programs each from GSM8K, Corr2Cause, and ProofNet datasets, computing stack depth and cyclomatic complexity to verify reported structural differences (e.g., Corr2Cause: cc_avg=6.75, ProofNet: depth_avg=3.41).