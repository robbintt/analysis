---
ver: rpa2
title: 'NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation
  Environment'
arxiv_id: '2510.04368'
source_url: https://arxiv.org/abs/2510.04368
tags:
- agents
- agent
- prompt
- negotiation
- buyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NegotiationGym is a configurable multi-agent simulation framework
  for modeling negotiation and cooperation scenarios. The system enables agents to
  self-optimize by iteratively interacting, observing outcomes, and refining strategies.
---

# NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment

## Quick Facts
- **arXiv ID**: 2510.04368
- **Source URL**: https://arxiv.org/abs/2510.04368
- **Reference count**: 7
- **One-line result**: Self-optimizing agents in multi-agent simulations improve negotiation performance through iterative feedback and strategy refinement.

## Executive Summary
NegotiationGym is a configurable multi-agent simulation framework for modeling negotiation and cooperation scenarios. The system enables agents to self-optimize by iteratively interacting, observing outcomes, and refining strategies through a dedicated negotiation coach. Agents are defined with utility functions and can be coached using a dedicated negotiation coach that analyzes past transcripts and suggests improved strategies. In a buyer-seller laptop negotiation case study with 20 simulation runs, the framework demonstrated measurable improvements in agent performance. When both agents were optimized, cumulative average utilities increased and deal failures decreased compared to unoptimized runs. The system provides a user-friendly API and GUI for configuring scenarios, running simulations, and analyzing outcomes, enabling research on multi-agent social dynamics and autonomous strategy improvement.

## Method Summary
NegotiationGym uses AutoGen's SelectorGroupChat to manage multi-turn negotiations between agents. Agents are initialized with JSON configurations defining their roles, utility functions, and self-improvement settings. The simulation runs episodes where agents negotiate until termination conditions are met. After each episode, agents with self-improvement enabled invoke a negotiation coach that analyzes the interaction history and rewrites the agent's system prompt to maximize future utility. The framework tracks cumulative utilities and deal rates across multiple runs to evaluate performance improvements. The core optimization loop involves reflecting on past interactions, extracting strategy insights, and encoding these into the agent's prompt for the next episode.

## Key Results
- When both agents were optimized, cumulative average utilities increased and deal failures decreased compared to unoptimized runs.
- The framework demonstrated measurable improvements in agent performance through self-optimization.
- Buyer agents gained more utility from optimization than seller agents due to greater strategic flexibility.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agents improve through gradient-free prompt optimization where scalar utility scores guide prompt rewriting rather than model weight updates.
- **Mechanism:** The `UtilityAgent` class invokes an `optimize` function that concatenates interaction history into a "reflection prompt." An LLM acting as a coach analyzes this history against the utility outcome and rewrites the agent's system prompt to maximize future utility.
- **Core assumption:** The underlying LLM possesses sufficient reasoning capability to correlate past transcript features with utility deltas and encode successful behaviors into natural language instructions.
- **Evidence anchors:**
  - [abstract]: "agents can self-optimize by... observing outcomes, and modifying their strategies for future rounds."
  - [section 3]: "[the default implementation] asks the back-end LLM to rewrite the current system_prompt so as to increase the measured utility... and replaces the old prompt in place."
  - [corpus]: Weak/missing. While neighbors like *IndoorWorld* discuss multi-agent environments, they do not explicitly validate this specific prompt-rewriting optimization loop.
- **Break condition:** If the reflection prompt exceeds the LLM's context window or if the utility signal is too noisy (e.g., stochastic opponent behavior), the strategy refinement may fail to converge.

### Mechanism 2
- **Claim:** Bilateral optimization (both agents learning) may reduce deal failures as an emergent by-product, even without explicit instructions to close deals faster.
- **Mechanism:** When both agents refine their strategies, they appear to develop more efficient signaling and concession patterns, finding the "Zone of Possible Agreement" (ZOPA) quicker before hitting turn limits.
- **Core assumption:** There is a valid deal to be made (overlap in private constraints); if budgets and floor prices do not overlap, optimization cannot force a deal.
- **Evidence anchors:**
  - [section 4]: "The both-reflect mode, where both agents are optimized, has the fewest no-deals... agents learn to close deals fast and minimize net surplus loss as an emergent by-product."
  - [abstract]: "When both agents were optimized, cumulative average utilities increased and deal failures decreased compared to unoptimized runs."
  - [corpus]: Weak/missing. Neighbors focus on simulation frameworks (e.g., *SOTOPIA-S4*) but do not provide evidence for this specific emergent efficiency.
- **Break condition:** If the "coach" inadvertently encourages aggressive tactics (e.g., "never concede"), bilateral optimization could increase deadlocks rather than efficiency.

### Mechanism 3
- **Claim:** Optimization efficacy seems to vary by role flexibility, with buyers (who have wider action spaces) potentially gaining more utility from feedback than sellers anchored by hard constraints.
- **Mechanism:** The seller is constrained by a fixed floor price, limiting strategic maneuverability. The buyer has a budget surplus to leverage and can vary timing and anchoring tactics, providing a richer space for prompt optimization to exploit.
- **Core assumption:** The utility functions accurately capture the asymmetry of the negotiation (e.g., surplus is split relative to the asking price).
- **Evidence anchors:**
  - [section 4]: "We observe the highest cumulative utility for the buyer... One explanation is that the buyer's negotiation position is inherently more flexible."
  - [figure 1]: Shows steeper improvement curves for the "buyer reflect" mode compared to "seller reflect."
  - [corpus]: Weak/missing. The provided neighbors do not analyze buyer-seller asymmetry in LLM optimization.
- **Break condition:** If the seller's strategy space is expanded (e.g., adding value-adds like warranties), this asymmetry might diminish or invert.

## Foundational Learning

- **Concept: AutoGen SelectorGroupChat**
  - **Why needed here:** NegotiationGym is built on AutoGen. The `SelectorGroupChat` manages the shared history $H$ and determines which agent speaks next. Understanding this is required to customize turn-taking or debug conversation flow.
  - **Quick check question:** Does the simulation proceed in a strict round-robin order, or can the `Selector` dynamically choose the next speaker based on context?

- **Concept: Scalar Utility Functions**
  - **Why needed here:** The system optimizes agents by maximizing a single scalar value returned by `compute_utility(E)`. You must understand how to encode complex goals (price, time, relationship) into this single metric.
  - **Quick check question:** If a deal is reached at a great price but takes 100 turns, how would you modify the utility function to penalize inefficiency?

- **Concept: In-Context Strategy Learning**
  - **Why needed here:** Agents "learn" by appending new strategy sentences to their system prompts. This is distinct from fine-tuning; it relies on the LLM's ability to follow newly injected instructions.
  - **Quick check question:** Where is the newly generated negotiation strategy stored for the next episodeâ€”in the model weights or the prompt text?

## Architecture Onboarding

- **Component map:** Orchestrator -> Controller -> Agents -> Environment -> Interface
- **Critical path:**
  1. Load JSON config $\rightarrow$ Map to Python classes.
  2. Run Episode: Agents interact via `SelectorGroupChat` until `STOP_NEGOTIATION` or turn limit.
  3. Evaluate: `compute_utility(E)` calculates scores based on private constraints (budget/floor).
  4. Optimize: If enabled, `learn_from_feedback(E)` calls the Coach/Prompt Engineer to rewrite the agent's system prompt.
  5. Repeat: Next episode starts with the updated prompt.

- **Design tradeoffs:**
  - *Coaching vs. Direct Reflection:* Using a dedicated "Negotiation Coach" agent adds cost/latency but may provide higher-quality strategy extraction than self-reflection (which Huang et al. [2023] note can degrade performance).
  - *Simplicity vs. Realism:* The framework uses simple price-based utilities; real-world scenarios with multi-factor utilities require custom `UtilityAgent` subclasses.
  - *Reproducibility:* Stochastic model outputs require averaging over many runs ($N=20$ in the paper) to stabilize conclusions.

- **Failure signatures:**
  - *Infinite Loops:* Agents repeating the same offer without terminating; requires tuning termination conditions.
  - *Prompt Drift:* The "Coach" might suggest strategies that violate the agent's core persona or constraints over successive runs.
  - *No-Deal Traps:* Over-optimized agents may become too stubborn, causing deal rates to plummet (visible in the 10-turn limit scenario).

- **First 3 experiments:**
  1. **Baseline Verification:** Run the laptop negotiation in "no-reflect" mode for 20 runs to establish the default deal rate and utility split.
  2. **Coaching Ablation:** Enable "buyer-reflect" only. Verify if the buyer's cumulative average utility increases as shown in Figure 1.
  3. **Stress Test:** Modify the `termination_condition` to a low turn count (e.g., 10) to observe if "both-reflect" agents successfully minimize no-deals compared to "no-reflect" agents.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does buyer optimization yield greater utility gains than seller optimization under identical coaching mechanisms, and what role-aware learning strategies could address this asymmetry?
- **Basis in paper:** [explicit] The authors observe that "the buyer gains more from feedback-driven optimization than the seller" and state this "motivates the need for role-aware learning strategies in future work."
- **Why unresolved:** The paper offers hypotheses (buyer flexibility, anchoring advantages) but does not test them experimentally.
- **What evidence would resolve it:** Controlled experiments varying agent constraints and strategy spaces independently of role labels.

### Open Question 2
- **Question:** How does self-optimization performance vary across different LLM backends beyond GPT-4o?
- **Basis in paper:** [explicit] The authors state "we did not investigate other models beyond GPT-4o, but we expect significant differences between models."
- **Why unresolved:** No experiments with alternative models were conducted; the framework's optimization loop may interact differently with models' instruction-following tendencies.
- **What evidence would resolve it:** Benchmarking the same negotiation scenarios across multiple LLM families (e.g., Claude, Llama, Gemini) with identical prompts.

### Open Question 3
- **Question:** How does agent behavior change when utility functions incorporate multifaceted, non-price factors such as relationship maintenance or time constraints?
- **Basis in paper:** [explicit] The authors note "we use a simple price-based utility in our case study, but real-world utilities may be multifaceted, encompassing many other factors."
- **Why unresolved:** The case study isolates a single scalar utility; complex objectives may produce different optimization dynamics and emergent behaviors.
- **What evidence would resolve it:** Experiments with composite utility functions combining price, relationship scores, and temporal penalties.

### Open Question 4
- **Question:** Can tool-use and external data access improve negotiation grounding and reduce economically suboptimal behaviors?
- **Basis in paper:** [explicit] The authors state "current simulations lack real-world grounding, and future extensions such as tool-use would enable agents to query external sources."
- **Why unresolved:** Agents currently operate solely on priors encoded in prompts; unknown whether external data changes strategy or reduces risks like budget violations.
- **What evidence would resolve it:** Integrating tool-calling for market price lookup and measuring utility shifts and constraint violations.

## Limitations
- The paper relies on prompt-based optimization rather than model weight updates, which may have limited scalability and robustness compared to fine-tuning approaches.
- Asymmetric results (buyers gaining more than sellers) are attributed to role flexibility, but the exact mechanism is not experimentally isolated.
- The coach's feedback quality is not directly evaluated - we don't know if the suggested strategies actually improve performance versus random or no coaching.

## Confidence
- **High confidence**: The framework's architecture and simulation methodology are well-specified and reproducible. The claim that bilateral optimization reduces deal failures is supported by the presented data.
- **Medium confidence**: The mechanism of prompt-based optimization working through reflection prompts is plausible given AutoGen's design, but the specific effectiveness for negotiation strategy refinement needs validation.
- **Low confidence**: The explanation for asymmetric utility gains (buyer flexibility) is speculative without controlled experiments varying role constraints.

## Next Checks
1. **Coach Quality Validation**: Run a controlled experiment comparing outcomes when the coach suggests strategies versus when it suggests random sentences, to verify the coach adds value beyond mere reflection.
2. **Role Constraint Manipulation**: Modify the seller to have a variable floor price (like the buyer's budget) and test if the asymmetric advantage disappears, confirming the flexibility hypothesis.
3. **Prompt Drift Detection**: Log the system prompts across optimization episodes to check if strategies drift from the agent's core persona or become repetitive, potentially causing performance degradation.