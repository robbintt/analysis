---
ver: rpa2
title: 'Language Models Might Not Understand You: Evaluating Theory of Mind via Story
  Prompting'
arxiv_id: '2506.19089'
source_url: https://arxiv.org/abs/2506.19089
tags:
- room
- story
- characters
- stories
- enters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StorySim, a synthetic benchmark for evaluating
  theory of mind (ToM) in large language models (LLMs) through controllable story
  generation. Unlike prior benchmarks vulnerable to data contamination, StorySim generates
  novel stories from a high-level storyboard specification, enabling precise manipulation
  of character perspectives and events.
---

# Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting

## Quick Facts
- arXiv ID: 2506.19089
- Source URL: https://arxiv.org/abs/2506.19089
- Reference count: 40
- Primary result: LLMs perform significantly worse on Theory of Mind tasks compared to World Modeling tasks, especially when reasoning about human agents

## Executive Summary
This paper introduces StorySim, a synthetic benchmark for evaluating theory of mind (ToM) in large language models (LLMs) through controllable story generation. Unlike prior benchmarks vulnerable to data contamination, StorySim generates novel stories from a high-level storyboard specification, enabling precise manipulation of character perspectives and events. The authors evaluate multiple state-of-the-art LLMs on first- and second-order ToM tasks alongside world modeling (WM) tasks that control for general reasoning ability.

Results show that most models perform better on WM tasks than ToM tasks, and accuracy drops significantly when reasoning about human agents versus inanimate objects. Error analysis reveals models often rely on heuristics like recency bias and first-common-location memorization rather than genuine ToM reasoning. While larger models generally perform better, even the best models struggle with second-order ToM, indicating fundamental limitations in their ability to model others' mental states.

## Method Summary
StorySim generates synthetic stories from a STORYBOARD specification (C, ϕ, G, E, n) where C is 8 random character names, ϕ defines 6 connected locations with 3 outgoing edges per vertex, G is the graph structure, E specifies events, and n=100 is story length. Stories involve characters moving between locations with key events marking when characters cross paths and when target T moves unbeknownst to S1. The benchmark evaluates first-order ToM (tracking S1's belief about T's location), second-order ToM (adding S2 who later meets T), and world modeling tasks tracking human movements and inanimate objects. Models are tested zero-shot and 3-shot across 100 stories per condition with varying mislead distances.

## Key Results
- Models perform significantly worse on ToM tasks (64% first-order, 57% second-order) compared to WM tasks (85% human, 93% inanimate)
- Accuracy drops when reasoning about human agents (85%) versus inanimate objects (93%)
- Larger models like GPT-4 and DeepSeek-R1 outperform smaller models, but even best performers struggle with second-order ToM
- Error analysis shows models rely on heuristics (first common location, recency bias) rather than genuine ToM reasoning

## Why This Works (Mechanism)
The StorySim framework works by generating synthetic stories with controlled parameters that isolate specific aspects of theory of mind reasoning. By manipulating character movements and perspective-taking requirements while keeping other variables constant, the benchmark can precisely measure when and how models fail at tracking others' mental states. The synthetic generation prevents data contamination while enabling systematic variation of difficulty through mislead distance and task complexity.

## Foundational Learning
**Theory of Mind (ToM)**: The ability to attribute mental states like beliefs, intents, and knowledge to oneself and others. Needed to understand what models are being tested for; quick check: Can the model distinguish between what a character knows versus what actually happened?

**First-Order vs Second-Order ToM**: First-order involves tracking one agent's beliefs about another; second-order involves tracking beliefs about beliefs. Needed to understand task complexity; quick check: Does the model understand that S2 needs to reason about S1's false belief about T?

**False Belief Task**: Classic ToM test where an agent holds incorrect beliefs about reality. Needed to frame the benchmark's evaluation criteria; quick check: Can the model identify when a character's belief contradicts actual events?

**World Modeling (WM)**: Tracking physical state changes without mental state attribution. Needed as control condition; quick check: Can the model track location changes without needing to understand character perspectives?

**Synthetic Data Generation**: Creating novel data programmatically rather than sampling from existing corpora. Needed to prevent data contamination; quick check: Are generated stories sufficiently different from pretraining data?

## Architecture Onboarding

**Component Map**: STORYBOARD -> Story Generator -> Story Collection -> LLM Evaluation -> Accuracy Metrics -> Error Analysis

**Critical Path**: The evaluation pipeline flows from storyboard specification through synthetic story generation to model inference and analysis. The critical dependency is the storyboard definition, which determines all subsequent story characteristics and evaluation conditions.

**Design Tradeoffs**: Synthetic generation enables controlled experiments and prevents contamination but may lack ecological validity compared to naturalistic stories. The choice of 8 characters and 6 locations balances complexity with computational feasibility. Zero-shot evaluation tests raw capabilities while 3-shot provides insight into in-context learning.

**Failure Signatures**: Models exhibiting first common location heuristic output the first shared location between S1 and T rather than tracking S1's mental state. Recency bias manifests as correct answers only when ToM-relevant events are recent. Context length errors occur when reasoning chains exceed model capacity, particularly for smaller models at low mislead distances.

**First Experiments**: 
1. Generate 10 test stories with known correct answers to verify story generation pipeline
2. Test a simple baseline model (e.g., pattern matching) to establish floor performance
3. Evaluate one LLM with the primary prompt to verify evaluation pipeline functionality

## Open Questions the Paper Calls Out
**Open Question 1**: Do LLMs possess a sound representation of goal-directedness, or do they rely on heuristics? The current study restricted its scope to false-belief tasks (tracking locations) rather than character intentions or goals.

**Open Question 2**: How does ToM performance change when stories involve simultaneous or periodic actions rather than sequential events? The StorySim framework currently generates stories where only one character performs one action at a time.

**Open Question 3**: Do ToM limitations identified in synthetic benchmarks reliably predict failures in socially-sensitive real-world applications? It is unclear if the "First Common Location" heuristic or recency bias identified translates to errors in domains like healthcare or resume filtering.

## Limitations
- Synthetic generation, while preventing contamination, may lack ecological validity compared to naturalistic stories
- The benchmark cannot definitively rule out data contamination given use of common names and spatial reasoning patterns
- Error analysis relies on post-hoc interpretation of failure modes rather than systematic validation of heuristic use

## Confidence
**High Confidence**: Models perform significantly worse on ToM tasks compared to WM tasks, with human agents showing lower accuracy than inanimate objects. This pattern is consistent across multiple models and conditions.

**Medium Confidence**: Models rely on heuristics rather than genuine ToM reasoning. While error patterns strongly suggest heuristic use, alternative explanations cannot be fully excluded.

**Low Confidence**: StorySim is completely free from data contamination. The synthetic generation process reduces but does not eliminate contamination risk.

## Next Checks
1. Evaluate models on StorySim stories with deliberately unusual character names and location descriptions not found in standard pretraining corpora, then compare performance drops to baseline results.

2. Systematically vary story parameters to break the first-common-location heuristic and observe whether model performance changes as predicted.

3. Test whether models that perform well on StorySim ToM tasks also show improved performance on established ToM benchmarks like Unexpected Transfer or Smarties tasks, establishing external validity of the synthetic benchmark.