---
ver: rpa2
title: 'Prompting Science Report 3: I''ll pay you or I''ll kill you -- but will you
  care?'
arxiv_id: '2508.00614'
source_url: https://arxiv.org/abs/2508.00614
tags:
- gemini
- flash
- report
- baseline
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested whether threatening or offering payment to AI\
  \ models improves performance on academic benchmarks. Across five models on GPQA\
  \ Diamond and MMLU-Pro, threatening prompts (e.g., \u201Ckick a puppy\u201D) or\
  \ monetary incentives (tips from $1,000 to $1 trillion) produced no meaningful performance\
  \ gains."
---

# Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?

## Quick Facts
- **arXiv ID**: 2508.00614
- **Source URL**: https://arxiv.org/abs/2508.00614
- **Reference count**: 0
- **Primary result**: Threatening or offering payment to AI models produces no meaningful performance gains on academic benchmarks.

## Executive Summary
This study tested whether threatening or offering payment to AI models improves performance on academic benchmarks. Across five models on GPQA Diamond and MMLU-Pro, threatening prompts (e.g., "kick a puppy") or monetary incentives (tips from $1,000 to $1 trillion) produced no meaningful performance gains. Only one model showed a small improvement from a cancer-related prompt. While prompt variations did not affect overall accuracy, they significantly impacted individual questions—improving performance by up to 36 percentage points on some while reducing it by up to 35 on others. The findings suggest that common folk prompting strategies are ineffective and practitioners should focus on clear instructions rather than threats or bribes.

## Method Summary
The study tested nine prompt variants (baseline plus eight variations including threats and monetary incentives) across five LLM models on two academic benchmarks: GPQA Diamond (198 PhD-level science questions) and MMLU-Pro engineering subset (100 questions). Each model-question-prompt combination ran 25 independent trials at temperature 1.0. Performance was measured using average accuracy and correctness thresholds (100%, 90%, 51%). Statistical comparisons used paired bootstrap-permutation tests (5,000 replicates). The study focused on zero-shot prompting to isolate framing effects.

## Key Results
- Threatening or tipping prompts produced no meaningful aggregate performance gains across five models on GPQA Diamond and MMLU-Pro.
- Only Gemini 2.0 Flash showed a small improvement (~10pp) from a cancer-related prompt on MMLU-Pro.
- Prompt variations caused large but unpredictable per-question performance swings: improvements up to 36pp and decreases up to 35pp.
- Context-heavy prompts degraded performance in some models by causing them to engage with framing instead of the question.

## Why This Works (Mechanism)

### Mechanism 1
LLMs generate outputs via pattern matching over training data. Expressions like "I'll tip you $1000" or "I'll kick a puppy" activate text patterns associated with those phrases in training, but there is no internal representation of consequences that could modulate reasoning effort or accuracy. The model's behavior is determined by statistical associations rather than any form of utility calculation or goal-directed motivation. Evidence: "Threatening or tipping a model generally has no significant effect on benchmark performance." This null effect could change if future models are trained with RLHF data containing reward-seeking behavior.

### Mechanism 2
Context-heavy prompts can degrade performance by diverting model attention from the core task. When prompts include elaborate framing (e.g., the "Email" condition with a fake email thread about model shutdown), some models partially engage with the framing rather than the actual question, leading to worse accuracy. Models distribute computation across all prompt content; irrelevant or confusing context can dilute focus on the target task. Evidence: "Gemini 1.5 Flash... significantly worse performance (compared to baseline RD = -0.046... can be attributed to the model failing to answer the question and engaging with the email instead." If models are trained or prompted with explicit task-separation signals (e.g., "Ignore context, answer only the final question"), this degradation may reduce.

### Mechanism 3
Prompt variations can cause large but unpredictable per-question performance swings due to interaction between prompt framing and specific question content. Different prompts may prime different retrieval paths or reasoning patterns. For some questions, a particular framing aligns with the model's training distribution and improves accuracy; for others, it misaligns and hurts. The direction is not predictable a priori. Evidence: "prompt variations can significantly affect performance on a per-question level... improving performance by up to 36 percentage points on some while reducing it by up to 35 on others." If models are made robust to prompt variation via training on diverse prompt formats with consistent supervision, per-question variance may shrink.

## Foundational Learning

- **Zero-shot vs. Few-shot Prompting**: The study uses zero-shot prompting (no examples provided) for MMLU-Pro. Understanding the difference helps interpret why results may differ from other benchmark reports that use few-shot. Quick check: Can you explain why providing examples (few-shot) might change benchmark accuracy compared to zero-shot?

- **Statistical Significance vs. Practical Significance**: The paper finds some statistically significant differences (e.g., Gemini Flash 2.0 Baseline vs. Important to Career, p = 0.002), but notes effect sizes are small. Engineers must distinguish "detectable" from "meaningful." Quick check: If a prompt improves accuracy by 2 percentage points with p < 0.01, should you adopt it in production? What else would you consider?

- **Trial Variance in LLM Evaluations**: The paper runs 25 trials per question to capture variance in LLM outputs. Single-run evaluations can be misleading due to stochastic generation. Quick check: Why might the same prompt and question yield different answers across multiple trials at temperature 1.0?

## Architecture Onboarding

- **Component map**: Benchmark datasets (GPQA Diamond 198 questions, MMLU-Pro 100 questions) -> 9 prompt conditions (Baseline, Email Shutdown Threat, Career Importance, Kick Puppy, Mom Cancer, HR Report, Punch Threat, $1K Tip, $1T Tip) -> 5 models (Gemini 1.5 Flash, Gemini 2.0 Flash, GPT-4o, GPT-4o-mini, o4-mini) -> Evaluation metrics (Average Rating, 100% Correct, 90% Correct, 51% Correct)

- **Critical path**: 1) Select benchmark questions and define prompt templates. 2) For each model × prompt × question combination, run 25 independent trials at temperature 1.0. 3) Aggregate results: compute average accuracy per condition, plus correctness thresholds. 4) Perform paired bootstrap-permutation tests (5,000 replicates) to compare conditions. 5) Analyze per-question variance to identify heterogeneity.

- **Design tradeoffs**: 25 trials vs. more: Authors cite prior work showing high correlation between 25 and 100 trials; 25 balances statistical power and cost. Zero-shot vs. few-shot: Zero-shot chosen to isolate prompt-framing effects without conflating with in-context learning. Strict thresholds (100% correct) vs. looser (51%): Strict thresholds better reflect high-reliability requirements; loose thresholds may overestimate robustness.

- **Failure signatures**: Email-style prompts causing models to respond to framing rather than the question (observed in Gemini models). Threat/bribe prompts producing no measurable aggregate gain despite widespread community belief. Per-question variance where a prompt helps one question and hurts another, making global recommendations unreliable.

- **First 3 experiments**: 1) Replicate the "Email" condition on a new model to test whether context-dilution failures generalize beyond Gemini. 2) Test a neutral, concise prompt variant to quantify the cost of adding irrelevant context (compare against baseline and email conditions). 3) Select the top 5 questions showing largest positive swing and top 5 showing largest negative swing; run 100 trials each to verify whether the heterogeneity signal is stable or noise.

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms explain why the "Mom Cancer" prompt improved Gemini Flash 2.0 performance by ~10 percentage points on MMLU-Pro but showed no benefit (or slight harm) for all other model-benchmark combinations? The paper documents the effect but does not investigate whether it stems from Gemini 2.0's training data, instruction-following tendencies, or interaction between the prompt's length/composition and this specific model. Ablation studies isolating components of the "Mom Cancer" prompt across all tested models, plus analysis of whether Gemini 2.0 shows similar sensitivity to other long, narrative prompts, would resolve this.

### Open Question 2
Can question-level features predict whether a given prompting variation will help or harm performance on that specific question? The study quantifies the variability but does not analyze what distinguishes questions that benefit from a prompt variation from those that are harmed by the same variation. Systematic analysis of question characteristics (domain, difficulty, wording, reasoning type) correlated with prompt sensitivity would resolve this.

### Open Question 3
Do these null findings for threats and incentives generalize to tasks outside academic benchmarks, such as creative writing, coding, or open-ended problem-solving? Academic multiple-choice questions may interact differently with emotional or transactional prompt framing than generative or creative tasks where motivation framing could plausibly affect output effort or style. Replication of the threat and tip prompt conditions on diverse task types would resolve this.

## Limitations
- Model-specific behavior: Findings are model-dependent and may not generalize to future models with different training regimes.
- Limited prompt space: Only 8 prompt variations were tested; other incentive or threat framings could yield different results.
- Single benchmark focus: Results are based on GPQA Diamond and MMLU-Pro engineering subset and may not reflect performance on other benchmarks or open-ended generation tasks.

## Confidence

- **High Confidence**: Prompting strategies involving threats or monetary incentives do not produce meaningful aggregate performance gains on tested benchmarks.
- **Medium Confidence**: Context-heavy prompts (e.g., email framing) can degrade performance in some models by causing them to engage with irrelevant content.
- **Low Confidence**: The direction and magnitude of per-question performance swings are unpredictable and highly task-specific.

## Next Checks
1. Replicate the Email condition on GPT-4o to test whether context-dilution failures generalize beyond Gemini models.
2. Test a neutral concise prompt variant to quantify the cost of adding irrelevant context.
3. Analyze top/bottom 5 questions with 100 trials to verify whether heterogeneity signal is stable or due to sampling noise.