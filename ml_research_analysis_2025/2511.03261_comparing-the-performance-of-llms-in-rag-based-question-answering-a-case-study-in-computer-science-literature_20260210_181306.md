---
ver: rpa2
title: 'Comparing the Performance of LLMs in RAG-based Question-Answering: A Case
  Study in Computer Science Literature'
arxiv_id: '2511.03261'
source_url: https://arxiv.org/abs/2511.03261
tags:
- llms
- questions
- answer
- answers
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared five LLMs (GPT-3.5, LLaMa2-7b-chat, Mistral-7b-instruct,
  Falcon-7b-instruct, and Orca-mini-v3-7b) for question-answering tasks on computer
  science literature using RAG. Mistral-7b-instruct with RAG achieved the highest
  accuracy (0.857) and precision (0.857) among open-source models for binary questions,
  while GPT-3.5 + RAG performed best overall with accuracy 0.905 and precision 0.905.
---

# Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature

## Quick Facts
- arXiv ID: 2511.03261
- Source URL: https://arxiv.org/abs/2511.03261
- Authors: Ranul Dayarathne; Uvini Ranaweera; Upeksha Ganegoda
- Reference count: 40
- Primary result: Open-source LLMs with RAG can match GPT-3.5 performance on CS literature QA when paired with better infrastructure

## Executive Summary
This study systematically compares five large language models (GPT-3.5, LLaMa2-7b-chat, Mistral-7b-instruct, Falcon-7b-instruct, and Orca-mini-v3-7b) for question-answering tasks on computer science literature using Retrieval-Augmented Generation (RAG). The research demonstrates that open-source models can achieve comparable performance to proprietary models like GPT-3.5 when optimized with appropriate RAG infrastructure. The study evaluates both binary and long-answer questions, measuring accuracy, precision, latency, and qualitative ratings from human experts and AI evaluators.

## Method Summary
The research employs a RAG pipeline built with LangChain, using SPECTER embeddings to create a vector store from 4,929 computer science abstracts (2023-2024) across three topics. The system chunks text into 1024-character segments with 200-character overlap, retrieves top-10 documents using cosine similarity threshold 0.6, and generates answers using five different LLMs. Binary questions are evaluated for accuracy and precision, while long-answer questions receive both human expert rankings (poor/average/excellent) and AI evaluation via Google's Gemini. The study also measures latency and infrastructure costs across models.

## Key Results
- Mistral-7b-instruct with RAG achieved highest accuracy (0.857) and precision (0.857) among open-source models for binary questions
- GPT-3.5 + RAG performed best overall with accuracy 0.905 and precision 0.905
- For long-answer questions, Mistral-7b-instruct received most "excellent" ratings from both human experts and Gemini AI
- Orca-mini-v3-7b showed shortest average latency (99.2 seconds) among open-source models, while LLaMa2-7b-chat had highest latency (107.5 seconds)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG reduces hallucination by grounding LLM responses in retrieved domain-specific documents
- Mechanism: When a user query enters the system, it is embedded and matched against a vector store of chunked documents. The top-k retrieved chunks are injected into the LLM prompt, providing external non-parametric memory that the model conditions on before generating an answer
- Core assumption: The retrieved chunks contain sufficient relevant information to answer the query, and the LLM can effectively synthesize this context
- Evidence anchors:
  - [abstract] "RAG is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination"
  - [section 4] "ChatGPT... lacks the skill to answer questions on recent content, underscoring that off-the-shelf models are not carrying out RAG if not customised" — accuracy dropped from 0.905 (GPT-3.5+RAG) to 0.476 (ChatGPT without RAG)
  - [corpus] Related medical QA study confirms RAG mitigates hallucination in clinical domain applications

### Mechanism 2
- Claim: Grouped Query Attention (GQA) provides a performance-efficiency balance that outperforms Multi-Query Attention (MQA) and matches Multi-Head Attention (MHA) quality
- Mechanism: GQA interpolates between MQA (fast but lower quality) and MHA (high quality but memory-intensive) by grouping query heads, enabling higher throughput and batch sizes while maintaining answer quality
- Core assumption: Attention mechanism architecture directly influences both latency and answer quality in RAG pipelines
- Evidence anchors:
  - [section 4] "Mistral uses a grouped query attention (GQA) which results in higher throughput by allowing for higher batch sizes... As GQA stand in the balance between MQA and MHA, it is fair to expect a significant performance improvement"
  - [section 4, Table 2] Mistral-7b-instruct achieved 0.857 accuracy vs Falcon's 0.619 (Falcon uses MQA)
  - [corpus] Limited direct corpus evidence on attention mechanism comparisons in RAG systems

### Mechanism 3
- Claim: Prompt template adherence is critical for instruction-tuned models to correctly interpret retrieved context
- Mechanism: Models like Mistral-7b-instruct require specific formatting tokens (e.g., `<s>[INST] instructions [/INST]`) to properly recognize instruction boundaries; without this, models may fail to synthesize retrieved chunks appropriately
- Core assumption: The model was fine-tuned on a specific prompt template, and deviating from it degrades instruction-following capability
- Evidence anchors:
  - [section 4, Table 5] When Mistral received prompts not following its template, it failed to answer yes/no questions appropriately; with correct formatting, it provided accurate, contextual responses
  - [section 4] "some LLMs required prompts to follow a specific format to generate better answers... when given a prompt to Mistral-7b-instruct without following its template, it failed to generate answers as expected"

## Foundational Learning

- **Concept: Vector Embeddings and Semantic Similarity**
  - Why needed here: The RAG pipeline converts text chunks into dense vectors using SPECTER embeddings, then retrieves via cosine similarity. Understanding that semantically similar text maps to nearby points in vector space is essential
  - Quick check question: Given two sentences with different words but similar meaning (e.g., "quantum speedup" vs "quantum advantage"), would their embeddings have high or low cosine similarity?

- **Concept: Retrieval-Augmented Generation Architecture**
  - Why needed here: The system chains together a retriever (FAISS vector store), a prompt formatter, and an LLM generator. Understanding this three-stage pipeline is prerequisite to debugging retrieval failures vs generation failures
  - Quick check question: If an LLM generates a factually incorrect answer, what are three possible failure points in the RAG pipeline?

- **Concept: Chunking Strategy and Context Windows**
  - Why needed here: The study uses 1024-character chunks with 200-character overlap to balance context preservation against token limits. Understanding the trade-off between chunk size and retrieval precision is critical
  - Quick check question: If you increase chunk size from 1024 to 2048 characters, what happens to retrieval granularity and what risk increases?

## Architecture Onboarding

- **Component map:**
  User Query → History-aware Query Rewriter (optional) → Query Embedding (SPECTER) → FAISS Vector Store (k=10, threshold=0.6) → Retrieved Chunks + QA Prompt Template → LLM Generator (GPT-3.5 / Mistral / etc.) → Response

- **Critical path:** The retrieval quality (similarity threshold, chunk quality, embedding model) dominates final answer accuracy. If retrieval returns irrelevant chunks, even the best LLM cannot generate correct answers.

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Open-source LLM (Mistral) | No API cost, data privacy | ~100x higher latency (105s vs 1.7s) |
  | Smaller chunks (512 chars) | More granular retrieval | More chunks to search, potential context fragmentation |
  | Higher similarity threshold (0.8) | Fewer false positives | May return empty results for edge queries |

- **Failure signatures:**
  - Empty/no relevant chunks retrieved → similarity threshold too high or knowledge base gap
  - Hallucinated facts despite RAG → chunks lack answer, or model ignores context (check prompt template)
  - Very high latency on open-source models → insufficient local hardware; consider GPU acceleration or API fallback
  - Inconsistent rankings from AI evaluators → Gemini variability; use multiple evaluation runs or human baselines

- **First 3 experiments:**
  1. **Retrieval ablation:** Run the same 30 QA pairs with k=5 vs k=10 retrieved chunks. Measure accuracy change to validate the chunk count parameter.
  2. **Model swap test:** Replace Mistral-7b-instruct with Mistral-7b (non-instruct variant) using identical prompts. Expect degraded performance, confirming instruction-tuning importance for RAG synthesis.
  3. **Latency profiling:** Measure end-to-end latency with GPU vs CPU-only inference for the same open-source model. Quantify the infrastructure upgrade benefit claimed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-enabled LLMs change when utilizing full-text research articles instead of limiting the knowledge base to abstracts?
- Basis in paper: [explicit] The authors list "utilising full-text research articles rather than limiting to abstracts" as a primary method to extend the research.
- Why unresolved: The current study restricted the dataset to abstracts due to "limitations in accessing full papers and the limited computational resources."
- What evidence would resolve it: A comparative benchmark of accuracy and latency scores using a dataset of full papers versus the current abstract-only dataset.

### Open Question 2
- Question: Do alternative retrieval mechanisms, such as hybrid search, outperform the vector store approach used in this study?
- Basis in paper: [explicit] The authors suggest "trying out different mechanisms to retrieve data in contrast to vector stores" for future work.
- Why unresolved: The current architecture relied exclusively on FAISS vector stores with SPECTER embeddings for semantic search.
- What evidence would resolve it: Evaluation metrics comparing the recall and precision of the current vector-only retrieval against keyword-based or hybrid retrieval methods.

### Open Question 3
- Question: To what extent does rigorous prompt engineering improve the accuracy and latency of open-source models compared to the baseline used?
- Basis in paper: [explicit] The paper states the prompt used "might not be optimal" and lists "incorporating prompt engineering" as an area for future extension.
- Why unresolved: The authors used a single "good enough" prompt and noted that open-source models often failed without specific formatting.
- What evidence would resolve it: A comparison of performance metrics using systematically optimized prompts against the baseline template provided in the study.

## Limitations

- The custom 30 QA pairs are highly specific to computer science literature and lack a standard benchmark, limiting generalizability and external validation.
- The study only tested one similarity threshold (0.6) and chunking strategy, potentially missing optimal RAG parameter configurations.
- Gemini AI evaluation shows high variance across runs, yet the paper doesn't report variance measures or confidence intervals for rankings.

## Confidence

- **Claim Cluster 1 (RAG Effectiveness)** - High confidence: Strong empirical evidence shows RAG significantly improves accuracy (0.905→0.476 for GPT-3.5) and mitigates hallucination
- **Claim Cluster 2 (Open-Source Model Performance)** - Medium confidence: Results are convincing but based on a small sample (30 questions) and single knowledge domain
- **Claim Cluster 3 (Architecture Design Choices)** - Low confidence: Key design decisions (chunk size, similarity threshold, prompt templates) lack ablation studies to validate optimality

## Next Checks

1. **Dataset Generalizability Test**: Apply the same RAG pipeline to a different domain (e.g., medical literature or legal documents) using the same 30 question templates to test cross-domain performance consistency
2. **Hyperparameter Sensitivity Analysis**: Systematically vary similarity threshold (0.4→0.8) and chunk size (512→2048 chars) to identify optimal RAG parameters for each model, measuring the trade-off between retrieval precision and generation quality
3. **Long-term Consistency Evaluation**: Run the Gemini AI evaluator on the same long-answer questions 10+ times to measure variance in rankings, and compare with human expert consistency scores to validate evaluation reliability