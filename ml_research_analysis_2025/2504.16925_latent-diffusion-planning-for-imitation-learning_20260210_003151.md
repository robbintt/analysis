---
ver: rpa2
title: Latent Diffusion Planning for Imitation Learning
arxiv_id: '2504.16925'
source_url: https://arxiv.org/abs/2504.16925
tags:
- learning
- data
- diffusion
- latent
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Diffusion Planning (LDP) is a new imitation learning method
  that separates the problem into planning over future states and extracting actions
  using diffusion models. By operating in a learned latent space, LDP can leverage
  both suboptimal and action-free data, unlike prior approaches that require optimal
  action labels.
---

# Latent Diffusion Planning for Imitation Learning

## Quick Facts
- arXiv ID: 2504.16925
- Source URL: https://arxiv.org/abs/2504.16925
- Reference count: 26
- Primary result: LDP achieves 0.65 success rate on visual robotic manipulation tasks by leveraging action-free and suboptimal data through separate planning and inverse dynamics modules

## Executive Summary
Latent Diffusion Planning (LDP) is a new imitation learning method that separates the problem into planning over future states and extracting actions using diffusion models. By operating in a learned latent space, LDP can leverage both suboptimal and action-free data, unlike prior approaches that require optimal action labels. LDP uses a variational autoencoder to create a compact latent space, then trains a planner and inverse dynamics model with diffusion objectives. The planner forecasts future latent states from current observations, while the IDM extracts actions between consecutive latent states. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, particularly when combined with action-free and suboptimal data. LDP achieves average success rates of 0.65 compared to 0.51 for standard Diffusion Policy, with further improvements to 0.95 when using additional data sources. The method also shows promise in real-world applications, outperforming Diffusion Policy on a Franka Lift task.

## Method Summary
LDP decouples imitation learning into two diffusion-based modules: a planner that forecasts future latent states and an inverse dynamics model (IDM) that extracts actions from state transitions. First, a Î²-VAE is trained on expert, suboptimal, and action-free images to create a compact latent space. The planner, a conditional U-Net, learns to forecast dense sequences of future latents from current observations. The IDM, an MLPResNet, learns to denoise actions from pairs of consecutive latents. At inference, LDP runs in a receding-horizon loop: it encodes the current observation, diffuses to generate a latent plan, extracts actions via the IDM, executes them, and replans. This architecture enables leveraging heterogeneous data sources that standard imitation learning cannot use.

## Key Results
- LDP achieves 0.65 average success rate on visual robotic manipulation tasks, outperforming Diffusion Policy's 0.51
- Combining action-free and suboptimal data improves LDP performance to 0.95 success rate
- Dense temporal forecasting outperforms sparse subgoal prediction, with LDP (dense) significantly outperforming LDP Hierarchical on the Square task (0.46 vs 0.31)

## Why This Works (Mechanism)

### Mechanism 1
The modular separation of planning and action execution enables the model to assimilate heterogeneous data sources that standard imitation learning cannot use. The architecture decouples "what to do" (planning future states) from "how to do it" (inverse dynamics). This allows the planner to train on action-free video data (providing trajectory priors) and the IDM to train on suboptimal play data (providing transition physics), merging these capabilities at inference time. Assumption: The latent state transitions learned from action-free data correspond to physically realizable trajectories that the IDM can map back to actions. Evidence: Shows performance gains when adding action-free data (+0.04 avg success) and suboptimal data (+0.30 avg success). Break condition: If the VAE latent space fails to disentangle task-relevant features, the planner may forecast physically impossible states that the IDM cannot act upon.

### Mechanism 2
Dense temporal forecasting in a learned latent space provides higher-fidelity control signals than sparse subgoal prediction. Unlike hierarchical methods that plan distant subgoals, LDP predicts a dense sequence of latents ($z_{k+1}, \dots, z_{k+H}$). This provides the IDM with immediate, fine-grained state deltas, crucial for contact-rich tasks like grasping where precise intermediate states matter. Assumption: The convolutional inductive biases in the U-Net planner effectively model temporal consistency across the dense latent horizon. Evidence: LDP (dense) significantly outperforms LDP Hierarchical (sparse) on the Square task (0.46 vs 0.31). Break condition: If the planning horizon is too short, the agent loses long-horizon context; if too long, inference latency may break real-time reactivity.

### Mechanism 3
Training the VAE on mixed-quality data creates a representation robust to the distributional shift of suboptimal rollouts. By training the VAE encoder/decoder on expert, action-free, and suboptimal images simultaneously, the latent space learns to represent the full visual domain of the task, preventing the planner from generating out-of-distribution latent codes when deviating slightly from expert paths. Assumption: Reconstruction loss is a sufficient proxy to ensure the latent space encodes dynamics-relevant features. Evidence: Using frozen DINOv2 embeddings fails (0% success) vs. learned VAE, suggesting generic visual features are insufficient. Break condition: If the VAE "averages" blurry reconstructions of diverse states, critical geometric details for manipulation may be lost.

## Foundational Learning

**Concept: Denoising Diffusion Probabilistic Models (DDPM)**
- Why needed here: LDP relies on diffusion for both the planner and the IDM. You must understand how networks learn to reverse a noising process to generate valid sequences (trajectories or actions).
- Quick check question: Can you explain why a U-Net is typically used as the denoiser in diffusion models?

**Concept: Inverse Dynamics Models (IDM)**
- Why needed here: The core separation in LDP depends on the IDM's ability to infer the action $a_t$ given state $s_t$ and $s_{t+1}$. Understanding the data efficiency and limitations of IDMs is critical.
- Quick check question: If two distinct actions lead to the same state transition (redundant degrees of freedom), how might a diffusion-based IDM handle this compared to a deterministic MLP?

**Concept: Variational Autoencoders (VAE)**
- Why needed here: The quality of the entire pipeline hinges on the VAE compressing images into a "plannable" latent space. Understanding the KL-divergence term ($\beta$-VAE) is necessary to tune the balance between reconstruction and latent regularity.
- Quick check question: What happens to the planner's diffusion process if the VAE latent space is not continuous or regularized?

## Architecture Onboarding

**Component map**: VAE Encoder -> Planner U-Net -> IDM MLPResNet -> Robot Execution

**Critical path**: The inference loop (Algorithm 1) is the critical path.
1. Encode: $z_k = E(x_k)$
2. Plan: Diffuse over $H$ steps to get latent trajectory
3. Act: For each step in action horizon, use IDM to extract action from consecutive latents
4. Execute: Send action to robot

**Design tradeoffs**:
- Frozen vs. Trained Embeddings: The DINOv2 ablation shows that planning over fixed, generic embeddings fails. You must train the VAE end-to-end on domain data.
- Architecture Selection: The paper uses a heavier U-Net for the Planner (high capacity for trajectory generation) and a lighter MLPResNet for the IDM (faster inference for immediate actions).

**Failure signatures**:
- Action-free data bottlenecks: If the planner learns visual priors that are physically impossible, the IDM will output erratic or high-torque actions trying to reach that state.
- Slow Inference: Unlike standard Behavior Cloning, LDP requires two diffusion steps (planning + IDM). If $T_p$ (planning steps) is too high, closed-loop reactivity drops.

**First 3 experiments**:
1. VAE Reconstruction Check: Verify the VAE can reconstruct key task states (grasping, lifting) from both expert and suboptimal data. If the VAE blurs the object, the planner will fail.
2. Planner Coherence Test: condition the planner on an initial state and decode the generated latent plan. Do the decoded images form a smooth video, or do they flicker incoherently?
3. Data Ablation: Run LDP with only expert data vs. LDP with expert + suboptimal data. Confirm the "Dense supervision" benefit appears specifically in the low-data regime.

## Open Questions the Paper Calls Out
1. Can representation learning objectives other than the $\beta$-VAE create a latent space that improves control performance? The authors state the VAE "might not learn the most useful features for control" and propose exploring "different representation learning objectives." A comparative study where LDP's encoder is trained with contrastive or dynamics-aware losses, demonstrating improved success rates on complex manipulation tasks, would resolve this.

2. Does Latent Diffusion Planning scale effectively to complex, long-horizon real-world manipulation tasks? The authors note that future work should "evaluate whether this can be used to further improve more complex real-world tasks." Real-world validation was limited to the relatively simple "Franka Lift" task, leaving high-dexterity or multi-stage tasks untested. Benchmarking LDP against baselines on dexterous real-world suites to verify scalability would resolve this.

3. How does LDP perform in large data regimes when utilizing recent diffusion model advancements? The paper acknowledges not exploring "recent improvements in diffusion models, which may be important in large data regimes." Experiments focused on a "low demonstration data regime" using a U-Net backbone, leaving high-capacity scaling unverified. Applying Transformer-based diffusion backbones to LDP with large-scale datasets would resolve this.

## Limitations
- The paper's claims about leveraging heterogeneous data sources rely on the assumption that the VAE learns a latent space where physically realizable trajectories are well-represented, which is not empirically validated.
- The computational overhead of running two diffusion processes during inference is not characterized, which could limit real-world applicability.
- The claim that dense temporal forecasting is superior to sparse subgoal prediction is only compared against one hierarchical baseline and doesn't explore different planning granularities systematically.

## Confidence

**High confidence**: The modular architecture design and the core experimental results showing LDP outperforms Diffusion Policy on the tested tasks.

**Medium confidence**: The mechanism explanations for why heterogeneous data helps - while the results support this, the paper doesn't directly test whether the planner generates physically realizable states or whether the IDM can map them back to actions.

**Low confidence**: The claim that dense temporal forecasting is superior to sparse subgoal prediction, as the comparison is only against one hierarchical baseline and doesn't explore different planning granularities systematically.

## Next Checks

1. **Physical feasibility validation**: After generating a latent plan, decode the sequence and verify that consecutive frames represent physically realizable state transitions (no object teleportation, no penetration through surfaces).

2. **Data efficiency analysis**: Systematically vary the ratio of expert to suboptimal/action-free data and measure LDP's performance curve to determine where the heterogeneous data provides the most benefit.

3. **Runtime characterization**: Measure the wall-clock time for LDP's inference loop (including both diffusion steps) and compare it to standard diffusion policy to quantify the computational overhead and its impact on closed-loop control latency.