---
ver: rpa2
title: A Training Framework for Optimal and Stable Training of Polynomial Neural Networks
arxiv_id: '2505.11589'
source_url: https://arxiv.org/abs/2505.11589
tags:
- polynomial
- training
- gradient
- clipping
- pnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of training high-degree Polynomial
  Neural Networks (PNNs) for privacy-preserving inference using Homomorphic Encryption
  (HE), where standard methods suffer from numerical instability and gradient explosion.
  The authors introduce a novel training framework combining two key innovations:
  a Boundary Loss that exponentially penalizes activation inputs outside a predefined
  stable range, and Selective Gradient Clipping that manages gradient magnitudes while
  preserving Batch Normalization statistics.'
---

# A Training Framework for Optimal and Stable Training of Polynomial Neural Networks

## Quick Facts
- arXiv ID: 2505.11589
- Source URL: https://arxiv.org/abs/2505.11589
- Reference count: 16
- Primary result: Novel training framework achieves stable, accurate PNN training with degrees up to 22 for HE-compatible inference, matching ReLU performance

## Executive Summary
This paper introduces a training framework that addresses the critical challenge of stabilizing high-degree Polynomial Neural Networks (PNNs) for Homomorphic Encryption (HE)-compatible inference. Standard training methods fail with high-degree polynomials due to numerical instability and gradient explosion. The authors propose two key innovations: a Boundary Loss that exponentially penalizes activation inputs outside a predefined stable range, and Selective Gradient Clipping that preserves Batch Normalization statistics while managing gradient magnitudes. This framework enables stable training of PNNs with polynomial degrees up to 22, achieving high accuracy and performance comparable to ReLU-based networks across diverse datasets.

## Method Summary
The framework trains PNNs by fitting polynomials to target activation functions (e.g., ReLU) via least squares on uniform samples in [-B, B]. The Boundary Loss exponentially penalizes inputs exceeding this range using Li = (1/|Xi|) Σ(e^max(|Xi,j|-αB,0) - 1), combined with standard cross-entropy. Selective Gradient Clipping excludes BatchNorm parameters from clipping to preserve adaptive learning. The method uses AdamW with degree-specific learning rates, reduces LR on plateau, and employs kaiming_uniform initialization. Critical hyperparameters include λ=1000 for boundary loss, B values calibrated to polynomial degree (10-14 for deg 2; 20-30 for deg 4; 35 for deg 8+), and α thresholds varying by degree.

## Key Results
- Stable training achieved for PNNs with polynomial degrees up to 22, previously impossible with standard methods
- High accuracy across diverse datasets (image, audio, human activity recognition) comparable to ReLU baselines
- Low-degree polynomials (e.g., degree 2) also achieve strong accuracy, demonstrating flexibility
- Extensive ablation studies confirm critical synergy between boundary loss and selective clipping for stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exponentially penalizing activation inputs outside a predefined stable range prevents divergence during PNN training.
- **Mechanism:** The Boundary Loss applies e^(max(|Xi,j| - αB, 0)) - 1 when inputs exceed threshold αB, creating steep gradient penalty that forces network to keep activations within polynomial's reliable approximation region. Composite loss L_total = L_CE + λ·ΣLi with λ=1000 ensures constraint dominates when violations occur.
- **Core assumption:** Polynomial approximations fitted to target activations are only valid within fitting interval [-B, B]; beyond this, highest-degree term dominates and causes unreliable outputs.
- **Break condition:** If B set too low relative to actual activation input distributions, boundary loss explodes at initialization (especially with dropout), causing immediate divergence rather than stabilization.

### Mechanism 2
- **Claim:** Excluding BatchNorm parameters from gradient clipping preserves adaptive learning capability essential for PNN stability.
- **Mechanism:** Standard gradient clipping rescales all gradients when ||∇θ||₂ > c, but this interferes with BatchNorm's γ and β parameters, which need unmodified gradients to adapt running statistics correctly. Selective clipping only rescales ∇θ_non-BN, leaving ∇θ_BN untouched.
- **Core assumption:** BatchNorm's ability to track distribution shifts during training requires gradient updates proportional to actual statistical deviations; clipping distorts this calibration.
- **Break condition:** If gradient magnitudes from boundary loss vastly exceed clipping threshold c, even non-BN parameters may receive distorted updates, though partially mitigated by clipping itself.

### Mechanism 3
- **Claim:** Polynomial fitting boundary B must scale with polynomial degree—higher degrees require larger B for stability, but excessively large B degrades approximation quality.
- **Mechanism:** Higher-degree polynomials can approximate target functions more accurately but have more extreme extrapolation behavior outside [-B, B]. Larger B provides more buffer before boundary loss activates, but too large a B means polynomial poorly approximates target function in critical input region where most data falls.
- **Core assumption:** Optimal B lies near lower end of stable training range for given polynomial degree—minimizing B maximizes local approximation quality while avoiding instability.
- **Break condition:** B set too small → immediate instability; B set too large → poor accuracy despite stable training.

## Foundational Learning

- **Concept: Polynomial approximation of non-linear functions**
  - Why needed here: PNNs replace ReLU/sigmoid with polynomial approximations; understanding that polynomials are only accurate within their fitting interval [-B, B] is essential for grasping why boundary constraints matter.
  - Quick check question: Given a degree-4 polynomial fitted to approximate ReLU on [-20, 20], what happens to output if input is 100?

- **Concept: Gradient explosion in deep networks with multiplicative structure**
  - Why needed here: High-degree polynomials multiply input values multiple times (x^d), creating exponentially scaling gradients that standard training cannot handle without intervention.
  - Quick check question: For f(x) = x^8, how does gradient magnitude at x=10 compare to x=1?

- **Concept: Batch Normalization's running statistics vs. learnable parameters**
  - Why needed here: Selective gradient clipping hinges on understanding that BN has both running mean/variance (not learned via gradients) and γ/β (learned via gradients), and why clipping distorts latter's calibration.
  - Quick check question: Which BN parameters receive gradient updates during backpropagation—running mean, running variance, γ, or β?

## Architecture Onboarding

- **Component map:**
  Input → [Linear/Conv] → [BatchNorm] → [Polynomial Activation] → [Dropout] → Output

- **Critical path:** Stability depends on interaction between: (1) B selection matching polynomial degree, (2) α threshold activating boundary loss before inputs reach unreliable extrapolation, (3) selective clipping preventing gradient explosion while preserving BN adaptation.

- **Design tradeoffs:**
  - Lower polynomial degree (d=2) → More stable, faster HE inference, but potentially lower accuracy on complex tasks
  - Higher polynomial degree (d≥8) → Better expressivity, can match ReLU performance, but requires careful B tuning and higher HE multiplicative depth
  - Larger B → More stability margin, but worse local approximation quality

- **Failure signatures:**
  - Immediate divergence at epoch 1 → B too small for polynomial degree
  - Gradual accuracy degradation with stable training → B too large
  - Training collapse mid-training with high-degree polynomials → Forgot selective clipping (BN parameters being clipped)
  - High variance in success across runs → B near critical threshold; increase slightly

- **First 3 experiments:**
  1. **Sanity check:** Train degree-2 PNN on MNIST with B=13, α=1.0, λ=1000, clipping threshold c=1.0. Verify accuracy approaches ReLU baseline (~99%). This validates basic pipeline.
  2. **Boundary calibration:** On CIFAR-10, sweep B values [10, 15, 20, 25, 30] for degree-2 and degree-4 polynomials. Plot accuracy vs. B to identify stability cliff and performance degradation point for each degree.
  3. **Ablation validation:** Train degree-4 PNN on CIFAR-10 with four configurations: (a) full method, (b) no boundary loss, (c) no gradient clipping, (d) standard clipping (including BN). Confirm only (a) achieves stable convergence and high accuracy.

## Open Questions the Paper Calls Out

- **Question:** Does the proposed framework support transfer learning where PNNs are pre-trained on large datasets and fine-tuned for specific Homomorphic Encryption (HE) tasks?
  - **Basis in paper:** Section 7.1 (Limitations) states: "The potential for transfer learning... was not explored and remains an area for future work."
  - **Why unresolved:** Authors focused exclusively on training PNNs from scratch for each specific dataset, leaving generalizability of learned representations and weight transferability unexamined.
  - **What evidence would resolve it:** Experiments showing PNNs pre-trained on large-scale datasets (e.g., ImageNet) using this framework can be successfully fine-tuned on downstream tasks with stable convergence.

- **Question:** What are the formal theoretical stability conditions and convergence guarantees provided by combined Boundary Loss and Selective Gradient Clipping?
  - **Basis in paper:** Section 7.1 notes that "development and validation of our framework are primarily empirical" and suggests "a deeper theoretical analysis... could further solidify the approach."
  - **Why unresolved:** Paper relies on extensive empirical ablation studies (Fig. 3) to demonstrate stability but doesn't provide mathematical proof explaining why specific synergy of these two techniques guarantees convergence.
  - **What evidence would resolve it:** Theoretical analysis or proof defining convergence bounds and stability properties of optimizer under proposed loss and clipping regime.

- **Question:** Can inference cost of high-degree PNNs trained with this framework be optimized to function efficiently in resource-constrained environments?
  - **Basis in paper:** Section 7.1 lists "Inference Cost of High-Degree Polynomials" as limitation, noting computational cost "remains a factor... particularly relevant for resource-constrained environments."
  - **Why unresolved:** While framework successfully stabilizes training for degrees up to 22, it doesn't address computational overhead or multiplicative depth constraints inherent to evaluating these high-degree polynomials during encrypted inference.
  - **What evidence would resolve it:** Integration of model compression or polynomial simplification techniques that reduce inference latency without destabilizing specific training dynamics introduced by framework.

## Limitations

- The polynomial fitting process depends on number of fitting samples (m) and sampling strategy, which are not precisely specified, making exact reproduction uncertain.
- Gradient clipping threshold (c) is only given as range (0.5-1.0), not exact value, likely affecting training dynamics and final accuracy.
- While ablation studies show two innovations are critical together, paper doesn't fully explore whether alternative boundary loss formulations or different selective clipping strategies could achieve similar results.
- B selection process relies on empirical tuning per degree rather than principled method, making it somewhat dataset-dependent and potentially limiting generalizability.

## Confidence

- **High confidence:** Core mechanism that boundary loss prevents instability by penalizing activations outside polynomial's reliable range; selective clipping approach of excluding BN parameters is well-validated through ablation.
- **Medium confidence:** Degree-dependent B scaling relationship is empirically observed but could benefit from theoretical grounding; specific λ=1000 hyperparameter choice appears effective but isn't extensively explored.
- **Low confidence:** Polynomial fitting process details (sample count, distribution) are underspecified, making exact reproduction uncertain; optimal B values may be dataset-specific rather than universally applicable.

## Next Checks

1. **Cross-dataset B transferability:** Train same degree-4 PNN on CIFAR-10 and then test on CIFAR-100 (same data distribution but different classes) to determine if B=25-30 remains optimal or requires adjustment, validating whether B is architecture-dependent versus dataset-dependent.

2. **Boundary loss alternative formulations:** Replace exponential boundary loss with quadratic penalty (e.g., (max(|X|-αB, 0))^2) while keeping selective clipping, to test whether exponential form is essential or if simpler penalties suffice for stability.

3. **HE inference validation:** Compile trained PNN models (particularly high-degree versions) to verify they actually achieve claimed low latency (7.2ms for deg-8) on target HE hardware, confirming training optimizations translate to practical inference benefits.