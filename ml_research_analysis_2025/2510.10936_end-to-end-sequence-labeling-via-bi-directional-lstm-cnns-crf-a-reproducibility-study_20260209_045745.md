---
ver: rpa2
title: 'End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility
  Study'
arxiv_id: '2510.10936'
source_url: https://arxiv.org/abs/2510.10936
tags:
- implementation
- sequence
- learning
- labeling
- reproducibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a successful reproduction of the BiLSTM-CNN-CRF
  architecture for sequence labeling, achieving 91.18% F1-score on CoNLL-2003 NER
  (original: 91.21%) and 97.52% accuracy on Penn Treebank WSJ POS tagging (original:
  97.55%). The architecture combines character-level CNN encoding for morphological
  features, word-level BiLSTM encoding for contextual information, and CRF-based structured
  prediction for coherent label sequences.'
---

# End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study

## Quick Facts
- **arXiv ID:** 2510.10936
- **Source URL:** https://arxiv.org/abs/2510.10936
- **Reference count:** 19
- **Primary result:** Reproduced 91.18% F1-score on CoNLL-2003 NER (original: 91.21%) and 97.52% accuracy on Penn Treebank WSJ POS tagging (original: 97.55%)

## Executive Summary
This paper presents a successful reproduction of the BiLSTM-CNN-CRF architecture for sequence labeling, achieving near-identical performance to the original 2016 results. The architecture combines character-level CNN encoding for morphological features, word-level BiLSTM encoding for contextual information, and CRF-based structured prediction for coherent label sequences. The authors provide a detailed PyTorch implementation with ablation studies showing each component's contribution, and release their code to facilitate further research. The work demonstrates the effectiveness of end-to-end neural sequence labeling without hand-crafted features.

## Method Summary
The BiLSTM-CNN-CRF architecture processes input sequences through three main components: character-level CNNs that capture morphological patterns via 1D convolutions and max-pooling, word-level BiLSTMs that encode bidirectional context, and a CRF layer that models tag transition constraints. The model uses 100-dimensional GloVe embeddings concatenated with character CNN outputs (30 filters, kernel size 3), processed by a 200-dimensional BiLSTM (100 per direction), followed by a CRF layer with Viterbi decoding. Training uses SGD with momentum (lr=0.015), dropout=0.5, gradient clipping at 5.0, and early stopping on development set performance.

## Key Results
- Achieved 91.18% F1-score on CoNLL-2003 Named Entity Recognition (original: 91.21%)
- Achieved 97.52% accuracy on Penn Treebank WSJ POS tagging (original: 97.55%)
- Ablation studies show character CNN contributes +4.44 F1 points, BiLSTM +1.16 F1 points, and CRF +0.35 F1 points
- Released PyTorch implementation with documentation and reproducible results

## Why This Works (Mechanism)

### Mechanism 1: Character-level CNN for Morphological Encoding
Character CNNs capture subword patterns that improve generalization to out-of-vocabulary words and morphology-rich inputs. Each character is embedded into a d_c-dimensional space, then 1D convolution with kernel size k extracts local n-gram patterns. Max-pooling over the entire word produces a fixed-size representation encoding prefixes, suffixes, and capitalization patterns. This works because morphological signals are predictive of sequence labels and these patterns are local to character windows. Evidence shows ablation improves F1 by +4.44 points when adding Character CNN to word embeddings (85.23 → 89.67).

### Mechanism 2: BiLSTM for Bidirectional Context
Bidirectional LSTMs encode left and right context for each token, enabling disambiguation based on surrounding words. Forward LSTM processes sequence left-to-right; backward LSTM processes right-to-left. Hidden states are concatenated: h_t = [→h_t; ←h_t]. This allows each position to access both preceding and following context before prediction. This works because label decisions benefit from future context (e.g., "Bank of America" requires seeing "America" to tag "Bank" correctly). Evidence shows ablation improves F1 by +1.16 points when adding BiLSTM (89.67 → 90.83).

### Mechanism 3: CRF for Structured Label Consistency
CRF layer enforces global label coherence by learning transition constraints between adjacent tags. CRF adds learnable transition scores T[y_i, y_{i+1}] to emission scores. During training, log-likelihood of correct sequence is maximized. During inference, Viterbi decoding finds the globally optimal path. This works because tag transitions have constraints (e.g., I-PER cannot follow I-ORG); modeling these explicitly reduces invalid sequences. Evidence shows CRF contributes +0.35 F1 points (90.83 → 91.18) and prevents impossible tag transitions.

## Foundational Learning

- **Concept: Conditional Random Fields (CRFs)**
  - **Why needed here:** CRFs model sequence-level decisions rather than independent per-token classifications. Understanding log-likelihood, Viterbi decoding, and transition matrices is essential for debugging structured prediction.
  - **Quick check question:** Can you explain why P(y|x) requires computing a partition function over all possible sequences?

- **Concept: Bidirectional RNNs and hidden state concatenation**
  - **Why needed here:** The architecture relies on combining forward and backward passes. Misunderstanding here leads to implementation errors in dimensionality and state management.
  - **Quick check question:** If BiLSTM hidden_dim=200, what is the dimension of the concatenated output at each timestep?

- **Concept: 1D Convolution and Max-Pooling for sequences**
  - **Why needed here:** Character CNN uses conv1d to extract n-gram features. Incorrect kernel size, padding, or pooling will break representation quality.
  - **Quick check question:** For word length m and kernel size k, how many convolution windows are produced before pooling?

## Architecture Onboarding

- **Component map:** Input tokens → [Character Embedding → CNN → MaxPool] → Concat → BiLSTM → Linear → CRF → Tags

- **Critical path:** Character CNN output must correctly concatenate with word embeddings before BiLSTM. Dimension mismatch here cascades through the entire model. CRF transition matrix initialization affects early training stability.

- **Design tradeoffs:**
  - Character vs. subword tokenization: Character CNN is language-agnostic but computationally heavier; BPE/SentencePiece may be more efficient for known vocabularies
  - SGD vs. Adam: Paper uses SGD with momentum (lr=0.015). Adam may converge faster but requires different hyperparameter tuning
  - BIO vs. BIOES: BIOES adds explicit boundary markers, improving multi-token entity detection but increasing tag set size

- **Failure signatures:**
  - F1 stuck ~85-86: Character CNN likely not learning (check embedding initialization, gradient flow)
  - Invalid tag transitions at inference: CRF not enforcing constraints (verify Viterbi implementation)
  - Slow convergence with large loss variance: Check gradient clipping (should be 5.0) and learning rate

- **First 3 experiments:**
  1. Baseline ablation: Run with word embeddings only, then add character CNN. Expect ~4 F1 gap per ablation table.
  2. Hyperparameter sensitivity: Sweep learning rate [0.01, 0.015, 0.02] and dropout [0.3, 0.5, 0.7] on dev set.
  3. Tag scheme comparison: Train with BIO vs. BIOES on same data; measure entity boundary accuracy difference.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the integration of modern contextualized embeddings (e.g., BERT or RoBERTa) render the character-level CNN component redundant?
- **Basis in paper:** [explicit] Section 7.3 and the Conclusion explicitly identify "integrating modern contextualized embeddings" as a direction for future work.
- **Why unresolved:** This study strictly reproduced the 2016 architecture using static GloVe embeddings to verify original results. It did not test how the character-level morphological feature extraction interacts with, or is superseded by, transformer-based subword tokenization.
- **What evidence would resolve it:** An ablation study on CoNLL-2003 replacing GloVe with BERT embeddings, comparing performance with and without the character-level CNN module active.

### Open Question 2
- **Question:** To what extent does the choice of optimizer (SGD vs. AdamW) impact the reproducibility difficulty and hyperparameter sensitivity noted by the authors?
- **Basis in paper:** [explicit] Section 8 states the "model exhibits sensitivity to learning rate and dropout settings," and Section 4.4 confirms the use of SGD.
- **Why unresolved:** The authors adhered to the original 2016 optimization settings (SGD with momentum) to ensure a fair comparison. It remains unknown if modern adaptive optimizers would resolve the sensitivity issues or alter the final performance ceiling.
- **What evidence would resolve it:** A comparative training run using AdamW or Adam to measure variance in final F1-scores and convergence speed against the SGD baseline.

### Open Question 3
- **Question:** How does the implementation of dynamic batching (necessitated by missing details in the original paper) affect gradient estimation compared to the original methodology?
- **Basis in paper:** [inferred] Section 6.1 notes the original paper "does not specify batching details," forcing the authors to implement dynamic batching with padding.
- **Why unresolved:** Differences in padding tokens and batch composition can subtly alter gradient updates. The authors assume their implementation is functionally equivalent, but this is not strictly verified.
- **What evidence would resolve it:** Analysis of training dynamics, specifically comparing gradient variance and loss curves between dynamic batching and a controlled static batching approach.

## Limitations
- Missing learning rate schedule specification may affect convergence patterns and final performance
- Character CNN effectiveness on morphologically-rich languages and non-English datasets remains untested
- Modern transformer-based tokenizers may provide better morphological feature extraction than character CNNs

## Confidence
- **High confidence:** The BiLSTM-CNN-CRF architecture is fundamentally sound and the reproduction achieved nearly identical performance (91.18% vs 91.21% F1)
- **Medium confidence:** While the reproduction matches original results on benchmark datasets, the lack of specification on learning rate scheduling and weight initialization means exact replication requires empirical tuning
- **Low confidence:** Claims about the architecture's effectiveness on non-English or morphologically-rich languages lack direct evidence

## Next Checks
1. **Learning rate schedule validation:** Run controlled experiments with constant vs. decaying learning rate schedules to quantify impact on convergence speed and final F1 score
2. **Cross-lingual transfer evaluation:** Train the architecture on CoNLL-2003 Spanish or German datasets to test generalization to morphologically-rich languages
3. **Modern baseline comparison:** Replace the character CNN with a small transformer encoder and measure performance trade-offs in accuracy vs. computational efficiency