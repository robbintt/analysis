---
ver: rpa2
title: 'MULTIAQUA: A multimodal maritime dataset and robust training strategies for
  multimodal semantic segmentation'
arxiv_id: '2512.17450'
source_url: https://arxiv.org/abs/2512.17450
tags:
- data
- dataset
- images
- modalities
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTIAQUA, a novel multimodal maritime dataset
  containing synchronized RGB, thermal, infrared, LIDAR, radar, and GPS/IMU data with
  pixel-level annotations. The dataset addresses the challenge of scene interpretation
  under difficult visual conditions, such as poor lighting and sun glare, which are
  common in maritime environments.
---

# MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation

## Quick Facts
- **arXiv ID**: 2512.17450
- **Source URL**: https://arxiv.org/abs/2512.17450
- **Reference count**: 40
- **Primary result**: Introduces MULTIAQUA dataset and achieves 72.24 mIoU on nighttime test data versus 50.52 baseline

## Executive Summary
This paper addresses the challenge of semantic segmentation in maritime environments under poor lighting conditions by introducing MULTIAQUA, a novel multimodal dataset containing synchronized RGB, thermal, infrared, LIDAR, radar, and GPS/IMU data with pixel-level annotations. The authors propose a robust training strategy that enables effective nighttime performance by training only on daytime data. Their approach uses a double forward pass during training—one with all modalities and another with RGB data zeroed out—to force the network to rely on auxiliary modalities. They also introduce modality-specific decoder heads to improve feature extraction from individual sensors. Experimental results demonstrate significant improvements in nighttime performance, with the CMNeXt model achieving 72.24 mIoU on nighttime test data compared to 50.52 with baseline methods.

## Method Summary
The method builds upon the CMNeXt architecture with a MiT-B4 backbone, introducing two key modifications: a double forward pass training strategy and modality-specific decoder heads. During training, the network performs two forward passes per iteration—the first with all available modalities (RGB, thermal, LIDAR) and the second with RGB input zeroed out. This forces the model to learn robust feature extraction from non-RGB modalities. Additionally, separate decoder heads are added for RGB and auxiliary modalities to improve individual feature quality before fusion. The final loss combines outputs from the main fused head, RGB head, and auxiliary head across both forward passes. The model is trained on daytime data from the MULTIAQUA dataset and evaluated on nighttime test sets to demonstrate zero-shot domain adaptation.

## Key Results
- CMNeXt with proposed modifications achieves 72.24 mIoU on nighttime test data versus 50.52 with baseline methods
- The approach demonstrates effective zero-shot domain adaptation from daytime to nighttime without requiring annotated nighttime data
- Method generalizes well to other multimodal datasets, showing consistent improvements across different domains and conditions
- Modality-specific decoder heads contribute to improved dynamic obstacle detection performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A double forward pass during training forces the network to learn robust feature extraction from non-RGB modalities.
- Mechanism: The first forward pass uses all available data (RGB, thermal, LIDAR), training the network for standard daytime conditions. The second forward pass, performed on the same data, zeroes out the RGB input. This compels the network to rely solely on auxiliary modalities to minimize loss, preventing it from becoming overly dependent on high-quality RGB data.
- Core assumption: The auxiliary modalities (thermal, LIDAR) contain sufficient information to perform segmentation even when RGB is absent.
- Evidence anchors:
  - [abstract]: "...training only on daytime data. Their approach uses a double forward pass during training—one with all modalities and another with RGB data zeroed out—to encourage the network to rely on auxiliary modalities."
  - [section V-A.1]: "In order to force the architecture to exploit additional modalities... we propose to train the network using two forward passes in each iteration."
  - [corpus]: Weak direct evidence for this specific double-pass mechanism; related works like "Lightweight Multimodal Artificial Intelligence Framework..." discuss multimodal fusion generally, but not this specific training strategy.
- Break condition: The performance gain at night fails to materialize, or daytime performance degrades severely, suggesting the network cannot reconcile the two optimization paths.

### Mechanism 2
- Claim: Modality-specific decoder heads improve feature quality by decoupling the learning process for each sensor type.
- Mechanism: Adding separate decoder heads for RGB and auxiliary modalities creates an explicit, separate loss signal for the features extracted from each. This forces each encoder branch to learn high-quality, discriminative features on its own merits before they are fused, making the final fusion step more of a selection task than a rescue mission for weak features.
- Core assumption: The network's fusion module will successfully learn to select the better features from the now-stronger independent branches.
- Evidence anchors:
  - [abstract]: "They also introduce modality-specific decoder heads to improve feature extraction from individual modalities."
  - [section V-A.2]: "These heads are guided towards producing good predictions on their own, thereby improving the performance of the network by decoupling the feature extraction and fusion parts..."
  - [corpus]: No direct evidence. Assumption: This builds on general principles of auxiliary tasks in multi-task learning.
- Break condition: The performance of the full model is no better than the best-performing single modality head, indicating fusion is failing or unnecessary.

### Mechanism 3
- Claim: Training on daytime data with robust training strategies generalizes to the nighttime domain without requiring annotated nighttime samples.
- Mechanism: By using sensors like thermal cameras and LIDAR—which are largely invariant to ambient light—the model learns a semantic representation of the scene (e.g., "obstacle," "water") based on physical properties (heat, depth) rather than visible-light textures. This representation is stable across the daytime-to-nighttime domain shift.
- Core assumption: The geometric and thermal properties of objects are consistent between day and night, and the model can learn to associate daytime RGB textures with these stable cross-modal features.
- Evidence anchors:
  - [abstract]: "...training only on daytime data... enables robust performance in low-light conditions"
  - [section V-A]: "The design of our method is similar to what the authors of HeatNet [59] proposed... we choose to train our model in an end-to-end manner and guide the training process..."
  - [corpus]: The related paper "Lightweight Multimodal Artificial Intelligence Framework for Maritime Multi-Scene Recognition" supports the challenge of environmental interference in maritime domains, justifying the need for robust multimodal solutions.
- Break condition: The model shows strong daytime performance but catastrophic failure on nighttime data, indicating the auxiliary modalities are not providing sufficient cues for scene understanding or the model has overfit to daytime-specific correlations.

## Foundational Learning

- **Concept**: Sensor Fusion
  - Why needed here: The core problem is that RGB cameras fail in low light. The proposed solution's entire value proposition is built on fusing data from other sensors to compensate.
  - Quick check question: Can a sensor fusion system still produce a valid output if one of its input sensors fails completely? (Answer: Yes, if it's designed for robustness, as this paper aims to achieve.)

- **Concept**: Semantic Segmentation
  - Why needed here: This is the specific task the paper addresses. The "ground truth" annotations are pixel-level labels, and mIoU (mean Intersection over Union) is the primary evaluation metric.
  - Quick check question: What is the fundamental output of a semantic segmentation model? (Answer: A map of the same height and width as the input image, where each pixel value represents a class label.)

- **Concept**: Domain Adaptation
  - Why needed here: The paper's central claim is a training strategy for domain adaptation (day → night) without requiring labels from the target domain.
  - Quick check question: In this paper, what represents the source domain and what represents the target domain? (Answer: Source domain = daytime data with labels. Target domain = nighttime data without labels.)

## Architecture Onboarding

- **Component map**: Input Layer (RGB, Thermal, LIDAR) -> Encoder (MiT-B4 backbone) -> Fusion Module (Self-Query Hub) -> Decoder Heads (Main fused head, RGB head, Auxiliary head) -> Output

- **Critical path**:
  1. Data Preparation: Align and calibrate all sensor streams to a common reference frame (e.g., the RGB camera's plane).
  2. Forward Pass 1: Pass all modalities through the network. Compute loss from the joint, RGB, and auxiliary heads.
  3. Forward Pass 2: Zero out the RGB input and pass data through the network again. Compute loss from the joint and auxiliary heads.
  4. Optimization: Backpropagate the combined loss to update network weights.

- **Design tradeoffs**:
  - **Robustness vs. Simplicity**: The double-forward-pass training is more computationally expensive per iteration but eliminates the need to collect and annotate expensive nighttime data.
  - **Performance vs. Complexity**: Adding modality-specific heads increases the number of parameters and GFLOPs but is shown to improve performance on key metrics like dynamic obstacle detection.

- **Failure signatures**:
  - **Catastrophic Drop at Night**: The model works well during the day but produces random output at night. The training strategy failed to force reliance on auxiliary modalities.
  - **Daytime Performance Degradation**: The modifications for night robustness hurt daytime accuracy too much, suggesting the auxiliary modality features are conflicting with RGB features during fusion.

- **First 3 experiments**:
  1. **Baseline Performance**: Train CMNeXt on the MULTIAQUA daytime split without any modifications (no double pass, no extra heads). Evaluate on both daytime validation and nighttime test sets to quantify the domain gap.
  2. **Ablation on Double Forward Pass**: Train CMNeXt with only the double forward pass strategy enabled. Compare results to the baseline to isolate the impact of this specific technique.
  3. **Ablation on Modality-Specific Heads**: Train CMNeXt with only the extra decoder heads enabled (single forward pass). Compare to baseline and double-pass results to understand their individual contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would employing complex data degradation schemes (e.g., simulating fog, noise, or rain) outperform the simple RGB zeroing strategy used in the double forward pass?
- Basis in paper: [explicit] The authors state in the Conclusion that their method "could be improved by more precisely modeling possible data degradations by employing different data degradation schemes."
- Why unresolved: The current study only validates the extreme case of setting RGB input to zero to simulate sensor failure or total darkness.
- What evidence would resolve it: A comparative ablation study on MULTIAQUA using varied degradation techniques (such as GAN-based noise injection or random masking) versus the zeroing approach.

### Open Question 2
- Question: Does the domain adaptation training strategy generalize effectively to other adverse weather conditions like fog, rain, or snow without requiring labeled data for those specific domains?
- Basis in paper: [explicit] The authors posit in the Conclusion that "this approach can be generalized to other difficult situations, such as fog, rain and snow," provided some sensors remain unaffected.
- Why unresolved: The paper empirically validates the method only on the day-to-night domain gap using the MULTIAQUA dataset.
- What evidence would resolve it: Evaluation of the trained models on maritime or automotive datasets featuring adverse weather (e.g., foggy scenes) to verify zero-shot performance.

### Open Question 3
- Question: Can an explicit data quality detection module improve fusion performance by dynamically weighing modalities based on their estimated utility?
- Basis in paper: [explicit] The Conclusion suggests future work could include "a module for data quality detection on each modality branch to predict how useful a modality is for final prediction."
- Why unresolved: The current fusion mechanism relies on implicit learning, which sometimes results in performance drops when noisy RGB data is included (noted in the ablation study discussion).
- What evidence would resolve it: Architecture modifications incorporating a quality estimator, demonstrating that explicitly gating noisy inputs yields higher mIoU than the current baseline.

## Limitations

- **Dataset Size and Generalization**: While MULTIAQUA provides diverse sensor data, the total number of frames and class diversity may limit the model's ability to generalize to all maritime scenarios.
- **Training Complexity**: The double forward pass strategy doubles the computational cost per training iteration, which may be prohibitive for large-scale deployment or real-time applications.
- **Sensor Dependency**: The approach relies on the availability and quality of auxiliary sensors (thermal, LIDAR). In scenarios where these sensors are degraded or unavailable, the robustness gains may diminish significantly.

## Confidence

- **High Confidence**: The dataset creation process, the core architectural modifications (modality-specific heads), and the general training strategy are well-defined and reproducible.
- **Medium Confidence**: The quantitative results showing improved nighttime performance are convincing, but the ablation studies and comparisons to state-of-the-art are less rigorous.
- **Low Confidence**: The qualitative analysis is limited to a few examples, and the paper does not provide a detailed error analysis to explain why the model fails in certain nighttime scenarios.

## Next Checks

1. **Ablation on Sensor Quality**: Conduct experiments where thermal or LIDAR data is artificially degraded (e.g., added noise, reduced resolution) to quantify the model's robustness to individual sensor failures.
2. **Cross-Dataset Evaluation**: Test the trained model on a different, publicly available multimodal maritime dataset (if one exists) to assess true generalization beyond MULTIAQUA.
3. **Daytime Performance Impact**: Perform a detailed analysis of how the nighttime-focused training strategy (double pass, auxiliary heads) affects daytime performance metrics. Identify if there is a significant trade-off.