---
ver: rpa2
title: 'From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning'
arxiv_id: '2501.11877'
source_url: https://arxiv.org/abs/2501.11877
tags:
- aggregation
- proposals
- arxiv
- https
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Aggregation Fine-Tuning (AFT), a novel supervised
  fine-tuning paradigm that trains models to synthesize multiple draft responses (proposals)
  into a single refined answer (aggregation). At inference, a propose-and-aggregate
  strategy iteratively generates and refines proposals to improve performance.
---

# From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning

## Quick Facts
- arXiv ID: 2501.11877
- Source URL: https://arxiv.org/abs/2501.11877
- Authors: Yafu Li; Zhilin Wang; Tingchen Fu; Ganqu Cui; Sen Yang; Yu Cheng
- Reference count: 40
- One-line primary result: A fine-tuned Llama3.1-8B-Base model achieves 41.3% LC win rate on AlpacaEval 2, surpassing much larger models like Llama3.1-405B-Instruct and GPT-4.

## Executive Summary
This paper introduces Aggregation Fine-Tuning (AFT), a novel supervised fine-tuning paradigm that trains models to synthesize multiple draft responses (proposals) into a single refined answer (aggregation). At inference, a propose-and-aggregate strategy iteratively generates and refines proposals to improve performance. Experiments show AFT significantly outperforms standard SFT, with a fine-tuned Llama3.1-8B-Base model achieving a 41.3% LC win rate on AlpacaEval 2, surpassing much larger models like Llama3.1-405B-Instruct and GPT-4. The method effectively combines sequential revision and parallel sampling to scale inference-time computation, demonstrating that AFT unlocks additional LLM capabilities without increasing model size or data volume.

## Method Summary
AFT trains models to predict a final aggregation given a query and multiple draft proposals, reducing prediction uncertainty compared to standard SFT. The propose-and-aggregate inference framework iteratively generates K proposals in parallel, then aggregates them into refined responses over L layers. This combines the exploration of parallel sampling with the iterative improvement of sequential revision. The method can use on-policy proposals (sampled from the model being trained) or off-policy proposals (from existing datasets), with on-policy showing better performance due to distribution alignment.

## Key Results
- A fine-tuned Llama3.1-8B-Base model achieves 41.3% LC win rate on AlpacaEval 2, surpassing Llama3.1-405B-Instruct and GPT-4
- AFT significantly outperforms standard SFT across multiple benchmarks (GSM8K, IFEval, MT-Bench)
- Propose-and-aggregate framework effectively combines sequential refinement and parallel sampling for flexible test-time scaling
- On-policy proposals yield stronger performance than off-policy (41.3% vs 40.3% LC win rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning on proposals reduces prediction uncertainty, enabling faster and more stable training convergence.
- Mechanism: AFT trains the model to predict the final aggregation given both the query and draft proposals. This reduces the token-level perplexity compared to standard SFT (which predicts responses from query alone), because the model receives partial solutions and semantic cues before predicting. The paper describes this as "mode-seeking" behavior—probability mass accumulates on high-reward responses more efficiently.
- Core assumption: The relationship between proposals and the target aggregation must be learnable; proposals must contain signal relevant to the final answer.
- Evidence anchors:
  - [abstract] "analytic findings reveal that by fitting answers conditioned on drafts, aggregation learning benefits from shaping a low-perplexity region"
  - [section 6.1] Figure 6 shows AFT data has significantly lower perplexity than SFT data before fine-tuning; training curves show faster convergence with minimal fluctuations
  - [corpus] Weak direct support—no corpus papers directly address proposal-conditioned perplexity reduction
- Break condition: If proposals are noisy or systematically misleading, conditioning on them may introduce bias rather than reduce uncertainty.

### Mechanism 2
- Claim: Iterative propose-and-aggregate at inference unifies sequential revision (depth) with parallel sampling (breadth) for flexible test-time scaling.
- Mechanism: At each aggregation layer, the model generates K proposals in parallel (breadth), then aggregates them into refined responses that become proposals for the next layer (depth). This combines the exploration of parallel sampling with the iterative improvement of sequential revision without requiring an external reward model.
- Core assumption: The model has learned a reliable aggregation capability during AFT; SFT models cannot perform this effectively (confirmed in Section 5.1).
- Evidence anchors:
  - [abstract] "By combining sequential refinement and parallel sampling, the propose-and-aggregate framework scales inference-time computation in a flexible manner"
  - [section 6.3] Figure 8 shows performance improves with both more proposals (width) and more aggregation layers (depth); Section 3.3 and Figure 3 explicitly frame this as unifying sequential revision and parallel sampling
  - [corpus] Weak support—corpus papers touch on parallel scaling and aggregation but not this specific unified framework
- Break condition: If aggregation quality degrades across layers (error accumulation), additional depth harms rather than helps.

### Mechanism 3
- Claim: On-policy proposals—sampled from the model being trained—yield stronger aggregation performance than off-policy proposals.
- Mechanism: On-policy proposals position the model within its own "comfortable region" of its output distribution. Since the model must learn to aggregate its own generations, the training task aligns with what it will encounter at inference. Off-policy proposals (from other models) may be out-of-distribution relative to the model's native generation patterns.
- Core assumption: The model's own proposals, while potentially lower quality than those from stronger models, provide better training signal because they match the inference distribution.
- Evidence anchors:
  - [abstract] AFT-on-policy achieves 41.3% LC win rate vs. 40.3% for AFT-off-policy (Llama3.1-8B-Base with aggregation)
  - [section 5.1] "AFT-on-policy obtains stronger performance compared with AFT-off-policy" attributed to learning within a low-perplexity region aligned with model capabilities
  - [section 6.1] Right panel of Figure 6 shows on-policy AFT has lower perplexity than off-policy; Table 4 shows ablations removing proposals or using pseudo-aggregations degrade performance
  - [corpus] No direct corpus support for on-policy vs. off-policy proposal comparison
- Break condition: If the base model's proposals are extremely low quality, the aggregation target may be unattainable, creating a train-inference gap.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) Basics**
  - Why needed here: AFT is presented as a modification to standard SFT. Understanding the baseline query→response mapping helps contrast it with query+proposals→aggregation.
  - Quick check question: Can you explain why standard SFT trains `P(response|query)` while AFT trains `P(aggregation|query, proposals)`?

- **Inference-Time Compute Scaling Paradigms**
  - Why needed here: The paper positions AFT within the broader landscape of test-time scaling methods (sequential revision, parallel sampling, graph search). Understanding these paradigms clarifies what propose-and-aggregate unifies.
  - Quick check question: What is the difference between sequential revision and parallel sampling as test-time scaling strategies, and which component of propose-and-aggregate corresponds to each?

- **Perplexity as a Training Signal Quality Metric**
  - Why needed here: The paper uses perplexity to explain why AFT converges faster and more stably. Understanding perplexity as a measure of prediction uncertainty is essential for interpreting Figure 6.
  - Quick check question: If a model has lower perplexity on a training dataset, what does that suggest about the alignment between that data and the model's current distribution?

## Architecture Onboarding

- **Component map:**
  1. **Data Construction Pipeline**
     - Proposal collection: Off-policy (existing datasets like UltraFeedback) or on-policy (ICL sampling from base model with temperature=1.0, top-k=50, top-p=0.95)
     - Reward model selection: Used to select top-K proposals from on-policy samples (e.g., FsfairX-LLaMA3-RM-v0.1)
     - Aggregation generation: Strong teacher model (Qwen2.5-72B-Instruct) synthesizes proposals into reference aggregation
  2. **Training Module**
     - Input: Query + K proposals (formatted with MoA-style prompt)
     - Target: Aggregated response
     - Standard next-token prediction loss; LoRA fine-tuning (rank=8)
  3. **Inference Module**
     - Proposal generator: Sample K responses from AFT model
     - Aggregator: Same AFT model takes query + proposals → generates refined response
     - Iterative loop: Aggregations become proposals for next layer (L layers total)

- **Critical path:**
  1. Data quality determines aggregation learning ceiling—poor proposals or inconsistent aggregations will not teach reliable synthesis
  2. Training must actually learn aggregation behavior, not just memorize responses (ablations in Table 4 show removing proposals degrades performance)
  3. Inference requires the trained model to both generate diverse proposals AND aggregate them effectively—these are coupled capabilities

- **Design tradeoffs:**
  - On-policy vs. Off-policy: On-policy yields better performance but requires generating proposals from the base model (higher upfront cost); off-policy uses existing data but may have distribution mismatch
  - Number of proposals (K): More proposals increase aggregation quality (Figure 8) but linearly increase compute
  - Aggregation layers (L): More layers improve quality but multiply FLOPs (approximately `L × 2 × N × F + F` vs. `2 × N × F` for Best-of-N)
  - Proposal diversity vs. quality: Both matter; quality has stronger effect (Section 6.2, Figure 7)

- **Failure signatures:**
  - SFT model used with propose-and-aggregate at inference → no improvement (Table 5, Figure 5); aggregation capability must be learned during training
  - Training with pseudo-aggregations (responses not derived from proposals) → degraded performance (Table 4); model learns spurious patterns
  - Low-diversity proposals → limited aggregation benefit (Figure 7); aggregation has little signal to synthesize
  - Excessive aggregation layers without monitoring → potential error accumulation or diminishing returns

- **First 3 experiments:**
  1. **Replicate data construction ablation:** Train with proposals removed from input (predict aggregation from query only) and with pseudo-aggregations (reference responses not synthesized from proposals). Compare to full AFT to validate that both the proposal conditioning AND the proposal-aggregation relationship are necessary.
  2. **Scale test-time compute along width and depth:** For a fixed AFT model, sweep K ∈ {1, 3, 5} and L ∈ {0, 1, 2, 3} on GSM8K and IFEval. Plot performance vs. FLOPs to identify compute-optimal operating points and confirm the width/depth tradeoff in Figure 8.
  3. **Compare on-policy vs. off-policy with matched proposal quality:** Use a reward model to filter off-policy proposals to match the quality distribution of on-policy proposals. Test whether the on-policy advantage persists, isolating whether the benefit comes from proposal quality or distribution alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Aggregation Fine-Tuning (AFT) framework be effectively adapted for tasks requiring short or symbolic outputs (e.g., multiple-choice QA) where textual aggregation is non-trivial?
- Basis in paper: [inferred] Section 5.2 states that the propose-and-aggregate method was not applied to tasks like MMLU or ARC-c because of the "limited information" provided in answers (single-choice options or yes/no responses).
- Why unresolved: The current method relies on synthesizing verbose text drafts; the mechanism for aggregating succinct, symbolic answers into a "refined" answer without simply voting is undefined in the paper.
- What evidence would resolve it: An extension of AFT methodologies tailored for classification or symbolic reasoning benchmarks, demonstrating performance gains over standard SFT on datasets like MMLU.

### Open Question 2
- Question: To what extent does AFT performance depend on the capabilities of the stronger "teacher" model used to generate the ground truth aggregations?
- Basis in paper: [inferred] Section 3.1 ("Aggregation Construction") specifies using a stronger model (Qwen2.5-72B-Instruct) to create ground truth, but it does not ablate the impact of teacher model size or quality.
- Why unresolved: It is unclear if the success is due to the student model learning a generalizable aggregation skill or if it is primarily distilling the specific capabilities of the much larger teacher model used for labeling.
- What evidence would resolve it: Ablation studies training AFT models using aggregations generated by smaller or identical-sized models, compared against the current strong teacher setup.

### Open Question 3
- Question: What is the compute-optimal trade-off between increasing inference-time compute (layers/proposals) versus increasing model parameters?
- Basis in paper: [inferred] Section 6.3 demonstrates performance scaling with width and depth, and Section 6.4 approximates FLOPs, but the paper does not establish a theoretical or empirical "compute optimality" curve.
- Why unresolved: While the paper shows small models can beat large ones, it does not define the precise threshold where spending FLOPs on AFT inference becomes less efficient than simply training a larger model.
- What evidence would resolve it: An empirical analysis comparing performance-per-FLOP for parameter scaling versus AFT inference scaling across a fixed set of computational budgets.

## Limitations
- The on-policy vs. off-policy proposal advantage is not definitively explained—it could be due to proposal quality, diversity, or distribution alignment, and the paper doesn't fully isolate these factors.
- The aggregation learning mechanism is inferred from perplexity reduction and performance gains, but direct evidence that the model learns to synthesize (not memorize) is limited to ablations that remove proposals or use pseudo-aggregations.
- The propose-and-aggregate framework's compute efficiency is stated but not benchmarked against alternatives like Best-of-N or beam search with comparable FLOPs.

## Confidence
- **High confidence:** AFT outperforms standard SFT and achieves state-of-the-art results on AlpacaEval 2; propose-and-aggregate effectively combines sequential and parallel scaling.
- **Medium confidence:** On-policy proposals are superior to off-policy; perplexity reduction explains faster convergence; aggregation layers improve performance with diminishing returns.
- **Low confidence:** The exact mechanism by which conditioning on proposals reduces uncertainty; whether aggregation is truly learned or relies on memorization; generalizability to other domains or model families.

## Next Checks
1. **Isolate proposal quality vs. distribution alignment:** Generate on-policy and off-policy proposals with matched quality (via reward model), then compare AFT-on-policy vs. AFT-off-policy to determine if the advantage is due to proposal distribution or inherent quality.
2. **Validate aggregation learning mechanism:** Train an AFT model with proposals in input but no proposal-aggregation relationship (e.g., random or pseudo-aggregations), and compare to full AFT to confirm the model learns synthesis, not memorization.
3. **Benchmark compute efficiency:** For equivalent FLOPs, compare propose-and-aggregate (sweeping K and L) against Best-of-N and beam search on GSM8K and IFEval to quantify whether AFT delivers better performance per compute.