---
ver: rpa2
title: 'Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic
  Impurities'
arxiv_id: '2501.09579'
source_url: https://arxiv.org/abs/2501.09579
tags:
- water
- surface
- data
- anomaly
- coreset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of anomaly detection in industrial
  surface inspection, where surface impurities like water stains and fingerprints
  degrade the performance of automated visual inspection systems. The authors propose
  a novel procedural method to generate synthetic water stains using Perlin noise
  and jittered sampling, enabling the creation of photorealistic synthetic datasets
  that incorporate impurities.
---

# Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities

## Quick Facts
- **arXiv ID:** 2501.09579
- **Source URL:** https://arxiv.org/abs/2501.09579
- **Reference count:** 40
- **Key outcome:** Introduces Sequential PatchCore for memory-efficient anomaly detection and procedural synthetic impurities for robust training.

## Executive Summary
This paper addresses the challenge of detecting anomalies on industrial surfaces contaminated with impurities like water stains and fingerprints. It proposes a procedural method using Perlin noise to generate synthetic water stains, enabling creation of photorealistic training datasets. The authors also introduce Sequential PatchCore, a memory-efficient variant of PatchCore that builds coresets sequentially, allowing high-resolution image training on consumer hardware. Experiments show that incorporating synthetic impurities improves robustness against false positives from non-defect anomalies, and fine-tuning on real data significantly enhances overall performance.

## Method Summary
The method combines procedural synthetic impurity generation with Sequential PatchCore anomaly detection. Synthetic water stains are created using Perlin noise and jittered sampling to simulate natural irregularities. Sequential PatchCore reformulates the coreset building process from global reduction to sequential accumulation, drastically reducing memory requirements. Transfer learning is enabled by pre-training on synthetic datasets and fine-tuning on real data through coreset melding. The approach uses MobileNetV3 as a frozen feature extractor and employs a distance-based replacement strategy to maintain coreset quality while minimizing memory usage.

## Key Results
- Sequential PatchCore reduces memory requirements from O(NP) to O(|M|) while maintaining competitive F1 scores.
- Models trained with synthetic water stains show improved robustness against false positives from common impurities.
- Fine-tuning on real data significantly improves precision and overall performance compared to synthetic-only training.
- Defect-wise recall metric provides better assessment of industrial anomaly detection performance.

## Why This Works (Mechanism)

### Mechanism 1: Sequential Coreset Construction for Memory Efficiency
Reformulating coreset building from global reduction to sequential accumulation drastically lowers memory requirements. The algorithm initializes an empty coreset and iteratively adds features, replacing existing members only when new candidates expand coverage. This prioritizes features that increase nominal zone coverage without storing the full dataset in RAM.

### Mechanism 2: Procedural Impurity Synthesis via Perlin Noise
Modeling surface impurities using Perlin noise creates photorealistic training data that reduces model sensitivity to non-defect anomalies. The procedural approach generates irregular, natural-looking boundaries for water stains, forcing the anomaly detector to learn feature representations of "messy but nominal" surfaces.

### Mechanism 3: Coreset Melding for Transfer Learning
Combining coresets from different domains via distance-based melding acts as efficient transfer learning. A pre-trained coreset (e.g., from synthetic data) is updated with patches from real data, adding only features that expand the nominal zone. This filters redundant information while preserving rare, critical features from both domains.

## Foundational Learning

- **Explicit vs. Implicit Coresets:** The paper modifies how the memory bank is built using a non-parametric approach (storing actual feature vectors) rather than a parametric one (learning a distribution boundary). *Quick check:* Does Sequential PatchCore learn a probability distribution of normal data, or store a subset of raw feature vectors? (Answer: It stores a subset of raw vectors).

- **Feature Space Geometry & Nearest Neighbors:** The sequential logic relies entirely on Euclidean distances between patch features. "Normality" is defined as proximity to this memory bank. *Quick check:* If a new patch has a larger distance to the coreset than the closest pair within the coreset, does it get added or rejected? (Answer: It replaces one of the close pair to expand the zone).

- **Procedural Texture Generation (Perlin Noise):** The impurity mechanism depends on generating controlled randomness. Perlin noise provides "smooth" randomness suitable for natural phenomena like water stains, unlike random static noise. *Quick check:* Why use Perlin noise instead of just cutting and pasting images of water stains? (Answer: It allows for infinite variation and precise control over density/shape via parameters, ensuring dataset diversity).

## Architecture Onboarding

- **Component map:** Feature Extractor (MobileNetV3) -> Coreset Builder (Sequential) -> Synthesizer (Offline Perlin noise) -> Inference Engine
- **Critical path:** The Distance Comparison Logic within the Coreset Builder. The condition `if d_p > d_m: replace coreset point` determines the coverage of the "nominal zone."
- **Design tradeoffs:** Sequential PatchCore trades single-pass high-memory usage for multi-pass low-memory usage, saving RAM but increasing training time. Training on impurities lowers defect detection slightly (false negatives) but improves robustness against false positives on water stains.
- **Failure signatures:** Runaway memory if chunk size is too high during inference, stagnant coreset if learning rate or noise parameters are wrong, domain mismatch if synthetic lighting differs from real lighting.
- **First 3 experiments:** (1) Memory benchmark: Train standard vs. Sequential PatchCore on same high-res images, plot RAM usage and F1 score. (2) Impurity ablation: Train models on Real Data only, Synthetic Clean, and Synthetic + Water Stains, test on real water stains. (3) Melding validation: Train coresets on Synthetic and Real separately, meld them, compare against Real-only training.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the domain gap specifically manifest within explicit coreset structures, and what mechanisms can effectively close this gap during transfer learning? The paper observes performance drops but relies on empirical finetuning rather than analyzing theoretical formation of the gap in feature space.

- **Open Question 2:** Can adapting the feature extractor to the target domain resolve the ambiguity between geometric defects (e.g., dents) and impurities (e.g., water stains)? The current methodology keeps the feature extractor fixed with ImageNet weights, adapting only the coreset memory bank.

- **Open Question 3:** Does extending the water stain model to include internal heterogeneity improve generalization compared to the current exponential decay approach? The current implementation uses a simplified uniform decay model, potentially limiting the diversity and realism of synthetic impurity training data.

## Limitations
- Synthetic impurity generation relies on Perlin noise parameters that remain unspecified, creating potential domain gap risks.
- Sequential construction's theoretical guarantee of coreset quality equivalence to global reduction is assumed but not rigorously proven.
- The defect-wise recall metric introduces a binary detection criterion that may oversimplify nuanced defect coverage assessment.

## Confidence

- **High Confidence:** Sequential PatchCore's memory efficiency improvement and core coreset building mechanism (experimental results show clear RAM reduction without significant F1 loss).
- **Medium Confidence:** Effectiveness of synthetic water stains for improving robustness against real water stains (supported by results, but parameter sensitivity unknown).
- **Medium Confidence:** Transfer learning benefit of coreset melding (empirically validated, but depends on unstated synthetic/real domain similarity).

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary Perlin noise parameters and measure resulting synthetic data's feature distance to real water stains in learned feature space. Quantify "closeness" threshold for effective transfer.

2. **Coreset Coverage Validation:** Implement diagnostic to visualize feature distribution of sequential coreset versus global coreset. Measure coverage ratio to verify sequential method doesn't sacrifice representational power.

3. **Memory vs. Accuracy Trade-off Curve:** Conduct experiments across range of coreset sizes for both standard and Sequential PatchCore. Plot memory usage against F1 score to provide hardware-constrained deployment guidelines.