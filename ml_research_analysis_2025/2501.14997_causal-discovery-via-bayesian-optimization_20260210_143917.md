---
ver: rpa2
title: Causal Discovery via Bayesian Optimization
arxiv_id: '2501.14997'
source_url: https://arxiv.org/abs/2501.14997
tags:
- data
- causal
- learning
- drbo
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DrBO, a novel Bayesian optimization (BO)-based
  method for causal discovery from purely observational data. The method addresses
  the challenge of efficiently finding high-scoring directed acyclic graphs (DAGs)
  by leveraging BO to intelligently explore the DAG space.
---

# Causal Discovery via Bayesian Optimization

## Quick Facts
- **arXiv ID:** 2501.14997
- **Source URL:** https://arxiv.org/abs/2501.14997
- **Reference count:** 40
- **Primary result:** Introduces DrBO, a Bayesian optimization method for causal discovery that outperforms state-of-the-art baselines in accuracy and efficiency.

## Executive Summary
This paper introduces DrBO, a novel Bayesian optimization (BO)-based method for causal discovery from purely observational data. The method addresses the challenge of efficiently finding high-scoring directed acyclic graphs (DAGs) by leveraging BO to intelligently explore the DAG space. Key innovations include: (1) a low-rank DAG representation that reduces the search space dimensionality, (2) dropout neural networks as a scalable surrogate model to estimate DAG scores, and (3) a continual training approach for the surrogate model. DrBO is shown to outperform state-of-the-art baselines in terms of both accuracy and computational efficiency across various settings, including linear/nonlinear data, dense graphs, high-dimensional graphs, and real-world structures. On synthetic data, DrBO achieves significantly lower Structural Hamming Distance (SHD) and higher BIC scores compared to baselines, often reaching optimal performance with fewer evaluations. On real-world structures, DrBO consistently achieves zero SHD on most datasets, demonstrating its effectiveness in practical scenarios.

## Method Summary
DrBO formulates causal discovery as a Bayesian optimization problem over the space of DAGs. The key innovation is representing DAGs in a low-rank continuous space parameterized by node potentials and an embedding matrix, transformed via a deterministic operator that guarantees acyclicity. This reduces the dimensionality of the search space from $O(d^2)$ to $d(1+k)$. A dropout neural network serves as a scalable surrogate model, trained continually with a replay buffer to estimate DAG scores efficiently. The acquisition function uses Thompson sampling on the surrogate's uncertainty estimates to propose new candidate DAGs. The method decomposes the global BIC score into node-wise local scores, enabling more efficient surrogate modeling.

## Key Results
- DrBO achieves significantly lower SHD and higher BIC scores than state-of-the-art baselines on synthetic data across linear/nonlinear, dense, and high-dimensional settings.
- On real-world structures (Sachs, BnLearn), DrBO consistently achieves zero SHD on most datasets.
- The low-rank representation with rank $k=d$ is sufficient for dense graphs, and the dropout surrogate model provides substantial computational efficiency gains over Gaussian Processes.
- DrBO reaches optimal performance with fewer evaluations than baselines, demonstrating superior sample efficiency.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping the combinatorial DAG space to a low-dimensional, unconstrained continuous space allows Bayesian Optimization (BO) to navigate the search efficiently.
- **Mechanism:** The method uses a low-rank representation (parameterized by node potentials $p$ and an embedding matrix $R$) transformed via a deterministic operator $\tau$ to generate DAGs. This reduces the search dimensionality from $O(d^2)$ to $d(1+k)$, making the acquisition function optimization tractable.
- **Core assumption:** The ground truth DAG structure lies within the expressivity of the low-rank embedding ($k \ll d$), or the restricted space still contains high-scoring approximations.
- **Evidence anchors:**
  - [abstract] Mentions "low-rank DAG representation that reduces the search space dimensionality."
  - [section 4.1] Defines $\tau(p, R) = H(\text{grad}(p)) \odot H(RR^\top)$ ensuring acyclicity and linear dimensionality.
  - [corpus] Corpus evidence for this specific low-rank parameterization is weak; neighbor papers focus on general Bayesian discovery or score consistency rather than the $\tau$ operator.
- **Break condition:** If the true causal graph is dense and full-rank, a small $k$ may restrict the search space, preventing discovery of the optimal DAG.

### Mechanism 2
- **Claim:** Dropout neural networks serve as scalable surrogate models to estimate DAG scores and uncertainty, replacing expensive Gaussian Processes (GPs).
- **Mechanism:** Instead of retraining GPs (which scale cubically), the system uses single-layer neural networks with dropout. Stochastic forward passes approximate posterior distributions, enabling Thompson sampling for acquisition without the computational bottleneck of GPs.
- **Core assumption:** Dropout variance provides a sufficiently accurate approximation of epistemic uncertainty for the BO loop.
- **Evidence anchors:**
  - [abstract] States: "replace Gaussian Processes... with dropout neural networks... allowing for... incorporation of uncertainty."
  - [section 4.3] Details the DropoutNN architecture and its interpretation as approximate Bayesian inference.
  - [corpus] Corpus support is limited; neighbor "Large-Scale Bayesian Causal Discovery" discusses scaling but not the specific dropout surrogate method used here.
- **Break condition:** If the surrogate underfits the score landscape (e.g., too few hidden units), the BO loop will suggest low-quality candidates, stalling convergence.

### Mechanism 3
- **Claim:** Decomposing the global DAG score into local node-wise components improves data efficiency for the surrogate model.
- **Mechanism:** Rather than learning a direct mapping from a complex graph structure to a scalar score, the model learns $d$ independent functions mapping parent sets to local scores (e.g., local MSE). The acquisition function aggregates these local predictions.
- **Core assumption:** The score function (like BIC) is decomposable, and local errors are independent enough to be modeled separately.
- **Evidence anchors:**
  - [abstract] Highlights "surrogate model learns the DAG score indirectly via node-wise local scores."
  - [section 4.4] Describes training $\text{DropoutNN}_i$ to predict local MSE based on parents $\text{pa}_G^{(j)}_i$.
  - [corpus] "Consistent DAG selection..." discusses BIC consistency, supporting the validity of the score, but not the decomposition mechanism specifically.
- **Break condition:** If the global score relies on complex cross-node dependencies not captured by local sums, the indirect model will misguide the search.

## Foundational Learning

- **Concept: Bayesian Optimization (BO)**
  - **Why needed here:** DrBO frames causal discovery as a black-box optimization problem over DAGs. You must understand surrogate models and acquisition functions to grasp how the method selects the next DAG to evaluate.
  - **Quick check question:** How does Thompson sampling utilize the uncertainty estimates from the surrogate model to propose the next candidate?

- **Concept: Acyclicity Constraints in Continuous Optimization**
  - **Why needed here:** The core innovation is relaxing the combinatorial DAG constraint into a continuous low-rank representation ($\tau$) that implicitly guarantees acyclicity.
  - **Quick check question:** Why is enforcing acyclicity typically difficult in continuous optimization, and how does the operator $H(\text{grad}(p))$ in the paper solve this?

- **Concept: BIC Score Decomposition**
  - **Why needed here:** The method relies on the BIC score being separable into node-wise local scores to train the surrogate models efficiently.
  - **Quick check question:** Given the BIC formula $S_{BIC} = 2 \ln p(D|\hat{\theta}, G) - |G| \ln n$, how can this be split into sums over individual nodes for an additive noise model?

## Architecture Onboarding

- **Component map:** Observational dataset $D$ -> Low-rank generator $\tau(p, R)$ creating DAG candidates $z \to G$ -> Ensemble of $d$ Dropout Neural Networks predicting local scores -> Aggregates local samples via Thompson Sampling to score candidates -> Selects top $B$ candidates; evaluates true score; updates Experience Replay buffer -> Continual training of NNs on new data + replay buffer

- **Critical path:** The "Indirect Modeling" pipeline. If the local networks $\text{DropoutNN}_i$ fail to predict $\ln \text{MSE}$ accurately for a node's parents, the aggregated acquisition score will be noisy, degrading sample efficiency.

- **Design tradeoffs:**
  - **Rank $k$:** Low $k$ restricts the graph space (risks missing true dense graphs) but drastically improves sample efficiency and runtime. High $k$ is more expressive but suffers from the curse of dimensionality.
  - **Continual Training vs. Full Retraining:** Continual training scales well (constant time per step) but risks "catastrophic forgetting" of early exploration data, mitigated here by a replay buffer.

- **Failure signatures:**
  - **Stagnating SHD:** Surrogate model is underfitting (increase hidden units or replay buffer size) or search space rank $k$ is too low.
  - **High Runtime per Step:** The number of preliminary candidates $C$ is too large for the hardware; reduce $C$ or batch size $B$.
  - **Erratic BIC trajectory:** Learning rate is too high, destabilizing the continual training of the surrogate.

- **First 3 experiments:**
  1. **Sanity Check (ER-4, $d=10$, Linear):** Verify that the implementation produces lower SHD than random search. Check if the BIC score consistently increases.
  2. **Ablation on Rank $k$:** Run DrBO on a dense graph (ER-8) with $k=2$ vs $k=8$. Confirm that higher $k$ is required to recover dense structures, verifying the tradeoff described in Appendix F.1.6.
  3. **Surrogate Scalability Test:** Compare the runtime per iteration of the Dropout NN surrogate against a standard Gaussian Process (if feasible) or against the "Direct" modeling approach to validate efficiency claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the DrBO framework be combined with active causal discovery strategies to recover causal structures more efficiently than purely observational methods?
- **Basis in paper:** [explicit] The authors state it "would be an interesting direction to combine our approach with active causal discovery methods."
- **Why unresolved:** The current study focuses exclusively on optimizing a score function derived from purely observational data.
- **What evidence would resolve it:** A hybrid algorithm demonstrating improved sample efficiency (lower SHD) by strategically incorporating interventional data compared to observational baselines.

### Open Question 2
- **Question:** How can the method be adapted to handle causal discovery problems where the assumption of causal sufficiency (no hidden confounders) is violated?
- **Basis in paper:** [explicit] The conclusion suggests the method "can be extended to solve causal discovery problems with hidden confounders, where the outputs are no longer DAGs."
- **Why unresolved:** The current formulation assumes causal sufficiency and outputs DAGs; hidden confounders typically require different graphical representations.
- **What evidence would resolve it:** A modified algorithm that successfully identifies ancestral graphs (e.g., MAGs) in systems with known latent variables.

### Open Question 3
- **Question:** Can incremental neural architecture search techniques prevent the surrogate model from underfitting during extended optimization runs?
- **Basis in paper:** [explicit] The authors note the fixed surrogate architecture "is prone underfitting at the end" and suggest architecture search as a mitigation.
- **Why unresolved:** The surrogate model's capacity is manually fixed, potentially limiting performance as the optimization landscape is explored more deeply over time.
- **What evidence would resolve it:** Ablation studies comparing the BIC score trajectories and final SHD of fixed-size versus dynamically scaling surrogate networks on high-dimensional graphs.

## Limitations

- The low-rank DAG representation (rank $k$) imposes a strong structural assumption that may restrict the search space for dense graphs, even with $k=d$.
- The method's performance on graphs with hundreds of nodes (beyond the tested $d=30$) is untested, leaving the "large-scale" applicability claim unverified.
- The continual training approach with a replay buffer mitigates catastrophic forgetting, but the optimal replay buffer size and learning rate schedule are not thoroughly explored.

## Confidence

- **High:** The paper's core claims about outperforming state-of-the-art baselines (PCD, GES, GOBI, NOTEARS) on the tested datasets (synthetic linear/nonlinear, real-world structures) are well-supported by the experimental results.
- **Medium:** The claims about the efficiency gains from the low-rank representation and dropout surrogate are supported by runtime comparisons, but the exact scaling behavior on larger, denser graphs is not fully characterized.
- **Medium:** The claim that the indirect modeling approach (decomposing the score) is superior to direct modeling is supported by ablation studies, but the comparison is limited to the specific experimental setup.

## Next Checks

1. **Dense Graph Scaling Test:** Evaluate DrBO on Erdős-Rényi graphs with edge probability $p=0.8$ and $d=50$ to test the limits of the low-rank representation and the surrogate model's scalability.
2. **Replay Buffer Ablation:** Systematically vary the replay buffer size and learning rate to find the optimal settings for the continual training approach, and measure the impact on convergence and final SHD.
3. **Direct vs. Indirect Modeling Comparison:** On a dense graph (ER-8, $d=30$), compare the performance and runtime of the indirect modeling approach (with decomposition) against a direct modeling approach that learns the global BIC score mapping without decomposition.