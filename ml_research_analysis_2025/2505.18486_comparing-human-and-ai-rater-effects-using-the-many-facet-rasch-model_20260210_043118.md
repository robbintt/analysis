---
ver: rpa2
title: Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model
arxiv_id: '2505.18486'
source_url: https://arxiv.org/abs/2505.18486
tags:
- rater
- scoring
- chatgpt
- gemini
- raters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared human and AI raters in scoring Chinese writing
  tasks using ten LLMs (ChatGPT 3.5, 4, 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5,
  1.5 Pro, 2.0, DeepSeek V3, and DeepSeek R1) against two human raters. Scoring accuracy
  was evaluated using Quadratic Weighted Kappa (QWK), intra-rater consistency using
  Cronbach Alpha, and rater effects using the Many-Facet Rasch model.
---

# Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model

## Quick Facts
- arXiv ID: 2505.18486
- Source URL: https://arxiv.org/abs/2505.18486
- Authors: Hong Jiao; Dan Song; Won-Chan Lee
- Reference count: 18
- Primary result: ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro achieved highest scoring accuracy and minimal rater effects compared to human raters

## Executive Summary
This study compared human and AI raters in scoring Chinese writing tasks using ten large language models (LLMs) and two human raters. The researchers evaluated scoring accuracy using Quadratic Weighted Kappa (QWK), intra-rater consistency using Cronbach Alpha, and rater effects using the Many-Facet Rasch model. ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro showed the highest scoring accuracy and consistency, with QWK values exceeding human-human agreement for holistic scoring. The Many-Facet Rasch model revealed minimal rater effects for AI raters, while human raters exhibited slight central tendency. These findings support using these top-performing LLMs for automated scoring in low-stakes assessments.

## Method Summary
The study scored 120 essays from 30 U.S. college students on four AP Chinese exam tasks (Story Narration and Email Response) using few-shot prompting with rubric-aligned exemplars. Ten LLMs were evaluated: ChatGPT 3.5, 4, 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, 1.5 Pro, 2.0, DeepSeek V3, and DeepSeek R1. Scoring accuracy was measured using QWK against two certified human raters, consistency was measured with Cronbach Alpha, and rater effects were analyzed using the Many-Facet Rasch model implemented in FACET software. The study also constructed ensemble models by averaging LLM scores.

## Key Results
- ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro achieved the highest QWK values (>0.85) for holistic scoring
- AI raters showed minimal rater effects in the Many-Facet Rasch model, while human raters exhibited central tendency (infit/outfit <0.7)
- No AI rater consistently outperformed others across all tasks
- Gemini 2.0 showed severe rater effect (parameter 1.25 logits) and occasional misfit
- Ensemble models occasionally improved accuracy but gains were task-dependent

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting with rubric-aligned examples enables LLMs to approximate human scoring patterns. The training protocol provides scored exemplars at each score level (0-6), rubric criteria, and explicit guidance on prioritizing Task Completion over Delivery/Language Use, anchoring the LLM's scoring distribution to human expert judgments rather than its own latent preferences.

### Mechanism 2
The Many-Facet Rasch model isolates rater severity/leniency as a stable parameter by modeling the joint contribution of student ability, task difficulty, and rater bias on observed scores. The model separates rater effect from student ability and essay difficulty, where a positive rater parameter indicates severity (lower scores after controlling for ability).

### Mechanism 3
Ensemble averaging of multiple LLM scores can improve accuracy by canceling idiosyncratic model-specific biases. The study constructs ensemble models by averaging scores and iteratively removing worst performers, though gains are task-dependent since systematic shared biases limit benefits.

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - Why needed: Primary metric for scoring accuracy; accounts for chance agreement and weights larger disagreements more heavily
  - Quick check: If QWK between AI and human is 0.85, what does this indicate about agreement beyond chance?

- **Concept: Rater effects (severity, leniency, central tendency)**
  - Why needed: Paper's core contribution is using Rasch models to detect these effects in AI raters
  - Quick check: A rater with infit mean square of 0.5 likely exhibits what pattern in their scores?

- **Concept: Many-Facet Rasch Model vs. Classical Reliability**
  - Why needed: Cronbach Alpha measures internal consistency; Rasch models partition variance sources
  - Quick check: Why might an AI rater have high Cronbach Alpha but still show significant rater severity in the FACET model?

## Architecture Onboarding

- **Component map:**
  - Input layer: Student essay text + task prompt + rubric + scored exemplars
  - LLM scoring engines: 10 models (GPT family, Claude, Gemini family, DeepSeek family)
  - Evaluation layer: QWK (accuracy), Cronbach Alpha (consistency), Many-Facet Rasch (rater effects via FACET software)
  - Aggregation layer: Ensemble models (averaging, selective pruning)

- **Critical path:**
  1. Define scoring rubric and collect benchmark human ratings
  2. Design few-shot prompt with exemplars spanning score range
  3. Score all responses with each LLM candidate
  4. Compute QWK for each LLM vs. each human rater
  5. Fit Many-Facet Rasch model; extract rater parameters
  6. If ensemble desired, iteratively prune lowest-performing models

- **Design tradeoffs:**
  - Single best model vs. ensemble: Single model simpler; ensemble adds robustness but with inconsistent gains
  - Holistic vs. analytic scoring: Analytic scores showed higher human-human QWK but lower AI-human QWK
  - Strict vs. lenient fit criteria: Paper uses 0.7-1.3 range; high-stakes applications may require 0.8-1.2

- **Failure signatures:**
  - QWK <0.7: Fails Williamson et al. (2012) threshold for acceptable agreement
  - Rater parameter |τᵢ| >0.3: Non-negligible severity/leniency
  - Infit/outfit <0.7: Central tendency (rater avoids extremes)
  - Infit/outfit >1.3: Inconsistent scoring (misfit)
  - Gemini 2.0 showed severe rater effect (1.25 logits) and occasional misfit (>1.3)

- **First 3 experiments:**
  1. Replicate QWK analysis on held-out essays to validate generalization
  2. Vary prompt design (zero-shot vs. few-shot vs. chain-of-thought) to quantify prompt sensitivity
  3. Fit Many-Facet Rasch model with partial credit specification to test rating scale structure

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the findings regarding minimal AI rater effects generalize to automated scoring in other content domains, such as English writing or STEM constructed-response items? The authors state that content was limited to AP Chinese and generalizability to other domains awaits further exploration.

- **Open Question 2:** How do advanced prompting strategies, such as Chain-of-Thought (CoT) or fine-tuning, impact AI rater effects and severity compared to the few-shot prompting method used in this study? The authors note a limitation regarding prompting methods and suggest exploring CoT and other fine-tuning methods.

- **Open Question 3:** What specific underlying scoring logic or internal features cause certain LLMs (e.g., Gemini 2.0) to exhibit extreme rater severity while others remain neutral? The authors conclude that further exploration is needed to understand AI rater scoring logic.

## Limitations

- Small sample size (30 students, 120 essays) provides limited statistical power for robust parameter estimation in the Many-Facet Rasch model
- Exact prompts for ER tasks and all four essays were not fully specified, creating reproducibility challenges
- The generalizability of results to other writing tasks, languages, or scoring rubrics is untested

## Confidence

- **High Confidence:** The comparative ranking of LLM scoring accuracy (ChatGPT 4o, Claude 3.5 Sonnet, Gemini 1.5 Pro showing highest QWK values) is supported by statistical analysis
- **Medium Confidence:** The practical recommendation to use top-performing LLMs for low-stakes automated scoring assumes observed patterns will generalize to new essays
- **Low Confidence:** The assertion that ensemble models can occasionally improve scoring accuracy is weakly supported, with only one ensemble showing consistent improvements

## Next Checks

1. Score a held-out set of 30-50 additional essays using ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro to verify top-ranked performance persists across different student samples

2. Systematically vary the few-shot prompting approach for the top three LLMs to quantify how prompt design influences scoring accuracy and identify the most robust prompt structure

3. Re-fit the Many-Facet Rasch model using a partial credit specification to test whether the assumption of common rating structure across tasks holds