---
ver: rpa2
title: 'Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated
  Dataset'
arxiv_id: '2602.00393'
source_url: https://arxiv.org/abs/2602.00393
tags:
- image
- captions
- metrics
- performance
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Transformer-based vision encoder-decoder models
  for Brazilian Portuguese image captioning, addressing the challenge of low-resource
  languages with limited native datasets. It compares models trained on machine-translated
  and manually annotated versions of Flickr30K, employing cross-source evaluation
  to assess translation impact.
---

# Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset

## Quick Facts
- arXiv ID: 2602.00393
- Source URL: https://arxiv.org/abs/2602.00393
- Reference count: 5
- Primary result: Swin-based transformers with Portuguese-native pre-training outperform larger multilingual VLMs for Brazilian Portuguese image captioning

## Executive Summary
This study evaluates Transformer-based vision encoder-decoder models for Brazilian Portuguese image captioning, addressing the challenge of low-resource languages with limited native datasets. It compares models trained on machine-translated and manually annotated versions of Flickr30K, employing cross-source evaluation to assess translation impact. Swin-based architectures consistently outperform others, with Swin-DistilBERTimbau achieving the highest scores across metrics. ViTucano, a Portuguese pre-trained model, surpasses larger multilingual VLMs in text-based metrics, while GPT-4 models excel in CLIP-Score for image-text alignment. Attention maps reveal systematic biases in gender, counting, and spatial reasoning.

## Method Summary
The study uses vision encoder-decoder (VED) transformers fine-tuned on Brazilian Portuguese image captioning data. Vision encoders (ViT-BASE, Swin-BASE, DeiT-BASE) process images into patch embeddings, while language decoders (BERTimbau-BASE, DistilBERTimbau-BASE, GPorTuguese-2) generate captions via cross-attention. Models are trained on Flickr-Translated (Google Translate captions) and Flickr-Native (human-annotated captions) datasets, with evaluation on both same-source and cross-source setups. Metrics include CIDEr-D, BLEU-4, ROUGE-L, METEOR, BERTScore, and CLIP-Score. The study compares native pre-training approaches against multilingual VLMs in zero-shot and fine-tuned settings.

## Key Results
- Swin-based encoders consistently outperform ViT and DeiT across both translated and native captioning scenarios
- Portuguese-native pre-trained decoders (DistilBERTimbau) and VLMs (ViTucano) outperform larger multilingual models on text-based metrics
- Cross-source evaluation reveals 6-9% performance degradation when testing native data with translated-trained models
- Attention analysis reveals systematic biases including gender misclassification, counting errors, and spatial inconsistencies

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Vision Encoding with Shifted Windows
- **Claim:** Swin-based encoders systematically outperform ViT and DeiT across both translated and native captioning scenarios.
- **Mechanism:** The Swin Transformer builds hierarchical representations through 4 stages of patch processing (4px→32px), applying self-attention within non-overlapping windows. The shifted window mechanism enables cross-window interaction, capturing both local texture details and global scene structure. This multi-scale processing aligns better with the hierarchical nature of natural language descriptions (objects→relationships→scene).
- **Core assumption:** Image captioning benefits from multi-scale visual features similar to how object detection requires hierarchical representations.
- **Evidence anchors:**
  - [abstract] "Swin-based architectures consistently outperform others, with Swin-DistilBERTimbau achieving the highest scores across metrics."
  - [Section 5.1.1, Table 3] Swin models achieve highest CIDEr-D (66.73 translated, 65.79 native), BLEU-4 (24.65 translated, 29.17 native), and other metrics vs. DeiT and ViT configurations.
  - [Section 3.1] "Swin Transformer builds on the strengths of ViT by introducing a hierarchical representation with shifted windows... significantly reducing computational cost compared to global attention approaches."
  - [corpus] Weak direct support; related Portuguese NLP works don't address vision architecture comparisons.
- **Break condition:** If your images are predominantly uniform textures without distinct objects or spatial relationships, hierarchical encoding gains diminish. ViT may suffice for simpler scenes.

### Mechanism 2: Language-Specific Pre-training for Low-Resource Transfer
- **Claim:** Portuguese-native pre-trained decoders (DistilBERTimbau) and VLMs (ViTucano) outperform larger multilingual models on text-based metrics despite having fewer parameters.
- **Mechanism:** Pre-training on Brazilian Portuguese corpora (BrWaC, Portuguese Wikipedia) builds language-specific tokenization, morphology understanding, and syntactic patterns. During fine-tuning, cross-attention layers inherit these linguistic priors, requiring less adaptation than multilingual models which must allocate capacity across languages.
- **Core assumption:** The linguistic gap between translated and native Portuguese captions is smaller than the gap between multilingual and monolingual pre-training objectives.
- **Evidence anchors:**
  - [abstract] "ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual VLMs (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics."
  - [Section 5.2, Table 5] ViTucano 2B (2.88B params) achieves CIDEr-D 62.03 vs. GPT-4o's 25.68 on translated data, despite GPT-4o being substantially larger.
  - [Section 5.1.1] Swin-DistilBERTimbau achieves highest BERTScore (72.30) and competitive CLIP-Score (53.26) on native data.
  - [corpus] "Amadeus-Verbo Technical Report" describes Portuguese-specific LLM training showing similar gains, supporting language-specific pre-training benefits.
- **Break condition:** If your target language has extremely limited pre-training corpora (<1B tokens), the quality gap between native and multilingual pre-training may favor multilingual models with better transfer from related languages.

### Mechanism 3: Cross-Attention as Vision-Language Grounding
- **Claim:** The decoder's cross-attention layer directly grounds generated words to image regions, but this grounding exhibits systematic biases.
- **Mechanism:** During decoding, the cross-attention layer computes attention weights between output word embeddings and encoded image patch embeddings (7×7 for Swin). High attention weights indicate which image regions the model "looks at" when generating each word. However, learned attention patterns inherit training data biases (gender associations, object co-occurrences).
- **Core assumption:** Cross-attention weights provide interpretable evidence of model reasoning, though they may not reflect true causal decision-making.
- **Evidence anchors:**
  - [abstract] "Attention maps reveal systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies."
  - [Section 5.3.2, Figures 8-11] Word-image attention visualizations show coherent grounding ("grelhando"→grill regions) alongside noise and bias ("homens"→all people regardless of gender).
  - [Section 4.4] "The cross-attention layer is the architectural element that associates vision and language modalities... we use the cross-attention layer of the last decoder block."
  - [corpus] No direct corpus evidence for cross-attention mechanisms in captioning.
- **Break condition:** Attention interpretation assumes linear, additive contributions. Transformer attention can exhibit "attention head superposition" where individual heads are not interpretable. For debugging, also probe intermediate activations.

## Foundational Learning

- **Concept: Vision Encoder-Decoder Architecture**
  - **Why needed here:** This is the core framework. Encoders (ViT, Swin, DeiT) process images into patch embeddings; decoders (BERTimbau, DistilBERTimbau, GPorTuguese-2) generate text autoregressively conditioned on these embeddings via cross-attention.
  - **Quick check question:** Can you sketch the data flow from image pixels through patch embedding, encoder self-attention, cross-attention, and decoder output? If not, review the original ViT and BERT papers.

- **Concept: CLIP-Score vs. Reference-Based Metrics**
  - **Why needed here:** The paper uses both metric types to reveal different failure modes. Reference-based metrics (BLEU, CIDEr) penalize correct descriptions that differ from reference phrasing. CLIP-Score measures image-text alignment directly but is insensitive to hallucinations absent from the image.
  - **Quick check question:** Why might a caption score high on CLIP-Score but low on BLEU? What does each metric capture that the other misses?

- **Concept: Domain Shift in Cross-Source Evaluation**
  - **Why needed here:** The paper's novel contribution is quantifying translation impact via cross-source experiments (train-translated/test-native). Understanding domain shift helps predict when models will generalize poorly.
  - **Quick check question:** In Table 4, why does CIDEr-D drop more than CLIP-Score when testing on native data with a translated-trained model? What does this reveal about each metric's sensitivity?

## Architecture Onboarding

**Component map:**
Image (224×224) → Patch Embedding (ViT: 16×16, Swin: 4→32px hierarchical) → Vision Encoder (12-24 layers, self-attention) → 768-dim embeddings (7×7 patches for Swin) → Cross-Attention Layer (decoder's query × encoder's key/value) → Language Decoder (DistilBERTimbau: 6 layers, autoregressive) → Text Output (Portuguese caption)

**Critical path:**
1. **Checkpoint selection:** Match encoder-decoder hidden dimensions (all tested models use 768-dim). The cross-attention projection layers are randomly initialized—this is the primary training bottleneck.
2. **Cross-attention warmup:** The first few epochs predominantly train cross-attention projections. Monitor loss separation between encoder-only freezing vs. full fine-tuning.
3. **Beam search decoding:** Beam size 5 is used; smaller beams reduce latency but increase repetition risk for Portuguese's rich morphology.

**Design tradeoffs:**
| Choice | Pros | Cons |
|--------|------|------|
| Swin vs. ViT encoder | Better multi-scale features, higher metrics | More complex implementation, 4-stage processing |
| DistilBERTimbau vs. BERTimbau | 40% fewer parameters, faster inference | Slight accuracy drop on some metrics |
| Native vs. Translated training data | Better linguistic authenticity, higher native test performance | Annotation cost, smaller vocabulary diversity |
| Full fine-tuning vs. encoder freezing | Better adaptation to target domain | Risk of catastrophic forgetting on vision features |

**Failure signatures:**
- **Counting errors:** Generated captions miscount objects (Figure 4: "three" vs. "four" boys). Swin's 7×7 patch grid may merge nearby objects.
- **Gender bias:** Attention maps show overgeneralization ("homens" for mixed-gender groups). Data-driven; requires bias mitigation in training data.
- **Over-short outputs:** DistilBERTimbau generates shorter captions (avg 12-13 words) than GPT-4o (avg 20+ words). Adjust beam search length penalty or use length-aware training.
- **Translation artifacts:** Models trained on translated data struggle with native expressions (Figure 3: 6-9% mean performance drop for GPorTuguese-2 models).

**First 3 experiments:**
1. **Reproduce Swin-DistilBERTimbau baseline:** Train on Flickr-Translated for 20 epochs, batch size 16, learning rate 5e-5. Validate that your CIDEr-D is within 2 points of reported 66.73. This confirms your data pipeline and training loop.

2. **Cross-source generalization test:** Take the checkpoint from experiment 1 and evaluate on Flickr-Native test set. Compare your CIDEr-D drop to the reported ~18-point drop (66.73→48.32). This validates cross-domain setup.

3. **Ablate decoder choice:** Replace DistilBERTimbau with GPorTuguese-2 using identical training. Expect higher BLEU/ROUGE but more severe cross-source degradation (Figure 3). This reveals the tradeoff between lexical fluency and domain robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the findings generalize to other datasets beyond Flickr30K, particularly culturally distinct Brazilian Portuguese datasets like #PraCegoVer?
- Basis in paper: [explicit] "This study focuses primarily on the Flickr30K dataset, which may limit generalizability to other image styles or cultural contexts... we aim to expand the investigation of VLMs for Brazilian Portuguese IC by incorporating additional native datasets, such as #PraCegoVer"
- Why unresolved: The study only tested on Flickr30K-derived datasets; the native dataset was a manually annotated extension of the same images, not a culturally distinct dataset like #PraCegoVer (Instagram posts).
- What evidence would resolve it: Replicating the methodology on additional datasets with different image styles and cultural contexts, comparing cross-source performance patterns.

### Open Question 2
- Question: Can sophisticated fine-tuning or adapter approaches improve large VLMs' performance for Brazilian Portuguese image captioning in low-resource settings?
- Basis in paper: [explicit] "we did not explore the impact of very large multi-modal transformers that have shown recent promise; adapting such models to Brazilian Portuguese might further bridge domain gaps. Future work will... investigate more sophisticated fine-tuning or adapter approaches, particularly for low-resource languages."
- Why unresolved: Large VLMs were evaluated only in zero-shot settings; fine-tuning strategies for adapting them to Brazilian Portuguese were not explored.
- What evidence would resolve it: Experiments fine-tuning large VLMs using parameter-efficient methods (LoRA, adapters) on Brazilian Portuguese data, comparing against VED baselines.

### Open Question 3
- Question: What methods can effectively mitigate the systematic biases (gender misclassification, counting errors, spatial inconsistencies) revealed through attention analysis?
- Basis in paper: [explicit] "Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies." Qualitative analysis notes the model "exhibited a preference for a gender-specific term, a behavior that is a potential source of errors."
- Why unresolved: The paper identifies these biases but does not propose or test any mitigation strategies.
- What evidence would resolve it: Implementing debiasing techniques (data augmentation, constrained decoding, embedding debiasing) and measuring error reduction.

### Open Question 4
- Question: How does prompt engineering affect VLM performance for Brazilian Portuguese image captioning, and what prompts optimize the trade-off between lexical matching and image-text alignment?
- Basis in paper: [explicit] "prompt engineering techniques to enhance VLMs performance... are valuable additions to increase the present study."
- Why unresolved: Only a single fixed prompt was used for all VLMs; prompt variations were not systematically explored.
- What evidence would resolve it: Systematic experiments varying prompt structure and constraints across VLMs, evaluating both text-based metrics and CLIP-Score.

## Limitations
- Single machine-translated dataset without comparing multiple translation approaches or human post-editing
- Attention-based interpretability assumes attention weights directly reflect model reasoning despite known superposition effects
- Limited generalizability beyond Brazilian Portuguese and Flickr30K domain

## Confidence
- **High Confidence:** Swin architecture superiority, Portuguese-native pre-training benefits, cross-source evaluation methodology
- **Medium Confidence:** Specific numerical performance differences, quantitative interpretation of attention map biases
- **Low Confidence:** Claims about why ViTucano outperforms GPT-4o in text-based metrics, generalizability beyond Brazilian Portuguese and Flickr30K domain

## Next Checks
1. **Cross-translation validation:** Repeat the entire study using a different translation method (e.g., human post-editing or alternative MT system) to isolate whether performance gaps stem from translation quality versus inherent domain shift between translated and native Portuguese.

2. **Attention robustness analysis:** Conduct a systematic ablation study varying beam search parameters and input image augmentations to determine if observed attention biases persist across different generation conditions, addressing the superposition concern in transformer attention.

3. **Zero-shot cross-lingual transfer test:** Train models exclusively on English Flickr30K captions and evaluate on Portuguese test sets to establish an upper bound for cross-lingual transfer, providing context for whether the 6-9% performance gap is acceptable given the annotation cost savings of machine translation.