---
ver: rpa2
title: 'Non-omniscient backdoor injection with one poison sample: Proving the one-poison
  hypothesis for linear regression, linear classification, and 2-layer ReLU neural
  networks'
arxiv_id: '2508.05600'
source_url: https://arxiv.org/abs/2508.05600
tags:
- poison
- data
- clean
- attack
- fpoi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that one poison sample suffices for successful
  backdoor injection across linear models and 2-layer ReLU networks. The attacker
  crafts a single poison sample in an unused data direction, ensuring it dominates
  gradients during training.
---

# Non-omniscient backdoor injection with one poison sample: Proving the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks

## Quick Facts
- arXiv ID: 2508.05600
- Source URL: https://arxiv.org/abs/2508.05600
- Reference count: 40
- One poison sample suffices for successful backdoor injection across linear models and 2-layer ReLU networks

## Executive Summary
This paper proves that a single poison sample can successfully inject a backdoor into linear regression, linear classification, and 2-layer ReLU neural networks. The attacker crafts one poison sample in a direction where clean data has negligible variance, ensuring it dominates the gradient during training. For linear models, when clean data lies in a subspace, the poisoned model remains functionally equivalent to the clean model on benign tasks while achieving 100% backdoor success. Experiments on real datasets validate these theoretical findings with minimal impact on benign task performance.

## Method Summary
The attacker identifies a direction u with low variance in clean data and creates one poison sample xp = η·u with label yp=1. During training, the large magnitude η ensures this single sample dominates the gradient, forcing the model to satisfy the loss condition imposed by the poison. At test time, the backdoor trigger adds a scaled version of u to inputs. For linear models, if clean data lies in a subspace orthogonal to u, the benign and malicious learning tasks decouple, maintaining functional equivalence on clean data while achieving perfect backdoor success.

## Key Results
- One poison sample suffices for 100% backdoor success across linear regression, linear classification, and 2-layer ReLU networks
- For linear models with clean data in a subspace, poisoned models remain functionally equivalent to clean models on benign tasks
- Theoretical bounds establish the minimum poison strength needed for gradient domination with probability 1-δ
- Experiments on Parkinsons, Abalone, Spambase, Phishing, and MNIST datasets validate theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1: Gradient Domination via Orthogonal Projection
- **Claim:** A single poison sample can capture the model's learning dynamics if scaled sufficiently in a direction where clean data has negligible magnitude.
- **Mechanism:** The attacker identifies a direction u with low (or zero) variance in clean data distribution μcl. They craft a poison sample xp = ηu with large magnitude η. During gradient descent, the contribution of this single sample to the gradient (scaled by η²) statistically overwhelms the aggregate contribution of n clean samples, forcing the optimizer to rotate the decision boundary to satisfy the loss condition imposed by xp.
- **Break condition:** If regularization parameter C is excessively large relative to η, or if clean data variance in direction u is significantly underestimated, clean data may neutralize the poison signal.

### Mechanism 2: Loss Decoupling in Subspace Learning
- **Claim:** Injecting a backdoor in a linear model can be achieved with zero degradation to the benign task if the poison lies in a direction orthogonal to clean data support.
- **Mechanism:** When clean data lies strictly within a subspace, its projection onto the orthogonal poison direction u is zero. The regularized loss decomposes into independent terms: one for the clean subspace and one for the poison direction. The optimizer learns two independent models: a benign classifier for the subspace and a malicious classifier for the poison direction, which sum to the final predictor.
- **Break condition:** If clean data has any non-zero variance in direction u, the "benign" and "malicious" learning tasks couple, potentially increasing statistical risk on clean data.

## Foundational Learning

- **Concept:** Regularized Hinge Loss and Gradient Descent
  - **Why needed here:** Proofs rely on analyzing gradient ∇L and how regularization (½||f||²) interacts with data-dependent terms to see why poison sample "steers" gradient.
  - **Quick check question:** In regularized hinge loss gradient, how does contribution of single sample x change as model prediction fᵀx moves from incorrect to correct classification?

- **Concept:** Concentration Inequalities (Cantelli's Inequality)
  - **Why needed here:** Paper moves from deterministic "zero variance" to probabilistic "low variance" scenarios. Lemma 6 uses Cantelli's inequality to bound probability that sum of clean data projections deviates from mean.
  - **Quick check question:** Why does paper use Cantelli's inequality rather than Chebyshev's to bound required poison strength η? (One-sided vs. two-sided bounds)

- **Concept:** Lagrangian Duality (Strong Duality)
  - **Why needed here:** Paper extends primal proof to dual optimization (common in SVM libraries). Understanding that primal minimum equals dual maximum ensures attack works regardless of solver implementation.
  - **Quick check question:** If strong duality holds, what do KKT conditions imply about solution found by dual solver compared to primal solution?

## Architecture Onboarding

- **Component map:** Clean Dataset Dcl + Poison Sample (xp, yp) → Training Set → Model (Linear/ReLU) → Backdoored Model → Benign Accuracy + Backdoor Success

- **Critical path:**
  1. Direction Selection: Estimate clean data statistics (μsignal, σsignal) to find low-variance direction u
  2. Magnitude Calculation: Use Theorem 7/9 bounds to select η that guarantees gradient dominance with probability 1-δ
  3. Training: Standard training pipeline; optimizer naturally converges to solution satisfying both benign and malicious constraints

- **Design tradeoffs:**
  - Stealth vs. Feasibility: Larger η makes attack more robust but may make poison sample look like outlier
  - Assumption strictness: "Functional equivalence" (Theorem 12) requires perfect orthogonality (σ²=0); "bounded risk" (Corollary 14) applies to general distributions but allows some performance degradation

- **Failure signatures:**
  - Attack Failure: High benign accuracy but 0% backdoor success (η too low or insufficient training)
  - Catastrophic Forgetting: Model learns backdoor but benign accuracy drops significantly (u not truly orthogonal, coupling learning tasks)

- **First 3 experiments:**
  1. Baseline Injection: Replicate "subspace" experiment (MNIST with zeroed dimension). Train linear classifier on clean data + one poison sample in zeroed dimension. Verify 100% attack success and functional equivalence on benign data.
  2. Sensitivity Analysis (RQ3/RQ5): Sweep training-time poison strength η from 0 to theoretical bound. Plot "Attack Success Rate" curve to validate sharp transition point.
  3. Non-omniscient Robustness: Simulate "estimation error" where attacker's estimate of σsignal is wrong. Measure how much estimation error attack can tolerate before success probability 1-δ drops below 90%.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the one-poison hypothesis be theoretically proven for deep neural networks with more than two layers (e.g., CNNs, Transformers)?
  - Basis in paper: [explicit] Conclusion states that "Promising future directions are thus... the transfer of our results to more complex models."
  - Why unresolved: Current theoretical analysis restricted to linear models and 2-layer ReLU networks; extending proofs to deep architectures requires managing hierarchical non-linear interactions.
  - What evidence would resolve it: Formal proof establishing bounds for one-poison hypothesis on 3-layer network or standard deep learning architecture.

- **Open Question 2:** What efficient defense mechanisms can prevent one-poison attacks without incurring performance penalties of differential privacy?
  - Basis in paper: [explicit] Conclusion notes, "Promising future directions are thus the development of efficient countermeasures," while observing existing methods come with large performance or accuracy penalty.
  - Why unresolved: Paper establishes vulnerability but does not propose solution balancing robustness against single-sample poisoning with high benign task accuracy.
  - What evidence would resolve it: Novel defense algorithm maintaining comparable benign accuracy while reducing one-poison attack success rate to near zero.

- **Open Question 3:** How robust is the attack if attacker incorrectly estimates mean or variance of clean data distribution in attack direction?
  - Basis in paper: [inferred] Theoretical bounds (Theorems 7 and 9) require precise statistics (μsignal and σ²signal) to calculate necessary poison strength η.
  - Why unresolved: Paper assumes non-omniscient knowledge of statistics but does not analyze degradation of attack success if estimated statistics deviate significantly from true values.
  - What evidence would resolve it: Empirical or theoretical sensitivity analysis showing attack success rate when μsignal and σ²signal are systematically over- or under-estimated.

## Limitations
- Theoretical proofs rely on idealized assumptions including clean data lying exactly in a subspace
- Extension to general distributions using concentration inequalities requires careful calibration of poison strength
- Neural network results show dependence on architecture scale (2048 perceptrons needed for 100% success)
- Assumes white-box attack model where attacker knows clean data statistics

## Confidence

**High Confidence:** The core mechanism of gradient domination via orthogonal projection in linear models, supported by formal proofs and empirical validation across multiple datasets.

**Medium Confidence:** The extension to 2-layer ReLU networks shows empirical success but lacks complete theoretical justification. The dependence on network width suggests the mechanism may be more complex than in linear models.

**Low Confidence:** The practical feasibility of non-omniscient attacks under realistic estimation error scenarios, as the paper provides theoretical bounds but limited empirical validation of estimation robustness.

## Next Checks

1. **Estimation Error Sensitivity:** Systematically vary the attacker's estimate of clean data variance in the attack direction and measure the resulting backdoor success rate and benign accuracy degradation to quantify the robustness of the probabilistic bounds.

2. **General Distribution Validation:** Test the bounded-risk guarantees (Corollary 14) on datasets where clean data has non-zero variance in the attack direction, measuring the actual trade-off between backdoor success and benign accuracy degradation.

3. **Cross-Model Generalization:** Evaluate whether the one-poison mechanism transfers to deeper neural networks (3+ layers) and other architectures like convolutional networks, measuring the minimum model capacity required for successful injection.