---
ver: rpa2
title: Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language
  Models
arxiv_id: '2510.23066'
source_url: https://arxiv.org/abs/2510.23066
tags:
- financial
- extraction
- documents
- pages
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of extracting structured financial
  data from scanned, multilingual, and heterogeneous financial documents that are
  typically long and noisy. The proposed method employs a multi-stage pipeline combining
  traditional image preprocessing (segmentation, deskew, normalization), multilingual
  OCR transcription, BM25-based page retrieval, and compact vision-language models
  (VLMs) for targeted extraction.
---

# Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models

## Quick Facts
- **arXiv ID**: 2510.23066
- **Source URL**: https://arxiv.org/abs/2510.23066
- **Reference count**: 36
- **Primary result**: Multi-stage pipeline achieves 8.8× higher field-level accuracy than large VLMs at 0.7% GPU cost and 92.6% less latency.

## Executive Summary
This paper addresses the challenge of extracting structured financial data from scanned, multilingual, and long financial documents. The proposed method employs a multi-stage pipeline combining traditional image preprocessing, multilingual OCR transcription, BM25-based page retrieval, and compact vision-language models for targeted extraction. This approach avoids the inefficiency and context limitations of directly applying large VLMs to entire documents. Experimental results show the pipeline achieves 8.8 times higher field-level accuracy compared to large VLMs, at only 0.7% of the GPU cost and 92.6% less latency, demonstrating substantial gains in both accuracy and efficiency.

## Method Summary
The method uses a four-stage pipeline: (1) Image preprocessing with OpenCV contour segmentation, PaddleOCR PPLCNet orientation detection, Hough transform deskew, and CLAHE normalization; (2) PaddleOCR v3 for transcription with bounding boxes and confidence scores; (3) BM25 keyword search to retrieve relevant pages; (4) MiniCPM-o-2.6 (8B) for extraction on retrieved pages only. The approach contrasts with a baseline of Qwen2.5-VL-72B processing full documents.

## Key Results
- Field-level accuracy 8.8× higher than large VLMs
- GPU cost reduced to 0.7% of large VLM baseline
- Latency reduced by 92.6%

## Why This Works (Mechanism)

### Mechanism 1: Precision Through Scope Reduction (Page Retrieval)
Pre-filtering documents to relevant pages with BM25 retrieval increases extraction accuracy and efficiency by reducing noise. By indexing OCR-transcribed text and retrieving only pages with high lexical overlap with target financial terms, the system isolates signal-bearing regions from irrelevant content. This limits the VLM's focus to its context window capacity, preventing attention dilution and hallucination.

### Mechanism 2: Compact Model Sufficiency via High-Quality Context
Compact VLMs can match or exceed extraction performance of large VLMs when provided with high-quality, pre-processed, and well-scoped inputs. The computational cost and context-length limitations of large VLMs are unnecessary when the problem is decomposed. A compact VLM's capacity is sufficient for mapping focused, clean image-text pairs to structured schema.

### Mechanism 3: Robustness via Classical Image Enhancement
Applying traditional image processing (deskew, denoising, CLAHE) prior to transcription mitigates low-quality scan artifacts. Classical, non-learned image transformations directly address physical scan defects. Deskew aligns text lines to horizontal, improving OCR line detection. CLAHE normalizes local contrast, separating faded text from noisy backgrounds.

## Foundational Learning

- **Concept: BM25 Retrieval Algorithm**
  - Why needed: This is the core of the page-retrieval stage. Understanding its reliance on term frequency-inverse document frequency (TF-IDF) principles is crucial for troubleshooting why certain pages are missed.
  - Quick check: Given a query for "net profit" and a document page containing "profit for the year," how would BM25 score this page compared to one containing only "profitability"?

- **Concept: OCR Confidence Scores & Bounding Boxes**
  - Why needed: The pipeline outputs these along with text. Understanding their utility allows for downstream filtering and providing layout cues to VLMs.
  - Quick check: If an OCR engine returns text with a confidence score of 0.45 and a bounding box overlapping with a table cell, what should the downstream system do with this information?

- **Concept: Vision-Language Model (VLM) Context Window**
  - Why needed: This is the primary bottleneck the pipeline solves. The constraint defines the motivation for the entire multi-stage architecture.
  - Quick check: A financial document has 150 pages. A VLM has a context window of 4096 tokens. A naive approach feeds the entire document's OCR text (150,000 tokens) into the model. What will happen?

## Architecture Onboarding

- **Component map**: Input -> Image Pre-processor -> OCR Engine -> Retrieval Indexer -> BM25 Retriever -> Extractor (Compact VLM) -> Output

- **Critical path**: The accuracy of the final extraction depends most heavily on the Image Pre-processor -> OCR -> Retriever chain. If OCR fails to capture key terms due to poor pre-processing, retrieval misses the page, and the VLM has zero chance of extraction.

- **Design tradeoffs**:
  - BM25 vs. Dense Embedding Retrieval: Chose BM25 for effectiveness on financial documents with repeated terms and computational lightness. Tradeoff is lower semantic understanding.
  - Compact vs. Large VLM: Chose compact (8B parameters) for massive cost/latency savings and sufficient performance on scoped tasks. Tradeoff is reduced ability to perform complex, long-context reasoning.
  - Modular vs. End-to-End: Chose modular pipeline for interpretability and debugging. Tradeoff is complexity of managing multiple components and potential for error propagation.

- **Failure signatures**:
  1. Zero Extraction: VLM returns empty JSON. Diagnosis: Check BM25 retrieval. Did it return any pages?
  2. Incorrect Value: VLM returns wrong number. Diagnosis: Check for OCR errors on the source page.
  3. OOM Errors (Large VLM Baseline): Occurs when input documents exceed model's context window or GPU memory.

- **First 3 experiments**:
  1. Validate Pre-processing: Run sample of 20 challenging scanned documents through pre-processing stage only. Visually inspect outputs and compare OCR accuracy on raw vs. processed images.
  2. Benchmark Retrieval: Manually label ground-truth page numbers for 10 target fields across 5 documents. Run BM25 retrieval and measure Recall@k.
  3. End-to-End Extraction vs. Baseline: Run full pipeline on 10 documents and compare field-level accuracy, latency, and GPU memory usage against a one-shot large VLM baseline on the same set.

## Open Questions the Paper Calls Out
1. How does the performance of the pipeline change when specific components, such as the BM25 retrieval or image pre-processing stages, are altered or removed?
2. Can automated semantic normalization effectively resolve the extraction errors caused by inconsistent terminology and currency scaling?
3. Can specialized dense retrieval methods be developed to outperform BM25 on financial documents where standard text embeddings fail?

## Limitations
- Confidential dataset prevents independent verification of claims
- BM25 keyword coverage not fully specified
- Prompt templates for compact VLM not provided

## Confidence
- **High Confidence**: The core multi-stage architecture is sound and well-justified by the efficiency-accuracy tradeoff
- **Medium Confidence**: Quantitative claims are internally consistent but cannot be independently verified due to confidential dataset
- **Low Confidence**: Specific performance on edge cases cannot be assessed without access to evaluation data

## Next Checks
1. Replicate on Public Data: Implement full pipeline on public, multilingual financial document dataset (e.g., TAT-DQA or EFQA) and measure field-level accuracy, GPU cost, and latency against large VLM baseline
2. Ablation Study on BM25: Conduct ablation study by removing BM25 retrieval stage and feeding all pages to compact VLM, quantifying drop in accuracy and increase in context window usage
3. Error Analysis on Challenging Documents: Select subset of documents with known difficulties and perform detailed error analysis, categorizing failures by stage to identify bottlenecks