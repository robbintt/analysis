---
ver: rpa2
title: 'MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark'
arxiv_id: '2501.16688'
source_url: https://arxiv.org/abs/2501.16688
tags:
- arxiv
- performance
- evaluation
- zhang
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MME-Industry, a novel benchmark designed
  to evaluate Multimodal Large Language Models (MLLMs) in industrial applications.
  The benchmark covers 21 distinct industries with 1,050 question-answer pairs, all
  manually crafted and validated by domain experts to ensure data integrity.
---

# MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark

## Quick Facts
- arXiv ID: 2501.16688
- Source URL: https://arxiv.org/abs/2501.16688
- Reference count: 6
- 10 state-of-the-art MLLMs evaluated across 21 industries, with Qwen2-VL-72B-Instruct achieving 78.66% (Chinese) and 75.04% (English) accuracy

## Executive Summary
This paper introduces MME-Industry, a novel benchmark for evaluating Multimodal Large Language Models in industrial applications. The benchmark covers 21 distinct industries with 1,050 expert-validated question-answer pairs, incorporating both visual and domain-specific knowledge requirements. Experiments on 10 leading MLLMs reveal significant performance variations across industries and languages, with technical domains generally showing higher accuracy than abstract ones. The study emphasizes the critical role of visual information processing and suggests domain-specific fine-tuning as a key direction for improving industrial MLLM performance.

## Method Summary
The benchmark evaluates MLLMs on 1,050 expert-validated QA pairs across 21 industries, with 50 questions per domain. Each question presents a high-resolution industrial image alongside a multiple-choice question with five options (A-E, where E means "no corresponding features"). Models are evaluated with and without image inputs to verify visual dependency. The evaluation uses standardized inference parameters (temperature=0.2, max_tokens=64, top_p=0.95) and exact string matching for answers. Both Chinese and English versions enable cross-lingual analysis, with performance gaps attributed to model scale differences.

## Key Results
- Qwen2-VL-72B-Instruct achieved the highest performance at 78.66% (Chinese) and 75.04% (English)
- Technical domains like Electronics and Chemical showed consistently high accuracy (>80%), while Financial and Education domains performed poorly (<50%)
- Without image inputs, average scores dropped to 16.46% (Chinese) and 16.78% (English), below the 25% random-guess threshold
- Model scale correlated with cross-lingual consistency, with larger models showing smaller performance gaps between languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual input provides critical grounding for industrial reasoning tasks that cannot be solved through text priors alone
- Mechanism: Models process high-resolution industrial images alongside domain-specific questions, enabling visual feature extraction that constrains the solution space for specialized knowledge retrieval
- Core assumption: The benchmark questions are genuinely dependent on visual context rather than solvable from linguistic patterns
- Evidence anchors:
  - [abstract]: "incorporates non-OCR questions that can be answered directly, along with tasks requiring specialized domain knowledge"
  - [section]: Without image inputs, average scores drop to 16.46% (CN) and 16.78% (EN)—below the 25% random-guess threshold, confirming no data leakage
  - [corpus]: Related benchmark MME-SCI similarly emphasizes visual grounding for scientific reasoning domains
- Break condition: If models achieve >30% accuracy without images, visual dependency claim weakens

### Mechanism 2
- Claim: Technical domains with well-defined visual taxonomies yield higher model performance than abstract or regulation-heavy domains
- Mechanism: Industries like Electronics (94% CN, 92% EN top scores) and Chemical (90% CN, 90% EN) present visually consistent equipment and processes that align with pre-training image distributions, whereas Financial (62% CN, 60% EN top scores) and Education (70% CN, 70% EN) require abstract reasoning or domain-specific text conventions
- Core assumption: Domain performance gaps reflect visual recognition difficulty rather than question wording complexity
- Evidence anchors:
  - [abstract]: "technical domains generally showing higher accuracy"
  - [section]: "In the Electronic domain, most models achieve above 80% accuracy... Educational and Financial sectors show consistently lower performance"
  - [corpus]: UniFinEval and FinMR benchmarks specifically target financial multimodal reasoning, suggesting this domain remains challenging across evaluation frameworks
- Break condition: If question difficulty analysis reveals Financial/Education questions are longer or more ambiguous, domain-complexity confound exists

### Mechanism 3
- Claim: Model scale correlates with cross-lingual consistency in industrial multimodal tasks
- Mechanism: Large models (72B+ parameters) like Qwen2-VL-72B maintain <4% performance gap between Chinese and English, while smaller models like MiniCPM-V-2.6 (8B) show >10% gaps, suggesting scale enables more robust multilingual visual grounding
- Core assumption: Parameter count is the primary differentiator; other architectural or training differences are secondary
- Evidence anchors:
  - [abstract]: "Qwen2-VL-72B-Instruct achieves 78.66% in Chinese and 75.04% in English"
  - [section]: "MiniCPM-V-2.6 shows the largest performance gap, scoring 18.47% CN vs 29.04% EN"
  - [corpus]: Corpus papers lack direct scale comparisons for industrial benchmarks; this inference is assumption-heavy
- Break condition: If Qwen2-VL's Chinese-centric training data explains its cross-lingual advantage rather than scale, mechanism weakens

## Foundational Learning

- **Multimodal Large Language Model (MLLM) Architecture**:
  - Why needed here: The benchmark assumes familiarity with how vision encoders (e.g., CLIP-style) connect to language backbones; results interpretability requires understanding that visual tokens are processed alongside text
  - Quick check question: Can you explain why removing OCR-based questions tests visual reasoning rather than text extraction?

- **Benchmark Construction Principles (Data Integrity)**:
  - Why needed here: The paper emphasizes manual expert validation to prevent "data leakage" from public datasets; understanding this is essential for interpreting the no-image baseline results
  - Quick check question: Why does a <25% score without images demonstrate absence of data leakage?

- **Cross-Lingual Evaluation Methodology**:
  - Why needed here: The benchmark provides Chinese and English versions; interpreting performance gaps requires understanding translation effects and multilingual model capabilities
  - Quick check question: What factors beyond model size could explain cross-lingual performance differences?

## Architecture Onboarding

- **Component map**: Input: High-res industrial image + Multiple-choice question (5 options: A-E) -> Model: Vision encoder + Language model (10 evaluated, including Qwen2-VL-72B, Claude-3.5-Sonnet, GPT-4o) -> Output: Single-letter response (A, B, C, D, or E for "cannot answer") -> Evaluation: Exact string match -> Accuracy percentage per domain

- **Critical path**:
  1. Image preprocessing (resize to model-specific token limits; Qwen2-VL uses 256–1280 dynamic resolution)
  2. Prompt formatting: [Image][Question] The options: (A)... (E) There are no corresponding features. Just answer with one letter.
  3. Response parsing: Accept full output string without extraction; evaluate exact match against ground truth

- **Design tradeoffs**:
  - **5-option format with "E" rejection option**: Prevents forced guessing but complicates accuracy interpretation; models may use E for API failures or input errors
  - **No OCR questions**: Increases difficulty but excludes text-heavy industrial scenarios (e.g., equipment labels)
  - **50 questions per domain**: Limits statistical power for per-domain conclusions but enables broad 21-domain coverage

- **Failure signatures**:
  - Image exceeds input size limit -> default output "E"
  - Model API decode failure -> default output "E"
  - Model outputs explanatory text instead of single letter -> counted as incorrect (instruction-following failure)
  - Very low scores with images (<30%) -> potential vision encoder failure or domain mismatch

- **First 3 experiments**:
  1. **Baseline replication**: Run Qwen2-VL-72B-Instruct on MME-Industry-CN and MME-Industry with temperature=0.2, max_tokens=64, top_p=0.95; verify ~75–78% accuracy range
  2. **Ablation study**: Run same model without image inputs; confirm scores drop to ~15–20% (below 25% random baseline)
  3. **Domain analysis**: Compare performance on Electronics (high) vs. Financial (low) domains to validate technical-domain advantage hypothesis; check if image-free scores differ proportionally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What underlying factors cause the significant performance disparity where models excel in technical domains (e.g., Electronics) but struggle in abstract ones (e.g., Education, Financial)?
- Basis in paper: [explicit] The analysis section states, "Further investigation is needed to understand the models' lower performance in educational and financial domains."
- Why unresolved: The paper quantifies the accuracy gap (e.g., >80% in Electronics vs. <50% in Financial) but does not isolate the cause, such as lack of specific pre-training data or the inherent visual ambiguity of non-physical concepts
- What evidence would resolve it: Ablation studies varying the visual complexity and required domain knowledge depth for these specific lagging sectors

### Open Question 2
- Question: To what extent can domain-specific fine-tuning mitigate the observed performance variances across different industrial sectors?
- Basis in paper: [explicit] The research implications note, "The performance disparity across domains suggests the need for domain-specific fine-tuning strategies."
- Why unresolved: The study evaluates general-purpose, pre-trained models (e.g., GPT-4o, Qwen2-VL) without testing the efficacy of fine-tuning on the proposed industrial data
- What evidence would resolve it: Experiments comparing baseline model performance against versions fine-tuned on industry-specific subsets of the MME-Industry dataset

### Open Question 3
- Question: How does the specific integration of visual context facilitate reasoning in technical industrial scenarios compared to text-only parametric knowledge?
- Basis in paper: [explicit] The analysis concludes, "The role of visual information processing capabilities warrants deeper exploration, particularly in technical domains where visual context significantly improves performance."
- Why unresolved: While the paper demonstrates that scores drop significantly without image inputs (e.g., Power sector dropping from 78% to 16%), it does not explain the internal mechanism of how visual features correct or enhance the model's reasoning process
- What evidence would resolve it: Qualitative analysis of attention maps or probing tasks showing how visual tokens activate industrial knowledge during the reasoning process

## Limitations

- **Dataset Accessibility**: The 1,050 image-QA pairs are not publicly available, preventing independent verification of results and limiting reproducibility
- **Cross-Lingual Confounds**: The study attributes performance differences primarily to model scale but doesn't account for potential translation quality variations or cultural domain knowledge differences
- **Scope Constraints**: Excluding OCR-based questions omits common industrial scenarios where text recognition is critical, limiting generalizability to real-world applications

## Confidence

- **High Confidence**: Visual dependency claims are strongly supported by the dramatic performance drop (<25% random baseline) when images are removed
- **Medium Confidence**: Technical domain advantage hypothesis is plausible but could be confounded by question difficulty variations or visual consistency differences
- **Low Confidence**: The scale-correlation claims for cross-lingual consistency rely heavily on Qwen2-VL-72B's specific architecture and Chinese training data

## Next Checks

1. **Independent Dataset Verification**: Request access to the MME-Industry dataset from authors and replicate the no-image baseline experiment on 3-4 randomly selected industries to confirm the <25% random performance floor

2. **Cross-Lingual Confound Analysis**: Conduct a controlled study comparing question pairs in both languages to quantify translation effects, word complexity differences, and cultural knowledge requirements that might explain performance gaps beyond model scale

3. **OCR Question Extension**: Create a supplementary evaluation subset with OCR-based industrial questions (equipment labels, safety signs, documentation) to test whether the current benchmark's exclusion of text-heavy scenarios creates blind spots in industrial MLLM assessment