---
ver: rpa2
title: 'Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models
  Outperform Commercial Large Language Models in Medical Tasks'
arxiv_id: '2506.20009'
source_url: https://arxiv.org/abs/2506.20009
tags:
- energy
- medical
- llms
- llama3
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'A modular retrieval-augmented generation framework was built for
  medical question answering using local open-source LLMs with embedded energy and
  CO2 monitoring. The llama3.1-8B-based RAG achieved 58.5% accuracy and 0.52 accuracy
  points per kWh, outperforming online models (o4-mini: 57% accuracy, 0.19 PPW; DeepSeekV3-R1:
  47.5% accuracy, 0.15 PPW).'
---

# Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks

## Quick Facts
- arXiv ID: 2506.20009
- Source URL: https://arxiv.org/abs/2506.20009
- Authors: Konstantinos Vrettos; Michail E. Klontzas
- Reference count: 26
- Primary result: Local RAG with llama3.1-8B achieves 58.5% accuracy and 0.52 PPW, outperforming o4-mini (57%, 0.19 PPW) and DeepSeekV3-R1 (47.5%, 0.15 PPW) while using 172% less electricity

## Executive Summary
This paper demonstrates that a modular retrieval-augmented generation (RAG) framework using local open-source LLMs can match or exceed commercial models in medical question answering accuracy while consuming significantly less energy. The llama3.1-8B-based RAG achieved 58.5% accuracy on medical MCQs, surpassing o4-mini and DeepSeekV3-R1, while using only 1.1 kWh versus 3 kWh for the commercial models. The framework incorporates real-time energy and CO2 monitoring, showing 2.7x better accuracy-per-kWh and 172% less electricity usage. Prompt engineering provided up to 9% accuracy gains, proving more effective than using specialized medical domain models.

## Method Summary
The authors built a modular RAG framework for medical question answering using Python 3.9 with FAISS v1.5.3 for vector search, langchain-community 0.2.16, and carbontracker v2.2.0 for energy monitoring. The corpus ("Cecil Textbook of Medicine") was chunked and indexed using FAISS with mxbai-embed-large embeddings. At inference, queries retrieve semantically relevant passages which are injected into prompts for the local LLM generator. The system was tested on 1,000 medical MCQs (500 from MedQA patient cases, 500 from PubMedQA research questions) using hardware with 64GB RAM and NVIDIA RTX 5000 32GB VRAM. Local models tested included llama3.1-8B (best), llama3-8B, llama3.2-1B, mistral-7B, and medgemma-4b-it.

## Key Results
- llama3.1-RAG achieved 58.5% accuracy, outperforming o4-mini (57%) and DeepSeekV3-R1 (47.5%)
- Local RAG consumed 1.1 kWh and emitted 473g CO2, while o4-mini and DeepSeek used 3 kWh and emitted 1140g and 1950g CO2 respectively
- Prompt engineering improved performance by up to 9% for MedGemma-RAG and 4% for llama3.1-RAG
- Local RAG achieved 2.7x more accuracy points per kWh (0.52 vs 0.19-0.15) and 172% less electricity usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation enables smaller-parameter local LLMs to match or exceed commercial model accuracy on medical QA tasks.
- Mechanism: Curated medical text (Cecil Textbook of Medicine) is chunked, embedded with mxbai-embed-large, and indexed via FAISS. At inference, queries retrieve semantically relevant passages which are injected into the prompt, grounding the generator in domain-specific evidence rather than relying solely on parametric knowledge.
- Core assumption: Retrieval corpus coverage aligns with the distribution of evaluation questions (USMLE-style clinical cases and PubMed research questions).
- Evidence anchors: [abstract] "RAG model built on llama3.1:8B achieved the highest accuracy (58.5%) and was significantly better than other models, including o4-mini and DeepSeekV3-R1." [section] "First, the curated medical literature is organized into chunks and indexed using FAISS... The data is then transformed into a high-dimensional vector representation using OllamaEmbeddings... with the 'mxbai-embed-large' model." [corpus] Related work (arxiv 44233) reports similar findings: "smaller language models (SLMs), combined with retrieval-augmented generation (RAG), achieve higher diagnostic and therapeutic performance than larger models."

### Mechanism 2
- Claim: Prompt engineering yields larger accuracy gains than switching to domain-specialized models.
- Mechanism: Structured prompt design (query + retrieved context + formatting instructions) improves the generator's ability to extract and apply relevant information. The paper reports 9% improvement for MedGemma-RAG and 4% for llama3.1-RAG from prompt optimization alone.
- Core assumption: Prompt improvements transfer across question types; the 4-9% gains observed generalize beyond this specific dataset.
- Evidence anchors: [abstract] "Prompt engineering improved performance by up to 9%." [section] "An intriguing finding is that prompt engineering has a more significant impact on the performance of the RAG than using a specialized medical domain LLM, like the recently released MedGemma." [corpus] Supporting evidence from Rubei et al. (cited in paper): "prompt engineering can improve the LLM's accuracy while also reducing energy usage."

### Mechanism 3
- Claim: Local inference on consumer-grade hardware achieves higher accuracy-per-kWh than cloud-based commercial APIs.
- Mechanism: Energy overhead in commercial systems includes data center infrastructure, multi-tenant serving, and network transmission. Local inference eliminates this overhead, and smaller models (8B parameters) require less compute per token. The paper measures this directly via carbontracker for local runs and estimates cloud consumption from literature (3 Wh/prompt assumption).
- Core assumption: The 3 Wh/prompt estimate for o4-mini and DeepSeekV3-R1 is accurate and representative; cloud energy estimates account for "Web Search" feature being enabled.
- Evidence anchors: [abstract] "The llama3.1-RAG achieved 2.7x times more accuracy points per kWh and 172% less electricity usage while maintaining higher accuracy." [section] "Total CPU/GPU electricity usage in kWh was 0.07/1 for llama3.1-RAG... The estimation of energy consumption and CO2 emissions for GPT and DeepSeek was based on the widely accepted assumption that a simple GPT-4 prompt uses 3Wh." [corpus] Weak direct evidence; corpus papers focus on accuracy/privacy rather than energy benchmarking. Samsi et al. (cited) shows "a large LLM (67B) uses significantly more energy than a model of 7B parameters."

## Foundational Learning

- Concept: Vector similarity search (FAISS + embeddings)
  - Why needed here: Understanding how queries map to retrieved documents is essential for debugging poor retrieval and optimizing chunk size/embedding model selection.
  - Quick check question: If a query retrieves irrelevant passages, would you adjust the embedding model, chunk size, or similarity threshold first—and why?

- Concept: RAG architecture components (Retriever, Generator, prompt construction)
  - Why needed here: The modular framework requires understanding how each component affects both accuracy and energy consumption independently.
  - Quick check question: What happens to latency and energy if you double the number of retrieved chunks passed to the generator?

- Concept: Energy/carbon monitoring for ML inference
  - Why needed here: The framework's distinguishing feature is real-time energy tracking via carbontracker; interpreting these metrics requires understanding GPU/CPU power measurement limitations.
  - Quick check question: Why might CPU energy consumption (1.0 kWh) exceed GPU consumption (0.07 kWh) in the llama3.1-RAG results?

## Architecture Onboarding

- Component map: Corpus → Chunking → Embedding (mxbai-embed-large) → FAISS Index → Query → Retriever (FAISS lookup) → Prompt Builder (query + retrieved context) → Generator (local LLM) → Response → Energy Monitor (carbontracker)
- Critical path: Retrieval quality determines ceiling for accuracy; prompt structure determines how much of that ceiling is realized; model size determines latency and energy cost.
- Design tradeoffs: Larger models (e.g., Mixtral 4×7B cited at 60% accuracy) may improve accuracy but require more VRAM and energy; more retrieval chunks increase context but raise token costs and latency; specialized medical models (MedGemma) underperformed general-purpose models (llama3.1) in this setup, suggesting RAG + prompt engineering may be more effective than domain pretraining for this task.
- Failure signatures: Low accuracy with high retrieval similarity → prompt construction issue; high accuracy but excessive latency → too many retrieved chunks or oversized model; inconsistent energy readings → carbontracker not capturing GPU correctly (ensure NVIDIA drivers and permissions).
- First 3 experiments: 1) Reproduce baseline: Run llama3.1-8B RAG on 100 MedQA questions with paper's prompt template; verify accuracy and energy readings are in expected range (55-60%, ~1.1 kWh per 1000 queries). 2) Ablate retrieval: Run same evaluation with retrieval disabled (zero-shot) to quantify RAG's contribution; expect ~20-30% accuracy drop based on prior work showing local LLMs perform poorly on medical tasks without augmentation. 3) Optimize chunking: Test 256 vs 512 vs 1024 token chunks with fixed overlap; measure accuracy/energy tradeoff to find optimal granularity for medical textbook content.

## Open Questions the Paper Calls Out

- How does the modular RAG framework perform on open-ended medical questions compared to multiple-choice queries? The authors acknowledge in the limitations that "the performance of the RAG was not evaluated on open-ended questions." The study restricted evaluation to the MedQA and PubMedQA datasets, which utilize multiple-choice formats. Benchmarking the framework on open-ended clinical datasets (e.g., MedDialog) using metrics like ROUGE or factual consistency would resolve this.

- To what extent does expanding the retrieval corpus beyond a single textbook improve diagnostic accuracy? The Discussion notes that "using a wider corpus... could potentially yield better performance." The system relied exclusively on the "Cecil Textbook of Medicine," potentially limiting the scope of retrievable knowledge. Ablation studies measuring accuracy and hallucination rates when diverse medical guidelines and journals are added to the vector store would resolve this.

- Do direct energy measurements of commercial API inference validate the estimated 3 Wh per prompt figure? The paper relies on literature estimates (3 Wh) for online models because it is "not possible to directly calculate the energy usage of the online models." The efficiency comparison depends on these external estimates rather than empirical, hardware-level data for the commercial models. Transparency reports from API providers or independent audits confirming the actual kWh usage per query for o4-mini and DeepSeek would resolve this.

## Limitations

- Energy comparison assumptions: The paper estimates commercial model energy at 3 Wh/prompt but acknowledges this is a literature-based assumption. Direct measurements of o4-mini and DeepSeekV3-R1 energy consumption were not performed.
- Retrieval corpus completeness: The study uses "Cecil Textbook of Medicine" as the sole knowledge source, which may not cover all PubMedQA research questions adequately.
- Generalizability concerns: Results are based on specific question types (MedQA and PubMedQA). The framework's performance on other medical domains, real-world clinical scenarios, or different question formats remains untested.

## Confidence

- High confidence: The accuracy advantage of RAG over non-RAG approaches (58.5% vs 47.5-57%) is well-supported by the experimental results. The energy monitoring methodology using carbontracker is directly implemented and measured.
- Medium confidence: The claim that local RAG achieves 2.7x better PPWkWh than commercial models depends on the 3 Wh/prompt assumption for cloud models, which introduces uncertainty. The prompt engineering improvements (4-9%) are demonstrated but may not generalize across all model types.
- Low confidence: The assertion that RAG combined with prompt engineering outperforms specialized medical models (MedGemma) may be dataset-specific, as the PubMedQA questions were stripped of context, potentially limiting the advantage of domain pretraining.

## Next Checks

1. Verify energy measurement methodology: Reproduce the energy monitoring setup on the same hardware configuration, specifically investigating why CPU energy consumption exceeds GPU energy in the reported results. Test with carbontracker on alternative hardware to establish baseline variance.
2. Test retrieval quality and corpus coverage: Manually examine retrieved passages for a sample of PubMedQA questions to assess whether the textbook corpus adequately covers research-based questions. Run ablation studies with different chunk sizes and embedding models to optimize retrieval relevance.
3. Evaluate model generalization: Test the framework on additional medical question sets (e.g., USMLE-style questions with full context, clinical vignettes) to determine if the 58.5% accuracy and energy efficiency advantages hold across diverse medical question types and difficulty levels.