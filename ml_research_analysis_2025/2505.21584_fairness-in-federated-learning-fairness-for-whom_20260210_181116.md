---
ver: rpa2
title: 'Fairness in Federated Learning: Fairness for Whom?'
arxiv_id: '2505.21584'
source_url: https://arxiv.org/abs/2505.21584
tags:
- fairness
- learning
- clients
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a critical gap in federated learning (FL)\
  \ fairness research: existing approaches optimize narrow system-level metrics while\
  \ overlooking real-world harms across diverse stakeholders. Through a systematic\
  \ review of 121 papers, the authors identify five recurring pitfalls\u2014narrow\
  \ focus on client-server architecture, mismatch between simulations and motivating\
  \ use-cases, conflating system protection with user protection, disconnected interventions,\
  \ and lack of multi-stakeholder alignment."
---

# Fairness in Federated Learning: Fairness for Whom?

## Quick Facts
- **arXiv ID:** 2505.21584
- **Source URL:** https://arxiv.org/abs/2505.21584
- **Reference count:** 31
- **Key outcome:** Systematic review identifies critical gaps in FL fairness research, proposing a harm-centered framework that links technical decisions to real-world harms across diverse stakeholders.

## Executive Summary
This paper identifies a critical gap in federated learning (FL) fairness research: existing approaches optimize narrow system-level metrics while overlooking real-world harms across diverse stakeholders. Through a systematic review of 121 papers, the authors identify five recurring pitfalls—narrow focus on client-server architecture, mismatch between simulations and motivating use-cases, conflating system protection with user protection, disconnected interventions, and lack of multi-stakeholder alignment. They propose a harm-centered framework that maps the FL lifecycle against potential sources of harm and stakeholder vulnerabilities, linking technical decisions to concrete risks. The framework shifts focus from optimizing isolated fairness metrics to understanding how biases propagate and manifest as quality-of-service, allocative, representational, privacy, and reputational harms.

## Method Summary
The authors conducted a systematic literature review of 121 papers from DBLP using keywords ('Fair*' + 'Federated learning'), ('Bias' + 'Federated Learning'), published through December 2024. Papers were filtered for research outputs across ML, networking/telecom, and economics venues, excluding non-research outputs, irrelevant "bias" usage, duplicates, predatory venues, and paywalled papers. The analysis categorized papers by fairness paradigm (participation fairness 8.4%, performance-centered 28.0%, group fairness-inspired 34.0%, collaborative fairness 21.6%), intervention points in FL lifecycle, evaluation datasets (synthetic vs. real-world), and stakeholder specificity.

## Key Results
- Only 10% of papers evaluated fairness methods on domain-specific data, with most relying on synthetic benchmarks like MNIST/CIFAR
- Existing fairness interventions target isolated lifecycle stages while neglecting upstream and downstream bias propagation effects
- Multiple fairness definitions (performance, group, collaborative, participation) can conflict simultaneously in real deployments, particularly in cross-silo settings
- The paper proposes a harm-centered framework mapping FL lifecycle stages to specific biases and resulting harms across five categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Harms in FL systems emerge through cumulative bias propagation across lifecycle stages, not isolated decision points.
- **Mechanism:** Technical choices at each stage (problem formulation → model initialization → client selection → aggregation → evaluation → deployment) introduce specific biases (historical, representation, participation, aggregation, collaboration, evaluation) that compound downstream. For example, synchronous aggregation deadlines exclude slower clients (participation bias), which then leads to aggregation bias when only faster clients' updates shape the global model, ultimately producing quality-of-service harms for underrepresented populations.
- **Core assumption:** Biases are interdependent across stages; intervening at one point without addressing upstream/downstream effects yields incomplete mitigation.
- **Evidence anchors:**
  - [abstract]: "interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects"
  - [section 4.4]: "Most interventions are localized on a few steps while others are overlooked"
  - [section 5.1]: Detailed mapping of each lifecycle stage to specific biases and resulting harms
  - [corpus]: Limited direct corpus validation; neighbor papers address fairness definitions but not lifecycle propagation specifically

### Mechanism 2
- **Claim:** Narrow system-level fairness metrics (performance variance, contribution-based rewards) obscure structural inequalities by abstracting away from who is harmed and how.
- **Mechanism:** FL research commonly defines fairness through the client-server architectural lens, treating clients as interchangeable units rather than embedded in institutional/social contexts. This abstraction error conflates protecting system efficiency with protecting vulnerable users. Collaborative fairness mechanisms that penalize "low contributors" may actually punish clients facing structural barriers (scarce data, limited compute), but this harm is invisible when evaluation uses synthetic clients on benchmark datasets.
- **Core assumption:** Proxy metrics (variance, Shapley values) do not reliably capture real-world harm distributions.
- **Evidence anchors:**
  - [abstract]: "existing approaches tend to optimize narrow system-level metrics... while overlooking how harms arise"
  - [section 4.2]: "fewer than 10% of papers evaluated their fairness methods on domain-specific data"
  - [section 4.3]: Collaborative fairness "allocates rewards based on contribution... clients with noisy or underrepresented data would be penalized"
  - [corpus]: "Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property" supports socio-technical framing

### Mechanism 3
- **Claim:** Multiple fairness definitions (performance, group, collaborative, participation) can be simultaneously relevant and conflicting in real FL deployments.
- **Mechanism:** Different stakeholders have divergent fairness interests. In a financial cross-silo FL setting: (1) institutions want equitable performance across silos (performance fairness), (2) customers within each silo need protection from demographic discrimination (group fairness), (3) institutions seek fair reward allocation (collaborative fairness). Optimizing one can undermine others—e.g., contribution-based rewards may penalize institutions serving minority populations whose data diverges from majority patterns.
- **Core assumption:** Real-world FL deployments involve multiple stakeholder groups with non-aligned fairness interests.
- **Evidence anchors:**
  - [abstract]: "lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once"
  - [section 4.5]: Detailed financial sector example showing how fairness definitions conflict
  - [section 3]: Categorization showing only 8.4% participation fairness, 28% performance, 34% group, 21.6% collaborative—research treats these as separate rather than integrated
  - [corpus]: "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification" addresses client-level fairness tradeoffs but not multi-definition conflicts explicitly

## Foundational Learning

- **Concept: Cross-silo vs. Cross-device FL distinction**
  - **Why needed here:** The paper's harm framework depends critically on identifying which stakeholders are affected. In cross-silo FL (institutions like hospitals), the client is an organization but downstream harms affect patients/customers. In cross-device FL (mobile phones), users are both data providers and model consumers.
  - **Quick check question:** Given a healthcare FL system where hospitals collaboratively train a diagnostic model, who are the clients and who are the end users affected by model decisions?

- **Concept: FL lifecycle stages and intervention points**
  - **Why needed here:** The paper argues that disconnected interventions (focusing only on aggregation or client selection) miss upstream and downstream bias propagation. Understanding the full lifecycle (problem formulation → initialization → selection → training → aggregation → evaluation → deployment) is prerequisite to lifecycle-aware fairness design.
  - **Quick check question:** If you only modify the aggregation weights to improve fairness, what upstream decisions might still propagate bias into the model?

- **Concept: Harm taxonomy (QoS, allocative, representational, privacy, reputational)**
  - **Why needed here:** The framework maps technical decisions to concrete harm types. Quality-of-service harms arise from performance disparities; allocative harms from unfair resource/benefit distribution; representational harms from stereotype reinforcement; privacy harms from disparate DP noise effects; reputational harms from misclassifying legitimate clients as adversaries.
  - **Quick check question:** A robustness mechanism flags outlier updates as potential poisoning attacks. What type of harm might this cause for clients with genuinely non-IID data?

## Architecture Onboarding

- **Component map:** Problem Formulation → Model Initialization → Client Selection → Aggregation → Evaluation → Deployment → Privacy/Robustness
- **Critical path:** Problem formulation → Client selection → Aggregation → Evaluation. Per the paper, biases introduced early (formulation, initialization) propagate through selection and aggregation, but evaluation often fails to detect disparities because it uses aggregate metrics or synthetic benchmarks.
- **Design tradeoffs:**
  - Synchronous vs. asynchronous aggregation: Sync excludes slow clients (participation bias) but simplifies convergence; async includes more clients but complicates consistency
  - Data-size weighting in aggregation: Standard FedAvg weights by dataset size, amplifying dominant clients; alternative weighting can improve fairness but may slow convergence
  - DP noise: Protects privacy but disproportionately degrades performance for underrepresented groups (evidence: Bagdasaryan et al. 2019, Ling et al. 2024 cited in paper)
  - Anomaly detection threshold: Lower thresholds catch more adversaries but increase false positives (reputational harms for legitimate non-IID clients)
- **Failure signatures:**
  - High average accuracy with high variance across clients → QoS harm potential
  - Clients with small/non-IID datasets consistently flagged as adversaries → reputational harm from robustness mechanisms
  - Fairness evaluated only on MNIST/CIFAR with synthetic splits → results may not transfer to deployment context
  - Single fairness metric optimized without considering others → hidden conflicts with stakeholder needs
- **First 3 experiments:**
  1. **Baseline audit:** Take existing FL system; log per-client performance, participation rates, and (if available) per-demographic-group metrics. Identify which lifecycle stages show highest variance/exclusion.
  2. **Data-context alignment check:** Verify evaluation dataset matches deployment context. If gap exists (e.g., using CIFAR for healthcare), identify at least one domain-appropriate alternative (e.g., FOLKTABLES, FLamby per paper's recommendations).
  3. **Multi-metric conflict probe:** For a single FL run, measure at least two fairness metrics (e.g., performance parity and group fairness). Document whether they align or conflict, and where in the lifecycle intervention would address each.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can conflicting fairness objectives (performance, group, collaborative fairness) be jointly optimized or reconciled in heterogeneous FL deployments?
- **Basis in paper:** [explicit] Section 4.5 states: "Performance, group, and collaborative fairness objectives may conflict, especially in heterogeneous systems... Future work should investigate how these fairness objectives interact and how conflicts between them can be resolved."
- **Why unresolved:** Current work treats fairness definitions in isolation; the authors found almost no papers consider multiple fairness paradigms simultaneously.
- **What evidence would resolve it:** Algorithms demonstrating Pareto-optimal trade-offs across multiple fairness metrics in realistic cross-silo FL scenarios.

### Open Question 2
- **Question:** What evaluation datasets and benchmarks can adequately capture the sociotechnical complexity of real-world FL fairness concerns?
- **Basis in paper:** [explicit] Takeaway #1 states: "Advancing fairness in FL will require better datasets, more realistic simulations, and evaluation strategies that reflect real-world risks." Fewer than 10% of surveyed papers used domain-specific data.
- **Why unresolved:** Most evaluations rely on centralized ML datasets (MNIST, CIFAR-10) split into synthetic clients, abstracting away institutional and social contexts.
- **What evidence would resolve it:** Development of federated benchmarks that reflect realistic data heterogeneity, resource constraints, and stakeholder diversity across domains like healthcare and finance.

### Open Question 3
- **Question:** How can contribution evaluation mechanisms account for structural inequalities rather than penalizing clients with scarce or nonconforming data?
- **Basis in paper:** [explicit] Section 5.1 states: "Measuring 'contribution' remains an open and contested problem... clients providing rare or minority data, which may be critical for generalization, are undervalued or excluded."
- **Why unresolved:** Existing approaches favor clients aligned with dominant patterns, creating feedback loops that reinforce inequality.
- **What evidence would resolve it:** Contribution metrics validated against downstream harm reduction for underrepresented populations, not just global utility gains.

## Limitations
- The harm-centered framework's empirical validation is limited to illustrative examples rather than systematic field testing across diverse deployment contexts.
- The review methodology's sensitivity to keyword variations and exclusion criteria may have omitted relevant papers using alternative fairness terminology.
- The framework assumes stakeholder vulnerability mapping can be generalized, but real-world power dynamics require more nuanced, context-specific analysis.

## Confidence
- **High confidence:** The identification of five research pitfalls (narrow focus, simulation-mismatch, system-protection conflation, disconnected interventions, multi-stakeholder misalignment) is well-supported by corpus analysis patterns.
- **Medium confidence:** The harm taxonomy mapping to lifecycle stages is logically coherent but requires empirical validation in operational FL systems.
- **Low confidence:** The claim that existing system-level metrics systematically obscure real-world harms needs systematic correlation studies between technical metrics and stakeholder impact measurements.

## Next Checks
1. **Cross-validation with deployment case studies:** Apply the harm framework to documented FL deployments (e.g., Google's Gboard, hospitals using FL for diagnostics) and compare predicted harm patterns against reported issues.
2. **Stakeholder feedback loop:** Test framework assumptions by consulting with diverse FL stakeholders (data providers, end users, regulators) to verify identified harm categories and their relative priorities.
3. **Multi-metric correlation analysis:** Collect performance variance, group fairness metrics, and stakeholder harm indicators from real FL systems to empirically test whether system-level fairness metrics predict actual harm distributions.