---
ver: rpa2
title: Composable Cross-prompt Essay Scoring by Merging Models
arxiv_id: '2505.18548'
source_url: https://arxiv.org/abs/2505.18548
tags:
- source
- adaptation
- merging
- target
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses cross-prompt automated essay scoring (AES)\
  \ under source-free domain adaptation, where models trained on multiple source prompts\
  \ must adapt to a new target prompt without access to source data. The key idea\
  \ is to merge individually fine-tuned source models via linear combinations of their\
  \ LoRA adapters (task vectors) instead of retraining jointly, and to optimize the\
  \ merging coefficients using an unsupervised objective\u2014Prior-encoded Information\
  \ Maximization (PIM)\u2014which encourages score discriminability regularized by\
  \ source-derived priors."
---

# Composable Cross-prompt Essay Scoring by Merging Models

## Quick Facts
- arXiv ID: 2505.18548
- Source URL: https://arxiv.org/abs/2505.18548
- Reference count: 40
- Primary result: Merging individually fine-tuned source models via LoRA task vectors and optimizing coefficients with Prior-encoded Information Maximization outperforms joint training on cross-prompt AES

## Executive Summary
This paper addresses cross-prompt automated essay scoring under source-free domain adaptation, where models trained on multiple source prompts must adapt to a new target prompt without access to source data. The key idea is to merge individually fine-tuned source models via linear combinations of their LoRA adapters (task vectors) instead of retraining jointly, and to optimize the merging coefficients using an unsupervised objective—Prior-encoded Information Maximization (PIM)—which encourages score discriminability regularized by source-derived priors. PIM is optimized efficiently via Bayesian optimization. Experiments on in-dataset and cross-dataset adaptation with LLMs show that the method consistently outperforms joint training on all sources, exceeds other merging strategies, maintains robustness under severe domain shifts, and is more computationally efficient than retraining-based baselines.

## Method Summary
The method operates in two phases: first, LoRA adapters are fine-tuned on each source prompt independently, and Beta distributions are fitted to scaled source scores to create priors. Second, Bayesian optimization searches for merging coefficients that maximize the PIM objective: combining KL divergence regularization from source priors with entropy minimization for confident predictions. The final model merges adapters using optimized coefficients, enabling adaptation to target prompts using only unlabeled target data and pre-trained source models.

## Key Results
- Merging LoRA task vectors via Bayesian-optimized coefficients outperforms joint training on all sources across multiple LLM architectures
- PIM objective with source-derived Beta priors significantly improves QWK over simple averaging and mutual information maximization
- Method maintains effectiveness when adapting from ASAP to cross-dataset PERSUADE2.0, demonstrating robustness to distribution shifts
- Bayesian optimization converges in 40 iterations, offering computational efficiency compared to retraining-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Merging Approximates Selective Joint Training
Linear combinations of LoRA-based task vectors can approximate joint training on selected source domains without accessing source data. Each source prompt yields a task vector τ_j = θ_j − θ_pre, and weighted combination θ_mrg = θ_pre + Σλ_j·τ_j enables post-hoc "soft selection" of beneficial sources.

### Mechanism 2: Prior-Encoded Information Maximization Guides Coefficient Search
An unsupervised objective combining source-derived priors with mutual information maximization identifies effective merging coefficients without target labels. PIM maximizes f(λ) = −KL(p(y|λ)||q(y)) − H(p(y|x,λ)), where the first term encourages predictions matching source-derived prior q(y) and the second term promotes confident predictions.

### Mechanism 3: Bayesian Optimization Efficiently Searches Continuous Coefficient Space
Bayesian optimization with Expected Improvement acquisition efficiently finds merging coefficients without gradient computation. The method treats f(λ) as black-box, using Gaussian Process surrogate modeling and balancing exploitation with exploration over the continuous coefficient space.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed: Task vectors are computed from LoRA adapters (B·A matrices), not full parameters. Understanding rank-r decomposition is essential for implementing the merge operation.
  - Quick check: Given weight matrix W∈R^(m×n) with LoRA matrices B∈R^(m×r), A∈R^(r×n), what is the effective task vector dimensionality per layer?

- **Mutual Information in Discriminative Models**
  - Why needed: PIM modifies standard MI maximization by replacing uniform prior with learned prior. Requires understanding entropy, conditional entropy, and KL divergence.
  - Quick check: Why does maximizing I(y;x) = H(p(y)) − H(p(y|x)) encourage both prediction confidence and class balance?

- **Beta Distribution Properties**
  - Why needed: Source scores are modeled as Beta(α,β) distributions; understanding mean μ=α/(α+β) and variance σ²=αβ/[(α+β)²(α+β+1)] is needed for prior computation.
  - Quick check: What constraints on α,β ensure unimodality? How does the unified Beta match mixture moments?

## Architecture Onboarding

Component map: Pre-trained LLM -> Per-source fine-tuning (LoRA adapters) -> Bayesian Optimization (PIM objective) -> Final merged model

Critical path: PIM objective computation is called ~40 times per target prompt and must efficiently extract p(y|x,λ) from LLM next-token probabilities over score tokens.

Design tradeoffs: Sample size for PIM (paper uses 64), prior unification strategy (averaging vs. moment-matching), coefficient bounds ([0,1] per λ_j vs. allowing negative values).

Failure signatures: All source models predict same score range → PIM maximizes "confidence" on wrong predictions; target score range far from sources → discretized prior q(y) may not cover target space; GP surrogate fails to improve after ~15 iterations.

First 3 experiments: 1) Verify individual source models outperform zero-shot on their own prompts, 2) Replicate PIM component ablation on held-out prompt, 3) Train sources on ASAP, adapt to single PERSAUDE2.0 prompt to validate cross-dataset robustness.

## Open Questions the Paper Calls Out

Can the method be adapted to handle extreme deviations between source and target score ranges? The authors note that extreme range differences (e.g., 0–60 vs. 0–4) lead to suboptimal predictions due to lack of diversity in source model outputs.

Can the high inference latency inherent to LLM-based scoring be reduced to allow for scalability to large volumes of target essays? The paper acknowledges that inference latency is significantly higher than encoder-based models, potentially limiting scalability.

How can the PIM objective be safeguarded against performance degradation when no source model captures the target prompt's semantics? The paper notes that if all source models fail to capture semantics, optimizing for discriminability becomes meaningless and may degrade performance.

## Limitations

- Source-free assumption fragility: Relies on source score distributions for prior construction, which may misguide optimization if target domain's distribution differs fundamentally
- Computational cost characterization gap: Lacks explicit runtime comparisons despite claiming efficiency advantages
- Tokenization dependency: Critical underspecification in exact mapping from LLM output tokens to discrete scores could cause silent failures

## Confidence

High Confidence:
- Merging LoRA adapters approximates selective joint training better than joint training on all sources
- Bayesian optimization finds better coefficients than random search
- PIM outperforms simple averaging across source models

Medium Confidence:
- Prior-encoded regularization improves robustness vs. standard mutual information
- Method generalizes to cross-dataset adaptation
- Computational efficiency vs. retraining-based baselines

Low Confidence:
- Beta distribution assumption for source score modeling
- Discretization strategy in unified prior construction
- Scalability to dozens of source prompts

## Next Checks

1. **Prior robustness test**: Hold out 2-3 ASAP prompts as sources, fit Beta priors, then adapt to remaining prompts. Systematically perturb source score distributions and measure PIM performance degradation.

2. **Ablation under extreme domain mismatch**: Train sources on ASAP (narrow score range [0,6]), adapt to PERSUADE2.0 prompts with [0,60] scale. Compare PIM with no prior, re-scaled prior, and score-range matching.

3. **Scalability stress test**: Extend M from 7 ASAP prompts to 20+ synthetic prompts with increasing overlap. Measure optimization iterations to convergence, final QWK vs. joint training runtime, and coefficient sparsity patterns.