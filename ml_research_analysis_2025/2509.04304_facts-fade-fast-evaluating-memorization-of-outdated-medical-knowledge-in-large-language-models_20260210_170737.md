---
ver: rpa2
title: 'Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in
  Large Language Models'
arxiv_id: '2509.04304'
source_url: https://arxiv.org/abs/2509.04304
tags:
- knowledge
- medical
- llms
- outdated
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MedRevQA and MedChangeQA, two QA datasets
  derived from Cochrane systematic reviews, to evaluate memorization of outdated medical
  knowledge in large language models. MedRevQA contains 16,501 QA pairs covering general
  biomedical knowledge, while MedChangeQA has 512 QA pairs where medical consensus
  has changed over time.
---

# Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models

## Quick Facts
- arXiv ID: 2509.04304
- Source URL: https://arxiv.org/abs/2509.04304
- Authors: Juraj Vladika; Mahdi Dhaini; Florian Matthes
- Reference count: 35
- Primary result: LLMs consistently rely on outdated medical knowledge, with performance declining on more recent questions

## Executive Summary
This paper introduces MedRevQA and MedChangeQA, two QA datasets derived from Cochrane systematic reviews, to evaluate how large language models memorize and apply outdated medical knowledge. The datasets contain 16,501 QA pairs for general biomedical knowledge and 512 pairs where medical consensus has changed over time. Experiments on eight diverse LLMs reveal consistent reliance on outdated knowledge, with performance deteriorating for more recent questions. Analysis traces outdated knowledge to pre-training data frequency and discusses potential mitigation strategies including RAG augmentation and knowledge editing.

## Method Summary
The authors created MedRevQA (16,501 QA pairs) and MedChangeQA (512 QA pairs) by scraping Cochrane systematic literature review abstracts from PubMed (2000–Jan 2024). Questions were extracted from Objectives sections and labels from Authors' conclusions, categorized as Supported, Refuted, or Not Enough Information. MedChangeQA pairs were identified where verdicts changed between review iterations. Models were evaluated zero-shot with temperature=0 and 512 token limit, comparing performance against both outdated and latest ground truth labels to measure "outdated knowledge persistence" via F1 difference.

## Key Results
- All eight evaluated LLMs show consistent reliance on outdated medical knowledge
- Performance declines as questions reference more recent years, indicating stronger memorization of older facts
- RAG augmentation with Top 1 PubMed result improves F1 scores by 3.2 to 16.2 points
- GPT models exhibit generic phrasing without study attribution when citing outdated reviews

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Based Encoding of Outdated Facts
Older medical knowledge appears more frequently in pre-training corpora, leading to stronger memorization. Scientific findings that have existed longer have had more time to propagate through web sources, citations, and follow-up studies. Higher token frequency during training correlates with stronger weight encoding. Break condition: If newer studies are upweighted during training or filtered for recency, frequency bias diminishes.

### Mechanism 2: Temporal Performance Decay via Static Parametric Knowledge
LLM performance degrades on more recent medical questions because parametric knowledge is frozen at training cutoff. Models encode facts into weights during pre-training. After cutoff, no mechanism exists to incorporate new evidence without retraining or external augmentation. Core assumption: Model parameters, once trained, do not self-update; knowledge conflict resolution tends to favor parametric memory. Break condition: If RAG or knowledge editing successfully overrides parametric memory, temporal decay is mitigated.

### Mechanism 3: Outdated Verdict Prediction from Legacy Consensus
Models predict outdated labels because training data captured earlier medical consensus states. Cochrane SLRs from earlier years encoded older verdicts. When queried without context, models retrieve these earlier consensus states from memory. Core assumption: Models cannot distinguish temporal versioning of facts without explicit time markers in prompts. Break condition: If prompts include publication date constraints or retrieval prioritizes recency, legacy consensus influence decreases.

## Foundational Learning

- **Parametric vs. Contextual Knowledge**
  - Why needed here: The paper evaluates whether models rely on internal (parametric) knowledge versus external (retrieved) evidence. Understanding this distinction is essential for interpreting outdated knowledge persistence.
  - Quick check question: Can you explain why a model might reject retrieved up-to-date information in favor of its internal parametric knowledge?

- **Knowledge Conflict Resolution**
  - Why needed here: The paper notes LLMs can "reject" augmented information and "resort to internal knowledge" (Section 1). This conflict behavior determines whether mitigation strategies work.
  - Quick check question: What happens when retrieved context contradicts a model's parametric memory?

- **Systematic Literature Reviews as Ground Truth**
  - Why needed here: The paper uses Cochrane SLRs because they represent the "highest quality evidence" in medical hierarchy (Section 3). Understanding this ensures proper interpretation of labels.
  - Quick check question: Why are SLRs preferred over individual studies for establishing ground truth labels?

## Architecture Onboarding

- **Component map**: MedRevQA (16,501 QA pairs) -> MedChangeQA (512 QA pairs) -> Zero-shot evaluation -> F1 comparison (outdated vs. latest ground truth) -> Infini-gram provenance tracing
- **Critical path**: Load MedChangeQA questions → Prompt model with zero-shot QA instruction → Extract predicted label → Compare against both outdated and latest ground truth → Compute F1 difference as "outdated knowledge persistence" metric
- **Design tradeoffs**: Silver labels (MedRevQA) scale annotation but introduce ~5-8% noise; gold labels (MedChangeQA) are accurate but limited to 512 pairs; Zero-shot evaluation isolates parametric knowledge but cannot measure retrieval-augmented performance; F1 difference metric is a proxy—it may conflate outdated knowledge with general model error
- **Failure signatures**: Models citing decade-old Cochrane reviews (Table 8 example: Mistral citing 2014 review for 2023 question); Generic phrasing without study attribution (GPT-4o pattern in Table 4); Positive F1 difference when evaluated against outdated labels but negative against latest labels (Table 2: Qwen shows -4.8 difference)
- **First 3 experiments**: Baseline benchmark: Run MedChangeQA evaluation on your model with zero-shot prompting; compute F1 outdated vs. F1 latest gap; RAG augmentation test: Retrieve top PubMed result, append abstract to prompt; measure F1 improvement (Table 3 shows +3.2 to +16.2 gains); Provenance inspection: Use Infini-gram or similar tool to check whether specific Cochrane reviews exist in your training corpus and their frequency by year

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced retrieval-augmented generation (RAG) strategies, specifically re-ranking evidence by recency and source quality, effectively mitigate the reliance on memorized outdated medical knowledge? The authors state that "Advanced RAG techniques are needed" and suggest "filtering and re-ranking of retrieved evidence by recency and source quality" as a necessary future step. This remains unresolved as the paper only demonstrated a simple retrieval strategy.

### Open Question 2
How effective are knowledge editing and machine unlearning techniques at updating specific medical facts in LLMs without degrading general performance? The discussion lists "machine unlearning" and "knowledge editing" as "promising directions for mitigation" that require future investigation. While identified as a solution, the study did not implement these techniques, and their specific efficacy on medical consensus shifts is unknown.

### Open Question 3
Does generating long-form answers with nuanced labels improve the accuracy of LLMs in identifying the state of current medical evidence compared to simple classification? The authors propose that "future work could also investigate more nuanced labels or long-form answer generation with explanations." The current methodology relies on rigid Supported/Refuted/NEI labels, which may mask the model's ability to express the uncertainty inherent in evolving evidence.

## Limitations

- The paper assumes frequency-based memorization is the primary mechanism for outdated knowledge persistence, but does not test whether frequency is the dominant factor versus other encoding biases
- Silver labels for MedRevQA may introduce systematic bias if the GPT-4o-mini prompt template favors certain linguistic patterns
- The 512 MedChangeQA gold labels represent only ~3% of the total dataset, limiting statistical power for fine-grained temporal analysis

## Confidence

- **High**: The empirical observation that model performance consistently declines on more recent medical questions (supported by F1 differences in Table 2 and Figure 4)
- **Medium**: The claim that pre-training data frequency drives outdated knowledge memorization (plausible but not experimentally isolated from other factors)
- **Medium**: The effectiveness of RAG augmentation in reducing outdated knowledge reliance (Table 3 shows improvement but does not test whether models can still reject correct context)

## Next Checks

1. **Controlled frequency ablation**: Create synthetic training corpora where medical facts from different years have controlled occurrence frequencies, then measure memorization strength versus frequency directly
2. **Context rejection test**: For each MedChangeQA question, retrieve multiple contexts (one outdated, one current) and measure whether models systematically prefer parametric knowledge over correct external evidence
3. **Temporal granularity analysis**: Recompute F1 differences using finer year-binning (e.g., 2018-2019 vs 2020-2021 vs 2022-2023) to detect non-linear decay patterns that might indicate specific knowledge revision events