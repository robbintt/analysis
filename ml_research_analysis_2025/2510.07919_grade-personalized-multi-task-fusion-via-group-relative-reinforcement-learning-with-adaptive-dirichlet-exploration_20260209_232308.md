---
ver: rpa2
title: 'GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning
  with Adaptive Dirichlet Exploration'
arxiv_id: '2510.07919'
source_url: https://arxiv.org/abs/2510.07919
tags:
- policy
- learning
- grade
- exploration
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRADE tackles the problem of personalized multi-task fusion in
  large-scale recommender systems, where static, manually-tuned weights fail to capture
  individual user intent and adapt to shifting user preferences. It proposes a novel
  framework using Group-relative Reinforcement learning with Adaptive Dirichlet Exploration
  to learn personalized fusion weights dynamically.
---

# GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploration

## Quick Facts
- **arXiv ID:** 2510.07919
- **Source URL:** https://arxiv.org/abs/2510.07919
- **Reference count:** 37
- **Primary result:** +0.595% CTR, +1.193% CVR, +1.788% OPM, +1.568% total order volume in production

## Executive Summary
GRADE introduces a personalized multi-task fusion framework for large-scale recommender systems that dynamically learns user-specific fusion weights via Group-relative Reinforcement learning with Adaptive Dirichlet Exploration. Unlike static, manually-tuned fusion formulas, GRADE adapts in real-time to shifting user preferences by sampling weight vectors from a Dirichlet distribution centered on the policy output, evaluating their relative performance within groups, and optimizing via a critic-free GRPO paradigm. Deployed in a marketplace with hundreds of millions of daily active users, GRADE achieves significant online improvements across multiple business metrics while preventing common reward hacking issues through a composite reward function combining sparse user feedback, dense model priors, and rule-based constraints.

## Method Summary
GRADE tackles personalized multi-task fusion by learning a policy network that outputs fusion weight vectors for ranking items. The framework operates in two stages: (1) Supervised pre-training using LambdaLoss on fusion scores to establish a reference policy, and (2) RL fine-tuning via GRPO where weight vectors are sampled from a Dirichlet distribution centered on the policy output. The policy is updated based on group-relative advantages calculated from a composite reward combining posterior user feedback (NDCG), dense prior predictions (NDCG), and format constraints (preventing weight polarization). The Dirichlet sampling naturally respects the probability simplex constraints while GRPO eliminates the need for a separate critic network, enabling stable learning in sparse reward environments.

## Key Results
- Significant online lift: +0.595% CTR, +1.193% CVR, +1.788% OPM, +1.568% total order volume
- Outperforms established baselines in production deployment
- Ablation studies confirm necessity of composite reward components to prevent metric hacking
- Optimal group size of 20 for GRPO advantage estimation

## Why This Works (Mechanism)

### Mechanism 1: Critic-free relative optimization
GRPO stabilizes learning in sparse reward environments by evaluating relative performance within sampled groups rather than requiring absolute value estimates from a critic network. The advantage is calculated as the normalized reward within a group of 20 candidates, eliminating high-variance critic training.

### Mechanism 2: Dirichlet distribution for constrained exploration
Dirichlet sampling naturally generates weight vectors on the probability simplex (non-negative, sum to one) without requiring projection or normalization. The distribution is centered on the policy output via concentration parameter α, ensuring structured local exploration.

### Mechanism 3: Composite rewards prevent hacking
The total reward combines sparse user feedback, dense model predictions, and rule-based constraints. This multi-component approach prevents the policy from optimizing single metrics at the expense of others, with format rewards specifically blocking weight polarization.

## Foundational Learning

- **Concept: Probability Simplex**
  - **Why needed here:** Fusion weights must be non-negative and sum to one, forming a probability simplex. This constraint explains why Dirichlet sampling is preferred over standard Gaussian exploration.
  - **Quick check question:** If a model outputs [0.8, 0.7, 0.5] for weights, what is the immediate problem and how does Softmax or Dirichlet solve it?

- **Concept: Policy Gradient (vs. Value-based methods)**
  - **Why needed here:** GRADE directly learns the policy mapping states to weights. Unlike Q-learning, Policy Gradient updates the probability distribution to make good actions more likely using relative advantages rather than absolute Q-values.
  - **Quick check question:** Why does the GRPO objective maximize log π(a|s) multiplied by the advantage Â? What happens to the probability of an action with a negative advantage?

- **Concept: Reward Hacking**
  - **Why needed here:** Optimizing single metrics often destroys other business objectives. The format reward mechanism is specifically designed to block this common failure mode.
  - **Quick check question:** In ablation studies, why does posterior-only optimization achieve high CTR (+1.04%) but negative GPM (-0.46%)?

## Architecture Onboarding

- **Component map:** User Profile, Query Features -> Policy Network (MMoE-like) -> Dirichlet Sampler -> Fusion Weights -> Ranking
- **Critical path:** Request -> Policy Net -> Dirichlet Sample -> Fusion Weights -> Ranking (Inference); Sync Policy -> Reference Model -> Generate G weight candidates -> Rank items -> Calculate Rewards -> Compute Group Advantage -> Update Policy (Training)
- **Design tradeoffs:** Group size G=20 balances variance reduction and computation cost; concentration α cycles between exploration and exploitation via cosine annealing
- **Failure signatures:** Weight collapse (one weight approaches 1.0), training stagnation (KL divergence drops too quickly), instability (rewards oscillate wildly)
- **First 3 experiments:** (1) Sanity check: Supervised policy vs. random weights, (2) Reward ablation: Only R_post to confirm hacking, (3) Hyperparameter sweep: G=5,20,40 and fixed vs. annealing α

## Open Questions the Paper Calls Out

### Open Question 1
Can the "Weight Format Reward" be replaced by a learned constraint mechanism, or is manual heuristic encoding required to prevent weight polarization and reward hacking? The paper demonstrates necessity but doesn't explore autonomous learning of safe weight boundaries.

### Open Question 2
Does an adaptive group size strategy (dynamic G) outperform the fixed or cosine-annealed group sizes used? The authors used fixed/scheduled values but didn't investigate feedback-driven dynamic adjustment.

### Open Question 3
How sensitive is GRADE to hyperparameter transferability, specifically reward coefficients (λ₁, λ₂, λ₃), when applied to non-e-commerce domains? The paper validates only on Kuaishou's marketplace data.

## Limitations
- Deployment scale claims lack detailed A/B test methodology and statistical power analysis
- Generalizability limited to single large-scale marketplace system without cross-domain validation
- Reward signal quality depends on reliability of dense prior model predictions

## Confidence

- **GRPO + Dirichlet exploration effectiveness:** High - supported by ablation studies and theoretical grounding
- **Composite reward prevents hacking:** High - validated by controlled ablation experiments
- **Online lift metrics:** Medium - claimed but detailed A/B test methodology not disclosed

## Next Checks

1. **Cross-domain replication:** Implement GRADE on public multi-task datasets (Amazon/KuaiRec) to verify transferability
2. **Prior model robustness:** Conduct controlled experiments with degraded/bias prior predictions to test policy convergence
3. **Group size sensitivity:** Systematically vary G in staged deployment to confirm G=20 optimality for specific traffic patterns