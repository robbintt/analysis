---
ver: rpa2
title: 'Understanding Sharpness Dynamics in NN Training with a Minimalist Example:
  The Effects of Dataset Difficulty, Depth, Stochasticity, and More'
arxiv_id: '2506.06940'
source_url: https://arxiv.org/abs/2506.06940
tags:
- sharpness
- minimalist
- training
- figure
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies sharpness dynamics in deep neural network training,\
  \ focusing on the progressive sharpening phenomenon where sharpness increases during\
  \ gradient descent before saturating near the edge of stability. The authors propose\
  \ a minimalist model\u2014a deep linear network with one neuron per layer\u2014\
  to analyze this behavior."
---

# Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More

## Quick Facts
- arXiv ID: 2506.06940
- Source URL: https://arxiv.org/abs/2506.06940
- Reference count: 40
- Primary result: Progressive sharpening in neural network training can be predicted from dataset properties, with sharpness scaling as Q^(D-1)/D where Q is dataset difficulty and D is network depth

## Executive Summary
This paper investigates the progressive sharpening phenomenon in deep neural network training, where sharpness increases during gradient descent before saturating near the edge of stability. The authors propose a minimalist model—a deep linear network with one neuron per layer—to analyze this behavior. They theoretically derive sharpness bounds at global minima as functions of dataset difficulty (Q) and layer imbalance, showing that larger datasets, deeper networks, and smaller learning rates lead to more progressive sharpening. Empirical results confirm that their predicted sharpness closely matches observed post-training sharpness in both linear and nonlinear networks.

## Method Summary
The method uses a deep linear network with a single neuron per layer, where the output is computed as (x^T u) ∏_{i=1}^{D-1} v_i. The authors analyze gradient flow (GF), gradient descent (GD), and stochastic gradient descent (SGD) dynamics. They compute dataset difficulty Q = Σᵢ d²ᵢ/σ²ᵢ from the SVD of the data matrix, track layer imbalance C(θ) = ||Π_W u||² - v₁², and measure sharpness S(θ) = λ_max(∇²L(θ)). The analysis shows how these quantities evolve during training and relate to final sharpness at convergence.

## Key Results
- Dataset difficulty Q directly predicts sharpness magnitude at global minima, scaling as Q^(D-1)/D
- Layer imbalance C(θ) increases under GD/SGD (preserved under GF), reducing final sharpness
- SGD amplifies layer imbalance growth compared to GD, leading to less progressive sharpening
- Depth amplifies sharpness for Q > 1 but reduces it for Q < 1
- The predicted sharpness σ²₁/N · Q^(D-1)/D closely matches empirical observations

## Why This Works (Mechanism)

### Mechanism 1: Dataset Difficulty Drives Sharpness Growth
- Progressive sharpening magnitude is predictable from dataset properties via Q = Σᵢ d²ᵢ/σ²ᵢ
- Larger Q → higher sharpness because optimizer must concentrate weight magnitude to achieve zero loss
- Relationship S(θ*) ~ σ²₁/N · Q^(D-1)/D holds for balanced solutions

### Mechanism 2: Layer Imbalance Conservation and Decay
- Under gradient flow, layer imbalance C(θ) is preserved exactly
- Under GD/SGD, C increases slowly, reducing final sharpness
- SGD increases C more than GD due to noise amplification

### Mechanism 3: Depth Amplifies Sharpness via Product Structure
- For Q > 1, increasing depth D increases sharpness as Q^(D-1)/D
- Each layer weight equals Q^(1/2D) in balanced solutions
- NTK spectral norm contains σ²₁ · Q^(D-1)/D term from data-dependent component

## Foundational Learning

- **Progressive Sharpening & Edge of Stability**
  - Why needed: Explains why sharpness grows during training and saturates near 2/η
  - Quick check: With η=0.01, expect EoS regime to begin around sharpness ≈ 200

- **Singular Value Decomposition of Data Matrix**
  - Why needed: Dataset difficulty Q defined in terms of SVD components
  - Quick check: Given X = UΣV^T and y in col(X), compute dᵢ = eᵢ^T y for each singular direction

- **Neural Tangent Kernel (NTK) and Sharpness Connection**
  - Why needed: Sharpness S(θ) = λ_max(∇²L(θ)) = 1/N · ||J(θ)J(θ)^T||₂ at global minima
  - Quick check: At global minimizer with zero loss, loss Hessian equals Gauss-Newton matrix

## Architecture Onboarding

- **Component map**: Data matrix X → SVD decomposition → Q computation → Minimalist model (u, v₁, v₂, ...) → GF/GD/SGD training → Track C(θ), S(θ) → Compare with predictions
- **Critical path**: 1) Compute SVD of X, 2) Project y onto singular directions to get dᵢ, 3) Calculate Q, 4) Initialize with αβ scheme, 5) Train with GF/GD/SGD tracking C and S, 6) Verify S(θ*) against bounds
- **Design tradeoffs**: Model simplicity vs realism, balanced vs imbalanced initialization, zero-loss vs non-zero minimum, GF approximation vs discrete GD
- **Failure signatures**: Sharpness prediction fails (check Q<1 or imbalanced initialization), no EoS behavior (check η and precision), C(θ) decreases unexpectedly (check C≤T₁ condition), blowup instead of convergence (check precision handling)
- **First 3 experiments**:
  1. Validate Q-prediction: Train 2-layer model on CIFAR2 subsets with N∈{100,300,500,1000}, verify correlation between predicted and empirical sharpness
  2. Depth scaling test: Train D∈{2,3,4,5} models on N=100 (Q<1) and N=300 (Q>1), confirm depth increases sharpness for Q>1
  3. Batch size and learning rate effects: Train with SGD using B∈{25,50,200,N} and η∈{2/3000,2/6000,2/12000,2/18000}, track C(θ) and S(θ)

## Open Questions the Paper Calls Out

- **Oscillation decay mechanism**: The reason for attenuated oscillations in the Edge of Stability regime is not well-understood theoretically, despite empirical observation
- **Extension to nonlinear networks**: Thorough analysis of sharpness dynamics under highly nonlinear equations remains necessary and is deferred to future work
- **Numerical precision effects**: How numerical precision fundamentally dictates the transition between non-monotonic convergence and loss blow-up is not fully understood

## Limitations

- Theoretical analysis assumes labels are in the column space of the data matrix (perfect interpolation)
- Minimalist model with single-neuron-per-layer may not capture practical network complexity
- Empirical validation primarily focused on 2-label CIFAR10 subsets and synthetic data

## Confidence

- **High confidence**: Relationship between dataset difficulty Q and sharpness at convergence, and prediction that larger Q leads to higher sharpness
- **Medium confidence**: Layer imbalance mechanism explaining differences between GF, GD, and SGD behavior
- **Medium confidence**: Depth-scaling relationship (Q^(D-1)/D) for sharpness

## Next Checks

1. Test the Q-sharpness prediction on non-image datasets (e.g., tabular or speech data) with varying difficulty levels, computing Q and comparing predicted vs empirical sharpness across at least 5 different datasets

2. Validate the depth-scaling relationship in networks with multiple neurons per layer and nonlinear activations, training MLPs on the same datasets and comparing sharpness scaling with depth

3. Analyze how early stopping affects the progressive sharpening phenomenon and layer imbalance mechanism, tracking C(θ) and S(θ) for training runs stopped at various loss thresholds below zero