---
ver: rpa2
title: 'From Persona to Person: Enhancing the Naturalness with Multiple Discourse
  Relations Graph Learning in Personalized Dialogue Generation'
arxiv_id: '2506.11557'
source_url: https://arxiv.org/abs/2506.11557
tags:
- dialogue
- persona
- coherence
- https
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating natural, coherent,
  and persona-consistent responses in personalized dialogue systems. The proposed
  MUDI framework integrates discourse relations and persona information using a graph-based
  approach.
---

# From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation

## Quick Facts
- arXiv ID: 2506.11557
- Source URL: https://arxiv.org/abs/2506.11557
- Reference count: 32
- Key outcome: MUDI framework improves personalized dialogue generation, achieving BLEU-1: 18.19, ROUGE-1: 16.59, QuantiDCE: 3.21, and Dist-1: 47.68 on ConvAI2.

## Executive Summary
This paper tackles the challenge of generating natural, coherent, and persona-consistent responses in personalized dialogue systems. The proposed MUDI framework leverages discourse relations and persona information using a graph-based approach. By annotating discourse relations with a large language model, constructing dialogue graphs, and employing a novel DialogueGAT encoder, MUDI captures implicit discourse relations and persona descriptions. A coherence-aware attention mechanism and dynamic weighting aggregation are used during response generation to balance coherence and persona consistency, outperforming existing methods in text similarity, coherence, and diversity on the ConvAI2 dataset.

## Method Summary
The MUDI framework integrates discourse relations and persona information through a graph-based approach. It uses a large language model to annotate discourse relations, constructs dialogue graphs, and processes them with a novel DialogueGAT encoder that captures implicit discourse relations and persona descriptions. A coherence-aware attention mechanism and dynamic weighting aggregation are employed during response generation to balance coherence and persona consistency. The framework is pre-trained with tasks like Shortest Path Prediction and Turn Classification, then fine-tuned for coherence relations classification and next response type prediction on the ConvAI2 dataset.

## Key Results
- MUDI achieves BLEU-1: 18.19 and ROUGE-1: 16.59, outperforming existing methods in text similarity.
- The framework improves coherence, as measured by QuantiDCE: 3.21, and diversity, with Dist-1: 47.68.
- MUDI demonstrates effectiveness in generating more natural and personalized dialogue responses.

## Why This Works (Mechanism)
MUDI leverages discourse relations and persona information to generate coherent and persona-consistent responses. The DialogueGAT encoder captures implicit discourse relations and persona descriptions through graph modeling. Coherence-aware attention and dynamic weighting aggregation during response generation ensure a balance between coherence and persona consistency. The multi-task learning framework, including pre-training and fine-tuning tasks, enhances the model's ability to generate high-quality responses.

## Foundational Learning
- **Discourse Relations**: Understanding how utterances connect in dialogue; needed to capture conversation flow; quick check: validate annotation quality.
- **Graph Neural Networks**: Processing relational data through message passing; needed to model dialogue structure; quick check: test on synthetic graph data.
- **Multi-task Learning**: Joint training on related tasks; needed to improve generalization; quick check: compare single-task vs. multi-task performance.

## Architecture Onboarding
- **Component Map**: ConvAI2 -> LLaMA-3-70B (discourse annotation) -> Dialogue Graph -> DialogueGAT -> BART -> MUDI
- **Critical Path**: Discourse annotation -> Graph construction -> DialogueGAT encoding -> Response generation
- **Design Tradeoffs**: Graph complexity vs. computational cost; annotation accuracy vs. scalability
- **Failure Signatures**: Poor discourse annotation quality leads to noisy graphs; imbalanced loss weights hinder convergence
- **First Experiments**:
  1. Validate LLaMA-3-70B's discourse relation annotations on a sample of utterance pairs.
  2. Implement a simplified version without graph modeling to isolate its contribution.
  3. Vary the Order-Attention decay rate to assess its impact on performance.

## Open Questions the Paper Calls Out
- **Robustness to Noisy Annotations**: How does DialogueGAT handle errors in LLM-annotated discourse relations?
- **Generalization to Longer Contexts**: Can MUDI scale to dialogues with longer contexts or different domains?
- **Computational Overhead**: What is the inference latency compared to standard Transformer baselines?

## Limitations
- Heavy dependence on LLaMA-3-70B discourse annotations without detailed prompting strategy or quality validation.
- Multiple unknown hyperparameters (loss weights, attention decay, k-hop distance) that significantly impact performance.
- Limited ablation study scope - the paper doesn't clearly show the individual contributions of each component.

## Confidence
- **High confidence**: Overall architectural framework (DialogueGAT, coherence-aware attention, dynamic weighting) is clearly specified and theoretically sound.
- **Medium confidence**: Multi-task learning objectives and their integration are well-described, though specific implementation choices remain unclear.
- **Medium confidence**: Empirical results are promising, but exact evaluation protocol details are limited.

## Next Checks
1. **Annotation Quality Validation**: Sample and manually verify LLaMA-3-70B's discourse relation annotations on 100 utterance pairs from ConvAI2.
2. **Component Ablation Study**: Implement a reduced version without graph modeling and without coherence-aware attention to isolate contributions.
3. **Hyperparameter Sensitivity Analysis**: Systematically vary one unknown parameter at a time to understand their impact on performance.