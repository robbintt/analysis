---
ver: rpa2
title: 'Lassoed Forests: Random Forests with Adaptive Lasso Post-selection'
arxiv_id: '2511.06698'
source_url: https://arxiv.org/abs/2511.06698
tags:
- forest
- error
- post-selection
- variance
- vanilla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework to combine random forests
  and Lasso selection by applying adaptive weighting, called Lassoed forest. The method
  addresses the issue that post-selection boosting random forests can degrade performance
  under certain signal-to-noise ratio conditions.
---

# Lassoed Forests: Random Forests with Adaptive Lasso Post-selection

## Quick Facts
- arXiv ID: 2511.06698
- Source URL: https://arxiv.org/abs/2511.06698
- Reference count: 36
- Primary result: Introduces Lassoed forest, a unified framework combining random forests and Lasso selection via adaptive weighting, showing improved prediction error across diverse biomedical datasets.

## Executive Summary
This paper presents Lassoed forest, a novel approach that unifies random forests and Lasso selection through adaptive weighting. The method addresses performance degradation issues in post-selection boosting random forests under certain signal-to-noise ratio conditions. By treating vanilla forest predictions as an offset and applying adaptive weights, Lassoed forest can theoretically and empirically outperform both vanilla and post-selection random forests across regression, classification, and survival analysis tasks.

## Method Summary
Lassoed forest combines random forests with adaptive Lasso post-selection through a cross-fitting framework. The method splits training data into two halves: one for forest training and one for Lasso training. It generates a prediction matrix from individual tree outputs, then searches over adaptive weights (θ) to find the optimal combination of forest predictions and Lasso selection. The adaptive weight is selected by minimizing cross-validation or out-of-bag error estimates, creating a flexible model that can outperform both pure random forests and pure post-selection approaches.

## Key Results
- Lassoed forest shows improved prediction error compared to vanilla and post-selection forests across various simulations
- Demonstrated superior performance on real-world biomedical datasets including cancer cell line sensitivity, immune checkpoint treatment response, and HIV drug resistance analysis
- The method is theoretically proven to strictly outperform both vanilla and post-selection forests under stated assumptions

## Why This Works (Mechanism)
Lassoed forest works by creating a weighted combination of random forest predictions and Lasso selection. The adaptive weight θ allows the model to flexibly balance between the stability of random forest predictions and the feature selection benefits of Lasso. By treating forest predictions as an offset term and applying adaptive weights, the method can effectively reduce variance while maintaining the predictive power of both components.

## Foundational Learning
- **Random Forest Basics**: Why needed - Core prediction component; Quick check - Verify forest generates accurate baseline predictions
- **Lasso Regularization**: Why needed - Feature selection and coefficient shrinkage; Quick check - Confirm λ selection via cross-validation
- **Cross-fitting Framework**: Why needed - Prevents overfitting by separating forest and Lasso training; Quick check - Ensure proper data splitting and independence
- **Adaptive Weight Selection**: Why needed - Balances forest and Lasso contributions; Quick check - Verify θ selection minimizes cross-validation error

## Architecture Onboarding

**Component Map**: Data Split -> Random Forest Training -> Prediction Matrix Generation -> Adaptive Weight Search -> Lasso Fitting -> Model Selection

**Critical Path**: The critical path involves generating accurate tree-level predictions, correctly implementing the adaptive weighting scheme, and properly selecting the optimal θ through cross-validation.

**Design Tradeoffs**: The method trades computational complexity (searching over θ values) for improved prediction accuracy and theoretical guarantees. The cross-fitting approach requires sufficient sample size but prevents overfitting.

**Failure Signatures**: 
- Incorrect tree prediction extraction results in prediction matrix with wrong dimensions
- Improper offset scaling leads to suboptimal θ selection
- Cross-validation errors not properly computed results in incorrect model selection

**3 First Experiments**:
1. Verify θ=0 produces identical results to vanilla random forest
2. Test θ=1 produces standard post-selection forest results
3. Validate adaptive weight selection on a simple synthetic dataset with known signal-to-noise ratio

## Open Questions the Paper Calls Out
None

## Limitations
- The method requires sufficient sample size for effective cross-fitting
- Computational cost increases with grid search over adaptive weights
- Performance depends on proper hyperparameter specification for both forest and Lasso components

## Confidence
- Theoretical framework: High
- Methodological soundness: High
- Empirical validation: Medium (due to lack of detailed hyperparameter specifications for real-world applications)

## Next Checks
1. Verify that the RF hyperparameter settings (mtry, max.depth) used for real-world datasets match those reported in the simulations, or document any deviations
2. Reproduce the adaptive weight selection process on at least one real-world dataset, ensuring the cross-validation error estimates are correctly computed and the optimal θ is selected
3. Test the Lassoed forest method on additional datasets with known signal-to-noise ratios to further validate the theoretical dominance claim across a wider range of conditions