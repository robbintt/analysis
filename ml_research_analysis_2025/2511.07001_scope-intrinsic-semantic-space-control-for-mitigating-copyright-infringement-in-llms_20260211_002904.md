---
ver: rpa2
title: 'SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement
  in LLMs'
arxiv_id: '2511.07001'
source_url: https://arxiv.org/abs/2511.07001
tags:
- subspace
- copyrighted
- sparse
- semantic
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses copyright infringement by LLMs, which can inadvertently
  reproduce copyrighted content. It introduces SCOPE, a method that reframes infringement
  mitigation as intrinsic semantic space control using sparse autoencoders (SAEs).
---

# SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs

## Quick Facts
- arXiv ID: 2511.07001
- Source URL: https://arxiv.org/abs/2511.07001
- Authors: Zhenliang Zhang; Xinyu Hu; Xiaojun Wan
- Reference count: 36
- Primary result: Achieves 70-71% win rate on infringement mitigation while preserving utility near baseline

## Executive Summary
This paper addresses copyright infringement by large language models through a novel approach called SCOPE that reframes the problem as intrinsic semantic space control. Instead of relying on external filters or parameter updates, SCOPE uses sparse autoencoders (SAEs) to project dense hidden states into a high-dimensional sparse semantic space where copyright-sensitive features can be isolated and suppressed. The method identifies a linear subspace that preferentially activates on copyrighted content and clamps these activations during decoding, achieving significant infringement reduction while maintaining general capability.

## Method Summary
SCOPE operates in two stages: first, it identifies a copyright-sensitive subspace by encoding both copyrighted and general corpora through a frozen LLM with a pretrained SAE, then ranking sparse dimensions by their Copyright Alignment Score Q(i). The top-n dimensions form the estimated copyrighted subspace. During inference, at each decoding step, the method encodes the hidden state to sparse activations, clamps any dimensions in the estimated subspace exceeding a threshold, reconstructs the modified activations back to dense space, and feeds them into the residual stream. This intrinsic control mechanism requires no retraining or external filtering.

## Key Results
- Achieves highest average win rate of 70-71% on infringement mitigation metrics across NewsQA and BookSum benchmarks
- Preserves general utility with MMLU accuracy near baseline levels (within 1-2 points)
- Demonstrates causal influence of suppressed subspace through reverse-intervention experiments showing win rate drops from 8.7% to 4.1% when features are amplified
- Maintains effectiveness across different model architectures (Gemma-2-9B-IT and Llama-3-8B-Instruct)

## Why This Works (Mechanism)

### Mechanism 1: Monosemantic Disentanglement via SAE Projection
Sparse autoencoders transform entangled dense representations into dimensions that separate copyright-sensitive semantics from general content. The SAE encoder maps hidden states h ∈ R^d to sparse z ∈ R^k using JumpReLU activation with L1 sparsity regularization, forcing each dimension to activate only on semantically coherent inputs and reducing polysemantic superposition.

### Mechanism 2: Copyright-Alignment-Guided Subspace Identification
A subspace that preferentially activates on copyrighted content is identified by ranking dimensions on their Copyright Alignment Score Q(i), which approximates the AUROC for distinguishing copyrighted vs. general corpora. The top-n scoring dimensions form the estimated copyrighted subspace, justified by the upper bound Q(S) ≤ max_i∈I Q(i).

### Mechanism 3: Causal Suppression via Activation Clamping
Clamping activations in the identified subspace during decoding causally reduces copyright infringement while preserving general capability. At each decoding step, after SAE encoding to z, any dimension in the estimated subspace with z_i > τ is set to zero, the modified z is decoded back to ĥ, and injected into the residual stream to steer subsequent generation away from the suppressed subspace.

## Foundational Learning

- **Polysemanticity vs. Monosemanticity**: Dense LLM neurons entangle multiple concepts, making targeted control impossible. The SAE transformation is motivated by the need to achieve monosemantic representations where each dimension corresponds to a single interpretable concept.
  - Quick check: If a single neuron activates strongly on both "copyrighted dialogue" and "general conversation," can you selectively suppress only the first without the SAE transformation?

- **Sparse Autoencoder Architecture**: SCOPE depends entirely on the SAE producing interpretable sparse features. Understanding how JumpReLU thresholds enforce sparsity and why reconstruction loss + L1 penalty yields disentangled representations is critical.
  - Quick check: If you increase the L1 penalty λ too much, what happens to reconstruction fidelity, and how would that affect downstream clamping accuracy?

- **Semantic Subspace Hypothesis**: The paper assumes copyright-relevant semantics occupy a distinct linear subspace, though this is not proven. If semantics are non-linearly distributed, the greedy linear subspace approach will fail.
  - Quick check: What would counter-evidence look like—that is, if copyright-relevant concepts were distributed across many weakly activated dimensions rather than concentrated in a separable subspace?

## Architecture Onboarding

- **Component map**: Input tokens → LLM Decoder Block → Hidden state h → SAE Encoder → Sparse activation z → Copyright Subspace Mask → Clamping → Modified z → SAE Decoder → Reconstructed ĥ → Residual stream → Continue decoding

- **Critical path**: SAE must be pre-trained and attached at the correct residual stream layer (ℓ = 20 for Gemma-2-9B-IT, ℓ = 25 for Llama-3-8B-Instruct). Subspace must be pre-computed using C_cr and C_gen before deployment. At each decoding step, clamping must occur before the SAE decoder reconstructs ĥ.

- **Design tradeoffs**: Subspace size n: higher n → better infringement mitigation but potential utility loss (optimal at n = 1000). Activation threshold τ: controls which activations are clamped (set at τ = 5). Corpus selection for Î estimation: narrow C_cr limits generalization; broad C_gen increases noise.

- **Failure signatures**: Utility degradation (MMLU accuracy or in-domain F1/ROUGE drops >2-3 points suggests over-suppression). Paraphrased leakage (if infringement metrics improve but semantic similarity remains high, Ť may capture surface tokens). Hallucination increase (if clamping disrupts coherence).

- **First 3 experiments**: 1) Validate SAE monosemanticity on target model by visualizing dimension activations. 2) Ablate subspace size n ∈ {500, 1000, 1500, 2000} to find optimal trade-off. 3) Implement reverse intervention to verify causal relevance by amplifying subspace features and observing infringement increase.

## Open Questions the Paper Calls Out

Does the identified copyrighted subspace genuinely capture infringement semantics, or does it merely overfit to distributional differences like topic or format? While the authors argue against overfitting by citing utility preservation and causal links, they acknowledge it remains a "critical question" and inherent risk in representation engineering.

Can adaptive or non-linear subspace methods improve mitigation performance over the current linear approximation? The current method assumes copyright features reside in a linear subspace, which may be a simplification of the complex, non-linear boundaries inherent in high-dimensional semantic spaces.

How can this methodology be adapted for closed-source proprietary models that lack accessible hidden states or pre-trained SAEs? SCOPE relies on projecting internal dense hidden states into a sparse space using an SAE attached to a specific residual stream, an architecture impossible to replicate without model weights.

## Limitations

- The method requires access to intermediate hidden states and pre-trained SAEs, restricting applicability to open-source models only
- Copyright-relevant semantics may not reside in a linear subspace, making the greedy selection method an approximation
- The approach relies on representative copyrighted and general corpora for subspace identification, which may not generalize to all infringement scenarios

## Confidence

- SAE-based semantic disentanglement (High): Well-established in prior literature, though model-specific validation remains needed
- Copyright-sensitive subspace identification via Q(i) scores (Medium): Novel metric with reasonable theoretical foundation but no external validation
- Causal suppression effectiveness (Medium): Supported by internal reverse-intervention experiments but limited to controlled scenarios
- General utility preservation (High): Empirically demonstrated across multiple benchmarks with quantifiable metrics

## Next Checks

1. **Generalization test across copyright domains**: Apply SCOPE to copyright corpora outside the original domain (e.g., news→academic papers, books→software code) to verify subspace identification transfers beyond training distributions

2. **Robustness against adversarial prompting**: Systematically test SCOPE's effectiveness against intentionally crafted prompts designed to bypass suppression while maintaining semantic similarity to copyrighted material

3. **Ablation study on corpus representativeness**: Quantify how variations in the ratio and composition of C_cr vs C_gen affect Q(i) scores and downstream suppression performance, particularly when general corpus overlaps significantly with copyrighted content