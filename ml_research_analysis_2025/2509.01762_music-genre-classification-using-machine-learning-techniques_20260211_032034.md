---
ver: rpa2
title: Music Genre Classification Using Machine Learning Techniques
arxiv_id: '2509.01762'
source_url: https://arxiv.org/abs/2509.01762
tags:
- music
- classification
- features
- genre
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses music genre classification using both traditional
  machine learning techniques and deep learning approaches. The core method idea involves
  extracting hand-crafted audio features from the GTZAN dataset, including temporal
  features like zero-crossing rate and spectral features like MFCCs and chroma, and
  using them to train classical classifiers like SVM, Random Forest, and Gradient
  Boosting.
---

# Music Genre Classification Using Machine Learning Techniques

## Quick Facts
- arXiv ID: 2509.01762
- Source URL: https://arxiv.org/abs/2509.01762
- Authors: Alokit Mishra; Ryyan Akhtar
- Reference count: 34
- Primary result: SVM with hand-crafted features (81%) outperforms CNN on mel-spectrograms (85%) on GTZAN dataset

## Executive Summary
This paper compares traditional machine learning techniques with deep learning for music genre classification using the GTZAN dataset. The study extracts hand-crafted audio features including MFCCs, chroma, spectral centroid, and zero-crossing rate, then trains classical classifiers like SVM alongside a CNN on mel-spectrograms. The key finding is that the SVM classifier achieves superior classification accuracy compared to the CNN model, attributed to the strong inductive bias of engineered features providing regularization that mitigates overfitting in data-constrained scenarios. The research highlights the enduring relevance of traditional feature extraction in practical audio processing tasks.

## Method Summary
The study employs two parallel classification pipelines on the GTZAN dataset. The first extracts 57 hand-crafted features (MFCCs, chroma, spectral centroid/bandwidth, ZCR, RMSE, tempo, and central moments) from 30-second audio clips using librosa with pre-emphasis filtering (α=0.97), then trains SVM classifiers with RBF kernels. The second generates mel-spectrograms and trains a CNN with two convolutional layers, ReLU activation, max pooling, dropout, and dense layers. Both approaches use stratified 80/20 train-test splits with normalization to [0,1]. The paper compares accuracy and F1-scores across clean and noisy test conditions.

## Key Results
- SVM classifier achieves 81% accuracy versus CNN's 85% on clean data
- CNN demonstrates superior noise robustness (79% vs SVM's 66% under noisy conditions)
- Feature engineering acts as regularization, preventing overfitting in data-constrained environments
- Substantial genre confusion occurs between rock/metal pairs regardless of model choice

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hand-crafted feature engineering acts as a strong regularizer in data-constrained environments, potentially allowing classical models to generalize better than high-capacity deep learning models.
- **Mechanism:** By manually extracting domain-specific features (MFCCs, Chroma, Spectral Centroid), the input dimensionality is reduced to ~57 highly informative variables. This constrains the hypothesis space, preventing the model from fitting to noise or spurious correlations that a CNN might learn from raw spectrograms when training samples are limited (n=1000).
- **Core assumption:** The extracted features capture the majority of the variance necessary for genre discrimination, making lost spatial information from raw audio redundant rather than informative.
- **Evidence anchors:** Abstract states strong inductive bias of engineered features provides regularization effect; section 6.2 discusses feature engineering as powerful regularization; corpus evidence from similar evaluations.

### Mechanism 2
- **Claim:** Convolutional Neural Networks exhibit superior robustness to signal noise compared to spectral summary statistics used by classical models.
- **Mechanism:** CNNs process the spectrogram as an image, learning local time-frequency patterns. Statistical features like spectral centroid are aggregated scalars; added noise shifts these distributions significantly. CNNs can "look past" noise if underlying spectral structure remains intact, utilizing translation invariance.
- **Core assumption:** Noise affects global statistical distributions more destructively than it obliterates local spatial hierarchies in the spectrogram.
- **Evidence anchors:** Section 5.3 shows CNN consistently outperformed across all settings (79% vs SVM 66%); section 7.1 discusses CNN robustness in real-world scenarios; corpus evidence is primarily from paper's internal experimental data.

### Mechanism 3
- **Claim:** Genre ambiguity in feature space (e.g., Rock vs. Metal) creates a hard ceiling for linear or kernel-based separators like SVM, which CNNs may partially bridge by capturing temporal evolution.
- **Mechanism:** PCA analysis shows overlapping clusters for high-energy genres. An SVM draws single hyperplane in this space. A CNN, processing sequence of features over time, can distinguish genres based on rhythmic structure or temporal progression rather than just aggregate spectral averages.
- **Core assumption:** Temporal ordering of features contains discriminative information lost when features are averaged into single vector for SVM.
- **Evidence anchors:** Section 4.4 shows genres like classical/jazz have distinct clusters while rock/metal overlap; section 5.2 validates PCA findings with substantial misclassifications between rock/metal; corpus evidence from spectral and rhythm feature performance evaluations.

## Foundational Learning

- **Concept: Inductive Bias**
  - **Why needed here:** Central thesis relies on trade-off between strong inductive bias of SVMs versus weak inductive bias of CNNs
  - **Quick check question:** If you switch from an SVM to a CNN, are you increasing or decreasing the strength of the model's initial assumptions about data structure? (Answer: Decreasing)

- **Concept: Bias-Variance Tradeoff**
  - **Why needed here:** Understand why simpler model might outperform complex one on small data; high variance leads to overfitting, high bias leads to underfitting
  - **Quick check question:** In GTZAN dataset (1000 samples), which component of tradeoff is primary risk for CNN? (Answer: Variance/Overfitting)

- **Concept: Mel-Frequency Cepstral Coefficients (MFCCs)**
  - **Why needed here:** Primary hand-crafted features driving SVM's performance; understanding them as compressed representation of spectral envelope is crucial
  - **Quick check question:** Do MFCCs capture temporal rhythm or spectral timbre? (Answer: Primarily spectral timbre/texture)

## Architecture Onboarding

- **Component map:** Input (GTZAN 30s .wav files) -> Preprocessing (Pre-emphasis filter -> Framing) -> Path A (Librosa Extraction -> CSV Aggregation -> SVM) or Path B (Mel-Spectrogram Generation -> CNN -> Dense Layers)

- **Critical path:** The feature extraction pipeline (Path A). Paper claims superior results here in specific contexts, relying on precise calculation of 57 distinct features. Error in windowing (n_fft=2048, hop=512) or pre-emphasis coefficient (α=0.97) will misalign feature space.

- **Design tradeoffs:**
  - CNN: Higher accuracy potential and noise robustness (85% clean/79% noisy), but high computational cost and requires more data to converge
  - SVM: Faster training and better performance on clean, small data (81% clean) per authors, but brittle to noise (drops to 66% noisy) and requires manual feature maintenance

- **Failure signatures:**
  - Genre Confusion: Expect 15-20% confusion between Rock and Metal regardless of model, due to spectral similarity
  - Overfitting in CNN: CNN achieving 90%+ training accuracy but <70% validation accuracy on GTZAN is likely memorizing spectrogram artifacts rather than genre features

- **First 3 experiments:**
  1. Baseline Replication: Train SVM using provided 30-second CSV features against CNN on raw spectrograms to verify reported 81% vs 85% split on clean data
  2. Noise Stress Test: Inject Gaussian noise at varying SNRs (e.g., 10dB, 5dB) into test set to replicate performance degradation curve and confirm CNN robustness
  3. Ablation Study: Retrain SVM removing one feature class at time (e.g., remove MFCCs, then remove Chroma) to determine which specific hand-crafted feature contributes most to "regularization effect"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a dual-stream architecture combining raw mel-spectrograms and hand-crafted features achieve higher accuracy than single-modality models?
- **Basis in paper:** Section 7.2 proposes developing hybrid networks that leverage both raw spectral representations and precomputed features to combine paradigm strengths
- **Why unresolved:** Study evaluated feature sets in isolation; did not test fused approach
- **What evidence would resolve it:** Accuracy comparison between fused dual-input model and standalone SVM (81%) and CNN (85%) baselines on GTZAN dataset

### Open Question 2
- **Question:** Does SVM model's strong inductive bias provide better generalization than CNN when evaluated on out-of-distribution datasets?
- **Basis in paper:** Section 7.2 calls for evaluating trained models on out-of-distribution dataset, hypothesizing SVM may generalize better
- **Why unresolved:** Evaluation restricted to GTZAN benchmark; authors note CNN may be overfitting to specific data distribution
- **What evidence would resolve it:** Performance retention metrics when models trained on GTZAN tested on distinct, diverse music dataset

### Open Question 3
- **Question:** Do SVM and CNN models rely on similar acoustic features for classification, as revealed by explainability techniques like SHAP and GradCAM?
- **Basis in paper:** Section 7.2 suggests Cross-Domain Model Interpretability using SHAP for SVM and Grad-CAM for CNN to yield musicological insights
- **Why unresolved:** Paper compares classification metrics but does not analyze whether models identify identical genre-defining patterns
- **What evidence would resolve it:** Visual attribution maps aligning SHAP values for MFCCs with GradCAM heatmaps on spectrograms

### Open Question 4
- **Question:** Would reported competitiveness of SVM persist if training dataset size were significantly increased beyond 1,000 samples?
- **Basis in paper:** Authors attribute SVM's success to data-constrained nature of dataset (Section 6.2), implying high-capacity CNN is data-starved
- **Why unresolved:** Experiments limited to fixed dataset size, preventing observation of scaling behavior of deep learning model
- **What evidence would resolve it:** Learning curve analysis showing accuracy trends for both models as training samples scale from hundreds to tens of thousands

## Limitations
- GTZAN dataset contains known quality issues (duplicates, distortions) that may inflate accuracy differences
- CNN architecture details remain incomplete, limiting reproducibility
- Noise robustness claims rely on internal testing without standardized SNR values

## Confidence
- SVM advantage claim: Medium (well-specified features, but dataset concerns)
- CNN robustness claim: Medium (internal data only, weak corpus support)
- Temporal evolution mechanism: Low (relies on PCA visualization rather than explicit temporal modeling comparisons)

## Next Checks
1. Verify dataset integrity by screening GTZAN for duplicates and distortions before training
2. Conduct hyperparameter sensitivity analysis for both SVM (C, gamma) and CNN (learning rate, batch size)
3. Test on larger, cleaner dataset (e.g., FMA) to assess scalability of SVM advantage