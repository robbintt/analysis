---
ver: rpa2
title: Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image
  Retrieval
arxiv_id: '2601.20391'
source_url: https://arxiv.org/abs/2601.20391
tags:
- query
- diffusion
- retrieval
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion-augmented interactive text-to-image retrieval (DAI-TIR)
  can be undermined by hallucinated visual cues in diffusion-generated query images.
  These cues conflict with the user's textual intent, degrading retrieval performance.
---

# Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval

## Quick Facts
- arXiv ID: 2601.20391
- Source URL: https://arxiv.org/abs/2601.20391
- Reference count: 40
- Diffusion-generated query images in interactive text-to-image retrieval can introduce hallucinated visual cues that conflict with textual intent, degrading performance.

## Executive Summary
Diffusion-augmented interactive text-to-image retrieval (DAI-TIR) leverages both textual and visual modalities to enhance retrieval accuracy, but diffusion-generated query images often contain hallucinated visual details that conflict with user intent. The proposed Diffusion-aware Multi-view Contrastive Learning (DMCL) framework addresses this by aligning multiple query views—text, diffusion, and fused—with the target image while suppressing hallucinated details. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to enforce multi-view alignment and reduce cross-view drift. Extensive experiments on five benchmarks demonstrate consistent improvements in multi-round Hits@10, with gains up to 7.37% over prior methods.

## Method Summary
The paper proposes DMCL, a training framework for DAI-TIR that mitigates hallucination by aligning multiple query views (text, diffusion, and fused) with the target image. It introduces semantic-consistency and diffusion-aware contrastive objectives to enforce alignment and suppress hallucinated details. The framework uses contrastive learning to pull together consistent views while pushing apart conflicting ones, particularly addressing the mismatch between hallucinated diffusion outputs and textual intent. DMCL is evaluated across five benchmarks, showing robust improvements in retrieval performance through better semantic alignment and reduced cross-view drift.

## Key Results
- DMCL achieves consistent improvements in multi-round Hits@10 across five benchmarks
- Maximum performance gain of 7.37% over prior methods
- Attention visualizations and embedding-space analyses confirm effective filtering of hallucinated cues

## Why This Works (Mechanism)
Diffusion-generated images in interactive retrieval can introduce visual details not present in the user's textual query, creating a mismatch between intended and retrieved results. DMCL works by creating multiple views of each query (text, diffusion, and fused) and using contrastive learning to align them with the target image. The semantic-consistency objective ensures that different views representing the same query intent remain close in embedding space, while the diffusion-aware contrastive objective specifically addresses and suppresses hallucinated details by reducing their influence on the final representation. This multi-view alignment prevents hallucinated cues from dominating the retrieval process.

## Foundational Learning
- **Diffusion models in retrieval**: Why needed - to generate rich visual representations from text; Quick check - can the model generate diverse, relevant images from text prompts
- **Contrastive learning**: Why needed - to align similar representations while separating dissimilar ones; Quick check - do positive pairs cluster together and negatives separate in embedding space
- **Multi-view learning**: Why needed - to leverage complementary information from different query representations; Quick check - are different views providing unique information that improves performance when combined
- **Hallucination in diffusion outputs**: Why needed - to understand the source of errors in visual-textual alignment; Quick check - can you identify visual details in diffusion outputs that don't match the text prompt
- **Interactive text-to-image retrieval**: Why needed - the target application domain; Quick check - does the system improve with user feedback across multiple retrieval rounds

## Architecture Onboarding

Component Map:
User Query -> Text Encoder, Diffusion Model, Fusion Module -> Multiple Views (Text, Diffusion, Fused) -> DMCL Contrastive Learning -> Refined Embeddings -> Image Retriever

Critical Path:
User query flows through text encoder and diffusion model to create multiple views, which are processed by DMCL's contrastive learning objectives to produce refined embeddings used for image retrieval. The semantic-consistency and diffusion-aware contrastive objectives form the core of the training pipeline.

Design Tradeoffs:
The framework trades increased computational complexity (processing multiple views) for improved retrieval accuracy and hallucination suppression. The multi-view approach requires more training resources but provides robustness against individual view failures. The contrastive learning framework adds training complexity but enables better alignment than single-view approaches.

Failure Signatures:
Poor performance when diffusion outputs are severely hallucinated, insufficient training data to learn proper view alignment, or when the fusion module cannot effectively combine information from different views. The system may also struggle with highly abstract or ambiguous queries where textual and visual representations diverge significantly.

First Experiments:
1. Test individual component contributions through ablation studies removing semantic-consistency or diffusion-aware objectives
2. Evaluate retrieval performance on synthetic queries with known hallucination patterns
3. Compare attention distributions between DMCL and baseline methods to verify hallucination suppression

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to curated academic datasets rather than large-scale real-world retrieval scenarios
- Computational overhead during training and inference not addressed
- Improvements show variance across benchmarks (1.24% to 7.37% Hits@10 gains), suggesting dataset-dependent effectiveness

## Confidence
- **High confidence**: Diffusion-generated images can introduce hallucinated visual cues that conflict with textual intent and degrade retrieval performance
- **Medium confidence**: DMCL's multi-view contrastive learning framework improves retrieval metrics with consistent but variable gains across benchmarks
- **Medium confidence**: Semantic-consistency and diffusion-aware contrastive objectives effectively suppress hallucinated details, primarily supported by qualitative visualizations

## Next Checks
1. Evaluate DMCL on a large-scale, real-world text-to-image retrieval dataset with diverse user queries to assess robustness beyond curated academic benchmarks
2. Conduct a user study measuring subjective retrieval quality and hallucination perception when using DMCL versus baseline methods
3. Measure and report the computational overhead (training time, inference latency, memory usage) introduced by the multi-view contrastive learning framework