---
ver: rpa2
title: 'Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with
  Cognitive Diagnosis'
arxiv_id: '2502.13996'
source_url: https://arxiv.org/abs/2502.13996
tags:
- knowledge
- unlearning
- arxiv
- diagnosis
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating and enhancing
  large language model (LLM) unlearning, particularly the removal of harmful capabilities
  like cyberattack knowledge. The authors propose UNCD (UNlearning evaluation using
  Cognitive Diagnosis), a framework that leverages Cognitive Diagnosis Modeling to
  provide fine-grained, interpretable assessments of unlearning effectiveness, moving
  beyond traditional single-value metrics like QA accuracy.
---

# Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis

## Quick Facts
- arXiv ID: 2502.13996
- Source URL: https://arxiv.org/abs/2502.13996
- Reference count: 40
- Primary result: UNCD framework provides fine-grained evaluation of LLM unlearning, uncovering residual harmful knowledge missed by single-value metrics

## Executive Summary
This paper addresses the challenge of evaluating and enhancing large language model (LLM) unlearning, particularly the removal of harmful capabilities like cyberattack knowledge. The authors propose UNCD (UNlearning evaluation using Cognitive Diagnosis), a framework that leverages Cognitive Diagnosis Modeling to provide fine-grained, interpretable assessments of unlearning effectiveness, moving beyond traditional single-value metrics like QA accuracy. They introduce UNCD-Cyber, a dedicated benchmark for evaluating multiple unlearning methods on Llama-3-8B and Mistral-7B, and demonstrate that UNCD uncovers limitations of single-value metrics by revealing residual harmful knowledge components. Furthermore, they present UNCD-Agent, an advanced approach that iteratively diagnoses and mitigates residual knowledge, achieving superior unlearning performance.

## Method Summary
The method applies Cognitive Diagnosis Modeling (CDM) to evaluate LLM unlearning by treating models as "students" and questions as exercises mapped to knowledge concepts via a Q-matrix. The framework uses three CDM approaches (Few-Shot, NCDM, ICDM) to infer knowledge states from response logs, then applies an iterative UNCD-Agent that diagnoses residual concepts and generates targeted unlearning data. The UNCD-Cyber benchmark evaluates eight unlearning methods (GA, NPO, RMU, TV) on cyberattack capability removal, measuring both forget performance (concept-level mastery) and retain performance (general capabilities).

## Key Results
- UNCD reveals that single-value QA accuracy metrics obscure residual harmful knowledge components in unlearned models
- UNCD-Agent iteratively diagnoses and targets specific residual concepts, achieving deeper unlearning than one-shot methods
- Combining multiple CDM approaches (Few-Shot, NCDM, ICDM) provides robust and consistent knowledge state estimation

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Diagnosis Decomposes Aggregate Metrics
Treating LLMs as "students" and applying Cognitive Diagnosis Modeling reveals fine-grained knowledge states that single-value QA accuracy obscures. The Q-matrix explicitly maps each evaluation question to specific knowledge concepts (e.g., "defense-evasion," "reconnaissance"). CDMs (NCDM, ICDM) infer latent knowledge states $F_s = [F_s^1, F_s^2, ..., F_s^K]$ from observable response logs, where $F_s^k$ quantifies mastery on concept $k$. This decomposition allows detecting residual harmful concepts even when aggregate accuracy drops.

### Mechanism 2: Iterative Diagnosis-Driven Unlearning Closes Residual Gaps
Systematic identification of high-mastery residual concepts followed by targeted unlearning data generation achieves deeper removal than one-shot methods. UNCD-Agent performs a "test and unlearn" loop: (1) diagnose post-unlearning knowledge states using CDM; (2) identify concepts with $F_s^k$ above threshold; (3) generate concept-specific unlearning data using GPT-4o; (4) apply additional unlearning. This targets the "long tail" of persistent concepts.

### Mechanism 3: Multi-Method Diagnosis Increases Robustness
Combining training-free few-shot knowledge tracing with neural CDMs (NCDM, ICDM) provides consistent diagnostics even under varying question availability. Three approaches—Few-Shot (qualitative labels from teacher LLM), NCDM (one-hot embeddings), ICDM (graph-based encoding)—produce correlated knowledge states. Agreement measured via Degree of Agreement (DOA) and prediction accuracy validates reliability.

## Foundational Learning

- **Concept: Cognitive Diagnosis Modeling (CDM)**
  - Why needed here: UNCD adapts educational psychometrics to LLM evaluation. Understanding how CDMs infer latent states from observed responses is essential for interpreting diagnosis results.
  - Quick check question: Given a Q-matrix mapping questions to concepts and a student's response log, what does CDM output?

- **Concept: Gradient Ascent vs. Negative Preference Optimization**
  - Why needed here: The paper benchmarks eight unlearning methods. Distinguishing loss functions (GA maximizes forget-set likelihood; NPO treats forget data as negative preference via DPO-style objective) clarifies why different methods yield divergent unlearning trajectories.
  - Quick check question: Why does NPO include a reference model $f_0$ while GA does not?

- **Concept: MITRE ATT&CK Framework**
  - Why needed here: UNCD-Cyber's knowledge concepts are derived from MITRE ATT&CK techniques and tactics. Familiarity with this taxonomy (e.g., "initial access," "persistence," "exfiltration") is necessary to interpret radar charts and domain-level diagnoses.
  - Quick check question: What is the relationship between a MITRE "tactic" and a "technique"?

## Architecture Onboarding

- **Component map:** Unlearn Dataset → Unlearning Methods (GA, NPO, RMU, TV, etc.) → Checkpoints → Evaluation Dataset → Response Logs R → Few-Shot KT, NCDM, ICDM → Knowledge States F_s → UNCD-Agent (if residual detected)

- **Critical path:**
  1. Construct Q-matrix with explicit concept-to-question mapping (UNCD-Cyber uses MITRE techniques/tactics + CS-Bench domains)
  2. Collect response logs from base model, unlearned model, and intermediate checkpoints
  3. Train CDMs on response logs; extract knowledge states per concept
  4. If $M_s$ (average mastery) exceeds threshold, trigger UNCD-Agent for targeted data generation and re-unlearning

- **Design tradeoffs:**
  - NCDM vs. ICDM: NCDM requires more questions (~5,000) but simpler implementation; ICDM needs fewer (~2,500) but requires graph construction and data augmentation
  - Few-Shot vs. Neural CDM: Few-Shot is efficient (100 questions) but produces coarse labels ("good"/"fair"/"bad"); neural CDMs provide real-valued precision but need training
  - Easy vs. Hard evaluation sets: Easy set isolates single concepts (cleaner diagnosis); hard set tests multi-concept integration but introduces attribution ambiguity

- **Failure signatures:**
  - Low DOA across CDMs: Indicates unreliable Q-matrix or insufficient response coverage
  - High forget performance but low retain performance: Over-aggressive unlearning (e.g., pure GA without GDR/KLR regularization)
  - Uneven radar chart with persistent spikes: Incomplete unlearning on specific concepts—candidate for UNCD-Agent targeting

- **First 3 experiments:**
  1. Baseline replication: Run GA, NPO, RMU on UNCD-Cyber with a new base model; verify QA accuracy correlates with $M_s$ (Pearson R > 0.9 expected per Table 2)
  2. Ablation on Q-matrix granularity: Compare domain-level vs. technique-level diagnosis to test whether finer granularity improves residual detection
  3. UNCD-Agent on synthetic residuals: Intentionally under-unlearn one concept (early stopping), then apply Agent to verify targeted reduction on that concept while preserving others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the UNCD-Agent's iterative "test and unlearn" loop be optimized to prevent the observed utility degradation?
- Basis in paper: Section 5 concludes that while UNCD-Agent successfully removes residual knowledge, it "still suffers from a slight utility degradation" (Figure 8).
- Why unresolved: The current implementation generates targeted unlearning data based on diagnosis but lacks a mechanism to strictly preserve general capabilities (fluency/MMLU) while erasing specific residual concepts.
- What evidence would resolve it: An iteration of UNCD-Agent that achieves near-zero mastery ($M_s$) on forget sets while maintaining baseline performance on MT-Bench and MMLU.

### Open Question 2
- Question: Can UNCD effectively evaluate "data influence removal" tasks, such as copyright erasure or privacy scrubbing?
- Basis in paper: Section 1 explicitly differentiates "capability removal" (the paper's focus) from "data influence removal," noting that capability removal cannot be solved by simple retraining, but the framework is only validated on the cyberattack capability domain.
- Why unresolved: UNCD relies on mapping knowledge concepts (KCs) via a Q-matrix (MITRE ATT&CK). It is unclear if specific textual memorization (e.g., Harry Potter text) can be decomposed into "knowledge concepts" suitable for Cognitive Diagnosis Modeling.
- What evidence would resolve it: Application of the UNCD framework to a benchmark like MUSE or TOFU, showing successful diagnosis of memorized text retention versus factual capability.

### Open Question 3
- Question: Does the correlation between QA accuracy and knowledge mastery ($M_s$) persist in larger or Mixture-of-Expert (MoE) models?
- Basis in paper: Section 4.1 limits experiments to Llama-3-8B and Mistral-7B. The paper notes "divergent unlearning trajectories" in these dense models, but knowledge distribution may differ significantly in larger architectures.
- Why unresolved: Cognitive Diagnosis Models assume a specific interaction function between student ability and exercise difficulty; the validity of this mapping for models with vastly different parameter counts or architectures (like MoE) is untested.
- What evidence would resolve it: Benchmarking UNCD on models larger than 70B parameters or MoE architectures (e.g., Mixtral) to verify if $M_s$ remains a consistent predictor of unlearning success compared to QA accuracy.

## Limitations

- Q-matrix construction quality remains a critical uncertainty without systematic validation of expert mappings
- CDM ground truth validity cannot be confirmed without human expert assessment of actual knowledge states
- UNCD-Agent efficacy verification lacks rigorous ablation studies comparing targeted versus uniform unlearning

## Confidence

**High Confidence (8/10):** The core mechanism of using cognitive diagnosis to decompose aggregate metrics into concept-level knowledge states is well-established in educational psychology and logically extends to LLM evaluation.

**Medium Confidence (6/10):** Claims about UNCD-Agent's superior performance are supported by qualitative evidence but lack quantitative comparisons against non-Agent baselines for the same number of unlearning steps.

**Low Confidence (4/10):** The generalizability of UNCD beyond cybersecurity domains remains unproven without evidence of effectiveness on other harmful capabilities.

## Next Checks

**Check 1:** Perform expert human validation of Q-matrix accuracy by having cybersecurity professionals independently map a subset of evaluation questions to MITRE techniques.

**Check 2:** Conduct ablation studies comparing UNCD-Agent to a baseline that performs the same number of unlearning steps without iterative diagnosis.

**Check 3:** Test UNCD on non-cybersecurity harmful capabilities by constructing a new benchmark for misinformation detection/removal.