---
ver: rpa2
title: Are LLMs complicated ethical dilemma analyzers?
arxiv_id: '2505.08106'
source_url: https://arxiv.org/abs/2505.08106
tags:
- ethical
- human
- llms
- expert
- dilemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) can
  emulate human ethical reasoning by evaluating their performance on structured ethical
  dilemmas. A benchmark dataset of 196 real-world dilemmas was created, with expert
  and non-expert human responses.
---

# Are LLMs complicated ethical dilemma analyzers?

## Quick Facts
- arXiv ID: 2505.08106
- Source URL: https://arxiv.org/abs/2505.08106
- Reference count: 10
- LLMs can generate structured ethical reasoning but struggle with historical context and nuanced resolution strategies

## Executive Summary
This study evaluates whether large language models can emulate human ethical reasoning by testing their performance on 196 real-world ethical dilemmas. A structured benchmark dataset with expert and non-expert human responses was created, and models were prompted to generate five-section responses (Introduction, Key Factors, Historical Perspectives, Resolution Strategies, Takeaways). Using a composite evaluation framework combining BLEU, DL distance, TF-IDF, and USE similarity metrics, GPT-4o-mini consistently outperformed other models, particularly in identifying key factors and resolution strategies. However, all models struggled with historical reasoning and nuanced strategy proposals. The study reveals LLMs' strengths in structured coherence while highlighting limitations in replicating human ethical depth.

## Method Summary
The study created a benchmark of 196 ethical dilemmas with expert opinions segmented into five structured components. LLMs were prompted to generate responses following the same five-section format, and their outputs were compared against four expert reference versions using a composite metric combining BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights were derived through inversion analysis against human rankings plus AHP pairwise comparisons. Non-expert human responses provided a baseline, though limited to Key Factors section only. Performance was evaluated both component-wise and aggregated across all sections.

## Key Results
- GPT-4o-mini achieved highest overall scores (0.5049) across all evaluation metrics
- All models scored lowest on Historical & Theoretical Perspectives section (0.35-0.38)
- Non-expert humans showed lower lexical alignment (BLEU scores below 0.2) but comparable semantic similarity to LLMs
- Claude-3.5-Sonnet performed worst overall (0.4111), particularly weak on semantic similarity metrics

## Why This Works (Mechanism)

### Mechanism 1: Structured Decomposition Enables Component-wise Evaluation
- Decomposing ethical reasoning into five fixed components allows systematic, interpretable comparison between LLM outputs and expert references
- A unified prompting strategy forces all models to produce outputs in the same five-section format, with each section evaluated independently
- Core assumption: Expert ethical reasoning can be meaningfully segmented into discrete, evaluable components without losing essential coherence

### Mechanism 2: Inversion-Based Metric Selection Aligns with Human Judgment
- Selecting evaluation metrics by minimizing inversion count against human rankings produces weights that better reflect perceived response quality
- Starting with 20+ candidate metrics, the authors manually ranked 10-20 model outputs and computed discordant pairs to select the four lowest-inversion metrics
- Core assumption: Human manual rankings constitute valid ground truth for metric selection, and low inversion count correlates with better evaluation

### Mechanism 3: Semantic-Lexical Divergence Reveals Reasoning Patterns
- LLMs outperform non-expert humans on lexical metrics but achieve comparable semantic similarity, indicating surface-level alignment vs. conceptual understanding
- Lexical metrics reward exact n-gram overlap (which LLMs optimize), while semantic metrics capture embedding-space similarity (where humans perform comparably)
- Core assumption: Lexical and semantic metrics capture distinct aspects of reasoning quality, revealing meaningful differences in reasoning approaches

## Foundational Learning

- **BLEU and N-gram Metrics**:
  - Why needed here: Quantify surface-level lexical overlap between model outputs and references
  - Quick check question: Can you explain why BLEU scores alone would be insufficient for evaluating ethical reasoning quality?

- **Universal Sentence Encoder (USE) and Semantic Embeddings**:
  - Why needed here: Captures meaning beyond lexical overlap, enabling evaluation of conceptual alignment even when phrasing differs
  - Quick check question: Why might USE similarity be higher than BLEU for human responses to ethical dilemmas?

- **Inversion Loss and Ranking Alignment**:
  - Why needed here: Provides a differentiable proxy for how well metric-produced rankings align with human judgment
  - Quick check question: Given ground truth ranking [1, 2, 3, 4], how many inversions exist in predicted ranking [3, 1, 2, 4]?

## Architecture Onboarding

- **Component map**: Raw dilemma descriptions → structured expert opinions via prompting → LLM inference (unified prompt) → five-section response → metric computation (BLEU, DL, TF-IDF, USE) → weight aggregation (inversion-derived weights + AHP) → final composite score

- **Critical path**: Prompt engineering for five-section format consistency → multi-reference evaluation (four expert versions per dilemma) → metric selection and weighting via inversion analysis → component-wise and aggregate score interpretation

- **Design tradeoffs**: Composite metrics vs. interpretability (four weighted metrics provide robustness but complicate failure diagnosis); expert-only vs. human baseline (non-expert responses provide realism but limited to Key Factors section); structured vs. open-ended (fixed sections enable comparison but may constrain nuanced reasoning)

- **Failure signatures**: Historical & Theoretical Perspectives (all models score lowest: 0.35-0.38, indicating difficulty with long-range coherence and contextual abstraction); Resolution Strategies (second-lowest: 0.25-0.30, suggesting challenges in generating nuanced, actionable recommendations); Claude-3.5-Sonnet (lowest overall: 0.4111, particularly weak on semantic similarity and TF-IDF cosine)

- **First 3 experiments**:
  1. Reproduce metric selection: Manually rank 10 model outputs, compute inversion counts for candidate metrics, verify weight derivation matches paper (semantic ~0.54, n-gram ~0.15, cosine ~0.23, lexical ~0.08)
  2. Component-wise failure analysis: Isolate Historical Perspectives section, analyze why all models underperform—hypothesize that this requires domain knowledge not well-represented in training data
  3. Human baseline extension: Collect full five-section responses from non-experts (with structured prompts) to test whether the semantic-lexical gap persists when humans are given the same scaffolding as LLMs

## Open Questions the Paper Calls Out

- Can supervised fine-tuning on this structured benchmark improve LLM alignment with expert reasoning, particularly in underperforming areas like resolution strategies?
- Does LLM performance on academic research ethics cases generalize to other ethical domains such as legal, environmental, or socio-political dilemmas?
- How do LLMs perform in collaborative or adversarial multi-agent systems when tasked with moral reasoning and conflict negotiation?

## Limitations

- The inversion-based metric selection process relies on manual rankings of only 10-20 model outputs, raising questions about generalizability and potential ranker bias
- Historical Perspectives and Resolution Strategies sections show consistently low performance across all models, but the specific domain knowledge requirements remain unclear
- Non-expert human responses were limited to Key Factors only, preventing full comparison across all five components
- The expert reference generation process (four distinct LLM-processed summaries) may introduce systematic biases not fully characterized

## Confidence

- High confidence: Component-wise evaluation framework structure and overall methodology for comparing LLM outputs to expert references
- Medium confidence: Metric weight derivation through inversion analysis and the semantic-lexical divergence interpretation
- Low confidence: Generalizability of findings to other ethical domains and the adequacy of USE embeddings for capturing ethical nuance

## Next Checks

1. Replicate the inversion-based metric selection by manually ranking 20 model outputs and verifying that the same four metrics (BLEU, DL distance, TF-IDF, USE) emerge with lowest inversion counts
2. Extend human baseline collection to include full five-section responses from non-experts to test whether semantic-lexical gap persists with identical prompting structure
3. Analyze specific failure patterns in Historical Perspectives section by examining expert references and testing whether domain-specific fine-tuning improves performance