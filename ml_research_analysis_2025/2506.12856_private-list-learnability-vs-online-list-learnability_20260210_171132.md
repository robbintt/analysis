---
ver: rpa2
title: Private List Learnability vs. Online List Learnability
arxiv_id: '2506.12856'
source_url: https://arxiv.org/abs/2506.12856
tags:
- learning
- k-list
- dimension
- proof
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between differential privacy
  (DP) and online learning in the context of PAC list learning, where learners output
  k predictions and incur loss if the true label is not in the list. The main question
  addressed is whether the equivalence between private learnability and online learnability
  (established in multiclass classification) extends to k-list learning.
---

# Private List Learnability vs. Online List Learnability

## Quick Facts
- arXiv ID: 2506.12856
- Source URL: https://arxiv.org/abs/2506.12856
- Reference count: 40
- One-line primary result: Equivalence between private learnability and online learnability for k-list learning does not hold when k > 1; both k-Littlestone and k-monotone dimensions are necessary conditions for DP k-list learnability.

## Executive Summary
This paper investigates whether the equivalence between private learnability and online learnability, established in multiclass classification, extends to k-list learning where learners output k predictions and incur loss if the true label is not in the list. The authors prove that this equivalence breaks down for k-list learning when k > 1. While finite k-Littlestone dimension remains a necessary condition for DP k-list learnability, it is not sufficient. The paper introduces the k-monotone dimension as another necessary condition, demonstrating that both dimensions must be finite for private k-list learnability. A key result shows that monotone functions with k+1 labels over natural numbers are online k-list learnable (with mistake bound 1) but not DP k-list learnable, establishing a fundamental separation between online and private learnability in the list setting.

## Method Summary
The paper employs Ramsey-theoretic techniques to establish lower bounds on sample complexity for differentially private k-list learners. Two main proof approaches are used: Ramsey theorem for b-ary trees combined with reduction from the Interior Point Problem for the k-Littlestone dimension bound, and classical Ramsey theorem for hypergraphs with packing arguments for the k-monotone dimension bound. The authors construct specific concept classes demonstrating the gap between these dimensions and prove that finite k-Littlestone dimension alone is insufficient for private learnability. The key insight is that k-list learning requires a more nuanced, potentially multi-parameter characterization than single-dimensional measures used in standard classification.

## Key Results
- Finite k-Littlestone dimension is necessary but not sufficient for DP k-list learnability.
- Finite k-monotone dimension is a distinct, necessary condition for DP k-list learnability, independent of k-Littlestone dimension.
- The class of monotone functions with k+1 labels over natural numbers is online k-list learnable (mistake bound 1) but not DP k-list learnable.
- Both k-Littlestone and k-monotone dimensions can be finite or infinite independently, requiring both to be finite for private learnability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Finite k-Littlestone dimension is a necessary condition for DP k-list learnability.
- **Mechanism:** Generalization of Ramsey theorems for binary trees to (k+1)-ary trees shows that infinite k-Littlestone dimension forces "comparison-based" behavior on large subtrees, which is reduced to the Interior Point Problem (IPP) known to be hard for differential privacy.
- **Core assumption:** The hardness of IPP scales effectively to the list-learning setting via the comparison-based reduction.
- **Evidence anchors:** Abstract statement of necessity, section 2.1 proof steps, and corpus support linking stability/privacy to combinatorial dimensions.
- **Break condition:** If reduction to IPP fails for (k+1)-ary trees, the lower bound may not hold.

### Mechanism 2
- **Claim:** Finite k-monotone dimension is a distinct, necessary condition for DP k-list learnability.
- **Mechanism:** Classical Ramsey theorem for hypergraphs identifies that infinite k-monotone dimension allows embedding of "monotone" structures that prevent private learning, generalizing threshold dimension barriers from binary classification.
- **Core assumption:** The definition of k-monotone dimension accurately captures "threshold-like" hardness for list learning.
- **Evidence anchors:** Abstract introduction of k-monotone dimension, section 2.2 proof showing algorithm's predictions depend only on comparisons.
- **Break condition:** If future work finds a learner that bypasses monotone structure constraints without violating privacy.

### Mechanism 3
- **Claim:** Equivalence between online learnability and private learnability breaks for k-list learning when k > 1.
- **Mechanism:** Monotone functions with k+1 labels over natural numbers have finite k-Littlestone dimension (implying online learnability) but infinite k-monotone dimension, making them not DP learnable.
- **Core assumption:** The construction of M_k(ℕ) is representative of the general separation for k > 1.
- **Evidence anchors:** Abstract statement of separation, section 4 proof of class with LD_k=1 and MD_k=∞.
- **Break condition:** This separation is proven; break would only apply if finite MD were proven insufficient in future work.

## Foundational Learning

- **Concept: k-List Learning**
  - **Why needed here:** Core object of study where learner outputs k labels and wins if true label is in set.
  - **Quick check question:** If a learner outputs a list of size k=3 for a label space of size 5, what is the probability of winning by random guessing? (Answer: 3/5).

- **Concept: k-Littlestone Dimension vs. Littlestone Dimension**
  - **Why needed here:** Standard LD characterizes online learning, but k-LD uses (k+1)-ary trees for lists, and interacts differently with privacy than standard LD.
  - **Quick check question:** Does finite k-Littlestone dimension guarantee private learnability in this paper? (Answer: No, only online learnability).

- **Concept: Ramsey Theory (Trees vs. Hypergraphs)**
  - **Why needed here:** Proofs rely on two different Ramsey tools; understanding which applies to which dimension (Trees → LD, Hypergraphs → MD) is key.
  - **Quick check question:** Which combinatorial tool establishes lower bound for k-monotone dimension? (Answer: Classical Ramsey theorem for hypergraphs).

## Architecture Onboarding

- **Component map:** Concept class C with (k+1)-ary tree structure -> Ramsey theorem application -> Comparison-based behavior -> Interior Point Problem reduction -> Sample complexity lower bound
- **Critical path:** 1) Check finite k-Littlestone Dimension? If no → Not Online/Private Learnable. 2) Check finite k-Monotone Dimension? If no → Not Private Learnable. 3) If both finite → Candidate for Private Learnability.
- **Design tradeoffs:** List size k increases arity of mistake trees (k+1), altering Ramsey theorem bounds; accuracy vs privacy tradeoff shows certain classes require unbounded samples regardless of accuracy targets.
- **Failure signatures:** High sample complexity when class shatters deep (k+1)-ary trees or contains long monotone sequences; separation occurs when online learnable class seems hard to make private due to infinite k-monotone dimension.
- **First 3 experiments:** 1) Implement monotone function class M_k(ℕ), verify online learner mistake bound of 1 and DP learner failure. 2) Vary list size k and measure shattered tree depth required to force privacy breach. 3) Attempt to construct DP learner for class with both finite LD and MD to probe sufficiency question.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is finiteness of both k-Littlestone dimension (LD_k) and k-monotone dimension (MD_k) sufficient for a concept class to be differentially privately (DP) PAC k-list learnable?
- **Basis in paper:** Abstract and introduction explicitly state this as open question asking whether classes with finite LD_k(C) and MD_k(C) are DP PAC k-list learnable.
- **Why unresolved:** Paper proves these dimensions are necessary conditions and neither is sufficient alone, but does not establish if their combination is sufficient.
- **What evidence would resolve it:** Proof showing finite LD_k and MD_k imply private k-list learnability, or counterexample class with finite dimensions but not privately learnable.

### Open Question 2
- **Question:** Can private k-list learnability be characterized by a single combinatorial dimension, or does it inherently require multi-parameter characterization?
- **Basis in paper:** Introduction states necessity of two incomparable dimensions "suggests that a more nuanced, potentially multi-parameter or non-combinatorial characterization may be required."
- **Why unresolved:** In standard multiclass learning, Littlestone and threshold dimensions are finite together, but in list learning these dimensions are independent, breaking single-parameter paradigm.
- **What evidence would resolve it:** Identification of single combinatorial parameter implying both dimensions, or formal proof that no single parameter suffices.

### Open Question 3
- **Question:** What are the upper bounds on sample complexity of DP PAC k-list learning in terms of k-Littlestone and k-monotone dimensions?
- **Basis in paper:** Paper establishes Ω(log* d) lower bounds for both dimensions but does not provide corresponding upper bounds or tight bounds.
- **Why unresolved:** Focus is on qualitative separation and establishing necessary conditions rather than quantifying precise sample complexity for sufficient case.
- **What evidence would resolve it:** Algorithmic construction with sample complexity upper bound as function of LD_k(C) and MD_k(C), ideally matching log* dependency.

## Limitations

- The sufficiency of both finite k-Littlestone and k-monotone dimensions together for private learnability remains an open question.
- Proof techniques rely on Ramsey-theoretic arguments whose quantitative bounds are not fully specified beyond Ω(log* d) notation.
- The separation is demonstrated for a specific construction (monotone functions with k+1 labels) but generality across concept classes requires further investigation.

## Confidence

- **High Confidence:** Necessity of finite k-Littlestone dimension for DP k-list learnability (Theorem 2).
- **High Confidence:** Necessity of finite k-monotone dimension as distinct requirement (Theorem 4).
- **Medium Confidence:** Specific construction of monotone functions M_k(ℕ) achieving the separation.
- **Low Confidence:** Sufficiency question (whether finite k-LD and k-MD together guarantee DP learnability) remains completely open.

## Next Checks

1. **Implement and Verify the Separation:** Code the monotone function class M_k(ℕ) and implement both the online learner (mistake bound 1) and attempt a DP learner. Measure whether DP learners fail to achieve reasonable sample complexity, confirming the theoretical separation.

2. **Construct Counterexamples for Sufficiency:** Search for or construct concept classes with both finite k-Littlestone and k-monotone dimensions where DP learning provably fails. This would establish that even both conditions together are insufficient.

3. **Quantitative Bound Analysis:** Work through the Ramsey-theoretic proofs to extract explicit constants in the sample complexity lower bounds, moving beyond the current Ω(log* d) notation to understand practical implications.