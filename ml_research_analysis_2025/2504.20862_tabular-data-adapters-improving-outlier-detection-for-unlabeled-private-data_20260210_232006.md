---
ver: rpa2
title: 'Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data'
arxiv_id: '2504.20862'
source_url: https://arxiv.org/abs/2504.20862
tags:
- dataset
- datasets
- public
- soft
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying outlier detection
  models trained on public datasets to private, unlabeled tabular data. The authors
  propose Tabular Data Adapters (TDA), a method that first identifies statistically
  similar public datasets, then transforms private data into a format compatible with
  public models using a shared autoencoder.
---

# Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data

## Quick Facts
- arXiv ID: 2504.20862
- Source URL: https://arxiv.org/abs/2504.20862
- Authors: Dayananda Herurkar; Jörn Hees; Vesselin Tzvetkov; Andreas Dengel
- Reference count: 40
- Primary result: TDA significantly outperforms baseline approaches, achieving up to 62.5% balanced accuracy compared to 57.7% for default OD methods, while requiring substantially less computational time (9.94 minutes vs 27.7 minutes).

## Executive Summary
This paper addresses the challenge of applying outlier detection models trained on public datasets to private, unlabeled tabular data. The authors propose Tabular Data Adapters (TDA), a method that first identifies statistically similar public datasets, then transforms private data into a format compatible with public models using a shared autoencoder. This enables generation of soft labels for unlabeled data, addressing the cold start problem in industrial applications. Experiments across 50 diverse tabular datasets show TDA significantly outperforms baseline approaches, achieving up to 62.5% balanced accuracy compared to 57.7% for default OD methods, while requiring substantially less computational time (9.94 minutes vs 27.7 minutes). The approach demonstrates robustness across healthcare and finance domains, offering a practical solution for leveraging public research models in real-world applications with unlabeled private data.

## Method Summary
TDA addresses the cold start problem in outlier detection for unlabeled private tabular data by leveraging pre-trained public models. The method consists of three core components: (1) dataset similarity measurement using PCA reconstruction error differences, (2) cross-domain data transformation via shared autoencoders, and (3) ensemble soft labeling with pre-optimized public models. The approach first identifies the most statistically similar public datasets to the private data, then trains a shared autoencoder to transform private samples into the format of the selected public dataset. Finally, it applies the best-performing OD models from the public dataset to the transformed data and fuses predictions via majority voting to generate soft labels. The method is evaluated across 50 diverse tabular datasets, demonstrating significant improvements in accuracy while reducing computational requirements.

## Key Results
- TDA achieves up to 62.5% balanced accuracy compared to 57.7% for default OD methods
- Method1 (Top1-DS) and Method2 (TopN-DS) outperform Avg OD and Default OD baselines in terms of Balanced Accuracy, F1-score, and PR-AUC
- Computational efficiency: TDA requires 9.94 minutes vs 27.7 minutes for baseline methods
- ROC-AUC values range from 0.683-0.689, slightly below baselines (0.690-0.691), indicating elevated false positive rates on borderline cases

## Why This Works (Mechanism)

### Mechanism 1: Dataset Similarity via PCA Reconstruction Error
- Claim: Selecting statistically similar public datasets improves soft label accuracy for private data.
- Mechanism: PCA is applied to both private and public datasets separately. Mean reconstruction error is computed across 1-100 principal components. Sum of absolute differences (SAD) between error curves ranks datasets by similarity—lower SAD indicates more similar data complexity and structure.
- Core assumption: Datasets with similar PCA reconstruction characteristics will have comparable outlier distributions, enabling effective model transfer.
- Evidence anchors:
  - [abstract] "identifying statistically similar public datasets"
  - [section 3.1] "The advantage of this measure over existing similarity methods is that it operates independently of feature dimensionality and does not rely on label information"
  - [section 5.3, Figure 8] "The results... reveal an upward trend: as dataset similarity increases, the balanced accuracy of soft label predictions also improves"
  - [corpus] Limited direct corpus validation; related work "From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection" addresses model selection for OD but uses different approaches.
- Break condition: If SAD scores are uniformly high across all public datasets (no clear "most similar" candidate), the approach degrades to near-baseline performance.

### Mechanism 2: Cross-Domain Transformation via Shared Latent Space
- Claim: Paired autoencoders with shared bottleneck layers enable format-compatible data transformation between structurally different datasets.
- Mechanism: Dedicated encoder-decoder pairs learn dataset-specific patterns; shared middle layers (64→32→64) capture transferable representations. During inference, private data traverses private encoder → shared layers → public decoder, producing "crossover" samples in public dataset format.
- Core assumption: Outlier patterns are preserved through the latent space transformation despite structural differences.
- Evidence anchors:
  - [abstract] "transforming private data (based on a shared autoencoder) into a format compatible with public models"
  - [section 3.1, Eq. 4] "ˆxco = Decpub(mθ(Encprv(xprv)))" defines crossover sample generation
  - [section 3.1, Eq. 5] "DS_Diff(Xprv, Xpub) > DS_Diff(Xprv, ˆXco)" validates transformation success
  - [corpus] "Cross-domain transformation for outlier detection on tabular datasets" [14, cited in paper] establishes prior art; corpus papers on domain adaptation provide weak indirect support.
- Break condition: If DS_Diff between private and crossover data exceeds private-public DS_Diff, transformation is rejected and the next-ranked public dataset is tried.

### Mechanism 3: Ensemble Soft Labeling with Pre-Optimized Models
- Claim: Fusing predictions from multiple pre-trained public models via majority voting produces more robust soft labels than single-model or default-parameter approaches.
- Mechanism: Top1-DS uses best m models from the most similar public dataset; TopN-DS uses best 1 model from each of top n similar datasets. Majority voting combines predictions into final binary labels.
- Core assumption: Public models retain discriminative power on transformed data, and ensemble averaging reduces transformation-induced noise.
- Evidence anchors:
  - [abstract] "generation of soft labels for unlabeled data, addressing the cold start problem"
  - [section 5.1, Table 1] Method1 achieves 0.639 balanced accuracy vs. 0.577 Default OD; Method2 achieves 0.625
  - [section 5.1] "Both of our methods outperform Avg OD and Default OD baseline models... in terms of Balanced Accuracy, F1-score, and PR-AUC"
  - [corpus] "Automated Machine Learning for Unsupervised Tabular Tasks" (LOTUS) addresses model selection for unsupervised tasks but uses optimal transport rather than transformation-based approaches.
- Break condition: ROC-AUC underperforms baselines (0.683-0.689 vs. 0.690-0.691), suggesting elevated false positive rates may occur on borderline cases.

## Foundational Learning

- Concept: **Principal Component Analysis (PCA) for Information Content**
  - Why needed here: The similarity measure relies on understanding how reconstruction error varies with component count.
  - Quick check question: If a dataset requires 50 PCs to achieve <10% reconstruction error, is it more or less complex than one requiring only 10 PCs?

- Concept: **Autoencoder Architecture with Shared Latent Space**
  - Why needed here: The transformation model uses paired autoencoders with a common bottleneck—understanding information flow is essential for debugging.
  - Quick check question: What happens to reconstruction quality if the shared layer dimension (32) is smaller than the information content required by either dataset?

- Concept: **Outlier Detection Evaluation Metrics for Imbalanced Data**
  - Why needed here: PR-AUC and balanced accuracy are emphasized over raw accuracy due to class imbalance.
  - Quick check question: Why is ROC-AUC potentially misleading when outliers represent <5% of samples?

## Architecture Onboarding

- Component map:
  - Similarity Module: Input: private + public datasets → Output: ranked public dataset list by SAD score
  - Transformation Module: Input: private samples + selected public dataset → Output: crossover samples in public format
  - Labeling Module: Input: crossover samples + pre-trained public models → Output: fused soft labels via majority voting

- Critical path:
  1. Compute PCA similarity between private and all candidate public datasets
  2. Train transformation autoencoder (1000 epochs, Adam optimizer) on private + top-ranked public data
  3. Verify DS_Diff criterion; if failed, retry with next-ranked public dataset
  4. Apply pre-optimized public models to crossover data
  5. Fuse predictions via majority voting

- Design tradeoffs:
  - Top1-DS vs. TopN-DS: Single transformation (9.94 min avg) vs. multiple transformations (27.28 min avg) with similar accuracy (0.639 vs. 0.625)
  - Shared layer size [64,32,64]: Smaller improves regularization but risks information loss; paper uses this fixed architecture
  - Number of models (m) and transformations (n): Paper shows m has minimal compute impact; n scales linearly with dataset count

- Failure signatures:
  - Transformation failure: DS_Diff(private, crossover) ≥ DS_Diff(private, public)—indicates latent space misalignment
  - Low similarity scores: All public datasets have SAD > 0.8 suggests no suitable transfer candidate exists
  - ROC-AUC degradation: Values below baseline indicate elevated false positive rates from transformation noise

- First 3 experiments:
  1. **Similarity validation**: On a held-out labeled dataset, compute SAD against all other datasets; verify that higher similarity correlates with higher soft label accuracy (replicate Figure 8 trend).
  2. **Transformation sanity check**: Train transformation model on two datasets with known overlap (e.g., train/test splits); verify crossover samples maintain DS_Diff criterion and outlier labels are preserved.
  3. **Ablation on shared layer dimension**: Test shared layer configurations [128,64,128], [64,32,64] (paper default), and [32,16,32]; measure impact on reconstruction loss and final balanced accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Method performance depends heavily on finding statistically similar public datasets; when similarity scores are uniformly high, the approach degrades to near-baseline performance
- Paper does not specify how input dimension mismatches are handled when applying fixed 128/64 architecture to datasets with different feature counts
- Hyperparameter settings for the "Best OD Algo" used in soft labeling are not provided, making exact reproduction difficult

## Confidence

**High Confidence:** The core mechanism of PCA-based dataset similarity (Mechanism 1) is well-supported by experimental results showing upward trends in balanced accuracy with increased similarity (Figure 8).

**Medium Confidence:** The shared autoencoder transformation (Mechanism 2) is theoretically sound and validated by DS_Diff criteria, but limited corpus validation exists for this specific architecture.

**Medium Confidence:** Ensemble soft labeling (Mechanism 3) shows strong empirical performance, though ROC-AUC degradation suggests elevated false positive rates on borderline cases.

## Next Checks
1. **Similarity validation:** On a held-out labeled dataset, compute SAD against all other datasets; verify that higher similarity correlates with higher soft label accuracy (replicate Figure 8 trend).
2. **Transformation sanity check:** Train transformation model on two datasets with known overlap (e.g., train/test splits); verify crossover samples maintain DS_Diff criterion and outlier labels are preserved.
3. **Ablation on shared layer dimension:** Test shared layer configurations [128,64,128], [64,32,64] (paper default), and [32,16,32]; measure impact on reconstruction loss and final balanced accuracy.