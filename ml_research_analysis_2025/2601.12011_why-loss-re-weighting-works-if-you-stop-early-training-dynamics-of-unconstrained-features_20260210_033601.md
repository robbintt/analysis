---
ver: rpa2
title: 'Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained
  Features'
arxiv_id: '2601.12011'
source_url: https://arxiv.org/abs/2601.12011
tags:
- learning
- loss
- training
- matrix
- reweighting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a small-scale model (SSM) to transparently
  analyze the early-training benefits of loss reweighting in imbalanced classification.
  The SSM abstracts the complexities of deep neural networks and high-dimensional
  data, using a bilinear model with squared loss to reveal how vanilla empirical risk
  minimization (ERM) preferentially learns majority class features over minority ones.
---

# Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features

## Quick Facts
- **arXiv ID**: 2601.12011
- **Source URL**: https://arxiv.org/abs/2601.12011
- **Reference count**: 40
- **Primary result**: Loss reweighting equalizes learning speeds of minority and majority features early in training, but benefits vanish at convergence.

## Executive Summary
This paper introduces a Small-Scale Model (SSM) to transparently analyze the early-training benefits of loss reweighting in imbalanced classification. The SSM abstracts the complexities of deep neural networks and high-dimensional data, using a bilinear model with squared loss to reveal how vanilla empirical risk minimization (ERM) preferentially learns majority class features over minority ones. The key finding is that reweighting equalizes the learning rates of all features by effectively flattening the spectrum of the label matrix, enabling simultaneous learning of majority and minority class distinctions. Theoretical analysis shows that reweighting compresses the effective learning window, independent of the imbalance ratio, contrasting with vanilla ERM where the window grows unboundedly with imbalance. This explains why reweighting benefits minority classes early in training, even though it has no effect in the terminal phase of overparameterized models.

## Method Summary
The authors analyze a Small-Scale Model (SSM) consisting of a bilinear unconstrained features model (UFM) with squared loss and simplex-encoded labels. The model uses gradient flow dynamics to track how singular values of the logit matrix evolve during training. They compare vanilla ERM with reweighted ERM using weights proportional to the inverse square root of class frequency (γ=0.5). The analysis employs spectral initialization and tracks the learning times of different feature directions, showing that reweighting equalizes these learning times regardless of the imbalance ratio. The theoretical framework is validated on both synthetic STEP-imbalanced data and real MNIST experiments.

## Key Results
- Vanilla ERM learns majority features significantly faster than minority features due to spectral properties of the label matrix
- Reweighting with γ=0.5 equalizes the effective learning rates of all feature directions by flattening the spectral spectrum
- The effective learning window under reweighting is bounded by √2-1, independent of imbalance ratio R
- Benefits of reweighting vanish at convergence in overparameterized regimes when models achieve zero training loss

## Why This Works (Mechanism)

### Mechanism 1: Sequential Spectral Learning in Vanilla ERM
In overparameterized models, vanilla empirical risk minimization (ERM) learns semantic features in a strict sequence dictated by class frequency. The gradient flow dynamics on the unconstrained features model (UFM) decouple along the singular vectors of the label matrix. The time T_i required to learn a feature i is inversely proportional to its singular value σ_i. Since majority class features correspond to larger singular values (σ_maj ∝ √R), they are learned significantly earlier than minority features (σ_min ≈ 1).

### Mechanism 2: Reweighting Flattens the Effective Spectrum
Loss reweighting with weights ω ∝ 1/√frequency restores balanced learning by equalizing the learning speeds of all feature directions. Reweighting modifies the gradient flow updates by introducing an effective weight matrix Λ. Theoretical analysis proves that with specific weights (γ=0.5), the effective singular values σ_i λ_i become uniform across majority and minority features. This compresses the "effective learning window" ΔT so it is bounded by √2-1 regardless of the imbalance ratio R.

### Mechanism 3: Terminal Phase Equivalence
The benefits of reweighting vanish if training proceeds to convergence in overparameterized regimes. In the terminal phase (t → ∞), the logits of the overparameterized model perfectly interpolate the simplex-encoded labels (L → Z). Both vanilla and reweighted ERM converge to identical global minimizers that memorize the training data, neutralizing the early-training dynamic advantages of reweighting.

## Foundational Learning
- **Concept: Unconstrained Features Model (UFM)**
  - Why needed here: This is the core abstraction used to make the dynamics mathematically tractable, replacing the complex DNN with a bilinear model.
  - Quick check question: Do you understand why treating embeddings H as free parameters is a valid proxy for an overparameterized DNN?

- **Concept: Spectral Dynamics of Gradient Descent**
  - Why needed here: The explanation relies on analyzing how Singular Value Decomposition (SVD) components of the logit matrix evolve over time.
  - Quick check question: Can you explain why a larger singular value in the label matrix leads to faster learning of the associated feature?

- **Concept: Early Stopping as Regularization**
  - Why needed here: The paper's utility depends entirely on stopping training before the "terminal phase" erases the benefits of reweighting.
  - Quick check question: How does the "Effective Learning Window" (ΔT) dictate the optimal stopping point?

## Architecture Onboarding
- **Component map**: Data (STEP-imbalanced) -> Model (Bilinear UFM) -> Loss (Squared with SEL) -> Intervention (Reweighting) -> Early Stopping
- **Critical path**: Initialize small → Forward pass (WH) → Compute weighted squared loss → Gradient Flow → Stop Early
- **Design tradeoffs**: The paper uses Squared Loss for theoretical clean dynamics; real DNNs use Cross-Entropy. The authors argue squared loss is a valid first-order approximation, but exact numerical dynamics may differ in practice.
- **Failure signatures**: If you train until the loss is exactly zero (convergence), the reweighting effect will disappear and performance will match vanilla ERM.
- **First 3 experiments**:
  1. Replicate Spectral Dynamics: Train the SSM on synthetic STEP-imbalanced data. Plot the singular values of the logits L(t) over time for both Vanilla and Reweighted ERM to verify sequential vs. simultaneous learning.
  2. Ablate on Imbalance Ratio: Run the SSM with increasing imbalance R ∈ {10, 100, 1000}. Measure the "Effective Learning Window" ΔT for Vanilla (should grow) vs. Reweighted (should stay bounded).
  3. Early Stopping Validation: Train a standard CNN on imbalanced MNIST. Compare test accuracy for minorities at early epochs vs. final convergence to empirically confirm the "Early Stopping" requirement.

## Open Questions the Paper Calls Out
- How do the training dynamics of loss reweighting change when using the cross-entropy loss function instead of the squared loss approximation? The theoretical results rely on the squared loss to decouple the gradient flow into independent scalar equations via the SVD of the label matrix; cross-entropy introduces non-linear interactions that make the dynamics analytically intractable.
- How does loss reweighting theoretically impact test-time generalization dynamics as opposed to training dynamics? The paper successfully models training dynamics but does not theoretically characterize the evolution of the generalization gap, relying instead on empirical test metrics.
- Do the "spectral flattening" dynamics persist when feature embeddings are constrained by fixed data and non-linear network architecture rather than being unconstrained? The SSM assumes embeddings are free variables optimized solely to minimize loss, whereas real DNNs have constrained geometry.

## Limitations
- The theoretical analysis relies on idealized assumptions (squared loss, spectral initialization) that may not fully transfer to practical deep learning scenarios.
- The precise weight setting (γ=0.5) for optimal spectral flattening may be sensitive to implementation details and data characteristics.
- The critical assumption that overparameterized models achieve perfect interpolation at convergence needs empirical validation across different model families.

## Confidence
- **High Confidence**: Sequential spectral learning mechanism in vanilla ERM and terminal phase equivalence claim
- **Medium Confidence**: Reweighting flattening mechanism (relies on precise weight calibration)
- **Low Confidence**: Direct applicability of SSM insights to large-scale deep learning systems

## Next Checks
1. **Cross-Architecture Validation**: Replicate the SSM findings using a standard CNN and transformer architecture on CIFAR-10-LT and ImageNet-LT to test if the spectral flattening mechanism holds beyond the bilinear model.
2. **Weight Sensitivity Analysis**: Systematically vary the reweighting parameter γ around 0.5 to identify the robustness of the spectral flattening effect and determine if near-optimal performance can be achieved with simpler heuristics.
3. **Beyond Interpolation**: Test whether the early-training benefits persist when models are stopped before reaching zero training loss (underfitting regime) versus the overparameterized interpolation regime, to better understand the practical boundaries of the theoretical findings.