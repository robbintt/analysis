---
ver: rpa2
title: 'GARDO: Reinforcing Diffusion Models without Reward Hacking'
arxiv_id: '2512.24138'
source_url: https://arxiv.org/abs/2512.24138
tags:
- reward
- gardo
- arxiv
- proxy
- hacking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles reward hacking in RL-based fine-tuning of diffusion
  models, where models exploit proxy rewards at the cost of real image quality and
  diversity. The authors propose GARDO, a framework that selectively applies KL regularization
  only to high-uncertainty samples (using ensemble reward disagreement), periodically
  updates the reference model for adaptive regularization, and reshapes advantages
  to encourage diversity.
---

# GARDO: Reinforcing Diffusion Models without Reward Hacking

## Quick Facts
- arXiv ID: 2512.24138
- Source URL: https://arxiv.org/abs/2512.24138
- Reference count: 40
- Key outcome: GARDO prevents reward hacking in diffusion model RL fine-tuning by selectively applying KL regularization to high-uncertainty samples, achieving higher proxy rewards and better generalization on unseen tasks without sacrificing sample efficiency

## Executive Summary
This paper addresses reward hacking in RL-based fine-tuning of diffusion models, where models exploit proxy rewards at the expense of real image quality and diversity. The authors propose GARDO, a framework that selectively applies KL regularization only to high-uncertainty samples (using ensemble reward disagreement), periodically updates the reference model for adaptive regularization, and reshapes advantages to encourage diversity. GARDO successfully balances sample efficiency, exploration, and diversity while preventing reward hacking. Across multiple tasks and metrics, it outperforms baselines like Flow-GRPO, achieving higher proxy rewards and better generalization on unseen tasks without sacrificing sample efficiency.

## Method Summary
GARDO modifies GRPO for diffusion model fine-tuning with three key innovations: (1) Gated KL regularization that applies KL penalties only to ~10% of samples with highest uncertainty (measured by ensemble disagreement between proxy and auxiliary rewards), (2) Adaptive reference model updates that periodically reset the KL anchor to the current policy when divergence exceeds threshold or after fixed steps, and (3) Diversity-aware advantage shaping that multiplies positive advantages by semantic diversity scores (DINOv3 feature cosine distances). The method uses LoRA fine-tuning with specific hyperparameters (α=32, r=64, batch size 6, group size 24, lr=3e-4, β=0.04, T=10 sampling steps).

## Key Results
- GARDO outperforms Flow-GRPO on GenEval and OCR tasks, achieving higher proxy rewards while maintaining better unseen metrics (Aesthetic, PickScore, ImageReward, ClipScore, HPSv3)
- On OCR tasks, GARDO reaches 0.96 accuracy in 600 steps while maintaining high diversity and unseen reward scores
- Diversity scores improve from 19.98 to 24.95 on GenEval task with diversity shaping enabled
- GARDO successfully prevents reward hacking, as evidenced by proxy reward increases accompanied by stable or improving unseen quality metrics

## Why This Works (Mechanism)

### Mechanism 1: Gated KL Regularization
Applying KL regularization selectively to high-uncertainty samples (~10% of each batch) prevents reward hacking while preserving sample efficiency. The framework quantifies uncertainty using ensemble disagreement between the proxy reward and auxiliary reward models (Aesthetic, ImageReward). When a sample's proxy win rate substantially exceeds its ensemble-averaged win rate, it signals anomalous proxy scores likely from out-of-distribution extrapolation. Only these flagged samples receive KL penalty; the remaining ~90% optimize freely toward high-reward regions.

### Mechanism 2: Adaptive Reference Model Updates
Periodically resetting the reference model to match the current policy prevents KL regularization from dominating training and enables sustained exploration beyond the initial policy's capabilities. The reference model is hard-reset to the current policy when KL divergence exceeds threshold or after m gradient steps. This prevents the static reference from becoming a suboptimal anchor that overly constrains the policy to "safe" regions near the pre-trained distribution.

### Mechanism 3: Diversity-Aware Advantage Shaping
Multiplicative reweighting of advantage terms based on feature-space diversity promotes mode coverage without destabilizing optimization. For samples with positive advantage, the advantage is reshaped by multiplying with cosine distance to nearest neighbor in DINOv3 feature embeddings. Only positive-advantage samples are reshaped—this prevents rewarding low-quality but diverse outputs. The multiplicative design avoids hyperparameter balancing between reward and diversity scales.

## Foundational Learning

- **Concept: Reward Hacking (Reward Gaming)**
  - Why needed: The paper's central problem is that proxy reward optimization can diverge from true quality. Understanding this mismatch is prerequisite to appreciating why selective regularization matters.
  - Quick check question: Why might a policy maximizing OCR accuracy generate images with perfect text but degraded backgrounds and visual artifacts?

- **Concept: KL-Regularized Policy Optimization**
  - Why needed: GARDO modifies the standard KL-regularized RL objective. Understanding the baseline objective J_β(π) = E_π[R̃] - β·D_KL(π|π_ref) is essential.
  - Quick check question: What tradeoff does the coefficient β control, and why might a fixed β create problems as training progresses?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed: GARDO is implemented on top of GRPO. Understanding how advantages are computed within groups of samples from the same prompt is necessary.
  - Quick check question: In GRPO, how is advantage A_i computed from rewards {R(x_i)} within a group, and what role does standard deviation normalization play?

## Architecture Onboarding

- **Component map:**
Prompt c → Current Policy π_θ → Generate G samples {x⁰_i} → Reward Branches: Proxy R̃, Auxiliaries {R̂₁, R̂₂} → Uncertainty U(x_i) = w(R̃) - mean({w(R̂ₙ)}) → Gated KL Selection: Top k% highest-U samples → KL penalty applied → Diversity Computation: DINOv3 features e_i → d_i = min(cosine distance to neighbors) → Advantage Shaping: A_shaped = A · d if A > 0, else A → Adaptive Reference Reset: if KL > ε_KL or steps > m → π_ref ← π_θ → Policy Update via GRPO objective

- **Critical path:** Uncertainty estimation → gate selection → selective KL penalty. If uncertainty proxy fails to identify spurious rewards, or if gate threshold k is misconfigured, the core balance between efficiency and hacking prevention collapses.

- **Design tradeoffs:**
  - Gate percentage k: Higher k = more safety but slower convergence (paper uses ~10%)
  - Reference update frequency (ε_KL, m): More frequent = better exploration but weaker regularization anchor
  - Auxiliary reward selection: More diverse ensembles improve uncertainty estimation but add compute overhead

- **Failure signatures:**
  - Proxy reward rises while unseen metrics (Aesthetic, HPSv3) fall → reward hacking not prevented (gate too permissive or uncertainty proxy failing)
  - Training plateaus with low proxy reward → KL penalty dominating (reference too constraining or updates too infrequent)
  - Diversity scores unchanged → feature space not capturing semantic variation or positive-advantage constraint too restrictive

- **First 3 experiments:**
  1. **Gate percentage ablation:** Train on OCR task with k ∈ {5%, 10%, 20%, 50%}. Plot proxy reward vs. unseen metrics (Aesthetic, ImageReward) to identify optimal tradeoff point.
  2. **Reference update timing analysis:** Log when reference resets occur (KL-triggered vs. step-triggered) and correlate with learning curve inflection points. Test aggressive (ε_KL large) vs. conservative settings.
  3. **Diversity shaping component validation:** Run three conditions—(a) full diversity shaping, (b) shaping on all samples (not just positive-advantage), (c) additive bonus instead of multiplicative. Compare stability and final diversity scores.

## Open Questions the Paper Calls Out

- **Question:** Can GARDO be effectively scaled to resource-intensive video generative models despite its dependency on auxiliary reward models for uncertainty estimation?
  - Basis in paper: The conclusion states, "the scalability of our approach to resource-intensive video generative models remains an open question for future investigation."
  - Why unresolved: Video generation requires significantly higher computational resources; running multiple auxiliary reward models to calculate uncertainty for video frames may introduce prohibitive latency or memory costs.
  - What evidence would resolve it: A demonstration of GARDO applied to a video diffusion baseline (e.g., Stable Video Diffusion) showing that the computational overhead of uncertainty estimation remains manageable while preventing reward hacking.

- **Question:** How sensitive is the uncertainty estimation mechanism to the choice of auxiliary reward models if they share similar biases or failure modes with the proxy reward?
  - Basis in paper: The method relies on ensemble disagreement using specific off-the-shelf models (Aesthetic, ImageReward) to identify high-uncertainty samples.
  - Why unresolved: The paper does not analyze cases where the auxiliary models might be highly correlated with the proxy reward, potentially failing to flag "spurious" proxy rewards as uncertain.
  - What evidence would resolve it: Ablation studies varying the auxiliary models or artificially correlating them with the proxy to observe if the gated KL mechanism loses its ability to prevent reward hacking.

- **Question:** Is there a theoretical justification for the optimal percentage of samples to gate with the KL penalty, or is the ~10% threshold purely empirical?
  - Basis in paper: The paper states, "Surprisingly... applying the KL penalty to only a small subset of samples (e.g., approximately 10%)... is sufficient," relying on a dynamic window heuristic.
  - Why unresolved: The authors provide an empirical finding but do not derive a theoretical bound or adaptive rule that guarantees this percentage is sufficient across different tasks or model scales.
  - What evidence would resolve it: A theoretical analysis linking the uncertainty distribution of the reward landscape to the necessary coverage of the KL penalty, or comprehensive sweeps across different gating percentages on diverse tasks.

## Limitations
- The gating mechanism relies on the assumption that ensemble disagreement reliably identifies out-of-distribution reward extrapolation, but this correlation is not directly validated
- The diversity computation depends on DINOv3 features capturing meaningful semantic variation without explicit verification
- The adaptive reference update frequency (ε_KL=1e-4, m=100) appears effective but may be sensitive to task/domain shifts

## Confidence
- High: Core problem framing (reward hacking in diffusion model RL fine-tuning) and GARDO's three-component structure
- Medium: Effectiveness of gated KL on preventing reward hacking (supported by experiments but mechanism not exhaustively validated)
- Medium: Diversity shaping improves semantic coverage (empirical results shown but feature space quality unverified)
- Low: Specific hyperparameter choices (gate percentage, reference update thresholds) generalize beyond tested domains

## Next Checks
1. Conduct ablation studies varying gate percentage k across multiple orders of magnitude to map the full tradeoff frontier between sample efficiency and reward hacking prevention
2. Test whether ensemble disagreement remains a reliable uncertainty proxy when auxiliary rewards become correlated with the proxy reward through training
3. Validate that DINOv3 feature space diversity correlates with human perceptual diversity judgments on generated samples