---
ver: rpa2
title: 'ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields
  with Survey-Mined Questions and Rubrics'
arxiv_id: '2509.00496'
source_url: https://arxiv.org/abs/2509.00496
tags:
- response
- rubric
- query
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RESEARCHQA leverages survey articles to create a large-scale scholarly
  question-answering benchmark, addressing the challenge of evaluating research synthesis
  systems across diverse fields. By automatically extracting queries and rubrics from
  over 54,000 survey articles, it generates 21,400 queries with 160,000 rubric items
  across 75 research fields.
---

# ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics

## Quick Facts
- arXiv ID: 2509.00496
- Source URL: https://arxiv.org/abs/2509.00496
- Reference count: 40
- Primary result: 21,400 scholarly queries with 160,000 rubric items across 75 fields, with expert validation showing 90% reflect PhD information needs

## Executive Summary
ResearchQA addresses the challenge of evaluating scholarly question-answering systems by creating a large-scale benchmark mined from academic survey articles. The methodology extracts 21,400 queries and corresponding rubrics from over 54,000 survey articles across 75 research fields, validated by PhD annotators. Evaluation of 18 systems reveals that no parametric or retrieval-augmented system exceeds 70% rubric coverage, with the top deep research system achieving only 75.3%, highlighting significant room for improvement in scholarly synthesis capabilities.

## Method Summary
The methodology involves mining scholarly queries and rubrics from academic survey articles using a multi-stage pipeline. First, top venues per field are identified from Google Scholar Metrics, then survey articles are retrieved and classified using gpt-4.1-mini. Hierarchical summaries are generated to extract standalone queries with low answer variability, paired with reference answers. Rubrics are generated using three prompt types (information, depth, citation) for both survey and parametric conditions, then merged via deduplication and reranking to top-8 items. The benchmark is evaluated using an ensemble LLM judge that combines direct preference prediction with rubric coverage scoring.

## Key Results
- 21,400 queries with 160,000 rubric items across 75 research fields
- 90% of mined queries validated as reflecting PhD information needs
- 87% of rubric items require meaningful coverage beyond trivial facts
- No parametric or RAG system exceeds 70% rubric coverage
- Top deep research system achieves 75.3% coverage
- Citation rubric items show 89% failure rate across all systems

## Why This Works (Mechanism)

### Mechanism 1: Grounding Evaluation in Derived Rubrics
The paper provides LLM evaluators with query-specific rubrics rather than asking them to predict preferences directly. By decomposing evaluation into granular checklist items (e.g., "Does the response cite X?"), the task shifts from open-ended assessment to binary verification, reducing cognitive load and hallucination in the judge. Evidence shows the ensemble judge reduces LLM-human disagreement from 12.7% to 9.6%.

### Mechanism 2: Survey Distillation for Expert Simulation
Mining queries from academic surveys creates a scalable proxy for expert-level information needs. Surveys naturally aggregate domain consensus, citations, and comparative analysis. The pipeline extracts hierarchical summaries and supporting sentences, forcing generated queries to be grounded in real literature structures rather than synthetic generation. Validation shows 90% of mined queries reflect PhD information needs.

### Mechanism 3: Hybrid Retrieval for Citation Coverage
"Deep Research" systems outperform standard RAG because they specifically address the "Citation" rubric items which standard retrieval frequently misses or hallucinates. These systems employ multi-step reasoning to identify, verify, and format citations explicitly. However, even the best system fails to address 89% of citation items fully, indicating this remains the hardest criterion.

## Foundational Learning

- **Concept: Rubric-Based Evaluation vs. Holistic Scoring**
  - Why needed here: The paper shifts from asking "Which response is better?" to "Does the response contain X, Y, and Z?"
  - Quick check question: Can you distinguish between a response that is "fluent but factually empty" versus one that is "clunky but comprehensive" using only a checklist?

- **Concept: Knowledge Leakage in Benchmarking**
  - Why needed here: The paper discusses how systems might retrieve the exact survey the query was mined from, artificially inflating scores.
  - Quick check question: If a model was trained on ArXiv papers, and the benchmark queries are mined from ArXiv papers, how do you distinguish between "reasoning" and "memorization"?

- **Concept: Parametric vs. Contextual Knowledge**
  - Why needed here: The paper evaluates systems across four tiers (Parametric, Naive RAG, Production RAG, Deep Research).
  - Quick check question: Why might a parametric model (no search) outperform a naive RAG model if the retrieval step retrieves irrelevant or contradictory documents?

## Architecture Onboarding

- **Component map:** Source Corpus (54K Academic Surveys) → Mining Pipeline (gpt-4.1-mini: Classifier, Query Generator, Rubric Generator) → Evaluation Target (18 Systems) → Ensemble Judge (gpt-4.1-mini: Rubric Scorer + Direct Preference Logic)

- **Critical path:** The Query & Rubric Generation step. If rubric items do not align with the query or are hallucinated, the entire evaluation metric is invalid. The "Hybrid Rubric" construction (deduping, reranking) is the central stabilization step.

- **Design tradeoffs:**
  - Cost vs. Quality: Chose gpt-4.1-mini for generation ($483) over gpt-4.1 ($2.4k), noting slight quality drops
  - Recall vs. Precision: Hybrid Rubric mixes survey-based items (high recall) and parametric items (general knowledge), trading survey-specific precision for broader coverage

- **Failure signatures:**
  - Citation Hallucination: Systems citing non-existent papers or misattributing findings (89% error rate)
  - Retrieval Misalignment: Naive RAG retrieves orthogonally related but non-answering documents
  - Length Bias: Deep research systems generate ~1.5k words, potentially "brute-forcing" coverage

- **First 3 experiments:**
  1. **Leakage Check:** Run mining pipeline on held-out surveys; measure score inflation when RAG has vs. lacks access
  2. **Rubric Ablation:** Evaluate responses using "Generic Rubrics" vs. "Hybrid Rubrics" to validate specific checklists improve correlation
  3. **Error Analysis:** Inspect top system's "Citation" failures: right paper retrieved but uncited, or retrieval failure entirely

## Open Questions the Paper Calls Out

### Open Question 1
Does the fixed upper limit of 8 rubric items result in a loss of evaluation validity for complex research queries that require more granular criteria? The current pipeline heuristically selects top 8 items to balance cost and performance, but there is no empirical validation that 8 items are sufficient to capture the nuance of all 21.4K queries across diverse fields. A study comparing correlation with human judgment for variable-length rubrics versus the fixed 8-item cap would resolve this.

### Open Question 2
Will retrieval of the source survey article create significant evaluation bias as commercial deep-research systems improve their search precision? Current systems might retrieve the survey but fail to utilize it effectively. As models improve, they may map survey text directly to the query, artificially inflating "Coverage %" without performing synthesis. Longitudinal analysis comparing "leaked" vs. "held-out" survey topics would measure this.

### Open Question 3
Can the agreement gap between ensemble LLM judges and human experts be closed by optimizing rubrics for high-error categories like citations and comparisons? The paper establishes hybrid rubrics help, but does not investigate if remaining disagreement stems from judge's inability to verify specific, high-difficulty criteria. An ablation study measuring human-LLM agreement specifically on "Citation" and "Comparison" items versus "Information" items would help.

## Limitations
- Reliance on LLM-generated rubrics and validation creates potential self-reinforcing errors
- Corpus contains only 54K surveys out of 8.7M publications, raising coverage bias concerns
- "Deep research" systems tested may have proprietary advantages not available to other systems

## Confidence
- **High Confidence:** Survey mining methodology and validation results (90% of queries reflect PhD needs, 87% of rubric items require meaningful coverage)
- **Medium Confidence:** Claim that no system exceeds 70% rubric coverage is robust, but relative performance differences should be interpreted cautiously
- **Low Confidence:** Absolute rubric coverage scores are meaningful only if LLM evaluation is perfectly calibrated, which the paper demonstrates is not the case

## Next Checks
1. **Leakage Quantification:** Run full evaluation pipeline with and without access to original survey articles for held-out test set to measure artificial score inflation
2. **Human-in-the-Loop Verification:** Have human experts manually evaluate random sample of model responses across all rubric categories to establish ground truth correlation
3. **Domain Generalization Test:** Evaluate systems on queries from fields not represented in training corpus to assess performance outside 75 benchmark domains