---
ver: rpa2
title: Explainable Counterfactual Reasoning in Depression Medication Selection at
  Multi-Levels (Personalized and Population)
arxiv_id: '2508.17207'
source_url: https://arxiv.org/abs/2508.17207
tags:
- counterfactual
- feature
- importance
- medication
- symptoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses counterfactual reasoning to explain AI-based depression
  medication selection between SSRIs and SNRIs based on HAM-D symptom scores. A diverse
  counterfactual explanations (DICE) method generates multiple alternative scenarios
  showing how symptom changes affect medication recommendations.
---

# Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)

## Quick Facts
- arXiv ID: 2508.17207
- Source URL: https://arxiv.org/abs/2508.17207
- Reference count: 31
- Primary result: AI-based depression medication selection between SSRIs and SNRIs explained through counterfactual reasoning, with Random Forest achieving ~0.85 performance metrics

## Executive Summary
This study introduces a counterfactual explanation method (DICE) to interpret AI-based depression medication selection between SSRIs and SNRIs using HAM-D symptom scores. The approach generates multiple alternative scenarios showing how symptom changes affect medication recommendations, providing interpretable AI CDSS by quantifying causal relationships between symptoms and medication selection. The method enables personalized treatment recommendations while accounting for real-world constraints, with depressed mood and loss of sexual interest identified as most influential features.

## Method Summary
The study uses a diverse counterfactual explanations (DICE) method to generate multiple alternative scenarios by perturbing HAM-D symptom scores. A Random Forest classifier was trained on clinical trial data (n=1468) to predict SSRI vs SNRI selection, achieving ~0.85 performance across accuracy, F1, precision, recall, and ROC-AUC. The DICE method generates counterfactuals by minimizing prediction loss plus distance penalty while maximizing diversity via determinantal point processes. Feature importance is derived from counterfactual perturbation frequency, with local importance calculated per patient and global importance aggregated across the population.

## Key Results
- Random Forest achieved highest performance among 17 classifiers with accuracy, F1, precision, recall, and ROC-AUC near 0.85
- Sample-based counterfactuals revealed depressed mood (HAM-D01) and loss of sexual interest (HAM-D14) as most influential features, while weight loss (HAM-D16) had minimal impact
- The approach provides interpretable AI CDSS by quantifying causal relationships between symptoms and medication selection at both personalized and population levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse counterfactual explanations reveal causal symptom-medication relationships by perturbing HAM-D scores.
- Mechanism: The DICE method generates k counterfactuals by minimizing prediction loss plus distance penalty, while maximizing diversity via determinantal point processes (Eq. 4-6). Each counterfactual represents minimal symptom score changes that would flip the SSRI/SNRI recommendation, isolating which symptoms causally drive the decision boundary.
- Core assumption: The trained classifier's decision boundary approximates real clinical prescribing logic, and perturbations on that boundary reveal meaningful causal structure.
- Evidence anchors: [abstract] states DICE generates multiple alternative scenarios showing how symptom changes affect medication recommendations; [section] defines optimization objective combining prediction loss, proximity, and DPP-based diversity.

### Mechanism 2
- Claim: Feature importance derives from counterfactual perturbation frequency, not model coefficients.
- Mechanism: Local importance = frequency a feature changes across k counterfactuals for one patient. Global importance = aggregated local importance across all instances. Features changed more often are more causally influential for the prediction.
- Core assumption: Frequent perturbation in counterfactual generation indicates genuine causal influence, not just model sensitivity to noise.
- Evidence anchors: [abstract] reports sample-based CFs revealed both local and global feature importance; [section] states local feature importance is determined by frequency of feature changes when generating multiple CFs.

### Mechanism 3
- Claim: Random Forest provides a sufficiently stable decision boundary for counterfactual exploration.
- Mechanism: Among 17 classifiers tested, Random Forest achieved ~0.85 across accuracy, F1, precision, recall, and ROC-AUC. Its ensemble structure handles mixed continuous/categorical HAM-D scores and resists overfitting on the n=1468 clinical trial dataset.
- Core assumption: Classification performance ~0.85 is adequate for meaningful counterfactual generation.
- Evidence anchors: [abstract] states Random Forest achieved highest performance; [section] shows Table 3 with Random Forest test ROC-AUC = 0.9253 and accuracy = 0.8497.

## Foundational Learning

- Concept: **Counterfactual Explanations in XAI**
  - Why needed here: Core method; without understanding what CFs are (hypothetical input perturbations that change predictions), the entire mechanism is opaque.
  - Quick check question: Given a model predicting "SSRI" for a patient, can you explain what a counterfactual would represent?

- Concept: **Hamilton Rating Scale for Depression (HAM-D)**
  - Why needed here: Input features are 17 HAM-D symptom scores; interpreting counterfactuals requires knowing what HAM-D01 (depressed mood) vs. HAM-D16 (weight loss) clinically mean.
  - Quick check question: What does a +2 change in HAM-D10 (psychic anxiety) indicate symptom-wise?

- Concept: **Determinantal Point Processes (DPP) for Diversity**
  - Why needed here: DICE uses DPP (Eq. 6) to enforce diversity among generated counterfactuals; without this concept, the diversity term is uninterpretable.
  - Quick check question: Why would maximizing det(K) produce more diverse counterfactuals?

## Architecture Onboarding

- Component map: Clinical trial HAM-D scores + medication labels (SSRI/SNRI) -> SMOTE + one-hot encoding -> Random Forest classifier -> DICE counterfactual generator with constraints -> Local feature importance (per-patient) and global feature importance (population)
- Critical path: 1) Load and categorize medications by dosing version, 2) Balance classes via SMOTE, encode HAM-D scores, 3) Train Random Forest with 5-fold CV, 4) Generate k=10 counterfactuals via DICE with constraints, 5) Compute local importance and aggregate to global importance
- Design tradeoffs: Dosing version selection changes class balance affecting model bias; number of counterfactuals k affects importance estimate quality vs compute cost; constraint specification improves realism but may reduce feasible counterfactuals
- Failure signatures: All counterfactuals require unrealistic symptom jumps (>3 on 0-4 scale) indicating model boundary too sharp or data sparse; global importance shows flat distribution suggesting classifier may not have learned meaningful patterns; counterfactuals contradict clinician intuition indicating model encodes spurious correlations
- First 3 experiments: 1) Replicate Random Forest training on V2 dosing, confirm test ROC-AUC ≈ 0.925, 2) Generate 10 counterfactuals for held-out patient, verify top local importance features match clinical expectations, 3) Run ablation without diversity term (λ₂=0), compare feature importance stability across runs

## Open Questions the Paper Calls Out

- Will the predictive performance and identified key symptoms (e.g., depressed mood, sexual interest) remain consistent when applied to more diverse patient demographics and real-world clinical settings? The study relied on a specific retrospective clinical trial dataset which may lack diversity found in broader clinical practice.
- Can the computational complexity of the DICE-based counterfactual algorithm be optimized to support real-time clinical deployment? Generating diverse counterfactuals is resource-intensive, potentially causing latency that hinders use during live patient consultations.
- How does the inclusion of novel pharmacotherapeutics and other antidepressant classes alter the model's feature importance and decision boundaries? The current study is limited to binary classification between SSRIs and SNRIs, limiting applicability for patients requiring other treatments.

## Limitations
- Clinical generalizability limited by single pharmaceutical sponsor's trial data, potentially introducing treatment selection bias with no comparator non-SSRI/SNRI medications
- DICE counterfactual diversity relies on DPP mathematical properties that may not align with clinical plausibility, generating technically diverse but clinically implausible scenarios
- Random Forest's feature importance through counterfactual frequency assumes local perturbations capture causal structure but remains unverified against ground-truth clinical decision processes

## Confidence
- High confidence in Random Forest performance metrics (~0.85 accuracy/ROC-AUC) due to 5-fold cross-validation and clear reporting
- Medium confidence in counterfactual explanation validity—methodologically sound but lacks direct clinical expert validation
- Low confidence in population-level feature importance rankings without external validation on independent clinical datasets

## Next Checks
1. Expert clinician review: Present top 10 counterfactual explanations for diverse patient profiles to 3 psychiatrists and assess clinical plausibility against their prescribing patterns
2. External dataset validation: Apply the trained Random Forest + DICE pipeline to an independent depression treatment dataset (if available) and verify feature importance rankings persist
3. Ablation study on diversity term: Generate counterfactuals with λ₂=0 (no DPP diversity) and compare feature importance stability and computational efficiency to assess whether diversity penalty meaningfully improves explanations