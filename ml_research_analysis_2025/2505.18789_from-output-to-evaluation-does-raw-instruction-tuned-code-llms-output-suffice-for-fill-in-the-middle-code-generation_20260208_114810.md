---
ver: rpa2
title: 'From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice
  for Fill-in-the-Middle Code Generation?'
arxiv_id: '2505.18789'
source_url: https://arxiv.org/abs/2505.18789
tags:
- code
- middle
- generation
- llms
- post-processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses whether post-processing is necessary for evaluating
  instruction-tuned LLM outputs in fill-in-the-middle (FIM) code generation. It finds
  that off-the-shelf instruction-tuned models perform poorly without fine-tuning,
  but lightweight supervised fine-tuning significantly improves FIM performance.
---

# From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?

## Quick Facts
- arXiv ID: 2505.18789
- Source URL: https://arxiv.org/abs/2505.18789
- Reference count: 40
- Primary result: Lightweight supervised fine-tuning significantly improves FIM performance, making post-processing unnecessary for complete-line middle segments.

## Executive Summary
This paper investigates whether post-processing is necessary for evaluating instruction-tuned LLM outputs in fill-in-the-middle (FIM) code generation tasks. The authors find that off-the-shelf instruction-tuned models perform poorly without fine-tuning, but lightweight supervised fine-tuning significantly improves FIM performance. Fine-tuned models produce outputs that don't require post-processing for complete-line middle segments, achieving higher accuracy than post-processed outputs in such cases. However, post-processing remains necessary when the middle is a random span of code to remove overlapping content.

## Method Summary
The authors collect Python functions from GitHub, filter them using Pyright type checking, and generate synthetic training data by splitting functions into prefix/middle/suffix triples using Mixtral-8x22B. They fine-tune Qwen2.5-Coder models (7B, 14B, 32B; base and instruct) for 5000 steps on H100-80GB GPUs with AdamW optimizer, batch size 256, max seq length 4096, learning rate 5e-6, and CosineAnnealing scheduler. The fine-tuned models are evaluated on HumanEval Infilling and SAFIM benchmarks using greedy decoding, comparing raw outputs versus post-processed outputs.

## Key Results
- Lightweight supervised fine-tuning significantly enhances FIM code generation performance
- Fine-tuned models produce outputs that don't require post-processing for complete-line middle segments
- Post-processing remains necessary for random-span infilling to remove overlapping content

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Boundary Awareness via SFT
The paper suggests that lightweight supervised fine-tuning aligns instruction-tuned models to the specific "stop" conditions required for FIM tasks, reducing the generation of extraneous code. By training on data where the middle segment strictly reconstructs the original function when concatenated with prefix and suffix, the model learns to predict the exact endpoint of generation.

### Mechanism 2: Semantic Validity over Heuristic Truncation
For complete-line infilling, raw model outputs achieve higher accuracy than post-processed outputs because they can generate semantically valid, multi-line alternatives that rigid truncation rules would destroy. The fine-tuned model generates solutions that are syntactically compatible with context, and pass@1 rewards functional correctness over exact structural matching.

### Mechanism 3: Random-Span Overlap Retention
Fine-tuning helps but is insufficient for random-span infilling (partial lines), necessitating post-processing to remove overlapping content. Random spans lack natural syntactic boundaries, and the model conditioned on the suffix often begins generation by repeating the suffix or prefix as it attempts to smooth the transition.

## Foundational Learning

- **Concept: Fill-in-the-Middle (FIM) Context Conditioning**
  - Why needed here: Unlike standard code completion, FIM requires processing suffix (future code) as conditioning input to generate middle. Understanding this bidirectional context is crucial for diagnosing why models generate overlapping content.
  - Quick check question: If a model generates correct code but appends first 5 tokens of provided suffix, has it solved FIM task correctly?

- **Concept: Synthetic Data Scaffolding**
  - Why needed here: The mechanism relies on a specific data generation pipeline using teacher LLM to create prefix/middle/suffix splits. Understanding this is crucial since the student model learns from synthetic approximations, not ground-truth human edits.
  - Quick check question: Why is verification (prefix + middle + suffix == original) a critical step in synthetic data pipeline?

- **Concept: Exact Match vs. Functional Correctness**
  - Why needed here: The paper argues against rigid truncation. To understand why raw outputs are "better," one must distinguish between matching reference solution exactly (textually) and passing unit tests (functionally).
  - Quick check question: Does higher pass@1 score always imply generated code looks identical to reference solution?

## Architecture Onboarding

- **Component map:** Data Engine (Mixtral-8x22B) -> Trainer (Qwen2.5-Coder with AdamW) -> Evaluator (HumanEval Infilling & SAFIM)
- **Critical path:** Data Curation (filter GitHub functions) -> Synthesis (generate splits with Figure 2 prompt; filter failed reconstructions) -> Fine-Tuning (single epoch over ~1M samples) -> Inference (greedy decoding with Figure 3 prompt)
- **Design tradeoffs:** Base vs. Instruct models yield slightly different sample efficiency; raw outputs save post-processing complexity but only reliable for complete-line scenarios
- **Failure signatures:** Catastrophic Repetition (infinite loops of suffix/prefix), Over-Generation (valid but verbose code), Degradation (performance drops if trained >1 epoch)
- **First 3 experiments:** 1) Baseline Verification (run off-the-shelf model with/without post-processing), 2) Ablation on "Middle" Types (evaluate fine-tuned model on Single-line vs. Random-span), 3) Overlap Detection (implement remove_overlap_prefix_middle function and test on fine-tuned model)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reduction in post-processing requirements for fine-tuned models generalize to programming languages with significantly different syntactic structures than Python?
- Basis: Authors state in Limitations that generalizability to other languages "remains an open question" due to focus on Python benchmarks
- Why unresolved: Study relies exclusively on Python-centric HumanEval Infilling and SAFIM
- What evidence would resolve it: Evaluating fine-tuned models on multi-lingual FIM benchmarks (e.g., MultiPL-E)

### Open Question 2
- Question: Do post-processing savings observed in benchmarks translate to complex, less constrained real-world code editing environments?
- Basis: Authors note in Limitations that performance "might vary in more complex or less constrained FIM scenarios"
- Why unresolved: Standard benchmarks represent constrained tasks rather than messy context of full repository editing
- What evidence would resolve it: User studies or evaluations using repository-level completion benchmarks (e.g., RepoBench)

### Open Question 3
- Question: Can modifications to training objective or data composition eliminate need for post-processing in random-span infilling tasks?
- Basis: Paper concludes fine-tuning removes need for post-processing in line-based tasks but "remains necessary when middle is random span"
- Why unresolved: Current fine-tuning teaches boundary awareness for whole lines but fails to fully constrain generation for partial line segments
- What evidence would resolve it: Experiments with specialized span-boundary tokens or curriculum learning targeting partial-line completion

## Limitations
- Reliance on synthetic training data generated by teacher model raises questions about generalization to human-written code patterns
- Focus on Python code limits generalizability to other programming languages with different syntax and structure
- The claim that raw outputs preserve valid multi-line alternatives relies on pass@1 scores rather than direct analysis of output structure

## Confidence

**High Confidence:** The finding that lightweight SFT significantly improves FIM performance compared to off-the-shelf models is well-supported by experimental results across multiple model sizes and both base and instruct variants.

**Medium Confidence:** The claim that post-processing is unnecessary for complete-line middle segments when using fine-tuned models, but still required for random-span infilling, is supported by results but depends heavily on specific evaluation metrics used.

**Low Confidence:** Mechanism explanations, particularly around why fine-tuned models avoid overlap generation for complete-line tasks, rely on internal assumptions about model behavior that aren't directly verified.

## Next Checks

1. **Cross-Language Generalization Test:** Evaluate fine-tuned Qwen2.5-Coder models on FIM tasks for Java and JavaScript functions using HumanEval Infilling benchmark adapted for these languages.

2. **Edge Case Robustness Analysis:** Create curated test suite of Python functions containing challenging patterns (nested comprehensions, decorators, context managers, complex control flow) and measure whether fine-tuned models maintain advantage over off-the-shelf models.

3. **Alternative Post-Processing Ablation:** Implement and evaluate alternative post-processing strategies beyond overlap removal function, such as semantic parsing to identify code boundaries or model-based confidence scoring to determine when truncation is safe.