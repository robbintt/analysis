---
ver: rpa2
title: Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher
  Sampling
arxiv_id: '2507.00518'
source_url: https://arxiv.org/abs/2507.00518
tags:
- vmf-exp
- action
- sampling
- large
- orono
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces von Mises-Fisher exploration (vMF-exp), a
  scalable method for exploring large action sets in reinforcement learning problems
  where hyperspherical embedding vectors represent these actions. vMF-exp involves
  initially sampling a state embedding representation using a von Mises-Fisher distribution,
  then exploring this representation's nearest neighbors, which scales to virtually
  unlimited numbers of candidate actions.
---

# Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling

## Quick Facts
- arXiv ID: 2507.00518
- Source URL: https://arxiv.org/abs/2507.00518
- Reference count: 40
- Primary result: Scalable exploration of large action sets using hyperspherical embeddings and von Mises-Fisher sampling, achieving performance comparable to Boltzmann Exploration while overcoming scalability issues

## Executive Summary
This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable method for exploring large action sets in reinforcement learning problems where actions are represented as hyperspherical embedding vectors. The method achieves scalability by requiring only sampling a vector of fixed size d, independent of the number of actions n, rather than computing softmax values for each action as in traditional Boltzmann Exploration. Under theoretical assumptions, vMF-exp asymptotically maintains the same probability of exploring each action as B-exp while overcoming its scalability issues. Experiments on simulated data, real-world public data, and successful large-scale deployment on a global music streaming service empirically validate the method's key properties and practical relevance.

## Method Summary
vMF-exp operates by sampling a d-dimensional vector from a von Mises-Fisher distribution centered at the current state embedding, then using an approximate nearest neighbor search to find the most similar action. This approach decouples the sampling complexity from the action set size, requiring only O(1) sampling time regardless of n. The method maintains theoretical exploration properties by integrating the von Mises-Fisher density over Voronoi cells on the hypersphere, achieving asymptotic equivalence to Boltzmann Exploration under specific distributional assumptions about the embedding vectors.

## Key Results
- vMF-exp scales to virtually unlimited numbers of candidate actions by sampling only fixed-size d-dimensional vectors
- The method achieves asymptotic equivalence to Boltzmann Exploration in probability of selecting each action
- Real-world deployment on a global music streaming service demonstrates practical relevance and effectiveness
- Experiments show vMF-exp can recommend more diverse playlists without compromising performance compared to truncated Boltzmann Exploration

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Decoupling for Scalability
- **Claim:** vMF-exp decouples sampling complexity from the action set size $n$ by operating in the embedding space $d$ rather than the discrete action space.
- **Mechanism:** Instead of computing softmax probabilities for all $n$ actions (B-exp), vMF-exp samples a single $d$-dimensional vector $\tilde{V}$ from a von Mises-Fisher distribution centered at the state $V$. This operation is $O(1)$ relative to $n$. The method then delegates the action retrieval to an Approximate Nearest Neighbor (ANN) search engine, which operates in sublinear time.
- **Core assumption:** The system has access to an efficient ANN engine capable of indexing unit-norm vectors.
- **Evidence anchors:** [abstract] "requires only sampling a vector of fixed size d, independent of the number of actions n" [section 3.2] "vMF-exp only requires sampling a d-dimensional vector... allowing $\tilde{V}$ to be sampled in constant time"

### Mechanism 2: Asymptotic Equivalence via Voronoi Cell Integration
- **Claim:** The probability of selecting an action $a$ using vMF-exp asymptotically approximates the Boltzmann probability as the action set grows.
- **Mechanism:** The probability of selecting action $i$ is defined as the integral of the vMF probability density function over the Voronoi cell of action $i$ on the hypersphere. As $n \to \infty$, these Voronoi cells shrink, causing the vMF density to become approximately constant over the cell. Under specific distributional assumptions, this integral converges to the Boltzmann softmax term.
- **Core assumption:** Embedding vectors are independent, identically distributed (i.i.d.), and follow a uniform distribution on the unit hypersphere ($X_n \sim U(S^{d-1})$).
- **Evidence anchors:** [abstract] "Under theoretical assumptions, vMF-exp asymptotically maintains the same probability... as Boltzmann Exploration" [section 4.2, Prop 4.1] "limit as n approaches infinity of the ratio [P_bexp / P_vmf] is 1"

### Mechanism 3: Unrestricted Radius Exploration
- **Claim:** vMF-exp ensures non-zero probability for exploring any action in the set, avoiding the "blind spots" of truncated exploration methods.
- **Mechanism:** Because the vMF distribution has support over the entire unit hypersphere, the sampled vector $\tilde{V}$ can theoretically land anywhere. Consequently, the nearest neighbor search can retrieve any action $i$, provided it is the closest neighbor to the sampled point. This satisfies Property P2 (Unrestricted radius).
- **Core assumption:** The ANN engine retrieves the true nearest neighbor (or close approximation) regardless of distance.
- **Evidence anchors:** [section 3.2] "fvMF... is strictly positive. Therefore, vMF-exp satisfies the unrestricted radius property (P2)."

## Foundational Learning

- **Concept: Hyperspherical Embeddings**
  - **Why needed here:** vMF-exp is explicitly designed for actions represented as unit vectors ($L_2$ norm = 1). Unbounded vectors do not lie on the $S^{d-1}$ manifold and break the geometric assumptions of the von Mises-Fisher distribution.
  - **Quick check question:** Are your action embeddings normalized to unit length prior to indexing?

- **Concept: von Mises-Fisher (vMF) Distribution**
  - **Why needed here:** This is the core sampling primitive. You must understand that $\kappa$ (concentration) controls the trade-off between exploitation (high $\kappa$, close to state) and exploration (low $\kappa$, uniform on sphere).
  - **Quick check question:** If $\kappa = 0$, what does the sampled distribution look like? (Answer: Uniform on the sphere).

- **Concept: Approximate Nearest Neighbor (ANN) Search**
  - **Why needed here:** vMF-exp relies on an external ANN index (e.g., Faiss, HNSW) to map the continuous sampled vector back to a discrete action ID. The latency and recall of this index determine the system's real-world performance.
  - **Quick check question:** Does your ANN index support inner product or cosine similarity search on normalized vectors?

## Architecture Onboarding

- **Component map:** Input State Vector $V$ (Unit Norm) -> Parameter Concentration $\kappa$ -> vMF Sampler -> Generates query vector $\tilde{V}$ -> ANN Index (Inner Product/Cosine) -> Finds $\text{argmax}_{i} \langle \tilde{V}, X_i \rangle$ -> Action ID $i$

- **Critical path:** The implementation of the **vMF Sampler**. While standard libraries exist (e.g., `scipy`), the paper notes using specific samplers (Pinz√≥n & Jung, 2023) for speed. If this sampler is slow, the "constant time" advantage over softmax is lost.

- **Design tradeoffs:**
  - **vMF-exp vs. TB-exp:** vMF-exp provides global exploration (unrestricted radius) but offers less control over the *exact* number of candidates considered compared to Truncated B-exp.
  - **Approximation Quality:** ANN recall vs. Latency. If ANN recall is low, the theoretical exploration properties might degrade as the "true" nearest neighbor is missed.

- **Failure signatures:**
  - **Mode Collapse:** If $\kappa$ is too high, the system acts greedily (no exploration).
  - **Noise:** If $\kappa$ is too low, the system recommends random/irrelevant items (uniform noise).
  - **Dimensionality Trap:** If embeddings are not uniform (violating Sec 4.1 assumptions), vMF-exp might exhibit different exploration biases than expected B-exp behavior.

- **First 3 experiments:**
  1. **Sampler Validation:** Sample 10,000 vectors with fixed $\kappa$ and $V$. Compute the mean cosine similarity with $V$. Verify it matches the theoretical mean of the vMF distribution.
  2. **Small-Scale Equivalence:** On a toy dataset (n=1000, uniform embeddings), run 100,000 trials. Compare the empirical frequency of action selection for vMF-exp vs. exact B-exp. Confirm convergence as per Prop 4.1.
  3. **Latency Benchmark:** Measure the end-to-end latency of generating an action with $n=10^6$ actions using vMF-exp + ANN vs. a truncated softmax baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical guarantees of vMF-exp be extended to non-uniform or clustered embedding distributions?
- **Basis in paper:** [explicit] Section 4.3 states that future work should study vMF-exp in clustered settings (e.g., music genres), as current guarantees are restricted to uniform distributions.
- **Why unresolved:** Real-world embeddings (e.g., GloVe, music) are rarely i.i.d. uniform, creating a gap between the proven asymptotic behavior and practical performance.
- **What evidence would resolve it:** Deriving convergence bounds or error rates for vMF-exp under mixture models or specific non-uniform distributions on the hypersphere.

### Open Question 2
- **Question:** How do approximation errors in ANN search engines quantitatively perturb the exploration probabilities of vMF-exp?
- **Basis in paper:** [explicit] Section 4.3 notes the analysis assumes exact neighbor retrieval, which may break down in practice and cause exploration perturbations.
- **Why unresolved:** vMF-exp relies on ANN for scalability, but the theoretical properties (P1, P2, P3) assume exact retrieval, leaving the impact of ANN precision unquantified.
- **What evidence would resolve it:** A theoretical analysis or empirical study measuring probability distortion and order preservation when using approximate indices like HNSW.

### Open Question 3
- **Question:** What is the significance of the second-order term in Proposition 4.4 for vMF-exp performance when the concentration parameter $\kappa$ is large?
- **Basis in paper:** [explicit] Section 4.3 identifies the investigation of this second-order term, which may be significant for large $\kappa$, as future work.
- **Why unresolved:** High $\kappa$ implies low entropy (more exploitation), and understanding the deviation from B-exp in this regime is critical for tuning the hyperparameter.
- **What evidence would resolve it:** Formalizing the $O(1/n^{2/(d-1)})$ deviation term to provide a corrected approximation or bounds for high-$\kappa$ scenarios.

## Limitations
- Theoretical guarantees rely on i.i.d. uniform embedding distributions, which real-world embeddings often violate
- ANN recall quality directly impacts the exploration properties and may cause systematic deviations from theoretical expectations
- Hyperparameter sensitivity requires careful tuning of concentration parameter $\kappa$ based on embedding dimensionality and problem structure

## Confidence
- **High Confidence:** The mechanism of decoupling sampling from action set size through $d$-dimensional sampling (Mechanism 1) is well-supported by both theoretical analysis and empirical evidence.
- **Medium Confidence:** The asymptotic equivalence to Boltzmann Exploration (Mechanism 2) holds under stated assumptions, but real-world embedding distributions may violate these assumptions.
- **Medium Confidence:** The unrestricted exploration property (Mechanism 3) is theoretically sound, though ANN recall quality directly impacts practical exploration coverage.

## Next Checks
1. **Embedding Distribution Analysis:** Characterize the empirical distribution of your action embeddings. Test whether they approximate uniformity on the hypersphere, or if clustering exists that might invalidate the theoretical guarantees.
2. **ANN Recall Impact Study:** Measure how ANN recall rates (e.g., 90% vs 99%) affect the empirical exploration distribution compared to exact nearest neighbor search. Quantify the deviation from theoretical expectations.
3. **Dimensionality Sensitivity Test:** Evaluate vMF-exp performance across different embedding dimensionalities (e.g., d=64, 128, 512) to identify potential "curse of dimensionality" effects on ANN efficiency and exploration quality.