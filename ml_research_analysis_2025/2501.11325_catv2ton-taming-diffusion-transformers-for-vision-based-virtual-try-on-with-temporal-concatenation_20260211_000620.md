---
ver: rpa2
title: 'CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with
  Temporal Concatenation'
arxiv_id: '2501.11325'
source_url: https://arxiv.org/abs/2501.11325
tags:
- video
- try-on
- virtual
- arxiv
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CatV2TON is a vision-based virtual try-on method using diffusion
  transformers to support both image and video try-on tasks. It temporally concatenates
  garment and person inputs and trains on a mixed image-video dataset.
---

# CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation

## Quick Facts
- **arXiv ID:** 2501.11325
- **Source URL:** https://arxiv.org/abs/2501.11325
- **Reference count:** 40
- **Primary result:** Achieves SSIM of 0.8902 on VITON-HD and 0.9222 on DressCode datasets, outperforming existing methods

## Executive Summary
CatV2TON introduces a unified vision-based virtual try-on method using diffusion transformers that handles both image and video try-on tasks. The key innovation is temporal concatenation of garment and person inputs, allowing a single model to process both static images and dynamic videos. For long video generation, the method employs an overlapping clip-based inference strategy with Adaptive Clip Normalization (AdaCN) to maintain temporal consistency. The approach also introduces ViViD-S, a refined video try-on dataset with filtered back-facing frames and enhanced 3D mask smoothing.

## Method Summary
CatV2TON uses a pre-trained diffusion transformer backbone initialized from EasyAnimateV4, with only self-attention layers and a duplicated pose encoder trainable. The model temporally concatenates garment and person inputs along the temporal dimension, enabling unified image and video processing. Training uses progressive scaling (256×192 → 512×384 → 832×624) with 3 stages, batch sizes from 16 to 1, and 4× A100 GPUs. For inference, long videos are generated via overlapping clips where final frames guide subsequent generation, with AdaCN normalizing feature statistics between segments to prevent color drift and flickering.

## Key Results
- Achieves SSIM of 0.8902 on VITON-HD dataset and 0.9222 on DressCode dataset
- Improves ViViD-S video try-on SSIM to 0.8727 and VFIDI to 13.5962
- Outperforms existing methods on both image and video tasks while using a single unified model architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single diffusion transformer can handle both image and video try-on tasks by temporally concatenating garment and person inputs.
- Mechanism: By treating garment and person as a sequence along the temporal dimension, the same model architecture can process a single frame (image try-on) or a sequence of frames (video try-on). The self-attention layers learn relationships between garment and person frames without requiring separate models.
- Core assumption: The pre-trained DiT backbone possesses sufficient inductive bias to transfer learned temporal dynamics to the try-on task.
- Evidence: The paper demonstrates robust try-on performance across static and dynamic settings using this unified approach.

### Mechanism 2
- Claim: An overlapping clip-based inference strategy with Adaptive Clip Normalization (AdaCN) maintains temporal consistency in long video generation.
- Mechanism: Long videos are divided into overlapping clips, with final frames of each generated clip serving as prompts for the next. AdaCN normalizes feature statistics of new clips to match guiding frames, preventing drift.
- Core assumption: Feature statistics (mean and standard deviation) in latent space are sufficient proxies for visual properties like color and tone.
- Evidence: The method successfully prevents color discrepancies and motion misalignment in generated long videos.

### Mechanism 3
- Claim: A refined video try-on dataset (ViViD-S) enhances model convergence and output quality.
- Mechanism: The dataset filters back-facing frames where only front-view garment images are available, and applies 3D mask smoothing using spatial and temporal average pooling followed by re-binarization.
- Core assumption: Back-facing frames are primarily noise for this task given lack of back-view garment data.
- Evidence: The refined dataset improves convergence speed and reduces flickering artifacts in video outputs.

## Foundational Learning

**Concept: Diffusion Transformers (DiTs)**
- Why needed here: CatV2TON's entire backbone is a Diffusion Transformer, replacing traditional U-Nets with self-attention layers operating on patches
- Quick check question: How does a DiT process an input video or image differently than a U-Net based diffusion model?

**Concept: Latent Space and VAEs**
- Why needed here: The model operates in a latent space encoded by a Video VAE, with AdaCN applied to these latent features
- Quick check question: In the context of CatV2TON, on what representation does the main DiT backbone operate?

**Concept: Classifier-Free Guidance (CFG)**
- Why needed here: The paper mentions dropping the garment condition during training (10% chance) to enable classifier-free guidance
- Quick check question: Why is the garment condition randomly dropped during a portion of the training steps?

## Architecture Onboarding

**Component map:** Person Video/Image, Garment Image, Clothing-Agnostic Mask, DensePose → Video VAE Encoder → DiT Backbone (N stacked blocks from EasyAnimateV4) → Pose Encoder (trainable duplicated block) → Adaptive Clip Normalization (inference only) → Video VAE Decoder

**Critical path:** Inputs are concatenated along temporal dimension → DiT backbone processes latents with pose features injected via element-wise addition → For long videos, overlapping clips are generated sequentially with AdaCN normalizing each new clip based on previous clip statistics

**Design tradeoffs:** Efficiency vs. Capability (freezing >80% of parameters reduces resources but may limit learning), Coherence vs. Complexity (overlapping clip strategy avoids memory cost but introduces transition complexity), Dataset Purity vs. Generality (filtering back-facing frames improves front-view performance but sacrifices rear-view capability)

**Failure signatures:** Color/flicker drift between video segments indicates AdaCN failure, poor pose alignment suggests Pose Encoder issues, garment bleeding indicates mask problems

**First 3 experiments:** 1) Baseline image try-on on single VITON-HD image, 2) Short video generation (32 frames) to evaluate temporal consistency without overlapping clips, 3) Long video ablation comparing AdaCN enabled vs disabled for color drift and flickering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can virtual try-on models be improved to strictly adhere to physical laws regarding clothing dynamics?
- Basis in paper: The authors state that there is a lack of foundational video generation models that can accurately simulate physical behaviors, leading to unrealistic artifacts during complex actions
- Why unresolved: The current DiT backbone prioritizes temporal coherence and visual texture over physically plausible cloth simulation
- What evidence would resolve it: Integration of physics-informed loss functions or hybrid architectures combining generative models with explicit cloth simulation engines

### Open Question 2
- Question: Can video try-on models synthesize realistic back-facing garment details without requiring explicit back-view training data?
- Basis in paper: ViViD-S filters out back-facing frames because generating try-ons for rear-facing poses is unfeasible due to lack of back-view garment information
- Why unresolved: The current solution is data-curation rather than generative capability
- What evidence would resolve it: A model capable of zero-shot inference on 360-degree rotation videos, successfully hallucinating consistent garment textures on the reverse side

### Open Question 3
- Question: Is Adaptive Clip Normalization (AdaCN) a robust long-term solution for temporal consistency?
- Basis in paper: AdaCN corrects color discrepancies and motion misalignment caused by feature shifts during sequential clip generation
- Why unresolved: While AdaCN aligns feature statistics, it may suppress necessary dynamic variations or fail to correct high-frequency structural errors in very long videos
- What evidence would resolve it: Ablation studies on video lengths exceeding training distribution comparing AdaCN against latent space propagation methods

## Limitations
- The model cannot handle back-facing poses due to lack of back-view garment data in training datasets
- Performance may degrade for videos with significant camera motion or extreme pose changes
- The overlapping clip strategy with AdaCN may introduce artifacts in very long videos beyond the training distribution

## Confidence

**High Confidence:** The mechanism of using temporal concatenation to unify image and video processing in a single DiT model is well-supported by the paper's architecture description and experimental results

**Medium Confidence:** The overlapping clip-based inference strategy with AdaCN is plausible but the normalization approach's sufficiency for maintaining temporal consistency across diverse video content remains partially validated

**Low Confidence:** The dataset filtering methodology effectiveness is based on reasonable assumptions but lacks ablation studies showing the impact of including vs. excluding back-facing frames

## Next Checks

1. **Architecture Fidelity Check:** Verify exact DiT dimensions by cross-referencing with EasyAnimateV4 implementation details and test whether the model can reproduce baseline image try-on results before video generation

2. **AdaCN Robustness Test:** Generate videos with varying levels of color contrast and motion to stress-test the normalization mechanism's ability to maintain temporal consistency

3. **Dataset Filtering Ablation:** Run controlled experiments training with and without the back-facing frame filtering to quantify the claimed benefits on convergence speed and output quality