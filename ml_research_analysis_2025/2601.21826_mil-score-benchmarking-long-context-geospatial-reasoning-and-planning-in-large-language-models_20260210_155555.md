---
ver: rpa2
title: 'Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in
  Large Language Models'
arxiv_id: '2601.21826'
source_url: https://arxiv.org/abs/2601.21826
tags:
- reasoning
- geospatial
- milscore
- planning
- military
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MilSCORE is a new benchmark for long-context geospatial reasoning
  in military planning, built from expert-authored questions grounded in real operational
  orders and maps. It evaluates VLMs on multi-hop, scenario-level tasks requiring
  integration of maps, documents, and structured data.
---

# Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models

## Quick Facts
- arXiv ID: 2601.21826
- Source URL: https://arxiv.org/abs/2601.21826
- Authors: Aadi Palnitkar; Mingyang Mao; Nicholas Waytowich; Vinicius G. Goecks; Tinoosh Mohsenin; Xiaomin Lin
- Reference count: 40
- Primary result: Current VLMs achieve only 21.7%–58.3% accuracy on 60 expert-authored geospatial reasoning questions requiring multi-hop reasoning over maps, documents, and structured data.

## Executive Summary
Mil-SCORE is a new benchmark for evaluating long-context geospatial reasoning in military planning, built from expert-authored questions grounded in real operational orders and maps. It tests VLMs on multi-hop, scenario-level tasks requiring integration of maps, documents, and structured data. Testing on a 60-question slice, current models achieve only 21.7%–58.3% accuracy, with GPT-4o outperforming others by completing multi-step reasoning loops within tight step limits. Higher-tier questions involving cross-source synthesis show the largest performance gaps, indicating substantial headroom for future model improvements in realistic, long-context military decision-making.

## Method Summary
Mil-SCORE uses a tool-using ReAct (Reason + Act) agent to evaluate VLMs on 60 expert-authored questions from a single OPORD scenario. The agent has four tools (list_files, read_document_text, query_spreadsheet, get_image_data) and a 10-step cap to enforce efficient evidence gathering. Questions are tiered by difficulty: Tier 1 (single-hop), Tier 2 (single-source multi-hop), and Tier 3 (cross-source multi-hop). Model answers are scored 0–3 by an LLM grader (GPT-5) based on boxed answer accuracy, with human grading used for verification on 15% of cases.

## Key Results
- GPT-4o leads with 58.3% accuracy by completing multi-step loops within the 10-step budget, while other models fail due to step exhaustion or hallucination.
- Claude Sonnet 4.5 shows a dramatic accuracy drop from 30% (Tier 1) to 0% (Tier 3), demonstrating the difficulty of cross-source synthesis.
- Tier 3 (cross-source) tasks show the largest performance gaps, confirming that integrating heterogeneous information is the primary bottleneck for current VLMs.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A tool-using ReAct loop improves performance on long-context, multi-hop geospatial reasoning by enforcing explicit evidence gathering before answer formulation.
- **Mechanism:** The evaluation agent operates via a ReAct loop. For a given question, the model must propose a tool call (e.g., `list_files`, `read_document_text`, `query_spreadsheet`, `get_image_data`). The system executes the tool and injects the result back into the context as a new message. This forces the model to reason over the newly retrieved evidence before taking the next step or producing a final boxed answer, preventing it from relying solely on parametric memory.
- **Core assumption:** Models can be instructed via a system prompt to reliably follow the tool-calling protocol and will not hallucinate tool outputs or answers without grounding.
- **Evidence anchors:
  - [Section 4.1]:** "The agent follows a simple ReAct loop: the model proposes a tool name with JSON arguments; the runtime executes the tool and immediately appends a ToolMessage..."
  - [Section 4.1]:** "...when the model calls get_image_data we both (a) return a ToolMessage acknowledging success, and (b) append a new user message that carries the downscaled image... so the model can actually 'see' the map."
  - [Corpus]:** Related work on "synergizing RAG and reasoning" [10] and "Spatial-RAG" [43] supports the efficacy of combining retrieval with reasoning loops, though direct evidence for Mil-SCORE's specific implementation is from the paper's methodology.
- **Break condition:** The mechanism fails if models do not adhere to the tool-calling contract (e.g., generating free-form text instead of a tool call), get stuck in repetitive tool-use loops, or if the tool descriptions are insufficient for the model to select the correct tool. The paper notes "max iterations reached" errors as a primary failure mode for some models.

### Mechanism 2
- **Claim:** A tiered task taxonomy (single-hop, multi-hop, cross-source) enables diagnostic assessment of model capabilities, revealing that complex cross-source synthesis is a primary performance bottleneck.
- **Mechanism:** Questions are categorized by difficulty and spatial-analysis type. By evaluating models across these tiers, the benchmark isolates specific failure modes. A large performance drop on Tier 3 (cross-source) tasks directly indicates a struggle with integrating heterogeneous information (e.g., combining map data with textual orders), whereas errors on Tier 1 point to failures in basic perception or recall. This allows for pinpointing where a model's reasoning process breaks down.
- **Core assumption:** The taxonomy accurately reflects distinct cognitive challenges, and performance differences are attributable to these complexity differences, not confounding factors.
- **Evidence anchors:
  - [Abstract]:** "Higher-tier questions involving cross-source synthesis show the largest performance gaps..."
  - [Section 4]:** Table 2 shows Claude Sonnet 4.5's accuracy dropping from 30% (Tier 1) to 0% (Tier 3), demonstrating the intended diagnostic gap.
  - [Corpus]:** Weak corpus evidence. While related benchmarks like GeoAnalystBench [45] use task categories, they do not establish a causal mechanism for why this *causes* better assessment, only that it is a standard practice.
- **Break condition:** The diagnostic value degrades if questions are miscategorized or if a model's performance on a tier is dominated by extraneous factors like token limits or phrasing ambiguities.

### Mechanism 3
- **Claim:** A constrained step budget in the agent loop serves as a proxy for operational efficiency, creating selection pressure that favors models capable of concise, planned evidence-gathering.
- **Mechanism:** The evaluation imposes a hard cap (e.g., 10 steps) on the agent's tool/think cycles. This forces the model to be efficient. Models that "narrate at length" or use turns for description without tool calls exhaust the budget before gathering evidence and fail. Models that are terse and immediately chain relevant tools succeed. This mimics real-world needs for rapid, efficient decision-making.
- **Core assumption:** The chosen step limit is a reasonable proxy for efficient problem-solving. It assumes a capable model should be able to plan a concise path to the answer.
- **Evidence anchors:
  - [Section 4]:** "GPT-4o leads the slice because it is the only model that consistently finishes multi-step loops within the 10-step cap... Larger 'reasoning' variants... burn through the 10-step budget..."
  - [Section 4.1]:** "All runs use... a cap of 10 tool/think steps."
  - [Corpus]:** No direct corpus evidence for this specific mechanism; it is an internal design choice of the Mil-SCORE protocol.
- **Break condition:** If the step limit is set too low for inherently complex tasks, it will artificially deflate model performance. If set too high, it fails to penalize inefficient reasoning.

## Foundational Learning

- **Concept:** **ReAct (Reason + Act) Pattern.**
  - **Why needed here:** This is the core operational pattern for the evaluation agent. Understanding it is essential to interpret the results, which depend heavily on whether a model can successfully complete the loop within the step limit.
  - **Quick check question:** What is the fundamental sequence of steps in a ReAct loop, and what must a model produce to trigger the next cycle?

- **Concept:** **Chain-of-Thought (CoT) Prompting.**
  - **Why needed here:** The tool-using agent is a form of CoT. The benchmark's goal is to test reasoning, and CoT is the method used to make that reasoning explicit and verifiable.
  - **Quick check question:** How does forcing a model to generate intermediate reasoning steps (a chain of thought) before a final answer potentially improve the accuracy of complex, multi-step reasoning tasks?

- **Concept:** **Long-Context Window vs. Effective Context Utilization.**
  - **Why needed here:** A central problem the paper identifies is that models with large context windows still fail because they do not *effectively utilize* the full context. They may focus on the beginning/end or get lost in irrelevant details.
  - **Quick check question:** What is the key difference between a model's *theoretical* context window size and its *effective* context utilization, and what failure mode does poor utilization lead to?

## Architecture Onboarding

- **Component map:** The core system is an evaluation harness containing: 1) A **Question Dataset** (expert-authored, tiered, categorized), 2) A **Scenario Corpus** (maps, OPORDs, GeoJSON data), 3) A **Tool-using Agent** (the VLM being tested, equipped with tools like file listing, document reading, etc.), 4) A **Runtime/Orchestrator** (executes the ReAct loop, enforces step limits, injects tool results), and 5) A **Grading Agent** (an LLM that compares the model's final boxed answer to the expert reference).
- **Critical path:** The key dependency for success is the model's ability to follow the **Tool Discovery → Tool Execution → Evidence Synthesis** path. The model must first use the `list_files` tool to discover available sources, then use the specific reading/querying tools to extract information, and finally synthesize that information into a coherent answer, all within the step limit.
- **Design tradeoffs:**
  - **Step Budget vs. Task Complexity:** A low step limit ensures efficiency but may cause failures on genuinely complex tasks requiring many retrieval steps. A high limit allows for complex reasoning but provides a weaker signal about efficiency.
  - **Zero-shot vs. Few-shot Prompting:** The baseline uses zero-shot, which is a harder test of pure capability. Few-shot prompting could improve performance by demonstrating the desired reasoning pattern but would require additional engineering.
  - **Automated vs. Human Grading:** Using an LLM as a grader (GPT-5 in the paper) allows for scalable evaluation but introduces potential bias. Human grading is more reliable but not scalable for large benchmarks.
- **Failure signatures:**
  - **Max Iterations Reached:** The most common failure, indicating the model was inefficient, narrating instead of acting, or stuck in a loop.
  - **Lack of Boxed Citation:** For Tier 1 questions, models may try to answer from memory without using tools, resulting in an incorrect format or hallucinated answer.
  - **Cross-Source Synthesis Failure:** Inability to correctly combine information from a map and a document, leading to factually incorrect conclusions.
- **First 3 experiments:**
  1.  **Baseline Performance Audit:** Run the provided 60-question slice against the target VLM (e.g., GPT-4o, Claude) using the default 10-step budget. Categorize each failure as "Max Iterations," "Incorrect Answer," or "Format Error" to establish a failure mode profile.
  2.  **Step Budget Sensitivity Analysis:** Re-run a subset of failed cross-source (Tier 3) questions with an increased step limit (e.g., 15 or 20 steps). This tests whether failures were due to task complexity or efficiency constraints.
  3.  **Ablation on Tool Discovery:** Manually provide the correct source filenames in the prompt for a set of questions and measure the performance change. This isolates the difficulty of "source discovery" from the difficulty of "content understanding."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can models be trained or prompted to better handle the selective reading required for long-context geospatial reasoning without exhausting step budgets?
- **Basis in paper:** [explicit] Authors note "substantial headroom" and that "current systems struggle with realistic, scenario-level long-context planning," with larger reasoning variants burning through the 10-step budget before completing evidence collection.
- **Why unresolved:** The paper identifies the problem but does not propose solutions for improving step efficiency or context prioritization in multi-hop reasoning loops.
- **What evidence would resolve it:** A model that achieves >70% accuracy on Tier 3 tasks while staying within a fixed step budget, or ablation studies showing which prompting/training strategies reduce unnecessary narration.

### Open Question 2
- **Question:** How does model performance scale with increased step limits, and what is the optimal step budget for balancing accuracy against API costs?
- **Basis in paper:** [inferred] The 10-step cap was imposed "to keep API costs bounded," and Claude Sonnet achieved 0% on Tier 3 primarily due to "max iterations reached" errors rather than reasoning failures.
- **Why unresolved:** The paper does not report results under relaxed step constraints, so it is unclear whether poor performance reflects reasoning limitations or artificial budget constraints.
- **What evidence would resolve it:** Performance curves plotting accuracy against step limits (e.g., 10, 20, 50 steps) for each model, with cost-per-question estimates.

### Open Question 3
- **Question:** Why do models sometimes perform better on cross-source Tier 3 tasks than on apparently simpler Tier 1/2 questions?
- **Basis in paper:** [explicit] Authors observe that "Tier 3 accuracies beat Tier 1/Tier 2 for the models that survive the loop," noting that Tier 1 questions often trigger hallucination when models skip tools, while Tier 3's forced tool-use pattern aligns better with agent instructions.
- **Why unresolved:** This counterintuitive finding suggests either a misalignment between tier definitions and true difficulty, or evaluation artifacts from the tool-use protocol.
- **What evidence would resolve it:** Ablation studies with direct prompting (no tools) versus tool-use protocols across all tiers, and human difficulty ratings correlated with model performance.

### Open Question 4
- **Question:** To what extent do findings from a single OPORD training scenario generalize to other military contexts or civilian geospatial planning domains?
- **Basis in paper:** [inferred] The dataset is "grounded in a single OPORD-style training scenario" with 50 maps from one operation, limiting diversity of tactical situations and geographic regions.
- **Why unresolved:** No experiments validate whether models trained or evaluated on this scenario transfer to different terrains, doctrine variants, or planning contexts.
- **What evidence would resolve it:** Cross-scenario transfer experiments using MilSCORE-trained models on held-out OPORD scenarios from different geographic regions or military branches.

## Limitations
- The 60-question slice represents only a subset of the full benchmark, limiting generalizability.
- The fixed 10-step budget may artificially constrain complex reasoning tasks, making efficiency differences appear as capability gaps.
- The LLM grader (GPT-5) introduces potential bias, though human grading was used to verify 15% of cases.

## Confidence
- **High Confidence**: The benchmark design itself (tiered taxonomy, expert-authored questions, tool-using ReAct agent) is methodologically sound and well-documented. The observation that cross-source synthesis tasks show the largest performance gaps is clearly supported by the data (Claude Sonnet 4.5: 30%→0% accuracy from Tier 1 to Tier 3).
- **Medium Confidence**: Claims about GPT-4o's superior performance and efficiency are supported by the evaluation but could be influenced by the step budget constraint rather than pure reasoning ability. The diagnostic value of the tiered taxonomy is demonstrated but relies on the assumption that categorization accurately reflects cognitive complexity.
- **Low Confidence**: The exact mechanism by which the 10-step budget creates meaningful selection pressure for efficiency is not empirically validated beyond observing that some models fail to complete tasks within this limit.

## Next Checks
1. **Step Budget Sensitivity Analysis**: Re-run the evaluation slice with variable step budgets (5, 10, 15, 20) to determine whether performance improvements at higher limits indicate true reasoning capability versus artificial constraint effects.
2. **Human Grading Verification**: Grade a stratified random sample (minimum 25%) of all model outputs using human experts to quantify grader bias and establish ground truth accuracy rates independent of the GPT-5 scoring system.
3. **Ablation Study on Tool Discovery**: For a subset of Tier 3 questions, manually provide correct source filenames in the prompt and compare performance to the standard ReAct loop, isolating whether failures stem from source discovery versus cross-source synthesis ability.