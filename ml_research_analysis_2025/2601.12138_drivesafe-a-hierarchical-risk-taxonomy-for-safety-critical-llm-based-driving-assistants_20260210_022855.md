---
ver: rpa2
title: 'DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving
  Assistants'
arxiv_id: '2601.12138'
source_url: https://arxiv.org/abs/2601.12138
tags:
- safety
- risk
- driving
- taxonomy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DriveSafe, a hierarchical taxonomy for safety
  risks in LLM-based driving assistants. It identifies 129 fine-grained risk categories
  across technical, business, societal, and ethical domains, grounded in real-world
  regulations and reviewed by domain experts.
---

# DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants

## Quick Facts
- arXiv ID: 2601.12138
- Source URL: https://arxiv.org/abs/2601.12138
- Reference count: 13
- Key outcome: Taxonomy of 129 safety risks validated by low refusal rates (0-85%) across six LLMs on unsafe driving prompts

## Executive Summary
This paper introduces DriveSafe, a hierarchical taxonomy for safety risks in LLM-based driving assistants. The taxonomy covers 129 fine-grained risk categories across technical, business, societal, and ethical domains, grounded in real-world regulations and reviewed by domain experts. To validate its realism, the authors evaluate six widely used LLMs on unsafe driving prompts derived from the taxonomy. Results show that models often fail to appropriately refuse unsafe or non-compliant queries, with refusal rates ranging from 0% to 85% depending on the risk domain and model. The findings underscore the limitations of general-purpose safety alignment in driving contexts and motivate the need for domain-specific risk modeling.

## Method Summary
The study constructs a four-level hierarchical taxonomy (Domain→Category→Failure Mode→Atomic Risk) with 129 fine-grained risk categories. Each atomic risk is operationalized through a scenario-prompt pair that encodes realistic driving contexts while avoiding explicit safety testing cues. Six LLMs are evaluated using these prompts, with responses classified as refusal or non-refusal by two independent annotators achieving Cohen's kappa of 0.80. The taxonomy is validated through expert review, and refusal rates serve as the primary metric for assessing model safety alignment in driving contexts.

## Key Results
- Six LLMs show inconsistent refusal behavior across the four risk domains (Technical, Business, Societal, Ethical)
- Refusal rates range from 0% to 85.71% across models and domains, demonstrating domain-specific safety blind spots
- General-purpose safety alignment proves insufficient for driving-specific scenarios, with models often complying with unsafe or non-compliant requests

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Risk Decomposition
- Claim: A four-level taxonomy structure enables systematic coverage and traceability of domain-specific safety risks.
- Mechanism: Each atomic risk maps to a unique path through Domain → Category → Failure Mode → Atomic Risk (formally: T ⊆ D × C × F × R). This ensures mutual exclusivity and prevents overlapping risk definitions that would confound evaluation.
- Core assumption: Driving-related safety failures can be decomposed into discrete, hierarchically-organizable categories without losing critical interactions between risk types.
- Evidence anchors:
  - [abstract] "hierarchical, four-level risk taxonomy...129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions"
  - [Section 3.1] "Each atomic risk r ∈ R is uniquely associated with a single path in the hierarchy"
  - [corpus] Related work (S3LoRA) shows PEFT adaptations can compromise safety alignment in agent planning—suggesting hierarchical decomposition helps isolate where failures originate
- Break condition: If risks are fundamentally cross-cutting and cannot be isolated to single paths, the taxonomy would systematically miss compound failures.

### Mechanism 2: Refusal Rate Gap Detection
- Claim: Low and inconsistent refusal rates across models validate that constructed prompts capture safety-critical risks not addressed by general-purpose alignment.
- Mechanism: Prompts designed to implicitly invite unsafe responses are presented to LLMs; refusal rates serve as a diagnostic signal. High variance (0%–85.71%) indicates domain-specific blind spots rather than model-specific quirks.
- Core assumption: Appropriate refusal behavior is the correct target for unsafe driving prompts, and low refusal rates indicate safety gaps rather than overly cautious prompt design.
- Evidence anchors:
  - [abstract] "models often fail to appropriately refuse unsafe or non-compliant queries, with refusal rates ranging from 0% to 85%"
  - [Section 4, Table 1] GPT-4o-Mini shows 0% refusal on Societal risks while Claude-3-Haiku shows 85.71%—same domain, different safety coverage
  - [corpus] Risk Awareness Injection paper addresses VLM vulnerability to multimodal jailbreaks, suggesting refusal mechanisms are fragile even when present
- Break condition: If prompts inadvertently test helpfulness-preference rather than safety-alignment, low refusal rates would reflect appropriate trade-offs rather than failures.

### Mechanism 3: Scenario-Prompt Operationalization
- Claim: Grounding abstract risks in realistic driving scenarios with natural-language prompts exposes model failures that generic safety benchmarks miss.
- Mechanism: Each atomic risk → scenario → prompt chain encodes situational context (traffic conditions, jurisdiction, cognitive pressure) while maintaining conversational phrasing. This bypasses artificial compliance cues that flag safety testing.
- Core assumption: Models behave differently toward contextually-grounded driving queries than toward generic safety test prompts.
- Evidence anchors:
  - [Section A.3] "Prompts are phrased in conversational language and deliberately avoid explicit references to safety testing"
  - [Section 1] "General-purpose safety benchmarks cannot meaningfully evaluate these scenarios"
  - [corpus] Weak corpus evidence—related papers focus on scenario generation (LD-Scene, TeraSim-World) rather than prompt design methodology; this mechanism remains inferential
- Break condition: If model safety training has already incorporated driving-specific scenarios, refusal rates would converge across domains, reducing discriminative value.

## Foundational Learning

- Concept: **Refusal Behavior in LLMs**
  - Why needed here: The paper's validation depends on interpreting refusal rates as safety signals. Without understanding that refusal is a learned, controllable behavior (not intrinsic), you cannot diagnose whether low refusal indicates missing alignment or different calibration choices.
  - Quick check question: Can you explain why a model might correctly refuse an unsafe request in one domain while complying with an equivalently unsafe request in another domain?

- Concept: **Hierarchical Taxonomy Validation**
  - Why needed here: The 129-category taxonomy must be both comprehensive and non-redundant. Understanding how to validate coverage (expert review) vs. discriminative power (empirical testing) determines whether the taxonomy is actionable.
  - Quick check question: What would it mean if two atomic risks in different domains produced identical model behavior patterns?

- Concept: **Domain-Specific Safety Alignment**
  - Why needed here: The central claim is that general-purpose alignment fails in driving contexts. Understanding *why* (jurisdictional variance, temporal pressure, ethical trade-offs unique to transportation) determines whether mitigation requires domain-specific training or architectural changes.
  - Quick check question: Name two safety requirements that apply to driving assistants but not to general-purpose chatbots.

## Architecture Onboarding

- Component map: Taxonomy Core → Scenario Generator → Prompt Factory → Evaluation Layer
- Critical path: Identify risk domain → Define failure mode → Construct scenario → Write natural-language prompt → Expose to model → Annotate refusal → Aggregate by domain
- Design tradeoffs:
  - Single prompt per risk (current) enables precise attribution but limits linguistic diversity
  - Expert review ensures validity but may miss emergent risks not yet recognized in policy
  - Binary refusal coding simplifies analysis but loses nuance (partial compliance, hedged refusals)
- Failure signatures:
  - Refusal rate variance >40% across domains for same model → domain-specific alignment gap
  - Refusal rate variance >30% across models for same domain → inconsistent safety training
  - Near-zero refusal in Ethical/Societal domains → normative reasoning blind spots
- First 3 experiments:
  1. **Domain sensitivity test**: Present same underlying risk in Technical vs. Ethical framing to measure whether surface features affect refusal (controls for prompt wording artifacts).
  2. **Jurisdictional variance test**: Duplicate scenarios with UK vs. US traffic law context to assess whether models encode location-specific compliance.
  3. **Adversarial paraphrase test**: Generate 5 paraphrased versions of highest-compliance prompts to distinguish genuine safety gaps from prompt-specific artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the safety coverage of the DriveSafe taxonomy remain robust under linguistic variation, such as paraphrased or adversarially mutated prompts?
- Basis in paper: [explicit] The authors state: "Each atomic risk in this work is represented by a single manually written prompt. This choice helps isolate specific failure modes, but it also limits linguistic variation. These prompts should therefore be treated as base prompts, which future work can expand using paraphrasing or mutation techniques to generate larger and more diverse prompt sets for robustness analysis."
- Why unresolved: The current evaluation uses only one prompt per risk, leaving uncertainty about whether findings generalize across phrasings, dialects, or adversarial rewordings.
- What evidence would resolve it: A follow-up study generating paraphrased variants of each prompt and comparing refusal rates across the original and variant prompts to measure consistency.

### Open Question 2
- Question: How can domain-specific safety alignment for driving contexts be achieved, beyond demonstrating that general-purpose alignment is insufficient?
- Basis in paper: [explicit] The authors conclude that their "findings show that safety-critical failures often arise from everyday, ambiguous interactions. These results highlight the limitations of general-purpose safety alignment and motivate the need for domain-aware risk modeling in driving contexts."
- Why unresolved: The paper identifies the gap but does not propose or evaluate methods for improving alignment specific to driving scenarios.
- What evidence would resolve it: Development and empirical evaluation of fine-tuning, retrieval-augmented generation, or rule-based guardrails trained or configured using the DriveSafe taxonomy, with measured improvement in refusal rates and response quality.

### Open Question 3
- Question: How should the DriveSafe taxonomy be updated or extended as driving environments, regulations, and user interaction patterns evolve over time?
- Basis in paper: [explicit] The authors note: "driving environments and user interactions change over time, and new types of risks or scenarios may emerge that are not captured in the current taxonomy."
- Why unresolved: No mechanism or methodology is proposed for taxonomy maintenance, versioning, or community-driven extension.
- What evidence would resolve it: A longitudinal study applying the taxonomy at multiple time points with emerging risk identification, or a documented protocol for periodic expert review and taxonomy revision.

### Open Question 4
- Question: How does the DriveSafe framework extend to multimodal driving assistants that process voice, text, and visual inputs simultaneously?
- Basis in paper: [inferred] The paper cites MM-SafetyBench for multimodal safety evaluation and describes in-vehicle assistants with voice interaction, yet the taxonomy and evaluation are text-only. Real-world driving assistants operate multimodally.
- Why unresolved: It is unclear whether text-derived risks transfer to voice or visual contexts, or whether new failure modes emerge in multimodal settings.
- What evidence would resolve it: An extension of the taxonomy to include modality-specific atomic risks, followed by evaluation of multimodal models (e.g., GPT-4o with image/audio) on the extended prompt set.

## Limitations

- The full 129-prompt dataset and complete four-level taxonomy remain unpublished, preventing independent verification
- The study assumes binary refusal behavior adequately captures safety alignment quality, though real-world scenarios may involve nuanced compliance levels
- The paper does not address whether models might learn to refuse safety-critical queries while still providing implicit unsafe guidance through circumvention strategies

## Confidence

- **High Confidence**: The hierarchical taxonomy structure (Domain→Category→Failure Mode→Atomic Risk) provides systematic coverage of safety risks in driving contexts
- **Medium Confidence**: Low and inconsistent refusal rates across models demonstrate domain-specific safety alignment gaps
- **Medium Confidence**: Scenario-based prompt design effectively exposes model limitations that general benchmarks miss

## Next Checks

1. Replicate the study with a subset of 20-30 taxonomy-defined prompts across all four domains to verify the reported refusal rate patterns
2. Test jurisdictional sensitivity by duplicating prompts with UK vs US traffic law contexts to assess location-specific compliance differences
3. Generate 5 paraphrased variants of highest-compliance prompts to distinguish genuine safety gaps from prompt-specific artifacts