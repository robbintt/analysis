---
ver: rpa2
title: Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided
  Markets
arxiv_id: '2510.14097'
source_url: https://arxiv.org/abs/2510.14097
tags:
- queue
- iptq
- length
- policy
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets

## Quick Facts
- **arXiv ID**: 2510.14097
- **Source URL**: https://arxiv.org/abs/2510.14097
- **Reference count**: 40
- **Primary result**: Achieves $\tilde{O}(T^{1-\gamma})$ regret with $\tilde{O}(T^{\gamma/2})$ average queue length for $\gamma \in (0, 1/6]$

## Executive Summary
This paper addresses the fundamental tension in online learning for two-sided markets between maintaining queue stability and achieving optimal learning regret. The authors develop a novel probabilistic two-price policy that simultaneously controls queue lengths through negative drift while collecting unbiased samples for learning demand curves. The key insight is using a randomized policy that applies the optimal price half the time for learning and a perturbed price half the time for queue control, allowing the system to achieve a near-optimal Pareto frontier between regret and queue length.

## Method Summary
The method employs a multi-layered approach combining projected gradient ascent on a fluid optimization problem with zero-order gradient estimation. Since the gradient depends on unknown demand functions, the algorithm estimates it using finite differences at random perturbed points. A bisection search maps these gradient estimates back to prices, while a probabilistic two-price policy applies the resulting prices in a way that maintains queue stability (negative drift) while ensuring unbiased sample collection. The system operates on a $2MN$ time slot schedule, with the inner loop running $M$ bisection iterations requiring $N$ clean samples per queue.

## Key Results
- Achieves $\tilde{O}(T^{1-\gamma})$ regret vs. $\tilde{O}(T^{\gamma/2})$ average queue length tradeoff
- Extends regret guarantees to systems with network structure (compatibility graphs)
- Introduces novel probabilistic policy that resolves the tension between learning and stability
- Provides the first near-optimal tradeoff for this setting, improving upon prior work

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Two-Price Policy for Queue-Regret Decoupling
- **Claim:** A probabilistic switching strategy allows the system to simultaneously maintain queue stability (negative drift) and collect unbiased data for learning demand/supply curves.
- **Mechanism:** When a queue $Q(t)$ is non-empty but below a threshold $q_{th}$, the policy sets the price to the estimated optimal price $p(t)$ with probability $1/2$ (to generate clean samples) and a perturbed price $p(t) + \alpha$ with probability $1/2$. The perturbation $\alpha$ introduces a negative drift to reduce queue length, while the probabilistic mixing ensures a constant fraction of time is dedicated to unbiased exploration.
- **Core assumption:** The derivatives of the demand functions are bounded away from zero (Assumption 6(2)), ensuring a small price perturbation $\alpha$ guarantees a meaningful reduction in arrival rate to create drift.
- **Evidence anchors:**
  - [Abstract] Mentions a "probabilistic component that resolves the tension between obtaining useful samples for fast learning and maintaining small queue lengths."
  - [Section 4.3] "For the in-between case of $0 < Q(t) < q_{th}$, we set the price to be $p(t)$ with probability $1/2$ and $p(t)+\alpha$ otherwise."
  - [Corpus] Related work on *Queue Length Regret Bounds for Contextual Queueing Bandits* discusses similar tension but in a scheduling context, not pricing.
- **Break condition:** If the probability of choosing the learning price is reduced significantly below $1/2$, the time to collect $N$ unbiased samples increases, causing the outer loop (gradient ascent) to stall, increasing regret.

### Mechanism 2: Zero-Order Gradient Ascent with Bisection Search
- **Claim:** The system can optimize a concave profit function without explicit gradient access by iteratively estimating arrival rates via price perturbations and bisection.
- **Mechanism:** The algorithm performs projected gradient ascent on a "fluid" optimization problem. To estimate the gradient at iteration $k$, it perturbs the current solution $x(k)$ in random directions $\pm \delta u$. It then uses a bisection search to find the prices that achieve the target arrival rates corresponding to these perturbed points, estimates the profit, and computes the gradient.
- **Core assumption:** The profit function $\lambda_i F_i(\lambda_i) - \mu_j G_j(\mu_j)$ is concave (Assumption 2), ensuring gradient ascent converges to a global optimum.
- **Evidence anchors:**
  - [Section 4.1] Describes the "Two-Point Zero-Order Method" where gradient $\hat{g}(k)$ is estimated using $f(x(k)+\delta u(k)) - f(x(k)-\delta u(k))$.
  - [Section 4.2] Details the Bisection Search used to map arrival rates back to prices when functions are unknown.
  - [Corpus] *Near-Optimal Regret Using Policy Optimization in Online MDPs* also leverages gradient methods in unknown environments but typically assumes state observability rather than the indirect rate-estimation required here.
- **Break condition:** If the estimation error $\epsilon$ of the bisection search is not small relative to the perturbation $\delta$ and step size $\eta$, the gradient direction becomes too noisy, preventing convergence.

### Mechanism 3: Selective Sample Discarding
- **Claim:** Discarding samples collected during queue-control periods is necessary to prevent bias in the demand/supply curve estimation, which would otherwise lead to suboptimal regret bounds.
- **Mechanism:** While the probabilistic policy applies the "drift price" $p(t)+\alpha$ half the time to control queues, these arrival samples are "biased" (they reflect the rate at a higher price, not the target rate). The algorithm only aggregates samples collected when the "learning price" $p(t)$ is active to update the empirical arrival rate estimates used in the bisection search.
- **Core assumption:** Arrivals are Bernoulli, and samples are i.i.d. conditioned on the price, allowing Wald's equation to bound the waiting time for $N$ clean samples.
- **Evidence anchors:**
  - [Section 4.3] "For each queue... we keep the arrival sample only when the original price is used, discarding the sample when the price is adjusted."
  - [Appendix L.4.1] Proves the bound on the number of time slots required using Wald's lemma.
  - [Corpus] Standard online learning papers (e.g., *Improved learning rates in multi-unit uniform price auctions*) typically use all samples; this selective discarding is specific to the queue-stability constraint.
- **Break condition:** If the system attempts to use all samples (including biased ones) to speed up learning, the estimation error of the fluid solution scales with $\Theta(\alpha)$, degrading the regret performance to the suboptimal levels of prior work.

## Foundational Learning

- **Concept: Lyapunov Drift Analysis**
  - **Why needed here:** This is the standard tool for proving queue stability. You must understand how to define a quadratic Lyapunov function $L(t) = \sum Q_i(t)^2$ and show that the expected drift $\mathbb{E}[L(t+1) - L(t) | Q(t)]$ is negative when queues are large.
  - **Quick check question:** Can you explain why the probabilistic two-price policy ensures negative drift even when the "learning price" might theoretically increase queue length?

- **Concept: Zero-Order (Bandit) Optimization**
  - **Why needed here:** Since the gradient of the profit function depends on unknown demand curves, the system estimates gradients using finite differences (function value differences) at random perturbed points.
  - **Quick check question:** How does the variance of the gradient estimate scale with the perturbation size $\delta$ and the estimation accuracy $\epsilon$?

- **Concept: Fluid Approximation in Queueing**
  - **Why needed here:** The "optimal" solution is derived from a deterministic fluid model (assuming infinite time/population). The algorithm attempts to track this fluid optimum while managing stochastic deviations (queues).
  - **Quick check question:** What is the physical interpretation of the "balance equations" (Eq 2) in the fluid optimization problem?

## Architecture Onboarding

- **Component map:** State Monitor -> Pricing Engine (Outer Loop Gradient Step, Inner Loop Bisection, Actuator Probabilistic Policy) -> Matching Engine

- **Critical path:** The **Bisection Search** is the bottleneck. To execute one gradient step, the system must run $M$ rounds of bisection, each requiring $N$ "clean" samples. Since $50\%$ of samples are discarded (biased), the system actually waits for roughly $2N$ arrivals per queue per bisection step.

- **Design tradeoffs:**
  - **Parameter $\gamma$ (Tradeoff Hyperparameter):** Controls the Pareto frontier.
    - Small $\gamma \to 0$: Low average queue length ($\tilde{O}(T^{\gamma/2}) \approx \text{const}$), but high regret ($\tilde{O}(T^{1-\gamma}) \approx T$).
    - Large $\gamma \to 1/6$: Lower regret ($\tilde{O}(T^{5/6})$), but larger queues.
  - **Step Size $\eta$:** Must be tuned relative to gradient variance.
  - **Threshold $q_{th}$:** Sets the "panic button" for maximum queue length. If set too low, the system spends too much time in "rejection" mode (max price), hurting profit.

- **Failure signatures:**
  1. **Queue Explosion:** If $\alpha$ is too small (weak drift) or arrival rates drift upwards, queues hit $q_{th}$ frequently, causing the algorithm to default to rejection prices and zero profit.
  2. **Stalled Learning:** If $\epsilon$ (bisection accuracy) is too tight, the system spends excessive time in the inner loop, never updating the gradient direction.
  3. **Oscillation:** If step size $\eta$ is too large, target arrival rates oscillate wildly, preventing the bisection search from converging on stable prices.

- **First 3 experiments:**
  1. **Single-Link Baseline:** Implement the system with one customer type and one server type ($I=J=1$). Verify the theoretical tradeoff by varying $\gamma \in (0, 1/6]$ and plotting Regret vs. Avg Queue Length on a log-log scale.
  2. **Stress Test (Matching Complexity):** Test a dense bipartite graph ($I=3, J=3$) to ensure the MaxWeight matcher and the gradient projection onto the feasible set scale correctly.
  3. **Ablation on Probabilistic Component:** Compare the "Probabilistic Two-Price" policy against a deterministic "Threshold" policy (like prior work). Measure the degradation in average queue length to quantify the improvement claimed in the paper.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the fundamental lower bound on regret for this two-sided learning setting, and can the gap between $\sqrt{T}$ and $T^{5/6}$ be closed?
  - **Basis in paper:** [explicit] The authors state, "we are not sure what the best possible regret is (somewhere between $\sqrt{T}$ and $T^{5/6}$), and it is an interesting future direction."
  - **Why unresolved:** The algorithm requires a line search to set arrival rates (since prices control rates indirectly), causing extra regret compared to standard bandit lower bounds; it is unclear if this is an inherent limitation of the problem structure.
  - **What evidence would resolve it:** A proof of a lower bound higher than $\sqrt{T}$ for this specific model, or a new algorithm achieving regret strictly better than $\tilde{O}(T^{5/6})$.

- **Open Question 2:** Can the near-optimal regret-queue length trade-off be achieved for the full range of $\gamma \in (0, 1/2]$ rather than the restricted range $(0, 1/6]$?
  - **Basis in paper:** [explicit] The authors identify the "restricted range of $\gamma \in (0, 1/6]$" as a limitation and state that "it is an interesting future direction" to determine if this range can be extended.
  - **Why unresolved:** The current learning error accumulates in a way that prevents regret from improving beyond $\tilde{O}(T^{5/6})$ as the allowable queue length increases (i.e., as $\gamma$ increases beyond $1/6$).
  - **What evidence would resolve it:** A modified algorithm or analysis that maintains the $1-\gamma$ vs $\gamma/2$ trade-off slope for values of $\gamma$ up to $1/2$.

- **Open Question 3:** Can the dependence of the regret and queue length bounds on the number of customer and server types ($I$ and $J$) be improved?
  - **Basis in paper:** [explicit] The authors note that the regret scales as $\tilde{O}(I^4 J^4 (I+J) T^{1-\gamma})$ and explicitly state, "We believe the dependencies on I and J can be improved."
  - **Why unresolved:** The current analysis focuses primarily on characterizing the correct dependence on the time horizon $T$, treating the network structure dependencies loosely.
  - **What evidence would resolve it:** A tightened theoretical analysis showing linear or logarithmic dependence on $I$ and $J$, or a modified algorithm with reduced sample complexity per queue type.

## Limitations

- **Multi-dimensional Generalization**: The paper's analysis hinges on specific properties of the single-queue case (bounded derivative of demand function) to guarantee drift. Extending this probabilistic two-price policy to multiple queues with complex matching constraints adds significant technical complexity not fully detailed in the main text.

- **Implementation Complexity**: The algorithm requires careful coordination between three components (gradient ascent, bisection search, and the probabilistic policy). The timing of these updates and the precise implementation of the projection oracle are crucial for the theoretical guarantees but are not fully specified.

- **Initial Condition Sensitivity**: The algorithm's performance is highly dependent on the initial feasible point $x(1)$ and the initial price bounds. If these are not chosen correctly, the system can become unstable from the outset.

## Confidence

- **High Confidence**: The core mechanism of the probabilistic two-price policy (Section 4.3) for decoupling learning and stability is well-defined and its necessity for avoiding biased gradient estimates is clearly articulated. The proof sketch for the single-queue regret bound (Appendix L.3) is coherent.

- **Medium Confidence**: The extension to the multi-queue case (Section 4.4) and the integration with the MaxWeight matching algorithm is logically sound but relies on unstated details in the projection and matching steps.

- **Low Confidence**: The precise parameters for the time-varying schedule (Appendix H) and their impact on the final tradeoff are not empirically validated within the paper, leaving their practical tuning an open question.

## Next Checks

1. **Bias Verification**: Implement a single-queue simulation and instrument the code to verify that the sample counter $n_{c,i}$ (Line 7, Algorithm 3) only increments when the *target* price is used, not the perturbed price. Compare the estimated gradient with and without this selective discarding to confirm it is necessary to avoid bias.

2. **Projection Oracle Implementation**: Implement the projection step required by Algorithm 1 onto the shrunk feasible set $D_1$ (Eq 15). Test this projection on a variety of points to ensure it is computationally efficient and correctly maintains the feasibility of the arrival rate vectors used in the gradient ascent.

3. **Single-Link Tradeoff Experiment**: Implement the single-link baseline ($I=J=1$) as specified. Run the system for $T=10^6$ time slots with the time-varying schedule (Appendix H). Vary the tradeoff parameter $\gamma \in (0, 1/6]$ and plot both the regret and the average queue length on a log-log scale to empirically verify the $\tilde{O}(T^{1-\gamma})$ vs. $\tilde{O}(T^{\gamma/2})$ Pareto frontier claimed in the paper.