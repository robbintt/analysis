---
ver: rpa2
title: Investigating AI in Peer Support via Multi-Module System-Driven Embodied Conversational
  Agents
arxiv_id: '2511.22269'
source_url: https://arxiv.org/abs/2511.22269
tags:
- mental
- support
- health
- system
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  drive embodied conversational agents for peer-based mental health support, specifically
  targeting young adults. Addressing the gap between rigid, scripted existing systems
  and the need for empathetic, context-aware support, the authors developed EmoCBT,
  a multi-module LLM-based system integrating Cognitive Behavioral Therapy (CBT) principles.
---

# Investigating AI in Peer Support via Multi-Module System-Driven Embodied Conversational Agents

## Quick Facts
- arXiv ID: 2511.22269
- Source URL: https://arxiv.org/abs/2511.22269
- Authors: Ruoyu Wen; Xiaoli Wu; Kunal Gupta; Simon Hoermann; Mark Billinghurst; Alaeddin Nassani; Dwain Allan; Thammathip Piumsomboon
- Reference count: 40
- Result: Multi-module LLM-based system with CBT techniques shows promise for peer mental health support with human oversight

## Executive Summary
This paper develops EmoCBT, a multi-module LLM-driven embodied conversational agent for peer-based mental health support targeting young adults. The system integrates emotional detection, CBT-structured dialogue, and crisis intervention with human oversight. A formative study with psychology practitioners identified design requirements, leading to a system that routes conversations based on sensitivity levels (Green/Yellow/Red). In user testing (N=10), practitioners found EmoCBT helpful for structured guidance but noted concerns about perceived rigidity and empathy. Quantitative ratings showed slight advantages for EmoCBT over generic conditions in trust, quality, and effectiveness.

## Method Summary
The study used a multi-module architecture with GPT-4o at its core, implementing conditional routing based on sensitivity classification. The Detection Module classified inputs into Green (casual conversation), Yellow (CBT-structured probing), or Red (crisis intervention) tiers using counselor-defined guidelines. Each tier triggered specific response modules, with a Memory Module maintaining conversation history and a Mediator Interface enabling human review. The study employed a within-subjects design with psychology practitioners role-playing as stressed students, comparing Generic vs. EmoCBT and Embodied vs. Voice-only conditions. Interactions lasted 5-7 minutes, followed by quantitative ratings and qualitative interviews.

## Key Results
- EmoCBT slightly outperformed Generic in trust (7.45 vs. 7.1), quality (7.3 vs. 7.2), and effectiveness (7.8 vs. 7.55) on 0-10 scales
- Practitioners valued EmoCBT's structured guidance but found it sometimes rigid and overly probing
- Younger participants preferred embodied agents while older participants found them distracting or mismatched
- Human oversight was frequently needed for high-stress content, with outputs often edited or rewritten

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graduated emotional triage enables context-appropriate response selection, improving perceived trust and quality.
- Mechanism: A detection module classifies user input into three sensitivity tiers (Green/Yellow/Red) using an LLM prompted with counselor-developed guidelines. Each tier triggers a distinct response pathway: casual conversation for Green, CBT-structured probing for Yellow, crisis resources for Red. This routing reduces generic or mismatched responses that eroded trust in prior formative testing.
- Core assumption: The classification prompt accurately captures clinically meaningful distinctions; misclassification at tier boundaries may route users to inappropriate pathways.
- Evidence anchors:
  - [abstract] "modules for emotional detection, response generation, deep conversation using CBT techniques, and crisis intervention"
  - [section] "Distinct strategies are applied based on the sensitivity category... Green: casual conversation; Yellow: further inquiry; Red: external resources"
  - [corpus] Corpus shows moderate support for multi-module architectures in mental health (avg FMR=0.443); no direct replication of triage effectiveness.
- Break condition: Tier misclassification propagates through downstream modules; edge-case inputs (sarcasm, cultural expressions) may bypass detection thresholds.

### Mechanism 2
- Claim: CBT-informed dialogue structure supports reflective emotional processing better than open-ended generation alone.
- Mechanism: When Yellow-level distress is detected, the Deep Conversation Module selects CBT techniques (e.g., cognitive reappraisal, reflective prompts) to guide users through emotional articulation. The system tracks conversation history via a memory module to maintain coherence and reinforce patterns.
- Core assumption: Users benefit from structured guidance during distress; overly directive responses may feel robotic or intrusive, as noted by participants.
- Evidence anchors:
  - [abstract] "Practitioners perceived benefits in structure and emotional guidance but noted concerns about perceived rigidity and empathy"
  - [section] "It helped me expand on my thoughts without feeling lost" (P6); "It kept asking about my emotions even after I already answered" (P3)
  - [corpus] "Cloning the Self for Mental Well-Being" (FMR=0.527) supports structured self-reflection; corpus lacks direct CBT-module comparisons.
- Break condition: Probing intensity exceeds user tolerance; users seeking companionship receive question-heavy responses instead.

### Mechanism 3
- Claim: Human-in-the-loop oversight mitigates risk for high-stakes inputs while preserving system autonomy for routine interactions.
- Mechanism: A mediator interface allows practitioners to review, approve, or edit system-generated responses before delivery. Red-tier inputs trigger resource provision rather than continued dialogue. This architecture keeps the system within peer-support boundaries and maintains accountability.
- Core assumption: Human oversight can scale sufficiently; delayed review does not undermine conversational flow or user trust.
- Evidence anchors:
  - [abstract] "LLMs can enhance peer support when combined with structured protocols and human oversight"
  - [section] "For high-stress content... outputs were frequently edited or rewritten" (formative study); "human contact remains essential"
  - [corpus] "Supporting Construction Worker Well-Being" (FMR=0.553) uses multi-agent oversight; corpus evidence for scalable HITL in mental health is limited.
- Break condition: Volume of interactions exceeds human review capacity; latency introduced by review disrupts perceived responsiveness.

## Foundational Learning

- Concept: **Cognitive Behavioral Therapy (CBT) fundamentals**
  - Why needed here: EmoCBT's Deep Conversation Module applies CBT techniques (cognitive reappraisal, reflective questioning). Understanding these principles helps engineers recognize when the system is operating correctly vs. generating irrelevant or overly prescriptive content.
  - Quick check question: Can you distinguish between cognitive reappraisal (reframing) and prescriptive advice-giving in a sample dialogue?

- Concept: **Peer support boundaries vs. clinical intervention**
  - Why needed here: The system is designed for non-clinical peer support, with clear escalation to professional resources. Engineers must understand where the system's scope ends to avoid designing features that inadvertently provide clinical advice.
  - Quick check question: If a user mentions self-harm ideation, should the system (a) continue CBT dialogue, (b) provide crisis resources, or (c) alert a human moderator?

- Concept: **LLM limitations in sensitive contexts**
  - Why needed here: The paper documents risks of generic, robotic, or inappropriate responses. Engineers must anticipate hallucination, cultural mismatch, and emotional insensitivity even with structured prompting.
  - Quick check question: Name two failure modes specific to LLMs in mental health contexts that prompt engineering alone cannot fully eliminate.

## Architecture Onboarding

- Component map:
  Detection Module -> Route to module based on sensitivity tier (Green/Yellow/Red) -> Empathetic Response Module (Green) OR Emotion Recognition Module + Deep Conversation Module (Yellow) OR Crisis Intervention Module (Red) -> Mediator Interface (optional review) -> Deliver to user -> Update Memory Module

- Critical path:
  Input → Detection Module (classify tier) → Route to appropriate module → Generate response → Mediator review (if configured) → Deliver to user → Update Memory Module

- Design tradeoffs:
  - **Structure vs. flexibility**: EmoCBT provides coherent guidance but felt rigid to some users; Generic was relaxed but less directive.
  - **Embodiment vs. voice-only**: Younger users valued non-verbal cues; older users found them distracting or mismatched. Voice-only reduced perceived pressure.
  - **Automation vs. oversight**: Full autonomy improves scalability; human review improves safety but introduces latency.

- Failure signatures:
  - Over-probing: System continues CBT questioning after user has fully expressed themselves (P3's complaint).
  - Tier misclassification: Sarcasm or cultural expressions trigger inappropriate escalation.
  - Generic escalation: Red-tier responses feel templated or impersonal, reducing trust at critical moments.

- First 3 experiments:
  1. **Tier threshold calibration**: Run a held-out dataset of labeled inputs through the Detection Module; measure precision/recall at each tier boundary; adjust prompt guidelines to reduce Yellow↔Green confusion.
  2. **Probing intensity A/B test**: Compare user ratings for two Deep Conversation variants—one with fixed question limits, one with adaptive probing based on response length—on perceived empathy and helpfulness.
  3. **Embodiment personalization pilot**: Allow users to select avatar appearance and voice; measure whether customization reduces discomfort reported by older participants and improves trust scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does sustained, repeated use of LLM-driven peer support agents affect user trust, emotional regulation outcomes, and the emergence of risks such as hallucinations?
- Basis in paper: [explicit] The authors state: "The short interaction duration also prevented assessment of longitudinal trust-building and support dynamics, which usually emerge over extended use... the risk of such issues [hallucinations] may grow with longer-term interactions."
- Why unresolved: The study limited interactions to 5-7 minutes per condition, and no longitudinal data was collected. Trust dynamics and safety risks manifest differently over extended use.
- What evidence would resolve it: Longitudinal deployment studies tracking real users over weeks or months, measuring trust trajectories, emotional regulation outcomes, and incident rates of inappropriate responses.

### Open Question 2
- Question: Can integrating multimodal AI analysis of non-verbal cues (facial expressions, posture, voice tone) improve emotional detection accuracy and perceived empathy compared to text/voice-only systems?
- Basis in paper: [explicit] "Integrating multimodal AI modules that analyse both verbal and non-verbal cues could improve EmoCBT's ability to interpret users' emotional states more accurately and respond with greater empathy."
- Why unresolved: Current system relies solely on verbal input, while practitioners noted that professional assessments depend heavily on non-verbal cues that may contradict spoken words.
- What evidence would resolve it: Comparative user studies between text-only, voice-only, and multimodal system variants measuring emotional detection accuracy and user-reported empathy scores.

### Open Question 3
- Question: How do cultural norms, language proficiency, and age influence the perceived effectiveness and acceptance of embodied vs. voice-only peer support agents?
- Basis in paper: [explicit] "Cultural norms, language proficiency, and individual communication styles may influence how AI responses are perceived, and future work will explore these factors in more diverse settings." Also, older participants reported discomfort with embodied agents while younger participants found them trust-enhancing.
- Why unresolved: The sample was limited to English-speaking practitioners in one context, and age/cultural factors were not systematically examined despite observed differences.
- What evidence would resolve it: Cross-cultural, multi-age group studies with diverse samples comparing acceptance ratings and interaction preferences across agent modalities.

### Open Question 4
- Question: What is the optimal balance between structured protocol-driven responses and user-led open conversation for different levels of emotional distress?
- Basis in paper: [inferred] Participants found EmoCBT helpful for reflection but sometimes "overwhelming or fatiguing" due to "fast-paced, question-heavy responses," while the Generic model felt "less formal" and helped users "open up." The authors suggest "a decision module that dynamically adjusts conversational styles based on the user's emotional state."
- Why unresolved: The study revealed trade-offs between structure (guidance, depth) and warmth (empathy, companionship), but did not test adaptive approaches.
- What evidence would resolve it: A/B testing of static vs. adaptive systems that modulate structure based on detected distress level, measuring user satisfaction, perceived empathy, and emotional outcomes.

## Limitations
- Small sample size (N=10 practitioners) and artificial role-play setup may not capture authentic user behavior
- Detection module classification boundaries not fully specified, creating uncertainty about misclassification rates
- Human-in-the-loop oversight mechanism lacks scalability validation under high-volume conditions
- Cultural and demographic factors influencing embodiment preferences were noted but not systematically evaluated

## Confidence
- Multi-module architecture effectiveness: **Medium** - Supported by qualitative feedback but limited quantitative differentiation between EmoCBT and control conditions
- CBT-informed structure benefits: **Medium** - Mixed practitioner feedback shows both benefits and rigidity concerns
- Human oversight necessity: **High** - Consistently validated across formative and user studies
- Tier classification accuracy: **Low** - Classification criteria and performance metrics not specified

## Next Checks
1. Conduct a larger-scale deployment (N≥50) with actual stressed young adult users to validate practitioner findings and measure real-world effectiveness differences between EmoCBT and generic systems
2. Implement A/B testing of the Detection Module with held-out labeled datasets to quantify tier classification accuracy and optimize boundary thresholds
3. Stress-test the human-in-the-loop workflow under simulated high-volume conditions (100+ concurrent conversations) to measure latency impacts and identify scalability bottlenecks