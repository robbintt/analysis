---
ver: rpa2
title: Multi-Modal Time Series Prediction via Mixture of Modulated Experts
arxiv_id: '2601.21547'
source_url: https://arxiv.org/abs/2601.21547
tags:
- time
- series
- mome
- experts
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Expert Modulation (EM), a novel paradigm for
  multi-modal time series prediction that conditions both routing and expert computation
  in a mixture-of-experts (MoE) framework on external textual signals, replacing conventional
  token-level fusion. EM consists of two components: Expert-independent Linear Modulation
  (EiLM), which modulates expert outputs via affine transformations conditioned on
  distilled context tokens, and Router Modulation (RM), which adjusts routing scores
  based on textual signals.'
---

# Multi-Modal Time Series Prediction via Mixture of Modulated Experts

## Quick Facts
- arXiv ID: 2601.21547
- Source URL: https://arxiv.org/abs/2601.21547
- Authors: Lige Zhang; Ali Maatouk; Jialin Chen; Leandros Tassiulas; Rex Ying
- Reference count: 40
- Primary result: Up to 44% relative improvement over baselines on energy data, 38% on environmental data, and 17.5% on finance trend prediction via multi-modal time series forecasting

## Executive Summary
This paper introduces Expert Modulation (EM), a novel paradigm for multi-modal time series prediction that conditions both routing and expert computation in a mixture-of-experts (MoE) framework on external textual signals, replacing conventional token-level fusion. The framework consists of two components: Expert-independent Linear Modulation (EiLM), which modulates expert outputs via affine transformations conditioned on distilled context tokens, and Router Modulation (RM), which adjusts routing scores based on textual signals. Evaluated on MT-Bench and TimeMMD, EM consistently improves forecasting accuracy across short- and long-horizon tasks, with relative improvements over baselines of up to 44% on energy data, 38% on environmental data, and 17.5% on finance trend prediction. The method achieves faster training convergence and smaller memory footprints compared to cross-attention fusion baselines.

## Method Summary
The proposed Expert Modulation (EM) framework conditions a mixture-of-experts (MoE) model on external textual signals for multi-modal time series prediction. Text is encoded by a frozen LLM (QwenMoE A2.7B) and distilled into compact context tokens via a trainable cross-attention module. These context tokens generate two types of modulation: Router Modulation (RM) adjusts routing scores to select different experts, and Expert-independent Linear Modulation (EiLM) applies per-expert affine transformations to scale and bias expert outputs. The framework operates on patched time series tokens with sparse MoE activation (Top-K gating) and is trained with LoRA fine-tuning on attention layers while keeping the LLM frozen. The approach demonstrates superior performance compared to token-level fusion baselines while requiring less memory and converging faster.

## Key Results
- Up to 44% relative improvement on energy data, 38% on environmental data, and 17.5% on finance trend prediction compared to baselines
- Consistently outperforms cross-attention fusion baselines across short- and long-horizon tasks on MT-Bench and TimeMMD datasets
- Achieves faster training convergence and smaller memory footprint than token-level fusion approaches
- Ablation studies confirm both EiLM and RM components contribute to performance gains
- Performance benefits from moderate context token usage (3-4 tokens) and sparse expert activation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse Top-K routing in Mixture-of-Experts acts as a denoising mechanism by truncating low-energy expert contributions.
- **Mechanism:** Each expert produces a directional signal; the router assigns amplification coefficients. Top-K selection discards components with small coefficients, analogous to energy-based truncation in PCA, suppressing potentially noisy or irrelevant expert outputs.
- **Core assumption:** Low routing energy correlates with low predictive utility or high noise for the current input.
- **Evidence anchors:**
  - [abstract] "Theoretical analysis interprets sparse MoE routing as a denoising mechanism that truncates low-energy expert contributions."
  - [section 4.1] "Geometrically, this corresponds to a truncation that discards directional components with low routing energy... analogous to a truncated principal component analysis (PCA) reconstruction."
  - [corpus] Not directly addressed in corpus papers; corpus focuses on MoE for multi-scale/frequency structures (e.g., FreqMoE, MoHETS).
- **Break condition:** If discarded low-energy contributions frequently contain task-critical information (e.g., rare but important patterns), performance will degrade compared to denser routing.

### Mechanism 2
- **Claim:** Conditioning MoE routing and expert computation directly on distilled textual signals enables more effective cross-modal control than token-level fusion.
- **Mechanism:** Text is encoded by an LLM and distilled into compact context tokens via cross-attention. These tokens generate (a) a routing shift vector (RM) that adjusts which experts are selected, and (b) per-expert affine parameters (EiLM) that scale and bias expert outputs. This bypasses the need to align heterogeneous time-text tokens in a shared embedding space.
- **Core assumption:** Distilled text tokens provide meaningful conditioning signals that are relevant to the temporal dynamics, and text tokens need not be mixed with temporal tokens at the representation level.
- **Evidence anchors:**
  - [abstract] "EM consists of two components: Expert-independent Linear Modulation (EiLM), which modulates expert outputs via affine transformations conditioned on distilled context tokens, and Router Modulation (RM), which adjusts routing scores based on textual signals."
  - [section 4.2] "Contextual signals do not interact directly with temporal tokens. Instead, they enter the MoE by modulating expert behavior through per-expert affine transformations and context-conditioned routing."
  - [corpus] Related MoE time series works (e.g., MoHETS, Seg-MoE) focus on architectural heterogeneity or multi-resolution segmentation; none explicitly condition expert computation on external modalities.
- **Break condition:** If textual signals are consistently noisy, inconsistent, or irrelevant to the target domain, modulation may inject misleading biases, reducing performance below uni-modal baselines.

### Mechanism 3
- **Claim:** Sparse expert activation and a moderate number of distilled context tokens balance expressivity and generalization in multi-modal time series forecasting.
- **Mechanism:** Sparse activation (K < E) retains only the most relevant experts per token, acting as an inductive bias. A small set of context tokens (e.g., 3–4) summarizes textual information without overfitting, providing sufficient conditioning without excessive parameterization.
- **Core assumption:** The information content in text relevant to time series prediction is compact; too many context tokens introduce redundancy or overfitting risk.
- **Evidence anchors:**
  - [section 5.5 & Figure 4(b)] "Sparse activation of experts tends to result in stronger performance... a moderate number of context tokens achieves the best results, reflecting a trade-off between model complexity and generalization capability."
  - [abstract] "Performance benefiting from moderate context token usage and sparse expert activation."
  - [corpus] Corpus works on MoE for time series (e.g., AdaMixT, Wavelet MoE) explore multi-scale expert design but do not analyze the interaction between sparsity and external modality tokens.
- **Break condition:** If the textual modality requires fine-grained, token-level alignment with temporal patches (e.g., specific event-to-timestamp matching), coarse context tokens may fail to provide sufficient conditioning.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) with Sparse Gating**
  - **Why needed here:** MoE is the architectural backbone; sparse gating selects a subset of experts per input, reducing computation and enabling specialization. Understanding routing and expert combination is essential to grasp how modulation is injected.
  - **Quick check question:** Can you describe how Top-K gating differs from dense softmax gating in an MoE layer?

- **Concept: Cross-Modal Integration Beyond Token Fusion**
  - **Why needed here:** The paper critiques token-level fusion and proposes function-level modulation. Familiarity with fusion strategies (early/late concatenation, cross-attention) highlights the novelty of conditioning expert behavior directly.
  - **Quick check question:** What are two common token-level fusion strategies, and what are their potential drawbacks when time series exhibit high heterogeneity?

- **Concept: Affine Feature Modulation (e.g., FiLM)**
  - **Why needed here:** EiLM uses affine transformations (scale and bias) to modulate expert outputs. Understanding conditioning via affine parameters helps explain how text influences the expert's computation.
  - **Quick check question:** In FiLM, how does the conditioning signal affect the feature map?

## Architecture Onboarding

- **Component map:** Text → LLM → Context Tokens → RM/EiLM Parameters → Modulated Routing & Expert Outputs → Aggregated Prediction

- **Critical path:**
  Text → LLM → Context Token Distillation → Router Modulation (routing shift vector) → Expert-Independent Linear Modulation (per-expert scale/bias) → MoE Backbone with Top-K gating → Aggregated Prediction

- **Design tradeoffs:**
  - **K (activated experts):** Lower K increases sparsity/denoising but risks under-capacity; higher K retains more signals but increases compute and may include noise
  - **Number of context tokens (m):** More tokens capture finer semantics but risk overfitting; fewer tokens may lose information
  - **RM vs. EiLM:** RM can alter expert selection (higher impact, risk of expert collapse); EiLM adjusts selected experts (fine-grained, more stable)
  - **Backbone complexity:** Linear/MLP backbones with modulation can outperform Transformer backbones without modulation, suggesting effective modulation may offset architectural simplicity

- **Failure signatures:**
  - **Expert collapse:** With RM, routing concentrates on a small subset of experts, reducing specialization. Observed in ablation studies (Figure 8)
  - **Noisy text sensitivity:** If text is inconsistent or misleading, performance may degrade minimally (case studies show graceful degradation) or, in worst cases, misguide predictions
  - **Over-modulation:** If EiLM parameters are too aggressive, they may override learned temporal patterns, hurting generalization

- **First 3 experiments:**
  1. **Component ablation:** Train MoME with EiLM only, RM only, and both disabled on a held-out validation set from MT-Bench or TimeMMD. Compare MSE/MAE and routing distributions to isolate each component's contribution
  2. **Sparsity sweep:** Vary K from 1 to E (total experts) on a short-horizon dataset (e.g., Environment from TimeMMD). Plot performance vs. K to validate the denoising hypothesis and identify optimal sparsity
  3. **Context token sensitivity:** Fix other hyperparameters and vary m ∈ {1, 2, 4, 8}. Evaluate on both short- and long-horizon tasks to confirm that moderate m yields best performance, as per the paper's observations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can temporal signals be used to modulate experts inside an LLM to enable time-series reasoning tasks?
- **Basis in paper:** [explicit] Section 6 states: "It is also possible to use temporal signals to modulate experts inside an LLM, enabling diverse tasks such as time series reasoning."
- **Why unresolved:** The current work only validates the unidirectional flow where text modulates time-series experts; the reverse architecture is conceptually proposed but not implemented or evaluated.
- **What evidence would resolve it:** Empirical evaluation of a "Time-to-Text" model where time-series embeddings modulate the FFN/expert layers of a pre-trained LLM on reasoning tasks (e.g., forecasting explanation).

### Open Question 2
- **Question:** How does the framework scale and perform when generalized to more than two modalities?
- **Basis in paper:** [explicit] Section 6 notes: "MoME can naturally be generalized to more than two modalities by equipping experts with additional modulation components and jointly conditioning on multiple auxiliary signals."
- **Why unresolved:** The paper limits experiments to the text-time-series pair (MMTSP); the interaction effects and complexity of adding a third modality (e.g., visual signals) remain untested.
- **What evidence would resolve it:** Experiments on a trimodal dataset (e.g., video, audio, text) demonstrating that the modulation mechanism remains stable and additive without causing training divergence or performance degradation.

### Open Question 3
- **Question:** How can the trade-off between Router Modulation performance gains and the disruption of natural expert specialization be optimized?
- **Basis in paper:** [inferred] Appendix F observes that Router Modulation (RM) "tends to lead to expert collapse" and disrupts the model's tendency to route similar tokens to similar experts, suggesting the need for a "nuanced trade-off."
- **Why unresolved:** While RM improves accuracy, the paper suggests it destabilizes the MoE's structural properties (specialization), implying the current mechanism may be suboptimal or require specific regularization not yet identified.
- **What evidence would resolve it:** Introduction of a regularization term or architecture modification that preserves the morphological/magnitude-based token-expert affinity while retaining the accuracy improvements of RM.

## Limitations

- The theoretical claims about sparse MoE routing as a denoising mechanism remain primarily intuitive rather than rigorously proven, lacking formal validation of the connection between routing energy and noise content
- Implementation details critical for reproducibility remain underspecified, including learning rate schedules, batch sizes, epoch counts, and LoRA hyperparameters
- The cross-modal integration mechanism's robustness to actively misleading or contradictory textual signals has not been systematically explored, representing a significant limitation for real-world deployment

## Confidence

**High Confidence:** The empirical performance improvements over baseline methods are well-supported by quantitative metrics across multiple datasets and tasks. The ablation studies demonstrating contributions from both EiLM and RM components are robust and convincing.

**Medium Confidence:** The theoretical interpretation of sparse MoE routing as denoising is plausible and geometrically intuitive, but lacks rigorous mathematical proof or extensive empirical validation that directly links routing energy to noise reduction.

**Medium Confidence:** The claim that function-level modulation (conditioning expert behavior) is more effective than token-level fusion is supported by comparative results, but the paper does not provide comprehensive ablation studies comparing different fusion strategies or exploring the full design space of cross-modal integration approaches.

## Next Checks

1. **Formalize the denoising hypothesis:** Design experiments that explicitly measure the correlation between routing energy and prediction error contribution. For example, systematically vary K and measure how much performance degrades when low-energy experts are included versus excluded. Additionally, analyze whether routing energy correlates with temporal frequency content or volatility measures that might indicate noise.

2. **Stress-test cross-modal robustness:** Create controlled experiments with progressively more misleading or contradictory textual signals. Generate synthetic news reports that conflict with actual time series patterns (e.g., bullish news during declining prices) and measure the model's sensitivity compared to uni-modal baselines. This would quantify the robustness claims and identify failure modes.

3. **Comprehensive ablation of fusion strategies:** Implement and compare multiple cross-modal integration approaches including early fusion (concatenated embeddings), late fusion (separate predictions combined), cross-attention fusion, and the proposed function-level modulation. Apply all strategies to the same MoE backbone with identical hyperparameter optimization to isolate the impact of the fusion mechanism itself.