---
ver: rpa2
title: LLM Pretraining with Continuous Concepts
arxiv_id: '2502.08524'
source_url: https://arxiv.org/abs/2502.08524
tags:
- concept
- cocomix
- concepts
- training
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Continuous Concept Mixing (CoCoMix) improves LLM pretraining by\
  \ augmenting next-token prediction with continuous concepts extracted from a pretrained\
  \ sparse autoencoder. The method predicts influential concepts using attribution\
  \ scores, compresses them into a compact vector, and interleaves them with token\
  \ representations to guide the model\u2019s reasoning."
---

# LLM Pretraining with Continuous Concepts

## Quick Facts
- **arXiv ID**: 2502.08524
- **Source URL**: https://arxiv.org/abs/2502.08524
- **Reference count**: 40
- **Primary result**: CoCoMix improves LLM pretraining by predicting continuous concepts from pretrained SAEs, achieving up to 21.5% better sample efficiency than standard next-token prediction

## Executive Summary
CoCoMix introduces a novel pretraining method that augments next-token prediction with continuous concept learning. The approach extracts concepts from a pretrained sparse autoencoder, predicts influential concepts using attribution scores, compresses them into a compact vector, and interleaves them with token representations. Experiments on models ranging from 69M to 1.38B parameters demonstrate consistent improvements over standard pretraining and knowledge distillation, with gains in sample efficiency, interpretability, and steerability.

## Method Summary
The method combines next-token prediction with continuous concept prediction using a pretrained sparse autoencoder. For each token, concepts are extracted from teacher model hidden states, attribution scores identify the most influential concepts, and these are predicted by the student model. The predicted concepts are compressed and interleaved with token embeddings at an intermediate layer, allowing the model to attend to both information sources independently. Training combines standard language modeling loss with concept prediction loss weighted by λ=0.1.

## Key Results
- 21.5% improvement in sample efficiency compared to standard next-token prediction
- Consistent performance gains across model scales (69M, 386M, 1.38B parameters)
- Enhanced interpretability through direct inspection and manipulation of predicted concepts
- Outperforms knowledge distillation while maintaining better interpretability

## Why This Works (Mechanism)

### Mechanism 1: Attribution-Guided Concept Selection
Selecting concepts via attribution scores (activation × gradient) rather than raw activation values improves sample efficiency by filtering out high-activation concepts that are semantically present but irrelevant to the specific prediction task, reducing label noise during concept prediction training.

### Mechanism 2: Interleaved Concept Mixing vs. Additive Intervention
Interleaving compressed concept vectors as separate sequence positions outperforms directly adding them to hidden states by allowing the transformer to attend to concepts independently, enabling learned attention patterns to determine concept-token relationships.

### Mechanism 3: SAE-Mediated Decomposition Over Direct Hidden State Prediction
Predicting sparse SAE concepts outperforms predicting full hidden states directly by isolating interpretable, semantically meaningful dimensions while suppressing noise, making the learning task more focused and reducing conflicting signals.

## Foundational Learning

- **Concept**: **Sparse Autoencoders (TopK SAE)**
  - Why needed here: SAEs decompose dense hidden states into sparse, interpretable concept dimensions. CoCoMix relies on pre-extracted concepts as supervision targets.
  - Quick check question: Given a 768-dim hidden state and SAE with 32K concept dimensions and K=32, what shape is the encoder output before/after TopK?

- **Concept**: **Gradient-Based Attribution**
  - Why needed here: Attribution scores identify which concepts matter for each specific next-token prediction, enabling targeted concept selection rather than using all active concepts.
  - Quick check question: Why multiply pre-activation (c_pre) rather than post-TopK activation when computing attribution? What information would be lost otherwise?

- **Concept**: **Weak-to-Strong Supervision**
  - Why needed here: CoCoMix extracts concepts from a 124M model to train larger models (386M, 1.38B). Understanding this transfer helps set expectations for teacher-student configurations.
  - Quick check question: Why might concept-based transfer outperform knowledge distillation when the student exceeds teacher capacity?

## Architecture Onboarding

- **Component map**: Input tokens → Embedding → Transformer layers (early) → [Layer L_concept] → Concept prediction head → TopK → Compression → Interleave → Remaining Transformer layers → LM head

- **Critical path**:
  1. Pretrain or obtain SAE on teacher model's hidden states
  2. For each training batch: compute teacher hidden states → SAE encode → attribution → select top-K_attr concept indices as labels
  3. Student forward pass: predict concepts at designated layer, compress, interleave
  4. Backprop through combined NTP + concept prediction loss

- **Design tradeoffs**:
  - **Layer position for concept prediction**: Earlier layers = faster feedback but less abstract representations; paper uses layer 4-6 depending on model size
  - **K_attr (selected concepts per position)**: Higher K = more supervision signal but more label noise; paper uses K=4
  - **Compression dimension**: Must match hidden dimension for interleaving; linear projection may lose concept specificity

- **Failure signatures**:
  - Concept prediction accuracy plateaus low: SAE concepts may not align with student's representation space
  - No improvement over NTP despite low concept loss: Compression layer weights near-zero—may need to reduce λ or check concept quality
  - Weak-to-strong transfer fails: Student outpaces teacher early; consider using larger teacher or filtering low-attribution concepts more aggressively
  - Training instability: Loss λ scaling too aggressive; paper uses λ=0.1

- **First 3 experiments**:
  1. **Sanity check**: Train 69M model with CoCoMix on 1B tokens. Verify concept prediction accuracy > random (should be ~25%+ for Top-4 of 32K). Compare validation PPL vs. NTP baseline.
  2. **Ablation**: Remove interleaving (concept prediction loss only). Confirm performance drops to validate both components are necessary.
  3. **Steerability test**: After training, identify a concept via SAE interpretation (e.g., "website address" from Figure 5). Amplify its logit during generation and verify output changes as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can continuous concepts be learned end-to-end during pretraining without relying on a fixed, pretrained Sparse Autoencoder (SAE) for distillation?
- **Basis in paper:** The conclusion states future work could explore "learning continuous concepts during pretraining without the need for distillation."
- **Why unresolved:** The current framework relies on a two-step process: extracting concepts from a frozen teacher model and then training the student. It is unknown if the student can learn an optimal concept space dynamically on its own.
- **What evidence would resolve it:** A study implementing a trainable SAE layer or concept head within the LLM that learns jointly with the next-token prediction objective, compared against the current fixed-teacher approach.

### Open Question 2
- **Question:** How does modifying concept selection criteria to remove undesirable or unsafe concepts affect the trade-off between model performance and bias?
- **Basis in paper:** Section 3.3 suggests exploring "selection criteria... removing undesirable concepts to reduce bias, e.g., selectively removing unsafe concepts for safe LLM pretraining" as a future direction.
- **Why unresolved:** The current research focuses exclusively on performance-based selection (attribution scores) and does not evaluate methods for filtering harmful or biased concepts.
- **What evidence would resolve it:** Experiments applying negative constraints to the attribution-based selection process to suppress specific safety-related concepts, followed by evaluation on both downstream task accuracy and safety benchmarks.

### Open Question 3
- **Question:** Does the reliance on a fixed SAE from a smaller teacher model limit the conceptual expression of significantly larger student models (e.g., >10B parameters)?
- **Basis in paper:** The experiments use a 124M parameter teacher to train students up to 1.38B parameters. While effective at this scale, it is methodologically unclear if a small model's concept space is sufficiently rich to supervise a much larger, more capable model without capping the student's potential.
- **Why unresolved:** The paper demonstrates "weak-to-strong" supervision but does not test the upper bounds of this dynamic where the capacity gap becomes extreme.
- **What evidence would resolve it:** A scaling law analysis comparing the performance delta of CoCoMix against standard NTP as the parameter count of the student increases exponentially relative to the fixed teacher.

## Limitations
- Attribution score reliability may break when gradients are saturated or concepts have low gradient magnitude but high causal importance
- Computational overhead from interleaving doubles sequence length for subsequent transformer layers
- Weak-to-strong transfer efficacy unexplored for extreme capacity gaps between teacher and student models

## Confidence

- **High Confidence**: The core architecture (attribution-guided concept selection + interleaving) is technically sound and the empirical results show consistent improvements across model scales and tasks. The ablation studies validate both components are necessary.

- **Medium Confidence**: The attribution mechanism provides better concept selection than alternatives, and interleaving is superior to additive mixing. While supported by experiments, these claims could benefit from more rigorous analysis of edge cases and alternative approaches.

- **Low Confidence**: The method's behavior in extreme scaling scenarios (very large students, different teacher-student ratios, alternative SAE architectures) remains unexplored. The paper doesn't address computational overhead implications for large-scale pretraining.

## Next Checks

1. **Attribution Robustness Test**: Compare CoCoMix performance using different attribution methods (Integrated Gradients, SHAP, gradient×activation) on the same dataset. Measure concept prediction accuracy and downstream performance to quantify attribution method sensitivity.

2. **SAE Quality Impact Analysis**: Systematically vary SAE hyperparameters (K value, concept dimensionality) and measure corresponding changes in CoCoMix performance. Include qualitative analysis of concept interpretability to establish correlation between concept quality and task performance.

3. **Large-Scale Computational Overhead Evaluation**: Implement CoCoMix at scale (1B+ parameter models) and measure wall-clock training time, memory consumption, and convergence rates versus standard NTP. Quantify the trade-off between performance gains and computational costs.