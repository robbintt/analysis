---
ver: rpa2
title: 'CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through
  Generative Artificial Intelligence'
arxiv_id: '2501.16557'
source_url: https://arxiv.org/abs/2501.16557
tags:
- instructions
- caring-ai
- users
- avatar
- reality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARING-AI, a system that uses generative
  AI to create contextualized AR instructions with humanoid avatars. The authors identified
  that existing AI-generated animations lack spatial and temporal context awareness
  for AR applications.
---

# CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through Generative Artificial Intelligence

## Quick Facts
- **arXiv ID:** 2501.16557
- **Source URL:** https://arxiv.org/abs/2501.16557
- **Reference count:** 40
- **Primary result:** A system that enables code-less, motion-capture-free authoring of contextualized AR instructions with humanoid avatars using generative AI

## Executive Summary
CARING-AI is a system that uses generative AI to create contextualized AR instructions with humanoid avatars. The authors identified that existing AI-generated animations lack spatial and temporal context awareness for AR applications. CARING-AI addresses this by having users provide contextual information through environmental scanning while wearing an AR headset, allowing the system to generate avatar animations that blend into specific physical environments. The system was evaluated through two user studies (N=12) showing it achieved good usability (SUS score 83.21/100), required less mental and physical demand compared to programming-by-demonstration baselines, and enabled faster instruction authoring (p=0.001).

## Method Summary
CARING-AI uses a HoloLens 2 headset to capture environmental context through SLAM trajectory tracking and RGB image capture. Object detection (YOLO-based) extracts semantic labels and 6DoF poses using MegaPose6D. User text/speech input is converted to step-by-step instructions via ChatGPT API and aligned with HumanML3D action labels. A modified Motion Diffusion Model conditions generation on user trajectory embeddings and generates humanoid motions, with temporal smoothing applied between action segments. Hand-object interactions use a specialized text-to-hand-object motion model. The system renders full-body or hand-object avatars in Unity 3D overlaid on the physical environment via MRTK.

## Key Results
- Achieved good usability with SUS score of 83.21/100
- Required less mental and physical demand compared to programming-by-demonstration baselines
- Enabled faster instruction authoring with statistically significant time savings (p=0.001)
- Supported multiple context types: asynchronous, ad-hoc, and remote scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: User navigation through environments provides contextual grounding that enables AI-generated humanoid motion to spatially and temporally blend into AR instructions.
- **Mechanism**: Users walk through the physical environment wearing an AR HMD (HoloLens 2), recording their trajectory via built-in SLAM and capturing RGB images at interaction points. Object detection (YOLO-based) extracts semantic labels and 6DoF poses (MegaPose6D), which are used to condition a modified Motion Diffusion Model (MDM). The trajectory data is injected as spatial conditioning signals into the diffusion process, shifting the generated motion frames to align with the user's recorded path.
- **Core assumption**: User walking paths accurately represent the spatial requirements of the intended instruction, and objects detected in the scanned environment correspond to those referenced in textual instructions.
- **Evidence anchors**:
  - [abstract] "By navigating in the environment, users naturally provide contextual information to generate humanoid-avatar animation as AR instructions that blend in the context spatially and temporally."
  - [section 4.3] "The user walks around from and to contexts where actions happen, and scans the entire required environment... CARING-AI records the surroundings by taking RGB images of the HMD FOV and starts recording the global trajectory of the user."
  - [corpus] Corpus evidence is weak for this specific navigation-conditioning mechanism; related work (CoCreatAR) addresses outdoor AR authoring through asymmetric collaboration but does not implement trajectory-conditioned motion generation.
- **Break condition**: If users do not physically navigate to all task-relevant locations, or if the environment is inaccessible (e.g., remote or hazardous), spatial context injection will be incomplete.

### Mechanism 2
- **Claim**: A diffusion-model-based temporal smoothing algorithm produces seamless multi-step humanoid animations by blending transition frames with a shifted sigmoid weighting function.
- **Mechanism**: Sequential motion segments generated independently by MDM exhibit discontinuities at boundaries. The system applies a temporal smoothing function f that blends two overlapping transition segments K1 and K2 of length L frames using α(t) = 1/(1+e^-(t-(L/2))). Blended frames are computed as K̃_t = α_t·K1_t + (1-α_t)·K2_t, then linearly interpolated to double the frame count for smoother playback. This enables unlimited-length motion sequences despite the model's 196-frame training limit.
- **Core assumption**: Transition segments contain sufficient frames for the sigmoid blend to produce perceptually smooth motion, and the underlying motion embeddings are compatible across segment boundaries.
- **Evidence anchors**:
  - [abstract] No direct mention; mechanism is described in technical sections.
  - [section 4.4.3] "We designed a temporal smoothing algorithm to generate an unlimited length of smooth avatar motion... The temporal smoothing function, (denoted as f), aims to mitigate the discontinuity among the transitional segments."
  - [corpus] Corpus does not provide evidence for this specific temporal smoothing approach; no comparable sigmoid-based motion blending found in neighbors.
- **Break condition**: When frame counts per action exceed model limits without proper chunking, or when transition segments are too short (L < ~10 frames) for effective blending.

### Mechanism 3
- **Claim**: Multi-scale avatar rendering (full-body vs. hand-object) adapts visualization to task complexity and user proximity, improving instruction clarity.
- **Mechanism**: The system supports two visualization scales: (1) full-body SMPL-X avatars for global navigation and body-coordinated tasks, and (2) hand-object models (hands + forearms + objects) for local manipulation tasks. Users select the scale via the AR interface ("Change Scale" button). Hand-object interactions use a specialized text-to-hand-object motion model, while full-body motions use the trajectory-conditioned MDM variant.
- **Core assumption**: Tasks can be cleanly categorized by scale, and users will correctly select the appropriate visualization for each instruction step.
- **Evidence anchors**:
  - [abstract] No direct mention; inferred from system description.
  - [section 4.5] "The available scales of the visualization are full-body avatars and hand-object avatars (i.e. only the hands, the forearms, and the objects are rendered)."
  - [section 6.2.3] "Users find it easy to switch between full-body pose and only hand (Q2: AVG= 4.08, SD = 1.00)."
  - [corpus] Corpus (ImaginateAR) mentions AI-assisted in-situ AR authoring but does not address multi-scale avatar rendering tradeoffs.
- **Break condition**: For tasks requiring both global navigation and fine manipulation in a single step, or for hand-object interactions with articulated/deformable objects not supported by the rigid-object model.

## Foundational Learning

- **Concept: Conditional Diffusion Models for Motion Generation**
  - **Why needed here**: CARING-AI's core motion generation relies on modifying MDM (Motion Diffusion Model) to accept spatial trajectory and textual conditions simultaneously. Understanding how diffusion models denoise sequential data and how conditioning signals guide generation is essential for debugging motion quality issues.
  - **Quick check question**: Can you explain how a diffusion model uses classifier-free guidance to balance between conditioned and unconditioned generation, and how CARING-AI's frame-wise conditioning (z_k per frame) differs from MDM's sequence-level conditioning?

- **Concept: 6DoF Pose Estimation and AR Object Registration**
  - **Why needed here**: The system uses MegaPose6D for estimating 6 degrees-of-freedom poses of objects from RGB images, then overlays virtual 3D models. Understanding pose estimation pipelines, coordinate system transformations between camera/world/avatar frames, and alignment error propagation is critical for the local-spatial-context generation component.
  - **Quick check question**: Given a detected object with 6DoF pose [R|t] in camera coordinates and the HMD's SLAM pose in world coordinates, what transformations are needed to place a virtual object at the correct location in the AR scene?

- **Concept: Immersive Authoring Interaction Patterns**
  - **Why needed here**: CARING-AI's workflow (scan → contextualize → generate → edit) is an example of immersive authoring where users create content through embodied interaction. Understanding prior patterns like Programming-by-Demonstration (PbD), their tradeoffs, and why certain interactions reduce cognitive load helps evaluate when to use CARING-AI vs. traditional authoring.
  - **Quick check question**: Compare CARING-AI's text-and-navigation approach to PbD's demonstration capture. What types of tasks would favor each approach, and what cognitive load dimensions (per NASA TLX) differ significantly between them?

## Architecture Onboarding

- **Component map**:
  User Input Layer -> ChatGPT API -> Step-by-step textual instructions
  Environment Scan (HoloLens 2) -> SLAM trajectory + RGB images
  Screenshot Capture -> Object Detection (YOLO) -> 6DoF Estimation (MegaPose6D)
  Text Alignment -> HumanML3D action labels
  Trajectory Conditioning -> Modified MDM (global-spatial)
  Hand-Object Model -> Text2HOI diffusion (local-spatial)
  Temporal Smoothing -> Sigmoid blend + Linear interpolation
  Full-body Avatar (SMPL-X) -> Unity 3D
  Hand-Object Avatar -> Unity 3D
  AR Overlay (MRTK) -> HoloLens 2 display

- **Critical path**: Environment Scan -> Object Detection/6DoF -> Trajectory Recording -> Motion Generation (GPU, ~36s/batch) -> Temporal Smoothing -> AR Visualization. The motion generation step is the latency bottleneck.

- **Design tradeoffs**:
  - **Generation latency vs. batch size**: 36-second batch generation supports up to 128 interactions but creates non-trivial wait time; real-time generation would require cloud offloading or model distillation.
  - **Hand-object fidelity vs. object complexity**: Current hand-object model handles only rigid objects; articulated/deformable objects fail (explicit limitation noted in Section 9.1).
  - **FOV constraints vs. avatar visibility**: HoloLens 2's 43°×29° FOV means full-body avatars moving outside FOV are invisible, potentially confusing users (noted in user study feedback P8).

- **Failure signatures**:
  - **Motion discontinuity at transitions**: Transition distance >0.1m indicates smoothing failure; check segment length L and frame overlap.
  - **Spatial misalignment**: Avatar-object distance >0.1m suggests 6DoF estimation error or coordinate transformation bug.
  - **Hand-object penetration/floating**: Rigid object model cannot handle articulated interactions; manifests as impossible grasping poses.
  - **Missing objects in scan**: Object detection false negatives cause missing virtual overlays; check lighting, occlusion, and detection confidence thresholds.

- **First 3 experiments**:
  1. **Latency profiling**: Measure end-to-end time for a 5-step instruction with varying numbers of objects and trajectory complexity. Identify whether object detection (~30ms/image), motion generation (~36s/batch), or smoothing is the bottleneck for your target use case.
  2. **Spatial accuracy validation**: Place objects at known ground-truth poses, run the full scan-to-overlay pipeline, and measure 6DoF estimation error. Test under varying lighting, occlusion levels, and object distances to characterize MegaPose6D's failure modes.
  3. **Temporal smoothing quality assessment**: Generate motion sequences with varying transition segment lengths (L = 5, 10, 20, 30 frames) and conduct a small user study (n=5) to identify the perceptual threshold for smooth vs. jerky transitions. Use transition distance metric (Equation 3) as a quantitative correlate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative AI models be adapted to handle complex interactions with non-rigid or deformable objects in AR instruction authoring?
- Basis in paper: [explicit] Section 9.1 identifies the limitation that the current system is restricted to "strictly rigid" objects and "cannot represent actions that involve changing the shape or form of an object such as tying a shoelace."
- Why unresolved: The current generative algorithms and datasets used (e.g., HumanML3D) do not capture the physics or states necessary for high-fidelity interactions with flexible materials.
- What evidence would resolve it: A system demonstration or user study showing the successful generation of physically plausible animations for tasks involving cloth, ropes, or articulated objects.

### Open Question 2
- Question: Does the CARING-AI authoring workflow generalize effectively to non-avatar AR instruction modalities, such as visual cues (arrows, lines) or spatial audio?
- Basis in paper: [explicit] Section 8.3 argues that the design space and pipeline "apply to the other modalities of AR cues and instructions," but this capability has not been implemented or tested.
- Why unresolved: The current interface and scanning method are optimized for humanoid kinematics (spatial/temporal trajectories); it is unverified if this interaction paradigm is intuitive for authoring abstract visual or audio cues.
- What evidence would resolve it: A comparative user study evaluating the efficiency and usability of the CARING-AI pipeline when adapted to generate non-avatar instructional assets.

### Open Question 3
- Question: How do industrial practitioners or non-academic experts evaluate the utility and usability of Gen-AI authoring compared to the academic participants in this study?
- Basis in paper: [explicit] Section 9.2 acknowledges that the formative study findings were "derived from academic researchers" and could be "further refined and expanded with diverse perspectives from industrial practitioners."
- Why unresolved: The reported high usability (SUS score 83.21) and low mental demand may be biased by the technical familiarity of the academic participants, potentially masking usability barriers for field technicians.
- What evidence would resolve it: A replication of the user study (N=12) with industrial workers or domain experts, specifically comparing NASA TLX scores and qualitative feedback on the "code-less" workflow.

## Limitations

- **Generative model quality and generalization**: The paper claims CARING-AI can generate high-quality, contextually appropriate humanoid animations without motion capture or manual programming. However, the evaluation focuses on usability metrics rather than the quality of the generated animations themselves.
- **Technical specification gaps**: Several critical implementation details are underspecified, including the exact architecture modifications to the diffusion model, the prompt engineering strategy for ChatGPT, and the specific trajectory embedding methodology.
- **Environmental dependency**: The system requires users to physically navigate and scan the entire environment, which may not be feasible for remote, hazardous, or inaccessible contexts.

## Confidence

**High confidence** in usability claims: The SUS score of 83.21/100 and NASA TLX comparisons are based on concrete user study data with statistical significance (p=0.001 for time-on-task).

**Medium confidence** in technical performance claims: Claims about transition distance metrics, spatial alignment accuracy, and generation latency are based on internal measurements without independent verification or comparison to baselines beyond the programming-by-demonstration study.

**Low confidence** in animation quality and contextual appropriateness: The paper provides limited evidence about whether generated animations actually convey correct instructions or whether the contextual grounding meaningfully improves over non-contextualized approaches.

## Next Checks

1. **Motion quality assessment**: Conduct a controlled study where participants attempt to follow CARING-AI instructions versus traditional text/video instructions for identical tasks. Measure completion accuracy, task completion time, and subjective understanding to validate whether the generated animations actually improve instruction comprehension.

2. **Environmental generalization testing**: Test the system across diverse environment types (minimal, cluttered, outdoor, low-light) and measure detection accuracy, pose estimation error, and motion generation quality. Quantify the minimum scanning requirements and characterize failure modes when environmental coverage is incomplete.

3. **Cross-user consistency evaluation**: Have multiple users author instructions for the same task in the same environment, then compare the generated animations for consistency in motion quality, spatial alignment, and task representation. This would validate whether the system produces reliable outputs across different users and scanning patterns.