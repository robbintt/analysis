---
ver: rpa2
title: 'Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring
  Bias in LLMs'
arxiv_id: '2511.05766'
source_url: https://arxiv.org/abs/2511.05766
tags:
- anchoring
- behavioral
- llms
- anchors
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) exhibit\
  \ anchoring bias\u2014a well-known human cognitive bias where initial numeric cues\
  \ influence judgments. To address the question of whether such biases reflect surface-level\
  \ mimicry or deeper probability shifts, the study introduces a two-pronged methodology:\
  \ (1) a log-probability-based behavioral analysis that measures systematic shifts\
  \ in the distribution of numeric outputs, and (2) an exact Shapley-value attribution\
  \ framework over structured prompt fields to quantify the anchor's influence on\
  \ these outputs."
---

# Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs

## Quick Facts
- arXiv ID: 2511.05766
- Source URL: https://arxiv.org/abs/2511.05766
- Reference count: 40
- Key outcome: LLMs exhibit anchoring bias through systematic probability reweighting, not just surface-level mimicry

## Executive Summary
This paper investigates whether large language models exhibit anchoring bias by introducing a two-pronged methodology combining log-probability-based behavioral analysis with exact Shapley-value attribution over structured prompt fields. The study tests six open-source LLMs across multiple prompt variations and anchor regimes, finding robust anchoring effects in larger models (Gemma-2B, Phi-2, Llama-2-7B) with measurable internal probability reweighting. The Anchoring Bias Sensitivity Score (ABSS) reveals that Gemma-2B and Phi-2 show the strongest bias, while smaller models display weaker or reversed effects. These findings demonstrate that anchoring bias in LLMs reflects genuine distribution shifts rather than mere surface imitation.

## Method Summary
The methodology employs a controlled experimental setup using six open-source LLMs with structured prompts containing four fields: {scene, comparative, absolute, anchor}. For each prompt, the model evaluates all 101 target percentages (0-100%) using teacher-forced log-probability scoring. Behavioral analysis measures SoftEV (expected value under the model's distribution) shifts, while exact Shapley-value attribution quantifies the anchor field's marginal contribution to log-probabilities across all 16 possible field subsets. The ABSS metric integrates both behavioral and attributional evidence with significance weighting, excluding V0 due to training contamination concerns.

## Key Results
- Gemma-2B and Phi-2 show consistent B+*** behavioral effects with SoftEV gaps of +4 to +53 points
- Exact Shapley attribution reveals +2.45 nats (×11.5 odds) for Gemma-2B anchor contribution
- GPT-Neo-125M displays negative ABSS (-0.35 nats, ×0.70), indicating opposite attributional shifts
- Larger models consistently show stronger anchoring bias sensitivity than smaller counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anchors systematically reweight the probability distribution over numeric outputs, shifting the expected value (SoftEV) toward the anchor direction.
- Mechanism: When an anchor is presented, the model adjusts log-probabilities across all candidate numeric targets (0–100%), increasing mass for values consistent with the anchor direction. This is measured via SoftEV (expected value under the model's distribution).
- Core assumption: The shift reflects internal probability reweighting rather than surface-level text mimicry.
- Evidence anchors:
  - [abstract] "log-probability-based behavioral analysis showing that anchors shift entire output distributions"
  - [section] Table 1 shows SoftEV gaps ranging from +4 to +53 points; Gemma-2B and Phi-2 show consistent B+*** across variations.
  - [corpus] Corpus papers on anchoring in LLMs confirm behavioral effects but mostly lack distribution-level analysis.
- Break condition: If SoftEV shifts disappear when controlling for semantic similarity between anchor and target, or if effects vanish under paraphrase.

### Mechanism 2
- Claim: The anchor field has measurable marginal contribution to the log-probability of target outputs, quantified via exact Shapley values.
- Mechanism: Shapley values enumerate all 2^4 subsets of prompt fields, computing the anchor's marginal contribution to log-probability for each target. Positive ΔShapley (e.g., +2.45 nats in Gemma-2B) indicates the anchor field increases target probability; negative values indicate suppression.
- Core assumption: Shapley attribution on log-probabilities captures causal influence of the anchor field on output probability, not just correlation.
- Evidence anchors:
  - [abstract] "exact Shapley-value attribution framework over structured prompt fields to quantify the anchor's influence on these outputs"
  - [section] Gemma-2B: A+*** with +2.45 nats (×11.5 odds); GPT-Neo-125M: A–*** with -0.35 nats (×0.70), showing discordance.
  - [corpus] Related work uses Shapley for token-level attribution (TokenSHAP) or semantic similarity payoffs, but not exact field-level attribution for bias measurement.
- Break condition: If Shapley values become unstable under minor prompt perturbations, or if A+ and B+ signals are consistently discordant across variations.

### Mechanism 3
- Claim: Model scale correlates with anchoring bias sensitivity; larger models show stronger and more coherent B+/A+ profiles.
- Mechanism: Assumption: Larger models integrate contextual cues (anchors) more strongly into probability estimates, manifesting as higher ABSS. Smaller models may lack the capacity for consistent context integration, leading to weaker or reversed effects.
- Core assumption: Scale enables more sophisticated context integration, which increases bias susceptibility.
- Evidence anchors:
  - [abstract] "Smaller models (GPT-2, Falcon-RW-1B, GPT-Neo-125M) displayed weaker or reversed anchoring effects"
  - [section] ABSS ranking: Gemma-2B, Phi-2, Llama-2-7B highest; GPT-Neo-125M shows negative ABSS (Figure 8).
  - [corpus: Weak/missing] Corpus does not provide systematic evidence on scale effects; most related work focuses on frontier models without size comparison.
- Break condition: If instruction tuning or RLHF modulates bias independently of scale, or if smaller models in the corpus show strong anchoring.

## Foundational Learning

- Concept: Shapley Values in Attribution
  - Why needed here: To quantify how much the anchor field contributes to the model's output probability.
  - Quick check question: How does the Shapley value ensure fair attribution among multiple contributing fields?

- Concept: Log-Probability Distribution Analysis
  - Why needed here: To detect systematic shifts in output distributions beyond surface-level text generation.
  - Quick check question: Why use log-probabilities instead of raw probabilities for measuring distribution shifts?

- Concept: Anchoring Bias in Cognitive Psychology
  - Why needed here: To understand the experimental design (Tversky & Kahneman replication) and expected directional effects.
  - Quick check question: What distinguishes anchoring bias from simple priming effects?

## Architecture Onboarding

- Component map:
  - Prompt template with 4 fields: {scene, comparative, absolute, anchor} -> Log-probability scorer -> Target set: 0–100% as strings -> SoftEV calculator -> Exact Shapley evaluator -> ABSS aggregator

- Critical path:
  1. Render prompts with high/low anchors for each variation.
  2. Score all 101 targets using sequence log-probabilities.
  3. Normalize to categorical distribution; compute SoftEV.
  4. Run exact Shapley attribution on anchor field for each target.
  5. Aggregate into per-variation and per-model ABSS scores.

- Design tradeoffs:
  - Exact Shapley vs. Monte Carlo: Exact is deterministic but limited to 4 fields; Monte Carlo scales but introduces variance.
  - Fixed targets vs. free generation: Controls variability but may miss naturalistic outputs.
  - Standard anchors vs. different anchors: Balances effect-size comparability with contamination risk.

- Failure signatures:
  - V0 showing excessively large shifts (likely training contamination, excluded from ABSS).
  - B+/A– discordance (behavioral shift without attributional support, e.g., GPT-Neo-125M).
  - Negative ABSS (attributional shifts opposite to anchoring direction).

- First 3 experiments:
  1. Replicate V0 on a new model to diagnose training-data contamination.
  2. Test prompt paraphrases to verify Shapley attribution stability under semantic-preserving changes.
  3. Compare standard vs. different anchor regimes to assess whether effects generalize beyond specific anchor pairs.

## Open Questions the Paper Calls Out

None

## Limitations

- Prompt Design Ambiguity: The paper uses 5 variations (V1-V5) in addition to V0, but only partial phrasing for these variations is shown in Appendix A.
- Model Size vs. Bias Relationship: While the paper claims larger models show stronger anchoring bias, the corpus analysis reveals no systematic evidence on scale effects.
- Shapley Attribution Validity: The exact Shapley framework assumes log-probability differences reflect causal influence of prompt fields, but may capture correlation rather than causation.

## Confidence

**High Confidence**: The existence of anchoring bias in Gemma-2B, Phi-2, and Llama-2-7B is well-supported by both behavioral and attributional evidence across multiple variations.

**Medium Confidence**: The claim that smaller models display weaker or reversed anchoring effects is supported by the data but may reflect training data differences rather than inherent model capacity limitations.

**Low Confidence**: The assertion that anchoring bias reflects "deeper probability shifts rather than surface-level mimicry" is plausible given the log-probability analysis, but cannot be definitively proven without ablation studies on internal representations.

## Next Checks

1. **Prompt Paraphrase Stability Test**: Create semantically equivalent paraphrases of the exact prompt templates and run the full Shapley attribution pipeline to verify attributional effects remain stable under semantic-preserving changes.

2. **Different Anchor Regime Generalization**: Conduct systematic analysis comparing standard vs. different anchors across all models to measure whether B+/A+ discordance patterns are consistent or model-specific.

3. **Internal Representation Ablation**: For top-performing models (Gemma-2B, Phi-2), conduct layer-wise ablation studies by progressively masking attention heads or intermediate representations during inference to determine whether anchoring bias emerges early or late in the forward pass.