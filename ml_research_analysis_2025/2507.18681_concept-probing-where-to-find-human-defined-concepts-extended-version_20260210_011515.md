---
ver: rpa2
title: 'Concept Probing: Where to Find Human-Defined Concepts (Extended Version)'
arxiv_id: '2507.18681'
source_url: https://arxiv.org/abs/2507.18681
tags:
- concept
- concepts
- representations
- layer
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an information-theoretic method to automatically\
  \ select the most suitable layer in a neural network for probing a given human-defined\
  \ concept. The approach evaluates each layer\u2019s representations based on two\
  \ characteristics: informativeness (mutual information with the concept) and regularity\
  \ (ease of prediction via a simple classifier)."
---

# Concept Probing: Where to Find Human-Defined Concepts (Extended Version)

## Quick Facts
- **arXiv ID:** 2507.18681
- **Source URL:** https://arxiv.org/abs/2507.18681
- **Reference count:** 19
- **Primary result:** Proposes an information-theoretic method to automatically select the optimal neural network layer for probing a given human-defined concept.

## Executive Summary
This paper addresses the challenge of selecting the most suitable layer in a neural network for probing human-defined concepts. The authors propose a method that evaluates each layer's representations based on two characteristics: informativeness (mutual information with the concept) and regularity (ease of prediction via a simple classifier). By combining these measures, the method identifies layers where concept probes can be trained more accurately. Experiments on six models across four datasets show the proposed method consistently outperforms baselines, achieving an average probe accuracy of 90.1% versus 77.6% for a layer-wise average baseline, and approaching the performance of an oracle that knows the best layer. The results demonstrate that considering both information and regularity enables efficient and effective concept probing.

## Method Summary
The method characterizes each layer's representations using informativeness (mutual information with the concept) and regularity (linear separability via logistic regression accuracy). For each layer l, it computes the uncertainty coefficient U(c|f_l(x)) = I(f_l(x);c)/H(c) using the Noshad et al. (2019) estimator, and regularity R(c|f_l(x)) as 5-fold logistic regression CV accuracy. The optimal layer l* is selected via l* = argmax_l [λ·U + (1-λ)·(R-1)/(k-1)], with λ=0.26. This layer selection is performed using up to 1000 balanced samples, after which probes (logistic regression, ridge, LightGBM, 2-layer NN, mapping network) are trained on ≤1000 samples with 20% validation.

## Key Results
- The proposed method achieves an average probe accuracy of 90.1% versus 77.6% for the layer-wise average baseline
- The method approaches the performance of an oracle that knows the best layer (95.8% average accuracy)
- The method runs in under 5 minutes for models with 50+ layers on a single CPU core

## Why This Works (Mechanism)

### Mechanism 1: Information Sufficiency via Uncertainty Reduction
The method calculates the Uncertainty Coefficient (normalized mutual information) U(C|f_l(x)) for each layer l. This quantifies the fraction of information about concept C provided by the layer's representations f_l(x) relative to the concept's total entropy H(C). High values suggest the necessary signal is present. If the estimated mutual information is low across all layers, the concept is likely not encoded in the model, or the estimator is failing in high dimensions.

### Mechanism 2: Geometric Regularity via Linear Separability
The method estimates Regularity R(C|f_l(x)) using the accuracy of a logistic regression classifier (a linear probe) with 5-fold cross-validation. This serves as a proxy for the Minimum Description Length (MDL), indicating how easily the boundary can be learned. If a concept requires a highly non-linear manifold to be separated, the logistic regression proxy will report low regularity, potentially rejecting a layer that a complex probe could actually utilize.

### Mechanism 3: Hierarchical Concept Accumulation
The selection algorithm l* = argmax_l (λU + (1-λ)R) finds the "sweet spot" layer where information is high but before it is compressed/diffused for the final classification task. In very deep or residual networks (e.g., ResNet50 used in experiments), information might be skip-connected, potentially smoothing out distinct "sweet spots," though the method still found effective layers.

## Foundational Learning

- **Concept: Mutual Information (Information Theory)**
  - **Why needed here:** This is the mathematical core of the "Informativeness" metric. You must understand I(X;Y) to grasp why the paper measures the reduction in uncertainty about the concept given the neural activations.
  - **Quick check question:** If I(f_l(x); C) = 0, what does that imply about the relationship between layer l and concept C?

- **Concept: Minimum Description Length (MDL)**
  - **Why needed here:** The paper connects "Regularity" to MDL—the idea that simple explanations (shorter description lengths) are better. This justifies why they use a simple logistic regression to test if a representation is "good" for probing.
  - **Quick check question:** Why might a layer with high Mutual Information but low Regularity (high MDL) be bad for training a probe?

- **Concept: Linear Probing**
  - **Why needed here:** The method uses a linear probe (logistic regression) not just as a tool, but as a diagnostic "regularity meter." Understanding the limitations of linear classifiers is crucial for knowing when this method might fail (e.g., highly non-linear concepts).
  - **Quick check question:** If the true concept boundary in the activation space is a spiral, will the "Regularity" metric (based on logistic regression) capture this successfully?

## Architecture Onboarding

- **Component map:** Original Model -> Data Module -> Scorer -> Selector -> Probe Trainer
- **Critical path:** 1. Extract activations f_l(x) for all layers for the sample dataset. 2. Estimate U (Info) and R (Regularity) independently for each layer. 3. Apply λ-weighting to select the optimal layer index. 4. Train the final "production" probe on that specific layer's activations.
- **Design tradeoffs:**
  - Lambda (λ) tuning: The paper sets λ=0.26, favoring Regularity. In domains where information is sparse, you might need to weight Informativeness higher.
  - Estimator Choice: The method relies on the Noshad et al. (2019) mutual information estimator. Switching estimators could significantly change the "Informativeness" landscape.
  - Probe Complexity: The paper argues that finding the layer allows for simpler final probes. If you use a complex final probe, you might not need this layer selection, but interpretability suffers.
- **Failure signatures:**
  - Concept Absence: If all layers have U ≈ 0 and R ≈ chance, the model does not encode the concept.
  - Overfitting the Selection: If selecting based on validation accuracy (Baseline in paper), you risk overfitting the probe to the validation set, whereas this method is structurally constrained by information geometry.
  - Resolution Loss: In very deep networks, intermediate layers might be too compressed; the method might select early layers that are "safe" but lack semantic depth.
- **First 3 experiments:**
  1. Sanity Check (Color): Run the method on a simple concept like "Redness" in a standard vision model. Verify that it selects an early convolutional layer (expected for color).
  2. Sanity Check (Class): Run on the final output class (e.g., "Is this a bird?"). Verify it selects the final or penultimate layer.
  3. Ablation: Compare the Mutual Information estimation speed vs. accuracy. The paper claims efficiency; verify if the MI calculation scales better than training N probes (exhaustive search).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The method's reliance on the Noshad et al. (2019) mutual information estimator introduces potential instability, particularly for high-dimensional activations in deeper layers.
- The fixed λ=0.26 weighting may not generalize across all concept types—concepts requiring complex decision boundaries might need different balances.
- The current study is limited to image classification datasets and computer vision architectures, leaving generalization to other domains uncertain.

## Confidence
- **High Confidence:** The core mechanism (balancing information sufficiency and regularity) is theoretically sound and empirically validated across multiple models and datasets.
- **Medium Confidence:** The specific λ=0.26 weighting is justified empirically but may require tuning for different domains or concept types.
- **Medium Confidence:** The mutual information estimator's performance in high dimensions is not thoroughly benchmarked against alternatives.

## Next Checks
1. **Estimator Sensitivity:** Compare the Noshad estimator against alternative MI estimators (e.g., MINE, KSG) to verify stability of the information scores across layers.
2. **λ Ablation Study:** Systematically vary λ from 0.1 to 0.9 to confirm the method's robustness and identify if concept types benefit from different weightings.
3. **Cross-Architecture Transfer:** Test whether the optimal λ and layer selection strategy transfer between architectures (e.g., from ResNet to ViT) without retraining the selection parameters.