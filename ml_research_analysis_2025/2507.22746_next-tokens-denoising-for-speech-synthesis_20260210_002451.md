---
ver: rpa2
title: Next Tokens Denoising for Speech Synthesis
arxiv_id: '2507.22746'
source_url: https://arxiv.org/abs/2507.22746
tags:
- arxiv
- speech
- diffusion
- audio
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dragon-FM, a text-to-speech (TTS) system that
  unifies autoregressive and flow-matching approaches to address the limitations of
  each paradigm. It uses a high-quality 48 kHz codec with 12.5 tokens per second to
  compress audio into discrete tokens, enabling efficient modeling while maintaining
  fidelity.
---

# Next Tokens Denoising for Speech Synthesis

## Quick Facts
- arXiv ID: 2507.22746
- Source URL: https://arxiv.org/abs/2507.22746
- Reference count: 10
- Key outcome: Dragon-FM achieves high-quality zero-shot podcast generation with FAD 1.9-2.6, SIM 0.916, and WER 2.74 at 12.5 Hz, using only 2-4 flow-matching steps vs 150 for VALL-E

## Executive Summary
This paper introduces Dragon-FM, a unified text-to-speech system that combines autoregressive and flow-matching approaches to overcome the limitations of each paradigm. The system processes 48 kHz audio as discrete tokens at 12.5 Hz using a custom Finite Scalar Quantization (FSQ) codec, then generates speech in 2-second chunks using autoregressive modeling across chunks and parallel flow-matching within each chunk. This hybrid approach achieves high-quality podcast generation with significantly reduced latency and iteration steps compared to existing TTS systems like VALL-E and E2.

## Method Summary
Dragon-FM processes audio in 2-second chunks, using autoregressive modeling across chunks for global coherence and parallel flow-matching denoising within each chunk for fast iterative synthesis. The system employs a custom 48 kHz codec with 12.5 tokens per second via Finite Scalar Quantization, compressing audio into discrete tokens while maintaining fidelity. A key innovation is using continuous flow-matching models to predict discrete FSQ tokens through their continuous embeddings, demonstrating the potential for multimodal generative modeling. The acoustic model leverages KV-cache across chunks and mean flow optimization to reduce flow-matching steps to 2-4.

## Key Results
- Achieves Fréchet Audio Distance (FAD) of 1.9-2.6 depending on configuration
- Reduces total number of function evaluations (TNFE) to 2-4 compared to 150 for VALL-E and 32 for E2
- Maintains high speaker similarity (SIM) of 0.916 and low word error rate (WER) of 2.74 at 12.5 Hz
- Demonstrates effective zero-shot podcast generation with minimal latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing audio in 2-second chunks with parallel flow-matching internally, while modeling chunk-to-chunk transitions autoregressively, yields lower latency than pure diffusion and better context than pure autoregression.
- Mechanism: The model uses a hybrid AR-diffusion strategy. It divides the discrete token sequence into chunks (e.g., 2 seconds). Across chunks, it applies autoregressive (AR) modeling with causal attention, which allows for KV-cache reuse and low first-byte latency. Within each chunk, it applies flow-matching denoising with bidirectional attention in parallel, reducing total AR steps and leveraging full intra-chunk context.
- Core assumption: The 2-second chunk size is a sweet spot—long enough to capture meaningful local acoustic dependencies (prosody, coarticulation) for parallel denoising, yet short enough to keep the AR sequence manageable for the global coherence model.
- Evidence anchors:
  - [abstract] "...processes audio codec tokens in 2-second chunks autoregressively across chunks while applying parallel flow-matching denoising within each chunk..."
  - [section 3.1, page 5] "This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Thus, the model leverages KV-cache across chunks..."
  - [corpus] Corpus evidence is limited for this specific chunked hybrid approach. FELLE (arXiv:2502.11128) uses token-wise flow matching, which is finer-grained. DiTAR (arXiv:2502.03930) also combines diffusion and AR but operates at a different granularity.
- Break condition: Performance would degrade if the chunk size were mis-specified. Too short, and the parallel denoising model lacks context for coherent synthesis; too long, and the AR component's latency benefits disappear.

### Mechanism 2
- Claim: Representing 48 kHz audio as a compact 12.5 Hz discrete token sequence using a Finite Scalar Quantization (FSQ) codec makes sequence modeling computationally tractable without sacrificing reconstruction fidelity.
- Mechanism: The custom codec compresses 48 kHz audio into a very low frame-rate representation (12.5 frames/second). Instead of a traditional VQ-VAE codebook, it uses FSQ to map continuous encoder outputs to a discrete set of tokens. This drastically shortens the sequence length the core acoustic model must process, reducing the quadratic cost of self-attention.
- Core assumption: The FSQ can quantize the audio features at such a low frame rate while preserving enough information for a separate decoder to reconstruct high-fidelity 48 kHz audio.
- Evidence anchors:
  - [abstract] "...model processes 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens per second... high-quality 48 kHz audio codec producing 12.5 discrete speech tokens per second via finite scalar quantization..."
  - [section 3.1, page 5] "We design a compact discrete speech token sequence at 12.5 tokens per second... A key innovation... is its integration of AR next-token prediction with parallel flow-matching denoising within the same token chunk."
  - [table 4, page 9] The ablation study shows the 12.5 Hz CodecB achieves a Speaker Similarity (SIM) of 0.916 and a Word Error Rate (WER) of 2.74, outperforming a 16 Hz Mel-vocoder baseline.
- Break condition: This mechanism fails if the information bottleneck at 12.5 Hz is too severe, leading to intelligibility loss or speaker similarity degradation. The paper’s ablations explore this boundary.

### Mechanism 3
- Claim: Using a continuous flow-matching model to predict discrete FSQ tokens is viable by treating the discrete tokens as continuous vectors and leveraging the model's intrinsic classification capability.
- Mechanism: The paper avoids using mel-spectrograms (continuous) or a separate discrete prediction head. Instead, it uses the continuous vector embeddings of the FSQ tokens. The flow-matching model is trained to denoise these embeddings. The authors posit that a well-trained continuous denoiser implicitly learns to classify the target discrete token in the FSQ's latent space.
- Core assumption: A continuous denoising model, without architectural modification, can be effectively repurposed for discrete token prediction by operating on their continuous embeddings.
- Evidence anchors:
  - [abstract] "...bridging continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers."
  - [section 3.1, page 5] "A key insight from our work is that continuous denoising models, without requiring architectural modifications, exhibit robust intrinsic classification capabilities. By designing effective token embeddings, we leverage this latent capacity to enhance performance."
  - [corpus] Corpus papers like "Continuously Augmented Discrete Diffusion" (arXiv:2510.01329) and "In-Situ Tweedie Discrete Diffusion Models" (arXiv:2510.01047) explore related problems of modeling discrete data with diffusion, suggesting this is an active area of research with competing approaches.
- Break condition: This mechanism would fail if the continuous denoising process frequently converged to vector representations that do not map cleanly to any valid discrete FSQ token, causing reconstruction artifacts.

## Foundational Learning

- Concept: **Autoregressive (AR) vs. Flow-Matching/Diffusion Models**
  - Why needed here: Dragon-FM's core innovation is hybridizing these two distinct paradigms. Understanding their individual strengths and weaknesses is crucial.
  - Quick check question: Can you explain why AR models have lower latency (due to KV-caching) but lack future context, while diffusion models have bidirectional context but lack a KV-cache mechanism?

- Concept: **Finite Scalar Quantization (FSQ)**
  - Why needed here: FSQ is the tokenizer at the heart of the system, bridging continuous and discrete domains.
  - Quick check question: How does FSQ differ from a traditional Vector Quantization (VQ) as used in a VQ-VAE codebook, and why might it be more suitable for this architecture?

- Concept: **Mean Flow Optimization**
  - Why needed here: This technique is cited as the reason for the drastic reduction in flow-matching steps (down to 2-4).
  - Quick check question: What is the core principle behind mean flow or consistency models that allows them to reduce the number of denoising steps?

## Architecture Onboarding

- Component map:
  1.  **48 kHz Codec**: Encoder + FSQ Tokenizer + Decoder. Compresses audio to 12.5 Hz tokens and reconstructs.
  2.  **Acoustic Model (Dragon-FM)**: A unified Transformer backbone.
      - Input: Text tokens + Speaker prompt + Previous chunk's KV-cache.
      - Core Loop: Autoregressively predicts the next 2-second chunk.
      - Intra-chunk Process: Uses few-step flow-matching with bidirectional attention to denoise the 25 tokens of the chunk in parallel.
      - Output: Continuous vectors that are mapped back to discrete FSQ tokens.
  3.  **Vocoder/Decoder**: The codec's decoder takes the FSQ tokens and synthesizes the final 48 kHz audio waveform.

- Critical path:
  1.  **Codec Quality**: The entire system fails if the 12.5 Hz codec cannot faithfully reconstruct audio. Check SIM and WER.
  2.  **Hybrid Generation**: The interplay between the AR outer loop and the flow-matching inner loop must be seamless. Errors in the AR step propagate to all future chunks.
  3.  **Token Prediction**: The flow-matching model must correctly predict continuous vectors that project to the target discrete FSQ tokens.

- Design tradeoffs:
  - **Chunk Size vs. Latency/Quality**: Smaller chunks (e.g., 1s) may reduce latency (Table 2-B1 shows better FAD) but increase AR steps. Larger chunks (e.g., >2s) reduce AR steps but may increase per-step latency and reduce the benefits of parallelism.
  - **Codec Fidelity vs. Sequence Length**: A higher token rate (e.g., 20 Hz, Table 2-C1) might improve reconstruction but increases sequence length, hurting attention efficiency.
  - **Flow-Matching Steps vs. Quality**: Fewer steps (e.g., 6 vs 24) are faster but may hurt FAD (Table 2-A3 vs A2).

- Failure signatures:
  - **Artifacts/Audio Glitches**: Could indicate codec failure, incorrect FSQ mapping, or too few flow-matching steps.
  - **Loss of Speaker Similarity**: Points to issues in the speaker prompt integration or the codec's ability to preserve timbre.
  - **Incoherent Long-Form Generation**: Suggests a breakdown in the AR modeling across chunks or a KV-cache issue.
  - **High Latency**: Could mean the chunk size is too small, flow-matching steps are too many, or KV-caching is not implemented correctly.

- First 3 experiments:
  1.  **Codec Ablation**: Train and evaluate the codec in isolation. Measure reconstruction quality (SIM, WER) at 12.5 Hz vs. baselines to validate the FSQ token representation before connecting the acoustic model.
  2.  **Ablation on Chunk Size & Flow Steps**: Using the full Dragon-FM model, run experiments varying chunk size (1s, 2s) and flow-matching steps (6, 12, 24) to plot the trade-off between generation quality (FAD) and inference speed (RTF/TNFE). Compare against Table 2.
  3.  **End-to-End Benchmark**: Compare the full system's latency and quality against baseline models (VALL-E, E2 TTS) on a standardized dataset. Measure RTF, first-byte latency, and FAD to reproduce the key claims in Table 3.

## Open Questions the Paper Calls Out
- Can the unified discrete-continuous approach be effectively extended to large-scale multimodal LLMs?
- What are the theoretical and performance limits of using continuous flow-matching to predict discrete tokens without explicit classification heads?
- How does chunk size reduction below 1 second impact the trade-off between bidirectional context quality and streaming latency?

## Limitations
- The exact configuration of the FSQ (number of levels, scalar ranges) is not detailed, making reproducibility challenging
- The mechanism for maintaining long-term dependencies across chunk boundaries beyond KV-cache reuse is not explicitly detailed
- Mean flow implementation details are referenced via external citation without pseudocode or detailed algorithmic description

## Confidence
- **High Confidence**: The hybrid AR + flow-matching architecture within chunks is a coherent and sound approach. The ablation studies provide strong empirical support for the chunk size (2s) and the number of flow steps (2-4) as optimal choices for the trade-off between quality and speed.
- **Medium Confidence**: The claim that a continuous flow-matching model can effectively predict discrete FSQ tokens by leveraging its "intrinsic classification capability" is supported by the experimental results but is theoretically less intuitive.
- **Medium Confidence**: The claim that the 12.5 Hz FSQ codec can maintain high fidelity for a 48 kHz reconstruction is supported by the ablation results, but the absolute performance numbers are only compared to a 16 Hz Mel-vocoder baseline.

## Next Checks
1. **FSQ Isolation Test**: Train and evaluate the FSQ codec in isolation on a standard dataset (e.g., LJSpeech). Measure the reconstruction quality (SIM, WER) at 12.5 Hz and compare it to a baseline VQ-VAE codec at the same frame rate. This will validate whether the FSQ design is the key contributor to the reported codec quality.

2. **Chunk Boundary Coherence Test**: Generate a long-form audio sample (e.g., 5+ minutes) with Dragon-FM and analyze the audio for any artifacts or loss of coherence specifically at the 2-second chunk boundaries. Use speaker diarization or a speaker verification model to check if speaker characteristics remain consistent across chunk transitions.

3. **Mean Flow Ablation**: Implement a version of Dragon-FM where the flow-matching within chunks uses a standard (non-mean) flow approach with 10-15 steps instead of the 2-4 step mean flow. Compare the FAD, RTF, and TNFE of this baseline to the proposed model to isolate the contribution of the mean flow optimization to the speed gains.