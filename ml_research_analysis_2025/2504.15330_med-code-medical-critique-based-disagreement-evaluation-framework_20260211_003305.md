---
ver: rpa2
title: 'Med-CoDE: Medical Critique based Disagreement Evaluation Framework'
arxiv_id: '2504.15330'
source_url: https://arxiv.org/abs/2504.15330
tags:
- evaluation
- medical
- critique
- framework
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Med-CoDE addresses the challenge of evaluating large language models
  in medical contexts, where traditional metrics like BLEU and ROUGE fail to capture
  nuanced medical context and potential errors. The framework uses a critique-based
  approach, employing a fine-tuned Phi-3 model to generate critiques and a BERT model
  to classify them into four levels of disagreement (None, Low, Moderate, High) between
  model predictions and ground-truth answers.
---

# Med-CoDE: Medical Critique based Disagreement Evaluation Framework

## Quick Facts
- **arXiv ID**: 2504.15330
- **Source URL**: https://arxiv.org/abs/2504.15330
- **Reference count**: 11
- **Primary result**: Fine-tuned Phi-3 + BERT pipeline achieves 71.72% human-eval accuracy vs GPT-3.5's 78.12% for medical disagreement classification

## Executive Summary
Med-CoDE addresses the challenge of evaluating large language models in medical contexts, where traditional metrics like BLEU and ROUGE fail to capture nuanced medical context and potential errors. The framework uses a critique-based approach, employing a fine-tuned Phi-3 model to generate critiques and a BERT model to classify them into four levels of disagreement (None, Low, Moderate, High) between model predictions and ground-truth answers. The framework was trained on a specialized dataset of 38,819 samples from medical QA datasets including MedQA, MedMCQA, MMLU, and PubMedQA. Experiments across four benchmark datasets showed that Med-CoDE effectively identified correct responses with the highest "None" disagreement probability and lowest "High" disagreement probability, while maintaining performance comparable to GPT-3.5 in human evaluations.

## Method Summary
The framework uses a two-stage pipeline: a Phi-3-mini (3.8B) model fine-tuned with LoRA to generate textual critiques of discrepancies between model predictions and ground truth, followed by a BERT-base classifier that categorizes these critiques into four disagreement levels. The training dataset was curated using GPT-4 to generate critiques and labels across 38,819 samples from medical QA datasets. The Phi-3 model was fine-tuned for 5 epochs with specific hyperparameters (batch 128, lr=1.41e-5, LoRA rank 16), while the BERT classifier was trained for 25 epochs with class weights to handle imbalance. The system processes question-answer pairs through the critique generator and then classifies the generated critique into ordinal disagreement levels.

## Key Results
- Phi-3 fine-tuned model generates critiques with average length of 58.95 words
- BERT classifier achieves 71.72% accuracy in human evaluation compared to GPT-3.5's 78.12%
- System correctly identifies correct responses with highest "None" disagreement probability and lowest "High" disagreement probability
- Framework maintains performance comparable to GPT-3.5 while being cost-effective for local deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating explicit natural language critiques captures semantic discrepancies between a model prediction and ground truth that string-based metrics miss.
- **Mechanism:** A fine-tuned language model (Phi-3) generates a textual explanation of the error. By forcing the evaluation into a "critique," the system moves beyond surface-level token matching to reasoning about medical accuracy.
- **Core assumption:** The critique generation model can successfully internalize medical logic from the training data to identify logical flaws, rather than just paraphrasing differences.
- **Evidence anchors:** The framework uses a "critique-based approach to quantitatively measure the degree of disagreement." The paper provides an example where a model substitutes equivalent medicine (X for Y); string matching fails, but "LLM-assisted accuracy could still be correct." Related work confirms that evaluation metrics based on string overlap (e.g., BLEU) often correlate poorly with human judgment in complex tasks.

### Mechanism 2
- **Claim:** Discretizing continuous textual critiques into four ordinal disagreement levels (None to High) allows for quantitative benchmarking of model reliability.
- **Mechanism:** A BERT classifier processes the free-text critique to predict a probability distribution over four classes. This maps unstructured evaluation text into a structured metric suitable for aggregation and comparison.
- **Core assumption:** The "disagreement" taxonomy (None, Low, Moderate, High) aligns with clinical safety requirements (e.g., "High" corresponds to dangerous hallucinations).
- **Evidence anchors:** The authors define specific labels: "High" indicates the response is "entirely incorrect," while "Moderate" implies "hallucinating." Table 1 shows the system achieves 71.72% accuracy in classification compared to GPT-3.5. Research on LLM calibration (e.g., CritiCal) suggests that structured critique outputs can aid in uncertainty estimation, supporting this discretization approach.

### Mechanism 3
- **Claim:** Supervised fine-tuning on GPT-4 generated labels creates a cost-effective proxy evaluator that approximates larger model performance.
- **Mechanism:** The system distills the evaluation capability of GPT-4 (used to create the ground-truth dataset) into a smaller local pipeline (Phi-3 + BERT). This reduces dependency on expensive API calls for large-scale evaluation.
- **Core assumption:** The GPT-4 generated critiques and labels in the training set are sufficiently high-quality ground truth (approx. 91% accuracy per human eval) to teach the student model.
- **Evidence anchors:** The dataset was curated "using the OpenAI GPT-4 model to build a fine-tuning dataset." Human evaluation of the GPT-4 labels found ~240 out of 265 samples (91%) were accurately critiqued. No direct corpus evidence confirms the distillation success rate of this specific architecture; this is an inference based on standard distillation principles.

## Foundational Learning

- **Concept:** **Semantic Evaluation vs. Lexical Matching**
  - **Why needed here:** The paper fundamentally argues that metrics like BLEU/ROUGE are invalid for medical QA because they rely on word overlap (n-grams) rather than meaning. Understanding why "semantic equivalence" differs from "string equality" is crucial.
  - **Quick check question:** If a model answers "Acetaminophen" but the ground truth is "Tylenol," would a lexical metric (BLEU) score high or low? (Answer: Low, because the strings share no common n-grams, despite identical meaning).

- **Concept:** **Ordinal Classification**
  - **Why needed here:** The framework maps critiques to "None, Low, Moderate, High." This is an ordinal scale (ordered), not just categorical. Understanding this helps in interpreting the results (e.g., "Moderate" is worse than "Low").
  - **Quick check question:** Can the disagreement classes be treated as pure categories (unordered), or does the order matter for calculating error severity? (Answer: Order matters; the metric implies a scale of correctness).

- **Concept:** **LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The authors fine-tune a 3.8B parameter model using LoRA (rank 16). Understanding parameter-efficient fine-tuning explains how they achieved adaptation with limited VRAM (20GB).
  - **Quick check question:** Why use LoRA instead of full fine-tuning for the critique generator? (Answer: To reduce computational cost and memory footprint while retaining the base model's general language capabilities).

## Architecture Onboarding

- **Component map:** Input Interface -> Phi-3 Critique Generator -> BERT Classifier -> Output Probability Vector

- **Critical path:** The prompt engineering in Figure 3 is the most fragile component. The prompt explicitly asks the model to generate a "precise, clear and short" critique. Any change in the input format or the generator's verbosity could shift the distribution of the critique text, breaking the BERT classifier which expects the style/length distribution of the training data.

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy:** The authors traded ~6.4% accuracy (78.12% vs 71.72%) for a cheaper, locally run evaluation pipeline (Phi-3 vs GPT-3.5).
  - **BERT Context Window:** The classifier uses a max sequence length of 208 tokens (Page 4). If the critique generator produces verbose explanations, they will be truncated, potentially losing the critical "reasoning" required for classification.

- **Failure signatures:**
  - **Classifier Collapse:** If the system predicts "None" for almost all inputs, check the class weights/balance during training (weights were [5.96, 1.34, 0.83, 0.52], suggesting "None" was under-represented or highly weighted to prevent this).
  - **Verbose Critiques:** If critiques exceed 208 tokens, the classifier receives incomplete sentences.
  - **Prompt Drift:** If the critique format changes (e.g., bullet points vs paragraph), the BERT model may misclassify.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run 50 samples through the pipeline and verify that "correct" medical answers (manually verified) result in "None" disagreement predictions with >0.7 probability.
  2. **Truncation Analysis:** Measure the token length of generated critiques on a test set to confirm they fit within the 208-token limit of the BERT classifier without significant information loss.
  3. **Adversarial Attack:** Input a plausible-sounding but medically incorrect prediction to see if the critique generator identifies the specific error or if it is fooled by the confident tone.

## Open Questions the Paper Calls Out

- **Question:** How does Med-CoDE perform when adapted to non-medical domains with domain-specific critique datasets?
  - **Basis in paper:** Section 6 states "This evaluation framework is adaptable for assessing large language models across various domain-specific tasks as well as general tasks, simply by modifying the critique dataset," but this claim is not experimentally validated.
  - **Why unresolved:** All experiments were conducted exclusively on medical QA benchmarks (MedQA, MedMCQA, MMLU, PubMedQA); no cross-domain evaluation was performed.
  - **What evidence would resolve it:** Results from applying Med-CoDE to legal, financial, or technical QA datasets with appropriately modified critique datasets.

## Limitations
- Dataset construction reliability depends on unverified quality of ~38,554 samples beyond the 265-sample human evaluation
- BERT classifier's 208-token context window may truncate critical reasoning in longer critiques
- Claims about cross-domain generalization remain speculative without experimental validation

## Confidence
- **High Confidence:** The core mechanism of using critique generation followed by classification for disagreement assessment is sound and supported by the experimental results
- **Medium Confidence:** The claim that the framework "maintains performance comparable to GPT-3.5" is supported by the 71.72% vs 78.12% comparison, but based on a relatively small human evaluation set
- **Low Confidence:** Claims about the framework's ability to generalize to entirely new medical domains or question types not represented in the 38k training samples are speculative

## Next Checks
1. **Class Distribution Validation:** Analyze the class distribution (None/Low/Moderate/High) in the evaluation results across all four benchmark datasets to verify the framework correctly identifies correct answers with "None" probability >0.7 and incorrect answers with "High" probability >0.7.

2. **Truncation Impact Assessment:** Measure the percentage of generated critiques exceeding 208 tokens and conduct an ablation study comparing full-length vs truncated critique classification accuracy.

3. **Cross-Dataset Generalization Test:** Evaluate the framework on a held-out medical QA dataset not used in training to assess whether the Phi-3+BERT pipeline maintains performance when encountering novel question distributions and medical content.