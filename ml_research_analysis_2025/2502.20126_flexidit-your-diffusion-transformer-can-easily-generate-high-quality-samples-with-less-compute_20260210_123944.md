---
ver: rpa2
title: 'FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples
  with Less Compute'
arxiv_id: '2502.20126'
source_url: https://arxiv.org/abs/2502.20126
tags:
- image
- steps
- patch
- arxiv
- flops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FlexiDiT, a method to convert pre-trained
  Diffusion Transformers into flexible models capable of processing inputs with varying
  compute budgets by adjusting patch sizes. By leveraging different patch sizes at
  different denoising steps, FlexiDiT can generate high-quality images and videos
  while reducing compute by up to 75% without compromising performance.
---

# FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute

## Quick Facts
- arXiv ID: 2502.20126
- Source URL: https://arxiv.org/abs/2502.20126
- Authors: Sotiris Anagnostidis; Gregor Bachmann; Yeongmin Kim; Jonas Kohler; Markos Georgopoulos; Artsiom Sanakoyeu; Yuming Du; Albert Pumarola; Ali Thabet; Edgar Schönfeld
- Reference count: 40
- Primary result: Converts pre-trained Diffusion Transformers into flexible models that reduce compute by up to 75% while maintaining sample quality through adaptive patch sizing

## Executive Summary
FlexiDiT introduces a method to convert pre-trained Diffusion Transformers into flexible models capable of processing inputs with varying compute budgets by adjusting patch sizes. By leveraging different patch sizes at different denoising steps, FlexiDiT can generate high-quality images and videos while reducing compute by up to 75% without compromising performance. The method is applicable to class-conditioned and text-conditioned image generation, as well as video generation, and is achieved through minimal architectural changes and fine-tuning.

## Method Summary
FlexiDiT works by adding flexible patch embedding layers to pre-trained DiTs and using weight projection to initialize these layers. During inference, a scheduler switches between large patches (weak model) for early steps and small patches (powerful model) for later steps. The method uses LoRA adapters for the new patch sizes and distillation training to align the weak and powerful model predictions. Weak-to-strong guidance is employed during sampling to improve sample quality.

## Key Results
- Reduces compute by up to 75% while maintaining sample quality
- Applicable to class-conditioned and text-conditioned image generation
- Applicable to video generation with temporal patch size adjustments
- Achieves results through minimal architectural changes and fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Spectral Progression of Denoising
The diffusion process naturally organizes frequency information temporally, with early steps prioritizing low-frequency features that require fewer spatial tokens. Large patch sizes reduce token count during these steps, lowering complexity from O(N²) without losing global structural information.

### Mechanism 2: Flexible Tokenization via Weight Projection
A single pre-trained model can process variable sequence lengths by projecting fixed embedding weights to match dynamic patch sizes using a pseudo-inverse projection matrix.

### Mechanism 3: Weak-to-Strong Guidance Distillation
Using a weak model's conditional prediction as the "unconditional" baseline in guidance provides a better steering signal than standard unconditional predictions, leveraging the weak model's focus on global structure.

## Foundational Learning

- **Vision Transformer (ViT) Tokenization**: Understanding how convolutions create tokens is prerequisite to modifying the embedding layers. Quick check: If an input image is 256×256 and patch size changes from 2×2 to 4×4, how does the sequence length change? (Answer: 128×128 to 64×64, i.e., 4x reduction).

- **Diffusion Noise Schedules (DDPM/Flow)**: The inference scheduler must decide when to switch from weak to strong model based on timestep t. Quick check: At high t (e.g., t=1000 in a 1000-step schedule), is the signal closer to pure noise or the data? (Answer: Pure noise / Low frequency).

- **Low-Rank Adaptation (LoRA)**: The paper proposes adding LoRAs to the DiT backbone for new patch sizes to preserve the original model's weights. Quick check: Why use LoRA instead of full fine-tuning if the original data is unavailable? (Answer: To preserve the original forward pass/safety guarantees and prevent catastrophic forgetting).

## Architecture Onboarding

- **Component map**: Input Image -> Select Patch Size p_t -> Project Weights -> Tokenize -> Transformer -> De-tokenize -> Noise Prediction
- **Critical path**: Input Image → Select Patch Size p_t → Project Weights → Tokenize → Transformer → De-tokenize → Noise Prediction
- **Design tradeoffs**: LoRA vs. Merged Weights (keeping LoRAs unmerged increases FLOPs slightly but keeps memory low), Packing for CFG (padding/packing strategies needed when sequence lengths differ)
- **Failure signatures**: Exposure Bias (accumulation of error if powerful model is not fine-tuned on weak model's distribution), Latency Bottleneck (small batch sizes may not yield proportional speedups)
- **First 3 experiments**:
  1. Spectral Validation: Replicate Figure 2 by applying high/low pass filters to single diffusion steps
  2. Embedding Projection: Implement pseudo-inverse projection for patch embedding layer and verify initialization
  3. Scheduler Ablation: Implement "Weak First" scheduler on ImageNet and plot FID vs. FLOPs

## Open Questions the Paper Calls Out
- Can a dynamic, sample-adaptive scheduler determine the optimal patch size per step and per sample more efficiently than the current static switch-point heuristic?
- What are the trade-offs in quality and efficiency when iteratively combining spatial and temporal weak models during video generation?
- Can the FlexiDiT framework be applied during the pre-training phase to accelerate convergence, rather than only as a post-hoc fine-tuning step?

## Limitations
- The spectral progression hypothesis lacks extensive corpus validation beyond internal evidence
- Weight projection mechanism lacks empirical validation on significantly different architectures
- Weak-to-strong guidance approach lacks clear theoretical guarantees about guidance quality preservation

## Confidence
- **High Confidence**: Basic architecture modifications (patch embedding projection, LoRA integration) are technically sound and well-specified
- **Medium Confidence**: Distillation training procedure and scheduled inference implementation details are clear but require careful reproduction
- **Low Confidence**: Theoretical claims about spectral progression and weak-to-strong guidance lack robust theoretical grounding

## Next Checks
1. **Spectral Validation Replicability**: Reproduce Figure 2 by applying band-pass filters to multiple diffusion timesteps on a pre-trained DiT
2. **Projection Initialization Verification**: Implement pseudo-inverse weight projection and measure initial MSE between projected weights and original forward pass
3. **Scheduler Sensitivity Analysis**: Systematically vary T_weak parameter from 20% to 80% and plot Pareto frontier of FID vs. FLOPs reduction