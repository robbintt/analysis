---
ver: rpa2
title: Scanning Trojaned Models Using Out-of-Distribution Samples
arxiv_id: '2501.17151'
source_url: https://arxiv.org/abs/2501.17151
tags:
- samples
- adversarial
- training
- attacks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TRODO, a novel method for scanning trojaned
  (backdoored) deep neural network models by leveraging out-of-distribution (OOD)
  samples. The key insight is that trojaned classifiers contain "blind spots" in their
  decision boundaries where OOD samples are misclassified as in-distribution.
---

# Scanning Trojaned Models Using Out-of-Distribution Samples

## Quick Facts
- arXiv ID: 2501.17151
- Source URL: https://arxiv.org/abs/2501.17151
- Authors: Hossein Mirzaei, Ali Ansari, Bahar Dibaei Nia, Mojtaba Nafez, Moein Madadi, Sepehr Rezaee, Zeinab Sadat Taghavi, Arad Maleki, Kian Shamsaie, Mahdi Hajialilue, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban
- Reference count: 40
- Primary result: Novel TRODO method detects trojaned models using OOD samples, achieving 79.4% accuracy in zero-data scenarios and 90.7% with minimal clean data.

## Executive Summary
This paper introduces TRODO, a novel method for scanning trojaned (backdoored) deep neural network models by leveraging out-of-distribution (OOD) samples. The key insight is that trojaned classifiers contain "blind spots" in their decision boundaries where OOD samples are misclassified as in-distribution. TRODO detects these blind spots by adversarially shifting OOD samples toward the in-distribution region and measuring the increase in their ID-Scores (maximum softmax probability). The method works even without access to clean training data (TRODO-Zero) and remains effective against adversarially trained trojaned models.

## Method Summary
TRODO detects trojaned models by generating near-OOD samples through harsh augmentations of validation data, then applying PGD adversarial perturbations to shift these samples toward the in-distribution region. The method measures the increase in ID-Score (maximum softmax probability) before and after perturbation. Detection is performed by comparing the average ∆ID-Score against a calibrated threshold derived from a surrogate clean model. The approach is trojan and label-mapping agnostic, working with various attack types and without requiring access to clean training data.

## Key Results
- TRODO achieves 79.4% accuracy in zero-data scenarios and 90.7% with minimal clean data
- Outperforms existing methods by 11.4% and 24.8% on standard and adversarially trained trojaned models respectively
- Maintains effectiveness across different trojan attack types and label-mapping strategies
- Demonstrates robustness against adaptive attacks with only 5-15% accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trojaned classifiers exhibit "blind spots"—distorted decision boundary regions where out-of-distribution samples are misclassified as in-distribution with high confidence.
- Mechanism: Backdoor injection during training causes benign overfitting to the trigger pattern, creating anomalous regions in the learned decision boundary. These blind spots are detectable because adversarial perturbations can more easily shift OOD samples into them compared to clean models, resulting in larger increases in ID-Score.
- Core assumption: The decision boundary distortion caused by trojaning is consistent across different attack types and label-mapping strategies.
- Evidence anchors: [abstract] "trojaned classifiers contain 'blind spots' in their decision boundaries where OOD samples are misclassified as in-distribution"; [section 1] "This often results in distorted areas of the learned decision boundary of the trojaned model, referred to as blind spots"
- Break condition: If a trojaned model's decision boundaries do not create distinguishable blind spots (e.g., highly sophisticated adaptive attacks), detection accuracy degrades.

### Mechanism 2
- Claim: Near-OOD samples (visually similar but semantically different from training data) produce more discriminative signatures than far-OOD samples.
- Mechanism: Near-OOD samples already reside closer to the model's decision boundary, making them more susceptible to adversarial perturbations that shift them into blind spots. The resulting ID-Score difference (∆ID-Score) between perturbed and unperturbed near-OOD samples is larger and more consistent for trojaned models.
- Core assumption: Near-OOD samples can be reliably generated through harsh augmentations when training data is unavailable.
- Evidence anchors: [abstract] "TRODO detects these blind spots by adversarially shifting OOD samples toward the in-distribution region and measuring the increase in their ID-Scores"; [section 4.1] "These samples improve the effectiveness of our proposed signature as they are more vulnerable to being misclassified as ID samples when they are adversarially perturbed"
- Break condition: If harsh augmentations fail to generate effective near-OOD samples (e.g., for domain-specific data), detection performance drops.

### Mechanism 3
- Claim: The signature holds even for adversarially trained trojaned models because such models remain vulnerable to perturbed OOD samples despite being robust to perturbed ID samples.
- Mechanism: Adversarial training robustifies the model against ID perturbations but does not address OOD vulnerabilities. Near-OOD samples exploit this asymmetry—the adversarial risk increases linearly with trigger-related parameters (Theorem 2), creating a detectable gap between clean and trojaned models.
- Core assumption: Adversarial training on poisoned data does not eliminate blind spots; it may even exacerbate OOD vulnerability.
- Evidence anchors: [abstract] "remains effective against adversarially trained trojaned models"; [section 1] "while adversarially robust classifiers are robust to perturbed ID samples, they are susceptible to perturbed OOD samples"
- Break condition: Adaptive attacks that specifically equalize ID-Score behavior for ID and OOD samples can reduce detection effectiveness (see Table 3 adaptive attack results).

## Foundational Learning

- Concept: Maximum Softmax Probability (MSP) as ID-Score
  - Why needed here: TRODO uses MSP to quantify how "in-distribution" a sample appears to the classifier; the difference before and after adversarial perturbation is the core detection signal.
  - Quick check question: Given a 10-class classifier, what MSP value indicates maximum uncertainty, and what value would you expect for a typical in-distribution sample?

- Concept: Projected Gradient Descent (PGD) Adversarial Attacks
  - Why needed here: PGD generates the perturbations that shift OOD samples toward blind spots; understanding the ε budget and iteration count is critical for tuning detection sensitivity.
  - Quick check question: How does the ε parameter control the tradeoff between perturbation strength and perceptibility in PGD?

- Concept: Backdoor/Trojan Attacks and Label Mapping
  - Why needed here: TRODO claims to be agnostic to attack type (BadNet, WaNet, etc.) and label mapping (All-to-One, All-to-All); understanding these variations helps evaluate generalization claims.
  - Quick check question: What is the difference between All-to-One and All-to-All label mapping in backdoor attacks, and why does All-to-All pose a greater detection challenge?

## Architecture Onboarding

- Component map: Validation data → Harsh augmentation → PGD perturbation → ID-Score computation → Threshold comparison → Trojan/Clean decision

- Critical path: Validation data → Harsh augmentation → PGD perturbation → ID-Score computation → Threshold comparison → Trojan/Clean decision

- Design tradeoffs:
  - Near-OOD vs. far-OOD: Near-OOD samples improve detection but require more careful selection/augmentation; far-OOD is simpler but less discriminative
  - ε budget: Too small misses blind spots; too large may cause false positives on clean models
  - Validation dataset choice: Must share visual features with training distribution but remain semantically distinct (Table 4 shows FID distance correlates with effectiveness)

- Failure signatures:
  - High false positive rate: ε too large or γ (boundary confidence level) too low
  - Low detection on adversarially trained models: Consider increasing PGD iterations or using near-OOD samples more aggressively
  - Adaptive attack degradation: Random transformations in OOD crafting provide some resilience, but sophisticated adaptive attacks (Table 3) can reduce accuracy by ~5-15%

- First 3 experiments:
  1. Reproduce the ∆ID-Score distribution comparison between clean and trojaned models on CIFAR-10 (Figure 2) to validate blind spot hypothesis before implementing full pipeline.
  2. Ablation on validation dataset selection (Table 4): Compare Tiny ImageNet vs. SVHN vs. STL-10 to understand near-OOD effect on your target domain.
  3. Adaptive attack stress test: Implement L_adaptive1 and L_adaptive2 from Section 9 on a small trojaned model to measure detection robustness degradation.

## Open Questions the Paper Calls Out

None

## Limitations

- Unknown implementation details for hard augmentations (specific parameters for ElasticTransform, rotation ranges, CutPaste configurations) could significantly affect detection performance.
- Hyperparameter calibration methodology lacks complete detail regarding surrogate model requirements and handling cases without clean models available.
- Adaptive attack countermeasures remain limited, with detection accuracy degrading by 5-15% against sophisticated adaptive attacks.

## Confidence

- **High confidence**: The core blind spot detection mechanism and zero-data scanning capability are well-supported by experimental results across multiple datasets and attack types.
- **Medium confidence**: The near-OOD sample generation strategy is effective but relies on augmentation choices that may not generalize well to all domains.
- **Medium confidence**: Claims about adversarial training robustness are plausible but only partially validated—the method works against standard adversarial training but degrades under adaptive attacks.

## Next Checks

1. **Blind spot mechanism validation**: Reproduce the ∆ID-Score distribution comparison between clean and trojaned models on CIFAR-10 (Figure 2) to empirically verify that trojaned models consistently show larger score increases under identical adversarial perturbations.

2. **Near-OOD augmentation sensitivity**: Conduct systematic ablation studies varying augmentation magnitudes and combinations to determine which specific parameters most strongly influence detection accuracy across different target domains.

3. **Adaptive attack stress testing**: Implement and evaluate the full suite of adaptive attacks described in Section 9 on trojaned models to quantify detection robustness degradation and identify potential mitigation strategies.