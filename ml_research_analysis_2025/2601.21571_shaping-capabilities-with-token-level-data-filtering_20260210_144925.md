---
ver: rpa2
title: Shaping capabilities with token-level data filtering
arxiv_id: '2601.21571'
source_url: https://arxiv.org/abs/2601.21571
tags:
- arxiv
- filtering
- https
- data
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing undesired capabilities
  in language models by shaping them during pretraining rather than applying post-hoc
  safeguards. The core method involves filtering pretraining data at the token level
  using classifiers trained to identify and remove content related to unwanted capabilities,
  such as medical knowledge, while preserving related desired capabilities like biology.
---

# Shaping capabilities with token-level data filtering

## Quick Facts
- arXiv ID: 2601.21571
- Source URL: https://arxiv.org/abs/2601.21571
- Authors: Neil Rathi; Alec Radford
- Reference count: 40
- Key outcome: Token-level pretraining data filtering achieves better capability shaping than document-level filtering, with effectiveness increasing at larger model scales

## Executive Summary
This paper addresses the problem of reducing undesired capabilities in language models by shaping them during pretraining rather than applying post-hoc safeguards. The core method involves filtering pretraining data at the token level using classifiers trained to identify and remove content related to unwanted capabilities, such as medical knowledge, while preserving related desired capabilities like biology. Token-level filtering is shown to be a Pareto improvement over document-level filtering, achieving the same reduction in undesired capabilities at a lower cost to desired ones. The approach scales effectively with model size: for 1.8B parameter models, token filtering leads to a 7000× compute slowdown on the forget domain. The method is also more robust to adversarial fine-tuning attacks than state-of-the-art unlearning methods and surprisingly improves the model's ability to be aligned on the forget domain through refusal training.

## Method Summary
The method uses a two-stage pipeline: first, tokens are labeled as "medical" or "not" using a Gemma 2 9B sparse autoencoder (SAE) with 16k latents. Tokens are labeled if they activate ≥2 medical latents by 4 SD or are adjacent to such tokens. Second, a small bidirectional language model (224M params) is trained on a balanced corpus of medical (PubMed) and non-medical (FineWeb-Edu) documents, with a linear probe fitted via L-BFGS to predict the token labels. During pretraining, high-scoring tokens are replaced with a special token and their loss is masked. This approach enables selective capability suppression while preserving related capabilities and improves alignment through better refusal generalization.

## Key Results
- Token filtering is a Pareto improvement over document filtering, achieving lower retain loss at equal forget loss
- For 1.8B parameter models, token filtering leads to a 7000× compute slowdown on the forget domain
- Models trained with token filtering generate refusals at 2× the rate of baseline models on HealthSearchQA
- Token filtering is more robust to adversarial fine-tuning than state-of-the-art unlearning methods

## Why This Works (Mechanism)

### Mechanism 1: Token-level precision enables selective capability suppression
Filtering at the token level achieves equivalent reduction in undesired capabilities at lower cost to desired capabilities compared to document-level filtering. Individual tokens within a document vary in their influence on model capabilities. Document-level filtering must remove entire documents to catch influential tokens, sacrificing benign tokens. Token filtering removes only influential tokens, preserving context and related capabilities.

### Mechanism 2: Filtering effectiveness amplifies with scale
Data filtering becomes proportionally more effective as model size increases. Larger models have lower scaling exponents on filtered domains—models trained on filtered data require exponentially more compute to match baseline performance on the forget domain. This may reflect increased sample efficiency at scale.

### Mechanism 3: Token masking preserves alignment-relevant representations
Models trained with token filtering (especially masking) can still learn to recognize and refuse queries on filtered domains. Loss masking allows models to process forget tokens during the forward pass, building representations that distinguish "seen but not learned" from "learned" content. This simplifies the refusal task to distinguishing trained vs. untrained tokens, rather than classifying domain-specific content.

## Foundational Learning

- **Concept: Influence functions / data attribution**
  - Why needed here: The paper builds on the insight that individual tokens vary in influence on downstream capabilities. Understanding data attribution helps explain why token-level precision matters.
  - Quick check question: Can you explain why removing one influential token might affect a capability more than removing many non-influential tokens?

- **Concept: Scaling laws for language models**
  - Why needed here: The key result that filtering becomes more effective with scale relies on computing "loss-matched compute slowdown"—how much more compute a baseline model needs to match filtered-model performance.
  - Quick check question: If a filtered model has loss L at compute C, and the baseline needs compute C' to reach loss L, what does C'/C > 1 imply about filtering effectiveness?

- **Concept: Sparse autoencoders (SAEs) for feature extraction**
  - Why needed here: The paper uses SAEs to identify domain-relevant features in a pretrained model, then labels tokens based on feature activations. This enables weakly-supervised classifier training.
  - Quick check question: Why might SAE features be better than keyword matching for identifying medical tokens in context?

## Architecture Onboarding

- **Component map:**
  1. Labeling pipeline: SAE (Gemma 2 9B) → identify medical latents → label tokens via activation thresholds → expand labels to adjacent tokens
  2. Classifier training: Train bidirectional LM on domain-upsampled corpus → fit linear probe with L-BFGS
  3. Filtering application: Run classifier over pretraining corpus → apply loss masking or token removal during training

- **Critical path:**
  1. Define forget/retain domain boundaries precisely (see §C.1 for medical definition)
  2. Generate token labels using SAE pipeline (requires pretrained SAE access)
  3. Train bidirectional classifier on ~8M labeled tokens
  4. Set filtering threshold based on F1 or desired removal rate
  5. Filter pretraining corpus; train model with loss masking from step 0

- **Design tradeoffs:**
  - Loss masking vs. removal: Masking preserves context coherence but may allow representation learning; removal is more aggressive but breaks context
  - Aggressive vs. conservative threshold: Aggressive filtering achieves better suppression but harms retain performance; scales better with larger models
  - Early vs. late filtering: Filtering from the start is ~10× more effective than filtering only the last 20% of training (Figure 22)

- **Failure signatures:**
  - Model generates coherent but incorrect responses in forget domain (hallucination without capability)
  - Retain domain performance degrades unexpectedly (threshold too aggressive for model size)
  - Refusal training fails to generalize (suggests document filtering was used instead of token)

- **First 3 experiments:**
  1. Threshold sweep: Train 500M models with token filtering at 3%, 6%, 12%, 25% removal rates; plot forget vs. retain loss to verify Pareto frontier
  2. Masking vs. removal comparison: Using same threshold, compare loss masking to token removal on both forget domain perplexity and retain domain coherence
  3. Timing ablation: Start filtering at 0%, 20%, 40%, 60% through training to quantify early-filtering advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does token-level filtering exhibit "U-shaped" scaling at larger model sizes (e.g., >7B parameters), where models regain lost capabilities from small amounts of leaked data?
- Basis in paper: Section 7, "Scaling further": "It could be the case, though, that we see ‘U-shaped’ scaling: sufficiently large and capable models might be able to grok dangerous capabilities from a small number of samples that slip through filtering..."
- Why unresolved: The experiments were limited to 1.8B parameter models. It is unknown if the 7000× compute slowdown holds or reverses as models become more sample-efficient and capable of in-context reasoning.
- What evidence would resolve it: Extending the scaling laws and training runs beyond the 7B scale to observe if the forget-domain loss curve deviates from the current power law or if the model recovers capabilities from sparse "leaked" tokens.

### Open Question 2
- Question: Can we bootstrap effective token-level filtering using self-supervised scalable oversight from a small number of weak labels, removing the need for a strong external classifier?
- Basis in paper: Section 7, "Weak-to-strong generalization": "Or pushing even further, can we bootstrap self-supervised scalable oversight from a small number of weak labels, such that a ‘strong’ classifier isn’t required at all?"
- Why unresolved: The current method relies on a capable external model (e.g., Claude or a SAE-equipped Gemma) to generate ground-truth labels. A solution that allows the model to "organize itself" without external supervision is theoretical.
- What evidence would resolve it: An algorithm where the pretraining model iteratively improves its own data filter starting from minimal supervision, successfully suppressing capabilities without the aid of a larger pre-trained supervisor.

### Open Question 3
- Question: Does filtering data based on direct "influence" on capabilities (via attribution methods) provide a Pareto improvement over the content-based classification used in this paper?
- Basis in paper: Section 7, "Shaping capabilities in pretraining": "A possible approach is to filter datapoints directly based on their influence on capabilities as determined by some attribution method... We believe that an important direction is to study whether this sort of paradigm... can be applied to pretraining..."
- Why unresolved: The paper proxies capability suppression by filtering tokens containing *relevant knowledge* (content-based). The platonic ideal is removing tokens that *cause* the capability, which may differ from content tokens (e.g., reasoning tokens).
- What evidence would resolve it: Comparing the "forget/retain" Pareto frontier of models trained on data filtered by influence functions (e.g., gradient-based attribution) versus the paper's classifier-based filtering.

### Open Question 4
- Question: Does token-level data filtering effectively shape "fuzzy" characteristics like misalignment risk, sycophancy, or scheming capabilities?
- Basis in paper: Section 7, "Filtering for alignment": "We hypothesize that our results likely extend to these domains." (referencing misalignment risk, character priors, and scheming capabilities).
- Why unresolved: The paper validates the method on "medical capabilities" (a domain of knowledge). It is untested whether the method works for behavioral traits or implicit risks like "scheming" which may not correspond to clear token labels.
- What evidence would resolve it: Applying the token-filtering pipeline to datasets associated with misalignment (e.g., fiction about AI taking over) and evaluating the resulting models on behavioral evaluations (e.g., honesty or power-seeking tests).

## Limitations
- The approach has only been validated on a single capability domain (medical knowledge) and may not generalize to other domains like coding or reasoning
- The scaling relationship at frontier model sizes (>100B parameters) remains unknown and could potentially reverse
- The method depends on having access to a strong pretrained model for SAE-based labeling, limiting its applicability in compute-constrained settings

## Confidence
**High Confidence Claims:**
- Token-level filtering achieves better Pareto efficiency than document-level filtering for 61M-1.8B models
- Filtering effectiveness scales with model size (verified across 3 orders of magnitude)
- Token filtering improves refusal training alignment compared to baseline

**Medium Confidence Claims:**
- 7000× compute slowdown at 1.8B scale represents asymptotic behavior (extrapolation from smaller models)
- SAE-based labeling produces sufficient quality for effective filtering (assumes no systematic biases in auto-interpretation)
- Loss masking preserves alignment-relevant representations (mechanism plausible but not fully characterized)

**Low Confidence Claims:**
- The method generalizes to other capability domains without modification
- The scaling trend continues indefinitely without reversal
- Weak supervision quality has minimal impact on filtering effectiveness

## Next Checks
1. **Cross-domain validation**: Apply the token filtering pipeline to a different capability domain (e.g., coding knowledge or mathematical reasoning) and measure whether the same Pareto improvement and scaling advantages hold. This would validate the domain-agnostic nature of the approach.

2. **Noise sensitivity analysis**: Systematically vary the quality of token labels (e.g., by adjusting SAE thresholds, using different auto-interpretation methods, or injecting synthetic noise) and measure the relationship between label accuracy and filtering effectiveness. This would quantify the method's robustness to weak supervision.

3. **Long-range dependency test**: Design an experiment where medical knowledge requires understanding relationships between tokens separated by large distances (>100 tokens). Measure whether token filtering can still effectively suppress capabilities when influential information is distributed rather than concentrated. This would test the fundamental assumption underlying the token-level advantage.