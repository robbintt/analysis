---
ver: rpa2
title: 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner
  Training for Long-Horizon Agent Tasks'
arxiv_id: '2510.05608'
source_url: https://arxiv.org/abs/2510.05608
tags:
- task
- plan
- executor
- global
- cabinet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EAGLET, an efficient and effective global
  planner training method for long-horizon agent tasks. The method addresses the challenge
  of planning hallucinations in LLM-based agents by decoupling high-level planning
  from local action execution through a plan-and-execute framework.
---

# A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks

## Quick Facts
- **arXiv ID:** 2510.05608
- **Source URL:** https://arxiv.org/abs/2510.05608
- **Reference count:** 40
- **Key outcome:** EAGLET achieves new SOTA on long-horizon agent tasks with 8x training cost reduction.

## Executive Summary
This paper introduces EAGLET, a method for training global planners for long-horizon agent tasks that addresses the challenge of planning hallucinations in LLM-based agents. The approach decouples high-level planning from local action execution through a plan-and-execute framework, training a plug-and-play planner using homologous consensus filtering and rule-based reinforcement learning. Experiments on ScienceWorld, ALFWorld, and WebShop demonstrate significant performance improvements over existing methods while reducing training costs substantially.

## Method Summary
EAGLET trains a global planner in two stages: first using supervised fine-tuning with synthetic plans filtered through homologous consensus (checking plan quality across novice and expert executors), then refined with rule-based RL using an Executor Capability Gain Reward that measures improvement across different executor capabilities. The planner generates high-level roadmaps that are injected into executor contexts, providing global foresight and reducing trial-and-error behavior.

## Key Results
- Achieves new state-of-the-art performance on ScienceWorld, ALFWorld, and WebShop benchmarks
- Reduces training costs by 8x compared to RL-based baselines
- Demonstrates strong generalization to unseen tasks without requiring manual effort or extra training data
- Shows that plan injection position matters, with instruction injection outperforming other locations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling global planning from local execution mitigates planning hallucinations and aimless trial-and-error in long-horizon tasks.
- **Mechanism:** Separates the agent into Global Planner ($\pi_g$) that generates high-level roadmap before interaction, and Executor ($\pi_\theta$) that grounds roadmap into environment actions by injecting plan into executor context.
- **Core assumption:** Executor model has sufficient instruction-following capability to utilize external plan rather than ignoring or contradicting it.
- **Evidence anchors:** Abstract states addresses "challenge of planning hallucinations... by decoupling high-level planning from local action execution." Section 1 explains "explicitly endows the agent with global foresight that implicit planning methods lack."
- **Break condition:** If plan injection context is too small to hold both plan and environmental history, or executor isn't fine-tuned to prioritize plan.

### Mechanism 2
- **Claim:** Training data quality for planner is improved by filtering synthesized plans through "Homologous Consensus."
- **Mechanism:** Validates generated plan $p$ by checking if it improves (or doesn't degrade) performance of two homologous executors - models sharing same architecture but differing in fine-tuning (e.g., base vs. RL-finetuned).
- **Core assumption:** High-quality plan should be robust enough to aid agents of varying skill levels within same model family; if it confuses novice or misleads expert, it's noisy.
- **Evidence anchors:** Section 3.1.2 describes HCF aims to filter "useless for task completion by homogeneous execution agents" using expert-level GiGPO and novice-level Llama-3.1. Table 9 shows filtering with heterologous models leads to worse performance.
- **Break condition:** If novice model is too weak to follow any instruction, or expert is so strong it ignores plan, filtering signal becomes constant.

### Mechanism 3
- **Claim:** Planner generalization is enhanced by using Executor Capability Gain Reward (ECGR) rather than simple task success.
- **Mechanism:** RL reward calculated as improvement in task completion rate relative to executor without plan, multiplied by decay factor for efficiency, summed over two homologous executors (novice + expert).
- **Core assumption:** Plan providing "gain" across different capability levels captures intrinsic structure of task rather than exploiting specific executor's quirks.
- **Evidence anchors:** Section 3.2.1 defines $R_{ECGR}$ as sum of rewards from expert and novice executors. Explains "reward based on single executor agent is heavily influenced by varying capabilities... introducing false patterns."
- **Break condition:** If computational budget doesn't allow for running dual rollouts (with/without plan) for two executors during training.

## Foundational Learning

- **Concept: Plan-and-Execute Architecture**
  - **Why needed here:** Core contribution is training "Planner" module. Must understand how frozen "Executor" consumes plan (string injection) vs. how standard agents (e.g., ReAct) interleave thought and action.
  - **Quick check question:** Can you explain difference between implicit planning (CoT during execution) and explicit planning (generating plan beforehand)?

- **Concept: Reinforcement Learning with Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Paper uses GRPO for RL stage. Understanding how it estimates advantages based on group scores (rather than separate value model) is needed to implement training loop.
  - **Quick check question:** How does GRPO differ from PPO regarding need for critic model?

- **Concept: Homologous Models**
  - **Why needed here:** Central to both filtering and reward mechanisms. Need to distinguish between "same architecture" (homologous) and "same capability" (identical models).
  - **Quick check question:** Why would using two different model families (e.g., Llama and Qwen) for filtering create noise in training data?

## Architecture Onboarding

- **Component map:** DeepSeek-V3.1-Think -> Homologous Consensus Filtering (Llama-3.1-8B-Instruct Novice + GiGPO-Llama-3.1-8B Expert) -> SFT Cold Start -> GRPO RL with ECGR -> Llama-3.1-8B-Instruct Planner

- **Critical path:**
  1. **Synthesis:** Advanced LLM generates draft plans from task trajectories
  2. **Filtering:** Run drafts through Novice/Expert to clean dataset
  3. **SFT:** Train Planner on cleaned dataset (Cold Start)
  4. **RL:** Train Planner using GRPO with ECGR (comparing Novice/Expert performance w/ vs w/o plan)

- **Design tradeoffs:**
  - **Plan Injection Position:** Tested Instruction vs. Thought vs. Observation. Instruction yielded best results (Table 5), likely due to highest attention priority
  - **Number of Homologous Models:** Using 2 (Novice/Expert) is sweet spot. Using 3 models (adding "Intermediate") increased cost without consistent gains (Table 9)
  - **Efficiency vs. Cost:** 8x speedup claim relies on Planner being smaller (8B) and training requiring fewer iterations (~50 vs ~400 for baselines)

- **Failure signatures:**
  - **Rigid Planning:** If SFT data isn't filtered, planner may memorize specific trajectories (overfitting) rather than general strategies, failing on "unseen" tasks
  - **Reward Hacking:** If using only single capable executor for reward, planner might output low-quality plans knowing executor can solve it anyway (Section 3.2.1)

- **First 3 experiments:**
  1. **Sanity Check (SFT Only):** Train planner on unfiltered synthetic data vs. HCF-filtered data on ALFWorld. Check "Seen" vs "Unseen" gap to verify overfitting reduction
  2. **Ablate Reward:** Train with simple "Task Success" reward vs. ECGR. Specifically, check if "Task Success" version degrades on weaker executor (showing it overfit to strong one)
  3. **Inference Injection:** Using trained planner, inject plan into Executor's Observation vs. Instruction to replicate finding that Instruction placement is critical

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the EAGLET framework be effectively extended to multi-modal environments where visual grounding and sensory input play a critical role in high-level planning?
  - **Basis in paper:** Limitations section states "experiments primarily focus on text-based interactive environments. Extending EAGLET to multi-modal settings may present additional challenges."
  - **Why unresolved:** Current study only validates planner in text-based simulations, leaving integration of visual observation into global plan synthesis and filtering pipeline unexplored.
  - **What evidence would resolve it:** Successful application of EAGLET to multi-modal benchmark demonstrating homologous consensus filtering and ECGR improve performance without extensive manual annotation.

- **Open Question 2:** Is it possible to replace reliance on diverse homologous executor agents for consensus filtering and reward computation with lightweight or self-improving evaluation strategies?
  - **Basis in paper:** Authors note methods "still rely on availability of diverse executor agents. Investigating more lightweight or self-improving evaluation strategies could broaden applicability."
  - **Why unresolved:** Current HCF and ECGR require running inference on multiple agents (expert and novice) for every task instance, incurring computational overhead and limiting deployment flexibility.
  - **What evidence would resolve it:** Development of variant achieving comparable planning quality using single agent with self-critique or model-free verification, removing dependency on suite of homologous models.

- **Open Question 3:** How robust is trained global planner in transferring knowledge across domains and tasks that possess significantly different structural requirements than training distribution?
  - **Basis in paper:** Limitations section concludes "Studying long-term transfer across domains and tasks of significantly different structures remains open question."
  - **Why unresolved:** While paper demonstrates generalization to "unseen" scenarios within same benchmarks, doesn't test cross-domain transfer (e.g., training on household tasks and testing on coding or mathematical reasoning).
  - **What evidence would resolve it:** Experiments showing planner trained on one domain (e.g., ALFWorld) can effectively guide executors in structurally distinct domain (e.g., ScienceWorld or WebShop) without retraining.

## Limitations
- The homologous consensus filtering mechanism assumes plans robust across executors of different capability levels are inherently better, but may not hold for tasks requiring specialized strategies only experts can follow
- Computational efficiency claim (8x speedup) depends on specific hardware/scale assumptions and smaller planner size (8B vs larger RL baselines)
- Method requires access to both novice and expert executor models for training, which may not be available in all domains

## Confidence
- **High confidence:** Core architectural innovation (plan-and-execute decoupling) is well-supported by experimental results across all three benchmarks with consistent performance improvements over baselines
- **Medium confidence:** Homologous consensus filtering mechanism is theoretically sound but lacks extensive ablation studies on edge cases where novice/expert disagreement might be informative
- **Medium confidence:** ECGR reward formulation appears effective but relative weighting between capability gain and format rewards is not fully explored

## Next Checks
1. **Cross-task generalization test:** Apply trained planner to fourth, previously unseen long-horizon task (e.g., HotpotQA or custom environment) to verify claims about generalization beyond three benchmark tasks
2. **Executor capability boundary analysis:** Systematically vary gap between novice and expert executors to identify when homologous filtering breaks down (e.g., when novice is too weak or expert is too strong)
3. **Plan injection sensitivity study:** Extend injection position analysis to include more granular variations (different injection timing, multiple injection points) and test on tasks requiring more dynamic replanning