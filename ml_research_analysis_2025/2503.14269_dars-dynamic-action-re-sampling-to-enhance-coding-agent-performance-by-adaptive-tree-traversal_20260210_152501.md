---
ver: rpa2
title: 'DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive
  Tree Traversal'
arxiv_id: '2503.14269'
source_url: https://arxiv.org/abs/2503.14269
tags:
- action
- file
- search
- issue
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Action Re-Sampling (DARS), an inference-time
  compute scaling method for software engineering agents that improves recovery from
  suboptimal decisions by branching at key decision points using depth-first exploration
  and long-horizon feedback. The approach selectively expands trajectories at critical
  actions like edit, append, create, and submit, based on causal impact analysis,
  reducing redundancy and improving efficiency compared to random sampling or exhaustive
  tree search methods.
---

# DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal

## Quick Facts
- arXiv ID: 2503.14269
- Source URL: https://arxiv.org/abs/2503.14269
- Reference count: 40
- Achieves 47% pass@1 rate on SWE-Bench Lite, state-of-the-art among open-source frameworks

## Executive Summary
This paper introduces Dynamic Action Re-Sampling (DARS), an inference-time compute scaling method for software engineering agents that improves recovery from suboptimal decisions by branching at key decision points using depth-first exploration and long-horizon feedback. The approach selectively expands trajectories at critical actions like edit, append, create, and submit, based on causal impact analysis, reducing redundancy and improving efficiency compared to random sampling or exhaustive tree search methods. Evaluated on SWE-Bench Lite with Claude 3.5 Sonnet V2, DARS achieves a state-of-the-art pass@1 rate of 47% among open-source frameworks, outperforming baselines by 2.6–4.2% across model sizes and leveraging a fine-tuned preference model for trajectory selection.

## Method Summary
DARS employs a selective branching mechanism that focuses computational resources on high-impact actions during agent execution. The method uses causal impact analysis to identify critical decision points where trajectory expansion would be most beneficial, then performs depth-first exploration with long-horizon feedback to evaluate alternative paths. A preference model, fine-tuned on trajectory data, guides the selection of promising branches to expand. This targeted approach reduces the computational overhead compared to exhaustive search while maintaining effectiveness in recovering from suboptimal agent decisions.

## Key Results
- Achieves 47% pass@1 rate on SWE-Bench Lite, outperforming baselines by 2.6–4.2%
- State-of-the-art performance among open-source frameworks
- Demonstrates effectiveness across multiple model sizes when using Claude 3.5 Sonnet V2
- Shows 2.6–4.2% improvement over baseline methods

## Why This Works (Mechanism)
The core mechanism leverages selective trajectory expansion at critical decision points rather than uniformly sampling all possible actions. By using causal impact analysis to identify high-leverage actions and applying depth-first exploration with long-horizon feedback, the method focuses computational resources where they are most likely to improve outcomes. The preference model further refines this process by learning from past trajectory data to prioritize the most promising branches for expansion.

## Foundational Learning

**Causal Impact Analysis**
*Why needed:* Identifies which actions have the most significant influence on downstream outcomes, enabling targeted computation allocation.
*Quick check:* Verify that identified critical actions correlate with improved performance metrics in ablation studies.

**Depth-First Exploration with Long-Horizon Feedback**
*Why needed:* Allows thorough evaluation of alternative trajectories at key decision points while maintaining computational efficiency.
*Quick check:* Compare exploration depth and breadth against alternative search strategies.

**Preference Model Fine-Tuning**
*Why needed:* Learns from historical trajectory data to predict which branches are most likely to lead to successful outcomes.
*Quick check:* Evaluate preference model accuracy on held-out trajectory data.

## Architecture Onboarding

**Component Map:** Action Selector -> Causal Impact Analyzer -> Preference Model -> Trajectory Expander -> Outcome Evaluator

**Critical Path:** Action Selection → Causal Impact Analysis → Branch Selection → Depth-First Exploration → Feedback Integration

**Design Tradeoffs:** The method trades computational efficiency for improved recovery from suboptimal decisions by focusing on critical actions rather than exhaustive exploration.

**Failure Signatures:** Potential issues include over-reliance on the preference model leading to exploration bias, computational overhead from causal analysis, and sensitivity to the definition of critical actions.

**First Experiments:**
1. Compare performance with and without causal impact analysis to isolate its contribution
2. Evaluate different preference model architectures for trajectory selection
3. Test robustness across varying levels of computational resources

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Performance claims on SWE-Bench Lite should be interpreted cautiously given rapid benchmark evolution
- Limited generalizability beyond software engineering contexts due to focus on specific action types
- Does not address computational overhead and latency trade-offs in detail
- Evaluation methodology lacks ablation studies on individual component contributions

## Confidence

- Performance claims on SWE-Bench Lite: Medium
- Technical methodology description: High
- Generalizability claims: Low
- Efficiency comparisons: Medium

## Next Checks

1. Conduct a comprehensive ablation study to isolate the contribution of each DARS component (adaptive tree traversal, causal impact analysis, preference model) to the overall performance improvement.

2. Evaluate the method on additional benchmarks beyond SWE-Bench Lite to assess generalizability, particularly on datasets requiring different action spaces or longer-range reasoning.

3. Perform detailed analysis of computational overhead and inference-time costs, including comparisons with alternative scaling methods under various resource constraints.