---
ver: rpa2
title: 'Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets
  for Causal Discovery'
arxiv_id: '2503.17037'
source_url: https://arxiv.org/abs/2503.17037
tags:
- causal
- data
- noise
- variance
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating realistic benchmark
  datasets for causal discovery by proposing a new method for sampling structural
  causal models (SCMs) that avoids artifacts like varsortability and R2-sortability.
  The core method, called UUMC SCM generation, samples coefficients and noise variances
  by drawing from a unit ball in the space of causal parameters, ensuring the generated
  data is unitless, Markov-consistent, and unrestricted.
---

# Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery

## Quick Facts
- **arXiv ID:** 2503.17037
- **Source URL:** https://arxiv.org/abs/2503.17037
- **Reference count:** 40
- **Primary result:** UUMC method generates SCMs that eliminate varsortability artifacts and produce realistic R2-sortability, improving causal discovery benchmark validity.

## Executive Summary
This paper addresses a critical problem in causal discovery benchmarking: existing methods for generating synthetic structural causal models (SCMs) introduce artifacts that allow algorithms to exploit spurious patterns rather than genuine causal structure. The proposed Unitless Unrestricted Markov-Consistent (UUMC) method samples coefficients and noise variances from a unit ball in parameter space, ensuring generated data is unitless, Markov-consistent, and unrestricted. This approach theoretically eliminates varsortability (where variance correlates with topological order) and produces realistic R2-sortability patterns. Experiments show UUMC benchmarks yield more meaningful comparisons between causal discovery algorithms, with PC-stable and GES performing better while NOTEARS performance decreases as expected.

## Method Summary
UUMC SCM generation samples coefficients and noise variances by drawing from a unit ball in the space of causal parameters. For each node with d parents, coefficients are drawn from N(0,1) and scaled by r/||a|| where r ~ U^{1/d}(0,1), ensuring the sum of squared coefficients plus noise variance equals 1. This creates unitless variables where E[X²_i] = 1 for all nodes. The method extends to time series through vector autoregression with appropriate noise handling. Key innovations include standardization to enforce unit variance and theoretical analysis showing the method eliminates varsortability while producing realistic R2-sortability distributions.

## Key Results
- UUMC generation eliminates varsortability, producing neutral sortability distributions centered at 0.5 versus traditional methods' 0.8+
- The method produces realistic R2-sortability patterns with mild reverse tendency, matching theoretical expectations for collider vs confounder asymmetry
- PC-stable and GES algorithms show improved F1 scores on UUMC benchmarks while NOTEARS performance decreases, suggesting more meaningful algorithm comparisons
- Theoretical analysis proves the method satisfies unitless, unrestricted, and Markov-consistent properties

## Why This Works (Mechanism)

### Mechanism 1: Unit Ball Sampling for Causal Parameters
Sampling coefficients and noise variances from a unit ball in parameter space eliminates nonphysical artifacts while preserving realistic causal structure. For each node with d parents, variance allocation is conceptualized as a d-dimensional unit ball where each coordinate represents a parent's contribution. Coefficients drawn from N(0,1) are scaled by r/||a|| where r ~ U^{1/d}(0,1), and noise fraction is (1-r²). This ensures the sum of squared coefficients plus noise variance equals 1, enforcing unit variance. The distribution depends only on local structure (number of parents), generalizing the local Markov property to the generation process itself.

### Mechanism 2: Theoretical Elimination of Varsortability
Standardizing variables during generation removes the artifact where variance correlates with topological order. Traditional methods draw coefficients from bounded intervals and fix noise variance at 1, creating accumulation where each downstream variable inherits and adds to variance from ancestors. UUMC standardizes after assigning each variable, breaking this accumulation chain. Theorem 1 proves the output is "unitless" (E[X²_i] = 1 for all i), eliminating the spurious correlation between variance and causal order.

### Mechanism 3: Realistic R2-Sortability Through Hub-Node Asymmetry
The method produces mild reverse R2-sortability, theoretically expected for any SCM-representable system. R2-scores reflect connectivity, not topological order. For a collider (A→B←C), R2_B is upper-bounded by 1-s²_b because children cannot predict parent noise. For a confounder (A←B→C), R2_B is lower-bounded by 1-min(s²_A, s²_C) because multiple children serve as independent observations of the parent, enabling near-perfect reconstruction. This asymmetry creates reverse R2-sortability on average.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) with linear additive Gaussian noise**
  - Why needed here: The entire method operates within linear X_i = Σ_j a_{ji}X_j + U_i where U_i ~ N(0, σ_i). Understanding this functional form is prerequisite to understanding why coefficient/noise sampling matters.
  - Quick check question: Given a 3-node chain X→Y→Z with unit coefficients and unit noise, what are the variances of X, Y, and Z? (Answer: 1, 2, 3—demonstrating varsortability)

- **Concept: Topological ordering and Markov properties**
  - Why needed here: The paper indexes variables by topological order (parents have lower indices). The local Markov property (X_i ⊥⊥ X_{<i} \ pa(X_i) | pa(X_i)) is extended to the generation process itself.
  - Quick check question: In the SCM X₁ := U₁, X₂ := a·X₁ + U₂, X₃ := b·X₁ + c·X₂ + U₃, what is pa(X₃) and what independence does the local Markov property imply?

- **Concept: Sortability metrics as benchmark artifacts**
  - Why needed here: Varsortability and R2-sortability are quantitative measures (0-1 scale, 0.5 = neutral) of how strongly variance/R2 correlates with causal order. Algorithms can inadvertently exploit these artifacts.
  - Quick check question: If an algorithm achieves 90% accuracy on a varsortable benchmark (sortability=0.9) but 60% on a neutral benchmark (sortability=0.5), what might you conclude about its real-world applicability?

## Architecture Onboarding

- **Component map:** DAG with adjacency matrix E → Coefficient sampler (d-ball method) → Noise allocator → Standardizer → Standardized coefficient matrix Â and noise vector ŝ

- **Critical path:** The d-ball sampling (drawing a''_i and r_i) is the core novelty. Errors here propagate to incorrect variance properties. The standardization step (computing σ'_i and scaling) corrects for parent correlations.

- **Design tradeoffs:**
  - **Reverse R2-sortability:** Accepted as physically realistic (confounders reconstruct better than colliders predict) vs. iSCM which also exhibits this but with restricted parameter ratios
  - **Unrestrictedness:** Allows arbitrarily large/small coefficient-to-noise ratios vs. iSCM which bounds them to [0.5, 2]
  - **Finite-sample varsortability:** Theoretical neutral expectation, but finite samples show variance—this is feature, not bug

- **Failure signatures:**
  - Algorithm returns NaN: Check that graph is acyclic and properly ordered (j≥i ⇒ e_{ji}=0)
  - Excessive reverse R2-sortability (>0.7): Likely error in r sampling distribution—verify U^{1/d}(0,1) not U(0,1)
  - Non-unit variance in generated data: Standardization uses infinite-sample correlation matrix R; finite samples will deviate slightly

- **First 3 experiments:**
  1. **Reproduce varsortability elimination:** Generate 100 ER graphs (N=20, p=0.3) with both UVN (traditional) and UUMC methods. Compute varsortability distribution. Verify UUMC centers near 0.5, UVN near 0.8+.
  2. **Compare algorithm rankings:** Run PC-stable, GES, and NOTEARS on both benchmark types. Verify NOTEARS performance drops on UUMC (per Figure 4) while PC-stable improves relatively.
  3. **Validate R2-asymmetry:** Generate collider (A→B←C) and confounder (A←B→C) structures with matched parameters. Compare R2_B distributions, confirming confounder hub R2 stochastically dominates collider hub R2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the hidden confounding induced by discrete subsampling of continuous systems be rigorously quantified and controlled for in the time series UUMC method?
- **Basis in paper:** "Though we bound the magnitude of hidden confounding in terms of the sampling rate, there is a need to rigorously address the issue of pervasive hidden-confounding that is a necessary consequence of subsampling continuous systems."
- **Why unresolved:** The current approach provides bounds based on sampling frequency but does not offer a complete theoretical or practical solution for eliminating or accounting for this confounding in generated benchmarks.
- **What evidence would resolve it:** A theoretical framework that quantifies the exact magnitude of hidden confounding as a function of sampling parameters, or an algorithmic modification that systematically eliminates the confounding while preserving desired statistical properties.

### Open Question 2
- **Question:** What causes the emergent R2-sortability tendency in the time series UUMC method, and is this a feature of realistic systems or an artifact to be removed?
- **Basis in paper:** "Surprisingly, we find that our method produces time series datasets that are slightly R2-sortable on average... [and] R2-sortability tendencies may emerge."
- **Why unresolved:** The paper does not provide a theoretical explanation for why the time series extension exhibits R2-sortability, nor does it empirically validate whether this matches real-world time series behavior.
- **What evidence would resolve it:** Empirical comparison with real-world time series datasets showing similar R2-sortability patterns, or theoretical analysis proving this is an inherent property of standardized VAR processes.

### Open Question 3
- **Question:** How does the neglect of noise term covariance in multivariate time series systems affect the realism and utility of UUMC-generated benchmarks?
- **Basis in paper:** "Our method accounts for additional noise due to subsampling, but neglects the covariance of the noise terms in multivariate systems."
- **Why unresolved:** The method assumes independent noise across variables, which may not hold in many real-world systems where contemporaneous noise correlations exist.
- **What evidence would resolve it:** Sensitivity analysis showing how benchmark evaluation results change when noise covariance is introduced, or analysis of real-world datasets quantifying typical noise covariance structures.

### Open Question 4
- **Question:** Beyond var- and R2-sortability, what other artifacts in SCM generation methods significantly affect the relative performance of causal discovery algorithms?
- **Basis in paper:** The paper shows that UUMC and iSCM have similar sortability properties but produce different algorithm performance patterns, concluding that "artifacts other than var- and R2-sortability... can affect the absolute and relative performances."
- **Why unresolved:** The paper demonstrates the existence of other artifacts but does not systematically identify or characterize them.
- **What evidence would resolve it:** Systematic ablation studies varying specific SCM generation parameters (e.g., restrictions on coefficient-to-noise ratios) while measuring impacts on algorithm performance rankings.

## Limitations
- Limited empirical validation beyond synthetic benchmarks, with no systematic comparison to real-world causal discovery datasets
- Assumes linear additive Gaussian noise structure, potentially missing non-linear causal mechanisms common in many domains
- Finite-sample varsortability variance is acknowledged but not thoroughly characterized across different graph structures and sample sizes

## Confidence
- **High Confidence:** Theoretical elimination of varsortability through unit ball sampling and standardization mechanism. This follows directly from the proof in Theorem 1 and the standardization procedure.
- **Medium Confidence:** Claims about improved causal discovery performance on UUMC benchmarks relative to traditional methods. While experimental results show improvements for PC-stable and GES, the comparisons use synthetic data rather than real-world benchmarks.
- **Medium Confidence:** The theoretical argument for realistic R2-sortability distributions based on collider vs confounder asymmetry. The mathematical reasoning is sound, but empirical validation across diverse graph structures is limited.

## Next Checks
1. **Real-World R2-Validation:** Apply the UUMC generation method to DAGs extracted from real-world causal discovery benchmarks (e.g., Sachs flow cytometry, ALARM network) and compare the R2-sortability distributions to those observed in actual measurement data from these systems.

2. **Non-Linear Extension Test:** Implement a non-linear variant of UUMC (e.g., using additive noise models with sigmoid or polynomial basis functions) and evaluate whether the theoretical properties (unitless, unrestricted, Markov-consistent) extend to this broader class of causal models.

3. **Finite-Sample Variance Characterization:** Conduct systematic experiments varying sample sizes (10, 100, 1000, 10000) and graph densities to empirically characterize the distribution of varsortability values under UUMC, testing whether the theoretical expectation of 0.5 holds across realistic finite-sample scenarios.