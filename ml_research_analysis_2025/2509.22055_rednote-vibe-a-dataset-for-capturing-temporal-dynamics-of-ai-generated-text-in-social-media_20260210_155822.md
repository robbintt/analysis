---
ver: rpa2
title: 'RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text
  in Social Media'
arxiv_id: '2509.22055'
source_url: https://arxiv.org/abs/2509.22055
tags:
- content
- social
- detection
- engagement
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RedNote-Vibe, the first longitudinal dataset
  for AI-generated text detection on social media, spanning five years from pre-LLM
  to July 2025. The dataset captures temporal dynamics and engagement patterns from
  the Chinese platform Xiaohongshu, containing 98,714 posts enriched with metadata
  including user engagement metrics.
---

# RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media

## Quick Facts
- arXiv ID: 2509.22055
- Source URL: https://arxiv.org/abs/2509.22055
- Reference count: 29
- 98,714 posts from Xiaohongshu platform spanning Jan 2020–July 2025 with metadata

## Executive Summary
RedNote-Vibe is the first longitudinal dataset capturing temporal dynamics of AI-generated text detection on social media, spanning five years from pre-LLM to July 2025. The dataset contains 98,714 posts from the Chinese platform Xiaohongshu, enriched with user engagement metrics including likes, comments, and collections. To address limitations of existing detection methods, the authors propose the PsychoLinguistic AIGT Detection Framework (PLAD), which leverages 31 psycholinguistic features across four dimensions to provide interpretable detection. Experiments show PLAD achieves superior performance compared to existing methods, with 88.78% accuracy on binary AIGT classification and 36.9% on 17-way model identification.

## Method Summary
PLAD extracts 31 psycholinguistic features across four dimensions—Emotional/Social Grounding, Cognitive Architecture, Lexical Identity, and Cohesion—then feeds them into tree-based classifiers (CatBoost/XGBoost/GBC). The framework uses a proxy LLM with Chain-of-Thought verification to score texts on abstract constructs like "Perspectival Complexity" or "Empathetic Engagement" against designed rubrics. The dataset is constructed from pre-LLM human seeds (before Nov 2022) used to generate AI variants via 17 LLMs from 6 providers, plus a post-LLM exploration set capturing real-world adoption patterns.

## Key Results
- PLAD achieves 88.78% accuracy on binary AIGT classification and 36.9% on 17-way model identification
- AI content adoption shows steady increase with inflection points coinciding with platform governance measures
- Human-generated content consistently achieves higher engagement than AI-generated content
- Authors combining human creativity with AI assistance show the highest engagement levels across all metrics

## Why This Works (Mechanism)

### Mechanism 1
Psycholinguistic features capture systematic differences between human and AI-generated text that persist across model families. PLAD extracts 31 features across four dimensions—Emotional/Social Grounding, Cognitive Architecture, Lexical Identity, and Cohesion—then feeds them into tree-based classifiers. The top discriminative features (Prosodic Rhythm Consistency, Type-Token Ratio, Imperfection) exploit AI's tendency toward uniformity and lack of natural disfluencies. Core assumption: Human writing exhibits irregular rhythms, self-corrections, and variable lexical patterns that LLMs trained for fluency systematically suppress.

### Mechanism 2
Proxy LLM evaluation enables scalable semantic feature extraction that correlates with human judgments. Instead of frequency-based word counting (e.g., LIWC), PLAD uses a proxy LLM with Chain-of-Thought verification to score texts on abstract constructs like "Perspectival Complexity" or "Empathetic Engagement" against designed rubrics. Core assumption: LLMs can reliably quantify psychological constructs in text when given structured evaluation criteria, and CoT verification reduces systematic bias.

### Mechanism 3
AI-augmented authors outperform both pure-human and pure-AI approaches by combining AI efficiency with human creativity. Authors who strategically integrate AI tools (3.5% of 829 authors analyzed) achieve higher engagement across likes, comments, and collections—suggesting AI assists drafting while humans add novelty, emotional resonance, and authenticity. Core assumption: Engagement metrics proxy content quality and audience connection; the hybrid workflow preserves human creative judgment while leveraging AI scalability.

## Foundational Learning

- Concept: **Psycholinguistic Feature Engineering**
  - Why needed here: PLAD's 31 features operationalize abstract psychological constructs (e.g., "Dialectical Argumentation Strength") into measurable signals; understanding what each dimension captures is essential for interpreting results.
  - Quick check question: Can you explain why Type-Token Ratio differs between human and AI text, and which dimension it belongs to?

- Concept: **Tree-Based Classification for Interpretability**
  - Why needed here: PLAD uses XGBoost/CatBoost specifically for feature importance rankings; unlike neural embeddings, these provide direct attribution to specific linguistic markers.
  - Quick check question: Why might a tree-based model outperform BERT for model identification (17-way) despite lower capacity?

- Concept: **Zero-Shot Generalization via Feature Persistence**
  - Why needed here: PLAD's value proposition includes detecting unseen models (e.g., GPT-o3) by capturing provider-level "style imprints" rather than memorizing specific model outputs.
  - Quick check question: What assumption must hold for features learned on GPT-4 to transfer to GPT-o3?

## Architecture Onboarding

- Component map:
  - Data Layer: RedNote-Vibe dataset (98,714 posts, pre-LLM seeds + synthetic AI variants + post-LLM exploration set)
  - Feature Extraction: 31 psycholinguistic features via (1) statistical computation (emoji density, TTR) and (2) proxy-LLM rubric evaluation
  - Classification Head: Tree-based ensemble (CatBoost/XGBoost/GBC) with cross-entropy loss
  - Interpretation Module: Feature importance via tree splits + SHAP analysis for engagement correlation

- Critical path:
  1. Load and validate dataset splits (pre-LLM human seeds → synthetic generation → post-LLM exploration)
  2. Extract 31 features per sample (batch proxy-LLM calls with CoT verification)
  3. Train classifier on labeled training/validation set
  4. Evaluate on three tasks (binary, 6-way provider, 17-way model)
  5. Run SHAP analysis to link features → engagement metrics

- Design tradeoffs:
  - Interpretability vs. raw accuracy: Tree models sacrifice ~1% accuracy vs. RoBERTa on binary task (89.62% vs 89.52%) but provide explicit feature importance
  - Feature orthogonality: Average pairwise correlation 0.188 confirms low redundancy, but 6 pairs exceed |r|>0.7—may indicate construct overlap
  - Temporal coverage: Pre-LLM seeds guarantee human labels, but exploration set lacks ground truth—limits causal claims about AI adoption trends

- Failure signatures:
  - Feature collapse: If AI text begins mimicking human imperfections, top features (Prosodic Rhythm, Imperfection) lose discriminative power—monitor via distribution drift
  - Proxy-LLM bias: If evaluation LLM systematically over/under-scores certain domains, feature quality degrades—validate against human annotations per domain
  - Provider overfitting: High 17-way accuracy may memorize model-specific artifacts rather than provider-level patterns—test zero-shot recall on held-out models

- First 3 experiments:
  1. Reproduce ablation: Train PLAD with each dimension removed; verify Lexical Identity causes largest drop (expected: 17-way F1 ~27 vs ~36 full)
  2. Zero-shot test: Exclude Gemini-2.5 from training, measure recall on unseen model (target: ~58% per Table 3); compare to BERT baseline (~52%)
  3. Engagement correlation: Train regressors for likes/comments/collections using PLAD features; confirm SHAP identifies Punctuation Ratio for likes, Word Frequency Entropy for collections, Interactive Stance for comments

## Open Questions the Paper Calls Out

### Open Question 1
Does the combination of human creativity with AI assistance causally increase engagement, or are AI-augmented authors simply more skilled content creators? The analysis is observational and correlational. The 3.5% of AI-augmented authors may possess other characteristics (experience, follower base, content quality) that drive engagement rather than the AI-human combination itself. A controlled experiment where the same authors create content under three conditions (human-only, AI-only, AI-augmented) with randomized assignment would resolve this.

### Open Question 2
How do platform-level governance measures causally affect AI content adoption trends on social media? The temporal correlation between governance events and AI content decline is observed but not experimentally validated. External confounders (seasonal patterns, algorithmic changes, user fatigue) could explain the plateaus. A quasi-experimental design comparing multiple platforms with different governance policies would resolve this.

### Open Question 3
To what extent do the identified psycholinguistic features generalize across languages and cultural contexts beyond Chinese social media? The dataset is exclusively from Xiaohongshu, a Chinese platform, with no cross-linguistic or cross-cultural validation. Different languages have distinct linguistic markers for AI-generated text. Replication studies applying PLAD to multilingual datasets from platforms like Twitter/X, Instagram, or TikTok across different linguistic and cultural contexts would resolve this.

### Open Question 4
What are the long-term co-evolutionary dynamics between human and AI-generated content styles on social media platforms? The current analysis is limited to a 5-year snapshot. Whether AI content is influencing human writing styles (or vice versa) remains unexamined due to lack of longitudinal stylistic tracking of individual authors over time. Longitudinal analysis tracking linguistic feature evolution in the same authors before and after AI tool adoption would resolve this.

## Limitations
- Proxy LLM Reliability: The framework depends on a proxy LLM with Chain-of-Thought verification for semantic feature extraction, but the specific model identity, configuration, and prompt templates are not disclosed.
- Ground Truth Limitations: While pre-LLM human seeds provide verified human labels, the post-LLM exploration set lacks ground truth annotations, limiting validation of temporal adoption claims.
- Feature Definition Gaps: The paper describes 31 features by name and importance ranking but does not provide complete extraction formulas or implementation details.

## Confidence
- High Confidence: Binary classification results (88.78% accuracy) and ablation study findings showing Lexical Identity dimension's importance (17-way F1: 36.16→26.81 when removed).
- Medium Confidence: Zero-shot generalization claims for unseen models are demonstrated on Gemini-2.5 but require additional validation on truly novel models.
- Medium Confidence: Engagement pattern findings (AI-augmented authors showing highest engagement) are based on correlational analysis without controlling for confounding variables.

## Next Checks
1. Reproduce Feature Extraction: Implement all 31 psycholinguistic features using the provided names and descriptions, focusing on verifying the proxy LLM semantic feature computation against reported feature distributions and pairwise correlations.
2. Test Zero-Shot Detection: Hold out additional models not seen during training (e.g., Claude 3.5, Grok 3) to evaluate PLAD's ability to generalize to truly novel models beyond the single Gemini-2.5 test case.
3. Validate Engagement Correlations: Using SHAP analysis, verify which features most strongly correlate with likes, comments, and collections in the dataset, comparing against the reported findings (Punctuation Ratio for likes, Word Frequency Entropy for collections, Interactive Stance for comments).