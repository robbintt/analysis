---
ver: rpa2
title: 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement
  Learning'
arxiv_id: '2506.04207'
source_url: https://arxiv.org/abs/2506.04207
tags:
- reasoning
- multimodal
- arxiv
- zhang
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing sophisticated
  multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs). The
  core method introduces a three-stage training curriculum: (1) a text-centric cold-start
  phase using high-difficulty textual data to establish foundational reasoning, (2)
  a multimodal reinforcement learning (RL) stage enhanced by a novel Prioritized Advantage
  Distillation (PAD) algorithm to mitigate gradient stagnation, and (3) a text-only
  RL refinement phase to consolidate reasoning and linguistic fluency.'
---

# Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.04207
- **Source URL:** https://arxiv.org/abs/2506.04207
- **Reference count:** 40
- **Primary result:** Three-stage training curriculum (text cold start → multimodal RL → text RL) achieves 53.1% average across multimodal and textual reasoning benchmarks, state-of-the-art for open-source 7B MLLMs.

## Executive Summary
This paper addresses the challenge of developing sophisticated multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) through a novel three-stage training curriculum. The approach begins with a text-centric cold start using high-difficulty textual data to establish foundational reasoning patterns, followed by multimodal reinforcement learning enhanced with a Prioritized Advantage Distillation (PAD) algorithm to mitigate gradient stagnation, and concludes with text-only reinforcement learning to consolidate reasoning and linguistic fluency. The resulting ReVisual-R1 model demonstrates state-of-the-art performance among open-source 7B MLLMs, scoring 53.1% average across challenging multimodal and textual reasoning benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, AIME24, AIME25, GPQA, and MATH500.

## Method Summary
The method employs a three-stage curriculum: (1) Text-only cold start with 283K carefully curated textual samples using LLaMA Factory (lr=2e-5, 5 epochs, 32K sequence length), (2) Multimodal reinforcement learning (MRL) with GRPO enhanced by PAD algorithm on 26K multimodal samples (filtering zero-advantage samples via |A| thresholds and prioritizing via temperature-controlled softmax), and (3) Text-only reinforcement learning (TRL) with vision tower frozen to preserve visual grounding while refining linguistic expression. The PAD algorithm addresses gradient stagnation in GRPO by computing |Â| for each sequence, filtering samples where |Â| < T_low, and prioritizing remaining samples through Softmax(τ) sampling. Training uses Qwen2.5-VL-7B-Instruct as base, EasyR1 framework, 8× A100-80G hardware, and achieves convergence through 200K total updates.

## Key Results
- ReVisual-R1 achieves 53.1% average across multimodal and textual reasoning benchmarks, state-of-the-art for open-source 7B MLLMs.
- Staged training (MRL→TRL) outperforms alternatives: 49.6 avg vs. 45.5 for TRL→MRL and 47.6 for mixed training.
- Text-only cold start with complex reasoning data (DeepMath) establishes stronger foundational reasoning than multimodal cold start, with average pass rates of 75% vs. 96% across datasets.
- PAD algorithm successfully mitigates gradient stagnation in multimodal RL, enabling stable training convergence where vanilla GRPO fails.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A text-only cold start with complex reasoning data establishes stronger foundational reasoning patterns for multimodal tasks than multimodal cold start data.
- **Mechanism:** Text-only datasets contain longer reasoning chains (avg. 8,208 tokens vs. 821 for multimodal datasets) with lower pass rates (75% vs. 96%), requiring more deliberative reasoning. This complexity trains abstract reasoning before visual grounding is introduced.
- **Core assumption:** Complex reasoning patterns learned in the text domain transfer to multimodal reasoning.
- **Evidence anchors:** Section 3.1 states current multimodal cold start datasets may lack sufficient complexity; Vision-R1 explores similar RL for MLLMs but doesn't emphasize text-centric cold start.

### Mechanism 2
- **Claim:** Prioritized Advantage Distillation (PAD) mitigates gradient stagnation in multimodal RL by filtering zero-advantage samples and re-weighting informative trajectories.
- **Mechanism:** PAD filters samples where |Â| < T_low and prioritizes remaining samples via temperature-controlled softmax sampling when GRPO produces uniform rewards across groups.
- **Core assumption:** Non-zero advantage samples provide more valuable learning signals than zero-advantage samples.
- **Evidence anchors:** Abstract notes standard GRPO suffers from gradient stagnation; Section 4.1.1 explains uniform rewards cause null advantages and zero policy gradients.

### Mechanism 3
- **Claim:** Staging multimodal RL before text-only RL yields better multimodal reasoning than mixed training or reverse ordering.
- **Mechanism:** MRL grounds textual reasoning in visual perception; subsequent TRL refines linguistic expression while freezing the vision tower to prevent catastrophic forgetting.
- **Core assumption:** Visual grounding should be established before intensive textual refinement; the model retains visual capabilities during text-only optimization.
- **Evidence anchors:** Table 3 shows CS+MRL+TRL achieves 49.6 avg vs. CS+TRL+MRL 45.5 avg vs. CS+Mixed-RL 47.6 avg.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The paper builds on GRPO; understanding its advantage computation clarifies why PAD is necessary.
  - **Quick check question:** For a group of 4 responses with rewards [1, 0, 0, 0], what are the normalized advantages?

- **Concept: Advantage Functions in RL**
  - **Why needed here:** PAD operates directly on advantage estimates to filter and prioritize samples.
  - **Quick check question:** If all samples in a group have reward 1, what happens to their advantages and why does this cause gradient stagnation?

- **Concept: Cold Start vs. RL Training**
  - **Why needed here:** The paper's central finding is that cold start design critically shapes downstream RL effectiveness.
  - **Quick check question:** Why might text-only cold start outperform multimodal cold start even for multimodal benchmarks?

## Architecture Onboarding

- **Component map:** Base Qwen2.5-VL-7B-Instruct → Text-only cold start (283K samples) → Multimodal RL with PAD (26K samples) → Text-only RL (30K samples, frozen vision)

- **Critical path:** Data curation (GRAMMAR) → Text-only cold start → MRL with PAD → TRL with frozen vision

- **Design tradeoffs:**
  - Text vs. multimodal cold start: complexity vs. modality-specific patterns
  - Staged vs. mixed RL: performance vs. training simplicity
  - PAD thresholds: gradient quality vs. data utilization
  - Temperature decay: exploration vs. exploitation

- **Failure signatures:**
  - Cold start failure: short chains, no self-correction
  - Gradient stagnation: reward plateaus, near-zero advantages
  - TRL forgetting: MRL scores drop after TRL
  - PAD over-filtering: very small batch sizes, slow training

- **First 3 experiments:**
  1. **Cold start ablation:** Compare text-only vs. multimodal cold start on the same base model before RL; measure response lengths and benchmark scores.
  2. **PAD validation:** Train MRL with PAD vs. vanilla GRPO; track gradient magnitudes, convergence speed, and final scores.
  3. **Stage ordering test:** Compare MRL→TRL vs. TRL→MRL vs. mixed training on identical data; measure per-modality retention.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical foundation explaining why text-centric optimization strategies, particularly the cold start phase, effectively enhance multimodal reasoning capabilities?
- **Basis in paper:** Section 6.3 states "a deeper theoretical account is needed to explain why these text-centric optimization strategies, such as the cold start, effectively enhance model reasoning capabilities, and to provide a more principled explanation for the staged RL methodology."
- **Why unresolved:** The paper demonstrates empirical gains but provides no formal theoretical framework explaining the transfer mechanism from text-only training to multimodal reasoning domains.
- **What evidence would resolve it:** A formal theoretical model with mathematical analysis explaining cross-modal transfer, or causal mechanistic studies identifying the representational changes induced by text-centric training.

### Open Question 2
- **Question:** Does the three-stage training curriculum with PAD scale effectively to larger architectures (MoE models, 70B+ parameters) and other foundational model families beyond Qwen?
- **Basis in paper:** Section 6.3 notes "scalability evidence is limited to mid-sized models" and "we have not yet validated these advantages across larger-scale architectures, such as MoE models, or generalized them to other foundational model families."
- **Why unresolved:** All experiments were conducted only on Qwen2.5-VL-3B and 7B models.
- **What evidence would resolve it:** Successful replication of the methodology on MoE architectures, 70B+ scale models, and alternative MLLM families (e.g., LLaVA-based) with comparable or improved gains.

### Open Question 3
- **Question:** What is the optimal curriculum recipe for the ratio and quality of multimodal versus textual data during cold start and RL phases across different skill clusters?
- **Basis in paper:** Section 6.3 states the need to "systematically investigate the interplay between data type and training stage" and "determine an optimal curriculum recipe that jointly maximizes performance across diverse skill clusters, notably in STEM reasoning, perceptual grounding, and general domains."
- **Why unresolved:** The paper uses fixed data compositions without systematic exploration of alternative configurations.
- **What evidence would resolve it:** Large-scale ablation studies varying data ratios, difficulty distributions, and modality mixes, with analysis of performance across different benchmark categories.

### Open Question 4
- **Question:** What are the underlying mechanisms causing the MRL→TRL ordering to outperform alternative training orderings and mixed-RL approaches?
- **Basis in paper:** Table 3 shows MRL→TRL (49.6 avg) substantially outperforms TRL→MRL (45.5 avg) and Mixed-RL (47.6 avg). The paper states this "affirms our core hypothesis" but provides only high-level intuition without mechanistic analysis.
- **Why unresolved:** The empirical preference for one ordering is demonstrated but not explained at the level of model internals or learning dynamics.
- **What evidence would resolve it:** Mechanistic interpretability studies analyzing representational changes during different stage orderings, or learning dynamics analysis showing how grounding-first versus refinement-first strategies affect gradient flow and feature acquisition.

## Limitations
- The effectiveness of text-only cold start for multimodal reasoning relies on the assumption that complex textual reasoning patterns transfer to multimodal contexts, but this transfer is not empirically validated in isolation.
- PAD's hyperparameters (T_low, T_high, τ schedule, ρ) are not specified, making it difficult to assess whether improvements are due to algorithm design or careful hyperparameter tuning.
- The staged training approach assumes visual grounding learned during MRL is preserved during TRL when the vision tower is frozen, but doesn't provide ablations showing what happens without freezing.

## Confidence

- **High confidence:** The overall experimental methodology is sound, with clear ablation studies comparing different training stages and baselines. The benchmark results are reproducible given the specified hardware and model checkpoints.
- **Medium confidence:** The core claims about text-only cold start superiority and PAD effectiveness are supported by ablation studies, but the underlying mechanisms lack direct empirical validation.
- **Low confidence:** The paper doesn't address potential overfitting to the GRAMMAR dataset or provide evidence that the learned reasoning generalizes to truly unseen problem types beyond the reported benchmarks.

## Next Checks

1. **Transfer validation:** Train a model with text-only cold start followed by text-only RL (skipping MRL entirely) and test on multimodal benchmarks to quantify how much visual grounding comes from the base model versus learned during MRL.

2. **PAD sensitivity analysis:** Systematically vary PAD's temperature schedule, filtering thresholds, and subsampling ratio to determine which components contribute most to performance gains versus simply being important hyperparameters.

3. **Catastrophic forgetting test:** Compare MathVista/MathVision performance before and after TRL when the vision tower is frozen versus when it's allowed to update, to quantify how much visual reasoning capability is preserved versus forgotten during text-only refinement.