---
ver: rpa2
title: 'NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and Phenotypic
  Text'
arxiv_id: '2503.21964'
source_url: https://arxiv.org/abs/2503.21964
tags:
- text
- attention
- image
- brain
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NeuroLIP, a novel cross-modal contrastive
  learning framework that integrates fMRI connectivity data with phenotypic textual
  descriptors while addressing two key challenges: interpretability and fairness.
  The method employs text token-conditioned attention (TTCA) and cross-modal alignment
  via localized tokens (CALT) to align brain region-level embeddings with disease-related
  phenotypic tokens, enabling token-level attention maps that reveal brain region-disease
  associations.'
---

# NeuroLIP: Interpretable and Fair Cross-Modal Alignment of fMRI and Phenotypic Text

## Quick Facts
- arXiv ID: 2503.21964
- Source URL: https://arxiv.org/abs/2503.21964
- Authors: Yanting Yang; Xiaoxiao Li
- Reference count: 29
- Primary result: AUC of 72.43% on ABIDE and 62.74% on ADHD-200 while achieving DPD of 7.92% on ABIDE and 8.59% on ADHD-200

## Executive Summary
NeuroLIP is a novel cross-modal contrastive learning framework that aligns fMRI connectivity data with phenotypic textual descriptors for neurodisorder classification. The method addresses two key challenges: interpretability and fairness. By employing text token-conditioned attention (TTCA) and cross-modal alignment via localized tokens (CALT), NeuroLIP enables token-level attention maps that reveal brain region-disease associations while mitigating bias through sensitive attribute disentanglement. Experiments on ABIDE and ADHD-200 datasets demonstrate superior performance compared to baseline methods while maintaining equitable predictions across demographic subgroups.

## Method Summary
NeuroLIP integrates fMRI connectivity data with phenotypic textual descriptors through a cross-modal contrastive learning framework. The method preserves regional brain embeddings and aligns them with individual text tokens using TTCA, which computes cross-attention between local text tokens and local image tokens. For fairness, NeuroLIP incorporates a loss for sensitive attribute disentanglement that maximizes attention distance between disease tokens and sensitive attribute tokens, along with a negative gradient technique that reverses CALT loss on sensitive attributes. The overall loss combines global contrastive loss, sensitive attribute-specific losses with gradient reversal, and attention distance loss to optimize both classification performance and fairness metrics.

## Key Results
- Achieved AUC of 72.43% on ABIDE and 62.74% on ADHD-200 datasets
- Improved fairness metrics with DPD of 7.92% on ABIDE and 8.59% on ADHD-200
- Demonstrated superior performance compared to baseline methods including BrainNetTF, CLIP, SigLIP, and FairCLIP
- Qualitative attention map visualization revealed neuroanatomical patterns aligned with diagnostic characteristics, validated by neuroscientific literature

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Cross-Modal Attention for Fine-Grained Alignment
Text Token-Conditioned Attention (TTCA) computes cross-attention between local text tokens as queries and local image tokens as keys/values, creating an attention matrix that explicitly links each text token to each brain region. This enables interpretable brain region-disease associations that global embedding approaches cannot provide, as disease-relevant information is spatially localized in specific brain regions.

### Mechanism 2: Negative Gradient Reversal for Sensitive Attribute Disentanglement
The overall loss includes sign(a) = -1 for sensitive attributes, which inverts the CALT sigmoid loss. Instead of minimizing distance between sensitive attribute tokens and brain regions, the model maximizes it, actively breaking spurious correlations between sensitive attributes and disease labels.

### Mechanism 3: Attention Distance Maximization for Bias Reduction
The attention loss computes Euclidean distance between attention weight vectors for disease tokens versus sensitive tokens. Minimizing the negative of this distance maximizes the separation between disease-relevant brain regions and sensitive-attribute-encoding regions, reducing unintended correlations in downstream predictions.

## Foundational Learning

- **Concept: Cross-Modal Contrastive Learning (CMCL)**: Why needed here - NeuroLIP extends CLIP-style image-text alignment to brain-region-token alignment; understanding contrastive loss and embedding space structure is prerequisite. Quick check: Given image embeddings v_i, v_j and text embeddings t_i, t_j, can you explain why maximizing ⟨v_i, t_i⟩ while minimizing ⟨v_i, t_j⟩ for i≠j learns aligned representations?

- **Concept: Multi-Head Self-Attention (MHSA) with Q/K/V Projections**: Why needed here - TTCA layer uses query (text tokens), key, and value (image tokens) projections; attention weight interpretation depends on understanding this mechanism. Quick check: If A = softmax(QK^T/√d), what does a high value of A[s,n] mean in terms of similarity between the s-th text token and n-th brain region?

- **Concept: Fairness Metrics (DPD, DEOdds, Equity-Scaled AUC)**: Why needed here - Paper optimizes Demographic Parity Difference and Difference in Equalized Odds; understanding these metrics is necessary to interpret results and tradeoffs. Quick check: Why might minimizing DPD (equalizing positive prediction rates across groups) conflict with maintaining high AUC if disease prevalence differs across groups?

## Architecture Onboarding

- **Component map**: fMRI connectivity matrix I ∈ R^(N×N) → brain node embeddings → MHSA → DEC module → local tokens v_loc ∈ R^(M×E), global token v_g ∈ R^E; Phenotypic string → text encoder → local tokens t_loc ∈ R^(S×E), global token t_g ∈ R^E; TTCA cross-attention → token-conditioned image embeddings v_ttca + attention matrix A ∈ R^(S×N); CALT loss with sign(a) → align disease tokens, reverse gradient for sensitive attributes; L_attn → maximize disease-sensitive attention separation; Classification from global tokens v_g, t_g

- **Critical path**: 1) fMRI preprocessing → connectivity matrix (PCC between ROI time series) 2) Vision/text encoding → local and global token embeddings 3) TTCA cross-attention → region-token alignment matrix A 4) CALT loss with sign(a) → align disease tokens, reverse gradient for sensitive attributes 5) L_attn → maximize disease-sensitive attention separation 6) Aggregate loss L = L_global + Σ_a sign(a)L_calt_a − βL_attn

- **Design tradeoffs**: Local vs. global tokens (preserving local v_loc enables interpretability but increases memory), β hyperparameter (set to 0.001; controls attention loss strength), sensitive attribute selection (domain knowledge required), assumption that gradient reversal assumes sensitive attributes are noise rather than signal

- **Failure signatures**: Attention weights uniform or collapsing to single region, DPD increases despite negative gradient + L_attn, classification AUC drops >5% with fairness components, attention maps inconsistent across CV folds

- **First 3 experiments**: 1) Reproduce Table 1 baseline comparison on ABIDE: Train NeuroLIP with 5-fold CV, compare AUC/DPD vs. BrainNetTF, CLIP, SigLIP, FairCLIP. Verify AUC ≈72.43%, DPD ≈7.92%. 2) Ablation study (Table 2): Systematically remove L_attn and negative gradient. Confirm removing L_attn worsens DPD (11.60 vs. 7.92), removing negative gradient worsens DPD (9.36 vs. 7.92), full model balances AUC and DPD best. 3) Attention visualization validation (Fig. 2): For ASD-positive tokens, extract A_disease and visualize on brain. Compare highlighted regions against Neurosynth meta-analysis for "autism." Confirm qualitative agreement with cited literature.

## Open Questions the Paper Calls Out

### Open Question 1
Can NeuroLIP maintain its superior fairness and classification performance when scaled to significantly larger, multi-site neuroimaging cohorts beyond ABIDE and ADHD-200? The current study validates the method on two specific datasets with relatively limited sample sizes, leaving scalability unproven. What evidence would resolve it: Successful application and benchmarking of NeuroLIP on large-scale datasets (e.g., UK Biobank) demonstrating consistent DPD and AUC metrics across a wider range of demographics and scanner protocols.

### Open Question 2
Does the negative gradient technique or attention distance loss unintentionally degrade the quality of disease-relevant feature representations when sensitive attributes are correlated with the disease pathology? The method enforces disentanglement via Lattn and negative gradients, but in medical data, biological sex or age often correlates with physiological differences. What evidence would resolve it: A comparative analysis of the semantic richness of the latent space to ensure diagnostic biological markers are preserved while sensitive markers are removed.

### Open Question 3
How sensitive is the text token-conditioned attention (TTCA) to the specific phrasing or ordering of the phenotypic text descriptors? The paper constructs text using a specific template but does not test the robustness of the text encoder to variations in this input structure. What evidence would resolve it: An ablation study measuring performance changes when the input text format is permuted, truncated, or injected with synonymic variations to test the robustness of the cross-modal alignment.

## Limitations

- Encoder architectures unspecified: Vision and text encoders are only referenced without architectural details, preventing exact reproduction and affecting interpretability of attention mechanisms
- Embedding dimensions unclear: Key hyperparameters like embedding dimension E, number of MHSA heads, and local token count M are not specified, limiting faithful replication
- Single dataset focus: Results may not generalize to other neurodisorders or clinical populations with different demographic distributions
- Sensitive attribute selection: Choice of SRS for ABIDE and sex for ADHD-200 is arbitrary—different sensitive attributes may yield different fairness-accuracy tradeoffs

## Confidence

- **High Confidence**: Core classification performance (AUC: 72.43% ABIDE, 62.74% ADHD-200) and fairness metrics (DPD: 7.92% ABIDE, 8.59% ADHD-200) are well-supported by ablation studies and comparative analysis against baselines
- **Medium Confidence**: Interpretability claims (attention maps aligning with Neurosynth meta-analyses) rely on qualitative validation; quantitative validation of brain-region-disease associations is limited
- **Medium Confidence**: Fairness mechanism (negative gradient + attention distance maximization) shows measurable improvements in DPD but may not generalize to all sensitive attributes or clinical contexts

## Next Checks

1. **Reproduce baseline comparison on ABIDE**: Train NeuroLIP with 5-fold CV, compare AUC/DPD vs. BrainNetTF, CLIP, SigLIP, FairCLIP. Verify target metrics (AUC ≈72.43%, DPD ≈7.92%) are achievable with reasonable encoder architectures.

2. **Cross-attribute fairness validation**: Replace SRS with age as sensitive attribute in ABIDE. Test whether negative gradient reversal still improves fairness without harming classification, revealing generalizability of the disentanglement mechanism.

3. **Attention map stability analysis**: For ASD-positive tokens, extract attention matrices across 5 CV folds. Quantify consistency of highlighted regions (e.g., superior parietal, occipital gyri) using spatial overlap metrics. Confirm that attention maps are stable and reproducible rather than noisy.