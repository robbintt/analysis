---
ver: rpa2
title: 'SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision
  LLM Weights'
arxiv_id: '2509.22944'
source_url: https://arxiv.org/abs/2509.22944
tags:
- quantization
- sinq
- matrix
- methods
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SINQ introduces a novel approach to post-training quantization
  for large language models that significantly reduces perplexity degradation at low
  bit-widths. The key innovation is adding a second-axis scale factor to quantization
  parameters and using a fast Sinkhorn-Knopp-style algorithm to normalize per-row
  and per-column variances, effectively approximating activation-aware quantization
  without requiring calibration data.
---

# SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights

## Quick Facts
- **arXiv ID**: 2509.22944
- **Source URL**: https://arxiv.org/abs/2509.22944
- **Reference count**: 35
- **Primary result**: SINQ reduces perplexity degradation by >50% compared to uncalibrated uniform quantization baselines on WikiText2 and C4 benchmarks at 3-4 bit precision

## Executive Summary
SINQ introduces a novel post-training quantization method for large language models that achieves calibration-free activation-aware quantization quality. The key innovation is a dual-scale parameterization (row and column scales) combined with a fast Sinkhorn-Knopp-style algorithm that normalizes per-row and per-column variances. This approach leverages the inherent correlation between weight matrix structure and typical activation magnitudes learned during training, effectively approximating activation-aware quantization without requiring calibration data. SINQ achieves over 50% reduction in perplexity gap compared to uncalibrated uniform quantization baselines while maintaining architecture-agnostic properties and negligible computational overhead.

## Method Summary
SINQ modifies standard post-training quantization by adding a second-axis scale factor to quantization parameters, creating a two-dimensional scale system where weights are approximated as W_approx = s ⊙ Q ⊙ t (with shifts). The method uses a fast Sinkhorn-Knopp-style algorithm that iteratively normalizes per-row and per-column variances to find optimal scales. This normalization process recovers column scales from the weight matrix structure that are predictive of typical activation magnitudes, approximating activation-aware quantization without forward passes. The algorithm converges to a balanced state where both row and column standard deviations are approximately equal, minimizing quantization error redistribution across the matrix.

## Key Results
- SINQ with RTN-INT4 reduces perplexity degradation by >50% compared to RTN-INT4 baseline on WikiText2 and C4 benchmarks
- SINQ with 3-bit quantization (RTN, HQQ, NF4 backends) matches or exceeds AWQ-4bit quality without requiring calibration data
- No-overhead SINQ variant achieves comparable quality to standard SINQ with only minor accuracy loss while maintaining inference speed
- SINQ maintains architecture-agnostic properties and can be combined with calibration or non-uniform quantization for further improvements

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Activation-Awareness from Weight-Activation Correlation
During Adam training, weight updates form as outer products of inputs and gradients. This induces an inverse relationship: columns receiving smaller inputs develop larger weights (to compensate), yielding higher per-column variance. By reading this variance pattern post-hoc, SINQ recovers a proxy for activation scales without forward passes. The training process consistently produces this weight-activation correlation across architectures, enabling calibration-free activation-aware quantization.

### Mechanism 2: Dual-Scale Redistribution of Outlier Errors
With single-axis scaling, a large outlier element forces a large scale for its entire row/column, reducing precision for all other elements. Dual scales (s ⊙ Q ⊙ t) allow independent adjustment: scale down the outlier's column, scale up its row, redistributing quantization error to where it's less harmful. This prevents single-axis outliers from dominating and enables more balanced error distribution across the matrix.

### Mechanism 3: Sinkhorn Iteration Prevents Kurtosis Blowup
Direct column scaling (by inverse std.dev to mimic activation-awareness) improves column balance but creates new row-wise outliers, evidenced by increased kurtosis. The Sinkhorn-inspired algorithm alternates between normalizing row and column standard deviations, converging to a balanced state where both are approximately equal. This balanced variance state minimizes the matrix imbalance metric and correlates with better quantization outcomes than optimizing either axis alone.

## Foundational Learning

- **Concept: Post-training quantization (PTQ) fundamentals** - Why needed: SINQ is a PTQ method; you need to understand scales, shifts, tiling, and round-to-nearest before appreciating the dual-scale modification. Quick check: Given a weight matrix tile, can you compute the scale and zero-point for symmetric and asymmetric int4 quantization?

- **Concept: Sinkhorn-Knopp algorithm** - Why needed: SINQ adapts this classic matrix balancing algorithm; understanding its iterative normalization logic is essential for implementing and debugging the scale search. Quick check: For a non-negative matrix, how does Sinkhorn iteration alternate between row and column normalization to converge to a doubly-stochastic matrix?

- **Concept: Activation-aware quantization (AWQ intuition)** - Why needed: SINQ's goal is to approximate AWQ's benefits without calibration data; knowing what AWQ optimizes for helps you understand what SINQ is trying to recover from weight structure alone. Quick check: In AWQ, why does scaling columns by the inverse of average input magnitude improve quantized output quality?

## Architecture Onboarding

- **Component map**: Weight matrix W → Sinkhorn normalizer → produces row scales s, column scales t → normalized matrix Ŵ → existing quantizer (RTN, HQQ, NF4, etc.) → produces Q, z, quantized scales → Dequantization: W_approx = s ⊙ (Q + z) ⊙ t

- **Critical path**: 1) Extract per-row and per-column std. devs from weight matrix 2) Run K iterations of Algorithm 1 updating log-scales to minimize imbalance I(W) 3) Track best imbalance across iterations 4) Apply final scales, quantize using chosen backend 5) At inference: apply column scale t to input activations (or absorb into preceding layer)

- **Design tradeoffs**: Standard SINQ vs. no-overhead SINQ (adds one element-wise multiply per linear layer vs. requires scale sharing constraints); with/without shifts (shifts improve Pareto front but add auxiliary parameters); auxiliary precision (quantized int8 vs. fp16 auxiliaries trade memory for flexibility); calibration-free vs. A-SINQ (adding AWQ calibration provides marginal gains but requires data and ~8× longer quantization time)

- **Failure signatures**: High perplexity gap vs. baseline (>50% of RTN gap) → check Sinkhorn convergence and scale application order; increased flip rates on specific tasks → some layer groups may need different iteration counts or step-size bounds; numerical instability in Algorithm 1 → ensure log-space computation and clamping; no-overhead variant degrades quality → verify scale sharing is correct for your architecture

- **First 3 experiments**: 1) Reproduce Table 1 (4-bit, Qwen3-14B): Implement SINQ with group size 64, K=20 iterations, dual-scale + shift. Compare perplexity against RTN and HQQ baselines. Target: C4 perplexity within 0.1 of reported 12.21. 2) Ablation on iteration count K: Run SINQ with K ∈ {5, 10, 20, 50, 100} on Qwen3-1.7B. Plot final imbalance I(W) and perplexity vs. K to validate diminishing returns. 3) No-overhead variant integration: Modify a GGUF export pipeline to include no-overhead SINQ preprocessing. Measure perplexity delta and verify no inference slowdown (compare to Table 9 patterns).

## Open Questions the Paper Calls Out

### Open Question 1
Does the correlation between column-wise weight standard deviations and input activation magnitudes hold across diverse training regimes (e.g., different optimizers, learning rate schedules, or architectural variants like state-space models)? The paper establishes this correlation empirically and provides theoretical justification based on Adam training in a simplified single-layer setting, but does not systematically validate across varied training configurations. Systematic evaluation across models trained with different optimizers, regularization techniques, and non-transformer architectures would resolve this.

### Open Question 2
Can the no-overhead SINQ variant achieve parity with standard SINQ when multiple layers must share a single input scale? The paper notes that scale sharing forces suboptimal quantization for individual layers, but the optimal strategy for scale-sharing groups is not investigated. Ablation studies optimizing scale grouping strategies would resolve this tradeoff.

### Open Question 3
Why does SINQ with uniform INT4 quantization sometimes outperform SINQ with non-uniform NF4 quantization (e.g., on Qwen3-32B)? Non-uniform quantization typically better matches weight distributions, but this reversal suggests SINQ's normalization may alter the distribution in ways that favor uniform levels. Analysis of weight distributions before/after SINQ normalization would clarify this.

## Limitations

- The weight-activation correlation assumption, while empirically validated, may not hold across all training regimes (different optimizers, architectures, or regularization strategies)
- The choice of Sinkhorn iteration count K and step-size bounds s_min/s_max are left to implementation details, creating potential reproducibility gaps
- Some architectural constraints (e.g., shared scales for Q/K/V layers) may limit the effectiveness of the no-overhead variant

## Confidence

- **High confidence**: The dual-scale parameterization and error-redistribution mechanism are well-supported by theoretical formulation (Eq. 2-3) and empirical evidence (Fig. 1, Table 1)
- **Medium confidence**: The weight-activation correlation mechanism is theoretically plausible and supported by Fig. 2a showing strong correlation in studied models, but the single-layer derivation may not fully capture multi-layer dynamics
- **Medium confidence**: The Sinkhorn iteration preventing kurtosis blowup is well-demonstrated empirically (Fig. 2c shows kurtosis control), but the theoretical justification for why balanced variances minimize quantization error could be more rigorous

## Next Checks

1. **Optimizer sensitivity test**: Apply SINQ to models trained with different optimizers (SGD, AdamW, LAMB) and learning rate schedules. Compare the weight-activation correlation strength (σ_W vs. 1/√|s_x|) and resulting quantization quality to validate the foundational assumption across training regimes.

2. **Layer-wise correlation analysis**: For each layer in a deep transformer, compute the Pearson correlation between column-wise weight standard deviations and average input magnitudes (from a small calibration set). Plot these correlations across layers to identify systematic patterns or failure modes in specific layer types.

3. **Extreme outlier stress test**: Construct synthetic weight matrices with varying outlier distributions (isolated extreme values vs. systematic channel-level anomalies) and apply SINQ with different iteration counts. Measure resulting row/column kurtosis and quantization error to characterize robustness boundaries and identify failure conditions for the dual-scale redistribution mechanism.