---
ver: rpa2
title: Improving annotator selection in Active Learning using a mood and fatigue-aware
  Recommender System
arxiv_id: '2507.23756'
source_url: https://arxiv.org/abs/2507.23756
tags:
- performance
- annotators
- fatigue
- annotator
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of selecting the best annotators
  for each query in Active Learning to minimize misclassifications. It proposes a
  Knowledge-Based Recommendation System (RS) that ranks available annotators based
  on their past accuracy, mood, and fatigue levels, as well as information about the
  queried instance.
---

# Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System

## Quick Facts
- **arXiv ID:** 2507.23756
- **Source URL:** https://arxiv.org/abs/2507.23756
- **Reference count:** 40
- **Primary result:** A knowledge-based RS that ranks annotators using past accuracy, mood, and fatigue outperforms baselines in AL annotation accuracy and model F1-score.

## Executive Summary
This paper addresses annotator selection in Active Learning by proposing a Knowledge-Based Recommender System (RS) that leverages annotators' historical performance, current mood, and fatigue levels. The system ranks available annotators for each queried instance to minimize misclassifications. Results show that incorporating internal factors like mood and fatigue reduces annotation errors and model uncertainty compared to ignoring these factors. The proposed method achieves nearly the same performance as an optimized baseline that simulates the annotator's labeling process.

## Method Summary
The study simulates Active Learning with a Random Forest classifier and 30 annotators per batch. Annotators have random base accuracies (~N(75,7)) and are assigned chronotypes (Lion, Bear, Wolf, Dolphin). Mood (1-10 scale) and fatigue (levels increasing every 20 annotations after 50) dynamically influence their performance (±6% per mood unit, 2-4% per fatigue level). The RS ranks annotators using a weighted aggregation of: past accuracy on labels confusing the model, overall past accuracy, and current performance modifiers from mood and fatigue. Uncertainty Sampling selects instances, and the top-ranked annotator provides labels until 1224 annotations or 99% accuracy.

## Key Results
- Incorporating mood and fatigue reduces annotation errors compared to ignoring internal factors.
- The proposed RS improves model accuracy and F1-score during training.
- The proposed method performs nearly as well as an optimized baseline based on simulated annotator performance.

## Why This Works (Mechanism)
The RS improves annotator selection by accounting for dynamic human factors (mood and fatigue) that affect labeling accuracy. By weighting past performance on confusing labels, overall accuracy, and current state, it selects annotators most likely to provide correct labels for the specific instance, reducing model uncertainty and improving training efficiency.

## Foundational Learning
- **Annotator Simulation:** Generating synthetic annotators with base accuracy, chronotype, mood, and fatigue dynamics.
  - *Why needed:* To realistically model human performance variation over time.
  - *Quick check:* Verify fatigue increases every 20 annotations after the 50th and mood fluctuates per chronotype.
- **Performance Modifiers:** Applying mood (±6% per unit) and fatigue (2-4% per level) to base accuracy.
  - *Why needed:* To simulate the impact of internal states on annotation quality.
  - *Quick check:* Confirm mood 8 (vs avg 5) on base 75% yields 93% effective accuracy.
- **Knowledge-Based RS:** Ranking annotators via weighted aggregation of past accuracy, mood, and fatigue.
  - *Why needed:* To select the optimal annotator for each query instance.
  - *Quick check:* Validate RS ranking order on a small test set with known values.

## Architecture Onboarding

**Component Map:**
Annotator Simulation -> Mood/Fatigue Dynamics -> Performance Modifier -> RS Scoring -> Annotator Ranking -> Label Query -> Model Retraining

**Critical Path:**
AL Loop: Uncertainty Sampling → RS Ranking → Annotator Selection → Label Simulation → Model Retraining

**Design Tradeoffs:**
- Simulated annotators enable controlled experiments but may not capture real human nuances.
- Arbitrary weights for mood/fatigue impact based on heuristics rather than empirical data.
- RS balances multiple factors but weight optimization is left for future work.

**Failure Signatures:**
- Incorrect fatigue accumulation (e.g., not resetting per day or incorrect level increments).
- Mood-performance modifier misapplied (e.g., using mood directly instead of deviation from average).
- RS ranking fails to prioritize annotators with high past accuracy on confusing labels.

**3 First Experiments:**
1. Test annotator simulation: Generate 30 annotators, verify base accuracies and chronotype assignments.
2. Validate performance modifiers: Apply mood and fatigue to base accuracy, check results are capped at 100%.
3. Implement and test RS ranking: Score a small set of annotators with known past accuracies, mood, and fatigue; confirm ranking order.

## Open Questions the Paper Calls Out
- Does the RS maintain effectiveness with real human annotators instead of simulated ones?
- How sensitive is performance to the specific weights assigned to mood and fatigue factors?
- Can biometric monitoring provide more accurate data for the RS than self-reported values?

## Limitations
- RS weighting scheme parameters are not explicitly defined, leading to potential reproduction variance.
- Chronotype-mood mappings and fluctuation patterns are vaguely described.
- Optimization baseline's specific search strategy is not detailed.
- Simulation-based approach may not fully capture real human annotation behavior.

## Confidence
- **High Confidence:** Experimental design, AL loop structure, annotator simulation framework, and general RS form.
- **Medium Confidence:** Definition of "Query Type" and RS aggregation logic.
- **Low Confidence:** Exact RS weight values, precise mood patterns per chronotype, and Optimization baseline details.

## Next Checks
1. Validate mood-performance application: For mood 8 (vs avg 5) on base 75%, effective accuracy should be 93% (capped at 100%).
2. Verify fatigue accumulation: Fatigue increases by one level every 20 annotations after the 50th, with 2-4% penalty compounded per level.
3. Cross-check RS ranking: Test RS scoring on a small set of annotators with predefined values; confirm ranking matches weighted aggregation logic.