---
ver: rpa2
title: 'Press Start to Charge: Videogaming the Online Centralized Charging Scheduling
  Problem'
arxiv_id: '2601.12543'
source_url: https://arxiv.org/abs/2601.12543
tags:
- charging
- scheduling
- time
- load
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the online centralized charging scheduling problem
  (OCCSP) for electric vehicles, where a central authority must decide in real time
  when to charge dynamically arriving EVs under capacity constraints to balance load
  across a finite planning horizon. To address this problem, the authors model it
  as a video game environment where charging blocks are placed within temporal and
  capacity constraints on a grid.
---

# Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem

## Quick Facts
- arXiv ID: 2601.12543
- Source URL: https://arxiv.org/abs/2601.12543
- Reference count: 30
- Primary result: Image-based CNN with DAgger reduces load imbalance and system costs in online EV charging scheduling

## Executive Summary
This paper introduces a gamification approach to the online centralized charging scheduling problem (OCCSP) for electric vehicles. The authors transform the EV charging scheduling problem into a video game where charging blocks are placed on a grid within temporal and capacity constraints. They develop an image-to-movement model (I2M) that predicts sequential 3-class actions using a CNN backbone, trained with Dataset Aggregation (DAgger) using an expert oracle. Extensive experiments demonstrate that this approach reduces peak-to-valley load imbalance compared to heuristic and vector-based methods, while theoretical analysis shows tighter generalization bounds than traditional vector representations.

## Method Summary
The approach models OCCSP as a video game environment where EVs arrive dynamically and must be scheduled within capacity constraints. An MIP oracle serves as an expert to generate demonstrations, which are sequences of grid states and movement actions (left, right, down). The I2M model uses a CNN backbone (3 conv layers + 2 linear layers) to predict sequential actions from grid images. Training proceeds via supervised learning followed by DAgger iterations where the agent generates trajectories, the oracle provides corrected actions for visited states, and the dataset is aggregated for retraining.

## Key Results
- I2M-DAgger consistently outperforms heuristic baselines (Row-filling, Smallest Deadline) and vector-based approaches across all tested scenarios
- The gamified approach reduces peak-to-valley load imbalance (Max-Min metric) by significant margins compared to prevailing practice
- Theoretical analysis shows image-based representations yield tighter generalization bounds (VC-dimension O(log T)) than vector-based representations (VC-dimension O(T log T))
- Real-world case study for Greater Montréal Area demonstrates potential to delay costly grid upgrades by reducing system costs by tens of millions annually

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image-based state representation with CNNs yields tighter generalization bounds than vector-based representation with MLPs as planning horizon increases.
- Mechanism: CNNs with global pooling have VC-dimension scaling as O(log T), while MLPs with vector input scale as O(T log T). The convolutional architecture exploits spatial structure in the grid representation.
- Core assumption: Optimal charging policy can be captured by local spatial patterns that convolutions can detect.
- Evidence anchors: Formal proofs in Section 5 (Lemma 1, Theorem 1) comparing Φ_I = Θ(log T) vs Φ_V = Θ(T log T).
- Break condition: If problem requires reasoning about very long-range temporal dependencies that cannot be captured by local convolution kernels.

### Mechanism 2
- Claim: Predicting sequential 3-class movement actions provides better generalization than predicting a T-class schedule-vector in a single shot.
- Mechanism: Natarajan dimension for 3 output classes is substantially smaller than for 96 output classes, yielding provably tighter generalization bounds.
- Core assumption: Effective number of independent decisions per episode satisfies conditions of Theorem 2.
- Evidence anchors: Formal derivation in Section 5 (Theorem 2, Remark 1) showing G_2 < G_1; empirical confirmation in Section 7.1 (Figure 7) with I2M achieving lower Max-Min (33.58) and RMSE (8.43).
- Break condition: If number of movement actions per block is very small or execution requires backtracking, sequential formulation may accumulate errors.

### Mechanism 3
- Claim: DAgger-style iterative expert correction reduces distributional drift and improves policy robustness compared to pure supervised learning.
- Mechanism: SL trains only on oracle trajectories, so when learner visits off-distribution states, it cannot recover. DAgger aggregates expert corrections on learner-visited states, progressively aligning training and deployment distributions.
- Core assumption: Expert provides correct labels for any reachable state; environment allows querying expert at arbitrary states.
- Evidence anchors: DAgger procedure in Section 6.3.3; empirical results in Section 7.2 (Figure 8) showing I2M-DAgger consistently outperforms I2M-SL across all scenarios.
- Break condition: If expert cannot solve completion problem quickly or expert labels are noisy/inconsistent, DAgger may not converge or may amplify errors.

## Foundational Learning

- **Imitation Learning and Distributional Shift**
  - Why needed: Core learning approach trains policies to mimic expert demonstrations; understanding why SL alone fails when learner encounters off-distribution states is essential for appreciating why DAgger is necessary.
  - Quick check: Given a policy trained on expert trajectories, what happens when it makes a small error that leads to a state not in the training distribution?

- **VC Dimension and Natarajan Dimension**
  - Why needed: Theoretical justification for gamification rests on complexity measures that bound generalization error; these concepts explain why certain input/output representations are provably more sample-efficient.
  - Quick check: Why does a hypothesis class with lower VC dimension typically require fewer training examples to achieve the same generalization guarantee?

- **Convolutional Neural Networks for Structured Grids**
  - Why needed: Gamified representation encodes scheduling problem as 2D grid (time × capacity); CNNs exploit spatial locality and translation invariance, which is why they scale more favorably than fully-connected networks.
  - Quick check: If grid width doubles (finer time discretization), how does parameter count of CNN with fixed filter sizes compare to MLP with fully-connected input?

## Architecture Onboarding

- **Component map:**
  EV Arrival Stream → Game Environment: Grid State (T × capacity) → Expert Oracle: MIP Solver → Demonstration Generator: (image, action) pairs → CNN Backbone: 3 conv layers + 2 linear layers → Movement Head: 3-class softmax → DAgger Loop: Policy rollout → Expert query → Data aggregation → Retrain

- **Critical path:**
  1. Implement game environment: 2D grid, block placement, feasible region masking, Tetris-like stacking
  2. Build MIP oracle: Minimize max-min load difference subject to capacity, temporal, and contiguity constraints
  3. Generate expert trajectories by running oracle on sample instances; extract movement sequences from optimal placements
  4. Train initial SL policy on expert data using CNN architecture
  5. Run DAgger: Execute policy to collect states; query expert for corrected actions; aggregate and retrain

- **Design tradeoffs:**
  - Image resolution vs. computational cost: Higher resolution improves granularity but increases image size
  - Number of DAgger iterations vs. expert query budget: More iterations improve robustness but require more oracle calls
  - Movement action set: Only 3 actions (left, right, down) keeps output space small; no explicit "wait" or "rotate" actions

- **Failure signatures:**
  - SL policy stuck in local optima: If Max-Min load difference plateaus well above oracle
  - DAgger divergence: If performance degrades across iterations, expert labels may be inconsistent
  - Capacity violations: If scheduled load exceeds p_cap, action masking or feasibility checking is broken
  - Deadline misses: If EVs are scheduled outside [arrival, departure] windows, forbidden-column logic is incorrect

- **First 3 experiments:**
  1. Baseline sanity check: Implement Row-filling heuristic and Oracle on Scenario 1; verify Oracle achieves lowest Max-Min and Row-filling performs worst
  2. Input representation ablation: Train I2M-SL and V2M-SL on identical data; confirm I2M achieves lower RMSE with non-overlapping confidence intervals
  3. DAgger convergence test: Starting from I2M-SL, run 5 DAgger iterations; plot Max-Min and RMSE per iteration to verify monotonic improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the "OR-as-labeler, ML-for-online" framework be effectively transferred to other combinatorial optimization problems with distinct constraint structures?
- Basis in paper: Conclusion states this pattern is "pragmatic, extensible, and... well worth pursuing in other sequential decision problems."
- Why unresolved: Paper only empirically validates method on specific EV charging domain.
- What evidence would resolve it: Empirical results applying gamified I2M approach to other resource allocation problems (e.g., job-shop scheduling or cloud task allocation) showing competitive performance with domain-specific heuristics.

### Open Question 2
- Question: Can image-to-movement model effectively schedule heterogeneous charging sessions with variable power levels and non-contiguous constraints?
- Basis in paper: Section 3.2 states model assumes "all charging blocks have the same power level u" and Section 3.3 requires charging slots to "form a contiguous sequence."
- Why unresolved: Tetris-like game mechanic relies on uniform block sizes and contiguous placement; real-world systems often require variable charging rates or allow interruptions.
- What evidence would resolve it: Modified game environment and model architecture handling variable-sized blocks and discrete interruptions without loss of generalization advantages.

### Open Question 3
- Question: How does agent's performance degrade when exposed to extreme distributional shifts or adversarial arrival patterns outside training distribution?
- Basis in paper: Section 7.3 limits sensitivity analysis to ±10% parameter perturbations, leaving model's brittleness to high-variance or adversarially ordered arrivals unknown.
- Why unresolved: Theoretical bounds rely on Natarajan dimension, but empirical validation doesn't test limits of agent's generalization under severe concept drift.
- What evidence would resolve it: Evaluation on "out-of-distribution" scenarios (e.g., sudden surge in arrivals or random failures) where oracle expert policy differs significantly from training data.

## Limitations

- Theoretical generalization bounds rely on assumptions about effective sample size and complexity measures that may not hold in practice
- DAgger implementation depends on oracle's ability to solve Model 4 quickly for arbitrary partial trajectories, which becomes computationally challenging as fixed decisions increase
- Empirical validation focuses on specific EV charging scenarios; transferability to other domains remains unproven

## Confidence

- **High confidence**: Gamification concept and empirical performance improvements are well-supported by extensive experiments across four scenarios with clear statistical significance
- **Medium confidence**: Theoretical generalization bounds are formally correct but practical tightness and finite-sample applicability remain to be verified empirically
- **Low confidence**: Claim that image-based representations with CNNs yield tighter generalization bounds than vector-based MLPs as T increases is supported by theory but not directly validated through controlled experiments

## Next Checks

1. **Sample complexity validation**: Systematically vary number of training instances (n) and plot test Max-Min vs. training set size for I2M and V2M models to empirically verify predicted scaling advantage

2. **Backbone architecture ablation**: Train I2M and V2M models using identical MLP backbones (not CNNs) to isolate whether performance differences stem from representation or architecture choices

3. **Oracle computation time profiling**: Measure time required for expert to solve Model 4 as number of fixed decisions increases, establishing practical limits of DAgger iterations in real-world deployment