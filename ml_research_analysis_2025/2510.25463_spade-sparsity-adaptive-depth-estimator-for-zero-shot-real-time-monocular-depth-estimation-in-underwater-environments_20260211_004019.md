---
ver: rpa2
title: 'SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular
  Depth Estimation in Underwater Environments'
arxiv_id: '2510.25463'
source_url: https://arxiv.org/abs/2510.25463
tags:
- depth
- sparse
- points
- underwater
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SPADE, a monocular depth estimation pipeline
  for underwater environments that combines a pretrained relative depth estimator
  with sparse depth priors to produce dense, metric scale depth maps. The two-stage
  approach first globally aligns the relative depth map using sparse depth points,
  then refines the final metric prediction with Cascade Conv-Deformable Transformer
  blocks.
---

# SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments

## Quick Facts
- arXiv ID: 2510.25463
- Source URL: https://arxiv.org/abs/2510.25463
- Authors: Hongjie Zhang; Gideon Billings; Stefan B. Williams
- Reference count: 36
- Key outcome: Two-stage monocular depth estimation pipeline combining pretrained relative depth with sparse metric priors, achieving >15 FPS on embedded hardware while improving accuracy and generalization in underwater environments.

## Executive Summary
SPADE introduces a novel two-stage pipeline for monocular depth estimation in underwater environments, leveraging a frozen pre-trained relative depth estimator and sparse depth priors to produce dense, metric-scale depth maps. The method decouples relative geometry from metric scaling, enabling zero-shot generalization without fine-tuning the backbone. By combining global alignment with deformable attention-based refinement, SPADE achieves state-of-the-art accuracy while maintaining real-time performance on embedded hardware. Evaluation across multiple underwater datasets demonstrates consistent performance across varying levels of depth prior sparsity, particularly excelling when sparse points are uniformly distributed rather than clustered.

## Method Summary
SPADE is a two-stage pipeline that first predicts affine-invariant relative depth using a frozen DepthAnythingV2 Small backbone, then globally aligns this to metric scale using sparse depth points via least-squares optimization. A refinement network with Cascade Conv-Deformable Transformer blocks predicts per-pixel scale corrections, combining the aligned depth with densified sparse priors and DINOv2 features. The system is trained on synthetic TartanAir data with VINS-Mono-generated sparse points, validated on real underwater datasets (FLSea VI, Lizard Island, custom water tank). Training uses a combined loss in inverse depth space with AdamW optimizer, achieving real-time inference on Jetson Orin NX while demonstrating robustness across varying sparsity levels.

## Key Results
- Achieves over 15 FPS on Jetson Orin NX while maintaining state-of-the-art accuracy
- Demonstrates improved accuracy and generalization over baselines across varying depth prior sparsity levels
- Shows superior performance with uniformly distributed sparse points compared to clustered visual features
- Effective for underwater intervention tasks requiring precise depth estimation in challenging visual conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoupling relative geometry from metric scaling enables zero-shot generalization in visually degraded underwater environments.
- **Mechanism**: The system freezes a pre-trained Vision Transformer (DepthAnythingV2) to predict affine-invariant relative depth (ordinal structure). It then solves a least-squares optimization problem using sparse metric priors to find a global scale and shift, converting relative depth to metric depth without fine-tuning the backbone.
- **Core assumption**: The pre-trained backbone has learned robust ordinal depth relationships that hold true even in underwater turbidity where the model was not explicitly trained.
- **Evidence anchors**:
  - [abstract]: "...combines pre-trained relative depth estimator with sparse depth priors..."
  - [Section III.B]: "...affine-invariant prediction from DepthAnythingV2 is converted into metric scale... optimal scale s and shift t can be found by minimising a least squares error..."
  - [Corpus]: Paper 57194 (StereoAdapter) supports the difficulty of domain adaptation in underwater stereo, validating the need for robust pre-trained structures, though SPADE's specific "freezing" strategy is unique to this method.
- **Break condition**: If the relative depth estimator produces inverted ordinality (e.g., due to extreme caustics or lighting), the global alignment will faithfully scale these errors, resulting in a metrically scaled but geometrically inverted scene.

### Mechanism 2
- **Claim**: Deformable attention mechanisms outperform fixed-grid transformers for depth completion when sparse inputs are highly uneven or scant.
- **Mechanism**: The Cascade Conv-Deformable Transformer (CCDT) block uses an offset network to predict spatial offsets, shifting attention keys to informative regions rather than relying on a fixed grid. This allows the network to "hallucinate" or propagate scale corrections from a few distant valid pixels to large unknown regions.
- **Core assumption**: The learned offset network can identify semantic or geometric similarity between a query pixel and a distant sparse depth point to justify propagating that scale cue.
- **Evidence anchors**:
  - [Section III.C.4]: "...deformable attention adaptively focuses on the more informative key-value pairs... reducing the risk of important information from being missed in fixed attention patterns."
  - [Section IV]: "DAT consistently outperformed both alternatives... as sparsity increased... DAT maintained superior accuracy."
  - [Corpus]: Paper 110360 (DropD-SLAM) discusses utilizing monocular estimators for SLAM, providing context for the utility of these depth maps, but does not validate the specific attention mechanism.
- **Break condition**: If the sparse points are entirely random noise (outliers) rather than valid depth measurements, the deformable attention may aggressively propagate incorrect scale factors, potentially degrading performance more than a conservative CNN.

### Mechanism 3
- **Claim**: Predicting a per-pixel scale correction map (residual scaling) generalizes better across depth ranges than direct metric regression.
- **Mechanism**: Instead of regressing absolute depth values, the refinement network predicts a multiplier $\hat{\epsilon}$. The final depth is the product of the globally aligned depth and this correction map ($\hat{z} = \tilde{z} \odot \hat{\epsilon}$). This limits the network's search space to correcting scale errors rather than learning absolute depth from scratch.
- **Core assumption**: The global alignment stage provides a "sufficiently correct" baseline such that the residual corrections are small and locally smooth.
- **Evidence anchors**:
  - [Section III.C.1]: "Compared with directly regressing absolute depth, this strategy improves generalisation across scenes with widely varying depth ranges."
  - [Section V]: "Predicting a scale correction map... reduces the risk of overfitting to specific depth ranges."
  - [Corpus]: Weak direct evidence in corpus for *scale correction maps* specifically; most related papers (e.g., Paper 57194) focus on direct domain adaptation.
- **Break condition**: If the global alignment fails significantly (e.g., $s < 0$ due to clustered points), the refinement network may lack the capacity to invert the structural errors, as it is designed for scale refinement, not structural correction.

## Foundational Learning

- **Concept**: **Affine-Invariant vs. Metric Depth**
  - **Why needed here**: The core premise relies on a pre-trained model that only predicts relative ordering (Affine-Invariant). You must understand that this output lacks physical units (meters) and has arbitrary scale/shift, which necessitates the "Global Alignment" module.
  - **Quick check question**: If you swap the DepthAnythingV2 backbone for a metric-depth model trained on indoor scenes (e.g., NYU-Depth), do you still need the least-squares global alignment step? (Answer: Likely yes, due to domain shift, but the dynamics would change).

- **Concept**: **Deformable Attention (DAT)**
  - **Why needed here**: Standard Vision Transformers (ViTs) use fixed windows or grids. To understand the contribution of SPADE, you must grasp how DAT learns *where* to look (via offsets) versus standard attention which looks *everywhere* or in *fixed patterns*.
  - **Quick check question**: Why would a fixed-grid Swin Transformer struggle with a "Laser Scaler" input (2 points) compared to DAT? (Answer: Fixed grids may dilute the signal of 2 points across a large window, whereas DAT can shift focus specifically to those coordinates).

- **Concept**: **Inverse Depth Space**
  - **Why needed here**: The paper optimizes loss and performs alignment in inverse depth space ($v = 1/d$).
  - **Quick check question**: Why does the paper optimize in inverse depth space rather than Euclidean depth? (Answer: To prioritize close-range accuracy, as errors in close-range objects are magnified in inverse space, forcing the model to focus on them).

## Architecture Onboarding

- **Component map**: DepthAnythingV2 (Small, Frozen) -> Relative Depth Map -> Least-Squares Optimization -> Scale/Shift params -> Globally Aligned Depth -> Joint Bilateral Upsampling -> Densified Scale Map -> U-Net Refinement Network -> CCDT blocks -> Final Depth

- **Critical path**: The **sparse depth densification** (Joint Bilateral Upsampling) is critical. If this step fails to propagate the sparse scale factors to neighbors effectively, the Refinement Net receives a mostly empty signal, forcing it to rely entirely on the DINOv2 features.

- **Design tradeoffs**:
  - **Backbone Size**: The authors use DepthAnythingV2 *Small* to hit 15 FPS on Jetson Orin NX. This sacrifices fine detail (e.g., the 3mm rope failure in the Water Tank test) for speed.
  - **Alignment Strategy**: The system switches between Scale-Shift alignment and Scale-Only alignment. Scale-Only is safer but puts a heavier burden on the refinement network to correct large biases.

- **Failure signatures**:
  - **Negative Scale Inversion**: If sparse points are clustered on a single plane, the least-squares fit may overfit the shift term, resulting in a negative scale ($s < 0$) where near objects appear far.
  - **Caustic Confusion**: The relative depth estimator struggles with caustics (rapid lighting changes), interpreting them as geometric features.
  - **Thin Structure Loss**: The small backbone and upsampling steps tend to smooth out sub-centimeter features (ropes/wires).

- **First 3 experiments**:
  1. **Sparsity Stress Test**: Run inference on the FLSea dataset while linearly reducing the number of sparse points from 400 to 10. Plot the RMSE degradation curve to verify the "robustness" claim of the CCDT block.
  2. **Alignment Fallback Check**: Synthesize inputs where sparse points are tightly clustered (simulating a flat wall viewed at an angle). Verify that the code correctly triggers the "scale-only" alignment path (Eq. 3-4) rather than producing negative depth.
  3. **Attention Ablation**: Replace the Deformable Attention (DAT) module with a standard Swin Transformer block in the refinement network. Evaluate specifically on the "Laser Scaler" (2-point) scenario to quantify the performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger relative depth backbones with model quantization improve performance on fine structures (e.g., thin ropes) while retaining real-time inference on embedded hardware?
- Basis in paper: [explicit] The authors state: "An avenue worth exploring is whether larger depth backbones with quantisation could boost performance on fine structures while retaining real-time inference on embedded compute."
- Why unresolved: The lightweight DepthAnythingV2 Small backbone struggles with very thin objects (e.g., 3mm ropes) compared to the slower, larger Marigold-DC model, creating a trade-off between detail and speed.
- What evidence would resolve it: Benchmarking a quantized version of a larger backbone (e.g., DepthAnythingV2 Large) on the water tank dataset while measuring FPS on the Jetson Orin NX.

### Open Question 2
- Question: How can the pipeline effectively mitigate the negative impact of underwater lighting effects, such as caustics, on relative depth prediction consistency?
- Basis in paper: [explicit] The discussion notes: "Unique underwater lighting effects, such as caustics, can reduce relative depth prediction consistency... This could be mitigated by pre-processing for caustic removal or fine-tuning."
- Why unresolved: The current pre-trained relative depth estimator suffers from inconsistencies in regions with rapidly changing illumination, and the authors have not yet tested the proposed mitigation strategies.
- What evidence would resolve it: Evaluating the pipeline on datasets with heavy caustics before and after applying caustic removal pre-processing or domain-specific fine-tuning.

### Open Question 3
- Question: To what extent does multi-modal fusion with active sensors (e.g., 3D sonar, LiDAR) improve depth estimation accuracy in feature-poor underwater environments compared to visual sparse priors?
- Basis in paper: [explicit] The conclusion states: "Future work will focus on multi-modal fusion of cameras with active sensors to further enhance underwater spatial awareness." Additionally, Table IV shows simulated uniform depth points yield lower errors than clustered visual features.
- Why unresolved: While the authors simulated uniform sparse inputs, they have not yet integrated real active sensor data to validate if this distribution advantage holds in practice for feature-poor regions.
- What evidence would resolve it: Integrating real 3D imaging sonar or LiDAR data into the pipeline and comparing the metric accuracy against visual-only sparse inputs in turbid or texture-less water conditions.

## Limitations
- Struggles with extremely thin structures (sub-centimeter features like ropes) due to lightweight backbone and upsampling smoothing
- Performance degrades when sparse depth points are clustered rather than uniformly distributed
- Relies heavily on the quality of the pre-trained relative depth estimator, inheriting biases related to underwater optical phenomena

## Confidence

**High Confidence**: The two-stage architecture (relative depth + global alignment + refinement) is well-specified and the FPS claims on embedded hardware are verifiable. The core mechanism of using deformable attention for sparse depth completion is supported by ablation studies.

**Medium Confidence**: Claims about generalization across varying sparsity levels and the superiority of scale-correction maps over direct metric regression are supported by experiments but could benefit from additional cross-dataset validation.

**Low Confidence**: The robustness claims for underwater environments are primarily validated on synthetic-to-real transfer (TartanAir to FLSea) rather than extensive real-world deployment data.

## Next Checks

1. **Sparsity Stress Test**: Systematically evaluate performance degradation as sparse point count decreases from 400 to 10 points on FLSea dataset to verify robustness claims.

2. **Attention Mechanism Ablation**: Replace deformable attention blocks with standard Swin Transformer blocks in the refinement network and measure performance impact specifically on the 2-point "Laser Scaler" scenario.

3. **Cross-Dataset Generalization**: Test the pre-trained SPADE model on a held-out underwater dataset not seen during any training or validation to assess true zero-shot capability.