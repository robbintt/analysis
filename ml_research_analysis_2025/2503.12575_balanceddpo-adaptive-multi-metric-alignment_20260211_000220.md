---
ver: rpa2
title: 'BalancedDPO: Adaptive Multi-Metric Alignment'
arxiv_id: '2503.12575'
source_url: https://arxiv.org/abs/2503.12575
tags:
- balanced
- score
- metrics
- preference
- diffusiondpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BALANCED DPO, a method for aligning text-to-image
  diffusion models with human preferences across multiple metrics simultaneously.
  The key idea is to aggregate preference labels from diverse metrics (Human Preference
  Score, CLIP score, Aesthetic quality, and PickScore) using majority voting in the
  preference distribution space, rather than mixing rewards directly.
---

# BalancedDPO: Adaptive Multi-Metric Alignment

## Quick Facts
- **arXiv ID**: 2503.12575
- **Source URL**: https://arxiv.org/abs/2503.12575
- **Reference count**: 40
- **Primary result**: Improves average win rates by 15%, 7.1%, and 10.3% on Pick-a-Pic, PartiPrompt, and HPD datasets respectively compared to DiffusionDPO

## Executive Summary
BALANCED DPO introduces a method for aligning text-to-image diffusion models with human preferences across multiple metrics simultaneously. The approach aggregates preference labels from diverse metrics (Human Preference Score, CLIP score, Aesthetic quality, and PickScore) using majority voting in the preference distribution space, rather than mixing rewards directly. This avoids scaling and dominance issues while maintaining the simplicity of the standard DPO pipeline. Experiments show state-of-the-art results, with BALANCED DPO achieving significant improvements over DiffusionDPO across multiple datasets and metrics.

## Method Summary
BALANCED DPO aligns text-to-image diffusion models using preference-based optimization across multiple metrics. The method aggregates preferences from four scoring functions (HPS, CLIP, PickScore, Aesthetic) via majority voting to create scale-agnostic consensus labels. These aggregated preferences drive a unified DPO loss while periodically refreshing the reference model every 100 steps to enable broader exploration. The approach is tested on SD1.5 and SDXL models using the Pick-a-Pic dataset with 2000 training steps, batch size 128, and learning rate 1e-8.

## Key Results
- Achieves 15% improvement in average win rate on Pick-a-Pic validation set
- Outperforms DiffusionDPO by 7.1% on PartiPrompt dataset
- Demonstrates 10.3% improvement on HPD dataset across all major metrics

## Why This Works (Mechanism)

### Mechanism 1: Majority Voting in Preference Distribution Space
Aggregating preferences via majority voting avoids scaling and dominance issues inherent in direct reward mixing. For each image pair, K reward models output binary preferences s_k ∈ {−1, +1}, and the aggregated preference s = sign(Σs_k) determines the winner. This converts heterogeneous continuous scores into a scale-agnostic consensus signal.

### Mechanism 2: Periodic Reference Model Refresh
Dynamically updating the reference model p_ref ← p_θ^t during training improves multi-metric alignment by allowing broader exploration before KL tightening. The reference model is refreshed every T=100 steps, letting the model explore diverse optimization directions while gradually stabilizing.

### Mechanism 3: Binary Preference Encoding Eliminates Scale Sensitivity
Converting continuous rewards to binary pairwise preferences removes the need for reward normalization or weight tuning. Each scorer outputs only which image wins (s_k = ±1), discarding magnitude information and sidestepping the challenge that different metrics operate on different scales.

## Foundational Learning

- **Bradley-Terry Preference Model**
  - Why needed here: DPO builds on BT to express p(x_w ≻ x_l | c) = σ(r(x_w) − r(x_l))
  - Quick check: Given two images with latent rewards r_w = 2.3 and r_l = 1.8, what is the BT probability that the first is preferred?

- **KL-Regularized Policy Optimization**
  - Why needed here: The DPO objective implicitly maximizes reward while constraining the policy to stay close to a reference via KL divergence
  - Quick check: What happens to policy diversity if β → ∞? What if β → 0?

- **Diffusion Model Reverse Process**
  - Why needed here: DiffusionDPO adapts DPO to diffusion by defining the likelihood p_θ(x_0|c) over the full denoising trajectory
  - Quick check: In a discrete-time diffusion, does p_θ(x_0) involve a single denoising step or a chain of transitions?

## Architecture Onboarding

- **Component map**: Dataset -> Scorers (K=4) -> Aggregator -> Loss -> Reference model -> Base models
- **Critical path**: Load pretrained diffusion model → Initialize p_ref → Sample image pairs → Query scorers → Compute majority vote → Calculate loss → Update θ → Refresh reference every T steps
- **Design tradeoffs**: More scorers K → richer consensus but higher overhead; smaller T → more exploration but potential instability; binary encoding vs. weighted aggregation
- **Failure signatures**: Dominance by one metric despite voting; training instability or loss divergence; no improvement over single-metric DPO
- **First 3 experiments**: 
  1. Replicate Table I win rates for BalancedDPO vs. DiffusionDPO on Pick-a-Pic validation
  2. Test K ∈ {2, 3, 4, 5} by adding/removing metrics
  3. Test T ∈ {50, 100, 200, 500} on held-out split and plot win rates vs. T

## Open Questions the Paper Calls Out

- How does the correlation or redundancy among the chosen reward metrics impact the stability and effectiveness of the majority voting aggregation?
- What are the theoretical convergence guarantees or risks associated with the dynamic updating of the reference model every T=100 steps?
- Does the binary "majority vote" aggregation lead to performance degradation in cases of weak consensus due to forced preference labels?

## Limitations

- Exact scoring function implementations (HPS, PickScore, Aesthetic) are not specified, requiring reimplementation
- Reward normalization and mixing strategies are avoided but the method discards potentially useful confidence information
- Reference refresh interval T=100 is claimed effective but not rigorously justified; stability boundaries are unknown
- Cross-dataset generalization may not hold for very different domains or metrics

## Confidence

- **High**: BalancedDPO outperforms DiffusionDPO on multiple validation datasets; majority voting aggregation avoids direct reward mixing issues
- **Medium**: Dynamic reference model updates improve multi-metric alignment; binary preference encoding eliminates scale sensitivity
- **Medium**: 15% win rate improvement on Pick-a-Pic is robust; ablation results on reference updates are internally consistent

## Next Checks

1. Test robustness by adding/removing scoring metrics (K ∈ {2,3,4,5}) to identify minimum K needed for consistent improvement
2. Systematically vary reference refresh interval T (50, 100, 200, 500 steps) and measure stability boundaries and performance tradeoffs
3. Verify whether BalancedDPO maintains advantage when evaluated on datasets with different metric distributions or prompt types