---
ver: rpa2
title: Social Bias in Popular Question-Answering Benchmarks
arxiv_id: '2505.15553'
source_url: https://arxiv.org/abs/2505.15553
tags:
- benchmarks
- benchmark
- language
- bias
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzed 30 popular QA/RC benchmarks,
  finding pervasive lack of transparency and social bias. Most papers provided minimal
  demographic information about annotators and few addressed bias prevention.
---

# Social Bias in Popular Question-Answering Benchmarks

## Quick Facts
- arXiv ID: 2505.15553
- Source URL: https://arxiv.org/abs/2505.15553
- Authors: Angelie Kraft; Judith Simon; Sonja Schimmler
- Reference count: 40
- Primary result: 30 popular QA benchmarks show pervasive transparency gaps and social bias, with underrepresentation of non-Western regions and non-male genders

## Executive Summary
This paper systematically analyzed 30 popular QA/RC benchmarks, finding pervasive lack of transparency and social bias. Most papers provided minimal demographic information about annotators and few addressed bias prevention. Quantitative analysis of 20 datasets revealed significant gender, occupation, religion, and geographic biases—particularly underrepresentation of non-Western regions and non-male genders. All but one benchmark were in English only. The findings demonstrate that current benchmarks incentivize biased LLM development by underrepresenting marginalized demographics, thus potentially exacerbating epistemic injustice.

## Method Summary
The authors conducted a two-part analysis: qualitative assessment of 30 popular QA/RC benchmarks for transparency regarding annotator demographics and bias mitigation efforts, and quantitative analysis of 20 datasets for representational bias using automated entity linking to Wikidata properties. The qualitative component examined paper sections on author backgrounds, annotator information, recruitment criteria, and bias prevention. The quantitative component analyzed demographic representation in gender, occupation, religion, and geography using ReFinED model for entity linking and Wikidata property extraction.

## Key Results
- All 30 benchmark papers lacked transparency regarding annotator demographics, with most providing no demographic information
- Quantitative analysis revealed significant underrepresentation of non-male genders (7.6% of entity mentions) and non-Western regions (e.g., only 3.3% of entities from Sub-Saharan Africa)
- 29 of 30 benchmarks were in English only, despite examining datasets from multiple countries
- Only 2 papers explicitly addressed bias prevention, while 16 mentioned potential biases but didn't discuss mitigation

## Why This Works (Mechanism)
The study demonstrates that benchmark transparency gaps directly enable biased dataset creation, which then propagates through LLM training and evaluation. Without transparent documentation of annotator demographics and bias prevention strategies, benchmarks systematically underrepresent marginalized populations. This creates evaluation metrics that favor models trained on Western, male-centric data, perpetuating epistemic injustice by treating these perspectives as universal.

## Foundational Learning
- **Epistemic injustice**: Why needed - framework for understanding how marginalized voices are systematically excluded from knowledge production; Quick check - can identify when certain demographic perspectives are treated as less credible
- **Benchmark transparency**: Why needed - essential for understanding potential biases in dataset creation; Quick check - can locate documentation of annotator recruitment and demographics
- **Entity linking methodology**: Why needed - automated approach to extract demographic information from text; Quick check - can explain how Wikidata properties map to gender, occupation, religion categories
- **Representational bias**: Why needed - quantifies demographic underrepresentation in datasets; Quick check - can calculate percentage of entity mentions by demographic category
- **Knowledge graphs**: Why needed - structured representation of entity properties for bias analysis; Quick check - can describe how Wikidata organizes demographic information
- **Positional objectivity**: Why needed - recognizes that "neutral" perspectives often reflect dominant group viewpoints; Quick check - can identify assumptions about universal applicability of Western perspectives

## Architecture Onboarding
- **Component map**: Benchmark papers -> Qualitative analysis pipeline -> Transparency assessment; Benchmark datasets -> Quantitative analysis pipeline -> Entity linking -> Wikidata property extraction -> Demographic bias metrics
- **Critical path**: Paper selection → Qualitative coding → Quantitative entity linking → Bias measurement → Analysis of correlations
- **Design tradeoffs**: Automated entity linking enables large-scale analysis but introduces potential Wikidata bias; English-only focus allows methodological consistency but misses linguistic diversity
- **Failure signatures**: Missing annotator demographics → Cannot assess representational bias; Entity linking errors → Incorrect demographic categorization; Limited geographic scope → Underrepresentation of non-Western perspectives
- **First experiments**: 1) Replicate transparency analysis on non-English benchmarks; 2) Conduct manual validation of 100 entity links to assess automated accuracy; 3) Compare bias metrics across different entity linking approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the causal relationship between annotator/creator demographics and the social biases found in benchmark datasets?
- Basis in paper: [explicit] The authors state: "Due to the lack of transparency across benchmarks, we were unable to investigate the causal relationship between the identity of those involved in the benchmark creation and the biases found in the benchmark datasets through statistical testing."
- Why unresolved: Benchmark papers rarely report annotator demographics, recruitment criteria, or positional context, preventing correlational or causal analysis.
- What evidence would resolve it: Future benchmarks with systematic documentation of annotator identities, enabling regression or controlled experiments linking annotator characteristics to dataset content biases.

### Open Question 2
- Question: Have QA/RC benchmarks become less biased and more transparent over time?
- Basis in paper: [explicit] The authors explicitly call for future work: "Studies conducted at larger scale should also systematically examine whether benchmarks have become less biased and more transparent over time."
- Why unresolved: This study examined only a snapshot of 30 popular benchmarks at one point in time and did not track temporal trends.
- What evidence would resolve it: Longitudinal analysis of benchmark papers and datasets across multiple years, measuring changes in transparency reporting and quantitative bias metrics over time.

### Open Question 3
- Question: To what extent do Wikidata and entity linker biases influence the measured bias results in this analysis?
- Basis in paper: [explicit] The authors note: "There is a certain risk that the biases of Wikidata and the entity linker may influence our results. This is hard to avoid in an analysis that utilizes automated processes."
- Why unresolved: The automated entity linking pipeline (ReFinED model and Wikidata) may propagate or amplify existing representational skews, particularly affecting commonsense and scholarly benchmarks.
- What evidence would resolve it: Manual validation on larger samples comparing linked entity properties against ground truth, or sensitivity analysis using alternative knowledge bases and entity linkers.

## Limitations
- Focus on English-language benchmarks only, potentially missing linguistic and cultural biases in non-English contexts
- Qualitative analysis may have missed nuanced bias mitigation discussions not using explicit terminology
- Quantitative analysis provides snapshot but may not represent evolving benchmark creation practices
- Corpus-based demographic analysis cannot capture lived experiences contributing to epistemic injustice

## Confidence
- **High**: Quantitative findings showing gender and geographic underrepresentation in benchmark datasets
- **Medium**: Qualitative findings about transparency gaps in benchmark papers
- **Low**: Implications about epistemic injustice without longitudinal studies of benchmark impacts on LLM development

## Next Checks
1. Replicate the analysis on non-English benchmarks to assess linguistic bias patterns
2. Conduct interviews with benchmark creators to understand barriers to diverse participation
3. Track demographic representation changes in benchmarks over time to measure progress toward equitable representation