---
ver: rpa2
title: High Quality Diffusion Distillation on a Single GPU with Relative and Absolute
  Position Matching
arxiv_id: '2503.20744'
source_url: https://arxiv.org/abs/2503.20744
tags:
- diffusion
- student
- rapm
- teacher
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RAPM (Relative and Absolute Position Matching),
  a diffusion distillation method that achieves high-quality image generation while
  requiring only a single GPU. The key innovation is matching both relative and absolute
  diffusion positions between student and teacher models, inspired by PCM but adapted
  for extreme single-GPU training with batch size 1.
---

# High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching

## Quick Facts
- arXiv ID: 2503.20744
- Source URL: https://arxiv.org/abs/2503.20744
- Authors: Guoqiang Zhang; Kenta Niwa; J. P. Lewis; Cedric Mesnage; W. Bastiaan Kleijn
- Reference count: 40
- Primary result: Achieves FID scores around 13.91-14.46 on SD V1.5 and 19.21 on SDXL using only 4 timesteps and single GPU training

## Executive Summary
This paper introduces RAPM (Relative and Absolute Position Matching), a diffusion distillation method that achieves high-quality image generation while requiring only a single GPU. The key innovation is matching both relative and absolute diffusion positions between student and teacher models, inspired by PCM but adapted for extreme single-GPU training with batch size 1. RAPM pre-computes teacher diffusion trajectories and uses two discriminators to align student positions with both relative positions (from teacher and frozen student) and absolute positions (from teacher alone).

## Method Summary
RAPM addresses the challenge of diffusion distillation on single GPUs by pre-computing teacher diffusion trajectories offline, eliminating the need for large batch statistics during training. The method uses LoRA adapters for both student and discriminator models to reduce trainable parameters. Two discriminators are employed: one matches relative positions between teacher and frozen student, while the other matches absolute positions to the teacher alone. The training alternates between updating discriminators and the student model, with detach operations ensuring stable gradient flow across timesteps.

## Key Results
- Achieves FID scores of 13.91-14.46 on SD V1.5 and 19.21 on SDXL using 4 timesteps
- Matches 1-timestep SOTA performance while requiring significantly fewer computational resources
- Ablation studies demonstrate that dual position matching (relative + absolute) produces sharper, more consistent images compared to relative-only approaches
- Training on a single GPU with batch size 1, compared to multi-GPU setups required by previous methods

## Why This Works (Mechanism)

### Mechanism 1
Pre-computing teacher trajectories enables batch-size-1 training by replacing real-image statistics with stored teacher guidance. Teacher trajectories are pre-computed offline using M-step DDIM and stored, serving as fixed anchors during student training.

### Mechanism 2
Dual discriminator matching (relative + absolute positions) stabilizes training and improves image quality. Relative position discriminator aligns student with intermediate guidance, while absolute discriminator anchors student directly to teacher's final position per timestep.

### Mechanism 3
Detach operations decouple timestep-wise position matching, enabling tractable gradient flow across coarse timesteps. Two detach operations treat relative positions as constant tensors and prevent backpropagation across timestep boundaries.

## Foundational Learning

- **DDIM (Denoising Diffusion Implicit Models) as ODE solver**: Understanding DDIM's deterministic sampling is prerequisite to following the position matching mathematics. Quick check: Can you explain why DDIM can be interpreted as a first-order ODE solver and how it differs from stochastic DDPM sampling?

- **LoRA (Low-Rank Adaptation)**: RAPM uses LoRA adapters to reduce trainable parameters, enabling single-GPU training. Quick check: How does LoRA reduce memory footprint during training compared to full fine-tuning, and where are the adapter modules typically inserted in a U-Net?

- **GAN-based discriminator training with alternating updates**: RAPM trains two discriminators alternately with student model. Quick check: What happens if discriminator and generator (student) update frequencies are mismatched, and why does the paper alternate every other iteration?

## Architecture Onboarding

- **Component map**: Teacher Model (θ) [frozen] → Pre-computed Trajectories {z̃_n} → Student Model (φ) [LoRA adapters] → z^φ_n (1-step DDIM) → Discriminator D_ϕ1 [LoRA] ← Relative Position ẑ_n (teacher→frozen-student) → Discriminator D_ϕ2 [LoRA] ← Absolute Position z̃_n (teacher only) → Combined Loss (Eq. 18)

- **Critical path**: Pre-compute trajectories → Initialize student/discriminators with LoRA → Alternating updates (discriminators on even iterations, student on odd) → Per-timestep loss accumulation → Backprop

- **Design tradeoffs**: Storage vs. compute (pre-computing 1K trajectories costs few hours but eliminates batch-size requirements), NFE vs. quality (4 timesteps to match 1-timestep SOTA), LoRA rank (lower rank = faster training but potentially lower capacity)

- **Failure signatures**: Dark/blurry images, inconsistent object structure across iterations (missing absolute position matching), FID curve fluctuation (batch size too small without trajectory pre-computation), slow convergence (removed second detach operation)

- **First 3 experiments**:
  1. Sanity check on pre-computed trajectories: Generate 10 teacher trajectories with M=25 DDIM steps on SD V1.5, visualize positions at coarse timesteps (N=4)
  2. Ablation on position matching: Train three student variants with batch-size-1: (a) relative-only, (b) absolute-only, (c) both (full RAPM)
  3. Hyperparameter sweep on weighting w_n: Test w_n ∈ {0.1, 0.5, 1.0, 2.0} for absolute position loss on held-out prompts

## Open Questions the Paper Calls Out

### Open Question 1
Is the pre-computation of teacher trajectories computationally scalable for future, more advanced diffusion model architectures? The paper states it is "not clear if the time complexity will be acceptable for more advanced diffusion models developed in the future."

### Open Question 2
Can RAPM be effectively optimized to achieve competitive performance in a single-step (N=1) generation setting? The paper compares RAPM at 4 steps against SOTA methods at 1 step but notably omits results for RAPM at 1 step.

### Open Question 3
How sensitive is the method to the accuracy and quantity of pre-computed teacher trajectories? The authors mention using 1K trajectories and setting M=25 for accuracy, but do not analyze the trade-off between the quality/quantity of this synthetic data and final student performance.

## Limitations
- Pre-computation of teacher trajectories may not scale to future larger diffusion models due to storage and compute requirements
- The method's effectiveness at extreme 1-timestep sampling remains unproven despite comparable FID claims
- Specific hyperparameter values (weighting factors w_n and relative position delta Δ) are not specified, potentially affecting training stability

## Confidence

- **High confidence**: The dual discriminator mechanism and detach operations are well-supported by ablation results and mathematical formulation
- **Medium confidence**: Storage/compute tradeoffs are sound, but long-term scalability for larger models remains uncertain
- **Low confidence**: Specific hyperparameter values and exact architecture details (particularly discriminator design) require reference to external PCM implementation

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically test different w_n weighting values (0.1, 0.5, 1.0, 2.0) and relative position delta Δ values to identify optimal settings for stable training

2. **Extreme sampling efficiency test**: Evaluate RAPM performance at 1-timestep sampling on SD V1.5 to validate whether FID scores truly match 1-timestep SOTA methods as claimed

3. **Scalability assessment**: Measure trajectory storage requirements and training time for SDXL vs. SD V1.5, projecting requirements for future larger diffusion models to quantify the stated limitation