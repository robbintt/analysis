---
ver: rpa2
title: Evaluating Compositional Scene Understanding in Multimodal Generative Models
arxiv_id: '2503.23125'
source_url: https://arxiv.org/abs/2503.23125
tags:
- uni00000051
- uni00000003
- uni0000004c
- prompts
- uni0000004a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the compositional visual understanding capabilities
  of state-of-the-art multimodal generative models, comparing text-to-image models
  (DALL-E 3) and vision-language models (GPT-4 variants, Claude 3.5 Sonnet, QWEN2-VL-72B,
  InternVL2.5-38B) against human performance. The evaluation used three experimental
  approaches: generating images from relational prompts (basic, reversed, and compositional),
  and learning relational concepts from real-world (Bongard-HOI) and synthetic (SVRT)
  images.'
---

# Evaluating Compositional Scene Understanding in Multimodal Generative Models

## Quick Facts
- arXiv ID: 2503.23125
- Source URL: https://arxiv.org/abs/2503.23125
- Reference count: 26
- Key outcome: Current multimodal generative models show compositional understanding but lack human-level robustness, particularly for complex multi-object scenes.

## Executive Summary
This study evaluates the compositional visual understanding capabilities of state-of-the-art multimodal generative models against human performance. Using text-to-image models (DALL-E 3) and vision-language models (GPT-4 variants, Claude 3.5 Sonnet, QWEN2-VL-72B, InternVL2.5-38B), the evaluation reveals that while models show improved performance on basic relational prompts, they fail on reversed and compositional prompts. Vision-language models outperform previous models on Bongard-HOI but perform significantly worse than humans. The results indicate current models lack the robustness and generality of human visual processing, particularly for complex multi-object scenes.

## Method Summary
The evaluation used three experimental approaches: generating images from relational prompts (basic, reversed, and compositional) using DALL-E 3 via Azure API, and learning relational concepts from real-world (Bongard-HOI) and synthetic (SVRT) images using vision-language models. VLMs were evaluated using few-shot classification with 1-9 labeled examples, while DALL-E 3 images were assessed through human agreement scoring. Human baselines were established for comparison, and various prompting strategies including chain-of-thought and interactive learning were tested but showed no improvement.

## Key Results
- DALL-E 3 showed significantly improved performance over DALL-E 2 on basic relational prompts (agreement >0.8 for common relations like "on" and "in"), but performance degraded substantially for reversed prompts and compositional prompts.
- All vision-language models outperformed previous models on Bongard-HOI but performed significantly worse than humans (GPT-4o: ~0.65 accuracy vs. human ~0.90).
- On SVRT, performance degraded with increasing object count, falling to chance levels for concepts with 6+ objects, while humans maintained high performance across all complexity levels.
- Neither chain-of-thought prompting nor interactive learning paradigms improved model performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DALL-E 3's improved performance over DALL-E 2 on relational prompts is driven by better surface-level correlation from improved training data, but this capability is brittle.
- Mechanism: Enhanced caption quality and model scale allow for stronger statistical correlation between high-likelihood text prompts and generated pixel patterns for common relations, without a robust compositional mechanism.
- Core assumption: Performance gains are primarily due to better data alignment rather than emergent symbolic reasoning.
- Evidence anchors:
  - [abstract] "DALL-E 3 showed significantly improved performance over DALL-E 2 on basic relational prompts (agreement >0.8 for common relations like 'on' and 'in'), but performance degraded substantially for reversed prompts."
  - [section] Section 2.2.2 finds a statistically significant correlation between prompt log-likelihood and image agreement, linking success to training data probability.
  - [corpus] "Object-centric Binding in Contrastive Language-Image Pretraining" highlights foundational CLIP limitations in compositional scenes, constraining downstream models.
- Break condition: Performance collapses on "reversed" prompts (e.g., "a rabbit chasing a tiger"), indicating reliance on training data likelihood rather than a robust, generative compositional mechanism.

### Mechanism 2
- Claim: Vision-Language Models (VLMs) learn relational concepts via in-context pattern matching but fail to generalize to complex, multi-object configurations due to a binding bottleneck.
- Mechanism: In-context learning allows VLMs to match simple relational patterns from few-shot examples. However, without explicit object-centric representations, their ability to bind and reason over multiple objects simultaneously degrades rapidly.
- Core assumption: The primary bottleneck is visual parsing (the binding problem), not the language model's reasoning capacity.
- Evidence anchors:
  - [abstract] "On SVRT, performance degraded with increasing object count, falling to chance levels for concepts with 6+ objects, while humans maintained high performance."
  - [section] Section 3.2.2 shows VLM performance was "statistically indistinguishable from chance levels for problems with 6 objects."
  - [corpus] "LayoutAgent" lacks explicit spatial reasoning and "SceneAlign" notes MLLMs struggle with faithful reasoning in complex scenes, reinforcing the multi-object limitation.
- Break condition: Performance drops to chance levels as the number of objects in a scene increases (>5), indicating an inability to compose many relations at once.

### Mechanism 3
- Claim: Relational understanding is not improved by chain-of-thought or interactive learning because the failure is in perceptual binding, not reasoning depth.
- Mechanism: Since the models fail to correctly parse and bind objects in complex scenes, providing more reasoning steps or feedback loops does not correct the flawed visual input.
- Core assumption: The failure is upstream of the reasoning module, within the visual representation itself.
- Evidence anchors:
  - [abstract] "Neither chain-of-thought prompting nor interactive learning paradigms improved model performance."
  - [section] Section A.8 details experiments showing no discernible improvement across various Chain-of-Thought conditions on the SVRT task.
  - [corpus] Corpus signals are weak or missing for direct evidence on this specific mechanism beyond the paper's own report.
- Break condition: Chain-of-thought and interactive learning provide no measurable benefit, distinguishing this from a pure reasoning deficit.

## Foundational Learning

- Concept: **The Binding Problem**
  - Why needed here: This is the core theoretical explanation offered in the paper for why models fail with multiple objects. It explains that a shared representational resource (like a transformer's hidden state) struggles to attribute features (like "red") to specific objects when multiple instances are present.
  - Quick check question: Why does a single feature map in a standard CNN struggle to represent "a red cube to the left of a blue sphere"?

- Concept: **Compositional Generalization**
  - Why needed here: The entire paper is a benchmark for this capability—the ability to understand novel combinations of known primitives (objects and relations).
  - Quick check question: Why is correctly generating "a rabbit chasing a tiger" after being trained on images of "a tiger chasing a rabbit" considered a critical test of compositionality?

- Concept: **In-Context Learning**
  - Why needed here: This is the evaluation paradigm used for all VLMs in the paper. Understanding how models use the 1-9 labeled examples in their prompt to classify a target image is essential to interpreting the results.
  - Quick check question: How does providing a set of positive and negative example images in a prompt allow a model to infer a relational rule without weight updates?

## Architecture Onboarding

- Component map: Text Encoder (DALL-E 3) -> Diffusion Image Generator (DALL-E 3); Vision Encoder (VLMs) -> Multimodal Fusion (VLMs) -> LLM Backbone (VLMs)
- Critical path: The vision encoder's generation of distinct, bindable object representations is the critical path. If objects are fused in the embedding space, no amount of reasoning in the LLM can correctly determine their relations.
- Design tradeoffs: The paper implies a tradeoff between end-to-end differentiable multimodal fusion (current SOTA) and explicit, structured representations (e.g., slot attention). The former excels at holistic, high-probability matches; the latter is required for robust multi-object composition.
- Failure signatures:
  - Likelihood Collapse: Sharp performance drop on "reversed" or improbable prompts.
  - Object Count Scaling Failure: Accuracy plummets to chance when scenes contain >5 objects.
  - Ineffective Reasoning Augmentation: Chain-of-thought or interactive feedback yields no gain.
  - Symmetric vs. Asymmetric: Better performance on symmetric relations ("near") vs. asymmetric ones ("in") when subject/object are swapped.
- First 3 experiments:
  1. Run the Reversed Prompt Ablation: Take 30 basic relational prompts, generate images, then reverse the prompts (e.g., "A on B" → "B on A") and generate again. Measure the agreement delta to quantify reliance on training data priors.
  2. Run the Object Count Scaling Test: On a synthetic task like SVRT, systematically vary the number of objects (2, 4, 6) in the test images while keeping few-shot examples constant. Plot accuracy vs. object count to identify the scaling breakpoint.
  3. Run the Reasoning Ablation: Compare baseline zero-shot/few-shot performance against Chain-of-Thought and interactive feedback paradigms on the same task. Confirm if reasoning augmentation has any effect, signaling whether the bottleneck is perceptual or reasoning-based.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed poor performance on compositional tasks stem primarily from multi-object scene parsing limitations rather than deficits in relational reasoning itself?
- Basis in paper: [explicit] The authors state: "This pattern of results suggests that the poor performance observed in the current generation of multimodal models may be due primarily to a more basic difficulty with parsing of multi-object scenes, rather than a difficulty with relational processing per se."
- Why unresolved: The study identified the pattern but did not isolate the specific mechanism; performance degraded with more objects but the root cause remains unclear.
- What evidence would resolve it: Experiments that decouple object count from relational complexity, such as testing models on decomposed vs. integrated scene representations as in Campbell et al. (2024).

### Open Question 2
- Question: Would integrating object-centric representation learning (e.g., slot-based approaches) into multimodal generative models improve their compositional scene understanding?
- Basis in paper: [explicit] The authors propose: "This suggests that a promising direction for future work may be to combine multimodal generative models with object-centric visual processing methods."
- Why unresolved: Slot-based methods have shown promise in visual reasoning tasks but have not been tested in combination with large-scale multimodal generative models.
- What evidence would resolve it: Comparing performance of standard multimodal models against variants augmented with object-centric architectures (e.g., slot attention) on SVRT and Bongard-HOI benchmarks.

### Open Question 3
- Question: What training or architectural modifications could enable models to learn complex relational concepts that neither chain-of-thought prompting nor interactive feedback improves?
- Basis in paper: [inferred] The authors found that "neither chain-of-thought prompting nor interactive learning paradigms improved model performance," yet did not identify what would.
- Why unresolved: Standard techniques for improving reasoning failed, suggesting the limitation is more fundamental to how models process visual scenes.
- What evidence would resolve it: Systematic evaluation of models trained with explicit relational objectives, object-centric priors, or compositional data augmentation.

## Limitations
- The evaluation framework relies on human judgment for DALL-E 3 output assessment, introducing potential subjectivity despite aggregation across multiple evaluators.
- The comparison between text-to-image and vision-language models operates under fundamentally different paradigms (generation vs. classification), which may confound direct performance comparisons.
- While Bongard-HOI and SVRT provide controlled benchmarks, they may not fully capture the breadth of real-world compositional reasoning.

## Confidence

- **High Confidence:** DALL-E 3's improved performance over DALL-E 2 on basic relational prompts, and the systematic degradation on reversed and compositional prompts. The object count scaling failure in SVRT is clearly demonstrated.
- **Medium Confidence:** The conclusion that VLMs learn via in-context pattern matching rather than robust compositional reasoning. While supported by data, alternative explanations (e.g., limited training data coverage) cannot be entirely ruled out.
- **Medium Confidence:** The assertion that chain-of-thought and interactive learning fail because the bottleneck is perceptual binding rather than reasoning depth. The null results are clear, but the mechanism interpretation requires additional validation.

## Next Checks
1. Cross-dataset validation: Test the same models on additional compositional reasoning benchmarks (e.g., gSCAN, CLOSURE) to verify whether the observed limitations generalize beyond Bongard-HOI and SVRT.
2. Alternative explanation testing: Conduct ablation studies varying training data likelihood and in-context example quality to distinguish between pattern matching and compositional understanding failure modes.
3. Architectural probing: Implement modified vision encoders with explicit object-centric representations (e.g., slot attention) and evaluate whether this mitigates the object count scaling failure, providing evidence for or against the binding problem hypothesis.