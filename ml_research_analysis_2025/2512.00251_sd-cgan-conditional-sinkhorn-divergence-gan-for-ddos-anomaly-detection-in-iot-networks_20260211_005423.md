---
ver: rpa2
title: 'SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in
  IoT Networks'
arxiv_id: '2512.00251'
source_url: https://arxiv.org/abs/2512.00251
tags:
- detection
- sd-cgan
- anomaly
- training
- sinkhorn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SD-CGAN is a conditional GAN enhanced with Sinkhorn Divergence
  for anomaly detection in IoT edge networks. It addresses instability, mode collapse,
  and class imbalance in GAN-based intrusion detection.
---

# SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks

## Quick Facts
- arXiv ID: 2512.00251
- Source URL: https://arxiv.org/abs/2512.00251
- Reference count: 34
- Key outcome: SD-CGAN achieves 98.62% precision, 98.99% F1-score, and 98.03% accuracy on CICDDoS2019 dataset using benign-only training for zero-day detection

## Executive Summary
SD-CGAN introduces a conditional GAN architecture enhanced with Sinkhorn Divergence for anomaly detection in IoT edge networks. The framework addresses key challenges in GAN-based intrusion detection including training instability, mode collapse, and class imbalance through geometry-aware loss functions and CTGAN-based synthetic augmentation. By training exclusively on benign traffic, SD-CGAN enables zero-day attack detection while maintaining computational efficiency suitable for edge deployment.

## Method Summary
SD-CGAN uses a 3-layer MLP generator conditioned on flow-type metadata to synthesize benign traffic patterns. The architecture replaces traditional adversarial loss with Sinkhorn Divergence for stable training and mode coverage. CTGAN augments minority attack classes to address class imbalance, while Random Forest feature selection and Pearson correlation filtering optimize the feature space. At inference, anomaly scores are computed via Sinkhorn divergence between test samples and generated approximations, with thresholds set at the 95th percentile of benign scores.

## Key Results
- Achieves 98.62% precision, 98.99% F1-score, and 98.03% accuracy on CICDDoS2019 dataset
- Demonstrates 98.37% accuracy in zero-day attack detection using benign-only training
- Shows 99.98% recall for UDP-Lag attack detection and 99.87% recall for SYN Flood detection
- Outperforms existing GAN and deep learning baselines in all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Sinkhorn Divergence stabilizes GAN training by providing meaningful gradients even when real and synthetic distributions have minimal overlap. The entropy-regularized optimal transport computes a "transport cost" between mini-batches of real and generated samples, yielding smooth gradient flow that avoids vanishing gradients and oscillatory convergence common in adversarial training.

### Mechanism 2
Training exclusively on benign traffic enables zero-day detection by learning a compact normal manifold and flagging statistical deviations. The generator learns to approximate the benign traffic distribution, and at inference, anomaly scores are computed via Sinkhorn divergence between test samples and generated approximations.

### Mechanism 3
CTGAN-based synthetic augmentation mitigates class imbalance without introducing distributional bias. CTGAN applies mode-specific normalization for continuous features and log-frequency sampling for categorical conditioning, preserving multimodal structure in minority classes.

## Foundational Learning

- **Optimal Transport and Sinkhorn Divergence**: Understanding why entropy-regularized transport stabilizes GAN training versus adversarial losses. Quick check: Can you explain why $W_\epsilon(\mu, \mu) \neq 0$ necessitates the debiased formulation?

- **Conditional GANs and Class-Conditioned Generation**: The architecture conditions generation on flow-type metadata to prevent mode collapse toward majority patterns. Quick check: How does concatenating condition vector $c$ with latent $z$ differ from unconditional generation?

- **One-Class Anomaly Detection**: The model flags deviations rather than classifying known attacks, critical for zero-day detection rationale. Quick check: What determines the 95th percentile threshold, and how would you adjust it for higher recall vs. precision?

## Architecture Onboarding

- **Component map**: Data preprocessing (feature selection via Random Forest, Min-Max normalization) -> CTGAN augmentation -> SD-CGAN generator training on benign -> inference scoring against threshold

- **Critical path**: Feature selection and normalization feed into CTGAN augmentation, which generates balanced training data for the SD-CGAN generator. The trained generator produces synthetic benign samples used as reference distribution for Sinkhorn-based anomaly scoring.

- **Design tradeoffs**: Sinkhorn vs. adversarial discriminator provides stability and smooth convergence at the cost of additional transport computation per batch. Benign-only training enables zero-day capability but cannot classify specific attack types. Fixed 95th percentile threshold provides conservative false positive control but may miss low-signal anomalies.

- **Failure signatures**: Loss plateau above 0.3 with oscillations may indicate batch size too small or regularization $\epsilon$ mismatched to feature scale. High false positive rate (>5%) suggests threshold miscalibration or insufficient benign coverage. Mode collapse evident if generated samples cluster narrowly despite conditioning.

- **First 3 experiments**: 1) Train identical generator architecture with Jensen-Shannon loss; compare convergence curve and final accuracy to validate Sinkhorn contribution. 2) Sweep anomaly threshold from 90th-99th percentile; plot precision-recall tradeoff to identify optimal operating point. 3) Train on benign + known attacks, test on held-out attack category to quantify generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
Does SD-CGAN maintain its detection efficacy when exposed to structurally novel attack families rather than perturbed versions of known attacks? The current evaluation simulates zero-day attacks using variations of existing attack categories in CICDDoS2019, leaving true out-of-distribution generalization unverified.

### Open Question 2
How can Explainable AI (XAI) techniques decode the non-linear dependencies and feature contributions in SD-CGAN's anomaly scoring? The paper focuses on detection performance but lacks analysis of why specific traffic flows are flagged, leaving the "black box" nature of the geometry-aware loss unaddressed.

### Open Question 3
How does SD-CGAN perform in multi-class classification scenarios compared to its current binary setup? The current methodology aggregates diverse attacks into a single anomaly class, failing to demonstrate if the conditional generator can distinguish between specific attack types.

### Open Question 4
What are the latency and energy consumption profiles of SD-CGAN when deployed on actual resource-constrained edge hardware? The paper claims computational efficiency suitable for edge deployment but lacks benchmarks on embedded hardware, relying instead on high-performance desktop CPU measurements.

## Limitations

- Threshold sensitivity to varying IoT deployment contexts requires dynamic calibration beyond fixed 95th percentile
- Feature selection opacity due to unspecified final feature set used for model training
- Scalability validation gap for heterogeneous resource constraints on real edge devices

## Confidence

- **High Confidence**: Core Sinkhorn divergence stabilization mechanism and benign-only training approach for zero-day detection are well-supported by convergence metrics and controlled experiments
- **Medium Confidence**: CTGAN augmentation benefits and zero-day detection generalization claims rely on statistical similarity tests and single holdout validation
- **Low Confidence**: Exact inference procedure for computing Sinkhorn divergence between individual test samples and generator output lacks specification

## Next Checks

1. Implement adaptive percentile thresholding based on local benign traffic baselines, then evaluate false positive variation across different IoT deployment scenarios

2. Systematically remove features identified by KS tests and measure performance degradation to quantify each feature's contribution to the 98.62% precision claim

3. Port the SD-CGAN inference pipeline to a resource-constrained edge platform (e.g., Raspberry Pi) and measure latency, memory usage, and accuracy under varying feature dimensions and batch sizes