---
ver: rpa2
title: Long-Short Distance Graph Neural Networks and Improved Curriculum Learning
  for Emotion Recognition in Conversation
arxiv_id: '2507.15205'
source_url: https://arxiv.org/abs/2507.15205
tags:
- graph
- emotion
- features
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Emotion Recognition in Conversation
  (ERC), which involves identifying the emotional state of each utterance in a conversation
  by considering both nearby and distant contextual influences. The authors propose
  a Long-Short Distance Graph Neural Network (LSDGNN) that constructs separate long-distance
  and short-distance modules based on a Directed Acyclic Graph (DAG) to capture multimodal
  features from different contextual ranges.
---

# Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation

## Quick Facts
- arXiv ID: 2507.15205
- Source URL: https://arxiv.org/abs/2507.15205
- Authors: Xinran Li; Xiujuan Xu; Jiaqi Qiao
- Reference count: 37
- Primary result: Proposed LSDGNN+ICL model achieves state-of-the-art performance, outperforming existing methods by 0.84% on IEMOCAP and 0.07% on MELD in weighted F1 score

## Executive Summary
This paper addresses the challenge of Emotion Recognition in Conversation (ERC), which involves identifying the emotional state of each utterance in a conversation by considering both nearby and distant contextual influences. The authors propose a Long-Short Distance Graph Neural Network (LSDGNN) that constructs separate long-distance and short-distance modules based on a Directed Acyclic Graph (DAG) to capture multimodal features from different contextual ranges. To reduce feature redundancy between these modules while enabling mutual enhancement, they introduce a Differential Regularizer and a BiAffine Module for feature interaction. Additionally, the paper presents an Improved Curriculum Learning (ICL) strategy that uses weighted emotional shifts based on similarity to prioritize learning of similar emotions, addressing the class imbalance problem common in ERC datasets. Experimental results on IEMOCAP and MELD datasets show that the proposed LSDGNN+ICL model achieves state-of-the-art performance.

## Method Summary
The LSDGNN model processes multimodal features (text via RoBERTa, audio/visual via FCN encoders) through parallel long-distance (ω=5) and short-distance (ω=1) DAGNN modules to capture different contextual ranges. A BiAffine module facilitates feature interaction between these streams while a Differential Regularizer enforces diversity in their learned representations. The Improved Curriculum Learning strategy computes difficulty based on "weighted emotional shifts" - prioritizing similar emotion transitions as harder samples to be learned later in training. The model is trained with specific hyperparameters for each dataset (IEMOCAP: 4 layers, dropout=0.4, batch=16, lr=0.0005; MELD: 2 layers, dropout=0.1, batch=64, lr=0.00001).

## Key Results
- LSDGNN+ICL achieves state-of-the-art performance with 0.84% improvement on IEMOCAP and 0.07% on MELD in weighted F1 score
- The Differential Regularizer significantly reduces feature redundancy between long and short distance modules
- Improved Curriculum Learning with weighted emotional shifts effectively addresses class imbalance in ERC datasets
- Optimal context window size is ω=5 for long-distance module, with performance degrading at larger values

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Context Aggregation via Differential Regularization
The architecture utilizes two parallel DAGNNs with a Differential Regularizer ($R_D = 1 / ||A_{short} - A_{long}||_F$) to maximize distance between adjacency representations, forcing them to learn non-overlapping features. This reduces redundancy compared to monolithic graph approaches by treating short-range and long-range dependencies as having different feature signatures.

### Mechanism 2: Similarity-Weighted Curriculum Learning
The ICL strategy defines difficulty based on "Weighted Emotional Shifts," prioritizing samples where emotional transitions are highly similar (e.g., Joy → Excitement) as harder samples to be focused on later in training. This addresses the class imbalance problem by ensuring minority classes receive appropriate gradient updates throughout training.

### Mechanism 3: BiAffine Feature Interaction
A BiAffine module is inserted between the long and short branches, computing attention weights to allow the short-distance context to query the long-distance context and vice versa. This mutual enhancement occurs when long-distance context provides "background" information that modulates the interpretation of "local" short-distance utterance features.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAG) in NLP**
  - Why needed: The paper relies on DAGs to strictly enforce temporal causality (past utterances influence future ones, but not vice versa) which is critical for streaming conversation analysis
  - Quick check: Does the graph allow an edge from a later utterance to an earlier one? (Answer: No, that would violate the acyclic property)

- **Concept: Curriculum Learning (CL)**
  - Why needed: The paper proposes "Improved CL." Understanding that CL is about sample ordering (easy → hard) is necessary to grasp why the "Difficulty Measure Function" matters
  - Quick check: In standard CL, would you train on noisy, boundary-case data first or clean, prototypical data first?

- **Concept: Multimodal Feature Fusion**
  - Why needed: The model combines Text (RoBERTa), Audio, and Video. Understanding concatenation (⊕) vs. attention-based fusion is key to Section 3.2 and 3.4.3
  - Quick check: If the audio feature dimension is 128 and text is 768, what is the dimension of the concatenated vector $H^0$? (Answer: 896)

## Architecture Onboarding

- **Component map:** Input (RoBERTa + FCNs) → Parallel LSDGNN blocks (Long-Distance ω=5 vs. Short-Distance ω=1) → BiAffine Module + Differential Regularizer → FFN → Softmax

- **Critical path:** 1. Feature Extraction (Eq. 1-2) → 2. Long/Short Graph Construction → 3. Parallel Graph Propagation with BiAffine Exchange (Eq. 4-10) → 4. Classification Loss + Differential Regularization Loss (Eq. 16)

- **Design tradeoffs:**
  - Window size (ω): Larger ω captures more history but introduces noise; found ω=5 optimal
  - CL Buckets: Splitting data into too many buckets (k=12 for MELD) vs too few (k=5 for IEMOCAP) depends on dataset variance

- **Failure signatures:**
  - Redundancy Collapse: If Differential Regularizer loss dominates (λ too high), accuracy may plummet as features become artificially distinct but semantically weak
  - Curriculum Stagnation: If "easy" buckets are too large, the model may overfit to simple patterns before ever seeing hard emotional shifts

- **First 3 experiments:**
  1. Verify Differential Necessity: Run LSDGNN with λ=0 (no regularizer) vs. λ=0.1 to observe the delta in Weighted F1 on IEMOCAP
  2. Optimize Context Window: Sweep ω ∈ {2, 4, 5, 6} on Long-Distance module to verify the "5" peak holds for your data
  3. Stress Test Curriculum: Train with k=1 (similar emotions = hard) vs. k=-1 (dissimilar = hard) to verify the claim that similar emotions are the "harder" samples

## Open Questions the Paper Calls Out

- Can non-linear or attention-based transformation methods for weighted emotional shifts outperform the current linear mapping in the Improved Curriculum Learning strategy? (The authors will explore more advanced transformation methods for weighted emotional shifts beyond the simple linear formula)
- How does the LSDGNN architecture adapt to multi-party conversations with more than three speakers or low-resource data regimes? (The authors explicitly list "investigate adaptive graph strategies for multi-party and low-resource scenarios" as future work)
- Is the optimal long-distance context window (ω=5) universal, or does it require dynamic adjustment based on conversation density? (Table 5 shows performance peaking at ω=5 and declining at ω=6 or ∞, suggesting the model is sensitive to context length)

## Limitations

- The Differential Regularizer's effectiveness relies on the assumption that short- and long-range features are inherently redundant, but this is validated only through ablation studies on two datasets
- The Improved Curriculum Learning strategy assumes that similar emotions are harder to distinguish, which may not generalize to all emotional taxonomies or cultural contexts
- The BiAffine module's attention mechanism could potentially collapse to uniform weights, providing no real feature interaction benefit

## Confidence

- **High Confidence**: The LSDGNN architecture's ability to capture multimodal features through parallel DAGNN modules (supported by ablation studies showing performance drops when components are removed)
- **Medium Confidence**: The Differential Regularizer's effectiveness in reducing feature redundancy (based on internal ablation results but lacking comparison to alternative regularization methods)
- **Medium Confidence**: The Improved Curriculum Learning strategy's assumption that similar emotions are harder to learn (validated on IEMOCAP and MELD but may not generalize)

## Next Checks

1. Test the Differential Regularizer's effectiveness by training with λ=0, 0.1, and 0.5 to observe the performance curve and determine if there's an optimal regularization strength
2. Validate the curriculum learning assumption by training with k=1 (similar emotions = hard) vs k=-1 (dissimilar = hard) on a third emotion dataset with different emotional categories
3. Analyze the BiAffine module's attention weights to verify they're learning meaningful patterns rather than collapsing to uniform distributions