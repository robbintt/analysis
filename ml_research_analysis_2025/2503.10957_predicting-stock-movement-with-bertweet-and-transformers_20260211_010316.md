---
ver: rpa2
title: Predicting Stock Movement with BERTweet and Transformers
arxiv_id: '2503.10957'
source_url: https://arxiv.org/abs/2503.10957
tags:
- price
- data
- transformer
- stock
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that applying BERTweet, a pre-trained language
  model for English Tweets, and transformer architectures to the task of stock movement
  prediction achieves competitive performance on the Stocknet dataset. The authors
  replace the variational movement decoder in the original Stocknet model with a transformer
  encoder and evaluate several model architectures, including simple feedforward networks,
  transformers, and cross-attention transformers.
---

# Predicting Stock Movement with BERTweet and Transformers

## Quick Facts
- arXiv ID: 2503.10957
- Source URL: https://arxiv.org/abs/2503.10957
- Reference count: 16
- Primary result: Cross-attention transformer with BERTweet achieves 0.1114 MCC on Stocknet dataset

## Executive Summary
This paper demonstrates that BERTweet, a pre-trained language model for English Tweets, combined with transformer architectures, achieves competitive performance in stock movement prediction on the Stocknet dataset. The authors replace the variational movement decoder in the original Stocknet model with a transformer encoder and evaluate several model architectures. The best results, measured by Matthews Correlation Coefficient (MCC), were obtained using a cross-attention transformer with 8 heads and an embedding dimension of 512, achieving an MCC of 0.1114 and an accuracy of 56.282%. This outperforms several previous models on the Stocknet dataset, including the original Stocknet models and a model that incorporated external data.

## Method Summary
The method involves predicting binary stock movement (up/down) using 5-day windows of historical prices and tweets. Tweets are embedded using BERTweet (frozen, 768-dim), averaged per stock per day, and concatenated with engineered price features. A cross-attention transformer (8 heads, embedding dim 512, 0 dropout) processes these modalities through self-attention and bidirectional cross-attention, followed by a sigmoid classifier. The model is trained with binary cross-entropy loss using batch size 128, learning rate 1e-5, Adam optimizer, and early stopping.

## Key Results
- Cross-attention transformer achieves MCC of 0.1114 and accuracy of 56.282% on Stocknet dataset
- Outperforms several previous models including original Stocknet models and MAN-SF
- Best results obtained with 8 heads, 512 embedding dimension, and zero dropout
- BERTweet embeddings kept frozen during training to accelerate computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training on Twitter data improves sentiment signal extraction for stock prediction compared to generic embeddings.
- Mechanism: BERTweet encodes Twitter-specific linguistic patterns (casual language, hashtags, mentions, emoji) that generic models like BERT or GloVe may misinterpret. By averaging tweet embeddings per-stock per-day, the model aggregates dispersed sentiment signals into a unified representation.
- Core assumption: Sentiment signals in tweets causally relate to subsequent price movement, consistent with semi-strong EMH (public information not fully priced).
- Evidence anchors: BERTweet is pre-trained specifically on Twitter corpus; embeddings are frozen during training; related work supports Twitter-specific modeling.
- Break condition: If sentiment in tweets is already priced in (semi-strong EMH holds perfectly), or if tweet volume/quality drops significantly, signal degrades.

### Mechanism 2
- Claim: Cross-attention between price and tweet modalities captures interaction effects that simple concatenation cannot.
- Mechanism: The cross-attention transformer projects both modalities to a common dimension, applies self-attention independently, then uses bidirectional cross-attention (price→tweets and tweets→price). This allows the model to learn which tweet patterns matter given specific price contexts, and vice versa.
- Core assumption: Price dynamics and tweet sentiment contain complementary information that interact non-linearly.
- Evidence anchors: Cross-attention transformer achieves best results; cross-attention blocks enable detection of complex interaction effects; related work shows cross-attention effective for multimodal stock prediction.
- Break condition: If one modality is noisy or uninformative (e.g., low tweet volume), cross-attention may amplify noise rather than signal.

### Mechanism 3
- Claim: Transformer self-attention over the 5-day window captures temporal dependencies more effectively than the original variational decoder.
- Mechanism: Sinusoidal position encodings preserve temporal order. Multi-head attention allows each head to learn different temporal patterns (e.g., momentum, mean-reversion signals) across the window without the bottleneck of sequential processing.
- Core assumption: 5-day window captures sufficient temporal signal; longer dependencies are not critical for next-day prediction.
- Evidence anchors: Transformer encoder replaces variational movement decoder; sinusoidal position encodings represent temporal structure; related work uses various architectures without clear consensus on optimal temporal modeling.
- Break condition: If critical signals exist beyond 5 days, or if position encodings fail to capture irregular trading gaps (weekends, holidays).

## Foundational Learning

- **Concept: Transformer attention (self-attention, multi-head, cross-attention)**
  - Why needed here: Core architecture component; understanding Q/K/V mechanics essential for debugging and hyperparameter tuning (heads, key dimension).
  - Quick check question: Given query matrix Q and key matrix K, what does softmax(QK^T/√d) compute and why is scaling needed?

- **Concept: Pre-trained language models and transfer learning (frozen vs. fine-tuned)**
  - Why needed here: BERTweet embeddings are frozen; understanding embedding extraction, batching, and cache strategies directly impacts training efficiency.
  - Quick check question: What are the tradeoffs between freezing vs. fine-tuning a pre-trained encoder for a downstream classification task?

- **Concept: Multimodal fusion strategies (concatenation vs. cross-attention vs. bilinear)**
  - Why needed here: Paper compares these explicitly; selecting fusion approach is a primary architectural decision.
  - Quick check question: When would simple concatenation outperform cross-attention for multimodal fusion?

## Architecture Onboarding

- **Component map**: Data layer (Stocknet dataset) -> Preprocessing (5-day windows) -> Embedding layer (BERTweet, frozen) -> Price features -> Fusion layer (Cross-attention transformer) -> Output (Sigmoid classifier)

- **Critical path**: 1. Pre-compute BERTweet embeddings offline 2. Implement efficient dataloader with prefetching 3. Tune learning rate (1e-5 stable) and batch size (128) before architectural changes

- **Design tradeoffs**: Frozen BERTweet enables faster training but may underfit compared to fine-tuning; cross-attention is more expressive but higher compute and risk of overfitting; auxiliary targets improve accuracy but lower MCC

- **Failure signatures**: Predicting nearly all positive or all negative → reduce learning rate, increase batch size; training instability → check gradient clipping, learning rate, data normalization; slow training → verify dataloader is not IO-bound

- **First 3 experiments**: 1. Baseline feedforward network with BERTweet embeddings + price concatenation 2. Cross-attention transformer with 8 heads, 512 embedding dim, 0 dropout 3. Ablation: compare concatenation-only vs. cross-attention fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining the cross-attention transformer architecture with adversarial sampling and auxiliary market information via graph attention outperform the current state-of-the-art on the Stocknet dataset?
- Basis in paper: The authors conclude that future directions include "combining the transformer architecture, adversarial sampling, auxiliary market information, and graph attention to evaluate the combined performance."
- Why unresolved: The current study isolates the transformer and BERTweet components to establish a baseline without these complementary techniques.
- What evidence would resolve it: Empirical results showing Matthews Correlation Coefficient (MCC) scores exceeding 0.195 (the current MAN-SF state-of-the-art) on the Stocknet test set.

### Open Question 2
- Question: Would fine-tuning the BERTweet language model weights yield higher accuracy or MCC than the frozen-weight approach used in the study?
- Basis in paper: The authors state that "parameters of BERTweet were kept frozen" to accelerate training, explicitly leaving the potential performance gains from backpropagation untested.
- Why unresolved: Freezing weights precludes the model from adapting linguistic representations to the specific domain of financial tweets.
- What evidence would resolve it: A comparative ablation study measuring performance deltas between frozen and fine-tuned BERTweet configurations.

### Open Question 3
- Question: Why does the optimal model configuration require zero dropout, and does this lack of regularization lead to instability across different random seeds?
- Basis in paper: The results section notes that "best results were somewhat surprisingly obtained with zero dropout," a finding that contradicts standard regularization practices without further explanation.
- Why unresolved: The paper reports the hyperparameter search result but does not analyze the underlying mechanism or robustness of this observation.
- What evidence would resolve it: Analysis of training dynamics and variance in MCC scores across multiple runs with zero dropout versus standard dropout rates.

## Limitations

- The paper's data acquisition paths and Stocknet dataset preprocessing details are unclear due to placeholder links
- Minor implementation details (normalization methods, feature engineering steps, cross-attention layer configurations) remain uncertain
- The claim of setting a new baseline is limited by lack of direct comparisons to all recent models in literature

## Confidence

- **High confidence**: Core architectural claims about BERTweet's effectiveness and transformer-based cross-attention fusion are well-supported by experimental results
- **Medium confidence**: Superiority of cross-attention approach is demonstrated, but modest performance gains suggest effect size may be sensitive to implementation details
- **Low confidence**: Generalization of findings to other stock prediction datasets or market conditions cannot be established from this single dataset study

## Next Checks

1. Verify data preprocessing integrity by reproducing exact ±0.5%/0.55% threshold filtering and 5-day window alignment on original Stocknet dataset
2. Conduct ablation studies comparing frozen vs. fine-tuned BERTweet embeddings to quantify impact of transfer learning decisions
3. Test model robustness by evaluating performance across different market regimes (bull vs. bear markets) within test period to assess temporal generalization