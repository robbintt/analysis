---
ver: rpa2
title: 'Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing:
  A 1.2M Corpus Development'
arxiv_id: '2510.03781'
source_url: https://arxiv.org/abs/2510.03781
tags:
- corpus
- hadith
- semantic
- narrations
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents Rezwan, an AI-driven pipeline that processes\
  \ over 1.2 million hadith narrations into a richly annotated corpus. Using Large\
  \ Language Models, the system performs chain\u2013text separation, translation into\
  \ 12 languages, diacritization, summarization, thematic tagging, and semantic clustering."
---

# Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development

## Quick Facts
- arXiv ID: 2510.03781
- Source URL: https://arxiv.org/abs/2510.03781
- Reference count: 0
- The paper presents Rezwan, an AI-driven pipeline that processes over 1.2 million hadith narrations into a richly annotated corpus.

## Executive Summary
Rezwan is an automated AI pipeline that processes over 1.2 million hadith narrations from classical Islamic texts into a richly annotated corpus. Using prompt-engineered Large Language Models, the system performs chain-text separation, translation into 12 languages, diacritization, summarization, thematic tagging, and semantic clustering. Expert evaluation on 1,213 samples shows near-human accuracy in structured tasks (e.g., 9.33/10 for chain-text separation) and high overall quality (8.46/10), though challenges remain in diacritization and semantic similarity detection. The AI approach completes tasks equivalent to over 700,000 person-hours of expert labor at a fraction of the cost, making large-scale hadith research feasible.

## Method Summary
The Rezwan pipeline uses LLM-based prompt engineering for hadith chain-text separation, implementing a "semantic unit" approach to avoid truncating long narrations. The system then applies sequential enrichment modules for translation (12 languages), diacritization, summarization, thematic tagging, and semantic clustering. Quality control filters and fuzzy string matching validate outputs against source texts. The six-month development required "few thousand dollars" in compute costs and processed approximately 1.5 million narrations (~392 million words) from the Maktabat Ahl al-Bayt digital repository, with manual filtering to reclassify misclassified texts.

## Key Results
- Expert evaluation on 1,213 samples achieved 9.33/10 for chain-text separation and 9.33/10 for summarization
- Overall corpus quality rated 8.46/10, significantly higher than the manually curated Noor Corpus (3.66/10)
- The AI system completed tasks equivalent to approximately 700,000 person-hours of expert labor
- Diacritization word-level error rate exceeded 15%, and semantic similarity detection scored 7.28/10

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Structured Segmentation
- Claim: Prompt-engineered LLMs can separate hadith chain (isnad) from text (matn) at near-human accuracy.
- Mechanism: LLMs leverage learned patterns of classical Arabic structure to identify narrator chains versus content, using a "semantic unit" approach to avoid truncating long narrations.
- Core assumption: The structural regularities of hadith texts are sufficiently consistent for LLMs to generalize from training data.
- Evidence anchors:
  - [abstract] "chain–text separation (9.33/10)"
  - [section 3.2] "prompt-engineered Large Language Models (LLMs) were employed to separate chain from text"
  - [corpus] Weak direct corpus support; neighbor papers focus on thematic clustering, not segmentation.
- Break condition: Highly irregular or hybrid texts (e.g., commentary interspersed with narration) may confuse the model.

### Mechanism 2: Multi-Layer Enrichment Pipeline
- Claim: Sequential LLM-powered modules can enrich narrations with translation, diacritization, summarization, and tagging at scale.
- Mechanism: Each enrichment task is treated as a separate prompt-based operation, allowing modular updates and error isolation.
- Core assumption: Errors in one layer do not catastrophically cascade downstream (partially mitigated by quality filters).
- Evidence anchors:
  - [abstract] "machine translation into twelve languages, intelligent diacritization, abstractive summarization, thematic tagging"
  - [section 3.3] "Each narration was enriched with multiple layers of metadata and linguistic annotations"
  - [corpus] Neighbor papers on multilingual adaptation (EMMA-500) suggest LLMs can handle multi-language transfer, but specific to hadith is limited.
- Break condition: Diacritization and semantic similarity show elevated error rates (2.49% character-level, 7.28/10 score); domain-specific terms remain challenging.

### Mechanism 3: Economic Feasibility via Exponential Effort-Accuracy Model
- Claim: AI reduces the human effort needed for high-accuracy corpus construction by exploiting diminishing returns in manual quality improvement.
- Mechanism: An exponential decay model (q(a) = q₀e^{-kH(a)}) captures how effort grows non-linearly as accuracy approaches perfection; AI operates at the efficient frontier.
- Core assumption: The effort-accuracy relationship holds for religious text processing as for other domains.
- Evidence anchors:
  - [section 6.1] Explicit formula and calibration against baseline H_tot
  - [section 6.3] "equivalent to approximately 700,000 person-hours of expert labor"
  - [corpus] No direct corpus validation of the economic model; neighbor papers do not address cost modeling.
- Break condition: If expert labor requirements are misestimated, the economic argument weakens; also assumes AI quality does not degrade at scale.

## Foundational Learning

- Concept: **Hadith Structure (Isnad/Matn)**
  - Why needed here: The pipeline hinges on correctly separating narrator chains from content.
  - Quick check question: Can you identify where the chain ends and the main text begins in a sample narration?

- Concept: **Arabic Diacritization**
  - Why needed here: >80% of texts lack vowel marks; diacritization is necessary for readability and downstream NLP.
  - Quick check question: Do you understand how missing diacritics creates ambiguity in Arabic words?

- Concept: **Prompt Engineering for Structured Output**
  - Why needed here: The pipeline relies on LLMs producing consistent, parseable outputs for segmentation and enrichment.
  - Quick check question: Can you write a prompt that forces an LLM to output JSON with specific fields?

## Architecture Onboarding

- Component map:
  - Source Data (Maktabat Ahl al-Bayt) -> Preprocessing (Manual filtering/reclassification) -> Segmentation Module (LLM-based boundary detection) -> Validation Layer (Fuzzy string matching) -> Enrichment Modules (Translation, diacritization, summarization, thematic tagging, semantic clustering) -> Quality Control (Automated filters) -> Final Corpus

- Critical path:
  1. Raw text → 2. Segmentation (chain/text split) → 3. Validation → 4. Enrichment layers (parallel where possible) → 5. Quality filtering → 6. Final corpus

- Design tradeoffs:
  - **Scale vs. Precision**: Automation enables 1.2M+ narrations but accepts ~5–15% error rates in some tasks.
  - **Modularity vs. Integration**: Separate enrichment modules allow updates but add orchestration complexity.
  - **General LLMs vs. Domain Adaptation**: Current system uses general LLMs; domain fine-tuning could improve semantic similarity and diacritization.

- Failure signatures:
  - **Segmentation errors**: Mixed chain/text or truncated narrations (check for semantic unit handling).
  - **Diacritization drift**: High character-level error rate (>2%) on classical terms.
  - **Semantic clustering noise**: Low scores (~7.3/10) on semantic similarity indicate embedding or clustering issues.
  - **Non-hadith inclusion**: 15.25% non-hadith texts detected—review source filtering.

- First 3 experiments:
  1. **Run chain–text separation on 100 manually labeled samples** to validate LLM accuracy and identify edge cases.
  2. **Benchmark diacritization** against existing tools (e.g., MADAMIRA) on a held-out set to quantify improvement needs.
  3. **Evaluate semantic clustering** on a known thematic subset (e.g., all narrations about prayer) to assess grouping coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-adapted fine-tuning of LLMs reduce diacritization word-level error rates below 15% for classical Arabic hadith texts?
- Basis in paper: [explicit] Section 7.2 states "Diacritization remains an open challenge due to the complexity of Arabic morphology" with word-level error rates above 15%.
- Why unresolved: The complexity of Arabic morphology and the conservative evaluation assumptions make this a persistent challenge that current general-purpose LLMs cannot fully address.
- What evidence would resolve it: A comparative study evaluating diacritization accuracy of domain-adapted/fine-tuned models vs. baseline LLMs on a held-out test set of classical hadith texts.

### Open Question 2
- Question: What architectural or training improvements are required to raise semantic similarity detection scores above 8.0/10 for religious texts with high interpretative complexity?
- Basis in paper: [explicit] Section 7.2 notes semantic similarity detection was less reliable (7.28/10), and section 5.2 reports it as the lowest-performing aspect alongside lexical similarity.
- Why unresolved: The inherent interpretative nature of meaning in religious texts makes semantic similarity detection difficult for current embedding-based approaches.
- What evidence would resolve it: Empirical comparison of different LLM architectures, embedding strategies, or retrieval-augmented approaches on hadith semantic similarity tasks with expert-annotated ground truth.

### Open Question 3
- Question: Does the 95.8% translation accuracy assumed for all 12 languages hold when each language is independently evaluated by native-speaking domain experts?
- Basis in paper: [inferred] Table 4 assumes 95.8% accuracy for 11 non-Persian translations without direct evaluation; only Persian translation was rigorously assessed in the expert evaluation.
- Why unresolved: Translation quality varies across language pairs and cultural contexts; assuming uniform quality may mask significant performance gaps.
- What evidence would resolve it: Expert evaluation of randomly sampled translations in each of the 12 target languages using the same evaluation framework applied to Persian.

### Open Question 4
- Question: Does the quality observed in the 1,213-sample evaluation generalize to rare theological concepts, unusual narration structures, and underrepresented sub-corpora?
- Basis in paper: [inferred] The random sample represents ~0.1% of the corpus; section 4.1 notes that narrations with >60% error were flagged but separately reported, potentially masking systematic failures in specific subdomains.
- Why unresolved: Random sampling may not adequately capture edge cases, rare theological nuances, or structurally atypical narrations that could have higher error rates.
- What evidence would resolve it: Stratified sampling evaluation targeting narrations with rare terms, complex chains, or from underrepresented source collections.

## Limitations
- Diacritization word-level error rate exceeds 15%, indicating room for improvement in classical Arabic text processing.
- Semantic similarity detection scored only 7.28/10, struggling with nuanced religious text semantics.
- 15.25% of processed texts were non-hadith, revealing limitations in source filtering and boundary detection.

## Confidence
- **High Confidence**: Chain-text separation accuracy (9.33/10), overall corpus quality (8.46/10), economic feasibility model.
- **Medium Confidence**: Thematic tagging and clustering (7.5/10), multilingual translation quality (not independently validated).
- **Low Confidence**: Diacritization accuracy (word-level >15%), semantic similarity detection (7.28/10).

## Next Checks
1. **Boundary detection validation**: Test chain-text separation on 100 manually labeled samples to identify edge cases and validate semantic unit handling.
2. **Diacritization benchmarking**: Compare system diacritization against MADAMIRA or similar tools on a held-out classical Arabic dataset.
3. **Semantic clustering evaluation**: Assess thematic coherence on a known subset (e.g., prayer-related narrations) using manual verification.