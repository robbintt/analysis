---
ver: rpa2
title: An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation
arxiv_id: '2510.13925'
source_url: https://arxiv.org/abs/2510.13925
tags:
- traffic
- retrieval
- hybrid
- query
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Revelation introduces an AI agent framework that transforms raw
  IoT packet captures into structured, semantically enriched representations for holistic
  traffic interpretation. The system integrates protocol logs, packet-layer records,
  flow summaries, transformer-based anomaly detection, and threat intelligence enrichment,
  indexed for retrieval-augmented question answering.
---

# An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation

## Quick Facts
- arXiv ID: 2510.13925
- Source URL: https://arxiv.org/abs/2510.13925
- Reference count: 32
- Revelation introduces an AI agent framework that transforms raw IoT packet captures into structured, semantically enriched representations for holistic traffic interpretation.

## Executive Summary
Revelation presents an AI agent framework that transforms raw IoT packet captures into structured, semantically enriched representations for holistic traffic interpretation. The system integrates protocol logs, packet-layer records, flow summaries, transformer-based anomaly detection, and threat intelligence enrichment, indexed for retrieval-augmented question answering. An LLM-guided agent performs reasoning over this corpus to produce evidence-grounded, human-readable interpretations of both benign and malicious behavior. Experimental results across four PCAPs and six open models show that hybrid retrieval (BM25 plus dense embeddings with reranking) substantially improves BLEU, ROUGE, METEOR, and BERTScore over dense-only retrieval, while also reducing verbosity by 45.6%. System profiling confirms low CPU, GPU, and memory overhead, demonstrating Revelation's efficiency and suitability for local deployment.

## Method Summary
Revelation processes IoT PCAP files through Zeek (protocol logs) and tshark (packet JSON), enriches them with flow summaries, anomaly detection via fine-tuned BERT, and threat intelligence, then indexes all artifacts for hybrid retrieval. The system combines BM25 lexical search, dense embeddings, keyword fallback, and cross-encoder reranking to retrieve relevant evidence chunks. An LLM-guided agent reasons over retrieved context and answers traffic interpretation queries. The framework was evaluated on four PCAPs (Normal, Uploading attack, Backdoor attack, DDoS HTTP flood) using 160 ground-truth Q&A pairs across six open models, measuring text quality (BLEU, ROUGE, METEOR, BERTScore) and system efficiency.

## Key Results
- Hybrid retrieval improves BERTScore-F1 by 5.3% and reduces verbosity by 45.6% compared to dense-only retrieval
- Text quality metrics show dramatic gains: ROUGE-L improves by 146.6%, BLEU by 391.7%, and METEOR by 83.0% with hybrid approach
- System profiling confirms low CPU, GPU, and memory overhead, demonstrating efficiency for local deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval (BM25 + dense embeddings + keyword fallback + cross-encoder reranking) produces more accurate and concise answers than dense-only retrieval for PCAP-grounded queries.
- Mechanism: Dense embeddings capture latent semantic relationships; BM25 provides precise lexical matching for protocol fields and identifiers; keyword fallback handles edge-case phrasing; cross-encoder reranking models deep query-chunk interactions to finalize relevance ordering.
- Core assumption: Traffic artifacts contain both semantic patterns (dense-suitable) and precise technical identifiers (lexical-suitable), requiring complementary signals.
- Evidence anchors:
  - [abstract]: "hybrid retrieval, which combines lexical and semantic search with reranking, substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with dense-only retrieval"
  - [Section III.A.2]: "BERTScore-F1 improves by 5.3%, while structure-sensitive metrics show even more dramatic relative gains: ROUGE-L improves by 146.6%, BLEU by 391.7%, and METEOR by 83.0%"
  - [corpus]: Weak direct evidence—neighbor papers focus on anomaly detection methods, not retrieval configurations for LLM-based interpretation.
- Break condition: If traffic corpus lacks sufficient lexical diversity or if chunk boundaries fragment key identifiers across segments, BM25 contribution degrades; if cross-encoder reranker is too slow for interactive use, latency may offset quality gains.

### Mechanism 2
- Claim: Converting raw PCAPs into multiple structured representations (protocol logs, packet-layer JSON, flow summaries, anomaly reports) enables cross-layer reasoning that isolated detection cannot provide.
- Mechanism: Zeek extracts protocol semantics; tshark exports hierarchical packet headers with targeted cleaning; flow summarizer reconstructs bidirectional sessions with application cues; fine-tuned BERT classifies anomalies; threat intelligence enriches public IPs. All artifacts are chunked, embedded, and indexed for unified retrieval.
- Core assumption: Different query types require different abstraction levels—forensic traces need packet-layer detail; situational awareness needs flow-level summaries; threat context needs anomaly reports.
- Evidence anchors:
  - [Section II.G.1]: "This harmonized representation enables cross-layer querying and traversal of traffic contexts with both breadth and depth"
  - [Section I]: "questions such as 'Which DNS queries were observed during abnormal device behavior?' remain unanswerable within conventional frameworks"
  - [corpus]: Neighbor papers (Gotham Dataset 2025, Smart Surveillance) emphasize multi-feature traffic representation for detection, indirectly supporting multi-view design.
- Break condition: If chunking fragments cross-layer context (e.g., separates flow from its anomaly label), retrieval may return incomplete evidence; if protocol diversity exceeds Zeek/tshark coverage, some telemetry is lost.

### Mechanism 3
- Claim: An LLM-guided agent with bounded toolset (retrieval-and-answer, web-lookup) produces more faithful, sourced responses than a bare LLM lacking PCAP-processing capabilities.
- Mechanism: Agent perceives top-k retrieved context, reasons over gaps, and acts by invoking retrieval for capture-grounded queries or web-lookup for out-of-scope IoT domain questions. Fidelity checks verify claims against cited passages before finalization.
- Core assumption: LLMs cannot natively parse PCAPs or access real-time threat intelligence; tool-mediated architecture provides necessary grounding.
- Evidence anchors:
  - [Section II.I]: "Tool use is essential because a pretrained language model cannot parse captures, perform protocol-aware computation, or access up-to-date intelligence on its own"
  - [Section I]: "placing a language model directly atop logs or packet traces produces brittle behavior and hallucinated explanations"
  - [corpus]: No direct validation—neighbor papers do not evaluate agent-based tool invocation for traffic interpretation.
- Break condition: If agent tool selection logic misclassifies query scope (e.g., invokes web-lookup for capture-answerable questions), responses may be under-grounded; if fidelity checks are too weak, hallucinations persist.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Revelation indexes traffic artifacts and retrieves relevant chunks to ground LLM responses, reducing hallucination.
  - Quick check question: Given a query about "MQTT topics in frequent bursts," which indexed artifact type should dominate retrieval—protocol logs, flow summaries, or anomaly reports?

- Concept: Hybrid Search (BM25 + Dense + Reranking)
  - Why needed here: Protocol fields and IPs benefit from lexical matching; semantic patterns benefit from dense embeddings; reranking resolves final relevance.
  - Quick check question: If a query contains a specific IP address and a conceptual question ("Is this IP associated with scanning?"), which retrieval stage handles each part?

- Concept: Transformer-Based Anomaly Detection for IoT
  - Why needed here: BERT classifier labels traffic as benign or one of 15 attack types, providing structured threat context for downstream retrieval.
  - Quick check question: If the anomaly detector misclassifies a benign flow as "Port Scanning," how might this propagate into agent answers?

## Architecture Onboarding

- Component map:
  Ingestion: Zeek (protocol logs), tshark (packet JSON), Flow Summarizer, Feature Extractor → BERT Anomaly Detector → Threat Intel Enricher
  Vectorization: Modality-aware chunking → domain-optimized embedding model → Chroma vector store (session-isolated)
  Query: Query vectorizer → Hybrid retriever (dense + BM25 + keyword fallback) → Deduplication → Cross-encoder reranker → Top-k evidence
  Agent: Perceive-reason-act loop → Tool invocation (retrieval-and-answer, web-lookup) → Fidelity checks → Final response

- Critical path: PCAP → Zeek/tshark → Flow Summarizer → Anomaly Detector → Enricher → Chunking → Embedding → Indexing → Hybrid Retrieval → Agent Response

- Design tradeoffs:
  - Hybrid vs. dense-only: Higher accuracy (+146.6% ROUGE-L, +391.7% BLEU) at cost of +52.3% latency and ~17 MB GPU memory
  - Local vs. cloud models: Local preserves privacy and control but limits model scale; cloud offers capacity at data exposure risk
  - Multi-artifact indexing: Enables cross-layer reasoning but increases storage and indexing complexity
  - Session-based retention (3 sessions LRU): Supports recent-capture focus; limits historical comparison

- Failure signatures:
  - Retrieval returns irrelevant chunks despite hybrid approach → check chunk boundary alignment with protocol sessions; verify embedding model domain relevance
  - Agent invokes web-lookup for capture-grounded queries → review intent classification logic in agent instructions
  - Anomaly detector confuses reconnaissance classes → inspect feature serialization for discriminative signals; verify training data balance
  - High verbosity in responses → verify reranking is active; dense-only runs show 51.5% more tokens

- First 3 experiments:
  1. Run dense-only vs. hybrid retrieval on same 160-question benchmark; compute BLEU, ROUGE, METEOR, BERTScore; expect 5–6% BERTScore-F1 gain and ~45% token reduction with hybrid.
  2. Profile system overhead (CPU, memory, GPU, latency) for both configurations; hybrid should show ~7–8 MB memory increase and ~17 MB GPU increase—confirm suitability for local deployment.
  3. Evaluate BERT anomaly detector on Edge-IIoTset 80/20 split; expect >99.8% accuracy; analyze confusion matrix for class-wise gaps (e.g., Fingerprinting vs. Port Scanning).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuned anomaly detector generalize to distinct IoT environments or zero-day attacks not represented in the Edge-IIoTset training data?
- Basis in paper: [inferred] The paper reports high anomaly detection performance (99.88% accuracy) using an 80/20 stratified split of the Edge-IIoTset dataset but does not evaluate the model on external datasets or unseen attack types.
- Why unresolved: Training and testing on the same dataset, even with stratification, does not demonstrate robustness against the diverse device configurations, protocol implementations, or novel threats found in different operational IoT networks.
- What evidence would resolve it: Performance metrics (accuracy, F1-score) from cross-dataset validation experiments where the model is trained on Edge-IIoTset and tested on separate datasets like Bot-IoT, MedBIoT, or real-world enterprise captures.

### Open Question 2
- Question: To what extent do the automated text quality metrics (BLEU, ROUGE) used in the evaluation correlate with the actual utility of the interpretations for human network operators?
- Basis in paper: [inferred] The evaluation relies exclusively on automated NLP metrics (BLEU, ROUGE, METEOR, BERTScore) to assess answer quality, despite the framework's goal of producing "human-readable" and "actionable" interpretations for operators.
- Why unresolved: Automated metrics measure lexical and semantic similarity to ground truth references but often fail to capture the logical coherence, factual accuracy, or operational value of a response from a human expert's perspective.
- What evidence would resolve it: A user study involving network administrators or security analysts rating the relevance, clarity, and actionability of the generated interpretations compared to the automated metric scores.

### Open Question 3
- Question: Can the framework be adapted to support real-time, streaming analysis of live network traffic rather than processing static PCAP files?
- Basis in paper: [inferred] While the Introduction notes that IoT networks generate "continuous streams," the System Design explicitly relies on ingesting "user-provided PCAPs," and the efficiency analysis measures batch query latency rather than streaming throughput.
- Why unresolved: The current architecture requires complete ingestion, feature extraction, and indexing into a vector database (Chroma) before querying; it is unclear if this pipeline can operate with sufficiently low latency to interpret live traffic events as they occur.
- What evidence would resolve it: System profiling data (CPU/GPU usage, latency) during a live ingestion scenario where the framework analyzes packets from a live network interface (e.g., via Zeek live mode) in near-real-time.

## Limitations
- Reported BLEU, ROUGE, and METEOR gains depend on the specific ground-truth Q&A set and may not generalize to arbitrary traffic queries
- The anomaly detector's strong performance on Edge-IIoTset assumes the dataset's 15 classes cover realistic IoT threats; real-world traffic may exhibit novel attack patterns
- The cross-encoder reranker introduces computational overhead that could limit interactive deployment in constrained edge devices
- The 3-session retention policy enables recent-capture focus but restricts longitudinal analysis of persistent threats

## Confidence

- **High confidence**: Hybrid retrieval mechanism combining BM25, dense embeddings, keyword fallback, and cross-encoder reranking demonstrably improves retrieval quality metrics (BLEU +391.7%, ROUGE-L +146.6%, METEOR +83.0%) while reducing verbosity by 45.6% compared to dense-only retrieval.
- **Medium confidence**: Multi-representation indexing (protocol logs, packet JSON, flow summaries, anomaly reports) enables cross-layer reasoning for complex traffic queries, though this is primarily supported by architectural design rather than direct empirical validation.
- **Low confidence**: Agent tool invocation logic reliably distinguishes capture-grounded queries from out-of-scope IoT domain questions; no direct evaluation of tool selection accuracy is provided.

## Next Checks

1. Evaluate retrieval quality on a held-out test set of traffic queries not included in the 160-question benchmark to assess generalization.
2. Profile agent tool selection accuracy by annotating a sample of queries with expected tool choice and comparing against actual agent behavior.
3. Test system resilience to novel IoT attack patterns by injecting unseen attack traffic into the pipeline and measuring anomaly detection and query-answering performance.