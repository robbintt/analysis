---
ver: rpa2
title: Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment
arxiv_id: '2512.18566'
source_url: https://arxiv.org/abs/2512.18566
tags:
- systems
- dform
- linear
- nonlinear
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DFORM (Diffeomorphic vector field alignment FOR learned Models)
  is a framework for comparing high-dimensional dynamical systems like recurrent neural
  networks, addressing the challenge that learned models often lack shared coordinate
  systems and that identifying low-dimensional motifs in nonlinear systems is intractable.
  DFORM learns a diffeomorphic transformation that aligns vector fields between two
  models in a one-to-one manner, enabling comparison of their underlying mechanisms
  despite coordinate system differences.
---

# Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment

## Quick Facts
- **arXiv ID**: 2512.18566
- **Source URL**: https://arxiv.org/abs/2512.18566
- **Reference count**: 20
- **One-line primary result**: DFORM learns diffeomorphic transformations that align vector fields between dynamical systems, enabling comparison despite coordinate system differences and identifying low-dimensional motifs in high-dimensional systems.

## Executive Summary
DFORM addresses the fundamental challenge of comparing high-dimensional dynamical systems like recurrent neural networks when they lack shared coordinate systems. Traditional comparison methods fail because learned models often operate in arbitrary coordinate frames, making direct state-space comparison meaningless. DFORM overcomes this by learning a smooth, invertible transformation that aligns the directional geometry of vector fields rather than the trajectories themselves. The method achieves this through pushforward vector field alignment using Neural ODEs, enabling one-to-one correspondence between system dynamics while preserving topological structure. DFORM also enables discovery of dynamical motifs by projecting high-dimensional systems onto low-dimensional templates, successfully identifying limit cycles and saddle points in complex systems.

## Method Summary
DFORM learns a diffeomorphic transformation φ that aligns vector fields between two dynamical systems f and g by maximizing orbital similarity - the cosine similarity between normalized vectors in the transformed coordinate space. The transformation is modeled as a composition of a linear affine layer and the flow of a time-invariant vector field parameterized by a Neural ODE, ensuring smoothness and invertibility. Training occurs in two stages: first optimizing only the linear component to find rough alignment, then fine-tuning the full nonlinear transformation. The method uses a multi-component loss including orbital similarity, pushforward distribution alignment, and Jacobian regularization, with sampling from distributions covering regions of interest in state space.

## Key Results
- Achieved >0.95 cosine similarity for equivalent linear systems, demonstrating accurate topological alignment
- Successfully identified saddle limit cycles in 18-dimensional systems by matching to 2D Hopf templates
- Robust to hyperparameter variations and performed well across multiple dynamical systems including Van der Pol oscillators and RNNs trained on fMRI data

## Why This Works (Mechanism)

### Mechanism 1: Pushforward Vector Field Alignment
DFORM enables comparison of systems with disparate coordinate systems by learning a transformation that aligns the directional geometry of underlying vector fields rather than state trajectories themselves. It operationalizes "smooth orbital equivalence" by learning a diffeomorphism φ and minimizing angular differences (maximizing cosine similarity) between the transformed source field φ_* f and target field g. This aligns system "orbits" while ignoring speed differences along trajectories. The method assumes systems can be related by smooth invertible coordinate transformations or share topological features that survive nonlinear warping.

### Mechanism 2: Neural ODE-based Diffeomorphism
The framework models complex invertible coordinate transformations by composing a linear affine map with the continuous flow of a time-invariant vector field parameterized by a Neural ODE. Unlike standard neural networks that may not be strictly invertible, this construction guarantees the mapping is a diffeomorphism by integrating the vector field forward and backward in time. The required coordinate transformation is approximated by continuous deformation of space over a fixed time interval driven by a stationary velocity field.

### Mechanism 3: Template Matching for Motif Discovery
DFORM identifies "improbable" dynamical features in high-dimensional systems by projecting them onto low-dimensional canonical templates. Instead of comparing two data-fit models, it aligns a high-dimensional system f to a low-dimensional template g (e.g., canonical Hopf bifurcation). By optimizing for invariant manifold alignment, it finds submanifolds in high-dim space that behave exactly like the low-dim motif, revealing hidden dynamical structure.

## Foundational Learning

- **Concept: Diffeomorphism & Orbital Equivalence**
  - **Why needed here**: DFORM relies on the premise that two systems are "equivalent" if their trajectories can be matched by a smooth coordinate change, regardless of specific coordinate values
  - **Quick check question**: Can you explain why two systems with identical limit cycles but stretched coordinate axes are considered "orbitally equivalent"?

- **Concept: Pushforward of a Vector Field**
  - **Why needed here**: The core loss function operates on the transformed vector field (φ_* f), not the original. Understanding how a vector at a point changes when the coordinate system is warped is essential
  - **Quick check question**: If a coordinate transformation scales the x-axis by 2, how does the velocity vector component ẋ transform?

- **Concept: Neural Ordinary Differential Equations (Neural ODEs)**
  - **Why needed here**: The architecture uses an ODE solver to calculate the transformation φ. Understanding this explains why the model is invertible (simply integrate backwards) and continuous
  - **Quick check question**: Why is a Neural ODE naturally suited to modeling a diffeomorphism compared to a standard ResNet?

## Architecture Onboarding

- **Component map**: Samples x∼p_x, y∼p_y -> Linear Layer H + Neural ODE v(x) -> Transformation φ -> Vector Fields f(x), g(y) -> Orbital Similarity Loss via Cosine Similarity between Jacobian-transformed source vectors and target vectors

- **Critical path**: 
  1. **Initialization**: Start with identity mapping
  2. **Linear Pre-training**: Train only the linear layer H to find rough alignment (minimizes risk of bad local minima)
  3. **Non-linear Training**: Unfreeze Neural ODE; train full system to minimize directional mismatch
  4. **Inverse Computation**: During training, φ⁻¹ is computed by reversing ODE integration time

- **Design tradeoffs**:
  - **Speed vs. Topology**: Default loss normalizes vectors (ignoring speed). For exact temporal dynamics, switch to unnormalized magnitude comparison
  - **Sampling Distribution**: Alignment is only valid where samples are drawn (usually standard normal or asymptotic states). Model ignores behavior in unsampled regions

- **Failure signatures**:
  - **Solution Multiplicity**: Model aligns vector fields but learned φ diverges from ground truth (Commutant problem). Expected; check alignment metrics, not specific φ map
  - **Distribution Drift**: If alignment loss decreases but pushforward distribution looks distorted, regularization on Neural ODE (||v||²) may be too weak

- **First 3 experiments**:
  1. **Sanity Check (Linear)**: Train Linear-DFORM on two linear systems with known orthogonal relationship. Verify cosine similarity > 0.95
  2. **Non-Linear Warp (Van der Pol)**: Train on oscillators with different μ parameters. Compare Linear-DFORM vs Full Neural ODE-DFORM performance on non-linear limit cycle shape
  3. **Motif Discovery**: Embed stable limit cycle in 20-dim system mixed with noise. Use DFORM to project to 2D Hopf template and verify cycle recovery in projected coordinates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can DFORM learn transformations directly from raw timeseries data rather than requiring explicit generative models?
- **Basis in paper**: [explicit] "The first direction is to learn the transformation directly from timeseries data instead of the generative models."
- **Why unresolved**: Current implementation relies on explicit vector fields f and g, whereas raw data requires approximating velocities via temporal differences
- **What evidence would resolve it**: Successful alignment results using velocity approximations derived from observed timeseries data

### Open Question 2
- **Question**: How can DFORM be mathematically extended to handle controlled systems with arbitrary time-varying inputs?
- **Basis in paper**: [explicit] "However, it is more challenging to handle time-varying inputs. We are currently actively investigating into this problem."
- **Why unresolved**: Current formulation absorbs only constant or piecewise-constant inputs into autonomous vector fields
- **What evidence would resolve it**: A derived formulation and successful implementation for systems driven by non-constant input functions

### Open Question 3
- **Question**: Do existing alignment methods produce qualitatively similar results to DFORM on standard neuroscience benchmarks?
- **Basis in paper**: [explicit] "Finally, it could be interesting to survey popular alignment and comparison methods... to understand whether they produce qualitatively similar or very different results."
- **Why unresolved**: DFORM was validated primarily against linear baselines rather than comprehensive suite of nonlinear comparison techniques like MARBLE
- **What evidence would resolve it**: A comparative study applying DFORM and other methods to representative set of computational models

## Limitations
- **Topological Assumptions**: Method assumes systems share common limit sets that can be related by diffeomorphism, breaking down for topologically distinct systems like chaotic vs. periodic systems
- **Non-identifiability**: Multiple coordinate transformations can produce equivalent alignments, meaning learned φ may differ significantly from "true" transformation without affecting alignment metrics
- **Distribution Dependence**: Alignment quality depends heavily on sampling distribution, only guaranteeing alignment where samples are drawn and potentially missing critical dynamics in unsampled regions

## Confidence
- **High Confidence**: The orbital similarity loss correctly measures topological equivalence for linear systems; the Neural ODE formulation guarantees diffeomorphism properties; the two-stage training procedure improves convergence
- **Medium Confidence**: Performance on nonlinear systems generalizes beyond tested examples; the method reliably identifies dynamical motifs in high-dimensional systems; hyperparameter robustness across diverse dynamical systems
- **Low Confidence**: Scalability to very high-dimensional systems (>100 dimensions); performance when comparing systems with vastly different complexity; robustness to noise in real-world applications

## Next Checks
1. **Topological Breakage Test**: Apply DFORM to a chaotic Lorenz system and periodic Van der Pol oscillator. Verify alignment scores remain low (<0.5) and method correctly identifies these systems as topologically distinct
2. **Distribution Coverage Sensitivity**: Train DFORM on Van der Pol oscillator using three sampling strategies: uniform over state space, asymptotic distribution, and restricted to small region. Measure how alignment quality varies with sampling coverage
3. **High-Dimensional Stress Test**: Embed simple 2D limit cycle in 50-dimensional system with random coupling terms. Use DFORM to project onto 2D template and verify cycle recovery. Measure how alignment degrades as dimensionality increases