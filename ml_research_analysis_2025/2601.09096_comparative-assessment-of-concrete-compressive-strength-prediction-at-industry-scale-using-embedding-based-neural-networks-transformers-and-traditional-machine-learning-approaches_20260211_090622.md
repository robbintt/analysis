---
ver: rpa2
title: Comparative Assessment of Concrete Compressive Strength Prediction at Industry
  Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine
  Learning Approaches
arxiv_id: '2601.09096'
source_url: https://arxiv.org/abs/2601.09096
tags:
- concrete
- strength
- compressive
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares machine learning models for predicting concrete
  compressive strength using a large industry-scale dataset. Five approaches are evaluated:
  linear regression, decision trees, random forests, transformer-based neural networks,
  and embedding-based neural networks.'
---

# Comparative Assessment of Concrete Compressive Strength Prediction at Industry Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine Learning Approaches

## Quick Facts
- arXiv ID: 2601.09096
- Source URL: https://arxiv.org/abs/2601.09096
- Reference count: 0
- Key result: Embedding-based neural network achieves R²=0.90, MAPE=2.50% on 28-day concrete compressive strength prediction from industry-scale data.

## Executive Summary
This study benchmarks five approaches for predicting concrete compressive strength at 7 and 28 days using a large industry-scale dataset of ~70,000 records. The methods evaluated include linear regression, decision trees, random forests, transformer-based neural networks, and embedding-based neural networks. The embedding-based neural network outperforms all others, achieving R² values of 0.87 and 0.90 for 7-day and 28-day predictions, with MAPEs of 3.37% and 2.50%, respectively. This performance is comparable to laboratory testing variability, demonstrating the model's suitability for automated quality control in large-scale construction. The embedding-based approach effectively captures nonlinear interactions among mixture design, placement, and environmental variables.

## Method Summary
The study evaluates five machine learning models for predicting concrete compressive strength from 9 input features: 7 numerical (cement content, water-cement ratio, temperatures, elapsed time, air content, slump) and 2 categorical (material code with 142 unique values, fracture type with 6 unique values). The embedding-based neural network uses learned embeddings for categorical variables (size = 2×⌊log₂K⌋ per category), concatenates them with standardized numerical inputs, applies LayerNorm, and processes through 5 fully connected blocks (Linear→LayerNorm→GELU). The output head uses Linear→ReLU. Models are trained on an 80/20 split, with performance measured by R², MAE (psi), and MAPE (%).

## Key Results
- Embedding-based NN achieves R²=0.90, MAPE=2.50% for 28-day strength prediction
- Random Forest achieves R²≈0.85-0.86, MAPE≈5.6% for 28-day prediction
- Transformer baseline achieves R²≈0.69 for 28-day prediction
- Linear regression and decision tree baselines perform significantly worse than ensemble methods

## Why This Works (Mechanism)
The embedding-based neural network outperforms traditional ML approaches by learning dense representations of categorical variables (Material Code, Fracture Type) that capture complex interactions between mixture design, placement, and environmental factors. Unlike one-hot encoding, learned embeddings can represent 142 material codes in a compact, meaningful space. The architecture's depth (5 FC blocks with GELU activations) enables modeling of highly nonlinear relationships in concrete strength development. This flexibility allows the model to match laboratory test variability, making it practical for industry deployment.

## Foundational Learning
- Concrete compressive strength prediction: Regression task predicting concrete strength at 7 and 28 days from mixture and placement variables. Why needed: Concrete strength is critical for structural safety and construction scheduling. Quick check: Verify target variables are 7-day and 28-day compressive strength values.
- Embedding layers for categorical variables: Dense vector representations for categorical features (size = 2×⌊log₂K⌋ per category). Why needed: Avoids curse of dimensionality from one-hot encoding 142 material codes. Quick check: Confirm embeddings are learned, not one-hot encoded.
- LayerNorm and GELU activations: Normalization and activation functions for deep networks. Why needed: Stabilizes training and enables modeling of nonlinear relationships. Quick check: Verify LayerNorm is applied after concatenation and before each GELU block.
- Random Forest regression: Ensemble of decision trees using bagging. Why needed: Strong baseline for tabular data with nonlinear relationships. Quick check: Confirm RF achieves R²≈0.85-0.86 with MAPE≈5.6% at 28-day.

## Architecture Onboarding
- Component map: Material Code, Fracture Type -> Embedding Layers -> Concatenate -> LayerNorm -> FC Block (×5) -> Linear -> ReLU -> CCS Prediction
- Critical path: Input features → learned embeddings → concatenation with numerical features → LayerNorm → 5×(Linear→LayerNorm→GELU) → Linear→ReLU → output
- Design tradeoffs: Embedding-based NN vs Transformer: Embedding-based NN uses learned embeddings for categoricals and processes all features through dense layers, while Transformer uses positional encodings and self-attention. Embedding-based NN achieves R²=0.90 vs Transformer's R²=0.69, suggesting dense processing better captures the tabular data's structure.
- Failure signatures: If RF baseline significantly underperforms (R² << 0.85), check for data leakage (Material Code overfitting) or improper train/test split. If embedding-based NN underperforms Transformer, verify embeddings are used instead of one-hot encoding.
- First experiments: 1) Implement embedding-based NN exactly as specified and train to verify R²≥0.87 (7-day) and R²≥0.90 (28-day). 2) Implement transformer baseline with learned position embeddings to confirm R²≈0.69. 3) Train RF baseline to confirm R²≈0.85-0.86, MAPE≈5.6%.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Embedding-based NN's exact architecture (hidden layer sizes, total parameters) is not specified
- Critical training hyperparameters (optimizer, learning rate, batch size, learning rate schedule, dropout/weight decay, early stopping criteria) are missing
- Unclear whether validation split was used for early stopping
- Train-test split methodology not explicitly stated

## Confidence
- High confidence: R²=0.90, MAPE=2.50% for 28-day prediction with embedding-based NN
- Medium confidence: Embedding-based NN's superiority over Transformer (R²=0.69) due to unspecified architectural details
- Low confidence: Exact reproduction of linear regression and decision tree baselines without regularization and tree depth settings

## Next Checks
1. Implement embedding-based NN exactly as specified (embedding sizes, LayerNorm, 5 FC blocks with GELU, ReLU output) and train using assumed hyperparameters (Adam, lr=1e-3, batch size 64-256, early stopping on validation MAE) to verify R² ≥ 0.87 at 7-day and R² ≥ 0.90 at 28-day.
2. Compare RF baseline's performance (target R² ≈ 0.85-0.86, MAPE ≈ 5.6%) to detect data leakage or train/test split issues.
3. Replicate transformer baseline using learned position embeddings (as specified) to confirm its R² ≈ 0.69 performance, validating that embedding-based NN's gain is not due to architectural differences alone.