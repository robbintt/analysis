---
ver: rpa2
title: 'LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and
  Watermarked Data'
arxiv_id: '2510.09007'
source_url: https://arxiv.org/abs/2510.09007
tags:
- unlearning
- data
- forget
- arxiv
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates how noisy forget sets\u2014\
  incomplete, rewritten, or watermarked data\u2014affect LLM unlearning performance.\
  \ Through extensive experiments with state-of-the-art methods RMU and NPO on benchmarks\
  \ WMDP and MUSE, we find that unlearning remains surprisingly robust to perturbations\
  \ when core semantic signals are preserved."
---

# LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data

## Quick Facts
- **arXiv ID:** 2510.09007
- **Source URL:** https://arxiv.org/abs/2510.09007
- **Reference count:** 40
- **Primary result:** Unlearning remains robust to noisy forget sets when core semantic signals are preserved

## Executive Summary
This study systematically investigates how noisy forget sets—incomplete, rewritten, or watermarked data—affect LLM unlearning performance. Through extensive experiments with state-of-the-art methods RMU and NPO on benchmarks WMDP and MUSE, we find that unlearning remains surprisingly robust to perturbations when core semantic signals are preserved. Our analysis reveals that surface-level variations have minimal impact on unlearning efficacy because key semantic components driving forgetting remain consistently influential despite substantial variation in surface form.

## Method Summary
The study evaluates two unlearning methods—Representation Misdirection Unlearning (RMU) and Negative Preference Optimization (NPO)—on noisy forget sets created through masking, rewriting, and watermarking. Experiments use WMDP benchmark with Zephyr-7B-beta for biosecurity knowledge and MUSE benchmark with ICLM-7B for memorized fiction. Forget sets are perturbed with 30% random token masking, LLM-based semantic rewriting, or watermarking (KGW logits-based with δ=2, SynthID sampling-based with m=4). Unlearning efficacy is measured via forget-set QA accuracy for WMDP and privacy metrics for MUSE, while general utility is assessed through MMLU accuracy.

## Key Results
- Unlearning efficacy remains stable with up to 30% token masking but degrades sharply beyond this threshold
- Rewritten and watermarked forget sets achieve comparable unlearning and utility as clean baselines when core semantic tokens are preserved
- Error sets from noisy and clean unlearning show >93% overlap, indicating semantic faithfulness of perturbations

## Why This Works (Mechanism)

### Mechanism 1: Semantic Saliency Preservation
Unlearning efficacy is robust to noise because perturbations preserve high-saliency semantic tokens while altering low-importance surface form. The optimization process assigns significantly higher gradient weights to domain-specific semantic tokens than to functional or stylistic tokens. This breaks when masking ratios exceed ~30% or watermarking strength degrades text fluency severely.

### Mechanism 2: Error Set Consistency
Models unlearned on noisy data forget the exact same factual items as models unlearned on clean data because the semantic core is preserved. The resulting error sets exhibit high overlap (>93%) with baseline unlearning, suggesting noise doesn't shift the boundary of what is forgotten, only the path taken to get there.

### Mechanism 3: Loss Invariance to Paraphrasing
State-of-the-art loss functions (RMU and NPO) rely on relative probabilities or representation geometry that are invariant to moderate lexical changes. NPO treats forget data as "negative examples" targeting the concept of the response, while RMU operates on hidden states and targets the underlying knowledge representation rather than exact tokens.

## Foundational Learning

- **Concept: Negative Preference Optimization (NPO)**
  - Why needed here: NPO is one of the two primary methods tested. Understanding that it treats forget data as "negative examples" is crucial to interpreting why it handles rewritten data well.
  - Quick check question: Does NPO minimize the probability of the response y given input x, or does it minimize the ratio of probabilities relative to a reference model?

- **Concept: Representation Misdirection Unlearning (RMU)**
  - Why needed here: RMU is the other primary method. It operates on hidden states rather than output probabilities, helping explain why surface-level noise doesn't stop it.
  - Quick check question: Does RMU attempt to minimize the loss on the output logits, or does it attempt to map the intermediate layer activations to random noise?

- **Concept: LLM Watermarking (Logits-based vs. Sampling-based)**
  - Why needed here: The paper treats watermarking as a type of noise. You must understand that logits-based watermarking (KGW) biases token selection while sampling-based (SynthID) uses tournament selection.
  - Quick check question: Which watermarking method (KGW or SynthID) was found to degrade unlearning efficacy more at high strengths, and why might that be related to text quality?

## Architecture Onboarding

- **Component map:** Forget Set -> Perturbation Engine -> Unlearner -> Saliency Analyzer -> Evaluator
- **Critical path:** Select hazardous data -> Apply perturbation -> Optimize model using NPO/RMU -> Verify efficacy drops while utility holds
- **Design tradeoffs:** Masking ratio (0-90%) vs compute/bandwidth vs semantic signal; Watermark strength vs detectability vs unlearning efficacy; Method choice (RMU vs NPO) based on knowledge type
- **Failure signatures:** Semantic drift from aggressive rewriting; Catastrophic collapse of utility; False negatives from excessive masking (>50%)
- **First 3 experiments:** 1) Masking ablation on WMDP with RMU to confirm 30% robustness threshold, 2) Saliency correlation by training on full text vs masked salient tokens only, 3) Watermark stress test with KGW at varying δ strengths measuring MMLU utility correlation

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the observed robustness generalize to adversarial data corruptions or localized unlearning techniques like targeted model editing? The study is restricted to natural perturbations and optimization-based methods, leaving the impact of adversarial corruptions and neuron-level interventions unexplored.

- **Open Question 2:** What are the causal or representation-level mechanisms that drive unlearning robustness beyond token-level saliency? The proposed saliency-based interpretation may oversimplify the true mechanisms of forgetting, explicitly calling for causal or representation-level analyses.

- **Open Question 3:** Can adversaries exploit the tolerance for low-fidelity forget data to degrade or manipulate model behavior? The discovered tolerance introduces potential risks where adversaries could exploit it by submitting low-fidelity or adversarial forget requests to degrade or manipulate model behavior.

## Limitations
- Experimental scope limited to two benchmarks (WMDP for biosecurity, MUSE for memorized fiction) and two model sizes (7B parameters)
- Watermarking analysis constrained to two specific algorithms (KGW and SynthID) with limited parameter variations
- Does not investigate temporal stability of unlearning under noisy conditions or potential for catastrophic forgetting of unrelated knowledge

## Confidence

- **Mechanism 1: Semantic Saliency Preservation** (Medium Confidence): Supported by observed robustness but lacks mechanistic proof through direct gradient attribution validation
- **Mechanism 2: Error Set Consistency** (Medium Confidence): Strong empirical finding but lacks statistical significance testing across multiple runs
- **Mechanism 3: Loss Invariance to Paraphrasing** (Medium Confidence): Theoretical basis is sound but lacks comprehensive empirical validation across perturbation strengths

## Next Checks

1. **Token-Level Gradient Attribution Study**: Conduct ablation experiments systematically varying the proportion of salient vs. non-salient tokens to directly measure their contribution to unlearning efficacy using integrated gradients or attention-based attribution methods.

2. **Latent Space Similarity Analysis**: Implement controlled paraphrasing experiments with varying semantic drift levels, then measure cosine similarity between original and paraphrased representations at RMU's targeted layer to quantify the threshold at which latent space invariance breaks down.

3. **Statistical Significance Testing of Error Set Overlap**: Perform multiple independent unlearning runs with different random seeds for each perturbation type and apply permutation tests to determine whether the reported >93% error set overlap is significantly different from random overlap.