---
ver: rpa2
title: 'Granary: Speech Recognition and Translation Dataset in 25 European Languages'
arxiv_id: '2505.13404'
source_url: https://arxiv.org/abs/2505.13404
tags:
- data
- speech
- translation
- pipeline
- granary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Granary addresses the scarcity of high-quality multilingual speech
  datasets by creating a large-scale, open-source collection of 643k hours of pseudo-labeled
  speech data across 25 European languages, targeting both ASR and AST tasks. The
  core method involves an efficient pseudo-labeling pipeline that processes noisy
  public speech corpora through audio segmentation, two-pass Whisper inference, language
  ID verification, hallucination filtering, and LLM-based punctuation and capitalization
  restoration.
---

# Granary: Speech Recognition and Translation Dataset in 25 European Languages

## Quick Facts
- **arXiv ID**: 2505.13404
- **Source URL**: https://arxiv.org/abs/2505.13404
- **Reference count**: 0
- **Primary result**: 643k hours of pseudo-labeled speech data across 25 European languages for ASR and AST tasks

## Executive Summary
Granary addresses the scarcity of high-quality multilingual speech datasets by creating a large-scale, open-source collection of 643k hours of pseudo-labeled speech data across 25 European languages, targeting both ASR and AST tasks. The core method involves an efficient pseudo-labeling pipeline that processes noisy public speech corpora through audio segmentation, two-pass Whisper inference, language ID verification, hallucination filtering, and LLM-based punctuation and capitalization restoration. For AST, it generates translation pairs using EuroLLM and applies quality estimation-based filtration. Models trained on Granary data achieve performance comparable to existing baselines using approximately 50% less data, with notable improvements on low-resource languages such as Croatian.

## Method Summary
The Granary dataset creation pipeline processes publicly available speech data through a multi-stage filtering system. First, audio is segmented into manageable chunks and passed through Whisper for initial transcription. A second Whisper pass with different parameters provides confidence scores for quality assessment. Language identification models verify that transcriptions match expected languages, while hallucination detection filters out nonsensical outputs. LLM-based models restore punctuation and capitalization to improve readability. For translation tasks, EuroLLM generates target language translations which undergo quality estimation filtering. The entire pipeline is designed for efficiency, enabling processing of massive audio corpora while maintaining reasonable quality standards for downstream model training.

## Key Results
- 643k hours of pseudo-labeled speech data across 25 European languages
- Model performance comparable to existing baselines using approximately 50% less data
- Notable improvements on low-resource languages such as Croatian

## Why This Works (Mechanism)
The effectiveness stems from the systematic multi-stage filtering approach that combines automated quality assessment with linguistic verification. The two-pass Whisper inference provides confidence metrics that enable selective data retention. Language ID verification ensures cross-lingual contamination is minimized, while hallucination filtering prevents nonsensical data from degrading model training. The LLM-based post-processing restores formatting that improves both model training efficiency and downstream usability.

## Foundational Learning
- **Pseudo-labeling**: Why needed - enables large-scale dataset creation without manual annotation; Quick check - compare pseudo-labels against small manually-verified samples
- **Quality estimation filtering**: Why needed - removes low-quality translation pairs that would harm model training; Quick check - measure correlation between quality scores and downstream BLEU scores
- **Multi-pass inference**: Why needed - different model configurations capture complementary information; Quick check - analyze agreement rates between first and second pass transcriptions

## Architecture Onboarding
- **Component map**: Audio corpus -> Segmentation -> Two-pass Whisper -> Language ID verification -> Hallucination filtering -> LLM post-processing -> Quality estimation (for AST) -> Dataset output
- **Critical path**: The two-pass Whisper inference followed by quality filtering represents the most computationally intensive and quality-critical sequence
- **Design tradeoffs**: Speed vs quality - aggressive filtering removes more data but improves overall dataset quality; Resource allocation - balancing compute costs across multiple filtering stages
- **Failure signatures**: High hallucination rates indicate model capacity issues; Language ID mismatches suggest training data contamination; Low confidence scores across datasets indicate systematic Whisper performance problems
- **First experiments**: 1) Run single-pass vs two-pass Whisper comparison on validation set; 2) Measure language ID verification accuracy on ground truth data; 3) Evaluate hallucination filtering precision-recall tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on pseudo-labeling introduces potential quality issues that are difficult to fully validate
- Quality assurance mechanisms are described but not extensively validated against ground truth data
- Performance comparison claims require careful interpretation due to unspecified baseline conditions

## Confidence
- **High confidence**: Dataset's scale and open-source availability
- **Medium confidence**: Technical pipeline's effectiveness and claimed performance improvements
- **Low confidence**: Long-term stability and generalizability of pseudo-labeled data

## Next Checks
1. Conduct human evaluation studies on randomly sampled pseudo-labels across multiple languages to establish ground truth accuracy rates and identify systematic error patterns
2. Perform ablation studies removing different filtering stages to quantify their individual contributions to overall data quality and model performance
3. Test model performance across diverse domains and recording conditions not represented in the training data to assess generalization capabilities and identify potential overfitting to pseudo-labeled data characteristics