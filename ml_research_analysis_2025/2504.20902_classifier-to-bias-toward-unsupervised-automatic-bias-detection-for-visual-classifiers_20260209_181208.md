---
ver: rpa2
title: 'Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual
  Classifiers'
arxiv_id: '2504.20902'
source_url: https://arxiv.org/abs/2504.20902
tags:
- bias
- biases
- detected
- classes
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces C2B, the first unsupervised bias detection\
  \ framework for pre-trained visual classifiers that operates without labeled data.\
  \ Given only a task description and a model, C2B uses a large language model to\
  \ generate bias proposals and corresponding captions, retrieves images via text-to-image\
  \ retrieval, and evaluates the model\u2019s accuracy across bias classes to compute\
  \ bias scores."
---

# Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers

## Quick Facts
- **arXiv ID**: 2504.20902
- **Source URL**: https://arxiv.org/abs/2504.20902
- **Reference count**: 40
- **Primary result**: First unsupervised bias detection framework that discovers biases without labeled data, outperforming supervised baselines by 7.5-12.3% in ground-truth bias detection

## Executive Summary
This paper introduces C2B, the first unsupervised bias detection framework for pre-trained visual classifiers that operates without labeled data. Given only a task description and a model, C2B uses a large language model to generate bias proposals and corresponding captions, retrieves images via text-to-image retrieval, and evaluates the model's accuracy across bias classes to compute bias scores. Evaluated on CelebA and ImageNet-X, C2B detects more ground-truth biases (7.5–12.3%) than a supervised baseline, with higher agreement (0.22–0.32) with VQA pseudo-labels. C2B achieves unbiased bias detection while eliminating the need for task-specific annotations, marking a significant step toward automated, task-agnostic bias discovery.

## Method Summary
C2B operates by first prompting an LLM with a task description to generate bias attributes and corresponding bias classes for each target class. The LLM then generates descriptive captions combining target classes and bias classes. A vision-language model retrieves images matching these captions from a large database (CC12M or Bing web search). The pre-trained classifier is evaluated on these retrieved images, and bias scores are computed based on accuracy differences across bias classes. The framework requires only a task description and pre-trained classifier, eliminating the need for labeled bias data or domain-specific knowledge.

## Key Results
- Detects 7.5-12.3% more ground-truth biases than supervised baselines on CelebA and ImageNet-X
- Achieves VQA pseudo-label agreement of 0.22-0.32, higher than supervised approaches
- Successfully identifies 27/35 ImageNet-X bias factors without requiring labeled data
- Retrieval accuracy for both target and bias classes is below 50%, yet still enables effective bias detection

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Bias Hypothesis Generation
The framework uses an LLM to generate bias proposals from a task description, creating a relevant, task-specific search space for bias discovery. The LLM is prompted to produce structured lists of potential bias attributes and corresponding bias classes that could influence classifier performance. This leverages the LLM's pre-trained knowledge of visual concepts and their attributes to propose plausible biases without requiring labeled data.

### Mechanism 2: Retrieval-Based Pseudo-Dataset Construction
A text-to-image retrieval model constructs functional pseudo-labeled datasets from LLM-generated captions. The VLM retrieves images matching descriptive captions that combine target and bias classes, treating these retrieved images as test sets where captions provide labels. This approach bypasses the need for expensive manual annotation while creating datasets for bias evaluation.

### Mechanism 3: Comparative Accuracy Scoring for Bias Detection
The framework measures differences in classifier accuracy across retrieved pseudo-groups to detect biases. By evaluating the pre-trained classifier on different bias class subsets and computing accuracy differences, C2B formalizes these performance gaps as bias scores. This assumes systematic performance differences reflect genuine model biases rather than retrieval artifacts.

## Foundational Learning

**LLM Prompt Engineering for Structured Output**: Why needed - The framework depends on consistent, machine-readable LLM output (JSON lists of attributes and classes). Quick check - Write a prompt instructing an LLM to output JSON listing potential visual biases for "facial expression recognition."

**Vision-Language Models (VLMs) for Cross-Modal Retrieval**: Why needed - The unsupervised data collection relies on VLMs like CLIP to bridge text captions and image retrieval. Quick check - Explain how CLIP creates shared embedding spaces enabling text-to-image search.

**Bias vs. Spurious Correlation**: Why needed - The goal is detecting spurious correlations (e.g., snow with snowboard) that models exploit instead of learning true features. Quick check - Give an example of a spurious correlation for "person riding a bike." What's the true feature vs. the spurious one?

## Architecture Onboarding

**Component map**: LLM Bias Proposer -> LLM Caption Generator -> VLM Retrieval Engine -> Classifier Inference -> Bias Scoring Module

**Critical path**: LLM Proposer -> Caption Generator -> Retrieval Engine -> Classifier Inference -> Scoring Module. Quality failures in initial LLM step amplify through pipeline.

**Design tradeoffs**:
- Open vs. Closed-Set Discovery: Open-set (flexible, can generate irrelevant proposals) vs. closed-set (constrained but precise)
- Retrieval Source: Web search API (diversity, high latency) vs. static database (fast, domain-limited)

**Failure signatures**:
- Trivial Biases: Proposing redundant biases (e.g., "age: old" for "young" classifier)
- Retrieval Mismatch: VLM retrieves images not matching caption semantics
- Compositional Blindness: Retrieval fails to capture target+bias conjunction

**First 3 experiments**:
1. End-to-End Replication: Run full C2B pipeline on simple task (e.g., "dogs vs. cats") to validate workflow and inspect LLM proposal quality
2. Retrieval Source Ablation: Compare CC12M vs. Bing retrieval for fixed task to quantify data diversity impact
3. Proposal vs. Ground-Truth Check: Manually define known biases for dataset and compare scores from manual list vs. LLM proposals

## Open Questions the Paper Calls Out

**Open Question 1**: How can the retrieval component be improved to handle compositional complexity and ensure higher fidelity in capturing both target classes and bias attributes? The current retrieval accuracy for matching both target and bias classes is below 50%, making bias score reliability dependent on retrieval quality.

**Open Question 2**: How can the framework be modified to automatically filter out tautological or trivial bias proposals generated by the LLM? The current LLM prompting lacks constraints to prevent proposing attributes semantically identical to target classes.

**Open Question 3**: Can C2B be extended to leverage non-visual metadata for detecting subtle, domain-specific biases in specialized fields like medical imaging? The current architecture cannot capture non-visual correlations such as hospital type or imaging device manufacturer.

## Limitations

- Retrieval accuracy below 50% for both target and bias classes, potentially introducing noise into bias detection
- LLM-generated bias proposals may include irrelevant or hallucinated biases without automatic filtering
- Computational resource intensive due to large-scale retrieval and evaluation across multiple bias proposals
- Limited evaluation on specialized domains where LLM knowledge may be insufficient

## Confidence

**High Confidence**: The unsupervised nature eliminating labeled data needs and improved ground-truth bias detection (7.5-12.3% increase) are well-supported by experimental results.

**Medium Confidence**: VQA pseudo-label agreement (0.22-0.32) validates detection quality, though absolute values suggest moderate alignment with human judgment.

**Low Confidence**: Effectiveness of LLM-driven bias proposal mechanism is uncertain due to lack of ground-truth evaluation for proposal quality itself.

## Next Checks

1. **Bias Proposal Quality Audit**: Manually evaluate LLM-generated bias proposals for a subset of classes to determine proportion of relevant, irrelevant, and hallucinated biases before they enter retrieval pipeline.

2. **Retrieval Accuracy vs. Bias Detection**: Systematically vary retrieved image count (k=10, 20, 50) and measure impact on both retrieval accuracy and bias detection performance to quantify data quantity vs. quality tradeoff.

3. **Domain Transfer Test**: Apply C2B to detect biases in classifier trained on completely different domain (e.g., medical imaging) where LLM's pre-trained knowledge may be limited, testing framework's generalizability beyond standard vision datasets.