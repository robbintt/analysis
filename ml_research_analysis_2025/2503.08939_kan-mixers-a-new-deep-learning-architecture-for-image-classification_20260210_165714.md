---
ver: rpa2
title: 'KAN-Mixers: a new deep learning architecture for image classification'
arxiv_id: '2503.08939'
source_url: https://arxiv.org/abs/2503.08939
tags:
- kan-mixers
- accuracy
- mlp-mixer
- architecture
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new deep learning architecture called KAN-Mixers
  for image classification. The KAN-Mixers architecture combines the strengths of
  MLP-Mixer and Kolmogorov-Arnold Networks (KAN) to address the limitations of existing
  architectures in refined feature extraction for images.
---

# KAN-Mixers: a new deep learning architecture for image classification

## Quick Facts
- arXiv ID: 2503.08939
- Source URL: https://arxiv.org/abs/2503.08939
- Reference count: 20
- Primary result: Proposed KAN-Mixers architecture achieves 0.9030 and 0.6980 average accuracy on Fashion-MNIST and CIFAR-10 datasets respectively using 5-fold cross-validation

## Executive Summary
This paper introduces KAN-Mixers, a novel deep learning architecture that combines the strengths of MLP-Mixer and Kolmogorov-Arnold Networks (KAN) for image classification. The architecture replaces fixed activation functions in standard MLP layers with learnable activation functions through KAN layers, potentially improving feature extraction capabilities. Evaluated on Fashion-MNIST and CIFAR-10 datasets, KAN-Mixers demonstrates superior performance compared to MLP, MLP-Mixer, and KAN baselines, though at the cost of increased computational complexity.

## Method Summary
KAN-Mixers architecture consists of patch embedding layers followed by repeated Mixer Blocks containing LayerNorm, token-mixing KAN layers, and channel-mixing KAN layers. The model uses KAN layers as the core components, replacing standard MLP layers with their learnable activation functions. The architecture was evaluated using 5-fold cross-validation on Fashion-MNIST (resized to 32x32) and CIFAR-10 datasets, with data augmentation including RandomHorizontalFlip and RandomRotation. Hyperparameter optimization was performed on CIFAR-10 to determine optimal patch size (4x4), dimension size (256), and 8 layers.

## Key Results
- Achieved 0.9030 average accuracy on Fashion-MNIST, outperforming MLP (0.8920), MLP-Mixer (0.8770), and KAN (0.8920) models
- Obtained 0.6980 average accuracy on CIFAR-10 with optimized hyperparameters
- Improvements were statistically significant at p=0.10 but not at stricter p=0.05 thresholds
- Demonstrated potential for refined feature extraction through learnable activation functions

## Why This Works (Mechanism)

### Mechanism 1
Replacing fixed activation functions with learnable ones improves feature extraction in mixer architectures. KAN layers learn activation functions (parameterized as splines) during training, allowing the network to adapt non-linearity to specific data distributions and capture more nuanced features in image patches. This mechanism assumes improved performance stems directly from activation function flexibility rather than other hyperparameter choices.

### Mechanism 2
Separating token-mixing and channel-mixing operations using distinct KAN layers allows for specialized spatial and feature-based processing. The architecture explicitly separates mixing information across spatial locations (patches/tokens) and across feature channels, with token-mixing KAN layers aggregating spatial information and channel-mixing KAN layers combining feature information. This separation of concerns is beneficial and KAN layers are capable of effectively handling both mixing tasks.

### Mechanism 3
Hyperparameter optimization is critical for achieving competitive performance, particularly on complex datasets like CIFAR-10. The paper employs random search hyperparameter optimization to identify optimal configurations (patch size 4x4, dim size 256, 8 layers, specific learning rate) that maximize validation accuracy. This suggests the model's capacity and receptive field are highly sensitive to these parameters and must be tuned to dataset characteristics.

## Foundational Learning

- **Kolmogorov-Arnold Networks (KAN)**: Core component replacing MLPs, placing learnable activation functions (typically B-splines) on edges rather than using fixed activations on nodes. *Quick check: In a KAN layer, where are the learnable non-linearities applied compared to a standard MLP layer?*

- **MLP-Mixer Architecture**: Architecture pattern that KAN-Mixers modifies, with data flow through patch embedding, Mixer Blocks (token-mixing and channel-mixing), and classifier head. *Quick check: What are the two distinct operations performed sequentially in a standard MLP-Mixer Block?*

- **Token Mixing vs. Channel Mixing**: Fundamental inductive bias of the architecture, understanding how transposition operation switches tensor orientation to apply mixing operations. *Quick check: To mix information across different spatial patches (tokens), on which dimension of the feature tensor would the operation be applied?*

## Architecture Onboarding

- **Component map**: Image Input -> Patch Embedding -> [LayerNorm -> Token KAN -> Channel KAN] (x8 layers) -> Pooling -> Classifier -> Output

- **Critical path**: Image Input -> Patch Embedding -> [LayerNorm -> Token KAN -> Channel KAN] (x8 layers) -> Pooling -> Classifier -> Output

- **Design tradeoffs**: 
  * Performance vs. Training Speed: KAN-Mixers may offer higher accuracy but KAN layers are computationally more expensive than standard MLPs
  * Data Efficiency: KAN-Mixers might be more data-efficient than MLP-Mixer for refined feature extraction, but this requires further investigation
  * Statistical Significance: Improvements were statistically significant at p=0.10 but not always at stricter p=0.05, indicating marginal performance gains

- **Failure signatures**: 
  * Slow Training: Ensure efficient KAN implementation is used rather than reference mathematical implementation
  * Poor Convergence on New Data: Re-run hyperparameter optimization if model fails to converge on new dataset
  * Overfitting: Apply data augmentation and ensure Dropout is active within Mixer Blocks

- **First 3 experiments**:
  1. Reproduce baseline: Re-train KAN-Mixers on Fashion-MNIST using hyperparameters from Table 2 to verify average accuracy near 0.9030
  2. Ablation study: Replace KAN layers in Mixer Block with standard MLP layers (using same hyperparameters) to measure performance delta
  3. Sensitivity analysis: Vary patch size (e.g., 8x8, 16x16) while keeping other hyperparameters fixed to observe impact on training time and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Does the KAN-Mixers architecture yield improved interpretability compared to standard MLP-Mixers? The introduction states KANs promise improved interpretability, but results section focuses exclusively on performance metrics without analyzing interpretability. No visualization or symbolic analysis of learnable activation functions was conducted to validate this specific claim.

### Open Question 2
Can KAN-Mixers maintain performance advantage on large-scale datasets (e.g., ImageNet)? Paper acknowledges MLP-Mixer usually requires large datasets to be competitive, but experiments are restricted to small datasets (CIFAR-10, Fashion-MNIST). It is unclear if KAN-based token-mixing scales efficiently with input size or if accuracy gains persist against state-of-the-art models.

### Open Question 3
How can computational efficiency of KAN-Mixers be improved to mitigate training latency? Conclusion identifies computational cost as deficiency, noting model is viable only when there are no restrictions regarding time required to train. While efficient implementation was used, fundamental overhead of learning activation functions via B-splines remains bottleneck.

## Limitations
- Performance improvements are marginal and statistically significant only at p=0.10 threshold, not stricter p=0.05
- Computational cost of KAN layers versus performance benefits is not explicitly analyzed, making practical viability unclear
- Specific mechanism by which learnable KAN activations outperform fixed MLP activations is not directly tested through controlled ablation experiments

## Confidence
- **High confidence**: Architectural framework combining KAN layers with MLP-Mixer's token/channel mixing structure is clearly specified and implementable
- **Medium confidence**: Performance improvements on Fashion-MNIST and CIFAR-10 are reported with proper cross-validation but rely on precise hyperparameter tuning and unvalidated baseline
- **Low confidence**: Specific mechanism by which learnable KAN activations outperform fixed MLP activations is not directly tested through controlled ablation experiments

## Next Checks
1. Implement the Efficient-KAN ablation: Replace KAN layers with standard MLP layers (using GELU activation) in Mixer Block while keeping all other hyperparameters identical, then measure performance difference on Fashion-MNIST
2. Scale to larger datasets: Evaluate KAN-Mixers on ImageNet-1K or Food-101 to test whether architecture maintains competitive performance on more complex, real-world image classification tasks
3. Analyze computational efficiency: Profile training time and memory usage of KAN-Mixers versus MLP-Mixer on identical hardware, calculating accuracy gain per training second to assess practical viability