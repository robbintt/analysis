---
ver: rpa2
title: 'AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment'
arxiv_id: '2512.03634'
source_url: https://arxiv.org/abs/2512.03634
tags:
- factual
- consistency
- association
- text
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AlignCheck, an interpretable framework for
  factual consistency assessment in both general and clinical domains. The method
  decomposes texts into atomic facts and introduces a flexible, schema-free methodology.
---

# AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment

## Quick Facts
- arXiv ID: 2512.03634
- Source URL: https://arxiv.org/abs/2512.03634
- Reference count: 9
- Introduces a schema-free framework for factual consistency assessment using atomic fact decomposition and weighted F1-BERTScore metrics

## Executive Summary
AlignCheck is a semantic metric framework for assessing factual consistency in text summarization across general and clinical domains. The method decomposes texts into atomic facts represented as (subject, predicate, object) triples and computes a weighted F1 score combined with BERTScore for soft predicate similarity. Unlike previous approaches, AlignCheck is schema-free and flexible, incorporating entity type TF-IDF weighting to control assessment complexity in intricate domains. The framework is evaluated on two datasets—AgreeFact for general texts and MIMIC-IV-Ext-BHC for clinical texts—demonstrating statistically significant differences between summarization models and highlighting the importance of fine-tuning strategy.

## Method Summary
AlignCheck decomposes source and target texts into atomic facts using named entity recognition (Spacy for general texts, MedCat for clinical texts). Each fact is represented as a (subject, predicate, object) triple, with predicates extracted from context around entities. The framework computes factual consistency by matching these triples between source and target using a combination of hard matching (on subject-object pairs) and soft matching (via BERTScore on predicates). A weighted F1 score is calculated, incorporating TF-IDF weights for entity types to prioritize more discriminative entities. The method is evaluated using statistical significance tests (Friedman test with post-hoc comparisons) on summarization models trained on both general and clinical datasets.

## Key Results
- AlignCheck achieves statistically significant differences between summarization models on both AgreeFact and MIMIC-IV-Ext-BHC datasets
- Performance varies significantly depending on summarization strategy, with fine-tuned instruction models showing superior factual consistency
- The weighted F1-BERTScore combination provides interpretable scoring that captures both exact and semantic predicate matches

## Why This Works (Mechanism)
AlignCheck works by breaking down complex text into atomic, verifiable facts and then comparing these facts between source and summary using both exact and semantic matching. The weighted F1 score emphasizes important entities while the BERTScore component captures semantic similarity in predicates, making the metric robust to paraphrasing while maintaining factual accuracy.

## Foundational Learning
- **Named Entity Recognition**: Required for identifying subjects and objects in text. Why needed: Forms the foundation of fact extraction. Quick check: Verify entity recognition accuracy on sample clinical and general texts.
- **Fact Decomposition**: Converting text into (subject, predicate, object) triples. Why needed: Enables atomic fact comparison. Quick check: Manually inspect extracted triples for semantic validity.
- **BERTScore**: Computes semantic similarity between predicates. Why needed: Captures paraphrasing and semantic equivalence. Quick check: Compare BERTScore outputs for semantically equivalent vs. different predicates.
- **Weighted F1 Scoring**: Incorporates TF-IDF weights to prioritize important entities. Why needed: Prevents rare but important entities from being overlooked. Quick check: Verify weight distribution across entity types.
- **Statistical Significance Testing**: Friedman test with post-hoc comparisons. Why needed: Validates that observed differences between models are meaningful. Quick check: Replicate p-values on small sample datasets.
- **Clinical NER**: MedCat for identifying medical entities. Why needed: General NER tools miss domain-specific medical concepts. Quick check: Compare MedCat vs. Spacy on clinical sample texts.

## Architecture Onboarding
**Component Map**: Text -> NER Pipeline -> Fact Extraction -> Triple Matching -> Weighted F1-BERTScore
**Critical Path**: Fact extraction and matching is the bottleneck; errors here propagate through the entire scoring pipeline
**Design Tradeoffs**: Schema-free approach sacrifices some precision for flexibility across domains, but requires robust predicate extraction
**Failure Signatures**: Empty fact sets (NER failure), BERTScore near zero (predicate extraction issues), inflated scores (incorrect weighting)
**First Experiments**: 1) Test NER accuracy on 10 sample documents from each domain, 2) Verify predicate extraction by manually inspecting generated triples, 3) Compare AlignCheck scores against human judgments on 20 sample summaries

## Open Questions the Paper Calls Out
- Can AlignCheck's scoring metric be integrated as a differentiable soft constraint within the objective function of generative models to improve factual consistency during training?
- Can True Negatives (TNs) and False Positives (FPs) serve as effective negative samples, and True Positives (TPs) as positive samples, for training a contrastive learning algorithm that improves factual generation?
- How robust is the AlignCheck scoring algorithm in highly noisy, real-world clinical or open-domain scenarios compared to the controlled datasets evaluated in this study?
- Does integrating AlignCheck into end-to-end generative model training improve downstream task performance, and what is the magnitude of this effect?

## Limitations
- Predicate extraction method is underspecified, preventing exact reproduction of the atomic fact decomposition pipeline
- Fine-tuning configurations for Llama 3-13B (learning rates, batch sizes, prompt formats) are not provided
- Clinical domain experiments lack detailed preprocessing and hyperparameter information
- Evaluation primarily focused on controlled datasets; real-world robustness remains unvalidated

## Confidence
- High confidence in general framework description (fact decomposition, weighted F1 concept, BERTScore integration)
- Medium confidence in exact implementation details (predicate extraction, TF-IDF weighting, BERTScore configuration)
- Low confidence in clinical domain experiments (fine-tuning procedures, exact data preprocessing)

## Next Checks
1. Verify predicate extraction by running the NER pipeline on sample AgreeFact documents and manually inspecting generated (s, p, o) triples
2. Confirm BERTScore implementation by comparing predicate similarity scores against ground truth annotations on a small validation set
3. Replicate the statistical significance analysis using Friedman test on model rankings from the AgreeFact dataset to verify reported p-values