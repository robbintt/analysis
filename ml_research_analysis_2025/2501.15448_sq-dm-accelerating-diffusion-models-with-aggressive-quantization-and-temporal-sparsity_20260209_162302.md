---
ver: rpa2
title: 'SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal
  Sparsity'
arxiv_id: '2501.15448'
source_url: https://arxiv.org/abs/2501.15448
tags:
- sparsity
- diffusion
- quantization
- sparse
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating diffusion models
  for high-quality image generation, which is computationally intensive due to the
  need for repeated model evaluations over many time steps. The authors propose a
  novel approach that combines aggressive 4-bit quantization of both weights and activations
  with temporal sparsity exploitation.
---

# SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity

## Quick Facts
- arXiv ID: 2501.15448
- Source URL: https://arxiv.org/abs/2501.15448
- Reference count: 40
- Achieves 6.91× speedup and 51.5% energy reduction with 4-bit quantization

## Executive Summary
This paper addresses the computational intensity of diffusion models by combining aggressive 4-bit quantization with temporal sparsity exploitation. The authors propose a mixed-precision quantization scheme that maintains high-precision blocks for sensitive layers while aggressively quantizing the majority of the model to 4-bit. They replace SiLU activations with ReLU to enable efficient unsigned 4-bit quantization and induce 65% activation sparsity. A heterogeneous dense-sparse accelerator architecture is designed to exploit the observed temporal per-channel sparsity patterns, achieving significant speedup and energy reduction while maintaining state-of-the-art image generation quality.

## Method Summary
The method involves replacing SiLU activations with ReLU in pre-trained EDM models, followed by finetuning for less than 10% of original training time. A block-wise sensitivity analysis identifies which layers can tolerate aggressive 4-bit quantization, leading to a mixed-precision approach where sensitive layers (first/last blocks, Skip, Embedding, Attention) use MXINT8 while Conv+ReLU blocks use INT4 with FP8 fine-grained scaling. The temporal per-channel sparsity pattern is exploited through a heterogeneous accelerator with dense and sparse processing elements, using a 30% threshold to route channels appropriately.

## Key Results
- 6.91× speedup and 51.5% energy reduction compared to FP16 dense baseline
- Maintains state-of-the-art FID scores: 1.89 (CIFAR-10), 2.20 (AFHQv2), 3.36 (FFHQ), 3.52 (ImageNet)
- Achieves 65% average activation sparsity through ReLU replacement
- Mixed-precision quantization enables 4-bit quantization for >90% of model compute

## Why This Works (Mechanism)

### Mechanism 1: Mixed-Precision Block Selection
The authors profile block-wise quantization sensitivity by isolating one block at 4-bit while others remain at 8-bit. Sensitive blocks (first/last ~5% of compute) stay at MXINT8; Conv+SiLU blocks (>90% of compute) drop to 4-bit with FP8 scale factors. The core assumption is that quantization error from middle layers doesn't propagate destructively through skip connections to degrade output quality.

### Mechanism 2: ReLU Replacement for Unsigned Quantization and Sparsity
SiLU outputs range [-0.278, ∞), requiring signed INT4 that wastes 6 of 16 levels on rarely-used negative values. ReLU outputs [0, ∞), allowing UINT4 that uses all 16 levels. ReLU's zero-clamping naturally creates sparsity (~65% average), which is crucial for hardware acceleration.

### Mechanism 3: Temporal Per-Channel Sparsity Partitioning
Activation sparsity in ReLU-based diffusion models exhibits predictable per-channel patterns that evolve across time steps. Channels exceeding 30% sparsity threshold route to sparse PE; denser channels route to dense PE. The sparsity detector updates channel classification every time step to maintain load balance.

## Foundational Learning

- **Concept: Uniform symmetric quantization with per-block scaling**
  - Why needed here: The paper builds on MXINT8 and VSQ formats; understanding why fine-grained scaling outperforms per-channel scaling is essential for interpreting Table I.
  - Quick check question: Given a tensor X with max(|X|)=12.7 and 4-bit unsigned quantization, what is the quantization scale factor?

- **Concept: U-Net architecture with skip connections**
  - Why needed here: The paper quantizes Skip blocks differently from Conv+SiLU blocks; understanding information flow explains why first/last blocks are sensitive.
  - Quick check question: In a U-Net, how does a skip connection from encoder block 3 to decoder block 3 affect gradient flow during finetuning?

- **Concept: Sparse accelerator efficiency threshold**
  - Why needed here: The paper notes unstructured sparsity requires ~87.5% for GPU speedup; understanding this motivates the per-channel structured approach.
  - Quick check question: Why does 65% random sparsity not yield speedup on standard GPUs, but 65% per-channel sparsity can be exploited?

## Architecture Onboarding

- **Component map:** Controller -> Sparsity-aware address generator -> Dense PE (DPE) and Sparse PE (SPE) -> Global buffer -> PPU with temporal sparsity detector

- **Critical path:** Controller signals sparsity detector to classify channels from previous layer output → Address generator fetches dense channels to DPE, sparse channels to SPE → DPE and SPE compute in parallel → Results merged and passed through PPU for next-layer sparsity detection → Process repeats across all layers, then advances to next time step

- **Design tradeoffs:**
  - Sparsity threshold (30%): Lower threshold → more channels marked sparse → SPE overload; higher threshold → DPE overload
  - Update frequency (every timestep): More frequent updates improve classification accuracy; overhead is minimal compared to compute
  - DPE/SPE ratio: Paper assumes 1:1 (128 multipliers each); different workloads may require rebalancing

- **Failure signatures:**
  - FID spike in early/late layers → sensitive blocks incorrectly quantized to 4-bit
  - Load imbalance (one PE idle while other saturated) → sparsity threshold misconfigured
  - Memory fetch stalls → channel-last mapping not aligned with sparse PE access pattern

- **First 3 experiments:**
  1. Block sensitivity profiling: Quantize one block at a time to 4-bit while keeping others at 8-bit; measure FID degradation to identify sensitive blocks for your target model
  2. ReLU finetuning convergence: Replace SiLU with ReLU in pretrained model; finetune and plot FID vs training iterations to confirm quality recovery
  3. Sparsity threshold sweep: Vary threshold from 10-50%; measure dense/sparse PE utilization balance and end-to-end speedup to find optimal operating point

## Open Questions the Paper Calls Out

- Can the proposed temporal per-channel sparsity exploitation and 4-bit quantization techniques be effectively adapted for video diffusion models? The paper explicitly plans to extend techniques to video generation but acknowledges the additional temporal dimension may alter activation sparsity patterns.

- Does the channel-last memory mapping and dense/sparse processing architecture generalize efficiently to Transformer-based diffusion models (DiTs)? The current approach is optimized for convolution-based U-Nets and may not align with the matrix-multiplication heavy operations in DiTs.

- Would implementing Quantization-Aware Training (QAT) for the 4-bit ReLU-based model yield significant quality improvements over the proposed finetuning and PTQ approach? The authors expect incremental quality gain but chose finetuning + PTQ to avoid QAT overhead.

## Limitations

- The methodology depends on specific temporal per-channel sparsity patterns that may not generalize across all diffusion models or tasks
- The 30% sparsity threshold and other architectural parameters may need re-optimization for different models or datasets
- Quantization sensitivity analysis is limited to EDM models, potentially limiting generalizability to other diffusion architectures
- ReLU replacement assumes smooth finetuning recovery within 10% of pretraining time, which may not hold for larger or more complex models

## Confidence

- **High Confidence:** Mixed-precision quantization sensitivity results showing first/last blocks being sensitive while middle blocks tolerate 4-bit quantization
- **Medium Confidence:** 65% activation sparsity figure and effectiveness of temporal per-channel sparsity detector
- **Low Confidence:** Scalability claim that 6.91× speedup and 51.5% energy reduction will hold for larger diffusion models or different generation tasks

## Next Checks

1. **Generalization Test:** Apply the same methodology to a different diffusion model architecture (e.g., Latent Diffusion) and measure whether the same block sensitivity patterns and quantization thresholds hold.

2. **Sparsity Robustness:** Systematically vary the sparsity threshold from 10% to 50% and measure the impact on both FID quality and hardware efficiency to identify the sensitivity of the heterogeneous PE mapping.

3. **Scaling Analysis:** Evaluate the method on progressively larger diffusion models (e.g., doubling parameter count) to determine whether the speedup/energy reduction scales proportionally or if there are diminishing returns.