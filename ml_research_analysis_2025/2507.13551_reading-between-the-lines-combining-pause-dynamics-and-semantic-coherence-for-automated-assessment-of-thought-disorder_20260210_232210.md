---
ver: rpa2
title: 'Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence
  for Automated Assessment of Thought Disorder'
arxiv_id: '2507.13551'
source_url: https://arxiv.org/abs/2507.13551
tags:
- coherence
- pause
- speech
- semantic
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that pause dynamics derived from ASR timestamps\
  \ significantly improve the automated prediction of formal thought disorder (FTD)\
  \ when combined with semantic coherence metrics. Using three datasets with different\
  \ speech contexts, the integration of pause features with semantic coherence\u2014\
  particularly through late fusion of separate models\u2014consistently enhanced predictive\
  \ performance (Spearman correlation up to 0.649, AUC up to 83.71%) compared to semantic-only\
  \ approaches."
---

# Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder

## Quick Facts
- arXiv ID: 2507.13551
- Source URL: https://arxiv.org/abs/2507.13551
- Reference count: 0
- Key outcome: Pause dynamics significantly improve automated prediction of formal thought disorder when combined with semantic coherence metrics

## Executive Summary
This study demonstrates that pause dynamics derived from ASR timestamps significantly improve the automated prediction of formal thought disorder (FTD) when combined with semantic coherence metrics. Using three datasets with different speech contexts, the integration of pause features with semantic coherence—particularly through late fusion of separate models—consistently enhanced predictive performance compared to semantic-only approaches. Pause features alone also robustly predicted FTD severity across datasets. These results indicate that temporal and semantic aspects of speech provide complementary information about thought disorganization, offering a scalable multimodal framework for objective assessment of disorganized speech in psychosis.

## Method Summary
The study analyzed three datasets of speech recordings from individuals with schizophrenia spectrum disorders. Speech was transcribed using WhisperX, which provided word-level timestamps for pause extraction. Semantic coherence was computed using sentence embeddings (SimCSE) with three aggregation strategies: sequential coherence, static centroid coherence, and cumulative centroid coherence. Pause features included summary statistics (mean, median, min, max, std, proportion) and optionally 764 TSFRESH-extracted features. Support Vector Regression (SVR) models were trained using leave-one-subject-out cross-validation. Three fusion strategies were compared: early fusion (feature concatenation), late fusion (averaging predictions), and semantic-only baselines.

## Key Results
- Pause features alone robustly predicted FTD severity across all three datasets
- Late fusion of pause and semantic models yielded highest performance (Spearman correlation up to 0.649, AUC up to 83.71%)
- Summary statistics consistently outperformed TSFRESH features for pause representation
- Pause duration showed weak to inverse correlations with semantic coherence, confirming complementary information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pause duration patterns encode cognitive planning disruptions that correlate with clinically-rated thought disorganization.
- **Mechanism:** Pauses occur when speakers require additional cognitive/linguistic planning. In thought disorder, impaired context processing forces more frequent or irregular pauses as speakers struggle to integrate prior context with ongoing speech. ASR-derived timestamps capture these temporal signatures objectively.
- **Core assumption:** Silent intervals reflect cognitive load rather than articulatory motor issues alone.
- **Evidence anchors:**
  - [abstract] "Pause features alone robustly predict the severity of FTD" across all three datasets
  - [section 4] "Impaired context processing... may manifest behaviorally as more pauses, as individuals struggle to integrate prior context with ongoing speech"
  - [corpus] Weak direct corpus support; neighbor papers address pause detection in exercise speech and fluency assessment but not psychosis mechanisms
- **Break condition:** If pauses are driven primarily by medication side effects, fatigue, or recording artifacts rather than cognitive planning deficits, the signal-to-noise ratio degrades.

### Mechanism 2
- **Claim:** Semantic coherence metrics quantify discourse-level disorganization by measuring vector similarity between speech units.
- **Mechanism:** Sentence embeddings (e.g., SimCSE) project utterances into a semantic space. Cosine similarity between adjacent sentences (sequential coherence) or between sentences and cumulative centroids (contextual coherence) captures topic drift and derailment—hallmarks of formal thought disorder.
- **Core assumption:** Embedding models trained on general text adequately represent semantic relationships in disordered speech.
- **Evidence anchors:**
  - [section 2.3.3] Three formulations: sequential, static centroid, and cumulative centroid coherence
  - [section 3.1.1] SimCSE embeddings with cumulative centroid aggregation yielded strongest correlations (AVH ρ=0.544, PsyCL ρ=0.349)
  - [corpus] Neighbor papers on dementia detection from speech use similar semantic coherence approaches, supporting generalizability to cognitive impairment
- **Break condition:** ASR errors distort transcripts enough to artificially depress coherence scores; severe disorganization produces neologisms or domain-specific terms not represented in pretrained embeddings.

### Mechanism 3
- **Claim:** Late fusion outperforms early fusion because pause and semantic features provide partially independent information about thought disorder.
- **Mechanism:** Separate models learn modality-specific patterns without one feature set dominating due to scale/dimensionality differences. Averaging predictions at the decision level preserves interpretability while combining complementary signals.
- **Core assumption:** The relationship between temporal and semantic features is not strictly linear or additive.
- **Evidence anchors:**
  - [abstract] "Late fusion yielding the most robust and consistent gains in all three datasets"
  - [section 3.2] Late fusion achieved highest average Spearman correlation (ρ=0.455) vs. early fusion (ρ=0.418) and semantic-only (ρ=0.413)
  - [table 3] Weak correlations between pause duration and sequential coherence (e.g., AVH ρ=-0.084), confirming independence
  - [corpus] No direct corpus support for fusion strategies in this clinical domain
- **Break condition:** If future work finds strong non-linear interactions between pause and semantic features that simple averaging cannot capture, more sophisticated fusion (e.g., attention-based) would be needed.

## Foundational Learning

- **Leave-one-subject-out cross-validation (LOSO-CV):**
  - Why needed here: Prevents speaker-level data leakage; multiple transcripts per participant must be held out together to evaluate generalization to new speakers.
  - Quick check question: If you used standard k-fold CV on transcripts without grouping by speaker, what would the model performance estimate represent?

- **Semantic coherence formulations:**
  - Why needed here: Different aggregation strategies (sequential, static centroid, cumulative centroid) capture distinct aspects of disorganization—local vs. global vs. contextual coherence.
  - Quick check question: Why might cumulative centroid coherence outperform sequential coherence in open-ended narratives but not in structured picture descriptions?

- **Late vs. early fusion:**
  - Why needed here: The paper demonstrates that combining modalities at the prediction level is more robust than feature concatenation, a design choice relevant to any multimodal system.
  - Quick check question: What dimensionality and scale mismatches between pause features (6 summary statistics or 764 TSFRESH features) and semantic features could cause early fusion to underperform?

## Architecture Onboarding

- **Component map:**
  - Audio -> WhisperX (ASR + timestamps + diarization) -> transcripts with word-level alignments
  - Transcripts -> NLTK segmentation -> sentence units -> SimCSE embeddings -> coherence scores (sequential/centroid) -> TARDIS (TSFRESH) -> semantic feature vectors
  - Timestamps -> pause duration extraction -> summary statistics (6 features) OR TSFRESH features (764)
  - Feature vectors -> SVR (RBF kernel) -> predictions -> late fusion (averaging)

- **Critical path:**
  1. ASR quality (WER 14.5% on AVH) directly impacts both pause accuracy and coherence computation; severely disorganized speech degrades transcription
  2. Segmentation choice (NLTK vs. WhisperX boundaries) affects coherence scores—NLTK outperformed WhisperX across datasets
  3. Feature selection (top 200 via F-statistics) within each LOSO fold prevents overfitting with high-dimensional TSFRESH features

- **Design tradeoffs:**
  - Summary statistics (6 features) vs. TSFRESH (764): Summary stats generalize better for AVH/PsyCL; TSFRESH outperforms in TOPSY's structured task
  - SimCSE vs. other embeddings: SimCSE performed best but results are embedding-dependent; no universal optimal choice
  - SVR vs. deep learning: SVR chosen for interpretability and small sample sizes; larger datasets may benefit from transformers

- **Failure signatures:**
  - Semantic-only models show high variance across embedding/aggregation choices -> likely overfitting or dataset artifacts
  - Pause features show inverse correlations with severity (e.g., TOPSY mean pause ρ=-0.271) -> may indicate compensatory strategies rather than direct pathology
  - Cross-dataset performance drops -> task structure and illness stage interact strongly with feature relevance

- **First 3 experiments:**
  1. Replicate the pause extraction pipeline on a held-out recording: run WhisperX, extract timestamps, compute the six summary statistics, verify pause proportion aligns with reported medians (17-18%)
  2. Train a semantic-only SVR on AVH with LOSO-CV using SimCSE + cumulative centroid + TARDIS features; confirm ρ ≈ 0.54
  3. Implement late fusion by training separate pause-only and semantic-only models, then averaging predictions; verify improvement over unimodal baselines on your chosen dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do distinct pause patterns result from speech task demands or illness stage?
- Basis in paper: [explicit] The authors note that datasets confounded task structure with illness stage and call for future studies to "better disentangle task structure from illness-related factors."
- Why unresolved: This study analyzed distinct cohorts (e.g., first-episode vs. established SSD) performing different tasks (e.g., picture description vs. open diaries), making it impossible to isolate the cause of the observed variability in pause patterns.
- What evidence would resolve it: A study applying multiple speech elicitation tasks (structured and open-ended) to the same clinical population at a similar illness stage.

### Open Question 2
- Question: Can domain-specific ASR fine-tuning mitigate transcription errors in severely disorganized speech?
- Basis in paper: [explicit] The paper states that ASR "performance degraded with severe disorganization" and suggests that "refining ASR systems through psychosis-specific fine-tuning could improve transcription accuracy."
- Why unresolved: The off-the-shelf WhisperX model showed a positive correlation between Word Error Rate and thought disorder severity, indicating that standard models struggle with the specific speech characteristics of severe FTD.
- What evidence would resolve it: Demonstrating reduced Word Error Rates in high-severity FTD samples after fine-tuning ASR models on clinical speech datasets.

### Open Question 3
- Question: Do deep learning architectures modeling the dynamic interplay of features outperform late fusion?
- Basis in paper: [explicit] The authors note that their late fusion strategy "may oversimplify the complex interplay between temporal and semantic features," suggesting "more sophisticated fusion methods might further enhance predictive performance."
- Why unresolved: The study only evaluated early (feature concatenation) and late (model averaging) fusion, leaving the potential of complex, simultaneous interaction modeling unexplored.
- What evidence would resolve it: Comparative performance metrics showing that transformer-based or attention-based multimodal fusion models significantly outperform the late fusion benchmarks established in this paper.

## Limitations

- Small sample sizes across all three datasets (46, 44, and 37 participants) limit statistical power and generalizability
- Focus on pause dynamics captures only one temporal dimension of speech disruption; other temporal features like speech rate, turn-taking, or prosody were not examined
- Semantic coherence approach depends heavily on embedding quality and may not capture domain-specific semantic relationships in disordered speech

## Confidence

- **High confidence**: The general finding that pause features provide complementary information to semantic coherence, supported by consistent patterns across all three datasets and statistically significant improvements in predictive performance.
- **Medium confidence**: The specific fusion architecture recommendations (late fusion outperforming early fusion), as these results depend on the particular feature extraction and model choices made in this study.
- **Medium confidence**: The mechanism linking pause dynamics to cognitive planning disruptions, though plausible and supported by behavioral observations, lacks direct experimental validation in the paper.
- **Low confidence**: Cross-dataset generalization claims, given the small sample sizes and task heterogeneity across datasets.

## Next Checks

1. **Control for confounds**: Replicate the pause analysis while controlling for medication status, fatigue levels, and recording quality to isolate cognitive planning effects from other pause sources.
2. **Temporal feature expansion**: Test whether adding speech rate, turn-taking patterns, or prosodic features to the pause feature set provides additional predictive power beyond pause duration alone.
3. **Domain-specific embedding evaluation**: Compare SimCSE-based coherence scores against embeddings fine-tuned on clinical speech data or domain-specific language models to assess whether general embeddings adequately capture semantic relationships in disordered speech.