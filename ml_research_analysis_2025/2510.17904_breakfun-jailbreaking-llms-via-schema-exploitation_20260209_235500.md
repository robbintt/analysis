---
ver: rpa2
title: 'BreakFun: Jailbreaking LLMs via Schema Exploitation'
arxiv_id: '2510.17904'
source_url: https://arxiv.org/abs/2510.17904
tags:
- schema
- breakfun
- prompt
- harmful
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BreakFun, a jailbreak attack exploiting\
  \ the tension between LLMs\u2019 strong instruction-following and safety alignment.\
  \ BreakFun uses a three-part prompt combining an innocent framing, a Chain-of-Thought\
  \ distraction, and a carefully crafted \u201CTrojan Schema\u201D to compel harmful\
  \ content generation."
---

# BreakFun: Jailbreaking LLMs via Schema Exploitation

## Quick Facts
- **arXiv ID:** 2510.17904
- **Source URL:** https://arxiv.org/abs/2510.17904
- **Reference count:** 28
- **Primary result:** Introduces BreakFun jailbreak attack exploiting schema deception, achieving 89% average ASR across 13 models.

## Executive Summary
BreakFun is a jailbreak attack that exploits the tension between LLMs' strong instruction-following and safety alignment by using a Trojan Schema to mislead models into generating harmful content. The attack combines an innocent framing, Chain-of-Thought distraction, and a carefully crafted schema to compel harmful responses. Evaluated on 13 diverse LLMs, BreakFun achieves an average 89% attack success rate, with some models reaching 100%. The authors also propose an Adversarial Prompt Deconstruction defense that detects such attacks by extracting semantic content from structured prompts.

## Method Summary
The BreakFun attack constructs prompts by concatenating three components: an Innocent Framing to establish benign context, a Trojan Schema (code snippet) that appears to define a legitimate task structure, and a Chain-of-Thought distraction to shift model attention away from safety considerations. The Trojan Schema is customized for different harm categories using 10 variants derived from a Core Task Schema. The attack is evaluated using an LLM-as-judge to verify non-refusal and actionable content generation. The proposed defense, Adversarial Prompt Deconstruction, uses a guardrail model to perform literal transcription, CoT unwrapping, and logical OR analysis to detect harmful semantic content masked by structured prompts.

## Key Results
- BreakFun achieves 89% average Attack Success Rate across 13 evaluated LLMs
- Attack reaches 100% success rate on several Tier 1 local models
- Ablation study confirms Trojan Schema is the key causal factor for success
- APD defense achieves high detection rates across three model families

## Why This Works (Mechanism)
The attack exploits a fundamental tension in LLMs between strong instruction-following capabilities and safety alignment mechanisms. By embedding harmful requests within what appears to be a legitimate schema or code structure, the model's syntactic compliance overrides its safety filters. The Chain-of-Thought distraction further shifts attention away from content safety evaluation, while the Trojan Schema creates a contextual framework that the model interprets as legitimate task definition.

## Foundational Learning
- **Schema Deception:** Understanding how structured data formats can be exploited to bypass safety mechanisms - needed to grasp the core vulnerability; quick check: can you identify how a Python schema could be repurposed for harmful content?
- **Ablation Studies:** Method for isolating causal factors by systematically removing components - needed to validate which elements drive attack success; quick check: what happens to ASR when Trojan Schema is removed?
- **LLM-as-Judge Evaluation:** Using models to evaluate outputs rather than human annotation - needed for scalable and consistent success measurement; quick check: how does the judge prompt force classification without triggering refusals?
- **Guardrail Defense Design:** Creating secondary models to monitor and filter harmful outputs - needed to understand the defense mechanism; quick check: what are the three stages of APD analysis?
- **API vs Local Model Differences:** Understanding performance variations between commercial APIs and open models - needed to interpret ASR results; quick check: why might API models show lower vulnerability?
- **Temperature and Seed Control:** Using deterministic settings for reproducible results - needed for scientific validation; quick check: what temperature and seed values ensure reproducibility?

## Architecture Onboarding

**Component Map:** Innocent Framing -> Trojan Schema -> CoT Distraction -> LLM

**Critical Path:** The Trojan Schema is the critical component - without it, the attack fails (confirmed by ablation study). The CoT distraction and innocent framing support the schema's effectiveness but are not independently sufficient.

**Design Tradeoffs:** The attack trades complexity (three-part prompt structure) for effectiveness against safety-aligned models. The defense trades computational overhead (additional guardrail inference) for detection capability.

**Failure Signatures:** 
- Low ASR indicates either schema logic flaws or model patch effectiveness
- High false positives in APD suggest overly aggressive semantic extraction
- Consistent failures across all models point to fundamental implementation errors

**First Experiments:**
1. Test BreakFun on a Tier 1 local model (e.g., LLaMA 3.1) with a simple harmful request to verify schema mechanics
2. Remove the Trojan Schema and confirm ASR drops to near-zero, validating the ablation study
3. Test the APD defense on a known BreakFun prompt to verify detection capability

## Open Questions the Paper Calls Out
- Can BreakFun methodology be adapted to exploit non-executable formats like XML/YAML that lack code simulation context?
- Is the APD defense effective against orthogonal jailbreaking techniques like multilingual or persona-based attacks?
- Does mechanistic interpretability confirm that "cognitive misdirection" actually shifts internal attention from safety checks to syntactic compliance?

## Limitations
- ASR figures for commercial API models (84% average) may be outdated due to patch cycles since submission
- APD defense computational overhead and false positive rates on borderline-benign queries remain incompletely characterized
- The attack's effectiveness may be time-dependent as commercial providers implement countermeasures

## Confidence

**High Confidence:** The fundamental vulnerability exploited (schema manipulation to bypass safety alignment) is valid and reproducible. The ablation study methodology is sound.

**Medium Confidence:** ASR figures for commercial API models may be outdated. The 84% average likely understates current effectiveness.

**Medium Confidence:** APD defense efficacy is well-demonstrated, but real-world performance with live traffic and adaptive attackers requires further validation.

## Next Checks
1. Remove the Trojan Schema from BreakFun and verify ASR drops to near-zero, confirming the ablation study's conclusion about the schema's causal role.
2. Test BreakFun against current versions of Claude, GPT, and other API models to establish whether the 84% ASR figure still holds or has degraded.
3. Systematically evaluate the APD guardrail on a comprehensive "Borderline-Benign" dataset to quantify false positive rates and determine operational thresholds for deployment.