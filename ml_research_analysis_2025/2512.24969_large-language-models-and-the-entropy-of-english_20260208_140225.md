---
ver: rpa2
title: Large language models and the entropy of English
arxiv_id: '2512.24969'
source_url: https://arxiv.org/abs/2512.24969
tags:
- length
- code
- entropy
- characters
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses large language models (LLMs) to measure the entropy
  of English text across contexts up to 10^4 characters, far beyond prior human-based
  or model-based studies. By computing conditional entropy and code lengths across
  multiple LLMs (OLMo 2 1B, Llama 3.2 1B, Qwen3 8B, and a custom 1.7B model) and diverse
  corpora (C4, Wikipedia, poetry, news), the authors show that entropy continues to
  decrease with context length without plateauing, indicating long-range dependencies
  in language.
---

# Large language models and the entropy of English

## Quick Facts
- arXiv ID: 2512.24969
- Source URL: https://arxiv.org/abs/2512.24969
- Reference count: 0
- Primary result: Entropy of English continues decreasing with context length up to 10^4 characters, showing long-range dependencies

## Executive Summary
This paper uses large language models to measure the entropy of English text across contexts up to 10^4 characters, far beyond prior human-based or model-based studies. By computing conditional entropy and code lengths across multiple LLMs and diverse corpora, the authors show that entropy continues to decrease with context length without plateauing, indicating long-range dependencies in language. The distribution of code lengths develops a power-law tail toward zero at large context lengths, suggesting emergent certainty in predictions for a growing fraction of tokens.

## Method Summary
The authors sample random strings from corpora (C4, Wikipedia, poetry, news), delete first B characters randomly, tokenize, and compute next-token log-probs for each prefix length up to N tokens (4096 for most models, 2048 for DCLM 1.7B). They average L(N) over 30,000-80,000+ strings to obtain mean code length per character. The custom 1.7B DCLM model was trained on 28B tokens from DCLM-baseline shard using AdamW with specific hyperparameters. Four models were evaluated: OLMo 2 1B, Llama 3.2 1B, Qwen3 8B, and the custom DCLM 1.7B.

## Key Results
- Code length continues decreasing out to 10^4 characters with no sign of saturation in C4 corpus
- Mutual information between characters separated by thousands of positions decays as power law with exponent ~0.12
- Distribution of code lengths develops power-law tail toward zero at large context lengths
- Large-N structure is learned more slowly than short-range dependencies during training

## Why This Works (Mechanism)

### Mechanism 1
Mean code length from LLM predictions provides an upper bound on true conditional entropy of text. Given token sequence t₁...t_K, LLM outputs P(t_{K+1}|t₁...t_K) and code length ℓ = -log P bounds conditional entropy. Averaging over samples yields L(K) = ⟨ℓ⟩_data as empirical entropy estimate.

### Mechanism 2
Absence of entropy plateau at large N implies direct interactions spanning ~10⁴ characters. If all interactions had finite range R, then L(N > R) would be constant. The observed continuous decrease in code length out to N ~ 10⁴ characters requires dependencies extending across these distances.

### Mechanism 3
Long-range structure is learned more gradually than short-range dependencies during training. Small-N code lengths approach asymptotic values quickly, while large-N performance improves slowly over training. This suggests the model first captures local patterns before exploiting distant dependencies.

## Foundational Learning

- **Conditional entropy H(X|Y) = -Σ P(x,y) log P(x|y)**
  - Why needed here: The entire paper frames entropy per character given past context; understanding this is essential to interpret code length curves.
  - Quick check question: If you know the previous 100 characters perfectly, what does conditional entropy tell you about predicting the 101st?

- **Mutual information I(X;Y) = H(X) - H(X|Y)**
  - Why needed here: Used to directly measure correlations between characters at distance d, providing model-independent confirmation of long-range structure.
  - Quick check question: Why does power-law decay of I(d) suggest scale-invariant correlations?

- **Code length and optimal compression (Shannon's source coding theorem)**
  - Why needed here: The paper uses code length ℓ = -log P as the operational measure; this connects entropy to compression.
  - Quick check question: If entropy is 1.5 bits/character, what is the average code length for optimal encoding?

## Architecture Onboarding

- **Component map**: Input text -> Tokenizer -> Context window truncation -> Transformer backbone -> Output layer -> Logits -> Compute -log P(actual_next_token)

- **Critical path**: Input text → tokenize → truncate to context window → forward pass → extract logits → compute -log P(actual_next_token) → average over samples

- **Design tradeoffs**: Longer context windows capture more dependencies but increase compute quadratically; smaller models agree with larger ones at large N, suggesting entropy estimates are robust to model scale; tokenization granularity affects character-level interpretation.

- **Failure signatures**: Plateau in L(N) at moderate N suggests model cannot exploit long-range structure; large divergence between conditional entropy and code length indicates miscalibrated probabilities; high variance across text samples may indicate corpus heterogeneity.

- **First 3 experiments**:
  1. Replicate Figure 1 on your target model: compute L(N) across context lengths on C4 subset to establish baseline entropy curve.
  2. Cross-corpus comparison (Figure 2): test whether your application domain shows similar non-plateauing behavior or diverges.
  3. Training checkpoint analysis (Figure 4): if training your own model, log L(N) at multiple checkpoints to verify that long-range structure emerges gradually.

## Open Questions the Paper Calls Out

### Open Question 1
Can statistical mechanics "toy models" be constructed to explain the observed slow decay of entropy and long-range correlations in language? The conclusion states, "It would be attractive to have toy models, in the spirit of statistical mechanics, that could explain these behaviors, even in outline." This study provides empirical constraints but does not offer a theoretical generative model for the discovered structures.

### Open Question 2
Does the conditional entropy of English eventually plateau, or does it continue to decay to zero at context lengths beyond 10^4 characters? The authors note the code length decrease is "consistent with the conjecture that L(N → ∞) might actually vanish," though they observe a decay much slower than previous estimates. Current computational limits restrict analysis to approximately 10^4 characters.

### Open Question 3
Do the observed differences in entropy across genres (e.g., poetry vs. Wikipedia) reflect intrinsic properties of the text or biases in the models' training data? The authors write, "it is not clear whether this reflects inherent features of the real text or each genre's relationship to the models' training sets." LLMs are trained on massive, opaque datasets where certain genres may be overrepresented.

## Limitations

- Corpus heterogeneity may obscure genre-specific structural differences
- Model calibration uncertainty affects reliability of entropy estimates
- Tokenization effects could create or mask apparent long-range structure

## Confidence

- Corpus Heterogeneity and Representativeness: Medium
- Model Calibration and Distributional Approximation: Low
- Tokenization Effects and Character-Level Interpretation: Medium
- Power Law Exponent Stability: Low
- Training Dynamics Interpretation: Medium

## Next Checks

1. **Corpus-Specific Entropy Curves**: Repeat the L(N) analysis separately on each corpus (news, poetry, narrative) to determine whether long-range dependencies are genre-universal or corpus-specific.

2. **Model Calibration Assessment**: Systematically evaluate next-token probability calibration across models using proper scoring rules and compare to true entropy estimates.

3. **Tokenization Sensitivity Analysis**: Repeat the analysis using character-level models to determine whether the observed long-range structure persists when tokenization variability is eliminated.