---
ver: rpa2
title: Using Large Language Models to Suggest Informative Prior Distributions in Bayesian
  Statistics
arxiv_id: '2506.21964'
source_url: https://arxiv.org/abs/2506.21964
tags:
- prior
- distributions
- priors
- llms
- informative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to use large language models (LLMs)
  to suggest informative prior distributions in Bayesian statistical modeling, addressing
  the difficulty and subjectivity of manually constructing such priors. An extensive
  prompt was developed to elicit multiple sets of priors from LLMs (Claude Opus, Gemini
  2.5 Pro, ChatGPT-4o-mini), requiring detailed justification and confidence weighting.
---

# Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics

## Quick Facts
- arXiv ID: 2506.21964
- Source URL: https://arxiv.org/abs/2506.21964
- Reference count: 26
- Primary result: LLM-suggested priors correctly identified directional associations but often provided overconfident moderately informative priors

## Executive Summary
This paper proposes using large language models to suggest informative prior distributions in Bayesian modeling, addressing the difficulty and subjectivity of manual prior construction. The authors developed an extensive prompt requiring LLMs to justify, reflect on, and propose multiple prior sets, then evaluated the suggested priors using KL divergence from the MLE distribution. Claude Opus performed best, maintaining non-zero expected effects in weakly informative priors, while ChatGPT and Gemini defaulted to unnecessarily vague priors with mean zero. Though LLM priors correctly identified directional associations in two real datasets, prediction improvements were not statistically significant due to large dataset sizes.

## Method Summary
The method uses structured prompt engineering to elicit prior distributions from LLMs (Claude Opus, Gemini 2.5 Pro, ChatGPT-4o-mini) for Bayesian regression models. The prompt requires LLMs to explain reasoning, propose multiple prior sets, and justify confidence levels. For each variable, priors are requested at two levels: moderately informative (incorporating demonstrated knowledge) and weakly informative (maintaining uncertainty). The suggested priors are evaluated by computing KL divergence from the MLE distribution (approximated as Gaussian). Bayesian models are then fit using R-INLA, and prediction performance is assessed via cross-validation.

## Key Results
- All LLMs correctly identified directional associations (e.g., male sex increases heart disease risk)
- Claude Opus maintained non-zero means in weakly informative priors, while ChatGPT and Gemini defaulted to mean=0
- Moderately informative priors showed higher KL divergence (overconfidence) compared to weakly informative priors
- Prediction improvements with LLM priors were not statistically significant in large datasets
- KL divergence effectively detected prior-data conflict in concrete strength example

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Elicitation via Prompt Engineering
- **Claim**: Requiring LLMs to justify, reflect on, and propose multiple prior sets produces more reliable outputs than simple queries.
- **Core assumption**: LLMs encode sufficiently accurate domain knowledge from training corpora.
- **Evidence**: [abstract] "We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices." [section 2.1] "The objective is not only to obtain prior distributions from the LLMs, but also to require them to explain their reasoning, propose multiple sets of priors... This makes the resulting suggestions more reliable and interpretable than those from a simple 'ask and receive' approach."

### Mechanism 2: Directional Knowledge Preservation with Uncertainty Calibration Gap
- **Claim**: LLMs reliably identify correct directional associations but systematically misestimate effect magnitudes.
- **Core assumption**: Directional knowledge in training data is accurate and representative.
- **Evidence**: [abstract] "All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males)." [section 3.1] "For the variable age, all three priors assume a far stronger association with CAD than what is observed in the data."

### Mechanism 3: Weakly Informative Prior Design Divergence (Claude-specific)
- **Claim**: Maintaining non-zero expected effects in weakly informative priors outperforms defaulting to mean=0.
- **Core assumption**: Prior directional knowledge has value even under high uncertainty.
- **Evidence**: [abstract] "Claude showed a significant advantage by maintaining non-zero expected effects in weakly informative priors rather than defaulting to zero." [section 3.1] "ChatGPT and Gemini defaulted to a mean of 0, which was 'unnecessarily vague' given their demonstrated knowledge. In contrast, Claude did not."

## Foundational Learning

- **Bayesian Prior Distributions (Informative vs. Weakly Informative)**
  - *Why needed*: The paper's core object; understanding what makes a prior contribute information vs. being non-informative is essential.
  - *Quick check*: A prior N(0, 10²) on a regression coefficient—does it express any belief about the sign of the effect?

- **Kullback-Leibler Divergence**
  - *Why needed*: Primary evaluation metric; quantifies how "surprised" the prior is by the data (via MLE distribution).
  - *Quick check*: If D_KL(p||q) = 0, what is true about the relationship between p and q?

- **MLE Distribution as Prior Evaluation Benchmark**
  - *Why needed*: The paper uses MLE distribution (not posterior) to assess prior quality—an uncommon approach in pure Bayesian analysis.
  - *Quick check*: Why might comparing a prior directly to the MLE detect prior-data conflict more clearly than comparing to the posterior?

## Architecture Onboarding

### Component map:
Prompt Engineering -> LLM API calls -> Prior Extraction -> KL Divergence Calculation -> Bayesian Model Fitting -> Performance Validation

### Critical path:
1. Define model structure (logistic/linear regression with Gaussian priors on coefficients)
2. Construct structured prompt with variable descriptions and model specification
3. Call LLM API and parse prior distributions
4. Compute MLE distribution
5. Calculate KL divergence for prior quality assessment
6. Fit Bayesian model with R-INLA
7. Validate prediction performance via cross-validation

### Design tradeoffs:
- **Moderately vs. Weakly Informative Priors**: More informative priors leverage domain knowledge but risk overconfidence; weaker priors are safer but may discard valuable information
- **MLE vs. Posterior for Evaluation**: MLE comparison directly detects prior-data conflict but is not pure Bayesian; posterior comparison conflates prior and likelihood influence
- **LLM Selection**: Claude preserves directional knowledge in weakly informative priors; ChatGPT/Gemini default to uninformative

### Failure signatures:
- **Prior-Data Conflict**: KL divergence >> 1 (Figure 1 cement example)
- **Unnecessarily Vague Priors**: Mean=0 in weakly informative priors despite demonstrated directional knowledge (ChatGPT/Gemini)
- **Meta-level Overconfidence**: LLMs rated moderately informative priors higher confidence (60-65%) despite higher KL divergence
- **No Prediction Gain**: Large datasets overwhelm prior influence—expect minimal improvements

### First 3 experiments:
1. **Baseline Replication**: Replicate the heart disease experiment with the paper's prompt on a different LLM version; verify directional accuracy and compute KL divergences
2. **Prompt Ablation**: Remove each prompt component (justification, multiple proposals, confidence scoring) individually to identify drivers of prior quality
3. **Small Dataset Test**: Apply methodology to n<100 samples where prior influence should be larger; test for statistically significant prediction improvements over uninformative priors

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can LLM-generated priors improve out-of-distribution (OOD) generalization compared to standard non-informative priors?
- **Basis**: [explicit] The authors state that exploring the potential of LLMs in OOD generalization is a "promising avenue" where priors might bridge data distributions.
- **Why unresolved**: The current experiments used standard datasets where the likelihood dominated the prior, resulting in no significant predictive improvement.
- **What evidence would resolve it**: Demonstrated improvements in predictive accuracy on test sets with systematic covariate shifts when using LLM-suggested priors.

### Open Question 2
- **Question**: Does the "simulated literature review" component of the prompt cause LLMs to generate overconfident prior widths?
- **Basis**: [explicit] The authors ask, "Could the 'simulated literature review' part of the prompt encourage the models to recall strong, textbook associations without the necessary nuance?"
- **Why unresolved**: The study observed overconfidence in moderate priors but did not isolate the specific prompt components responsible for the lack of nuance.
- **What evidence would resolve it**: Ablation studies comparing the calibration of priors generated with and without instructions to simulate literature reviews.

### Open Question 3
- **Question**: Can this elicitation methodology be effectively generalized to complex Bayesian structures like hierarchical or latent variable models?
- **Basis**: [explicit] The authors propose it is "interesting to explore this approach for other Bayesian models, such as latent variable models [and] hierarchical models."
- **Why unresolved**: The methodology was only validated on generalized linear models (logistic and linear regression).
- **What evidence would resolve it**: Successful application of the prompt-based approach to hierarchical models, yielding well-calibrated posteriors and convergence.

## Limitations
- The primary evaluation metric (KL divergence from MLE) is not standard in Bayesian literature, raising questions about its relevance for actual Bayesian inference
- Complete prompt specification is unavailable ("for the sake of brevity"), creating significant reproducibility barriers
- No statistically significant prediction improvements in large datasets indicate that prior quality gains may not translate to practical performance benefits
- Systematic overconfidence in effect magnitudes suggests fundamental limitations in how training data represents quantitative relationships

## Confidence
- **High confidence**: LLMs can reliably identify directional associations (heart disease: male risk > female; concrete: water reduces strength). Claude maintains non-zero means in weakly informative priors while ChatGPT/Gemini default to mean=0.
- **Medium confidence**: KL divergence from MLE effectively detects prior-data conflict and prior quality issues. Structured prompting improves prior reliability over simple queries.
- **Low confidence**: LLM priors provide statistically significant prediction improvements. The KL divergence evaluation metric meaningfully predicts Bayesian model performance.

## Next Checks
1. **Prompt ablation study**: Systematically remove each prompt component (justification, multiple proposals, confidence scoring) to identify which elements drive improvements in prior quality and directional accuracy
2. **Small dataset experiment**: Apply the methodology to n<100 samples where prior influence should dominate; test for statistically significant prediction improvements over uninformative priors
3. **Alternative evaluation framework**: Compare KL divergence evaluation against standard Bayesian metrics (posterior predictive checks, prior/posterior overlap) to validate the chosen evaluation approach