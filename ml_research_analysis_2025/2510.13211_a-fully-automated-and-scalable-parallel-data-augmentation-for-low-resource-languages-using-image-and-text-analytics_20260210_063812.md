---
ver: rpa2
title: A fully automated and scalable Parallel Data Augmentation for Low Resource
  Languages using Image and Text Analytics
arxiv_id: '2510.13211'
source_url: https://arxiv.org/abs/2510.13211
tags:
- article
- sentence
- language
- languages
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel fully automated and scalable methodology
  to build bilingual parallel corpora for low-resource languages using image and text
  analytics. The approach leverages newspaper article images as pivots to map articles
  across different language editions, followed by sentence-level alignment using language-agnostic
  embeddings.
---

# A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics

## Quick Facts
- arXiv ID: 2510.13211
- Source URL: https://arxiv.org/abs/2510.13211
- Reference count: 20
- Primary result: Built over 14,000 sentence pairs for Konkani-Marathi and Punjabi-Hindi; achieved STS score of 3.70 and improved MT BLEU by ~3 points.

## Executive Summary
This paper introduces a fully automated, scalable pipeline to build bilingual parallel corpora for low-resource languages by leveraging shared images across newspaper editions. Images act as visual pivots to identify corresponding articles in different language editions, followed by sentence-level alignment using language-agnostic embeddings. Evaluated on Konkani-Marathi and Punjabi-Hindi, the method produces high-quality sentence pairs without human annotation and improves downstream machine translation performance. The approach is language-agnostic, does not require parallel training data, and offers a novel solution to the scarcity of parallel resources for low-resource language pairs.

## Method Summary
The method uses a 4-stage pipeline: (1) a crawler downloads e-newspaper PDFs and labels pages by date and language; (2) an article extractor segments pages using layout analysis and OCR ensemble (EasyOCR, PaddleOCR, Tesseract) with majority voting to extract text from headlines, images, captions, and content; (3) an article mapper uses SIFT to match visually similar images across language editions from the same date, forming article pairs; (4) a sentence mapper aligns sentences within these pairs using language-agnostic embeddings (LaBSE) and cosine similarity, outputting the parallel corpus. The approach was validated on Konkani-Marathi and Punjabi-Hindi, generating over 14,000 sentence pairs, with extrinsic evaluation via fine-tuned mT5 improving BLEU by ~3 points.

## Key Results
- Generated 14,448 sentence pairs for Konkani-Marathi (STS score: 3.70).
- Improved baseline MT BLEU by ~3 points (from 23.5 to 26.4) on picture captions.
- Validated method on two low-resource language pairs (Konkani-Marathi, Punjabi-Hindi).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Images shared across language editions can serve as reliable pivots to identify article pairs covering the same news content.
- Mechanism: Newspaper publishers reuse images across language editions to reduce editorial costs. By detecting visually similar images using feature-matching algorithms (e.g., SIFT), the system identifies candidate article pairs without needing linguistic overlap or translation.
- Core assumption: Newsrooms reuse images for the same story across editions; the visual similarity is preserved across digital uploads (scale, illumination, rotation only).
- Evidence anchors:
  - [abstract] "leverages newspaper article images as pivots to map articles across different language editions"
  - [section 1] "a lot of local newspapers in LRLs... also have editions in other languages and they re-use the pictures across the different language versions"
  - [corpus] Related work on low-resource corpora construction focuses on text-based mining; image pivots are underexplored in neighbors.
- Break condition: If editions use different images for the same story, or the same image for unrelated stories, the pivot fails.

### Mechanism 2
- Claim: Language-agnostic sentence embeddings (LaBSE) can align semantically equivalent sentences across scripts and language families without parallel training data.
- Mechanism: LaBSE maps sentences from different languages into a shared embedding space where cosine similarity reflects semantic equivalence. This bypasses the need for bilingual lexicons or length-based heuristics.
- Core assumption: The pre-trained multilingual embedding space generalizes well to the specific low-resource language pair (including potential script differences).
- Evidence anchors:
  - [abstract] "sentence-level alignment using language-agnostic embeddings"
  - [section 3.4] "We convert sentences into these vectors independently first and then find the cosine similarity between them... LAS gives the best results"
  - [corpus] Neighbor papers on low-resource corpora rely more on dictionary/lexical methods; LaBSE-like embedding alignment for Indic pairs is less commonly validated.
- Break condition: If LaBSE has poor coverage for the specific languages or dialects, embeddings will not cluster semantically similar sentences.

### Mechanism 3
- Claim: Combining multiple OCR engines via majority voting improves extraction accuracy for complex newspaper layouts.
- Mechanism: Each OCR (EasyOCR, PaddleOCR, Tesseract) makes independent character-level predictions. Majority voting reduces idiosyncratic errors from any single engine.
- Core assumption: OCR errors are partially uncorrelated across engines, and the layout analysis can segment articles accurately in multi-column formats.
- Evidence anchors:
  - [section 3.2] "use a combination of EasyOCR, PaddleOCR and Tessaract and use majority voting for final decision"
  - [corpus] No direct neighbor validation of multi-OCR ensembling for Indic newspaper digitization.
- Break condition: If all engines fail on a specific script or degradation pattern, voting offers no gain.

## Foundational Learning

- Concept: **Scale-Invariant Feature Transform (SIFT)**
  - Why needed here: Understand how image keypoints are detected and matched robustly to scale/rotation; critical for tuning thresholds in article mapping.
  - Quick check question: Would SIFT still match if an image is cropped significantly or overlaid with text captions?

- Concept: **Language-Agnostic Sentence Embeddings (LaBSE)**
  - Why needed here: Enables sentence alignment without bilingual dictionaries; need to understand embedding quality, normalization, and similarity thresholding.
  - Quick check question: If two sentences are paraphrases rather than direct translations, would LaBSE still assign high similarity?

- Concept: **Semantic Textual Similarity (STS) Scoring**
  - Why needed here: Human evaluation protocol for corpus quality; anchors quality claims and enables comparison across methods.
  - Quick check question: What does an STS score of 3.7 imply about the average alignment quality in this corpus?

## Architecture Onboarding

- Component map: Crawler -> Article Extractor -> Article Mapper -> Sentence Mapper
- Critical path: Image match -> article pair -> OCR correctness -> sentence embedding quality -> alignment threshold tuning. Failures propagate downstream.
- Design tradeoffs:
  - SIFT vs. deep visual features: SIFT is fast and sufficient for exact/near-exact matches; deep features may handle edits but increase compute.
  - LAS vs. length-based/lexical alignment: LAS is more robust to reordering but requires GPU and large multilingual model.
  - OCR ensemble vs. single engine: Ensemble improves accuracy but adds latency and complexity.
- Failure signatures:
  - Low article match rate -> likely image reuse policy differs; consider headline/URL pivots as backups.
  - High sentence misalignment -> LaBSE may not cover dialect/script variants; inspect per-sentence similarity distributions.
  - Poor downstream BLEU -> check corpus noise, domain mismatch, or insufficient coverage of target grammatical structures.
- First 3 experiments:
  1. Pilot on one month of editions: Measure article match rate, OCR accuracy (spot-check 100 samples), and STS on 200 sentence pairs.
  2. Threshold sweep for image matching: Vary SIFT match-score threshold; plot precision/recall against manually verified article pairs.
  3. Embedding ablation: Compare LAS vs. length-based vs. lexical overlap on a held-out annotated subset; report STS and downstream BLEU delta.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on consistent image reuse policies across language editions, which may vary by publisher or editorial workflow.
- Reproducibility is limited by unspecified thresholds for SIFT image matching and LaBSE cosine similarity.
- STS evaluation lacks granularity on alignment precision per sentence pair.
- Downstream BLEU evaluation depends on picture captions as a proxy task, which may not reflect general translation quality.
- Assumes availability of multilingual e-newspapers with overlapping image usage, limiting generalizability.

## Confidence
- High confidence: The core feasibility of using newspaper images as pivots to map articles across languages and the subsequent improvement in BLEU scores from the automatically constructed dataset.
- Medium confidence: The robustness of SIFT for article mapping across language editions and the effectiveness of LaBSE for sentence alignment in low-resource language pairs, due to limited validation details and threshold specifications.
- Low confidence: The generalizability of the STS score of 3.7 across diverse low-resource language pairs and the long-term sustainability of the method given potential changes in newspaper publishing practices or image usage policies.

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the SIFT image matching threshold and LaBSE cosine similarity threshold; evaluate precision/recall of article and sentence alignment against a manually annotated validation set.
2. **Cross-Publisher Generalizability Test**: Apply the pipeline to a second bilingual newspaper pair (e.g., different publisher or language family); measure article match rate, STS, and downstream BLEU to assess robustness.
3. **Error Analysis on Downstream Impact**: Sample misaligned sentence pairs from the corpus; translate them with the fine-tuned mT5 model and evaluate whether errors stem from sentence alignment noise or model limitations.