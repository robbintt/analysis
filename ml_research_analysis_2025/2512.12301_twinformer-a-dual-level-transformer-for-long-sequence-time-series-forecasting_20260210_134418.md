---
ver: rpa2
title: 'TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting'
arxiv_id: '2512.12301'
source_url: https://arxiv.org/abs/2512.12301
tags:
- attention
- forecasting
- time
- informer
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TwinFormer introduces a hierarchical Transformer for long-sequence
  time-series forecasting, dividing inputs into non-overlapping temporal patches processed
  in two stages: Local Informer models intra-patch dynamics, and Global Informer captures
  inter-patch dependencies, both using top-k sparse attention. A lightweight GRU aggregates
  globally contextualized patch tokens for direct multi-horizon prediction, achieving
  linear O(kLd) complexity.'
---

# TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting

## Quick Facts
- arXiv ID: 2512.12301
- Source URL: https://arxiv.org/abs/2512.12301
- Authors: Mahima Kumavat; Aditya Maheshwari
- Reference count: 26
- Primary result: Achieves best MAE and RMSE in 17 out of 27 top-two positions on eight real-world datasets, outperforming strong baselines including PatchTST, iTransformer, FEDFormer, Informer, and vanilla Transformers

## Executive Summary
TwinFormer introduces a hierarchical Transformer architecture for long-sequence time-series forecasting that achieves linear O(kLd) complexity. The model divides input sequences into non-overlapping temporal patches, processes each patch independently with a Local Informer to capture intra-patch dynamics, then applies a Global Informer to the pooled patch tokens to model inter-patch dependencies. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. Evaluated across eight real-world datasets spanning six domains, TwinFormer demonstrates state-of-the-art performance, particularly excelling at long-horizon forecasting (up to 720 steps ahead).

## Method Summary
TwinFormer addresses the quadratic complexity challenge in long-sequence time-series forecasting through a hierarchical patch-based approach. The architecture processes input sequences by first dividing them into non-overlapping temporal patches, then applying a Local Informer with top-k sparse attention to each patch independently to capture fine-grained local dynamics. The resulting patch tokens are mean-pooled and passed to a Global Informer that applies top-k sparse attention across patches to model long-range dependencies. Finally, a lightweight GRU sequentially processes the globally contextualized patch embeddings, using only the final hidden state for direct multi-horizon prediction via a linear projection. The model uses min-max normalization, embedding dimension d=32, and top-k=5 for sparse attention, achieving linear complexity O(kLd) where k is the number of attention heads, L is sequence length, and d is embedding dimension.

## Key Results
- Achieves best MAE and RMSE in 17 out of 27 top-two positions across eight datasets
- Outperforms strong baselines including PatchTST, iTransformer, FEDFormer, Informer, and vanilla Transformers
- Demonstrates consistent performance across diverse domains: weather, electricity, traffic, disease surveillance, and stock prices
- Maintains effectiveness at long horizons (up to 720 steps ahead)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Two-Stage Attention
Input sequences are divided into non-overlapping patches, with a Local Informer applying top-k sparse attention within each patch independently, followed by mean pooling to create compact patch tokens. A Global Informer then applies top-k sparse attention across pooled patches to capture inter-patch dependencies, reducing sequence length from L to N_p = L/P before global processing. This separates fine-grained local pattern modeling from long-range dependency capture, achieving linear O(kLd) complexity instead of quadratic.

### Mechanism 2: Top-k Sparse Attention
For each query, the model computes all attention logits, retains only the k largest values (k=5), and masks others to -∞ before softmax. This directly selects the strongest query-key matches rather than probabilistic sampling used in ProbSparse, providing more stable and effective sparsity for time-series forecasting. The pre-softmax masking produces different gradients than post-softmax masking, affecting training dynamics.

### Mechanism 3: GRU-based Sequential Aggregation
After the Global Informer produces N_p contextualized patch embeddings, a lightweight GRU processes them chronologically, maintaining a hidden state that progressively integrates information. Only the final hidden state is used for prediction via a linear projection. This sequential processing captures irreversible temporal structure across patches more efficiently than convolutional or attention-based decoders.

## Foundational Learning

- **Sparse Attention Patterns**: Understanding how top-k differs from ProbSparse and why pre-softmax masking matters for training stability. Quick check: Why does masking attention logits to -∞ before softmax produce different gradients than masking attention weights after softmax?

- **Patch-based Tokenization**: Core mechanism for reducing O(L²) to O(kLd) while preserving local semantic structure. Quick check: Given sequence length L=336 and patch size P=16, how many patch tokens N_p does TwinFormer process in the Global Informer?

- **Recurrent Hidden State Aggregation**: Understanding why the final GRU hidden state suffices for multi-step prediction without autoregressive decoding. Quick check: What information does the GRU final hidden state encode that a simple mean-pooling of Global Informer outputs would not?

## Architecture Onboarding

- **Component map**: Input (L×F) → MinMax normalization → Linear embedding (L×d) → Patching → N_p patches of size P×d → LocalInform → Pool → GlobalInform → GRU → LinearHead
- **Critical path**: Input → Patch → LocalInform → Pool → GlobalInform → GRU → LinearHead
- **Design tradeoffs**:
  - Patch size P: Larger P → fewer patches (faster global stage) but coarser local modeling. Paper uses P=48 (input length) with varying horizons.
  - Top-k value: Smaller k → faster but may miss distributed dependencies. Paper uses k=5 fixed.
  - GRU hidden dimension vs d: Paper uses same dimension throughout.
  - Non-overlapping vs overlapping patches: Paper uses non-overlapping for simplicity.
- **Failure signatures**:
  - Training loss plateaus above baselines → Check if patch size is too large (washing out local patterns) or too small (Global Informer sees overly long sequence).
  - Long-horizon predictions (H=720) degrade sharply relative to short horizons → May indicate GRU hidden state has insufficient capacity; try increasing d.
  - Top-k attention underperforms ProbSparse on specific dataset → Dataset may have distributed dependencies; consider increasing k or reverting to ProbSparse.
- **First 3 experiments**:
  1. Reproduce ablation (Table 3): Compare top-k vs ProbSparse on 2-3 datasets to validate sparse attention choice.
  2. Patch size sweep: Test P∈{8, 16, 32, 48, 64} on one dataset to find optimal locality vs efficiency trade-off.
  3. Aggregator comparison: Compare GRU vs LSTM vs mean-pooling (no aggregator) to verify GRU contribution on your target dataset.

## Open Questions the Paper Calls Out
1. How can the architecture be modified to explicitly model temporal dependencies among predicted future time steps while preserving computational efficiency?
2. What mechanisms could prevent top-k sparse attention from discarding important long-range dependencies that fall outside the top-k selection?
3. How sensitive is TwinFormer's performance to patch size P, and does the optimal P vary across domains with different periodicities?
4. Would overlapping patches improve performance by preserving boundary information between adjacent patches?

## Limitations
- The paper does not explicitly state the patch size P, which is critical for determining N_p and the claimed O(kLd) complexity.
- The superiority claim for top-k sparse attention is based on limited datasets and overstates what appears to be dataset-dependent performance.
- The architecture description suggests single-layer Local and Global Informers, but this is not explicitly confirmed.

## Confidence
- **High**: Hierarchical two-stage attention design, overall evaluation results (17/27 top-two positions), GRU as final aggregator, top-k sparse attention as component
- **Medium**: Top-k vs ProbSparse performance comparison (dataset-dependent), single-layer architecture assumption
- **Low**: Patch size P, complete hyperparameter set, exact number of layers

## Next Checks
1. Implement TwinFormer with varying patch sizes P∈{8, 16, 32, 48, 64} on Electricity dataset to empirically determine optimal patch size.
2. Reproduce the sparse attention ablation (Table 3) on Power Consumption and IDEA.NS datasets to confirm top-k vs ProbSparse superiority.
3. Compare TwinFormer's GRU aggregator against mean-pooling and LSTM alternatives on Temperature dataset to verify sequential processing benefit.