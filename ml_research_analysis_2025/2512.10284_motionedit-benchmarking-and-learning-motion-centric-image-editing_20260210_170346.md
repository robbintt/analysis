---
ver: rpa2
title: 'MotionEdit: Benchmarking and Learning Motion-Centric Image Editing'
arxiv_id: '2512.10284'
source_url: https://arxiv.org/abs/2512.10284
tags:
- motion
- editing
- image
- motionnft
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of motion-centric image editing,
  where existing methods struggle to modify subject actions and interactions while
  preserving identity and structure. The authors introduce MotionEdit, a high-quality
  dataset and benchmark designed specifically for motion editing, featuring diverse
  and realistic motion transformations extracted from videos.
---

# MotionEdit: Benchmarking and Learning Motion-Centric Image Editing

## Quick Facts
- **arXiv ID**: 2512.10284
- **Source URL**: https://arxiv.org/abs/2512.10284
- **Reference count**: 40
- **One-line primary result**: MotionNFT improves motion fidelity and overall editing quality by over 10% across models like FLUX.1 Kontext and Qwen-Image-Edit using optical flow-based rewards

## Executive Summary
Motion-centric image editing—modifying subject actions, poses, and interactions while preserving identity and structure—remains challenging for existing methods. This paper introduces MotionEdit, a high-quality dataset of 10,157 frame pairs extracted from videos and verified for realistic motion transformations, and MotionNFT, a post-training framework that leverages optical flow-based motion alignment rewards to guide models toward accurate motion changes. The approach consistently improves motion fidelity and overall editing quality across multiple models, achieving over 10% gains in generative metrics while demonstrating superior motion alignment without sacrificing general editing ability.

## Method Summary
The method introduces MotionEdit, a dataset constructed from video frame pairs with MLLM-based filtering for setting consistency, significant motion change, and subject integrity. MotionNFT extends DiffusionNFT with a composite reward combining optical flow magnitude, direction, and movement regularization terms (50% motion reward + 50% MLLM reward). The framework computes motion alignment between input-edit and input-ground truth optical flows, using quantized rewards within a negative-aware fine-tuning loop. Training uses base models FLUX.1 Kontext [Dev] and Qwen-Image-Edit with FSDP and gradient checkpointing, balancing geometric motion cues with semantic preservation.

## Key Results
- MotionNFT achieves over 10% gains in generative metrics (Fidelity, Preservation, Coherence, Overall) across FLUX.1 Kontext and Qwen-Image-Edit models
- Motion Alignment Score (MAS) improvements demonstrate superior motion alignment without sacrificing general editing ability
- Dataset construction yields 3-5.8× greater motion magnitude than prior image editing datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optical flow-based motion alignment rewards provide explicit geometric supervision for motion-centric image editing
- **Mechanism**: Given input image, model-edited image, and ground-truth image triplet, optical flow fields are computed between input→edited and input→ground-truth pairs. A composite reward combines motion magnitude consistency, direction consistency, and movement regularization terms. This reward is quantized and used within DiffusionNFT framework to guide policy updates toward velocity fields that produce edits matching ground-truth motion
- **Core assumption**: Optical flow between input and ground-truth frames accurately captures intended motion transformation; flow alignment correlates with perceptually correct motion edits
- **Evidence anchors**: Abstract states MotionNFT leverages motion alignment measurement between input-edit and input-ground truth optical flows; section 4.2 details D_mag, D_dir, M_move terms and composite reward formulation; corpus shows neighbor papers use optical flow for motion tasks
- **Break condition**: If optical flow estimation fails (occlusions, large displacements beyond model capacity), reward signal becomes noisy; if motion edits require 3D reasoning not captured by 2D flow, alignment may not correlate with edit quality

### Mechanism 2
- **Claim**: Negative-aware fine-tuning with motion rewards improves motion editing without sacrificing general editing ability
- **Mechanism**: MotionNFT extends DiffusionNFT by computing motion alignment rewards for sampled edits. During training, the model learns both positive velocity (toward high-reward edits) and negative velocity (away from low-reward edits). Optimality reward normalization ensures consistent positive/negative assignment across prompts
- **Core assumption**: Motion reward distribution is stable enough for normalization; contrasting positive and negative samples provides useful learning signal for motion editing
- **Evidence anchors**: Abstract states MotionNFT computes motion alignment rewards guiding models toward accurate motion transformations; section 4.1 details DiffusionNFT preliminaries and loss function L(θ); corpus lacks direct validation of negative-aware fine-tuning for motion editing
- **Break condition**: If reward model is miscalibrated or reward distribution shifts significantly during training, optimality rewards become unreliable; if negative samples are not truly "negative" for motion, contrastive signal harms learning

### Mechanism 3
- **Claim**: Video-derived motion edit triplets provide higher-quality supervision than existing image editing datasets for motion-centric tasks
- **Mechanism**: Frame pairs extracted from temporally smooth T2V videos capture realistic motion transitions with preserved identity/background. MLLM-based filtering ensures setting consistency, significant motion change, and subject integrity. Edit prompts are refined from MLLM-generated motion summaries
- **Core assumption**: Video-derived frame pairs contain faithful, physically plausible motion edits; MLLM filtering criteria correlate with edit quality
- **Evidence anchors**: Abstract states high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos; section 3.2 and Figure 6 show dataset construction pipeline and MotionEdit's 3-5.8× greater motion magnitude than prior datasets; corpus shows neighbor papers work with video motion but don't validate video-to-edit-triplet conversion quality
- **Break condition**: If video frames contain subtle artifacts, camera motion, or subject changes not caught by MLLM filtering, ground-truth targets become unreliable; if prompt refinement loses motion specificity, instruction alignment degrades

## Foundational Learning

- **Concept: Optical Flow Estimation**
  - **Why needed here**: Core to motion reward mechanism; computes per-pixel displacement vectors between image pairs to quantify motion magnitude and direction
  - **Quick check question**: Can you explain why optical flow between input and ground-truth frames represents the "intended motion" for editing?

- **Concept: Flow Matching Models (FMMs) for Diffusion**
  - **Why needed here**: Base architecture (FLUX.1 Kontext) uses flow matching; understanding velocity field prediction is necessary to modify training with NFT
  - **Quick check question**: How does the velocity field v_θ differ from noise prediction in DDPMs?

- **Concept: Reinforcement Learning from Rewards in Diffusion**
  - **Why needed here**: MotionNFT frames fine-tuning as policy optimization with reward signals; understanding reward normalization and positive/negative sample contrast is critical
  - **Quick check question**: Why is reward normalization (optimality reward) necessary before using rewards in the NFT loss?

## Architecture Onboarding

- **Component map**: Video collection quality → MLLM filtering accuracy → optical flow reliability → reward signal quality → training stability → motion edit performance
- **Critical path**: The reward calculator is the most critical novel component, connecting optical flow estimation to model training through quantized motion rewards
- **Design tradeoffs**:
  - Motion vs. semantic reward balance: 50-50 weighting performed best in ablations; pure motion reward degrades semantic fidelity
  - Flow estimator capacity vs. speed: Lightweight UniMatch (335M params) used for efficiency; larger models may improve reward accuracy
  - Quantization levels: 6 discrete levels approximate human ratings; more levels may increase granularity but also noise sensitivity
- **Failure signatures**:
  - Reward collapse: Motion reward plateaus or degrades during training (observed in MLLM-only baseline in ablation)
  - Under-motion: Edits remain near-identical to input despite instructions (penalized by M_move term)
  - Multi-subject errors: Incorrect subject/limb moves when multiple agents present (observed in failure cases)
  - Identity loss: Subject appearance changes during motion edit (if preservation not enforced)
- **First 3 experiments**:
  1. **Dataset validation**: Randomly sample 20 triplets; manually verify prompt-image alignment, setting consistency, and motion faithfulness. Calculate motion magnitude distribution vs. stated 0.19 average
  2. **Reward sanity check**: Implement reward calculator in isolation; test on known-good/bad edit pairs. Verify correct motion edits receive high rewards (>0.8) and static/incorrect edits receive low rewards (<0.3)
  3. **Minimal training run**: Apply MotionNFT to FLUX.1 Kontext for 100 steps with 50-50 reward weighting. Track MAS, overall quality, and visual outputs every 25 steps. Compare against baseline and MLLM-only ablation curves

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can models be improved to handle complex multi-subject interactions where one subject must be moved relative to another without distorting non-interacting entities?
- **Basis in paper**: Section 9.2.3 states, "Challenging editing settings with multiple involving and non-involving subjects in images pose a major challenge," noting that models fail to position subjects correctly (e.g., orca/polar bear examples)
- **Why unresolved**: Current methods, including MotionNFT, struggle with the 3D spatial reasoning required to execute motion edits that involve occlusion or precise relative positioning of multiple agents
- **What evidence would resolve it**: Qualitative and quantitative success on a sub-benchmark of MotionEdit specifically targeting "Inter-Subject Interaction" without identity loss in background characters

### Open Question 2
- **Question**: Can the integration of stronger physics-based priors resolve failure cases involving "editing inertia" or implausible object dynamics?
- **Basis in paper**: Section 9.2.3 suggests "future work incorporating stronger physics-based priors or motion guidance could further resolve the remaining challenges," while Figure 13 shows baselines failing to capture downward falling physics
- **Why unresolved**: Current optical flow-based reward guides geometric alignment but lacks explicit understanding of gravity or physical plausibility, leading to static or "floating" edits
- **What evidence would resolve it**: A modified reward function or architecture that explicitly models physical constraints, resulting in higher Motion Alignment Scores (MAS) for the "Locomotion" category

### Open Question 3
- **Question**: What is the optimal strategy for balancing geometric motion rewards (optical flow) versus semantic rewards (MLLM) to prevent degradation in general editing capabilities?
- **Basis in paper**: Table 4 shows that relying solely on motion reward (1.0 * Motion) degrades performance, indicating geometric cues are "insufficient for maintaining semantic fidelity"
- **Why unresolved**: While paper identifies 0.5/0.5 balance as effective, it doesn't explore if this weighting is optimal for all motion categories or if dynamic weighting mechanism is needed
- **What evidence would resolve it**: An ablation study showing that adaptive or category-aware weighting scheme outperforms static 0.5/0.5 split on Overall metric in MotionEdit-Bench

## Limitations

- **Dataset availability**: MotionEdit dataset is not yet publicly released, preventing independent validation until availability
- **Optical flow dependency**: Motion reward mechanism critically depends on optical flow estimation accuracy; paper lacks systematic analysis of flow estimation failure cases (occlusions, large displacements, multiple subjects)
- **Generalization scope**: While improvements shown on motion-centric edits, paper lacks comprehensive evaluation on general image editing tasks beyond stated 10% generative metric improvements

## Confidence

- **High Confidence**: Core contribution of introducing motion-centric benchmark with higher-quality supervision than existing datasets is well-supported by dataset construction methodology and comparative motion magnitude analysis (3-5.8× increase)
- **Medium Confidence**: MotionNFT framework design is sound and follows established NFT principles, but specific reward formulation (50-50 weighting, normalization approach) lacks ablation validation across diverse scenarios
- **Medium Confidence**: Empirical improvements over baseline models are demonstrated, but analysis focuses on quantitative metrics without extensive qualitative user study validation

## Next Checks

1. **Dataset quality validation**: Once MotionEdit is available, conduct blind human evaluation comparing dataset quality against existing image editing datasets using the same MLLM metrics reported in the paper
2. **Reward calibration testing**: Implement the optical flow reward calculator and test on edge cases (occlusions, multiple subjects, extreme poses) to verify reward signal quality and identify failure modes
3. **Generalization benchmarking**: Evaluate MotionNFT-trained models on non-motion image editing tasks (e.g., style transfer, inpainting) to verify claim that motion rewards don't sacrifice general editing ability