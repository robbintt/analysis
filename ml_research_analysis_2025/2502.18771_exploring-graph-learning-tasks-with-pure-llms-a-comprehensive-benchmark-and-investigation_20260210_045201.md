---
ver: rpa2
title: 'Exploring Graph Learning Tasks with Pure LLMs: A Comprehensive Benchmark and
  Investigation'
arxiv_id: '2502.18771'
source_url: https://arxiv.org/abs/2502.18771
tags:
- graph
- node
- llms
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark evaluating the performance
  of large language models (LLMs) on graph learning tasks, comparing them to 16 traditional
  graph learning models. The study investigates both off-the-shelf and instruction-tuned
  LLMs across node classification and link prediction tasks, using standardized datasets
  and prompt designs.
---

# Exploring Graph Learning Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation

## Quick Facts
- **arXiv ID:** 2502.18771
- **Source URL:** https://arxiv.org/abs/2502.18771
- **Reference count:** 40
- **Primary result:** Instruction-tuned LLMs match or exceed traditional graph learning models, especially in few-shot settings and across domain transfer tasks.

## Executive Summary
This paper presents a comprehensive benchmark evaluating large language models (LLMs) on graph learning tasks, comparing them to 16 traditional graph learning models. The study investigates both off-the-shelf and instruction-tuned LLMs across node classification and link prediction tasks using standardized datasets and prompt designs. Results show that LLMs, especially larger models with instruction tuning, match or exceed traditional models, particularly in few-shot settings and across domain transfer tasks. Instruction tuning further enhances LLM performance, enabling strong generalization and robustness even under data scarcity. The study highlights LLMs' broader potential for graph learning and provides a foundation for future research in low-resource and real-world graph scenarios.

## Method Summary
The paper evaluates LLMs on text-attributed graphs using five prompt formats for node classification (ego, 1-hop/2-hop with/without labels) and nine for link prediction. Models tested include Llama3B/8B-Instruct, Qwen-plus, and GPT-4o. Instruction tuning uses LoRA adapters (rank=16, alpha=32, dropout=0.05) with one epoch training. Datasets include Cora, PubMed, OGBN-ArXiv, and OGBN-Products with 6:2:2, 6:2:3, and 8:2:90 train/val/test splits. Performance is measured by accuracy, comparing pure LLMs against 16 traditional graph learning baselines.

## Key Results
- Instruction-tuned Llama8B with 2-hop prompts achieves 86.35% average accuracy vs. 68.52% for untuned models (17.83 point gain)
- LLMs outperform traditional GNNs by 18-30% in cross-dataset transfer learning
- Continuous pre-training improves few-shot performance by 3-5 percentage points over instruction tuning alone
- Off-the-shelf small LLMs (Llama3B) perform near random guessing (~14%) on pure structure tasks without attributes

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Graph Aggregation
Transformer self-attention can emulate GNN message-passing when graph structure is serialized into text prompts. For a token representing target node v, the attention update decomposes as: self-update (target tokens) + neighbor aggregation (neighbor tokens) + contextual noise. When k-hop neighbors are explicitly provided in the prompt, the model implicitly simulates a k-layer GNN within a single forward pass. Core assumption: The prompt explicitly labels which tokens are "neighbors," allowing attention to learn structural prioritization.

### Mechanism 2: Instruction Tuning Reshapes Attention Patterns
Instruction tuning transforms generic semantically-driven attention into specialized structurally-aware attention. Minimizing cross-entropy loss on graph-specific instruction-response pairs forces the model to assign higher attention weights to tokens representing true graph neighbors, effectively learning the inductive bias that "correct reasoning requires focusing on explicitly provided neighborhood information." Core assumption: Graph-specific tuning data contains sufficient examples for the model to learn structural reasoning patterns.

### Mechanism 3: Continuous Pre-training as Unsupervised Graph Representation Learning
Task-agnostic unsupervised pre-training on graph data (e.g., link prediction) before instruction tuning improves downstream few-shot performance. Continuous pre-training on unlabeled graph data builds general structural representations, which then transfer to task-specific instruction tuning with minimal labeled data. Core assumption: Unlabeled graph data is more abundant than labeled data; structural patterns learned via link prediction transfer to node classification.

## Foundational Learning

- **Concept: Message Passing in GNNs**
  - Why needed here: The paper's theoretical justification relies on understanding how GNNs aggregate neighbor information—LLMs must learn to emulate this via attention.
  - Quick check question: Can you explain the difference between AGGREGATE and UPDATE functions in standard GNN message passing?

- **Concept: Instruction Tuning vs. In-Context Learning**
  - Why needed here: The paper distinguishes off-the-shelf LLMs (using prompts with/without few-shot examples) from instruction-tuned models—the latter shows 15-20 point improvements.
  - Quick check question: What parameter changes occur during LoRA-based instruction tuning versus in-context few-shot prompting?

- **Concept: Homophily in Graphs**
  - Why needed here: Robustness experiments show LLMs are more resilient to homophily reduction than GNNs—understanding node similarity is critical for interpreting results.
  - Quick check question: Why does "drop same" (removing same-class edges) degrade performance more than "drop random"?

## Architecture Onboarding

- **Component map:** Graph Encoder -> Text Prompt Generation -> Off-the-shelf LLM -> LoRA Adapter -> Fine-tuned Model
- **Critical path:** 1) Preprocess raw node attributes into text (limit 1-hop to 20-30 neighbors, 2-hop to 5-10) 2) For instruction tuning: Generate prompts from training nodes, format as System + User + Answer 3) Fine-tune LoRA adapters on graph-task instruction data 4) For link prediction: Train with 9 diverse prompt formats for better generalization
- **Design tradeoffs:** Context window vs. neighborhood coverage: 2-hop provides richer structure but hits token limits faster on dense graphs. Training cost vs. transferability: One-time tuning on large dataset (Arxiv) enables cross-domain transfer, but single-dataset training takes 18-30+ hours on 4×A100. Prompt diversity vs. overfitting: 9 formats improve link prediction by 1-5% over 2 formats but require more training data.
- **Failure signatures:** Off-the-shelf small LLMs (Llama3B) on pure structure: ~14% accuracy (random guessing level). CoT/BAG on large-class datasets (Arxiv 40 classes, Products 47 classes): Performance degrades due to category ambiguity and token limitations for few-shot examples. Missing structural keywords ("neighbor", "hop") in prompts: 3-5 point accuracy drop—models fail to recognize relational structure.
- **First 3 experiments:** 1) Baseline comparison: Run Llama8B-Instruct with ego, 1-hop w/o label, 2-hop w/o label prompts on Cora/PubMed to quantify structural information contribution 2) Instruction tuning ablation: Fine-tune Llama3B with LoRA using only ego prompts vs. 2-hop prompts to isolate the effect of neighborhood context 3) Few-shot robustness: Train on 5/10 samples per class, compare tuned Llama8B vs. GCN/GraphSAGE vs. foundational graph prompt models (GPF-plus, GraphPrompt) to validate data scarcity advantage

## Open Questions the Paper Calls Out

- **Can pure LLM approaches maintain high performance on graph-level tasks (e.g., graph classification/regression) or graphs with non-semantic, numerical features?** The current study focuses primarily on text-attributed graphs where LLMs have a natural semantic advantage. Benchmarks evaluating pure LLMs on graph classification datasets with purely numerical node features or minimal text remain needed.

- **What encoding strategies are required for LLMs to effectively process graphs with extremely large or dense neighborhoods within finite context windows?** The current approach relies on simple serialization which may exceed token limits for complex graphs. Development and evaluation of sampling or compression strategies that allow LLMs to process high-degree nodes without performance degradation are needed.

- **How can LLMs be trained to balance textual semantic priors against graph topological evidence when these signals conflict?** The paper identifies that models sometimes misclassify nodes by relying on textual cues while ignoring structural evidence. Ablation studies or new tuning objectives that penalize models for ignoring structural data in favor of textual priors during conflict scenarios are needed.

## Limitations
- Performance gains depend heavily on prompt format choices and may reflect alignment with templates rather than learned structural reasoning
- Cross-dataset transfer results are limited to graph-structured data with text attributes; effectiveness on non-text graphs remains unproven
- Computational cost comparison inadequately addresses total training costs—instruction tuning takes 18-30+ hours on 4×A100 GPUs while traditional GNNs train in minutes

## Confidence

**High Confidence** (Empirical evidence strongly supports):
- LLMs outperform traditional GNNs in few-shot settings (5-10 samples per class)
- Instruction tuning provides consistent 15-20 point accuracy improvements across datasets
- Continuous pre-training improves few-shot performance by 3-5 percentage points

**Medium Confidence** (Theoretical justification with supporting evidence but some gaps):
- Attention mechanisms can emulate GNN message passing when graph structure is serialized
- Two-stage pre-training (unsupervised + instruction tuning) generalizes better than single-stage approaches
- LLMs show better robustness to homophily reduction than traditional GNNs

**Low Confidence** (Limited evidence or significant untested assumptions):
- LLMs will maintain performance advantages as graph sizes scale beyond current benchmarks
- The proposed mechanisms generalize to non-text graph data or multimodal graphs
- Computational efficiency claims hold when comparing total training costs across approaches

## Next Checks

1. **Ablation Study on Prompt Format**: Systematically remove structural keywords ("neighbor", "hop") from prompts while keeping instruction tuning intact to isolate whether improvements come from learned reasoning versus prompt engineering. Compare performance drops against randomly shuffling token positions.

2. **Compute Efficiency Benchmark**: Measure total energy consumption and wall-clock time for Llama8B instruction tuning versus training GCN/GAT on the same hardware. Include data loading, model initialization, and evaluation phases to provide complete cost comparison for low-resource scenarios.

3. **Transfer Failure Analysis**: Design experiments where models are trained on one graph type (e.g., citation networks) and tested on structurally different graphs (e.g., molecular graphs or social networks). Document accuracy drops and analyze whether attention patterns learned from text attributes transfer to non-text graphs.