---
ver: rpa2
title: 'Beyond More Context: How Granularity and Order Drive Code Completion Quality'
arxiv_id: '2510.06606'
source_url: https://arxiv.org/abs/2510.06606
tags:
- code
- context
- files
- python
- kotlin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of retrieving effective context
  for code completion in large repositories. The authors develop retrieval strategies
  at both file and chunk levels, focusing on context size and ordering effects.
---

# Beyond More Context: How Granularity and Order Drive Code Completion Quality

## Quick Facts
- arXiv ID: 2510.06606
- Source URL: https://arxiv.org/abs/2510.06606
- Authors: Uswat Yusuf; Genevieve Caumartin; Diego Elias Costa
- Reference count: 18
- Primary result: Chunk-based retrieval using static analysis improves code completion by 6% over file-level retrieval

## Executive Summary
This study addresses the challenge of retrieving effective context for code completion in large repositories. The authors develop retrieval strategies at both file and chunk levels, focusing on context size and ordering effects. Their chunk-based retrieval using static analysis improves performance by 6% over file-level retrieval and 16% over no-context baseline for Python. They introduce local-scope trimming and hybrid strategies, demonstrating that finer-grained retrieval and careful context selection significantly enhance code completion quality. The approach achieves competitive results across Python and Kotlin, with chunk-based methods consistently outperforming file-level retrieval, highlighting the importance of retrieval granularity and ordering in context collection pipelines.

## Method Summary
The authors tackle repository-level code completion through a Fill-in-the-Middle (FIM) task using JetBrains competition datasets for Python and Kotlin. Their pipeline involves chunking files using Tree-sitter into semantically coherent units (classes, functions, methods), then ranking these chunks using BM25 against prefix+suffix queries. They retrieve top-5 chunks in reversed (ascending) similarity order and apply local-scope trimming to restrict context to enclosing syntactic blocks. For Kotlin, they add 3 more chunks if the total remains under 2000 tokens. The method is evaluated across three models (Mellum:4b, Qwen2.5-Coder:7b, Codestral:22B) using average chrF score as the primary metric.

## Key Results
- Chunk-based retrieval using static analysis improves performance by 6% over file-level retrieval and 16% over no-context baseline for Python
- Reversing retrieval order yields a 3% improvement over default descending order for Python, attributed to truncation and primacy/recency bias
- Local-scope trimming of prefix and suffix improves performance for both Python and Kotlin, reaching an average chrF score of 0.65 comparable to the best chunk-level retrieval strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finer-grained retrieval (chunks) improves code completion over file-level retrieval when context windows are constrained.
- Mechanism: Chunking via syntax-aware parsing (Tree-sitter) segments code into semantically coherent units (classes, functions, methods). BM25 then ranks these smaller units, allowing higher-relevance content to fit within token limits while excluding irrelevant portions of otherwise useful files.
- Core assumption: Relevance is unevenly distributed within files; relevant signal can be isolated at function/class granularity.
- Evidence anchors:
  - [abstract] "chunk-based retrieval using static analysis improves performance by 6% over file-level retrieval"
  - [section III.C] "Top-5 BM25-ranked chunks yielded the best results for Python with a 3% improvement"
  - [corpus] Neighbor paper "A Systematic Analysis of Chunking Strategies for Reliable Question Answering" reports systematic evaluation of chunking strategies impacting RAG reliability; "Practical Code RAG at Scale" corroborates task-aware retrieval granularity effects.
- Break condition: If most relevant context spans multiple tightly coupled functions/classes that get fragmented, chunking may lose cross-unit dependencies.

### Mechanism 2
- Claim: Reversing retrieval order (ascending similarity) can yield small performance gains under left-side truncation regimes.
- Mechanism: When combined context exceeds the model's window, the environment truncates from the left. Reversed order places less-similar items at the truncation boundary, preserving more-relevant chunks on the right (recency). This also tests for primacy/recency bias in LLMs.
- Core assumption: Truncation is asymmetric (left-side) and relevance correlates with BM25 score; primacy/recency effects exist but are secondary to truncation survival.
- Evidence anchors:
  - [abstract] "careful context selection significantly enhance[s] code completion quality"
  - [section III.B] "reversing the order yields a 3% improvement over the default descending order" for Python; attributed to truncation and primacy/recency bias.
  - [corpus] No direct corpus evidence on ordering effects; this is a gap in related work.
- Break condition: If context fits entirely within the window, ordering effects likely diminish; gains are contingent on overflow scenarios.

### Mechanism 3
- Claim: Local-scope trimming of prefix/suffix improves completion quality by reducing noise.
- Mechanism: Restrict prefix/suffix to the nearest enclosing syntactic block (function/class definition). This focuses the model on immediately relevant structural context and removes distant, potentially distracting code.
- Core assumption: The most useful signals for filling the middle are local to the enclosing definition; distant code adds more noise than signal.
- Evidence anchors:
  - [section III.D] "Trimming the prefix and suffix improves performance for both Python and Kotlin... reaches an average chrF score of 0.65 comparable to the best chunk-level retrieval strategy"
  - [section IV] "Choosing the right context budget is nontrivial: more context can help, but too much adds noise"
  - [corpus] Weak direct evidence; corpus papers focus on retrieval, not prefix/suffix trimming.
- Break condition: If the missing code depends on context outside the enclosing block (e.g., sibling methods, class-level invariants), local-scope trimming may exclude necessary signals.

## Foundational Learning

- Concept: Fill-in-the-Middle (FIM) task formulation
  - Why needed here: The entire evaluation uses FIM (prefix + suffix + context → generate middle). Understanding FIM is prerequisite to designing retrieval and trimming.
  - Quick check question: Given a function with masked body, what constitutes prefix, suffix, and what context types could help?

- Concept: BM25 lexical ranking
  - Why needed here: All retrieval experiments use BM25 to rank files/chunks by lexical similarity to the query (prefix+suffix).
  - Quick check question: How does BM25 differ from embedding-based semantic search, and when might lexical matching fail for code?

- Concept: Context window truncation behavior
  - Why needed here: Ordering experiments depend on understanding how platforms truncate overflow (left vs. right).
  - Quick check question: If your context exceeds 16K tokens and truncation is left-side, where should you place the highest-relevance chunk?

## Architecture Onboarding

- Component map: Chunker (Tree-sitter) -> Retriever (BM25) -> Orderer -> Local-Scoper -> Composer -> Model inference
- Critical path: Query → Chunk retrieval (Top-K) → Ordering → Local-scope trimming → Prompt composition → Model inference
- Design tradeoffs:
  - Chunk granularity vs. context fragmentation: Finer chunks increase precision but may break cross-unit dependencies
  - K selection vs. noise: More chunks add coverage but risk diluting signal; optimal K varies by repository size
  - Lexical (BM25) vs. semantic retrieval: BM25 is simple and fast; semantic embeddings may improve recall but add complexity
- Failure signatures:
  - Chunk-level underperformance: May indicate over-fragmentation or poor query formulation for BM25
  - Ordering reversals hurting performance: May indicate truncation regime differs or context fits within window
  - Local-scope trimming degrades results: May indicate missing cross-function/class dependencies
- First 3 experiments:
  1. Baseline comparison: No-context vs. single recent file vs. Top-2 BM25 files (measure chrF delta)
  2. Granularity sweep: Compare Top-5 file vs. Top-5 standard chunk vs. Top-5 method-level chunk on same test set
  3. Ordering + truncation probe: Test ascending vs. descending order with artificial context overflow to confirm truncation-side behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does context ordering significantly impact performance when the retrieved context fits entirely within the model's token window?
- Basis in paper: [explicit] The authors state, "an open problem remains whether there would be a notable impact if the context fits within the model’s token window."
- Why unresolved: The observed performance gains from reversing file order may be confounded by the environment's behavior of truncating input from the left when exceeding the context limit.
- What evidence would resolve it: Ablation studies varying context order while strictly controlling for context window saturation and truncation.

### Open Question 2
- Question: To what extent can hybrid lexical-semantic retrieval strategies improve code completion quality over BM25 alone while maintaining production viability?
- Basis in paper: [explicit] Discussion point 4 highlights that "The remaining challenge is selecting models that integrate well with production environments" despite the potential of semantic embeddings.
- Why unresolved: The study relied primarily on lexical similarity (BM25); semantic retrieval introduces latency and integration complexities not addressed by the current experiments.
- What evidence would resolve it: Comparative evaluation of embedding-based retrieval latency and accuracy against the lexical baseline in a real-world setting.

### Open Question 3
- Question: Can a single, unified context collection strategy achieve optimal performance across structurally distinct programming languages?
- Basis in paper: [explicit] Discussion point 5 notes, "Cross-language generalization remains an open challenge" as different languages responded better to different chunking strategies.
- Why unresolved: Python performed best with standard chunking, while Kotlin benefitted more from method-level chunking, forcing the authors to use per-language settings.
- What evidence would resolve it: Testing a unified set of hyperparameters across a multi-language benchmark to see if a single strategy can suffice.

## Limitations
- The study's effectiveness is tied to competition datasets with undisclosed preprocessing details, making exact reproduction challenging
- The 6% improvement for Python may not generalize to other languages or repositories with different coupling patterns
- Local-scope trimming improvements lack strong empirical backing from the broader literature, as most corpus work focuses on retrieval rather than prefix/suffix trimming strategies

## Confidence

- **High confidence**: Chunk-based retrieval using static analysis improves performance over file-level retrieval (6% improvement quantified)
- **Medium confidence**: Local-scope trimming of prefix/suffix consistently improves completion quality across Python and Kotlin
- **Medium confidence**: Reversed retrieval order provides benefits under left-side truncation regimes
- **Low confidence**: Generalization of chunk granularity effects to non-competition datasets and different programming languages

## Next Checks

1. **Truncation regime validation**: Create controlled experiments with artificial context overflow to verify whether truncation occurs from left or right side across different platforms, then test if ascending order consistently outperforms descending order only under left-side truncation.

2. **Cross-language generalization**: Apply the identical chunk-based retrieval pipeline to additional programming languages (e.g., Java, JavaScript) using the same BM25 and Tree-sitter approach to determine if the 6% improvement over file-level retrieval holds beyond Python and Kotlin.

3. **Local-scope boundary impact**: Systematically vary the scope of prefix/suffix trimming (enclosing block vs. full file) on a diverse set of completion tasks to quantify the trade-off between noise reduction and loss of cross-block dependencies, measuring both chrF scores and completion success rates.