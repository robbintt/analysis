---
ver: rpa2
title: Disentangling the Roles of Representation and Selection in Data Pruning
arxiv_id: '2507.03648'
source_url: https://arxiv.org/abs/2507.03648
tags:
- data
- selection
- instances
- representations
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically disentangles the roles of data representation
  and selection algorithms in data pruning for NLP model training. The authors decompose
  data pruning methods into two key components: (1) data representations extracted
  from a reference model, such as training dynamics, hidden states, or gradients,
  and (2) selection algorithms guided by objectives like maximizing difficulty, diversity,
  or relevance to validation data.'
---

# Disentangling the Roles of Representation and Selection in Data Pruning

## Quick Facts
- **arXiv ID**: 2507.03648
- **Source URL**: https://arxiv.org/abs/2507.03648
- **Reference count**: 40
- **Primary result**: Data representations have greater influence on pruning effectiveness than selection algorithms; gradient-based representations outperform hidden states

## Executive Summary
This paper systematically analyzes data pruning methods for NLP by decomposing them into two components: data representations extracted from a reference model, and selection algorithms guided by objectives like difficulty, diversity, or relevance. Through theoretical analysis and empirical experiments on synthetic and NLP tasks (CAD, WinoGrande, DialogSum), the authors demonstrate that representations have larger influence on instance selection than selection algorithms themselves, with gradients and losses being more informative than hidden states. The best pruning method depends heavily on the specific task and data budget, with gradient-based methods like LESS generally performing well across settings.

## Method Summary
The authors decompose data pruning into two components: (1) data representations from reference models (training dynamics, hidden states, gradients, losses), and (2) selection algorithms with objectives like maximizing difficulty, diversity, or relevance. They analyze three criteria for effective representations: accounting for distance to decision boundary, containing label information, and being discriminative for important instances. Empirical experiments use CAD, WinoGrande, and DialogSum datasets with various representation-algorithm combinations, comparing against random selection and a dummy baseline. Representation extraction involves training a reference model and computing representations at multiple checkpoints, while selection algorithms score and rank instances based on their chosen objectives.

## Key Results
- Representations have larger influence on instance selection than selection algorithms themselves
- Gradients and losses are more informative than hidden states for selection
- Selection algorithms do not always achieve their stated objectives, sometimes selecting instances that contradict their goals
- Best pruning method depends heavily on task and data budget, with gradient-based methods like LESS performing well across settings

## Why This Works (Mechanism)

### Mechanism 1: Gradient representations encode task-relevant geometry
- Claim: Gradient-based representations yield better instance selection than hidden states or losses alone.
- Mechanism: Gradients integrate distance to decision boundary via zi = y·w^T·h and label agreement via yiyj terms. Discriminative power Cg peaks near the boundary for misclassified instances and decays for confident predictions, enabling prioritization of informative, diverse examples while ignoring redundant easy ones.
- Core assumption: Reference model's decision boundary approximates target model's boundary; binary classification analysis generalizes to multi-class/generation via per-token considerations.
- Evidence anchors:
  - [abstract] "better representations, e.g., training gradients, generally lead to a better selection of instances, regardless of the chosen selection algorithm"
  - [section 3.1] Theorem 3.3 and Corollary 3.4 show Cℓ decreases monotonically with prediction confidence, while Cg peaks near the boundary when α ≳ 1.544
  - [corpus] Weak/no direct corpus corroboration; neighbor papers address unrelated selection/pruning problems
- Break condition: If reference and target models diverge significantly in their decision boundaries (e.g., different architectures, pretraining regimes), gradient informativeness may degrade.

### Mechanism 2: Representations dominate selection via distance-space conditioning
- Claim: Choice of representation has greater influence on selected subsets than choice of selection algorithm.
- Mechanism: Selection algorithms operate on pairwise distances in representation space. When representations differ (e.g., h vs. ∇ℓ), distance distributions diverge substantially, yielding low selection overlap even with identical algorithms. Conversely, shared representations produce similar distance rankings, causing high overlap across different algorithms.
- Core assumption: Selection algorithms predominantly rely on relative distance rankings or cluster structures derived from representations.
- Evidence anchors:
  - [abstract] "representations are more influential than selection algorithms"
  - [section 3.2] Overlap ratios: ∇ℓ-difficultyproto overlaps 0.79 with ∇ℓ-relevanceless but only 0.26 with h-difficultyproto (CAD); gradient-loss overlap reaches 0.75
  - [corpus] Not directly addressed in corpus; related work on algorithm selection (arXiv:2504.11440) focuses on solver selection, not representation effects
- Break condition: If selection algorithms use non-distance-based criteria (e.g., LLM prompting, rule-based filters), representation influence may weaken.

### Mechanism 3: Objective-alignment requires representation-algorithm compatibility
- Claim: Selection algorithms do not reliably achieve their stated objectives (e.g., maximizing difficulty).
- Mechanism: Algorithms are heuristic-driven and representation-dependent. Hidden-state-based prototypicality selects instances far from centroids, which may lie far from the decision boundary—these are confidently predicted (easy) examples, contradicting the "difficulty" objective. Loss-based hard-to-learn selects high-loss instances, which better align with difficulty but may include noisy/mislabeled data.
- Core assumption: Intended objectives (difficulty, diversity, relevance) have ground-truth definitions tied to model training dynamics.
- Evidence anchors:
  - [abstract] "algorithms designed for the same objective can select drastically different instances"
  - [section 3.2, Figure 2a] h-difficultyproto selects points far from the decision boundary (correctly predicted, "easier" instances)
  - [section 4.1] Hidden-state methods perform comparably to random selection; difficulty methods require >30% budgets
  - [corpus] No corpus evidence directly addresses objective-alignment failures
- Break condition: If representations inherently encode the objective (e.g., gradients for influence, loss for difficulty), alignment improves—but this depends on representation quality.

## Foundational Learning

- Concept: Decision boundary geometry in classification
  - Why needed here: Theoretical analysis hinges on zi = y·w^T·h (signed distance to boundary) and how representations encode this quantity.
  - Quick check question: Given a binary classifier with weight vector w and hidden state h, what does a large positive zi indicate about the instance's difficulty?

- Concept: Gradient-based influence estimation
  - Why needed here: LESS and self-influence use ∇θℓ to estimate training data impact; understanding this requires knowing why gradient similarity correlates with influence.
  - Quick check question: Why might two instances with similar gradients have similar effects on model updates?

- Concept: Spectral norm and Jacobian sensitivity
  - Why needed here: Paper uses spectral norms (Cℓ, Cg) to quantify discriminative power; this measures how sensitive representations are to input changes near the boundary.
  - Quick check question: If a representation's Jacobian spectral norm is near zero for easy examples, what does that imply about its ability to distinguish among them?

## Architecture Onboarding

- Component map: Original Dataset D → Reference Model M_ref (pretrained or lightly fine-tuned) → Representation Extractor → Training dynamics / Hidden states h / Gradients ∇ℓ → Selection Algorithm → Difficulty / Diversity / Relevance scoring → Selected Subset S (size = budget × |D|) → Main Model M_main (trained on S)

- Critical path:
  1. Train reference model for T epochs, saving checkpoints at intervals
  2. Compute representations (gradients recommended): project ∇ℓ to ~1024D for scalability (per Park et al., 2023)
  3. Run selection algorithm; for LESS, compute gradient similarity to validation data
  4. Apply label balancing if dataset is imbalanced (LESS over-selects majority class)
  5. Train main model on S; evaluate on held-out test set

- Design tradeoffs:
  - Gradients vs. hidden states: Gradients are more informative but computationally expensive (backprop per instance); hidden states are cheap but low discriminative power
  - Difficulty vs. relevance: Difficulty methods excel with high budgets; relevance methods handle distribution shift better
  - Cluster count (k): Paper uses √N; too few clusters reduces diversity, too many increases redundancy
  - Checkpoint selection: First 5 checkpoints most consistent with full training dynamics (Spearman ρ ≥ 0.96)

- Failure signatures:
  - Performance ≈ random selection → Likely using hidden states or low-budget difficulty methods
  - Performance ≈ dummy baseline → Difficulty method at very low budget (<15%); model fails to converge
  - Majority-class bias → LESS without label matching on imbalanced data
  - High variance across seeds → Selection algorithm sensitive to representation noise; consider ensembling

- First 3 experiments:
  1. **Ablate representations with fixed algorithm**: Run Prototypicality (difficulty) with h, ℓ, and ∇ℓ on a held-out validation task; measure selection overlap and downstream performance to verify representation dominance claim.
  2. **Budget sweep for algorithm comparison**: Test Hard-to-Learn, LESS, and Random at 5%, 15%, 30%, 50%, 70% budgets; identify which algorithm excels at which budget regime for your task.
  3. **Objective-alignment sanity check**: Visualize selected instances in 2D (via PCA/t-SNE of gradients); verify that "difficulty" methods select near-boundary points, not easy examples far from boundary.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis focuses on binary classification with linear models; generalization to multi-class and generative tasks relies on unproven assumptions about per-token behavior
- Representation influence may degrade when reference and target models have substantially different architectures or pretraining
- Study uses specific NLP datasets and tasks; results may not transfer to other domains or task types
- Computational constraints limited representation dimensionality to 1024D; optimal dimensionality may vary by task

## Confidence
- **High Confidence**: Representation dominance over selection algorithms (empirically demonstrated across multiple datasets and algorithms)
- **Medium Confidence**: Gradient superiority over hidden states (supported by theory and experiments, but computational expense may limit practical adoption)
- **Medium Confidence**: Algorithm-objective misalignment (clearly shown for hidden-state prototypicality, but other algorithms may perform better with tuned parameters)
- **Low Confidence**: Budget-dependent algorithm performance (observed trends but limited budget granularity in experiments)

## Next Checks
1. Test gradient-based representations with non-NLP tasks (e.g., tabular or vision data) to verify cross-domain applicability
2. Implement a multi-reference model ensemble to measure robustness when reference and target models diverge
3. Conduct a systematic budget sweep (5% increments from 5% to 70%) for each algorithm-representation pair to precisely map performance boundaries