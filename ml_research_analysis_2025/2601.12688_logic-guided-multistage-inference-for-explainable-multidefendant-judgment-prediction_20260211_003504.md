---
ver: rpa2
title: Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction
arxiv_id: '2601.12688'
source_url: https://arxiv.org/abs/2601.12688
tags:
- prediction
- guilt
- legal
- mmsi
- defendant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of assigning guilt and sentencing
  in multidefendant criminal cases, where intertwined actions and roles complicate
  judicial decision-making. To improve role differentiation and sentencing accuracy,
  the authors propose a masked multistage inference (MMSI) framework that incorporates
  oriented masking to isolate defendant-specific information and broadcasts inferred
  guilt labels into a sentencing regression model.
---

# Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction

## Quick Facts
- arXiv ID: 2601.12688
- Source URL: https://arxiv.org/abs/2601.12688
- Authors: Xu Zhang; Qinghua Wang; Mengyang Zhao; Fang Wang; Cunquan Qu
- Reference count: 40
- This work addresses the challenge of assigning guilt and sentencing in multidefendant criminal cases, where intertwined actions and roles complicate judicial decision-making. To improve role differentiation and sentencing accuracy, the authors propose a masked multistage inference (MMSI) framework that incorporates oriented masking to isolate defendant-specific information and broadcasts inferred guilt labels into a sentencing regression model. The framework aligns with judicial logic by first inferring guilt from criminal facts and then using these inferences to inform sentencing from court views. Evaluated on the custom IMLJP dataset for intentional injury cases, MMSI significantly outperforms existing baselines and even surpasses SOTA large language models in both accuracy and interpretability. The method offers a robust solution for enhancing intelligent judicial assistance in multidefendant scenarios.

## Executive Summary
This paper tackles the complex problem of multidefendant judgment prediction in Chinese criminal courts, where defendants in the same case often have intertwined actions and roles. The authors propose a Masked Multistage Inference (MMSI) framework that separates the task into two logical stages: first inferring each defendant's guilt role (principal vs. accomplice) from criminal facts, then using these inferences to predict prison sentences from court views. By introducing oriented masking to isolate defendant-specific context and broadcasting guilt labels into sentencing regression, the framework mimics judicial reasoning while improving both accuracy and interpretability. Evaluated on a custom IMLJP dataset of intentional injury cases, MMSI significantly outperforms existing baselines and even surpasses large language models in multidefendant scenarios.

## Method Summary
The MMSI framework uses a two-stage architecture: Stage 1 employs oriented masking (replacing target defendant names with `[X]`) to classify guilt role (principal/accomplice) from fact descriptions using BERT. Stage 2 takes court view text, fuses it with the predicted guilt label (broadcast as a vector matching the [CLS] embedding dimension), and performs regression to predict prison term. The oriented masking forces the model to rely on contextual cues rather than name patterns for role differentiation. Comparative data pairs from same-case defendants with opposite roles provide contrastive supervision for guilt inference. The framework is evaluated on the custom IMLJP dataset of 17,253 intentional injury cases.

## Key Results
- MMSI achieves 0.9105 accuracy and 0.8766 F1 on guilt inference, outperforming baselines (Split: 0.6281, Original: 0.5000)
- For prison term prediction, MMSI reaches 0.7069 ImpScore vs. 0.5795 for mT5 and 0.5443 for HRN
- The framework significantly outperforms large language models like DeepSeek-V3 on multidefendant judgment prediction
- Oriented masking and comparative data pairing strategies show clear benefits over standard approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Oriented masking improves role differentiation in multidefendant cases by normalizing defendant names to a unified token.
- Mechanism: Replace all occurrences of the target defendant's name with a special `[X]` token before encoding. This forces the model to attend to contextual cues (action verbs, causal relationships) rather than relying on surface name patterns, effectively creating defendant-specific representations from shared text.
- Core assumption: Role-relevant information is encoded in the surrounding context rather than in the defendant's name string itself; the model can learn to associate `[X]` with culpability patterns across cases.
- Evidence anchors:
  - [abstract]: "an oriented masking mechanism clarifies roles"
  - [section III-C]: Table I shows MASK achieving 0.9105 accuracy vs. 0.6281 for Split and 0.5000 for Original on guilt inference
  - [corpus]: Weak direct evidence; related work focuses on general LJP interpretability rather than specific masking strategies
- Break condition: If cases have highly heterogeneous role-specific language patterns (e.g., different crime types with distinct terminology), the learned `[X]`-context associations may not transfer well.

### Mechanism 2
- Claim: Label broadcasting integrates guilt information into sentencing prediction more effectively than joint multitask learning.
- Mechanism: First predict guilt label (principal/accomplice) from fact descriptions using classification, then broadcast the scalar label into a vector matching the [CLS] embedding dimension and add it element-wise to the court-view representation before regression. This cascades information across stages rather than learning both tasks simultaneously.
- Core assumption: Guilt responsibility is causally upstream of sentencing and provides a sufficient signal for role-based sentence adjustment; the binary label captures the essential sentencing-relevant role distinction.
- Evidence anchors:
  - [abstract]: "Predicted guilt labels are further incorporated into a regression model through broadcasting"
  - [section III-E]: Eq. shows hfused = hCVd[CLS] + vg where vg = 1 · ŷg ∈ Rd
  - [corpus]: RLJP paper similarly incorporates logic rules but uses first-order logic rather than label propagation
- Break condition: If sentencing involves factors beyond principal/accomplice status that correlate with guilt (e.g., prior criminal history, remorse signals not captured in the binary label), the broadcast signal becomes insufficient.

### Mechanism 3
- Claim: Comparative data pairs from same-case defendants improve the model's sensitivity to nuanced role distinctions.
- Mechanism: Extract principal-accomplice pairs from cases containing both roles, creating training samples where identical or near-identical fact descriptions have opposite labels. This contrastive structure forces the model to learn fine-grained distinctions rather than relying on case-level features.
- Core assumption: Same-case pairs provide natural, high-quality contrastive supervision; the role distinction is learnable from subtle contextual differences rather than requiring explicit causal reasoning.
- Evidence anchors:
  - [abstract]: "comparative data construction strategy improves the model's sensitivity to culpability distinctions"
  - [Appendix B, Table VII]: Pairs achieves 0.8766 F1 vs. 0.8085 for Random on FD-based guilt inference
  - [corpus]: No direct corpus comparison; adjacent work (HRN, mT5) uses standard sampling without explicit pairing
- Break condition: If the dataset contains few mixed-role cases (paper notes 70.5% of cases have uniform roles), the pairing strategy provides limited additional signal.

## Foundational Learning

- Concept: **Transformer attention and masked language modeling**
  - Why needed here: Oriented masking builds on the intuition that BERT's attention mechanism learns contextual relationships; replacing names with `[X]` tests whether the model attends to role-relevant context vs. name-specific patterns.
  - Quick check question: Can you explain why masking a token and predicting it (as in MLM) differs from masking a token and using it as a structural anchor (as in oriented masking)?

- Concept: **Cascaded/multistage inference architectures**
  - Why needed here: MMSI explicitly separates guilt inference (Stage 1) from sentencing regression (Stage 2), emulating judicial decision logic rather than end-to-end prediction.
  - Quick check question: How does label broadcasting differ from simply concatenating FD and CV embeddings as input to a single-stage model?

- Concept: **Legal judgment prediction task structure**
  - Why needed here: LJP involves multiple subtasks (charge, articles, prison term); this work adds role differentiation as an intermediate target between facts and sentencing.
  - Quick check question: Why might treating prison term prediction as regression (continuous output) outperform classification (discrete bins) for multidefendant sentencing?

## Architecture Onboarding

- Component map:
  Input: Name + Fact Description (FD) + Court View (CVd)
           ↓
  Preprocessing: Oriented Masking (replace target name with [X])
           ↓
  Stage 1: BERT encoder → [CLS] → Linear classifier → Guilt label (0/1)
           ↓
  Stage 2: BERT encoder(CVd) → [CLS] embedding
           + Broadcast guilt vector (vg = 1·ŷg ∈ R^768)
           ↓
         Fused embedding → Linear regressor → Prison term (months)

- Critical path:
  1. Correctly identify defendant name positions for masking (failure here corrupts both stages)
  2. Stage 1 classification accuracy (low accuracy propagates errors to Stage 2)
  3. Embedding dimension alignment for label broadcasting (vg must match [CLS] dimension)
  4. Loss weighting if using joint training variant (α, β hyperparameters)

- Design tradeoffs:
  - Two-stage vs. joint training: Two-stage ensures explicit guilt inference but requires separate training; joint training (MMSIInject) achieves comparable results but requires careful loss balancing (Table X shows sensitivity to α, β)
  - Regression vs. classification for sentencing: Regression captures continuous sentence lengths but is harder to evaluate; classification bins reduce output space but lose granularity
  - Masking vs. sentence splitting: Masking preserves full context but dilutes defendant-specific signals; splitting isolates relevant sentences but may miss cross-sentence dependencies

- Failure signatures:
  - Guilt inference accuracy near 50%: Model failing to learn role distinctions (check masking implementation, data pairing quality)
  - Sentencing predictions clustering around mean: Regression head underfitting (increase epochs, check label broadcasting connection)
  - Large performance gap between principal and accomplice predictions: Class imbalance or insufficient paired training data
  - CVd-based predictions no better than FD-only: Label broadcasting not effectively integrated (verify element-wise addition, embedding dimensions)

- First 3 experiments:
  1. **Ablate masking strategy**: Train guilt inference with Original, Split, and MASK preprocessing on the same FD data; expect MASK > Split > Original (replicate Table I)
  2. **Test label broadcasting**: Train sentencing regression with CV-only vs. CV+Feat (broadcast labels) vs. FD+CV concatenation; expect CV+Feat best (replicate Table II)
  3. **Validate data pairing**: Compare Random vs. Pairs dataset construction on guilt inference; expect Pairs to outperform at equal sample sizes (replicate Table VII)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MMSI framework be effectively extended to predict probation outcomes (binary suspension and duration regression) alongside imprisonment?
- Basis in paper: [explicit] The authors state in the conclusion that "judgment prediction ideally encompasses all final court verdicts" and suggests "extending the framework into these areas" as future work.
- Why unresolved: The current study restricts its scope to fixed-term imprisonment, while probation involves distinct legal factors such as social impact and remorse that differ from core guilt determination.
- What evidence would resolve it: A modified MMSI architecture incorporating classification and regression heads for probation, evaluated on a dataset annotated with probation terms.

### Open Question 2
- Question: How can the framework better capture nuanced sentencing cues to improve accuracy for cases involving longer prison terms?
- Basis in paper: [inferred] Section IV notes that MMSI performance deteriorates on cases with longer sentences compared to DeepSeek-V3, suggesting that "additional sentencing-related cues embedded in the FD remain underutilized."
- Why unresolved: The current method relies heavily on principal/accomplice role information but fails to extract broader contextual features from Fact Descriptions that influence severe sentencing.
- What evidence would resolve it: An ablation study or architectural enhancement showing improved ImpScore on long-sentence subsets by incorporating advanced feature extraction from Fact Descriptions.

### Open Question 3
- Question: Can the MMSI framework and its oriented masking mechanism generalize to other types of crimes beyond intentional injury cases?
- Basis in paper: [explicit] The authors acknowledge in the conclusion that "this work focuses on intentional injury cases, but future efforts will aim to extend the... tasks to other types of crimes."
- Why unresolved: The current model is validated exclusively on the IMLJP dataset (intentional injury), and different crimes involve unique sentencing factors and legal logic.
- What evidence would resolve it: Successful evaluation of the MMSI model on multi-defendant datasets covering diverse crime types (e.g., theft, fraud) without significant performance degradation.

## Limitations
- Dataset Specificity: The custom IMLJP dataset is specifically curated for intentional injury cases in Chinese courts. While the framework's logic is generalizable, empirical validation on other crime types or legal systems is absent.
- Causal Attribution: The paper demonstrates correlation between guilt inference and sentencing accuracy but doesn't establish whether the broadcast label truly captures causal relationships in judicial reasoning or merely serves as a strong predictive feature.
- Implementation Details: Critical implementation specifics are underspecified, including the exact pruning rules for creating CVd and the dimensional alignment for guilt vector broadcasting.

## Confidence

**High Confidence**: The oriented masking mechanism improves role differentiation over baseline preprocessing strategies (Original/Split), supported by direct ablation comparisons (MASK: 0.9105 vs. Split: 0.6281 guilt inference accuracy).

**Medium Confidence**: The multistage inference framework outperforms end-to-end alternatives and LLMs on the IMLJP dataset, though this claim depends on the specific dataset construction and evaluation metrics chosen.

**Low Confidence**: The claim that MMSI "surpasses SOTA large language models in both accuracy and interpretability" lacks comparison details and quantitative evidence for interpretability advantages.

## Next Checks

1. **Dataset Transferability**: Evaluate MMSI on a non-Chinese or non-intentional-injury dataset to test framework generalizability beyond the IMLJP corpus.

2. **Causal Analysis**: Conduct ablation studies where guilt labels are randomized or corrupted to measure the actual causal contribution of label broadcasting versus mere feature correlation.

3. **Interpretability Verification**: Compare attention weight patterns and feature importance scores between MMSI and end-to-end baselines to substantiate interpretability claims beyond accuracy metrics.