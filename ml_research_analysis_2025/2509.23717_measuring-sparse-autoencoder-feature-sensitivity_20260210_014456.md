---
ver: rpa2
title: Measuring Sparse Autoencoder Feature Sensitivity
arxiv_id: '2509.23717'
source_url: https://arxiv.org/abs/2509.23717
tags:
- feature
- sensitivity
- text
- activating
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable method to measure feature sensitivity
  in Sparse Autoencoders (SAEs). Instead of relying on natural language explanations,
  the approach uses language models to generate text similar to a feature's activating
  examples and then measures how often the feature activates on this generated text.
---

# Measuring Sparse Autoencoder Feature Sensitivity

## Quick Facts
- **arXiv ID:** 2509.23717
- **Source URL:** https://arxiv.org/abs/2509.23717
- **Reference count:** 40
- **Primary result:** Introduces explanation-free method to measure feature sensitivity in SAEs, finding that interpretable features often fail to activate on semantically similar inputs and that sensitivity declines with SAE width.

## Executive Summary
This paper introduces a scalable method to measure feature sensitivity in Sparse Autoencoders (SAEs) without relying on natural language explanations. The approach uses language models to generate text similar to a feature's activating examples and measures how often the feature activates on this generated text. This explanation-free method directly evaluates whether features reliably activate on semantically similar inputs, avoiding potential errors from intermediate descriptions.

The study finds that many interpretable SAE features have poor sensitivity—they fail to activate on generated text that humans judge as similar to original activating examples. Additionally, the research reveals that average feature sensitivity declines with increasing SAE width across 7 different SAE variants, identifying a new challenge for SAE scaling. Human evaluation confirms that when features fail to activate on generated text, that text genuinely resembles the original activating examples, validating the method's reliability.

## Method Summary
The paper proposes an explanation-free approach to measuring feature sensitivity in Sparse Autoencoders. Instead of relying on natural language explanations, the method uses language models to generate text similar to a feature's activating examples and then measures how often the feature activates on this generated text. This direct approach avoids potential errors from intermediate descriptions and evaluates whether features reliably activate on semantically similar inputs. The method scales efficiently and provides a quantitative measure of feature sensitivity that can be applied across different SAE architectures and datasets.

## Key Results
- Many interpretable SAE features exhibit poor sensitivity, failing to activate on generated text that humans judge as similar to original activating examples
- Average feature sensitivity declines with increasing SAE width across 7 different SAE variants
- Human evaluation validates that when features fail to activate on generated text, that text genuinely resembles the original activating examples

## Why This Works (Mechanism)
The method works by creating a direct measurement pipeline that bypasses the need for natural language explanations, which can introduce errors and subjectivity. By using language models to generate semantically similar text and measuring actual activation rates, the approach captures the true sensitivity of features to variations in input that humans would consider equivalent. This explanation-free design allows for scalable and objective measurement of feature behavior.

## Foundational Learning

**Sparse Autoencoders (SAEs)**: Neural network components that learn to compress and reconstruct inputs with sparsity constraints, creating interpretable features. *Why needed:* SAEs are the primary target for sensitivity measurement in this work. *Quick check:* Understand how SAEs create sparse representations and why sparsity aids interpretability.

**Feature Sensitivity**: The degree to which a learned feature activates consistently on semantically similar inputs. *Why needed:* Central metric being measured and analyzed throughout the paper. *Quick check:* Grasp that sensitivity measures reliability of feature activation across similar inputs.

**Language Model Generation**: Using large language models to create synthetic examples similar to given prompts. *Why needed:* Core technique for generating test inputs without manual annotation. *Quick check:* Understand how LMs can produce semantically similar but syntactically varied text.

**Human Evaluation Protocols**: Structured methods for having humans assess semantic similarity between text examples. *Why needed:* Validates that generated text is genuinely similar to original activating examples. *Quick check:* Recognize the importance of human judgment in validating automated measurements.

## Architecture Onboarding

**Component Map**: Language Model Generator -> Feature Activation Measurement -> Human Validation Pipeline

**Critical Path**: Generate similar text → Measure feature activation rates → Compare against baseline → Validate with human evaluation

**Design Tradeoffs**: Explanation-free vs. interpretable methods; automated vs. manual evaluation; sensitivity vs. other SAE quality metrics

**Failure Signatures**: Features that activate inconsistently across semantically similar inputs; sensitivity that decreases with SAE width; human validation that contradicts automated measurements

**First Experiments**:
1. Test sensitivity measurement on a simple SAE with known interpretable features
2. Compare explanation-free sensitivity scores against traditional NL-based methods
3. Measure sensitivity across different SAE widths using the same underlying architecture

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The method focuses solely on sensitivity without examining how this metric relates to other SAE objectives like interpretability or steering effectiveness
- The relationship between sensitivity and SAE utility remains unclear
- The study does not explore whether poor sensitivity represents a fundamental limitation or a training artifact

## Confidence
- **High confidence**: Methodological innovation and core finding that many interpretable features exhibit poor sensitivity
- **Medium confidence**: Human validation results, as methodology relies on subjective human judgment
- **Low confidence**: Broader claims about SAE utility, as the study focuses on sensitivity alone

## Next Checks
1. Test the sensitivity measurement method across additional SAE architectures and datasets to determine if the width-sensitivity relationship holds generally
2. Conduct ablation studies to isolate whether sensitivity declines stem from architectural choices, training procedures, or dataset characteristics
3. Evaluate whether sensitivity correlates with other SAE quality metrics like steering effectiveness or interpretability scores to understand its practical significance