---
ver: rpa2
title: 'NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with
  Extended Realistic Scenarios'
arxiv_id: '2503.19267'
source_url: https://arxiv.org/abs/2503.19267
tags:
- data
- offline
- policy
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper extends the NeoRL benchmark with NeoRL-2, adding seven
  simulated tasks that incorporate realistic challenges often encountered in real-world
  applications: time delays, external factors, safety constraints, data from traditional
  control methods, and limited data availability. These features make the benchmark
  more representative of practical scenarios, such as robotics, industrial control,
  and healthcare.'
---

# NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios

## Quick Facts
- arXiv ID: 2503.19267
- Source URL: https://arxiv.org/abs/2503.19267
- Authors: Songyi Gao; Zuolin Tu; Rong-Jun Qin; Yi-Hao Sun; Xiong-Hui Chen; Yang Yu
- Reference count: 40
- Primary result: Extends NeoRL benchmark with 7 simulated tasks incorporating delays, external factors, safety constraints, and limited data to test real-world offline RL challenges

## Executive Summary
NeoRL-2 extends the NeoRL benchmark with seven simulated tasks that incorporate realistic challenges often encountered in real-world applications: time delays, external factors, safety constraints, data from traditional control methods, and limited data availability. These features make the benchmark more representative of practical scenarios such as robotics, industrial control, and healthcare. Experiments using state-of-the-art offline RL algorithms reveal that current methods often struggle to outperform the behavior policy in these challenging environments.

## Method Summary
NeoRL-2 introduces seven simulated tasks (Pipeline, Simglucose, RocketRecovery, RandomFrictionHopper, DMSD, Fusion, SafetyHalfCheetah) featuring realistic challenges like action delays, external factors, and PID-collected data. The benchmark evaluates model-free algorithms (BC, CQL, EDAC, MCQ, TD3BC) and model-based methods (MOPO, COMBO, RAMBO, Mobile) on static datasets collected from sub-optimal SAC policies or PID controllers. Performance is measured via normalized scores (0-100) relative to random and expert policies, with online evaluation used to assess final policies.

## Key Results
- Current offline RL methods struggle to outperform behavior policies in NeoRL-2's challenging environments
- Model-free algorithms generally outperform model-based methods, with TD3BC showing the most stable performance
- Tasks with delays, external factors, and safety constraints pose particular difficulties for existing algorithms
- Model-based methods exhibit higher instability, with RAMBO showing error bars up to ±307 in some tasks

## Why This Works (Mechanism)

### Mechanism 1: Conservative Data Collection Creates Distribution Shift Challenge
- Claim: Deterministic sub-optimal policies create narrow data distributions that prevent offline RL algorithms from learning better policies.
- Mechanism: When data comes from conservative behavior policies (20-75% performance ranking) or traditional PID controllers, the state-action coverage is limited. Offline RL algorithms then struggle with out-of-distribution (OOD) actions when attempting policy improvement beyond the behavior policy.
- Core assumption: Sub-optimal behavior policies create narrower state-action distributions than optimal or exploratory policies would.
- Evidence anchors:
  - [abstract]: "data collected from traditional control methods (e.g., PID controllers), and limited data availability"
  - [section 4.2]: "In most real-world tasks, there is typically only one policy for collecting offline datasets, leading to a lack of diversity. So, in NeoRL-2 it is important to clarify that all policies are deterministic when interacting with the environment to collect data."
  - [corpus]: Weak direct evidence; neighboring papers focus on algorithmic solutions rather than data generation mechanisms.
- Break condition: If algorithms successfully extrapolate to OOD actions through conservative Q-learning, uncertainty estimation, or model-based rollouts that adequately penalize uncertainty.

### Mechanism 2: Environmental Complexity Factors Break MDP Assumptions
- Claim: Time delays, external factors, and global safety constraints cause tasks to deviate from standard MDP/POMDP assumptions, degrading algorithm performance.
- Mechanism: (1) Delays make action effects persist over extended horizons, inducing non-Markovian dynamics. (2) External factors introduce uncontrollable variance not influenced by policy decisions. (3) Safety constraints restrict data collection to only safe states/actions, creating incomplete state-space coverage.
- Core assumption: These factors cumulatively create information incompleteness that violates the design assumptions of standard offline RL algorithms.
- Evidence anchors:
  - [abstract]: "current methods often struggle to outperform the data-collection behavior policy, particularly in tasks with delays, external factors, and safety constraints"
  - [section 3.2]: "All of these factors directly lead to the incompleteness of the observed information, gradually deviating from the standard MDP/POMDP assumptions. As many current offline reinforcement learning algorithms overlook these issues, these challenges could pose significant difficulties."
  - [corpus]: No direct corpus evidence on delay/external factor mechanisms.
- Break condition: If algorithms incorporate RNN architectures for delays, causal analysis for external factors, or constrained MDP/safe RL methods for safety constraints.

### Mechanism 3: Model-Based Methods Suffer from Transition Model Inaccuracy
- Claim: Model-based offline RL algorithms exhibit higher instability because learned transition models fail to capture true dynamics when trained on PID-generated or narrow data distributions.
- Mechanism: PID controllers use feedback control that may create spurious correlations. When model-based algorithms learn dynamics via behavior cloning on this data, they fail to capture correct causal relationships between states and actions.
- Core assumption: Assumption: The dynamics model learned from offline data accurately reflects true environment transitions for planning.
- Evidence anchors:
  - [section 5.2]: "Model-based algorithms demonstrate lower stability compared to model-free algorithms, particularly evident in the RocketRecovery, Fusion, and SafetyHalfCheetah tasks, where MOPO and RAMBO exhibit larger error bars"
  - [Appendix E]: "the transition models in the model-based algorithms do not provide accurate transitions through the BC method. This might be due to the offline dataset collected through the traditional control method (e.g. PID), which tends to make the transition model learned by BC hard to identify the correct causal relationship"
  - [corpus]: Weak evidence; corpus papers don't directly address PID-specific transition model failures.
- Break condition: If transition models can be learned with sufficient accuracy despite narrow data distributions, or if uncertainty penalties adequately prevent model exploitation.

## Foundational Learning

- **Concept: Offline RL vs Online RL Paradigm**
  - Why needed here: The benchmark exclusively evaluates offline methods that cannot interact with the environment during training, fundamentally changing the algorithm design requirements.
  - Quick check question: Why does offline RL require special handling of distribution shift that online RL does not?

- **Concept: Distributional Shift and OOD Actions**
  - Why needed here: Core challenge in NeoRL-2—algorithms attempt to improve beyond behavior policy but training data doesn't cover better actions, leading to Q-function overestimation.
  - Quick check question: What happens when a learned Q-function overestimates values for actions not present in the offline dataset?

- **Concept: Conservative Q-Learning vs Uncertainty-Based Regularization**
  - Why needed here: These represent the two main algorithmic families benchmarked (CQL/MCQ/TD3BC vs EDAC/MOPO/COMBO/Mobile) with fundamentally different approaches to handling distribution shift.
  - Quick check question: How does CQL's explicit penalty on OOD Q-values differ from EDAC's ensemble-based uncertainty estimation?

## Architecture Onboarding

- **Component map:**
  - SAC online training → select sub-optimal policy (20-75% rank) OR PID controller (DMSD only) → deterministic data collection → train/validation split (typically 100K/20K samples)
  - 7 Task Simulators: Pipeline (delay), Simglucose (delay + external), RocketRecovery (external + limited), RandomFrictionHopper (external), DMSD (PID data), Fusion (extremely limited), SafetyHalfCheetah (constraints + limited)
  - Algorithm Families: Model-free (BC, CQL, EDAC, MCQ, TD3BC) + Model-based (MOPO, COMBO, RAMBO, Mobile)
  - Evaluation: Online policy evaluation only (no stable OPE methods per paper)

- **Critical path:**
  1. Map task characteristics (Table 1) to identify which challenges each task presents
  2. Implement baseline with hyperparameter search spaces (Tables 6-8)
  3. Run 3 random seeds per configuration using same seeds across all algorithms
  4. Select best hyperparameters based on mean performance
  5. Report final training epoch policy via online evaluation

- **Design tradeoffs:**
  - Model-free stability vs model-based data efficiency: Model-free shows lower variance; model-based higher potential but unstable (RAMBO error bars up to ±307)
  - BC regularization weight (TD3BC α): Higher = more conservative but limits improvement (α=0.05-0.2)
  - Conservative penalty strength (CQL α, MCQ λ): Balance between preventing overestimation and enabling learning
  - Rollout length h for model-based: Longer rollouts increase compounding model errors

- **Failure signatures:**
  - Large standard errors (RAMBO ±307, MOPO ±105): Model exploitation or adversarial instability
  - Negative normalized scores: Severe model misspecification or distribution shift
  - All algorithms below data scores (SafetyHalfCheetah): Constraint + limited data combination too challenging
  - Model-based underperforming model-free: Transition model fails to capture dynamics

- **First 3 experiments:**
  1. **Establish behavior policy baseline:** Run deterministic behavior policy to confirm data score targets (28-76 normalized scores across tasks per Table 2).
  2. **TD3BC sanity check across all tasks:** Use TD3BC (highest success rate >80%) with recommended α=0.2, policy_noise=0.5-2.5 to verify implementation correctness. Expect improvements on DMSD (+22) and RandomFrictionHopper.
  3. **Debug one challenging task:** Focus on SafetyHalfCheetah (all algorithms fail) or Simglucose (model-free CQL/EDAC fail catastrophically). Compare CQL vs MOPO to diagnose whether issue is distribution shift or transition model accuracy. Check if constraint violation or delay handling is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectures that account for partial observability, such as RNNs, effectively mitigate the performance degradation caused by high-latency transitions in offline RL?
- Basis in paper: [explicit] Section A.1 states that "addressing time delays could involve using RNN architecture or designing algorithms that account for latency."
- Why unresolved: Current SOTA methods failed to significantly improve over behavior policies in delayed tasks (Pipeline, Simglucose), likely because delays induce non-Markovian properties that standard feed-forward networks cannot handle.
- What evidence would resolve it: Demonstrating that an RNN-enhanced offline RL agent can achieve normalized scores significantly higher than the behavior policy on the Pipeline task.

### Open Question 2
- Question: Can causal analysis techniques be integrated into offline RL to successfully disentangle policy effects from uncontrollable external factors?
- Basis in paper: [explicit] Section A.1 suggests "Tasks influenced by external factors can be analyzed using causal analysis techniques."
- Why unresolved: External factors (e.g., wind in RocketRecovery) increase variance and complexity. Current algorithms struggle to outperform behavior policies in these environments, indicating an inability to model or ignore these exogenous variables.
- What evidence would resolve it: An algorithm that explicitly models causal graphs of external variables achieving robust performance improvements on the RocketRecovery task compared to model-free baselines.

### Open Question 3
- Question: How can offline RL agents learn to satisfy global safety constraints when the training datasets contain exclusively "safe" behaviors and lack violation examples?
- Basis in paper: [explicit] Section A.1 notes that "solutions like safe RL may not be directly applicable, as real-world data might lack unsafe scenarios," posing a challenge for the SafetyHalfCheetah task.
- Why unresolved: Standard Safe RL often learns from failure, but NeoRL-2 datasets are collected via conservative policies that never violate constraints. Algorithms must infer implicit boundaries without seeing the consequences of crossing them.
- What evidence would resolve it: A method that infers safety boundaries from the distribution of expert data and maintains those constraints during policy optimization, outperforming the behavior policy on SafetyHalfCheetah.

## Limitations
- Limited algorithm diversity: Only 8 algorithms evaluated compared to the broader landscape of offline RL methods
- Small sample size: Only 3 random seeds used per experiment, limiting statistical confidence
- Online evaluation dependency: Requires environment interaction for final policy evaluation, which may not be available in truly offline settings

## Confidence

- **High confidence:** The dataset generation methodology and task implementation are clearly specified. The normalized scoring framework using expert and random baselines is well-defined.
- **Medium confidence:** The experimental results show clear patterns (model-free algorithms generally outperform model-based, TD3BC shows the most stable performance), but the limited number of algorithm comparisons and seeds (3) restricts generalizability.
- **Low confidence:** The claim that NeoRL-2 "can effectively help offline RL researchers develop novel algorithms" is future-oriented and not empirically validated in this paper.

## Next Checks

1. **Algorithm Diversity Test:** Evaluate additional offline RL algorithms (e.g., IQL, AWAC, BRAC) to determine if observed performance patterns hold across a broader algorithmic space.

2. **Sample Efficiency Analysis:** Conduct experiments with varying dataset sizes (e.g., 10K, 50K, 100K samples) to quantify the impact of limited data availability on algorithm performance.

3. **OPE Method Validation:** Implement and compare multiple offline policy evaluation methods to verify that online evaluation results accurately reflect true policy performance without requiring environment interaction.