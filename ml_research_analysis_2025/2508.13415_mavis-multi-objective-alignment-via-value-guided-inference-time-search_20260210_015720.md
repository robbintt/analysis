---
ver: rpa2
title: 'MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search'
arxiv_id: '2508.13415'
source_url: https://arxiv.org/abs/2508.13415
tags:
- value
- training
- policy
- each
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MAVIS, a novel inference-time alignment\
  \ framework that dynamically balances multiple, often conflicting, objectives (e.g.,\
  \ helpfulness, harmlessness, humor) in large language models without modifying model\
  \ weights. MAVIS trains small, per-objective value models that, at inference time,\
  \ are combined using user-specified weights to tilt the base model\u2019s output\
  \ distribution toward desired trade-offs."
---

# MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search

## Quick Facts
- **arXiv ID**: 2508.13415
- **Source URL**: https://arxiv.org/abs/2508.13415
- **Reference count**: 16
- **Key outcome**: MAVIS dynamically balances multiple, often conflicting, objectives in large language models without modifying model weights, expanding the achievable Pareto frontier and approaching the performance of models fine-tuned for exact user preferences.

## Executive Summary
MAVIS is a novel inference-time alignment framework that enables dynamic balancing of multiple, potentially conflicting objectives (e.g., helpfulness, harmlessness, humor) in large language models. Unlike traditional fine-tuning, MAVIS trains small, per-objective value models that, at inference time, are combined using user-specified weights to tilt the base model's output distribution toward desired trade-offs. The method uses an iterative training algorithm to learn token-level Q-values and supports integration with test-time search strategies like beam search. Empirically, MAVIS expands the achievable Pareto frontier beyond combining fine-tuned per-objective models and approaches the performance of models fine-tuned for exact user preferences, while offering computational advantages suitable for edge-device deployment.

## Method Summary
MAVIS reframes multi-objective alignment as a token-level inference-time problem. It trains small value models to estimate Q-values for each objective, then combines these at inference using user-specified weights to create a tilting function that adjusts the base model's output distribution. The framework uses iterative soft policy improvement with KL-regularization to train value models on trajectories from previous guided policies, ensuring monotonic improvement toward the optimal KL-regularized policy. This approach avoids the coherence problems of policy mixtures and enables dynamic trade-off adjustment without modifying model weights.

## Key Results
- MAVIS consistently matches or exceeds MORLHF baselines in terms of Pareto optimality across multiple objective pairs
- MAVIS outperforms RSoup and MOD baselines on the evaluated dialogue task, expanding the achievable Pareto frontier
- The method achieves competitive performance to exact preference fine-tuned models while requiring only one base model and small value heads

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Value-Guided Distribution Tilting
MAVIS achieves multi-objective alignment by learning per-objective Q-functions that tilt the base model's output distribution at each decoding step without modifying weights. The decoding policy re-weights tokens using the exponential of combined Q-values: π_MAVIS(a_t|s_t) ∝ π_ref(a_t|s_t) exp(β · Σ_m λ_m Q*_m(s_t, a_t)). This creates a "tilting function" that shifts probability toward tokens with higher expected future reward under the weighted objective.

### Mechanism 2: Iterative Soft Policy Improvement with KL-Regularization
MAVIS iteratively trains value models on trajectories from previous guided policies, yielding monotonic improvement toward the optimal KL-regularized policy. Starting with π_0 = π_ref, each iteration collects rollouts using current policy π_k, computes regularized targets via Monte Carlo averaging over tree-structured completions, and trains V_k via regression. Theorem 1 proves Q^{π_{k+1}} ≥ Q^{π_k}.

### Mechanism 3: Linear Q-Value Combination Avoids Policy Mixture Artifacts
Instead of interpolating between models (MOD) or weights (RSoup), MAVIS interpolates value estimates. This resolves trade-offs "during generation" rather than producing incoherent mixtures of mutually exclusive high-probability actions when objectives conflict.

## Foundational Learning

- **KL-Regularized Policy Optimization**: MAVIS explicitly trades off reward maximization against staying close to π_ref. Understanding the soft policy iteration derivation (soft actor-critic family) clarifies why exp(Q/η) is the right tilting form.
  - Quick check: Can you derive why π*(a|s) ∝ π_ref(a|s) exp(Q*(s,a)/η) is optimal for max E[R] - η·D_KL(π||π_ref)?

- **Monte Carlo Value Estimation with Sparse Rewards**: LLM rewards are terminal-only. MAVIS uses tree rollouts to estimate intermediate values. Understanding TD(λ), Monte Carlo returns, and the "deadly triad" explains why they avoid bootstrapping.
  - Quick check: Why might bootstrapping destabilize training when function approximation, off-policy data, and bootstrapping combine?

- **Pareto Frontier and Multi-Objective Optimization**: The paper evaluates methods by whether they expand the achievable Pareto frontier. Understanding convex combinations of rewards vs. policies is essential.
  - Quick check: If two models are fine-tuned for objectives A and B separately, why does averaging their weights not guarantee Pareto-optimal outputs for intermediate A-B preferences?

## Architecture Onboarding

- **Component map**: Base LLM (π_ref) -> Value Models {V_m} -> Tree Data Generator -> MAVIS Decoder
- **Critical path**: 1) SFT base model on task format (one epoch), 2) Generate tree rollouts from π_ref, label leaves with rewards, 3) Train V_0 via regression on averaged leaf rewards (Monte Carlo targets), 4) Iterate: decode with MAVIS(π_ref, V_{k-1}), collect new trees, train V_k with KL penalty in targets, 5) At inference: load π_ref + {V_m}, specify weights λ, decode with top-k tilting
- **Design tradeoffs**: Tree depth L vs. training coverage (deeper trees = better estimates but exponentially more compute); Top-k size (small k reduces overhead but may discard valuable low-probability tokens; k=15 worked well); β vs. η (jointly control KL)
- **Failure signatures**: High KL but low reward (value model overestimates Q for out-of-distribution tokens; reduce β or increase η); Incoherent outputs with conflicting objectives (λ weights may be poorly calibrated; try normalizing per-objective value scales); No improvement across iterations (check that tree rollouts use current policy π_k, not stale π_ref)
- **First 3 experiments**: 1) Single-objective sanity check: Train V_helpfulness, verify MAVIS decoding matches or exceeds PPO reward at similar KL on held-out prompts, 2) Two-objective Pareto sweep: Vary λ ∈ {0.0, 0.2, ..., 1.0}, plot reward pairs; compare to RSoup and MOD baselines to confirm frontier expansion, 3) Ablation on top-k: Run MAVIS with k ∈ {5, 10, 15, 25} and measure reward-KL tradeoff and latency; confirm k=15 is in the sweet spot for your hardware

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the value models be distilled directly into the generative model to reduce inference latency? The conclusion identifies "distill[ing] the value models into smaller modules that use the generative model itself as a backbone" as a promising direction to reduce overhead.
- **Open Question 2**: Why does the required number of value model training iterations vary significantly between task types (e.g., summarization vs. assistant chat)? The authors "conjecture that additional value model training is not required [for summarization] because more benefit is derived from predicting future results... compared to the task of acting as a helpful... assistant."
- **Open Question 3**: Does the Monte Carlo (MC) rollout approach for value estimation remain efficient and stable for tasks with long generation horizons? The paper uses MC rollouts to avoid the "deadly triad" of bootstrapping, but experiments are restricted to short sequences ($T=128$ and $T=48$).

## Limitations
- Empirical scope is narrow: all results rely on LLaMA-2 7B; scaling to 70B+ models or highly sparse reward tasks (e.g., coding) is unproven
- Tree rollouts are limited to depth 2; longer-horizon reasoning tasks may require deeper trees, incurring exponential cost
- The method assumes the base model has non-zero probability on high-value tokens—if π_ref's top-k never includes them, MAVIS cannot recover

## Confidence
- **High confidence**: Token-level Q-value tilting produces coherent outputs; MAVIS consistently matches or exceeds baselines on the tested two-objective dialogue task; the monotonic improvement theorem holds under stated assumptions
- **Medium confidence**: Pareto frontier expansion is statistically robust in the given domain but may not transfer to other tasks or model families; computational overhead is practical for the tested setup but may scale poorly for deeper trees or larger k
- **Low confidence**: Claims about edge-device deployment feasibility are based on model size, not actual hardware benchmarking; cross-task generalization (e.g., to summarization or reasoning) is untested; iterative training may stall on tasks with very sparse or delayed rewards

## Next Checks
1. **Scale and Domain Transfer Test**: Apply MAVIS to a 70B-parameter model on a task with delayed rewards (e.g., step-by-step math problem solving), measuring Pareto frontier expansion, KL stability, and wall-clock latency
2. **Value Model Robustness Test**: Introduce noise or bias into reward models and evaluate whether MAVIS can still produce calibrated trade-offs; compare against baseline mixtures under the same corrupted rewards
3. **Hardware Feasibility Test**: Deploy MAVIS inference on an actual edge device (e.g., Raspberry Pi 4 with 8 GB RAM) using quantized value models; benchmark token latency and memory usage versus a baseline mixture-of-experts fine-tune