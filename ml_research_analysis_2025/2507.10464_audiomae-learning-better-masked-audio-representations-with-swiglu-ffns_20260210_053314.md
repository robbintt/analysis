---
ver: rpa2
title: 'AudioMAE++: learning better masked audio representations with SwiGLU FFNs'
arxiv_id: '2507.10464'
source_url: https://arxiv.org/abs/2507.10464
tags:
- audio
- masked
- audiomae
- learning
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AudioMAE++, a self-supervised masked autoencoder
  for learning general-purpose audio representations. The model enhances the standard
  transformer architecture by incorporating macaron-style feedforward networks and
  gated linear units (SwiGLU), which have shown promise in speech recognition.
---

# AudioMAE++: learning better masked audio representations with SwiGLU FFNs

## Quick Facts
- arXiv ID: 2507.10464
- Source URL: https://arxiv.org/abs/2507.10464
- Reference count: 0
- Outperforms MAE baselines with up to 4× more parameters while achieving state-of-the-art results

## Executive Summary
AudioMAE++ is a self-supervised masked autoencoder that improves audio representation learning by incorporating macaron-style feedforward networks with SwiGLU activation units. Pretrained on AudioSet, the model achieves state-of-the-art results across 10 diverse audio classification tasks, with an overall score of 91.8±0.2 for the Base model and 93.7±0.2 for the Large model. Notably, the Base model outperforms MAE baselines with up to 4× more parameters, demonstrating superior scaling efficiency.

## Method Summary
AudioMAE++ builds upon the standard MAE architecture by replacing standard feedforward networks with macaron-style SwiGLU blocks. The model uses 80% masking during pretraining and reconstructs the full spectrogram from the encoded visible patches. Key architectural choices include removing rotary positional embeddings and carefully scaling decoder capacity when increasing encoder size. The model is pretrained on AudioSet and evaluated on a diverse set of 10 downstream audio tasks spanning music, speech, and environmental sound classification.

## Key Results
- AudioMAE++ Base (141.9M params) achieves 91.8 overall score, outperforming MAE-Base (85.1M params) at 88.1 and MAE-Huge (629.8M params)
- AudioMAE++ Large (640.1M params) achieves 93.7 overall score, setting new state-of-the-art results among SSL models
- Decoder capacity acts as a bottleneck for scaling, with optimal performance achieved when decoder width is increased proportionally to encoder size
- Removing rotary positional embeddings improves performance by 0.6 points on utterance-level tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard feedforward networks (FFN) with Macaron-style SwiGLU blocks improves parameter efficiency and representation quality in Masked Autoencoders (MAE).
- **Mechanism:** The SwiGLU unit implements a gating mechanism (an element-wise product of two linear projections, one gated by Swish) that controls information flow, likely allowing for smoother gradient propagation and better modeling of non-linear audio features compared to standard ReLU/GELU. The "Macaron" style sandwiches attention between two FFNs, potentially balancing the signal-to-noise ratio in the residual stream.
- **Core assumption:** The improved performance stems from the increased expressivity and gating capabilities of SwiGLU specifically for spectrogram data, rather than merely the increase in parameter count.
- **Evidence anchors:**
  - [abstract] Mentions AudioMAE++ Base outperforms MAE baselines with up to 4× more parameters.
  - [section 4.1] Table 2 shows AudioMAE++-Base (141.9M params) scoring 91.8 vs MAE-Base (85.1M params) at 88.1, and even beating MAE-Huge (629.8M params).
  - [corpus] General corpus support for advanced FFN variants is noted in "Flash Multi-Head Feed-Forward Network," though direct evidence for audio-specific SwiGLU superiority is limited to this paper's results.
- **Break condition:** Performance gains disappear if the model width/parameter count is normalized strictly against the baseline, suggesting the gain is purely from capacity scaling rather than architectural efficiency.

### Mechanism 2
- **Claim:** Decoder capacity acts as a bottleneck for scaling encoder representations.
- **Mechanism:** In an MAE, the encoder learns by backpropagating the reconstruction error from the decoder. If the decoder lacks the capacity (width $d_{dec}$) to model the complex mapping from encoded latent space back to the high-dimensional spectrogram, it generates a weak learning signal, capping the encoder's potential regardless of its size.
- **Core assumption:** The MSE reconstruction loss on masked patches is a sufficiently correlated proxy for downstream task utility.
- **Evidence anchors:**
  - [section 4.2] Table 4 shows increasing decoder dimension from 384 to 512 boosts overall score from 91.7 to 93.7 for a Large encoder, with diminishing returns after.
- **Break condition:** Increasing decoder width yields no performance gain, indicating the bottleneck has shifted to the encoder or the data itself.

### Mechanism 3
- **Claim:** Rotary Positional Embeddings (RoPE) degrade performance for short-context audio classification tasks compared to fixed sinusoidal embeddings.
- **Mechanism:** RoPE encodes relative positional information, which is beneficial for long-sequence modeling. However, for short, fixed-length 2-second crops used in this pretraining, RoPE may introduce unnecessary complexity or fail to preserve the absolute locality required for distinct audio event classification.
- **Core assumption:** The negative impact is due to the task type (utterance-level classification) and input duration, rather than a fundamental incompatibility with audio spectrograms.
- **Evidence anchors:**
  - [section 4.2] Table 3 explicitly shows removing RoPE from both encoder and decoder improves performance (91.4 → 92.0).
- **Break condition:** RoPE improves performance when evaluated on tasks requiring long-range temporal dependencies (e.g., long-form ASR), indicating the failure is context-length specific.

## Foundational Learning

- **Concept: Masked Autoencoding (MAE)**
  - **Why needed here:** This is the core pretraining paradigm. Understanding that the model learns by predicting masked patches of a spectrogram is essential to grasping why architectural changes (like SwiGLU) affect how the model "fills in the blanks."
  - **Quick check question:** How does the 80% masking ratio used here compare to typical vision MAE ratios, and what does this imply about audio redundancy?

- **Concept: Gated Linear Units (GLU) & SwiGLU**
  - **Why needed here:** The "++" in AudioMAE++ refers specifically to this modification. You must understand that this is a replacement for the standard dense layer, acting as a learnable on/off switch for hidden units.
  - **Quick check question:** In a SwiGLU layer, what is the specific function of the Swish activation compared to a standard Sigmoid in a traditional GLU?

- **Concept: Spectrogram Patching**
  - **Why needed here:** The input to the transformer is not raw audio but 2D patches of a mel-spectrogram. The interaction between time and frequency axes in these patches defines the token structure.
  - **Quick check question:** What are the spatial dimensions ($t \times f$) of the patches used, and how does the 2-second crop limit the total number of tokens?

## Architecture Onboarding

- **Component map:** 16kHz Audio → Log-Mel Spectrogram (80 bins) → 2-sec Crop [200x80] → Patching [4x16] → Linear Proj + Fixed 2D Pos Embedding + CLS Token → Stack of Transformer++ Blocks (Macaron style: Half-FFN → Attn → SwiGLU-FFN) → Shallow Transformer++ blocks (width $d_{dec}$) + Mask Tokens → Linear projection to pixel space (MSE Loss)

- **Critical path:**
  1. **Preprocessing:** Extracting 2-second spectrogram crops.
  2. **Encoder Forward:** Processing only the *visible* (20%) patches through the SwiGLU-enabled blocks.
  3. **Decoder Alignment:** Appending mask tokens to encoder outputs and reconstructing the full spectrogram.

- **Design tradeoffs:**
  - **SwiGLU vs. Standard FFN:** SwiGLU increases parameters by ~33-50% for the same hidden dimension but provides better scaling efficiency (evidenced by Base beating Huge models).
  - **Decoder Width:** Scaling the encoder requires a corresponding increase in decoder width ($d_{enc} \uparrow \implies d_{dec} \uparrow$) to avoid bottlenecks.

- **Failure signatures:**
  - **RoPE on Short Clips:** Using Rotary Embeddings causes a consistent drop in performance (approx. 0.5 points) on utterance-level tasks.
  - **Undersized Decoder:** A Large Encoder with a Base-sized decoder ($d_{dec}=384$) fails to reach state-of-the-art performance (stuck at ~91.7 vs 93.7).

- **First 3 experiments:**
  1. **Ablation on RoPE:** Train two Base models, one with RoPE and one with fixed embeddings, on the 2-second crop setup to verify the reported performance drop.
  2. **Decoder Bottleneck Test:** Using the Large encoder, sweep decoder dimensions ($d_{dec} \in \{384, 512, 768\}$) to confirm the "elbow" in performance scaling.
  3. **SwiGLU vs FFN Efficiency:** Train two Tiny models—one with SwiGLU, one without—to compare convergence speed and final s(m) score with parameter counts roughly equalized (adjusting width).

## Open Questions the Paper Calls Out
None

## Limitations
- Core architectural claims (SwiGLU, decoder scaling) rest on single-paper evidence without external replication
- Downstream evaluations rely on tasks with potentially overlapping metrics, which could inflate perceived generalization
- Pretraining dataset (AudioSet) is limited in acoustic diversity compared to specialized datasets

## Confidence
- **High confidence**: Decoder capacity as a bottleneck (strong ablation evidence, consistent scaling pattern)
- **Medium confidence**: SwiGLU architectural improvement (internal ablations strong, but no external validation or encoder-only comparison)
- **Low confidence**: RoPE exclusion (empirical but not theoretically grounded; may be dataset/task specific)

## Next Checks
1. **Encoder-Only Transfer**: Train a SwiGLU-enhanced encoder without a decoder, using a contrastive loss (e.g., SimSiam), to isolate the architectural benefit from the reconstruction objective
2. **Cross-Domain Generalization**: Evaluate AudioMAE++ on non-AudioSet pretraining data (e.g., ESC-50 or Freesound) to test robustness of the gains across acoustic domains
3. **Long-Context Behavior**: Extend the pretraining input duration to 10 seconds and re-evaluate RoPE's impact to determine if the exclusion is truly context-length dependent