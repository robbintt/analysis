---
ver: rpa2
title: 'Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication
  Made Simple'
arxiv_id: '2601.16294'
source_url: https://arxiv.org/abs/2601.16294
tags:
- gemm
- performance
- sfc-ca
- memory
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of optimizing General Matrix
  Multiplication (GEMM) on modern CPU platforms with matrix multiplication accelerators,
  where performance is highly sensitive to platform-specific parameters and matrix
  shapes, leading to suboptimal performance "glass jaws" in vendor libraries. The
  authors propose a novel approach using Space Filling Curves (SFC) combined with
  Communication-Avoiding (CA) algorithms to partition the GEMM computation space in
  a locality-aware manner.
---

# Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple

## Quick Facts
- arXiv ID: 2601.16294
- Source URL: https://arxiv.org/abs/2601.16294
- Authors: Evangelos Georganas; Alexander Heinecke; Pradeep Dubey
- Reference count: 38
- Performance: Up to 2x geometric mean speedup over vendor libraries for GEMM on x86 and Arm CPUs

## Executive Summary
This paper addresses the challenge of optimizing General Matrix Multiplication (GEMM) on modern CPU platforms with matrix multiplication accelerators. Current vendor libraries often exhibit performance "glass jaws" - suboptimal performance on certain matrix shapes and platforms due to their sensitivity to platform-specific parameters. The authors propose a novel approach using Space Filling Curves (SFC) combined with Communication-Avoiding (CA) algorithms to partition the GEMM computation space in a locality-aware manner. This approach minimizes data movement and achieves platform-oblivious, shape-oblivious matrix multiplication performance.

The implementation, based on Tensor Processing Primitives, is remarkably compact at approximately 30 lines of code while outperforming vendor libraries by up to 2x on average across various GEMM shapes and CPU platforms. The method leverages SFC properties to implicitly map work across threads and extend to CA algorithms, effectively addressing the data movement bottleneck that plagues traditional approaches.

## Method Summary
The authors propose a communication-avoiding approach to GEMM using Space Filling Curves (SFC) to partition the computation space. The method combines the locality-preserving properties of SFC with the data movement minimization benefits of Communication-Avoiding (CA) algorithms. The computation is organized into tiles, with the SFC providing a one-dimensional ordering that preserves spatial locality when mapped to higher dimensions. This ordering implicitly maps work across threads while maintaining data locality. The approach extends naturally to CA algorithms by recursively applying the SFC partitioning strategy. The implementation uses Tensor Processing Primitives as the underlying framework, resulting in a compact codebase of approximately 30 lines that achieves significant performance improvements over vendor libraries across various matrix shapes and CPU platforms.

## Key Results
- Achieves up to 2x geometric mean speedup over vendor libraries for GEMM operations
- Demonstrates platform-oblivious performance across x86 and Arm CPU architectures
- Maintains shape-oblivious performance across diverse matrix dimensions and aspect ratios
- Compact implementation of ~30 lines using Tensor Processing Primitives

## Why This Works (Mechanism)
The approach works by leveraging the locality-preserving properties of Space Filling Curves to organize the computation space. SFC provides a one-dimensional ordering that preserves spatial locality when mapped to higher dimensions, which is crucial for maintaining data locality during parallel execution. This ordering implicitly maps work across threads while minimizing data movement between memory hierarchy levels. When combined with Communication-Avoiding algorithms, the SFC partitioning strategy recursively reduces data movement by increasing computational intensity at each level of recursion. The method effectively addresses the "glass jaw" problem where vendor libraries perform poorly on specific matrix shapes by providing a uniform partitioning strategy that works well across all shapes.

## Foundational Learning
**Space Filling Curves**: Continuous fractal curves that pass through every point in a multi-dimensional space - needed to preserve locality when mapping multi-dimensional work to one dimension for thread scheduling; quick check: verify the SFC used provides good locality preservation for the target dimensionality.

**Communication-Avoiding Algorithms**: Algorithms designed to minimize data movement between memory hierarchy levels by increasing computational intensity - needed to reduce the bottleneck of memory bandwidth; quick check: measure arithmetic intensity before and after applying CA techniques.

**Tensor Processing Primitives**: A framework for expressing tensor computations that can target different hardware backends - needed to provide a portable implementation that can leverage platform-specific optimizations; quick check: confirm the implementation generates appropriate code for each target platform.

**Locality-Aware Partitioning**: Division of computation space that preserves data locality to minimize cache misses - needed to maintain high performance across diverse matrix shapes; quick check: profile cache miss rates across different partitioning strategies.

**Recursive Blocking**: Technique of recursively dividing matrices into smaller blocks to improve cache utilization - needed to achieve optimal data reuse in multi-level memory hierarchies; quick check: verify that the blocking strategy matches the cache sizes of the target architecture.

## Architecture Onboarding

**Component Map**: GEMM workload -> SFC partitioning -> CA algorithm recursion -> Tensor Processing Primitives -> CPU execution

**Critical Path**: The critical path involves the SFC-based partitioning of the computation space, followed by recursive application of CA algorithms, and finally execution through the Tensor Processing Primitives framework. The performance bottleneck is primarily determined by the efficiency of data movement minimization rather than computational throughput.

**Design Tradeoffs**: The main tradeoff is between the simplicity and generality of the SFC approach versus the potential for platform-specific optimizations that vendor libraries might employ. While the SFC method provides uniform performance across shapes and platforms, it may not extract the absolute maximum performance possible with highly tuned, platform-specific code paths. The choice of Tensor Processing Primitives provides portability but may limit access to certain low-level hardware features.

**Failure Signatures**: Performance degradation would manifest as increased cache misses and memory bandwidth saturation, particularly for edge cases in matrix shapes or sizes that don't align well with the SFC partitioning. The method may also show reduced effectiveness when the computational intensity cannot overcome memory latency, such as for very small matrices or when the memory hierarchy is deeply nested.

**First Experiments**:
1. Benchmark the SFC-based implementation against vendor libraries across a matrix of different shapes and sizes
2. Profile cache behavior and memory bandwidth utilization to verify data movement minimization
3. Test the implementation on multiple CPU architectures (x86, Arm) to validate platform-obliviousness

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on Tensor Processing Primitives may limit generalizability to other programming models
- Performance variability across different problem sizes and cache hierarchies not extensively analyzed
- Claims of universal applicability may not hold for extreme matrix dimensions or edge cases
- Comparison focuses on vendor libraries without exploring state-of-the-art research implementations

## Confidence
- **High**: Core SFC-CA methodology and its ability to outperform vendor libraries on tested platforms
- **Medium**: Generalizability claims beyond specific CPU architectures and GEMM shapes evaluated
- **Low**: Assertion of universal applicability without extensive validation across broader hardware configurations

## Next Checks
1. Evaluate performance on GPU and specialized matrix multiplication accelerators to assess cross-platform generalizability
2. Test extreme matrix shapes (very tall, wide, or non-square matrices) to identify potential performance cliffs
3. Compare against recent research implementations of CA algorithms to establish relative positioning in the state-of-the-art