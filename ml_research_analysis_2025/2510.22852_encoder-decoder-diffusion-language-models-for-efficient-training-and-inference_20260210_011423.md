---
ver: rpa2
title: Encoder-Decoder Diffusion Language Models for Efficient Training and Inference
arxiv_id: '2510.22852'
source_url: https://arxiv.org/abs/2510.22852
tags:
- diffusion
- decoder
- encoder
- e2d2
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces E2D2, an encoder-decoder architecture for
  discrete diffusion language models that achieves faster inference and training than
  decoder-only approaches. The key idea is to separate clean token representation
  (handled by an encoder) from iterative denoising (handled by a lightweight decoder),
  reducing computational cost by amortizing encoder calls.
---

# Encoder-Decoder Diffusion Language Models for Efficient Training and Inference

## Quick Facts
- **arXiv ID**: 2510.22852
- **Source URL**: https://arxiv.org/abs/2510.22852
- **Reference count**: 40
- **Primary result**: E2D2 achieves 3x faster inference and 2x fewer training FLOPs than decoder-only diffusion baselines, with better quality-throughput trade-offs.

## Executive Summary
E2D2 introduces an encoder-decoder architecture for discrete diffusion language models that amortizes encoder computation and enables faster training and inference. By separating clean token representation (encoder) from iterative denoising (decoder), E2D2 reduces computational cost while maintaining or improving generation quality. The approach supports both block and full-sequence diffusion parameterizations and demonstrates superior throughput and efficiency on summarization, translation, and mathematical reasoning tasks.

## Method Summary
E2D2 rethinks diffusion language models by using an encoder-decoder architecture instead of decoder-only. The encoder processes the clean input once, and a lightweight decoder handles iterative denoising. This amortizes encoder computation, reducing inference and training costs. The architecture supports both block diffusion (chunked denoising) and full-sequence diffusion, with attention masks tailored to preserve conditioning. Key optimizations include KV caching for fast inference and fused attention kernels for throughput. The design achieves better quality-throughput trade-offs by trading increased decoder steps for reduced overall computation.

## Key Results
- E2D2 achieves 3x faster inference throughput than masked diffusion models with KV caching support.
- Training efficiency improves by 2x fewer FLOPs compared to block diffusion models of equal size.
- On GSM8K, CNN/DM, and WMT14 de-en, E2D2 matches or outperforms decoder-only diffusion baselines in quality while being more efficient.

## Why This Works (Mechanism)
E2D2 works by amortizing the expensive encoder computation across multiple denoising steps. In decoder-only models, the entire model must be run at every step, but E2D2 only recomputes the decoder portion. This is especially effective for block diffusion, where the same clean representation is used for all steps within a block. The lightweight decoder reduces per-step cost, and KV caching further accelerates inference by reusing encoder activations. The design trades increased decoder depth for reduced total computation, achieving better quality-throughput trade-offs.

## Foundational Learning
- **Discrete diffusion for text**: Gradual noising and iterative denoising of token sequences; needed for understanding the core generation process and how block/full-sequence variants differ.
- **Block diffusion vs full-sequence diffusion**: Block diffusion denoises chunks of tokens iteratively, while full-sequence diffusion denoises all tokens at once; needed to grasp the architectural differences and efficiency implications.
- **Attention mask patterns**: Encoder uses block-causal mask; decoder uses block-diagonal (self) + offset block-causal (cross to encoder); needed to ensure correct conditioning and avoid information leakage.
- **KV caching**: Storing encoder key/value activations to reuse across denoising steps; needed to understand the speedup mechanism and how it differs from decoder-only models.
- **Layer mapping for decoder initialization**: Decoder layers initialized from "closest to output" layers of base model; needed to replicate the architecture and ensure fine-tuning stability.
- **Fused attention kernels**: Custom GPU kernels to reduce overhead; needed to understand why reported throughput may be difficult to reproduce with standard implementations.

## Architecture Onboarding
- **Component map**: Input -> Encoder (once) -> Decoder (iterative denoising) -> Output
- **Critical path**: Clean input processed by encoder, decoder iteratively denoises using cached encoder KV, attention masks ensure correct conditioning.
- **Design tradeoffs**: Amortizing encoder computation vs. increased decoder steps; lightweight decoder for efficiency vs. maintaining generation quality; fused kernels for throughput vs. portability.
- **Failure signatures**: Training divergence due to incorrect attention masks (e.g., future token leakage); degraded performance if decoder layers not initialized from "closest to output" layers; throughput bottleneck if KV cache not reused.
- **First experiments**: 1) Profile GPU memory during inference to confirm encoder KV cache reuse; 2) Visualize attention masks to ensure correct block-causal and offset patterns; 3) Compare decoder layer initialization strategies for fine-tuning stability.

## Open Questions the Paper Calls Out
None

## Limitations
- Throughput gains depend on custom fused attention kernels not publicly available, making exact replication difficult.
- Precise decoder layer initialization ("closest to output") is not fully specified, risking instability if deviated from.
- Attention mask implementation is subtle; incorrect masks can cause training failure or information leakage.

## Confidence
- Architectural innovation and task setup: **High**
- Reproducibility of reported throughput gains: **Low**
- Reproducibility of training efficiency and fine-tuning stability: **Medium**

## Next Checks
1. Profile GPU memory usage during inference to confirm that encoder KV cache is reused across denoising steps (should remain flat).
2. Visualize attention masks during training to ensure correct block-causal and offset block-causal patterns with no leakage.
3. Compare layer mapping strategies (e.g., contiguous vs. strided) for decoder initialization and assess fine-tuning stability.