---
ver: rpa2
title: 'Demystifying the Accuracy-Interpretability Trade-Off: A Case Study of Inferring
  Ratings from Reviews'
arxiv_id: '2503.07914'
source_url: https://arxiv.org/abs/2503.07914
tags:
- interpretability
- scores
- sentiment
- interpretable
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the accuracy-interpretability trade-off in
  machine learning models for predicting product ratings from textual reviews. The
  authors introduce a Composite Interpretability (CI) score that quantifies interpretability
  based on simplicity, transparency, explainability, and model complexity.
---

# Demystifying the Accuracy-Interpretability Trade-Off: A Case Study of Inferring Ratings from Reviews

## Quick Facts
- **arXiv ID:** 2503.07914
- **Source URL:** https://arxiv.org/abs/2503.07914
- **Authors:** Pranjal Atrey; Michael P. Brundage; Min Wu; Sanghamitra Dutta
- **Reference count:** 8
- **Primary result:** Accuracy ranges from 40-60% across model configurations; interpretable models sometimes outperform black-box counterparts.

## Executive Summary
This study investigates the accuracy-interpretability trade-off in machine learning models for predicting product ratings from textual reviews. The authors introduce a Composite Interpretability (CI) score that quantifies interpretability based on simplicity, transparency, explainability, and model complexity. Analyzing 26 distinct model configurations combining various embedding methods, classification algorithms, and sentiment analysis tools, they find that model performance generally improves as interpretability decreases, though this relationship is not strictly monotonic. Notably, interpretable models outperform black-box counterparts in some instances.

## Method Summary
The study analyzes 26 model configurations combining embedding methods (CountVectorizer, TF-IDF, Word2Vec), classification algorithms (logistic regression, naive Bayes, SVM, neural networks), and sentiment analysis tools (VADER, BERT). Each configuration is evaluated on accuracy for 5-class rating prediction from product reviews. The Composite Interpretability (CI) score aggregates simplicity, transparency, explainability (each weighted 0.2), and parameter count (weighted 0.4) to quantify interpretability. Composite models concatenate embeddings with sentiment scores before classification.

## Key Results
- Accuracy ranges from 40-60% across different configurations
- Composite models using BERT sentiment scores typically show improved performance
- The accuracy-interpretability trade-off is not strictly monotonic, with interpretable models outperforming black-box models in some instances
- Naive Bayes with Word2Vec embeddings underperforms due to violated independence assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining BERT sentiment scores with simpler classifiers can yield accuracy gains comparable to full black-box models while preserving partial interpretability.
- **Mechanism:** BERT's domain-specific pre-training on product reviews captures sentiment-rating correlations more effectively than general embeddings. When these pre-computed sentiment scores are concatenated to review embeddings, downstream classifiers receive a stronger signal-to-noise ratio, reducing the learning burden on the classifier itself.
- **Core assumption:** The test distribution shares characteristics with the product review data on which BERT was fine-tuned.
- **Evidence anchors:** [abstract] "composite models using BERT sentiment scores typically showing improved performance"; [Page 3, Results] "the integration of BERT sentiment scores as an additional input generally contributes to an enhancement in accuracy"

### Mechanism 2
- **Claim:** Naive Bayes classifiers underperform when paired with Word2Vec embeddings due to violated independence assumptions.
- **Mechanism:** Naive Bayes assumes conditional independence among input features. Word2Vec embeddings encode semantic similarity through vector proximity—features are inherently correlated. This mismatch causes NB to miscalculate posterior probabilities, leading to degraded accuracy.
- **Core assumption:** The independence violation meaningfully affects classification decisions rather than canceling out across classes.
- **Evidence anchors:** [Page 3, Results] "We hypothesize that the observed trend is due to the NB classifier's assumption of conditional independence among features. Since Word2Vec captures contextual relationships between words, this assumption leads to suboptimal performance"; [Page 3, Table 5] NB with Word2Vec achieves only 30.70–36.10% accuracy across datasets

### Mechanism 3
- **Claim:** The accuracy-interpretability trade-off is weakly correlated but not deterministic—specific task characteristics can invert expected rankings.
- **Mechanism:** Interpretability scores aggregate simplicity, transparency, explainability, and parameter count. Accuracy depends on feature-quality fit and inductive bias alignment. Since these dimensions are partially orthogonal, their relationship is noisy: a "less interpretable" model with well-matched inductive bias can underperform a "more interpretable" model on unsuitable data.
- **Core assumption:** The CI scoring weights (simplicity/transparency/explainability at 0.2 each, parameters at 0.4) meaningfully reflect interpretability as perceived by practitioners.
- **Evidence anchors:** [Page 3, Results] "the trend is not very strictly monotonic with several outliers"; [Page 3, Results] "there are instances where interpretable models are more advantageous"

## Foundational Learning

- **Concept: Interpretability Dimensions (Simplicity, Transparency, Explainability)**
  - **Why needed here:** The CI score aggregates three qualitative dimensions rated by experts plus one quantitative dimension (parameter count). Without understanding what each dimension captures, you cannot interpret CI scores or identify which interpretability aspect matters most for your deployment context.
  - **Quick check question:** Can you explain why a model might score high on transparency but low on explainability?

- **Concept: Feature Independence Assumption in Naive Bayes**
  - **Why needed here:** Mechanism 2 hinges on recognizing when NB's independence assumption will be violated by correlated embeddings. This informs model selection beyond raw accuracy comparisons.
  - **Quick check question:** Given a 300-dimensional Word2Vec embedding where adjacent dimensions are weakly correlated (r ≈ 0.3), would you expect NB to significantly underperform? Why or why not?

- **Concept: Composite Model Architecture**
  - **Why needed here:** The paper's composite approach (embedding + sentiment score → classifier) requires understanding how to concatenate heterogeneous features and propagate them through downstream models.
  - **Quick check question:** If you have TF-IDF vectors (shape: 5000) and a BERT sentiment score (scalar), how do you combine them for input to logistic regression?

## Architecture Onboarding

- **Component map:** Raw review text → preprocessing → Embedding module (CountVectorizer | TF-IDF | Word2Vec) → Optional sentiment module (VADER | BERT) → Classification head (Logistic Regression | Naive Bayes | SVM | Neural Network) → Composite branch: Concatenate embedding + sentiment score before classification

- **Critical path:**
  1. Preprocess text (stopwords, punctuation)
  2. Generate embeddings via selected method
  3. (Optional) Compute BERT sentiment score
  4. Concatenate embedding + sentiment (if composite)
  5. Train classifier; evaluate accuracy on 5-class rating prediction
  6. Compute CI score using Eq. 1–2 for interpretability comparison

- **Design tradeoffs:**
  - **VADER vs. BERT sentiment:** VADER is interpretable (CI ≈ 0.20) but weakly correlated with ratings (r ≈ 0.4); BERT is opaque (CI = 1.00) but strongly correlated (r ≈ 0.8). Choose based on whether interpretability or accuracy is the constraint.
  - **Word2Vec + NB:** Avoid; independence assumption violation yields 30–36% accuracy. Use LR or SVM instead.
  - **Complexity vs. diminishing returns:** NN-BS (most complex) does not consistently outperform SVM-BS or LR-BS despite higher parameter count.

- **Failure signatures:**
  - **NB + Word2Vec:** Accuracy drops 10–20 percentage points below baseline; confusion matrix shows class collapse toward majority classes.
  - **NB + BERT sentiment:** Accuracy *decreases* relative to NB alone—BERT's continuous score violates NB's feature distribution assumptions.
  - **Cross-domain transfer:** VADER trained on social media underperforms on product reviews; expect moderate correlations (0.3–0.5).

- **First 3 experiments:**
  1. **Baseline grid:** Train LR, NB, SVM, NN with TF-IDF embeddings on all 4 datasets. Record accuracy and compute CI scores. Confirm non-monotonic trend replicates.
  2. **Composite ablation:** Add BERT sentiment scores to each classifier. Compare LR-BS vs. LR, SVM-BS vs. SVM. Isolate accuracy gain attributable to sentiment augmentation.
  3. **NB + Word2Vec diagnostic:** Train NB with progressively reduced Word2Vec dimensions (300 → 100 → 50 via PCA). Measure whether independence violation effect diminishes with lower-dimensional, decorrelated features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an analytical framework based on feature importance scores and decision boundaries effectively replace expert assessments in calculating the Composite Interpretability (CI) score?
- Basis in paper: [explicit] The authors state in the Future Work section that they aim to develop an advanced framework "replacing expert assessments with analytical factors such as feature importance scores and decision boundaries."
- Why unresolved: The current CI score relies on subjective surveys of 20 domain experts, which does not scale well and introduces human bias.
- What evidence would resolve it: A correlation analysis showing that a new formulaic metric (using feature importance/complexity) aligns closely with human expert rankings across a diverse set of models.

### Open Question 2
- Question: Do the non-monotonic trade-offs observed between interpretability and accuracy in product review rating inference generalize to other NLP tasks or high-stakes domains?
- Basis in paper: [inferred] The Limitations section notes the study is restricted to predicting product ratings and "results may not be generalizable to other application domains."
- Why unresolved: The specific characteristics of review text (e.g., length, sentiment polarity) may influence the trade-off differently than structured clinical data or financial logs.
- What evidence would resolve it: Replicating the CI score methodology on distinct datasets (e.g., medical records or loan applications) to see if glass-box models still outperform black-box models in specific instances.

### Open Question 3
- Question: How can post-hoc Explainable AI (XAI) techniques, such as LIME, provide deeper insights into the decision-making processes of the composite models analyzed?
- Basis in paper: [explicit] The authors explicitly suggest in Future Work that leveraging advances in XAI, specifically mentioning "techniques such as LIME," may enable deeper insights into model accuracy and interpretability.
- Why unresolved: The current study relies on intrinsic interpretability (model structure) rather than post-hoc explanations, leaving the internal logic of complex composite models opaque.
- What evidence would resolve it: A study applying LIME to the best-performing black-box composite models to determine if the generated explanations align with human intuition.

## Limitations
- CI score weighting (0.2, 0.2, 0.2, 0.4) lacks cross-validation; alternative weightings could shift interpretability rankings significantly
- No statistical significance testing reported for accuracy differences between configurations
- Cross-domain generalization of BERT sentiment scores untested beyond product reviews

## Confidence
- **High:** Non-monotonic accuracy-interpretability relationship, NB + Word2Vec underperformance
- **Medium:** BERT sentiment augmentation improves accuracy, CI scoring validity
- **Low:** Interpretability rankings generalize across domains, CI weights are optimal

## Next Checks
1. Perform paired t-tests on accuracy distributions across model configurations to identify statistically significant differences
2. Apply alternative CI weightings (e.g., 0.3, 0.3, 0.3, 0.1) and verify robustness of interpretability rankings
3. Test BERT sentiment transfer to non-product domains (social media, medical reviews) and measure accuracy degradation