---
ver: rpa2
title: Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised
  Contrastive Learning and Self-Training
arxiv_id: '2512.13770'
source_url: https://arxiv.org/abs/2512.13770
tags:
- learning
- graph
- multi-view
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of semi-supervised multi-view
  graph classification by proposing MV-SupGCN, a novel framework that integrates supervised
  contrastive learning and self-training. The core idea involves using a joint loss
  combining cross-entropy and supervised contrastive loss to improve intra-class compactness
  and inter-class separability.
---

# Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training

## Quick Facts
- arXiv ID: 2512.13770
- Source URL: https://arxiv.org/abs/2512.13770
- Authors: Huaiyuan Xiao; Fadi Dornaika; Jingjun Bi
- Reference count: 40
- Key result: Proposes MV-SupGCN, achieving state-of-the-art performance on seven datasets with up to 71.50% accuracy on Citeseer, 98.32% on NGs, and 85.65% on 3Sources

## Executive Summary
This paper introduces MV-SupGCN, a novel framework for semi-supervised multi-view graph classification that integrates supervised contrastive learning and self-training. The approach addresses the challenge of leveraging unlabeled data in multi-view settings by combining cross-entropy loss with supervised contrastive loss to improve class separation. The framework employs two distinct graph construction methods per view and integrates pseudo-labeling to fully utilize unlabeled data. Extensive experiments demonstrate superior performance compared to state-of-the-art methods across seven diverse datasets.

## Method Summary
MV-SupGCN addresses semi-supervised multi-view graph classification by integrating supervised contrastive learning with self-training. The framework uses a joint loss combining cross-entropy and supervised contrastive loss to enhance intra-class compactness and inter-class separability. For each view, the model constructs graphs using either KNN or semi-supervised methods to capture complementary structural information. Pseudo-labeling is employed to generate labels for unlabeled data, which are then incorporated into both the contrastive learning objective and cross-entropy loss. The model is trained end-to-end with multiple GCN layers per view, followed by fusion and classification layers.

## Key Results
- Achieves state-of-the-art performance on seven datasets with significant accuracy improvements
- Outperforms existing methods by up to 6.89% on Citeseer, 1.48% on NGs, and 1.92% on 3Sources
- Demonstrates superior stability with consistent performance across different data splits and label rates
- Shows effectiveness of supervised contrastive learning and self-training components through ablation studies

## Why This Works (Mechanism)
The framework leverages supervised contrastive learning to learn discriminative representations by pulling samples of the same class closer while pushing different classes apart in the embedding space. This addresses the limitation of standard cross-entropy loss in multi-view settings where views may contain complementary but noisy information. Self-training with pseudo-labeling enables the model to progressively incorporate unlabeled data, expanding the training set and improving generalization. The dual graph construction methods (KNN and semi-supervised) capture both local neighborhood structure and global connectivity patterns, providing complementary views that enhance the model's ability to learn robust representations.

## Foundational Learning

**Graph Convolutional Networks**: Why needed - to learn node representations by aggregating information from neighbors; Quick check - verify layer stacking and aggregation functions preserve graph structure

**Supervised Contrastive Learning**: Why needed - to improve class separability beyond standard cross-entropy; Quick check - ensure positive and negative pairs are correctly constructed based on labels

**Multi-View Learning**: Why needed - to leverage complementary information from different graph constructions; Quick check - validate that views contribute distinct information rather than redundancy

**Self-Training**: Why needed - to utilize unlabeled data for improving model generalization; Quick check - monitor pseudo-label quality and confidence thresholds

**Semi-Supervised Learning**: Why needed - to learn from limited labeled data in real-world scenarios; Quick check - ensure consistent performance across different label rates

## Architecture Onboarding

**Component Map**: Input views -> Graph Construction (KNN/Semi-supervised) -> GCN Layers -> Fusion -> Classifier -> Cross-Entropy Loss; Input views -> Graph Construction -> GCN Layers -> Contrastive Loss

**Critical Path**: Multi-view GCN feature extraction -> Fusion layer -> Classification head -> Joint loss computation (Cross-entropy + Supervised Contrastive Loss)

**Design Tradeoffs**: The choice between KNN and semi-supervised graph construction balances computational efficiency against capturing global structure; increasing GCN layers improves representation capacity but risks overfitting on limited labeled data

**Failure Signatures**: Poor performance on specific views may indicate graph construction issues; instability across runs suggests pseudo-labeling confidence thresholds need adjustment; convergence problems may require tuning contrastive loss weight

**First Experiments**: 1) Validate single-view performance with only cross-entropy loss to establish baseline; 2) Test supervised contrastive learning with fully labeled data to measure standalone contribution; 3) Evaluate self-training effectiveness with fixed pseudo-labels on a subset of data

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity and memory requirements not analyzed, limiting practical deployment assessment
- Impact of different k values in KNN graph construction not explored or justified
- Pseudo-labeling threshold and confidence criteria unspecified, affecting reproducibility
- Statistical significance test methodology and p-values not reported, weakening claims of superiority

## Confidence

**High**: The core framework design integrating supervised contrastive learning with self-training is technically sound and well-motivated

**Medium**: The experimental results showing performance improvements over baselines, though comprehensive dataset coverage supports credibility

**Medium**: The claims about superior stability, pending clarification on statistical testing methodology

## Next Checks

1. Conduct ablation studies varying the supervised contrastive loss weight and pseudo-labeling thresholds to establish robustness across hyperparameter settings

2. Implement computational complexity analysis comparing MV-SupGCN with baselines in terms of time and memory requirements per training epoch

3. Perform cross-dataset validation by training on one dataset category (e.g., social networks) and testing on another (e.g., citation networks) to assess generalizability beyond similar data distributions