---
ver: rpa2
title: 'HistoryBankQA: Multilingual Temporal Question Answering on Historical Events'
arxiv_id: '2509.12720'
source_url: https://arxiv.org/abs/2509.12720
tags:
- event
- events
- temporal
- reasoning
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We introduce HistoryBank, the largest multilingual historical
  event database with 10M+ events across 10 languages, constructed from Wikipedia
  timelines and infoboxes. We also present HistoryBankQA, a comprehensive temporal
  QA benchmark with six tasks: fact retrieval, duration reasoning, comparative analysis,
  counting, sequencing, and recurrence identification.'
---

# HistoryBankQA: Multilingual Temporal Question Answering on Historical Events

## Quick Facts
- **arXiv ID:** 2509.12720
- **Source URL:** https://arxiv.org/abs/2509.12720
- **Reference count:** 32
- **Primary result:** Largest multilingual historical event database (10M+ events, 10 languages) with comprehensive temporal QA benchmark showing multi-step reasoning challenges

## Executive Summary
HistoryBankQA introduces a comprehensive multilingual temporal question answering benchmark built on HistoryBank, the largest multilingual historical event database with 10M+ events across 10 languages. The benchmark includes six temporal reasoning tasks: fact retrieval, duration reasoning, comparative analysis, counting, sequencing, and recurrence identification. Evaluation across five models (GPT4o and smaller variants) reveals that while GPT4o performs best, even smaller models show strong reasoning abilities on certain tasks. Surprisingly, Hindi and Indonesian languages yield the highest model performance, suggesting that scale alone does not determine effectiveness in temporal reasoning.

## Method Summary
The HistoryBank database was constructed by extracting events from Wikipedia infoboxes and "On This Day" pages across 10 languages, yielding over 10 million historical events. Question generation employed synthetic methods using RecurrentGemma-2B-IT and Gemma-3B-IT models, with Flan-T5-base validating event pairs for duration tasks. The resulting HistoryBankQA benchmark contains up to 10,000 samples per task per language. All models were evaluated in zero-shot settings using JSON-formatted prompts specified in Appendix E, with exact match accuracy as the primary metric.

## Key Results
- GPT4o achieves highest overall performance but smaller models show competitive results on ordering tasks
- CountQA Between_events shows lowest accuracy (<0.21) due to error propagation across three subtasks
- Hindi and Indonesian languages yield highest model performance despite lower resource volumes
- Ordering tasks (Order Span Start/End) are significantly easier than quantitative interval arithmetic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-step temporal reasoning exhibits error propagation, where early-stage failures cascade into final answer errors.
- **Mechanism:** Tasks requiring sequential operations (date extraction → parsing → arithmetic) compound uncertainty. Each step introduces independent error probability; the paper notes that "even a small mistake in identifying one of the event dates can cascade into an incorrect final answer."
- **Core assumption:** Errors distribute independently across reasoning steps (unproven; could be correlated).
- **Evidence anchors:**
  - [section 6.1]: "Between_events" CountQA shows lowest accuracy because it combines three subtasks: identifying dates, sorting events, and counting between boundaries.
  - [abstract]: Gap Between, Overlap, and Duration Difference tasks show consistently lowest accuracy across all models.
  - [corpus]: Related work on multilingual reasoning (arXiv:2507.05418) similarly finds that multi-hop reasoning degrades in lower-resource languages.

### Mechanism 2
- **Claim:** Temporal reasoning performance is not solely determined by pretraining data scale but by structural regularity in the source language's temporal representations.
- **Mechanism:** Hindi and Indonesian yield highest model performance despite being relatively lower-resource. The paper suggests this may relate to "how temporally structured information is represented in that language's Wikipedia and how well the model captures its grammatical and formatting conventions."
- **Core assumption:** Wikipedia infobox temporal structures differ systematically across languages (paper shows variation but doesn't prove causality).
- **Evidence anchors:**
  - [section 6.2]: Hindi and Indonesian consistently yield highest average model performance across all benchmark tasks.
  - [table 2]: Events per infobox varies significantly (Italian: 2.87, French: 0.56), suggesting structural differences.
  - [corpus]: No direct corpus evidence for this specific mechanism; related multilingual QA work (arXiv:2601.06644) doesn't address temporal reasoning.

### Mechanism 3
- **Claim:** Qualitative temporal comparisons (before/after) are easier for LLMs than quantitative interval arithmetic.
- **Mechanism:** Order Span Start/End tasks achieve 0.78-0.90 accuracy on Gemma-2, while Overlap and Duration Difference fall below 0.20. The paper attributes this to partial temporal cues being sufficient for ordering but not for precise interval calculations.
- **Core assumption:** Models can resolve relative ordering without fully parsing both event dates (plausible but not directly measured).
- **Evidence anchors:**
  - [section 6.1]: "Ordering tasks (Order Span Start/End) are comparatively easier... Gemma in particular achieves >0.8 in several languages."
  - [section 6.3]: "Fine-grained interval arithmetic is a general weakness, exacerbated in smaller models."
  - [corpus]: Temporal reasoning benchmarks (TRAM, TempReason cited in paper) similarly find arithmetic-heavy tasks more difficult.

## Foundational Learning

- **Concept:** Allen's interval algebra (temporal relations: before, after, overlaps, during, etc.)
  - **Why needed here:** RelationQA explicitly tests these relations (Inclusion = "during," Overlap = "overlaps," etc.). Understanding them clarifies why some subtasks are harder.
  - **Quick check question:** Given two events with start/end dates (2020-01-01 to 2020-06-01) and (2020-04-01 to 2020-12-01), what is their Allen relation?

- **Concept:** Zero-shot evaluation (no task-specific fine-tuning)
  - **Why needed here:** All model evaluations use zero-shot prompting. This measures intrinsic pretraining knowledge rather than adaptation ability.
  - **Quick check question:** Why might zero-shot evaluation understate a model's temporal reasoning capacity after fine-tuning?

- **Concept:** Error propagation in multi-hop reasoning
  - **Why needed here:** Explains why CountQA Between_events (<0.21 accuracy) is harder than Between_dates (0.45-0.62). More reasoning hops = more failure points.
  - **Quick check question:** If Step A has 80% accuracy and Step B has 70% accuracy, what's the expected accuracy of A→B sequentially?

## Architecture Onboarding

- **Component map:** Wikipedia dump → infobox extraction → LLM-based event parsing → event pairing → synthetic question generation → zero-shot model evaluation

- **Critical path:**
  1. Wikipedia dump → infobox extraction
  2. LLM-based event parsing (dates + descriptions)
  3. Event pairing for span-based tasks
  4. Synthetic question generation
  5. Zero-shot model evaluation with exact match accuracy

- **Design tradeoffs:**
  - Synthetic questions enable scale (10M+ events, 6 task types) but may introduce artifacts (paper acknowledges "unnatural phrasing, limited diversity").
  - Infobox-focused extraction prioritizes structural regularity over free-text richness; may miss nuanced historical narratives.
  - Zero-shot evaluation isolates pretraining quality but ignores retrieval-augmented or few-shot strategies (paper acknowledges limitation).

- **Failure signatures:**
  - Low accuracy on arithmetic-heavy tasks (Duration Diff, Overlap) with all models below 0.30
  - Sharp drops in Bengali/Hindi for smaller models (LLaMA <0.10 on complex tasks)
  - RecurrenceQA has 0 examples for German, French, Italian, Portuguese, Russian (Table 3)—data availability gap

- **First 3 experiments:**
  1. **Baseline probe:** Evaluate your model on all 6 tasks with English data. Compare against Table 4 (GPT4o) and Tables 6-9 (smaller models) to identify which temporal reasoning types are weakest.
  2. **Error analysis on CountQA:** Sample 50 "Between_events" failures. Categorize errors as: (a) date extraction failure, (b) sorting failure, (c) counting logic failure. This validates the error-propagation hypothesis.
  3. **Language-specific probe:** Test Hindi vs. English on Order Span Start. If Hindi outperforms as the paper suggests, examine whether your model's tokenizer handles Hindi temporal expressions differently (character vs. subword tokenization).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does retrieval-augmented generation (RAG) or few-shot prompting impact model performance on the HistoryBankQA benchmark compared to the zero-shot setting?
- **Basis in paper:** [explicit] Section 8 (Limitations) explicitly notes, "We do not explore few-shot learning, retrieval augmented generation (RAG), or more advanced prompting strategies, which may yield better performance."
- **Why unresolved:** The authors restrict their evaluation protocol strictly to zero-shot settings to diagnose intrinsic capabilities, leaving the potential performance gains from external knowledge retrieval or in-context examples untested.
- **What evidence would resolve it:** A comparison of accuracy scores across the six temporal tasks when models are augmented with the HistoryBank database via RAG versus the reported zero-shot baselines.

### Open Question 2
- **Question:** What nuances in reasoning quality are revealed through human evaluation of model responses compared to the automatic exact match metrics used in the study?
- **Basis in paper:** [explicit] Section 8 (Limitations) states, "Human evaluation of model responses, especially for open-ended or comparative tasks, could provide more nuanced insights into reasoning quality and answer plausibility."
- **Why unresolved:** The study relies entirely on automatic metrics and structured answer keys, which cannot distinguish between a near-miss reasoning error and a complete failure in logic.
- **What evidence would resolve it:** A qualitative human annotation study analyzing the "reasoning" field provided by the models to determine if incorrect answers stem from retrieval failures or calculation errors.

### Open Question 3
- **Question:** Can the benchmark be effectively expanded to evaluate complex temporal inferences involving causality, counterfactuals, and narrative coherence rather than factual recall?
- **Basis in paper:** [explicit] Section 8 (Limitations) observes that "many questions still rely on factual recall rather than complex temporal inference involving causality, counterfactuals, or narrative coherence."
- **Why unresolved:** The current benchmark tasks (e.g., sequencing, duration calculation) focus on structured extraction and arithmetic, stopping short of the higher-level narrative synthesis required for true historical understanding.
- **What evidence would resolve it:** The creation of new task datasets within HistoryBankQA that require models to identify causal links between events or simulate alternative historical outcomes.

### Open Question 4
- **Question:** What specific linguistic or formatting conventions in Hindi and Indonesian contribute to higher model performance despite these languages having lower Wikipedia resource volumes than English or German?
- **Basis in paper:** [inferred] The Abstract and Section 6.2 note the surprising result that "Hindi and Indonesian languages yield the highest model performance," highlighting that scale alone does not determine effectiveness, but the paper does not isolate the root cause.
- **Why unresolved:** The authors establish the correlation but leave the underlying mechanisms—such as more consistent date formatting in those languages or different distributions of infobox types—as an open area of investigation.
- **What evidence would resolve it:** An error analysis across languages correlating performance with syntactic structure, date format regularity, and infobox complexity.

## Limitations

- Synthetic question generation may introduce unnatural phrasing and limited diversity, potentially creating an evaluation ceiling
- Zero-shot evaluation approach may underestimate model capabilities that would emerge with retrieval augmentation or few-shot prompting
- The correlation between Hindi/Indonesian performance and structural regularity remains correlative rather than causal without controlled experiments

## Confidence

**High Confidence:** The observation that multi-step temporal reasoning shows error propagation, with CountQA Between_events achieving <0.21 accuracy while simpler Between_dates reaches 0.45-0.62. This follows directly from task definitions and measured results without requiring additional assumptions.

**Medium Confidence:** The finding that ordering tasks (Order Span Start/End) are easier than quantitative interval arithmetic (Overlap, Duration Difference), with Gemma-2 achieving >0.8 on ordering but <0.20 on arithmetic. This is well-supported by results but could shift with different prompting strategies.

**Low Confidence:** The explanation that Hindi/Indonesian success stems from structural regularity in temporal representations rather than dataset scale. While the correlation exists, the mechanism lacks direct validation and could be confounded by other factors.

## Next Checks

1. **Error decomposition analysis:** For CountQA Between_events, manually categorize 50 failure cases into date extraction, sorting, and counting errors to quantify the error propagation mechanism. Compare the distribution against what would be expected from independent error probabilities.

2. **Prompt sensitivity test:** Vary temperature, max tokens, and prompt formatting for one complex task (Duration Difference) across all models. Measure accuracy variance to determine if current zero-shot results reflect inherent reasoning limits or prompt sensitivity.

3. **Cross-lingual consistency check:** Select 100 English fact retrieval questions and translate them into Hindi, Indonesian, and Bengali. Evaluate all models on these parallel questions to isolate whether performance differences stem from language-specific patterns or question difficulty variations.