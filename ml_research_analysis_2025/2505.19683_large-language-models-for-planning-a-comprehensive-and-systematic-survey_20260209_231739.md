---
ver: rpa2
title: 'Large Language Models for Planning: A Comprehensive and Systematic Survey'
arxiv_id: '2505.19683'
source_url: https://arxiv.org/abs/2505.19683
tags:
- arxiv
- planning
- preprint
- language
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive and systematic survey of Large
  Language Model (LLM) planning, covering planning definitions, methodologies, evaluation
  frameworks, and underlying mechanisms. It categorizes existing approaches into three
  main types: External Module Augmented Methods (combining LLMs with planners or memory),
  Finetuning-based Methods (using trajectory data or feedback signals), and Searching-based
  Methods (decomposition, exploration, and decoding strategies).'
---

# Large Language Models for Planning: A Comprehensive and Systematic Survey

## Quick Facts
- **arXiv ID:** 2505.19683
- **Source URL:** https://arxiv.org/abs/2505.19683
- **Reference count:** 40
- **Primary result:** Comprehensive survey categorizing LLM planning methods into external module augmentation, finetuning, and search-based approaches, analyzing evaluation frameworks and identifying future research directions.

## Executive Summary
This paper provides a systematic survey of Large Language Model (LLM) planning, covering definitions, methodologies, evaluation frameworks, and underlying mechanisms. The authors categorize existing approaches into three main types: External Module Augmented Methods (combining LLMs with planners or memory), Finetuning-based Methods (using trajectory data or feedback signals), and Searching-based Methods (decomposition, exploration, and decoding strategies). The survey analyzes evaluation datasets across digital, embodied, everyday, and vertical scenarios, along with corresponding metrics. Key findings include the dominance of fine-tuning methods, the growing popularity of multimodal agents, and the need for improved generalization and efficiency. The paper concludes with future directions such as RL integration, edge deployment, and universal planning systems.

## Method Summary
The survey systematically categorizes LLM planning methods into three main approaches: (1) External Module Augmented Methods that integrate LLMs with planners or memory modules for structured reasoning and long-term information storage; (2) Finetuning-based Methods that enhance planning through trajectory data or feedback signals using imitation learning or reinforcement learning; and (3) Searching-based Methods that decompose tasks, explore planning spaces, or optimize decoding strategies using algorithms like Tree of Thoughts or Monte Carlo Tree Search. The evaluation framework covers 40+ benchmark datasets across digital, embodied, everyday, and vertical scenarios, with metrics including success rate, execution efficiency, and constraint satisfaction.

## Key Results
- Fine-tuning methods dominate current LLM planning approaches, showing superior performance on specialized domains
- Multimodal agents are gaining popularity in vertical scenarios, integrating visual and textual reasoning capabilities
- Significant challenges remain in generalization across domains and efficient deployment on resource-constrained devices

## Why This Works (Mechanism)

### Mechanism 1: External Module Augmentation
Integrating LLMs with external symbolic planners or memory modules improves performance on complex planning tasks by leveraging structured reasoning and long-term information storage. The paper describes "External Module Augmented Methods" where an LLM translates natural language problem descriptions into formal representations (like PDDL) passed to dedicated symbolic planners using algorithms like A*. Memory modules store historical states or past successful trajectories, providing context during planning. Core assumption: accurate translation to formal languages and relevant memory retrieval. Break condition: incorrect PDDL generation or flawed memory retrieval misguides planning.

### Mechanism 2: Finetuning with Experience Data
Fine-tuning LLMs on trajectory data or feedback signals enhances intrinsic planning ability. "Finetuning-based Methods" update LLM parameters using expert demonstrations (imitation learning) or environmental feedback (reinforcement learning). Methods like FireAct use task-solving trajectories, while others employ process reward models for intermediate guidance. Core assumption: high-quality training data and accurate feedback signals. Break condition: errors in training trajectories or misleading reward signals lead to flawed planning strategies.

### Mechanism 3: Search-Based Exploration
Search algorithms exploring multiple reasoning paths improve LLM output quality and reliability. "Searching-based Methods" use LLMs to generate candidate thoughts or actions organized into search spaces (trees or graphs). Algorithms like Tree of Thoughts, Monte Carlo Tree Search, or A* navigate these spaces, evaluating candidates to select promising paths toward goals. Core assumption: diverse candidate generation and appropriate evaluation heuristics. Break condition: narrow generation, excessive computational cost, or poor evaluation functions misdirect search.

## Foundational Learning

- **Planning as a Markov Decision Process (MDP):** Why needed: Formal framework (S, A, T, s_init, S_goal) essential for understanding planner-enhanced methods and theoretical language used throughout. Quick check: Explain the role of state transition function T in planning.

- **Chain-of-Thought (CoT) Prompting:** Why needed: LLM's inherent capability supporting complex planning, foundational for searching-based methods like Tree of Thoughts. Quick check: Difference between direct answer generation versus Chain-of-Thought generation.

- **Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF):** Why needed: Core to feedback-based finetuning methods using reward signals to update LLM parameters. Quick check: Role of reward model in feedback-based finetuning loop.

## Architecture Onboarding

- **Component map:** LLM Agent -> External Planner Module / Memory Module / Reward/Feedback Module / Search Algorithm

- **Critical path:**
  1. Define Planning Task: Formalize problem with state space, action space, and goals
  2. Select Method Category: Choose between external augmentation, finetuning, or search-based methods
  3. Implement Core Logic: Central loop where LLM generates thoughts/actions processed by chosen method
  4. Iterate and Refine: Selection/expansion/backtracking for search, training loops for finetuning

- **Design tradeoffs:**
  - External Planners vs. Native LLM Reasoning: Higher reliability with external planners but requires accurate translation; native reasoning is more flexible but less reliable
  - Finetuning vs. In-Context Learning: Specialized performance with finetuning but costly; search/prompting is more generalizable but computationally expensive
  - Exploration vs. Exploitation: Critical tuning parameter affecting plan quality and time to find optimal path

- **Failure signatures:**
  - Translation Error: LLM generates unparsable PDDL causing planner failure
  - Hallucinated Action: LLM proposes invalid actions causing plan execution failure
  - Search Stagnation: Algorithm fails to find goal path within computational budget
  - Reward Hacking: Model exploits reward model to generate nonsensical high-reward plans

- **First 3 experiments:**
  1. Implement LLM+P pipeline (GPT-4 + PDDL planner) on Blocksworld problems, measuring PDDL success rate and final plan quality
  2. Apply Tree of Thoughts framework to multi-step reasoning problems, comparing against chain-of-thought prompting on accuracy and LLM calls
  3. Set up feedback-based finetuning on grid-world navigation, comparing sparse/dense/learned reward signals on learning speed and stability

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can standard reinforcement learning algorithms be adapted to accommodate the unique properties of LLMs—such as high-dimensional trajectory spaces and context sensitivity—to effectively enhance planning capabilities?
**Basis in paper:** [explicit] Section 6.1 outlines "Reinforcement Learning in LLM-based Planning" as a key future direction, explicitly stating that standard algorithms "must be adapted to accommodate... high-dimensional trajectory spaces [and] context sensitivity."
**Why unresolved:** Current RL methods struggle with vast, sparse-reward landscapes of complex planning tasks, failing to balance exploration and exploitation within LLM architecture.
**What evidence would resolve it:** Modified RL algorithms (specialized reward shaping or hierarchical RL) demonstrating significantly improved success rates on complex, long-horizon benchmarks compared to standard baselines.

### Open Question 2
**Question:** Can generalization in LLM-based planning be driven more by enhanced reasoning and abstraction rather than the mere memorization of task-specific training data?
**Basis in paper:** [explicit] Section 6.6 ("Enhancing Generalization Ability") raises this "compelling question," contrasting limited adaptability of current methods with potential for reasoning-driven generalization.
**Why unresolved:** Unclear whether performance gains stem from genuine abstract planning or overfitting to specific trajectory distributions, resulting in degraded performance on unseen scenarios.
**What evidence would resolve it:** Empirical results showing universal LLM-based planner maintaining high performance across diverse, out-of-distribution domains without relying on similar in-context examples.

### Open Question 3
**Question:** To what extent can model compression and transfer learning techniques preserve robust planning capabilities while downsizing models for deployment on resource-constrained edge devices?
**Basis in paper:** [explicit] Section 6.4 ("Edge Deployment for the Planning of Agents") identifies this as promising direction, noting that "downsizing models often leads to significant decline in planning performance."
**Why unresolved:** Compression techniques often degrade complex reasoning pathways required for multi-step planning, and trade-off between model size and planning capability is not fully solved.
**What evidence would resolve it:** Successful deployment of compressed/distilled agent model on mobile/robotic hardware achieving comparable Success Rates and Planning Efficiency to large-scale models on standard benchmarks.

## Limitations
- Reproducibility challenges due to proprietary models and unpublished hyperparameter configurations
- Rapidly evolving field means some approaches may be superseded quickly
- Limited direct comparison data across different method categories

## Confidence
- **High confidence:** Superiority of fine-tuning methods is supported by systematic categorization and multiple examples
- **Medium confidence:** Dominance of multimodal agents in vertical scenarios based on current trends but may not reflect future developments
- **Medium confidence:** Effectiveness of search-based methods depends heavily on task complexity and evaluation heuristics varying across studies

## Next Checks
1. Implement comparative study using same LLM model and evaluation framework across all three method categories on standardized benchmark like ALFWorld or Mind2Web
2. Conduct ablation studies on external module augmentation pipeline to quantify impact of PDDL translation accuracy versus planner performance
3. Design experiments to test generalization capabilities of finetuned models across different domains, measuring catastrophic forgetting and transfer learning effectiveness