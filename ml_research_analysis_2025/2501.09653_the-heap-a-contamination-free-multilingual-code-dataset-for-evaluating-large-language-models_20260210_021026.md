---
ver: rpa2
title: 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large
  Language Models'
arxiv_id: '2501.09653'
source_url: https://arxiv.org/abs/2501.09653
tags:
- dataset
- code
- datasets
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces The Heap, a large-scale multilingual code
  dataset containing 57 programming languages designed to enable fair evaluation of
  large language models without data contamination. The dataset addresses the challenge
  of limited fresh code available for downstream LLM evaluation by focusing on non-permissively
  licensed code and performing extensive deduplication against commonly used training
  datasets like The Stack.
---

# The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2501.09653
- Source URL: https://arxiv.org/abs/2501.09653
- Reference count: 22
- Primary result: The Heap provides 32.7M unique code files across 57 languages with deduplication flags for contamination-free LLM evaluation

## Executive Summary
This paper introduces The Heap, a large-scale multilingual code dataset containing 57 programming languages designed to enable fair evaluation of large language models without data contamination. The dataset addresses the challenge of limited fresh code available for downstream LLM evaluation by focusing on non-permissively licensed code and performing extensive deduplication against commonly used training datasets like The Stack. By providing pre-deduplicated code that large language models are unlikely to have been trained on, The Heap enables researchers to conduct robust, contamination-free evaluations of LLM performance on code-related tasks.

## Method Summary
The authors collect up to 50,000 repositories per language from GitHub, filter for non-permissive licenses, and apply both exact deduplication using SHA-256 hashing and near-deduplication using MinHash LSH with a Jaccard similarity threshold of 0.7. The resulting dataset contains 32.7 million unique files across 733,663 repositories, with each file annotated with metadata including quality indicators and duplicate status flags.

## Key Results
- 32.7 million unique files across 733,663 repositories
- 57 programming languages represented with up to 50,000 repositories per language
- SHA-256 exact matching and MinHash LSH near-matching against 5 reference datasets
- Files flagged with Boolean indicators for exact and near duplicates

## Why This Works (Mechanism)

### Mechanism 1: License-Based Contamination Barrier
- Claim: Using non-permissive licenses creates a structural deterrent against including this data in LLM training corpora.
- Mechanism: Non-permissive licenses (GPL, AGPL, LGPL) carry legal obligations requiring derivative works to be open-sourced. Commercial LLM developers typically avoid such code to prevent licensing contamination of their models, leaving this data "fresh" for evaluation.
- Core assumption: Model developers rationally avoid non-permissively licensed data due to legal/commercial risk.
- Evidence anchors: [abstract] "collected from GitHub repositories with non-permissive licenses"; [section II.B] "Non-permissively licensed code is usually removed due to potential licensing issues"
- Break condition: If actors train on non-permissive data regardless of licensing, the barrier fails.

### Mechanism 2: Exact Deduplication via Normalized Hashing
- Claim: SHA-256 hashing of normalized files (comments/whitespace removed) identifies exact duplicates across training datasets with low false-positive risk.
- Mechanism: Strip language-specific comments via regex → remove whitespace → compute SHA-256 → flag files whose hashes match those in The Stack v1/v2, Red Pajama, GitHub Code, CodeParrot.
- Core assumption: Normalization preserves semantic identity while catching trivial modifications (license header removal, reformatting).
- Evidence anchors: [section III] "remove all comments (using a regex, based on the programming language) and whitespace from each file"; [section III-A] "SHA-256 hash... selected for its low collision probability"
- Break condition: Semantically equivalent but syntactically different code (variable renaming, refactoring) bypasses detection.

### Mechanism 3: Near Deduplication via MinHash LSH
- Claim: MinHash locality-sensitive hashing with 7-character shingles and Jaccard threshold 0.7 captures near-duplicate code that exact matching misses.
- Mechanism: Convert normalized files to character shingles → compute MinHash signatures with 128 permutations → use LSH to bucket similar documents → flag pairs exceeding 0.7 Jaccard similarity as near-duplicates.
- Core assumption: The parameterization (shingle=7, threshold=0.7, 128 permutations) appropriately balances precision/recall for code similarity.
- Evidence anchors: [section III-B] "shingle size of 7 characters, as code files typically use a smaller set of characters compared to natural language"; [section III-B] "Files with a Jaccard similarity above 0.7 are flagged as near duplicates"
- Break condition: Semantic clones with low textual similarity (e.g., control-flow restructuring) remain undetected.

## Foundational Learning

- **Data Contamination in LLM Evaluation**
  - Why needed here: The paper's core premise is that contamination produces "overly optimistic results"—understanding this is essential to grasp why deduplication matters.
  - Quick check question: Can you explain why a model's performance on its training data is not a valid measure of its generalization capability?

- **MinHash and Locality-Sensitive Hashing (LSH)**
  - Why needed here: Near-deduplication relies on MinHash to approximate Jaccard similarity and LSH to enable scalable similarity search across millions of files.
  - Quick check question: Given two code files with Jaccard similarity 0.65, would the paper's 0.7 threshold flag them as near-duplicates?

- **Precision-Recall Trade-off in Deduplication**
  - Why needed here: The paper explicitly biases toward higher recall ("40%-60% precision-recall weight distribution"), accepting more false positives to ensure contamination is caught.
  - Quick check question: Why is high recall more important than high precision when the goal is preventing contamination in evaluation data?

## Architecture Onboarding

- **Component map:** GitHub API queries -> Filter by license + language -> Extract files by extension -> Normalize files -> Deduplication engine -> Output with metadata

- **Critical path:**
  1. Query GitHub for repos with target language + non-permissive license (sorted by stars descending)
  2. Extract files matching language extensions
  3. Normalize files (strip comments/whitespace) for hashing only
  4. Compute SHA-256 and MinHash signatures; compare against reference datasets
  5. Store unmodified files with duplicate flags (do not delete flagged files)

- **Design tradeoffs:**
  - **Flagging vs. removing:** Preserves max data but shifts filtering burden to downstream users
  - **Comment stripping:** Detects license-stripped copies but may over-flag documentation-heavy code with identical logic
  - **7-character shingles:** Tuned for code's limited charset vs. natural language's k=9; may inflate similarity for boilerplate-heavy languages (noted for Java in Section V)

- **Failure signatures:**
  - High false positives from boilerplate: Java files flagged as near-duplicates due to shared structural patterns
  - Missed semantic clones: Refactored code with different identifiers/control flow passes undetected
  - License misattribution: Repo-level license doesn't always match file-level reality

- **First 3 experiments:**
  1. **Spot-check near-duplicates:** Sample 50 files flagged as near-duplicates vs. their matches; manually verify Jaccard scores correspond to actual similarity
  2. **Contamination signal test:** Evaluate a model known to be trained on The Stack vs. The Heap; compare performance delta as a contamination indicator
  3. **Boilerplate sensitivity:** Run duplicate detection on a Java subset before/after stripping common imports/boilerplate; quantify false-positive reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different deduplication granularities (e.g., file-level vs. fragment-level) on the effectiveness of contamination prevention in code datasets?
- Basis in paper: [explicit] The authors state there is "limited research on what constitutes an effective deduplication strategy" and suggest potential issues with "duplicates at a lower granularity level than file-based deduplications."
- Why unresolved: Current methods rely on file-level hashing (SHA-256) or near-duplication (MinHash), which may not capture code fragments copied across different files or contexts.
- What evidence would resolve it: A comparative analysis of LLM performance on evaluation sets decontaminated using semantic or fragment-level analysis versus standard file-level deduplication.

### Open Question 2
- Question: How does the presence of non-English natural language in source code (comments and identifiers) affect the performance and evaluation of code-focused Large Language Models?
- Basis in paper: [explicit] The paper identifies the "presence of multiple natural languages within code" as an "under-explored research area" and proposes future work to adopt natural language tagging for files.
- Why unresolved: Current datasets often lack annotations for the natural language used within the code, making it difficult to isolate performance issues related to language barriers versus code logic.
- What evidence would resolve it: Empirical results from LLMs evaluated on subsets of The Heap annotated with specific natural languages, comparing performance against English-only code.

### Open Question 3
- Question: Can dataset-scale membership inference attacks reliably detect if an LLM was trained on "The Heap" despite deduplication efforts?
- Basis in paper: [explicit] The authors note that "membership inference attacks" have been extended to the "scale of datasets" and suggest this "should make it possible in the near future to test for the inclusion of The Heap."
- Why unresolved: While deduplication is a preventative measure, verifying that a model has *not* seen the data currently relies on trust or per-sample inference; dataset-level verification is not yet standard.
- What evidence would resolve it: The successful application of a robust membership inference attack that identifies The Heap's presence in a non-compliant model's training data.

## Limitations

- The license-based contamination barrier may be ineffective if commercial models train on non-permissive data regardless of legal risk
- The 0.7 Jaccard threshold may miss semantically equivalent but syntactically different code (e.g., variable renaming, control-flow restructuring)
- The normalization approach may over-flag documentation-heavy code with identical logic but different presentation

## Confidence

- **High confidence**: The dataset construction methodology is clearly specified with reproducible steps
- **Medium confidence**: The effectiveness of non-permissive licensing as a contamination barrier is plausible but not empirically validated
- **Medium confidence**: The claim that The Heap enables "fair" evaluation is supported by the deduplication methodology but not directly tested through downstream LLM evaluation experiments

## Next Checks

1. **Manual verification of near-duplicates:** Sample 100 files flagged as near-duplicates and their matches to verify that Jaccard similarity corresponds to actual semantic similarity, particularly for boilerplate-heavy languages like Java

2. **Contamination signal test:** Evaluate a model known to be trained on The Stack using both The Stack data and The Heap data, measuring performance differences to quantify contamination impact

3. **License compliance audit:** Sample 50 repositories from The Heap to verify that their actual file-level licensing matches the repository-level non-permissive license assumption, checking for permissive files within non-permissive repositories