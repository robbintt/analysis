---
ver: rpa2
title: 'VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for
  Referring Expression Comprehension'
arxiv_id: '2601.12781'
source_url: https://arxiv.org/abs/2601.12781
tags:
- object
- viro
- reasoning
- no-target
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VIRO introduces verification-integrated reasoning operators for
  robust referring expression comprehension. By embedding lightweight verification
  modules within each reasoning step, it suppresses cascading errors from false detections
  and invalid spatial relations, enabling explicit no-target detection.
---

# VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension

## Quick Facts
- **arXiv ID**: 2601.12781
- **Source URL**: https://arxiv.org/abs/2601.12781
- **Reference count**: 40
- **Primary result**: State-of-the-art 61.1% balanced accuracy on gRefCOCO no-target split with sub-0.3% program failure rate

## Executive Summary
VIRO introduces verification-integrated reasoning operators for robust referring expression comprehension (REC). By embedding lightweight verification modules within each reasoning step, it suppresses cascading errors from false detections and invalid spatial relations, enabling explicit no-target detection. The decoupled design generates a program once and reuses it across images, achieving high throughput and scalability. VIRO attains state-of-the-art balanced accuracy of 61.1% on gRefCOCO no-target and standard benchmarks, with sub-0.3% program failure rates and strong performance on egocentric video data.

## Method Summary
VIRO is a two-stage neuro-symbolic REC system. In the pre-execution stage, an LLM (Qwen2.5-72B-Instruct-AWQ) generates a symbolic program from a natural language query using few-shot prompting. A program validator checks syntax and semantics, allowing up to 5 retries for corrections. In the execution stage, a program interpreter sequentially executes operators (e.g., FIND, PROPERTY, FIND_DIRECTION) on the image. Each Verification Reasoning Operator (VRO) performs its function and validates outputs using thresholds or logical constraints, returning empty sets (∅) for invalid results and enabling early-exit termination. The system integrates open-vocabulary detection (GroundingDINO/GLIP), CLIP-based verification, and depth models (DepthAnythingV2) to filter false positives and handle no-target cases robustly.

## Key Results
- Achieves 61.1% balanced accuracy on gRefCOCO no-target split, outperforming prior methods
- Sub-0.3% program failure rate with early-exit handling for no-target cases
- Near-linear scalability (1-query-N-images) versus quadratic growth for coupled methods
- Strong performance on RefAdv (adversarial) and RefEgo (egocentric video) benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Operator-Level Verification for Error Suppression
Embedding verification within each reasoning operator prevents cascading errors from false positives and enables explicit no-target detection. Each VRO executes its function and validates outputs against confidence thresholds or logical constraints. If validation fails, the operator returns an empty set (∅), triggering early-exit termination rather than forcing a prediction.

### Mechanism 2: Uncertainty-Based Filtering with Adaptive Thresholds
CLIP-based verification with per-label adaptive thresholds suppresses OVD hallucinations better than fixed thresholds. For each candidate bounding box, compute verification score as average probability against common categories. Accept only if score exceeds a per-label threshold calibrated using ImageNet auxiliary data.

### Mechanism 3: Decoupled Program Generation from Execution
Separating LLM-based program synthesis from execution reduces latency and enables scalable 1-query-N-images processing. The LLM generates a symbolic program once per query, which the interpreter executes sequentially across multiple images, amortizing synthesis cost.

## Foundational Learning

- **Open-Vocabulary Detection (OVD)**
  - Why needed here: GroundingDINO/GLIP provide candidate bounding boxes for natural language queries; understanding their hallucination problem is critical for appreciating VIRO's verification layer.
  - Quick check question: Can you explain why an OVD might return high-confidence detections for objects that don't exist in an image?

- **CLIP Vision-Language Alignment**
  - Why needed here: Core to the Uncertainty Verification mechanism; CLIP's contrastive learning enables the binary classification verification step.
  - Quick check question: How does CLIP's contrastive pre-training enable zero-shot classification of image-text pairs?

- **Neuro-Symbolic Program Synthesis**
  - Why needed here: VIRO uses LLMs to generate symbolic programs (sequences of operators) rather than end-to-end neural predictions.
  - Quick check question: What are the trade-offs between symbolic program execution and end-to-end neural inference for compositional reasoning?

## Architecture Onboarding

- **Component map:**
  - Query input → LLM (Qwen2.5-72B-Instruct-AWQ) → Program Generator → Program Validator (syntax/semantic checks, max 5 retries)
  - → Program Interpreter → VROs (FIND, PROPERTY, FIND_DIRECTION, LOCATE, etc.) → OVD (GroundingDINO/GLIP), CLIP verification, or depth model (DepthAnythingV2)
  - → Early-exit on ∅ or RESULT outputs bounding box

- **Critical path:**
  1. Query Q input → LLM generates program P
  2. Validator checks P; if invalid, feedback to LLM for regeneration
  3. Interpreter executes P sequentially on image I
  4. FIND operator: OVD proposals → CLIP verification → filter or return ∅
  5. Spatial operators: geometric/logical verification → filter or return ∅
  6. Either all operators complete → RESULT outputs bounding box, OR any operator returns ∅ → early-exit, no-target

- **Design tradeoffs:**
  - OVD detection threshold: Higher threshold → better TNR, lower TPR; authors choose 0.2 for balance
  - CLIP backbone: ViT-H/14 gives 71.9% TPR at 1.39 FPS; ViT-L/14 gives 68.8% TPR at 1.79 FPS
  - Fixed vs. adaptive thresholds: Adaptive improves TNR from 43.1% to 50.2% with small TPR drop (74.4%→71.9%)

- **Failure signatures:**
  - Program generation failure (>0.3%): Malformed syntax, wrong operator names, mismatched arguments—caught by validator
  - Execution failure: All proposals filtered by UV → ∅ returned (intended for no-target cases)
  - False negatives: Over-aggressive UV filtering removes true positives—check threshold calibration

- **First 3 experiments:**
  1. **Reproduce Table 5 ablation**: Run detector-only baseline, add operators, then add LV, then add UV with fixed threshold, then UV with adaptive threshold. Verify TNR increases from 22.8% → 50.2%.
  2. **OVD threshold sweep**: Vary GroundingDINO detection threshold from 0.1 to 0.5 on gRefCOCO no-target split. Reproduce Figure 4 tradeoff curve.
  3. **Scalability test**: Run 1-query-1000-images experiment. Measure total time for VIRO vs. HYDRA/NAVER. Verify near-linear scaling for VIRO (Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can VIRO's verification-integrated operators be effectively extended to interactive embodied AI scenarios where robots engage in multi-turn dialogue with humans?
- **Basis in paper**: The conclusion states: "the modularity of our pipeline suggests promising extensions to interactive domains... the ability to parse natural language into verifiable symbolic programs opens the door to future applications where robots and humans engage in dialogue... ensuring that ambiguous commands are rejected before action."
- **Why unresolved**: The paper demonstrates VIRO on static-image and egocentric video REC, but interactive dialogue requires handling follow-up queries, disambiguation requests, and sequential instruction execution over time.
- **What evidence would resolve it**: Empirical evaluation of VIRO in a human-robot interaction setting with multi-turn dialogue benchmarks, measuring success rate, safety (rejection of unsafe commands), and disambiguation accuracy.

### Open Question 2
- **Question**: Would replacing heuristic-based verification thresholds with learned verification modules improve VIRO's precision-recall trade-off, particularly in reducing false negatives caused by aggressive filtering?
- **Basis in paper**: Section 4.3 and Appendix A.1.3 show that threshold selection creates a TPR-TNR trade-off. The authors acknowledge "true positives may be filtered out" with stricter thresholds, and use ImageNet-based calibration to mitigate CLIP label bias.
- **Why unresolved**: Heuristic thresholds and ImageNet calibration may not generalize optimally across diverse object categories, linguistic expressions, and visual domains.
- **What evidence would resolve it**: Ablation experiments comparing current threshold-based UV with learned verification heads trained on REC or grounding data, evaluated on balanced accuracy across both target-present and no-target splits.

### Open Question 3
- **Question**: Can VIRO maintain its low program failure rate (<0.3%) and verification robustness when scaling to significantly larger operator vocabularies for more complex relational and temporal reasoning?
- **Basis in paper**: VIRO uses a finite set of 11 predefined operators, but this constrained design may limit expressiveness for queries requiring comparative reasoning, temporal relations, or counting operations.
- **Why unresolved**: The paper does not evaluate how program generation or execution complexity scales as the operator vocabulary expands, nor whether the LLM program generator would produce more syntax errors with a larger operator set.
- **What evidence would resolve it**: Systematic evaluation of program failure rates, balanced accuracy, and throughput when adding operators for comparative, temporal, and quantitative reasoning, tested on datasets requiring such operations.

## Limitations
- CLIP-based verification reliability depends on ImageNet calibration data coverage; rare categories may have unreliable thresholds
- Decoupled architecture assumes query reuse across images; dynamic query-image pairing may limit scalability benefits
- Verification effectiveness varies with OVD false positive similarity to true positives; CLIP discrimination may fail for visually similar hallucinations

## Confidence

- **High Confidence**: The decoupled architecture achieving sub-0.3% program failure rates and linear scalability versus quadratic growth for coupled methods
- **Medium Confidence**: The CLIP-based verification improving TNR from 43.1% to 50.2% with minimal TPR loss; effectiveness depends on CLIP's discriminative power for OVD false positives which varies by category
- **Medium Confidence**: State-of-the-art 61.1% balanced accuracy on gRefCOCO no-target; strong performance on RefAdv and RefEgo suggests robustness, but comprehensive ablation of each verification mechanism is limited

## Next Checks

1. **Threshold Calibration Robustness**: Test CLIP-based adaptive threshold performance on rare categories (e.g., "giraffe," "zebra") not well-represented in ImageNet calibration data to quantify verification reliability gaps.

2. **Scalability in Dynamic Query Settings**: Evaluate VIRO's throughput when queries are image-specific rather than reused, measuring whether program generation overhead negates decoupled architecture benefits.

3. **Verification Ablation on False Positives**: Conduct controlled experiments injecting synthetic OVD false positives with varying visual similarity to true positives, measuring CLIP verification's discrimination accuracy across similarity ranges.