---
ver: rpa2
title: 'Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time
  Control'
arxiv_id: '2510.17122'
source_url: https://arxiv.org/abs/2510.17122
tags:
- uni00000013
- learning
- policy
- continuous
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Continuous Q-Score Matching (CQSM), a reinforcement
  learning method for continuous-time control that preserves action-evaluation capability
  without discretization. It characterizes continuous-time Q-functions via martingale
  conditions and links diffusion policy scores to Q-function action gradients through
  the dynamic programming principle.
---

# Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control

## Quick Facts
- **arXiv ID:** 2510.17122
- **Source URL:** https://arxiv.org/abs/2510.17122
- **Reference count:** 40
- **Primary result:** CQSM outperforms continuous-time policy gradient and little q-learning baselines in early-stage performance on LQ tasks.

## Executive Summary
This paper introduces Continuous Q-Score Matching (CQSM), a reinforcement learning method for continuous-time control that preserves action-evaluation capability without discretization. The method characterizes continuous-time Q-functions via martingale conditions and links diffusion policy scores to Q-function action gradients through the dynamic programming principle. CQSM employs a score-based policy improvement algorithm using denoising score matching. Theoretical closed-form solutions are provided for linear-quadratic control problems, and numerical experiments demonstrate improved stability and sample efficiency in continuous-time reinforcement learning settings.

## Method Summary
CQSM formulates continuous-time RL as a stochastic differential equation control problem where the Q-function is characterized by a martingale condition. The method learns a Q-function and a score-based policy simultaneously: the Q-function is trained to satisfy the martingale orthogonality condition, while the policy (represented as a score function) is updated to align with the action gradient of the Q-function. The algorithm uses denoising score matching for policy improvement and includes a regularization term to keep the policy close to a base distribution. Implementation relies on discrete sampling with time step Δt, though the theoretical framework is continuous.

## Key Results
- CQSM achieves higher rewards more quickly than continuous-time policy gradient and little q-learning baselines in LQ control tasks
- The method demonstrates improved stability in early-stage performance compared to existing approaches
- Numerical experiments on LQ tasks show robustness to varying time discretization steps Δt

## Why This Works (Mechanism)

### Mechanism 1: Martingale Orthogonality for Continuous-Time Evaluation
The paper posits that continuous-time Q-functions can be uniquely characterized and learned by enforcing a specific martingale condition, avoiding discretization errors inherent in traditional TD learning. The authors derive a process Ms dependent on the Q-function, cumulative reward, and score penalty. If Ms is a martingale, the Q-function is correct. The algorithm minimizes deviation from this martingale property using an orthogonality condition, effectively turning policy evaluation into a continuous-time regression problem.

### Mechanism 2: Policy Improvement via Score-Q Gradient Alignment
Policy optimization is achieved by aligning the score function (the drift term of the action diffusion) with the action gradient of the Q-function (∇a Q). The dynamic programming principle (HJB equation) suggests that maximizing the Hamiltonian leads to the optimal score. Theorem 2 proves that updating the score Ψ to be proportional to ∇a Q guarantees monotonic improvement in the Q-value.

### Mechanism 3: Implicit Regularization via KL-Divergence Cost
The inclusion of a quadratic score penalty (½λ‖Ψ‖²) in the objective acts as a regularizer, keeping the learned policy close to a base distribution. The objective penalizes large score magnitudes and is interpreted as the KL divergence between the controlled policy and a base uncontrolled policy, preventing the policy from deviating too aggressively from the exploration prior.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs) & Itô Calculus**
  - **Why needed here:** The core formulation of the environment and the derivation of the HJB equation rely on modeling state-action evolution as diffusion processes driven by Brownian motion.
  - **Quick check question:** Can you explain why standard chain rules fail when differentiating functions of stochastic processes (necessitating Itô's lemma)?

- **Concept: Martingale Theory**
  - **Why needed here:** The method replaces traditional Bellman backups with a martingale orthogonality condition. Understanding what makes a process a martingale is essential to grasp the loss function.
  - **Quick check question:** What does it imply for a process Mt if E[Mt+dt | Ft] = Mt?

- **Concept: Score-Based Generative Models (Diffusion)**
  - **Why needed here:** The policy is not a direct action mapping but a "score" function Ψ that guides a reverse-time diffusion process to generate actions.
  - **Quick check question:** How does the score function ∇a log π(a|s) relate to the direction of increasing probability density?

## Architecture Onboarding

- **Component map:** Environment -> Critic (Qθ) -> Actor (Score Ψv) -> Action Sampling

- **Critical path:**
  1. Sampling: Generate actions by simulating the reverse action SDE using Ψv (Denoising)
  2. Environment Step: Execute action, observe next state and reward
  3. Critic Update: Compute TD error δ using the martingale orthogonality condition (Eq. 13/Algorithm 1)
  4. Actor Update: Minimize MSE between current score Ψv and target λ⁻¹∇a Qθ

- **Design tradeoffs:**
  - **Time Discretization Δt:** While theory is continuous, implementation relies on discrete sampling. Large Δt risks approximation errors; small Δt increases compute. Experiments show robustness to varying Δt (Fig 3).
  - **Parameterization:** LQ experiments use quadratic parameterization for Q, while MuJoCo experiments would require deep networks. The paper assumes the Q function is sufficiently smooth (C¹,²,²).

- **Failure signatures:**
  - **Q-function Collapse:** If regularization λ is insufficient or architecture is wrong, Q might become action-independent (losing gradient signal)
  - **Denoising Instability:** If score network Ψv is not Lipschitz, simulated action diffusion might diverge

- **First 3 experiments:**
  1. **LQ Convergence Check:** Implement the Linear-Quadratic setup (Section 5) with Δt=0.1. Verify if parameters θ converge to the analytical optimum defined in Eq. (20)
  2. **Ablation on Δt:** Run the LQ task with Δt ∈ {0.01, 0.1, 1.0} to confirm that CQSM maintains performance stability compared to baselines (as suggested by Figure 3)
  3. **Gradient Alignment Visualization:** Plot Ψv(x,a) vs ∇a Qθ(x,a) during training. They should align as training progresses, confirming Theorem 2

## Open Questions the Paper Calls Out

- **Open Question 1:** Can CQSM be extended to handle mean-variance objectives in finite-horizon settings despite the inherent time-inconsistency problems?
  - **Basis in paper:** The conclusion states that "extending the approach to handle an important portfolio selection with a mean-variance objective poses a challenging problem due to inherent time inconsistency."
  - **Why unresolved:** The current framework focuses on standard reward accumulation, whereas mean-variance objectives introduce dynamic inconsistency that breaks standard dynamic programming principles used in CQSM.
  - **What evidence would resolve it:** A theoretical extension of the martingale condition or HJB equation that accounts for variance constraints, along with empirical validation in portfolio selection tasks.

- **Open Question 2:** Does optimizing the diffusion term σa lead to improved exploration and overall performance compared to the fixed schedules currently used?
  - **Basis in paper:** The authors identify "the optimization of the diffusion term σa" as a promising direction that "could lead to improved exploration and performance."
  - **Why unresolved:** The current method treats σa as a fixed or schedule-based hyperparameter rather than a learnable component of the policy dynamics.
  - **What evidence would resolve it:** Demonstrating that a learnable state-dependent σa stabilizes training or increases sample efficiency in sparse reward environments.

- **Open Question 3:** What is the theoretical convergence rate of the CQSM algorithm?
  - **Basis in paper:** The conclusion suggests that "a theoretical convergence rate analysis of CQSM could offer deeper insights and guide further enhancements."
  - **Why unresolved:** The paper provides theoretical closed-form solutions for LQ control but does not offer general convergence bounds or rates for the stochastic approximation updates in complex environments.
  - **What evidence would resolve it:** A formal proof bounding the convergence rate of the parameter updates under standard assumptions.

## Limitations
- The method's reliance on martingale conditions and smooth Q-function gradients remains untested in high-dimensional, non-linear environments
- Generalization to complex MuJoCo benchmarks is claimed but not empirically demonstrated in the provided information
- The paper does not address potential instability from dual estimation of Q and its gradient, or the impact of time discretization on martingale approximation accuracy

## Confidence

- **High confidence:** The theoretical framework (martingale characterization, score improvement theorem) is sound within the LQ setting
- **Medium confidence:** Numerical experiments on LQ tasks demonstrate improved sample efficiency and stability over baselines, but the sample size is limited
- **Low confidence:** Generalization to non-linear MuJoCo benchmarks is claimed but not empirically demonstrated in the provided information

## Next Checks

1. **High-dimensional generalization:** Replicate the method on standard continuous control benchmarks (e.g., MuJoCo HalfCheetah, Walker2d) and compare performance and sample efficiency against PPO and SAC baselines

2. **Gradient alignment robustness:** Analyze the alignment between Ψv and ∇a Qθ during training on a non-linear task. Quantify the impact of noisy gradients on policy stability and convergence

3. **Martingale approximation error:** Systematically vary the time discretization Δt on a non-linear environment and measure the deviation from the true martingale condition. Assess the impact on Q-function accuracy and policy performance