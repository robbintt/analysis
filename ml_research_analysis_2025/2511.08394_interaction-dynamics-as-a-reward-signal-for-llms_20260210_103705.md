---
ver: rpa2
title: Interaction Dynamics as a Reward Signal for LLMs
arxiv_id: '2511.08394'
source_url: https://arxiv.org/abs/2511.08394
tags:
- user
- interaction
- distance
- goal
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TRACE, a novel reward signal derived from\
  \ the geometric properties of a dialogue\u2019s embedding trajectory, termed \u201C\
  conversational geometry.\u201D Unlike traditional text-based reward models, TRACE\
  \ quantifies interaction dynamics\u2014such as pacing, coherence, and topical drift\u2014\
  without relying on raw textual content, making it inherently privacy-preserving.\
  \ In pairwise preference evaluations, a TRACE-only reward model achieved a predictive\
  \ accuracy of 68.20%, statistically indistinguishable from an LLM baseline analyzing\
  \ full transcripts (70.04%)."
---

# Interaction Dynamics as a Reward Signal for LLMs

## Quick Facts
- arXiv ID: 2511.08394
- Source URL: https://arxiv.org/abs/2511.08394
- Authors: Sian Gooding; Edward Grefenstette
- Reference count: 2
- Primary result: TRACE reward model achieves 68.20% pairwise accuracy, statistically indistinguishable from LLM baseline (70.04%), with hybrid model reaching 80.17%.

## Executive Summary
This paper introduces TRACE, a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory, termed "conversational geometry." Unlike traditional text-based reward models, TRACE quantifies interaction dynamics—such as pacing, coherence, and topical drift—without relying on raw textual content, making it inherently privacy-preserving. In pairwise preference evaluations, a TRACE-only reward model achieved a predictive accuracy of 68.20%, statistically indistinguishable from an LLM baseline analyzing full transcripts (70.04%). A hybrid model combining both signals achieved the highest performance at 80.17%, confirming that interaction dynamics and textual content are complementary sources of reward signal. The method offers both a scalable alignment framework and a diagnostic tool for understanding the distinct interaction patterns that drive successful human-AI collaboration.

## Method Summary
TRACE works by mapping conversation turns to vectors in semantic embedding space and computing geometric features from the resulting trajectory. These features capture interaction dynamics including volatility (distance between turns), drift (distance from goal), and temporal gaps. A Random Forest Regressor takes these features as input and outputs a scalar satisfaction score. The method is trained on approximately 2,100 conversations with satisfaction ratings, using Leave-One-User-Out cross-validation to evaluate pairwise preference prediction accuracy. The key innovation is that this approach works entirely on embedding trajectories rather than raw text, making it privacy-preserving while still capturing rich interaction dynamics.

## Key Results
- TRACE-only reward model achieves 68.20% pairwise accuracy on preference prediction
- LLM baseline analyzing full transcripts achieves 70.04% accuracy, statistically indistinguishable from TRACE
- Hybrid model combining both signals achieves highest performance at 80.17%
- Models disagree on 38.7% of preference pairs, confirming complementary information
- Different conversational signatures of success emerge for different task types (efficiency for troubleshooting vs. exploration for creativity)

## Why This Works (Mechanism)

### Mechanism 1: Geometric Proxy for Conversational Flow
The trajectory of a dialogue through semantic embedding space provides a predictive signal of user satisfaction that is comparable to analyzing the raw text. The system maps conversation turns to vectors and calculates "conversational geometry" features—distances between turns (Volatility), distance from the goal (Drift), and temporal gaps. These features are fed into a Random Forest Regressor to predict a scalar satisfaction score. The core assumption is that the topology of the embedding space accurately reflects semantic relationships such that "distance" equates to "relevance" or "coherence," and "trajectory" equates to "interaction quality."

### Mechanism 2: Orthogonal Complementarity of Dynamics and Content
Interaction dynamics (the "how") and textual content (the "what") are orthogonal signals; combining them yields significantly higher performance than either alone. The text-based LLM baseline captures semantic correctness and task completion. The TRACE model captures rhythm, effort matching, and stability. A hybrid model merges these distinct predictions, correcting cases where one signal is ambiguous (e.g., a correct answer delivered with frustrating latency). The core assumption is that the error modes of text-based evaluators are uncorrelated with the error modes of geometric evaluators.

### Mechanism 3: Non-Linear Interaction of Narrative Arcs
User satisfaction is not an average of turn-level scores but emerges from higher-order interactions, specifically "recovery" from errors and "expectation management." Statistical analysis reveals that specific feature interactions, such as [User Self-Consistency × Trend in Model Relevance], govern satisfaction. A "Broken Promise" (good start, high later volatility) is penalized more than a consistently mediocre interaction. The core assumption is that users value the *trajectory* of the interaction (improving vs. degrading) more than the absolute state at any single turn.

## Foundational Learning

- **Concept: Semantic Embedding Space**
  - Why needed here: TRACE is entirely built on vector arithmetic. You cannot understand "Goal Drift" or "Volatility" without understanding that words/concepts are mapped to vectors where distance ≈ difference in meaning.
  - Quick check question: If the distance between Turn 1 and Turn 2 is 0.9 (on a scale of 0-1), is the conversation likely coherent or volatile?

- **Concept: Pairwise Preference Optimization**
  - Why needed here: The paper validates TRACE not by predicting an absolute "4/5" score, but by predicting "Conversation A is better than B." This is the standard loss function for RLHF (e.g., DPO).
  - Quick check question: Why might a model achieve high pairwise accuracy but poor absolute correlation with ratings?

- **Concept: Non-Linear Feature Interactions (GAMMs)**
  - Why needed here: The paper explicitly rejects linear models (e.g., "more cohesion = better") in favor of non-linear "sweet spots" (e.g., cohesion is good only if relevance is improving). Understanding this is key to interpreting the reward signal.
  - Quick check question: Why might "high semantic cohesion" result in low satisfaction if the model is providing irrelevant answers?

## Architecture Onboarding

- **Component map:** Embedder -> Geometric Feature Extractor -> Reward Model (Random Forest) -> Hybrid Aggregator
- **Critical path:** The most sensitive component is the **Geometric Feature Extractor**. Errors in calculating `Trend in Model Relevance` or `User Self-Consistency` will propagate non-linearly into the reward prediction.
- **Design tradeoffs:**
  - Privacy vs. Granularity: TRACE is privacy-preserving (works on embeddings/vectors, not text), but it cannot diagnose *why* a specific fact was wrong, only *that* the interaction flow was poor.
  - Generalizability vs. Specificity: The paper finds different "signatures of success" for different tasks (e.g., Creativity vs. Troubleshooting). A global TRACE model might underperform compared to task-specific ones.
- **Failure signatures:**
  - "The Broken Promise": Low `Initial Response Distance` (good start) followed by High `Late Conversation Volatility`. The model detects a betrayal of expectations.
  - "Mismatched Effort": Low `User Self-Consistency` (user is trying hard) + Negative `Trend in Model Relevance` (model getting worse).
- **First 3 experiments:**
  1. Baseline Replication: Implement the top 5 features from Table 3 on a public dialogue dataset to verify correlation with human ratings.
  2. Ablation Study: Train the reward model *without* the "Goal Orientation" features. Does performance drop specifically in "Troubleshooting" tasks?
  3. Hybrid Integration: Run a standard LLM-as-a-judge baseline and combine it with your TRACE implementation. Verify if the "38.7% disagreement rate" holds for your data.

## Open Questions the Paper Calls Out

### Open Question 1
Does optimizing for TRACE via reinforcement learning (RL) genuinely lead to conversational agents that humans prefer, compared to text-only reward signals? The paper validates the signal *predictively* (offline) but stops short of demonstrating *causative* improvements in an online RL training loop. A training run where an LLM is fine-tuned using TRACE as the primary reward signal, followed by a human evaluation showing it outperforms a baseline trained on text-based rewards, would resolve this.

### Open Question 2
How can a reward model dynamically adapt to the "intent-dependent" geometric signatures of success identified in the analysis? The current model is trained on mixed data; it is unclear if a single reward function can accurately weigh these conflicting dynamics (e.g., volatility is bad for support but acceptable for brainstorming) without explicit knowledge of the user's current goal. An ablation study comparing a "context-aware" TRACE model (conditioned on task category) against a universal model would help resolve this.

### Open Question 3
Are the geometric properties of "conversational geometry" invariant to the specific embedding model used to represent the semantic space? The paper assumes these distances capture "conversational geometry," but does not test if these trajectories remain consistent if the underlying embedding model changes. A robustness analysis showing TRACE's predictive accuracy is maintained when computed using different state-of-the-art embedding models would resolve this.

## Limitations
- Proprietary dataset prevents independent verification of claimed accuracy metrics
- Embedding model choice is unspecified, potentially affecting geometric trajectories
- LLM baseline architecture and model details are underspecified beyond prompt template
- Cannot diagnose why specific facts were wrong, only that interaction flow was poor

## Confidence

- **High Confidence:** The core architectural claim that geometric features derived from dialogue trajectories can predict satisfaction (validated by the 68.20% pairwise accuracy)
- **Medium Confidence:** The orthogonality claim between interaction dynamics and textual content (supported by the 38.7% disagreement rate and hybrid performance gain)
- **Low Confidence:** The specific numerical performance metrics (68.20%, 70.04%, 80.17%) without access to the dataset and full implementation details

## Next Checks
1. Replicate the top 5 TRACE features from Table 3 on a public dialogue dataset (e.g., TopicalChat) to verify correlation with human ratings
2. Conduct an ablation study removing "Goal Orientation" features to test their specific contribution to task performance differences
3. Implement the LLM-as-a-judge baseline and hybrid model to independently verify the claimed disagreement rate and performance gains