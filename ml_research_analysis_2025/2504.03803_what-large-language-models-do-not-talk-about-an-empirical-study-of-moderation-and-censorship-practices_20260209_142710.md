---
ver: rpa2
title: 'What Large Language Models Do Not Talk About: An Empirical Study of Moderation
  and Censorship Practices'
arxiv_id: '2504.03803'
source_url: https://arxiv.org/abs/2504.03803
tags:
- censorship
- other
- description
- political
- omission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper distinguishes hard censorship (explicit refusals) from
  soft censorship (selective omissions) in LLM responses about political figures.
  It evaluates 14 state-of-the-art models across 6 UN languages, using a consensus-based
  approach to identify omissions of praise or allegations tied to internationally
  recognized norms.
---

# What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices

## Quick Facts
- arXiv ID: 2504.03803
- Source URL: https://arxiv.org/abs/2504.03803
- Reference count: 40
- Key outcome: Distinguishes hard censorship (refusals) from soft censorship (omissions) in LLM responses about political figures across 14 models and 6 UN languages, finding censorship shaped by geopolitical origin and query language.

## Executive Summary
This paper presents a comprehensive empirical study of moderation and censorship practices in large language models, distinguishing between hard censorship (explicit refusals) and soft censorship (selective omissions) in responses about political figures. The authors evaluate 14 state-of-the-art models across 6 UN languages, using a consensus-based approach to identify omissions of praise or allegations tied to internationally recognized norms. Results show significant variation in censorship patterns, with Russian LLMs displaying the highest refusal rates and Chinese models showing notable soft censorship for domestic figures. The study reveals that censorship is predominantly tailored to an LLM provider's domestic audience, raising important questions about transparency and ideological diversity in model moderation.

## Method Summary
The study employs a two-phase evaluation approach using 2,371 political figures from the Pantheon dataset. For hard censorship detection, responses to "Tell me about [Person X]" are classified by an evaluator LLM as valid, no answer, or refusal (error/canned/generated). Soft censorship is detected by mapping responses to normative frameworks (UNSDGs, UDHR articles, internationally proscribed crimes) and identifying attributes mentioned by ≥80% of models but omitted by specific models. The consensus-based approach treats majority-reported attributes as expected information, with deviations classified as soft censorship. The study analyzes 14 LLMs across 6 UN languages, comparing censorship patterns for domestic versus foreign political figures.

## Key Results
- Hard censorship rates vary widely by model, with Russian LLMs (GigaChat, YandexGPT) showing the highest refusal rates for domestic political figures
- Soft censorship patterns differ notably across models, with Chinese models showing higher omission rates for figures from China
- Censorship patterns are shaped by both the geopolitical origin of the model and the query language used
- Generated refusals (learned behavior) dominate for most models, while Russian models heavily use canned refusals (predefined messages)

## Why This Works (Mechanism)

### Mechanism 1: Geopolitical Alignment of Safety Layers
Providers implement safety layers that implicitly or explicitly flag entities sensitive to their specific geopolitical environment. The observed patterns (e.g., Russian models refusing Russian figures) are a result of deliberate alignment strategies or data curation choices rather than random noise.

### Mechanism 2: Inter-Model Consensus as a Proxy for Completeness
Soft censorship is detected by using the majority consensus of a diverse model panel as a proxy for "expected" information content. If ≥80% of models mention a specific normative detail about a political figure, that detail is considered factual and relevant.

### Mechanism 3: Taxonomy of Refusal Implementation
Distinguishing between "Error," "Canned," and "Generated" refusals reveals the architectural layer where censorship is enforced. Deterministic responses indicate external rules, while probabilistic responses indicate model weights.

## Foundational Learning

- **RLHF vs. Guardrails**: Needed to interpret the "Hard Censorship" taxonomy and distinguish between censorship learned by the model versus censorship enforced by external code.
  - Quick check: Does the refusal look like a generic API error, or a conversational sentence generated by the model?

- **Normative Frameworks (UNSDG, UDHR)**: Needed because the entire "Soft Censorship" evaluation relies on mapping model outputs to these frameworks.
  - Quick check: Can you name one specific article from the UDHR or a specific UN Sustainable Development Goal used to judge the political figures?

- **LLM-as-a-Judge**: Needed because the architecture relies on an external LLM to evaluate the responses of the subject models.
  - Quick check: What bias might the "Judge" LLM introduce if it shares the same training data origins as the subject models?

## Architecture Onboarding

- **Component map**: Input (LIA Dataset of 2,371 political figures) → Prompt Generator ("Tell me about [Person]") → Subject Layer (14 LLMs queried in 6 UN languages) → Evaluation Layer (Hard Censorship: ternary classification; Soft Censorship: normative mapping) → Aggregation Layer (Consensus Calculator at 80% threshold)

- **Critical path**: The dependency chain flows from generating raw responses → filtering refusals/hallucinations → applying normative mapping → calculating consensus omissions. The Soft Censorship analysis is invalid if Hard Censorship refusal rate is too high.

- **Design tradeoffs**: English-Centric Norms (Western/UN-centric frameworks may bias detection), Response Length (shorter responses more likely flagged as omissions), and Limited Coverage (low consensus rates for non-Western figures reduce soft censorship signal).

- **Failure signatures**: Hallucination/Refusal Ambiguity (small models often "refuse" because they don't know the figure), Canned Detection (requires robust clustering beyond simple string matching), and Consensus Validity (assumes majority opinion equals truth).

- **First 3 experiments**:
  1. Baseline Refusal Rate: Run "Tell me about [Person]" on domestic and foreign models for 100 local political figures to observe "Domestic vs. Foreign" refusal delta.
  2. Evaluator Consistency: Take 50 valid responses and run evaluator LLM check twice to ensure judge is deterministic in "only contributed" vs "only harmed" labeling.
  3. Canned vs. Generated: Prompt a model with 10 distinct banned figures; if 9/10 responses are identical strings, you have isolated a "Canned Refusal" guardrail.

## Open Questions the Paper Calls Out

- Should LLMs be designed to reflect the cultural values and ideological narratives of specific regions, or can alternative approaches mitigate the potential destabilizing effects of divergent LLM outputs on public discourse?
- How can researchers objectively distinguish between soft censorship (intentional omissions) and legitimate disagreement regarding the relevance of factual statements when a "universal ground truth" is absent?
- Does the reliance on a consensus of predominantly Western LLMs to identify "soft censorship" systematically overestimate omission rates for non-Western models?

## Limitations
- The consensus-based soft-censorship detection method assumes the majority opinion of a diverse LLM panel reliably indicates the "truth" or expected relevance of normative details.
- The normative frameworks used (UNSDG, UDHR, internationally proscribed crimes) are Western/UN-centric, potentially biasing detection against models trained in regions with different value systems.
- The study does not account for potential LLM judge bias if the evaluator LLM shares training data origins with the subject models.

## Confidence
- **Hard Censorship by Model and Region**: High confidence - patterns clearly demonstrated with substantial data
- **Soft Censorship Patterns**: Medium confidence - consensus method innovative but hinges on assumption that LLM consensus equals expected truth
- **Censorship Tailoring to Provider's Audience**: Medium confidence - evidence suggests geopolitical alignment but alternative explanations not fully ruled out

## Next Checks
1. **Consensus Validity Test**: Manually verify "ground truth" for 100 well-documented political figures and run consensus algorithm to validate 80% threshold captures expected attributes.

2. **Evaluator Bias Audit**: Select 50 responses from different origin models, run through evaluator LLM twice and compare results; swap evaluator LLM and compare soft censorship scores.

3. **Short Response Confounder Test**: Analyze correlation between response length and soft censorship scores; filter to responses under 50 tokens and recalculate rates to confirm confounding variable.