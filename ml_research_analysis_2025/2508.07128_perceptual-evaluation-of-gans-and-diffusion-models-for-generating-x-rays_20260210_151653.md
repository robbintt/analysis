---
ver: rpa2
title: Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays
arxiv_id: '2508.07128'
source_url: https://arxiv.org/abs/2508.07128
tags:
- images
- image
- synthetic
- radiologists
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates Generative Adversarial Networks (GANs) and
  Diffusion Models (DMs) for synthesizing chest X-rays conditioned on four abnormalities
  (Atelectasis, Lung Opacity, Pleural Effusion, and Enlarged Cardiac Silhouette) using
  real and synthetic images from the MIMIC-CXR dataset. Three radiologists of varied
  experience participated in two tasks: identifying synthetic images in real-synthetic
  pairs and assessing conditionality accuracy.'
---

# Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays

## Quick Facts
- arXiv ID: 2508.07128
- Source URL: https://arxiv.org/abs/2508.07128
- Reference count: 10
- Primary result: Synthetic chest X-rays from GANs and DMs are generally indistinguishable from real images, with specific architecture-dependent strengths for different pathologies

## Executive Summary
This study evaluates Generative Adversarial Networks (GANs) and Diffusion Models (DMs) for generating synthetic chest X-rays conditioned on four abnormalities. Three radiologists with varying experience levels assessed both the realism and conditionality accuracy of synthetic images compared to real ones from the MIMIC-CXR dataset. The results show that synthetic images are generally not discernible from real ones, with radiologists correctly identifying synthetic images only 50.4% of the time when making a decision. DMs produced more visually realistic images overall, but GANs performed better for specific conditions like the absence of Enlarged Cardiac Silhouette.

## Method Summary
The study used 160 real PA chest X-rays from MIMIC-CXR and generated 320 synthetic images (160 StyleGAN2, 160 RoentGen). Images were conditioned on four abnormalities (Atelectasis, Lung Opacity, Pleural Effusion, Enlarged Cardiac Silhouette) in both present and absent states. Three radiologists evaluated images using two tasks: identifying synthetic images in real-synthetic pairs and assessing conditionality accuracy using Likert scales. StyleGAN2 employed class-specific binary models while RoentGen used text-conditioned prompts. Evaluation metrics included Undecided Answers Rate (UAR), Correct Answer Rate (CAR), Accuracy, TPR (Sensitivity), and TNR (Specificity).

## Key Results
- Radiologists only made a decision about which image was synthetic 58.5% of the time, with correct identification at 50.4% when decisions were made
- DMs produced more visually realistic images overall, but GANs excelled at generating "no Enlarged Cardiac Silhouette" conditions
- Specific visual cues like high radiolucency and incomplete pulmonary fields were identified as indicators of synthetic images, particularly for DMs
- Lung Opacity generation showed low conditional accuracy across all image sources, suggesting resolution or contrast limitations

## Why This Works (Mechanism)

### Mechanism 1: Perceptual Realism via Distribution Matching
- GANs learn through adversarial feedback where a discriminator forces the generator to approximate the real image distribution. DMs iteratively denoise random noise into images, learning to reverse a gradual corruption process. Both approaches implicitly capture spatial correlations and intensity distributions characteristic of real radiographs.

### Mechanism 2: Conditional Generation with Architecture-Dependent Tradeoffs
- Conditioning signals (binary classes for GANs, text prompts for DMs) bias the sampling process toward latent regions associated with target pathological features. GANs use class-conditional batch normalization or auxiliary classifiers; DMs cross-attend to text embeddings during denoising.

### Mechanism 3: Expert Heuristic Detection of Generative Artifacts
- Expert radiologists possess learned priors about normal anatomy and pathology presentation. When synthetic images deviate from these priors in specific ways (excessive radiolucency, incomplete fields), experts flag them as artificial using pattern-matching heuristics refined through clinical training.

## Foundational Learning

- **Concept: GAN vs Diffusion Model fundamentals**
  - Why needed here: The study directly compares these architectures, assuming readers understand their distinct training paradigms and failure modes.
  - Quick check question: Can you explain why GANs may produce sharper images but struggle with mode diversity, while DMs offer better coverage at higher computational cost?

- **Concept: Conditional generation mechanisms**
  - Why needed here: The core task involves generating images with specific pathologies present or absent, requiring understanding of how conditioning is implemented.
  - Quick check question: How does class-conditional GAN training differ from text-conditional diffusion generation in terms of flexibility and precision?

- **Concept: FID limitations for medical imaging**
  - Why needed here: The paper critiques FID as a proxy metric that doesn't capture radiological fidelity or diagnostic relevance.
  - Quick check question: Why might a synthetic image have a good FID score but fail clinical validation? Consider pathological accuracy vs. distributional similarity.

## Architecture Onboarding

- **Component map:** Real MIMIC-CXR images → Train/fine-tune StyleGAN2 + RoentGen → Generate conditioned synthetic images → Present to radiologists → Compute UAR, CAR, TPR, TNR metrics
- **Critical path:** Real MIMIC-CXR images → Train/fine-tune StyleGAN2 + RoentGen → Generate conditioned synthetic images → Present to radiologists → Compute UAR, CAR, TPR, TNR metrics
- **Design tradeoffs:**
  - DMs: Higher visual realism, natural language conditioning flexibility, but worse conditional accuracy on "absence" states
  - GANs: Better binary conditional accuracy for specific conditions (e.g., no ECS), but less flexible text-based control
  - Resolution: 256×256 used for evaluation (computationally tractable) vs. DICOM standards (>1000×1000, clinically representative)
  - Prompting: Single template ("Pleural Effusion", "No pleural effusion") limits generalizability of DM results
- **Failure signatures:**
  - High radiolucency in DM images conditioned on "no ECS" (radiologists: CAR = 65.9%)
  - Incomplete/cropped pulmonary fields in DM images conditioned on "no LO" (radiologists: CAR = 80.0%)
  - Lung Opacity generation: Low TPR across all image sources (real: ~0.42, GAN: ~0.07, DM: ~0.27), suggesting resolution or contrast limitations
  - External devices (pacemakers, catheters) causing false positives in realism detection by junior radiologists
- **First 3 experiments:**
  1. Prompt engineering study: Test alternative DM prompting strategies (e.g., "Normal cardiac silhouette without enlargement" vs. "No Enlarged Cardiac Silhouette") to improve conditional accuracy on absence states.
  2. Resolution sensitivity analysis: Compare conditional accuracy for Lung Opacity detection at 256×256 vs. 512×512 vs. 1024×1024 to isolate resolution effects from model limitations.
  3. Cross-dataset validation: Replicate the evaluation protocol on a different chest X-ray dataset (e.g., CheXpert, NIH ChestX-ray14) to assess generalizability of GAN vs. DM tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can alternative prompt engineering strategies improve the conditional accuracy of Diffusion Models for chest X-ray generation to match or exceed the binary conditionality of GANs?
- **Basis in paper:** The authors state, "Future work should study whether changing the prompting of a text-condition generation can improve the results for DM models observed in this study," noting that GANs currently outperform DMs in specific binary conditional tasks.
- **Why unresolved:** The study only utilized a single, simple prompt template (e.g., "No [abnormality]") for the Diffusion Model, leaving the impact of more complex or varied natural language prompts on conditional fidelity unknown.
- **What evidence would resolve it:** A comparative study evaluating Diffusion Model outputs across a diverse set of prompt structures (few-shot, descriptive, negated) against GAN binary outputs to measure changes in conditional accuracy.

### Open Question 2
- **Question:** What architectural or training dynamics cause Diffusion Models to generate images with high radiolucency when conditioned on the absence of Enlarged Cardiac Silhouette (ECS)?
- **Basis in paper:** The authors observe that GANs outperformed DMs for the "absence of ECS" largely due to "high radiolucency present in some DM-generated images," and explicitly note that "the reasons behind this behavior are unclear and have not been further explored."
- **Why unresolved:** The paper identifies the visual artifact (radiolucency) and the specific condition under which it occurs, but does not investigate the latent space or denoising steps that lead to this specific failure mode.
- **What evidence would resolve it:** An ablation study analyzing the latent representations or attention maps of the Diffusion Model during the generation of "no ECS" images to identify the source of the radiolucency artifact.

### Open Question 3
- **Question:** Is the low conditional accuracy for Lung Opacity caused by a loss of critical contrast information during the model training process?
- **Basis in paper:** Regarding the poor results for Lung Opacity (LO), the authors suspect "critical information for generating lung opacity contrasts may be lost during the model training process," and conclude that "Further investigation in this line is left as future work."
- **Why unresolved:** While the low resolution (256x256) is a known limitation, the authors hypothesize a specific training-related information loss regarding contrast, which remains unverified.
- **What evidence would resolve it:** A frequency-domain analysis comparing the texture and contrast spectra of the original training set against the generated synthetic images to quantify the loss of specific radiographic features.

## Limitations
- Limited generalization to other modalities and datasets beyond MIMIC-CXR PA chest X-rays
- Resolution constraints (256×256) may not capture diagnostic details present in clinical DICOM standards
- Radiologist sample size of only three participants limits statistical power and generalizability
- Different conditioning approaches (binary vs. text) between GANs and DMs create comparison confounds

## Confidence
- **High confidence**: GANs vs DMs produce comparable overall realism under controlled evaluation; both architectures show specific failure modes detectable by radiologists
- **Medium confidence**: DMs generate more visually realistic images overall, but GANs excel at specific conditions like "no ECS"; conditional accuracy varies substantially by pathology type
- **Low confidence**: Generalizability of specific visual cues (high radiolucency, incomplete fields) across different imaging contexts and model architectures

## Next Checks
1. Resolution sensitivity analysis: Repeat evaluation at 512×512 and 1024×1024 to determine if observed artifacts (high radiolucency, incomplete fields) persist at clinically relevant resolutions
2. Prompt engineering study: Test alternative DM prompting strategies for "absence" conditions (e.g., "Normal cardiac silhouette without enlargement" vs. "No Enlarged Cardiac Silhouette") to improve conditional accuracy
3. Cross-dataset validation: Replicate the entire evaluation protocol on CheXpert or NIH ChestX-ray14 to assess whether GAN vs DM tradeoffs generalize beyond MIMIC-CXR