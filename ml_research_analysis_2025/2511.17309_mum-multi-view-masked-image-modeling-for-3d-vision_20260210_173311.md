---
ver: rpa2
title: 'MuM: Multi-View Masked Image Modeling for 3D Vision'
arxiv_id: '2511.17309'
source_url: https://arxiv.org/abs/2511.17309
tags:
- vision
- croco
- training
- image
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends masked autoencoders to multi-view image modeling
  by uniformly masking all views and using a lightweight decoder with inter-frame
  attention. The resulting model, MuM, is trained on 20M frames from diverse 3D datasets
  and achieves strong performance on 3D vision tasks including feedforward reconstruction,
  dense matching, and pose estimation, outperforming state-of-the-art feature encoders
  DINOv3 and CroCo v2 while using 30x less training compute.
---

# MuM: Multi-View Masked Image Modeling for 3D Vision

## Quick Facts
- arXiv ID: 2511.17309
- Source URL: https://arxiv.org/abs/2511.17309
- Authors: David Nordström; Johan Edstedt; Fredrik Kahl; Georg Bökman
- Reference count: 40
- Key outcome: Multi-view masked image modeling achieves strong 3D vision performance with 30x less compute than state-of-the-art methods

## Executive Summary
This paper introduces MuM, a multi-view extension of masked autoencoders (MAE) for 3D vision tasks. The key innovation is a uniform masking strategy across all views combined with lightweight decoder architecture featuring inter-frame attention. Trained on 20 million frames from diverse 3D datasets, MuM achieves state-of-the-art performance on feedforward reconstruction, dense matching, and pose estimation tasks while requiring significantly less computational resources than competing methods like DINOv2 and CroCo v2.

## Method Summary
MuM extends the MAE framework to multi-view settings by uniformly masking all input views and employing a lightweight decoder with inter-frame attention mechanisms. The model is trained on a large corpus of 20 million frames from various 3D datasets, learning robust feature representations for 3D vision tasks. The architecture maintains simplicity compared to prior approaches while achieving superior performance through its uniform masking strategy and efficient attention design.

## Key Results
- Outperforms state-of-the-art feature encoders DINOv2 and CroCo v2 on 3D vision tasks
- Achieves 30x reduction in training compute compared to competing methods
- Excels at geometric understanding tasks including reconstruction, dense matching, and pose estimation

## Why This Works (Mechanism)
MuM's effectiveness stems from its uniform masking strategy across multiple views, which forces the model to learn view-invariant representations. The lightweight decoder with inter-frame attention enables efficient cross-view information aggregation without excessive computational overhead. By training on diverse 3D datasets, the model develops robust geometric understanding capabilities that generalize across different 3D vision tasks.

## Foundational Learning
- **Masked Autoencoders (MAE)**: Self-supervised learning method where input is partially masked and model learns to reconstruct missing parts - needed for learning robust visual representations without labels
- **Multi-view Geometry**: Understanding 3D structure from multiple 2D views - needed for 3D vision tasks like reconstruction and pose estimation
- **Inter-frame Attention**: Mechanism for aggregating information across different views - needed for learning view-invariant representations
- **Feedforward Reconstruction**: Direct 3D reconstruction from images without iterative optimization - needed for efficient 3D modeling
- **Dense Matching**: Establishing correspondences between points in different views - needed for accurate 3D reconstruction and pose estimation
- **Geometric Understanding**: Model's ability to comprehend 3D spatial relationships - needed for robust performance across 3D vision tasks

## Architecture Onboarding

**Component Map**: Input views -> Uniform Masking -> Encoder -> Latent Representations -> Inter-frame Attention -> Lightweight Decoder -> Output

**Critical Path**: The encoder-decoder architecture with uniform masking and inter-frame attention represents the critical path for learning effective multi-view representations.

**Design Tradeoffs**: The uniform masking strategy sacrifices some view-specific detail for improved view-invariance and computational efficiency. The lightweight decoder prioritizes speed over maximal reconstruction fidelity.

**Failure Signatures**: Poor performance on tasks requiring view-specific details, potential issues with highly dynamic scenes where view consistency is violated, and possible limitations in handling extreme lighting variations.

**First Experiments**:
1. Ablation study on masking ratio to determine optimal uniform masking percentage
2. Comparative evaluation on synthetic multi-view datasets to establish baseline performance
3. Cross-dataset generalization test to assess robustness across different 3D environments

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on synthetic or curated datasets with limited real-world validation
- Claims of geometric understanding superiority need validation on more diverse geometric tasks
- Compute efficiency comparison methodology requires scrutiny for fair benchmarking

## Confidence
**Multi-view MAE extension effectiveness**: High - Well-supported by ablation studies and controlled experiments
**State-of-the-art performance**: Medium - Results are promising but require independent verification across different hardware setups
**Geometric understanding superiority**: Medium - Strong performance on reported tasks but generalizability to other geometric problems remains to be validated

## Next Checks
1. Test MuM on in-the-wild multi-view datasets (e.g., outdoor scenes, variable lighting conditions) to assess robustness beyond controlled synthetic environments
2. Evaluate MuM's feature representations on additional geometric understanding tasks not reported in the paper, such as surface normal estimation, depth completion, or multi-view stereo matching on standard benchmarks
3. Independently replicate the 30x compute efficiency claim using standardized benchmarking protocols across different hardware configurations to ensure fair comparison with DINOv2 and CroCo v2