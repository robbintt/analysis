---
ver: rpa2
title: 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity'
arxiv_id: '2511.03146'
source_url: https://arxiv.org/abs/2511.03146
tags:
- reasoning
- answer
- visual
- spatial
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MME-CC is a new benchmark designed to test multimodal large language\
  \ models (MLLMs) on vision-based reasoning tasks. It organizes 11 representative\
  \ tasks into three categories\u2014spatial, geometric, and visual knowledge reasoning\u2014\
  using 1,173 carefully annotated questions."
---

# MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity

## Quick Facts
- arXiv ID: 2511.03146
- Source URL: https://arxiv.org/abs/2511.03146
- Reference count: 40
- Primary result: MME-CC evaluates MLLMs on vision-based reasoning with 1,173 samples across 11 tasks; closed-source models lead but spatial/geometric reasoning remains weak (≤30%)

## Executive Summary
MME-CC introduces a new benchmark designed to systematically evaluate multimodal large language models on vision-centric cognitive reasoning tasks. Unlike existing benchmarks that emphasize textual reasoning or lack systematic coverage, MME-CC organizes 11 representative tasks into three categories—spatial, geometric, and visual knowledge reasoning—using carefully annotated questions where all task-critical information is encoded in images. The benchmark reveals that while top models like Gemini-2.5-Pro achieve strong performance on visual knowledge tasks, spatial and geometric reasoning remains particularly challenging, with all models scoring below 30% on many tasks. Chain-of-Thought reasoning follows a three-stage process with continuous visual extraction, and common failure modes include orientation mistakes, poor cross-view consistency, and difficulty following counterfactual instructions.

## Method Summary
The MME-CC benchmark consists of 1,173 annotated samples across 11 subtasks organized into three categories: Spatial Reasoning (319 samples), Geometric Reasoning (605 samples), and Visual Knowledge Reasoning (249 samples). Data collection involves multi-image inputs from sources like Google Maps, real estate listings, game screenshots, and web images, with post-processing including ID assignment, cropping, and resolution standardization. The evaluation uses an LLM-as-a-judge protocol with DeepSeek-V3-0324, achieving 95% human agreement on 99 validation samples. Models are evaluated with binary scoring (0/1) against gold references, with open-source models using temperature=1.0 and top-p=0.7, while proprietary models use default API settings. The annotation process involves a 10-person team with multi-stage quality control, filtering out items with >95% accuracy in model-based screening.

## Key Results
- Closed-source models like Gemini-2.5-Pro lead overall, but spatial and geometric reasoning tasks show particularly poor performance (≤30%)
- Chain-of-Thought reasoning follows a three-stage process (extract → reason → verify) with continuous visual extraction throughout, not just at initialization
- Adding explicit textual description instructions ("You should first describe the relevant content in the image") yields consistent performance gains across tasks
- Common error patterns include orientation mistakes, poor cross-view consistency, and difficulty following counterfactual instructions
- The Maze task proves exceptionally difficult, with all models scoring ≤2% accuracy, indicating fundamental limitations in sustained multi-step visual simulation

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-thought reasoning in MLLMs follows a hierarchical three-stage process with continuous visual extraction, not a single upfront encoding. Models decompose visual reasoning into: (1) problem understanding and information collection, (2) core analysis and reasoning with hypothesis testing, and (3) conclusion formation with verification. Critically, visual extraction recurs throughout all stages rather than occurring only at initialization. Extended reasoning chains provide opportunities for iterative verification and correction of intermediate inferences.

### Mechanism 2
Vision-grounded benchmark design forces models to rely on visual understanding by eliminating textual shortcuts. MME-CC ensures all task-critical information is encoded in images rather than accompanying text, preventing models from exploiting answer-bearing text, format priors, or OCR shortcuts that inflate scores on text-heavy benchmarks. This prevents models from preferring lower-cost textual shortcuts when available.

### Mechanism 3
Explicit textual grounding before reasoning stabilizes visual perception and improves downstream inference. Adding the instruction "You should first describe the relevant content in the image according to the prompt, and then answer the question" yields consistent performance gains across tasks. This anchors visual perception in explicit language before reasoning begins, reducing ambiguity in feature interpretation.

## Foundational Learning

- **Multimodal Large Language Model (MLLM) architecture**: Why needed here - MME-CC evaluates MLLMs specifically; understanding the vision encoder + LLM fusion helps interpret why spatial/geometric reasoning lags behind visual knowledge tasks. Quick check question: Can you explain how a vision encoder's output tokens are integrated with text tokens in a typical MLLM?

- **Chain-of-Thought (CoT) reasoning**: Why needed here - The paper's central finding is that CoT follows a three-stage pattern with continuous visual extraction; without understanding CoT, the error pattern analysis is opaque. Quick check question: What is the difference between zero-shot CoT and a fixed reasoning template, and which would you expect to generalize better to novel visual tasks?

- **LLM-as-a-judge evaluation protocol**: Why needed here - MME-CC uses DeepSeek-V3-0324 as a judge with 95% human agreement; understanding this protocol is essential for reproducing or extending the benchmark. Quick check question: What are two failure modes of LLM-as-a-judge that could cause systematic over- or under-scoring?

## Architecture Onboarding

- **Component map**: Task definition → Image collection + manual annotation → Post-processing (cropping, ID assignment) → Model-based filtering → LLM-as-a-judge evaluation → CoT stage classification + error pattern taxonomy

- **Critical path**: Start with the Satellite Image Matching and Indoor Deduplication Counting tasks—these expose the core cross-view identity persistence failure mode most clearly. The Maze task (≤2% accuracy across all models) serves as a ceiling probe for geometric planning.

- **Design tradeoffs**:
  - Sample size vs. quality: 1,173 samples is small compared to many benchmarks, but the multi-stage human-in-the-loop pipeline prioritizes validity over scale
  - Language-independence vs. accessibility: Removing textual shortcuts improves construct validity but makes the benchmark harder to use for models with weak visual encoders
  - Free-form output vs. automated scoring: Open-ended responses require LLM-as-a-judge, introducing scoring noise

- **Failure signatures**:
  - Orientation errors: Model fails to propagate reference frames across views (Satellite Image Matching, Indoor Directional Reasoning)
  - Identity persistence failures: Same object counted multiple times across views (Indoor Deduplication Counting)
  - Counterfactual instruction violations: Model outputs literal visual content instead of following inverted rules (Counterfactual Instruction task)
  - Excessive verification loops: Repeated "wait" statements in CoT indicate stalled reasoning

- **First 3 experiments**:
  1. Baseline reproduction: Run your target MLLM on the full MME-CC set using the provided evaluation protocol. Compare category-level scores to Table 4 to validate your setup.
  2. Description ablation: Apply the "describe first, then answer" prompt modification to 3 subtasks (one per category). Measure delta vs. baseline to assess textual grounding effects on your model.
  3. Error pattern audit: Manually review 20 failure cases from your model's worst-performing subtask. Classify errors using the paper's taxonomy (orientation, identity, instruction following) to identify systematic gaps.

## Open Questions the Paper Calls Out

### Open Question 1
Does explicit textual description before reasoning improve visual cognition fundamentally, or merely compensate for inadequate visual grounding in current architectures? The authors find that adding "You should first describe the relevant content in the image" yields consistent gains across tasks, and state "the improvements mainly arise from better textual alignment rather than stronger intrinsic visual reasoning." The paper identifies the phenomenon but does not determine whether this is a training deficiency or an architectural limitation that requires new visual-linguistic coupling mechanisms.

### Open Question 2
What is the optimal balance between verification behavior and reasoning efficiency in visual CoT, and can adaptive verification be learned? The paper observes that models "frequently employ 'wait' style pauses" and notes that "excessive pausing... leads to stalling and repetitive verification," while also hypothesizing that "long reasoning chains dilute attention, obscure crucial visual details." The tradeoff between verification thoroughness and efficiency is documented but not quantified, and no mechanism is proposed for dynamically adjusting verification intensity.

### Open Question 3
Why do all evaluated models catastrophically fail at sustained multi-step visual simulation (e.g., Maze at ≤2%), and what architectural components are missing? The paper reports that on the Maze task, "which requires continued rule-based simulation and path planning, no model exceeds 2%." The paper documents the failure but only hypothesizes causes (long chains diluting attention). It remains unclear whether the limitation is due to working memory capacity, lack of explicit spatial representations, or inability to maintain state across reasoning steps.

## Limitations

- Small sample size (1,173 total) may limit statistical power for detecting fine-grained differences between models, particularly at the subtask level
- Reliance on LLM-as-a-judge evaluation introduces potential scoring bias that could systematically favor certain reasoning patterns or response styles
- Benchmark focuses exclusively on vision-centric reasoning, making it unsuitable for assessing models' full multimodal capabilities or text-heavy reasoning domains

## Confidence

- **High confidence**: Chain-of-Thought follows three-stage process with continuous visual extraction; vision-grounded design effectively eliminates textual shortcuts; spatial and geometric reasoning remain weak compared to visual knowledge tasks
- **Medium confidence**: Explicit textual grounding before reasoning improves performance; error patterns are systematic and task-specific; 16-model comparison provides robust baseline
- **Low confidence**: Generalization of findings to non-MLLM architectures; scalability of benchmark to larger sample sizes; external validity across different cultural/visual contexts

## Next Checks

1. Cross-validation of LLM-as-a-judge: Run human scoring on 200 randomly selected samples from different subtasks to verify the 95% agreement holds across all categories and identify potential scoring biases
2. Error pattern replication: Test the three major error types (orientation, identity persistence, counterfactual instruction violations) across three additional MLLMs not included in the original study to assess generalizability
3. Sample size sensitivity analysis: Evaluate whether the current benchmark size provides sufficient statistical power by calculating confidence intervals for the observed performance differences and simulating detection thresholds for smaller sample subsets