---
ver: rpa2
title: Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering
  Pipeline
arxiv_id: '2503.21155'
source_url: https://arxiv.org/abs/2503.21155
tags:
- feature
- features
- dataset
- datasets
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Large Language Models (LLMs) to embed
  domain-specific knowledge into datasets as an initial step in the feature engineering
  pipeline. By providing only feature names and target objectives to LLMs, the method
  generates feature combinations that improve model convergence and reduce computational
  costs.
---

# Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline

## Quick Facts
- arXiv ID: 2503.21155
- Source URL: https://arxiv.org/abs/2503.21155
- Authors: João Eduardo Batista
- Reference count: 40
- Primary result: LLM-generated features improved model performance in 1/3 of tested datasets while preserving data privacy

## Executive Summary
This paper proposes using Large Language Models (LLMs) to embed domain-specific knowledge into datasets as an initial step in the feature engineering pipeline. By providing only feature names and target objectives to LLMs, the method generates feature combinations that improve model convergence and reduce computational costs. The approach was tested on 11 datasets using M3GP and M6GP algorithms with Ridge and Random Forest regressors. Results showed consistent improvements in test performance for one-third of the datasets, with only one out of 77 test cases showing decreased performance. M6GP was successfully extended to symbolic regression tasks, producing competitive results with M3GP while generating smaller models.

## Method Summary
The paper presents a two-stage feature engineering pipeline where (1) an LLM generates domain-specific feature combinations from feature names and objectives only, followed by (2) GP-based feature engineering (M3GP or M6GP) for symbolic regression/classification. The method uses GPT-4o via a two-prompt strategy: first asking "Which features should I use?" for the objective, then asking "Are there any combinations of features that might improve results?" The recommended features are added to the dataset, and M3GP/M6GP perform symbolic regression/classification using a wrapper-based fitness function (Ridge/RF performance via 2-fold CV). The pipeline was tested on 11 datasets from UCI and USGS repositories, comparing baseline models against the full pipeline with and without GPT-generated features.

## Key Results
- LLM-generated features improved test performance in 3 out of 11 datasets (CSS, PM, IM10)
- The approach decreased model performance in only 1 out of 77 test cases
- M6GP successfully extended to symbolic regression, producing competitive results with 95% smaller models in classification
- GP convergence accelerated when seeded with LLM features, reducing required generations
- Data privacy preserved by not exposing dataset samples to LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can generate useful feature combinations from feature names and target objectives alone, without accessing dataset samples.
- **Mechanism**: The LLM's pre-trained knowledge encodes domain relationships (e.g., cement + water ratios for concrete strength). When prompted with feature names, it retrieves relevant transformations via RAG capabilities and semantic associations, generating arithmetic combinations (ratios, sums, polynomial/log transforms) that capture known physical or statistical relationships.
- **Core assumption**: The LLM's training corpus contains sufficient domain knowledge about the relationships between the features and target variable to suggest meaningful combinations.
- **Evidence anchors**:
  - [abstract]: "The proposed approach only provides the names of the features in the dataset and the target objective to the LLM... consistent improvements to test performance were only observed for one-third of the datasets (CSS, PM, and IM10), possibly due to problems being easily explored by LLMs"
  - [section 4.1]: "Since the CSS and IM10 tasks use features that have well-known meanings, this seems to imply that GPT is better at recommending features for well-studied problems."
  - [corpus]: Related work (LKD-KGC, LMAR) supports domain-specific knowledge extraction from LLMs, though none specifically address feature engineering pipelines.
- **Break condition**: Fails when feature names are opaque or domain is obscure (e.g., "MOVL", "WAV" datasets excluded due to "lack of documentation on the meaning of each variable").

### Mechanism 2
- **Claim**: Adding LLM-recommended features to the initial dataset reduces the number of generations required for GP-based feature engineering to converge.
- **Mechanism**: GP algorithms start with randomly generated individuals. By seeding the dataset with domain-relevant features, the initial population contains individuals that incorporate these features, effectively "biasing the models into learning the data correctly" and reducing random exploration in early generations.
- **Core assumption**: The GP algorithm's tree-based representation can incorporate pre-constructed features as terminal nodes, and the fitness function will favor individuals using useful LLM-generated features.
- **Evidence anchors**:
  - [abstract]: "By doing so, our results show that the evolution can converge faster, saving us computational resources."
  - [section 4.1]: "Using the median RMSE from Ridge-GPT (6.901) as a baseline, we see that it took 33/13 and 64/15 generations for M3GP-Ridge/M3GP-Ridge-GPT and M6GP-Ridge/M6GP-Ridge-GPT to surpass it, respectively."
  - [corpus]: No direct corpus evidence for GP acceleration via LLM-seeding; this appears to be a novel contribution.
- **Break condition**: When LLM features are noisy or irrelevant, GP must still explore fully; paper notes "GP is robust to bad feature combinations."

### Mechanism 3
- **Claim**: A two-stage pipeline (LLM feature construction → GP feature engineering) improves final model performance while maintaining data privacy.
- **Mechanism**: Stage 1 uses LLMs to inject domain knowledge as additional features. Stage 2 (M3GP/M6GP) performs implicit feature selection and construction, discarding noisy LLM features while retaining useful ones. The wrapper-based fitness evaluation (2-fold cross-validation with Ridge/RF) optimizes features for the specific downstream model.
- **Core assumption**: The GP stage can effectively filter LLM-generated noise without overfitting, and the combination provides complementary benefits (domain knowledge + data-driven optimization).
- **Evidence anchors**:
  - [abstract]: "this approach only decreased the model performance in 1/77 test cases"
  - [section 3.3]: "While the feature combinations recommended by LLMs may be noisy, they can be later thrown away in the second feature engineering step. As such, the benefits of obtaining useful combinations compensate for the risk of obtaining bad ones."
  - [corpus]: Weak corpus support for multi-stage pipelines; corpus focuses on LLM-driven KG construction rather than feature engineering.
- **Break condition**: If GP overfits to training data or LLM generates too many noisy features overwhelming the selection process.

## Foundational Learning

- **Concept: Genetic Programming (GP) for Feature Engineering**
  - Why needed here: M3GP and M6GP use tree-structured GP to evolve arithmetic expressions that transform input features. Understanding how trees represent expressions (operators as internal nodes, features as leaves) is essential.
  - Quick check question: Given features X1, X2, can you sketch a GP tree representing (X1 + X2) / X1?

- **Concept: Wrapper-Based Feature Engineering**
  - Why needed here: M3GP/M6GP use a downstream model's performance (Ridge/RF via 2-fold CV) as fitness. This couples feature engineering to the specific model, different from filter methods like PCA.
  - Quick check question: Why might features optimized for Ridge perform poorly with Random Forest?

- **Concept: Multi-Objective Optimization (M6GP)**
  - Why needed here: M6GP jointly optimizes performance AND model size (classification) or maximum absolute error (regression), trading off accuracy for parsimony/robustness.
  - Quick check question: If fitness = [RMSE, MAE], how would Pareto dominance determine which individuals survive?

## Architecture Onboarding

- **Component map**:
  ```
  [Feature Names + Objective] → [LLM (GPT-4o)] → [Feature Recommendations]
                                              ↓
  [Original Dataset] → [Dataset + LLM Features] → [M3GP/M6GP] → [Evolved Features]
                                                       ↓
                                              [Ridge/RF Wrapper] ← Fitness Signal
  ```

- **Critical path**: The LLM prompt engineering (Use Case 1 in paper) is the lowest-effort, highest-leverage step. The paper uses a two-prompt strategy: (1) "Which features should I use?" → (2) "Are there any combination of features that might improve the results?" This separation improved consistency.

- **Design tradeoffs**:
  - LLM features: Free to add (no privacy risk), but may be noisy; paper chose NOT to use LLM for feature selection to avoid discarding useful features permanently
  - Wrapper choice: Ridge wrapper → more generations (100), simpler models; RF wrapper → fewer generations (30), better performance but larger models (up to 6400 parameters)
  - M3GP vs M6GP: M3GP single-objective, larger models; M6GP multi-objective, up to 95% smaller models in classification

- **Failure signatures**:
  - No improvement in 5/11 datasets (features too domain-specific or obscure)
  - IS dataset showed overfitting spike in later generations (M3GP-Ridge median RMSE doubled gen 84→100)
  - One experiment (PT dataset with M6GP-RF-GPT) showed significant WAF reduction

- **First 3 experiments**:
  1. **Baseline replication**: Run Ridge, RF, DT on any dataset (e.g., CSS from UCI) without feature engineering to establish baseline RMSE.
  2. **LLM feature construction only**: Prompt GPT-4o with feature names from the same dataset, add recommended features, re-run Ridge/RF. Compare RMSE improvement.
  3. **Full pipeline test**: Install m3gp/m6gp Python libraries, run M3GP-Ridge with 30 generations on the LLM-augmented dataset. Track test RMSE per generation to observe convergence acceleration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can structural complexity metrics be defined for M6GP in symbolic regression that minimize model bloat without degrading predictive performance?
- **Basis in paper**: [explicit] The Conclusion states that future work includes "obtaining structural complexity functions that can be used by M6GP for symbolic regression tasks," noting that current complexity functions "ruined the performance of the models."
- **Why unresolved**: While M6GP minimizes size for classification, the authors could not find a comparable metric for regression that balanced model simplicity with the accuracy provided by the Maximum Absolute Error (MAE) objective.
- **What evidence would resolve it**: A study introducing new complexity metrics (e.g., semantic complexity or functional complexity) for M6GP regression that results in statistically smaller models without a significant drop in RMSE compared to the current implementation.

### Open Question 2
- **Question**: Do "reasoning models" provide superior feature recommendations compared to standard generative LLMs in the feature engineering pipeline?
- **Basis in paper**: [explicit] The author explicitly intends to keep exploring this approach by "exploring other approaches, such as reasoning models, to obtain feature recommendations."
- **Why unresolved**: The current study relied on GPT-4o via a web app, which generates recommendations based on pattern matching and RAG. It is unknown if models designed for explicit logic chains would generate fewer hallucinations or more mathematically relevant feature combinations.
- **What evidence would resolve it**: A comparative experiment running the proposed pipeline using a reasoning-focused LLM (e.g., O1) versus a generative model (e.g., GPT-4o) on the same datasets, measuring the "survival rate" of LLM-suggested features in the final GP model.

### Open Question 3
- **Question**: Does the prevalence of domain-specific knowledge in an LLM's training data correlate with the magnitude of performance improvement in feature engineering?
- **Basis in paper**: [inferred] The authors note that consistent improvements were only seen in one-third of the datasets (CSS, PM, IM10) and hypothesize this is "possibly due to problems being easily explored by LLMs," suggesting the LLM performed better on "well-studied problems."
- **Why unresolved**: The paper does not quantify the "commonality" of the datasets in public literature. It is unclear if the lack of improvement in other datasets (like Istanbul Stocks) was due to the LLM lacking specific domain knowledge or the features being inherently non-combinatorial.
- **What evidence would resolve it**: An analysis correlating the frequency of dataset-specific terminology in the LLM's training corpus (or general web search indexes) against the performance gain (RMSE reduction) observed in the proposed pipeline.

### Open Question 4
- **Question**: To what extent does the exclusion of dataset samples from LLM prompts limit the quality of feature construction compared to data-exposed methods?
- **Basis in paper**: [inferred] The authors acknowledge that "by providing the datasets to the LLM, we can obtain better results... something not feasible in our pipeline," yet they do not quantify this trade-off against their privacy-preserving constraint.
- **Why unresolved**: While the privacy constraint is necessary, the performance cost of using "Feature Name Only" prompting versus "Sampled Data" prompting remains unmeasured.
- **What evidence would resolve it**: A benchmark comparing the test performance of models generated by the privacy-preserving method against a non-private method where the LLM receives a statistical summary or a small sample of the data rows.

## Limitations

- The LLM-based feature construction showed improvements in only 3 out of 11 datasets, with no consistent performance gains across all tasks
- The approach appears most effective for well-studied problems with interpretable feature names, while failing to improve results for more domain-specific tasks
- The method's effectiveness heavily depends on the quality of feature names provided to the LLM - datasets with poorly documented features were excluded entirely
- While claiming data privacy benefits, the paper doesn't address potential privacy concerns from the LLM provider side or evaluate the computational overhead of querying LLMs at scale

## Confidence

- **High Confidence**: The GP-based feature engineering pipeline (M3GP/M6GP) performs as expected, with statistically significant improvements in test performance and consistent convergence acceleration when seeded with LLM features. The multi-objective optimization in M6GP successfully reduces model size while maintaining performance.
- **Medium Confidence**: The mechanism by which LLMs generate useful feature combinations is plausible but not fully validated. The paper shows LLMs work well for some datasets but doesn't provide systematic evaluation of why certain feature recommendations succeed or fail.
- **Low Confidence**: The claimed data privacy benefits and computational cost savings. While the method doesn't expose dataset samples to LLMs, the overall computational overhead (LLM queries + GP evolution) and potential privacy risks from LLM providers are not thoroughly examined.

## Next Checks

1. **Dataset diversity test**: Apply the method to a broader range of datasets with varying feature interpretability, including synthetic datasets where ground truth feature relationships are known. This would validate whether improvements are task-specific or generalizable.

2. **Prompt engineering ablation**: Systematically vary the LLM prompts (single vs. two-prompt approach, different prompt templates) across all datasets to quantify the impact of prompt design on feature recommendation quality and consistency.

3. **Privacy risk assessment**: Evaluate the actual data privacy implications by testing whether LLM-generated features could leak information about the original dataset, and measure the total computational cost including LLM API calls versus traditional feature engineering approaches.