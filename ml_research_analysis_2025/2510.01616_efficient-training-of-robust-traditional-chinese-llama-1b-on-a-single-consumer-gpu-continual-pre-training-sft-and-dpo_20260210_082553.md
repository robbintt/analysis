---
ver: rpa2
title: 'Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer
  GPU: Continual Pre-training, SFT, and DPO'
arxiv_id: '2510.01616'
source_url: https://arxiv.org/abs/2510.01616
tags:
- chinese
- language
- traditional
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PureTC-1B, a three-stage pipeline that improves
  Traditional Chinese (TC) linguistic stability in small language models by reducing
  token-level code-switching. The method uses parameter-efficient LoRA adapters to
  combine Continual Pre-training (CPT) on TC data, Supervised Fine-Tuning (SFT) on
  instruction tasks, and Direct Preference Optimization (DPO) to prefer TC outputs.
---

# Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO

## Quick Facts
- arXiv ID: 2510.01616
- Source URL: https://arxiv.org/abs/2510.01616
- Reference count: 0
- Primary result: PureTC-1B reduces non-TC output tokens by 51.3% relative and increases TC pass rate from 10.4% to 30.3% on 660-prompt benchmark

## Executive Summary
This paper introduces PureTC-1B, a three-stage pipeline that improves Traditional Chinese (TC) linguistic stability in small language models by reducing token-level code-switching. The method uses parameter-efficient LoRA adapters to combine Continual Pre-training (CPT) on TC data, Supervised Fine-Tuning (SFT) on instruction tasks, and Direct Preference Optimization (DPO) to prefer TC outputs. On a 660-prompt benchmark under fixed decoding, PureTC-1B achieves a 51.3% relative reduction in non-TC output tokens (OLR) and raises the Pass@TC success rate from 10.4% to 30.3%. On a Named Entity Translation task, it reduces incorrect-language tokens by 77.2% versus Llama-3B and 57.2% versus Qwen-1.5B. The pipeline is adapter-only, hardware-friendly, and effective at stabilizing TC generation in 1B-class models.

## Method Summary
PureTC-1B is a three-stage pipeline designed to stabilize Traditional Chinese generation in small language models. It begins with Continual Pre-training (CPT) on a TC-specific corpus using LoRA adapters, followed by Supervised Fine-Tuning (SFT) on TC-aligned instruction data, and concludes with Direct Preference Optimization (DPO) to reinforce preference for TC outputs. The approach is entirely adapter-based, enabling training on a single consumer GPU without modifying the base model weights.

## Key Results
- 51.3% relative reduction in non-TC output tokens (OLR) on 660-prompt benchmark
- Pass@TC success rate increases from 10.4% to 30.3%
- 77.2% fewer incorrect-language tokens on Named Entity Translation vs Llama-3B; 57.2% vs Qwen-1.5B

## Why This Works (Mechanism)
The pipeline works by progressively aligning the model to TC linguistic patterns at three levels: CPT grounds the tokenizer and subword distribution in TC data, SFT adapts the model to TC-aware instruction following, and DPO fine-tunes reward signals to prefer TC outputs over code-switched or non-TC text. LoRA adapters enable these stages without modifying base model weights, making the approach computationally efficient.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Enables efficient parameter updates by decomposing weight updates into low-rank matrices; needed for single-GPU training and model preservation
  - Quick check: Verify rank selection and adapter dimension don't degrade inference speed
- **Continual Pre-training (CPT)**: Extends base model training on domain-specific data to realign subword tokenization; needed to reduce out-of-distribution drift
  - Quick check: Monitor perplexity drop on held-out TC corpus
- **Direct Preference Optimization (DPO)**: Optimizes policy to prefer TC outputs based on pairwise preference data; needed to encode output preference without explicit reward modeling
  - Quick check: Confirm preference dataset balance between TC and non-TC outputs
- **Code-switching metrics (OLR, Pass@TC)**: Quantify linguistic stability in multilingual contexts; needed for objective evaluation of TC output quality
  - Quick check: Validate metric computation aligns with paper definitions
- **Fixed decoding evaluation**: Ensures reproducible, deterministic output for fair comparison; needed to isolate model behavior from sampling variance
  - Quick check: Confirm no randomness in decoding setup across runs
- **Adapter-only fine-tuning**: Preserves base model and reduces memory; needed for practical deployment on limited hardware
  - Quick check: Compare adapter parameter count to full model size

## Architecture Onboarding
- **Component map**: Base LLaMA-1B -> CPT LoRA -> SFT LoRA -> DPO LoRA
- **Critical path**: CPT (TC grounding) → SFT (instruction alignment) → DPO (preference shaping)
- **Design tradeoffs**: Adapter-only fine-tuning saves memory and preserves base model but may limit expressivity compared to full fine-tuning
- **Failure signatures**: Overfitting to narrow instruction set, preference collapse in DPO, or TC subword misalignment after CPT
- **First experiments**: 1) Run CPT and measure perplexity on TC corpus, 2) Apply SFT and test on instruction set, 3) Run DPO and compare Pass@TC before/after

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of external validation on diverse TC corpora beyond the fixed 660-prompt set
- Absence of systematic ablation study on individual stage contributions
- Reliance on single-decoding settings may not reflect real-world sampling behavior

## Confidence
- **High**: Core claim that PureTC-1B reduces non-TC output tokens and improves TC stability, supported by consistent benchmark results
- **Medium**: Hardware efficiency claim, due to limited technical detail on resource utilization
- **Low**: General applicability to other languages or model sizes, as no cross-linguistic or scale experiments are reported

## Next Checks
1. Replicate the pipeline on a held-out TC corpus not seen during fine-tuning to assess generalization
2. Perform an ablation study isolating the effects of CPT, SFT, and DPO stages on TC output stability
3. Evaluate model robustness under varied decoding strategies (e.g., sampling, temperature scaling) to ensure stability beyond fixed decoding