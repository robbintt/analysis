---
ver: rpa2
title: Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on Norwegian
  Dialectal Slot and Intent Detection
arxiv_id: '2501.03870'
source_url: https://arxiv.org/abs/2501.03870
tags:
- data
- norwegian
- training
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates improving slot and intent detection for
  Norwegian dialects, a challenging low-resource setting. The authors compare several
  strategies: fine-tuning on English or Norwegian data, injecting character-level
  noise, training on auxiliary tasks (dialect identification, POS tagging, dependency
  parsing, or NER), and assembling layers from models trained on different languages/tasks.'
---

# Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on Norwegian Dialectal Slot and Intent Detection

## Quick Facts
- arXiv ID: 2501.03870
- Source URL: https://arxiv.org/abs/2501.03870
- Reference count: 34
- Primary result: Best models achieved 97.6% intent accuracy and 85.6% slot F1 on test set

## Executive Summary
This paper investigates improving slot and intent detection for Norwegian dialects, a challenging low-resource setting. The authors compare several strategies: fine-tuning on English or Norwegian data, injecting character-level noise, training on auxiliary tasks (dialect identification, POS tagging, dependency parsing, or NER), and assembling layers from models trained on different languages/tasks. Noise injection and Layer Swapping were most effective, with the latter producing the best slot predictions by combining layers from English and dialectal models. The study highlights the difficulty of improving slot-filling performance and shows that Layer Swapping can regularize models and improve cross-lingual robustness.

## Method Summary
The authors fine-tuned mDeBERTa v3 using MaChAmp for baselines and JointBERT for Layer Swapping experiments on the VarDial 2025 shared task for Norwegian dialectal slot and intent detection. They compared training on English SID data, machine-translated Norwegian data, and small dialectal datasets. Character-level noise (10-30% word selection with random character deletion/insertion) was injected into training data. Auxiliary tasks included dialect identification, POS tagging, dependency parsing, and NER, trained via multi-task or intermediate-task approaches. Layer Swapping involved replacing the token embeddings and first two encoder layers of an English SID expert with corresponding layers from a Norwegian SID expert trained on dialectal data.

## Key Results
- Layer Swapping combining English and Norwegian SID experts produced the best slot predictions (85.6% F1)
- Character-level noise injection improved intent accuracy, with effects varying by noise level and PLM
- Training on English data yielded better slot filling, while Norwegian data improved intent classification
- Syntax-related auxiliary tasks (POS tagging, dependency parsing) improved slot filling by 0.4-1.5 pp for English data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level noise injection improves cross-dialectal transfer for SID tasks.
- Mechanism: By randomly deleting or inserting characters in training data, models learn to be robust to spelling variations common in dialectal text. This forces the model to rely less on exact subword tokenization patterns and more on semantic understanding.
- Core assumption: Dialectal variation manifests partially through non-standard spellings that affect subword tokenization.
- Evidence anchors:
  - [abstract] "We find noise injection to be beneficial..."
  - [section 4.2] "Training on the noised data can make a model more robust to spelling variation that results in subword tokenization differences."
  - [corpus] Weak corpus evidence; similar technique mentioned in related Bavarian dialect work (Krückl et al., 2025).
- Break condition: If dialectal variation is primarily lexical or syntactic rather than orthographic, noise injection may provide limited benefit.

### Mechanism 2
- Claim: Layer Swapping combines task-specific and language-specific knowledge across models.
- Mechanism: Replacing early encoder layers (0, 1) of a task expert (English SID) with layers from a language expert (Norwegian dialect) transfers language understanding while preserving task capability. Early layers capture lower-level linguistic features, while later layers maintain task-specific decision boundaries.
- Core assumption: Different transformer layers encode qualitatively different information, with early layers capturing language-specific patterns and later layers capturing task-specific patterns.
- Evidence anchors:
  - [abstract] "Layer Swapping...worked surprisingly well; a combination of models trained on English and small amounts of dialectal data produced the most robust slot predictions."
  - [section 5.4] "We found that in general, performance decreased as later layers were reverted...reverting layers 0 and 1 slightly increased performance."
  - [corpus] Bandarkar et al. (2024) demonstrated Layer Swapping for 32-layer decoder models; this paper adapts it to 12-layer encoders.
- Break condition: If task and language representations are distributed uniformly across layers rather than hierarchically, layer swapping may not transfer effectively.

### Mechanism 3
- Claim: Intermediate-task training on syntactic tasks improves slot filling for cross-lingual transfer.
- Mechanism: Pre-training on POS tagging or dependency parsing before SID fine-tuning provides structural language knowledge that aids in identifying slot boundaries, even when transferred across languages.
- Core assumption: Syntactic knowledge generalizes better across languages than surface-level features.
- Evidence anchors:
  - [section 5.3] "For the English SID training data, the syntax-related tasks (POS tagging and dependency parsing) improve slot filling by between 0.4 and 1.5 pp."
  - [section 5.3] "For slot filling, intermediate-task training generally achieves better results than simultaneous multi-task learning."
  - [corpus] Related Bavarian study (Krückl et al., 2025) found auxiliary task effects depend on task and target.
- Break condition: If source and target languages have fundamentally different syntactic structures, or if slot boundaries do not align with syntactic constituents, this mechanism weakens.

## Foundational Learning

- **Slot and Intent Detection (SID)**: Why needed here: The core task being optimized. Intent classification identifies the user's goal (e.g., "set reminder"), while slot filling extracts relevant entities (e.g., "bread" as item). These sub-tasks respond differently to transfer methods. Quick check question: Can you explain why improving intent accuracy might not improve slot F1?

- **Subword Tokenization and Cross-lingual Transfer**: Why needed here: The paper explicitly connects noise injection benefits to subword tokenization differences. Understanding how tokenizers handle unseen words/dialects is crucial for diagnosing transfer failures. Quick check question: How might a BPE tokenizer behave differently on "Minner(sic)meg" vs. "Min mæ"?

- **Multi-task Learning vs. Intermediate-task Training**: Why needed here: The paper compares these two auxiliary task strategies. Joint training learns tasks simultaneously; intermediate-task training learns auxiliary tasks first, then transfers to the target task. Quick check question: Which approach would you try first if your auxiliary task data was much larger than your target task data?

## Architecture Onboarding

- Component map: PLM (mDeBERTa/NorBERT/ScandiBERT) → Fine-tuning Framework (MaChAmp or JointBERT) → Task Heads: Intent Classifier + Slot CRF Decoder → Layer Swapping (optional): Replace layers 0-1 from Norwegian expert

- Critical path:
  1. Establish baselines with each PLM on English, MT Norwegian, and dialectal dev data
  2. Apply noise injection (10-30% word selection) to MT Norwegian data
  3. Create experts: EnSID (English-trained) and NorSID (dialect-trained)
  4. Identify replaceable layers via reversion ablation (layers 0, 1 were most robust)
  5. Assemble model by swapping early layers from NorSID into EnSID

- Design tradeoffs:
  - Training data choice: English data → better slot filling; Norwegian data → better intent classification (Table 2 shows 80.7% vs 96.3% for ScandiBERT)
  - Noise level: Higher noise (30%) helps intent accuracy but not slot F1; optimal is PLM-dependent
  - Auxiliary tasks: Dialect ID harms performance; syntactic tasks help slot filling but not intents

- Failure signatures:
  - Low strict F1 but high loose F1 → indicates span prediction issues (likely from poor label projection in MT data)
  - Large dev-test performance gap → overfitting to small dialectal training set
  - Intent accuracy drops with auxiliary tasks → task interference in shared representations

- First 3 experiments:
  1. **Baseline sweep**: Fine-tune mDeBERTa on English SID data, MT Norwegian, and dialectal dev set (90% split). Report intent accuracy and slot F1 on held-out test.
  2. **Noise ablation**: Train on MT Norwegian with 0%, 10%, 20%, 30% character-level noise. Measure correlation between split-word ratio differences and SID performance.
  3. **Layer Swapping pilot**: Train EnSID expert on English, NorSID expert on dialectal data. Revert layers 0,1 of EnSID to pretrained weights, then replace with NorSID layers. Compare to individual experts on both English and Norwegian test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics make a model's layers suitable for assembly in Layer Swapping?
- Basis in paper: [explicit] Page 8 states, "Further analysis is needed to better understand what makes layers useful for assembling into a model, this is left for future work."
- Why unresolved: The authors successfully combined layers from an English SID expert and a Norwegian SID expert, but failed to replicate the success with other language experts (MLM-finetuned), and the Mean Absolute Value of parameters (used in prior work) did not correlate with success here.
- What evidence would resolve it: A comparative analysis of layer-wise training dynamics and parameter shifts between successful (e.g., NorSID expert) and unsuccessful (e.g., NDC MLM) experts to identify predictive metrics for assembly compatibility.

### Open Question 2
- Question: Can the utility of auxiliary tasks for Slot and Intent Detection be predicted based on the target task and training data?
- Basis in paper: [explicit] Page 9 notes, "Which auxiliary tasks help SID performance depends on the target-task training data and SID subtask... remains hard to predict, requiring further research."
- Why unresolved: The results were mixed; syntax-related tasks (POS, dependency parsing) improved slot filling for English data but had negligible or negative effects on intent detection or when using Norwegian data. Dialect identification actively hurt performance.
- What evidence would resolve it: A systematic study testing various auxiliary tasks across different language pairs and SID subtasks to derive heuristics (e.g., task similarity, data domain overlap) that forecast performance gains.

### Open Question 3
- Question: Why does reverting the first two layers of a fine-tuned expert improve performance on out-of-language data?
- Basis in paper: [inferred] Page 7 observes that reverting layers 0 and 1 of the EnSID expert increased slot F1 by 3.0 points, a result the authors found "somewhat surprising." Page 5 explicitly leaves "further analysis of layer-wise training dynamics" to future work.
- Why unresolved: The authors hypothesize that fine-tuning the initial layers on English data may be "counterproductive to robustness" by overfitting to the source language, but they did not conduct a mechanistic analysis to confirm this.
- What evidence would resolve it: Probing experiments on the embeddings of the reverted vs. fine-tuned initial layers to measure language-specific vs. task-specific information retention.

### Open Question 4
- Question: Does training on auxiliary tasks using dialectal varieties yield better performance than using standard language data?
- Basis in paper: [explicit] Page 9 states a limitation: "we were not able to further investigate the effect of including auxiliary datasets in standard vs. dialectal varieties."
- Why unresolved: The authors relied on existing treebanks (like NDT for news/government) which are in standard Bokmål/Nynorsk, leaving the potential benefit of dialect-specific syntactic or NER data unexplored.
- What evidence would resolve it: An experiment comparing the current baselines against models trained on auxiliary datasets specifically annotated for Norwegian dialects (e.g., dialectal NER).

## Limitations
- Small dialectal training set (90% split from dev data) constrains generalizability
- Reliance on machine-translated training data may introduce artifacts affecting slot boundary alignment
- Noise injection mechanism lacks rigorous ablation studies on noise type and probability tuning

## Confidence
**High Confidence**: The observation that Layer Swapping combining English and Norwegian experts produces the best slot predictions (85.6% F1) is well-supported by ablation studies and consistent with the architectural rationale. The claim that noise injection benefits intent accuracy while having limited impact on slot F1 is directly supported by Table 2 results.

**Medium Confidence**: The mechanism explaining why early layer swapping (layers 0-1) is effective relies on broader literature about transformer layer representations but lacks direct evidence from this specific model. The claim that syntactic auxiliary tasks improve slot filling is supported by the 0.4-1.5 pp improvements but depends on the quality of label projection in the MT data.

**Low Confidence**: The assertion that character-level noise injection primarily helps by making models robust to subword tokenization differences is speculative. While the paper provides a plausible mechanism, it lacks direct evidence showing the correlation between tokenization differences and performance gains. The claim that intermediate-task training is generally superior to multi-task learning for cross-lingual transfer is based on limited comparisons and may be task-dependent.

## Next Checks
1. **Noise Type Ablation**: Systematically compare character insertion vs deletion noise, varying insertion/deletion ratios (0.1, 0.2, 0.3) and measure their differential impact on subword tokenization differences and SID performance. This would validate the proposed mechanism linking noise to tokenization robustness.

2. **Layer-wise Impact Analysis**: Conduct a more granular layer swapping experiment, reverting/swapping layers 0-11 individually and in combinations, to precisely identify which layers capture language-specific vs task-specific information. This would test the hierarchical information encoding assumption.

3. **Cross-linguistic Generalization**: Apply the Layer Swapping methodology to a different language pair (e.g., English-German SID) using publicly available datasets to assess whether the approach generalizes beyond Norwegian dialects and mDeBERTa architecture.