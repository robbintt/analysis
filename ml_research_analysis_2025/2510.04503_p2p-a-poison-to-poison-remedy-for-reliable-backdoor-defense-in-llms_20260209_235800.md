---
ver: rpa2
title: 'P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs'
arxiv_id: '2510.04503'
source_url: https://arxiv.org/abs/2510.04503
tags:
- backdoor
- algorithm
- attacks
- attack
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large language models
  (LLMs) to data-poisoning backdoor attacks during fine-tuning, where models can be
  manipulated to produce undesired outputs when specific triggers are present. The
  proposed Poison-to-Poison (P2P) algorithm defends against such attacks by injecting
  benign triggers with safe alternative labels into training samples and fine-tuning
  using prompt-based learning.
---

# P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs

## Quick Facts
- arXiv ID: 2510.04503
- Source URL: https://arxiv.org/abs/2510.04503
- Reference count: 28
- This paper proposes a defense mechanism that injects benign triggers with safe alternative labels to suppress malicious backdoors in LLMs.

## Executive Summary
This paper addresses the vulnerability of large language models to data-poisoning backdoor attacks during fine-tuning. The proposed Poison-to-Poison (P2P) algorithm defends against such attacks by injecting benign triggers with safe alternative labels into training samples and fine-tuning using prompt-based learning. This re-poisoning strategy forces the model to associate trigger-induced representations with safe outputs, thereby overriding the effects of malicious triggers. Extensive experiments on classification, mathematical reasoning, and summary generation tasks demonstrate that P2P significantly reduces attack success rates while maintaining task performance.

## Method Summary
P2P works by sampling a subset (20-30%) of the training data, injecting benign triggers into these samples, and mapping their labels to an extended label space (h(i)=i+n). The model is then fine-tuned using LoRA with these re-poisoned samples, treating the benign triggers as prompts. At inference, the benign trigger is prepended to inputs and predictions are restricted to the extended label space. This forces the model to associate trigger patterns with safe outputs, suppressing malicious backdoor effects while maintaining clean accuracy.

## Key Results
- P2P reduces attack success rates from nearly 100% to below 10% across various attack methods
- The defense maintains or improves clean accuracy on sentiment analysis tasks
- The approach works effectively on classification, mathematical reasoning, and summary generation tasks
- Larger models maintain clean accuracy better under defense while requiring more compute for LoRA updates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting a known, benign backdoor into the training data (re-poisoning) can suppress the influence of unknown malicious backdoors.
- **Mechanism:** The algorithm creates a subset of training samples containing a "benign trigger" mapped to a safe, alternative label space. During fine-tuning, the model is forced to learn a strong association between this benign trigger and the secure output. Because the model's capacity to learn trigger-based shortcuts is finite, the benign trigger dominates the gradient updates, effectively overwriting or overshadowing the malicious trigger's representation.
- **Core assumption:** The model will prioritize the actively reinforced benign backdoor association over the latent malicious one during the fine-tuning window.
- **Evidence anchors:** [abstract] "injects benign triggers with safe alternative labels... forcing the model to associate trigger-induced representations with secure outputs, suppressing malicious backdoor effects."
- **Break condition:** If the malicious trigger is semantically or structurally similar to the benign trigger in a way that causes interference rather than override, or if the learning rate is too low to override the existing weights, the defense fails.

### Mechanism 2
- **Claim:** Treating the benign trigger as a "prompt" allows for controllable steering of the model's output distribution during inference.
- **Mechanism:** The benign trigger is not just a static token; it is integrated into the input as a prompt (e.g., `x' = τ(p+x)`). By training the model with this specific prompt-token configuration via prompt-based learning, the model learns to condition its output on the presence of this prompt. At inference, this prompt forces the model into a "safe mode" where it predicts from the alternative label space, bypassing the malicious behavior.
- **Core assumption:** The model exhibits strong prompt-following capabilities and the prompt representation is sufficiently distinct from the malicious trigger to allow independent manipulation.
- **Evidence anchors:** [abstract] "fine-tunes the model on this re-poisoned dataset by leveraging prompt-based learning."
- **Break condition:** If the model ignores the prompt instruction due to conflicting strong priors or if the prompt is incorrectly verbalized, the output will revert to the malicious label.

### Mechanism 3
- **Claim:** Remapping the label space (e.g., mapping class $i$ to $i+n$) during training creates a mutually exclusive prediction path for trigger inputs.
- **Mechanism:** For classification tasks, the label mapping function $h(\cdot)$ assigns trigger-embedded samples to a new label (e.g., $h(i) = i+n$). During inference, the model's prediction is restricted to this extended label space. This ensures that even if the malicious trigger activates, its target label is outside the permissible output set defined by the current (benign) prompt context.
- **Core assumption:** The alternative label space is sufficiently isolated from the original space to prevent overlap or confusion in the model's classifier head.
- **Evidence anchors:** [section 4.2] "maps the original label y to a new target label y'... we only consider predictions in the extended label space."
- **Break condition:** In generation tasks (rather than classification), the "label space" is a text prefix. If the generation task is unconstrained and the model is highly creative, it may generate the malicious payload despite the prefix constraint.

## Foundational Learning

- **Concept: Prompt-based Learning (Soft Prompts/Verbalizers)**
  - **Why needed here:** P2P relies on optimizing the label space and input triggers as prompts. Understanding how verbalizers map hidden states to specific tokens is critical for implementing the label remapping ($h(\cdot)$).
  - **Quick check question:** Can you explain how a verbalizer connects the continuous output of an LLM to a discrete class label in prompt learning?

- **Concept: Data Poisoning Backdoor Attacks (BadNets, AddSent)**
  - **Why needed here:** To defend against the threat, one must understand the threat model—specifically how triggers (rare characters, sentences) create shortcuts in the model's decision boundary.
  - **Quick check question:** In a BadNets attack, does the model learn the semantic content of the trigger or simply the statistical correlation between the trigger pattern and the target label?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The paper utilizes LoRA for fine-tuning. Understanding low-rank adaptation is necessary to replicate the training setup and manage GPU memory constraints.
  - **Quick check question:** When using LoRA, which weights are frozen, and which are updated during the backpropagation step?

## Architecture Onboarding

- **Component map:**
  1. **Dataset Reconstructor:** Iterates through training data $D$, selects a subset $S$ (ratio $\rho$), and applies the benign trigger $\delta$ and label mapping $h(\cdot)$.
  2. **LoRA Fine-Tuner:** Wraps the base LLM (LLaMA/Qwen) and trains on the reconstructed dataset $D^*$ using cross-entropy loss on the prompt-driven inputs.
  3. **Inference Wrapper:** A mandatory wrapper that injects the benign prompt $p$ into every user input and restricts the output decoding to the alternative label space.

- **Critical path:**
  1. Constructing the benign backdoor dataset (Error here propagates to training).
  2. Fine-tuning convergence where "Clean Accuracy" (CA) remains stable while "Attack Success Rate" (ASR) drops.
  3. **Deployment:** Ensuring the inference wrapper is *never* bypassed, as the model is now dependent on the benign trigger for safe operation.

- **Design tradeoffs:**
  - **Benign Sample Ratio ($\rho$):** Higher ratios (e.g., 0.3) improve defense robustness but risk contaminating the model's understanding of the original task if the trigger is intrusive.
  - **Trigger Visibility:** Using rare tokens (BadNets style) is easier to defend but less stealthy; using syntactic triggers is stealthier but harder to override.
  - **Model Size:** The paper notes larger models maintain CA better under defense, but require more compute for the LoRA updates.

- **Failure signatures:**
  - **High ASR (>20%) with High CA:** The benign trigger injection ratio was too low, or the learning rate was insufficient to override the malicious backdoor.
  - **Low CA (<80%) with Low ASR:** The benign backdoor was too aggressive (over-poisoning), degrading the model's feature extraction for clean data.
  - **Inference Mismatch:** The model outputs gibberish or the original malicious label. This indicates the inference wrapper failed to inject the benign prompt or the verbalizer mapping is misaligned with the training $h(\cdot)$.

- **First 3 experiments:**
  1. **Baseline Verification:** Train a Qwen-3 model on SST-2 poisoned with BadNets (2% ratio). Verify that ASR is $\approx 100\%$ and CA is high to confirm the backdoor exists.
  2. **Hyperparameter Sweep (Ratio $\rho$):** Apply P2P with benign sample ratios of 0.1, 0.2, and 0.3 on the poisoned model. Plot ASR vs. CA to find the "sweet spot" (likely 0.2-0.3 as per paper).
  3. **Inference Ablation:** Run inference on the defended model *without* the benign trigger prepended. Expect high ASR (attack succeeds) to prove the defense relies on the trigger's presence at runtime.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the P2P defense mechanism be adapted for zero-shot scenarios without requiring the injection of benign triggers into the input during the inference stage?
  - **Basis in paper:** [explicit] The authors state in the "Limitations" section that "during inference, benign triggers must be added to the input, which restricts its applicability in zero-shot scenarios."
  - **Why unresolved:** The current methodology relies on the presence of the benign trigger to steer the output into the secure label space, making the defense mechanism inoperable if the user does not provide the specific trigger during inference.
  - **What evidence would resolve it:** A modified framework where the defensive behavior is internalized into the model weights or system prompt, allowing the model to resist attacks in a standard zero-shot setting without inference-time input modification.

- **Open Question 2:** What are the specific security risks associated with the "reserved samples" used in the dataset reconstruction phase, and how can they be mitigated?
  - **Basis in paper:** [explicit] The "Limitations" section notes that "reserved samples may entail potential security risks, which necessitate further investigation."
  - **Why unresolved:** While P2P uses these samples to build benign backdoors, the paper does not analyze whether the process of selecting or modifying these subsets introduces new vulnerabilities, such as data leakage or susceptibility to adaptive attacks targeting the defense mechanism itself.
  - **What evidence would resolve it:** A comprehensive security analysis or ablation study identifying failure modes specifically related to the construction and deployment of the reserved sample subset.

- **Open Question 3:** How effectively does the P2P algorithm generalize to pure vision models and more complex multimodal architectures beyond the text-centric or text-image classification tasks tested?
  - **Basis in paper:** [explicit] The authors explicitly list as a limitation that the "generalization capability requires further validation on more vision and multimodal models."
  - **Why unresolved:** The current study focuses heavily on LLMs and includes only one multimodal experiment (Hateful Memes), leaving the efficacy of Poison-to-Poison on large-scale vision transformers or video-audio models unverified.
  - **What evidence would resolve it:** Experimental results applying the P2P methodology to state-of-the-art vision models (e.g., ViT) and complex multimodal systems (e.g., video understanding) to verify if the label-space expansion defense holds true across modalities.

## Limitations

- The defense mechanism fundamentally requires the benign trigger to be present at inference time, creating a dependency that could itself become a vulnerability if the trigger injection fails or is blocked.
- The paper's experimental validation relies on controlled attack settings with known trigger types, which may not reflect real-world threat scenarios where malicious triggers are more sophisticated and adaptive.
- The paper does not thoroughly explore the performance degradation that might occur when the benign trigger is accidentally triggered on clean inputs, potentially causing false positives in the alternative label space.

## Confidence

- **High Confidence** in the core mechanism showing that benign re-poisoning can suppress malicious backdoors in controlled experimental settings.
- **Medium Confidence** in the scalability and robustness of the approach across diverse real-world scenarios.
- **Low Confidence** in the practical deployment implications, particularly regarding the mandatory inference wrapper requirement and the potential for user experience degradation.

## Next Checks

1. **Adaptive Attack Resistance Test:** Design an adaptive attacker that observes the P2P defense mechanism and attempts to create triggers that are semantically similar to the benign trigger but still achieve the malicious goal. Evaluate whether the defense maintains its effectiveness under this more sophisticated threat model.

2. **Trigger Accident Rate Measurement:** Conduct a study measuring how often the benign trigger is accidentally triggered in normal conversation or on clean inputs across different domains and user populations. Quantify the false positive rate and its impact on user experience and task performance.

3. **Zero-Shot Transfer Evaluation:** Test whether models defended with P2P maintain their backdoor resistance when deployed in zero-shot or few-shot scenarios where the benign trigger is not actively reinforced through fine-tuning. This would validate whether the defense creates lasting changes to the model's internal representations or merely relies on short-term gradient dominance.