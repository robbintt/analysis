---
ver: rpa2
title: Haphazard Inputs as Images in Online Learning
arxiv_id: '2504.02912'
source_url: https://arxiv.org/abs/2504.02912
tags:
- features
- data
- learning
- haphazard
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of handling haphazard inputs in
  online learning, where data arrives with varying dimensions and inconsistent features.
  The proposed method, HI2, transforms the varying feature space into fixed-dimension
  image representations on the fly, allowing the use of any vision-based models.
---

# Haphazard Inputs as Images in Online Learning

## Quick Facts
- arXiv ID: 2504.02912
- Source URL: https://arxiv.org/abs/2504.02912
- Authors: Rohit Agarwal; Aryan Dessai; Arif Ahmed Sekh; Krishna Agarwal; Alexander Horsch; Dilip K. Prasad
- Reference count: 40
- Key outcome: HI2 transforms varying feature space into fixed-dimension image representations on the fly, allowing the use of any vision-based models.

## Executive Summary
This paper addresses the challenge of handling haphazard inputs in online learning, where data arrives with varying dimensions and inconsistent features. The proposed HI2 method transforms the varying feature space into fixed-dimension image representations on the fly, allowing the use of any vision-based models. The approach involves converting numerical data into bar plot images, where each feature is uniquely identified by a color, and the normalized feature values are represented as bar heights. The image representation can seamlessly handle missing features, making it suitable for real-time streaming applications where the feature space is dynamic and unpredictable.

## Method Summary
HI2 addresses haphazard inputs by converting variable-dimension feature vectors into fixed-size images through a color-coded bar plot representation. Each feature receives a unique color, and observed features are plotted as bars with heights proportional to normalized values. The method employs streaming Z-score normalization with clipping to handle outliers, and dynamically calculates bar geometry based on the number of observed features. The resulting 224x224x3 tensor is fed into a pre-trained vision model (ResNet-34) for classification. The entire pipeline operates in an online learning setting where each instance is processed sequentially, making predictions and updating model parameters in real-time without storing historical data.

## Key Results
- HI2 achieves superior balanced accuracy (67.13%) on magic04 dataset compared to baselines
- Z-score normalization outperforms Min-Max normalization, particularly in handling outliers
- ResNet-34 architecture outperforms ViT-Small on the tested datasets
- Method handles missing features seamlessly through selective plotting of observed features

## Why This Works (Mechanism)

### Mechanism 1: Feature-Color Mapping for Visual Identification
The method assigns a unique, fixed color to each feature, allowing vision models to track and differentiate features across instances. The visual model learns to associate specific color patterns and spatial relationships with class labels, treating the presence or absence of a bar of a certain color as part of the input signal. This assumes the vision model can learn robust associations between feature-color identities and target variables from the 2D spatial layout.

### Mechanism 2: Dimensionality Consolidation via Image Transformation
The problem of variable feature dimensions is solved by converting 1D, variable-length feature vectors into 2D, fixed-size images. This transformation allows standard pre-trained vision models to be used, as they require fixed input dimensions. The bar width and spacing are dynamically calculated based on observed features, ensuring the image is always full and contains the essential discriminative information for classification.

### Mechanism 3: Streaming Normalization for Visual Consistency
Z-score streaming normalization is critical for rendering meaningful bar plots and stabilizing model training. Raw feature values with vastly different ranges could cause a single feature to dominate the visualization. Z-score normalization brings all features to comparable scale centered around zero, while clipping prevents outliers from skewing the visual representation. This assumes relative magnitude compared to historical distribution is more stable than raw values.

## Foundational Learning

- **Online Learning vs. Batch Learning**: The model processes one instance at a time, predicting and updating without seeing all data. The entire solution is built for this constraint. *Quick check: If you could store the entire dataset and make multiple passes, would streaming Z-score still be optimal?*

- **Fixed vs. Varying Feature Space**: Standard models require fixed inputs, but "haphazard inputs" have dynamic feature counts. Understanding this mismatch is key to appreciating the image mapping solution. *Quick check: Can a neural network with 100 input neurons directly accept an 85-feature input vector? Why or why not?*

- **Pre-trained Models and Transfer Learning**: HI2 leverages pre-trained vision models that already know how to identify edges, colors, and shapes. The paper assumes these models can learn to interpret synthetic bar charts. *Quick check: Why would a model trained on natural images classify synthetic bar charts of sensor data?*

## Architecture Onboarding

- **Component map**: Haphazard Input Stream -> Feature Tracker -> Image Transformer -> Vision Classifier -> Online Updater
- **Critical path**: Feature-Color Mapping must assign unique colors immediately; Streaming Normalization must update running statistics before normalization; Tensor-based Image Generation must be fast enough for online setting; Parameter Update must use ground truth label after each prediction
- **Design tradeoffs**: Bar Graph chosen over pie charts (lose magnitude) and line graphs (require temporal storage); ResNet vs. ViT comparison shows ResNet-34 performs better; Z-score preferred over Min-Max for outlier robustness
- **Failure signatures**: Empty or garbage images from corrupted normalization stats lead to random performance; Performance collapse on new features if model cannot generalize; Slow inference if image generation is not optimized
- **First 3 experiments**: Replicate on magic04 dataset to confirm ~67.13% balanced accuracy; Ablate normalization by using Min-Max instead of Z-score to observe performance drop; Swap ResNet-34 backbone with MobileNetV2 to test model-agnostic claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the image-based representation be modified to preserve temporal dependencies in haphazard input streams?
- **Basis**: The current bar graph representation does not preserve temporal information, and future work should focus on this limitation.
- **Why unresolved**: The current method treats each time instance as a static, independent image, severing sequential links between data points critical in online streaming analysis.
- **What evidence would resolve it**: A modified HI2 architecture incorporating temporal context (e.g., using Recurrent Neural Networks or 3D CNNs) demonstrating improved performance on time-dependent streaming datasets.

### Open Question 2
- **Question**: Can the HI2 approach be optimized to outperform classical models on highly imbalanced datasets?
- **Basis**: On the highly imbalanced a8a dataset, HI2 underperformed compared to classical methods like FAE or OLVF, identifying class imbalance as a specific area for improvement.
- **Why unresolved**: While the method leverages powerful deep learning models, it apparently inherits their potential weaknesses regarding extreme class imbalance.
- **What evidence would resolve it**: Integration of class-balancing techniques (e.g., weighted losses or oversampling within the vision model) allowing HI2 to statistically surpass classical baselines on datasets with imbalance ratios greater than 75%.

### Open Question 3
- **Question**: What is the performance degradation when the number of observed features at a single time instance exceeds the resolution of the fixed-dimension image canvas?
- **Basis**: For large numbers of observed features, the method constructs larger canvases and reshapes to 224x224, admitting that reshaping may lead to information loss.
- **Why unresolved**: The paper assumes observed features are sparse, but if a system suddenly transmits dense vectors (e.g., 1000+ simultaneous readings), downscaling required to fit vision model input may destroy visual separability.
- **What evidence would resolve it**: A scalability analysis measuring classification accuracy as observed feature count per instance increases from tens to thousands, quantifying loss from image resizing.

## Limitations
- Color discrimination may fail with extremely large feature spaces where visual separation becomes impossible
- Streaming normalization may struggle with sudden concept drift where feature distributions change rapidly
- Performance is untested on multi-class problems beyond binary classification

## Confidence

- **High Confidence**: The core mechanism of transforming variable-dimension inputs into fixed-size images works as described, given the clear mathematical formulation and ablation studies supporting normalization choices
- **Medium Confidence**: Superiority over baselines is demonstrated, but comparison only covers three specific methods and doesn't address more sophisticated online learning approaches
- **Low Confidence**: The claim that "any vision-based model" can be used is not thoroughly validated beyond ResNet-34 vs ViT-Small comparison, and performance may vary significantly with different model architectures

## Next Checks
1. **Robustness to Feature Order Permutation**: Test whether shuffling feature order between instances while keeping values constant affects classification performance to reveal dependence on spatial coherence assumptions
2. **Scalability Test**: Evaluate performance as feature count increases beyond tested datasets (e.g., synthetic data with 1000+ features) to identify breaking points in color discrimination and bar visibility
3. **Multi-class Extension**: Apply the method to multi-class classification problems (e.g., MNIST with missing pixels simulated as missing features) to validate claims beyond binary classification