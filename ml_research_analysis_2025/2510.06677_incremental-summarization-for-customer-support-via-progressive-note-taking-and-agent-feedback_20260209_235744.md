---
ver: rpa2
title: Incremental Summarization for Customer Support via Progressive Note-Taking
  and Agent Feedback
arxiv_id: '2510.06677'
source_url: https://arxiv.org/abs/2510.06677
tags:
- agent
- summary
- customer
- notes
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an incremental summarization system for customer
  support agents that generates concise bullet notes during conversations to reduce
  context-switching effort. The approach combines a fine-tuned Mixtral-8x7B model
  with a DeBERTa-based classifier to filter trivial content, while agent edits provide
  continuous feedback for model refinement.
---

# Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback

## Quick Facts
- **arXiv ID:** 2510.06677
- **Source URL:** https://arxiv.org/abs/2510.06677
- **Reference count:** 40
- **Primary result:** 3% reduction in average case handling time and up to 9% for complex cases with agent satisfaction scores exceeding 80%

## Executive Summary
This paper introduces an incremental summarization system for customer support agents that generates concise bullet notes during conversations to reduce context-switching effort. The approach combines a fine-tuned Mixtral-8x7B model with a DeBERTa-based classifier to filter trivial content, while agent edits provide continuous feedback for model refinement. Deployed in production, the system achieved a 3% reduction in average case handling time and up to 9% for complex cases, with agent satisfaction scores exceeding 80%. These results demonstrate that real-time incremental summarization with live human feedback effectively enhances both summary quality and agent productivity at scale.

## Method Summary
The system employs piecewise incremental generation where the LLM outputs only new bullets per turn while preserving prior (edited) content as prefix context. A DeBERTa classifier filters candidate bullets to retain only high-value content categories, reducing summary length by ~25%. Agent edits serve dual roles: immediate context updates for subsequent generations and offline training data for preference alignment via SFT and ORPO. The fine-tuned Mixtral-8x7B model operates on 2Ã—A100 GPUs with p50 latency of 600ms, while the DeBERTa classifier runs on A10G with p50 latency of 20ms.

## Key Results
- 3% reduction in average case handling time and up to 9% for complex cases
- Agent satisfaction scores exceeding 80% in production deployment
- Mixtral-FB (with feedback) achieved completeness score of 0.842 vs 0.824 for Mixtral-NF (no feedback)

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Incremental Generation with State Preservation
Generating only new bullets per turn while preserving prior (edited) content reduces context-switching more effectively than bulk or chunk-based summarization. The LLM receives prior accepted bullets as prefix context and is instructed to output only incremental updates. Agent edits are immediately injected into subsequent prompts, so corrections persist rather than being overwritten.

### Mechanism 2: Two-Stage Relevance Filtering for Conciseness
A downstream classifier trained on domain-specific categories improves signal-to-noise ratio without materially harming coverage. The summarization LLM proposes candidate bullets; a DeBERTa classifier retains only utterances matching five high-value categories, reducing summary length by ~25% while maintaining completeness.

### Mechanism 3: Dual-Path Human-in-the-Loop Feedback Flywheel
Immediate online feedback plus periodic offline preference alignment yields compounding quality gains beyond static SFT alone. Agent edits update the live prompt context in real time, while validated before/after edit pairs train preference alignment models to better match agent preferences.

## Foundational Learning

- **Concept: Prefix Prompting / In-Context State Passing**
  - **Why needed here:** The system must maintain coherent multi-turn summarization without reprocessing full history each turn. Prefix prompting passes prior accepted bullets as context so the model outputs only new information while respecting agent corrections.
  - **Quick check question:** Can you trace how a bullet generated at turn N is preserved (or updated) in the prompt at turn N+3 after two agent edits?

- **Concept: Preference Optimization (DPO/ORPO)**
  - **Why needed here:** Offline refinement uses curated before/after edit pairs to align model outputs with agent preferences, moving beyond supervised fine-tuning alone.
  - **Quick check question:** Given a (prompt, chosen, rejected) triplet, would you know how to prepare it for ORPO vs SFT, and why the paper uses both?

- **Concept: LLM-as-Judge Evaluation**
  - **Why needed here:** Standard metrics poorly capture factual accuracy and task utility. The paper uses GPT-4o-based evaluation with structured rubrics for completeness/truthfulness.
  - **Quick check question:** If the LLM-judge shows 15% disagreement with human auditors on a new language, how would you diagnose whether the issue is prompt design, rubric ambiguity, or model bias?

## Architecture Onboarding

- **Component map:** Mixtral-8x7B (progressive note-taking) -> DeBERTa classifier (relevance filtering) -> UI display -> Agent edit -> Updated prefix context -> Next turn generation
- **Critical path:** Real-time inference latency must stay under ~2s p95; classifier must not filter high-signal utterances; feedback flywheel depends on edit volume and quality
- **Design tradeoffs:** 8-bit quantization saves 50% GPU with ~1% quality drop; TensorRT-LLM vs vLLM shows 20% latency reduction; conciseness vs completeness tradeoff requires threshold tuning
- **Failure signatures:** Latency spike indicates GPU contention or input token growth; classifier over-filtering shows sudden drop in bullet count; preference data drift appears as retrained model regression
- **First 3 experiments:** 1) Measure end-to-end latency under peak load to confirm p95 < 2s; 2) Run A/B testing with/without classifier on 1k cases; 3) Audit 500 before/after edit pairs for quality and measure acceptance rate

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation of truthfulness via binary LLM-judge rubrics may not fully capture nuanced factual errors in complex multi-turn support conversations
- 80+ agent satisfaction score and 3% average handling time reduction lack variance reporting and confidence intervals
- Classifier performance shows moderate per-category F1 scores (0.659-0.869), suggesting potential coverage gaps

## Confidence

**High Confidence:** The piecewise incremental generation mechanism with prefix prompting is technically sound and well-supported by ablation results showing Mixtral-FB outperforming Mixtral-NF on completeness metrics.

**Medium Confidence:** The 3% average handling time reduction and 80+ agent satisfaction scores are likely valid given production deployment scale, but lack statistical variance reporting.

**Low Confidence:** Long-term effectiveness of the feedback flywheel under varying edit quality and volume remains unproven. The LLM-judge's correlation with human evaluation across different conversation types and languages is not thoroughly validated.

## Next Checks

1. **Statistical Significance Validation:** Re-analyze handling time reduction data with confidence intervals and subgroup analysis to confirm 3% average reduction is robust.

2. **Classifier Robustness Test:** Conduct A/B testing with/without classifier across 5,000 cases spanning different support domains and languages to validate the completeness tradeoff.

3. **Feedback Quality Audit:** Sample 1,000 before/after edit pairs from production logs; human-annotate whether edits genuinely improve summary quality versus reflecting personal preference or noise.