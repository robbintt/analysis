---
ver: rpa2
title: 'Classification of User Reports for Detection of Faulty Computer Components
  using NLP Models: A Case Study'
arxiv_id: '2503.16614'
source_url: https://arxiv.org/abs/2503.16614
tags:
- label
- user
- dataset
- reports
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of effectively utilizing user-generated
  textual reports to detect faulty computer components, such as CPU, memory, motherboard,
  and others. To tackle this, the authors developed a labeled dataset of 341 user
  reports and applied Natural Language Processing (NLP) models based on transformer
  architectures to classify these reports.
---

# Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study

## Quick Facts
- arXiv ID: 2503.16614
- Source URL: https://arxiv.org/abs/2503.16614
- Reference count: 2
- Primary result: Few-shot learning with MiniLM achieved 79% accuracy and F1 score for classifying user reports about faulty computer components

## Executive Summary
This study addresses the challenge of classifying user-generated textual reports to detect faulty computer components. The authors developed a labeled dataset of 341 user reports covering 8 component classes and evaluated NLP models across zero-shot, one-shot, and few-shot learning paradigms. The few-shot approach using a 6-layer MiniLM model achieved the highest performance with 79% accuracy and F1 score, demonstrating that limited training data can be effectively leveraged for this classification task.

## Method Summary
The researchers created a dataset of 341 user reports by having 32 IT professionals simulate user interactions, which were validated by an expert. The dataset was split into training/validation (70%) and held-out test (30%) sets. They evaluated three learning paradigms using transformer models: zero-shot (no training examples), one-shot (one example per class), and few-shot (23 examples per class). Models included BART, DeBERTa, and MiniLM variants, with training using AdamW optimizer (lr=0.001, 50 epochs, batch size 12 for few-shot). Performance was measured using accuracy and F1 score across 15 runs.

## Key Results
- Few-shot learning with 6-layer MiniLM achieved 79% accuracy and 79% F1 score
- Zero-shot models outperformed one-shot models despite requiring no training data
- Smaller models (6MLM with 22.7M parameters) outperformed larger variants in few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with 23 samples per class enables effective domain adaptation for fault classification
- Mechanism: Sentence transformers fine-tuned on limited domain-specific samples learn to map user-reported symptoms to component-specific semantic regions in embedding space, leveraging pre-trained linguistic knowledge while adapting class boundaries
- Core assumption: User reports contain discriminative linguistic patterns that correlate with specific faulty components
- Evidence anchors:
  - [abstract]: "The few-shot learning approach, specifically using the 6-layer MiniLM model, achieved the highest performance with an accuracy of 79% and an F1 score of 79%"
  - [section 4.2]: "In our experiments, the few-shot methods use 23 samples of each class for the retraining phase, while the one-shot approaches use only one sample. The limited number of samples for one-shot classifiers was insufficient"
  - [corpus]: Related paper "Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis" addresses similar task with LLMs, suggesting mechanism generalizes but methodology differs
- Break condition: Performance degrades if user reports contain ambiguous symptoms spanning multiple components (e.g., "screen flickers and system crashes" could implicate video card, memory, or motherboard)

### Mechanism 2
- Claim: Smaller models (6-layer MiniLM, 22.7M parameters) outperform larger counterparts when training data is limited
- Mechanism: Reduced model capacity constrains the hypothesis space, preventing memorization of noise and forcing learning of generalizable patterns; regularization through architectural simplicity
- Core assumption: The relationship between model capacity and overfitting is non-linear and depends on data regime
- Evidence anchors:
  - [section 4.2]: "Although 6MLM has fewer parameters than the others, in cases where training data is limited or noisy, a simpler model might generalize better to unseen data since it is less likely to overfit the training data"
  - [section 4.2, Table IIIc]: 6MLM achieved 79% F1 vs 12MLM's 75% despite having 22.7M vs 33.4M parameters
  - [corpus]: No direct corpus validation; related work on bug classification uses varied model sizes without systematic comparison
- Break condition: As training data increases beyond threshold (unknown, but >23 samples/class), larger models should eventually outperform smaller ones

### Mechanism 3
- Claim: Pre-trained NLI knowledge in zero-shot models outperforms minimal fine-tuning (one-shot) when parameter count is substantially larger
- Mechanism: Zero-shot models (BART 407M, DeBERTa 184M) encode rich semantic relationships from NLI pre-training that transfer to classification; one-shot sentence transformers (22.7M-278M, but mostly smaller) lack sufficient fine-tuning signal to override pre-trained knowledge
- Core assumption: NLI pre-training (predicting entailment between sentence pairs) provides transferable representations for component classification
- Evidence anchors:
  - [section 4.2]: "the zero-shot approaches seem to be better than the one-shot approaches. This behavior is explainable since the selected zero-shot models have more parameters than the other approaches, except MMP"
  - [section 3.2]: "BART Large... is pre-trained with (MNLI)... a bitext classification task dataset that predicts if one sentence entails another"
  - [corpus]: No direct corpus validation; assumption that NLI transfers to technical fault classification needs external verification
- Break condition: One-shot should surpass zero-shot when models have comparable parameter counts AND the single sample is highly representative

## Foundational Learning

- Concept: **Zero-shot vs One-shot vs Few-shot Learning Paradigms**
  - Why needed here: The paper's central comparison requires understanding how sample count per class (0, 1, 23) affects model behavior
  - Quick check question: If you increase from 23 to 100 samples per class, would you still expect the smaller 6MLM to outperform 12MLM?

- Concept: **Sentence Transformers (Siamese BERT Networks)**
  - Why needed here: Unlike token-level BERT, sentence transformers produce fixed-dimensional embeddings suitable for semantic similarity and classification
  - Quick check question: Why might sentence-level embeddings be more appropriate than token-level for classifying "My laptop screen flickers when I adjust the angle"?

- Concept: **Natural Language Inference (NLI) for Zero-shot Classification**
  - Why needed here: Understanding how NLI pre-training enables BART/DeBERTa to classify without examples
  - Quick check question: How does predicting "entailment/contradiction/neutral" between sentence pairs translate to multi-class component classification?

## Architecture Onboarding

- Component map:
  IT Professional Forms (32 experts) → Forum Scraping → Validation (dedup, label check) → Stratified 70/30 Split

- Model Families:
  Zero-shot: BART-Large (407M), DeBERTa-v3 variants (184M) — NLI pre-trained
  Few-shot: MiniLM (22.7M/33.4M), MPNet (109M-278M) — Sentence transformers

- Training: AdamW (lr=0.001, β1=0.9, β2=0.999), 50 epochs, batch=12 (few-shot) or 4 (one-shot)

- Critical path:
  1. Dataset creation with domain expert validation (8 component classes, 341 total samples)
  2. Stratified split: P1 (70%) for training/validation, P2 (30%) held-out test
  3. Model selection based on MNLI benchmark performance
  4. Fine-tuning with paradigm-specific sample counts (k=0, 1, or 23 per class)
  5. 15-run averaged evaluation on P2 with accuracy, F1, confusion matrices

- Design tradeoffs:
  - **Model size vs generalization**: 6MLM (22.7M) outperforms 12MLM (33.4M) in few-shot — smaller is better with limited data
  - **Zero-shot convenience vs accuracy**: Zero-shot requires no labeling but yields ~57% vs few-shot's 79%
  - **Data collection effort vs marginal gain**: 1→23 samples yielded +29% accuracy (50%→79%); diminishing returns expected beyond

- Failure signatures:
  - One-shot underperforming zero-shot (51% vs 57%): indicates single sample insufficient for meaningful gradient updates
  - Class confusion in matrices: some components (e.g., motherboard vs memory) share similar symptom language
  - 12MLM underperforming 6MLM: potential overfitting signal — monitor training vs validation loss curves

- First 3 experiments:
  1. **Replicate baseline**: Reproduce 6MLM few-shot (k=23) on the same dataset to validate infrastructure; target ~79% F1 (±2%)
  2. **Scaling test**: Run few-shot with k=50 and k=100 per class across 6MLM and 12MLM to identify crossover point where larger model wins
  3. **Cross-domain validation**: Test best model on held-out forum data not in original dataset to measure real-world generalization gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the classification performance compare when extending the text-based approach to audio classification integrated with smart speakers?
- Basis in paper: [explicit] Section 5 states, "we plan to extend this application to audio classification and integrate the models’ prediction to smart speakers."
- Why unresolved: The current study is restricted to textual analysis; the authors have not yet evaluated the impact of Automatic Speech Recognition (ASR) errors, background noise, or user speech patterns on the model's ability to detect faulty components.
- Evidence: An experiment evaluating the end-to-end pipeline using audio inputs from users interacting with a smart speaker, comparing the resulting F1 scores against the text-based MiniLM baseline.

### Open Question 2
- Question: Can privacy-preserving mechanisms be incorporated into the transformer models without significantly degrading the 79% accuracy achieved in the few-shot learning scenario?
- Basis in paper: [explicit] Section 5 lists as future work the aim to "incorporate privacy features into NLP models."
- Why unresolved: The current experiments utilized standard transformer architectures without specific privacy constraints (e.g., differential privacy or federated learning), leaving the trade-off between data security and model utility unexplored.
- Evidence: A comparative study measuring the accuracy drop (if any) when training the 6-layer MiniLM model using Federated Learning or Differential Privacy on the same dataset.

### Open Question 3
- Question: Does the few-shot model's performance hold when classifying unscripted reports from non-expert users compared to the reports simulated by IT professionals?
- Basis in paper: [inferred] Section 3.1 notes that 32 IT professionals "simulated the user's interaction" to build the dataset, which may introduce a distribution shift compared to real-world noisy user data.
- Why unresolved: IT professionals may use more precise terminology and syntax than general users; therefore, the 79% accuracy may not generalize to actual ambiguous or poorly described user complaints found in production environments.
- Evidence: A cross-domain evaluation where the model is tested on a separate validation set of authentic, unedited support tickets from actual consumers.

## Limitations
- Dataset size is relatively small (341 reports) for NLP tasks, raising concerns about overfitting and generalizability
- Evaluation on a single dataset without cross-validation limits understanding of model robustness
- Comparison between zero-shot and one-shot approaches confounded by parameter count differences

## Confidence

- **High confidence**: Few-shot learning (23 samples/class) outperforms one-shot learning (1 sample/class) is well-supported by experimental results
- **Medium confidence**: Smaller models (6MLM) outperform larger ones (12MLM) in few-shot scenarios is supported but may not generalize to larger datasets
- **Low confidence**: NLI pre-training directly enables effective zero-shot classification for technical fault detection is speculative without external validation

## Next Checks

1. **Dataset size sensitivity analysis**: Systematically vary the number of samples per class (k=5, 10, 23, 50, 100) and measure performance of both 6MLM and 12MLM to identify the precise crossover point where larger models begin outperforming smaller ones.

2. **Cross-dataset validation**: Evaluate the best-performing model (6MLM few-shot) on an independent corpus of user reports from different forums or time periods to measure real-world generalization and domain shift effects.

3. **Parameter-matched ablation**: Compare zero-shot performance using models with similar parameter counts to the one-shot sentence transformers (e.g., smaller BART variants) to isolate whether performance differences are due to architecture or capacity.