---
ver: rpa2
title: Procedural Game Level Design with Deep Reinforcement Learning
arxiv_id: '2510.15120'
source_url: https://arxiv.org/abs/2510.15120
tags:
- agent
- learning
- terrain
- flower
- island
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a procedural content generation (PCG) framework
  that integrates two deep reinforcement learning (DRL) agents within Unity: a hummingbird
  agent that learns to collect flowers and an island agent that generates diverse,
  context-aware flower layouts. The agents are trained using the Proximal Policy Optimization
  (PPO) algorithm.'
---

# Procedural Game Level Design with Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.15120
- Source URL: https://arxiv.org/abs/2510.15120
- Reference count: 22
- Two DRL agents trained with PPO achieve 90.2% success rate in collecting flowers across varied layouts

## Executive Summary
This paper presents a procedural content generation (PCG) framework that integrates two deep reinforcement learning (DRL) agents within Unity: a hummingbird agent that learns to collect flowers and an island agent that generates diverse, context-aware flower layouts. The agents are trained using the Proximal Policy Optimization (PPO) algorithm. The hummingbird agent receives auxiliary observations such as terrain normals, relative flower positions, and spawn parameters, enabling robust navigation and generalization. The island agent learns to place flowers based on obstacle positions, initial hummingbird state, and performance feedback, dynamically adjusting spawn radius and congestion. Experimental results show the hummingbird agent achieves a 90.2% success rate in collecting all flowers across varied layouts, with an average of 12.4 flowers collected per episode and a reward of 1.35 per step. The island agent's placement quality improves over training, demonstrating emergent behaviors and adaptability. This co-adaptive setup highlights the potential of DRL in autonomous game level design, where environment and agent evolve together to create engaging, procedurally generated content.

## Method Summary
The framework employs two interconnected DRL agents trained in a Unity environment. The hummingbird agent learns navigation and flower collection using PPO, with state inputs including terrain normals, relative flower positions, and spawn parameters. The island agent generates flower layouts based on obstacle positions, initial hummingbird state, and performance feedback, adjusting spawn radius and congestion dynamically. Both agents share a common training loop where the island agent's output serves as the environment for the hummingbird agent. The PPO algorithm provides stable learning through clipped objective functions and adaptive learning rates. The co-adaptive nature allows the island agent to evolve layouts that challenge yet remain navigable for the hummingbird agent, creating a feedback loop that enhances both content generation and agent performance.

## Key Results
- Hummingbird agent achieves 90.2% success rate in collecting all flowers across varied layouts
- Average of 12.4 flowers collected per episode with reward of 1.35 per step
- Island agent demonstrates emergent behaviors and adaptive placement quality over training

## Why This Works (Mechanism)
The framework's success stems from the co-adaptive relationship between environment generation and agent behavior. The island agent learns to create layouts that are challenging but solvable, while the hummingbird agent develops robust navigation strategies using rich auxiliary observations. The PPO algorithm provides stable learning through trust region optimization, preventing catastrophic policy updates. The feedback loop between agents creates a self-improving system where content quality and agent capability evolve in tandem. The auxiliary observations (terrain normals, relative positions, spawn parameters) give the hummingbird agent comprehensive environmental understanding, while the island agent's access to obstacle positions and performance metrics enables context-aware placement decisions.

## Foundational Learning
- Proximal Policy Optimization (PPO): Why needed - Stable policy updates without destructive learning rates; Quick check - Monitor KL divergence and reward stability
- Auxiliary observations: Why needed - Provide rich environmental context for better decision making; Quick check - Compare performance with/without each observation type
- Co-adaptive training: Why needed - Creates self-improving content generation; Quick check - Measure content quality and agent performance over training epochs

## Architecture Onboarding

Component Map: Unity Environment -> Island Agent (PPO) -> Generated Layout -> Hummingbird Agent (PPO) -> Navigation Actions -> Reward Feedback -> Island Agent

Critical Path: Layout Generation → Environment Setup → Agent Observation → Policy Execution → Reward Calculation → Policy Update

Design Tradeoffs: The framework balances computational complexity against generation quality, using a single PPO implementation for both agents rather than specialized algorithms. This reduces implementation overhead but may limit optimization for each agent's specific task. The co-adaptive approach trades training stability for emergent content quality.

Failure Signatures: Training instability may occur if the island agent generates layouts too difficult for the hummingbird agent to solve, creating a negative feedback loop. Poor hummingbird performance can cause the island agent to overcompensate with overly simple layouts. Auxiliary observation quality directly impacts navigation success - missing or noisy observations lead to suboptimal policies.

Three First Experiments:
1. Test hummingbird agent performance with individual auxiliary observations disabled to measure contribution
2. Run island agent with fixed versus adaptive spawn parameters to evaluate dynamic adjustment benefits
3. Compare co-adaptive training against pre-generated static layouts for both agent performance and content quality

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental conditions and evaluation metrics lack full detail for independent verification
- Quality and diversity of generated layouts not systematically analyzed against established PCG metrics
- Generalizability to different game genres or complex environments not validated

## Confidence
High: Framework viability and PPO implementation stability
Medium: Reported success rates and convergence properties
Low: Quality/diversity claims of generated content and cross-domain generalizability

## Next Checks
1. Conduct comparative study measuring quality and diversity of generated layouts against established PCG methods using standardized metrics like playability, novelty, and surprise scores
2. Perform ablation studies to determine contribution of each auxiliary observation to hummingbird agent performance
3. Test framework generalizability by applying to different game environments (e.g., indoor levels, 3D mazes) and evaluating performance degradation or adaptation requirements