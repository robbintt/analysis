---
ver: rpa2
title: Kolmogorov-Arnold Network for Transistor Compact Modeling
arxiv_id: '2503.15209'
source_url: https://arxiv.org/abs/2503.15209
tags:
- functions
- modeling
- transistor
- symbolic
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kolmogorov-Arnold Networks (KANs) for transistor
  compact modeling, addressing the interpretability limitations of conventional neural
  network approaches. The authors propose KAN and Fourier KAN (FKAN) architectures,
  which employ learnable activation functions instead of traditional layer weights,
  enabling both high precision and interpretability.
---

# Kolmogorov-Arnold Network for Transistor Compact Modeling

## Quick Facts
- **arXiv ID:** 2503.15209
- **Source URL:** https://arxiv.org/abs/2503.15209
- **Reference count:** 32
- **Primary result:** KAN and FKAN achieve superior prediction accuracy for critical transistor figures of merit while offering unique interpretability through symbolic formula extraction

## Executive Summary
This paper introduces Kolmogorov-Arnold Networks (KANs) and Fourier KANs (FKANs) for transistor compact modeling, addressing the interpretability limitations of conventional neural network approaches. The authors systematically evaluate KAN and FKAN architectures against industry-standard compact models and multi-layer perceptrons (MLPs) for FinFET devices. Results demonstrate that KAN achieves 0.09% error for drain current and 0.03% error for charges, while also enabling the extraction of interpretable symbolic formulas from learned data patterns. Despite challenges in capturing derivative behavior for circuit simulation, KAN represents a transformative advancement in bridging interpretability and precision in neural network-driven transistor compact modeling.

## Method Summary
The authors implement KAN and FKAN architectures using learnable univariate activation functions (B-splines and Fourier series) instead of traditional layer weights. They generate 7nm FinFET datasets using HSPICE with MOSFET Level 72 compact models, sweeping VD and VG from 0-0.82V. Training employs LBFGS for KAN (with grid refinement from G=2 to G=16) and Adam for FKAN and MLP. Loss functions incorporate value, derivative, and higher-order derivative terms. The method includes a symbolic regression algorithm to extract closed-form formulas from learned KAN structures.

## Key Results
- KAN achieves 0.09% MAPE error for drain current compared to industry-standard compact model
- KAN achieves 0.03% MAPE error for charges (QD, QS) versus BSIM reference
- Symbolic regression capability enables derivation of interpretable analytical formulas from learned data patterns

## Why This Works (Mechanism)

### Mechanism 1: Edge-Based Learnable Activation for High-Accuracy Regression
Replacing fixed node activations with learnable univariate functions (splines) on edges allows the network to approximate complex transistor I-V and C-V curves with higher precision and fewer parameters than standard MLPs. The Kolmogorov-Arnold Representation Theorem decomposes complex multivariate functions into sums of univariate functions, and by parameterizing these functions as B-splines with learnable coefficients, the network gains "adaptive non-linearity" fitting nuanced curves more closely.

### Mechanism 2: Symbolic Formula Extraction via Spline Fitting
The structure of KAN allows extraction of closed-form symbolic formulas, transforming a black-box neural network into an interpretable analytical model. Because learned functions reside on edges as splines, they can be post-processed to fit basic mathematical primitives (e.g., sin, cos, tanh). This allows the network topology to be pruned and distilled into human-readable equations revealing input dependencies.

### Mechanism 3: Derivative Waviness from Piecewise Construction
While KANs achieve low MAPE on values, their piecewise nature introduces high-frequency oscillations in higher-order derivatives, degrading performance in transient circuit simulations. KANs rely on B-splines (piecewise polynomials) and FKANs rely on sums of sine/cosine terms. While these fit the "value" curve well, the derivatives accumulate the "wiggles" inherent in connecting polynomial segments or harmonic series, causing convergence issues in SPICE.

## Foundational Learning

- **B-Splines (Basis Splines)**: KANs learn coefficients of B-spline basis functions rather than traditional weights. Understanding how control points influence curve segments is essential for diagnosing overfitting or wavy derivatives. *Quick check:* If you increase grid size G in a KAN, does the spline become more rigid or more flexible, and how does this affect overfitting risk?

- **FinFET Compact Modeling (BSIM)**: The paper benchmarks against "industry-standard compact models" (BSIM) that predict currents (ID) and terminal charges (QD, QS, QG) based on terminal voltages (VG, VD). *Quick check:* Why is predicting the derivative of current (transconductance, gm) often more critical for circuit design than predicting the current value itself?

- **Kolmogorov-Arnold Representation Theorem**: This theorem justifies the KAN architecture, positing that any multivariate continuous function can be represented as a superposition of continuous functions of one variable and addition. *Quick check:* How does this theorem fundamentally change the location of "learnable parameters" compared to the Universal Approximation Theorem used for MLPs?

## Architecture Onboarding

- **Component map:** Input Nodes (VG, VD) -> KAN Layer (matrix of edges with learnable B-spline functions φ(x)) -> Nodes (simple summation operators ∑) -> Output (predicted ID or Q)

- **Critical path:**
  1. Generate TCAD/SPICE data (VG, VD → I, Q)
  2. Apply logarithmic scaling to ID to handle dynamic range
  3. Train KAN starting with small grid (G=2) and iteratively increasing to G=16
  4. Optionally run iterative symbolic regression algorithm to extract formulas

- **Design tradeoffs:**
  - Grid Size (G) vs. Derivative Smoothness: Increasing G improves fitting accuracy but increases derivative oscillations
  - KAN vs. FKAN: KAN is more compact and interpretable; FKAN is easier to train but has larger parameter count
  - Symbolic vs. Accurate: Converting to symbolic formula often results in substantial accuracy loss

- **Failure signatures:**
  - Training Divergence: "NaN" losses during ID training requiring seed changes
  - Frequency Drift: Ring oscillator simulations show accumulated delay causing frequency mismatch
  - Derivative Oscillation: Visible "wiggles" in gm plots, specifically in saturation regions

- **First 3 experiments:**
  1. Train small MLP and KAN on ID data, compare MAPE and training stability
  2. Plot gm = ∂ID/∂VG for both models against SPICE data, look for waviness artifact
  3. Train KAN for QS and attempt standard symbolic regression, verify formula identifies QS depends primarily on VG

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the inherent waviness in the higher-order derivatives of KAN and FKAN models be eliminated to ensure convergence in transient SPICE circuit simulations?
- **Basis in paper:** [explicit] Authors identify "challenges in capturing derivative behavior" (Abstract) and note that "faulty derivative behavior" (Section IV-C) leads to significant errors in ring oscillator frequency due to the piecewise nature of spline and Fourier functions.
- **Why unresolved:** Reducing grid size to smooth derivatives significantly increases prediction error, creating an unresolved trade-off.
- **What evidence would resolve it:** Demonstration of KAN-based model producing smooth gm derivatives and successful ring oscillator simulation with frequency error <1%.

### Open Question 2
- **Question:** What modifications are required to stabilize KAN training and improve generalization for drain current (ID) modeling when using logarithmic conversion functions?
- **Basis in paper:** [explicit] Section IV-B states that KAN demonstrates "poor compatibility with the log-scale conversion function" and "inferior generalization" for ID, often resulting in training instability (NaN losses).
- **Why unresolved:** Current architecture requires seed changes to recover from failed training and struggles to generalize on test data compared to MLPs.
- **What evidence would resolve it:** Consistent training convergence across random seeds and test MAPE for ID that matches or exceeds MLP benchmarks without manual intervention.

### Open Question 3
- **Question:** Can domain-specific physics knowledge be integrated into the KAN symbolic regression process to generate compact, usable analytical formulas?
- **Basis in paper:** [inferred] Authors note that symbolic regression currently produces "convoluted" formulas or requires iterative fixing, suggesting "KANs can greatly benefit from the expertise and knowledge of conventional transistor modeling techniques" (Section IV-D).
- **Why unresolved:** Current method of fitting basic arithmetic functions to splines results in high error (91% MAPE) or overly complex equations.
- **What evidence would resolve it:** A KAN-derived symbolic formula that is both mathematically compact and maintains the reported high prediction accuracy (<0.1% error).

## Limitations

- KAN training instability for ID with NaN losses and inconsistent convergence requiring extensive hyperparameter tuning and random seed selection
- Severe trade-off between fitting accuracy and derivative smoothness that breaks circuit simulations
- Computationally expensive grid refinement strategy that may not guarantee convergence across different transistor technologies

## Confidence

- **High confidence**: MAPE comparisons between architectures on the same dataset (quantitative, reproducible)
- **Medium confidence**: Claims about KAN's superior accuracy for charges (QD, QS) where training was stable
- **Low confidence**: Claims about KAN's general applicability to transistor modeling given the severe training instability for ID and extensive manual intervention required

## Next Checks

1. Test KAN stability across multiple FinFET technology nodes (7nm, 5nm, 3nm) to verify the approach isn't dataset-specific
2. Implement automated convergence diagnostics and grid refinement to replace the manual, computationally expensive process described
3. Evaluate KAN performance on transient circuit simulations with different topologies (not just ring oscillators) to quantify the derivative waviness impact on practical design scenarios