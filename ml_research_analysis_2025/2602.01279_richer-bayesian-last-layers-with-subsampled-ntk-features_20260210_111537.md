---
ver: rpa2
title: Richer Bayesian Last Layers with Subsampled NTK Features
arxiv_id: '2602.01279'
source_url: https://arxiv.org/abs/2602.01279
tags:
- uncertainty
- bayesian
- features
- neural
- last
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Rich-BLL, a method to improve Bayesian Last
  Layer (BLL) uncertainty estimates by incorporating contributions from earlier layers
  through a low-dimensional kernel correction. The approach approximates the full
  empirical Neural Tangent Kernel (eNTK) features using a linear transformation of
  last-layer features, enabling richer uncertainty estimates at the computational
  cost of standard BLL inference.
---

# Richer Bayesian Last Layers with Subsampled NTK Features

## Quick Facts
- arXiv ID: 2602.01279
- Source URL: https://arxiv.org/abs/2602.01279
- Reference count: 40
- Primary result: Introduces Rich-BLL, a method to improve Bayesian Last Layer (BLL) uncertainty estimates by incorporating contributions from earlier layers through a low-dimensional kernel correction.

## Executive Summary
This paper presents Rich-BLL, a method that enhances the uncertainty estimates of Bayesian Last Layer (BLL) models by incorporating contributions from earlier layers through a low-dimensional kernel correction. The approach approximates the full empirical Neural Tangent Kernel (eNTK) features using a linear transformation of last-layer features, enabling richer uncertainty estimates at the computational cost of standard BLL inference. A uniform subsampling scheme further reduces computational cost while maintaining accuracy. The method provably yields more conservative uncertainty estimates than BLL and is validated across regression, contextual bandits, and image classification tasks.

## Method Summary
Rich-BLL improves upon standard Bayesian Last Layer (BLL) methods by incorporating richer feature representations from earlier layers of a neural network. It approximates the full empirical Neural Tangent Kernel (eNTK) features using a linear transformation of last-layer features, allowing for more expressive uncertainty estimates without increasing inference cost. The method introduces a uniform subsampling scheme to reduce computational cost while maintaining accuracy. Rich-BLL is designed to be a drop-in replacement for BLL, offering improved calibration and predictive performance across multiple domains including regression, contextual bandits, and image classification tasks.

## Key Results
- Rich-BLL consistently improves calibration and predictive performance compared to standard BLL across regression, contextual bandits, and image classification tasks.
- The method provides more conservative uncertainty estimates than BLL, as theoretically proven and empirically validated.
- Subsampling eNTK features reduces computational cost with minimal degradation in performance, maintaining Rich-BLL's efficiency advantage over full eNTK methods.

## Why This Works (Mechanism)
Rich-BLL works by approximating the full empirical Neural Tangent Kernel (eNTK) features using a linear transformation of last-layer features. This allows the method to capture richer representations from earlier layers without the computational cost of full eNTK computation. The uniform subsampling scheme further reduces computational burden while maintaining the quality of uncertainty estimates. By incorporating these richer features, Rich-BLL produces more conservative uncertainty estimates than standard BLL, as the additional information from earlier layers leads to a more cautious prediction when the model is less certain.

## Foundational Learning
- **Neural Tangent Kernel (NTK)**: A kernel that describes the behavior of infinitely wide neural networks during training. Why needed: NTK provides a theoretical foundation for understanding neural network training dynamics and enables kernel-based approximations of neural networks. Quick check: Can you explain how NTK relates to the Jacobian of the network with respect to its parameters?
- **Bayesian Last Layer (BLL)**: A method for Bayesian inference in neural networks by placing a prior only on the last layer weights. Why needed: BLL offers a computationally efficient way to obtain uncertainty estimates without full Bayesian inference. Quick check: How does BLL differ from full Bayesian neural network inference in terms of computational cost and expressiveness?
- **Empirical Neural Tangent Kernel (eNTK)**: The finite-width approximation of the NTK, computed using the actual neural network Jacobian. Why needed: eNTK provides a more accurate representation of the network's behavior compared to the theoretical NTK, especially for finite-width networks. Quick check: What are the computational challenges associated with computing the full eNTK for large networks?

## Architecture Onboarding
The paper focuses on supervised learning tasks where uncertainty quantification is important. Rich-BLL is designed to work with neural networks that have been pre-trained on relevant datasets. The method is particularly applicable to scenarios where standard BLL underestimates uncertainty, such as in regression tasks, contextual bandits, and image classification. Implementation requires access to intermediate layer features, which may necessitate architectural modifications or additional forward passes during training. The subsampling scheme is designed to work with any neural network architecture, though the authors note that layer selection and subsampling rates may need tuning for optimal performance.

## Open Questions the Paper Calls Out
- How does the performance of Rich-BLL scale with network depth and width?
- What is the optimal strategy for selecting which layers to include in the kernel correction?
- Can the subsampling scheme be made adaptive rather than uniform?
- How does Rich-BLL perform in out-of-distribution detection scenarios?
- What are the theoretical limits of uncertainty improvement that can be achieved through kernel corrections?

## Limitations
- Rich-BLL requires access to intermediate layer features, which may increase memory usage during training and inference.
- The performance of the method depends on the quality of the linear transformation used to approximate eNTK features, which may not capture all relevant information from earlier layers.
- Subsampling, while reducing computational cost, may lead to information loss and potentially less accurate uncertainty estimates in some scenarios.
- The method's effectiveness may vary depending on the specific neural network architecture and dataset characteristics.
- The theoretical guarantees of more conservative uncertainty estimates may not always translate to practical improvements in all domains.

## Confidence
High confidence in the technical validity of the method, given the theoretical proofs provided and the comprehensive empirical evaluation across multiple domains. The authors' approach of approximating eNTK features through a linear transformation of last-layer features is a reasonable and computationally efficient solution to the problem of enriching BLL uncertainty estimates. The subsampling scheme further enhances the method's practicality without significantly compromising performance. However, some limitations in the applicability of the theoretical guarantees to all real-world scenarios should be noted.

## Next Checks
- Verify the theoretical proofs of more conservative uncertainty estimates under various assumptions about the neural network and data distribution.
- Examine the sensitivity of Rich-BLL's performance to different subsampling rates and layer selection strategies.
- Investigate the method's behavior on out-of-distribution data and its effectiveness in detecting distribution shifts.
- Compare the computational efficiency of Rich-BLL with other uncertainty quantification methods, particularly in large-scale applications.
- Explore potential extensions of the method to other types of neural network architectures beyond those tested in the paper.