---
ver: rpa2
title: Exploring In-Context Learning for Frame-Semantic Parsing
arxiv_id: '2507.23082'
source_url: https://arxiv.org/abs/2507.23082
tags:
- frame
- task
- examples
- frames
- framenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an in-context learning framework for frame-semantic
  parsing using FrameNet data and large language models. The method generates prompts
  from frame definitions and examples to perform frame identification and semantic
  role labeling without model fine-tuning.
---

# Exploring In-Context Learning for Frame-Semantic Parsing

## Quick Facts
- **arXiv ID:** 2507.23082
- **Source URL:** https://arxiv.org/abs/2507.23082
- **Authors:** Diego Garat; Guillermo Moncecchi; Dina Wonsever
- **Reference count:** 0
- **Primary result:** In-context learning achieves 94.3% F1 for frame identification and 77.4% F1 for semantic role labeling on violent event frames without model fine-tuning.

## Executive Summary
This paper introduces an in-context learning framework for frame-semantic parsing using FrameNet data and large language models. The method generates prompts from frame definitions and examples to perform frame identification and semantic role labeling without model fine-tuning. Experiments on violent event frames show competitive performance: 94.3% F1 for frame identification and 77.4% F1 for semantic role labeling across six different LLMs. Results demonstrate that in-context learning is a practical alternative to traditional fine-tuning for domain-specific frame-semantic parsing tasks.

## Method Summary
The approach uses in-context learning with FrameNet data, where prompts contain four sections: Goal (task description), Events (frame definitions and examples), Guidelines (output formatting instructions), and Examples (demonstrations). The method tests both single-step Frame Semantic Parsing (FSP) and two-step pipeline (Frame Identification followed by Frame Semantic Role Labeling). Six violent event frames are used with 150 examples for in-context learning and 100 for evaluation. The evaluation uses strict micro F1 scores with temperature=0.01 for LLM calls.

## Key Results
- 94.3% F1 for frame identification (FI) task
- 77.4% F1 for semantic role labeling (FSRL) task
- Performance scales with shot count, plateauing around 150 shots for FSRL
- Task decoupling (FI → FSRL) improves argument extraction by preserving context capacity

## Why This Works (Mechanism)

### Mechanism 1: Schema Alignment via Definition-Example Pairing
- **Claim:** LLMs require both formal frame definitions and concrete annotation examples to map latent linguistic knowledge to the specific FrameNet schema.
- **Mechanism:** The prompt injects the FrameNet schema (definitions of Frames and Frame Elements) into the context window. The model uses this as a "reference manual." Examples then act as "calibration data," showing the model exactly how to apply the schema (e.g., how to bracket relative clauses).
- **Core assumption:** The LLM possesses sufficient latent semantic knowledge to understand the concepts (e.g., "Killing") but lacks the specific labeling convention without explicit instruction.
- **Evidence anchors:** [Section 5.1 / Table 5] Ablation shows zero-shot performance is 0.0% for the FSP task without frame info, but jumps significantly when frame info is added, and further still when examples (shots) are added.

### Mechanism 2: Context Window Optimization via Task Decoupling
- **Claim:** Splitting the parsing task into two stages (Identification → Labeling) improves argument extraction by reserving context window capacity for frame-specific details.
- **Mechanism:** In the single-step (FSP) approach, the prompt must stuff definitions and examples for all possible frames into the context, limiting the "shots" per frame. By first identifying the frame (FI), the second-stage (FSRL) prompt can be dynamically constructed using only the definition and examples for that specific detected frame, maximizing the relevance of the limited context window.
- **Core assumption:** The Frame Identification (FI) step is accurate enough that providing the correct frame definition for the wrong frame does not introduce noise (The paper reports 94.3% FI F1, making this a relatively safe assumption).
- **Evidence anchors:** [Section 3.1] "In the FSP task... the prompt must include examples for all selected frames... This limits the number of examples... In contrast, the FSRL task benefits from prior FI, allowing the prompt to be tailored specifically to the detected frames."

### Mechanism 3: Boundary Enforcement via Strict Guidelines
- **Claim:** Performance relies on constraining the LLM's generative freedom to force valid, parseable outputs matching strict span boundaries.
- **Mechanism:** The prompt explicitly defines output formatting rules (likely JSON or structured text). This forces the model to attend to token-level span boundaries rather than generating conversational approximations.
- **Core assumption:** The LLM's instruction-following capability is robust enough to adhere to the requested format consistently across different sentence structures.
- **Evidence anchors:** [Section 3.2] "The Guidelines section provides detailed instructions on how to perform the task while constraining the output format to reduce errors and facilitate downstream processing."

## Foundational Learning

- **Concept: Frame Semantics (FrameNet)**
  - **Why needed here:** The entire architecture depends on understanding that words (Lexical Units) trigger frames (scenarios) which have specific participants (Frame Elements). Without this, the prompt generation is meaningless.
  - **Quick check question:** Can you explain the difference between a Lexical Unit (Target) and a Frame Element (Argument) in the context of the "Killing" frame?

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** This paper proposes ICL as an alternative to training. You must understand that the model weights are frozen (θ is constant), and learning happens solely via the prompt context (x).
  - **Quick check question:** If you change the prompt template, does the model's knowledge about "Killing" change, or does its activation of that knowledge change?

- **Concept: Strict vs. Loose Evaluation Metrics**
  - **Why needed here:** The paper reports high F1 but highlights annotation inconsistencies (relative clauses). Understanding Micro-F1 and strict boundary matching is required to interpret the 77.4% result correctly.
  - **Quick check question:** In a strict evaluation, if the gold label is "the man" but the model predicts "man", is this a True Positive, False Positive, or False Negative? (Answer: FP + FN, strict matching requires exact span overlap).

## Architecture Onboarding

- **Component map:** FrameNet DB -> Prompt Generator (Jinja2) -> LLM Interface -> Evaluator
- **Critical path:** The Prompt Generator → LLM Interface. Specifically, the selection of "shots" (examples). The paper shows that selecting the right number of shots (e.g., 100 for FI, 150 for FSRL) is the primary lever for performance (Section 5).
- **Design tradeoffs:**
  - *Single-Step (FSP) vs. Two-Step (FI → FSRL):*
    - Single-Step: Lower latency (1 call), but lower accuracy and limited shots per frame due to context window limits.
    - Two-Step: Higher latency (2 calls), but higher accuracy and dynamic prompting (Section 3.1).
  - **Shot Count:** Adding shots improves precision/recall but increases token cost and latency. The paper suggests diminishing returns after ~150 shots for FSRL (Fig 3).
- **Failure signatures:**
  - **Relative Clause Ambiguity:** Models struggle to match the exact boundaries of gold annotations when relative clauses are involved (Section 5.1, Fig 5).
  - **Zero-Shot Collapse:** Removing definitions/examples causes performance to drop to near zero (Section 5.5, Table 5, Col 'a').
- **First 3 experiments:**
  1. **Reproduce Ablation (Table 5):** Run the FSP task with 0 shots, removing the "Events" section (definitions). Verify that F1 drops to 0.0 to confirm the mechanism of "Schema Alignment."
  2. **Scale Test (Context Limit):** Attempt the FSP task (single-step) with all 6 frames and 150 shots per frame. Observe if the prompt exceeds context limits or if performance degrades due to "diluted" context.
  3. **Boundary Consistency Check:** Run inference on 20 examples containing relative clauses. Compare model output boundaries vs. Gold boundaries manually to quantify the "Strict Evaluation" gap described in Section 5.1.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can ICL-based FSP scale effectively to the full set of 1,220+ FrameNet frames, or does performance degrade significantly beyond small frame subsets?
- **Open Question 2:** How should FSP evaluation metrics account for systematic annotation inconsistencies in FrameNet, particularly regarding relative clause boundaries and missing frame elements?
- **Open Question 3:** How does ICL performance vary across semantically distinct frame types beyond violent events?
- **Open Question 4:** Does batching multiple test items in a single prompt significantly degrade FSP accuracy compared to single-item processing?

## Limitations
- Experiments limited to six violent event frames due to prompt size constraints, limiting generalizability
- Strict boundary matching evaluation may penalize valid alternative annotations
- Performance depends heavily on selecting optimal shot counts, which appear empirically tuned
- Task decoupling assumes perfect frame identification (94.3% F1) without testing error propagation

## Confidence
- **High Confidence:** ICL performs competitively vs fine-tuning for this specific task/domain; Schema alignment via definition-example pairing is necessary; Task decoupling improves argument extraction
- **Medium Confidence:** Performance is primarily determined by shot count selection; Strict formatting guidelines ensure parseable outputs; Relative clause boundary detection represents the main failure mode
- **Low Confidence:** Results generalize to non-violent frames or full FrameNet; 94.3% FI accuracy assumption holds across all sentence types; Performance ceiling is determined by LLM pretraining vs. prompt quality

## Next Checks
1. **Metric Robustness Test:** Evaluate the same 100 test sentences using a relaxed boundary matching criterion (partial overlap) to quantify how much performance drop is due to strict evaluation vs. model limitations.
2. **Frame Generalization Test:** Apply the best-performing ICL configuration to 3-5 non-violent frames (e.g., Commerce, Motion, Communication) to assess whether the 77.4% FSRL F1 transfers beyond the tested domain.
3. **FI Error Propagation Test:** Manually examine 50 false negatives from the FI step to determine if these propagate to FSRL errors, quantifying the actual impact of the 94.3% F1 assumption on end-to-end performance.