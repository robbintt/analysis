---
ver: rpa2
title: Vision Foundation Models for Domain Generalisable Cross-View Localisation in
  Planetary Ground-Aerial Robotic Teams
arxiv_id: '2601.09107'
source_url: https://arxiv.org/abs/2601.09107
tags:
- data
- images
- localisation
- network
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for cross-view localisation of planetary
  rovers using ground-view images and aerial maps, addressing the challenge of scarce
  real space data for training machine learning models. The approach uses dual-encoder
  deep neural networks with semantic segmentation via vision foundation models (LLMDet
  + SAM 2) to bridge the synthetic-real domain gap.
---

# Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams

## Quick Facts
- **arXiv ID:** 2601.09107
- **Source URL:** https://arxiv.org/abs/2601.09107
- **Reference count:** 27
- **Primary result:** Achieved accurate cross-view localisation on planetary analogue data using synthetic training data with vision foundation model segmentation

## Executive Summary
This paper presents a method for absolute localisation of planetary rovers using ground-view images matched to aerial maps. The key innovation is using vision foundation models (LLMDet + SAM 2) for semantic segmentation to bridge the synthetic-to-real domain gap, enabling training on synthetic data while achieving accurate localisation on real planetary analogue data. The system employs a dual-encoder transformer network with triplet loss for cross-view matching, combined with a particle filter for temporal state estimation. The approach demonstrates accurate localisation performance with the segmentation-based method outperforming manual segmentation baselines.

## Method Summary
The method uses a dual-encoder transformer network trained on synthetic data with semantic segmentation via vision foundation models to achieve cross-view localisation on real planetary analogue data. Ground-view images are processed through LLMDet (object detection with rock prompt) and SAM 2 (segmentation) to create rock masks, which are then matched to corresponding aerial patches using learned embeddings. A particle filter integrates sequential measurements over time for robust state estimation. The network is trained using triplet loss on synthetic mask pairs, then transferred to real data without fine-tuning.

## Key Results
- Achieved 0.12m median distance error with LLMDet + SAM 2 segmentation vs 0.10m with manual segmentation
- Demonstrated 80.2% top-20% matching accuracy on real masked validation data vs 46.9% for RGB-only synthetic training
- Successfully tracked rover trajectories across complex paths in planetary analogue facility
- Processing time of 15 seconds per image exceeds 1Hz sampling rate, highlighting computational constraints

## Why This Works (Mechanism)

### Mechanism 1: Semantic Segmentation for Domain Transfer
- **Claim:** Semantic segmentation via foundation models reduces the synthetic-to-real domain gap by transforming domain-specific RGB images into domain-invariant rock masks.
- **Mechanism:** Raw RGB images from both synthetic and real sources are processed through LLMDet followed by SAM 2, producing boolean masks that encode only rock geometry regardless of lighting, texture, or color differences across domains.
- **Core assumption:** Rock shapes and spatial distributions are sufficiently similar between synthetic and real environments that mask-level representations transfer across domains.
- **Evidence anchors:** Synthetic-mask-trained network achieves 80.2% top-20% matching on real masked validation data vs. 46.9% for RGB-only synthetic training.
- **Break condition:** If real terrain has significantly different rock size distributions, shapes, or densities than synthetic training data, the mask-based approach may fail.

### Mechanism 2: Dual-Encoder Network Learning
- **Claim:** Dual-encoder transformer networks learn cross-view embeddings by optimizing triplet loss, enabling ground-view images to be matched to corresponding aerial patches.
- **Mechanism:** Separate transformer encoders process ground-view and aerial-view images into normalized 32-dimensional embeddings. The network is trained with soft-margin triplet loss that minimizes distance between matching pairs while maximizing distance to non-matching negatives.
- **Core assumption:** The spatial correspondence between ground-view camera position and aerial-patch location is learnable through the defined geometric relationship.
- **Evidence anchors:** Formal triplet loss definition with soft-margin formulation and established dual-encoder approach for terrestrial cross-view localisation.
- **Break condition:** If the geometric assumption about camera-to-aerial-patch alignment does not hold, embedding similarity may not correlate with correct localisation.

### Mechanism 3: Particle Filter Integration
- **Claim:** Particle filter state estimation integrates sequential network measurements to maintain temporally consistent localisation estimates despite single-frame ambiguity.
- **Mechanism:** Particles representing rover state hypotheses are propagated using a constant-velocity motion model. At each timestep, aerial patches are extracted for each particle, embeddings are computed, and particle weights are updated based on cosine similarity.
- **Core assumption:** The motion model provides a reasonable prior for state propagation between measurements.
- **Evidence anchors:** Full particle filter formulation with state space, prediction, and measurement update equations showing successful tracking across six validation trajectories.
- **Break condition:** If initial particle distribution is far from true state or motion model variance is mismatched to actual rover dynamics, particles may diverge from true trajectory.

## Foundational Learning

- **Concept: Triplet Loss / Metric Learning**
  - Why needed here: The dual-encoder network is trained using triplet loss to learn embeddings where similar cross-view pairs are close and dissimilar pairs are far.
  - Quick check question: Given three embeddings (anchor, positive, negative), would increasing the margin α in the soft-margin triplet loss make the network more or less strict about separating positives from negatives?

- **Concept: Particle Filters (Sequential Monte Carlo)**
  - Why needed here: The system's temporal state estimation relies entirely on particle filtering.
  - Quick check question: If particle weights become highly concentrated on a single particle after a few timesteps but localisation is incorrect, what specific filter operation should you investigate?

- **Concept: Vision Transformers (ViT-style architectures)**
  - Why needed here: The dual-encoder backbone uses transformer blocks with patch embeddings, positional encodings, and class tokens.
  - Quick check question: In the described architecture, what is the role of the class token, and what would happen if positional encodings were removed?

## Architecture Onboarding

- **Component map:** Ground-image capture → LLMDet+SAM2 segmentation → embedding computation → particle filter weight update → state estimate
- **Critical path:** Ground-image capture → LLMDet+SAM2 segmentation → embedding computation → particle filter weight update → state estimate. Latency bottleneck is segmentation (811ms mean per image) + per-particle inference (30ms × 500 particles = ~15s total per timestep).
- **Design tradeoffs:**
  - Precision vs. recall in segmentation: High precision (0.934) prioritized over recall (0.671) because false positives are more damaging to localisation than false negatives.
  - Synthetic-only training vs. real-data fine-tuning: Synthetic masks enable zero-real-label training but with higher error than fine-tuned networks.
  - Particle count vs. compute: 500 particles provide robust coverage but result in 15s processing time, exceeding 1Hz sampling rate.
- **Failure signatures:**
  - Complete divergence: Particle cloud spreads or converges to wrong location.
  - High single-run bearing error: Run E shows elevated bearing error compared to other runs.
  - Lost-in-space failure: Network fails to converge from uniform grid initialization in some runs.
- **First 3 experiments:**
  1. Validate segmentation on your target domain: Run LLMDet+SAM2 pipeline on sample images; compute precision/recall vs. manual masks.
  2. Establish baseline network performance: Train dual-encoder on synthetic RGB only, evaluate on held-out synthetic pairs.
  3. Test particle filter with known-good initialisation: Initialize particles in tight cloud around ground-truth start position on a single trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the absolute localisation estimates from the cross-view network be optimally fused with relative visual odometry to correct drift while handling the non-Gaussian noise characteristics of the deep learning measurement model?
- Basis in paper: The conclusion states that "valuable future work would include... integrating this absolute localisation method with traditional odometry-based relative localisation."
- Why unresolved: The current method relies on a particle filter driven solely by the cross-view network and a constant velocity motion model, without incorporating the incremental motion estimates typically provided by visual odometry on planetary rovers.
- What evidence would resolve it: Experiments showing reduced localisation error and trajectory smoothness when combining the cross-view particle filter updates with visual odometry constraints.

### Open Question 2
- Question: Can the network architecture and foundation model pipeline be compressed or quantized to achieve real-time inference speeds on current space-qualified radiation-hardened processors?
- Basis in paper: The paper notes that valuable future work includes "reducing network size and complexity for better inference times on space-grade hardware," acknowledging that the current 15-second processing time per image on an RTX 2080 Ti exceeds the 1 Hz sampling rate.
- Why unresolved: The reliance on large foundation models (LLMDet, SAM 2) and a Vision Transformer backbone requires substantial compute resources incompatible with existing space-certified hardware.
- What evidence would resolve it: A study reporting frame rates and localization accuracy after applying model distillation, pruning, or quantization to the segmentation and localization networks.

### Open Question 3
- Question: What specific geometric or semantic ambiguities caused the failure of the segmentation-based method in "Run A" during the lost-in-space experiment, and can the feature representation be modified to prevent divergence?
- Basis in paper: Figure 10 and Section V-C-3 show that while the fine-tuned RGB network succeeded, the "Mask-LLMDet+SAM" method "diverges for the third (run A)" in a lost-in-space scenario.
- Why unresolved: The paper demonstrates the failure but does not analyze whether the divergence was due to a lack of rock features, repetitive semantic masks, or an initialization issue specific to that trajectory's starting location.
- What evidence would resolve it: A breakdown of the feature embeddings for Run A compared to successful runs, determining if the segmentation masks lack the necessary entropy to distinguish the starting location.

## Limitations
- Domain gap validity remains unverified beyond this specific planetary analogue facility
- Generalisation to alternative visual landmarks (craters, boulders) requires validation
- Lost-in-space robustness frequency and recovery strategies are not fully characterised

## Confidence
- **High confidence:** Core technical methodology follows established patterns in cross-view localisation research with appropriate planetary adaptation
- **Medium confidence:** Synthetic-to-real transfer via semantic segmentation shows promising results in tested planetary analogue facility but requires broader validation
- **Low confidence:** Claims about performance in truly unstructured planetary environments extend beyond empirical evidence provided

## Next Checks
1. **Cross-terrain validation:** Test the complete pipeline on at least two additional planetary analogue sites with significantly different rock distributions, sizes, and densities to verify domain invariance claims.
2. **Alternative landmark evaluation:** Replace rock segmentation with crater or boulder detection on the same dataset to quantify performance sensitivity to landmark choice.
3. **Initialisation robustness testing:** Systematically evaluate lost-in-space performance across all validation trajectories with randomised initial particle distributions to characterise failure rates and recovery capabilities.