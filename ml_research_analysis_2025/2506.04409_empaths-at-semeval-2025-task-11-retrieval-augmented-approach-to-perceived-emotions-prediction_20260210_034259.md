---
ver: rpa2
title: 'Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived
  Emotions Prediction'
arxiv_id: '2506.04409'
source_url: https://arxiv.org/abs/2506.04409
tags:
- emotion
- majority
- label
- vote
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EmoRAG, a retrieval-augmented system for perceived
  emotion detection in multilingual text. The approach uses a database of training
  examples, a retriever to fetch similar examples, and an ensemble of large language
  models to predict emotions.
---

# Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction

## Quick Facts
- **arXiv ID**: 2506.04409
- **Source URL**: https://arxiv.org/abs/2506.04409
- **Reference count**: 2
- **Primary result**: Retrieval-augmented emotion detection system achieves 0.638 F1-micro and 0.590 F1-macro across 28 languages without model fine-tuning

## Executive Summary
This paper presents EmoRAG, a retrieval-augmented system for perceived emotion detection in multilingual text. The approach uses a database of training examples, a retriever to fetch similar examples, and an ensemble of large language models to predict emotions. Predictions are aggregated using label-specific F1-weighted voting. The system achieves strong results across 28 languages in the SemEval-2025 Task 11, with an average F1-micro score of 0.638 and F1-macro of 0.590, outperforming several monolingual baselines without requiring model fine-tuning.

## Method Summary
The EmoRAG system employs a retrieval-augmented approach for multilingual emotion detection. It constructs a database of training examples, uses a retriever to fetch similar examples for each input text, and employs an ensemble of large language models to predict emotions. The final predictions are determined through label-specific F1-weighted voting across the ensemble. This approach achieves strong performance across 28 languages without requiring model fine-tuning, demonstrating the effectiveness of retrieval augmentation for cross-lingual emotion detection tasks.

## Key Results
- Achieved 0.638 F1-micro and 0.590 F1-macro across 28 languages
- Outperformed several monolingual baselines without fine-tuning
- Demonstrated effectiveness of retrieval-augmented approach for multilingual emotion detection

## Why This Works (Mechanism)
The retrieval-augmented approach works by leveraging semantic similarity between input texts and a curated database of training examples. When presented with new text, the retriever identifies relevant examples that share contextual and emotional characteristics, providing the LLM ensemble with additional context beyond the input alone. This contextual enrichment helps models better understand subtle emotional cues, especially in multilingual settings where direct translation may miss cultural or linguistic nuances. The label-specific F1-weighted voting mechanism ensures that predictions are calibrated based on each model's historical performance on specific emotion categories, improving overall robustness and accuracy.

## Foundational Learning
- **Semantic similarity retrieval**: Understanding how text similarity measures work is essential because the system relies on finding relevant training examples to augment predictions. Quick check: Verify that cosine similarity or other metrics effectively capture emotional content across languages.
- **Ensemble learning with weighted voting**: The aggregation mechanism requires understanding how individual model strengths can be combined based on historical performance. Quick check: Confirm that F1-weighted voting outperforms simple majority voting on validation sets.
- **Multilingual emotion detection**: Recognizing that emotional expressions vary across languages and cultures is crucial for interpreting the system's cross-lingual performance. Quick check: Test whether the system performs consistently across different language families.

## Architecture Onboarding
- **Component map**: Input text -> Retriever -> Training example database -> LLM ensemble -> Label-specific F1-weighted voting -> Final emotion predictions
- **Critical path**: The retriever must efficiently find relevant examples, the LLM ensemble must generate consistent predictions, and the voting mechanism must properly weight contributions from different models.
- **Design tradeoffs**: Static training database vs. dynamic updates, multiple LLM calls vs. computational efficiency, complex voting vs. simpler aggregation methods.
- **Failure signatures**: Poor retrieval relevance leading to misleading context, model disagreement causing unstable voting, language-specific performance drops indicating cultural/emotional expression gaps.
- **3 first experiments**: 1) Test retrieval relevance scores on held-out examples, 2) Compare ensemble voting vs. single best model performance, 3) Evaluate system on out-of-distribution emotional expressions.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on single dataset without cross-validation, limiting confidence in reported metrics
- Static training database may not generalize to emerging emotional expressions or out-of-distribution texts
- Computational overhead of retrieval step and multiple LLM calls not addressed for real-world deployment

## Confidence
- Medium - The reported results are based on a single evaluation and lack extensive ablation studies or cross-validation, limiting generalizability claims.

## Next Checks
1. Conduct k-fold cross-validation on the same dataset to assess model stability and check for overfitting
2. Perform ablation studies removing the retrieval component to quantify its contribution versus the LLM ensemble alone
3. Test the system on an external multilingual emotion dataset to evaluate generalization beyond the original test set