---
ver: rpa2
title: 'AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified
  Representations'
arxiv_id: '2501.09954'
source_url: https://arxiv.org/abs/2501.09954
tags:
- design
- space
- learning
- hardware
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient design space exploration
  (DSE) for hardware accelerators in AI workloads, where the design space is highly
  non-uniform, non-convex, and growing exponentially. Traditional DSE techniques rely
  on iterative search-based methods that are time-consuming and often fail to find
  optimal solutions.
---

# AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified Representations

## Quick Facts
- arXiv ID: 2501.09954
- Source URL: https://arxiv.org/abs/2501.09954
- Authors: Jamin Seo; Akshat Ramachandran; Yu-Chuan Chuang; Anirudh Itagi; Tushar Krishna
- Reference count: 36
- One-line primary result: AIRCHITECT V2 achieves 15% higher accuracy in identifying optimal design points and 1.7× latency reduction compared to existing DSE techniques

## Executive Summary
This paper addresses the challenge of efficient design space exploration (DSE) for hardware accelerators in AI workloads, where the design space is highly non-uniform, non-convex, and growing exponentially. Traditional DSE techniques rely on iterative search-based methods that are time-consuming and often fail to find optimal solutions. The paper proposes AIRCHITECT V2, a learning-based approach that uses a transformer-based encoder-decoder model combined with contrastive learning and a novel Unified Ordinal Vector (UOV) representation. The contrastive learning encodes the complex design space into a uniform intermediate representation, while the UOV blends classification and regression benefits to effectively explore large DSE spaces without sacrificing accuracy. Experimental results show that AIRCHITECT V2 outperforms existing techniques by 15% in identifying optimal design points and achieves a 1.7x improvement in inference latency on identified hardware architectures for unseen model workloads.

## Method Summary
AIRCHITECT V2 is a two-stage learning framework that decouples feature extraction from hardware prediction. In Stage 1, a transformer encoder learns a uniform embedding space using contrastive learning (infoNCE loss) to cluster similar workloads and an auxiliary performance head for latency prediction. In Stage 2, the frozen encoder's features are decoded to predict hardware configurations using the Unified Ordinal Vector (UOV) representation, which discretizes continuous design parameters into ordered buckets. The model is trained on 100K synthetically generated (workload, optimal hardware) pairs from ConfuciuX/MAESTRO and evaluated on 20K held-out samples, achieving layer-level prediction accuracy as the primary metric.

## Key Results
- 15% improvement in identifying optimal design points compared to existing techniques
- 1.7× improvement in inference latency on identified hardware architectures for unseen model workloads
- Achieves 91% accuracy (vs 79% for AIRCHITECT v1) through contrastive learning and UOV representation

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Smoothing of the Latent Space
The authors claim that contrastive learning transforms a non-uniform, non-convex performance landscape into a uniform, smooth intermediate representation, making the design space easier for the decoder to navigate. An infoNCE loss is applied during Stage 1 training, pulling "positive" samples (workloads sharing the same design class/bucket) closer in the embedding space while pushing "negative" samples (different classes) apart. This regularization prevents the model from overfitting to the long-tailed distribution of the training data. The assumption is that organizing the latent space by class similarity creates a continuous manifold where interpolation between design points is meaningful and leads to valid hardware configurations.

### Mechanism 2: Unified Ordinal Vectors (UOV) for Hybrid Regression
The paper claims that the UOV output representation outperforms pure classification (limited scalability) and pure regression (unconstrained/low accuracy) by blending the two. UOV discretizes the continuous output variable (e.g., PE count) into $K$ ordered buckets. The model outputs a vector where values increase monotonically up to the target bucket and are zero beyond it. The loss function penalizes errors in bucket classification (discrete) and the specific value within the bucket (continuous). The assumption is that hardware design parameters have an ordinal nature where proximity matters, which standard one-hot classification ignores.

### Mechanism 3: Two-Stage Training Decoupling
The authors claim that separating the training process into an "embedding stage" (encoder) and a "prediction stage" (decoder) improves generalization over end-to-end joint training. The encoder is first trained to converge on a robust feature space using the contrastive loss. The encoder weights are then frozen, and the decoder is trained specifically to map those fixed features to hardware configs. This prevents the "noise" of the specific output regression task from disrupting the formation of the general feature space. The assumption is that a good representation of the workload can be learned independently of the specific hardware mapping function, similar to pre-training in NLP.

## Foundational Learning

- **Concept: Design Space Exploration (DSE)**
  - Why needed here: This is the core problem domain. Understanding that hardware design spaces are "non-convex" and "long-tailed" is essential to grasp why simple search (like gradient descent) fails and why the authors had to invent a specific learning architecture.
  - Quick check question: Why does a "long-tailed" distribution of optimal designs make standard supervised learning difficult for DSE?

- **Concept: Contrastive Learning (Self-Supervised)**
  - Why needed here: This is the engine of the "Stage 1" encoder. Without understanding how positive/negative pairs pull clusters apart, the mechanism for creating a "uniform representation" remains opaque.
  - Quick check question: In the context of this paper, what defines a "positive" pair vs. a "negative" pair during the encoder training?

- **Concept: Ordinal Regression**
  - Why needed here: The "Unified Ordinal Vector" is a form of ordinal regression. Recognizing this helps explain why the model handles "buckets" (classification) and "values" (regression) simultaneously.
  - Quick check question: How does the UOV representation encode the value "7" differently than a standard one-hot encoded vector representing class 7?

## Architecture Onboarding

- **Component map:** Inputs (Workload + Dataflow) -> Transformer Encoder -> Contrastive Head + Performance Head -> Fixed Features -> Transformer Decoder -> UOV Heads (PE count, Buffer size) -> Output Buckets

- **Critical path:**
  1. Dataset Generation: Use ConfuciuX/MAESTRO to generate (Workload → Optimal HW) pairs
  2. Preprocessing: Discretize output targets (PEs, Buffers) into UOV buckets (K=16)
  3. Stage 1 Training: Train Encoder with Contrastive Loss (L_C) + Performance Loss (L_perf)
  4. Stage 2 Training: Freeze Encoder. Train Decoder + UOV Heads using Unification Loss (L_o)

- **Design tradeoffs:**
  - Bucket Count (K): Higher K → finer resolution but larger model size/dimensionality (Paper settles on 16)
  - Embedding Dimension: Must be large enough to separate thousands of workload classes but small enough for inference speed
  - Architecture: Transformer (high capacity) vs. MLP (AIRCHITECT v1, faster but lower accuracy)

- **Failure signatures:**
  - Latent Collapse: Visualization shows all embeddings clustering into a single ball (Contrastive loss weight too low or temperature τ wrong)
  - Bucket Saturation: Model predicts the same bucket for every input (typically the most frequent bucket in the long-tailed distribution)
  - Regression Drift: Correct bucket predicted, but regressed value within bucket is nonsensical (Unification loss weighting issue)

- **First 3 experiments:**
  1. Overfit Sanity Check: Train on a small subset (e.g., 1k samples) to verify the model can overfit (reach ~100% accuracy) on training data, ensuring the UOV decoding logic is implemented correctly
  2. Embedding Visualization (t-SNE): After Stage 1, plot the latent space colored by UOV bucket. Verify that it looks like Figure 5b (distinct clouds) rather than 5a (random noise)
  3. K-Sweep: Run inference varying K (e.g., 4, 8, 16, 32) to replicate Figure 8b and identify the accuracy saturation point for your specific target workload set

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Unified Ordinal Vector (UOV) representation effectively support multi-objective optimization (e.g., joint energy and latency minimization) rather than single-metric optimization?
  - Basis in paper: Section III-B states the model outputs configurations "geared towards improving overall latency and/or energy," yet the experimental results (§IV-C) and the loss function (L_perf) focus exclusively on minimizing latency
  - Why unresolved: The UOV output scheme maps inputs to a specific "optimal" configuration bucket based on a single ordering. Simultaneous multi-objective optimization would require the model to predict a Pareto frontier or a conditional output based on variable weights, which the current single-head decoder structure does not address
  - What evidence would resolve it: An evaluation where the model is trained with a weighted cost function (e.g., Energy × Latency) or a multi-head output for different objectives, demonstrating distinct predicted configurations for different objective weights

- **Open Question 2:** How sensitive is the contrastive learning embedding space to the accuracy and biases of the ground-truth data generator (ConfuciuX)?
  - Basis in paper: Section III-A notes the dataset is generated by "executing ConfuciuX [12]... with the optimization metric (i.e. reward) set as latency"
  - Why unresolved: ConfuciuX is a Reinforcement Learning (RL) agent which may converge to local optima. If the training labels (ground-truth optimal designs) contain noise or sub-optimal biases, the contrastive learning stage might reinforce these errors by clustering around "false" positives, limiting the student model's potential to surpass the teacher
  - What evidence would resolve it: A comparison of model accuracy when trained on a subset of data verified by exhaustive search versus the standard RL-generated dataset, or an analysis of the variance in the embedding space for conflicting data points

- **Open Question 3:** Can the method generalize to hardware-mapping co-optimization, where the mapping strategy is a predicted output rather than a fixed input?
  - Basis in paper: Table I explicitly lists "dataflow" as an Input feature, while the Output is restricted to hardware resources (PE count, Buffer size). Section I notes that performance depends on the "complex interaction between mapping strategies and allocated hardware resources"
  - Why unresolved: The current formulation assumes a fixed mapping provided as input to predict the best hardware. Real-world DSE often requires finding the optimal pair of hardware and mapping simultaneously, which would significantly expand the output design space complexity
  - What evidence would resolve it: An extension of the model where mapping parameters (e.g., tiling factors) are included in the UOV output representation, and an evaluation of the model's ability to predict valid, high-performance mappings for unseen workloads

## Limitations
- Dataset Generality: The ConfuciuX/MAESTRO dataset is based on synthetic workload generation; real-world workloads may exhibit different patterns that the learned embeddings cannot capture
- Transformer Hyperparameters: Key architectural details (number of layers, attention heads, hidden dimensions) are not specified, making exact reproduction challenging
- Scalability of UOV: While UOV improves accuracy for the studied design space (~10^9), the bucketing approach may not scale to orders-of-magnitude larger spaces

## Confidence
- High Confidence: The core contribution of contrastive learning + UOV for non-convex DSE is technically sound and well-supported by ablation studies
- Medium Confidence: Claims of 15% accuracy improvement and 1.7× latency reduction are specific to the ConfuciuX-generated dataset and MAESTRO simulation; real hardware validation is not provided
- Low Confidence: Claims about outperforming LLM-based approaches (LLM-DSE) are based on comparisons to published results rather than direct benchmarking

## Next Checks
1. Cross-Dataset Validation: Apply the trained model to a held-out dataset of real DNN workloads (e.g., from MLPerf) to test generalization beyond synthetic ConfuciuX samples
2. Hyperparameter Sensitivity Analysis: Systematically vary K (4→32), τ (0.1→0.7), and α (0.5→0.9) to map the accuracy-latency trade-off and identify robust operating points
3. Ablation on Real Hardware: Deploy the top-3 recommended accelerator configurations from AIRCHITECT v2 on actual FPGA/ASIC prototypes to verify MAESTRO-predicted latency improvements