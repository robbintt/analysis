---
ver: rpa2
title: Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection
arxiv_id: '2507.04468'
source_url: https://arxiv.org/abs/2507.04468
tags:
- prompt
- sarcasm
- text
- prompts
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DMDP, a prompt tuning framework for few-shot\
  \ multimodal sarcasm detection. The key innovation is using modality-disentangled\
  \ prompts\u2014separate text and visual prompts injected across multiple layers\
  \ of CLIP's encoders\u2014along with a cross-modal prompt alignment module and a\
  \ gating mechanism to stabilize training."
---

# Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection

## Quick Facts
- arXiv ID: 2507.04468
- Source URL: https://arxiv.org/abs/2507.04468
- Reference count: 40
- Primary result: DMDP achieves up to 2.8% higher accuracy than baselines using 13× fewer parameters in few-shot multimodal sarcasm detection

## Executive Summary
This paper introduces DMDP, a prompt tuning framework for few-shot multimodal sarcasm detection that uses modality-disentangled prompts—separate text and visual prompts injected across multiple layers of CLIP's encoders—along with a cross-modal prompt alignment module and a gating mechanism to stabilize training. Experiments on MMSD and MMSD2.0 datasets show DMDP outperforms all baselines in few-shot and extremely low-resource settings, achieving up to 2.8% higher accuracy and using over 13× fewer parameters than large vision-language models. Cross-dataset evaluations demonstrate strong generalization to new domains.

## Method Summary
DMDP employs gated, modality-specific deep prompts for text and visual encoders in CLIP. It introduces separate learnable tokens for text and vision, injected into multiple transformer layers (up to S=9). A cross-modal alignment layer projects text prompts to visual space, ensuring coherent representations. A gating mechanism initialized near zero stabilizes training by gradually introducing prompt influence. The WeightMod module computes modality-specific weights based on frozen CLIP embeddings to emphasize relevant modalities for each input.

## Key Results
- DMDP achieves 2.8% higher accuracy than SOTA models on MMSD 1% few-shot split
- Uses 13.4× fewer parameters than Flamingo and 3.2× fewer than CLIP-MOE
- Strong cross-dataset generalization: 64.3% accuracy on RedEval when trained on MMSD 1%
- Gating mechanism stabilizes training, starting from near-zero influence and gradually increasing

## Why This Works (Mechanism)

### Mechanism 1: Modality-Disentangled Prompts
- **Claim:** Separate text and visual prompts capture distinct sarcasm patterns that unified prompts miss
- **Core assumption:** Sarcasm manifests in three distinct patterns requiring modality-specific processing: (1) image-text incongruence, (2) image-driven sarcasm, (3) text-driven sarcasm
- **Evidence:** Ablation shows unified prompts cause Acc drop of 2.7% (MMSD) and 2.8% (MMSD2.0)

### Mechanism 2: Deep Prompt Injection with Layer-wise Sharing
- **Claim:** Injecting prompts across multiple encoder layers enables hierarchical feature learning for subtle sarcasm signals
- **Core assumption:** Different CLIP encoder layers capture progressively abstract representations; earlier layers handle low-level features while later layers integrate complex patterns
- **Evidence:** Performance increases with depth up to layer 9, then degrades at layers 10-12

### Mechanism 3: Cross-Modal Prompt Alignment via Projection
- **Claim:** Projecting text prompts to visual space creates coherent cross-modal representations that detect incongruence
- **Core assumption:** Text provides stronger semantic grounding than images; projecting text→vision works better than vision→text
- **Evidence:** Reversing direction (V→T projection) causes larger drops: 1.1% and 1.7% accuracy

## Foundational Learning

- **Concept: Prompt Tuning vs. Fine-tuning**
  - **Why needed here:** DMDP freezes CLIP weights and only optimizes prompt tokens. Understanding this distinction is critical—fine-tuning would overfit on ~200 samples, while prompt tuning leverages pre-trained knowledge
  - **Quick check question:** Can you explain why updating only 2 prompt tokens per layer (c=2) generalizes better than updating encoder weights?

- **Concept: CLIP's Dual-Encoder Architecture**
  - **Why needed here:** DMDP modifies both text and vision encoders symmetrically. You need to understand how patch embeddings (vision) and word embeddings (text) flow through transformer blocks to trace prompt injection points
  - **Quick check question:** In CLIP, where do the final image and text representations come from, and how do they interact?

- **Concept: Gating for Training Stabilization**
  - **Why needed here:** Randomly initialized prompts introduce noise to frozen pre-trained weights. The gating mechanism (Equation 18) starts at g≈0 (τ=0.5) and learns to open gradually
  - **Quick check question:** Why is gating applied only to the first layer's text prompts, not all layers?

## Architecture Onboarding

- **Component map:** Input: (Text tokens T, Image patches I) → WeightMod: Computes r from frozen CLIP embeddings → Gated Text Prompts: $P^0_T = g \cdot r \cdot P^0_T$ → Projection Layer: $H_\theta$ maps $P^T$ → $P^V$ → Deep Prompt Injection: Layers 1-9 with sharing mechanism → Frozen CLIP Encoders: Process [prompts + input] → Classifier: Concatenate final embeddings, predict

- **Critical path:** The projection layer $H_\theta$ is the only cross-modal bridge. If this fails, text and vision prompts learn independently, missing incongruence detection. Trace: text prompt quality → projection accuracy → visual prompt relevance → final attention distribution

- **Design tradeoffs:**
  - Prompt length (c=2): Longer prompts overfit; shorter prompts under-specify. Paper found c=2 optimal
  - Prompt depth (S=9): Deeper = better until layer 9; layers 10-12 disrupt integration
  - Directionality: Text→Vision projection outperforms Vision→Text. Assumption: text provides cleaner semantic priors

- **Failure signatures:**
  - Over-reliance on keywords: Model predicts "sarcastic" when seeing "sarcasm" in image text, ignoring context
  - Missing contextual knowledge: Fails on sarcasm requiring real-world knowledge (e.g., Easter eggs in January)
  - High variance across few-shot splits: Expect ±1-3% accuracy variation; use 6 runs (3 seeds × 2 splits)

- **First 3 experiments:**
  1. Reproduce ablation on prompt depth: Train with S∈{1,3,6,9,12} on MMSD 1% split. Verify performance peaks at S=9
  2. Test projection directionality: Swap $P^V = H(P^T)$ for $P^T = H(P^V)$. Confirm ~1-2% accuracy drop
  3. Cross-dataset zero-shot: Train on MMSD2.0 1%, test on RedEval without adaptation. Target: >65% accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can DMDP be extended to effectively capture sarcasm that relies on implicit external social or temporal context?
- **Basis in paper:** The error analysis identifies "Limited recognition of contextual sarcasm" as a key failure mode, citing examples where understanding the joke requires social knowledge (e.g., seasonal availability of products) rather than just visual-textual incongruity
- **Why unresolved:** The current framework depends on the frozen CLIP backbone and prompt tuning, which restricts the model to the knowledge encoded during pre-training without a mechanism for accessing external or temporal facts
- **What evidence would resolve it:** Demonstrating a method that integrates a retrieval-augmented mechanism or external knowledge graph into the prompt tuning process to improve performance on context-dependent samples

### Open Question 2
- **Question:** Why does the direction of cross-modal prompt projection significantly impact performance, and can this asymmetry be mitigated?
- **Basis in paper:** Section 9.7 reports that projecting textual prompts to visual prompts works well, but reversing this (visual-to-text projection) causes a performance drop (Acc: ↓ 1.1% in MMSD)
- **Why unresolved:** The paper hypothesizes that text provides better "semantic grounding," but it does not investigate the structural reasons why visual features fail to guide textual prompts effectively in this specific architecture
- **What evidence would resolve it:** An analysis of the semantic alignment between the text and image latent spaces in CLIP, or the introduction of a symmetric projection mechanism that matches the performance of the text-to-visual approach

### Open Question 3
- **Question:** How can the model be regularized to avoid over-reliance on superficial salient keywords (e.g., the word "sarcasm" appearing in an image)?
- **Basis in paper:** The error analysis notes an "Over-reliance on salient keywords," where the model incorrectly predicts sarcasm if the word "sarcasm" appears in the image, even if the content is a genuine question
- **Why unresolved:** The current gating mechanism controls prompt influence but lacks a specific component to filter out "shortcut" features or explicitly penalize attention to misleading meta-keywords
- **What evidence would resolve it:** Ablation experiments utilizing adversarial training or attention masking to suppress reliance on these specific keywords, resulting in improved accuracy on adversarial or meta-referential samples

## Limitations

- Cross-modal projection directionality assumption (text→vision) is never rigorously justified beyond empirical observation
- Optimal prompt depth (S=9) is task-specific and likely varies across datasets and domains
- Gating mechanism only stabilizes text prompts while visual prompts remain uncontrolled, creating asymmetric treatment

## Confidence

- **High Confidence:** DMDP outperforms baselines in few-shot settings (Accuracy improvements of 1.2-2.8% across datasets are statistically significant and robust across 6 runs)
- **Medium Confidence:** Deep prompt injection (S=9) provides optimal performance (well-validated within MMSD/MMSD2.0 but likely task-dependent)
- **Low Confidence:** Cross-modal projection directionality (Text→Vision outperforms Vision→Text) and gating necessity (only one ablation exists, no comparison to alternative stabilization methods)

## Next Checks

1. **Directionality Test:** Swap the projection direction to P^T = H(P^V) and measure accuracy drop. A significant decrease (>1%) would confirm the claimed semantic hierarchy assumption

2. **Depth Sensitivity Analysis:** Systematically vary S from 1 to 12 on MMSD 1% split to verify the exact performance peak and degradation pattern matches Figure 6

3. **External Domain Generalization:** Train DMDP on MMSD 1% and evaluate on RedEval or other multimodal sarcasm datasets not used in training to test cross-dataset robustness claims