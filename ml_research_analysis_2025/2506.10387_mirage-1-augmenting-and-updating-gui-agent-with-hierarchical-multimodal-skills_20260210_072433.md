---
ver: rpa2
title: 'Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills'
arxiv_id: '2506.10387'
source_url: https://arxiv.org/abs/2506.10387
tags:
- skills
- task
- skill
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mirage-1, a GUI agent designed to address
  long-horizon task planning and domain gaps between offline and online environments.
  The agent employs a Hierarchical Multimodal Skills (HMS) module to progressively
  abstract trajectories into execution skills, core skills, and meta-skills, providing
  structured knowledge for complex task planning.
---

# Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills

## Quick Facts
- **arXiv ID**: 2506.10387
- **Source URL**: https://arxiv.org/abs/2506.10387
- **Reference count**: 40
- **Primary result**: 79% improvement on AndroidLH benchmark

## Executive Summary
Mirage-1 introduces a Hierarchical Multimodal Skills (HMS) module to address long-horizon task planning and domain gaps in GUI agents. The agent progressively abstracts trajectories into execution, core, and meta-skills, providing structured knowledge for complex task planning. To bridge the domain gap, it employs a Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm that leverages offline-acquired skills to reduce the action search space during online exploration. Experimental results demonstrate significant improvements across multiple benchmarks, with the most notable being a 79% improvement on the newly introduced AndroidLH benchmark.

## Method Summary
Mirage-1 builds a Hierarchical Multimodal Skills module from offline trajectories using GPT-4o to abstract raw actions into three skill tiers: Execution Skills (specific task instances), Core Skills (generalizable functions), and Meta Skills (high-level abilities). The agent employs Skill-Augmented Monte Carlo Tree Search (SA-MCTS) for online exploration, which uses these skills to guide sub-goal selection and reduce the search space. A Decision Reflector module validates proposed actions against historical trajectory patterns before execution. The system maintains a knowledge base of skills that grows as the agent explores new tasks, with regular refinement to maintain retrieval efficiency.

## Key Results
- Outperforms previous agents by 32% on AndroidWorld benchmark
- Achieves 19% improvement on MobileMiniWob++ benchmark
- Demonstrates 79% improvement on newly introduced AndroidLH benchmark
- Shows 15% improvement on Mind2Web-Live benchmark

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Skill Abstraction for Planning
The HMS module processes offline trajectories to create a three-tier hierarchy that allows generalization from specific historical actions to new long-horizon tasks. By abstracting Execution Skills into Core Skills and Meta Skills, the planner can decompose complex user goals into executable sub-goals using retrieved knowledge rather than planning from scratch.

### Mechanism 2: Skill-Guided Search Space Reduction
SA-MCTS integrates offline skills to reduce the search space by prioritizing high-value sub-goals over random exploration. During the Expansion phase, it samples sub-goals based on Core Skills rather than generating random actions, directing the search toward paths likely to succeed based on prior knowledge.

### Mechanism 3: Reflective Error Correction via Execution Skills
The Decision Reflector evaluates proposed actions against historical trajectory patterns (Execution Skills) before execution, preventing irreversible errors in dynamic GUI environments. It predicts state changes and assigns quality scores, allowing the system to reject low-quality actions before they cause problems.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**: Essential for understanding how Mirage-1 balances exploration and exploitation. *Quick check*: How does the UCB1 formula balance visiting promising nodes vs. unexplored nodes?

- **Hierarchical Task Networks (HTN)**: The HMS structure mirrors HTN planning by decomposing high-level abstract tasks into primitive actions. *Quick check*: What is the difference between a "primitive task" (Execution Skill) and a "compound task" (Core Skill) in this context?

- **Visual Grounding**: The Operator relies on this to translate text descriptions into screen coordinates. *Quick check*: How does the agent map the description "click the search bar" to specific (x,y) coordinates on a screenshot?

## Architecture Onboarding

- **Component map**: User Goal -> Planner retrieves Meta/Core Skills from HMS -> Planner outputs sub-goal -> Operator generates action description -> Reflector validates action vs. Execution Skills -> Grounding Model -> Click/Input on GUI

- **Critical path**: 1) User Goal to Planner retrieves skills from HMS, 2) Planner outputs sub-goal to Operator generates action description, 3) Reflector validates action against Execution Skills, 4) Grounding Model executes click/input on GUI

- **Design tradeoffs**: Inference Cost - multiple LLM calls per step increase latency; Generalization vs. Specificity - abstracting too much loses utility while abstracting too little harms generalization

- **Failure signatures**: Infinite Loops (Reflector repeatedly rejects Operator's actions), Skill Hallucination (Planner retrieves irrelevant skills), Domain Drift (SA-MCTS fails due to misleading priors)

- **First 3 experiments**: 1) Ablate HMS Tiers - run tasks using only Execution Skills to quantify hierarchical abstraction contribution, 2) Visualize MCTS Pruning - log search tree of SA-MCTS vs standard MCTS to verify skill prior reduces branching, 3) Latency Profiling - measure Decision Reflector time cost for real-time viability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the computational overhead of the SA-MCTS algorithm be reduced for real-time deployment?
- **Basis**: Section A explicitly states computational overhead in inference processing
- **Why unresolved**: Iterative tree search and skill retrieval introduce latency
- **What evidence would resolve it**: Benchmarking inference latency against success rates with pruning heuristics

### Open Question 2
- **Question**: To what extent does decoupling the planner and grounding model introduce bottlenecks?
- **Basis**: Section A identifies architectural limitation where grounding misinterpretation leads to positioning errors
- **Why unresolved**: Modular design uses off-the-shelf models that may not align with planner intent
- **What evidence would resolve it**: Comparing modular approach against end-to-end trained agent

### Open Question 3
- **Question**: How does HMS maintain retrieval accuracy as the memory bank scales?
- **Basis**: Section 3.3 mentions regular refinement, Section 4.6 shows sensitivity to skill sources
- **Why unresolved**: Growing skill library may degrade retrieval precision over time
- **What evidence would resolve it**: Analysis of retrieval accuracy over long horizons with expanded HMS

## Limitations
- Computational overhead in inference processing due to iterative tree search
- Architectural bottleneck where grounding model misinterpretation causes positioning errors
- Potential skill collision and retrieval accuracy degradation as HMS scales

## Confidence
- **High**: Three-tier skill hierarchy concept is clearly defined and logically sound
- **Medium**: SA-MCTS framework with skill-guided sub-goal sampling is technically valid but depends on accurate priors
- **Low**: Performance gains on AndroidLH and Mind2Web-Live are plausible but not fully verifiable without skill modules and hyperparameters

## Next Checks
1. **Ablation of HMS Abstraction**: Run AndroidLH tasks using only Execution Skills to isolate contribution of hierarchical reasoning to long-horizon success

2. **SA-MCTS Search Space Analysis**: Log and visualize MCTS tree expansion for both SA-MCTS and standard MCTS on novel task to verify skill priors reduce branching factor

3. **Reflective Latency Measurement**: Profile Decision Reflector time cost including LLM calls for quality scoring to determine if overhead is acceptable for real-time mobile interactions